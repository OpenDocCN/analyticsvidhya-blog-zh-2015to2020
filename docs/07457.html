<html>
<head>
<title>Recurrent Neural Network and it’s variants….</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络及其变体。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/recurrent-neural-network-and-its-variants-de75f9ee063?source=collection_archive---------7-----------------------#2020-06-26">https://medium.com/analytics-vidhya/recurrent-neural-network-and-its-variants-de75f9ee063?source=collection_archive---------7-----------------------#2020-06-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/586760be84d223e0ad7ffa11c45395d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFOzE0TEMFERg3G5_5HiPA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">RNN及其变体。(<a class="ae iu" href="http://dprogrammer.org/" rel="noopener ugc nofollow" target="_blank">d程序员洛佩兹</a>)</figcaption></figure><p id="6774" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">人类在阅读的时候，基于他们之前对单词的理解来理解每个单词。我们不会忘记所有的信息，从零开始思考。RNN的工作也是如此，通过保存先前的时间信息并预测下一个时间。</em></p><p id="d022" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">在本帖中，我们将从了解RNN是什么以及它是如何工作的开始。我们将讨论LSTM和GRU，它们是RNN的其他变体。我们将了解标准RNN有什么问题:爆炸梯度，消失梯度等，以及如何克服这一切。</strong></p><p id="4b43" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di"> S </span> <strong class="ix hj"> <em class="jt">标准递归神经网络:</em> </strong></p><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es kd"><img src="../Images/1cf60b07e4e1d3f8b772b4c9e6440c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*AcY9CMkX-AfpVKzm94c2VQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">典型的RNN街区</figcaption></figure><p id="2da4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们首先了解RNN的一个街区。如上图所示，我们有一个RNN区块。不同于神经网络，RNN具有两个输入<strong class="ix hj"> <em class="jt"> xt </em> </strong>和<strong class="ix hj"> <em class="jt"> x(t-1) </em> </strong>，其中<strong class="ix hj"> <em class="jt"> xt </em> </strong>是当前时间步长输入，其可以是句子的单词、单词的字符、音频的赫兹等。<strong class="ix hj"><em class="jt">【x(t-1)</em></strong>是先前的时间步长输入，这不过是先前的RNN块激活。每个输入都有自己的权重，这里我们用权重<strong class="ix hj"> <em class="jt"> Wx </em> </strong>为<strong class="ix hj"> <em class="jt"> xt </em> </strong>和<strong class="ix hj"> <em class="jt"> Wa </em> </strong>为<strong class="ix hj"><em class="jt">【x【t-1】</em></strong>。这里，我们有上面讨论的块的以下两个等式:</p><p id="1a1c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">方程一:a(t)= tanh(wx . x(t)+wa . x(t-1)+ba)<br/>方程二:y(t)= soft max(wy . a(t)+by)</em></strong></p><p id="cd5d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的等式1中可以看出，我们对两个不同时间步长的权重和输入的点积求和，并添加偏差。然后，线性方程通过激活函数‘tanh’挤压-1和1之间的值，得到<strong class="ix hj"> <em class="jt"> a(t) </em> </strong>。在第二个等式中，激活输出<strong class="ix hj"> <em class="jt"> a(t) </em> </strong>被传递给softmax函数，如图所示。</p><p id="22fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们了解了RNN的单个块操作，将这些块组合起来，我们将得到如下所示的RNN模型:</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/570edb14185793a1694e2ff57c9bcb00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C50Nc0WHgqdin7MrYsvHLA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">RNN模型</figcaption></figure><p id="ddaa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了计算模型的损耗，计算单个块的损耗并求和，得到总损耗。以下等式表示RNN的损失:</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/a5b3b9ba49ea5f14eadbaf4441ba4b88.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*xVlbvfHH3P2VutZWl53Cfg.png"/></div></figure><p id="e329" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">反向传播</strong>:针对RNN的反向传播在每个时间点完成。在时间步长<strong class="ix hj"><em class="jt">【T】</em></strong>，损失<strong class="ix hj"> <em class="jt"> L </em> </strong>相对于权重矩阵<strong class="ix hj"> <em class="jt"> W </em> </strong>的导数表示为:</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es kk"><img src="../Images/7972faf78e3ed51022f3ac36bb655447.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*r9o0GNUPTvtPf3QKSnNYfw.png"/></div></figure><p id="175c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们已经了解了什么是RNN，它是如何工作的，让我们看看这些模型有什么问题，以及如何克服这些问题。RNN的一个问题是在反向传播步骤中的爆炸/消失梯度，发生的原因是因为RNN被限制在大约十个时间步骤的时间内回顾。因为在返回信号梯度变得非常小的过程中，时间向后推移会导致梯度消失。</p><p id="5489" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了解决消失梯度的问题，RNN使用了不同的门，这给了我们一些不同的版本:LSTM和格鲁。让我们讨论这两个:</p><p id="47af" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> LSTM: </strong></p><p id="26bc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了理解LSTM，让我们看看框图:</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/8f53f1ac2864f9df81c190871cfa858a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0WCFmY7c6w_Pc8S7LY7csQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es km"><img src="../Images/73a210725be19c03a061bb02a00077fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*6anPx7vGewJu1OwZEdJBQg.png"/></div></figure><p id="4ddf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">LSTMs的关键是单元状态，即穿过上图顶部的水平线。</p><p id="6940" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">细胞状态有点像传送带。它沿着整个链条直线向下，只有一些微小的线性相互作用。信息很容易不加改变地沿着它流动。LSTM确实有能力删除或添加细胞状态的信息，由称为门的结构仔细调节。</p><p id="2064" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">LSTM有三个不同的门忘记门，输入门和输出门。下面将逐一讨论:</p><h2 id="04a8" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jg ky kz la jk lb lc ld jo le lf lg lh bi translated">忘记大门</h2><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es li"><img src="../Images/583255b163fd0a298f3078a0ec01a3a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*FMEplQssx-kb79gU6t1HvQ.png"/></div></figure><p id="d9a6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">遗忘门负责从单元状态中移除信息。LSTM理解事物不再需要的信息或者不太重要的信息通过滤波器的乘法被移除。这是优化LSTM网络性能所必需的。</p><p id="8716" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这需要两个输入；h_t-1和x_t。</p><p id="0e9e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">h_t-1是前一个单元的隐藏状态或前一个单元的输出，x_t是该特定时间步长的输入。给定的输入乘以权重矩阵，并添加偏差。随后，sigmoid函数应用于该值。sigmoid函数输出一个向量，值的范围从0到1，对应于单元格状态中的每个数字。基本上，sigmoid函数负责决定保留哪些值，丢弃哪些值。如果单元状态中的特定值输出“0 ”,则意味着遗忘门希望单元状态完全遗忘该信息。类似地，1表示遗忘门想要记住整个信息。sigmoid函数的矢量输出乘以单元状态。</p><h2 id="5264" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jg ky kz la jk lb lc ld jo le lf lg lh bi translated">输入门</h2><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/c963be72dc153c9956112e357031bf8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*enmA9OS0aMcmc8UBvvpr0Q.png"/></div></figure><p id="e3a5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输入门负责向单元状态添加信息。如上图所示，添加信息基本上分为三步。</p><ol class=""><li id="547e" class="lk ll hi ix b iy iz jc jd jg lm jk ln jo lo js lp lq lr ls bi translated">通过使用sigmoid函数来调节需要添加到单元状态的值。这基本上非常类似于遗忘门，并充当来自h_t-1和x_t的所有信息的过滤器。</li><li id="f221" class="lk ll hi ix b iy lt jc lu jg lv jk lw jo lx js lp lq lr ls bi translated">创建一个包含所有可能值的向量，这些值可以被添加到(如从h_t-1和x_t所感知的)单元状态中。这是使用<strong class="ix hj"> tanh </strong>函数完成的，该函数输出从-1到+1的值。</li><li id="64d9" class="lk ll hi ix b iy lt jc lu jg lv jk lw jo lx js lp lq lr ls bi translated">将调节滤波器(sigmoid gate)的值乘以创建的向量(tanh函数),然后通过加法运算将该有用信息添加到单元状态。</li></ol><p id="c73a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦这三个步骤的过程完成，我们确保只有那个信息被添加到单元状态，是<em class="jt">重要的</em>，而不是<em class="jt">多余的。</em></p><h2 id="a859" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jg ky kz la jk lb lc ld jo le lf lg lh bi translated">输出门</h2><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/e9b1a594869a93556db4517cccca3f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*riL840fnPxdwDYU5Hpgaig.png"/></div></figure><p id="6f35" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出门的功能又可以分为三步:</p><ol class=""><li id="5e0a" class="lk ll hi ix b iy iz jc jd jg lm jk ln jo lo js lp lq lr ls bi translated">在将<strong class="ix hj"> tanh </strong>函数应用于单元格状态后创建一个向量，从而将值缩放到范围-1到+1。</li><li id="c21e" class="lk ll hi ix b iy lt jc lu jg lv jk lw jo lx js lp lq lr ls bi translated">使用h_t-1和x_t的值制作过滤器，使得它可以调节需要从上面创建的向量输出的值。该滤波器再次采用了sigmoid函数。</li><li id="4954" class="lk ll hi ix b iy lt jc lu jg lv jk lw jo lx js lp lq lr ls bi translated">将此调节过滤器的值乘以步骤1中创建的向量，并将其作为输出发送出去，还发送到下一个单元的隐藏状态。</li></ol><p id="c1b3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> GRU: </strong></p><p id="697a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">GRU是LSTM递归神经网络的变体。</p><p id="9b7e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，GRU网络有一个重置和更新“门”,帮助确保其内存不会被跟踪短期依赖性所接管。网络学会如何使用它的门来保护它的记忆，这样它就能够进行长期预测。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/dc52ef4ceeb1fcf9cd4a93bc38bfe0fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n1ea0-dO2ejDGwrMImT5yw.png"/></div></div></figure><p id="ede3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">直观地说，重置门决定了如何将新的输入与先前的存储器相结合，而更新门定义了要保留多少先前的存储器。如果我们将reset设置为全1，将update gate设置为全0，我们又会得到简单的RNN模型。使用门控机制来学习长期依赖性的基本思想与LSTM中的相同，但是有几个关键的区别:</p><ul class=""><li id="309c" class="lk ll hi ix b iy iz jc jd jg lm jk ln jo lo js ma lq lr ls bi translated">GRU有两个门，LSTM有三个门。</li><li id="c475" class="lk ll hi ix b iy lt jc lu jg lv jk lw jo lx js ma lq lr ls bi translated">GRUs不具备与暴露隐藏状态不同的内存(<em class="jt"> ct </em>)。它们没有LSTMs中的输出门。</li><li id="76e3" class="lk ll hi ix b iy lt jc lu jg lv jk lw jo lx js ma lq lr ls bi translated">输入门和遗忘门通过更新门<strong class="ix hj"> <em class="jt"> z </em> </strong>耦合，复位门<strong class="ix hj"> <em class="jt"> r </em> </strong>直接应用于先前的隐藏状态。因此，LSTM中的复位门的责任实际上被分成了<strong class="ix hj"><em class="jt"/></strong>和<strong class="ix hj"><em class="jt"/></strong>两部分</li><li id="7e1d" class="lk ll hi ix b iy lt jc lu jg lv jk lw jo lx js ma lq lr ls bi translated">当计算输出时，我们不应用第二非线性。</li></ul><p id="4d50" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请分享，如果你觉得帖子有帮助，请鼓掌。</p><p id="d7f9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">来源:</strong></p><div class="mb mc ez fb md me"><a href="https://arxiv.org/abs/1909.09586" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hj fi z dy mj ea eb mk ed ef hh bi translated">理解LSTM——长短期记忆递归神经网络教程</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">长短期记忆递归神经网络(LSTM-RNN)是最强大的动态分类器之一</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">arxiv.org</p></div></div></div></a></div><div class="mb mc ez fb md me"><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hj fi z dy mj ea eb mk ed ef hh bi translated">了解LSTM网络</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">2015年8月27日发布人类不是每秒钟都从零开始思考。当你读这篇文章时，你…</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">colah.github.io</p></div></div></div></a></div></div></div>    
</body>
</html>