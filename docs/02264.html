<html>
<head>
<title>Understanding K-Means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解K均值</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-k-means-be2054cd18ea?source=collection_archive---------13-----------------------#2019-12-08">https://medium.com/analytics-vidhya/understanding-k-means-be2054cd18ea?source=collection_archive---------13-----------------------#2019-12-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b946c1eee739f2535f5536a691041394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XeMtT-MJIEVjiqojGOC2A.jpeg"/></div></div></figure><blockquote class="iq"><p id="efd7" class="ir is hi bd it iu iv iw ix iy iz ja dx translated">我们只能看到前方不远的地方，但我们可以看到那里有很多需要做的事情~艾伦·图灵</p></blockquote><p id="9a5f" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ja hb bi translated">想象一下，一家杂货店或一家银行已经积累了大量的客户数据。他们的目标可能是相似的，因为他们都想扩展或向更多的客户提供服务。也许他们想更好地了解他们的客户，并奖励他们对企业的忠诚。在这两种情况下，他们如何从数据中识别客户类型组来实现这些目标呢？本文将关注K-Means过程，以及K-Means++初始化机制如何帮助K-Means聚类方法解决一个聚类问题。</p><h1 id="95d8" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">集群</strong></h1><p id="4d20" class="pw-post-body-paragraph jc jd hi je b jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv lb jx jy ja hb bi lc translated"><span class="l ld le lf bm lg lh li lj lk di"> C </span>聚类是一种无监督的机器学习方法，通过利用未标记的数据集，帮助机器学习为观察值分配标签。换句话说，聚类试图将具有相似特征或行为的观察结果分组，这些特征或行为与其他聚类或组中的观察结果的特征不同。聚类的一些应用包括模式识别、图像处理和市场细分。这可以从图1中观察到，图1说明了客户是如何根据支出分数和年收入的相似特征进行聚类的。这是一种直观的方法，可以帮助我们获得关于相似观察的深刻信息。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/d89842599e49d90e83fefc7fd67d5af6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WXd-boAk3d9JEHn7s2BpsQ.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图1:原始数据集散点图与聚类数据点图</figcaption></figure><p id="89a2" class="pw-post-body-paragraph jc jd hi je b jf lu jh ji jj lv jl jm jn lw jp jq jr lx jt ju jv ly jx jy ja hb bi translated">K-Means、DBSCAN和OPTICS等算法试图解决聚类问题，每种算法都具有独特的特征，可以根据数据集和聚类问题的应用来使用。Scikit-learn提供了不同聚类方法及其特征的表格。</p><blockquote class="lz ma mb"><p id="e663" class="jc jd mc je b jf lu jh ji jj lv jl jm md lw jp jq me lx jt ju mf ly jx jy ja hb bi translated"><a class="ae mg" href="https://scikit-learn.org/stable/modules/clustering.html#k-means" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/clustering . html # k-means</a></p></blockquote><h1 id="0829" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">K-表示</strong></h1><p id="6b4c" class="pw-post-body-paragraph jc jd hi je b jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv lb jx jy ja hb bi translated">K-means是数据挖掘实践中应用最广泛的聚类分析方法之一。K-means是一个迭代过程，它试图通过<em class="mc"> K </em>不相交的聚类<em class="mc"> C </em>来划分<em class="mc"> N </em>个样本。每个聚类<em class="mc"> C </em>，也称为“质心”，代表一个聚类内样本的平均值。K-Means的主要任务是选择使点和它们各自的质心之间的平方距离最小的质心。这个标准被称为“组内平方和”(WCSS)。Scikit-learn的“KMeans”算法提供了一个WCSS作为。inertia_ '方法，稍后将在示例代码中展示。</p><h1 id="4620" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">K均值过程</strong></h1><blockquote class="lz ma mb"><p id="cd0b" class="jc jd mc je b jf lu jh ji jj lv jl jm md lw jp jq me lx jt ju mf ly jx jy ja hb bi translated">可以从https://www . ka ggle . com/vjchoudhary 7/customer-segmentation-tutorial-in-python访问该流程的数据集</p></blockquote><p id="2116" class="pw-post-body-paragraph jc jd hi je b jf lu jh ji jj lv jl jm jn lw jp jq jr lx jt ju jv ly jx jy ja hb bi translated"><em class="mc">这个K均值过程涉及一个二维数据集。</em></p><h1 id="a9d5" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">第一步</strong></h1><p id="24a0" class="pw-post-body-paragraph jc jd hi je b jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv lb jx jy ja hb bi translated"><strong class="je hj">对于二维或三维数据集，创建一个散点图来可视化数据点的分散。</strong></p><pre class="lm ln lo lp fd mh mi mj mk aw ml bi"><span id="6afb" class="mm ka hi mi b fi mn mo l mp mq"># Importing the necessary libraries<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span><span id="e950" class="mm ka hi mi b fi mr mo l mp mq">dataset = pd.read_csv('Mall_Customers.csv')</span><span id="5144" class="mm ka hi mi b fi mr mo l mp mq">X = dataset.loc[:,['Annual Income (k$)','Spending Score (1-100)']].values</span><span id="b073" class="mm ka hi mi b fi mr mo l mp mq"># Visualizing the data points with a scatter plot</span><span id="98c3" class="mm ka hi mi b fi mr mo l mp mq">plt.scatter(X[:,0], X[:,1], s = 100, edgecolors='black', marker='o')<br/>plt.title('Mall Clients')<br/>plt.xlabel('Annual Income (k$)')<br/>plt.ylabel('Spending Score (1-100)')<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/7e0356a946463d9fdeef8239136109a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*UyST6xKEVfKDqedkWbhW9g.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图2:商场顾客消费得分与年收入散点图</figcaption></figure><h1 id="aa2b" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">第二步</h1><p id="6536" class="pw-post-body-paragraph jc jd hi je b jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv lb jx jy ja hb bi translated">选择<em class="mc"> K </em>簇。在将模型应用到数据集之前，这是需要考虑的重要步骤。我们需要一个可量化的指标来理解或评估一定数量的集群与不同数量的集群相比表现如何。</p><p id="bce8" class="pw-post-body-paragraph jc jd hi je b jf lu jh ji jj lv jl jm jn lw jp jq jr lx jt ju jv ly jx jy ja hb bi translated">我们如何决定选择多少个集群？</p><p id="958b" class="pw-post-body-paragraph jc jd hi je b jf lu jh ji jj lv jl jm jn lw jp jq jr lx jt ju jv ly jx jy ja hb bi translated"><em class="mc">解</em> : <strong class="je hj">类内平方和(sk-learn:'。惯性_ ')。</strong></p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/a2556840946133932541a16f0e44e517.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*j5nuxvdgh6X16S7tHC9C5Q.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">等式:WCSS最小化</figcaption></figure><ul class=""><li id="b392" class="mu mv hi je b jf lu jj lv jn mw jr mx jv my ja mz na nb nc bi translated">WCSS为所有聚类确定数据点与其各自质心之间的总平方距离。</li><li id="123b" class="mu mv hi je b jf nd jj ne jn nf jr ng jv nh ja mz na nb nc bi translated"><em class="mc">肘法</em>:肘法是WCSS值对聚类数的作图。通过观察我们在WCSS中开始看不到显著变化的点，可以确定最佳聚类数。在下面的示例中，5个集群是集群的最佳数量。</li></ul><pre class="lm ln lo lp fd mh mi mj mk aw ml bi"><span id="bf6d" class="mm ka hi mi b fi mn mo l mp mq"># Using the elbow method to find the optimal number of clusters<br/># Importing the KMeans algo from sklearn library<br/>from sklearn.cluster import KMeans</span><span id="d9fe" class="mm ka hi mi b fi mr mo l mp mq">wcss = []</span><span id="6734" class="mm ka hi mi b fi mr mo l mp mq"># In each iteration of this loop, we are going to do 2 things:<br/># 1. We will fit the KMeans algo to our data 'X'<br/># 2. We will compute the Within Cluster Sum of Squares and append to <br/>#    our WCSS empty list</span><span id="4ebe" class="mm ka hi mi b fi mr mo l mp mq">for i in range(1,11):<br/>    kmeans = KMeans(n_clusters = i, init = 'random', max_iter = 300,<br/>                    n_init = 10, random_state = 0)<br/>    kmeans.fit(X)<br/>    wcss.append(kmeans.inertia_)<br/>    <br/>plt.plot(range(1,11), wcss)<br/>plt.title('The Elbow Method')<br/>plt.xlabel('Number of Clusters')<br/>plt.ylabel('WCSS')<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/0020a45e3611776ae08eb4af647339dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*SuPIL2ylf0tSJH3TE4hOaQ.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图3:肘法</figcaption></figure><h1 id="f82a" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">第三步</strong></h1><p id="447f" class="pw-post-body-paragraph jc jd hi je b jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv lb jx jy ja hb bi translated">选择随机初始<em class="mc"> K </em>点。这些点是均匀随机选择的。在下图中，我们注意到圆圈标记被选作随机初始质心。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/9b96d945dd2a60ecb7c5108a2dac639e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*N_FUrOEJbCUPvwEw-4RvFw.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图4:选择初始点的图示</figcaption></figure><h1 id="9c13" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">第四步</h1><p id="ad1e" class="pw-post-body-paragraph jc jd hi je b jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv lb jx jy ja hb bi translated">将每个数据点分配给最近的质心。K-means算法将点分配给它们最接近的质心。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/7fea3532685c87cbab0fa320540f15c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*BZ1UMUSQC0nueXcRw1spnA.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图5:分配给最近质心的数据点的图示</figcaption></figure><h1 id="3491" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">第五步</h1><p id="c747" class="pw-post-body-paragraph jc jd hi je b jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv lb jx jy ja hb bi translated">重新计算平均值，并为每个聚类重新分配每个质心的位置。在这一点上，每个聚类内的所有点的x和y被平均，以找到它们各自质心的新位置。对于每个聚类，然后计算新旧质心之间的差，以分配每个质心的新位置。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/f875c0a9bb82ef434d5017d96b688ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*z6wFk04CpdNRinyV56kWiw.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图6:质心被</figcaption></figure><h1 id="8b61" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">第六步</h1><p id="64cc" class="pw-post-body-paragraph jc jd hi je b jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv lb jx jy ja hb bi translated">将每个数据点重新分配给新的最近质心。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nm"><img src="../Images/58a9ee70bcbcdfb94482235e8b50c7da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*oslAbEghgLygYwalb6hL6w.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图7:重新分配给新定位的质心的点的图示</figcaption></figure><ul class=""><li id="feaa" class="mu mv hi je b jf lu jj lv jn mw jr mx jv my ja mz na nb nc bi translated">该算法迭代最后两步，直到算法收敛(聚类中的质心和点没有显著变化)。</li></ul><pre class="lm ln lo lp fd mh mi mj mk aw ml bi"><span id="762b" class="mm ka hi mi b fi mn mo l mp mq"># Applying K-Means to the mall dataset<br/># Note: If you need to increase the number of clusters, you need to add color names to represent each cluster. This is the only change to make the code versatile</span><span id="c05a" class="mm ka hi mi b fi mr mo l mp mq">kmeans = KMeans(n_clusters= 5, init= 'random', max_iter=300, n_init=10,<br/>                random_state=0)</span><span id="b715" class="mm ka hi mi b fi mr mo l mp mq">y_kmeans = kmeans.fit_predict(X)</span><span id="d744" class="mm ka hi mi b fi mr mo l mp mq"># Visualizing the clusters<br/>clusters = set(y_kmeans)<br/>colors = ['red', 'blue', 'green', 'cyan', 'magenta']<br/>labels = ['Cluster {}'.format(i+1) for i in set(y_kmeans)]</span><span id="6f56" class="mm ka hi mi b fi mr mo l mp mq">for clust, color, labels in zip(clusters, colors, labels):<br/>    plt.scatter(X[y_kmeans == clust, 0], X[y_kmeans == clust, 1], <br/>                s=100, c = color, label = labels, edgecolors='black')</span><span id="b730" class="mm ka hi mi b fi mr mo l mp mq">centroids = kmeans.cluster_centers_</span><span id="06b1" class="mm ka hi mi b fi mr mo l mp mq">plt.scatter(centroids[:, 0], centroids[:, 1], s = 200, c = 'yellow', <br/>            label = 'Centroids', edgecolors='black', marker='s')</span><span id="5e2f" class="mm ka hi mi b fi mr mo l mp mq">plt.title('Clusters of Clients')<br/>plt.xlabel('Annual Income (k$)')<br/>plt.ylabel('Spending Score (1-100)')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nn"><img src="../Images/bea8516caffbac93fe9de5565f4148e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*HfEqXW4zVy_4ZccOOKUOyw.png"/></div></figure><h1 id="f3a0" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> K-Means的局限性</strong></h1><p id="25ae" class="pw-post-body-paragraph jc jd hi je b jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv lb jx jy ja hb bi translated">K-Means适用于非常大的数据样本和中等数量的聚类。然而，算法的初始化过程限制了算法设置良好的初始化质心点的能力。例如，随机质心点可以彼此靠近放置，也可以远离数据点放置。这导致算法需要时间来收敛，尤其是当有数百万个数据点时。</p><h1 id="b891" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> K-Mean++算法:解</strong></h1><p id="116c" class="pw-post-body-paragraph jc jd hi je b jf kx jh ji jj ky jl jm jn kz jp jq jr la jt ju jv lb jx jy ja hb bi translated">K-Means++算法是一种随机播种技术，由David Arthur和Sergei Vassilvitskii提出，它有助于初始化中心以创建良好的聚类。该算法从数据点阵列中随机选择中心。K-Means使用同样的方法。然而，K-Means++算法根据数据点与最近中心的平方距离(已经选定)对数据点进行加权。换句话说，它随机选择第一个质心。然后，计算离质心的距离。再次随机选择第二质心，并且以如下概率选择与新质心相关联的点:</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es no"><img src="../Images/93099589fb128a6c9ae8101931448f8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*ehqMbcqxvz0Ps6n1Fmex0Q.png"/></div></figure><p id="2589" class="pw-post-body-paragraph jc jd hi je b jf lu jh ji jj lv jl jm jn lw jp jq jr lx jt ju jv ly jx jy ja hb bi translated">D(x)表示从数据点到最近质心的最短距离。重复这个过程，直到找到所有初始中心。再次使用K-Means算法来重新定位中心并将数据点重新分配给重新定位的中心。</p><ul class=""><li id="570b" class="mu mv hi je b jf lu jj lv jn mw jr mx jv my ja mz na nb nc bi translated">K-Means++算法可以用在Sklearn的‘K Means’函数中。必须在“init”或初始值设定项参数中简单声明“kmeans++”。</li><li id="ba39" class="mu mv hi je b jf nd jj ne jn nf jr ng jv nh ja mz na nb nc bi translated">最终，K-Means++算法加快了K-Means算法的收敛速度。</li></ul></div></div>    
</body>
</html>