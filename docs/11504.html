<html>
<head>
<title>Tensorflow 2 for Deep Learning -Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于深度学习的 tensor flow 2-线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tensorflow-2-for-deep-learning-linear-regression-bb1c3735ffa4?source=collection_archive---------13-----------------------#2020-12-06">https://medium.com/analytics-vidhya/tensorflow-2-for-deep-learning-linear-regression-bb1c3735ffa4?source=collection_archive---------13-----------------------#2020-12-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/47c1ee57c3b62149102cad5940fbeb11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uMCAt8mqtrVerAD-vuWMww.jpeg"/></div></div></figure><p id="3862" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">代码文件将在:</em><a class="ae jp" href="https://github.com/ashwinhprasad/Tensorflow-2.0" rel="noopener ugc nofollow" target="_blank"><em class="jo">【https://github.com/ashwinhprasad/Tensorflow-2.0】</em></a>获得</p><h1 id="7657" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">什么是线性回归？</h1><p id="df60" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">线性回归基本上是用一条线的一个方程，找出 2 个变量之间的线性关系。通过找到线性关系，我的意思是找到参数(斜率和截距)。</p><p id="de29" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> y = m*x + c <br/> y:因变量<br/> x:自变量<br/> m:斜率<br/> c:截距</strong></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/bee1efc8afe7511b00a20365f42ff02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*4-ZmTASTSsWLZcdI7MTnPA.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">两个变量之间的线性关系示例</figcaption></figure><p id="c82d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上图中，SAT 数学是独立变量，大学 GPA 分数是因变量，很明显这两个变量之间存在线性关系。因此，一旦我们找到斜率和截距，我们可以用一条线来预测大学新生的大学 gpa 分数。</p><h1 id="d9d7" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">使用 Tensorflow 2 进行线性回归</h1><h2 id="31b9" class="lc jr hi bd js ld le lf jw lg lh li ka jb lj lk ke jf ll lm ki jj ln lo km lp bi translated"><strong class="ak"> 1。导入所需的库</strong></h2><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="093d" class="lc jr hi lr b fi lv lw l lx ly"><em class="jo">#importing the libraries<br/></em><strong class="lr hj">import</strong> <strong class="lr hj">tensorflow</strong> <strong class="lr hj">as</strong> <strong class="lr hj">tf<br/>import</strong> <strong class="lr hj">pandas</strong> <strong class="lr hj">as</strong> <strong class="lr hj">pd<br/>import</strong> <strong class="lr hj">numpy</strong> <strong class="lr hj">as</strong> <strong class="lr hj">np<br/>import</strong> <strong class="lr hj">matplotlib.pyplot</strong> <strong class="lr hj">as</strong> <strong class="lr hj">plt<br/>import</strong> <strong class="lr hj">seaborn</strong> <strong class="lr hj">as</strong> <strong class="lr hj">sns</strong></span></pre><h2 id="7ce0" class="lc jr hi bd js ld le lf jw lg lh li ka jb lj lk ke jf ll lm ki jj ln lo km lp bi translated"><strong class="ak"> 2。导入数据集</strong></h2><p id="1df9" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">导入数据集。这个数据集是来自 sklearn 的一个简单数据集。您可以使用自变量和因变量具有线性关系的任何数据集</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="0ddf" class="lc jr hi lr b fi lv lw l lx ly"><em class="jo">#data preprocessing</em><br/><em class="jo">#train set</em><br/>data_train = pd.read_csv('train.csv')<br/>data_test = pd.read_csv('test.csv')<br/><br/><em class="jo">#removing null values</em><br/>data_train = data_train.dropna(axis=0,how='any')</span></pre><h2 id="f877" class="lc jr hi bd js ld le lf jw lg lh li ka jb lj lk ke jf ll lm ki jj ln lo km lp bi translated"><strong class="ak"> 3。列车试运行</strong></h2><p id="3917" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">将数据分为训练集和测试集</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="8346" class="lc jr hi lr b fi lv lw l lx ly"><em class="jo">#xtrain, x_test , y_train, y_test<br/></em>x_train = data_train['x']<br/>y_train = data_train['y']<br/>x_test = data_test['x']<br/>y_test = data_test['y']</span></pre><h2 id="917d" class="lc jr hi bd js ld le lf jw lg lh li ka jb lj lk ke jf ll lm ki jj ln lo km lp bi translated"><strong class="ak"> 4。绘制数据</strong></h2><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="813f" class="lc jr hi lr b fi lv lw l lx ly">plt.figure(figsize=(8,6)) plt.scatter(x_train,y_train)</span></pre><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/cb3e374da7da02ddd4790e5d4c6ef5b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*LIDVh8REzWDXt4k2KK1i-w.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">数据:x 与 y</figcaption></figure><h2 id="6552" class="lc jr hi bd js ld le lf jw lg lh li ka jb lj lk ke jf ll lm ki jj ln lo km lp bi translated"><strong class="ak"> 5。将数据转换成数字</strong></h2><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="09e9" class="lc jr hi lr b fi lv lw l lx ly"><em class="jo">#converting to numpy</em><br/>x_train = np.array(x_train).reshape(-1,1)<br/>y_train = np.array(y_train)<br/>print(x_train.shape,y_train.shape)</span><span id="c1d7" class="lc jr hi lr b fi ma lw l lx ly"><strong class="lr hj">output:</strong> <br/>(699, 1) (699,)</span></pre><h2 id="07d4" class="lc jr hi bd js ld le lf jw lg lh li ka jb lj lk ke jf ll lm ki jj ln lo km lp bi translated">6。使用 TF2 创建序列模型</h2><p id="4732" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">顺序层允许将一层堆叠在另一层之上，使数据能够在其中流动</p><p id="db81" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有两种方法可以做到这一点，如下面的单元格所示</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="fd72" class="lc jr hi lr b fi lv lw l lx ly"><strong class="lr hj">from</strong> <strong class="lr hj">tensorflow.keras.models</strong> <strong class="lr hj">import</strong> Sequential<br/><strong class="lr hj">from</strong> <strong class="lr hj">tensorflow.keras.layers</strong> <strong class="lr hj">import</strong> Dense<br/><br/>model = Sequential()<br/>model.add(Dense(1))<br/><br/><em class="jo">"""</em><br/><em class="jo">(Alternative way for dedfining a sequential model)</em><br/><em class="jo">model = Sequential([</em><br/><em class="jo">        Dense(1)</em><br/><em class="jo">])"""</em></span></pre><h2 id="a35f" class="lc jr hi bd js ld le lf jw lg lh li ka jb lj lk ke jf ll lm ki jj ln lo km lp bi translated">7.优化器和梯度下降</h2><p id="38d1" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">我们在这里使用小批量梯度下降优化器。这个问题的均方损失</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="abcb" class="lc jr hi lr b fi lv lw l lx ly"><strong class="lr hj">from</strong> <strong class="lr hj">tensorflow.keras.optimizers</strong> <strong class="lr hj">import</strong> SGD<br/><strong class="lr hj">from</strong> <strong class="lr hj">tensorflow.keras.losses</strong> <strong class="lr hj">import</strong> mse<br/>model.compile(optimizer=SGD(learning_rate=0.0001),loss=mse)<br/>train = model.fit(x_train,y_train,epochs=50)</span></pre><h2 id="c0e8" class="lc jr hi bd js ld le lf jw lg lh li ka jb lj lk ke jf ll lm ki jj ln lo km lp bi translated">8.技术性能分析</h2><p id="4152" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">损失随着时间的推移而减少，模型的拟合线似乎最适合下面的数据</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="c141" class="lc jr hi lr b fi lv lw l lx ly"><em class="jo">#loss over time</em><br/>plt.plot(train.history['loss'],label='loss')<br/>plt.xlabel('epochs')<br/>plt.ylabel('loss')<br/>plt.legend()</span></pre><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/308ae04f7198a7ab139461e5334c1fbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*0oUSzmo1TblcbS-swLEhcA.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">随着时间的推移而损失</figcaption></figure><h2 id="47db" class="lc jr hi bd js ld le lf jw lg lh li ka jb lj lk ke jf ll lm ki jj ln lo km lp bi translated">9.在测试集上测试模型</h2><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="ce97" class="lc jr hi lr b fi lv lw l lx ly"><em class="jo">#testing the model</em><br/>y_pred = model.predict(np.array(x_test).reshape(-1,1))<br/>plt.figure(figsize=(10,8))<br/>plt.plot(x_test,y_pred,color='red',linewidth=2,label='Line of best Fit')<br/>plt.scatter(x_test,y_test,label='Test data plots')<br/>plt.legend()</span></pre><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/0caa4c754a0a68f12568f325f648c8b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*S5__qxuEbwqVGBVAxyPYoA.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">最佳拟合线</figcaption></figure><h1 id="c48e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">结论</h1><p id="bbbf" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">该模型从一种称为梯度下降的算法中学习这些参数(斜率和偏差)。要了解梯度下降如何工作，请查看:<a class="ae jp" rel="noopener" href="/analytics-vidhya/linear-regression-with-gradient-descent-derivation-c10685ddf0f4">https://medium . com/analytics-vid hya/linear-regression-with-gradient-descent-derivation-c 10685 ddf 0 f 4</a>。<br/>因此，最佳拟合直线的参数应使得直线上的点与垂直远离直线的数据点之和之间的差异最小。这可以通过梯度下降博客帖子更好地理解。</p><p id="f896" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦我们得到这条新线的方程，我们就可以预测 x 的新值。</p></div></div>    
</body>
</html>