<html>
<head>
<title>Understanding Attention Mechanism: Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解注意机制:自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/https-medium-com-understanding-attention-mechanism-natural-language-processing-9744ab6aed6a?source=collection_archive---------1-----------------------#2020-01-28">https://medium.com/analytics-vidhya/https-medium-com-understanding-attention-mechanism-natural-language-processing-9744ab6aed6a?source=collection_archive---------1-----------------------#2020-01-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><h1 id="aac5" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">目录</h1><ol class=""><li id="16f2" class="jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">介绍</li><li id="7e61" class="jk jl hi jm b jn kc jp kd jr ke jt kf jv kg jx jy jz ka kb bi translated">注意力模型</li><li id="131e" class="jk jl hi jm b jn kc jp kd jr ke jt kf jv kg jx jy jz ka kb bi translated">注意力是如何工作的？</li><li id="1baa" class="jk jl hi jm b jn kc jp kd jr ke jt kf jv kg jx jy jz ka kb bi translated">代码走查</li><li id="d9b4" class="jk jl hi jm b jn kc jp kd jr ke jt kf jv kg jx jy jz ka kb bi translated">结论</li><li id="1421" class="jk jl hi jm b jn kc jp kd jr ke jt kf jv kg jx jy jz ka kb bi translated">参考</li></ol><h1 id="3d45" class="im in hi bd io ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj bi translated">1.介绍</h1><p id="e245" class="pw-post-body-paragraph km kn hi jm b jn jo ko kp jp jq kq kr jr ks kt ku jt kv kw kx jv ky kz la jx hb bi translated">注意机制是深度学习领域的最新进展之一，尤其适用于机器翻译、图像字幕、对话生成等自然语言处理任务。这是一种为提高编码器解码器(seq2seq) RNN模型的性能而开发的机制。在这篇博文中，我将尝试解释文本分类任务的注意力机制。</p><h1 id="aff8" class="im in hi bd io ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj bi translated">2.注意力模型</h1><p id="29f7" class="pw-post-body-paragraph km kn hi jm b jn jo ko kp jp jq kq kr jr ks kt ku jt kv kw kx jv ky kz la jx hb bi translated">提出关注作为对编码器-解码器模型的限制的解决方案，该模型将输入序列编码为一个固定长度的向量，从该向量在每个时间步长解码输出。这个问题被认为是解码长序列时的一个问题，因为它使得神经网络难以处理长句，尤其是那些比训练语料库中的句子更长的句子。</p><blockquote class="lb lc ld"><p id="eeec" class="km kn le jm b jn lf ko kp jp lg kq kr lh li kt ku lj lk kw kx ll lm kz la jx hb bi translated">注意力被提出来作为一种对齐和翻译的方法。</p></blockquote><p id="3b3d" class="pw-post-body-paragraph km kn hi jm b jn lf ko kp jp lg kq kr jr li kt ku jt lk kw kx jv lm kz la jx hb bi translated">当模型试图预测下一个单词时，它会在源句子中搜索最相关信息集中的一组位置。该模型然后基于与这些源位置和所有先前生成的目标单词相关联的上下文向量来预测下一个单词。</p><p id="3c87" class="pw-post-body-paragraph km kn hi jm b jn lf ko kp jp lg kq kr jr li kt ku jt lk kw kx jv lm kz la jx hb bi translated">注意力模型不是将输入序列编码到单个固定的上下文向量中，而是开发一个上下文向量，该向量专门针对每个输出时间步长进行过滤。</p><h1 id="50be" class="im in hi bd io ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj bi translated">3.注意力是如何工作的？</h1><p id="bdf8" class="pw-post-body-paragraph km kn hi jm b jn jo ko kp jp jq kq kr jr ks kt ku jt kv kw kx jv ky kz la jx hb bi translated">Attention的基本思想是，每次模型试图预测输出单词时，它只使用输入中最相关的信息集中的部分，而不是整个句子，即它试图给予少数输入单词更多的重要性。让我们看看它是如何工作的:</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div class="er es ln"><img src="../Images/6b255d7ab693868612915ecfa452fd32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wa4zt-LcMWRIYLfiHfBKvA.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">注意机制图解:英语到韩语的翻译</figcaption></figure><p id="d056" class="pw-post-body-paragraph km kn hi jm b jn lf ko kp jp lg kq kr jr li kt ku jt lk kw kx jv lm kz la jx hb bi translated">注意，编码器的工作方式类似于编码器-解码器模型，但解码器的行为不同。正如你从图片中看到的，解码器的隐藏状态是用上下文向量、先前的输出和先前的隐藏状态计算的，并且它对每个目标单词都有单独的上下文向量c_i。这些上下文向量被计算为向前和向后方向上的激活状态和字母的加权和，并且这些字母表示输入对输出单词的生成给予了多少关注。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div class="er es lz"><img src="../Images/210f8347512edba9e6659a2cf54d9c95.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*rjIVbJMcDi1ZMyZbblaunA.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">输出字1的上下文向量</figcaption></figure><p id="9e2f" class="pw-post-body-paragraph km kn hi jm b jn lf ko kp jp lg kq kr jr li kt ku jt lk kw kx jv lm kz la jx hb bi translated">这里，“a”表示向前和向后的激活，α表示每个输入单词给予输出单词的注意。</p><h1 id="0bad" class="im in hi bd io ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj bi translated">4.注意力机制的代码走查</h1><p id="ed13" class="pw-post-body-paragraph km kn hi jm b jn jo ko kp jp jq kq kr jr ks kt ku jt kv kw kx jv ky kz la jx hb bi translated">我采用了包含50，000条电影评论文本的<a class="ae ma" href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb" rel="noopener ugc nofollow" target="_blank"> <strong class="jm hj"> IMDB数据集</strong> </a> <strong class="jm hj"> </strong>。它已经被预处理，使得单词序列已经被转换成整数序列，其中每个整数代表字典中的一个特定单词。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mb"><img src="../Images/c71ea230f81477399ad13c18a18d75e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zwWkC5JNMuPU11HVWSjZxA.png"/></div></div></figure><h1 id="514f" class="im in hi bd io ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj bi translated">4.1自我关注</h1><p id="3e84" class="pw-post-body-paragraph km kn hi jm b jn jo ko kp jp jq kq kr jr ks kt ku jt kv kw kx jv ky kz la jx hb bi translated">现在，创建一个自我关注层，把输入的句子作为数字的向量嵌入。有两种主要方法来执行这种嵌入预训练嵌入，如<a class="ae ma" href="https://render.githubusercontent.com/view/ipynb??" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>或<a class="ae ma" href="https://render.githubusercontent.com/view/ipynb??" rel="noopener ugc nofollow" target="_blank"> GloVe </a>或随机初始化。这里，我使用了随机初始化。我们将使用双向RNN。这只是两个rnn的连接，一个从左到右处理序列，另一个从右到左处理。通过使用两个方向，我们可以获得更强的编码，因为每个单词都可以使用其两侧邻居的上下文进行编码，而不仅仅是单侧。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mg"><img src="../Images/b6985dc7c78a54f0bbfb7d3a6626d0ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fNUSctA_pHso2HKp02DMrg.png"/></div></div></figure><p id="e985" class="pw-post-body-paragraph km kn hi jm b jn lf ko kp jp lg kq kr jr li kt ku jt lk kw kx jv lm kz la jx hb bi translated">最后一层与单个输出节点紧密相连。使用sigmoid激活函数，该值是一个介于0和1之间的浮点数。一个模型需要一个损失函数和一个优化器来训练。我们的模型是一个二元分类问题，模型输出一个概率。我们将使用<code class="du mh mi mj mk b">binary_crossentropy</code>损失函数。以128个样本的小批量训练模型2个时期。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es ml"><img src="../Images/dfa233bcc52d7af30d7a98c8edcfe391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmnL9eKQdNs85YOilEBNxw.png"/></div></div></figure><h1 id="374f" class="im in hi bd io ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj bi translated">4.2多头关注</h1><p id="13e9" class="pw-post-body-paragraph km kn hi jm b jn jo ko kp jp jq kq kr jr ks kt ku jt kv kw kx jv ky kz la jx hb bi translated">现在，创建一个多头注意力层，输入LSTM单位，并嵌入输入的句子作为一个数字向量。有两种主要方法来执行这种嵌入预训练嵌入，如<a class="ae ma" href="https://render.githubusercontent.com/view/ipynb??" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>或<a class="ae ma" href="https://render.githubusercontent.com/view/ipynb??" rel="noopener ugc nofollow" target="_blank"> GloVe </a>或随机初始化。这里，我使用了随机初始化。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mm"><img src="../Images/d5383fa51ade6273f3c84095c9d56ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hc4hJdFlXDXahLIrmpkIPw.png"/></div></div></figure><p id="c071" class="pw-post-body-paragraph km kn hi jm b jn lf ko kp jp lg kq kr jr li kt ku jt lk kw kx jv lm kz la jx hb bi translated">最后一层与单个输出节点紧密相连。使用sigmoid激活函数，该值是一个介于0和1之间的浮点数。一个模型需要一个损失函数和一个优化器来训练。我们的模型是一个二元分类问题，模型输出一个概率。我们将使用<code class="du mh mi mj mk b">binary_crossentropy</code>损失函数。以128个样本的小批量训练模型2个时期。</p><figure class="lo lp lq lr fd ls er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mn"><img src="../Images/d65a3e0afeeb430ef2c87f03bfdd8f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*daV5ddZgyhDfxfhpuUH7Ag.png"/></div></div></figure><h1 id="46e2" class="im in hi bd io ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj bi translated">4.3自我和多头注意机制的比较</h1><figure class="lo lp lq lr fd ls er es paragraph-image"><div class="er es mo"><img src="../Images/c797a829b168cea68d029dbb28aa63bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*A8OwZ0RLvhbuY_7Xn7HpGg.png"/></div></figure><h1 id="3bf5" class="im in hi bd io ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj bi translated">5.结论</h1><p id="eede" class="pw-post-body-paragraph km kn hi jm b jn jo ko kp jp jq kq kr jr ks kt ku jt kv kw kx jv ky kz la jx hb bi translated">注意机制是NLP任务中非常有用的技术，因为它增加了准确性和bleu分数，并且可以有效地用于长句。注意机制的唯一缺点是非常耗时并且难以并行化。</p><h1 id="e69f" class="im in hi bd io ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj bi translated">6.参考</h1><ol class=""><li id="aeab" class="jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><a class="ae ma" href="https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/</a></li><li id="dc89" class="jk jl hi jm b jn kc jp kd jr ke jt kf jv kg jx jy jz ka kb bi translated"><a class="ae ma" href="https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f" rel="noopener" target="_blank">https://towards data science . com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c 9482 aecf 4 f</a></li><li id="ce15" class="jk jl hi jm b jn kc jp kd jr ke jt kf jv kg jx jy jz ka kb bi translated">【https://medium.com/@joealato/attention-in-nlp-734c6fa9d983 T4】</li><li id="3397" class="jk jl hi jm b jn kc jp kd jr ke jt kf jv kg jx jy jz ka kb bi translated"><a class="ae ma" href="https://www.coursera.org/lecture/nlp-sequence-models/attention-model-intuition-RDXpX" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/lecture/NLP-sequence-models/attention-model-intuition-RDXpX</a></li><li id="dab5" class="jk jl hi jm b jn kc jp kd jr ke jt kf jv kg jx jy jz ka kb bi translated"><a class="ae ma" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.0473</a></li><li id="2a9c" class="jk jl hi jm b jn kc jp kd jr ke jt kf jv kg jx jy jz ka kb bi translated"><a class="ae ma" href="https://androidkt.com/text-classification-using-attention-mechanism-in-keras/" rel="noopener ugc nofollow" target="_blank">https://androidkt . com/text-class ification-using-attention-mechanism-in-keras/</a></li></ol></div></div>    
</body>
</html>