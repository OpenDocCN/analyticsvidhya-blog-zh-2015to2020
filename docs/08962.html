<html>
<head>
<title>Reducing computational constraints in SimCLR using Momentum Contrast V2 (MoCo-V2) in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch中的动量对比V2 (MoCo-V2)减少SimCLR中的计算约束</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/simclr-with-less-computational-constraints-moco-v2-in-pytorch-3d8f3a8f8bf2?source=collection_archive---------4-----------------------#2020-08-20">https://medium.com/analytics-vidhya/simclr-with-less-computational-constraints-moco-v2-in-pytorch-3d8f3a8f8bf2?source=collection_archive---------4-----------------------#2020-08-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e686af17a9e433d7e3f3407e459c0ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KFzw-JYIUH5Y8VJm"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">赫克托·j·里瓦斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="1ef3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在之前的<a class="ae iu" rel="noopener" href="/analytics-vidhya/understanding-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-d544a9003f3c?source=your_stories_page---------------------------">博客文章</a>中，我们在PyTorch 中实现了<strong class="ix hj"> SimCLR框架。在一个只有1250幅训练图像的5个类别的简单数据集上理解和实现它是一个有趣的练习。从<a class="ae iu" href="http://cse.iitkgp.ac.in/~arastogi/papers/simclr.pdf" rel="noopener ugc nofollow" target="_blank"> SimCLR论文</a>中，我们看到了框架<strong class="ix hj">如何受益于更大的模型和更大的批量</strong>，并且如果有足够的计算能力，可以产生与监督模型相当的结果。但是这些要求使得框架的计算量相当大。如果我们能够拥有这个框架的简单性和强大功能，并且有更少的计算需求，这样每个人都可以使用它，这不是很好吗？<a class="ae iu" href="https://arxiv.org/pdf/2003.04297.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> Moco-v2 </strong> </a>前来救援。</strong></p><h2 id="0948" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">数据集</h2><p id="1cb7" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">这次我们将<strong class="ix hj">在PyTorch </strong>中在更大的数据集上实现Moco-v2，并在Google Colab上训练我们的模型。这次我们将使用由Fast的杰瑞米·霍华德制作的<a class="ae iu" href="https://github.com/fastai/imagenette/" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> Imagenette和Imagewoof数据集</strong> </a>。AI 。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kt"><img src="../Images/e99ba1d87a3f77c7e669b36d143ba1b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nWWw-xtcHOEvboz83r8CQg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Imagenette数据集中的一些图像</figcaption></figure><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kt"><img src="../Images/9e9bdd9abfc65392526a80b5b5d1e4f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e84otk0ul3xS_65l94qWeA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来自Imagewoof数据集的一些图像</figcaption></figure><p id="97ca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些数据集的快速总结(更多信息请参见<a class="ae iu" href="https://github.com/fastai/imagenette" rel="noopener ugc nofollow" target="_blank">此处</a>):</p><ul class=""><li id="ba73" class="ky kz hi ix b iy iz jc jd jg la jk lb jo lc js ld le lf lg bi translated">Imagenette由来自Imagenet的<strong class="ix hj"> 10个容易分类的类</strong>组成，共有9479个训练和3935个验证集图像。</li><li id="decb" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js ld le lf lg bi translated">Imagewoof是来自Imagenet的10个困难类别的数据集——困难是因为所有类别都是狗的品种。总共有9035个训练集和3939个验证集图像。</li></ul><h1 id="7fe5" class="lm ju hi bd jv ln lo lp jz lq lr ls kd lt lu lv kg lw lx ly kj lz ma mb km mc bi translated">对比学习——综述</h1><p id="5e04" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">对比学习在自我监督学习中的工作方式是基于这样的想法:我们希望来自同一类别的不同外观的图像具有相似的表示。但由于我们不知道哪些图像属于同一类别，通常所做的是将同一图像的不同外观的<strong class="ix hj">表示彼此拉近</strong>。这些成对的不同观点被称为正对。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es md"><img src="../Images/7f4486441d6b5a58c828cb3930f1b48f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FkTlxA6-qix3IX4vhtzcQw.png"/></div></figure><p id="6a6e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是常数表示满足了这个想法。所以，另外，<strong class="ix hj">我们希望来自不同类别的不同外观的图像有彼此远离的表现。</strong>但是，同样，由于缺乏关于类别的信息，相反<strong class="ix hj">与类别无关的不同图像的不同外观的表示被彼此推开</strong>。这些成对的不同观点被称为否定对。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es md"><img src="../Images/9cc5ebfe171e55198d0b3d34409d15fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qvWVeF4fhLHOiN84NHr8cQ.png"/></div></figure><p id="79f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这种背景下，一幅图像的前景如何？展望可以被认为是以一种改变的方式看待图像的某一部分，它本质上是图像的一种转换。根据手头的任务，有些转换可能比其他转换更有效。SimCLR表明，应用<strong class="ix hj">随机裁剪和颜色抖动</strong>可以很好地完成各种任务，包括图像分类。这基本上来自于网格搜索，从旋转、裁剪、剪切、噪声、模糊、sobel过滤等选择中选择一对变换。通过<strong class="ix hj">神经网络</strong>完成从前景到表示空间的<strong class="ix hj">映射，并且典型地，为此目的使用<strong class="ix hj"> resnet </strong>。</strong></p><p id="c845" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从图像到表示的管道如下所示。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/684b330503d44a934732a1a8a60fb6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0bYRv7XBQPpbnxMjO7i1RQ.jpeg"/></div></div></figure><h1 id="8b7d" class="lm ju hi bd jv ln lo lp jz lq lr ls kd lt lu lv kg lw lx ly kj lz ma mb km mc bi translated">负对是如何产生的？</h1><p id="0227" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">从同一个图像中，由于随机裁剪，我们可以得到多个表示。这样，我们就可以生成正对。但是负对是如何产生的呢？<strong class="ix hj">负像对是来自不同图像的表示</strong>。SimCLR论文在同一批中创建了这些。如果一个批处理包含N个图像，那么对于每个图像，我们得到2个表示，这总共占2*N个表示。对于一个特定的表示x，有一个表示与x形成正对(与x来自同一个图像的那个),其余的(正好2*N - 2)与x形成负对。</p><p id="a923" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们手头有大量的阴性样本，那么这种表现会有所改善。但是在SimCLR的情况下，只有当我们具有大批量时，才能完成大量的负样本，这导致了更高的计算能力要求。<strong class="ix hj">动量对比(MoCo)提供了生成阴性样本的替代方法。</strong>让我们详细了解一下。</p><h2 id="1f28" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">动态词典</h2><p id="da6b" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">我们可以用稍微不同的方式来看待对比学习方法，即<strong class="ix hj">将查询与键</strong>匹配。不再只有一个编码器，我们现在有<strong class="ix hj">两个编码器——一个用于查询，另一个用于键。</strong>此外，为了有大量的否定样本，我们有<strong class="ix hj">一个大的编码密钥字典。</strong></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/eb1a5643b93c6946113ffe5def900eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wvWN9acS5AlXMM0nKghRvg.png"/></div></div></figure><p id="990d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个上下文中的正对意味着查询与键匹配。如果查询和密钥来自同一个图像，则它们匹配。<em class="mg">一个编码的查询应该与其匹配的键相似，而与其他的不相似[1]。</em></p><p id="51f1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于负对，<strong class="ix hj">我们维护一个大字典，其中包含来自先前批次</strong>的编码密钥。它们充当手头查询的负样本。字典以队列的形式维护，最新的一批入队，最老的一批出队。通过改变该队列的大小，可以改变负样本的数量。</p><h2 id="1a9a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">这种方法的挑战</strong></h2><ul class=""><li id="5ad3" class="ky kz hi ix b iy ko jc kp jg mh jk mi jo mj js ld le lf lg bi translated">随着键编码器的改变，在较晚的时间点排队的键可能变得与很早就排队的键不一致。<strong class="ix hj">为了使对比学习方法起作用，与查询进行比较的所有关键字必须来自相同或相似的编码器，以使比较有意义且一致。</strong></li><li id="7c73" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js ld le lf lg bi translated">另一个挑战是<strong class="ix hj">使用反向传播来学习关键编码器参数是不可行的，因为这需要计算队列中所有样本的梯度</strong>(这将产生一个大的计算图)。</li></ul><p id="9f1b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了解决这两个问题，MoCo将关键编码器实现为查询编码器【1】的基于动量的移动平均值<em class="mg">。这意味着它以这种方式更新关键编码器参数:</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/5aa0ca4f7ad0c322eef22ae46b2f6c3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*SU62A_JbzkO2pVvk7sBD3A.png"/></div></figure><p id="9193" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中m保持非常接近1(例如，典型值是0.999)，这确保我们在不同时间从相似的编码器获得编码的密钥。</p><h1 id="f5e1" class="lm ju hi bd jv ln lo lp jz lq lr ls kd lt lu lv kg lw lx ly kj lz ma mb km mc bi translated">损失函数——信息</h1><p id="c325" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated"><strong class="ix hj">我们希望一个查询接近它的所有正样本，而远离它的所有负样本</strong>。这由InfoNCE损失函数捕获，其中InfoNCE代表信息噪声对比估计。对于正关键字是kᵣ的查询q，信息损失函数被定义为:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/01ceea03dec87660463836f7b0286d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJgltw25Xwyh2MFVv9_ryw.png"/></div></div></figure><p id="923b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以改写成这样的形式:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/d5eef65710425c2c992615ed22f4d34d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mOJLQZiuyXEqiwOfDSL7_Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jv">当q和k之间的相似度</strong> ᵣ <strong class="bd jv">增加时，以及当q和负样本之间的相似度</strong>减少时，损失值减少</figcaption></figure><p id="06c5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该损失函数可在PyTorch中编码如下:</p><figure class="ku kv kw kx fd ij"><div class="bz dy l di"><div class="mn mo l"/></div></figure><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/309f8d40dbcbc4365a52d93421f0c486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G3N8nGKiy-YpvYX-u7UTQg.png"/></div></div></figure><p id="5d4f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们再来看看这个损失函数，并将其与分类交叉熵损失函数进行比较。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/c5fc19ea7dcf22caec9b9ba77fc58b68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tq-TzN2qIa3xDlCo9Mqa8A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">这里，predᵢ是数据点属于iᵗʰ类的概率值预测，而trueᵢ是该点属于iᵗʰ类的实际概率值(可能很模糊，但主要是一次性的)。</figcaption></figure><p id="3f83" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你是这个话题的新手，你可以观看这个视频来更好地理解交叉熵。另外，请注意，分数通常通过softmax之类的函数转换为概率值。</p><p id="9087" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">我们可以把信息损失函数看作交叉熵损失。数据样本“q”的正确类是rᵗʰ类，底层分类器是基于softmax的，它试图在K+1个类之间进行分类。</strong></p><p id="c80d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">信息NCE损失也与编码表示之间的互信息有关；关于这一点的更多细节见[4]。</p><h1 id="1405" class="lm ju hi bd jv ln lo lp jz lq lr ls kd lt lu lv kg lw lx ly kj lz ma mb km mc bi translated">MoCo-v2框架</h1><p id="6e7a" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">现在，让我们把所有的东西放在一起，看看整个动量对比算法是什么样子的。</p><p id="12fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">第一步:</strong>我们必须得到查询和键编码器。<strong class="ix hj">最初，键编码器的参数与查询编码器</strong>的参数相同。他们是彼此的复制品。随着训练的进行，关键编码器将成为查询编码器的移动平均值(在该值上缓慢前进)。</p><p id="642d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于计算能力的限制，我们使用Resnet-18架构来实现。在通常的resnet架构之上，我们添加了一些密集层，以将表示的维度降低到25。这些层中的一些将在稍后充当投影头，就像我们在<a class="ae iu" rel="noopener" href="/analytics-vidhya/understanding-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-d544a9003f3c?source=your_stories_page---------------------------"> SimCLR </a>中所做的一样。</p><figure class="ku kv kw kx fd ij"><div class="bz dy l di"><div class="mn mo l"/></div></figure><p id="1619" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">步骤2: </strong>现在，我们已经有了编码器，并假设我们已经建立了其他重要的数据结构，是时候开始训练循环并理解管道了。</p><p id="ed3a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">这一步是从训练批次中获取编码的查询和键。</strong>我们通过它们的L2范数来归一化这些表示。</p><p id="69c8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">只是一个约定警告，所有后续步骤中的代码都将位于批处理和历元的循环中。我们还将张量“k”从其梯度中分离出来，因为我们不需要计算图中的关键编码器部分，因为动量更新方程会更新我们的关键编码器。</p><figure class="ku kv kw kx fd ij"><div class="bz dy l di"><div class="mn mo l"/></div></figure><p id="2060" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">步骤3: </strong>现在，我们将查询、键和队列传递给我们之前定义的损失函数，并将值存储在一个列表中。然后，像往常一样，我们对损失值调用backward函数并运行优化器。</p><figure class="ku kv kw kx fd ij"><div class="bz dy l di"><div class="mn mo l"/></div></figure><p id="114f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第四步:我们将最新的一批放入队列。如果我们的队列大小大于我们定义的最大队列大小(在K中),那么我们将最早的批处理从队列中出列。<strong class="ix hj">通过使用torch.cat可以完成入队操作，通过对张量进行简单的索引切片可以完成出队操作。</strong></p><figure class="ku kv kw kx fd ij"><div class="bz dy l di"><div class="mn mo l"/></div></figure><p id="0024" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">第五步:</strong>现在我们到了训练循环的最后一步，即更新关键编码器。我们使用下面的for循环来实现。</p><figure class="ku kv kw kx fd ij"><div class="bz dy l di"><div class="mn mo l"/></div></figure><h2 id="3e5a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">一些训练细节</h2><p id="d979" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">对于Imagenette和Imagewoof数据集，训练resnet-18模型需要将近18小时的GPU时间。为此，我们使用了Google Colab的GPU (16GB)。我们使用的批量大小为256，tau值为0.05，学习速率为0.001，我们最终将其降低到1e-5，权重衰减为1e-6。我们的队列大小是8192，关键编码器的动量值是0.999。</p><h1 id="0f2d" class="lm ju hi bd jv ln lo lp jz lq lr ls kd lt lu lv kg lw lx ly kj lz ma mb km mc bi translated">结果</h1><p id="c217" class="pw-post-body-paragraph iv iw hi ix b iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">顶部的3层(将relu视为一层)定义了我们的投影头，为了图像分类的下游任务，我们移除了它。在剩下的网络之上，我们训练了一个线性分类器。</p><p id="be58" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用MoCo v2，我们在使用10%的标记训练数据时，获得了64.2%的Imagenette准确率。相比之下，使用最先进的方法进行监督学习，可以达到近95%的准确率，这可以从<a class="ae iu" href="https://github.com/fastai/imagenette" rel="noopener ugc nofollow" target="_blank">排行榜</a>中看出。</p><p id="25a7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于Imagewoof，我们对10%的标记数据获得了38.6%的准确率。在这个数据集上的对比学习表现低于我们的预期。我们怀疑这是因为，首先，数据集相当难，因为所有的类都是狗的种类。第二，我们认为颜色是这些阶级的一个基本区别特征。应用颜色抖动可能会导致来自不同类别的多个图像具有相互混合的表示。相比之下，监督方法在这方面的准确率接近90%。</p><p id="bec0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">能够弥合自我监督模型和监督模型之间差距的设计变更:</strong></p><ol class=""><li id="d443" class="ky kz hi ix b iy iz jc jd jg la jk lb jo lc js mr le lf lg bi translated">使用更大更宽的模型。</li><li id="75e5" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js mr le lf lg bi translated">使用更大的批处理和字典大小。</li><li id="5df6" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js mr le lf lg bi translated">如果可以的话，使用更多的数据。同时引入所有未标记的数据。</li><li id="967f" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js mr le lf lg bi translated">根据大量数据训练大型模型，然后对其进行提炼。</li></ol><p id="f647" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">一些有用的链接:</strong></p><ul class=""><li id="9c26" class="ky kz hi ix b iy iz jc jd jg la jk lb jo lc js ld le lf lg bi translated"><a class="ae iu" href="https://colab.research.google.com/drive/1AepjEbcHPw2Z-xY8iJkvou-Njnn0VZmd?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab的笔记本链接</a></li><li id="d68d" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js ld le lf lg bi translated"><a class="ae iu" href="https://github.com/thunderInfy/mocov2-imagewoof-results" rel="noopener ugc nofollow" target="_blank"> Imagewoof结果Github Repo </a></li><li id="4385" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js ld le lf lg bi translated"><a class="ae iu" href="https://github.com/thunderInfy/simclr-with-momentum" rel="noopener ugc nofollow" target="_blank"> Imagenette结果Github Repo </a></li><li id="d74f" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js ld le lf lg bi translated"><a class="ae iu" href="https://github.com/thunderInfy/imagewoof" rel="noopener ugc nofollow" target="_blank">图像数据集链接</a></li><li id="e07f" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js ld le lf lg bi translated"><a class="ae iu" href="https://github.com/thunderInfy/imagenette" rel="noopener ugc nofollow" target="_blank"> Imagenette数据集链接</a></li></ul><h1 id="823e" class="lm ju hi bd jv ln lo lp jz lq lr ls kd lt lu lv kg lw lx ly kj lz ma mb km mc bi translated">参考</h1><ol class=""><li id="5eab" class="ky kz hi ix b iy ko jc kp jg mh jk mi jo mj js mr le lf lg bi translated">【用于无监督视觉表征学习的动量对比，何，，范，，谢赛宁，和Ross Girshick </li><li id="a039" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js mr le lf lg bi translated"><a class="ae iu" href="https://arxiv.org/pdf/2003.04297.pdf" rel="noopener ugc nofollow" target="_blank">用动量对比学习改进基线，陈，范，Ross Girshick，何</a></li><li id="7535" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js mr le lf lg bi translated">视觉表征对比学习的简单框架，陈婷、西蒙·科恩布利思、穆罕默德·诺鲁齐和杰弗里·e·辛顿。</li><li id="1be3" class="ky kz hi ix b iy lh jc li jg lj jk lk jo ll js mr le lf lg bi translated"><a class="ae iu" href="https://arxiv.org/pdf/1807.03748.pdf" rel="noopener ugc nofollow" target="_blank">对比预测编码的表征学习，Aaron van den Oord，Yazhe Li和Oriol Vinyals </a></li></ol></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><p id="a0e4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="mg">原载于2020年8月13日</em><a class="ae iu" href="https://www.analyticsvidhya.com/blog/2020/08/moco-v2-in-pytorch/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2020/08/moco-v2-in-py torch/</a><em class="mg">。</em></p></div></div>    
</body>
</html>