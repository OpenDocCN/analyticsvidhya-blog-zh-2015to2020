<html>
<head>
<title>Take a peek at Deep Reinforcement Learning for NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">看一看NLP的深度强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/take-a-peek-at-deep-reinforcement-learning-for-nlp-a8e37fbd7640?source=collection_archive---------2-----------------------#2019-09-27">https://medium.com/analytics-vidhya/take-a-peek-at-deep-reinforcement-learning-for-nlp-a8e37fbd7640?source=collection_archive---------2-----------------------#2019-09-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bb0405fe2f467fa5a42f5e798496e792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HpKh-MGZIq-GyRAiqEUnSw.png"/></div></div></figure><p id="bf4a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本博客将致力于发展对深度神经网络架构的基本理解，该网络旨在处理以自然语言为特征的状态和动作空间。本博客的实验结果被微软关于 <a class="ae jp" href="https://arxiv.org/pdf/1511.04636v5.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jo">用自然语言动作空间</em> </a> <em class="jo">进行深度强化学习的研究工作参考。</em></p></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><h1 id="f028" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">什么是强化学习？</h1><p id="522b" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">因此，一如既往，我的这个博客也将从头开始，我们将从理解什么是强化学习开始。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es la"><img src="../Images/6694258426e33359e15dc2150764f947.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*QVsnwatDVz8wcqJUsLJejw.png"/></div></figure><p id="d6ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">强化学习是<em class="jo">机器学习</em>的一个领域，因此也是<em class="jo">人工智能</em>的一个分支。它允许机器和软件代理自动确定特定上下文中的理想行为，以最大化其性能。简单的奖励反馈是代理学习其行为所必需的，这就是所谓的强化信号。</p><p id="6014" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这只是人类如何开始学习事物，我们如何通过知道什么是对的和什么是错的而进化。以一个刚出生的婴儿为例，他不知道发生了什么？怎么办？随着他的成长，他的父母阻止他做那些不应该做的事情，当他在沙发上撒尿时，他会收到母亲的负面反馈。当他问候客人时，他得到了积极的反馈。这就是强化学习！！</p><p id="351d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">孩子是代理人，他的父母是环境。代理执行一项任务，并获得相应的奖励(无论是负面的还是正面的)。这个想法是训练机器像人类一样思考和行动。</p><p id="74aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">强化学习允许机器或软件代理基于来自环境的反馈来学习其行为。这种行为可以一劳永逸地学会，或者随着时间的推移不断适应。如果仔细地对问题建模，一些强化学习算法可以收敛到全局最优；这是最大化回报的理想行为。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/8fff3ae40d9e2df3117e2bb71f99c4ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*ofRVirC98HxduA5Mcl_0jQ.png"/></div></figure><h2 id="6b2e" class="lg jy hi bd jz lh li lj kd lk ll lm kh jb ln lo kl jf lp lq kp jj lr ls kt lt bi translated">强化学习模型:</h2><ul class=""><li id="710b" class="lu lv hi is b it kv ix kw jb lw jf lx jj ly jn lz ma mb mc bi translated">环境状态集:S</li><li id="c5bf" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">动作集:A</li><li id="5905" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">状态之间的转换规则</li><li id="204a" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">决定状态转换的直接回报的规则</li><li id="af14" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">描述代理观察内容的规则</li></ul><h2 id="1331" class="lg jy hi bd jz lh li lj kd lk ll lm kh jb ln lo kl jf lp lq kp jj lr ls kt lt bi translated">q-学习:</h2><ul class=""><li id="bdaa" class="lu lv hi is b it kv ix kw jb lw jf lx jj ly jn lz ma mb mc bi translated">它用于学习强化学习的策略。</li><li id="5f2c" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">策略:在给定当前状态的情况下，代理选择操作时应遵循的规则。</li><li id="bfd1" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">Q-Learning:为决策过程找到最佳策略。</li><li id="9b7b" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">方法:学习一个行动值函数，也称为Q函数，它计算在训练收敛后采取行动的预期效用。</li><li id="7b47" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">Q-function[Q(s，a)]:返回状态s下动作a的Q值。</li></ul><h2 id="7b82" class="lg jy hi bd jz lh li lj kd lk ll lm kh jb ln lo kl jf lp lq kp jj lr ls kt lt bi translated">q值:</h2><p id="0f77" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">在统计假设检验，特别是多重假设检验中，<strong class="is hj"><em class="jo">q</em>-值</strong>提供了一种控制肯定错误发现率(pFDR)的手段。正如<em class="jo"> p </em>值给出了通过拒绝具有等于或小于<em class="jo"> p </em>值的任何结果的零假设而获得的预期假阳性率一样，<em class="jo"> q </em>值给出了通过拒绝具有等于或小于<em class="jo"> q </em>值的任何结果的零假设而获得的预期pFDR。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/1d2579489aacf14bf3644c559818736e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ndqY3G90iUeIzwcwWL-9Cw.png"/></div></div></figure><p id="33df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我敢肯定，你们中的一些人听说过“AlphaGo”游戏，或者肯定听说过一些机器人击败了世界冠军。所有的荣誉都归功于深度Q网络。</p><p id="917b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，需要理解的是，当训练学习者玩像AlphaGo和象棋这样的游戏时，智能体的动作空间很小，但状态空间很大。例如，在国际象棋游戏中，状态空间是整个20X20棋盘，但行动空间很小，就像棋子只能在1-2个方向上移动。深度Q网络已经被证明是非常有效的。</p><p id="76e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是如果我们讨论语言理解的强化学习呢？</p><h1 id="d09c" class="jx jy hi bd jz ka mj kc kd ke mk kg kh ki ml kk kl km mm ko kp kq mn ks kt ku bi translated">语言理解的强化学习</h1><h2 id="7a91" class="lg jy hi bd jz lh li lj kd lk ll lm kh jb ln lo kl jf lp lq kp jj lr ls kt lt bi translated">文本理解的顺序决策问题；</h2><ul class=""><li id="9764" class="lu lv hi is b it kv ix kw jb lw jf lx jj ly jn lz ma mb mc bi translated">例如，对话、任务完成、基于文本的游戏…</li><li id="48d1" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">代理在时间t观察作为文本串的状态，例如state-text s(t)。</li><li id="b4e9" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">代理也知道一组可能的动作，每一个都被描述为一个字符串文本，例如，动作文本</li><li id="430a" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">代理试图理解“状态文本”和所有可能的“行动文本”，并采取正确的行动——正确意味着最大化长期回报。</li><li id="9636" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">然后，环境状态转换到一个新的状态，代理人立即收到奖励。</li></ul><h2 id="9b2f" class="lg jy hi bd jz lh li lj kd lk ll lm kh jb ln lo kl jf lp lq kp jj lr ls kt lt bi translated">NLP的RL中的无界动作空间；</h2><p id="b177" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">不仅状态空间很大，动作空间也很大。动作的特征是无限的自然语言描述。例如，如果对模型说“嘿！你过得怎么样？我只是在等神盾局的数据流，但是没电了。好吧，我的这个输入文本是模型的状态空间(相当重)，动作空间是每个可用的文本组合(或者无穷大)。这个问题对于这样一个巨大的行动空间来说，仍然是深Q网络中的问题。于是提出了深度强化关联网络。</p><h2 id="e78c" class="lg jy hi bd jz lh li lj kd lk ll lm kh jb ln lo kl jf lp lq kp jj lr ls kt lt bi translated">深度强化关联网络(DRRN):</h2><p id="fb03" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">DRRN的思想是将状态和动作投射到一个连续的空间(作为向量)。q函数是状态向量和动作向量的相关函数。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/d024a22dfcfc5315e899574197e055ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lV0CDDv-a4Up1phxlhUSlA.png"/></div></div></figure><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/132c141271e375e672a5d37925e5aa2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cbR1YiI74tvRsI4caXnLAQ.png"/></div></div></figure><p id="8bf2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图2说明了利用内积交互作用函数的学习(DRRN的应用)。我们使用主成分分析(PCA)将100维的最后隐藏层表示(在内积之前)投影到2-D平面。向量嵌入以小值开始，并且在600集的经验重放训练之后，嵌入非常接近收敛嵌入(4000集)。最优动作(动作1)的嵌入向量收敛到与状态嵌入向量的正内积，而动作2收敛到负内积。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/1115fc51126121d3f3e15da2d7c0b010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tpcxTUIb5-MrjsCszc8nvw.png"/></div></div></figure><p id="2992" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图显示了不同模型的学习曲线，其中DQNs和DRRN中的隐藏层的维数都设置为100。经过大约4000集的经验回放训练后，所有的方法都趋同了。DRRN比其他三个基线收敛得更快，并获得更高的平均回报。我们假设这是因为DRRN架构更善于捕捉状态文本和动作文本之间的相关性。“拯救约翰”的较快收敛可能是由于较小的观测空间和/或其状态转移的确定性。</p><h2 id="a2c2" class="lg jy hi bd jz lh li lj kd lk ll lm kh jb ln lo kl jf lp lq kp jj lr ls kt lt bi translated">我们讨论的内容:</h2><ul class=""><li id="fd10" class="lu lv hi is b it kv ix kw jb lw jf lx jj ly jn lz ma mb mc bi translated">我们讨论了强化学习以及深度Q网络(DQN)如何在小行动空间任务(AlphaGo)中表现得非常好。</li><li id="ff88" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">用于NLP的深度强化学习(例如基于文本的游戏)为什么以及如何不同于一个动作空间很小的常规游戏。</li><li id="e396" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">讨论了NLP强化学习中的无界动作空间，以及深度强化关联网络(DRRN)如何在两个基于文本的游戏(拯救约翰和死亡机器)上更快地收敛。</li></ul></div></div>    
</body>
</html>