<html>
<head>
<title>Everything you need to know about Convolutional Neural Networks (CNNs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于卷积神经网络(CNN)你需要知道的一切</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/everything-you-need-to-know-about-convolutional-neural-networks-cnns-3a82f7aa29c5?source=collection_archive---------3-----------------------#2020-10-28">https://medium.com/analytics-vidhya/everything-you-need-to-know-about-convolutional-neural-networks-cnns-3a82f7aa29c5?source=collection_archive---------3-----------------------#2020-10-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/5b62db015d534b891aaae25fa004ccbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*lJ3wVL_JGs4udYZTNZJOng.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">不确定？看文章！</figcaption></figure><h1 id="d5aa" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">为什么我们需要卷积神经网络？</h1><p id="30b5" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">当像<strong class="jq hj">多层感知器</strong>这样的传统神经网络用于计算机视觉任务时，由于输入(平面化图像)的大尺寸和随后添加的多个互连的密集层，它们具有<strong class="jq hj">太多的可训练参数</strong>。在接受展平的输入时，它们还会<strong class="jq hj">忽略图像的空间信息</strong>。这就是卷积神经网络的用武之地！</p><h1 id="3ffd" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">使用 CNN 的优势</h1><ul class=""><li id="4394" class="km kn hi jq b jr js jv jw jz ko kd kp kh kq kl kr ks kt ku bi translated">与传统的多层感知器相比，它们使用更少的参数。</li><li id="47ab" class="km kn hi jq b jr kv jv kw jz kx kd ky kh kz kl kr ks kt ku bi translated">它们具有<strong class="jq hj">局部不变性</strong>和<strong class="jq hj">复合性。</strong></li></ul><p id="1bde" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">局部不变性</strong>允许我们将图像分类为包含特定对象，而不管<strong class="jq hj">中</strong>对象出现在图像的什么位置。<strong class="jq hj">另一方面，组合性</strong>允许网络<strong class="jq hj">在网络更深处学习更丰富的特征</strong>。我们一会儿会学到更多。</p><h1 id="a0cb" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是卷积？</h1><p id="1606" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">根据定义，卷积运算是两个矩阵的<strong class="jq hj">元素乘法，之后是求和</strong>。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/73a44d8f365882178b3c7228bb8e65df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gs8QPBVDvKOPyxCHojpeQQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">卷积的一个简单例子</figcaption></figure><p id="b773" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">在上面给出的例子中，我们<strong class="jq hj">将</strong>一个<strong class="jq hj"> 3x3 矩阵</strong>(过滤器/内核)与<strong class="jq hj"> 6x6 矩阵</strong>(输入/图像)进行卷积。对于每个卷积，我们取矩阵的元素乘积，对它们求和，并将它们放入结果矩阵<strong class="jq hj">的一个单元中</strong>(输出)。计算过程如图所示。我们一会儿将学习步幅和填充。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lo"><img src="../Images/cd7a5df8874d57367eb90465c50f3a41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*VJCoCYjnjBBtWDLrugCBYQ.gif"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">gif，显示过滤器如何在输入要素图中移动以生成输出要素图</figcaption></figure><p id="d4dc" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">上面显示的 gif 说明了过滤器在输入特征图上的<strong class="jq hj">移动。让我们以边缘检测滤波器为例来更好地理解这一点。</strong></p><p id="d239" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">下面给出了一个用于检测<strong class="jq hj">垂直边缘</strong>的 3×3 滤波器的例子。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lp"><img src="../Images/6e24fe90600c314f01de4696217a97b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3nJ1K_v3O37uXez4wVWhVA.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">垂直边缘检测滤波器</figcaption></figure><p id="2b56" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">该滤镜对图像上突然发生的<strong class="jq hj">强度值变化</strong>非常敏感。这一特性使其能够检测垂直边缘，这就是为什么 4x4 输出在原始图像中存在垂直边缘<strong class="jq hj">的地方具有高亮度值。</strong></p><p id="2e69" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">现在我们已经了解了卷积的工作原理，让我们看看卷积滤波器、非线性激活、池化和反向传播的组合如何产生能够自主学习滤波器的<strong class="jq hj">卷积网络</strong>，这些滤波器能够<strong class="jq hj">检测和提取图像中出现的高级和低级特征</strong>。</p><h1 id="e492" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">卷积神经网络的构建模块</h1><p id="50e8" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">下面给出了在构建能够执行复杂计算机视觉任务的传统 CNN 中起关键作用的层。</p><ul class=""><li id="4fcf" class="km kn hi jq b jr la jv lb jz lq kd lr kh ls kl kr ks kt ku bi translated">卷积层(CONV)</li><li id="5c10" class="km kn hi jq b jr kv jv kw jz kx kd ky kh kz kl kr ks kt ku bi translated">活化层</li><li id="d4e3" class="km kn hi jq b jr kv jv kw jz kx kd ky kh kz kl kr ks kt ku bi translated">汇集层(池)</li><li id="e67e" class="km kn hi jq b jr kv jv kw jz kx kd ky kh kz kl kr ks kt ku bi translated">全连接层</li><li id="1417" class="km kn hi jq b jr kv jv kw jz kx kd ky kh kz kl kr ks kt ku bi translated">批量标准化(BN)</li><li id="9107" class="km kn hi jq b jr kv jv kw jz kx kd ky kh kz kl kr ks kt ku bi translated">辍学</li></ul><p id="b3ff" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">当这些层以一定的顺序排列时，就形成了一个卷积神经网络(如下所示)</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lt"><img src="../Images/2b29dc9bd86438d260e61043302ed13b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yZSWvRdnt5LRQs3R3nc5Ug.jpeg"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">典型 ConvNet 架构示例</figcaption></figure><h1 id="87c9" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">1.卷积层(CONV)</h1><p id="2417" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">卷积层由具有特定宽度<strong class="jq hj">和高度</strong>的<strong class="jq hj"> K 个可学习滤波器</strong>组成(几乎总是相同的，因为它们是正方形)。</p><p id="cef7" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">对于 ConvNet 的<strong class="jq hj">输入</strong>，每个滤镜的<strong class="jq hj">深度</strong>等于图像中的<strong class="jq hj">通道数(例如 RGB 图像中的 3)，如下所示。</strong></p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lu"><img src="../Images/39a98fbf0d99e8bdc7696238858a2db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*QgiVWSD6GscHh9nt55EfXg.gif"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">卷积层，具有可学习的过滤器和偏差</figcaption></figure><p id="5586" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">在上面给出的 gif 中，我们使用了<strong class="jq hj">两个 3x3 滤镜</strong>并将它们与一个<strong class="jq hj"> 7x7 大小</strong>的<strong class="jq hj"> RGB 图像</strong>(有 3 个通道)进行卷积，补零等于 1(我们稍后会了解这一点)。可学习的参数是两个滤波器中的<strong class="jq hj"> 3x3x3x2 </strong> (54)权重，以及<strong class="jq hj">两个偏置项</strong>(每个滤波器一个)。我们使用的<strong class="jq hj">过滤器数量(K) </strong>，<strong class="jq hj">控制<strong class="jq hj">输出量</strong>的<strong class="jq hj">深度</strong>。在这种情况下，K=2，因此输出体积的深度也是 2。</strong></p><p id="f2c5" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">我们将滤波器的每一层与输入体积的每一层进行点积(因为我们确保它们具有相同的深度)，然后将每个元素(以及偏置项)相加，并将输出附加到输出体积中。跨距值是 2，所以我们跳过两个像素(我们稍后会了解这一点)。</p><p id="40ca" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">这些维度是不是有点混乱？在我们了解用于确定它们的简单公式之前，让我们先了解一下<strong class="jq hj">步幅和填充</strong>。</p><h2 id="44ce" class="lv ir hi bd is lw lx ly iw lz ma mb ja jz mc md je kd me mf ji kh mg mh jm mi bi translated">进展</h2><p id="b834" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">跨距是一个决定<strong class="jq hj">如何在输入音量中移动过滤器</strong>的值。例如，2 的步幅<strong class="jq hj">值意味着我们<strong class="jq hj">在每一步跳过 2 个像素</strong>。这将使输出尺寸低于输入尺寸(下采样)，如下所示。</strong></p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/0cb41d80675690b325b39c724f94938d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*XlHVz54zkYmnaUXejEf5Dg.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">显示步幅值为 2 的示例</figcaption></figure><p id="af40" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">此图说明了当 S=2 时，我们如何在执行卷积运算时跳过两个像素(沿 x 轴两个像素，沿 y 轴两个像素)。</p><h2 id="1608" class="lv ir hi bd is lw lx ly iw lz ma mb ja jz mc md je kd me mf ji kh mg mh jm mi bi translated">填料</h2><p id="bb03" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">假设我们希望输出体积的大小与输入体积的大小相同，也就是说，我们不想进行下采样。实现这一点的一种方法是通过<strong class="jq hj">零填充</strong>(用零填充图像的边界)，以便输出音量的大小与输入音量相匹配。这种类型的填充(用 P 表示)称为“相同”填充，如下所示。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mk"><img src="../Images/d42cc12d5f2503245b5a52f88bfb1774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i676KhjvYxaQZH5JBwBw9w.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">填充(P) = 1 的零填充</figcaption></figure><p id="3fa7" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">上面解释的术语(步幅、填充和过滤器大小)的值不是任意设置的。我们遵循一套确定各种维度的规则。我们来看看规则。</p><p id="b6e6" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">使用的符号:</strong></p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es ml"><img src="../Images/bba8df912d762dddc284c9eab30ebf38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tP1IBeRA-i32_urM8Hw8HQ.png"/></div></div></figure><ol class=""><li id="4587" class="km kn hi jq b jr la jv lb jz lq kd lr kh ls kl mm ks kt ku bi translated">输出量由以下因素决定:</li></ol><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mn"><img src="../Images/0e11842752053121888b5dd124b7b14e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VAJeTfThPBHNlqVttGd5IQ.png"/></div></div></figure><p id="5977" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">上面的等式显示了输出的维度如何依赖于输入的<strong class="jq hj">维度、滤波器大小、填充和步幅值</strong>。</p><p id="49a5" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">2.为了构建一个有效的 CONV 层，下面的等式应该<strong class="jq hj">总是</strong>给出一个<strong class="jq hj">整数值</strong></p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/83c9b8c906ac7cdab71c1f69f38fb53f.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*8wu8GcQzMFU0A-TkJIKOvw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">该值应该是整数</figcaption></figure><h1 id="1723" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">2.活化层</h1><p id="5bad" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">激活层<strong class="jq hj">不完全是“层”</strong>，因为它<strong class="jq hj">没有任何可训练参数</strong>。在卷积层之后使用它来增加非线性，以便 CNN 可以学习复杂的非线性函数。像<strong class="jq hj"> ReLU </strong>、eLU、或者漏 ReLU 这样的激活函数，都是在 ReLU 引领下流行使用的(w.r.t 流行度和结果)。在这一层中，输出维度与输入维度相同。ReLU(校正线性单位)函数如下所示:</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/430fd7265d217b5cb7a40c38a5c6ec6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*0jrJkmWrGVmeccnRo8sImg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">ReLU 函数</figcaption></figure><p id="0345" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj"> ReLU </strong>功能是<strong class="jq hj">使用最广泛的</strong>激活功能。主要是因为与其他激活功能不同，它<strong class="jq hj">不会同时激活所有的神经元</strong>。从上面给出的公式中，我们可以注意到，它将所有的负输入转换为零，神经元没有被激活。实际上，ReLU 的收敛速度比 tanh 和 sigmoid 激活函数快六倍。</p><h1 id="3562" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">3.汇集层(池)</h1><p id="ed09" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">汇集层用于逐渐<strong class="jq hj">减小输入体积的空间大小</strong>(高度和宽度)(来自前一层)。执行<strong class="jq hj">下采样</strong>是一种<strong class="jq hj">简单</strong>的方式，有助于<strong class="jq hj">减少</strong>可训练参数的<strong class="jq hj">数量，有助于<strong class="jq hj">控制过拟合</strong>。有两种主要类型的池:最大和平均池。</strong></p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/546a2b3522a072f591d64ef6f617ed36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*qGjcCGjQnlPc5f7f5mUZCg.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">最大池和平均池</figcaption></figure><h2 id="898c" class="lv ir hi bd is lw lx ly iw lz ma mb ja jz mc md je kd me mf ji kh mg mh jm mi bi translated">最大池化</h2><p id="adcc" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">Max pooling 只是取<strong class="jq hj">特征图</strong>的<strong class="jq hj">每个面片</strong>中的<strong class="jq hj">最大值</strong>。通过提供表示的抽象形式<strong class="jq hj">，这样做是为了<strong class="jq hj">防止过度拟合</strong>。它还<strong class="jq hj">通过减少可学习参数的数量来降低计算成本</strong>。</strong></p><h2 id="b3e0" class="lv ir hi bd is lw lx ly iw lz ma mb ja jz mc md je kd me mf ji kh mg mh jm mi bi translated">平均池</h2><p id="d2cc" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">平均池只是取特征图中每个面片的<strong class="jq hj">平均值。它将所有值都考虑在内，并将其传递到下一层，这意味着所有值都用于要素映射和生成输出。</strong></p><h1 id="ba95" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 4。全连接层(FC) </strong></h1><p id="7727" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">全连接层由<strong class="jq hj">堆叠的神经元</strong>组成，这些神经元<strong class="jq hj">全连接</strong>到<strong class="jq hj">前一层</strong>中的所有<strong class="jq hj">激活</strong>。这些层与传统多层感知器中使用的层相同。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mr"><img src="../Images/03cf20781ad2dc3ce58f180a96f041e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZ64eGkTGWE4f_nxa1ATnQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">CONV 层的展平输出被传递到 FC 层</figcaption></figure><p id="9ade" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">这些层</strong>通常在接近 ConvNet 末端的<strong class="jq hj">处被发现，并被用于<strong class="jq hj">优化目标</strong>，例如类分数(在分类问题中)。</strong></p><h1 id="cc79" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">5.批量标准化(BN)</h1><p id="b06a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">批量标准化层在激活层之后应用，并用于在将给定输入量的激活值传递到网络中的<strong class="jq hj">下一层</strong>之前<strong class="jq hj">标准化</strong>。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es ms"><img src="../Images/f610453becee0d330b25987c4a09548f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IJLwrLmG4BTStE6_tImZjQ.png"/></div></div></figure><p id="77d8" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">上面给出了<strong class="jq hj">归一化输入</strong>的方程式。ε被设置为较小的值(以防止被零除)</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mt"><img src="../Images/36af15108ab54058caab0e1335242323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ggl_1pyEo0wUx5APYuJh9w.png"/></div></div></figure><p id="d951" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">计算<strong class="jq hj">小批量</strong>的平均值和方差的公式，用于标准化输入。</p><p id="4622" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">批处理规范层在<strong class="jq hj">减少<strong class="jq hj">历元</strong>的数量<strong class="jq hj">训练网络</strong>方面极其有效，即它有助于<strong class="jq hj">更快</strong>和<strong class="jq hj">更稳定的训练</strong>。它还有助于<strong class="jq hj">减少内部协变量偏移</strong>(网络中各层输入分布的变化)。</strong></p><h1 id="b9d5" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">6.脱落层</h1><p id="c1d0" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">脱落层<strong class="jq hj">是 ConvNet 的一种正则化形式</strong>，因为它旨在以测试精度为代价，通过提高测试精度来防止过拟合。</p><p id="6fa2" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">对于我们的训练集中的每个小批量，丢弃层(具有我们设置的概率 p)<strong class="jq hj">随机断开网络架构中从前一层到下一层的输入</strong>。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mu"><img src="../Images/d710190fbdc795f0ec3433de029bd853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*II8c0lO4yZpR46k8oG--ng.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">p=0.5 时的辍学</figcaption></figure><p id="2c22" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">Dropout 确保多个冗余节点在出现类似输入时激活。这有助于我们的模型更好地概括。</p><p id="b2ca" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">综上所述，我们来看看<strong class="jq hj"> AlexNet </strong>，这是 Alex Krizhevsky 在 2012 年设计的一款非常著名的 ConvNet。它遵循:</p><p id="afa9" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">-&gt;池- &gt; Conv- &gt;池- &gt; F.C- &gt; Softmax </strong>种流。Softmax 用于生成类别概率。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mv"><img src="../Images/30cfd28b1a0173ecff5d52be3ca0879e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3wLKU1Zmmq1mZLpSNFmGQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">AlexNet</figcaption></figure><p id="0451" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">在下一篇文章的<strong class="jq hj">中，我们将了解更多关于<strong class="jq hj"> CNN 架构</strong>的内容，比如上面展示的 AlexNet。</strong></p><p id="b384" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">CNN 报道到此结束。希望这篇文章有助于提高你的理解。</p><p id="9829" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">这篇文章的内容是互联网上各种来源的信息的混合体，结合了我对它们的理解。</p><p id="90b8" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">参考资料:</p><ul class=""><li id="2d9e" class="km kn hi jq b jr la jv lb jz lq kd lr kh ls kl kr ks kt ku bi translated">Adrian rose Brock——使用 Python 进行计算机视觉的深度学习</li></ul><div class="mw mx ez fb my mz"><a href="https://cs231n.github.io/convolutional-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hj fi z dy ne ea eb nf ed ef hh bi translated">用于视觉识别的 CS231n 卷积神经网络</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">目录:卷积神经网络非常类似于以前的普通神经网络…</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">cs231n.github.io</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn ik mz"/></div></div></a></div><p id="d9eb" class="pw-post-body-paragraph jo jp hi jq b jr la jt ju jv lb jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">如果你喜欢这篇文章，请给它鼓掌，并确保在评论中留下任何反馈/建议/问题:)</p></div></div>    
</body>
</html>