<html>
<head>
<title>Gradient Descent from Scratch: Understanding &amp; Implementing the Algorithm on Boston Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的梯度下降:在波士顿数据集上理解和实现算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-from-scratch-understanding-implementing-the-algorithm-on-boston-dataset-9d916b89d697?source=collection_archive---------4-----------------------#2019-12-09">https://medium.com/analytics-vidhya/gradient-descent-from-scratch-understanding-implementing-the-algorithm-on-boston-dataset-9d916b89d697?source=collection_archive---------4-----------------------#2019-12-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/432ada9b4b498dea3d05729730b316bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MC0AlHDYopom9Mz3.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">马萨诸塞州波士顿的天际线。</figcaption></figure><p id="046a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">梯度下降是一个非常基本的算法，每个开始机器学习之旅的人在最开始都很熟悉。</p><blockquote class="js jt ju"><p id="1ebe" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr hb bi translated">我们先来听听定义:梯度下降是一种寻找函数最小值的优化算法(像代价函数)。为了使用梯度下降找到函数的局部最小值，人们采取与当前点处梯度的负值成比例的步骤。</p></blockquote><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es jz"><img src="../Images/2d8e6922cdf31447554b5094ec41fd51.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*4wYeb3VpTIBb6wW8wYvOmQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">我们必须收敛到成本w.r.t参数(w)最小的点，才能胜出。</figcaption></figure><p id="8a77" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因为这篇文章是关于算法的实现，所以我假设你精通算法的理论和工作。尽管我们将在讨论中阐明这一点。现在就上车吧！</p><h1 id="f6ec" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated"><strong class="ak">我们将在著名的波士顿数据集上实现批量梯度下降。</strong></h1><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/7baa8d72281eb44db56519411d2f567f.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/0*uI5E67mySKCzClwu.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">让我们对批量梯度下降进行编码</figcaption></figure><h2 id="b147" class="ld kf hi bd kg le lf lg kk lh li lj ko jf lk ll ks jj lm ln kw jn lo lp la lq bi translated">导入基本库</h2><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="be41" class="ld kf hi ls b fi lw lx l ly lz">import numpy as np<br/>import pandas as pd</span></pre><h2 id="8727" class="ld kf hi bd kg le lf lg kk lh li lj ko jf lk ll ks jj lm ln kw jn lo lp la lq bi translated">从sklearn导入波士顿数据集</h2><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="e2c9" class="ld kf hi ls b fi lw lx l ly lz">from sklearn.datasets import load_boston<br/>boston = load_boston()<br/>boston.keys()</span></pre><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/fd9baa7d35046369811dac4dc49ef5e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nMyx65vclDryEtrAGn1JYQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">波士顿数据集中存在的键</figcaption></figure><h2 id="c12f" class="ld kf hi bd kg le lf lg kk lh li lj ko jf lk ll ks jj lm ln kw jn lo lp la lq bi translated">让我们先将原始数据和目标值保存到一些变量中，然后再修改它们。</h2><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="2120" class="ld kf hi ls b fi lw lx l ly lz">X = boston.data<br/>y = boston.target</span></pre><h2 id="fb81" class="ld kf hi bd kg le lf lg kk lh li lj ko jf lk ll ks jj lm ln kw jn lo lp la lq bi translated">现在，让我们将数据转换成熊猫数据框架，并对其进行一点探索</h2><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="5b65" class="ld kf hi ls b fi lw lx l ly lz">boston = pd.DataFrame(boston.data, columns = boston.feature_names)</span><span id="1416" class="ld kf hi ls b fi mb lx l ly lz">boston.head()</span></pre><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/2945247c22e6e9065dd4464c5676ac23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bVRLnfDBNzxUMd2q8nPcag.png"/></div></div></figure><p id="97b2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jv">输入数据是我们的训练数据，有13个不同的特征列，每个列有506个实例。</em></p><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="725b" class="ld kf hi ls b fi lw lx l ly lz">boston.describe()</span></pre><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/db3937d97b70ebc0be0bb186f676755e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FeQxnyZ-UAcmNLUSOVGdEg.png"/></div></div></figure><blockquote class="js jt ju"><p id="b960" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr hb bi translated"><em class="hi">我们可以清楚地看到数据的不规则规模，因为我们要实施梯度下降，当规模不合适时，需要很长时间才能收敛到参数的最优值，所以我们需要对数据进行缩放。</em></p></blockquote><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="3e41" class="ld kf hi ls b fi lw lx l ly lz">from sklearn.preprocessing import StandardScaler<br/>Scale = StandardScaler()<br/>boston = Scale.fit_transform(X)</span><span id="5764" class="ld kf hi ls b fi mb lx l ly lz">#since StandardScaler returns the output it numpy array form we need to convert it into dataframe again with accurate column names.</span><span id="4ed9" class="ld kf hi ls b fi mb lx l ly lz">boston = pd.DataFrame(boston, columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'])</span><span id="8982" class="ld kf hi ls b fi mb lx l ly lz">boston.describe()</span></pre><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/a5a14ba09d498a5969724b3489acd4ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A_URXxTKQztgGpuXoKkDdA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">与之前的比例相比，现在的数据比例看起来更好(现在观察要素的标准差、最大值和最小值)</figcaption></figure><p id="b9a2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">看起来更好！现在，在应用梯度下降之前，我们必须在数据集中添加一个所有值都等于1的列，为什么要添加一个额外的列呢？因为对于每个特征(x1，x2，x3…)在参数向量(θ)中都有一个参数(weight-m1，m2，m3…)。我们还必须注意(y = m1.x1 + m2.x2 + …)中的<strong class="iw hj">偏置项(b) </strong>。+ ' <strong class="iw hj"> b </strong>')用于每个特征x[j]。类似地，在参数向量中，每个特征都有一个权重(m1，m2…)和一个偏差项“b”的权重。</p><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="e24c" class="ld kf hi ls b fi lw lx l ly lz">boston['bias']=1<br/>boston</span></pre><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/a92810d2c23e6233a01e8a27620127b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j4qsq4weQZLNCukROGnN9g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">偏置列被添加到最右边</figcaption></figure><blockquote class="js jt ju"><p id="6cfc" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr hb bi translated">现在我们将数据分为训练数据和测试数据</p></blockquote><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="939d" class="ld kf hi ls b fi lw lx l ly lz">from sklearn.model_selection import train_test_split<br/>X_train, X_test , y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)</span></pre><p id="a2a4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="jv">现在我们来谈谈！我们已经做了一些必要的预处理，让我们写下将使用梯度下降优化参数的代码，以便最小化成本函数</em> </strong>。</p><p id="60ac" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里的成本函数是什么？</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/4877c2b8da49bd10e4fccf1f146f9cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*8kUi7BghDehfZ7ORGl7jXg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd kg">这是我们的成本函数</strong></figcaption></figure><p id="863d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是均方误差，因为第一个“Y”项是我们计算的目标值，并且我们知道使用线的等式，Y = m.x + c</p><p id="0e79" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，我们可以将“成本”函数定义如下</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/4fb831786512e177b57df7e20b92de4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*E56ENiIIdEXF1q7TaMTC-g.png"/></div></figure><p id="49a5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果你学过均方误差函数，我希望你熟悉这些符号。让我们再回忆一下。</p><p id="ce44" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">大写&amp; <strong class="iw hj">粗体</strong>(有些是粗体，有些只是斜体)的符号是向量，而小写&amp;斜体的符号是标量。</p><p id="1002" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jv">m</em>’:实例数</p><p id="c883" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jv">(一):</em>第I个数据实例</p><p id="cf15" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jv"> y(i): </em>它是第I个实例对应的实际目标值</p><p id="7b4e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">X: 它是输入向量。<strong class="iw hj"> X(i) </strong>是输入的第I个实例。</p><p id="eb84" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">θ(Theta)</strong>:<strong class="iw hj">θ</strong>是对应于每个特征(m1，m2，m3…..mn，b)<strong class="iw hj">θ</strong>向量取转置形式，以便取其与输入向量<strong class="iw hj"/><em class="jv">(y(I)= m . X(I)+b)</em>的点积</p><blockquote class="js jt ju"><p id="233c" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr hb bi translated">在进入编码部分之前，让我们讨论一下这个算法的关键。</p></blockquote><p id="4011" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">梯度下降的基本思想是迭代地调整参数，以便最小化成本函数。梯度下降测量误差函数相对于参数向量θ的局部梯度，它沿着梯度下降的方向前进。一旦梯度为零，我们就到达了最小值。</p><p id="4005" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们如何从这里开始？我们从取一些随机值到θ向量(包含权重m1，m2…)开始，这也叫做<em class="jv">随机初始化。</em>我们使用随机初始化的θ计算成本函数。我们肯定会收到一个非常不可取的和巨大的MSE(成本)值。然后，我们将逐步改进它，每次采取微小的步骤，旨在降低成本函数，直到它最终收敛到最小值。<br/>这张图更符合我们目前的讨论:</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/bedf6f4276ac14eb28483a678580dd14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1W7EnJifMS9zxa6F.png"/></div></div></figure><p id="dc9c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">取成本函数w.r.t的梯度作为参数(θj)意味着计算如下:</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/49ba4f6d6afa4fa145814b98ced8db94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*6KaS6L-FWunBcJBaXOiSpg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd kg">这是成本函数的偏导数或参数j的‘梯度’</strong></figcaption></figure><blockquote class="js jt ju"><p id="becf" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr hb bi translated">看这里:对<em class="hi">成本</em>求偏导数就是术语<em class="hi">梯度</em>的含义。<br/>将成本降至最低，重量(θ)就是梯度下降的意义所在。</p></blockquote><p id="b254" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">还有一点需要注意的是，这个非常重要的家伙叫做<strong class="iw hj"><em class="jv"/></strong>学习率，用η表示。这是梯度下降中的一个超参数，它基本上控制着我们在每一步从当前梯度值下降多少(向最小值的递增步骤——见上图)。很明显，如果您认为η越小，算法将花费更多时间收敛到最小值(或更多迭代/步骤)。同样，η越大，它可能收敛得更快，但也有跳到曲线另一侧的风险。</p><p id="0d88" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">从那以后，我们谈了很多。现在让我们进入正题，对我们讨论的所有内容进行编码。</strong></p><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="6188" class="ld kf hi ls b fi lw lx l ly lz">X_train = np.array(X_train)</span></pre><p id="6746" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，让我们定义我们的成本函数:</p><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="f0d8" class="ld kf hi ls b fi lw lx l ly lz">def cost(X, y, theta):<br/>    k = X.shape[0]<br/>    total_cost = 0<br/>    for i in range(k):<br/>        total_cost += 1/k * (y[i] - ( theta.dot(X[i,:]) )**2<br/>    return total_cost</span></pre><blockquote class="js jt ju"><p id="b43b" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr hb bi translated"><em class="hi">因此，如果我们试图写出它的方程式</em>，这个成本函数就是我们上面讨论过的。</p></blockquote><p id="f555" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，我们将首先为梯度下降的1个步骤编写代码，随机初始化参数，使用它们计算梯度，然后通过从它们中减去梯度来更新参数值。我们称这个函数为<em class="jv">阶跃梯度</em></p><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="38b8" class="ld kf hi ls b fi lw lx l ly lz">def step_gradient(X, y, learning_rate, theta):<br/>    k = X.shape[0]<br/>    n = X.shape[1]<br/>    gradients = np.zeros(n)<br/>    for i in range(k):<br/>        for j in range(n):<br/>            <strong class="ls hj">gradients[j] += (-2/k) * ( y[i] - (theta.dot(X[i,:])) ) * X[i,j]</strong><br/>    theta = theta - learning_rate * gradients<br/>    return theta</span></pre><blockquote class="js jt ju"><p id="ba99" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr hb bi translated"><strong class="iw hj">这里发生了什么:</strong>上面的函数是算法的关键，看看它清楚地做了什么，对于存储在“theta”中的每个权重(下标“j”)，它使用我们上面讨论的相同等式计算梯度(gradient[j])，即成本w.r.t .对该权重或参数(j)的偏导数。运行完整的循环将得到完整的梯度向量，它包含14个值，分别对应于14个参数(存储在θ中),您刚刚根据这些参数计算了这些梯度。<br/>一旦我们有了梯度向量，它指向抛物线上的某个位置，如上图所示，我们只需要沿着相反的方向向下下降，朝着成本最小的地方下降(参考图片)，这就是我们通过从θ减去梯度在数学上所做的。我们也用学习率来控制我们需要下降或减少多少。</p></blockquote><p id="a0e3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，那只是我们下降的1步，它不会在仅仅1次迭代中收敛！<br/>我们现在需要编写主梯度下降函数，该函数将为定义的迭代次数(也称为历元)调用阶跃梯度函数。<br/>此外，还记得我们讨论过θ的随机初始化，这里我们将初始化我们的θ，它将在上面的阶跃梯度函数中使用。</p><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="c704" class="ld kf hi ls b fi lw lx l ly lz">def gradient_descent(X, y, learning_rate, iterations):<br/>    k = X.shape[0]<br/>    n = X.shape[1]<br/>    theta = np.zeros(n)                <em class="jv">#random initialization</em><br/>    for i in range(iterations):</span><span id="791c" class="ld kf hi ls b fi mb lx l ly lz">        theta = step_gradient(X, y, learning_rate, theta)<br/>        print(i, 'cost:', cost(X, y, theta))</span><span id="8ea1" class="ld kf hi ls b fi mb lx l ly lz">    return theta</span></pre><p id="8ac0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">搞定了。我们已经记下了主要业务。我还做的是，在每次迭代之后，我将打印更新的成本值，以查看我们如何改进。<br/>现在，让我们巧妙地定义一个“运行”函数，我们可以使用它来运行我们上面编码的所有内容，它将为算法提供参数和训练数据。</p><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="5309" class="ld kf hi ls b fi lw lx l ly lz">def run(X, y):<br/>    learning_rate = 0.04<br/>    iterations = 300<br/>    theta = gradient_descent(X, y, learning_rate, iterations)<br/>    return theta</span></pre><p id="6123" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们在我们的数据上试试这个</p><pre class="ka kb kc kd fd lr ls lt lu aw lv bi"><span id="b726" class="ld kf hi ls b fi lw lx l ly lz">theta = run(X_train, y_train)</span></pre><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/541f4b3a1037a8bcb8fd4cfa2f756939.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*MRpNo368AgaBXSgwUWQAOA.png"/></div></figure><p id="46e3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">太棒了。它工作得非常好。就看看它是怎么开始的，随机初始化参数，成本是490。我们逐渐开始大幅度降低成本，经过300次迭代后(下图),我们成功地将成本降低到21。</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/4151d0f24f5f2d3683661bfdc2b2b158.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*uxRhIkXaNfrTmzbM4j9oKg.png"/></div></figure><p id="62b1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">所以，这是一个好的开始和一个好的结局。我试着按照别人教我的方式写下每一件事。自己实现它会给你很多清晰的东西，这是你仅仅通过学习概念或使用Scikit-Learn所不能得到的。如果你觉得在某个地方卡住了，或者有任何不清楚的地方，请随时通过Linkedin 联系我。谢谢你。</p><p id="ce0c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jv">都是乡亲们！:)</em></p></div></div>    
</body>
</html>