<html>
<head>
<title>Text Analytics - Mining the Abundance of Wealth underneath the vastness of Text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分析——挖掘浩瀚文本下的丰富财富</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-analytics-mining-the-abundance-of-wealth-underneath-the-vastness-of-text-324149e9e449?source=collection_archive---------0-----------------------#2018-12-31">https://medium.com/analytics-vidhya/text-analytics-mining-the-abundance-of-wealth-underneath-the-vastness-of-text-324149e9e449?source=collection_archive---------0-----------------------#2018-12-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/f89a48ac4d76aae3a18c3d8828268533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*8FrPnHkWKwRaponJ9TPIKw.jpeg"/></div></figure><p id="6570" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi jk translated">三十年前，操作电脑和使用互联网可能是人们渴望拥有的技能。到了世纪之交，它不再是一项可以给你吹嘘权利的技能。成了必需品！就像用电的转变一样——在很短的时间内，从奢侈品变成了绝对的基本必需品。</p><p id="0565" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">也就是说，随着电脑/互联网成为我们教育的一部分，千禧一代学习如何使用电脑/互联网要容易得多。我们实时地体验着世界的变化，没有什么可以忘却的。对于前几代人来说，情况要艰难得多，因为他们习惯于每天在工作场所进行手工操作。</p><p id="8f5a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">很自然，几乎每个千禧一代都曾试图帮助他/她的父母学习如何操作电脑，进而学习互联网。十年前，在其中一次会议上，我在想，如果计算机和/或互联网能够以自然语言与我的父母互动，即理解他们的需求并给出期望的结果，那该有多好。父母和孩子都可以避免在这个话题上面对面的痛苦。当时这似乎是一厢情愿的想法。</p><p id="e6e4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">快进到今天。好，好，好，这就是现在的现实！！！想想Alexa enabled fire-stick，它使用户能够用自然语言与设备进行交互，设备转到所需的媒体来播放所需的视频。在计算机中实现类似的可行性(比如导航到特定位置、打开excel文件、保存文档、根据用户的口述写文章)即使还没有实现，也是触手可及的。</p><p id="7214" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在那张一厢情愿的清单上再加上几个，你就会有一个关于时间的白日梦。就像，我希望我不用起床就能打开房间的灯。我希望我和理发店的约会能定下来，而不用我打电话给理发店。从公司客户关怀的角度来看，CEO可能渴望减少手动回答各种常见问题的需求。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/53c037e665908b623162919c2cac5f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HgRbOhYMhA1ZOaFU.jpg"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">来源:https://bit.ly/2GLdxdB<a class="ae kc" href="https://bit.ly/2GLdxdB" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="abb8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们现在有了所有这些问题的答案。我们已经在KJO的<em class="kd"> Koffee with Karan </em>中看到Google Echo如何对自然语言做出反应并使房间的灯光变暗，我们看到桑德尔·皮帅启动Google Assistant，演示机器人与理发师互动以预约预约时间，现在几乎每个公司都有机器人回答客户的询问。</p><p id="55c0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所有这一切的关键是文本分析和自然语言处理取得的进步。文本分析是一个非常有趣的领域，在数据科学领域越来越重要。</p><p id="d361" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇文章中，我试图介绍文本分析的基础知识，以及如何将它用于情感分析。作为这篇文章的后续，我将介绍文本分析如何用于主题建模和网络分析。这里是我们将涉及到的主题的细节。</p><p id="63da" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">目录</strong></p><ol class=""><li id="e615" class="ke kf hi io b ip iq it iu ix kg jb kh jf ki jj kj kk kl km bi translated"><strong class="io hj">文本分析到底是什么？</strong></li><li id="4594" class="ke kf hi io b ip kn it ko ix kp jb kq jf kr jj kj kk kl km bi translated"><strong class="io hj">文本挖掘的基础——解码&amp;量化文本</strong></li><li id="e779" class="ke kf hi io b ip kn it ko ix kp jb kq jf kr jj kj kk kl km bi translated"><strong class="io hj">情感分析——一个应用</strong></li><li id="9cb4" class="ke kf hi io b ip kn it ko ix kp jb kq jf kr jj kj kk kl km bi translated"><strong class="io hj">多算法建模</strong></li><li id="8581" class="ke kf hi io b ip kn it ko ix kp jb kq jf kr jj kj kk kl km bi translated"><strong class="io hj">合奏的力量</strong></li></ol><h1 id="7ea2" class="ks kt hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">1.到底什么是文本分析？</h1><p id="0b97" class="pw-post-body-paragraph im in hi io b ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj hb bi translated">文本分析是检查大量书面资源以生成新信息，并将非结构化文本转换为结构化数据以用于进一步分析的过程。它是从文本中获取高质量信息的过程。总体目标是，通过应用自然语言处理(NLP)和分析方法，将文本转化为数据进行分析。</p><p id="099b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">文本挖掘可以识别隐藏在大量文本数据中的事实、关系和断言。这些事实被提取并转化为结构化数据，用于分析、可视化、与数据库或仓库中的结构化数据集成，以及使用机器学习(ML)系统进一步细化。</p><h1 id="e3fd" class="ks kt hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak"> 2。基础-解码&amp;量化文本</strong></h1><p id="584a" class="pw-post-body-paragraph im in hi io b ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj hb bi translated">虽然文本被认为是非结构化的，但在高级人类语言中包含着大量的复杂性和细微差别，这使得文本分析成为收集关于人们及其想法和感觉的见解的极其肥沃的土壤。</p><p id="60f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">挑战是显而易见的。数据是非结构化的，而且数量庞大，除非找到一种方法来量化数据的各个方面，否则信息推导将总是缓慢、主观和有限的。文本挖掘解决结构和规模问题的过程是数据科学的切入点。基本方法是将文本转化为数字，这样我们就可以使用机器来分析大量的文档，并通过数学算法来发现真知灼见。</p><p id="81ad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们看一个处理数据集的真实示例，以及将非结构化数据转换为结构化数据的步骤，以便能够根据我们的目标应用我们选择的算法。我下载了一个关于美国航空公司乘客推文的数据集。我们的目标是建立一个模型，该模型可以根据客户的评论来预测客户的情绪(无需人工干预来解释)。<strong class="io hj">数据可以从data.world下载，链接是</strong> <a class="ae kc" href="https://data.world/data-society/twitters-about-us-airline" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj">关于美国航空公司的推特</strong> </a> <strong class="io hj">。</strong></p><p id="62a3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">文本挖掘过程的第一步是以某种方式组织和构建数据，以便对其进行定性和定量分析。基础结构是将文本转换成术语文档矩阵(TDM)或文档术语矩阵(DTM)。此外，可以进行一些数据可视化，以获得一些关于哪些单词使用频率更高的初步见解，等等。因为在这个阶段可以做很多事情，但是下面给出了主要的或必要的必须做的事情</p><p id="db89" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">I .清理数据集中感兴趣的列，该列是我们想要从中获取信息的文本。清理基本上包括删除空白、链接、表情符号、非英语单词、数字、标点符号、非增值单词、不常用单词等步骤。</p><p id="cf24" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">二。绘制一个词云，它给出了tweets集合中词频的图形表示。</p><p id="70f0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">三。可视化，以初步了解目标变量的水平是如何分布的</p><p id="79a6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">四。将文本转换为矩阵格式，然后可用于模型构建。</p><p id="842d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下面演示了在深入研究模型构建之前，我将如何着手构建基础，以预测客户对某家航空公司的反馈时的情绪。</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="23f0" class="ma kt hi lw b fi mb mc l md me"><strong class="lw hj">#Import the data and check the dimensions</strong><br/>Tweets = read.csv("twitter-airline-sentiment.csv")</span><span id="001c" class="ma kt hi lw b fi mf mc l md me">dim(Tweets)<br/>[1] 14640    15</span><span id="2bfe" class="ma kt hi lw b fi mf mc l md me">names(Tweets)<br/> [1] "tweet_id"                     "airline_sentiment"            "airline_sentiment_confidence"g <br/> [4] "negativereason"               "negativereason_confidence"    "airline"                     <br/> [7] "airline_sentiment_gold"       "name"                         "negativereason_gold"         t10] "retweet_count"                "text"                         "tweet_coord"                 <br/>Variables of interest are <!-- -->airline-sentiment<!-- --> and <!-- -->text<!-- -->. The visual distribution  of various sentiments expressed can be seen below that was extracted from the data downloaded.e </span><span id="8c24" class="ma kt hi lw b fi mf mc l md me">a= ggplot(Tweets, aes(x=airline_sentiment))<br/>  <br/>a+geom_bar(aes(y=..count.., fill=airline_sentiment)) +<br/>  scale_fill_brewer(palette="Dark2") + labs(x="Emotion Categories", y="")+<br/>  ggtitle("Sentiment Comparison")+coord_flip()+<br/>  scale_x_discrete(labels=c("Positive","Neutral","Negative"))+guides(fill=FALSE)</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/3e57c65eeec85c64fb6bff9557e373a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*a-VO2OHIoZUthw3R1Gl_rw.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">大多数旅客对航空公司表现出积极的态度</figcaption></figure><p id="2182" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下一步是如上所述清理数据</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="b261" class="ma kt hi lw b fi mb mc l md me"><strong class="lw hj">#Preparing for sentiment analysis by creating a corpus of tweets</strong></span><span id="4d3e" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Call the required library<br/></strong>library(tm)</span><span id="8177" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Remove links, replace '&amp;amp' with 'and'</strong><br/>Tweets$text &lt;- gsub("http(s?)([^ ]*)", " ", Tweets$text, ignore.case = T)<br/>Tweets$text &lt;- gsub("&amp;amp", "and", Tweets$text)</span><span id="af21" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Remove words starting with @</strong><br/>Tweets$text=gsub("@\\w+ *", "",Tweets$text)</span><span id="7855" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Remove words starting with #</strong><br/>Tweets$text=gsub("#\\w+ *", "",Tweets$text)</span><span id="1432" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Convert the text column to corpus for futher data cleaning using tm library functions</strong><br/>Tweet.corpus=Corpus(VectorSource(Tweets$text))</span><span id="5843" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Checking the first 10 tweets in the corpus created</strong><br/>inspect(Tweet.corpus[1:10])</span><span id="1521" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Data (corpus/tweet) cleaning as part of data preparation</strong><br/>Tweet.corpus=tm_map(Tweet.corpus,tolower)<br/>Tweet.corpus=tm_map(Tweet.corpus,stripWhitespace)<br/>Tweet.corpus=tm_map(Tweet.corpus,removeNumbers)</span><span id="332e" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#converting/encoding elements of character vectors to the native encoding or UTF-8 respectively.</strong></span><span id="57f3" class="ma kt hi lw b fi mf mc l md me">Tweet.corpus=tm_map(Tweet.corpus, function(x) iconv(enc2utf8(x), sub = "byte"))<br/>Tweet.corpus=tm_map(Tweet.corpus,removePunctuation)<br/>more_stopwords=c(stopwords('english'),'http*',"<a class="ae kc" href="http://twitter.com/VirginAmerica" rel="noopener ugc nofollow" target="_blank">@VirginAmerica</a>","<a class="ae kc" href="http://twitter.com/NYTimes" rel="noopener ugc nofollow" target="_blank">@NYTimes</a>","flight","cancelled","thanks","AA","DM")<br/>Tweet.corpus=tm_map(Tweet.corpus,removeWords,more_stopwords)</span><span id="98e2" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#checking the first 10 tweets.</strong><br/>inspect(Tweet.corpus[1:10])</span><span id="1ec2" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#The 9th tweet shows an emoticon/non-english character.</strong><br/>[9] Well, I didn'tbut NOW I DO! :-D</span><span id="3b5b" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Remove non-eglish words if any</strong><br/>Tweet.corpus=tm_map(Tweet.corpus,function(x) iconv(x,"latin1","ASCII",sub = ""))<br/></span></pre><p id="530a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">词干</strong></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mh"><img src="../Images/3ea4f2f040379da6fa5ad63af98575bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*53qMGR9kcycyeREbkctWJQ.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">去除无附加值单词的词干的图形表示</figcaption></figure><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="510b" class="ma kt hi lw b fi mb mc l md me"><strong class="lw hj">#Remove words which are derivations of the root word.  For example, the stem of  "computational", "computers" and "computation" is "computer". Only computer is retained.Stemming reduces words to unify across documents.</strong></span><span id="f7cf" class="ma kt hi lw b fi mf mc l md me">Tweet.corpus=tm_map(Tweet.corpus,stemDocument)</span></pre><p id="09f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">从清理后的文本中准备术语文档矩阵</strong></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mm"><img src="../Images/ddd6160f21604ee6dc35e371d3e61a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-4nuCkachirjttI-QpaBg.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">术语文档矩阵— TDM</figcaption></figure><p id="22d0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">清理完文本后，每条推文(技术上称为文档)都需要转换为单个单词的集合，并且需要统计每个单词在所有文档/推文中的出现频率。这是通过将tweets集合转换成类似矩阵的结构来实现的，其中列代表tweets/文档，行代表单个单词。这种结构被称为TDM(术语文档矩阵)。TDM的转置是DTM(文档术语矩阵)，其中TDM中的tweets和单词交换位置以构成DTM。</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="e284" class="ma kt hi lw b fi mb mc l md me"><strong class="lw hj">#Building a term document </strong>matrixTweets.TDM=TermDocumentMatrix(Tweet.corpus)</span><span id="da7f" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Check the dimension of the TDM. We have 14,640 tweets and 8162 terms.</strong><br/>Tweets.TDM<br/>&lt;&lt;TermDocumentMatrix (terms: 8162, documents: 14640)&gt;&gt;<br/>Non-/sparse entries: 113858/119377822<br/>Sparsity           : 100%<br/>Maximal term length: 46<br/>Weighting          : term frequency (tf)</span><span id="dfb2" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Check out 5 rows and 5 columns of the TDM created</strong><br/>inspect(Tweets.TDM[1:5,1:5])<br/>&lt;&lt;TermDocumentMatrix (terms: 5, documents: 5)&gt;&gt;<br/>Non-/sparse entries: 5/20<br/>Sparsity           : 80%<br/>Maximal term length: 8<br/>Weighting          : term frequency (tf)<br/>Sample             :<br/>          Docs<br/>Terms      1 2 3 4 5<br/>  commerci 0 1 0 0 0<br/>  experi   0 1 0 0 0<br/>  plus     0 1 0 0 0<br/>  said     1 0 0 0 0<br/>  tacki    0 1 0 0 0</span></pre><p id="04b6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">删除稀疏术语</strong></p><p id="eb2d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所创建的TDM是一个高维对象，除非系统的计算能力很高，否则在其上建立模型可能相当耗时。因此，从TDM中移除频率非常低的项是理想的。这可以通过使用如下所示的函数来实现，该函数的目标是减少TDM，使单词的相对频率高于某个阈值(在本例中高于98%)。</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="a5f8" class="ma kt hi lw b fi mb mc l md me">Tweets.IMP=removeSparseTerms(Tweets.TDM,0.98)<br/>Tweets.IMP<br/>&lt;&lt;TermDocumentMatrix (terms: 67, documents: 14640)&gt;&gt;<br/>Non-/sparse entries: 36247/944633<br/>Sparsity           : 96%<br/>Maximal term length: 8<br/>Weighting          : term frequency (tf)</span></pre><p id="c9f9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">TDM现在减少到67个字，这似乎更容易管理，并且可以被认为是由对模型有影响的字组成的。所有其他单词的个别频率小于语料库中总单词数的4%,都被删除。</p><p id="c8d3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">可视化——词频和词云</strong></p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="ad22" class="ma kt hi lw b fi mb mc l md me">wordFreq=data.frame(apply(Tweets.IMP,1,sum))<br/>names(wordFreq)="Frequency"<br/>wordFreq$Terms=row.names(wordFreq)<br/>row.names(wordFreq)=NULL<br/>wordFreq=wordFreq[,c(2,1)]<br/>a=ggplot(wordFreq,aes(Terms,Frequency, fill=Terms))<br/>a+geom_bar(stat = "identity")+coord_flip()+ggtitle("Frequency Comparison")+guides(fill=FALSE)</span><span id="5d11" class="ma kt hi lw b fi mf mc l md me">findFreqTerms(Tweets.IMP,1000)<br/>[1] "now"  "hour" "help" "get"  "can" </span><span id="04c9" class="ma kt hi lw b fi mf mc l md me">#High frequency i.e. very commonly used terms as is seen in the bargraph below</span><span id="a65f" class="ma kt hi lw b fi mf mc l md me">findAssocs(Tweets.IMP,"now",0.05)<br/>$now<br/> hour delay  miss   hrs  wait <br/> 0.07  0.07  0.06  0.06  0.06 <br/>#greater than 5 percent correlation of word "now" </span><span id="6ded" class="ma kt hi lw b fi mf mc l md me">findAssocs(Tweets.IMP,"help",0.1)<br/>$help<br/>pleas   can  need <br/> 0.13  0.13  0.12</span><span id="6bf3" class="ma kt hi lw b fi mf mc l md me">#help has a minimum correlation of 10% with words pleas,can,need</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mn"><img src="../Images/ae4218ad8573d9757d85b23743de61f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rOyy9_Cx6HnwiZjkVb7dbw.jpeg"/></div></div></figure><div class="ju jv jw jx fd ab cb"><figure class="mo ij mp mq mr ms mt paragraph-image"><img src="../Images/8f3ca0488d48de96bde4ea5b2aa25ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*XzatkF3UE__HlCtADwRxzA.png"/></figure><figure class="mo ij mp mq mr ms mt paragraph-image"><img src="../Images/6fc91cb6b9689c7b4fe02435a10413a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*OpbmSrACNAfZpoKYqBxBnA.png"/><figcaption class="jy jz et er es ka kb bd b be z dx mu di mv mw translated">选择不同的词频阈值构建两个不同的词云。</figcaption></figure></div><p id="4364" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下面是构建单词云的代码。图表中的单词越大越密，出现的频率就越高。</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="c62c" class="ma kt hi lw b fi mb mc l md me">library("RColorBrewer")<br/>pal=brewer.pal(8,"Dark2")<br/>library(wordcloud)</span><span id="058d" class="ma kt hi lw b fi mf mc l md me">#wordcloud1<br/>wordcloud(Tweet.corpus,min.freq = 250,max.words = 30000,random.order = TRUE,<br/>          colors=pal,vfont=c("script","plain"))<br/>#wordcloud2<br/>wordcloud(Tweet.corpus,min.freq = 100,max.words = 40000,random.order = TRUE,<br/>          colors=pal,vfont=c("script","plain"))</span></pre><h1 id="cad6" class="ks kt hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak"> 3。情绪分析</strong></h1><blockquote class="mx my mz"><p id="6c5d" class="im in kd io b ip iq ir is it iu iv iw na iy iz ja nb jc jd je nc jg jh ji jj hb bi translated">有了公众情绪，什么都不会失败。没有它，任何事情都不会成功——亚伯拉罕·林肯</p></blockquote><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/e769910d819f95c77dd3ed209dab7719.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*PmGon3pU9fMxR4sE_aS50A.png"/></div></figure><p id="109e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">情感分析是确定一篇文章是正面的、负面的还是中性的过程。分析后的数据量化了公众对某些产品、人物或想法的情绪或反应。情绪分析有助于大型企业中的数据分析师评估公众意见，进行细致入微的市场研究，监控品牌和产品声誉，以及了解客户体验。</p><p id="6ce1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">情感分析使用数据挖掘过程和技术来提取和捕获用于分析的数据，以便辨别文档或文档集合的主观意见，如博客帖子、评论、新闻文章和社交媒体馈送，如推文和状态更新。<br/> <br/>情绪分析允许组织跟踪以下内容:</p><ul class=""><li id="d36b" class="ke kf hi io b ip iq it iu ix kg jb kh jf ki jj ne kk kl km bi translated">品牌接受度和知名度</li><li id="b6b8" class="ke kf hi io b ip kn it ko ix kp jb kq jf kr jj ne kk kl km bi translated">新产品认知和预期</li><li id="343d" class="ke kf hi io b ip kn it ko ix kp jb kq jf kr jj ne kk kl km bi translated">公司声誉</li><li id="dcbf" class="ke kf hi io b ip kn it ko ix kp jb kq jf kr jj ne kk kl km bi translated">火焰/火焰探测</li></ul><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/006ad81a0ef81cf911a27786ae378e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*6QyDqzDwGYcj7d5P0hVOPg.png"/></div></figure><p id="8e22" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">继续我们到目前为止一直在讨论的例子，我们有各种乘客关于美国航空公司的推文，我们的下一个目标是建立一个模型，可以根据他/她关于体验的推文预测任何乘客的情绪。我们将在下一节讨论模型构建的细节。我们在这里跳过了一部分，即通过将单词包/语料库与正面和负面单词的字典进行比较，将每条推文标记为正面、中性或负面。我们不进行这一步，因为数据集已经将情感定义为变量之一。所以我们会用“情绪”这个变量来训练模型。如果这个变量不可用，我们必须首先将每条推文标记为一种情绪，并使用这个标记来训练一个模型。我会用杰弗里·布林著名的算法和字典来标记这些情绪。参考他的<a class="ae kc" href="https://datamatters.blog/tag/sentiment-analysis/" rel="noopener ugc nofollow" target="_blank">教程</a>了解更多关于情感评分算法的细节。</p><h1 id="a870" class="ks kt hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak"> 4。型号&amp;结果</strong></h1><p id="9ce3" class="pw-post-body-paragraph im in hi io b ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj hb bi translated">分割数据以训练模型，然后在看不见的数据上测试模型的典型步骤如下所示。我选择建立五个模型——随机森林、决策树、SVM、最大熵和Bagging。</p><p id="68ca" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所有上述算法的理论方面已经在我早期的几篇博客中提供了。</p><p id="7a31" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">关于随机森林、决策树和装袋的工作细节，请参见<a class="ae kc" rel="noopener" href="/analytics-vidhya/random-forest-a-model-designed-to-provide-structure-in-chaos-e267d559ca04">随机森林——一种在混沌中提供结构的模型</a>。</p><p id="15f0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">参考<a class="ae kc" rel="noopener" href="/analytics-vidhya/comprehensive-support-vector-machines-guide-using-illusion-to-solve-reality-ad3136d8f877">综合支持向量机指南——利用幻觉解决现实！</a>详细了解SVM的工作情况。</p><p id="da55" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从我们在前面部分离开的地方继续，我们使用caret将数据分成训练和测试部分。事实上，我首先从14000多条推文中挑选出最初的2000条，以便用我的计算能力建立模型。计算能力非常强的人可以尝试在整个数据集上建立模型。使用RTextTools库训练模型。详情如下</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="5fd8" class="ma kt hi lw b fi mb mc l md me"><strong class="lw hj">#Data Splitting</strong></span><span id="fee6" class="ma kt hi lw b fi mf mc l md me">library(caret)<br/><strong class="lw hj">#Building model on the first 2000 tweets due to memory limitations</strong></span><span id="1464" class="ma kt hi lw b fi mf mc l md me">Tweets1=Tweets[1:2000,]<br/>Index=createDataPartition(Tweets1$airline_sentiment,times = 1,p=0.7,list = FALSE)</span><span id="800c" class="ma kt hi lw b fi mf mc l md me">Tweets1$type=NA<br/>Tweets1$type[Index]="train"<br/>Tweets1$type[-Index]="test"</span><span id="00a0" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Remove links</strong><br/>Tweets1$text &lt;- gsub("http(s?)([^ ]*)", " ", Tweets1$text, ignore.case = T)<br/><strong class="lw hj">#Replace '&amp;amp'</strong><br/>Tweets1$text &lt;- gsub("&amp;amp", "and", Tweets1$text)<br/><strong class="lw hj">#Remove words starting with @</strong><br/>Tweets1$text=gsub("@\\w+ *", "",Tweets1$text)<br/><strong class="lw hj">#Remove words starting with #</strong><br/>Tweets1$text=gsub("#\\w+ *", "",Tweets1$text)<br/><br/>Tweet.corpus=Corpus(VectorSource(Tweets1$text))</span><span id="b6cf" class="ma kt hi lw b fi mf mc l md me">#converting/encoding elements of character vectors to the native encoding or UTF-8 respectively,<br/>Tweet.corpus=tm_map(Tweet.corpus, function(x) iconv(enc2utf8(x), sub = "byte"))<br/>Tweet.corpus=tm_map(Tweet.corpus,removePunctuation)<br/>more_stopwords=c(stopwords('english'),'http*',"<a class="ae kc" href="http://twitter.com/VirginAmerica" rel="noopener ugc nofollow" target="_blank">@VirginAmerica</a>","<a class="ae kc" href="http://twitter.com/NYTimes" rel="noopener ugc nofollow" target="_blank">@NYTimes</a>","flight","cancelled","thanks","AA","DM")<br/>Tweet.corpus=tm_map(Tweet.corpus,removeWords,more_stopwords)<br/><strong class="lw hj">#Remove non-eglish words if any</strong><br/>Tweet.corpus=tm_map(Tweet.corpus,function(x) iconv(x,"latin1","ASCII",sub = ""))<br/>Tweets1$text= data.frame(text = sapply(Tweet.corpus, as.character), stringsAsFactors = FALSE)</span><span id="e7ec" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Build the models<br/></strong>#Call the required library to build the models</span><span id="2dd2" class="ma kt hi lw b fi mf mc l md me">library(RTextTools)<br/>models = train_models(container, algorithms=c("RF","TREE","SVM","MAXENT","BAGGING"))</span><span id="d84c" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Test the model</strong></span><span id="3dda" class="ma kt hi lw b fi mf mc l md me">results = classify_models(container, models)<br/>head(results)<br/>  FORESTS_LABEL FORESTS_PROB TREE_LABEL TREE_PROB SVM_LABEL  SVM_PROB MAXENTROPY_LABEL<br/>1             1        0.765          1 0.6486486         1 0.9089133                1<br/>2             1        0.720          1 0.6486486         1 0.6766424                1<br/>3             1        0.980          1 0.6486486         1 0.6363762                1<br/>4             1        0.540          1 0.6486486         3 0.4018248                2<br/>5             3        0.710          1 0.6486486         3 0.7301996                3<br/>6             1        0.675          1 0.6486486         1 0.7626651                1<br/>  MAXENTROPY_PROB BAGGING_LABEL BAGGING_PROB<br/>1       1.0000000             1         0.96<br/>2       0.9981853             1         1.00<br/>3       0.9838523             1         1.00<br/>4       0.8151790             3         0.52<br/>5       0.9998144             3         0.84<br/>6       1.0000000             1         0.96</span></pre><p id="0f9e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">精度对比</strong></p><p id="9308" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">然后根据测试数据和混淆矩阵对模型进行测试，以测试通过每个模型实现的准确性。详见下文。可以看出，从SVM模型获得的准确度最高，为70.73%。在尝试的模型中，决策树似乎表现最差。如果要从五个人中选一个，你很可能会选择SVM。</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="03e8" class="ma kt hi lw b fi mb mc l md me"><strong class="lw hj">#RandomForest</strong></span><span id="0dc6" class="ma kt hi lw b fi mf mc l md me">CM_RF=table(Tweets1$airline_sentiment[as.numeric(row.names(Tweets1[Tweets1$type=="test",]))], results[,"FORESTS_LABEL"])<br/>Acc_RF=sum(diag(CM_RF)/sum(CM_RF))</span><span id="d78d" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#Decision Tree</strong><br/>CM_Tree=table(Tweets1$airline_sentiment[as.numeric(row.names(Tweets1[Tweets1$type=="test",]))], results[,"TREE_LABEL"])<br/>Acc_Tree=sum(diag(CM_Tree)/sum(CM_Tree))</span><span id="b4cb" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#SVM</strong><br/>CM_SVM=table(Tweets1$airline_sentiment[as.numeric(row.names(Tweets1[Tweets1$type=="test",]))], results[,"SVM_LABEL"])<br/>Acc_SVM=sum(diag(CM_SVM)/sum(CM_SVM))</span><span id="d97b" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#MAXENT</strong><br/>CM_MAXENT=table(Tweets1$airline_sentiment[as.numeric(row.names(Tweets1[Tweets1$type=="test",]))], results[,"MAXENTROPY_LABEL"])<br/>Acc_MAXENT=sum(diag(CM_MAXENT)/sum(CM_MAXENT))</span><span id="4432" class="ma kt hi lw b fi mf mc l md me"><strong class="lw hj">#BAGGING</strong><br/>CM_BAGGING=table(Tweets1$airline_sentiment[as.numeric(row.names(Tweets1[Tweets1$type=="test",]))], results[,"BAGGING_LABEL"])<br/>Acc_BAGGING=sum(diag(CM_BAGGING)/sum(CM_BAGGING))</span><span id="3db0" class="ma kt hi lw b fi mf mc l md me">Compare=data.frame(Models=c("RF","DecisionTree","SVM","MAXENT","BAGGING"),Accuracy=c(Acc_RF,Acc_Tree,Acc_SVM,Acc_MAXENT,Acc_BAGGING))</span><span id="3259" class="ma kt hi lw b fi mf mc l md me">Compare<br/>       Models   Accuracy<br/>1           RF 0.6889632<br/>2 DecisionTree 0.6270903<br/>3          SVM 0.7073579<br/>4       MAXENT 0.6755853<br/>5      BAGGING 0.6672241</span></pre><h1 id="2a95" class="ks kt hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak"> 5。合奏的力量</strong></h1><p id="9a09" class="pw-post-body-paragraph im in hi io b ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj hb bi translated">RTextTools有一个令人惊叹的功能，如果我们有一个模型，它是一些或所有已经训练过的模型的集合，它可以创建关于准确度的分析。此外，该函数还会调出recall和n组合的统计数据所覆盖的数据比例，其中n是所选模型的数量。</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="2271" class="ma kt hi lw b fi mb mc l md me">analytics = create_analytics(container, results)<br/>summary(analytics)</span><span id="b34d" class="ma kt hi lw b fi mf mc l md me">ENSEMBLE SUMMARY<br/><br/>       n-ENSEMBLE COVERAGE n-ENSEMBLE RECALL<br/>n &gt;= 1                1.00              0.69<br/>n &gt;= 2                1.00              0.69<br/>n &gt;= 3                0.99              0.70<br/>n &gt;= 4                0.84              0.74<br/>n &gt;= 5                0.64              0.79<br/><br/><br/>ALGORITHM PERFORMANCE<br/><br/>       SVM_PRECISION           SVM_RECALL           SVM_FSCORE    BAGGING_PRECISION <br/>           0.6433333            0.5600000            0.5866667            0.5833333 <br/>      BAGGING_RECALL       BAGGING_FSCORE    FORESTS_PRECISION       FORESTS_RECALL <br/>           0.4600000            0.4466667            0.6066667            0.5000000 <br/>      FORESTS_FSCORE       TREE_PRECISION          TREE_RECALL          TREE_FSCORE <br/>           0.5100000            0.3933333            0.4200000            0.3900000 <br/>MAXENTROPY_PRECISION    MAXENTROPY_RECALL    MAXENTROPY_FSCORE <br/>           0.5633333            0.5533333            0.5566667</span></pre><p id="203d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">可以通过以下代码提取分析摘要</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="dbf6" class="ma kt hi lw b fi mb mc l md me">analytics@ensemble_summary<br/>       n-ENSEMBLE COVERAGE n-ENSEMBLE RECALL<br/>n &gt;= 1                1.00              0.69<br/>n &gt;= 2                1.00              0.69<br/>n &gt;= 3                0.99              0.70<br/>n &gt;= 4                0.84              0.74<br/>n &gt;= 5                0.64              0.79</span></pre><p id="b72f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">应该选择四个模型用于集成，以获得覆盖率(84%)和准确度(74%)的最佳组合。单独来看，SVM给出了最好的结果，但是从集合总结中可以看出，集合可以给出更高的准确度。</p><p id="48d1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">通过选择具有4个最佳准确度结果的模型，决策树失败了。最终的系综构建如下所述。</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="a711" class="ma kt hi lw b fi mb mc l md me">results1&lt;-results[,c(1,5,7,9)]<br/><strong class="lw hj"># Function to build an ensemble model using "For" loop</strong></span><span id="0708" class="ma kt hi lw b fi mf mc l md me">results1$majority=NA<br/>for(i in 1:nrow(results1))<br/>{<br/> <strong class="lw hj">#Getting the frequency distribution of the classifications </strong><br/>  p&lt;-data.frame(table(c(results1$FOREST_LABEL[i],results1$SVM_LABEL[i],<br/>                        results1$MAXENTROPY_LABEL[i],results1$BAGGING_LABEL[i])))<br/>  #Choosing the classification that occurs maximum<br/>  #Putting this value into the new column "majority"<br/>  <br/>  results1$majority[i]&lt;-paste(p$Var1[p$Freq==max(p$Freq)])<br/>  rm(p)<br/>}<br/>results1$majority&lt;-as.numeric(results1$majority)<br/>table(results1$majority)</span></pre><p id="d3a0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">比较单独和通过集合模型获得的准确度分数。</p><pre class="ju jv jw jx fd lv lw lx ly aw lz bi"><span id="bd66" class="ma kt hi lw b fi mb mc l md me">Compare=data.frame(Models=c("RF","DecisionTree","SVM","MAXENT","BAGGING","ENSEMBLE"),                 Accuracy=c(Acc_RF,Acc_Tree,Acc_SVM,Acc_MAXENT,Acc_BAGGING,0.74))</span><span id="d654" class="ma kt hi lw b fi mf mc l md me">Compare<br/>        Models  Accuracy<br/>1           RF 0.6889632<br/>2 DecisionTree 0.6270903<br/>3          SVM 0.7073579<br/>4       MAXENT 0.6755853<br/>5      BAGGING 0.6672241<br/>6     ENSEMBLE 0.7400000</span></pre><h1 id="4f40" class="ks kt hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak">结论</strong></h1><p id="d48b" class="pw-post-body-paragraph im in hi io b ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj hb bi translated">到目前为止，我们专注于通过在一个单词序列上训练算法来建立预测情绪的模型。我们通过四个模型的集合达到了74%的准确率，这还不错。然而，我认为，如果我们在两个单词或三个单词甚至更多单词的序列上训练模型，情感可以被更好地预测。例如，就个人而言，“喜欢”这个词可能会给人积极情绪的印象。然而，如果句子是“我不喜欢乘坐XYZ航空公司的航班”，这种情绪显然是负面的。在我的后续文章中，我将探索使用两个单词和三个单词序列来训练模型。不过需要注意的是，使用这种方法，训练数据的大小将呈指数级增长，计算能力的限制将会大大增加。敬请关注下一篇博客！！！</p></div></div>    
</body>
</html>