<html>
<head>
<title>Categorical Embedder: Encoding Categorical Variables via Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类嵌入器:通过神经网络编码分类变量</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/categorical-embedder-encoding-categorical-variables-via-neural-networks-b482afb1409d?source=collection_archive---------1-----------------------#2020-02-06">https://medium.com/analytics-vidhya/categorical-embedder-encoding-categorical-variables-via-neural-networks-b482afb1409d?source=collection_archive---------1-----------------------#2020-02-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e6df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在你之前—</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="936c" class="jm jn hi ji b fi jo jp l jq jr"><strong class="ji hj">pip install categorical_embedder</strong></span></pre><p id="d794" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我先说一下。</p><p id="9650" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道机器学习模型喜欢数字数据。我们转换我们的输入(文本、图像、语音等)。)转换成数字，然后输入到ML模型中。像CatBoost和LightGBM这样的模型确实可以处理分类变量，但是在实际训练开始之前，它们也可以通过不同的技术将它们转换成数字。如虚拟编码、标签编码、目标编码、频率编码等方法。服务于我们的目的，但我们能做得更好吗？我们有没有更好的方法来编码我们的分类变量，来解释我们的目标变量？</p><p id="2752" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">答案是肯定的！我们可以使用神经网络以嵌入的形式更好地表示我们的分类变量。</p><p id="f775" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">什么是嵌入？</p><blockquote class="js jt ju"><p id="fece" class="if ig jv ih b ii ij ik il im in io ip jw ir is it jx iv iw ix jy iz ja jb jc hb bi translated">“嵌入是类别的固定长度向量表示”</p></blockquote><p id="155d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嵌入仅仅意味着用一组固定的数字来表示一个类别。假设我有一个数据集，它有一个列<strong class="ih hj"> <em class="jv">【主题】</em> </strong>，其中有5个唯一值:物理、化学、生物、地理和数学。然后，我们可以用一组数字(假设为3)来表示这些类别:</p><figure class="jd je jf jg fd ka er es paragraph-image"><div class="er es jz"><img src="../Images/6a7e11336b5219eb41165f4b7d369cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*M1Q0J3NDoB5v6oxNp89zmA.png"/></div></figure><p id="d613" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原因3:我稍后会谈到这一点。</p><p id="a998" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">传统方法:</strong></p><p id="dc64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">传统上，我们通过以下方式将分类变量转换成数字</p><ul class=""><li id="03ae" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated">一个热编码</li><li id="8b3b" class="kd ke hi ih b ii km im kn iq ko iu kp iy kq jc ki kj kk kl bi translated">标签编码</li></ul><p id="a2be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在一个热编码中，我们构建与该特征中的唯一类别的数量一样多的特征，并且对于每一行，我们将1分配给表示该行的类别的特征，而其余的特征被标记为0。当一个要素中有大量类别(唯一值)导致数据非常稀疏时，这种技术就会出现问题。由于每个向量与其他向量的距离相等，变量之间的关系就消失了。</p><p id="506b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将分类特征转换为数字的另一种方法是使用一种称为标签编码的技术。标签编码只是将该列中的每个值转换成一个整数。这种技术非常简单，但是会导致特征类别之间的比较，因为它使用了数字排序。该模型可能认为化学优先于物理，类似地，生物的权重高于化学(事实并非如此)。</p><p id="46ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">分类嵌入是如何工作的？</strong></p><p id="d5da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，每个分类变量类别被映射到一个n维向量。这种映射由神经网络在标准的监督训练过程中学习。继续我们的示例，如果我们想要使用“主题”列作为特征，那么我们将以监督的方式训练神经网络，获得每个类别的向量，并生成如下的5x3矩阵。</p><figure class="jd je jf jg fd ka er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kr"><img src="../Images/b2c653ec0da63912b9d2d47c71f7821b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pz7NlIEwLshiXtrIAuj7Jw.png"/></div></div></figure><p id="48e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之后，我们将用数据中相应的向量替换每个类别。</p><p id="b0bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么分类嵌入是更好的选择？</strong></p><ul class=""><li id="7c31" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated">我们限制每个类别所需的列数。当列具有高基数时，这很有用。</li><li id="c49e" class="kd ke hi ih b ii km im kn iq ko iu kp iy kq jc ki kj kk kl bi translated">从神经网络获得的生成嵌入揭示了分类变量的内在性质。这意味着相似的类别将有相似的嵌入。</li></ul><h2 id="5ca1" class="jm jn hi bd kw kx ky kz la lb lc ld le iq lf lg lh iu li lj lk iy ll lm ln lo bi translated"><strong class="ak">包:分类嵌入器</strong></h2><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="9446" class="jm jn hi ji b fi jo jp l jq jr">pip install categorical_embedder</span></pre><p id="06e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">借助此软件包，您可以为数据中的分类变量生成嵌入:</p><figure class="jd je jf jg fd ka er es paragraph-image"><div class="ab fe cl lp"><img src="../Images/ccb11827b365f8866078fead8cac287d.png" data-original-src="https://miro.medium.com/v2/1*b0DbjN2gGk1Olf952gHQbA.gif"/></div></figure><p id="a1d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是生成分类嵌入的简单代码:</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="5d8f" class="jm jn hi ji b fi jo jp l jq jr">import categorical_embedder as ce<br/>from sklearn.model_selection import train_test_split</span><span id="f684" class="jm jn hi ji b fi lq jp l jq jr">df = pd.read_csv('HR_Attrition_Data.csv')<br/>X = df.drop(['employee_id', 'is_promoted'], axis=1)<br/>y = df['is_promoted']</span><span id="2d70" class="jm jn hi ji b fi lq jp l jq jr">embedding_info = ce.get_embedding_info(X)<br/>X_encoded,encoders = ce.get_label_encoded_data(X)</span><span id="e2bd" class="jm jn hi ji b fi lq jp l jq jr">X_train, X_test, y_train, y_test = train_test_split(X_encoded,y)</span><span id="0291" class="jm jn hi ji b fi lq jp l jq jr">embeddings = ce.get_embeddings(X_train, y_train, categorical_embedding_info=embedding_info, <br/>                            is_classification=True, epochs=100,batch_size=256)</span></pre><p id="32d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更详细的<a class="ae lr" href="https://github.com/Shivanandroy/CategoricalEmbedder/blob/master/example_notebook/Example%20Notebook.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>可以在这里找到</p><figure class="jd je jf jg fd ka"><div class="bz dy l di"><div class="ls lt l"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">分类嵌入器:示例笔记本</figcaption></figure><p id="f930" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">范畴嵌入器里面是什么？</strong></p><ul class=""><li id="a5ce" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated"><code class="du ly lz ma ji b">ce.get_embedding_info(data,categorical_variables=None)</code>:该函数识别数据中的所有分类变量，确定其嵌入大小。分类变量的嵌入大小由其唯一值数量的最小值50或一半决定，即列的嵌入大小= Min(50，该列中唯一值的数量)可以在<code class="du ly lz ma ji b">categorical_variables</code>参数中传递分类变量的显式列表。如果<code class="du ly lz ma ji b">None</code>，该函数自动获取所有数据类型为<code class="du ly lz ma ji b">object</code>的变量</li><li id="578e" class="kd ke hi ih b ii km im kn iq ko iu kp iy kq jc ki kj kk kl bi translated"><code class="du ly lz ma ji b">ce.get_label_encoded_data(data, categorical_variables=None)</code>:该函数使用sk learn . preprocessing . label encoder对所有分类变量进行标签编码(整数编码)，并返回标签编码的数据帧用于训练。Keras/TensorFlow或任何其他深度学习库都希望数据是这种格式。</li><li id="5e12" class="kd ke hi ih b ii km im kn iq ko iu kp iy kq jc ki kj kk kl bi translated"><code class="du ly lz ma ji b">ce.get_embeddings(X_train, y_train, categorical_embedding_info=embedding_info, is_classification=True, epochs=100,batch_size=256)</code>:该函数训练一个浅层神经网络，并返回分类变量的嵌入。在引擎盖下，它是一个2层神经网络架构，具有1000和500个具有“ReLU”激活的神经元。它需要4个输入- <code class="du ly lz ma ji b">X_train</code>、<code class="du ly lz ma ji b">y_train</code>、<code class="du ly lz ma ji b">categorical_embedding_info</code>:get _ embedding _ info函数的输出和<code class="du ly lz ma ji b">is_classification</code> : <code class="du ly lz ma ji b">True</code>用于分类任务；<code class="du ly lz ma ji b">False</code>用于回归任务。</li></ul><p id="3057" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于分类:<code class="du ly lz ma ji b">loss = 'binary_crossentropy'; metrics = 'accuracy'</code>和回归:<code class="du ly lz ma ji b">loss = 'mean_squared_error'; metrics = 'r2'</code></p><p id="b768" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">请在这里找到</strong> <a class="ae lr" href="https://github.com/Shivanandroy/CategoricalEmbedder" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> Github回购</strong> </a> <strong class="ih hj">和</strong> <a class="ae lr" href="https://github.com/Shivanandroy/CategoricalEmbedder/blob/master/example_notebook/Example%20Notebook.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">示例笔记本</strong> </a> <strong class="ih hj">。</strong></p></div></div>    
</body>
</html>