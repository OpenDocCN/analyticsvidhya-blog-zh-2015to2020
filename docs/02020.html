<html>
<head>
<title>Principal Component Analysis(PCA) — Detail Explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析(PCA) —详细解释</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/principal-component-analysis-pca-detailed-explanation-using-python-dff5fff8c5af?source=collection_archive---------13-----------------------#2019-11-26">https://medium.com/analytics-vidhya/principal-component-analysis-pca-detailed-explanation-using-python-dff5fff8c5af?source=collection_archive---------13-----------------------#2019-11-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="7900" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">简介:</h1><p id="66c5" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">主成分分析是基本的降维技术，它帮助我们降低数据集的维数并排除不需要的特征。当数据集有数百维时，主成分分析很重要，我们可以用它来提取包含更多信息的特征。由于我们不能可视化超过3个维度，我们可以使用PCA来减少维度以可视化数据。</p><p id="98da" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在这个博客中，我们将涵盖以下主题-</p><ol class=""><li id="ee32" class="kg kh hi jf b jg kb jk kc jo ki js kj jw kk ka kl km kn ko bi translated">PCA基础。</li><li id="617b" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">高级数学。</li><li id="c386" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">使用Python实现。</li></ol><p id="8c51" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">什么是主成分？</strong></p><p id="87c5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">主成分分析是一种数学工具，有助于将数据集的维度/相关变量/特征从大量变量减少到不相关的变量。在这个过程中，被丢弃的特征是那些不能充分解释数据的特征，而保留了那些更能解释数据的特征，因此被称为主成分。</p><h2 id="ccbf" class="ku ig hi bd ih kv kw kx il ky kz la ip jo lb lc it js ld le ix jw lf lg jb lh bi translated"><strong class="ak">主成分分析如何降维？</strong></h2><p id="1bec" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们看一些PCA如何决定保留哪些特征的例子，</p><p id="db5d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">例1- </strong>假设2D数据集X有两个特征(F1，F2)，我们想把它变成1D。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/a3bce433dff9652a6372f285af4de77e.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/0*Lg2ypO2uGXv646Yb.png"/></div></figure><p id="b58b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在上图中,“x”代表2D图中绘制的数据点。F1、F2是数据集(X)中的要素。橙色选择显示F1上数据点的分布。红色选择显示F2上数据点的分布。</p><p id="af34" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们可以观察到F1上的数据点的分布/方差大于F2上的数据点的分布/方差，这意味着F1比F2更多地解释了数据集X。</p><p id="2f36" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们的任务是使给定的数据集成为1D，因此我们可以排除的特征是F2，因为它具有关于数据集的较少信息。</p><p id="183f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">示例2 — </strong>假设数据集(X)有2个特征(F1，F2)，我们必须将给定数据集的维数减少到1D。</p><p id="0a9b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">数据点的分布如下图所示:</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/1ad26a4d3677a7625d3884e4ebe66b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/0*30GbxyfZUYOuXN_C.png"/></div></figure><p id="9d11" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在上图中，x '代表数据点绘制在2D图中。F1、F2是数据集(X)中的要素。</p><p id="611f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在本例中，两个特征上的数据分布完全相同。这里，PCA找到新的方向F2 ’,在该方向上数据点具有更大的分布，如下图所示。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/e8ed10e9268e7f8d0bc1b404eb643ae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/0*fV3nv7oPDh79ruVF.png"/></div></figure><p id="689f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在F2’比F1’具有更大的价差/方差。F1’从f1偏移ѳ角(θ), F2’从F2偏移相同的ѳ角(θ)。</p><p id="d436" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">因为我们有具有较小分布的F1’和具有较大分布的F2’，所以我们可以排除F1’并将所有数据点投影到F2’上，并使用F2’来可视化1D的数据。</p><p id="3ce6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">由上面的例子我们可以说，<br/> 1。我们旋转轴来寻找方差最大的F2’。<br/> 2。放弃F1。</p><h2 id="4dd8" class="ku ig hi bd ih kv kw kx il ky kz la ip jo lb lc it js ld le ix jw lf lg jb lh bi translated"><strong class="ak">PCA的数学表示- </strong></h2><p id="7bea" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们不需要F2 '作为一个整体，方向足以在它上面绘制和投影所有的数据点。所以我们称F2 '为u1，它是沿着F2 '方向的单位向量。</p><p id="2633" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">因此，u1是这样一个方向，如果我们在这个方向上投影每个点，扩散/方差将是最大的，它的长度是1，即</p><p id="55fe" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">||u1|| = 1</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lq"><img src="../Images/31b9b19cea38c6c67a4b707d3fc2c1a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/0*S-ekHxQGyib8Lc-A.png"/></div></figure><p id="43e0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在上图中，我们绘制了一个称为xi的数据点，并将其投影到我们新发现的方向u1上，称为xi。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lr"><img src="../Images/7a0eb4742c5030396ee372c2ddfdf992.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/0*4QL4KtVdWMZXYLNW.png"/></div></figure><p id="3ef7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">因为单位向量u1 (||u1||)的长度是1，我们只取u1.xi的点积的表示，现在对于每个点xi，我们可以用u1计算xi。</p><p id="f215" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">如果我们将I从1到n变化的xi的平均值作为x̄，并将其乘以u1转置，我们将得到Xi的平均向量“作为x̄”,如下所示</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es ls"><img src="../Images/a7c0097c6523b5d7f43bab998e89c10a.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/format:webp/0*i_FbztiYY-gJPgJI.png"/></div></figure><p id="db89" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> Q1 </strong> —找到u1，使得xi投影到u1上的点的方差最大，即</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lt"><img src="../Images/d68069f8eef6eaed5957d0a8e0de9879.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/0*oiq3ATY2MwWUj5VQ.png"/></div></figure><p id="07bc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">根据定义，xi的投影可以写成如下:</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lu"><img src="../Images/2fe48487595a04d7d4d3ac6af2448383.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/0*KO81AglPFl34FyVU.png"/></div></figure><p id="3e25" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们的数据集x是<a class="ae lv" href="https://www.youtube.com/watch?time_continue=7&amp;v=Mg5ixxRmnA0" rel="noopener ugc nofollow" target="_blank">列标准化</a> d，即x的平均向量(x̄)将为零。所以我们的等式会变成</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lw"><img src="../Images/829e55143ea73111c4622f6160584d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/0*O7FlWciFwnNsrq6U.png"/></div></figure><p id="409f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">PCA的替代公式——距离最小化</strong></p><p id="9783" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">与前面的问题不同，在距离最小化中，PCA找到所有投影点的距离都较小的u1。</p><p id="f401" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">当我们在特定方向上获得最大方差时，相同方向是所有数据点之间距离最小的方向。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/c36ad61f9a07fae22cab159a78e8a19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/0*dvCft6H43ZrNZ5xG.png"/></div></figure><p id="9dae" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">di =点的距离。</p><p id="7dfb" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> Q2- </strong>找出使距离平方和最小的u1。</p><p id="2b7d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们画一个点，试着找出它离u1的距离。在下图中，我们把xi作为一个点。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lx"><img src="../Images/29b2521ea2d39ed31c7bde504a5110e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/0*vbZh9Ckg9dMC5ZO3.png"/></div></figure><p id="8a55" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在这里，图表形成直角三角形，使用毕达哥拉斯定理，我们可以计算每个点的距离di。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es ly"><img src="../Images/2e47bdde22e2013f24d0811c96019235.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/0*3Cyy9Nt16UjFUNjF.png"/></div></figure><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lz"><img src="../Images/54b1fce7fb967711a88741f3f72df2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/0*tPe4Ld7a9SO3wjkV.png"/></div></figure><p id="90ed" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在使用这个数据，我们可以找到u1，在u1上所有点的平方距离的和是最小的。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es ma"><img src="../Images/6281901bb3301761be07db5adda057f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/0*pQV7xHdSvL746jE_.png"/></div></figure><p id="dcab" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们已经把di平方的值放到上面的公式里了。</p><h2 id="8f71" class="ku ig hi bd ih kv kw kx il ky kz la ip jo lb lc it js ld le ix jw lf lg jb lh bi translated">查找u1</h2><p id="5b29" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，我们将找到上述两个优化问题的解决方案，这意味着我们将尝试找到方向u1，</p><p id="8731" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">1.让我们取一个数据集X，它是n×d矩阵，列是标准化的。</p><p id="f562" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">2.计算X的<a class="ae lv" href="https://www.youtube.com/watch?time_continue=732&amp;v=1HLLmFmy3Uw" rel="noopener ugc nofollow" target="_blank">协方差矩阵</a>称之为S，将为d×d矩阵。</p><p id="205c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">3.求S的特征值，它们是λ1，λ2，λ3，…，λd</p><p id="bfc9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们假设λ1 &gt;= λ2 &gt;= λ3 &gt;=…，&gt; = λd</p><p id="7fb7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这意味着λ1是最大特征值。</p><p id="9a07" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">计算特征值和向量非常简单，因为在python中，Numpy库中有一个函数eigen可以给出特征值。</p><p id="4f40" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">4.对于每个特征值都有相应的特征向量，计算特征向量v1，v2，v3，…，vd。</p><p id="c67f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">特征向量有一个很好的性质，每对特征向量都是相互垂直的。</p><p id="3db8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">5.最大的特征向量是u1，</p><p id="f170" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">u1 = v1</p><h2 id="e6a2" class="ku ig hi bd ih kv kw kx il ky kz la ip jo lb lc it js ld le ix jw lf lg jb lh bi translated"><strong class="ak">观察结果</strong></h2><p id="3dfe" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">1.V1是具有最大传播的向量，V2对应于第二大传播方向，以此类推。</p><p id="6563" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">2.λi/总和(λI)表示通过选择Vi向量，我们将保留多少百分比的信息/分布。</p><h2 id="766b" class="ku ig hi bd ih kv kw kx il ky kz la ip jo lb lc it js ld le ix jw lf lg jb lh bi translated">使用主成分分析的实时降维示例:</h2><p id="1612" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将从Kaggle.com获得MNIST数据集，并尝试在2D将其可视化。</p><p id="ccf8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">关于MNIST-这是手写数字图像的数据集。每张图片的尺寸都是28 x 28。我们在这个数据集中有4200个数据点，从中将实现15000个数据点。我们将使用名为sklearn的python库将MNIST数据集的维数从784维减少到2维。</p><p id="b62c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">要在MNIST上应用PCA，Python代码如下所示:</p><pre class="lj lk ll lm fd mb mc md me aw mf bi"><span id="6da7" class="ku ig hi mc b fi mg mh l mi mj">#First read the csv(comma separated file) containing MNIST dataset.<em class="mk"><br/>import pandas as pd<br/>import numpy as np</em></span><span id="94ad" class="ku ig hi mc b fi ml mh l mi mj"><em class="mk"># save the labels to a Pandas series <br/>targetd0 = pd.read_csv(‘./mnist_train.csv’)<br/>l = d0['label']</em></span><span id="b528" class="ku ig hi mc b fi ml mh l mi mj"><em class="mk"># Drop the label feature<br/>d = d0.drop("label",axis=1)</em></span></pre><p id="70da" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这个数据集中的标签是写在图像中的实际数字。对于每个数据点，已经给出了标签。</p><pre class="lj lk ll lm fd mb mc md me aw mf bi"><span id="51e4" class="ku ig hi mc b fi mg mh l mi mj"><em class="mk"># Pick first 15K data-points to work on for time-efficiency.<br/>labels = l.head(15000)<br/>data = d.head(15000)</em></span></pre><p id="e459" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们从总数42000点中抽取了15000点，并存储在可变数据中。</p><pre class="lj lk ll lm fd mb mc md me aw mf bi"><span id="4d2f" class="ku ig hi mc b fi mg mh l mi mj">from sklearn.preprocessing import StandardScaler</span><span id="c5f7" class="ku ig hi mc b fi ml mh l mi mj">standardized_data = StandardScaler().fit_transform(data)</span><span id="df87" class="ku ig hi mc b fi ml mh l mi mj">#find the co-variance matrix which is : A^T * A<br/>sample_data = standardized_data</span><span id="34fa" class="ku ig hi mc b fi ml mh l mi mj">#matrix multiplication using numpy<br/>covar_matrix = np.matmul(sample_data.T , sample_data)</span><span id="3d3d" class="ku ig hi mc b fi ml mh l mi mj">#finding the top two eigen-values and corresponding eigen-vectors<br/>#for projecting onto a 2-Dim space.<br/>from scipy.linalg import eigh</span><span id="daeb" class="ku ig hi mc b fi ml mh l mi mj">#the parameter ‘eigvals’ is defined (low value to high value)<br/>#eigh function will return the eigen values in ascending order<br/>#this code generates only the top 2 (782 and 783) eigenvalues.<br/>values, vectors = eigh(covar_matrix, eigvals=(782,783))</span><span id="4644" class="ku ig hi mc b fi ml mh l mi mj">#projecting the original data sample on the plane<br/>#formed by two principal eigen vectors by vector-vector #multiplication.<br/>import matplotlib.pyplot as plt<br/>new_coordinates = np.matmul(vectors, sample_data.T)</span><span id="80cf" class="ku ig hi mc b fi ml mh l mi mj">print(“ resultanat new data points’ shape “, vectors.shape, “*”, sample_data.T.shape,” = “, new_coordinates.shape)</span><span id="40a6" class="ku ig hi mc b fi ml mh l mi mj">#appending label to the 2d projected data<br/>new_coordinates = np.vstack((new_coordinates, labels)).T</span><span id="a5f1" class="ku ig hi mc b fi ml mh l mi mj">#creating a new data frame for plotting the labeled points.<br/>dataframe = pd.DataFrame(data=new_coordinates, columns=(“1st_principal”, “2nd_principal”, “label”))</span><span id="f040" class="ku ig hi mc b fi ml mh l mi mj">print(dataframe.head())</span><span id="63f0" class="ku ig hi mc b fi ml mh l mi mj">#plotting the 2d data points with seaborn<br/>import seaborn as sn<br/>sn.FacetGrid(dataframe, hue=”label”, size=6).map(plt.scatter, ‘1st_principal’, ‘2nd_principal’).add_legend()plt.show()</span></pre><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es mm"><img src="../Images/9135e462bba8eb8fdae631197f63fa16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/0*M39bfJ9ZVykKaC5n.png"/></div></figure><p id="ea36" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在上面的例子中，我们通过计算所有需要的项，如协方差、特征值、特征向量等来计算PCA。因此，为了绕过所有这些步骤，我们在python中有一个名为sklearn库，我们现在将学习它。</p><h2 id="421c" class="ku ig hi bd ih kv kw kx il ky kz la ip jo lb lc it js ld le ix jw lf lg jb lh bi translated">使用sklearn实现PCA:</h2><pre class="lj lk ll lm fd mb mc md me aw mf bi"><span id="7f91" class="ku ig hi mc b fi mg mh l mi mj">#initializing PCA<br/>from sklearn import decomposition<br/>pca = decomposition.PCA()</span><span id="2037" class="ku ig hi mc b fi ml mh l mi mj">#set number of parameter as 2<br/>pca.n_components = 2<br/>pca_data = pca.fit_transform(sample_data)</span><span id="0e86" class="ku ig hi mc b fi ml mh l mi mj">#pca_reduced will contain the 2-d projects of simple data<br/>print(“shape of pca_reduced.shape = “, pca_data.shape)</span><span id="0414" class="ku ig hi mc b fi ml mh l mi mj">#attaching the label for each 2-d data point<br/>pca_data = np.vstack((pca_data.T, labels)).T</span><span id="28d5" class="ku ig hi mc b fi ml mh l mi mj">#creating a new data from which help us in plotting the result data<br/>pca_df = pd.DataFrame(data=pca_data, columns=(“1st_principal”, “2nd_principal”, “label”))</span><span id="7c61" class="ku ig hi mc b fi ml mh l mi mj">sn.FacetGrid(pca_df, hue=”label”, size=6).map(plt.scatter, ‘1st_principal’, ‘2nd_principal’).add_legend()<br/>plt.show()</span></pre><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es mn"><img src="../Images/a3e5ef5db7e00bc2811e8994fef7d559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/0*TwVBEg7QuCupIvnT.png"/></div></figure><p id="8489" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">sklearn是一个非常有用的python库，它将我们上面的所有步骤简化为一行。在sklearn中，有一个称为分解的模块，它具有PCA功能，只需对给定的数据集应用PCA，而不需要像以前的代码那样单独计算任何特征值和特征向量。因此，我们在上面创建了一个新的变量来存储PCA。使用该变量，我们将维数减少到2，特征值、特征向量、协方差的所有计算将在单个函数<em class="mk">fit _ transform(sample _ data)</em>中进行，然后我们将检查数据的形状，最后我们将把它放入dataframe并绘制出来。上图显示了我们的PCA的输出，和之前的一样，只是旋转了90度。</p><h2 id="3762" class="ku ig hi bd ih kv kw kx il ky kz la ip jo lb lc it js ld le ix jw lf lg jb lh bi translated">PCA的局限性-</h2><ol class=""><li id="c430" class="kg kh hi jf b jg jh jk jl jo mo js mp jw mq ka kl km kn ko bi translated">它不适用于循环数据传播，因为它丢失了大量信息。</li><li id="a506" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">对于不同象限中分离良好的群集，它也可能丢失大量信息。</li><li id="8464" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">如果数据像正弦波，那么PCA也会丢失数据。</li></ol><h2 id="eb15" class="ku ig hi bd ih kv kw kx il ky kz la ip jo lb lc it js ld le ix jw lf lg jb lh bi translated">确认-</h2><p id="7b64" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">感谢<a class="ae lv" href="https://www.appliedaicourse.com/course/applied-ai-course-online/" rel="noopener ugc nofollow" target="_blank">应用人工智能课程</a>和团队传授理念。</p><h1 id="629e" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">参考文献-</h1><ol class=""><li id="4b00" class="kg kh hi jf b jg jh jk jl jo mo js mp jw mq ka kl km kn ko bi translated"><a class="ae lv" href="https://www.appliedaicourse.com/course/applied-ai-course-online/" rel="noopener ugc nofollow" target="_blank">https://www . applied ai course . com/course/applied-ai-course-online</a></li><li id="df3d" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated"><a class="ae lv" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/</a></li></ol></div></div>    
</body>
</html>