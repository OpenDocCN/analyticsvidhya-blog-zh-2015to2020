<html>
<head>
<title>Cross Entropy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交叉熵</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/cross-entropy-fca0c6ea5006?source=collection_archive---------16-----------------------#2020-12-30">https://medium.com/analytics-vidhya/cross-entropy-fca0c6ea5006?source=collection_archive---------16-----------------------#2020-12-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="78f8" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在<a class="ae jc" href="https://en.wikipedia.org/wiki/Information_theory" rel="noopener ugc nofollow" target="_blank">信息论</a>中，如果用于集合的编码方案针对估计的概率分布而不是真实分布进行了优化，则同一底层事件集合上的两个<a class="ae jc" href="https://en.wikipedia.org/wiki/Probability_distribution" rel="noopener ugc nofollow" target="_blank">概率分布</a> p和q之间的<strong class="ig hi">交叉熵</strong>测量识别从集合中提取的事件所需的平均<a class="ae jc" href="https://en.wikipedia.org/wiki/Bit" rel="noopener ugc nofollow" target="_blank">比特数</a>。(参考:维基百科)</p><p id="347b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">从纯数学的角度来看，<strong class="ig hi">交叉熵</strong>是一种误差度量，将一组预测概率与一组预测概率进行比较。交叉熵是一种误差度量，它将一组计算出的输出节点与来自训练数据的值进行比较。简单地说，举个例子，如果骰子四个面的概率是(0.15，0.35，0.20，0.30)，但是经过上千次试验，如果我们确定真实概率是(0.20，0.30，0.25，0.25)，那么交叉熵将是</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/c9585cd815b4aa073bdca1ddd6f79da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*Ykbd7QEQkRj0ffz0WcoUog.png"/></div></figure><p id="9a39" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">交叉熵表示模型认为的输出分布和原始分布之间的距离。它被定义为，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jl"><img src="../Images/1cd915e94fdeffc06dc3f683c34c14e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*iY_mKwmP1iNlxOFe2xaXFA.png"/></div></figure><p id="4561" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">交叉熵度量是广泛使用的平方误差的替代方法。当节点激活可以被理解为表示每个假设可能为真的概率时，即当输出是概率分布时，使用该函数。因此，它被用作在输出层具有softmax激活的神经网络中的损失函数。</p><p id="2b25" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">交叉熵损失</strong>或对数损失，衡量分类模型的性能，其输出是0到1之间的概率值。交叉熵损失随着预测概率偏离实际标签而增加。因此，当实际观察值为1时，预测0.015的概率将是糟糕的，并导致高损失值。完美的模型的对数损失为0。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jm"><img src="../Images/e13648afa9386e1ea1b269f20880a8eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*LMHaTR6cvPsxHnploN_r6A.png"/></div></figure><p id="061e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">我们在上面看到，当预测概率接近1时，测井曲线损失缓慢减少，而当预测概率减少时，测井曲线损失呈指数增加。</p><p id="a6ba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在二进制分类中，类的数量等于2，交叉熵可以计算为:</p><blockquote class="jn"><p id="233b" class="jo jp hh bd jq jr js jt ju jv jw jb dx translated">−(ylog(p)+(1−y)log(1−p))−(ylog⁡(p)+(1−y)log⁡(1−p))</p></blockquote><p id="c3b4" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">在多类分类中，我们为每个观察值的每个类标签计算单独的损失，并对结果求和。</p><blockquote class="jn"><p id="564b" class="jo jp hh bd jq jr js jt ju jv jw jb dx translated">ylog(p)</p></blockquote><p id="7dfb" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">其他损失函数的简短参考。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es kc"><img src="../Images/9f952d52a9fd051a662a0bb2ce6a681f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pe7lBWMCBzX1NH60H3W81Q.png"/></div></div></figure><h2 id="3ec4" class="kh ki hh bd kj kk kl km kn ko kp kq kr ip ks kt ku it kv kw kx ix ky kz la lb bi translated"><strong class="ak">交叉熵或均方误差</strong></h2><p id="f5c5" class="pw-post-body-paragraph ie if hh ig b ih lc ij ik il ld in io ip le ir is it lf iv iw ix lg iz ja jb ha bi translated">对于分类问题，交叉熵(或softmax loss，但交叉熵效果更好)可能是优于MSE的选择。</p><p id="ef61" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为什么这样分类任务中的决策边界很大(与回归相比)。MSE没有对错误分类进行足够的惩罚，但对于回归来说是正确的损失，其中两个可预测值之间的距离很小。</p><p id="022d" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">如果我们的网络输出层具有sigmoid或softmax非线性，交叉熵是更好的选择，我们的目标是最大化分类的可能性。现在如果我们假设目标是连续的和正态分布的，并且更可能使用MSE(与线性输出层相结合)。然而，我们可以用MSE损失来训练分类器，它可能会工作得很好(然而，在sigmoid/softmax非线性的情况下不太好，在这种情况下，线性输出层将是更好的选择)。对于回归问题，我们大多倾向于使用MSE。另一种分类方法是使用边际损失，这基本上相当于在你的网络上放置一个(线性)SVM。</p><p id="84fe" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">CE是一组预测概率和一组实际概率(或N选1编码的训练标签)之间的误差的度量。交叉熵误差也称为对数损失。SE是一种常见的错误形式。它只是一组预测值和一组实际值之间的平方差之和。通常，当使用反向传播训练时，交叉熵往往比SE更快地给出更好的训练结果，但是SE比CE更不稳定。</p></div></div>    
</body>
</html>