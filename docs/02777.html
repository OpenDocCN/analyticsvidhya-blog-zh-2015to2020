<html>
<head>
<title>Geometry makes PCA and T-SNE So Easy!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">几何让PCA和T-SNE变得如此简单！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/geometry-made-pca-and-t-sne-so-easy-b8b35f209730?source=collection_archive---------17-----------------------#2020-01-01">https://medium.com/analytics-vidhya/geometry-made-pca-and-t-sne-so-easy-b8b35f209730?source=collection_archive---------17-----------------------#2020-01-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/f018b25c7ad5e31bf0eab5c2559ceba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*s8qAD156uGsm3cfFpoHOnA.jpeg"/></div></figure><p id="597b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇文章中，我们将看到当我们戴上几何眼镜来理解主成分分析和T-SNE降维时，事情是如何变得简单的。</p><p id="039d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">什么是降维？？？</strong></p><p id="aeae" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们得先知道基本的，对！！否则机器学习有能力狠狠踢我们一脚。</p><p id="480b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我被踢过很多次。</p><p id="4dd0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">降维简单来说就是在机器学习语言中降低维度或者特征，这样我们就可以得到一个更易解释的模型。</p><p id="1634" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">我给你举个例子</strong>，假设我们有784个特征，通常和MNIST数据集打过交道的人都会明白我为什么要取784个特征。</p><p id="13ee" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你不明白，没关系，想象任何一个大的数字，试着把每个特征看成一个维度。</p><p id="cbb2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你能在脑海中思考784个维度吗？</p><p id="2f1f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你能想象784维空间的分散点吗？</p><p id="b845" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">这就是为什么降维很重要。</strong></p><p id="1b82" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">让我给你举另一个例子</strong>，假设你是一名数据科学家，你必须向不了解机器学习的客户解释你的模型，你将如何让他们了解784特征或维度的工作。</p><p id="16d3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">用简单的语言“<strong class="io hj">模型的可解释性</strong>”。</p><p id="b263" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是降维之所以重要的第二个原因。</p><p id="588e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">现在让我给你举第三个例子</strong>，假设你在一家基于互联网的公司工作，那里的输出必须在毫秒或更短的时间内，所以“时间复杂度”和“空间复杂度”很重要。</p><p id="0515" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">更多的功能需要更多的时间，这些公司负担不起。</p><p id="b811" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">简单！</p><p id="e5ef" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以简而言之，<strong class="io hj"> <em class="jk">降维</em> </strong>基本有三个原因</p><ol class=""><li id="1a3c" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated">可视化。</li><li id="ac79" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">可解释性。</li><li id="4499" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">时空复杂度。</li></ol><p id="caf6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">在本文中，我们将专门致力于降低可视化的维度。</strong></p><p id="04a5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以可视化784维会非常困难，所以有一种技术可以减少这些维度。</p><p id="7a4d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以第一个也是最古老的技术是"<strong class="io hj"> PCA </strong>"</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es jz"><img src="../Images/ced49f39950c3140f3ae993758f42deb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*at1ypI6PEGz9yPYnoa2PvQ.jpeg"/></div></div></figure><p id="a84a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">PCA代表<strong class="io hj">主成分分析。</strong></p><p id="b690" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了让每个人都简单明了，让我们把2D转换成1D，如果我们能做到的话，我们将把线性代数应用到更高维度。</p><p id="e9b1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以让我们从几何学上理解什么是真正的PCA。</p><p id="4e12" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">主成分分析的几何直觉</strong></p><p id="36aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们的目标是让2D改信1D教。</p><p id="7467" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">案例1 </strong></p><p id="6042" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们举个例子，我们有两个特征F1(头发的黑色)和F2(人的高度)</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es ki"><img src="../Images/3affe16b7a1942aba30f74b757c87c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnbO-sDLuD5hxl8VCUx3qg.jpeg"/></div></div></figure><p id="093b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">假设头发的黑度是一个实数，假设有一些标准来衡量头发的黑度。</p><p id="982a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在图中，你可以看到分布或散射，假设这是一个印第安人的分布。</p><p id="7b70" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，我们可以很容易地说，几乎同样黑色头发的人有很大的身高差距，或者我们知道大多数印度人有黑色头发，这就是为什么它覆盖了所有的身高。</p><p id="0c66" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果我把这个分布带到美国，情况会有所不同，我们可以看到金发，黑发等等。</p><p id="c12a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以在高度轴上分布较多而头发的黑色分布很少。</p><p id="1573" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此可以说，在印度，头发的黑色不会给我们提供太多信息，因为传播非常低，它不会给我们的模型增加任何价值。</p><p id="e0ee" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以我可以去掉这个特征(黑色的头发)，因为我们知道大多数印度人都有黑色的头发，所以传播很少。</p><p id="f32f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以基本上这个想法是，如果我们被迫跳过一个特征，我们将跳过对我来说不太重要的特征。</p><p id="d553" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">换句话说，PCA保持具有最大扩散或方差的方向。</strong></p><p id="ff6a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">案例二</strong></p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es ki"><img src="../Images/88b3ce26a25839aabf25942cf876653e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCU-8Xs6GzfFltmuZWCeLg.jpeg"/></div></div></figure><p id="cc3c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，在这种情况下，传播在两侧是相等的，所以我们不能丢弃一个特征。</p><p id="5e74" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">现在该怎么办？</strong></p><p id="c737" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，如果我们可以将我们的轴旋转θ到最大方差或最大扩散的方向(参考图像)，那么我们可以删除具有最小方差的特征。</p><p id="722f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在我们例子中，第一步是找到F1 '和F2 '，第二步是去掉F2 '，因为F2 '方差很小。</p><p id="52ae" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以关键是，如果我定义X= 2维数据集，我们希望找到一个方向F1 ’,使得投影到F1’上的方差最大。</p><p id="d691" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">怎么做？？？</strong></p><p id="ab5a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这涉及到PCA的数学目标，我们将在后面讨论。</p><p id="fbdf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">但是现在只要理解这一点，它将涉及一些优化问题(距离最小化)</p><p id="5549" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> T-SNE </strong></p><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/d984852b439770e41ff17ea135a0dd32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*I0503ahKqRl5bAtaUr1LUg.png"/></div></figure><p id="d4a4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">它是由Jefry Newton(也被称为深度学习之父)创建的</p><p id="9c68" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是一项创立于2008年的艺术技术。</p><p id="ac69" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">什么是SNE霸王龙？？？</strong></p><p id="ac41" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">T- <strong class="io hj"> t分布</strong>(现在只要记住t分布或学生t分布是在人口方差未知时引入的)</p><p id="7387" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在接下来的几篇文章中，我将详细讨论t分布，但现在就让它去吧。</p><p id="f528" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">S- <strong class="io hj">随机</strong>(概率分布)或者可以记为对同一数据集多次应用T-SNE算法，它会在不同时间给出略有不同的结果。</p><p id="60e7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">N- <strong class="io hj">邻域</strong></p><p id="358a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">邻域仅仅意味着如果有六个点x1、x2、x3、x4、x5、x6，并且x1到x2、x3和x4之间的距离相对小于x5和x6，那么</p><p id="29f0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">x1=N(x2，x3，x4)意味着x2，x3，x4是x1的邻域。</p><p id="ccfc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">而x5和x6不是。</p><p id="1729" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">E- <strong class="io hj">嵌入</strong></p><p id="b7cb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">假设D是一个很大的数字，例如784</p><p id="527a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">它仅仅意味着对于D维xi中的每一个点(任何点)，我们都在寻找低维中的对应点。</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kk"><img src="../Images/f4d225799729d9e25990618211323b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Azq2ZI2Z8ZarLNpZQ4d1yg.jpeg"/></div></div></figure><p id="dcce" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们如何找到对应点？</p><p id="8d21" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将在探索它的数学目标时讨论它，现在我们将集中在几何直觉上。</p><p id="05d3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我本想只在这篇文章中介绍T-SNE几何直觉，但这对于一篇文章来说太多了。</p><p id="36a6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，在下一篇文章中，我将涉及这一点。</p><p id="c914" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">感谢阅读…</p></div></div>    
</body>
</html>