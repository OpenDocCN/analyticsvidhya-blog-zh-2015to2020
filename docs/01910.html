<html>
<head>
<title>Topic Modeling with Latent Dirichlet Allocation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于潜在狄利克雷分配的主题建模</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/modeling-with-latent-dirichlet-allocation-3b198f1a7bae?source=collection_archive---------2-----------------------#2019-11-21">https://medium.com/analytics-vidhya/modeling-with-latent-dirichlet-allocation-3b198f1a7bae?source=collection_archive---------2-----------------------#2019-11-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8af9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在现代互联网和社交媒体时代，人们的意见、评论和建议已经成为政治科学和商业的宝贵资源。由于现代技术，我们现在能够最有效地收集和分析这些数据。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/74dff698031fbc8c5ffc305964f70849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EuaUNAwKBv5JhfGHzkhNkA.jpeg"/></div></div></figure><p id="201c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将深入研究<strong class="ih hj">情感分析</strong>，并学习如何使用主题建模将电影评论归类到不同的类别。我们将使用来自互联网的50，000条电影评论的数据集<strong class="ih hj">电影数据库(IMDb) </strong>。</p><p id="ce7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">本文节选自Sebastian Raschka和Vahid Mirjalili的《Python机器学习</em> <a class="ae jq" href="https://www.amazon.com/Python-Machine-Learning-scikit-learn-TensorFlow/dp/1789955750?utm_source=AnalyticsVidhya&amp;utm_medium=referral&amp;utm_campaign=Outreach_PEN" rel="noopener ugc nofollow" target="_blank"> <em class="jp">第三版</em> </a> <em class="jp">一书。这本书是用Python进行机器学习和深度学习的综合指南。这个新的第三版针对TensorFlow 2.0、GANs、强化学习和其他流行的Python库进行了更新。在本文中，我们将讨论一种流行的主题建模技术，称为潜在狄利克雷分配(LDA)。</em></p><p id="aa28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">主题建模</strong>描述了将主题分配给未标记文本文档的广泛任务。例如，一个典型的应用是对报纸文章的大型文本语料库中的文档进行分类。在主题建模的应用中，我们的目标是为这些文章分配类别标签，例如，体育、金融、世界新闻、政治、本地新闻等等。因此，在机器学习的广泛类别的背景下，我们可以将主题建模视为聚类任务，这是无监督学习的一个子类。</p><h1 id="c182" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">用LDA分解文本文档</h1><p id="4aef" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">由于LDA背后的数学是相当复杂的，并且需要贝叶斯推理的知识，我们将从从业者的角度来处理这个话题，并且用外行的术语来解释LDA。然而，感兴趣的读者可以在下面的研究论文中读到更多关于LDA的内容:Latent Dirichlet Allocation，David M. Blei，Andrew Y. Ng和Michael I. Jordan，Journal of Machine Learning Research 3，pages:993–1022，Jan 2003。</p><p id="ed23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LDA是一个生成概率模型，它试图找到在不同文档中频繁出现的单词组。这些频繁出现的单词代表我们的主题，假设每个文档都是不同单词的混合。LDA的输入是我们在本章前面讨论的单词袋模型。给定一个单词袋矩阵作为输入，LDA将其分解成两个新矩阵:</p><p id="8255" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文档-主题矩阵</p><p id="1e56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">词到主题矩阵</p><p id="0c57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LDA以这样一种方式分解单词袋矩阵，如果我们将这两个矩阵相乘，我们将能够以尽可能低的误差再现输入，单词袋矩阵。实际上，我们感兴趣的是LDA在词袋矩阵中发现的那些主题。唯一的缺点可能是我们必须预先定义主题的数量——主题的数量是LDA的一个超参数，必须手动指定。</p><h1 id="dee6" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">带scikit的LDA学习</h1><p id="841a" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在这一小节中，我们将使用scikit-learn中实现的<strong class="ih hj"><em class="jp">LatentDirichletAllocation</em></strong>类来分解电影评论数据集，并将其分类到不同的主题中。在下面的示例中，我们将分析限制在10个不同的主题，但是鼓励读者试验算法的超参数，以进一步探索可以在该数据集中找到的主题。</p><p id="30c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们将使用本章开始时创建的电影评论的本地<strong class="ih hj"> <em class="jp"> movie_data.csv </em> </strong>文件将数据集加载到熊猫<strong class="ih hj"> <em class="jp"> DataFrame </em> </strong>中:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="f86f" class="kz js hi kv b fi la lb l lc ld">import pandas as pd<br/>df = pd.read_csv(‘movie_data.csv’, encoding=’utf-8')</span></pre><p id="b045" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将使用已经熟悉的<strong class="ih hj"> <em class="jp">计数矢量器</em> </strong>来创建单词袋矩阵作为LDA的输入。为了方便起见，我们将通过<strong class="ih hj"><em class="jp">stop _ words = ' English '</em></strong>使用scikit-learn的内置英语停用词库:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="b5af" class="kz js hi kv b fi la lb l lc ld">from sklearn.feature_extraction.text import CountVectorizer</span><span id="58bc" class="kz js hi kv b fi le lb l lc ld">count = CountVectorizer(stop_words=’english’, max_df=.1, max_features=5000)<br/>X = count.fit_transform(df[‘review’].values)</span></pre><p id="d7c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，我们将要考虑的单词的最大文档频率设置为10%(<strong class="ih hj"><em class="jp">max _ df = . 1</em></strong>)，以排除在文档中出现频率过高的单词。移除频繁出现的单词的基本原理是，这些单词可能是出现在所有文档中的常见单词，因此不太可能与给定文档的特定主题类别相关联。此外，我们将被考虑的单词的数量限制为最频繁出现的5000个单词(<strong class="ih hj"><em class="jp">max _ features = 5000</em></strong>)，以限制该数据集的维度，从而改进LDA执行的推理。但是<strong class="ih hj"> <em class="jp"> max_df=.1 </em> </strong>和<strong class="ih hj"><em class="jp">max _ features = 5000</em></strong>都是任意选择的超参数值，鼓励读者在比较结果的同时进行调优。</p><p id="05b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的代码示例演示了如何将<strong class="ih hj"><em class="jp">LatentDirichletAllocation</em></strong>估计器拟合到单词袋矩阵，并从文档中推断出10个不同的主题(注意，在笔记本电脑或标准台式计算机上，模型拟合可能需要5分钟或更长时间):</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="45ac" class="kz js hi kv b fi la lb l lc ld">from sklearn.decomposition import LatentDirichletAllocation<br/>lda = LatentDirichletAllocation(n_components=10,<br/>random_state=123, learning_method=’batch’)<br/>X_topics = lda.fit_transform(X)</span></pre><p id="c372" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过设置<strong class="ih hj"><em class="jp">learning _ method = ' batch '</em></strong>，我们让<strong class="ih hj"> <em class="jp"> lda </em> </strong>估计器在一次迭代中基于所有可用的训练数据(词袋矩阵)进行估计，这比替代的'<strong class="ih hj"> <em class="jp">在线</em> </strong>'学习方法慢，但可以导致更准确的结果(设置<strong class="ih hj"><em class="jp">learning _ method = ' online '</em></strong>类似于在线或小批量学习)。</p><p id="0049" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在拟合LDA之后，我们现在可以访问<strong class="ih hj"> <em class="jp"> lda </em> </strong>实例的<strong class="ih hj"> <em class="jp"> components_ </em> </strong>属性，该属性存储了一个矩阵，其中包含按升序排列的10个主题中的每一个主题的单词importance(此处为<strong class="ih hj"> <em class="jp"> 5000 </em> </strong>):</p><p id="4c40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lf lg lh kv b">lda.components_.shape</code> <em class="jp"> <br/> (10，5000) </em></p><p id="520d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了分析结果，让我们为10个主题中的每一个打印五个最重要的单词。请注意，单词重要性值按升序排列。因此，要打印前五个单词，我们需要以相反的顺序对主题数组进行排序:</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="58cd" class="kz js hi kv b fi la lb l lc ld">n_top_words = 5<br/>feature_names = count.get_feature_names()<br/>for topic_idx, topic in enumerate(lda.components_):<br/>  print(“Topic %d:” % (topic_idx + 1))<br/>  print(“ “.join([feature_names[i]<br/>  for i in topic.argsort()<br/>    [:-n_top_words — 1:-1]]))</span></pre><p id="b13a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">话题1: <br/>最差分钟烂剧本蠢<br/>话题2: <br/>家庭母亲父亲孩子女孩<br/>话题3: <br/>美国战争dvd音乐电视<br/>话题4: <br/>人类观众影院艺术感<br/>话题5: <br/>警察小伙汽车撞死人命案<br/>话题6: <br/>恐怖屋云雨女<br/>话题7: <br/>角色表演喜剧演员表演<br/>话题8: <br/>系列剧集战争插曲</em></p><p id="a1b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据阅读每个主题的五个最重要的单词，您可能会猜测LDA确定了以下主题:</p><ol class=""><li id="6523" class="li lj hi ih b ii ij im in iq lk iu ll iy lm jc ln lo lp lq bi translated">一般来说，糟糕的电影(不是真正的主题类别)</li><li id="0e42" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">关于家庭的电影</li><li id="5797" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">战争电影</li><li id="a55d" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">艺术电影</li><li id="ae32" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">犯罪电影</li><li id="3c6e" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">恐怖电影</li><li id="960f" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">喜剧电影</li><li id="1af8" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">与电视节目有某种联系的电影</li><li id="d853" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">根据书改编的电影</li><li id="f93e" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">动作电影</li></ol><p id="3319" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了确认基于评论的分类是有意义的，让我们从恐怖电影类别中绘制三部电影(恐怖电影属于索引位置<strong class="ih hj"> <em class="jp"> 5 </em> </strong>处的类别6):</p><pre class="je jf jg jh fd ku kv kw kx aw ky bi"><span id="1d52" class="kz js hi kv b fi la lb l lc ld">horror = X_topics[:, 5].argsort()[::-1]<br/>for iter_idx, movie_idx in enumerate(horror[:3]):<br/>  print(‘\nHorror movie #%d:’ % (iter_idx + 1))<br/>  print(df[‘review’][movie_idx][:300], ‘…’)</span></pre><p id="009f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">恐怖电影#1: <br/>《德拉库拉之家》与前年的《弗兰肯斯坦之家》的基本前提相同；即宇宙中最著名三个怪物；德古拉、弗兰肯斯坦的怪物和狼人一起出现在电影中。自然，这部电影因此相当混乱，但事实是… </em></p><p id="2210" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">恐怖电影#2: <br/>好吧，我现在到底在看什么狗屁垃圾？《魔女之山》是有史以来最不连贯、最疯狂的西班牙剥削电影之一，然而，与此同时，它也奇怪地引人注目。这里绝对没有任何有意义的东西，我甚至怀疑那里……</em></p><p id="f860" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">恐怖电影# 3:<br/>&lt;br/&gt;&lt;br/&gt;恐怖电影时间，日式风格。Uzumaki/Spiral从开始到结束完全是一场闹剧。这是一个有趣的节日，但有时它太依赖于媚俗而不是恐怖。这个故事很难简明扼要地概括:一个无忧无虑的普通少女开始走向fac … </em></p><p id="9d40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用前面的代码示例，我们打印了前三部恐怖电影中的前300个字符。这些评论——尽管我们不知道它们具体属于哪部电影——听起来像是对恐怖电影的评论(然而，有人可能会认为<strong class="ih hj"> <em class="jp">恐怖电影#2 </em> </strong>也很适合主题类别1:一般是糟糕的电影)。</p><h1 id="0ba3" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">摘要</h1><p id="c224" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在这篇文章中，我们看了机器学习的一个特殊应用，情感分析，这已经成为互联网和社交媒体时代的一个有趣的话题。我们被引入主题建模的概念，使用LDA以无监督的方式将电影评论分类到不同的类别中。<a class="ae jq" href="https://www.packtpub.com/in/data/python-machine-learning-third-edition?utm_source=AnalyticsVidhya&amp;utm_medium=referral&amp;utm_campaign=Outreach_PEN" rel="noopener ugc nofollow" target="_blank"> <em class="jp"> Python机器学习，第三版</em> </a>是用Python进行机器学习和深度学习的综合指南。</p><p id="5e19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">关于作者</strong></p><p id="a861" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Sebastian Raschka拥有多年的Python编码经验，他举办过几次关于数据科学、机器学习和深度学习的实际应用的研讨会，包括在SciPy(Python科学计算的领先会议)上的机器学习教程。他目前是UW大学统计学助理教授，专注于机器学习和深度学习研究。</p><p id="a5c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他的工作和贡献最近获得了2016-2017年度部门杰出研究生奖以及ACM Computing Reviews的2016年度最佳奖。在空闲时间，Sebastian喜欢为开源项目做贡献，他实现的方法现在已成功用于机器学习竞赛，如Kaggle。</p><p id="96fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Vahid Mirjalili获得了机械工程博士学位，研究大规模分子结构计算模拟的新方法。目前，他正在密歇根州立大学计算机科学与工程系重点研究机器学习在各种计算机视觉项目中的应用。</p><p id="50d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然Vahid的广泛研究兴趣集中在深度学习和计算机视觉应用上，但他特别感兴趣的是利用深度学习技术来扩展生物数据(如人脸图像)的隐私，以便信息不会超出用户打算透露的范围。此外，他还与一个研究自动驾驶汽车的工程师团队合作，在那里他设计了用于行人检测的多光谱图像融合的神经网络模型。</p></div></div>    
</body>
</html>