<html>
<head>
<title>Coding PPO from Scratch with PyTorch (Part 3/4)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PyTorch从头开始编写PPO代码(第3/4部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/coding-ppo-from-scratch-with-pytorch-part-3-4-82081ea58146?source=collection_archive---------0-----------------------#2020-09-17">https://medium.com/analytics-vidhya/coding-ppo-from-scratch-with-pytorch-part-3-4-82081ea58146?source=collection_archive---------0-----------------------#2020-09-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f942f7d3e41dd4cb6b08e70fc0af8c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1onC-8G2CMKfvhVBMnFuQw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">我的4部分系列的路线图。</figcaption></figure><p id="a316" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">欢迎阅读本系列的第3部分，在这里我们将使用PyTorch从头开始编写近似策略优化(PPO)代码。如果你还没有看过<a class="ae js" rel="noopener" href="/@eyyu/coding-ppo-from-scratch-with-pytorch-part-1-4-613dfc1b14c8">第一部</a>和<a class="ae js" rel="noopener" href="/@eyyu/coding-ppo-from-scratch-with-pytorch-part-2-4-f9d8b8aa938a">第二部</a>，请先阅读。</p><p id="1e99" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面是代码:<a class="ae js" href="https://github.com/ericyangyu/PPO-for-Beginners" rel="noopener ugc nofollow" target="_blank">https://github.com/ericyangyu/PPO-for-Beginners</a></p><p id="265b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将从我们停止的地方开始:步骤5。这里是伪代码的概述，也可以在<a class="ae js" href="https://spinningup.openai.com/en/latest/algorithms/ppo.html#id7" rel="noopener ugc nofollow" target="_blank">这里</a>找到:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jt"><img src="../Images/0f776daa3f4e5d3f0dc9113666949b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AX2vLguiKvxn-YIntIx18w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">OpenAI的Spinning Up doc上PPO的伪代码。</figcaption></figure><p id="0b0f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在我们继续之前，请注意，对于步骤6和7，我们将在每个迭代中执行多个时期。我们将在后面的<code class="du jz ka kb kc b">_init_hyperparameters</code>中添加历元的数量<em class="jy"> n </em>，作为超参数。我们这样做是因为如果你注意到，在步骤5-7中，参数θ和φ上有一个<em class="jy"> k </em>下标<em class="jy"> </em>。这表明第k次迭代的参数与训练中的当前时刻之间存在差异，这意味着每次迭代也有其自己的一组时期要运行。一旦我们到达这些步骤，我们将再次看到这一点，并为它们编写代码。</p><p id="26d9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们看看第5步。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kd"><img src="../Images/9d75a9723124a7427d74887f9c961ef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dpGCgqhYde1ByuUil9rAew.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">第五步</figcaption></figure><p id="3c5c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里我们将使用<a class="ae js" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions:~:text=reward%2Dplus%2Dnext%2Dvalue.-,Advantage%20Functions,The%20advantage%20function%20%20corresponding%20to%20a%20policy%20%20describes%20how%20much%20better%20it%20is%20to%20take%20a%20specific%20action%20%20in%20state%20%2C%20over%20randomly%20selecting%20an%20action%20according%20to%20%2C%20assuming%20you%20act%20according%20to%20%20forever%20after.%20Mathematically%2C%20the%20advantage%20function%20is%20defined%20by" rel="noopener ugc nofollow" target="_blank">定义的优势函数</a>。TL；这是用Vᵩₖ:修正的方程式</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ke"><img src="../Images/c252508979bab3e167204678497943ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePa3qJANc6seTA6NSIRpaA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">优势功能。</figcaption></figure><p id="f52a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">其中，Q^π是状态动作对(s，a)的q值，而Vᵩₖ是由我们的评论家网络在第<em class="jy"> k </em>次迭代上跟随参数φ确定的一些观察值<em class="jy"> s </em>。</p><p id="0f5a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们对公式<a class="ae js" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#advantage-functions:~:text=reward%2Dplus%2Dnext%2Dvalue.-,Advantage%20Functions,The%20advantage%20function%20%20corresponding%20to%20a%20policy%20%20describes%20how%20much%20better%20it%20is%20to%20take%20a%20specific%20action%20%20in%20state%20%2C%20over%20randomly%20selecting%20an%20action%20according%20to%20%2C%20assuming%20you%20act%20according%20to%20%20forever%20after.%20Mathematically%2C%20the%20advantage%20function%20is%20defined%20by" rel="noopener ugc nofollow" target="_blank">进行了修改，此处</a>指定预测值跟随第<em class="jy"> k </em>次迭代的参数φ，这很重要，因为稍后在步骤7中，我们需要重新计算第<em class="jy"> i </em>个时期的参数φ之后的V(s)。然而，由于q值是在每次部署之后确定的，并且Vᵩₖ(s必须在我们对我们的网络执行多次更新之前确定(否则，Vᵩₖ(s将随着我们更新我们的critic网络而改变，这被证明是不稳定的并且与伪代码不一致)，并且优势必须在我们的纪元循环之前计算。</p><p id="7a9f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们处理这个问题的方法是用子程序分别计算我们的q值和预测值Vᵩₖ(s。我们已经用<code class="du jz ka kb kc b">compute_rtgs</code>计算出了q值，所以我们只需要担心Vᵩₖ(s).</p><p id="212b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们创建一个函数<code class="du jz ka kb kc b">evaluate</code>来计算Vᵩₖ(s).</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="8a68" class="kj kk hi kc b fi kl km l kn ko">def evaluate(self, batch_obs):<br/>  # Query critic network for a value V for each obs in batch_obs.<br/>  V = self.critic(batch_obs).squeeze()</span><span id="bad1" class="kj kk hi kc b fi kp km l kn ko">  return V</span></pre><p id="a5c9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请注意，我们对critic网络上正向传递的返回张量执行了一个<code class="du jz ka kb kc b">squeeze</code>操作。如果你不知道它的作用，它基本上改变了张量的维度。例如，在[[1]，[2]，[3]]上调用<code class="du jz ka kb kc b">squeeze</code>，将返回[1，2，3]。由于batch_obs保留了形状(每批的时间步长，观察的维度)，将batch_obs传递到我们的critic网络中返回的张量是(每批的时间步长，1)，而我们想要的形状只是(每批的时间步长)。<code class="du jz ka kb kc b">squeeze</code>就行了。如果你想深入了解，这里有一些关于<code class="du jz ka kb kc b">squeeze</code>的<a class="ae js" href="https://pytorch.org/docs/stable/generated/torch.squeeze.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="eb66" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">接下来，我们可以简单地计算优势:</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="b171" class="kj kk hi kc b fi kl km l kn ko"># Calculate V_{phi, k}<br/>V = self.evaluate(batch_obs)</span><span id="1a78" class="kj kk hi kc b fi kp km l kn ko"># ALG STEP 5<br/># Calculate advantage<br/>A_k = batch_rtgs - V.detach()</span></pre><p id="8e45" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">注意，我们使用<code class="du jz ka kb kc b">V.detach()</code>,因为<code class="du jz ka kb kc b">V</code>是一个需要梯度的张量。然而，该优势将需要在每个历元循环中重复使用，并且在第<em class="jy"> k </em>次迭代中与优势相关联的计算图在随机梯度上升的多个历元中将是无用的。</p><p id="7e95" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，我们在这段代码中使用的唯一技巧之一是:优势规范化。通过反复试验，我发现使用原始优势会使PPO训练非常不稳定(是的，甚至比我们在第1部分中描述的图表更不稳定，方差更高)。虽然归一化优势并不在伪代码中，但在实践中它是极其重要的，因为当不同的维度在规模上也不同时，数值算法表现不佳。我本来打算把优势规范化放在第4部分，因为从技术上来说它是一种优化，但是我发现为了保持PPO的合理性能水平，几乎有必要把它放在代码中。所以，这就是:</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="6044" class="kj kk hi kc b fi kl km l kn ko"># Normalize advantages<br/>A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)</span></pre><p id="ad5e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">注意我们在优势的标准差上加1e-10，只是为了避免被0除的可能。</p><p id="8bb1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以下是目前为止的代码:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/e9ba614f017341b2859afee262488a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_GiR2v9TZeOuR-X9FkYcg.png"/></div></div></figure><p id="c80a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们看看第6步。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/bd4a0597fef7328175dc60ee68d87364.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T7oDTPoqCfRM7ZDMGPjfgA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">第六步。</figcaption></figure><p id="8c5b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">啊，是的，房间里的大象。PPO的生命和血液。这个公式告诉我们如何更新演员网络的参数θ。对我们来说幸运的是，我们需要的大部分东西要么已经计算出来，要么可以用现有的子程序计算出来。</p><p id="bf44" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，让我们解决左代理函数中的比率问题。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/5dcd672930c91f5e14899f1bec7ca086.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*VavMuUkdCLP65zcOK0zpNg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">参数为θ的动作概率与参数为θₖ.的动作概率之比</figcaption></figure><p id="16d0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">好的，我们需要计算我们在最近一次展示中采取的行动的对数概率。同样，对于为什么我们发现日志问题而不是原始行动问题，这里的<a class="ae js" href="https://cs.stackexchange.com/questions/70518/why-do-we-use-the-log-in-gradient-based-reinforcement-algorithms" rel="noopener ugc nofollow" target="_blank"/>是解释原因的资源，这里的<a class="ae js" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient:~:text=2.%20The%20Log%2DDerivative%20Trick.%20The%20log%2Dderivative,combined%20with%20chain%20rule%2C%20we%20get%3A" rel="noopener ugc nofollow" target="_blank"/>是另一个资源。</p><p id="586b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">底部的对数问题集将与第<em class="jy"> k </em>次迭代(我们已经有了<code class="du jz ka kb kc b">batch_log_probs</code>)的参数θ相关，而顶部正好在当前时期(原始伪代码也假设多个时期)。让我们在<code class="du jz ka kb kc b">evaluate</code>中完成它，而不是定义一个全新的子程序来计算log probs。</p><p id="197b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，让我们修复<code class="du jz ka kb kc b">evaluate</code>来返回动作的日志问题。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="4d92" class="kj kk hi kc b fi kl km l kn ko">def evaluate(self, batch_obs, batch_acts):<br/>  ...<br/>  # Calculate the log probabilities of batch actions using most <br/>  # recent actor network.<br/>  # This segment of code is similar to that in get_action()<br/>  mean = self.actor(batch_obs)<br/>  dist = MultivariateNormal(mean, self.cov_mat)<br/>  log_probs = dist.log_prob(batch_acts)</span><span id="4f97" class="kj kk hi kc b fi kp km l kn ko">  # Return predicted values V and log probs log_probs<br/>  return V, log_probs</span></pre><p id="2395" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">接下来，让我们修复之前进行的evaluate调用，以解包一个额外的返回值来计算advantage。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="9d0c" class="kj kk hi kc b fi kl km l kn ko">V, _ = self.evaluate(batch_obs, batch_acts)</span><span id="40c6" class="kj kk hi kc b fi kp km l kn ko">A_k = batch_rtgs - V.detach()</span><span id="d4b1" class="kj kk hi kc b fi kp km l kn ko">...</span></pre><p id="c1c2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，让我们开始我们的纪元循环，在我们的演员和评论家网络上执行多次更新。历元数是一个超参数，所以我们也可以将它添加到<code class="du jz ka kb kc b">_init_hyperparameters</code>中。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="d532" class="kj kk hi kc b fi kl km l kn ko">for _ in range(self.n_updates_per_iteration):<br/>  # epoch code</span><span id="18d1" class="kj kk hi kc b fi kp km l kn ko">def _init_hyperparameters(self):<br/>  ...<br/>  self.n_updates_per_iteration = 5</span></pre><p id="0445" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">注意，我任意选择了5。现在再次注意比率公式:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/5dcd672930c91f5e14899f1bec7ca086.png" data-original-src="https://miro.medium.com/v2/resize:fit:274/format:webp/1*VavMuUkdCLP65zcOK0zpNg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">参数为θ的动作概率与参数为θₖ.的动作概率之比</figcaption></figure><p id="258a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们已经有了最底层的日志问题。我们只需要找到π_θ(aₜ|sₜ)，我们也可以用<code class="du jz ka kb kc b">evaluate</code>来表示。注意，这是我们第二次调用<code class="du jz ka kb kc b">evaluate</code>，这一次它将在纪元循环中而不是之前。我们之前调用的<code class="du jz ka kb kc b">evaluate</code>只提取<code class="du jz ka kb kc b">V</code>用于计算优势，就在这个纪元循环之前。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="6bf3" class="kj kk hi kc b fi kl km l kn ko">for _ in range(self.n_updates_per_iteration):<br/>  # Calculate pi_theta(a_t | s_t)<br/>  _, curr_log_probs = self.evaluate(batch_obs, batch_acts)</span></pre><p id="7265" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，由于<code class="du jz ka kb kc b">batch_log_probs</code>和<code class="du jz ka kb kc b">curr_log_probs</code>都是对数概率，我们可以将它们相减，然后用<em class="jy"> e </em>对对数求幂。很酷的小魔术。注意<code class="du jz ka kb kc b">curr_log_probs</code>没有被分离，这意味着它有一个与之相关联的计算图，我们希望在以后计算梯度时将它作为反向传播的一部分。这是我们计算图表的开始。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="c676" class="kj kk hi kc b fi kl km l kn ko"># Calculate ratios<br/>ratios = torch.exp(curr_log_probs - batch_log_probs)</span></pre><p id="02f2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，让我们来计算替代损失。替代损失是我们将在步骤6中取最小值的两个损失。第一个替代损失使用原始比率来计算比率*优势，而第二个替代损失剪辑比率，以确保我们在梯度上升期间不会在任何方向上走得太远。这应该很容易，因为我们已经找到了公式的所有小部分。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="c4e3" class="kj kk hi kc b fi kl km l kn ko"># Calculate surrogate losses<br/>surr1 = ratios * A_k<br/>surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip) * A_k</span><span id="cf95" class="kj kk hi kc b fi kp km l kn ko">...</span><span id="c21f" class="kj kk hi kc b fi kp km l kn ko">def _init_hyperparameters(self):<br/>  ...<br/>  self.clip = 0.2 # As recommended by the paper</span></pre><p id="89ab" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我使用torch.clamp，它将参数1或者参数2和参数3之间的比率绑定为各自的下限和上限。有关<code class="du jz ka kb kc b">torch.clamp</code>、<a class="ae js" href="https://pytorch.org/docs/stable/generated/torch.clamp.html" rel="noopener ugc nofollow" target="_blank">的一些文档，请点击这里</a>。</p><p id="a4d1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最后，我们计算我们的全部演员损失。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="291d" class="kj kk hi kc b fi kl km l kn ko">actor_loss = (-torch.min(surr1, surr2)).mean()</span></pre><p id="ef31" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们后退一步。按照伪代码，我们取两个替代损失的最小值。我们有负面影响，因为我们试图通过随机梯度上升来最大化这一损失或性能/目标函数，但我们将使用的优化器是Adam，它可以最小化这一损失。因此，最小化负损失可以最大化性能函数。然后，我们取平均值，生成一个单一的损失作为浮动。</p><p id="4c62" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们在演员网络上进行反向传播。首先，我们需要为我们的actor参数定义Adam optimizer。让我们在<code class="du jz ka kb kc b">__init__</code>中这样做。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="1702" class="kj kk hi kc b fi kl km l kn ko">from torch.optim import Adam</span><span id="c816" class="kj kk hi kc b fi kp km l kn ko">class PPO:<br/>  def __init__(self, env):<br/>    ...<br/>    self.actor_optim = Adam(self.actor.parameters(), lr=self.lr)</span><span id="67bc" class="kj kk hi kc b fi kp km l kn ko">  def _init_hyperparameters(self):<br/>    ...<br/>    self.lr = 0.005</span></pre><p id="817c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">同样，<code class="du jz ka kb kc b">lr</code>或学习率是任意定义的。让我们现在做我们的反向传播，并在我们的演员网络执行一个时代。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="6381" class="kj kk hi kc b fi kl km l kn ko"># Calculate gradients and perform backward propagation for actor <br/># network<br/>self.actor_optim.zero_grad()<br/>actor_loss.backward()<br/>self.actor_optim.step()</span></pre><p id="2fe4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这就是可怕的第六步！以下是目前为止的代码:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kt"><img src="../Images/a19ba84d4f6cbc6463d4cf5f9fa58750.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vH8EDcu0S8Zo4TzFzWnLFQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">__init__</figcaption></figure><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ku"><img src="../Images/3eae18bf6a972f5308c27cdf1c148c2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8cgVX0977Qs9Q9tYQnEDQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">学习</figcaption></figure><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/b66c6edbf7b246c2fb21b810aace1989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*LjFa9K7Bm2oZAd5w27vTxA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">评估，_ init _超参数</figcaption></figure><p id="19d9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们继续倒数第二步，第7步。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/0a9e1689587f4702f0406c83cc397856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1AEewuupaFtri8n05FfHmA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">第七步。</figcaption></figure><p id="173a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这可能看起来真的很可怕，但我们真的只是用当前时期的预测值的均方误差(Vᵩ(sₜ)来更新critic参数，并进行奖励。我们将使用来自<code class="du jz ka kb kc b">torch.nn</code>的给定类，它将为我们计算MSE，<code class="du jz ka kb kc b">torch.nn.MSELoss</code>。这里有一些<a class="ae js" href="https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html" rel="noopener ugc nofollow" target="_blank">文档</a>。如果你愿意，你也可以自己写MSE函数来代替torch的，应该不会太难。</p><p id="764c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，让我们为我们的评论家网络定义另一个Adam优化器。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="0504" class="kj kk hi kc b fi kl km l kn ko">self.critic_optim = Adam(self.critic.parameters(), lr=self.lr)</span></pre><p id="5c14" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然后，我们需要计算Vᵩ(sₜ)和奖励。幸运的是，我们已经用<code class="du jz ka kb kc b">batch_rtgs</code>计算了奖励积分。对我们来说更幸运的是，我们可以找到Vᵩ(sₜ),只需对现有代码做一个简单的修改:在epoch循环中调用<code class="du jz ka kb kc b">evaluate</code>的地方，只保留返回的<code class="du jz ka kb kc b">V</code>,而不是用<code class="du jz ka kb kc b">_</code>忽略它。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="bc95" class="kj kk hi kc b fi kl km l kn ko"># Calculate V_phi and pi_theta(a_t | s_t)    <br/>V, curr_log_probs = self.evaluate(batch_obs, batch_acts)</span></pre><p id="ce7a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最后但同样重要的是，计算预测值和奖励的MSE，并在critic网络上反向传播。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="fc08" class="kj kk hi kc b fi kl km l kn ko">critic_loss = nn.MSELoss()(V, batch_rtgs)</span><span id="de91" class="kj kk hi kc b fi kp km l kn ko"># Calculate gradients and perform backward propagation for critic network    <br/>self.critic_optim.zero_grad()    <br/>critic_loss.backward()    <br/>self.critic_optim.step()</span></pre><p id="8a5b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请注意，由于我们正在对计算图进行第二次反向传播，并且actor和critic损失计算图都在图中向上收敛了一点，因此我们需要为actor或critic添加一个<code class="du jz ka kb kc b">retain_graph=True</code>到<code class="du jz ka kb kc b">backward</code>(取决于我们首先反向传播哪个)。否则，我们将得到一个错误，表明当第二次试图通过图返回时，缓冲区已经被释放。</p><p id="fc32" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这就是第7步！以下是目前为止的代码:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/bfa3cc83d81500acfde7f281c143f889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*Vk1UY_nrMHQQqYX-rZ3lxA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">__init__</figcaption></figure><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/d4c127ccc90212c925fef11ecb82a918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZLruw838IRWjRwrIpUhbSQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">学习</figcaption></figure><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/c8e20dc3ea91e95e20f3c4bc7fd584f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tESMmBO6BRtFzwxVIf2uug.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">其余的代码，从第6步开始就没有改变。</figcaption></figure><p id="3d68" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，最后一步…第八步。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es la"><img src="../Images/e6a2601ff702a07e498063fc6a508fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*9GIyolqn4HOtxXFR3R4feg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">第八步。</figcaption></figure><p id="dfa2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">是的，很可能是最难的一步。只要等到你看到第9步，这是很难的一步。</p><p id="e862" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">更严肃地说，我们不能忘记增加我们在第2部分开始时设置的<code class="du jz ka kb kc b">t_so_far</code>，以便记录要运行多少次迭代。让我们利用从<code class="du jz ka kb kc b">rollout</code>返回的<code class="du jz ka kb kc b">batch_lens</code>来做这件事。</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="519d" class="kj kk hi kc b fi kl km l kn ko">import numpy as np</span><span id="c089" class="kj kk hi kc b fi kp km l kn ko">...</span><span id="bcbf" class="kj kk hi kc b fi kp km l kn ko"># ALG STEP 2<br/>while t_so_far &lt; total_timesteps:<br/>  ... <br/>  batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = self.rollout()</span><span id="3bf6" class="kj kk hi kc b fi kp km l kn ko">  # Calculate how many timesteps we collected this batch   <br/>  t_so_far += np.sum(batch_lens)</span></pre><p id="b9db" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">而现在你有了一个功能齐全的裸机PPO！为了测试到目前为止是否一切正常，您可以在您的<strong class="iw hj"> ppo.py </strong>底部运行这段代码:</p><pre class="ju jv jw jx fd kf kc kg kh aw ki bi"><span id="cb6e" class="kj kk hi kc b fi kl km l kn ko">import gym<br/>env = gym.make('Pendulum-v0')<br/>model = PPO(env)<br/>model.learn(10000)</span></pre><p id="5d12" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果程序运行没有任何错误(大约需要10秒钟)，您就成功了。如果到目前为止有任何问题或错误，请让我知道。你也可以用主要的<a class="ae js" href="https://github.com/ericyangyu/PPO-for-Beginners" rel="noopener ugc nofollow" target="_blank"> PPO初学者代码</a>来交叉引用你现在拥有的。</p><p id="7ef9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在你可能想知道:Eric，你只写了<strong class="iw hj"> ppo.py </strong>和<strong class="iw hj"> network.py </strong>，但是在第1部分中你还有<strong class="iw hj"> main.py </strong>、<strong class="iw hj"> arguments.py </strong>和<strong class="iw hj"> eval_policy.py </strong>。您还拥有许多其他功能，如日志记录、保存演员和评论家网络、解析命令行参数、自定义超参数等等。另外，<a class="ae js" href="https://github.com/ericyangyu/PPO-for-Beginners" rel="noopener ugc nofollow" target="_blank"> PPO for初学者</a>代码看起来和上面的截图有点不一样。</p><p id="c579" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">你完全正确。然而，为了简单起见，我不会在本系列中讨论我是如何编写这些部分的，因为这与学习如何用PyTorch编写一个基本的PPO实现无关。我保证所有额外的东西都放在最基本的PPO之上，它是其余代码的基础。</p><p id="8995" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">相反，我鼓励您亲自探索额外的特性如何与基本的PPO实现同步工作。我强烈推荐使用<code class="du jz ka kb kc b">pdb</code>，或者python调试器，从<strong class="iw hj"> main.py </strong>中的<code class="du jz ka kb kc b">if __name__ == '__main__':</code>开始逐步调试我的代码。如果你不知道如何使用<code class="du jz ka kb kc b">pdb</code>，在这里快速入门<a class="ae js" href="https://www.youtube.com/watch?v=bHx8A8tbj2c&amp;ab_channel=RealPython" rel="noopener ugc nofollow" target="_blank">。如果你已经是使用<code class="du jz ka kb kc b">pdb</code>的专家，这里有</a><a class="ae js" href="https://docs.python.org/3/library/pdb.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="b123" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">存储库<strong class="iw hj"> README.md </strong>包含如何在“Usage”下运行代码的说明。我所有的文件和代码都设计得非常详细，并且有很好的文档记录，所以你可以尽情地探索它们。我把它们组织起来，希望事情尽可能简单和模块化。</p><p id="c23c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">本系列的第3部分到此结束；此时，您应该已经从伪代码中完全实现了PPO，并且应该能够从第1部分中看到的图表中获得性能。我知道这有很多材料需要消化，所以如果你有任何问题，不要犹豫，联系我在eyyu@ucsd.edu或只是评论如下。</p><p id="f2d6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在第4部分中，我们将探索一些可以在基本的PPO上执行的优化，以提高性能并减少差异。然而，第4部分可能还需要一段时间。在此之前，希望这个系列到目前为止对你有所帮助！</p></div></div>    
</body>
</html>