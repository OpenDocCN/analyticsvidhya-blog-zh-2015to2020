<html>
<head>
<title>Object Localization with Keras and W&amp;B</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 Keras 和 W&amp;B 的目标定位</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/object-localization-with-keras-2f272f79e03c?source=collection_archive---------6-----------------------#2020-11-03">https://medium.com/analytics-vidhya/object-localization-with-keras-2f272f79e03c?source=collection_archive---------6-----------------------#2020-11-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="5617" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><strong class="ak">该报告使用 Keras 中的边界框回归技术探索了目标定位，并以交互方式可视化了模型的权重预测&amp;偏差</strong></h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/765ddc76583cb849b668a03e7b8bae22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fCMI0Kwfxpqfus56"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由<a class="ae jn" href="https://unsplash.com/@nhillier?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">尼克·希利尔</a>在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h2 id="c4b5" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><a class="ae jn" href="https://wandb.ai/wandb/object_localization/reports/Object-Localization-with-Keras-and-W-B--VmlldzoyNzA2Mzk" rel="noopener ugc nofollow" target="_blank">互动报道</a> | <a class="ae jn" href="https://colab.research.google.com/drive/1LWbgjNbQLQHgfY2WH0lKBEFRIZXdqT86?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab 笔记本</a></h2><h1 id="700a" class="km jp hi bd jq kn ko kp ju kq kr ks jy io kt ip kc ir ku is kg iu kv iv kk kw bi translated">介绍</h1><p id="11a0" class="pw-post-body-paragraph kx ky hi kz b la lb ij lc ld le im lf jz lg lh li kd lj lk ll kh lm ln lo lp hb bi translated">对象定位是<strong class="kz hj">在图像中定位特定对象类别</strong>的实例的任务，通常通过指定以实例为中心的紧密裁剪的边界框。相反，对象检测是定位所有目标对象的所有可能实例的任务。</p><p id="f9bb" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">物体定位也叫“带定位的分类”。这是因为可以稍微修改执行图像分类的架构来预测边界框坐标。查看<a class="ae jn" href="https://www.youtube.com/watch?v=GSwYGkTfOKk" rel="noopener ugc nofollow" target="_blank">吴恩达关于物体定位的讲座</a>或查看<a class="ae jn" href="https://www.pyimagesearch.com/2020/10/05/object-detection-bounding-box-regression-with-keras-tensorflow-and-deep-learning/" rel="noopener ugc nofollow" target="_blank">物体检测:Keras、TensorFlow 和深度学习的边界框回归</a>作者<a class="ae jn" href="https://www.pyimagesearch.com/author/adrian/" rel="noopener ugc nofollow" target="_blank"> Adrian Rosebrock </a>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lv"><img src="../Images/7a80550ad3ee321c46d4e32f9da1df5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sW8lbS9CwbyE5cXm.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jq">图 1 </strong>:图像分类、物体定位、物体检测的区别。(<a class="ae jn" href="http://datahacker.rs/deep-learning-object-localization/" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="336a" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">在本报告中，我们将构建一个对象定位模型，并在一个合成数据集上对其进行训练。我们将<strong class="kz hj">交互可视化我们模型的预测</strong>权重&amp;偏差。</p><h1 id="ce1a" class="km jp hi bd jq kn ko kp ju kq kr ks jy io kt ip kc ir ku is kg iu kv iv kk kw bi translated">数据集</h1><p id="8f84" class="pw-post-body-paragraph kx ky hi kz b la lb ij lc ld le im lf jz lg lh li kd lj lk ll kh lm ln lo lp hb bi translated">我们将使用一个基于 MNIST 数据集的合成数据集来完成目标定位任务。这个数据集是由劳伦斯·莫罗尼制作的。想法是，代替 28×28 像素的 MNIST 图像，它可以是 NxN(100×100)，并且任务是预测手指位置的边界框。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/3b0459296d22458f7ede3ee6ce613544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/0*I2TprktWKDTjJD9Z.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jq">图 2 </strong>:数据集的样本。每个图像都是 100x100 像素。</figcaption></figure><h2 id="6fee" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">下载数据集</h2><p id="43fb" class="pw-post-body-paragraph kx ky hi kz b la lb ij lc ld le im lf jz lg lh li kd lj lk ll kh lm ln lo lp hb bi translated">这个<a class="ae jn" href="https://github.com/lmoroney/synthetic_datasets" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>是数据集的原始来源。但是，由于这个<a class="ae jn" href="https://github.com/lmoroney/synthetic_datasets/issues/2" rel="noopener ugc nofollow" target="_blank">问题</a>，我们将使用原始存储库的<a class="ae jn" href="https://github.com/ayulockin/synthetic_datasets" rel="noopener ugc nofollow" target="_blank"> my fork </a>。</p><p id="2d3b" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">我们还有一个<code class="du lx ly lz ma b">.csv</code>训练和测试文件，其中包含图像名称、标签和边界框坐标。注意，坐标被缩放到<code class="du lx ly lz ma b">[0, 1]</code>。</p><pre class="iy iz ja jb fd mb ma mc md aw me bi"><span id="c3ed" class="jo jp hi ma b fi mf mg l mh mi">!git clone https://github.com/ayulockin/synthetic_datasets<br/>%cd synthetic_datasets/MNIST/<br/>%mkdir images</span><span id="5626" class="jo jp hi ma b fi mj mg l mh mi">!unzip -q MNIST_Converted_Training.zip -d images/<br/>!unzip -q MNIST_Converted_Testing.zip -d images/</span></pre><h2 id="9085" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">使用<code class="du lx ly lz ma b">tf.data</code>的数据加载器</h2><p id="bb8a" class="pw-post-body-paragraph kx ky hi kz b la lb ij lc ld le im lf jz lg lh li kd lj lk ll kh lm ln lo lp hb bi translated">我们将使用<code class="du lx ly lz ma b">tf.data.Dataset</code>构建我们的输入管道。我们的模型必须预测图像的类别(所讨论的对象)和给定输入图像的边界框坐标。</p><p id="94c8" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">在<em class="mk">模型</em>部分，您将认识到该模型是一个多输出架构。查看<a class="ae jn" href="https://www.pyimagesearch.com/author/adrian/" rel="noopener ugc nofollow" target="_blank"> Adrian Rosebrock </a>的<a class="ae jn" href="https://www.pyimagesearch.com/2018/06/04/keras-multiple-outputs-and-multiple-losses/" rel="noopener ugc nofollow" target="_blank">Keras:Multiple outputs and Multiple loss</a>以了解更多信息。</p><p id="654a" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated"><strong class="kz hj">下图所示的</strong> <code class="du lx ly lz ma b"><strong class="kz hj">tf.data.Dataset</strong></code> <strong class="kz hj">流水线处理多输出训练</strong>。我们将随图像一起返回标签和边界框坐标的字典。<strong class="kz hj">键的名称应该与输出层的名称相同。</strong></p><pre class="iy iz ja jb fd mb ma mc md aw me bi"><span id="9f36" class="jo jp hi ma b fi mf mg l mh mi">AUTO = tf.data.experimental.AUTOTUNE<br/>BATCH_SIZE = 32</span><span id="f0bb" class="jo jp hi ma b fi mj mg l mh mi">@tf.function<br/>def preprocess_train(image_name, label, bbox):<br/>    image = tf.io.read_file(TRAIN_IMG_PATH+image_name)<br/>    image = tf.image.decode_png(image, channels=1)</span><span id="d24b" class="jo jp hi ma b fi mj mg l mh mi">    return image, {'label': label, 'bbox': bbox} # Notice here</span><span id="8ce2" class="jo jp hi ma b fi mj mg l mh mi">@tf.function<br/>def preprocess_test(image_name, label, bbox):<br/>    image = tf.io.read_file(TEST_IMG_PATH+image_name)<br/>    image = tf.image.decode_png(image, channels=1)</span><span id="52db" class="jo jp hi ma b fi mj mg l mh mi">    return image, {'label': label, 'bbox': bbox} # Notice here</span><span id="721c" class="jo jp hi ma b fi mj mg l mh mi">trainloader = tf.data.Dataset.from_tensor_slices((train_image_names, train_labels, train_bbox))<br/>testloader = tf.data.Dataset.from_tensor_slices((test_image_names, test_labels, test_bbox))</span><span id="03d9" class="jo jp hi ma b fi mj mg l mh mi">trainloader = (<br/>    trainloader<br/>    .map(preprocess_train, num_parallel_calls=AUTO)<br/>    .shuffle(1024)<br/>    .batch(BATCH_SIZE)<br/>    .prefetch(AUTO)<br/>)</span><span id="5739" class="jo jp hi ma b fi mj mg l mh mi">testloader = (<br/>    testloader<br/>    .map(preprocess_test, num_parallel_calls=AUTO)<br/>    .batch(BATCH_SIZE)<br/>    .prefetch(AUTO)<br/>)</span></pre><h1 id="a5e8" class="km jp hi bd jq kn ko kp ju kq kr ks jy io kt ip kc ir ku is kg iu kv iv kk kw bi translated">模型</h1><h2 id="6153" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">包围盒回归</h2><p id="69d2" class="pw-post-body-paragraph kx ky hi kz b la lb ij lc ld le im lf jz lg lh li kd lj lk ll kh lm ln lo lp hb bi translated">在我们建立模型之前，让我们简单讨论一下包围盒回归。在机器学习文献中，回归是将输入值 X 映射到连续输出变量 y 的任务。</p><p id="fa84" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">因此，我们返回一个数字而不是一个类，在我们的例子中，我们将返回 4 个与边界框相关的数字(x1，y1，x2，y2)。我们将用一幅图像和一个真实边界框来训练这个系统，并使用 L2 损失来计算预测边界框和真实边界框之间的损失。查看此视频以了解更多关于边界框回归的信息。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/e4d4194d3ba513412ce8369915958362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-7pdw0pSFbwNJmpL.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jq">图 3 </strong>:物体定位任务的包围盒回归通用模型架构。(<a class="ae jn" href="https://leonardoaraujosantos.gitbook.io/artificial-inteligence/machine_learning/deep_learning/object_localization_and_detection#localize-objects-with-regression" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="e2bb" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">回到模型，图 3 正确地总结了模型架构。该模型由三部分组成——卷积块(特征提取器)、分类头和回归头。</p><p id="3785" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">这是一种多输出配置。正如在<em class="mk">数据集</em>一节中提到的，<code class="du lx ly lz ma b">tf.data.Dataset</code>输入管道返回一个字典，其键名是分类头和回归头的输出层的名称。</p><p id="4757" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">下面显示的代码片段为对象本地化构建了我们的模型架构。</p><pre class="iy iz ja jb fd mb ma mc md aw me bi"><span id="90a5" class="jo jp hi ma b fi mf mg l mh mi">def get_model():<br/>    inputs = Input(shape=(100,100,1))<br/>    x = Conv2D(32, (3,3), activation='relu')(inputs)<br/>    x = MaxPooling2D((3,3))(x)<br/>    x = Conv2D(32, (3,3), activation='relu')(x)<br/>    x = MaxPooling2D((3,3))(x)<br/>    x = Conv2D(64, (3,3), activation='relu')(x)<br/>    x = GlobalAveragePooling2D()(x)</span><span id="470a" class="jo jp hi ma b fi mj mg l mh mi">    classifier_head = Dropout(0.3)(x)<br/>    # Notice the name of the layer.<br/>    classifier_head = Dense(10, activation='softmax', name='label')(classifier_head)</span><span id="3a81" class="jo jp hi ma b fi mj mg l mh mi">    reg_head = Dense(64, activation='relu')(x)<br/>    reg_head = Dense(32, activation='relu')(reg_head)<br/>    # Notice the name of the layer.<br/>    reg_head = Dense(4, activation='sigmoid', name='bbox')(reg_head)</span><span id="a05e" class="jo jp hi ma b fi mj mg l mh mi">    return Model(inputs=[inputs], outputs=[classifier_head, reg_head])</span></pre><p id="eccf" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">给予多个标题的名称被用作<code class="du lx ly lz ma b">losses</code>字典的关键字。请注意，分类头的激活功能是<code class="du lx ly lz ma b">softmax</code>，因为它是一个多级分类设置(0-9 位数)。回归头的激活功能是<code class="du lx ly lz ma b">sigmoid</code>，因为边界框坐标在<code class="du lx ly lz ma b">[0, 1]</code>的范围内。</p><p id="4391" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">损失函数被适当地选择。我们可以选择给不同的损失函数不同的权重。</p><pre class="iy iz ja jb fd mb ma mc md aw me bi"><span id="1b33" class="jo jp hi ma b fi mf mg l mh mi">losses = {'label': 'sparse_categorical_crossentropy',<br/>          'bbox': 'mse'}</span><span id="9362" class="jo jp hi ma b fi mj mg l mh mi">loss_weights = {'label': 1.0,<br/>                'bbox': 1.0}</span></pre><h1 id="7e39" class="km jp hi bd jq kn ko kp ju kq kr ks jy io kt ip kc ir ku is kg iu kv iv kk kw bi translated"><code class="du lx ly lz ma b">BBoxLogger</code> -预测的交互式可视化</h1><p id="6699" class="pw-post-body-paragraph kx ky hi kz b la lb ij lc ld le im lf jz lg lh li kd lj lk ll kh lm ln lo lp hb bi translated">在进行对象定位或对象检测时，您可以在权重和偏差中交互式地显示模型的预测。您可以记录样本图像以及地面真实值和预测边界框值。你甚至可以记录多个框，可以记录信心分数，借据分数等。点击查看文档<a class="ae jn" href="https://docs.wandb.com/library/log#images-and-overlays" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="d1c9" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">Stacey Svetlichnaya 的报告<a class="ae jn" href="https://wandb.ai/stacey/yolo-drive/reports/Bounding-Boxes-for-Object-Detection--Vmlldzo4Nzg4MQ" rel="noopener ugc nofollow" target="_blank">对象检测的边界框</a>将带您了解该工具的交互控件。它涵盖了日志图像和边界框坐标的各种麻烦。</p><p id="65fb" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">下面显示的代码片段是我们的<code class="du lx ly lz ma b">BBoxLogger</code>回调的助手函数。函数<code class="du lx ly lz ma b">wandb_bbox</code>以所需的格式返回图像、预测边界框坐标和地面真实坐标。注意，传递的值有<code class="du lx ly lz ma b">dtype</code>，它是<code class="du lx ly lz ma b">JSON</code>可序列化的。比如你的<code class="du lx ly lz ma b">pred_label</code>应该是<code class="du lx ly lz ma b">float</code>型而不是<code class="du lx ly lz ma b">ndarray.float</code>。</p><pre class="iy iz ja jb fd mb ma mc md aw me bi"><span id="9685" class="jo jp hi ma b fi mf mg l mh mi">def wandb_bbox(image, p_bbox, pred_label, t_bbox, true_label, class_id_to_label):<br/>    return wandb.Image(image, boxes={<br/>        "predictions": {<br/>            "box_data": [{<br/>                "position": {<br/>                    "minX": p_bbox[0],<br/>                    "maxX": p_bbox[2],<br/>                    "minY": p_bbox[1],<br/>                    "maxY": p_bbox[3]<br/>                    },<br/>                "class_id" : pred_label,<br/>                "box_caption": class_id_to_label[pred_label]<br/>            }],<br/>          "class_labels": class_id_to_label<br/>        },<br/>        "ground_truth": {<br/>            "box_data": [{<br/>                "position": {<br/>                    "minX": t_bbox[0],<br/>                    "maxX": t_bbox[2],<br/>                    "minY": t_bbox[1],<br/>                    "maxY": t_bbox[3]<br/>                    },<br/>                "class_id" : true_label,<br/>                "box_caption": class_id_to_label[true_label]<br/>            }],<br/>          "class_labels": class_id_to_label<br/>        }<br/>    })</span></pre><p id="262c" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">我们的<code class="du lx ly lz ma b">BBoxLogger</code>是一个定制的 Keras 回调函数。我们可以将它传递给<code class="du lx ly lz ma b">model.fit</code>来记录我们的模型在一个小型验证集上的预测。权重和偏差将自动覆盖图像上的边界框。</p><pre class="iy iz ja jb fd mb ma mc md aw me bi"><span id="c687" class="jo jp hi ma b fi mf mg l mh mi">class BBoxLogger(tf.keras.callbacks.Callback):<br/>    def __init__(self):<br/>        super(BBoxLogger, self).__init__()<br/>        self.val_images, label_bbox = next(iter(testloader))<br/>        self.true_labels = label_bbox['label']<br/>        self.true_bbox = label_bbox['bbox']<br/><br/>    def on_epoch_end(self, logs, epoch):<br/>        localization_list = []<br/><br/>        for idx in range(len(self.val_images)):<br/>            # get image<br/>            image = self.val_images[idx]<br/>            # get ground truth label and bbox coordinates.<br/>            true_label = int(self.true_labels[idx].numpy())<br/>            t_bbox = self.true_bbox[idx]<br/>            # get model prediction.<br/>            pred_label, p_bbox = model.predict(np.expand_dims(image, 0))<br/>            # get argmax of the prediction<br/>            pred_label = int(np.argmax(pred_label[0])) <br/>            # get wandb image<br/>            localization_list.append(wandb_bbox(image, <br/>                                                p_bbox[0].tolist(),<br/>                                                pred_label, <br/>                                                t_bbox.numpy().tolist(), <br/>                                                true_label, <br/>                                                class_id_to_label))<br/><br/>        wandb.log({"predictions" : localization_list})</span></pre><p id="1b97" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">我们将很快看到结果。</p><h1 id="cebd" class="km jp hi bd jq kn ko kp ju kq kr ks jy io kt ip kc ir ku is kg iu kv iv kk kw bi translated">结果</h1><p id="9a8f" class="pw-post-body-paragraph kx ky hi kz b la lb ij lc ld le im lf jz lg lh li kd lj lk ll kh lm ln lo lp hb bi translated">现在到了激动人心的部分。我用 10 倍的耐心训练了早停模型。随意训练模型更长的时期，并使用其他超参数。在这个 colab 笔记本上尝试实验。</p><div class="mm mn ez fb mo mp"><a href="https://colab.research.google.com/drive/1LWbgjNbQLQHgfY2WH0lKBEFRIZXdqT86?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">谷歌联合实验室</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">编辑描述</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">colab.research.google.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd jh mp"/></div></div></a></div><p id="3907" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">由于我们有多个与我们的任务相关的损失，我们将有多个度量来记录和监控。权重和偏差使用<code class="du lx ly lz ma b">keras.WandbCallback</code>回调自动记录所有指标。</p><p id="080d" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated"><strong class="kz hj">查看此互动报告，了解完整结果。</strong></p><div class="mm mn ez fb mo mp"><a href="https://wandb.ai/wandb/object_localization/reports/Object-Localization-with-Keras-and-W-B--VmlldzoyNzA2Mzk" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">基于 Keras 和 W&amp;B 的目标定位</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">目标定位是在图像中定位特定目标类别的实例的任务，通常通过…</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">wandb.ai</p></div></div><div class="my l"><div class="ne l na nb nc my nd jh mp"/></div></div></a></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nf"><img src="../Images/d76ffbaaef02eb5b932c6900d636adf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VBpxiXW7-VkZ-OAwTsGKPQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jq">图 4: </strong>训练和验证损失。(<a class="ae jn" href="https://wandb.ai/wandb/object_localization/reports/Object-Localization-with-Keras-and-W-B--VmlldzoyNzA2Mzk" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="0a34" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated"><code class="du lx ly lz ma b">BBoxLogger</code>的结果如下所示。在交互报告中，单击下面媒体面板中的⚙️图标(BBoxLogger 的结果)来检查交互控件。您可以同时或分别可视化基本事实和预测边界框。您甚至可以选择不想可视化的类。</p><h2 id="6d18" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">观察</h2><ul class=""><li id="c278" class="ng nh hi kz b la lb ld le jz ni kd nj kh nk lp nl nm nn no bi translated">该模型准确地对图像进行分类。通过查看上面显示的分类指标，可以进一步确认这一点。对于类似 MNIST 的数据集，期望它具有高精度。</li><li id="6a54" class="ng nh hi kz b la np ld nq jz nr kd ns kh nt lp nl nm nn no bi translated">边界框坐标的预测看起来不错。我们应该等待并欣赏神经网络的力量。只需几行代码，我们就能找到这些数字。</li></ul><h2 id="31eb" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">丰富</h2><p id="2cea" class="pw-post-body-paragraph kx ky hi kz b la lb ij lc ld le im lf jz lg lh li kd lj lk ll kh lm ln lo lp hb bi translated">我们可以改进边界框预测的几件事情是:</p><ul class=""><li id="fac1" class="ng nh hi kz b la lq ld lr jz nu kd nv kh nw lp nl nm nn no bi translated">增加我们模型和训练的回归网络的深度。这可能会导致过度合身，但值得一试。</li><li id="5272" class="ng nh hi kz b la np ld nq jz nr kd ns kh nt lp nl nm nn no bi translated">训练当前模型。冻结卷积层和分类网络，并为更多的时期训练回归网络。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nx"><img src="../Images/de3d2cd3306c45617da133f6bdc1c375.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3d-X5HkuPRfSOFz2BJRAsA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jq">图 5:</strong>bbox logger 的结果。(<a class="ae jn" href="https://wandb.ai/wandb/object_localization/reports/Object-Localization-with-Keras-and-W-B--VmlldzoyNzA2Mzk" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="53c1" class="km jp hi bd jq kn ko kp ju kq kr ks jy io kt ip kc ir ku is kg iu kv iv kk kw bi translated">结论</h1><p id="006f" class="pw-post-body-paragraph kx ky hi kz b la lb ij lc ld le im lf jz lg lh li kd lj lk ll kh lm ln lo lp hb bi translated">我希望你喜欢这篇简短的教程，它讲述了如何使用 Keras 构建一个对象定位架构，并使用交互式包围盒可视化工具来调试包围盒预测。</p><p id="1d5b" class="pw-post-body-paragraph kx ky hi kz b la lq ij lc ld lr im lf jz ls lh li kd lt lk ll kh lu ln lo lp hb bi translated">你可以在这里找到更多我的作品<a class="ae jn" href="https://wandb.ai/ayush-thakur/" rel="noopener ugc nofollow" target="_blank"/>。点击查看互动报道<a class="ae jn" href="https://wandb.ai/wandb/object_localization/reports/Object-Localization-with-Keras-and-W-B--VmlldzoyNzA2Mzk" rel="noopener ugc nofollow" target="_blank">。会喜欢你的反馈。:D</a></p></div></div>    
</body>
</html>