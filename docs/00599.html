<html>
<head>
<title>Understanding Proximal Policy Optimization (PPO) and its implementation on Mario Game Environment</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解最近策略优化及其在马里奥游戏环境中的实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-proximal-policy-optimization-ppo-and-its-implementation-on-mario-game-environment-31ab4ee024ab?source=collection_archive---------1-----------------------#2019-08-12">https://medium.com/analytics-vidhya/understanding-proximal-policy-optimization-ppo-and-its-implementation-on-mario-game-environment-31ab4ee024ab?source=collection_archive---------1-----------------------#2019-08-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b3f4" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">解释了最近策略优化背后的概念和思想及其在Mario gym环境中的实现</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/06715d37e3861f050c1fb6698c200efb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-f2c2xAruwwsRL1WxbesoA.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiKubXs0vLjAhUIM48KHS2ODrYQjhx6BAgBEAI&amp;url=https%3A%2F%2Fwww.nintendo.co.uk%2FGames%2FNES%2FSuper-Mario-Bros--803853.html&amp;psig=AOvVaw0zlpb3XKUaYk5fsaURCd9R&amp;ust=1565332148386846" rel="noopener ugc nofollow" target="_blank">让人工智能准备好玩马里奥</a></figcaption></figure><p id="bcb4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">强化学习基本上分为两类，即策略梯度和价值函数，它们各有利弊。在本帖中，我们将讨论最先进的策略优化技术，即PPO或近似策略优化。</p><p id="de76" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">PPO上OpenAI的一句话:</p><blockquote class="kk kl km"><p id="187e" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated">近似策略优化(PPO ),其性能与最先进的方法相当或更好，但实施和调整更简单。</p></blockquote><p id="b26d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在深入PPO的细节之前，我们需要了解一些事情，其中之一是代理函数的概念，它将帮助我们理解使用PPO背后的动机。</p><blockquote class="kk kl km"><p id="fdfc" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated"><strong class="jq hj">代理功能:</strong></p></blockquote><p id="7474" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">代理函数可以被定义为梯度的近似形式，它更像一个新对象的梯度。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kr"><img src="../Images/a2cbf9f468e9c9fa4727be26fc7084ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*EaoK1q3CHSuv8QQxXEqdpw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">梯度和代理函数</figcaption></figure><p id="7317" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们使用这种创新的梯度，以便我们可以执行梯度上升来更新我们的策略，这可以被认为是直接最大化代理函数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ks"><img src="../Images/1baabc1c70ce436e7976c7fa312449c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pJbEtLftES-kgKeonxUYQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">代理函数有助于实现最优策略(图片来自Udacity深度强化学习nanodegree)</figcaption></figure><p id="9376" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">但是使用代理函数仍然给我们留下了一个问题，如果我们继续重用过去的轨迹，同时继续更新我们的策略，我们将会看到在某一点上新策略可能会偏离旧策略足够远，以至于我们以前用代理函数进行的所有近似都可能变得无效。这就是使用PPO的真正优势所在。</p><blockquote class="kk kl km"><p id="35a6" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated"><strong class="jq hj">裁剪代理函数:</strong></p></blockquote><p id="5a7d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这就是三个字:</p><ol class=""><li id="852e" class="kt ku hi jq b jr js ju jv jx kv kb kw kf kx kj ky kz la lb bi translated">策略(要实现的最佳策略)</li><li id="10c9" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">近端(代理函数的剪辑)</li><li id="9b3f" class="kt ku hi jq b jr lc ju ld jx le kb lf kf lg kj ky kz la lb bi translated">优化(代理函数的使用)和它们的实际意义浮现出来，这导致了算法的命名。</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lh"><img src="../Images/42dfc378644a870afb262898e2574c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PL8c7SyAZSageoqAQt0Nmg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">代理函数的剪辑(图片来自Udacity深度强化学习nanodegree)</figcaption></figure><p id="85fb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对替代函数的剪裁使其变平，从而使收敛到最优策略变得更容易和方便。在这种剪裁下，当我们开始对我们的当前策略应用梯度上升时，更新保持与在正常代理函数中发生的相同，但是当我们到达平台时更新停止。迅速地，因为回报函数是平坦的并且梯度是零，这直接暗示策略更新将停止并且我们的最优策略将被实现。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es li"><img src="../Images/4c3f1316d34392918fe9c0584f105f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*G53Q7Y-5cyGnMdeVNUmzPA.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">思想等于爆炸。</figcaption></figure><p id="b022" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在了解到如此复杂的强化学习算法可以如此容易地理解之后，mind肯定等于炸了。</p><p id="a0d0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们还在Mario环境中实现了代码，所以请保持稳定并集中精力。</p><h1 id="edcd" class="lj lk hi bd ll lm ln lo lp lq lr ls lt io lu ip lv ir lw is lx iu ly iv lz ma bi translated">安装和运行Mario环境</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mb"><img src="../Images/30d12d77456677be70862e020b7cc820.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/1*TInAEXIC1R5H0bsx-p0ZSQ.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://pypi.org/project/gym-super-mario-bros/" rel="noopener ugc nofollow" target="_blank">我们出发吧</a></figcaption></figure><p id="1969" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">以下命令将帮助您安装超级马里奥兄弟环境-</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="f4c6" class="mh lk hi md b fi mi mj l mk ml">pip install gym-super-mario-bros</span></pre><p id="b544" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这个代码片段将帮助您渲染env，并让您使用它，习惯动作和状态空间-</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="8a4f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">有关环境的更多详情，请参考<a class="ae jn" href="https://pypi.org/project/gym-super-mario-bros/" rel="noopener ugc nofollow" target="_blank">本</a>。</p><h1 id="cb2d" class="lj lk hi bd ll lm ln lo lp lq lr ls lt io lu ip lv ir lw is lx iu ly iv lz ma bi translated">超级马里奥兄弟的编码PPO</h1><p id="03fc" class="pw-post-body-paragraph jo jp hi jq b jr mo ij jt ju mp im jw jx mq jz ka kb mr kd ke kf ms kh ki kj hb bi translated">为了方便起见，我们将使用OpenAI给出的基线，因为他们有大量的RL算法，并不断更新他们的GitHub库。</p><blockquote class="kk kl km"><p id="d18f" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated">pip用法说明Python 2使用pip，Python3使用pip3</p></blockquote><p id="7a09" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们将首先下载所需的软件包，然后下载RL代码的<a class="ae jn" href="https://github.com/openai/baselines" rel="noopener ugc nofollow" target="_blank">基线</a>库-</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="f80c" class="mh lk hi md b fi mi mj l mk ml">sudo apt-get install <strong class="md hj">zlib1g-dev libopenmpi-dev ffmpeg</strong><br/>sudo apt-get update<br/><br/>pip3 install <strong class="md hj">opencv-python cmake anyrl gym-retro joblib atari-py</strong> </span><span id="50a8" class="mh lk hi md b fi mt mj l mk ml">git clone <strong class="md hj">https://github.com/openai/baselines.git</strong></span></pre><p id="671b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">安装Tensorflow(根据您的要求安装CPU或GPU)</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="e81d" class="mh lk hi md b fi mi mj l mk ml">pip install tensorflow-gpu # for GPU</span><span id="b578" class="mh lk hi md b fi mt mj l mk ml">pip install tensorflow # for CPU</span></pre><p id="7a49" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，安装基线包-</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="20d9" class="mh lk hi md b fi mi mj l mk ml">cd <strong class="md hj">baselines</strong></span><span id="74d3" class="mh lk hi md b fi mt mj l mk ml">pip3 <strong class="md hj">install -e .</strong></span></pre><p id="f793" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用基线中给出的RL代码的语法代码通常是这样的</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="fe74" class="mh lk hi md b fi mi mj l mk ml">python -m baselines.run --alg=<strong class="md hj">&lt;name of the algorithm&gt;</strong> --env=<strong class="md hj">&lt;environment_id&gt;</strong> [additional arguments]</span></pre><p id="1d0a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">例如，如果我们想用PPO2训练一个控制MuJoCo人形机器人的全连接网络，时间步长为20M，我们应该这样写</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="1c88" class="mh lk hi md b fi mi mj l mk ml">python -m baselines.run --alg=<strong class="md hj">ppo2</strong> --env=<strong class="md hj">Humanoid-v2</strong> --network=<strong class="md hj">mlp</strong> --num_timesteps=<strong class="md hj">2e7</strong></span></pre><p id="a6e0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在此之后，确保你的健身房复古和雅达利-py已经成功。有关这些的更多信息，请参考<a class="ae jn" href="https://openai.com/blog/gym-retro/" rel="noopener ugc nofollow" target="_blank">复古</a>和<a class="ae jn" href="https://github.com/openai/atari-py" rel="noopener ugc nofollow" target="_blank">雅达利</a>。</p><blockquote class="kk kl km"><p id="d60c" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated"><strong class="jq hj">注意——要在健身房环境中使用基线代码直接运行马里奥，请这样做:</strong></p><p id="e167" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated">为了导入ROMS，你需要从<a class="ae jn" href="http://www.atarimania.com/rom_collection_archive_atari_2600_roms.html" rel="noopener ugc nofollow" target="_blank"> Atari 2600 VCS ROM集合</a>中下载<code class="du mu mv mw md b">Roms.rar</code>并解压<code class="du mu mv mw md b">.rar</code>文件。完成后，运行:</p><p id="273c" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated"><code class="du mu mv mw md b">python -m atari_py.import_roms &lt;path to folder&gt;</code></p><p id="1be9" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated">这应该会在导入rom时打印出它们的名称。rom将被复制到您的<code class="du mu mv mw md b">atari_py</code>安装目录。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es li"><img src="../Images/ff291c42f6a6c7fd4f11cf3fdc9b94bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*vvsGP034rFPEG2383wJwlA.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">当您几乎完成安装时，突然出现了一些错误。</figcaption></figure><p id="2a1e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，我们使用以下命令开始训练-</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="c139" class="mh lk hi md b fi mi mj l mk ml"><strong class="md hj">python3 -m baselines.run --alg=ppo2 --env=SuperMarioBros-Nes <br/>--gamestate=Level3-1.state --num_timesteps=1e7</strong></span></pre><p id="1d5d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了在训练期间保存模型，在末尾添加以下参数，并且在训练之后加载模型也是如此-</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="b65d" class="mh lk hi md b fi mi mj l mk ml"><strong class="md hj">--save_path=./PATH_TO_MODEL</strong></span><span id="3d2c" class="mh lk hi md b fi mt mj l mk ml"><strong class="md hj">--load_path=./PATH_TO_MODEL</strong></span></pre><p id="3e24" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">好了，你可以开始训练你的马里奥去营救公主了。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mx"><img src="../Images/16ea6e1ff5f2fc99b5527f812f49037f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*x9KbECW50GFSie5B76ruOQ.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">完成代码的庆祝活动！！祝贺</figcaption></figure></div></div>    
</body>
</html>