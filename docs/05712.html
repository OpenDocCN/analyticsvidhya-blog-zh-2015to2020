<html>
<head>
<title>Apache Airflow in a Digital bank Production</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数字银行生产中的Apache气流</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/apache-airflow-in-a-digital-bank-production-8003fb66e6ca?source=collection_archive---------7-----------------------#2020-04-30">https://medium.com/analytics-vidhya/apache-airflow-in-a-digital-bank-production-8003fb66e6ca?source=collection_archive---------7-----------------------#2020-04-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="27d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">生产背景:</strong>我们在生产中有数百条数据管道(主要是Apache spark ),用于将原始数据增量接收到基于更新的数据仓库中，这些管道还应定期运行(每小时或每天),并应满足SLA要求，以便向下游团队提供数据。输入的原始数据将由上游团队在预定时间后的任何时间放置在数据湖的登陆位置。Apache Airflow是一个开源的工作流管理平台，可以方便地调度、监控和重新运行生产中失败的任务。</p><p id="47e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache Airflow是基于python构建的，可以很容易地扩展它来满足我们的需求。简而言之，为了执行我们的管道，我们需要通过编写模板化的python代码来创建dag，并将其放在“dag包”(主节点上的＄AIRFLOW _ HOME/DAGs目录)中，之后airflow调度程序将自动拾取该DAG，解析它并将所有任务发送到任务队列中，以便在air flow工作节点上独立执行。Airflow web服务器将从元数据数据库中获取所有DAG元数据信息，并在WebUI中显示它们。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/5cd152027767ba99d98b49a1ef47c74a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KZI_DBSO6XNS_DNMbWatww.png"/></div></div></figure><p id="30f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是气流DAG的构建模块:</p><ol class=""><li id="e889" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">导入python包</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jy"><img src="../Images/60f5004fc9bb3dea43c8bc0f8a34d8f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y2gERpLXi99w4lQS5kHvXg.png"/></div></div></figure><p id="5066" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.通过传递dag名称、默认参数、开始日期、计划间隔、类似玉米的计划表达式、电子邮件到警报等来创建Dag对象</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jz"><img src="../Images/7a8001fd98596a594bd88cd37cfc6e00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FnExdrSN8GqIWjbpLYIexg.png"/></div></div></figure><p id="ba7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.通过将参数传递给特定的操作员来创建任务(即操作员实例)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ka"><img src="../Images/6a633c1e2ca5a51c1a1b361742a7161a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m1zkAIgIAk-K6KPDSoy0Jg.png"/></div></div></figure><p id="30fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.使用bitshift操作符("&lt;&gt; ")或通过python方法<strong class="ih hj"> set_upstream </strong>()或<strong class="ih hj"> set_downstream </strong>())设置任务之间的依赖关系。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kb"><img src="../Images/a6b3ccf5cd2fe2f2665630aa25651ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4w5AauyzA-_KjwxXN_isQ.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">生产环境中的气流架构示例(设计遵循组织的文化和标准)。</figcaption></figure><p id="9156" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">部署策略</strong>:在创建了Airflow DAG之后，开发人员将在版本控制存储库中维护DAG以及数据管道的源代码(在本例中为Bitbucket)。CI/CD管道将使用Jenkins从repo中提取和构建源代码，并将所有airflow worker节点中的可执行jar及其依赖项和DAG文件部署到所有Airflow master节点的“dagbag”位置(通常为$AIRFLOW_HOME/dags/)。</p><p id="6a43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">传感器的利用:</strong>如管线背景所述，上游团队将在预定时间后的任何时间将待处理的<strong class="ih hj">原始数据</strong>复制到登陆地点(此处为S3)。在这里，我们使用S3Sensor和S3PrefixSensors操作符创建了传感器任务，以持续地探测S3位置，只要输入位置中存在具有匹配命名格式的文件，传感器就会返回布尔值“true ”,然后DAG中的实际Spark任务就会启动。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kg"><img src="../Images/8ea5f2e27af20221f097fedfd2d33de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ukOkTn52wwCfzpXchjs0qA.png"/></div></div></figure><p id="b2ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Worker node setup </strong>:由于我们知道同一个DAG的所有任务可能会在不同的Worker node上独立执行，所以任务会在可用的节点上随机执行。为了正确地完成这项工作，我们应该用相同的配置、元数据和目录结构维护所有工作节点。</p><p id="64ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，task“T2”是BashOperator的一个实例，通过调用一个Bash脚本并使用下面的<code class="du kh ki kj kk b">bash_command</code>参数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kl"><img src="../Images/887089370d39dc6e034765dfdf4ae746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BH7soZU4q0i55u7Kmsb6qQ.png"/></div></div></figure><p id="5692" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">任务“t2”可以在任意一个工作者节点上随机执行。因此，脚本文件“test.sh”应该可以在同一位置的每个worker节点上使用(“/home/batcher/test.sh”)，并且具有读取权限。</p><p id="b087" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如前所述，气流目标是在工作节点上独立运行任务，因此不可能在任务之间共享数据/信息。然而，如果任务需要从其他任务获取一些元数据或信息，为了满足这种需求，airflow将拥有Xcoms来从一个任务向另一个任务共享信息。其中源任务是通过调用<code class="du kh ki kj kk b">xcom_push()</code>方法随时将信息推送到XComs。任务将调用<code class="du kh ki kj kk b">xcom_pull()</code>来检索XComs。</p><p id="aaf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">气流工作者和火花限制</strong>:要在任何工作者节点上执行任何特定的任务，需要呈现该操作者的所有依赖关系。由于大多数管道都是基于spark的，所以在spark网关节点上配置了气流工作节点，通过执行气流任务在spark集群上启动spark作业。但是这里的限制是spark作业应该处于客户端模式，这使得airflow能够获得spark日志和作业状态。如果集群模式强制启动spark作业，则气流将无法获取spark日志和作业状态。</p><p id="f207" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Airflow CLI: </strong> Airflow具有非常丰富的命令行界面，允许在DAG上进行多种类型的操作、启动服务以及支持开发和测试。</p><p id="af2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，我们可以通过命令独立测试DAG中的每个任务</p><pre class="je jf jg jh fd km kk kn ko aw kp bi"><span id="b094" class="kq kr hi kk b fi ks kt l ku kv">airflow test dag_id task_id execution_date<br/>airflow run dag_id task_id execution_date</span></pre><p id="86fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">回填:</strong> Airflow将在回填日期范围内自动重新运行先前失败的任务实例。</p><pre class="je jf jg jh fd km kk kn ko aw kp bi"><span id="f980" class="kq kr hi kk b fi ks kt l ku kv">airflow backfill [-h] [-t TASK_REGEX] [-s START_DATE] [-e END_DATE] [-m] dag_id</span></pre><p id="69af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">远程日志和指标可视化:</strong>用户可以使用<code class="du kh ki kj kk b">base_log_folder</code>设置在<code class="du kh ki kj kk b">airflow.cfg</code>中指定一个日志文件夹。默认情况下，它位于<code class="du kh ki kj kk b">AIRFLOW_HOME</code>目录中。</p><p id="4b38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，用户可以在云存储中提供一个远程位置来存储日志和日志备份。要启用此功能，<code class="du kh ki kj kk b">airflow.cfg</code>必须配置有“<strong class="ih hj"> remote_logging = True </strong>”，然后Airflow可以在AWS S3、谷歌云存储或弹性搜索中远程存储日志。</p><p id="c7d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Elastic Search中，如果我们可以将id为“{ { Dag _ id } }-{ { task _ id } }-{ { execution _ date } }-{ { try _ number } }”并且json_fields = asctime，filename，lineno，levelname，message的日志存储起来，那么我们就可以通过在Elastic Search上查询来可视化Kiband或Grafana上的完整气流指标。</p><p id="4af9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">挑战:</strong>在Airflow平台中最大的挑战是维护调度程序和web服务器守护进程。如果webserver守护进程被终止或崩溃，那么用户将无法使用WebUI，但这不会影响任何任务执行计划。我们可以通过用多个web服务器守护进程维护多个主服务器来管理这个问题。要解决这个问题，也可以重启airflow webserver。</p><p id="799e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是万一调度程序被终止或崩溃，它将停止调度任务，并在集群中造成混乱。需要注意的是，我们不能在同一个集群中维护多个调度器，如果我们这样做，将会发生任务重复。当当前运行的Dag数量很大时，调度程序的机会更多，因为它无法查询airflow元数据数据库(在我们的例子中是MariaDB)。为了缓解这个问题，我们使用了一个数据库代理(在我们的例子中是MaxScale)来处理来自调度器的查询。该代理将对请求进行负载平衡，从而向气流元数据数据库的主节点发送写查询，向工作节点发送读查询。</p><p id="18d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如有任何疑问，请通过Linkedin联系我:【www.linkedin.com/in/anilpalwai T2】</p></div></div>    
</body>
</html>