<html>
<head>
<title>Understanding Decision Tree!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解决策树！！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-decision-tree-3591922690a6?source=collection_archive---------1-----------------------#2020-09-06">https://medium.com/analytics-vidhya/understanding-decision-tree-3591922690a6?source=collection_archive---------1-----------------------#2020-09-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/9455e347e31d0aa924e38a1fcb3877fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OBxVVAkN3sthF26oOMTA8g.png"/></div></div></figure><h1 id="9457" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是决策树？</h1><ul class=""><li id="361a" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="jq hj">决策树</strong>是一种决策支持工具，它使用决策及其可能结果的树状模型，包括偶然事件结果、资源成本和效用。</li><li id="c88c" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated"><strong class="jq hj">决策树学习</strong>是统计学和机器学习中使用的预测建模方法之一。它使用决策树从对某个项目的观察(用树枝表示)到对该项目的目标值的结论(用树叶表示)。</li><li id="5c5b" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">决策树是一种用于分类和回归任务的非参数监督学习方法。</li><li id="19b4" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">目标是创建一个模型，通过学习从数据特征推断的简单决策规则来预测目标变量的值。</li></ul><h1 id="c646" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">决策树的类型:</h1><h2 id="50b9" class="kl ir hi bd is km kn ko iw kp kq kr ja jv ks kt je jx ku kv ji jz kw kx jm ky bi translated"><strong class="ak">回归树</strong></h2><p id="3e60" class="pw-post-body-paragraph kz la hi jq b jr js lb lc jt ju ld le jv lf lg lh jx li lj lk jz ll lm ln kb hb bi translated">具有连续目标变量的决策树(例如:一套房子的价格)。</p><ul class=""><li id="c493" class="jo jp hi jq b jr lo jt lp jv lq jx lr jz ls kb kc kd ke kf bi translated">为了建立回归树，我们首先使用递归二分分裂，即每个数据样本作为一个节点来分裂数据。</li><li id="168c" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">具有最小残差平方和或均方误差的点被认为是分裂的节点。</li><li id="0a30" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">每个节点在达到限制时停止分裂，这意味着进一步的分裂将具有少于 n 个观察值。</li><li id="5ec3" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">为了对分裂背后有个直观的了解，我强烈推荐这个<a class="ae lt" href="https://www.youtube.com/watch?v=g9c66TUylZ4" rel="noopener ugc nofollow" target="_blank"><strong class="jq hj"><em class="lu">stat quest</em></strong></a>的视频。</li></ul><h2 id="5804" class="kl ir hi bd is km kn ko iw kp kq kr ja jv ks kt je jx ku kv ji jz kw kx jm ky bi translated"><strong class="ak">分类树</strong></h2><p id="ffd7" class="pw-post-body-paragraph kz la hi jq b jr js lb lc jt ju ld le jv lf lg lh jx li lj lk jz ll lm ln kb hb bi translated">具有分类目标变量的决策树。(例如。:在泰坦尼克号数据中，无论乘客是否幸存)。</p><ul class=""><li id="6bf2" class="jo jp hi jq b jr lo jt lp jv lq jx lr jz ls kb kc kd ke kf bi translated">它非常类似于回归树，但是残差平方和不用于分割节点。</li><li id="2469" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">它使用了一些我们将在下面讨论的其他技术。</li></ul><h1 id="ac4b" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">决策树的假设:</h1><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/29989bbf29e1a968702cc3d2d4ac45b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kspChgJ5ddMkeUgv"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">照片由<a class="ae lt" href="https://unsplash.com/@cadop?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马修·施瓦茨</a>在<a class="ae lt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="9df9" class="pw-post-body-paragraph kz la hi jq b jr lo lb lc jt lp ld le jv me lg lh jx mf lj lk jz mg lm ln kb hb bi translated"><strong class="jq hj"> <em class="lu">由于决策树是非统计方法，它不对训练数据或预测残差做出任何假设；例如，没有分布、独立性或恒定方差假设。</em> </strong></p><p id="3c51" class="pw-post-body-paragraph kz la hi jq b jr lo lb lc jt lp ld le jv me lg lh jx mf lj lk jz mg lm ln kb hb bi translated">但是，决策树有一些非统计假设:</p><ul class=""><li id="2d8c" class="jo jp hi jq b jr lo jt lp jv lq jx lr jz ls kb kc kd ke kf bi translated">开始时，整个训练集被认为是<strong class="jq hj">根。</strong></li><li id="a22f" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">特征值最好是分类的。如果这些值是连续的，则在构建模型之前会将其离散化。</li><li id="f39f" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">记录是基于属性值递归分布的。</li><li id="5688" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">通过使用一些统计方法来完成将属性放置为树的根或内部节点的顺序。</li></ul><h1 id="41a1" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">决策树是如何工作的？</h1><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/c044730cbd545491b072329f38b656a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VK9GFnmlSNIkxACa"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">在<a class="ae lt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lt" href="https://unsplash.com/@adeolueletu?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Adeolu Eletu </a>拍摄的照片</figcaption></figure><ul class=""><li id="b53a" class="jo jp hi jq b jr lo jt lp jv lq jx lr jz ls kb kc kd ke kf bi translated">它适用于分类和连续的输入和输出变量。</li><li id="47f9" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">我们根据输入变量中最重要的分割器/区分器将总体或样本分成两个或多个同类集合(或子总体)。</li><li id="d87c" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">如何分割的决定严重影响了树的准确性。<strong class="jq hj"> <em class="lu">分类树和回归树的判定标准不同。</em> </strong></li><li id="4476" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">决策树使用多种算法来决定将一个节点拆分成两个或多个子节点。子节点的创建增加了结果子节点的同质性。换句话说，我们可以说节点的纯度随着目标变量的增加而增加。</li><li id="7e52" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">然而，问题是算法的贪婪本质。决策树分割所有可用变量上的节点，然后选择产生最相似子节点的分割。</li></ul><h1 id="4b57" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">决策树的不同类型算法</h1><h2 id="43da" class="kl ir hi bd is km kn ko iw kp kq kr ja jv ks kt je jx ku kv ji jz kw kx jm ky bi translated">→ ID3</h2><ul class=""><li id="ca2b" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="jq hj"> ID3 </strong> ( <strong class="jq hj">迭代二分法 3 </strong>)是 Ross Quinlan[1]发明的一种算法，用于从数据集生成决策树。</li><li id="cd19" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">ID3 不保证最优解。</li><li id="d323" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">它可以收敛于局部最优。</li><li id="9128" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">它使用贪婪策略，在每次迭代中选择局部最佳属性来分割数据集。</li><li id="5ec0" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">算法的最优性可以通过在搜索最优决策树的过程中使用回溯来提高，代价是可能花费更长的时间。</li><li id="9ad0" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">ID3 可能会过度拟合训练数据。为了避免过度拟合，较小的决策树应该优先于较大的决策树。这种算法通常生成小树，但并不总是生成尽可能最小的决策树。</li><li id="eda3" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">ID3 在连续数据上比在分解数据上更难使用(分解数据有离散数量的可能值，因此减少了可能的分支点)。如果任何给定属性的值都是连续的，那么就有更多的地方来分割该属性上的数据，并且搜索最佳分割值可能会非常耗时。</li><li id="1de3" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">它主要致力于计算熵和信息增益。</li></ul><blockquote class="mi"><p id="da6c" class="mj mk hi bd ml mm mn mo mp mq mr kb dx translated"><a class="ae lt" href="https://en.wikipedia.org/wiki/ID3_algorithm" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/ID3_algorithm</a></p></blockquote><h2 id="1e23" class="kl ir hi bd is km ms ko iw kp mt kr ja jv mu kt je jx mv kv ji jz mw kx jm ky bi translated">→ C4.5 算法</h2><ul class=""><li id="ef6b" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">C4.5 是昆兰早期 ID3 算法的扩展。C4.5 生成的决策树可以用于分类，为此，C4.5 通常被称为统计分类器。</li><li id="2d04" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">C4.5 使用信息熵的概念，以与 ID3 相同的方式从一组训练数据构建决策树。</li><li id="4f99" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">处理连续和离散属性—为了处理连续属性，C4.5 创建了一个阈值，然后将列表分为属性值高于阈值的列表和属性值小于或等于阈值的列表</li></ul><blockquote class="mi"><p id="aa20" class="mj mk hi bd ml mm mn mo mp mq mr kb dx translated"><a class="ae lt" href="https://en.wikipedia.org/wiki/C4.5_algorithm" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/C4.5_algorithm</a></p></blockquote><h2 id="001a" class="kl ir hi bd is km ms ko iw kp mt kr ja jv mu kt je jx mv kv ji jz mw kx jm ky bi translated">→分类和回归树(CART)</h2><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/fd5cbbb35319dedfd6ca3ff07131b6da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*VgSGVcU1Ghi4lXcW1GKk5g.png"/></div></figure><ul class=""><li id="bc81" class="jo jp hi jq b jr lo jt lp jv lq jx lr jz ls kb kc kd ke kf bi translated">分类和回归树(CART)是一种非参数决策树学习技术，根据因变量是分类变量还是数值变量，生成分类树或回归树</li><li id="076f" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">决策树由基于建模数据集中变量的规则集合形成:<br/> 1。<strong class="jq hj"> <em class="lu">选择基于变量值的规则，得到最佳分割，以区分基于因变量的观察值。<br/> 2。一旦选择了一个规则并将一个节点一分为二，相同的过程被应用于每个“子”节点(即，它是一个递归过程)。<br/> 3。当购物车检测到无法继续获利，或符合一些预设的停止规则时，分割停止。(或者，尽可能分割数据，然后在稍后修剪树)。</em> </strong></li><li id="1b24" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">树的每个分支都以一个终端节点结束。每个观察值都属于一个且恰好是一个终端节点，每个终端节点都由一组规则唯一定义。</li><li id="cc13" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">它用于二元分类。</li><li id="8ce5" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">在回归树的情况下，它使用最小平方作为度量来选择特征。</li><li id="0d00" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">预测分析的一个非常流行的方法是随机森林。</li><li id="af3f" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">它们甚至可以帮助检测异常/异常值。在 Neptune 博客 的这篇<a class="ae lt" href="https://bit.ly/3BrZXTt" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">文章中阅读更多关于它们的内容。</strong></a></li></ul><h2 id="b8f5" class="kl ir hi bd is km kn ko iw kp kq kr ja jv ks kt je jx ku kv ji jz kw kx jm ky bi translated">→卡方自动交互检测(CHAID)</h2><ul class=""><li id="1029" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="jq hj">卡方自动交互检测</strong> ( <strong class="jq hj"> CHAID </strong>)是一种决策树技术，基于调整显著性测试。</li><li id="818b" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">CHAID 经常在直接营销的背景下用于选择消费者群体，并预测他们对一些变量的反应如何影响其他变量，尽管其他早期应用是在医学和精神病学研究领域</li><li id="c3f5" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">像其他决策树一样，CHAID 的优点是其输出高度可视化，易于解释。由于默认情况下使用多路分割，因此需要相当大的样本量才能有效工作，因为样本量小的受访者群体很快就会变得太小，无法进行可靠的分析。</li><li id="b16d" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">与多元回归等替代方法相比，CHAID 的一个重要优势是它是非参数化的。</li></ul><h1 id="bdf3" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">决策树的不同分裂标准</h1><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es my"><img src="../Images/a2a467095d353faae5144d15067eec81.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*aSowTg14XWkOWUuTYpgjyA.png"/></div></figure><h2 id="0b8f" class="kl ir hi bd is km kn ko iw kp kq kr ja jv ks kt je jx ku kv ji jz kw kx jm ky bi translated">→熵:</h2><ul class=""><li id="9d96" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">决策树是从根节点自上而下构建的，包括将数据划分为包含具有相似值(同质)的实例的子集。</li><li id="58d2" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated"><strong class="jq hj"> ID3 算法</strong>利用熵来计算样本的同质性。</li><li id="75a6" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">其范围在 0 到 1 之间，其中接近 1 的值表示我们有纯分割，接近 0 的值表示</li></ul><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/4739ac62e6fddfd52931fa38a2925583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oPOuTzo9N-HEDFy-kgROeA.png"/></div></div></figure><h2 id="1040" class="kl ir hi bd is km kn ko iw kp kq kr ja jv ks kt je jx ku kv ji jz kw kx jm ky bi translated">→信息增益:</h2><ul class=""><li id="81f4" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">信息增益基于数据集在属性上分割后熵的减少。</li><li id="e837" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">构建决策树就是要找到返回最高信息增益的属性(即最相似的分支)。</li><li id="eb2c" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">熵是计算信息增益的标准。</li><li id="1f8e" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">信息增益基于在属性上分割数据集后熵的减少。它是用于构建决策树的主要参数。<strong class="jq hj">具有最高信息增益的属性将首先被测试/分割。</strong></li><li id="5586" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated"><strong class="jq hj">信息增益=基础熵—新熵</strong></li></ul><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/d51e712b35c35ac824d9693e5cdbd67d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjYdXUkyTd63yFGnebbJKA.png"/></div></div></figure><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es nb"><img src="../Images/9ca8a5b4825f778c20e082aa3e9e8753.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*ok8w6FDK7HudVpsVR_TF7w.png"/></div></figure><h2 id="e0d4" class="kl ir hi bd is km kn ko iw kp kq kr ja jv ks kt je jx ku kv ji jz kw kx jm ky bi translated">→基尼指数:</h2><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/56d9adab1000cb257dcf505c4ac4795b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eU2jzcZa-_C90gGFMbzqlQ.png"/></div></div></figure><ul class=""><li id="bb37" class="jo jp hi jq b jr lo jt lp jv lq jx lr jz ls kb kc kd ke kf bi translated">基尼指数是一种衡量随机选择的元素被错误识别的频率的指标。这意味着基尼系数越低的属性越好。</li><li id="bff1" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">CART 算法使用的基尼指数</li></ul><figure class="lw lx ly lz fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/9739fe2e5ec484e647ec0e90601eb6e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*l0-4815KqVIW1PE2GxMy1Q.png"/></div></figure><blockquote class="mi"><p id="e4cd" class="mj mk hi bd ml mm mn mo mp mq mr kb dx translated">要了解基尼指数背后的数学原理，请参考此<a class="ae lt" href="https://www.youtube.com/watch?v=7VeUPuFGJHk" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></blockquote><h2 id="0767" class="kl ir hi bd is km ms ko iw kp mt kr ja jv mu kt je jx mv kv ji jz mw kx jm ky bi translated">→差异:</h2><ul class=""><li id="be41" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">当目标变量是连续的时，这是决策树使用的分裂度量。</li><li id="8f45" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">它检查每个拆分的方差，并采用解释较小方差的拆分。</li></ul><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/1e798a51ce2c0bec48e1e59c22f705ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d5wG8JigpAFhtug18kWeng.jpeg"/></div></div></figure><blockquote class="nf ng nh"><p id="8fe7" class="kz la lu jq b jr lo lb lc jt lp ld le ni me lg lh nj mf lj lk nk mg lm ln kb hb bi translated">s 是预分割样本指数。<br/> S_t 是分裂检验为真的样本指数集。S_f 是分裂检验为假的样本指数的集合。</p></blockquote><blockquote class="mi"><p id="a155" class="mj mk hi bd ml mm mn mo mp mq mr kb dx translated">为了更好地理解连续特征的分割，你可以参考这个来自 StatQuest 的<a class="ae lt" href="https://www.youtube.com/watch?v=g9c66TUylZ4&amp;t=4s" rel="noopener ugc nofollow" target="_blank"> youtube 视频。</a></p></blockquote><h1 id="0076" class="iq ir hi bd is it iu iv iw ix iy iz ja jb nl jd je jf nm jh ji jj nn jl jm jn bi translated">决策树的贪婪本性</h1><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es no"><img src="../Images/52c911975d6b3b0e797ba67dde642eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0J8OaJUXkUSrEE6n6q4Eg.jpeg"/></div></div></figure><ul class=""><li id="dd23" class="jo jp hi jq b jr lo jt lp jv lq jx lr jz ls kb kc kd ke kf bi translated">决策树考虑许多特征，并允许基于这些特征的一系列分割。</li><li id="560c" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">值得注意的是，它们是一类非常通用的模型，可以达到很高的精度，但是找到最佳决策树的任务不容易手动执行，但可以通过计算来完成，因此，我们使用所谓的贪婪方法来选择我们的决策树。</li><li id="8619" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">这里要记住的重要一点是，使用这个过程我们不会得到最优的决策树，但是，我们会得到一个很好的决策树来预测我们的目标数据。</li><li id="f3dc" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">另一件要注意的事情是，我们实际上可以拆分数据，这样任何对象都可以被完全拆分，并且我们可以通过正确的预测获得叶节点中的每个观察值。</li><li id="8f8c" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">这将给我们的训练数据集带来 100%的准确性，但这被称为<a class="ae lt" rel="noopener" href="/analytics-vidhya/over-fitted-and-under-fitted-models-f5c96e9ac581"><strong class="jq hj"/></a>过拟合，我们总是希望避免这样做。</li><li id="31f8" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">这是因为这样的模型不是一个一般化的模型，它不能很好地预测新的和看不见的例子。</li></ul><h1 id="2f78" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">优点和缺点:</h1><h2 id="b041" class="kl ir hi bd is km kn ko iw kp kq kr ja jv ks kt je jx ku kv ji jz kw kx jm ky bi translated">优势:</h2><ol class=""><li id="8a34" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb np kd ke kf bi translated">易于使用和理解。</li><li id="0855" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb np kd ke kf bi translated">可以处理分类数据和数字数据。</li><li id="0777" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb np kd ke kf bi translated">抵抗异常值，因此需要很少的数据预处理。</li><li id="4cd3" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb np kd ke kf bi translated">可以轻松添加新功能。</li><li id="938f" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb np kd ke kf bi translated">可用于通过使用集成方法来构建更大的分类器。</li></ol><h2 id="5fa2" class="kl ir hi bd is km kn ko iw kp kq kr ja jv ks kt je jx ku kv ji jz kw kx jm ky bi translated">缺点:</h2><ol class=""><li id="f279" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb np kd ke kf bi translated">容易过度拟合。</li><li id="26c0" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb np kd ke kf bi translated">需要对他们的表现进行某种衡量。</li><li id="4ef5" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb np kd ke kf bi translated">需要小心调整参数。</li><li id="65fe" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb np kd ke kf bi translated">如果某些职业占优势，可能会创建有偏见的学习树。</li></ol><h1 id="0794" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">快乐学习！！！！！</strong></h1></div><div class="ab cl nq nr gp ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="hb hc hd he hf"><p id="00f7" class="pw-post-body-paragraph kz la hi jq b jr lo lb lc jt lp ld le jv me lg lh jx mf lj lk jz mg lm ln kb hb bi translated">喜欢我的文章？请为我鼓掌并分享它，因为这将增强我的信心。此外，我每周日都会发布新文章，所以请保持联系，以了解数据科学和机器学习基础系列的未来文章。</p><p id="c59d" class="pw-post-body-paragraph kz la hi jq b jr lo lb lc jt lp ld le jv me lg lh jx mf lj lk jz mg lm ln kb hb bi translated">另外，请务必在 LinkedIn 上与我联系。</p><figure class="lw lx ly lz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nx"><img src="../Images/a9b6b3687df8cf3b00b95d3311cf89b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*39N3NVBfmsx5MgUW"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">马库斯·斯皮斯克在<a class="ae lt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div></div>    
</body>
</html>