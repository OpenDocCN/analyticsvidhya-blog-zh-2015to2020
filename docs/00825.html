<html>
<head>
<title>Scraping Kijiji Home Rental Advertisements using Beautiful Soup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">刮木吉吉房屋出租广告使用美丽的汤</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/scraping-kijiji-home-rental-advertisements-using-beautiful-soup-5e286af9d96?source=collection_archive---------5-----------------------#2019-09-11">https://medium.com/analytics-vidhya/scraping-kijiji-home-rental-advertisements-using-beautiful-soup-5e286af9d96?source=collection_archive---------5-----------------------#2019-09-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0da7c11b6d49afcb1f360558ed69182a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KUIfu-eMzXec45TKAnZI8Q.jpeg"/></div></div></figure><p id="83ec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我想开发一个机器学习模型，可以预测加拿大安大略省的房屋租赁价格。但遗憾的是，我没有找到任何数据集来训练我的模型。所以我决定从Kijiji收集租赁清单，以下是我的做法。</p><p id="2750" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">什么是Kijiji？</strong></p><blockquote class="jo jp jq"><p id="d1dc" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">Kijiji是一种在线分类广告服务，作为在线社区的集中网络运营，按城市和城市区域组织，用于发布本地广告。它是易贝的全资子公司，成立于2005年3月。</p></blockquote><p id="9aff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我会一步一步地列出所有的过程。</p><p id="cd45" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第一步:开始</strong></p><p id="d7c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一步是确定向服务器发出什么请求来获取数据。通过查看开发人员工具中的网络选项卡，我们可以很容易地做到这一点。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jv"><img src="../Images/5e66e7fd365e77326ffcda97bd77f36a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wlTssm0T-s2yJ7lSNnC-kA.png"/></div></div></figure><p id="7138" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我注意到的第一件事是，安大略省所有房屋/公寓出租的列表都有相同的URL，中间没有页码。</p><p id="d90c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在每一页上，大约有10-20个广告，如下所示。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ka"><img src="../Images/c4b2e68fc8d82bc6336c51d0438931b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eUQzFj6-oVL1L5xRw2AoHg.png"/></div></div></figure><p id="1917" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我决定抓取几个页面，只是为了获得每个列表的URL以供进一步处理，并将这些URL存储到一个数组中，并且我还将这些抓取的URL保存到一个文本文件中，因为由于频繁访问服务器，我面临着来自服务器的连接拒绝。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kb"><img src="../Images/64624287e4cd02187d6f4076e3f32764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zwdIgW2_c7JhI-XS5sEVUg.png"/></div></div></figure><p id="ee3d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每个广告列表都有一个URL，它实际上位于类别<strong class="is hj"> <em class="jr">标题</em> </strong>的div标签下，如上图所示。</p><p id="885f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第二步:现在开始编码</strong></p><p id="817e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们导入所需的库</p><pre class="jw jx jy jz fd kc kd ke kf aw kg bi"><span id="5d1b" class="kh ki hi kd b fi kj kk l kl km">import requests<br/>import urllib.request<br/>import time<br/>from bs4 import BeautifulSoup<br/>import re<br/>import pandas as pd<br/>import numpy<br/>import time<br/>from os import path</span></pre><p id="c6fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为我只关注安大略的列表，所以让我们硬编码一些URL</p><pre class="jw jx jy jz fd kc kd ke kf aw kg bi"><span id="c648" class="kh ki hi kd b fi kj kk l kl km">url = 'https://www.kijiji.ca/b-for-rent/ontario'<br/>baseurl = 'https://www.kijiji.ca'<br/>baseForOntario = '/c30349001l9004'<br/>pageNos = '/page-'</span><span id="3bcc" class="kh ki hi kd b fi kn kk l kl km"># A sample URL would look like <br/># <a class="ae ko" href="https://www.kijiji.ca/b-for-rent/ontario/page-8/c30349001l9004" rel="noopener ugc nofollow" target="_blank">https://www.kijiji.ca/b-for-rent/ontario/page-8/c30349001l9004</a></span></pre><p id="5fd7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们创建一个只抓取URL的函数，因为我想要的数据实际上在广告中。所以需要先获取广告网址。</p><p id="d302" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">步骤2.1:获取广告网址</strong></p><pre class="jw jx jy jz fd kc kd ke kf aw kg bi"><span id="6fbe" class="kh ki hi kd b fi kj kk l kl km"># I saved those links into a textFile to avoid duplicated / to <br/># reduce number of hits on the server</span><span id="1b00" class="kh ki hi kd b fi kn kk l kl km"># if file exists read the Advt URLs from text file and <br/># perform the scraping</span><span id="c0eb" class="kh ki hi kd b fi kn kk l kl km">if path.exists('links.txt'):<br/>   with open('links.txt', 'r') as f:<br/>        linksFromText = f.readlines()<br/>   f.close()<br/>   # A fucntion call to scrape the actual data<br/>   getDetails(linksFromText)</span><span id="4706" class="kh ki hi kd b fi kn kk l kl km"># if file does not exists perform the scraping</span><span id="3828" class="kh ki hi kd b fi kn kk l kl km">else:<br/>    for i in range(noPages):<br/>        url_final = url+pageNos+str(i)+baseForOntario<br/>        response = requests.get(url_final)<br/>        soup = BeautifulSoup(response.text, "html.parser")<br/>        advtTitles = soup.findAll('div', attrs={'class' : 'title'})<br/>        try:<br/>           for link in advtTitles:<br/>               adlink = baseurl+link.find('a')['href']<br/>               adurl.append(adlink)<br/>        except Exception as e:<br/>              print(e)<br/>        time.sleep(1)<br/>    # a fuction to write the <br/>   saveLinks(adurl)<br/>    # a fuction to do the actual scraping<br/>    getDetails(adurl)</span></pre><p id="abef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦完成，我们将有一堆列表如下。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/5381fc024d3434e2cffc255980a750ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n9flHwfpLbjHsYODEaoXFg.png"/></div></div></figure><p id="3541" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">观察网址后，我发现有两种类型的广告。</p><ol class=""><li id="999b" class="kq kr hi is b it iu ix iy jb ks jf kt jj ku jn kv kw kx ky bi translated">正常的</li><li id="9098" class="kq kr hi is b it kz ix la jb lb jf lc jj ld jn kv kw kx ky bi translated">公寓列表</li></ol><p id="c44b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">主要区别在于这两种广告具有不同的属性。在收集实际数据之前，我们必须考虑它。</p><p id="5d18" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">步骤2.2:获取实际数据</strong></p><p id="f114" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一个普通的清单应该是这样的。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/084530b7b976bc631059e2571619bbb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TUccfzM58eP8TpIRLWSbhg.png"/></div></div></figure><p id="0e8c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">公寓列表如下图所示。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/23ac255423c890019161e03dbc3bf472.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MYhu0wxMtd7KMkwtxMuSmA.png"/></div></div></figure><p id="a69c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让代码从站点中抓取所需的数据</p><p id="8417" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我更感兴趣的数据是，</p><ol class=""><li id="2d78" class="kq kr hi is b it iu ix iy jb ks jf kt jj ku jn kv kw kx ky bi translated">酒店(带家具或不带家具/宠物友好型等)</li><li id="e8ae" class="kq kr hi is b it kz ix la jb lb jf lc jj ld jn kv kw kx ky bi translated">发布日期</li><li id="ef2d" class="kq kr hi is b it kz ix la jb lb jf lc jj ld jn kv kw kx ky bi translated">地址</li><li id="e2da" class="kq kr hi is b it kz ix la jb lb jf lc jj ld jn kv kw kx ky bi translated">租金价格</li><li id="ba25" class="kq kr hi is b it kz ix la jb lb jf lc jj ld jn kv kw kx ky bi translated">描述</li><li id="9485" class="kq kr hi is b it kz ix la jb lb jf lc jj ld jn kv kw kx ky bi translated">标题</li></ol><p id="9858" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的代码将抓取每个广告列表的主要特征。</p><pre class="jw jx jy jz fd kc kd ke kf aw kg bi"><span id="a405" class="kh ki hi kd b fi kj kk l kl km"># Lets pass our array into a function to do the task</span><span id="bc9d" class="kh ki hi kd b fi kn kk l kl km">for url in urls:<br/>    listDetails = ""<br/>    url = url.rstrip('\n')<br/>    response = requests.get(url)<br/>    soup = BeautifulSoup(response.text, "html.parser")<br/>    try:<br/>       adTitle = soup.select_one("h1[class*=title-2323565163]").text<br/>       title.append(adTitle)<br/>       adPrice = soup.select_one("span[class*=currentPrice-2842943473]").text<br/>       prices.append(adPrice)<br/>       adDescription = soup.find_all('div', attrs={'class' : 'descriptionContainer-3544745383'})<br/>       description.append(adDescription)<br/>       adLocation = soup.find('span', attrs={'class' : 'address-3617944557'})<br/>       location.append(adLocation)<br/>       date = soup.find('time')<br/>       datePosted.append(date)</span></pre><p id="cc61" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们检查URL是否包含<em class="jr">‘v-apartments-condots’</em>，基于此，让我们抓取像卧室数量等特征</p><pre class="jw jx jy jz fd kc kd ke kf aw kg bi"><span id="b30a" class="kh ki hi kd b fi kj kk l kl km">apartment = 'v-apartments-condos'</span><span id="c36e" class="kh ki hi kd b fi kn kk l kl km">#if an apartment listing then</span><span id="fe5e" class="kh ki hi kd b fi kn kk l kl km">if apartment in url:<br/>   adfts = soup.find_all('li', attrs={'class' : 'realEstateAttribute-3347692688'})<br/>   for ft in adfts:<br/>       dd = ft.find_all('div')<br/>       listDetails = listDetails + str(dd) + " || "<br/>       features.append(listDetails)<br/>       listingType.append(apartment)<br/>       urlToSave.append(url)<br/>       adid = getAdId(url)<br/>       adId.append(adid)<br/>else:</span><span id="585b" class="kh ki hi kd b fi kn kk l kl km"># if not an apartment listing then<br/>     adfts = soup.find_all('dl', attrs={'class' : 'itemAttribute-983037059'})<br/>     for ft in adfts:<br/>         dd = ft.find('dd').text<br/>         dt = ft.find('dt').text<br/>         listDetails = listDetails + str(dt) + " : " + str(dd) + " || "<br/>         features.append(listDetails)<br/>         listingType.append(roomRent)<br/>         urlToSave.append(url)<br/>         adid = getAdId(url)<br/>         adId.append(adid)</span></pre><p id="f6cd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我面临的两个主要挑战是，</p><ol class=""><li id="9744" class="kq kr hi is b it iu ix iy jb ks jf kt jj ku jn kv kw kx ky bi translated">内存问题</li><li id="36c1" class="kq kr hi is b it kz ix la jb lb jf lc jj ld jn kv kw kx ky bi translated">状态代码500</li></ol><p id="fc2a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">内存问题的一个快速解决方案是，我决定在一段时间后将收集到的数据保存到CSV文件中，并清除所有阵列以释放内存。我一次就刮掉了大约8000个广告。: :)</p><p id="445d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于第二个问题，我必须通过调用time.sleep()来设置每个请求之间的延迟</p><p id="e657" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我的GitHub上有完整的代码:-<a class="ae ko" href="https://github.com/amald94/kijiji-scraper/tree/master" rel="noopener ugc nofollow" target="_blank">https://github.com/amald94/kijiji-scraper/tree/master</a></p><p id="d3e0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">信用</strong></p><p id="a4be" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">digitalrealty:我相信多伦多的图片来自这个网站。</p></div></div>    
</body>
</html>