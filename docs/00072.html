<html>
<head>
<title>The Ultimate Guide to 12 Dimensionality Reduction Techniques (with Python codes)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">12种降维技术的终极指南(带Python代码)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-ultimate-guide-to-12-dimensionality-reduction-techniques-with-python-codes-2c2afdbc09e3?source=collection_archive---------1-----------------------#2018-08-26">https://medium.com/analytics-vidhya/the-ultimate-guide-to-12-dimensionality-reduction-techniques-with-python-codes-2c2afdbc09e3?source=collection_archive---------1-----------------------#2018-08-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2b75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您曾经处理过包含一千多个要素的数据集吗？超过5万的特性怎么样？我有，让我告诉你这是一个非常具有挑战性的任务，尤其是如果你不知道从哪里开始！拥有大量变量既是一件好事，也是一件坏事。我们有大量数据可供分析，这很好，但由于规模太大，这很有挑战性。</p><p id="a2f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在微观层面上分析每一个变量是不可行的。我们可能需要几天或几个月的时间来执行任何有意义的分析，并且我们会为我们的业务损失大量的时间和金钱！更不用说这将需要大量的计算能力。我们需要一种更好的方法来处理高维数据，以便我们可以快速从中提取模式和见解。那么我们如何处理这样一个数据集呢？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/5746d8b9e3a1eda259b302ec7d886c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*MOmDjcWCp4Y5Jr7Y.jpg"/></div></figure><p id="f21e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然是使用降维技术。您可以使用这一概念来减少数据集中的要素数量，而不必丢失太多信息并保持(或提高)模型的性能。正如您将在本文中看到的，这是处理大型数据集的一种非常强大的方法。</p><p id="6417" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个全面的指南，介绍各种可以在实际场景中使用的降维技术。在深入探讨我提到的12种不同的技术之前，我们将首先理解这个概念是什么以及为什么我们应该使用它。每种技术都有自己的Python实现，可以让您很好地熟悉它。</p><h1 id="6d08" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">目录</h1><ol class=""><li id="9f91" class="kj kk hi ih b ii kl im km iq kn iu ko iy kp jc kq kr ks kt bi translated">什么是降维？</li><li id="1627" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">为什么需要降维？</li><li id="f0ce" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">常见降维技术<br/> 3.1缺失值比率<br/> 3.2低方差滤波器<br/> 3.3高相关滤波器<br/> 3.4随机森林<br/> 3.5后向特征消除<br/> 3.6前向特征选择<br/> 3.7因子分析<br/> 3.8主成分分析<br/> 3.9独立成分分析<br/> 3.10基于投影的方法<br/>3.11t-分布式随机邻居嵌入(t-SNE) <br/> 3.12 UMAP</li><li id="c358" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">各种降维技术的应用</li></ol><h1 id="9433" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">1.什么是降维？</h1><p id="f8d3" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">我们每天都在生成大量的数据。事实上，世界上90%的数据都是在最近3-4年内生成的！这些数字确实令人难以置信。以下是正在收集的数据类型的一些示例:</p><ul class=""><li id="65f0" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">脸书收集你喜欢的东西、分享的内容、发布的内容、你去过的地方、你喜欢的餐馆等数据。</li><li id="529e" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">你的智能手机应用程序会收集你的大量个人信息</li><li id="d0d9" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">亚马逊收集你购买、浏览、点击等的数据。在他们的网站上</li><li id="a2ab" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">赌场会跟踪每位顾客的一举一动</li></ul><p id="ca98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着数据生成和收集的不断增加，可视化和推断变得越来越具有挑战性。最常见的可视化方法之一是通过图表。假设我们有两个变量，年龄和身高。我们可以使用年龄和身高之间的散点图或线图，并轻松地将它们的关系可视化:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lg"><img src="../Images/b3b1158a3c644a36ca4e811cd4a578cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/0*Yc0EGLeAckM0dmPG.png"/></div></figure><p id="01ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在考虑这样一种情况，假设我们有100个变量(p=100)。在这种情况下，我们可以有100(100–1)/2 = 5000个不同的图。将它们分别形象化没有多大意义，对吗？在我们有大量变量的情况下，最好选择这些变量的子集(我们将使用AV的 <a class="ae lj" href="https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">练习题:Big Mart Sales III </strong> </a>中的数据集(在此链接上注册并从数据部分下载数据集)。</p><p id="2979" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.常见的降维技术</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lh"><img src="../Images/57a296fba2df1260cc5206a85cf4f6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/0*vMkn1XWcTU3Qmnjd.png"/></div></figure><p id="ce9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">降维有两种不同的方式:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/b7ab73295f47dbc666cb1da3334127b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/0*nRSVdbmNC_TnifaI.png"/></div></figure><p id="02fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过只保留原始数据集中最相关的变量(这种技术称为特征选择)</p><h1 id="0d71" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">通过寻找一个更小的新变量集，每个变量都是输入变量的组合，包含与输入变量基本相同的信息(这种技术被称为降维)</h1><p id="5f70" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">我们现在来看看各种降维技术，以及如何用Python实现它们。</p><ul class=""><li id="d316" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">3.1缺失值比率</li><li id="683a" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">假设给你一个数据集。你的第一步是什么？在构建模型之前，您自然会希望首先探索数据。浏览数据时，您发现数据集缺少一些值。现在怎么办？您将尝试找出这些缺失值的原因，然后估算它们或完全删除有缺失值的变量(使用适当的方法)。</li><li id="f3da" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">如果我们有太多的缺失值(比如超过50%)怎么办？我们应该估算缺失值还是丢弃变量？我倾向于删除变量，因为它没有太多的信息。然而，这并不是一成不变的。我们可以设置一个阈值，如果任何变量中缺失值的百分比超过该阈值，我们将删除该变量。</li><li id="351c" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">让我们用Python实现这种方法。</li><li id="e224" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">首先，让我们加载数据:</li></ul><p id="73fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lu">注意:读取数据时需要添加文件的路径。</em></p><h1 id="2b41" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">现在，我们将检查每个变量中缺失值的百分比。我们可以用<em class="lu">。isnull()。sum() </em>来计算这个。</h1><p id="d545" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">正如您在上表中看到的，没有太多的丢失值(实际上只有两个变量有它们)。我们可以使用适当的方法估算这些值，或者我们可以设置一个阈值，比如说20%，并删除丢失值超过20%的变量。让我们看看如何在Python中实现这一点:</p><ul class=""><li id="e76c" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">所以要使用的变量存储在“variable”中，它只包含那些缺失值小于20%的特征。</li><li id="f7d3" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">3.2低方差滤波器</li></ul><p id="40f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑我们数据集中的一个变量，其中所有的观察值都相同，比如说1。如果我们使用这个变量，你认为它能改进我们将要建立的模型吗？答案是否定的，因为这个变量的方差为零。</p><h1 id="a918" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">所以，我们需要计算每个变量的方差。然后删除与数据集中的其他变量相比方差较小的变量。这样做的原因，正如我上面提到的，是因为方差低的变量不会影响目标变量。</h1><p id="8643" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">让我们首先使用已知<em class="lu"> Item_Weight </em>观察值的中值来估算<em class="lu"> Item_Weight </em>列中的缺失值。对于<em class="lu"> Outlet_Size </em>列，我们将使用已知<em class="lu"> Outlet_Size </em>值的模式来估算缺失值:</p><p id="a589" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们检查是否所有缺失的值都已填充:</p><p id="9f80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">瞧啊。我们都准备好了。现在让我们计算所有数值变量的方差。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="1fc5" class="lp jm hi ll b fi lq lr l ls lt"># import required libraries <br/>import pandas as pd <br/>import numpy as np <br/>import matplotlib.pyplot as plt</span></pre><p id="b55a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上面的输出所示，与其他变量相比，<em class="lu"> Item_Visibility </em>的方差非常小。我们可以安全地删除这个列。这就是我们如何应用低方差滤波器。让我们用Python来实现它:</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="4ab4" class="lp jm hi ll b fi lq lr l ls lt"># read the data <br/>train=pd.read_csv("Train_UWu5bXk.csv")</span></pre><p id="f06b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码给出了方差大于10的变量列表。</p><p id="c9da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.3高相关性滤波器</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="4773" class="lp jm hi ll b fi lq lr l ls lt"># checking the percentage of missing values in each variable train.isnull().sum()/len(train)*100</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/5ce327611cb334fa82e64c8141a23c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/0*WDs9r-X9ExwRf0Tk.png"/></div></figure><p id="8231" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">两个变量之间的高度相关性意味着它们具有相似的趋势，并且可能携带相似的信息。这可能会大大降低某些模型的性能(例如，线性和逻辑回归模型)。我们可以计算本质上是数值的独立数值变量之间的相关性。如果相关系数超过某个阈值，我们可以删除其中一个变量(删除一个变量是非常主观的，应该始终记住这个域)。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="0b92" class="lp jm hi ll b fi lq lr l ls lt"># saving missing values in a variable <br/>a = train.isnull().sum()/len(train)*100 <br/># saving column names in a variable <br/>variables = train.columns <br/>variable = [ ] <br/>for i in range(0,12):<br/>    if a[i]&lt;=20:   #setting the threshold as 20%<br/>        variable.append(variables[i])</span></pre><p id="6bdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一般来说，我们应该保留那些与目标变量表现出良好或高度相关性的变量。</p><h1 id="c29c" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">让我们用Python来执行相关计算。我们将首先删除因变量(<em class="lu"> Item_Outlet_Sales </em>)，并将剩余变量保存在新的数据框架(<em class="lu"> df </em>)中。</h1><p id="05d4" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">太好了，我们的数据集中没有任何高度相关的变量。通常，如果一对变量之间的相关性大于0.5–0.6，我们应该认真考虑去掉其中一个变量。</p><p id="6501" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.4随机森林</p><p id="9203" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林是最广泛使用的特征选择算法之一。它具有内置的特性重要性，因此您不需要单独编程。这有助于我们选择更小的特征子集。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="fff8" class="lp jm hi ll b fi lq lr l ls lt">train['Item_Weight'].fillna(train['Item_Weight'].median, inplace=True) train['Outlet_Size'].fillna(train['Outlet_Size'].mode()[0], inplace=True)</span></pre><p id="4919" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要通过应用一个热编码将数据转换成数字形式，因为随机森林(Scikit-Learn实现)只接受数字输入。让我们也放弃ID变量(<em class="lu"> Item_Identifier </em>和<em class="lu"> Outlet_Identifier </em>)，因为它们只是唯一的数字，目前对我们来说并不重要。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="9e8f" class="lp jm hi ll b fi lq lr l ls lt">train.isnull().sum()/len(train)*100</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/908910abda9429704b4a596223654881.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/0*Rev-I6P0moaw-7gh.png"/></div></figure><p id="436e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">拟合模型后，绘制特征重要性图:</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="c0d4" class="lp jm hi ll b fi lq lr l ls lt">train.var()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es lx"><img src="../Images/53c867a0af62118e25d8dc7bca4996b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/0*mtthS_13XSzFTZln.png"/></div></div></figure><p id="2593" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于上图，我们可以手动选择最顶端的特征来降低数据集中的维度。或者，<strong class="ih hj">我们可以使用<em class="lu"> sklearn </em>的<em class="lu"> SelectFromModel </em>来完成此操作</strong>。它根据权重的重要性选择特征。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="0c31" class="lp jm hi ll b fi lq lr l ls lt">numeric = train[['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']]<br/>var = numeric.var()<br/>numeric = numeric.columns<br/>variable = [ ]<br/>for i in range(0,len(var)):<br/>    if var[i]&gt;=10:   #setting the threshold as 10%<br/>       variable.append(numeric[i+1])</span></pre><p id="2ad2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.5反向特征消除</p><h1 id="06ae" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">按照以下步骤理解和使用“反向特征消除”技术:</h1><p id="d1b3" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">我们首先获取数据集中存在的所有n个变量，并使用它们来训练模型</p><p id="2cec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们计算模型的性能</p><p id="2710" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们在消除每个变量(n次)后计算模型的性能，即，我们每次丢弃一个变量，并在剩余的n-1个变量上训练模型</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="b4ba" class="lp jm hi ll b fi lq lr l ls lt">df=train.drop('Item_Outlet_Sales', 1) <br/>df.corr()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/3491b01a33f93352b236c13ae64cae4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/0*-QAB65FKZcl8gQp-.png"/></div></figure><p id="2036" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们确定移除后对模型性能产生最小(或没有)变化的变量，然后删除该变量</p><h1 id="6bda" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">重复这个过程，直到没有变量可以删除</h1><p id="6120" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated"><strong class="ih hj">该方法可用于建立线性回归或逻辑回归模型</strong>。让我们看看它的Python实现:</p><p id="c64e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要指定要选择的算法和特征数量，并从向后特征消除中获取变量列表。我们还可以使用“<em class="lu"> rfe.ranking_ </em>”命令来检查变量的排名。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="214c" class="lp jm hi ll b fi lq lr l ls lt">from sklearn.ensemble import RandomForestRegressor<br/>df=df.drop(['Item_Identifier', 'Outlet_Identifier'], axis=1)<br/>model = RandomForestRegressor(random_state=1, max_depth=10)<br/>df=pd.get_dummies(df)<br/>model.fit(df,train.Item_Outlet_Sales)</span></pre><p id="e4e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.6前进功能选择</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="cb4c" class="lp jm hi ll b fi lq lr l ls lt">features = df.columns<br/>importances = model.feature_importances_<br/>indices = np.argsort(importances)[-9:]  # top 10 features<br/>plt.title('Feature Importances')<br/>plt.barh(range(len(indices)), importances[indices], color='b', align='center')<br/>plt.yticks(range(len(indices)), [features[i] for i in indices])<br/>plt.xlabel('Relative Importance')<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/61684a9b0b1cba8354686c90df7e453d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/0*_d5pImQhz91MX-AO.png"/></div></figure><p id="db22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我们上面看到的反向特征消除的相反过程。我们不是消除特征，而是试图找到改善模型性能的最佳特征。这项技术的工作原理如下:</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="ec75" class="lp jm hi ll b fi lq lr l ls lt">from sklearn.feature_selection import SelectFromModel<br/>feature = SelectFromModel(model)<br/>Fit = feature.fit_transform(df, train.Item_Outlet_Sales)</span></pre><h1 id="9de0" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">我们从一个单一的特征开始。本质上，我们分别使用每个特征训练模型n次</h1><p id="19b6" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">给出最佳性能的变量被选为起始变量</p><ul class=""><li id="77ea" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">然后我们重复这个过程，一次添加一个变量。产生最高性能增长的变量被保留</li><li id="463f" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">我们重复这个过程，直到模型的性能没有明显的改善</li><li id="ca2e" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">让我们用Python实现它:</li><li id="3b76" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">这将返回一个数组，其中包含变量的F值以及与每个F值对应的p值。参考<a class="ae lj" href="http://www.statisticshowto.com/probability-and-statistics/f-statistic-value-test/" rel="noopener ugc nofollow" target="_blank">此链接</a>了解更多关于F值的信息。出于我们的目的，我们将选择F值大于10的变量:</li><li id="b7f8" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">这给了我们基于前向特征选择算法的最上面的变量。</li></ul><p id="13d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意</strong> : <strong class="ih hj">后向特征消除和前向特征选择都是耗时且计算量大的</strong>。它们实际上仅用于具有少量输入变量的数据集。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="cd86" class="lp jm hi ll b fi lq lr l ls lt">from sklearn.linear_model import LinearRegression<br/>from sklearn.feature_selection import RFE<br/>from sklearn import datasets<br/>lreg = LinearRegression()<br/>rfe = RFE(lreg, 10)<br/>rfe = rfe.fit_transform(df, train.Item_Outlet_Sales)</span></pre><p id="86f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们所看到的技术通常在我们的数据集中没有大量变量时使用。这些或多或少都是特征选择技术。在接下来的部分中，我们将使用时尚MNIST数据集，该数据集由属于不同类型服装的图像组成，例如t恤、裤子、包等。<strong class="ih hj">数据集可以从</strong> <a class="ae lj" href="https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-apparels/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">中下载识别服装</strong> </a> <strong class="ih hj">练习题。</strong></p><h1 id="f68a" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">该数据集总共有70，000幅图像，其中60，000幅在训练集中，其余10，000幅是测试图像。就本文的范围而言，我们将只研究训练图像。训练文件是zip格式的。解压缩zip文件后，您将获得一个. csv文件和一个包含这60，000张图像的train文件夹。每个图像的相应标签可以在“train.csv”文件中找到。</h1><p id="b566" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">3.7因素分析</p><ul class=""><li id="0c4f" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">假设我们有两个变量:收入和教育。这些变量可能具有很高的相关性，因为受教育程度较高的人往往收入明显较高，反之亦然。</li><li id="f3c5" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">在因子分析技术中，变量按照它们的相关性进行分组，即特定组中的所有变量在它们之间具有高相关性，但是与其他组的变量具有低相关性。在这里，每个组被称为一个因子。与数据的原始维度相比，这些因素的数量很少。然而，这些因素很难观察到。</li><li id="85ea" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">让我们首先读入火车文件夹中包含的所有图像:</li><li id="e595" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">注意:你必须用你的train文件夹的路径替换glob函数中的路径。</li></ul><p id="a260" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将把这些图像转换成一个<em class="lu"> numpy </em>数组格式，这样我们就可以执行数学运算并绘制图像。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="1b99" class="lp jm hi ll b fi lq lr l ls lt">from sklearn.feature_selection import f_regression<br/>ffs = f_regression(df,train.Item_Outlet_Sales )</span></pre><p id="c87f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你在上面看到的，这是一个三维数组。我们必须将其转换为一维，因为所有即将出现的技术都只接受一维输入。为此，我们需要使图像变平:</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="69fb" class="lp jm hi ll b fi lq lr l ls lt">variable = [ ]<br/>for i in range(0,len(df.columns)-1):<br/>    if ffs[0][i] &gt;=10:<br/>       variable.append(df.columns[i])</span></pre><p id="fbb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们创建一个数据帧，包含每幅图像中每个像素的像素值，以及它们对应的标签(对于标签，我们将使用<em class="lu"> train.csv </em>文件)。</p><p id="4f5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将使用因子分析来分解数据集:</p><p id="be1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，<em class="lu"> n_components </em>将决定变换数据中的因子数量。转换数据后，是时候可视化结果了:</p><p id="d057" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看起来很神奇，不是吗？我们可以在上图中看到所有不同的因素。这里，x轴和y轴表示分解因子的值。正如我前面提到的，很难单独观察这些因素，但是我们已经成功地降低了数据的维度。</p><h1 id="2e07" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">3.8主成分分析</h1><p id="db0a" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">PCA是一种帮助我们从现有的大量变量中提取一组新变量的技术。这些新提取的变量被称为主成分。你可以参考<a class="ae lj" href="https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>了解更多关于PCA的知识。作为快速参考，以下是您在继续下一步之前应该了解的关于PCA的一些要点:</p><p id="ab59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主成分是原始变量的线性组合</p><p id="9694" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">提取主成分的方式是第一个主成分解释数据集中的最大方差</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="5472" class="lp jm hi ll b fi lq lr l ls lt">import pandas as pd<br/>import numpy as np<br/>from glob import glob<br/>import cv2<br/>images = [cv2.imread(file) for file in glob('train/*.png')]</span></pre><p id="e13a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二主成分试图解释数据集中的剩余方差，并且与第一主成分不相关</p><p id="7e1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第三个主成分试图解释前两个主成分无法解释的差异，依此类推</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="72ac" class="lp jm hi ll b fi lq lr l ls lt">images = np.array(images)<br/>images.shape</span></pre><p id="f9d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">(60000, 28, 28, 3)</p><p id="40ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在继续之前，我们将从数据集中随机绘制一些图像:</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="9190" class="lp jm hi ll b fi lq lr l ls lt">image = []<br/>for i in range(0,60000):<br/>    img = images[i].flatten()<br/>    image.append(img)<br/>image = np.array(image)</span></pre><p id="612c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们使用Python实现PCA并转换数据集:</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="8171" class="lp jm hi ll b fi lq lr l ls lt">train = pd.read_csv("train.csv")     # Give the complete path of your train.csv file<br/>feat_cols = [ 'pixel'+str(i) for i in range(image.shape[1]) ]<br/>df = pd.DataFrame(image,columns=feat_cols)<br/>df['label'] = train['label']</span></pre><p id="496f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，<em class="lu"> n_components </em>将决定变换数据中主成分的数量。让我们想象一下使用这4个组成部分解释了多少差异。我们将使用<em class="lu">解释_方差_比率_ </em>来计算相同的值。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="1fbd" class="lp jm hi ll b fi lq lr l ls lt">from sklearn.decomposition import FactorAnalysis<br/>FA = FactorAnalysis(n_components = 3).fit_transform(df[feat_cols].values)</span></pre><p id="d626" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，蓝线代表按组件解释的差异，而橙线代表累计解释的差异。<strong class="ih hj">我们只用四个成分就能解释数据集中大约60%的差异。</strong>现在让我们试着想象一下这些分解的组件:</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="9f57" class="lp jm hi ll b fi lq lr l ls lt">%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>plt.figure(figsize=(12,8))<br/>plt.title('Factor Analysis Components')<br/>plt.scatter(FA[:,0], FA[:,1])<br/>plt.scatter(FA[:,1], FA[:,2])<br/>plt.scatter(FA[:,2],FA[:,0])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es me"><img src="../Images/48921fc42e2810756fdf9e0d0b4dc021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5EVtLUnLvnHqzyyJ.png"/></div></div></figure><p id="e3b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们添加到PCA技术中的每一个额外的维度捕获的模型中的方差越来越少。第一个组件是最重要的，其次是第二个，然后是第三个，依此类推。</p><h1 id="923b" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">我们还可以使用<strong class="ih hj">奇异值分解</strong> (SVD)将我们的原始数据集分解成它的成分，从而降低维度。要了解SVD背后的数学原理，请参考本文。</h1><p id="226e" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">奇异值分解将原始变量分解成三个组成矩阵。它主要用于移除数据集中的冗余要素。它使用特征值和特征向量的概念来确定这三个矩阵。由于本文的范围，我们不会深入研究它的数学原理，但是让我们坚持我们的计划，即减少我们数据集中的维度。</p><ul class=""><li id="324b" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">让我们实现SVD并分解我们的原始变量:</li><li id="17a8" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">让我们通过绘制前两个主要成分来可视化转换后的变量:</li><li id="5cbe" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">上面的散点图非常清晰地向我们展示了分解后的组件。如前所述，这些组件之间没有太多关联。</li><li id="24ea" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">3.9独立成分分析</li></ul><p id="77a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">独立分量分析(ICA)是基于信息论的，也是应用最广泛的降维技术之一。PCA和ICA的主要区别在于PCA寻找不相关的因素，而ICA寻找独立的因素。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="d197" class="lp jm hi ll b fi lq lr l ls lt">rndperm = np.random.permutation(df.shape[0])<br/>plt.gray()<br/>fig = plt.figure(figsize=(20,10))<br/>for i in range(0,15):<br/>    ax = fig.add_subplot(3,5,i+1)<br/>    ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28*3)).astype(float))</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mf"><img src="../Images/0b9b8f65e809cd13d5d22a67f9d5fe76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ELNJKacR16VFOODF.png"/></div></div></figure><p id="7384" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果两个变量不相关，这意味着它们之间没有线性关系。如果它们是独立的，就意味着它们不依赖于其他变量。例如，一个人的年龄与他/她吃什么或看多长时间电视无关。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="5468" class="lp jm hi ll b fi lq lr l ls lt">from sklearn.decomposition import PCA<br/>pca = PCA(n_components=4)<br/>pca_result = pca.fit_transform(df[feat_cols].values)</span></pre><p id="6cce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">该算法假设给定变量是一些未知潜变量的线性混合。它还假设这些潜在变量是相互独立的</strong>，即它们不依赖于其他变量，因此它们被称为观察数据的独立成分。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="4ced" class="lp jm hi ll b fi lq lr l ls lt">plt.plot(range(4), pca.explained_variance_ratio_)<br/>plt.plot(range(4), np.cumsum(pca.explained_variance_ratio_))<br/>plt.title("Component-wise and Cumulative Explained Variance")</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/8205842c70c68eaa916ebd5ed456b2be.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/0*v8P65lasMNstR8eL.png"/></div></figure><p id="947d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们直观地比较一下PCA和ICA，以便更好地理解它们的不同之处:</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="7748" class="lp jm hi ll b fi lq lr l ls lt">import seaborn as sns<br/>plt.style.use('fivethirtyeight')<br/>fig, axarr = plt.subplots(2, 2, figsize=(12, 8))<br/>sns.heatmap(pca.components_[0, :].reshape(28, 84), ax=axarr[0][0], cmap='gray_r')<br/>sns.heatmap(pca.components_[1, :].reshape(28, 84), ax=axarr[0][1], cmap='gray_r')<br/>sns.heatmap(pca.components_[2, :].reshape(28, 84), ax=axarr[1][0], cmap='gray_r')<br/>sns.heatmap(pca.components_[3, :].reshape(28, 84), ax=axarr[1][1], cmap='gray_r')<br/>axarr[0][0].set_title(<br/>"{0:.2f}% Explained Variance".format(pca.explained_variance_ratio_[0]*100),<br/>fontsize=12<br/>)<br/>axarr[0][1].set_title(<br/>"{0:.2f}% Explained Variance".format(pca.explained_variance_ratio_[1]*100),<br/>fontsize=12<br/>)<br/>axarr[1][0].set_title(<br/>"{0:.2f}% Explained Variance".format(pca.explained_variance_ratio_[2]*100),<br/>fontsize=12<br/>)<br/>axarr[1][1].set_title(<br/>"{0:.2f}% Explained Variance".format(pca.explained_variance_ratio_[3]*100),<br/>fontsize=12<br/>)<br/>axarr[0][0].set_aspect('equal')<br/>axarr[0][1].set_aspect('equal')<br/>axarr[1][0].set_aspect('equal')<br/>axarr[1][1].set_aspect('equal')<br/><br/>plt.suptitle('4-Component PCA')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mh"><img src="../Images/6bee61aa2d4108c9257a164e9b6e6717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UcNJDu27Yyt_NcpR.png"/></div></div></figure><p id="7966" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，图像(a)表示PCA结果，而图像(b)表示同一数据集上的ICA结果。</p><p id="2b8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主成分分析的方程是x = Wχ。</p><p id="6dd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，</p><p id="4f61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x是观察值</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="9f77" class="lp jm hi ll b fi lq lr l ls lt">from sklearn.decomposition import TruncatedSVD <br/>svd = TruncatedSVD(n_components=3, random_state=42).fit_transform(df[feat_cols].values)</span></pre><p id="9b22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">w是混合矩阵</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="736e" class="lp jm hi ll b fi lq lr l ls lt">plt.figure(figsize=(12,8))<br/>plt.title('SVD Components')<br/>plt.scatter(svd[:,0], svd[:,1])<br/>plt.scatter(svd[:,1], svd[:,2])<br/>plt.scatter(svd[:,2],svd[:,0])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mi"><img src="../Images/f2bc5ee15faf4283fb6201d9c55970e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H2NSJP_lBSC2xi5k.png"/></div></div></figure><p id="88e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">χ是来源还是独立成分</p><h1 id="2d39" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">现在，我们必须找到一个非混合矩阵，使组件变得尽可能独立。测量组件独立性的最常用方法是非高斯性的:</h1><p id="1dfd" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">根据中心极限定理，独立分量之和的分布趋于正态分布(高斯)。</p><p id="c2ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们可以寻找使独立分量的每个分量的峰度最大化的变换。峰度是分布的三阶矩。想了解更多关于峰度的知识，请点击这里。</p><p id="2243" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最大化峰度将使分布非高斯，因此我们将得到独立的分量。</p><p id="fac0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述分布是非高斯的，这又使得分量独立。让我们尝试用Python实现ICA:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mj"><img src="../Images/44f61dc35d18cda6cf1ecf7c4cb84bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/0*s1EOOetLYoeE8E9N.png"/></div></figure><p id="f7c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，<em class="lu"> n_components </em>将决定转换后的数据中组件的数量。我们使用ICA将数据转换成3个分量。让我们想象一下它对数据的转化有多好:</p><p id="b79b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些数据被分成不同的独立部分，在上图中可以非常清楚地看到。x轴和Y轴表示分解的独立分量的值。</p><p id="1ef7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们来看一些利用投影技术减少数据维数的方法。</p><ul class=""><li id="0c99" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">3.10基于预测的方法</li><li id="2eb9" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">首先，我们需要理解什么是投影。假设我们有两个向量，向量<strong class="ih hj"> a </strong>和向量<strong class="ih hj"> b </strong>，如下所示:</li><li id="9fec" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">我们想找到<strong class="ih hj"> a </strong>在<strong class="ih hj"> b </strong>上的投影。设a和b之间的角度为∅.投影(<strong class="ih hj"> a1 </strong>)看起来像这样:</li></ul><p id="4ff6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> a1 </strong>是平行于<strong class="ih hj"> b的向量</strong>因此，我们可以使用下面的等式得到向量a在向量b上的投影:</p><ul class=""><li id="1667" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">这里，</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mk"><img src="../Images/422d19d624f40f84b7ea904f952e214a.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/0*bkfbcWVilq0-yp7M.png"/></div></figure><ul class=""><li id="c7ca" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">a1 =到b的投影</li><li id="4d2d" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">b̂= b方向的单位矢量</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/3d2216694fca1090ae4b6ff2e699cb67.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/0*mET7TpqBxycfDklv.png"/></div></figure><p id="15b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过将一个向量投影到另一个向量上，可以降低维数。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="df9d" class="lp jm hi ll b fi lq lr l ls lt">from sklearn.decomposition import FastICA <br/>ICA = FastICA(n_components=3, random_state=12) <br/>X=ICA.fit_transform(df[feat_cols].values)</span></pre><p id="a5f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在投影技术中，多维数据通过将其点投影到低维空间来表示。现在我们将讨论不同的预测方法:</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="c10c" class="lp jm hi ll b fi lq lr l ls lt">plt.figure(figsize=(12,8))<br/>plt.title('ICA Components')<br/>plt.scatter(X[:,0], X[:,1])<br/>plt.scatter(X[:,1], X[:,2])<br/>plt.scatter(X[:,2], X[:,0])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mm"><img src="../Images/c6a2c4a46c1b16838f5e8bb475a12bb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h2w5pUWgiYWppTF6.png"/></div></div></figure><p id="b80c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">投影到感兴趣的方向:<br/>感兴趣的方向取决于具体的问题，但通常，投影值为非高斯的方向被认为是感兴趣的<br/>与ICA(独立分量分析)相似，投影寻找最大化投影值峰度的方向，作为非高斯性的度量</p><p id="b356" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到流形上的投影:</p><h1 id="4a1d" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">从前，人们认为地球是平的。在地球上无论你走到哪里，它看起来都是平坦的(暂且忽略山脉)。但是如果你一直朝一个方向走，你会回到你开始的地方。如果地球是平的，那就不会发生。地球看起来是平的，只是因为与地球相比，我们实在是太小了。</h1><p id="2862" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">地球看起来是平的这些小部分是流形，如果我们将所有这些流形结合起来，我们就可以获得地球的大尺度视图，即原始数据。类似地，对于一条n维曲线，小的平坦部分是流形，这些流形的组合将给出原始的n维曲线。让我们看看投影到流形上的步骤:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mn"><img src="../Images/7335c9358f1856c542ed83a7ce778903.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/0*RBbKwW40SW3gEM0N.png"/></div></figure><p id="8761" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们首先寻找一个接近数据的流形</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mo"><img src="../Images/a41ab871044169d6e656d11053af322f.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/0*bKWfQMqq8qS_SJZj.png"/></div></figure><p id="b3b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后把数据投射到那个流形上</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mp"><img src="../Images/08e0f4c10d073469913e43a753c62c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/0*vA9TleiBLaVMeAA0.png"/></div></figure><p id="456c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后为了表示，我们展开流形</p><ul class=""><li id="9afd" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">有各种技术来获得流形，并且所有这些技术由三个步骤组成:<br/>从每个数据点收集信息以构建以数据点为顶点的图<br/>将上面生成的图转换成用于嵌入步骤的合适输入<br/>计算(nXn)本征方程</li><li id="59ff" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">让我们用一个例子来理解流形投影技术。</li></ul><p id="3e4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果一个流形对任意阶是连续可微的，则称它为光滑流形或可微流形。ISOMAP是一种旨在恢复非线性流形的完整低维表示的算法。它假设流形是光滑的。</p><p id="5e94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它还假设对于流形上的任意一对点，两点之间的测地线距离(曲面上两点之间的最短距离)等于欧氏距离(直线上两点之间的最短距离)。让我们首先想象一对点之间的测地线和欧几里德距离:</p><ul class=""><li id="fb66" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">这里，</li><li id="93ab" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">Dn1n2 =和X2之间的测地线距离</li></ul><p id="c3d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dn1n2 =和X2之间的欧几里德距离</p><p id="a335" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ISOMAP假设这两个距离相等。现在让我们来看一下这种技术的更详细的解释。如前所述，所有这些技术都基于一个三步方法。我们将详细了解这些步骤:</p><ul class=""><li id="17bd" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated"><strong class="ih hj">邻里关系图:</strong></li><li id="eca1" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">第一步是计算所有数据点对之间的距离:dij = dχ(xi，xj) = || xi-xj || χ <br/>这里，dχ(xi，xj)= Xi和xj之间的测地线距离<br/>| | Xi-XJ | | = Xi和XJ之间的欧氏距离</li><li id="6f5b" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">计算距离后，我们确定哪些数据点是流形的邻居</li><li id="02e5" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">最后生成邻域图:G=G(V,ℰ)，其中顶点集合V = {x1，x2，…，xn}是输入数据点，边集ℰ = {eij}表示点之间的邻域关系</li></ul><p id="4ef8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算图形距离:</strong></p><p id="4d8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们通过图的距离来计算流形中成对的点之间的测地线距离</p><p id="637e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图距离是图G中所有点对之间的最短路径距离</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mq"><img src="../Images/f14f27c69b107cd9a47bebdebdd74af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/0*p1yJFvNywcHAShQ3.png"/></div></figure><p id="b393" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">嵌入:</strong></p><ul class=""><li id="9d66" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">一旦我们有了距离，我们就形成了平方图距离的对称(nXn)矩阵</li><li id="2a76" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">现在我们选择嵌入向量来最小化测地线距离和图形距离之间的差异</li></ul><p id="abb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，图G通过(t×n)矩阵嵌入到Y中</p><p id="4063" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用Python实现它，更清楚地了解我在说什么。我们将通过等距映射进行非线性降维。对于可视化，我们将只取数据集的一个子集，因为在整个数据集上运行它将需要大量时间。</p><ul class=""><li id="4d06" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">使用的参数:</li><li id="546e" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated"><em class="lu"> n_neighbors </em>决定每个点的邻居数量</li><li id="61ba" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated"><em class="lu"> n_components </em>决定流形的坐标数</li></ul><p id="3461" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">n_jobs  = -1将使用所有可用的CPU内核</p><ul class=""><li id="eb95" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">可视化转换后的数据:</li><li id="049b" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">从上面可以看出，这些组件之间的相关性非常低。事实上，与我们之前使用SVD获得的组件相比，它们的相关性更低！</li></ul><p id="892f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.11 t-分布式随机邻居嵌入(t-SNE)</p><ul class=""><li id="9ef8" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">到目前为止，我们已经了解到，对于具有大量变量的数据集，PCA是降维和可视化的好选择。但是如果我们可以使用更先进的东西呢？如果我们可以很容易地以非线性的方式搜索模式呢？SNE霸王龙就是这样一种技术。我们主要可以使用两种方法来绘制数据点:</li><li id="5b39" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">局部方法:它们将流形上的邻近点映射到低维表示中的邻近点。</li><li id="bef1" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">全局方法:他们试图在所有尺度上保持几何形状，即把流形上的邻近点映射到低维表示中的邻近点，以及把远点映射到远点。</li></ul><p id="6f8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">t-SNE是为数不多的能够同时保留数据的局部和全局结构的算法之一</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="baa0" class="lp jm hi ll b fi lq lr l ls lt">from sklearn import manifold <br/>trans_data = manifold.Isomap(n_neighbors=5, n_components=3, n_jobs=-1).fit_transform(df[feat_cols][:6000].values)</span></pre><p id="47ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它计算高维空间和低维空间中点的概率相似性</p><ul class=""><li id="df3d" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">数据点之间的高维欧几里德距离被转换成表示相似性的条件概率:</li><li id="638e" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">𝛔i和xj是数据点，||xi-xj||代表这些数据点之间的欧氏距离，而xj是高维空间中数据点的方差</li><li id="ffe4" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">对于对应于高维数据点xi和xj的低维数据点yi和yj，可以使用下式计算类似的条件概率:</li></ul><p id="ce9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中||yi-yj||表示yi和yj之间的欧氏距离</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="a7a6" class="lp jm hi ll b fi lq lr l ls lt">plt.figure(figsize=(12,8))<br/>plt.title('Decomposition using ISOMAP')<br/>plt.scatter(trans_data[:,0], trans_data[:,1])<br/>plt.scatter(trans_data[:,1], trans_data[:,2])<br/>plt.scatter(trans_data[:,2], trans_data[:,0])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mr"><img src="../Images/27272020e1f5dfe5cbf526b9bb5f8648.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tKoq5sN5VbB-n_34.png"/></div></div></figure><p id="8727" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在计算了两个概率之后，它会最小化两个概率之间的差异</p><h1 id="d13c" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">你可以参考<a class="ae lj" href="https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>来更详细地了解SNE霸王龙。</h1><p id="c4c5" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">我们现在将在Python中实现它，并可视化结果:</p><ul class=""><li id="d898" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated"><em class="lu"> n_components </em>将决定转换后的数据中组件的数量。可视化转换数据的时间:</li><li id="7268" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">在这里，您可以清楚地看到使用强大的t-SNE技术转换的不同组件。</li><li id="73eb" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">3.12 UMAP</li><li id="83fe" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">t-SNE在大型数据集上工作得非常好，但它也有其局限性，例如丢失大规模信息，计算时间慢，以及无法有意义地表示非常大的数据集。统一流形近似和投影(UMAP)是一种降维技术，与t-SNE相比，它可以在较短的运行时间内保留尽可能多的局部数据结构和更多的全局数据结构。听起来很有趣，对吧？</li><li id="c921" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">UMAP的一些主要优势是:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ms"><img src="../Images/57debe9781c084dc0db30f29046e3a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/0*ynjyDdE8lHzQZQwX.png"/></div></figure><p id="6059" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它可以轻松处理大型数据集和高维数据</p><ul class=""><li id="7245" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">它结合了可视化的能力和降低数据维度的能力</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mt"><img src="../Images/c515d3f758faa9b9f1df889d6f633a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/0*NVaBc0e3nqLjcDuW.png"/></div></figure><p id="04ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了保留本地结构，它还保留了数据的全局结构。UMAP将流形上的邻近点映射到低维表示中的邻近点，并对远点进行同样的操作</p><ul class=""><li id="2d4b" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">该方法使用k-最近邻的概念，并使用随机梯度下降来优化结果。它首先计算高维空间中的点之间的距离，将其投影到低维空间中，然后计算这个低维空间中的点之间的距离。然后，它使用随机梯度下降来最小化这些距离之间的差异。为了更深入地了解UMAP是如何工作的，请看<a class="ae lj" href="https://arxiv.org/pdf/1802.03426.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文</a>。</li></ul><p id="d6ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考<a class="ae lj" href="http://umap-learn.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">此处</a>查看UMAP的文档和安装指南。我们现在将使用Python实现它:</p><p id="7f20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="587b" class="lp jm hi ll b fi lq lr l ls lt">from sklearn.manifold import TSNE <br/>tsne = TSNE(n_components=3, n_iter=300).fit_transform(df[feat_cols][:6000].values)</span></pre><p id="5500" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lu"> n_neighbors </em>确定使用的相邻点的数量</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="fa83" class="lp jm hi ll b fi lq lr l ls lt">plt.figure(figsize=(12,8))<br/>plt.title('t-SNE components')<br/>plt.scatter(tsne[:,0], tsne[:,1])<br/>plt.scatter(tsne[:,1], tsne[:,2])<br/>plt.scatter(tsne[:,2], tsne[:,0])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mu"><img src="../Images/a5c96525fb932c8acb61f1d78ea49dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iEvY29xeHzotZKYK.png"/></div></div></figure><p id="c3f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lu"> min_dist </em>控制允许嵌入的紧密程度。较大的值确保嵌入点分布更加均匀</p><h1 id="802b" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">让我们想象一下这种转变:</h1><p id="fe0d" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">维度已经减少，我们可以看到不同的转换组件。转换后的变量之间的相关性非常小。让我们比较一下UMAP和SNE霸王龙的结果:</p><p id="7055" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，与从t-SNE获得的成分之间的相关性相比，从UMAP获得的成分之间的相关性要小得多。因此，UMAP倾向于给出更好的结果。</p><ul class=""><li id="d33e" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated"><strong class="ih hj">正如UMAP的GitHub repository中提到的，在保存数据的全局结构方面，它通常比t-SNE表现得更好。这意味着它通常可以提供更好的数据“大图”视图，并保留本地邻居关系。</strong></li><li id="cff6" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">深呼吸。我们已经讨论了相当多的降维技术。下面简单总结一下它们各自可以用在什么地方。</li><li id="4a6d" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">4.简要总结何时使用每种降维技术</li></ul><p id="94fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这一节中，我们将简要总结我们所涉及的每一种降维技术的用例。重要的是要了解你可以在哪里，应该在哪里使用某种技术，因为它有助于节省时间，精力和计算能力。</p><p id="29e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺失值比率</strong>:如果数据集有太多的缺失值，我们使用这种方法来减少变量的数量。我们可以去掉那些有大量缺失值的变量</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="3727" class="lp jm hi ll b fi lq lr l ls lt">import umap<br/>umap_data = umap.UMAP(n_neighbors=5, min_dist=0.3, n_components=3).fit_transform(df[feat_cols][:6000].values)</span></pre><p id="3ee8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">低方差过滤器</strong>:我们应用这种方法从数据集中识别和删除常量变量。低方差变量不会过度影响目标变量，因此可以安全地删除这些变量</p><ul class=""><li id="3a1b" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated"><strong class="ih hj">高相关性过滤器</strong>:一对相关性高的变量增加了数据集中的多重共线性。因此，我们可以使用这种技术找到高度相关的特征，并相应地删除它们</li><li id="543f" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">随机森林:这是最常用的技术之一，它告诉我们数据集中每个特征的重要性。我们可以找到每个特征的重要性，并保留最上面的特征，从而实现降维</li></ul><p id="066a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">后向特征消除</strong>和<strong class="ih hj">前向特征选择</strong>技术都需要大量的计算时间，因此通常用于较小的数据集</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="8a6c" class="lp jm hi ll b fi lq lr l ls lt">plt.figure(figsize=(12,8))<br/>plt.title('Decomposition using UMAP')<br/>plt.scatter(umap_data[:,0], umap_data[:,1])<br/>plt.scatter(umap_data[:,1], umap_data[:,2])<br/>plt.scatter(umap_data[:,2], umap_data[:,0])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mv"><img src="../Images/1f4f8ecdf084fd90135f3b8461ba7c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7n5REiRyqStO2wA5.png"/></div></div></figure><p id="6fe7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">因素分析</strong>:这种技术最适合于我们拥有高度相关的一组变量的情况。它根据变量的相关性将变量分成不同的组，并用一个因子表示每个组</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mw"><img src="../Images/15d23f5f5e71e145d75f3fc07f5647b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-eFD7u1uVVcanpgW.png"/></div></div></figure><p id="c57b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">主成分分析</strong>:这是处理线性数据最广泛使用的技术之一。它将数据分成一组部分，这些部分试图解释尽可能多的差异</p><p id="7c2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">独立分量分析</strong>:我们可以使用ICA将数据转换成独立分量，用较少的分量来描述数据</p><p id="5c3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ISOMAP </strong>:当数据是强非线性时，我们使用这种技术</p><h1 id="c347" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">t-SNE :当数据是强非线性的时候，这种技术也很有效。它对于可视化也非常有效</h1><p id="a57c" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated"><strong class="ih hj"> UMAP </strong>:这种技术适用于高维数据。它的运行时间比t-SNE短</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mx"><img src="../Images/6b90575c11eff95cf9d86b10bf5cffff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LWepirvNynxPbnXr.png"/></div></div></figure><ul class=""><li id="1e6f" class="kj kk hi ih b ii ij im in iq lc iu ld iy le jc lf kr ks kt bi translated">结束注释</li><li id="99a6" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">这是你在任何地方都能找到的关于降维的最全面的文章！我从中获得了很多乐趣，并找到了一些新的方法来处理大量我以前没有用过的变量(比如UMAP)。</li><li id="1d83" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated">处理成千上万的要素是任何数据科学家的必备技能。我们每天生成的数据量是前所未有的，我们需要找到不同的方法来使用它。降维是一种非常有用的方法，对我来说非常有用，无论是在专业场合还是在机器学习黑客马拉松中。</li><li id="ce91" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated"><em class="lu">原载于2018年8月26日</em><a class="ae lj" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/" rel="noopener ugc nofollow" target="_blank"><em class="lu">【www.analyticsvidhya.com】</em></a><em class="lu">。</em></li><li id="1cdf" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated"><strong class="ih hj">后向特征消除</strong>和<strong class="ih hj">前向特征选择</strong>技术都需要大量的计算时间，因此通常用于较小的数据集</li><li id="1b9a" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated"><strong class="ih hj">因素分析</strong>:这种技术最适合我们拥有高度相关的变量集的情况。它根据变量的相关性将变量分成不同的组，并用一个因子表示每个组</li><li id="13ec" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated"><strong class="ih hj">主成分分析</strong>:这是处理线性数据最广泛使用的技术之一。它将数据分成一组部分，这些部分试图解释尽可能多的差异</li><li id="c933" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated"><strong class="ih hj">独立分量分析</strong>:我们可以使用ICA将数据转换成独立分量，用较少的分量来描述数据</li><li id="21bb" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated"><strong class="ih hj"> ISOMAP </strong>:当数据是强非线性时，我们使用这种技术</li><li id="a6f5" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated"><strong class="ih hj"> t-SNE </strong>:当数据是强非线性的时候，这种技术也很有效。它对于可视化也非常有效</li><li id="1e03" class="kj kk hi ih b ii ku im kv iq kw iu kx iy ky jc lf kr ks kt bi translated"><strong class="ih hj"> UMAP </strong>:这种技术适用于高维数据。它的运行时间比t-SNE短</li></ul><h1 id="f022" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">结束注释</h1><p id="645c" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">这是你在任何地方都能找到的关于降维的最全面的文章！我从中获得了很多乐趣，并找到了一些新的方法来处理大量我以前没有用过的变量(比如UMAP)。</p><p id="db35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">处理成千上万的要素是任何数据科学家的必备技能。我们每天生成的数据量是前所未有的，我们需要找到不同的方法来使用它。降维是一种非常有用的方法，对我来说非常有用，无论是在专业场合还是在机器学习黑客马拉松中。</p></div><div class="ab cl my mz gp na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="hb hc hd he hf"><p id="c895" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lu">原载于2018年8月26日</em><a class="ae lj" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/" rel="noopener ugc nofollow" target="_blank"><em class="lu">www.analyticsvidhya.com</em></a><em class="lu">。</em></p></div></div>    
</body>
</html>