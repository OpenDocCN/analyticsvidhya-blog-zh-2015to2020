<html>
<head>
<title>Deep Dream: Visualizing the features learnt by Convolutional Networks in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度梦:可视化PyTorch中卷积网络学习的特征</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-dream-visualizing-the-features-learnt-by-convolutional-networks-in-pytorch-b7296ae3b7f?source=collection_archive---------2-----------------------#2020-02-22">https://medium.com/analytics-vidhya/deep-dream-visualizing-the-features-learnt-by-convolutional-networks-in-pytorch-b7296ae3b7f?source=collection_archive---------2-----------------------#2020-02-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="69d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当涉及到计算机视觉相关任务时，卷积神经网络(CNN)是最有效的机器学习工具之一。他们的有效性可以从以下事实来衡量:大多数计算机视觉竞赛，如ILSVRC、PASCAL VOC和COCO，已经被使用基于CNN的创新架构来实现其目标的参赛作品所主导。</p><p id="09b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，询问“在给定的CNN中，各种过滤器学习了什么特征”是有趣的。这个问题不仅从普通的“好奇心”的角度来看是有趣的，更重要的是知道这个问题的答案可以给我们非常有用的洞察力来改善我们CNN的性能。例如，ILSVRC-2013挑战赛的获奖作品(Clarifai)是通过改进上一年的获奖作品(AlexNet)而设计的。这些改进是通过在AlexNet上应用特征可视化技术(Deconvnets)选择的。参见<a class="ae jd" href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" rel="noopener ugc nofollow" target="_blank">这篇</a>论文(由ILSVRC-2013获奖者撰写)了解更多细节，或者<a class="ae jd" rel="noopener" href="/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103">这篇</a>博客进行精彩评论。</p><p id="0b9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我们将学习如何使用一种叫做“激活最大化”的技术来可视化CNN学习的特征，这种技术从一个由随机初始化的像素组成的图像开始，这些像素的值被慢慢调整，以最大化我们希望可视化的层的输出。这是在<a class="ae jd" href="https://www.researchgate.net/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network" rel="noopener ugc nofollow" target="_blank">这篇</a>论文中首次介绍的，并在<a class="ae jd" href="https://arxiv.org/abs/1312.6034" rel="noopener ugc nofollow" target="_blank">这篇</a>论文中首次应用于CNN。然而，对CNN的激活最大化的天真应用倾向于产生非常高频率的图像，这些图像看起来一点也不像人们每天遇到的真实世界的自然图像。例如，参见<a class="ae jd" href="https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis" rel="noopener ugc nofollow" target="_blank">此处</a>对该问题的详细描述以及解决这些问题的常用方法的讨论。在本帖中，我们将限制自己使用三种简单的正则化技术来使图像更有意义:</p><ol class=""><li id="d54c" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">从一张28 x 28的小图片开始，慢慢放大到想要的尺寸，比如这里的<a class="ae jd" href="https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030" rel="noopener" target="_blank"/>。</li><li id="5ad5" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">惩罚大像素值</li><li id="0ed0" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">不利于图像中大的像素梯度，即不利于相邻像素值的任何急剧变化。</li></ol><p id="409c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以让我们开始吧。包含我尝试过的各种东西的完整代码可以在我的<a class="ae jd" href="https://github.com/praritagarwal/Visualizing-CNN-Layers/blob/master/Activation%20Maximization.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。这篇文章是基于笔记本中的试验#6。在这里，我将详细介绍一下代码。顺便提一下，我最近还看到了一个很棒的Keras——Keras的创造者Francois Chollet实现了同样的技术。我强烈推荐大家看看他的<a class="ae jd" href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html" rel="noopener ugc nofollow" target="_blank">帖子</a>。</p><p id="28ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从加载一个预训练模型开始:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="d4b7" class="kb kc hi jx b fi kd ke l kf kg">import torch<br/>from torchvision import models<br/>model = models.googlenet(pretrained = True)</span></pre><p id="31a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然我看到的大多数关于激活最大化的博客都倾向于使用VGG16作为他们的预训练模型，没有什么特别的原因，除了尝试一些不同的东西，我将使用GoogLeNet。这个博客中的几乎所有代码都可以直接应用于任何其他经过训练的CNN。</p><p id="bb6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们感兴趣的是可视化模型已经学习的内容，而不是重新训练模型，因此，我们应该冻结模型参数，以便它们在反向传播期间不会改变。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="809e" class="kb kc hi jx b fi kd ke l kf kg">for param in model.parameters():<br/>    param.requires_grad_(False)</span></pre><p id="5209" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，模型中的各个层可以通过赋予它们的唯一名称轻松访问。因此，让我们列出模型中不同层的名称:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="95e0" class="kb kc hi jx b fi kd ke l kf kg">list(map(lambda x: x[0], model.named_children()))</span></pre><p id="82d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在GoogLeNet上，这会产生以下输出</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="4ccc" class="kb kc hi jx b fi kd ke l kf kg">['conv1',<br/> 'maxpool1',<br/> 'conv2',<br/> 'conv3',<br/> 'maxpool2',<br/> 'inception3a',<br/> 'inception3b',<br/> 'maxpool3',<br/> 'inception4a',<br/> 'inception4b',<br/> 'inception4c',<br/> 'inception4d',<br/> 'inception4e',<br/> 'maxpool4',<br/> 'inception5a',<br/> 'inception5b',<br/> 'avgpool',<br/> 'dropout',<br/> 'fc']</span></pre><p id="8afa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">出于演示目的，我将(随机)选择名为“inception4a”的图层。我们现在必须为这一层注册一个向前的钩子。<a class="ae jd" href="https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks" rel="noopener ugc nofollow" target="_blank">挂钩</a>提供对所需图层的输出和grad _ ouput的简单访问。顾名思义，向前钩子在向前传递期间执行，并允许我们查看/修改层的输出。类似地，在向后传递期间执行向后挂钩，并允许我们查看/修改层的grad _ ouput。查看这个<a class="ae jd" href="https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/" rel="noopener ugc nofollow" target="_blank">博客</a>和这个<a class="ae jd" href="https://www.kaggle.com/sironghuang/understanding-pytorch-hooks/notebook" rel="noopener ugc nofollow" target="_blank"> kaggle内核</a>以获得更多关于钩子的信息。这里的实现基于pytorch讨论板上的这个<a class="ae jd" href="https://discuss.pytorch.org/t/visualize-feature-map/29597/2" rel="noopener ugc nofollow" target="_blank">讨论</a>。为了注册一个前向钩子，我们首先定义下面的工厂函数，它返回一个函数对象，我们将使用它作为钩子:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="1a25" class="kb kc hi jx b fi kd ke l kf kg">activation = {} # dictionary to store the activation of a layer</span><span id="4215" class="kb kc hi jx b fi kh ke l kf kg">def create_hook(name):<br/> def hook(m, i, o):<br/>   # copy the output of the given layer<br/>   activation[name] = o<br/> <br/> return hook</span></pre><p id="6fbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在注册挂钩:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="5116" class="kb kc hi jx b fi kd ke l kf kg"># register a forward hook for layer inception4a<br/>model.inception4a.register_forward_hook(create_hook(‘4a’))</span></pre><p id="2b1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，PyTorch上的预训练模型要求输入图像“必须加载到[0，1]的范围内，然后使用<code class="du ki kj kk jx b">mean = [0.485, 0.456, 0.406]</code>和<code class="du ki kj kk jx b">std = [0.229, 0.224, 0.225]</code>进行归一化”。因此，我们将在图像上定义以下转换:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="b116" class="kb kc hi jx b fi kd ke l kf kg"># normalize the input image to have appropriate mean and standard deviation as specified by pytorch</span><span id="25db" class="kb kc hi jx b fi kh ke l kf kg">from torchvision import transforms</span><span id="7471" class="kb kc hi jx b fi kh ke l kf kg">normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],<br/>                                 std=[0.229, 0.224, 0.225])</span><span id="2dd0" class="kb kc hi jx b fi kh ke l kf kg"># undo the above normalization if and when the need arises </span><span id="e823" class="kb kc hi jx b fi kh ke l kf kg">denormalize = transforms.Normalize(mean = [-0.485/0.229, -0.456/0.224, -0.406/0.225], std = [1/0.229, 1/0.224, 1/0.225] )</span></pre><p id="d92f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们定义一个函数来生成由随机初始化的像素组成的图像。为了允许在反向传播期间调整图像，我们必须将图像的“requires_grad_”标志设置为真。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="bc33" class="kb kc hi jx b fi kd ke l kf kg">import numpy as np</span><span id="258c" class="kb kc hi jx b fi kh ke l kf kg">Height = 28<br/>Width = 28</span><span id="07ec" class="kb kc hi jx b fi kh ke l kf kg"># generate a numpy array with random values<br/>img = np.single(np.random.uniform(0,1, (3, Height, Width)))</span><span id="9d3f" class="kb kc hi jx b fi kh ke l kf kg"># convert to a torch tensor, normalize, set the requires_grad_ flag<br/>im_tensor = normalize(torch.from_numpy(img)).requires_grad_(True)</span></pre><p id="6d56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们还定义一个函数来反规格化图像，并将颜色通道移动到最后的维度，以便使用matplotlib的imshow显示它。这在使用open-cv的resize方法调整图像大小时也很方便。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="a439" class="kb kc hi jx b fi kd ke l kf kg"># function to massage img_tensor for using as input to plt.imshow()<br/>def image_converter(im):<br/>    <br/>    # move the image to cpu<br/>    im_copy = im.cpu()<br/>    <br/>    # for plt.imshow() the channel-dimension is the last<br/>    # therefore use transpose to permute axes<br/>    im_copy = denormalize(im_copy.clone().detach()).numpy()<br/>    im_copy = im_copy.transpose(1,2,0)<br/>    <br/>    # clip negative values as plt.imshow() only accepts <br/>    # floating values in range [0,1] and integers in range [0,255]<br/>    im_copy = im_copy.clip(0, 1) <br/>    <br/>    return im_copy</span></pre><p id="53bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们之前提到的，我们希望惩罚图像中像素值的任何急剧变化，即我们将惩罚图像中像素值的x和y导数。这可以通过用<a class="ae jd" href="https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/sobel_derivatives/sobel_derivatives.html" rel="noopener ugc nofollow" target="_blank">索贝尔滤镜</a>或<a class="ae jd" href="https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=scharr#scharr" rel="noopener ugc nofollow" target="_blank">沙尔滤镜</a>创建卷积层来完成。我们可以定义一个卷积层，它可以接受以下任何一种滤波器:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="cc16" class="kb kc hi jx b fi kd ke l kf kg">import torch.nn as nn</span><span id="8237" class="kb kc hi jx b fi kh ke l kf kg"># class to compute image gradients in pytorch<br/>class RGBgradients(nn.Module):<br/>    def __init__(self, weight): # weight is a numpy array<br/>        super().__init__()<br/>        k_height, k_width = weight.shape[1:]<br/>        # assuming that the height and width of the kernel are always odd numbers<br/>        padding_x = int((k_height-1)/2)<br/>        padding_y = int((k_width-1)/2)<br/>        <br/>        # convolutional layer with 3 in_channels and 6 out_channels <br/>        # the 3 in_channels are the color channels of the image<br/>        # for each in_channel we have 2 out_channels corresponding to the x and the y gradients<br/>        self.conv = nn.Conv2d(3, 6, (k_height, k_width), bias = False, <br/>                              padding = (padding_x, padding_y) )<br/>        # initialize the weights of the convolutional layer to be the one provided<br/>        # the weights correspond to the x/y filter for the channel in question and zeros for other channels<br/>        weight1x = np.array([weight[0], <br/>                             np.zeros((k_height, k_width)), <br/>                             np.zeros((k_height, k_width))]) # x-derivative for 1st in_channel<br/>        <br/>        weight1y = np.array([weight[1], <br/>                             np.zeros((k_height, k_width)), <br/>                             np.zeros((k_height, k_width))]) # y-derivative for 1st in_channel<br/>        <br/>        weight2x = np.array([np.zeros((k_height, k_width)),<br/>                             weight[0],<br/>                             np.zeros((k_height, k_width))]) # x-derivative for 2nd in_channel<br/>        <br/>        weight2y = np.array([np.zeros((k_height, k_width)), <br/>                             weight[1],<br/>                             np.zeros((k_height, k_width))]) # y-derivative for 2nd in_channel<br/>        <br/>        <br/>        weight3x = np.array([np.zeros((k_height, k_width)),<br/>                             np.zeros((k_height, k_width)),<br/>                             weight[0]]) # x-derivative for 3rd in_channel<br/>        <br/>        weight3y = np.array([np.zeros((k_height, k_width)),<br/>                             np.zeros((k_height, k_width)), <br/>                             weight[1]]) # y-derivative for 3rd in_channel<br/>        <br/>        weight_final = torch.from_numpy(np.array([          weight1x, weight1y, <br/>weight2x, weight2y,<br/>weight3x, weight3y])).type(torch.FloatTensor)<br/>        <br/>        if self.conv.weight.shape == weight_final.shape:<br/>            self.conv.weight = nn.Parameter(weight_final)<br/>            self.conv.weight.requires_grad_(False)<br/>        else:<br/>            print('Error: The shape of the given weights is not correct')<br/>    <br/>    # Note that a second way to define the conv. layer here would be to pass group = 3 when calling torch.nn.Conv2d<br/>    <br/>    def forward(self, x):<br/>        return self.conv(x)</span></pre><p id="f720" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">事实证明，对于3 x 3内核，Scharr滤波器优于Sobel滤波器，因此我们将使用Scharr滤波器:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="e440" class="kb kc hi jx b fi kd ke l kf kg"># Scharr Filters</span><span id="eed5" class="kb kc hi jx b fi kh ke l kf kg">filter_x = np.array([[-3, 0, 3], <br/>                     [-10, 0, 10],<br/>                     [-3, 0, 3]])</span><span id="68fa" class="kb kc hi jx b fi kh ke l kf kg">filter_y = filter_x.T<br/>grad_filters = np.array([filter_x, filter_y])</span></pre><p id="5ab5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们创建一个上面定义的卷积层的实例，把它传递给Scharr过滤器。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="2f68" class="kb kc hi jx b fi kd ke l kf kg">gradLayer = RGBgradients(grad_filters)</span></pre><p id="bfa4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们也定义一个函数，它使用上面定义的gradLayer来计算输入图像的x和y导数，并返回它们的均方根值。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="b511" class="kb kc hi jx b fi kd ke l kf kg"># function to compute gradient loss of an image </span><span id="e93e" class="kb kc hi jx b fi kh ke l kf kg">def grad_loss(img, beta = 1, device = 'cpu'):<br/>    <br/>    # move the gradLayer to cuda<br/>    gradLayer.to(device)</span><span id="6167" class="kb kc hi jx b fi kh ke l kf kg">    gradSq = gradLayer(img.unsqueeze(0))**2<br/>    <br/>    grad_loss = torch.pow(gradSq.mean(), beta/2)<br/>    <br/>    return grad_loss</span></pre><p id="6a8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，让我们把所有东西都搬到GPU上。如果你没有GPU或者你想在你的cpu上进行计算，你可以跳过下面的步骤。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="cb31" class="kb kc hi jx b fi kd ke l kf kg">device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/>print('Calculations being executed on {}'.format(device))</span><span id="b2ac" class="kb kc hi jx b fi kh ke l kf kg">model.to(device)<br/>img_tensor = im_tensor.to(device)</span></pre><p id="ff6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们之前提到的，我们也会慢慢提升形象。我们将使用opencv的resize()方法来实现这一点(如果您愿意，也可以使用<a class="ae jd" href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Resize" rel="noopener ugc nofollow" target="_blank">torch vision . transforms . resize()</a>)。因此，我们需要导入cv2。我们还需要matplotlib.pyplot和torch.optim。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="7b09" class="kb kc hi jx b fi kd ke l kf kg">import cv2<br/>from torch import optim<br/>import sys<br/>import matplotlib.pyplot as plt</span></pre><p id="7651" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在准备调整我们的随机图像，使其最大化我们卷积层所选节点的输出。出于本文的目的，让我选择索引值为225的节点。</p><p id="11bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将优化图像20次迭代，然后以1.05的因子重新缩放。我将重复这个循环45次，以得到最终尺寸为249 x 249的图像。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="08bb" class="kb kc hi jx b fi kd ke l kf kg">unit_idx = 225 # the neuron to visualize<br/>act_wt = 0.5 # factor by which to weigh the activation relative to the regulizer terms</span><span id="c11b" class="kb kc hi jx b fi kh ke l kf kg">upscaling_steps = 45 # no. of times to upscale<br/>upscaling_factor = 1.05<br/>optim_steps = 20# no. of times to optimize an input image before upscaling</span></pre><p id="fb15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将运行两个嵌套循环来优化我们的图像，然后按如下方式放大它:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="e431" class="kb kc hi jx b fi kd ke l kf kg">model.eval()<br/>for mag_epoch in range(upscaling_steps+1):<br/>    optimizer = optim.Adam([img_tensor], lr = 0.4)<br/>    <br/>    for opt_epoch in range(optim_steps):<br/>        optimizer.zero_grad()<br/>        model(img_tensor.unsqueeze(0))<br/>        layer_out = activation['4a']<br/>        rms = torch.pow((layer_out[0, unit_idx]**2).mean(), 0.5)<br/>        # terminate if rms is nan<br/>        if torch.isnan(rms):<br/>            print('Error: rms was Nan; Terminating ...')<br/>            sys.exit()<br/>        <br/>        # pixel intensity<br/>        pxl_inty = torch.pow((img_tensor**2).mean(), 0.5)<br/>        # terminate if pxl_inty is nan<br/>        if torch.isnan(pxl_inty):<br/>            print('Error: Pixel Intensity was Nan; Terminating ...')<br/>            sys.exit()<br/>            <br/>        # image gradients<br/>        im_grd = grad_loss(img_tensor, beta = 1, device = device)<br/>        # terminate is im_grd is nan<br/>        if torch.isnan(im_grd):<br/>            print('Error: image gradients were Nan; Terminating ...')<br/>            sys.exit()<br/>        <br/>        loss = -act_wt*rms + pxl_inty + im_grd        <br/>        # print activation at the beginning of each mag_epoch<br/>        if opt_epoch == 0:<br/>            print('begin mag_epoch {}, activation: {}'.format(mag_epoch, rms))<br/>        loss.backward()<br/>        optimizer.step()<br/>        <br/>    # view the result of optimising the image<br/>    print('end mag_epoch: {}, activation: {}'.format(mag_epoch, rms))<br/>    img = image_converter(img_tensor)    <br/>    plt.imshow(img)<br/>    plt.title('image at the end of mag_epoch: {}'.format(mag_epoch))<br/>    plt.show()<br/>    <br/>    img = cv2.resize(img, dsize = (0,0), <br/>                     fx = upscaling_factor, fy = upscaling_factor).transpose(2,0,1) # scale up and move the batch axis to be the first<br/>    img_tensor = normalize(torch.from_numpy(img)).to(device).requires_grad_(True)</span></pre><p id="cc55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的代码片段中，我们为损失函数定义了三种贡献:</p><ol class=""><li id="28c7" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">rms:这是我们选择的卷积单元产生的输出张量中元素的均方根值。我们希望最大化这一点。</li><li id="5b82" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">pxl_inty:这是我们图像中像素值的均方根值。出于正则化的目的，我们希望惩罚大像素值，从而保持pxl_inty较低。</li><li id="4f00" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">im_grd:这是像素值的x和y导数的均方根值。通过保持低电平，我们可以确保像素值不会发生急剧变化。</li></ol><p id="88db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，损失函数由下式给出</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="806c" class="kb kc hi jx b fi kd ke l kf kg">loss = -act_wt*rms + pxl_inty + im_grd</span></pre><p id="ea3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中‘act _ wt’是我们分配给‘rms’相对于pxl_inty和im_grd的权重。因此，与图像中的像素强度和梯度相比，改变act_wt改变了单元激活的重要性。我们还检查在迭代的任何一点，这些是否成为nan，在这种情况下，我们终止代码。</p><p id="08e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，在外部循环中，即放大循环中，每次我们调整图像尺寸时，我们都生成新的img_tensor，因此，我们必须在每个放大时期的开始重建我们的优化器。</p><p id="946b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">瞧啊。我们完了。执行上面的循环会生成以下图像:</p><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kl"><img src="../Images/edca18b03cf610fd5a6cbfb8b56bdecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oF5cLsqxUWBGhn7fo8F0UA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">左上至右下:第0、第9、第18、第27、第36和第45倍放大结束时的图像</figcaption></figure><p id="315e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我没有偏见的话，那么这样产生的最终图像似乎包含了很多眼睛一样的特征。因此，我们可以推断，这里讨论的卷积单元必须在输入图像中寻找“眼睛”。看看其他卷积节点最终学到了什么将会很有趣。以下是每层中前10个最活跃单元的可视化效果:</p><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/3da79c52ff8da65aef2eaf5a62f7e79a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M1HiWcAu5q7C62UrBl0FTg.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/cfa679328916796b3cd5991b1401acd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*94Yw7quhXdD8Mxm6quE-XQ.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/033e9476f2e30be5826b0f471378e772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*smH55tJ8hHskjzktr88zJQ.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/9f58eb18c522fce8ef17bf2bd0a8ba75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gu-8cx4gjtd_kWpjtiHSRg.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/ee2ffa7ad53642f04edc4655eac171b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kobsTCX7EvEw1FfwiSxhFg.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/d29d34b700ccf3fa9eb39aa5203ffaaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DxyncEpHmKfCZPqnvuss3w.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/9e2b7d98262bcbc6d423c60be8aeefb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y6GPhiXOjZZnHsL84ODWuA.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/0d1914207e662d50326dcf1143b6e2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZX653-cNFXXH7WLx5SJhYg.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/5912e2cac87bd0c26eb4af3cb7796153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHoOlUePRONlS-FXuxY-8w.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/7fd3d25003b1cdeb03b69053b2079549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EFiSse8e_HDQAHhvcGh9gw.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ky"><img src="../Images/3876ebbc9c50c51010502f227fd88c54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uGzwGx49GDT03ydxf9Q80w.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/9e35269969fe4da27371dff7d511d47a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6ANyUXT9NaIdajGi6USzg.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/479659902efa186d84a9812a8a01ddc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fh3NB1YQCtbNrcnSKsu_WQ.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/c5922c96e3ff9581c987f2e90f883610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THCmRe9rC02YAOg19HLMlw.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/9ea7c9a05965bc54cbac72af745a56ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hGAvFIFfxg5wGkJifoZJig.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kz"><img src="../Images/2994d678c86eb0cbdf95d742c17aded0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VEo71-qOPiLTDJrVgQlt8Q.png"/></div></div></figure><p id="1bf6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看起来CNN中的大多数单位最终都学习了不同种类的纹理。偶尔，有一些单位会学习面部特征，比如眼睛等等。我不知道为什么，但对我来说，它看起来像在第4a层-第4e层和第5a层的单位有最可辨别的特征。在许多地方都有人认为，CNN的高层最终会学习用于训练的图像的内容，而低层则相反，它们最终会学习图像的纹理。从这个角度来看，我希望最后一层，即inception5b能够产生包含高度明显的人类可解释组件的图像。然而，对于主要包含非常高频率模式的相应图像来说，情况似乎并非如此。也许，我应该尝试一个大于3 x 3滤镜的渐变图层。Mahendran和Vedaldi也主张使用抖动来规范这些高频模式的出现。这是我没有包括在内的东西，但尝试一下会很有趣。</p><p id="0c15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望你会和我一样玩得开心。😃</p></div></div>    
</body>
</html>