<html>
<head>
<title>Building An MLP Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建MLP神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-an-mlp-neural-network-53f946c1d804?source=collection_archive---------24-----------------------#2020-07-06">https://medium.com/analytics-vidhya/building-an-mlp-neural-network-53f946c1d804?source=collection_archive---------24-----------------------#2020-07-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/18fcbc39db6ff7318c97967d3291e30b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KUAh4mY67XT-nqaR"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@maximalfocus?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Maximalfocus </a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h1 id="a5b3" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">建立MLP神经网络的步骤包括:</h1><h1 id="a19f" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">1.预处理数据:</h1><ul class=""><li id="0d96" class="jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">根据目标的需要，执行数据清理，如重复数据删除，删除不必要的元素，如URL等。(如果处理文本数据)</li><li id="0d98" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">将文本数据转换成数字向量。(如果处理文本数据)</li><li id="747a" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">不要忘记标准化数据。</li></ul><h1 id="ab4e" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">2.选择架构:</h1><p id="37fe" class="pw-post-body-paragraph kq kr hi jv b jw jx ks kt jy jz ku kv ka kw kx ky kc kz la lb ke lc ld le kg hb bi translated">根据目标的需要:</p><ul class=""><li id="aacc" class="jt ju hi jv b jw lf jy lg ka lh kc li ke lj kg kh ki kj kk bi translated">选择要构建的MLP的适当层数。</li><li id="a7c3" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">在每层中选择适当数量的神经元。</li><li id="ccba" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">可以通过对不同数量的层/神经元执行超参数调谐来选择适当数量的层/神经元。</li></ul><h1 id="63b4" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">3.重量初始化:</h1><p id="67fa" class="pw-post-body-paragraph kq kr hi jv b jw jx ks kt jy jz ku kv ka kw kx ky kc kz la lb ke lc ld le kg hb bi translated">选择适当的(使用超参数调整)随机权重初始化方案，例如:</p><ul class=""><li id="4f0f" class="jt ju hi jv b jw lf jy lg ka lh kc li ke lj kg kh ki kj kk bi translated">从所有权重= 0开始(很少使用)</li><li id="7980" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">统一初始化(适用于Sigmoid激活功能)</li><li id="57e4" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">Xavier/Glorot初始化:</li></ul><blockquote class="lk"><p id="3284" class="ll lm hi bd ln lo lp lq lr ls lt kg dx translated">-制服</p><p id="2189" class="ll lm hi bd ln lo lu lv lw lx ly kg dx translated">-正常</p></blockquote><ul class=""><li id="635f" class="jt ju hi jv b jw lz jy ma ka mb kc mc ke md kg kh ki kj kk bi translated">He初始化</li></ul><blockquote class="lk"><p id="e11b" class="ll lm hi bd ln lo lp lq lr ls lt kg dx translated">-制服</p><p id="69c1" class="ll lm hi bd ln lo lu lv lw lx ly kg dx translated">-正常</p></blockquote><ul class=""><li id="4b15" class="jt ju hi jv b jw lz jy ma ka mb kc mc ke md kg kh ki kj kk bi translated">更多权重初始化方案，查看Keras文档<a class="ae iu" href="https://keras.io/api/layers/initializers/" rel="noopener ugc nofollow" target="_blank">此处</a>。</li></ul><h1 id="b105" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">4.选择激活功能:</h1><p id="3594" class="pw-post-body-paragraph kq kr hi jv b jw jx ks kt jy jz ku kv ka kw kx ky kc kz la lb ke lc ld le kg hb bi translated">选择适当的(使用超参数调谐)激活功能，例如:</p><ul class=""><li id="0f15" class="jt ju hi jv b jw lf jy lg ka lh kc li ke lj kg kh ki kj kk bi translated">Sigmoid(由于<strong class="jv hj">“消失梯度”</strong>问题，应避免使用)</li><li id="c808" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">Tanh(由于<strong class="jv hj">“消失梯度</strong>”问题，应避免使用)</li><li id="e8ac" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">ReLu(回归任务的首选)</li><li id="6cf2" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">Softmax(分类任务的首选)</li><li id="02a0" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">更多激活功能，查看Keras文档<a class="ae iu" href="https://keras.io/api/layers/activations/" rel="noopener ugc nofollow" target="_blank">此处</a>。</li></ul><h1 id="3cbf" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">5.选择优化器:</h1><ul class=""><li id="2b1e" class="jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">由于曲线中的“鞍点”问题，我们通常避免在深度学习(不像机器学习)中使用SGD进行优化，SGD无法正确处理这些问题。</li></ul><blockquote class="lk"><p id="d9d1" class="ll lm hi bd ln lo lp lq lr ls lt kg dx translated">鞍点是曲线上斜率为0的点，通常不是曲线上的局部极值。</p></blockquote><ul class=""><li id="c9d0" class="jt ju hi jv b jw lz jy ma ka mb kc mc ke md kg kh ki kj kk bi translated">因此，我们选择其他优化技术，例如:</li></ul><blockquote class="lk"><p id="c5df" class="ll lm hi bd ln lo lp lq lr ls lt kg dx translated">亚当</p><p id="a59b" class="ll lm hi bd ln lo lu lv lw lx ly kg dx translated">阿达德尔塔</p><p id="34b0" class="ll lm hi bd ln lo lu lv lw lx ly kg dx translated">阿达格勒</p><p id="9890" class="ll lm hi bd ln lo lu lv lw lx ly kg dx translated">-阿达马克斯</p></blockquote><ul class=""><li id="d261" class="jt ju hi jv b jw lz jy ma ka mb kc mc ke md kg kh ki kj kk bi translated">更多优化，查看Keras文档<a class="ae iu" href="https://keras.io/api/optimizers/" rel="noopener ugc nofollow" target="_blank">这里</a>。</li></ul><h1 id="37c3" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">6.使用批处理规范化(可选):</h1><ul class=""><li id="d567" class="jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">通常，批量标准化用于非常深的分层MLP的情况。</li><li id="60d6" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">MLP初始层中权重值的微小变化可能会导致深层的巨大变化。</li><li id="590f" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">因此，在小变化导致大变化之前，我们批量标准化MLP中的深层。</li></ul><h1 id="6bce" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">7.使用辍学(可选):</h1><ul class=""><li id="a594" class="jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">辍学是指在一个层中放弃一定数量的神经元，这导致了MLP的正则化。</li><li id="2e0c" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">一定数量的神经元随机关闭，因此它们的输出不用于模型建立。</li><li id="a1bd" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">正则化用于“偏差-方差权衡”，指的是“过拟合”或“欠拟合”模型。</li></ul><h1 id="54f5" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">8.选择损失函数:</h1><p id="8792" class="pw-post-body-paragraph kq kr hi jv b jw jx ks kt jy jz ku kv ka kw kx ky kc kz la lb ke lc ld le kg hb bi translated">用于计算模型在训练期间应该寻求最小化的数量的一些损失函数是:</p><ul class=""><li id="3db1" class="jt ju hi jv b jw lf jy lg ka lh kc li ke lj kg kh ki kj kk bi translated">交叉熵(分类任务首选)</li><li id="3828" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">均方误差(回归任务的首选)</li><li id="3311" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">绝对平均误差</li><li id="4542" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">更多损失函数，查看Keras文档<a class="ae iu" href="https://keras.io/api/losses/" rel="noopener ugc nofollow" target="_blank">此处</a>。</li></ul><h1 id="7584" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">9.梯度监控和削波(如果需要):</h1><ul class=""><li id="fd4a" class="jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">一些激活函数导致“消失梯度”问题和“爆炸梯度”问题。</li><li id="d4b3" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">消失梯度问题指的是梯度变得如此之小，这是由于梯度的许多小值乘积，以至于它们对于优化目的变得无效，因为权重从不收敛。</li></ul><blockquote class="lk"><p id="0392" class="ll lm hi bd ln lo lp lq lr ls lt kg dx translated">使用ReLu激活函数解决渐变消失问题。</p></blockquote><ul class=""><li id="82a2" class="jt ju hi jv b jw lz jy ma ka mb kc mc ke md kg kh ki kj kk bi translated">爆炸梯度问题指的是梯度变得非常大，这是由于大量的大值梯度乘积，由于权重从不收敛，它们对于优化目的变得无效。</li></ul><blockquote class="lk"><p id="cda7" class="ll lm hi bd ln lo lp lq lr ls lt kg dx translated">使用L2范数裁剪解决爆炸梯度问题。</p></blockquote><ul class=""><li id="5d70" class="jt ju hi jv b jw lz jy ma ka mb kc mc ke md kg kh ki kj kk bi translated">这两个问题经常发生在深层神经网络中的Sigmoid和Tanh激活函数。</li></ul><h1 id="f550" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">10.模型评估:</h1><ul class=""><li id="55a3" class="jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">找出所建模型的精度和损耗。</li><li id="ccf5" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">绘制“测试损失与历元数”图，这可以提供关于模型每个历元的错误率的见解。</li></ul></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><p id="74a2" class="pw-post-body-paragraph kq kr hi jv b jw lf ks kt jy lg ku kv ka ml kx ky kc mm la lb ke mn ld le kg hb bi translated"><a class="ae iu" href="https://github.com/deveshSingh06/" rel="noopener ugc nofollow" target="_blank">代码实现，跟着我上GitHub </a>。</p></div></div>    
</body>
</html>