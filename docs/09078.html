<html>
<head>
<title>Big Bird: Transformers for Longer Sequences — 2020</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大鸟:更长序列的变形金刚——2020</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/big-bird-transformers-for-longer-sequences-2020-8adac1a01448?source=collection_archive---------17-----------------------#2020-08-24">https://medium.com/analytics-vidhya/big-bird-transformers-for-longer-sequences-2020-8adac1a01448?source=collection_archive---------17-----------------------#2020-08-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5ec9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总结。通过稀疏注意力改进 Transformer 来利用更长的序列。</p><p id="fb9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原文(和更好的乳胶)位于:<a class="ae jd" href="https://atakanokan.com/papersummaries/2020-big-bird-transformers-for-longer-sequences/" rel="noopener ugc nofollow" target="_blank">https://atakanokan . com/paper summaries/2020-big-bird-transformers-for-longer-sequences/</a></p><h1 id="6865" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">信息</h1><p id="2491" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">链接:<a class="ae jd" href="https://arxiv.org/abs/2007.14062" rel="noopener ugc nofollow" target="_blank"> Arxiv </a></p><p id="1780" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文作者:谷歌研究</p><p id="4c5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇论文为什么重要？:解决了传统注意机制的二次记忆依赖性，提出了可以处理较长序列的稀疏注意。</p><p id="73b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码:NA</p><h1 id="336b" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">摘要</h1><p id="576f" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">本文通过引入稀疏注意机制，解决了 Transformer 模型使用完全注意的局限性，稀疏注意机制使用与序列长度成线性比例的内存。</p><h1 id="0faa" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">投入</h1><p id="8f12" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">从\(\textbf{x}\)开始，这是标记化的输入序列(通过经典的空格分隔、字节对编码或单词块等)。):</p><p id="2912" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">\[\textbf{x} = (x_{1}，。。。，x_{n})\]</p><p id="15b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中\(x_{1}\)对应于第一个令牌。\(n\)是序列长度。</p><h1 id="b5f9" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">稀疏的注意力</h1><p id="92a4" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">\(D\)是一个有向图，其顶点集为\([n] = {1，…，n}\)，其中有向边表示包含注意机制的内积。\(N(i)\)表示\(D)中节点\(i\)的外邻居集。</p><p id="ddeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">\(i^{th}\)注意机制的输出向量是:</p><p id="ad19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">\[Attn _ { d }(x)_ { I } = \ textbf { x } _ { I }+\sum_{h=1}^{h} \西格玛(q_{h}(\textbf{x}_{i}k_{h}(\textbf{x}_{n(i)})^{t})\ cdot v _ { h }(\ textbf { x } _ { n(I)}))\]</p><p id="d151" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中\(Q_h\)是查询函数，而\(K_{h}\)是键函数，而\(V_{h}\)是值函数。\(\sigma\)是一个评分函数(softmax 或 hardmax)。\(H\)是多头注意力模块中的头数。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/c04d86c91b46a85e7b344d4f8c28ee9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*btMf0WWoZfb1EKq7.png"/></div></div></figure><p id="f885" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Matrix \(A\) (attention matrix)是一个二进制值\(n\)x\(n\)矩阵，其中如果 query \(i\)关注 key \(j\)，则矩阵(A(i，j)=1\，否则为零。当 A 全是 1 时，它就是传统的完全注意机制。由于每个令牌都关心其他令牌，所以内存需求是二次的。</p><p id="fe1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">稀疏注意力由以下三个部分的合并组成(如图 1 所示):</p><p id="7b36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机关注</strong></p><p id="7b2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个查询涉及超过\(r\)个随机键。数学上，\(A(i，\cdot) = 1\)表示\(r\)个随机选择的密钥。</p><p id="2d07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">(滑动)窗口注意</strong></p><p id="708b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在 NLP 数据中存在大量的引用局部性，即关于一个令牌的信息可以从它的相邻令牌中导出。为了利用这一点，BigBird 使用了宽度为(w)的滑动窗口注意力。位置\(i\)处的查询涉及从\(i — w/2\)到\(i + w/2\)键。数学上，\(A(i，i-w/2:i+w/2) = 1\)。</p><p id="b99c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">全球关注</strong></p><p id="b4af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">全局令牌是关注序列中所有令牌以及所有令牌关注谁的令牌。BigBird 以两种方式利用这个全局令牌概念:</p><ul class=""><li id="e5e3" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated">BIGBIRD-ITC(内部转换器构造):使一些现有的令牌“全局化”，并使它们参与整个输入序列。</li><li id="481e" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">BIGBIRD-ETC(扩展的变压器构造):添加\(g\)附加的“全局”令牌(例如 CLS)来处理所有现有的令牌。这将矩阵\(A\)的列和行扩展了\(g\)行/列。</li></ul><h1 id="0abf" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">结果</h1><p id="8c5d" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">稀疏注意力使得该机制能够关注 8 倍长的序列。可以使用梯度检查点来处理 8 倍以上的长序列。以下是 NLP 任务的结果。省略了基因组学相关的结果。</p><h2 id="792c" class="lh jf hi bd jg li lj lk jk ll lm ln jo iq lo lp js iu lq lr jw iy ls lt ka lu bi translated">预训练和 MLM</h2><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lv"><img src="../Images/911b4886314dac00f23143ad24295eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/0*p5tkHXR0oFykk0FZ.png"/></div></figure><h2 id="4e82" class="lh jf hi bd jg li lj lk jk ll lm ln jo iq lo lp js iu lq lr jw iy ls lt ka lu bi translated">仅编码器任务</h2><p id="bd9d" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated"><strong class="ih hj">问题回答</strong></p><p id="8654" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BigBird-ETC 优于所有其他型号。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lw"><img src="../Images/f085f5ade77ab0d69433b6250cbe57ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3g_szduAdm7RJdbO.png"/></div></div></figure><p id="8b31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">文档分类</strong></p><p id="d33c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">提高 SotA % 5 点。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lx"><img src="../Images/caf699a302f320f01a858607b0acb8f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pJQ9wDEBNInLpjOc.png"/></div></div></figure><h2 id="9dac" class="lh jf hi bd jg li lj lk jk ll lm ln jo iq lo lp js iu lq lr jw iy ls lt ka lu bi translated">编码器-解码器任务</h2><p id="4a87" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">当也使用飞马座预训练时:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ly"><img src="../Images/770657bc17ff11b0db50c513d9f07e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lLV9QlRb6gvyOcP9.png"/></div></div></figure></div><div class="ab cl lz ma gp mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hb hc hd he hf"><p id="65ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="mg">原载于 2020 年 8 月 24 日 https://atakanokan.com</em><a class="ae jd" href="https://atakanokan.com/papersummaries/2020-big-bird-transformers-for-longer-sequences/" rel="noopener ugc nofollow" target="_blank"><em class="mg"/></a><em class="mg">。</em></p></div></div>    
</body>
</html>