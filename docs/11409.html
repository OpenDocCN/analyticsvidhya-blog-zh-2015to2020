<html>
<head>
<title>Understanding the Vision Transformer and Counting Its Parameters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解视觉转换器并计算其参数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-vision-transformer-and-counting-its-parameters-988a4ea2b8f3?source=collection_archive---------0-----------------------#2020-12-02">https://medium.com/analytics-vidhya/understanding-the-vision-transformer-and-counting-its-parameters-988a4ea2b8f3?source=collection_archive---------0-----------------------#2020-12-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/84be4301f60d7e051662c567e7306f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pxLTgDV89uNmGhGZ"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">西蒙·米加吉在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="941a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本帖中，我将分享我对Vision Transformer架构的理解。本帖中的所有图片都是原创内容，基于论文和其他教程中的知识，在适当的地方会提到这些内容。</p><p id="7cc2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> Transformer architectures </strong>自2016年提出以来，一直是自然语言处理(NLP)任务的重大突破。谷歌的<a class="ae iu" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">伯特</a>和开放人工智能的<a class="ae iu" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a> / <a class="ae iu" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>架构已经成为各种任务的最先进解决方案，包括语言建模、文本摘要和问题回答。</p><p id="3d8b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其目的是证明递归神经网络可以被完全取代，并且可以仅使用注意力机制来开发解决方案——因此在<a class="ae iu" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank"> Transformer论文</a>标题中出现双关语。详细描述最初的Transformer架构的工作方式超出了本文的范围，但是有大量可靠的教程<a class="ae iu" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">和视频</a>解释了它的工作方式。</p><p id="1a0f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇文章的重点是介绍视觉转换器(ViT)架构的概述，这是在为谷歌提交的新<a class="ae iu" href="https://openreview.net/pdf?id=YicbFdNTTy" rel="noopener ugc nofollow" target="_blank">论文</a>中提出的，该论文已提交ICLR 2021审查。尽管之前有人尝试过使用注意力机制来完成计算机视觉任务(<a class="ae iu" href="https://papers.nips.cc/paper/2019/file/3416a75f4cea9109507cacd8e2f2aefc-Paper.pdf" rel="noopener ugc nofollow" target="_blank">这个</a>、<a class="ae iu" href="https://arxiv.org/pdf/1904.10509.pdf" rel="noopener ugc nofollow" target="_blank">这个</a>或<a class="ae iu" href="https://arxiv.org/pdf/1906.02634.pdf" rel="noopener ugc nofollow" target="_blank">这个</a>)，但从可扩展性和效率的角度来看，ViT都是最有前途的架构。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jt"><img src="../Images/47d129f7cf089c7f1c489ff77f15afee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*etrfupovfQO4JbBm26Y_cw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">视觉转换建筑</figcaption></figure><p id="1ca2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该架构包含3个主要组件。</p><ol class=""><li id="077f" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated">补丁嵌入。</li><li id="570f" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated">通过堆叠变压器编码器提取特征。</li><li id="aceb" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated">分类头。</li></ol><p id="b404" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">每个组件将在接下来的段落中详细介绍，重点是转换和可训练参数。有些张量的第一维是<strong class="ix hj"> <em class="km"> b </em> </strong>，代表批量大小，但为了简单起见，在注释中会忽略这个和广播操作。</p><h1 id="5d9a" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">补丁嵌入</h1><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/c8dcae124f14308ac43a68d3d36a2ed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rXQ_lZn9-4X-UGkagsO0Ng.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">补丁嵌入</figcaption></figure><p id="36fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在第一步中，形状为<strong class="ix hj"> <em class="km">(高度、宽度、通道)</em> </strong>的输入图像被嵌入到形状为<strong class="ix hj"> <em class="km"> (n+1，d)</em></strong><em class="km"/>的特征向量中，随后进行一系列变换。这对应于论文中的等式(1):</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/3fbf762de68146d82752634c31d55d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oqOzUEXMYvp7groZvs2Gtg.png"/></div></div></figure><ol class=""><li id="de44" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js kd ke kf kg bi translated">图像被分割成形状为<strong class="ix hj"> <em class="km"> (p，p，c) </em> </strong>、<em class="km"> </em>的<em class="km"> n </em>个正方形小块，其中<em class="km"> p </em>是预定义的参数，按照光栅顺序(从左到右，从上到下)。</li><li id="046f" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated">面片被展平，产生形状为<strong class="ix hj"> <em class="km"> (1，p *c) </em> </strong> <em class="km">的<strong class="ix hj"><em class="km"/></strong>条线矢量。</em></li><li id="9c73" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated">展平的面片与形状为<strong class="ix hj"> (p *c，d) </strong>的<strong class="ix hj">可训练</strong>嵌入张量相乘，学习将每个展平的面片线性投影到尺寸<strong class="ix hj"> <em class="km"> d </em> </strong>。该尺寸<em class="km"> d </em>在架构中是恒定的，并且在大多数组件中使用。结果是<strong class="ix hj"> <em class="km"> n </em> </strong> <em class="km"> </em>形状的嵌片<strong class="ix hj"> <em class="km"> (1，d)。</em> </strong></li><li id="d23b" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated">形状的可学习的<em class="km">【cls】</em>记号<strong class="ix hj"> <em class="km"> (1，d) </em> </strong>被预先加到补丁嵌入的序列上。这个令牌的思想来自于BERT论文，其中只有对应于这个令牌的最后一个表示(transformer L的输出)被馈送通过分类层。直观地说，这代表了补丁表示的集合。</li><li id="361f" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js kd ke kf kg bi translated">将具有相同形状的<strong class="ix hj"/><strong class="ix hj">【eₚₒₛ】</strong><strong class="ix hj"><em class="km">(n+1，d)</em></strong><em class="km"/>可训练的位置嵌入张量添加到连接的投影序列中。该张量学习每个补片的1D位置信息，以便在序列中添加每个补片的空间表示。</li></ol><p id="4da4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结果，z₀，是堆叠变压器编码器的第一个输入。L个堆叠编码器代表该架构的第二个组件。每个变换器将表示为<strong class="ix hj"> <em class="km"> (n+1，d) </em> </strong> <em class="km"> </em>张量的特征作为输入，并产生相同维数的输出。</p><h1 id="9568" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">变压器编码器</h1><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ln"><img src="../Images/2da316e73551fcbf8877d9bd55667b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cELt46MSsAuwk6PzvDstUQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">变压器编码器</figcaption></figure><p id="d335" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在第二步中，网络使用L个transformer编码器的堆栈，从嵌入的补丁中学习更多的抽象特征。这对应于论文中的等式(2)和(3)。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/28e7c9cd140e03c0ded356b03da99c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ldq8m57q82BMf4yPg_nsYw.png"/></div></div></figure><p id="7554" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编码器组件包含一个多头注意力(MHA)机制和一个2层MLP，其间有层规范化和残差连接。</p><p id="ceda" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">层规范化有助于稳定隐藏状态动态并减少训练时间。这是通过对每个训练示例的平均值和标准偏差进行缩放来完成的(与批处理规范相反，批处理规范是对每个特征进行缩放)。得到的特征乘以一个比例因子并加到一个移位因子上，两者都可以在训练期间学习。</p><p id="e9c0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">剩余连接提供了梯度替代路径，以解决非常深的架构中梯度消失的问题。</p><p id="44ab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该组件中的可训练重量位于MHA装置和MLP重量内。由于MLP有两层(隐藏层和输出层)，因此将有两个权重矩阵:</p><ul class=""><li id="f2e8" class="jy jz hi ix b iy iz jc jd jg ka jk kb jo kc js lp ke kf kg bi translated">Wₕ之形<strong class="ix hj"> <em class="km">(维·dₘₗₚ)</em></strong></li><li id="1f5a" class="jy jz hi ix b iy kh jc ki jg kj jk kk jo kl js lp ke kf kg bi translated">Wₒ的形状<strong class="ix hj"><em class="km">【dₘₗₚ的d】</em></strong></li></ul><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/93fead71a2f2e5fbd9cedbdc40300646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HUx39Zt-DkLMQ7ts46kk8A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">注意机制</figcaption></figure><p id="f01b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">L个堆叠变压器的每一个都包括多头注意力(MHA)步骤，对应于论文附录中的等式(5)、(6)、(7)和(8)。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/155d394f985e68b83d3473cddc877c73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tJC6RfUN2PJmWKBLCBJz0Q.png"/></div></div></figure><p id="1529" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">来自先前编码器的隐藏状态被分成k个头，产生形状为<strong class="ix hj"> (n，dₕ) </strong>的k个特征张量。根据多头注意力直觉，多头允许该机制从抽象表示的不同方面进行学习。</p><p id="ab2f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">每个乘以形状为<strong class="ix hj"> (dₕ，dₕ).)的3个可训练矩阵Qi，Ki，Vi</strong>这等同于等式5，因为在U= <strong class="ix hj"> (d，3dₕ) </strong>中，对于每个头部正好有3个矩阵，每个形状为<strong class="ix hj"> (dₕ，dₕ) </strong>。</p><p id="8894" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Qi、Ki和Vi表示输入在3个子空间中的投影。我们可以把Q中的每条线看作是我们感兴趣的面片的学习投影，而K中的线则是我们与Q进行比较的其他面片。学习V和K来表达V中的特征的重要性或权重，以计算最终的“关注度”。</p><p id="3f30" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，在每个头部上，缩放的点积注意力张量<strong class="ix hj"> (A) </strong>被计算为<strong class="ix hj"> Ki </strong>和<strong class="ix hj"> Qi </strong>矩阵之间的乘积的软最大值，用头部尺寸的平方根归一化。该矩阵中的第<em class="km"> i- </em>行是查询<em class="km"> i </em>关注度的概率分布函数，意为补丁<em class="km"> i </em>的查询与其他哪些补丁的关键字最相似。</p><p id="3f9c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自我关注是<strong class="ix hj"> A </strong>和<strong class="ix hj"> v </strong>之间的乘积，其形状为<strong class="ix hj"> <em class="km"> (n+1，dₕ).</em> </strong>行<em class="km"> i </em>列<em class="km"> j </em>的元素是a中第<em class="km"> i </em>行pdf的特征j的加权平均值。</p><p id="784f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自我关注矩阵在第二维度上连接，产生一个<strong class="ix hj"> <em class="km"> (n+1，d) </em> </strong>张量，然后通过将它与一个<strong class="ix hj"> <em class="km"> (d，d) </em> </strong> <strong class="ix hj">可训练的</strong>张量有效地相乘，使其通过单个线性层。这个线性层非常重要，因为它允许从所有头部学习特征作为集合。</p><h1 id="3077" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">分类头</h1><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/605f72e1ed3176180d6834068c11b585.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BmFosjhGmyEVBrquRm91pA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">分类头</figcaption></figure><p id="a7b9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如前所述，在分类头中只使用了最后一个表示的<strong class="ix hj"><em class="km">【cls】</em></strong>标记。对于预训练，使用2层MLP，因此有两个权重矩阵——形状为<strong class="ix hj"><em class="km">【d，dₘₗₚ】</em></strong>的Wₕ和形状为<strong class="ix hj"><em class="km">【dₘₗₚ，d】</em></strong>的Wₒ。对于微调，使用单个线性层，因此只有单个张量，形状为<strong class="ix hj"> <em class="km"> (d，n_cls) </em> </strong> <em class="km">。</em> <strong class="ix hj"> <em class="km"> </em> </strong>在每种情况下，网络的最终输出是一个形状为<strong class="ix hj"><em class="km">【1，n _ cls】</em></strong>，<strong class="ix hj"> <em class="km"> </em> </strong>的向量，其中包含与每个<strong class="ix hj"><em class="km">【n _ cls】</em></strong>类相关的概率。</p><h1 id="b356" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">可训练参数</h1><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/2a263f9cc039ebed8b08f868f35fb578.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*wiMIbC1lNjxZGpD3G52HkQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">视觉转换器参数[1]</figcaption></figure><p id="1c2f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们以ViT-Base架构为例，计算参数的数量。建筑中的可训练张量。</p><p id="5cb0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在补丁嵌入步骤中，两个嵌入矩阵占786.432个参数。</p><pre class="ju jv jw jx fd lu lv lw lx aw ly bi"><span id="2101" class="lz ko hi lv b fi ma mb l mc md">p²*c*d + (n+1)*d = 256*3*768 + 256*768 = 786.432</span></pre><p id="dba7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在编码器堆栈中，共有84.943.656个参数。</p><pre class="ju jv jw jx fd lu lv lw lx aw ly bi"><span id="713a" class="lz ko hi lv b fi ma mb l mc md">L*(k*d*3*dₕ + d*d + d*<em class="km">dₘₗₚ + dₘₗₚ*d</em>) <br/>= 12*(12*768*3*64 + 768*768 + 2*768*3072) <br/>= 12*(1.769.472 + 589.824 + 4.718.592) <br/>= 12*589.824(3+1+8)<br/>= 12*589.824*12<br/>= 84.934.656</span></pre><p id="f79a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于微调(例如，在ImageNet上)，还有其他768.000参数。</p><pre class="ju jv jw jx fd lu lv lw lx aw ly bi"><span id="9902" class="lz ko hi lv b fi ma mb l mc md">d*ncls = 768 * 1000 = 768.000</span></pre><p id="f0f8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">加上这些，我们得到大约8600万英镑，如论文中所列。</p><h1 id="1c3d" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">结果</h1><p id="0895" class="pw-post-body-paragraph iv iw hi ix b iy me ja jb jc mf je jf jg mg ji jj jk mh jm jn jo mi jq jr js hb bi translated">当在JFT-300M上进行预训练时，ViT架构在ImageNet上获得了最先进的性能。然而，作者指出，当使用小数据集进行预训练时，性能会急剧下降，并且较大的模型更适合于较大的数据集。这个问题最有可能是因为变压器网络中缺乏电感偏置。他们能够处理任何序列，而不知道它的顺序关系。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/7079a07a6278df435fb3514612904a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*6E0sMokCCygjtdAyvbe-5Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">准确度与训练前数据集[1]</figcaption></figure><h1 id="cb4a" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">结论</h1><p id="416c" class="pw-post-body-paragraph iv iw hi ix b iy me ja jb jc mf je jf jg mg ji jj jk mh jm jn jo mi jq jr js hb bi translated">对我来说，能够可视化这个架构中的转换和流程非常有帮助。希望你的情况也是如此。干杯！</p><h1 id="6782" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">参考</h1><p id="ca13" class="pw-post-body-paragraph iv iw hi ix b iy me ja jb jc mf je jf jg mg ji jj jk mh jm jn jo mi jq jr js hb bi translated">[1] A. Dostovitskiy等人。艾尔。、<a class="ae iu" href="https://arxiv.org/pdf/2010.11929.pdf" rel="noopener ugc nofollow" target="_blank">一幅图像抵得上16x16的文字:规模化图像识别的变形金刚</a> (2020) ICLR 2021(在审)<br/> [2] A .瓦斯瓦尼等。艾尔。、<a class="ae iu" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a> (2017)，NIPS2017年会议录<br/>【3】j .德夫林等。艾尔。，<a class="ae iu" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a> (2018)</p></div></div>    
</body>
</html>