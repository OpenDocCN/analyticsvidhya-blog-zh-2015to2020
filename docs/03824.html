<html>
<head>
<title>Multi-Armed Bandit Analysis of Thompson Sampling Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">汤普森采样算法的多臂土匪分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-armed-bandit-analysis-of-thompson-sampling-algorithm-6375271f40d1?source=collection_archive---------3-----------------------#2020-02-21">https://medium.com/analytics-vidhya/multi-armed-bandit-analysis-of-thompson-sampling-algorithm-6375271f40d1?source=collection_archive---------3-----------------------#2020-02-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1081" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">汤普森抽样算法利用贝叶斯概率方法来模拟各种武器的报酬分布。作为一个简短的总结，贝叶斯规则简单地表述如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/1e034d9e8cd8d8e97e3146a28af971d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/0*4-pk_KnvQ9i4QiRq.png"/></div></figure><p id="a779" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<code class="du jl jm jn jo b">D</code>代表观察到的数据，<code class="du jl jm jn jo b">P(θ|D)</code>是我们的后验概率，<code class="du jl jm jn jo b">P(D|θ)</code>是观察到给定θ的数据的可能性，<code class="du jl jm jn jo b">P(θ)</code>是θ分布的先验信念。</p><p id="ade3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们之前的分析(<a class="ae jp" rel="noopener" href="/@kfoofw/multi-armed-bandit-analysis-of-epsilon-greedy-algorithm-8057d7087423">ε-贪婪</a>、<a class="ae jp" rel="noopener" href="/@kfoofw/multi-armed-bandit-analysis-of-softmax-algorithm-e1fa4cb0c422"> Softmax </a>、<a class="ae jp" rel="noopener" href="/@kfoofw/multi-armed-bandit-analysis-of-upper-confidence-bound-algorithm-4b84be516047">置信上限</a>)中，我们假设arm可以建模为伯努利分布，θ_arm代表每次试验成功奖励的参数。因此，对于每个臂的N_arm试验总数和成功奖励的k_arm计数，每个臂可以用由N_arm、k_arm和θ_arm参数化的二项分布来表示。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jq"><img src="../Images/dc807b04e131d469b878ac6eabfadfe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/0*gSYvFxAsN5apS28Y.png"/></div></figure><p id="f6c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们采用贝塔分布作为参数假设来模拟θ的先验分布。贝塔分布是<code class="du jl jm jn jo b">a</code>和<code class="du jl jm jn jo b">b</code>的函数，代表给定θ的成功和失败次数。在先验的情况下，它代表我们认为arm成功和不成功试验的伪计数，这代表我们对特定arm的奖励功能的初始观点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jr"><img src="../Images/29e28f17b56f9d10e60bff571da0a469.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*CD7KIcNXmFJsKw03.png"/></div></figure><p id="99e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个臂的后验概率的分母代表数据的分布，可以重写为<code class="du jl jm jn jo b">k_arm</code>和<code class="du jl jm jn jo b">N_arm</code>的某种分布。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es js"><img src="../Images/2ee1b8c044a45421e16b4af2d6f0b1d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/0*60xTAcIUK7jqQ9TR.png"/></div></figure><p id="b6a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">综上所述，后验概率实际上可以用β分布来表示，其参数为:</p><ul class=""><li id="c9e4" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated"><code class="du jl jm jn jo b">a_posterior</code> = <code class="du jl jm jn jo b">a_prior + k</code></li><li id="3e4a" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><code class="du jl jm jn jo b">b_posterior</code> = <code class="du jl jm jn jo b">b_prior + N - k</code></li></ul><p id="5d0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了证明这一点，请考虑以下情况:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kh"><img src="../Images/4fa403fb9dd7f8573f25349d0a9b560c.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/0*_JREKytGRonVpl1f.png"/></div></figure><p id="d6fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们假设每个手臂的贝塔分布基于它的先验参数和它的播放历史。在每次试验迭代中，我们将从每个臂的后验Beta分布中随机取样，并选择为下一轮提供最大值的臂。这允许Thompson采样基于每个臂的单独后验贝塔分布在探索和利用之间进行平衡。不像其他武器那样经常探索的武器肯定会有更大的方差，这为根据随机抽样挑选武器创造了机会。</p><p id="3285" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择最佳手臂后，我们将玩它，并通过该手臂的历史记录(尝试次数和成功奖励)更新游戏。请注意，每次成功的奖励计数将增加<code class="du jl jm jn jo b">a_posterior</code>，而每次不成功的奖励计数将增加<code class="du jl jm jn jo b">b_posterior</code>。</p><p id="1f2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">出于本练习的目的，我们将使用由β(a = 1，b = 1)表示的均匀先验分布。</p><p id="b1d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的分析是基于约翰·迈尔斯·怀特的书<a class="ae jp" href="https://www.oreilly.com/library/view/bandit-algorithms-for/9781449341565/" rel="noopener ugc nofollow" target="_blank">《网站优化的强盗算法》</a>。尽管Thompson采样没有在本书中涉及，但我使用他的模板创建了Thompson采样的代码，以便更容易参考过去的算法文章。为了进一步理解代码，我加入了一些注释以便于理解。</p><p id="c407" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是创建Thompson采样算法设置和逐步更新arms的计数和值的代码。</p><ul class=""><li id="daaf" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">计数:代表手臂被拉动的记录时间。</li><li id="d5f2" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">值:代表已知的平均报酬。在伯努利臂的情况下，值代表奖励的概率，范围从0到1。</li></ul><pre class="je jf jg jh fd ki jo kj kk aw kl bi"><span id="8578" class="km kn hi jo b fi ko kp l kq kr">from scipy.stats import beta</span><span id="44b5" class="km kn hi jo b fi ks kp l kq kr">class ThompsonSampling():<br/>    def __init__(self, counts, values, a, b):<br/>        self.counts = counts # Count represent counts of pulls for each arm. For multiple arms, this will be a list of counts.<br/>        self.values = values # Value represent average reward for specific arm. For multiple arms, this will be a list of values.<br/>        <br/>        # Beta parameters<br/>        self.a = a<br/>        self.b = b<br/>        return</span><span id="f7fa" class="km kn hi jo b fi ks kp l kq kr">    # Initialise k number of arms<br/>    def initialize(self, n_arms):<br/>        self.counts = [0 for col in range(n_arms)]<br/>        self.values = [0.0 for col in range(n_arms)]</span><span id="a733" class="km kn hi jo b fi ks kp l kq kr">        # Uniform distribution of prior beta (A,B)<br/>        self.a = [1 for arm in range(n_arms)]<br/>        self.b = [1 for arm in range(n_arms)]<br/>        return<br/>    <br/>    # Thompson Sampling selection of arm for each round<br/>    def select_arm(self):<br/>        n_arms = len(self.counts)<br/>        <br/>        # Pair up all beta params of a and b for each arm<br/>        beta_params = zip(self.a, self.b)<br/>        <br/>        # Perform random draw for all arms based on their params (a,b)<br/>        all_draws = [beta.rvs(i[0], i[1], size = 1) for i in beta_params]<br/>        <br/>        # return index of arm with the highest draw<br/>        return all_draws.index(max(all_draws))<br/>    <br/>    # Choose to update chosen arm and reward<br/>    def update(self, chosen_arm, reward):<br/>        # update counts pulled for chosen arm<br/>        self.counts[chosen_arm] = self.counts[chosen_arm] + 1<br/>        n = self.counts[chosen_arm]<br/>        <br/>        # Update average/mean value/reward for chosen arm<br/>        value = self.values[chosen_arm]<br/>        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward<br/>        self.values[chosen_arm] = new_value<br/>        <br/>        # Update a and b<br/>        <br/>        # a is based on total counts of rewards of arm<br/>        self.a[chosen_arm] = self.a[chosen_arm] + reward<br/>        <br/>        # b is based on total counts of failed rewards on arm<br/>        self.b[chosen_arm] = self.b[chosen_arm] + (1-reward)<br/>        <br/>        return</span></pre><p id="bcec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据之前文章的讨论，我们将使用伯努利分布来表示每条手臂的奖励函数。</p><pre class="je jf jg jh fd ki jo kj kk aw kl bi"><span id="a3a1" class="km kn hi jo b fi ko kp l kq kr">class BernoulliArm():<br/>    def __init__(self, p):<br/>        self.p = p<br/>    <br/>    # Reward system based on Bernoulli<br/>    def draw(self):<br/>        if random.random() &gt; self.p:<br/>            return 0.0<br/>        else:<br/>            return 1.0</span></pre><p id="ae6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了进行任何进一步的分析，需要一个操作脚本来处理模拟，其中:</p><ul class=""><li id="bc9d" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">num_sims:表示独立模拟的数量，每个模拟的长度等于“地平线”。</li><li id="6e24" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">horizon:表示每轮模拟的时间步长/试验次数</li></ul><pre class="je jf jg jh fd ki jo kj kk aw kl bi"><span id="a709" class="km kn hi jo b fi ko kp l kq kr">def test_algorithm(algo, arms, num_sims, horizon):<br/>    <br/>    # Initialise variables for duration of accumulated simulation (num_sims * horizon_per_simulation)<br/>    chosen_arms = [0.0 for i in range(num_sims * horizon)]<br/>    rewards = [0.0 for i in range(num_sims * horizon)]<br/>    cumulative_rewards = [0 for i in range(num_sims * horizon)]<br/>    sim_nums = [0.0 for i in range(num_sims *horizon)]<br/>    times = [0.0 for i in range (num_sims*horizon)]<br/>    <br/>    for sim in range(num_sims):<br/>        sim = sim + 1<br/>        algo.initialize(len(arms))<br/>        <br/>        for t in range(horizon):<br/>            t = t + 1<br/>            index = (sim -1) * horizon + t -1<br/>            sim_nums[index] = sim<br/>            times[index] = t<br/>            <br/>            # Selection of best arm and engaging it<br/>            chosen_arm = algo.select_arm()<br/>            chosen_arms[index] = chosen_arm<br/>            <br/>            # Engage chosen Bernoulli Arm and obtain reward info<br/>            reward = arms[chosen_arm].draw()<br/>            rewards[index] = reward<br/>            <br/>            if t ==1:<br/>                cumulative_rewards[index] = reward<br/>            else:<br/>                cumulative_rewards[index] = cumulative_rewards[index-1] + reward<br/>                <br/>            algo.update(chosen_arm, reward)<br/>    <br/>    return [sim_nums, times, chosen_arms, rewards, cumulative_rewards]</span></pre><h1 id="f0b1" class="kt kn hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">模拟手段差异较大的兵种</h1><p id="45d8" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">类似于之前对ε-greedy所做的分析，模拟包括以下内容:</p><ul class=""><li id="eec9" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">创造5个兵种，其中四个平均奖励0.1，最后一个/最好的平均奖励0.9。</li><li id="9dc9" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">将模拟输出保存到制表符分隔的文件中</li><li id="508f" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">创建5000个独立的模拟</li></ul><p id="e46c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本例中，由于Thompson采样算法没有任何超参数(除了我们假设的Beta(1，1)先验)，我们创建了一组5000个模拟。</p><p id="f715" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择5000个独立模拟是因为我们想要确定平均性能。每个模拟可能受随机性质/运行的影响，并且性能可能由于随机机会而有偏差。因此，运行合理的大量模拟来评估平均均值/性能非常重要。</p><pre class="je jf jg jh fd ki jo kj kk aw kl bi"><span id="5fc3" class="km kn hi jo b fi ko kp l kq kr">import random</span><span id="8d28" class="km kn hi jo b fi ks kp l kq kr">random.seed(1)<br/># out of 5 arms, 1 arm is clearly the best<br/>means = [0.1, 0.1, 0.1, 0.1, 0.9]<br/>n_arms = len(means)<br/># Shuffling arms<br/>random.shuffle(means)</span><span id="be6c" class="km kn hi jo b fi ks kp l kq kr"># Create list of Bernoulli Arms with Reward Information<br/>arms = list(map(lambda mu: BernoulliArm(mu), means))<br/>print("Best arm is " + str(np.argmax(means)))</span><span id="69e7" class="km kn hi jo b fi ks kp l kq kr">f = open("standard_ts_results.tsv", "w+")</span><span id="8cb2" class="km kn hi jo b fi ks kp l kq kr"># Create simulations for ThompsonSampling<br/>algo = ThompsonSampling([], [], [], [])<br/>algo.initialize(n_arms)<br/>results = test_algorithm(algo, arms, 5000, 250)<br/>    <br/># Store data<br/>for i in range(len(results[0])):<br/>    f.write("\t".join([str(results[j][i]) for j in range(len(results))]) + "\n")<br/>f.close()</span></pre><p id="7f28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用一些数据预处理和基本的Altair可视化，我们可以绘制出拉最佳手臂的概率。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/afe7d39a52bae0eadbc76ca80a47b4b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*kacC26nAY1buEINt.png"/></div></figure><p id="d78a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">汤普森采样算法显示出对最佳arm选择的相对快速的收敛。在40次试验中，选择最佳手臂的平均比率在95%左右。与我们迄今为止看到的其他算法相比，这是非常令人印象深刻的。</p><p id="8c10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">开始时，所有武器都被认为是平等的，因为它们都有相同的前科。因此，选择最佳臂的比率总是从20%开始，这是从5个臂中选择最佳臂的随机机会。随着试验的进行，该算法让手臂的播放历史接管，并通过后验Beta分布的每次更新快速识别最佳手臂。注意，与我们在UCB1算法中观察到的情况相比，该进展也是平滑的。</p><p id="b87f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就性能而言，似乎没有任何渐近线限制，因为随着试验的进展，该图继续向选择最佳臂的100%机会收敛。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/0dc3ef158d0662d9ccc3394492863124.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/0*srWfufRvYCBDuS6h.png"/></div></figure><p id="c296" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">鉴于汤普森采样算法设法在早期识别最佳手臂的选择，它开始快速积累奖励。在250步的时间范围结束时，它平均达到大约215个累积点，这意味着它超过了几乎所有其他算法，如Epsilon Greedy、Softmax和UCB1(除了Softmax <code class="du jl jm jn jo b">tau</code> = 0.1，它大约有218个点)。</p><p id="cac7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与我们在UCB1中看到的类似，在试验的早期阶段，累积奖励图中有一个轻微的弯曲。这很快被拉直为一条直线(根据最佳手臂的平均奖励，它应该接近0.9)。</p><h1 id="9794" class="kt kn hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">平均差异相对较小的武器的模拟</h1><p id="8199" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">之前的分析是对回报差异很大的武器的模拟练习。我们将分析扩展到两臂相对较近的情况。</p><p id="fa1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的例子中，我们模拟了5个分支，其中4个分支的平均值为0.8，而最后一个/最佳分支的平均值为0.9。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/6a012946de95cac922b1d3b149641ad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*IDOs504kUuC9jAo3.png"/></div></figure><p id="674f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个实验中，所有分支的回报之间的差异减少了，Softmax和UCB算法都经历了严重的恶化，因为它们选择最佳分支的比率下降到了0.20到0.30的范围。然而，对于Thompson采样，选择最佳臂的比率现在接近0.68，这是迄今为止最好的结果(即使与Eps-Greedy <code class="du jl jm jn jo b">epsilon</code> = 0.2相比，后者大约为0.65)。</p><p id="cbec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管随着实验的进行，梯度看起来确实有某种形式的减小，但似乎没有一条硬渐近线。我的假设是，在更长的时间范围内，汤普森抽样算法将继续向100%增长。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/2e97e4f253e1e8012b89f06fded6ace9.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/0*gmaTrM2N-h_yeSgi.png"/></div></figure><p id="0650" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于这两个分支的平均回报率很接近，因此验证总体累积遗憾会更有意思。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/02e0a89f35c4be5d1c3c5d4113e16f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/0*FhI90ZQTjKMEB-t5.png"/></div></figure><p id="2fa5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们观察到，汤普森抽样是表现最好的，累积遗憾为12.1。与UCB1和Softmax算法相比，这要优越得多，同时也略微超过了最好的Epsilon Greedy算法(其范围为12.3到14.8)。随着图中相对较高的逐渐减少，这可以解释为什么汤普森采样优于其他算法。</p><h1 id="dcc1" class="kt kn hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">摘要</h1><p id="8c11" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">在Thompson采样算法的分析中，我们从Baye规则开始，并对先验使用Beta分布的参数假设。每只手臂的回报函数的总体后验概率是二项式可能性和贝塔先验的组合，可以表示为另一个贝塔分布。</p><p id="1f4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与其他算法相比，Thompson采样算法在性能上提供了鲁棒性，而不管平均回报接近的arms和平均回报差异大的arms。</p><p id="1291" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于本项目对bandit模拟分析的参考，请参考本<a class="ae jp" href="https://github.com/kfoofw/bandit_simulations" rel="noopener ugc nofollow" target="_blank"> Github repo </a>。关于实际代码的快速参考，请参考这个<a class="ae jp" href="https://github.com/kfoofw/bandit_simulations/blob/master/python/notebooks/analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>。</p></div></div>    
</body>
</html>