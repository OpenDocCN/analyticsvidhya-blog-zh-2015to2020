<html>
<head>
<title>Understanding Hadoop HDFS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解Hadoop HDFS</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-hadoop-hdfs-e0821c5fadc3?source=collection_archive---------10-----------------------#2020-04-10">https://medium.com/analytics-vidhya/understanding-hadoop-hdfs-e0821c5fadc3?source=collection_archive---------10-----------------------#2020-04-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e0ca78061fcf14bbb332bea818df7d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vH0yWboPy9ptgOSqClcS4Q.png"/></div></div></figure><p id="024b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">HDFS (Hadoop分布式文件系统)是一个分布式文件系统，用于在创纪录的时间内存储和检索带有流数据的大型文件。它是Hadoop Apache框架的基本组件之一，更准确地说是它的存储系统。HDFS Hadoop是Apache的顶级项目之一。</p><h1 id="2191" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">体系结构</h1><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/bac67b5437f1d8979860c9fcbed096e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2-nGyuVc538WmwE9"/></div></div></figure><h2 id="5591" class="kr jp hi bd jq ks kt ku ju kv kw kx jy jb ky kz kc jf la lb kg jj lc ld kk le bi translated">定义</h2><p id="8769" class="pw-post-body-paragraph iq ir hi is b it lf iv iw ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn hb bi translated"><strong class="is hj"> HDFS客户端:</strong>HDFS客户端代表用户与NameNode和Datanode交互，完成用户请求。</p><p id="3b89" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> NameNode </strong>:是HDFS文件系统的核心。它保存文件系统中所有文件的目录树，并跟踪文件数据在群集中的保存位置。它本身并不存储这些文件的数据，而是存储关于这些文件的数据或元数据。</p><p id="8d1f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> DataNode </strong>:在【HadoopFileSystem】中存储数据。一个功能文件系统有多个DataNode，数据在它们之间复制。</p><p id="d986" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">文件和块:</strong>文件是我们想要存储的数据，当我们将文件存储到HDFS时，它被分成块，每个块的默认大小在Hadoop 2.x中是128/256 MB，在Hadoop 1.x中是64 MB</p><p id="d4ec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">块复制:</strong>复制每个块是为了提供容错和可用性，默认的副本数量是3。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/9f324b0cfcabccdbe8f2a71a8e86b969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wKL8WaUWljwEVJ6z2clAQ.png"/></div></div></figure><h1 id="c573" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">写操作流程图</h1><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/75a6b63f1044c94fd3e35b579929da3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xGXXB7x64hxVBKDI.png"/></div></div></figure><ol class=""><li id="196b" class="lm ln hi is b it iu ix iy jb lo jf lp jj lq jn lr ls lt lu bi translated">HDFS客户端调用<strong class="is hj">分布式文件系统</strong>上的create()方法来创建文件。</li><li id="b205" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated"><strong class="is hj"> DistributedFileSystem </strong>通过RPC调用与NameNode交互，在文件系统命名空间中创建一个新文件，并与(file_size，dest_path…)相关联。</li><li id="5c3c" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated">Namenode确保该文件不存在，并且客户端具有创建该文件的权限。如果所有这些检查都通过了，namenode就会记录下这个新文件。<strong class="is hj"> DistributedFileSystem </strong>返回<strong class="is hj"> FSDataOutputStream </strong>供客户端<strong class="is hj"> </strong>开始向Datanode写入数据。</li><li id="0cd6" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated">当客户端开始写入数据时，<strong class="is hj"> DFSOutputStream </strong>(包装在<strong class="is hj"> FSDataOutputStream </strong>中)将客户端的数据拆分成数据包，并将其写入一个内部队列，称为<strong class="is hj">数据</strong>队列<strong class="is hj">队列</strong>。然后<strong class="is hj">数据流处理器</strong>使用这个数据队列，它负责通过选择合适的DataNode列表来告诉NameNode分配新的块。</li><li id="008d" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated"><strong class="is hj"> DFSOutputStream </strong>还维护另一个数据包队列，称为<strong class="is hj"> ack队列</strong>，它正在等待来自DataNodes的确认。</li><li id="c0a6" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated">HDFS客户端在完成数据写入时调用流上的<strong class="is hj"> close() </strong>方法。</li><li id="7f76" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated">然后，<strong class="is hj"> FSDataOutputStream </strong>向NameNode发送确认。</li></ol><h1 id="ffa8" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">读取操作流程图</h1><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/5416659f591f0bf7c1942f03a9900b3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XS4r6rkY8i3D5Q-z9JOIFA.png"/></div></div></figure><ol class=""><li id="b9aa" class="lm ln hi is b it iu ix iy jb lo jf lp jj lq jn lr ls lt lu bi translated">HDFS客户端调用<strong class="is hj">文件系统对象</strong>上的open()方法，这对于HDFS来说是分布式文件系统的一个实例。</li><li id="ac01" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated"><strong class="is hj"> DistributedFileSystem </strong>然后使用RPC调用NameNode来获取文件块的位置，对于每个块，NameNode从客户端返回包含该块副本的close Datanodes的地址。然后<strong class="is hj">分布式文件系统</strong>返回一个<strong class="is hj"> FSDataInputStream </strong>给客户端，客户端可以从这里读取数据。</li><li id="6e85" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated">然后客户端调用<strong class="is hj"> FSDataInputStream </strong>对象上的<strong class="is hj"> read() </strong>方法。</li><li id="127a" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated">包含文件块地址的<strong class="is hj"> DFSInputStream </strong>(包装在<strong class="is hj"> FSDataInputStream </strong>)连接到最近的DataNode以读取文件中的第一个块。</li><li id="eeb2" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated">到达文件末尾后，<strong class="is hj"> DFSInputStream </strong>关闭与该DataNode的连接，并为下一个块找到最合适的DataNode。</li><li id="22fa" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated">当客户端读取完数据后，调用<strong class="is hj"> FSDataInputStream </strong>上的<strong class="is hj"> close() </strong>。</li></ol><h1 id="c8e8" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">HDFS命令</h1><ol class=""><li id="b578" class="lm ln hi is b it lf ix lg jb mb jf mc jj md jn lr ls lt lu bi translated"><strong class="is hj">HDFS DFS–mkdir/path/directory _ name</strong>:创建目录。</li><li id="e0ff" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated"><strong class="is hj">hdfs DFS-ls/path</strong>:to<strong class="is hj"/>登记HDFS现有的文件和目录。</li><li id="addd" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated"><strong class="is hj">HDFS DFS-put&lt;localsrc&gt;&lt;dest&gt;</strong>:it<strong class="is hj"/>将文件或目录从本地文件系统复制到Hadoop文件系统中的目的地。</li><li id="79aa" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated"><strong class="is hj">HDFS DFS-get&lt;src&gt;&lt;local dest&gt;:</strong>it<strong class="is hj"/>将文件或目录从Hadoop文件系统复制到本地文件系统中的目的地。</li><li id="a885" class="lm ln hi is b it lv ix lw jb lx jf ly jj lz jn lr ls lt lu bi translated"><strong class="is hj">hdfs DFS–cat/path _ to _ file _ in _ HDFS</strong>:<strong class="is hj"/>在HDFS中读取文件，并在控制台或stdout上显示文件内容。</li></ol><h1 id="d371" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">面向HDFS的Java API</h1><h2 id="104f" class="kr jp hi bd jq ks kt ku ju kv kw kx jy jb ky kz kc jf la lb kg jj lc ld kk le bi translated">在HDFS写作</h2><pre class="kn ko kp kq fd me mf mg mh aw mi bi"><span id="e787" class="kr jp hi mf b fi mj mk l ml mm">FileSystem fileSystem = FileSystem.get(conf);<br/>Path path = new Path("/path/to/file.ext");<br/>if (fileSystem.exists(path)) {<br/>  System.out.println("File " + dest + " already exists");<br/>  return;<br/>}<br/>FSDataOutputStream out = fileSystem.create(path);<br/>InputStream in = new BufferedInputStream(new FileInputStream(<br/>new File(source)));<br/>byte[] b = new byte[1024];<br/>int numBytes = 0;<br/>while ((numBytes = in.read(b)) &gt; 0) {<br/>   out.write(b, 0, numBytes);<br/>}<br/>in.close();<br/>out.close();<br/>fileSystem.close();</span></pre><h2 id="97d4" class="kr jp hi bd jq ks kt ku ju kv kw kx jy jb ky kz kc jf la lb kg jj lc ld kk le bi translated">阅读HDFS的作品</h2><pre class="kn ko kp kq fd me mf mg mh aw mi bi"><span id="74dc" class="kr jp hi mf b fi mj mk l ml mm">FileSystem fileSystem = FileSystem.get(conf);<br/>Path path = new Path("/path/to/file.ext");<br/>if (!fileSystem.exists(path)) {<br/>  System.out.println("File does not exists");<br/>  return;<br/>}<br/>FSDataInputStream in = fileSystem.open(path);<br/>int numBytes = 0;<br/>while ((numBytes = in.read(b))&gt; 0) {<br/>  System.out.prinln((char)numBytes));<br/>}<br/>in.close();<br/>out.close();<br/>fileSystem.close();</span></pre><h1 id="d09d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">感谢阅读！</h1></div></div>    
</body>
</html>