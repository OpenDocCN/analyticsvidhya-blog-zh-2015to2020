<html>
<head>
<title>Learning Rate Decay and methods in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的学习速率衰减和方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b?source=collection_archive---------2-----------------------#2020-09-03">https://medium.com/analytics-vidhya/learning-rate-decay-and-methods-in-deep-learning-2cee564f910b?source=collection_archive---------2-----------------------#2020-09-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d2cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当使用随机或小批量梯度下降和恒定学习速率训练神经网络时，我们的算法通常以有噪声的方式(MBGD中噪声较小)收敛于最小值，并最终远离实际最小值振荡，为了克服这种情况，研究人员提出了一种建议，即随着时间的推移衰减学习速率，这有助于网络收敛于局部最小值并避免振荡。</p><p id="8c97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过这篇文章，我试图给你一个简单的直觉，关于使用<strong class="ih hj">学习率衰减</strong>及其各种方法来优化和推广你的神经网络。</p><h1 id="baac" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">学习率衰减</h1><p id="aa3b" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">学习率衰减是一种训练现代神经网络的技术。它开始以较大的学习速率训练网络，然后慢慢地减少/衰减它，直到获得局部最小值。根据经验观察，这有助于优化和推广。</p><h2 id="928e" class="kg je hi bd jf kh ki kj jj kk kl km jn iq kn ko jr iu kp kq jv iy kr ks jz kt bi translated">它是如何工作的？</h2><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/5c51d8d121b94038a57242184c1f958c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0H7_exu-hFDwXJe9LBzW0A.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">以恒定学习速率收敛的算法(有噪声，用蓝色表示)</figcaption></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lk"><img src="../Images/ffc501baee3a73554398da633f928f01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mbf-EvrLScSbQlyXU3xQSg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">算法收敛，同时学习率随时间衰减(噪音较小，用绿色表示)</figcaption></figure><p id="4bb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们具有恒定学习速率的第一幅图像中，我们的算法在朝着最小值迭代时所采取的步骤是如此嘈杂，以至于在某些迭代之后，它似乎在最小值周围徘徊，并且实际上没有收敛。</p><p id="99ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是在第二张图中，学习率随着时间的推移而降低(用绿线表示)，由于学习率最初很大，我们仍然有相对较快的学习，但是随着趋向最小值，学习率变得越来越小，最终在最小值附近的更小区域中振荡，而不是远离最小值。</p><h2 id="b300" class="kg je hi bd jf kh ki kj jj kk kl km jn iq kn ko jr iu kp kq jv iy kr ks jz kt bi translated"><strong class="ak">学习率衰减(常用方法):</strong></h2><blockquote class="ll"><p id="90ca" class="lm ln hi bd lo lp lq lr ls lt lu jc dx translated"><em class="lv"> " α </em> =(1/(1+ <em class="lv">十进制数</em>×<em class="lv">epoch number))*</em><em class="lv">α</em>0 "</p></blockquote><blockquote class="lw lx ly"><p id="efe1" class="if ig lz ih b ii ma ik il im mb io ip mc md is it me mf iw ix mg mh ja jb jc hb bi translated">1个时期:1次通过数据</p><p id="4a9a" class="if ig lz ih b ii ij ik il im in io ip mc ir is it me iv iw ix mg iz ja jb jc hb bi translated">α:学习率(当前迭代)</p><p id="0659" class="if ig lz ih b ii ij ik il im in io ip mc ir is it me iv iw ix mg iz ja jb jc hb bi translated">α0:初始学习率</p><p id="533c" class="if ig lz ih b ii ij ik il im in io ip mc ir is it me iv iw ix mg iz ja jb jc hb bi translated">decayRate:方法的超参数</p></blockquote><p id="337e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更直观，让我们举一个上面方法的粗略例子:</p><p id="34a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设α0 = 0.2，衰减率=1，那么对于每个时期，我们可以检查学习率α的下降:</p><p id="75cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">纪元1: alpha 0.1</p><p id="d546" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">纪元2:阿尔法0.067</p><p id="d472" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">纪元3:阿尔法0.05</p><p id="f8f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">纪元4:阿尔法0.04</p><p id="0d13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是将学习率衰减应用于训练神经网络的一种典型方法(通常使用)，希望我能够为您提供对该主题的直观感受，让我们检查更多学习率衰减的方法。</p><h1 id="99b6" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">其他方法</h1><ol class=""><li id="64ad" class="mi mj hi ih b ii kb im kc iq mk iu ml iy mm jc mn mo mp mq bi translated"><strong class="ih hj">指数衰减:</strong></li></ol><blockquote class="ll"><p id="d59d" class="lm ln hi bd lo lp mr ms mt mu mv jc dx translated">"α = ( decayRate^(epochNumber) )*α0 "</p></blockquote><p id="0428" class="pw-post-body-paragraph if ig hi ih b ii ma ik il im mb io ip iq md is it iu mf iw ix iy mh ja jb jc hb bi translated">该函数将指数衰减函数应用于所提供的初始学习速率，使得学习速率随时间呈指数衰减。</p><p id="a1b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种方法的十倍体总是小于1，0.95是从业者最常用的。</p><p id="04e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。离散楼梯:</strong></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mw"><img src="../Images/d8d0c47613895ac77285b789e9b97f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*HCPpnapq5s_LpHHQD435KA.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图片来自<a class="ae mx" href="https://mc.ai/learning-rate-decay-and-methods-in-deep-learning-3/" rel="noopener ugc nofollow" target="_blank"> mc.ai </a></figcaption></figure><p id="191a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种方法中，学习率在每隔一定的时间间隔后以一些不连续的步骤降低，例如，每隔10秒，学习率降低一半。</p><p id="252f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。基于纪元编号:</strong></p><blockquote class="ll"><p id="b602" class="lm ln hi bd lo lp lq lr ls lt lu jc dx translated">"α = ( k/ √epochNumber )* α0 "</p></blockquote><p id="dd32" class="pw-post-body-paragraph if ig hi ih b ii ma ik il im mb io ip iq md is it iu mf iw ix iy mh ja jb jc hb bi translated">在这种方法中，我们取某个常数“k ”,并将其除以纪元编号的平方根。</p><p id="64b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。基于小批量:</strong></p><blockquote class="ll"><p id="58d1" class="lm ln hi bd lo lp lq lr ls lt lu jc dx translated">"α = ( k/ √t)* α0 "</p></blockquote><p id="ba60" class="pw-post-body-paragraph if ig hi ih b ii ma ik il im mb io ip iq md is it iu mf iw ix iy mh ja jb jc hb bi translated">在这种方法中，我们取某个常数“k ”,并将其除以小批量的平方根。(此方法仅用于小批量梯度下降。)</p><p id="7821" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。手动衰减:</strong></p><p id="0607" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种方法中，实践者手动检查算法的性能，并且手动每天或每小时等地降低学习速率。</p><h1 id="e348" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结论</h1><p id="8edb" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">学习率衰减是一种优化和推广深度神经网络的先进技术，其方法在深度学习领域得到了广泛应用，一些深度学习API框架如<em class="lz"> KERAS </em>已经内置了对这些方法的支持。希望这篇文章对你们有所帮助。</p><p id="112e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读🙂</p><p id="082e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lz">演职员表:吴恩达</em></p><p id="88f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lz">参考:</em> <a class="ae mx" href="http://www.coursera.org/learn/deep-neural-network" rel="noopener ugc nofollow" target="_blank"> <em class="lz">改进深度神经网络- </em>超参数调优</a> …</p></div></div>    
</body>
</html>