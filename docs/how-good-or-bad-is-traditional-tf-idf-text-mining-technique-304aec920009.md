# 传统的 TF-IDF 文本挖掘技术有多好(或多坏)？

> 原文：<https://medium.com/analytics-vidhya/how-good-or-bad-is-traditional-tf-idf-text-mining-technique-304aec920009?source=collection_archive---------3----------------------->

嗨，数据伙伴们，爱好者们和有志者们！

基于各种教科书、博客和在线论文中的知识和研究，为您提供一个关于 TF-IDF 技术的博客。

![](img/407a5499ceaf9fbff18f09beaac823b4.png)

[来源](https://www.pexels.com/search/concept/)

文本挖掘是从文本和 web 文档中进行高质量信息检索的领域中众所周知的技术。在这个领域中一个非常流行和流行的策略是 ***向量化词频和逆文档频率*** (TF-IDF)表示法。事实上，谷歌搜索引擎在搜索一个单词时也使用这种技术。它基于 ***无监督学习技术*** 。TF-IDF 将您的文档文本转换为一个单词包*，然后为每个单词分配一个加权项，该加权项使用以下公式计算。*

*![](img/7e9147fd21c3ba1fe534a1bc6ff51275.png)*

*帕拉维*

****其中 TF(t，d) =(文档 d 中术语/单词 t 的出现次数)/(文档 d 中的总术语数)和****

**这是计算文档中术语的加权频率的传统方法，这种方法足够好并且简单。但是，它忽略了许多在处理文本时应该相关的细节。**

# **传统方法的工作原理及其局限性:**

1.  **它基于单词在文档中的出现频率来计算单词的加权项，但是它不考虑单词在文本中的位置、语义以及与文档中其他单词的共现。**
2.  **由于 TF-IDF 是一种无监督的特征选择技术，它并没有说明术语/单词与特定类别的相关性，而是仅受文档的限制。**
3.  **此外，逆文档频率具有不考虑在文档和整个文档语料库中更频繁出现的单词的辨别能力。这意味着根据传统的方法，如果一个词出现在每个文档中，IDF 将赋予该词 0 的权重，因此，IDF 认为该词与语料库中的其他术语没有太大的相关性。因此，如果上面的公式简单地说，TD*IDF 得分(权重)越高，该术语就越稀有，反之亦然。理想情况下，IDF 应该考虑那些在文档中更频繁出现的单词，以便它能够更好地将自己与群集中的其余单词(除了 ***停用词*** )进行分类。**
4.  **这种技术直接在字数统计空间中计算文档相似性，因此对于大型文档来说非常慢。**

**许多研究人员在文本分类领域进行了工作，并提出了 TF-IDF 的多个修改版本来处理传统方法的局限性。已经进一步解释了这些改进的技术中的一些。**

# ****术语频率相对频率(TF-RF):****

**这是基于项的类间分散系数，计算如下:**

****T3W(I，j) = TF(i，j)* IDF(I)* D(I)****

*****其中 D(i) = (1/n)∑(F(t，i) — avg(F(t，i))*****

**这里，n 是类别的数量，F(t，I)是具有术语 t 的文档的数量，并且属于术语 I 所属的类别。所以这个等式检验了一个术语对分类过程的贡献。如果一个项均匀分布在各个类中，那么类间离差 D 将会很低，反之亦然。**

# ****基于修改逆文档频率的 TF-IDF:****

**计算方法如下:**

*****W(i，j) = TF(i，j) *修改后的 IDF(i)*****

*****修改后的 IDF(I)=******Log10((整个语料库中的文档数+1)/包含术语/单词 I 的文档总数)*****

**与传统的 IDF 不同，该系数不会忽略在所有文档中更频繁出现并且在特定文档中具有高 TF 的单词。但是需要特殊处理来处理所有的 ***停用词*** ，因为它们在所有文档中出现的频率更高。**

# **基于课程频率的 TF-IDF:**

**这种方法也是基于如上所述的修改的逆文档频率。除此之外，类别频率系数也被添加到最终等式中，如下所述:**

*****W(i，j) = TF(i，j) *修正 IDF(i) *类频率*****

*****类频率= n(c(i，j))/N(c(i))*****

**其中 n(c(i，j))表示具有术语 j 并且属于文档 I 所属的类别 c(i)的文档的总数。N(c(i))表示 c(i)类文件的总数。这种策略将使术语在类中的频率标准化，并根据它们的类频率赋予它们重要性。**

**研究人员和科学家已经实施了几个修改过的策略，你可以让他们通读所有可用的研究论文。**

## **快乐阅读！**

**可以通过 [LinkedIn](https://www.linkedin.com/in/pallavi-ahuja-22590114/) 与我取得联系，并随时对博客中陈述的任何误导性信息发表评论:)**