<html>
<head>
<title>Apache Spark Primer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阿帕奇火花初级读本</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/apache-spark-primer-ca1a6d060fc8?source=collection_archive---------18-----------------------#2020-02-12">https://medium.com/analytics-vidhya/apache-spark-primer-ca1a6d060fc8?source=collection_archive---------18-----------------------#2020-02-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b2c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache Spark是一个用于大规模数据处理的开源、快速、分布式集群计算框架。Spark是一个执行引擎，不仅可以在Hadoop YARN上运行，还可以在Apache Mesos、Kubernetes、standalone或云中(AWS/Microsoft Azure/Google Cloud)运行。Spark可以访问不同的数据源，比如HDFS、Hive、HBase、Cassandra、S3、本地文件系统等等。</p><p id="b59b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于任何像Apache Hadoop这样的分布式集群计算框架，以下是最重要的组件:</p><ul class=""><li id="2d4c" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">存储数据的文件系统:HDFS</li><li id="309b" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">数据处理框架:MapReduce</li><li id="98f5" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">资源管理器和作业调度器:YARN</li></ul><p id="9a1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，Spark是MapReduce更快更好的替代品，它将是运行在纱线和HDFS上的执行引擎。它也可以在独立(单节点)模式下运行。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es jr"><img src="../Images/b0fa655c3f24bb9094d3b6594e030573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LvkHFF_3avoL06inqCVXjw.png"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">阿帕奇火花栈</figcaption></figure><p id="b001" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Apache Spark Stack具有以下重要组件:</p><p id="b4a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kh"> Spark SQL和Dataframes: </em> </strong> Spark SQL是一个帮助使用SQL查询处理结构化数据的模块。使用Spark SQL和dataframe，您可以对现有的HiveQL仓库执行查询，或者使用JDBC或ODBC连接对ORC、Avro、Parquet、JSON执行查询。你可以用Java、Scala、R或者Python来编写你的Spark SQL。</p><p id="0a97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="kh">Spark Streaming:</em></strong>Spark Streaming帮助我们构建可伸缩、容错的流媒体应用。Spark streaming支持Scala、Java和Python。您可以对历史数据和流数据运行查询。出现故障时，它会恢复数据，因此具有容错能力。它可以从HDFS、卡夫卡、Flume、社交媒体API和消息服务中读取数据。</p><p id="37f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kh"> MLlib </em> </strong> : MLlib在Spark engine之上提供可扩展的机器学习库。它支持许多分类、回归和聚类ML算法和许多数据处理实用程序。</p><p id="e716" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="kh">GraphX:</em></strong>grp ahx是一个用于图形和图形并行计算的Spark组件。</p><p id="15fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">重要的Spark概念:</strong></p><ul class=""><li id="b407" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj"><em class="kh">RDD—</em></strong>rdd(弹性分布式数据集)是Spark的基本构建模块，它是<em class="kh">内存中的</em>对象。rdd类似于Java中的集合对象。RDD分为<em class="kh">分区</em>(跨集群的datanode拆分)<em class="kh">不可变</em>(创建后不能更改)和<em class="kh">弹性</em>(如果datanode崩溃，RDD可以重建为RDD的血统，元数据由Spark维护)。在RDD上只能执行两种类型的操作:<strong class="ih hj"><em class="kh"/></strong>(通过filter或groupby等处理创建/转换成另一个RDD)和<strong class="ih hj"> <em class="kh">动作</em> </strong>(请求结果)。转换有两种类型:<strong class="ih hj"> <em class="kh">窄</em> </strong>转换(一个输入分区将给出一个输出分区，例如过滤操作)和<strong class="ih hj"> <em class="kh">宽</em> </strong>转换(多个输入分区将通过网络在彼此之间传输和共享数据，即<em class="kh">混洗</em>，例如分组或归约操作)</li><li id="f90e" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">惰性评估— </strong> Spark在惰性评估模式下工作，即只有在请求结果时才执行转换。因此，Spark程序将在执行类似获取前10条记录的操作时实际执行。由于这种模式，Spark将创建一个优化的物理执行计划。</li><li id="be15" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">数据框架— </strong>如果你使用Python，你必须习惯使用熊猫数据框架。Spark还支持dataframe(从1.3版开始)，或者更确切地说，它现在是Spark中的标准数据结构，是基于RDD类型构建的。Spark也支持数据集(从1.6版开始)，但只在Scala和Java中存在。数据集是数据帧的扩展，但具有编译时类型安全。从Spark 2.0开始，数据集和数据帧的API已经合并。</li></ul><p id="3fb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Spark 2.0架构:</strong></p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es ki"><img src="../Images/459648c2f68df81fa686a5c67e3d9f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wVO4P48vXiMqMljbBfnE5Q.jpeg"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">Spark 2.0架构</figcaption></figure><p id="2e8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark集群是一个典型的<em class="kh">主从架构</em>，其中主节点协调工作节点上的进程。主节点运行一个<strong class="ih hj"> <em class="kh">驱动程序</em> </strong>程序，该程序是一个独立的JVM进程。驱动程序在单个工作节点上启动任务，该工作节点在特定工作节点上的rdd上运行。驱动程序托管着<strong class="ih hj"> <em class="kh"> SparkContext </em> </strong>，它就像一个通向Spark必须提供的所有东西的网关。在驱动程序内部，运行许多其他服务，如<em class="kh"> SparkEnv、DAGScheduler、SparkUI、TaskScheduler、</em>等。</p><p id="c639" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark应用程序将使用SparkContext作为入口点，然后在rdd/data frame上执行不同的操作(转换和动作)。这些操作被表示为一个<strong class="ih hj"><em class="kh"/></strong>(有向无环图)。创建DAG后，spark在内部分阶段创建了一个物理执行计划。每个阶段被分割成任务，这些任务在工作者节点上的rdd的<strong class="ih hj"> <em class="kh">分区</em> </strong>上工作。使用钨的Spark 2.x比Spark 1.x优化得多，速度也快得多。在Spark 2.x中，SparkContext被包装在<strong class="ih hj"> <em class="kh"> SparkSession </em> </strong>中，它不仅封装了SparkContext，还封装了<em class="kh"> SQLContext、HiveContext、</em>等。YARN或MESOS之类的集群管理器负责与工人协调，并协调作业的执行。运行Spark应用程序代码的每个worker节点(Compute node)将有多个执行任务的<strong class="ih hj"> <em class="kh">执行器</em> </strong>。<strong class="ih hj"> <em class="kh">任务</em> </strong>(基本执行单元)是代码中实际的转换和动作。</p><p id="2b65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark附带Python(pyspark)和Scala(spark-shell)的交互式shell</p><p id="c5c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要在需要使用外部jar的情况下启动Spark 2.0 PySpark shell，请使用:</p><pre class="js jt ju jv fd kj kk kl km aw kn bi"><span id="7fc8" class="ko kp hi kk b fi kq kr l ks kt">export SPARK_MAJOR_VERSION=2<br/>pyspark --master yarn --jars /usr/hdp/current/sqoop-client/lib/oracle-jdbc-driver.jar</span></pre><p id="2701" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要运行python应用程序，可以使用spark-submit:</p><pre class="js jt ju jv fd kj kk kl km aw kn bi"><span id="ecc7" class="ko kp hi kk b fi kq kr l ks kt">### Save this as sour_spark_base.py ###<br/>### Import required packages<br/>from pyspark.sql import SparkSession<br/>from pyspark.conf import SparkConf</span><span id="29ae" class="ko kp hi kk b fi ku kr l ks kt">### Set configurations<br/>conf = SparkConf().setAll([('spark.sql.shuffle.partitions', '10'), ('spark.app.name', '"SourSparkApp"'), ('spark.executor.cores', '2'), ('spark.cores.max', '2'), ('spark.driver.memory','2g')])</span><span id="588d" class="ko kp hi kk b fi ku kr l ks kt">### Create SparkSession instance<br/>spark = SparkSession.builder.config(conf=conf).getOrCreate()</span><span id="e4fe" class="ko kp hi kk b fi ku kr l ks kt">### Read data from a Database<br/>df=spark.read.format('jdbc').options(url="jdbc:oracle:thin:USER_NAME/PASSWORD@database_connection:1521/db_name",dbtable="my_db_table",driver="oracle.jdbc.driver.OracleDriver").load()</span><span id="d426" class="ko kp hi kk b fi ku kr l ks kt">### Apply filter i.e. Narrow transformation to create a new RDD<br/>df_filter=df.filter(df.APPLICATION=='My_app')</span><span id="bac0" class="ko kp hi kk b fi ku kr l ks kt">### Apply groupBy i.e. Wide transformation to create a new RDD<br/>df_group=df_filter.groupBy("CURRENCY","TYPE").agg({"AMOUNT":"sum"}).withColumnRenamed("sum(AMOUNT)","sum_AMOUNT")</span><span id="dd03" class="ko kp hi kk b fi ku kr l ks kt">### Create a global temporary view to execute SQL statements on it<br/>df.createGlobalTempView("GLOBAL_TEMP_DATA_TABLE")</span><span id="3906" class="ko kp hi kk b fi ku kr l ks kt">### Create a RDD using a SparkSQL query <br/>spark_table = spark.sql("select CURRENCY,TYPE,sum(AMOUNT)  FROM global_temp.GLOBAL_TEMP_DATA_TABLE where APPLICATION='My_app' group by CURRENCY,TYPE")</span><span id="55c8" class="ko kp hi kk b fi ku kr l ks kt">### Perform Action i.e. Showing the SQL query results<br/>print(spark_table.show())</span><span id="5396" class="ko kp hi kk b fi ku kr l ks kt">### Perform Action i.e. Write the results to HDFS as a csv<br/>df_group.write.save('/dev/datalake/sour_spark_v2/data_agg_v2', format='csv', mode='append')</span><span id="4b26" class="ko kp hi kk b fi ku kr l ks kt">### Stop the Spark driver<br/>spark.stop()</span></pre><p id="4359" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在使用spark-submit运行这个python程序，如下所示:</p><pre class="js jt ju jv fd kj kk kl km aw kn bi"><span id="7f54" class="ko kp hi kk b fi kq kr l ks kt">/usr/hdp/current/spark2-client/bin/spark-submit — conf spark.executor.instances=4 — conf spark.executor.cores=2 — conf spark.executor.memory=4g — conf spark.driver.memory=4g — conf spark.cleaner.periodicGC.interval=5min — conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=/SERVICE-DATA/opt/dtl/softwares/anaconda3/bin/python — master yarn — deploy-mode cluster — jars /usr/hdp/current/sqoop-client/lib/oracle-jdbc-driver.jar sour_spark_base.py</span></pre><p id="c3a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kh">部署模式</em>指示驱动程序运行的位置，有两种类型:</p><ul class=""><li id="30a7" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><em class="kh">客户端</em> —驱动程序将在集群之外运行，从集群提交spark应用程序，例如从边缘节点或您的笔记本电脑。因此，如果您的会话终止或断开，那么您的Spark应用程序也会终止。</li><li id="a657" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><em class="kh">集群</em> —驱动程序将在集群内部的一个数据/工作节点上运行。驱动程序作为一个专用的、独立的进程在Worker内部运行。</li></ul><p id="bcea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了监视和查看Spark作业的日志，您可以使用<strong class="ih hj"> <em class="kh"> Spark UI </em> </strong>(默认情况下在端口4040上)。在Spark历史服务器上，您的旧作业将可用。</p><p id="2772" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，您可以用三种方式设置<em class="kh">火花配置</em>:</p><ul class=""><li id="9ff8" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">spark-defaults.conf —由集群管理员设置的集群范围的spark属性</li><li id="fd6e" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">spark-submit——使用spark-submit执行应用程序时</li><li id="292c" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">SparkConf()。setAll —在您的spark计划中</li></ul><p id="05c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设置spark属性时，程序中的SparkConf()优先于spark-submit，spark-submit优先于spark-defaults.conf。</p><p id="b81b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kh">催化剂优化器:</em> </strong></p><p id="cbea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark查询的执行如下所述。首先，您的程序被解析，逻辑计划由数据帧或SparkSQL查询上的所有转换创建。Catalyst优化器将优化计划，以使用不同的方法创建优化的逻辑计划，如谓词下推、投影修剪、空传播、表达式简化等。然后选择基于成本的最佳逻辑计划，并创建多个物理计划。然后，钨执行引擎将根据执行成本模型选择最佳的物理计划。从这个最终的RDD和字节码生成。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es kv"><img src="../Images/3573223804b6c784cf67ce40e01e677a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ttE3CMDaJNHIxC64IC7uLw.png"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">火花优化和执行计划</figcaption></figure><p id="d78d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们在这里介绍了是什么让Spark成为目前最受欢迎的大数据处理引擎，以及Spark 2.0的新功能和架构，这些功能和架构使Spark更加强大，更易于使用。</p><p id="772d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下一章中，我们将介绍<a class="ae kw" rel="noopener" href="/analytics-vidhya/spark-sql-and-dataframes-72e58fe90f94">结构化Spark API</a>，即数据帧、数据集和Spark SQL。</p></div></div>    
</body>
</html>