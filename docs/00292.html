<html>
<head>
<title>A picture is worth a Thousand Words- Lets figure out the relevant ones</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一张图片胜过千言万语——让我们找出相关的</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-picture-is-worth-a-thousand-words-lets-figure-out-the-relevant-ones-15cbb56443ae?source=collection_archive---------2-----------------------#2019-03-10">https://medium.com/analytics-vidhya/a-picture-is-worth-a-thousand-words-lets-figure-out-the-relevant-ones-15cbb56443ae?source=collection_archive---------2-----------------------#2019-03-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1d2e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">教计算机描述图片</h2></div><p id="90a6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">图像字幕是指根据图像中的对象和动作从图像生成文本描述的过程。这是一个由3部分组成的实现图像字幕的系列，由Andrej Karapathy <strong class="iz hj"> </strong>在他的博士论文<a class="ae jt" href="https://cs.stanford.edu/people/karpathy/main.pdf" rel="noopener ugc nofollow" target="_blank">中提出。</a></p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ju"><img src="../Images/12f86d0d7677c74ac9b26b5961a75272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L23MWigQNu2bVA_YLN4i-A.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">使用神经网络的计算机生成字幕</figcaption></figure><p id="903d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个过程中，我们将学习神经网络的基础知识，在Keras(tensor flow包装器)中创建卷积神经网络(CNN)，探索最先进的NLP模型(Sequence to Sequence，Glove，BERT等)，并使用LSTM将CNN和NLP模型堆叠在一起，以生成图像的字幕。</p><p id="39d9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将以此为基础，基于预先训练好的图像和字幕向量创建推荐系统，然后使用一个实时WebApp作为字幕生成和推荐的测试平台。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es kk"><img src="../Images/a4862d4b6f0dd863f618df6e4f6e956f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W5IWkE4ZbuS3ryDfUFsDyg.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">基于第一只鞋的样式的建议</figcaption></figure><p id="2816" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">目录(第1部分):</p><ul class=""><li id="d949" class="kl km hi iz b ja jb jd je jg kn jk ko jo kp js kq kr ks kt bi translated">神经网络基础</li><li id="6331" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated">用于图像识别的卷积神经网络</li><li id="b272" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated">设置Google Colab笔记本</li><li id="49da" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated">在Keras中创建用于图像分类的神经网络</li></ul></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><p id="7617" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">神经网络基础知识:</strong></p><p id="8c02" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">神经网络是一种机器学习类型，它模仿人脑来模拟自身。这通过一种算法创建了一个人工神经网络，允许计算机通过合并新数据进行学习。</p><p id="e292" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它接受几个输入，通过来自多个隐藏层的多个神经元进行处理，并使用输出层返回结果。这个结果估计过程在技术上被称为“<strong class="iz hj">正向传播</strong>”。</p><p id="985b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我们将结果与实际输出进行比较。任务是使神经网络的输出尽可能接近实际(期望)输出。这些神经元中的每一个都对最终输出产生一些误差。你如何减少误差？</p><p id="2918" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们试图最小化对错误贡献更多的神经元的值/权重，这发生在返回神经网络的神经元并找到错误所在的时候。这个过程被称为“<strong class="iz hj">反向传播</strong>”。反向传播(BP)更新权重以最小化每个神经元产生的误差。</p><p id="9bb3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了减少这些迭代次数以最小化误差，神经网络使用一种称为<strong class="iz hj">“梯度下降”</strong>的常见算法，这有助于快速有效地优化任务。更多关于梯度下降<a class="ae jt" href="https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="b3cd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">多个<strong class="iz hj">时期</strong>(前向和后向传播)的目的只是优化多个层的权重和偏差，以最小化误差。</p><p id="e2d4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">各种类别的神经网络:</p><ul class=""><li id="57a7" class="kl km hi iz b ja jb jd je jg kn jk ko jo kp js kq kr ks kt bi translated">卷积神经网络(CNN)</li><li id="b105" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated">递归神经网络(RNN)</li><li id="1652" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated">LSTM和格鲁</li></ul><p id="0f95" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们在这里深入研究CNN，我将在随后的帖子中涉及其他类别。</p></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><p id="3ea4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">卷积神经网络(CNN):它们主要用于图像处理任务(分类、对象检测、定位等)，并构成4个主要操作，即卷积、非线性、汇集和分类，如下所述:</p><ol class=""><li id="0740" class="kl km hi iz b ja jb jd je jg kn jk ko jo kp js lg kr ks kt bi translated"><strong class="iz hj">卷积:</strong>卷积的主要目的是从输入图像中提取特征。卷积通过使用输入数据的小方块学习图像特征来保持像素之间的空间关系。正如我们上面所讨论的，每个图像都可以被认为是像素值的矩阵。考虑像素值仅为0和1的5×5图像:</li></ol><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lh"><img src="../Images/e9ce5948c094c135074bb15620b4dc89.png" data-original-src="https://miro.medium.com/v2/resize:fit:254/0*7xHjnE0YCpLOoVqN"/></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">以矩阵形式输入图像</figcaption></figure><p id="91b1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，考虑如下所示的另一个3 x 3矩阵:</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es li"><img src="../Images/ebf31200df70a2b1009e11e6843ee859.png" data-original-src="https://miro.medium.com/v2/resize:fit:148/0*vmNLweEzjTFLVwKL"/></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">随机初始化的权重矩阵</figcaption></figure><p id="9a28" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，可以计算5 x 5图像和3 x 3矩阵的卷积，如下面的动画所示:</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lj"><img src="../Images/0e7de8a67e0c197106c87d417e45b499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/1*ZCjPUFrB6eHPRi4eyP6aaA.gif"/></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">图像*权重矩阵=卷积特征</figcaption></figure><p id="f496" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将橙色矩阵在原始图像(绿色)上滑动1个像素(也称为<strong class="iz hj">‘步幅’</strong>)，对于每个位置，我们计算<a class="ae jt" href="https://en.wikipedia.org/wiki/Matrix_multiplication" rel="noopener ugc nofollow" target="_blank">元素乘法</a>(两个矩阵之间)，并将乘法输出相加，以获得最终整数，该整数形成输出矩阵(粉色)的单个元素。注意，3×3矩阵在每一步中只“看到”输入图像的一部分。</p><p id="f2f6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在CNN术语中，3×3矩阵被称为“<strong class="iz hj">滤波器</strong>，通过在图像上滑动滤波器并计算点积形成的矩阵被称为“卷积特征”或“激活图”或“<strong class="iz hj">特征图</strong>”。</p><p id="4a2e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">图像上的卷积运算:</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lk"><img src="../Images/6696be14fd8e2a392c954e1f0a4755dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Y0tvNnH7sXHw2zP7TrCKgA.gif"/></div></figure><p id="2608" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意这两种不同的过滤器如何从相同的原始图像生成不同的特征图。在实践中，CNN <em class="ll">在训练过程中自己学习</em>这些过滤器的值，提取的图像特征越多，我们的网络就越能更好地识别看不见的图像中的模式。</p><p id="d640" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> 2。非线性(ReLu): </strong></p><p id="fd77" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ReLu代表整流线性单元，是一种非线性操作。ReLU的目的是在我们的ConvNet中引入非线性，因为我们希望ConvNet学习的大部分真实数据都是非线性的。其输出由下式给出:</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lm"><img src="../Images/ccf9739087dfef074ea6e67a53afd6f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/0*hr5ij1tsgfb4Mz3d"/></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">Relu操作</figcaption></figure><p id="3895" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ReLu将特征图中的所有负像素值替换为零。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ln"><img src="../Images/d7e41f920dd921860e5ba87a5a86e495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYpFlRz3HDz9lcJkbLaBcA.png"/></div></div></figure><p id="4727" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> 3。池化(减少图像的尺寸):</strong></p><p id="7915" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">空间池(也称为二次采样或下采样)减少了每个要素地图的维度，但保留了最重要的信息。空间池可以有不同的类型:最大池，平均池等。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lo"><img src="../Images/2375522f16ca7710f70d7afcab914bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/0*9_zro3kc-ccq64LX.JPG"/></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">最大池和平均池</figcaption></figure><h2 id="50d5" class="lp lq hi bd lr ls lt lu lv lw lx ly lz jg ma mb mc jk md me mf jo mg mh mi mj bi translated">4.用于分类的全连接(密集)层:</h2><p id="54ba" class="pw-post-body-paragraph ix iy hi iz b ja mk ij jc jd ml im jf jg mm ji jj jk mn jm jn jo mo jq jr js hb bi translated">术语“完全连接”意味着前一层中的每个神经元都连接到下一层中的每个神经元。卷积层和池层的输出代表输入图像的高级特征。全连接图层的目的是使用这些特征根据训练数据集将输入影像分类到不同的类别中。</p><p id="7c51" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用Softmax函数进行最终分类<strong class="iz hj">。Softmax函数</strong>获取任意实值分数的向量，并将其压缩为总和为1的介于0和1之间的值的向量。</p><h2 id="fd81" class="lp lq hi bd lr ls lt lu lv lw lx ly lz jg ma mb mc jk md me mf jo mg mh mi mj bi translated">卷积网络的整个训练过程可以总结如下:</h2><p id="f106" class="pw-post-body-paragraph ix iy hi iz b ja mk ij jc jd ml im jf jg mm ji jj jk mn jm jn jo mo jq jr js hb bi translated"><strong class="iz hj">步骤1: </strong>我们用随机值初始化所有过滤器和参数/权重</p><p id="7771" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">第二步:</strong>网络将训练图像作为输入，通过正向传播步骤(卷积、ReLU和汇集操作以及全连接层中的正向传播)，并找到每一类的输出概率。</p><p id="08e0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">步骤3: </strong>计算输出层的总误差(所有4类的总和)</p><ul class=""><li id="fd53" class="kl km hi iz b ja jb jd je jg kn jk ko jo kp js kq kr ks kt bi translated"><strong class="iz hj">总误差= ∑(目标概率-输出概率)</strong></li></ul><p id="ab2c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">步骤4: </strong>使用反向传播计算误差相对于网络中所有权重的<em class="ll">梯度</em>，并使用<em class="ll">梯度下降</em>更新所有滤波器值/权重和参数值，以最小化输出误差。</p><p id="42c6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">步骤5: </strong>对训练集中的所有图像重复步骤2–4。</p><p id="b4b9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上述步骤<em class="ll">训练</em>ConvNet——这实质上意味着conv net的所有权重和参数现在都已经过优化，可以正确地对来自训练集的图像进行分类。</p><p id="0ffd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当一个新的(看不见的)图像被输入到ConvNet中时，网络将经历正向传播步骤，并输出每个类别的概率(对于一个新的图像，使用已经被优化以正确分类所有先前训练示例的权重来计算输出概率)</p></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><p id="430a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">设置Google Colab笔记本:你只需要一个Google Drive账户</p><p id="51ec" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Google研究团队描述的Colab是一个免费的Jupyter笔记本环境，不需要设置，完全在云中运行。使用Colab，您可以编写和执行代码，保存和共享您的分析，以及访问强大的计算资源(包括GPU)，所有这些都可以从您的浏览器中免费获得。</p><p id="10a5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们基于<strong class="iz hj"> Google Colab </strong>中的<strong class="iz hj"> Keras </strong>使用<strong class="iz hj"> GPU </strong>为<strong class="iz hj">图像分类</strong>创建一个<strong class="iz hj"> CNN模型</strong>:</p><ol class=""><li id="0691" class="kl km hi iz b ja jb jd je jg kn jk ko jo kp js lg kr ks kt bi translated">转到<a class="ae jt" href="https://colab.research.google.com/notebooks/welcome.ipynb" rel="noopener ugc nofollow" target="_blank">https://colab.research.google.com/notebooks/welcome.ipynb</a></li><li id="1dd6" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js lg kr ks kt bi translated">点击运行时-&gt;更改运行时类型-&gt;为GPU激活的环境选择GPU。在我的测试中，我发现在使用类似系统配置的情况下，GPU至少比CPU快10倍。</li></ol><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mp"><img src="../Images/b25958dc9776c67b678f23232105700e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0oxB_YAcSder1sNmcj2png.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">在GPU驱动的环境中使用Google Colab的新笔记本电脑</figcaption></figure></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><p id="4d56" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">在Keras中建立CNN模型进行图像分类(好老</strong> <a class="ae jt" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj"> MNIST数据库</strong> </a> <strong class="iz hj"> ): </strong></p><pre class="jv jw jx jy fd mq mr ms mt aw mu bi"><span id="bfb9" class="lp lq hi mr b fi mv mw l mx my">#Keras Model Creation</span><span id="d1ea" class="lp lq hi mr b fi mz mw l mx my">import keras<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Conv2D, Flatten<br/>#create model<br/>model = Sequential()<br/>#add model layers</span><span id="e24c" class="lp lq hi mr b fi mz mw l mx my">model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28,28,1)))<br/>model.add(Conv2D(32, kernel_size=3, activation='relu'))<br/><br/>model.add(Dense(10, activation='softmax'))</span></pre><p id="6c3a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用的型号类型是<strong class="iz hj">序列</strong>。它允许你一层一层地建立模型。我们使用“add()”函数向模型中添加层。我们的前两层是Conv2D层。这些卷积层将处理我们的输入图像，这些图像被视为二维矩阵。“密集”是我们将在输出图层中使用的图层类型。密集是一种标准图层类型，在许多情况下用于神经网络。我们的输出层将有10个节点，每个节点对应一个可能的结果(0–9，即10位数)。激活是“softmax”。Softmax使输出总和达到1，因此输出可以解释为概率。然后，该模型将根据哪个选项的概率最高来进行预测。</p><pre class="jv jw jx jy fd mq mr ms mt aw mu bi"><span id="d2f2" class="lp lq hi mr b fi mv mw l mx my"># Keras Model Compile</span><span id="83e9" class="lp lq hi mr b fi mz mw l mx my">#compile model <em class="ll">using accuracy to measure model performance</em><br/>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</span></pre><p id="460b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编译模型需要三个参数:<strong class="iz hj">优化器、损失和指标</strong></p><p id="c155" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">优化器控制学习速率。我们将使用“adam”作为我们的优化器，因为它可以在整个培训过程中调整学习率。学习率决定了计算模型最佳权重的速度。较高的学习速率意味着快速收敛，因此拟合不足，而较低的学习速率意味着模型拟合需要较长的时间。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es na"><img src="../Images/0d887f93a4e6138031ec934e874902f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/0*Zzl5hqDpvK0MNDUV.jpeg"/></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">学习率与损失</figcaption></figure><p id="850d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用‘分类交叉熵’作为我们的损失函数。这是最常见的分类选择。分数越低，表示模型的性能越好。<strong class="iz hj">准确性’</strong>度量用于在我们训练模型时查看验证集的准确性分数。</p><pre class="jv jw jx jy fd mq mr ms mt aw mu bi"><span id="a1b5" class="lp lq hi mr b fi mv mw l mx my">#train the model<br/>model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)</span><span id="2639" class="lp lq hi mr b fi mz mw l mx my">#Make Predictions in the test set<br/>model.predict(X_test[:4])<br/></span></pre><p id="59d5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基于此，我们能够在测试数据集上看到97%的准确率。我的代码Colab笔记本:</p><div class="nb nc ez fb nd ne"><a href="https://colab.research.google.com/drive/1QI_VDLYS_d5fS1QTXqicTxmLs2HkWDs-" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">谷歌联合实验室</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">编辑描述</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">colab.research.google.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ke ne"/></div></div></a></div><p id="b9cf" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下一部分，我们将在Flickr 30K数据集<a class="ae jt" href="http://bryanplummer.com/" rel="noopener ugc nofollow" target="_blank">http://bryanplummer.com/</a>上工作，它有5个与每张图片相关的标题。我们将建立一个编码器-解码器网络，然后根据字幕进行监督训练，以根据我们的图像输入生成自定义字幕。</p><p id="b4de" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">非常感谢你的阅读。快乐学习！！！</p><p id="1633" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">参考资料:</p><ul class=""><li id="a8dc" class="kl km hi iz b ja jb jd je jg kn jk ko jo kp js kq kr ks kt bi translated">https://github.com/karpathy/neuraltalk2<a class="ae jt" href="https://github.com/karpathy/neuraltalk2" rel="noopener ugc nofollow" target="_blank"/></li><li id="3953" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated"><a class="ae jt" href="https://towardsdatascience.com/a-deeper-understanding-of-nnets-part-1-cnns-263a6e3ac61" rel="noopener" target="_blank">https://towards data science . com/a-deeper-understand-of-nnets-part-1-CNNs-263 a6 E3 AC 61</a></li><li id="e8dd" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated"><a class="ae jt" href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2017/05/neural-network-from-scratch-in-python-and-r/</a></li><li id="a6f0" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated"><a class="ae jt" href="https://towardsdatascience.com/image-captioning-in-deep-learning-9cd23fb4d8d2" rel="noopener" target="_blank">https://towards data science . com/image-captioning-in-deep-learning-9cd 23 FB 4d 8d 2</a></li><li id="4955" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated"><a class="ae jt" href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" rel="noopener ugc nofollow" target="_blank">https://ujjwalkarn . me/2016/08/11/直观-解释-convnets/ </a></li><li id="ea10" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated"><a class="ae jt" rel="noopener" href="/nybles/create-your-first-image-recognition-classifier-using-cnn-keras-and-tensorflow-backend-6eaab98d14dd">https://medium . com/ny bles/create-your-first-image-recognition-classifier-using-CNN-keras-and-tensor flow-back end-6 eaab 98d 14 DD</a></li><li id="ada3" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated"><a class="ae jt" href="https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8" rel="noopener" target="_blank">https://towards data science . com/image-captioning-with-keras-teaching-computers-to-description-pictures-c 88 a 46 a 311 b 8</a></li><li id="5dba" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated"><a class="ae jt" href="https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5" rel="noopener" target="_blank">https://towards data science . com/building-a-convolutionary-neural-network-CNN-in-keras-329 fbbadc5 F5</a></li><li id="cfed" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated"><a class="ae jt" href="https://towardsdatascience.com/creating-a-movie-recommender-using-convolutional-neural-networks-be93e66464a7" rel="noopener" target="_blank">https://towards data science . com/creating-a-movie-recommender-using-convolutionary-neural-networks-be93e 66464 a 7</a></li><li id="e0b2" class="kl km hi iz b ja ku jd kv jg kw jk kx jo ky js kq kr ks kt bi translated"><a class="ae jt" href="https://towardsdatascience.com/getting-started-with-google-colab-f2fff97f594c" rel="noopener" target="_blank">https://towards data science . com/getting-started-with-Google-cola b-F2 fff 97 f 594 c</a></li></ul></div></div>    
</body>
</html>