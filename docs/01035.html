<html>
<head>
<title>Optimizing arch64 Edge devices for Maximum Performance on ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化arch64 Edge设备以在ML上实现最高性能</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimizing-arch64-edge-devices-for-maximum-performance-on-ml-cc3d008f675b?source=collection_archive---------9-----------------------#2019-09-26">https://medium.com/analytics-vidhya/optimizing-arch64-edge-devices-for-maximum-performance-on-ml-cc3d008f675b?source=collection_archive---------9-----------------------#2019-09-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/99230164418c78e91f4e57f0aa832c99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BmWhhS5USRcTAwH5XU8gZg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源images.google.com</figcaption></figure><p id="4b5e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi js translated">对于深度学习工程师来说，在云基础设施中训练大型模型是一项轻松的任务，例如<a class="ae kb" href="https://aws.amazon.com" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj">亚马逊AWS </strong> </a>，它在<a class="ae kb" href="https://aws.amazon.com/ec2/" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj"> EC2 </strong> </a>和<a class="ae kb" href="https://aws.amazon.com/sagemaker/" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj"> Sagemaker </strong> </a>中提供高性能计算引擎，以及模型托管等功能。但当涉及到在边缘设备上部署同样沉重的型号，如<a class="ae kb" href="https://archlinuxarm.org/platforms/armv7" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">arch V7</strong></a>(Raspberry 4)和<a class="ae kb" href="https://archlinuxarm.org/platforms/armv8" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">arch V8</strong></a>NVIDIA Jetson硬件时，这就成了一项艰巨的任务。为了克服Pi和Jetson等边缘设备的性能问题，我分享了一些优化技术，这些技术可以提高设备的性能，优化深度学习模型。</p><p id="3eb4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae kb" href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj"> NVIDIA Jetson </strong> </a>是全球领先的嵌入式AI计算平台。其用于深度学习和计算机视觉的高性能、低功耗计算使构建软件定义的自主机器成为可能。</p><p id="54be" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Jetson平台包括具有GPU加速并行处理的小型Jetson模块、具有开发人员工具和用于构建AI应用程序的综合库的JetPack SDK，以及具有加速开发的服务和产品的合作伙伴生态系统。</p><p id="467d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">NVIDIA Jetson系列有4种类型的设备</p><ol class=""><li id="40a2" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr kh ki kj kk bi translated">杰特森纳米</li><li id="3bab" class="kc kd hi iw b ix kl jb km jf kn jj ko jn kp jr kh ki kj kk bi translated">杰特森TX1</li><li id="756b" class="kc kd hi iw b ix kl jb km jf kn jj ko jn kp jr kh ki kj kk bi translated">杰特森TX2</li><li id="0dce" class="kc kd hi iw b ix kl jb km jf kn jj ko jn kp jr kh ki kj kk bi translated">杰特森AGX泽维尔</li></ol><p id="1ac3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">将机器学习部署到如此强大的边缘设备上总是很有趣，这些设备能够支持几乎所有的深度学习框架，如<a class="ae kb" href="https://pytorch.org" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">【py torch】</strong></a><a class="ae kb" href="https://www.tensorflow.org" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">tensor flow</strong></a><a class="ae kb" href="https://aws.amazon.com/mxnet/" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">MxNet</strong></a><strong class="iw hj">(这也是亚马逊AWS正式采用的服务，如</strong><a class="ae kb" href="https://aws.amazon.com/sagemaker/" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">【sage maker</strong></a><a class="ae kb" href="https://caffe.berkeleyvision.org" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">Caffe</strong></a>除此之外，NVIDIA还推出了名为<a class="ae kb" href="https://developer.nvidia.com/deepstream-sdk" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">Deepstream</strong></a><strong class="iw hj"/>的强大函数库，在解决物体检测和物体跟踪等问题时派上了用场。</p><p id="9441" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">尽管在Jetson硬件上有如此强大的计算，但在使用强大的深度学习算法(如<a class="ae kb" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">【YOLO】</strong></a>、<a class="ae kb" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj"> SSD </strong> </a>、<a class="ae kb" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj"> Faster-RCNN </strong> </a>等)时，性能仍然可能不足。</p><p id="42c6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">保持每秒18-20帧的帧速率是很重要的。</p><p id="18ff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">作为一名深度学习工程师，我过去遇到过这些问题，但在以有效的方式利用jetson机器的计算能力后，我们可以通过实时对象检测模型实现更好的每秒帧数。</p><p id="21a8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">下面是我们可以调整的指标，以便在Arch64架构上获得最佳性能。</strong></p><ol class=""><li id="8467" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr kh ki kj kk bi translated"><strong class="iw hj">运行杰特森时钟。</strong></li></ol><p id="e156" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这应该是第一步，因为jetson_clocks脚本会禁用DVFS调控器，并将时钟锁定到活动nvpmodel电源模式定义的最大值。</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="47d8" class="kz la hi kv b fi lb lc l ld le">$ sudo jetson_clocks.sh</span></pre><p id="b6ca" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.<strong class="iw hj">创建交换</strong></p><p id="0970" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">交换空间是当今计算的一个常见方面，与操作系统无关。Linux使用交换空间来增加主机可用的虚拟内存量。它可以在常规文件系统或逻辑卷上使用一个或多个专用交换分区或交换文件。交换空间是现代Linux系统中的第二种内存类型。交换空间的主要功能是当实际RAM填满并且需要更多空间时，用磁盘空间代替RAM内存。</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="38fc" class="kz la hi kv b fi lb lc l ld le">$ fallocate -l 8G swapfile<br/>$ sudo chmod 600 swapfile<br/>$ sudo mkswap swapfile<br/>$ sudo swapon swapfile</span></pre><p id="f677" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">检查内存并交换</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="a0dc" class="kz la hi kv b fi lb lc l ld le">$ free -m</span></pre><p id="a1c6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 3。调整能量分布。</strong></p><p id="ccdf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Nvidia提供了更改CPU和GPU设置的灵活性，以调整Jetson TX2的性能和功耗，nvpmodel工具提供了一些方便且易于切换的能源性能配置文件。</p><blockquote class="lf lg lh"><p id="9408" class="iu iv li iw b ix iy iz ja jb jc jd je lj jg jh ji lk jk jl jm ll jo jp jq jr hb bi translated">常见的能源概况—</p></blockquote><ol class=""><li id="d01f" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr kh ki kj kk bi translated"><strong class="iw hj"> Max Q </strong></li></ol><p id="e856" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Nvidia使用Max Q术语来表示最大处理效率，因此在这种模式下，TX2上的所有组件都配置为最大效率。这种配置使用最佳功率-吞吐量权衡值。</p><p id="2c4a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 2。最大压力</strong></p><p id="8dd7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了提高CPU的时钟频率，比Q消耗更多的功率。这种模式提高了性能，但牺牲了功耗。</p><p id="8496" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 3。最大数量</strong></p><p id="d157" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">根据Nvidia TX2 NVP模型的定义，在这种模式下，CPU和GPU的时钟频率高于Max-P，从而进一步降低了功耗。</p><p id="58ff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">能量分布的比较</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/48878066c2804754bdece35d47fc48c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tUJXR9rDfBA-hg2ooQHmGw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图arch64 <a class="ae kb" href="https://developer.ridgerun.com/wiki/index.php?title=Nvidia_TX2_NVP_model" rel="noopener ugc nofollow" target="_blank">【维基百科】</a>的能量分布图</figcaption></figure><p id="9fd9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了使用任何配置文件。我个人偏好是Max-N；)</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="347b" class="kz la hi kv b fi lb lc l ld le">sudo nvpmodel -m &lt;mode number for desired profile&gt;</span></pre><p id="5fc7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 4。使用NVDLA深度学习推理编译器加速深度学习模型</strong></p><p id="b1b2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为深度学习设计新的定制硬件加速器显然很受欢迎，但通过新的设计实现最先进的性能和效率是一个复杂而具有挑战性的问题。</p><p id="041b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">NVIDIA深度学习加速器(NVDLA)是一个免费的开放架构，它促进了设计深度学习推理加速器的标准方法。凭借其模块化架构，NVDLA可扩展、高度可配置，旨在简化集成和可移植性。硬件支持广泛的物联网设备。</p><p id="688c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">NVDLA引入了模块化架构，旨在简化配置、集成和可移植性；它揭示了用于加速核心深度学习推理操作的构建模块。NVDLA硬件由以下组件组成:</p><ul class=""><li id="f9fe" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr ln ki kj kk bi translated"><strong class="iw hj">卷积核</strong> —优化的高性能卷积引擎。</li><li id="edf1" class="kc kd hi iw b ix kl jb km jf kn jj ko jn kp jr ln ki kj kk bi translated"><strong class="iw hj">单数据处理器</strong> —用于激活功能的单点查找引擎。</li><li id="4ec3" class="kc kd hi iw b ix kl jb km jf kn jj ko jn kp jr ln ki kj kk bi translated"><strong class="iw hj">平面数据处理器</strong> —用于汇集的平面平均引擎。</li><li id="dfdb" class="kc kd hi iw b ix kl jb km jf kn jj ko jn kp jr ln ki kj kk bi translated"><strong class="iw hj">通道数据处理器</strong> —用于高级标准化功能的多通道平均引擎。</li><li id="ae5e" class="kc kd hi iw b ix kl jb km jf kn jj ko jn kp jr ln ki kj kk bi translated"><strong class="iw hj">专用内存和数据整形引擎</strong> —用于张量整形和复制操作的内存到内存转换加速。</li></ul><p id="e382" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 5。最后，NVDLA的替代优化编译器是TVM编译器。</strong></p><p id="982a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae kb" href="https://tvm.ai/about" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj"> TVM </strong> </a>是一个开放的深度学习编译器堆栈，适用于CPU、GPU和专门的加速器。它旨在弥合以生产力为重点的深度学习框架和以性能或效率为导向的硬件后端之间的差距。TVM提供了以下主要功能:</p><ul class=""><li id="6176" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr ln ki kj kk bi translated">将Keras、MXNet、PyTorch、Tensorflow、CoreML、DarkNet中的深度学习模型编译成各种硬件后端上的最小可部署模块。</li><li id="6561" class="kc kd hi iw b ix kl jb km jf kn jj ko jn kp jr ln ki kj kk bi translated">在更多后端上自动生成和优化张量运算符的基础设施，性能更好。</li></ul><p id="95ca" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">参考文献—</p><ol class=""><li id="e3b7" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr kh ki kj kk bi translated"><a class="ae kb" href="http://nvdla.org" rel="noopener ugc nofollow" target="_blank">http://nvdla.org</a></li><li id="311d" class="kc kd hi iw b ix kl jb km jf kn jj ko jn kp jr kh ki kj kk bi translated"><a class="ae kb" href="https://docs.nvidia.com/jetson/index.html" rel="noopener ugc nofollow" target="_blank">https://docs.nvidia.com/jetson/index.html</a></li><li id="4e81" class="kc kd hi iw b ix kl jb km jf kn jj ko jn kp jr kh ki kj kk bi translated"><a class="ae kb" href="https://tvm.ai" rel="noopener ugc nofollow" target="_blank"> https://tvm.ai </a></li></ol></div></div>    
</body>
</html>