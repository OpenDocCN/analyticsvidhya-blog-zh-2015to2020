<html>
<head>
<title>Learn how to use spaCy for Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解如何使用spaCy进行自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/learn-how-to-use-spacy-for-natural-language-processing-661805d3abae?source=collection_archive---------5-----------------------#2019-10-11">https://medium.com/analytics-vidhya/learn-how-to-use-spacy-for-natural-language-processing-661805d3abae?source=collection_archive---------5-----------------------#2019-10-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es im"><img src="../Images/1193e3a30234021e2b1c4b23e4b96ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qH3Rrck6BGb8vrkNJCS0AQ.png"/></div></div></figure><h2 id="5deb" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">为什么我们需要NLP:</h2><p id="7b68" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg jj kh ki kj jn kk kl km jr kn ko kp kq hb bi translated">自然语言处理(NLP)是机器学习的一个领域，它具有计算机理解、分析、操纵和潜在生成人类语言的能力。在引擎盖下，机器学习算法只不过是一堆数学计算，显然，如果我们将单词/句子传递给机器学习模型，它们不知道如何处理。因此，我们需要做的是将它们转换成ml模型可以理解并对其执行操作的向量。这也是所有NLP库如spaCy，nltk出现的原因。</p><h2 id="624d" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">斯帕西。</h2><p id="2f48" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg jj kh ki kj jn kk kl km jr kn ko kp kq hb bi translated">spaCy为任何NLP项目中常用的任务提供一站式服务，包括:</p><ul class=""><li id="d681" class="kr ks hi jy b jz kt kd ku jj kv jn kw jr kx kq ky kz la lb bi translated">标记化</li><li id="12b6" class="kr ks hi jy b jz lc kd ld jj le jn lf jr lg kq ky kz la lb bi translated">词汇化</li><li id="39c3" class="kr ks hi jy b jz lc kd ld jj le jn lf jr lg kq ky kz la lb bi translated">词性标注</li><li id="b08e" class="kr ks hi jy b jz lc kd ld jj le jn lf jr lg kq ky kz la lb bi translated">依存句法分析</li><li id="e095" class="kr ks hi jy b jz lc kd ld jj le jn lf jr lg kq ky kz la lb bi translated">单词到向量的转换</li><li id="1674" class="kr ks hi jy b jz lc kd ld jj le jn lf jr lg kq ky kz la lb bi translated">许多清理和规范化文本的传统方法</li></ul><p id="4c40" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">在本文中，我们将只关注单词到矢量的转换。</p><h2 id="2338" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">让我们开始吧:</h2><p id="9713" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg jj kh ki kj jn kk kl km jr kn ko kp kq hb bi translated">但首先要做的是。我们必须知道如何安装spaCy，pip会为我们完成这项任务，只需执行两个命令，我们就万事大吉了。</p><pre class="in io ip iq fd lk ll lm ln aw lo bi"><span id="0c73" class="iy iz hi ll b fi lp lq l lr ls">pip install spacy</span><span id="d4ee" class="iy iz hi ll b fi lt lq l lr ls">python -m spacy download en_vectors_web_lg</span></pre><p id="a45a" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">第一个命令为我们安装spaCy，第二个命令下载包含内置单词向量的spaCy模型。</p><p id="f5b8" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">当我们安装完spaCy后，让我们使用下面的链接下载一个包含来自twitter的tweets的数据集。</p><div class="lu lv ez fb lw lx"><a href="https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech/" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hj fi z dy mc ea eb md ed ef hh bi translated">推特情感分析</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">www.kaggle.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml iw lx"/></div></div></a></div><p id="9003" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">现在您已经下载了数据集，让我们使用pandas加载数据</p><pre class="in io ip iq fd lk ll lm ln aw lo bi"><span id="2b1d" class="iy iz hi ll b fi lp lq l lr ls">&gt;&gt;data = pd.read_csv("train.csv")</span><span id="fbeb" class="iy iz hi ll b fi lt lq l lr ls">&gt;&gt;tweets = data.tweet[:100]</span></pre><p id="57c4" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">tweets变量只保存tweets列。让我们来看看排名前五的推文。出于学习目的，我们选取了前100条推文。</p><pre class="in io ip iq fd lk ll lm ln aw lo bi"><span id="08da" class="iy iz hi ll b fi lp lq l lr ls">&gt;&gt;tweets.head().tolist()</span><span id="119b" class="iy iz hi ll b fi lt lq l lr ls">[' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',</span><span id="549b" class="iy iz hi ll b fi lt lq l lr ls">"@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked",</span><span id="eceb" class="iy iz hi ll b fi lt lq l lr ls">'  bihday your majesty',</span><span id="2c76" class="iy iz hi ll b fi lt lq l lr ls">'#model   i love u take with u all the time in urð\x9f\x93±!!! ð\x9f\x98\x99ð\x9f\x98\x8eð\x9f\x91\x84ð\x9f\x91\x85ð\x9f\x92¦ð\x9f\x92¦ð\x9f\x92¦  ',</span><span id="2153" class="iy iz hi ll b fi lt lq l lr ls">' factsguide: society now    #motivation']</span></pre><p id="8cb2" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">在我们使用spaCy之前，我们需要清理数据，以便我们可以从中获得有意义的单词。我们不要浪费时间讨论如何清理数据，因为我们的主要重点是使用spaCy进行单词到矢量的转换，因此我将粘贴我的代码，它清理数据并跳过解释部分，因为您知道如何清理数据。</p><pre class="in io ip iq fd lk ll lm ln aw lo bi"><span id="7149" class="iy iz hi ll b fi lp lq l lr ls">""" Cleaning Tweets """<br/>tweets = tweets.str.lower()</span><span id="978f" class="iy iz hi ll b fi lt lq l lr ls"># removing special characters and numbers<br/>tweets = tweets.apply(lambda x : re.sub("[^a-z\s]","",x) )</span><span id="f015" class="iy iz hi ll b fi lt lq l lr ls"># removing stopwords<br/>from nltk.corpus import stopwords<br/>stopwords = set(stopwords.words("english"))<br/>tweets = tweets.apply(lambda x : " ".join(word for word in x.split() if word not in stopwords ))</span><span id="7bba" class="iy iz hi ll b fi lt lq l lr ls">&gt;&gt;tweets.head.tolist()</span><span id="f921" class="iy iz hi ll b fi lt lq l lr ls">['user father dysfunctional selfish drags kids dysfunction run',  'user user thanks lyft credit cant use cause dont offer wheelchair vans pdx disapointed getthanked', <br/>'bihday majesty',  <br/>'model love u take u time ur',  <br/>'factsguide society motivation']</span></pre><p id="3f27" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">我们已经清理了推文，让我们直接进入空间。</p><h2 id="2841" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">使用空间创建令牌:</h2><p id="2f95" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg jj kh ki kj jn kk kl km jr kn ko kp kq hb bi translated">使用spaCy创建令牌是小菜一碟。</p><pre class="in io ip iq fd lk ll lm ln aw lo bi"><span id="42ea" class="iy iz hi ll b fi lp lq l lr ls">import spacy<br/>import en_vectors_web_lg<br/></span><span id="abc2" class="iy iz hi ll b fi lt lq l lr ls">&gt;&gt;nlp = en_vectors_web_lg.load()<br/>&gt;&gt;document = nlp(tweets[0])<br/>&gt;&gt;print("Document : ",document)<br/>&gt;&gt;print("Tokens : ")<br/>&gt;&gt;for token in document:<br/>       print(token.text)</span><span id="8752" class="iy iz hi ll b fi lt lq l lr ls">Document : user father dysfunctional selfish drags kids dysfunction</span><span id="d0b4" class="iy iz hi ll b fi lt lq l lr ls">Tokens :<br/>run<br/>user <br/>father <br/>dysfunctional<br/>selfish<br/>drags<br/>kids <br/>dysfunction <br/>run</span></pre><p id="42c1" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">en_vectors_web_lg.load()加载spaCy模型并将其存储到nlp变量中。这个模型是用一百万个单词训练出来的。</p><p id="263c" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">在nlp(string)中，我们传递文档，然后文档被转换为“spacy.tokens.doc.Doc”并存储在一个变量文档中。我们可以看到，当我们打印一个文档时，它看起来像一个字符串，但不要误导自己，它实际上是可以迭代的空间对象。当空间对象被迭代时，我们得到的是令牌。</p><h2 id="3dd0" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">令牌到向量:</h2><p id="7780" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg jj kh ki kj jn kk kl km jr kn ko kp kq hb bi translated">从token到vector的道路也很容易。让我借助代码向你展示</p><pre class="in io ip iq fd lk ll lm ln aw lo bi"><span id="cb01" class="iy iz hi ll b fi lp lq l lr ls">&gt;&gt;document = nlp(tweets[0])<br/>&gt;&gt;print(document)<br/>&gt;&gt;for token in document:<br/>       print(token.text, token.vector.shape)</span><span id="1086" class="iy iz hi ll b fi lt lq l lr ls">user (300,)<br/>father (300,) <br/>dysfunctional (300,) <br/>selfish (300,) <br/>drags (300,) <br/>kids (300,) <br/>dysfunction (300,) <br/>run (300,)</span></pre><p id="77fc" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">“token.vector”创建一个大小为(300，1)的向量。上面的代码是从单个句子/文档的每个单词中获取vector。但是，如果我们的语料库中有100个这样的句子/文档，我们是否要迭代每个句子，为每个单词创建一个向量，然后将它们相加。不，那是错误的方法。我们能做的就是使用nlp.pipe()。</p><h2 id="8e33" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">使用管道的句子到向量:</h2><p id="659b" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg jj kh ki kj jn kk kl km jr kn ko kp kq hb bi translated">nlp.pipe()将文本<strong class="jy hj">作为流</strong>进行处理，并分批缓冲它们，而不是一个接一个，并将每个文档转换为spacy对象。这通常更有效。然后我们可以做的不是遍历文档的每个标记，而是遍历每个文档，得到文档的向量，而不是单词。是不是令人印象深刻，我觉得是。</p><pre class="in io ip iq fd lk ll lm ln aw lo bi"><span id="c9c3" class="iy iz hi ll b fi lp lq l lr ls">&gt;&gt;document = nlp.pipe(tweets)<br/>&gt;&gt;tweets_vector = np.array([tweet.vector for tweet in document])<br/>&gt;&gt;print(tweets_vector.shape)</span><span id="6114" class="iy iz hi ll b fi lt lq l lr ls">(100, 300)</span></pre><p id="9832" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">因此我们得到了300维的100条推文的向量。现在我们可以做的是，使用这些创建的向量来设计一个简单的模型，如逻辑回归，或SVM来检测言论是否是种族主义者。而且只有简单的模型我们也可以用这些向量来训练神经网络。</p><h2 id="0e99" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">逻辑回归模型:</h2><p id="cc01" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg jj kh ki kj jn kk kl km jr kn ko kp kq hb bi translated">由于我们已经清理了我们的推文，并将推文创建为向量，我们可以使用这些向量来预测推文是否是种族主义者。数据集中的“标签”列的值为0和1。0表示推文不是种族主义，1表示推文是种族主义。注意，为了建模，我取了整个数据集，而不仅仅是前100个点。因此，让我们通过创建一个简单的模型来进行预测。</p><pre class="in io ip iq fd lk ll lm ln aw lo bi"><span id="11e8" class="iy iz hi ll b fi lp lq l lr ls">from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score</span><span id="c426" class="iy iz hi ll b fi lt lq l lr ls">X = tweets_vector<br/>y = data["label"]</span><span id="4663" class="iy iz hi ll b fi lt lq l lr ls">X_train,X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.3, random_state=0)</span><span id="c6e5" class="iy iz hi ll b fi lt lq l lr ls">model = LogisticRegression(C=0.1)<br/>model.fit(X_train, y_train)</span><span id="de37" class="iy iz hi ll b fi lt lq l lr ls">y_pred = model.predict(X_test)<br/>print("Accuracy on test data is : %0.2f" %(accuracy_score(y_test, y_pred)*100))</span><span id="74ff" class="iy iz hi ll b fi lt lq l lr ls">y_train_pred = model.predict(X_train)<br/>print("Accuracy on train data is : %0.2f" %(accuracy_score(y_train, y_train_pred)*100))</span><span id="e827" class="iy iz hi ll b fi lt lq l lr ls">-&gt; Accuracy on test data is : 94.49 <br/>-&gt; Accuracy on train data is : 94.50</span></pre><p id="5d43" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">既然您已经熟悉了spaCy，那么您可以使用它，并从spaCy的文档中探索更多关于它的内容，因为我们只是触及了它的皮毛。像上面讨论的词性标注、实体识别之类的事情也可以使用spaCy来完成。</p><p id="e5bd" class="pw-post-body-paragraph jw jx hi jy b jz kt kb kc kd ku kf kg jj lh ki kj jn li kl km jr lj ko kp kq hb bi translated">谢谢你。</p></div></div>    
</body>
</html>