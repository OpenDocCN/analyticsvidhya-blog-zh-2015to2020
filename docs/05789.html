<html>
<head>
<title>An Introduction To Decision Tree.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树介绍。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-introduction-to-decision-tree-907ee016dce2?source=collection_archive---------20-----------------------#2020-05-02">https://medium.com/analytics-vidhya/an-introduction-to-decision-tree-907ee016dce2?source=collection_archive---------20-----------------------#2020-05-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6ff7b01f004ddb8e4477ce49ee0b2cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qXQrN5_vQ0XqAXHw-AaYJA.jpeg"/></div></div></figure><p id="c387" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">决策树在现实生活中有许多相似之处，事实证明，它影响了广泛的<strong class="is hj">机器学习领域，</strong>涵盖了<strong class="is hj">分类</strong>和<strong class="is hj">回归。</strong>有时决策树也被称为<strong class="is hj"> CART，</strong>是分类和回归树的简称。在决策分析中，决策树可用于直观、明确地表示决策和决策制定。</p><p id="64f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇博客中，我们将讨论以下内容:</p><ul class=""><li id="c5e0" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">什么是决策树？</em>T11】</strong></li><li id="952c" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">决策树的类型</em> </strong></li><li id="76d1" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">关键术语</em> </strong></li><li id="fd52" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">如何创建决策树</em> </strong></li><li id="fc9d" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">基尼杂质</em> </strong></li><li id="b7df" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">卡方</em> </strong></li><li id="88da" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">信息增益</em> </strong></li><li id="f904" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">决策树的应用</em> </strong></li><li id="e180" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">解码超参数</em> </strong></li><li id="2fc8" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">编码算法</em> </strong></li><li id="7a26" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">决策树的优缺点。</em>T51】</strong></li><li id="f089" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx">总结和结论。</em>T55】</strong></li></ul></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="b360" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated"><em class="li">为什么选择决策树？</em></h1><p id="95d8" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">基于树的算法是用于分类和回归的相关<strong class="is hj">非参数</strong>和<strong class="is hj">监督</strong>方法的流行家族。如果你想知道监督学习是什么，它是一种机器学习算法，涉及用具有输入和输出标签的数据训练模型。</p><p id="d77d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">决策树看起来像一个模糊的颠倒的树，在根上有一个决策规则，随后的决策规则从下面展开。</p><p id="7007" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">也可以有没有任何决策规则的节点；这些被称为<strong class="is hj">叶节点。</strong></p></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="44cc" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">决策树的类型</h1><p id="963f" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">根据目标变量，决策树分为两种类型。</p><ol class=""><li id="4df8" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn lo ju jv jw bi translated"><strong class="is hj">分类变量决策树:</strong>这是算法有分类目标变量的地方。例如，假设要求您预测一台计算机的相对价格，分为三类:<strong class="is hj">低、中、</strong>或<strong class="is hj">高。</strong></li><li id="2644" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn lo ju jv jw bi translated"><strong class="is hj">连续变量决策树:</strong>在这种情况下，输入到决策树的特征(如房屋质量)将用于预测连续输出(如房屋价格)。</li></ol></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="a401" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">关键术语</h1><p id="7282" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">让我们看看决策树是什么样子的，以及当给定一个新的预测输入时，它们是如何工作的。</p><p id="23bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下图解释了决策树的基本结构。每棵树都有一个<strong class="is hj">根节点</strong>，输入在这里传递。这个根节点被进一步划分成决策节点集，其中结果和观察是有条件的。将单个节点划分为多个节点的过程称为<strong class="is hj">分裂。</strong>如果一个节点没有分裂成更多的节点，那么它被称为<strong class="is hj">叶节点、</strong>或<strong class="is hj">终端节点。</strong>决策树的一个子部分被称为<strong class="is hj">分支</strong>或<strong class="is hj">子树。</strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/00357abe4dba05000727435b77f83871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*DculLPA-IJZr013VrhI9Nw.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">决策树的示例</figcaption></figure><p id="a021" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">还有一个概念与分裂完全相反。如果有可以消除的决策规则，我们就把它们从树上砍下来。这个过程被称为<strong class="is hj">修剪</strong>，有助于最小化算法的复杂性。</p><p id="a354" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们对基本的决策树有了一个清晰的概念。让我们深入了解一下拆分是如何完成的，以及我们如何自己构建一个决策树。</p></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="0c48" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">如何创建决策树</h1><p id="44da" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">在这一节中，我们将讨论描述如何创建决策树的核心算法。这些算法完全依赖于目标变量，然而，这些算法不同于用于分类和回归树的算法。</p><p id="d2d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有几种技术可以用来决定如何分割给定的数据。决策树的主要目标是在节点之间进行最佳分割，从而以最佳方式将数据划分到正确的类别中。为此，我们需要使用正确的决策规则。规则直接影响算法的性能。</p><p id="3b2c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们开始之前，需要考虑一些假设:</p><ul class=""><li id="2e28" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">在开始时，整个数据被认为是根，此后，我们使用算法进行分裂或将根分成子树。</li><li id="5194" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">特征值被认为是分类的。如果这些值是连续的，则在构建模型之前会将它们分开。</li><li id="89cc" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">记录是基于属性值递归分布的。</li><li id="50c8" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">属性作为树的根或内部节点的排序是使用统计方法来完成的。</li></ul><p id="97fb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们从常用的拆分技术开始，从而构建决策树。</p><h1 id="75e3" class="kk kl hi bd km kn ly kp kq kr lz kt ku kv ma kx ky kz mb lb lc ld mc lf lg lh bi translated">基尼杂质</h1><p id="83f9" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">如果所有的元素都被正确地划分到不同的类中(一个理想的场景)，那么划分被认为是<strong class="is hj">纯的。</strong>基尼系数(发音类似“精灵”)用于衡量随机选择的样本被某个节点错误分类的可能性。它被称为“杂质”度量，因为它让我们了解模型与纯除法有什么不同。</p><p id="5f27" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基尼杂质分数的程度总是在0到1之间，其中0表示所有元素都属于某一类(或者划分是纯的)，1表示元素随机分布在各个类中。基尼系数为0.5表示元素被平均分配到某些类别中。基尼系数的数学符号由以下公式表示:</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/7e5e49f2c06fe5b52988a4e696b607e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q5rkuoTMAh7sFjft5Ukxqg.png"/></div></div></figure><p id="b9b2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<strong class="is hj"> <em class="jx"> Pi </em> </strong>是特定元素属于特定类的概率。</p><p id="2047" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们看看使用基尼系数作为指导来计算和构建决策树的伪代码。</p><pre class="lq lr ls lt fd me mf mg mh aw mi bi"><span id="1247" class="mj kl hi mf b fi mk ml l mm mn">Gini Index:<br/>    for each branch in a split:<br/>        Calculate percent branch represents  # Used for weighting<br/>        for each class in-branch:<br/>            Calculate the probability of that class in the given branch<br/>            Square the class probability<br/>        Sum the squared class probabilities<br/>        Subtract the sum from 1  # This is the Gini Index for that branch<br/>    Weight each branch based on the baseline probability<br/>    Sum the weighted Gini index for each split</span></pre><p id="ad37" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们现在来看一个解释上述算法的简单例子。考虑下面的数据表，其中每个元素(行)有两个描述它的变量和一个相关的类标签。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/cdce7c4a72cd5fd45953f0576133eab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*VJj58_uOouOEDXrvhH28yg.png"/></div></figure><p id="dfe6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">姬内指数示例:</p><ul class=""><li id="e849" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated"><strong class="is hj"> <em class="jx"> Var1的拆分基线:Var1 </em> </strong>有4个实例(4/10)等于1，6个实例(6/10)等于0。</li><li id="74af" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">for<strong class="is hj"><em class="jx">Var1</em></strong>= = 1&amp;<strong class="is hj"><em class="jx">Class</em></strong>= =<strong class="is hj"><em class="jx">A</em></strong>:1/4实例有Class等于<em class="jx"> A </em>。</li><li id="1735" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">对于<strong class="is hj"><em class="jx">var 1</em></strong>= = 1&amp;<strong class="is hj"><em class="jx">Class</em></strong>= =<strong class="is hj"><em class="jx">B</em></strong>:3/4实例有Class等于<em class="jx"> B </em>。</li><li id="3f92" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">这里的基尼指数是1-((1/4) + (3/4) ) = 0.375</li><li id="90b1" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">对于<strong class="is hj"><em class="jx">Var1</em></strong>= = 0&amp;<strong class="is hj"><em class="jx">类</em></strong><em class="jx"/>= =<strong class="is hj"><em class="jx">A</em></strong>:4/6实例有类等于<em class="jx"> A </em>。</li><li id="4f10" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">对于<strong class="is hj"><em class="jx">Var1</em></strong>= = 0&amp;<strong class="is hj"><em class="jx">Class</em>=</strong>=<strong class="is hj"><em class="jx">B</em></strong>:2/6实例有Class等于<em class="jx"> B </em>。</li><li id="e2b5" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj">基尼指数</strong>这里是1-((4/6) + (2/6) ) = 0.4444</li><li id="dd3a" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">然后，我们根据每个拆分所占数据的基线/比例，对每个拆分进行加权和求和。</li><li id="8d48" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi">4/10 * 0.375 + 6/10 * 0.444 = 0.41667</li></ul></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="43a4" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">信息增益</h1><p id="e423" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">信息增益描述了通过属性获得的信息量。它告诉我们属性有多重要。由于决策树的构建完全是为了找到确保高准确性的正确分裂节点，所以信息增益完全是为了找到返回最高信息增益的最佳节点。这是使用称为<strong class="is hj">熵</strong>的因子<strong class="is hj"> </strong>计算的。</p><p id="c405" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">熵定义了系统的无序程度。无序越多，熵就越大。当样本是完全同质的，那么熵是零，如果样本是部分有序的，比如说50%的样本是有序的，那么熵是一。</p><p id="09f8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这作为确定信息增益的基本因素。熵和信息增益一起用来构造决策树，算法称为<strong class="is hj"> ID3 </strong>。</p><p id="67ab" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们理解用于计算信息增益的一步一步的过程，从而构建决策树，</p><ul class=""><li id="323a" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">使用以下公式计算输出属性(分割前)的熵:</li></ul><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/6daf36325794e6d5bafab4346f129286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GKdJNqCUXRjzVl6j_Rcu3w.png"/></div></div></figure><p id="3fbc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里p是成功的概率，q是节点失败的概率。比如说，10个数据值中，5个属于<em class="jx">真</em>，5个属于<em class="jx">假</em>，那么<em class="jx"> c </em>计算为2，<em class="jx"> p_1 </em>和<em class="jx"> p_2 </em>计算为。</p><ul class=""><li id="2a70" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">使用公式计算所有输入属性的熵，</li></ul><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/7cbafa20f79f381884b0615c1412302f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2UbwjSAqoiUQkDMcdLrIOQ.png"/></div></div></figure><p id="6fed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">t是输出属性，</p><p id="3b5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">x是输入属性，</p><p id="47a7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">P(c)是X处可能出现的数据点的概率，以及</p><p id="f236" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">E(c)是与可能的数据点相关的熵w . r . t '' True'。</p><p id="7848" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设输入属性(优先级)有两个可能的值，低<em class="jx">和高</em>和<em class="jx">高</em>。关于<em class="jx">低，</em>有5个数据点相关，其中2个属于<em class="jx">真</em>，3个属于<em class="jx">假。</em>相对于<em class="jx">高</em>，其余5个数据点相关联，其中4个属于<em class="jx">真</em>，1个属于<em class="jx">假。那么E(T，X)就是，</em></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/2111aa8f23d521e5d9ded114eff59327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ud-5oOJFO48R4oUEqiosQ.png"/></div></div></figure><p id="7f34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在E(2，3)中，p是2，q是3。</p><p id="d0a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在E(4，1)中，p是4，q是1。</p><p id="7c60" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对给定数据集中的所有输入属性重复进行相同的计算。</p><ul class=""><li id="e1c7" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">使用上述两个值，通过从分割前的总熵中减去每个属性的熵来计算信息增益或熵的减少，</li></ul><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/248ab10d1f8b3e78d772723074c289ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HSh5x1n-evMPSXo7Ac1Y8w.png"/></div></div></figure><ul class=""><li id="509f" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">选择具有最高信息增益的属性作为分割节点。</li><li id="2223" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">根据分割重复步骤1-4，分割数据集。该算法一直运行，直到所有数据都被分类。</li></ul><p id="0cad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">要点记住:</strong></p><ul class=""><li id="e292" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">叶节点是没有熵的节点，或者熵为零的节点。在叶节点上不再进行进一步的分割。</li><li id="438e" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">只有需要进一步分裂的分支，即熵&gt; 0时(有杂质时)才需要经历这个分裂过程。</li></ul></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="0a50" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">卡方检验</h1><p id="d772" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">如果目标变量是分类的，如成功-失败/高-低，卡方方法很有效。该算法的核心思想是找出子节点和父节点之间存在的差异的统计显著性。用于计算卡方的数学方程是，</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/476b524af0c0b90bcf706b9c85f7a219.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zS6BjxRhZT0JZTVvIwwfRA.png"/></div></div></figure><p id="af36" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它代表目标变量的观察频率和预期频率之间的标准化差异的平方和。</p><p id="dda8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用卡方的另一个主要优势是，它可以在单个节点上执行多次分割，从而提高准确度和精度。</p></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="1af6" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">决策树的应用</h1><p id="fd94" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">决策树是机器学习领域中最基本也是最广泛使用的算法之一。它在分类和回归建模的不同领域得到了应用。由于其描绘可视化输出的能力，人们可以很容易地从建模过程流中获得洞察力。这里有几个可以使用决策树的例子，</p><ul class=""><li id="1a8c" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">企业管理</li><li id="c34b" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">客户关系管理</li><li id="1455" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">欺诈性声明检测</li><li id="8c9f" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">能耗</li><li id="b03c" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">医疗保健管理</li><li id="3e38" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">故障诊断</li></ul></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="f821" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">解码超参数</h1><p id="eaf6" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">Scikit-learn提供了一些与决策树分类器一起使用的功能或参数，以根据给定的数据提高模型的准确性。</p><ul class=""><li id="782f" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated"><strong class="is hj">判据:</strong>该参数用于衡量分割的质量。该参数的默认值设置为“Gini”。如果你想用熵增益来计算测度，可以把这个参数改成“熵”。</li><li id="405f" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj">分割器</strong>:该参数用于选择每个节点的分割器。如果希望子树具有最佳分割，可以将该参数设置为“最佳”。我们还可以有一个随机分割，其值设置为“随机”。</li><li id="7eef" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> max-depth: </strong>这是一个整数参数，通过它我们可以限制树的深度。该参数的默认值设置为None。</li><li id="b4c0" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> min_samples_split: </strong>该参数用于定义分割一个内部节点所需的最小样本数。</li><li id="419e" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj">max _ leaf _ nodes:</strong>max _ leaf _ nodes的默认值设置为无。该参数用于以最佳优先的方式生长具有max_leaf_nodes的树。</li></ul></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="91d0" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">算法编码</h1><h2 id="5a01" class="mj kl hi bd km mt mu mv kq mw mx my ku jb mz na ky jf nb nc lc jj nd ne lg nf bi translated">步骤1:导入模块</h2><p id="e4a5" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">构建决策树模型的第一步也是最重要的一步是导入必要的包和模块。我们从<strong class="is hj"> sklearn </strong>包中导入<strong class="is hj"> DecisionTreeClassifier </strong>类。这是一个内置的类，其中编码了整个决策树算法。在这个程序中，我们将使用可以从<strong class="is hj"> sklearn.datasets </strong>导入的<strong class="is hj"> iris </strong>数据集。<strong class="is hj"> pydotplus </strong>包用于可视化决策树。下面是代码片段，</p><pre class="lq lr ls lt fd me mf mg mh aw mi bi"><span id="e614" class="mj kl hi mf b fi mk ml l mm mn">import pydotplus<br/>from sklearn.tree import DecisionTreeClassifier <br/>from sklearn import datasets</span></pre><h2 id="61aa" class="mj kl hi bd km mt mu mv kq mw mx my ku jb mz na ky jf nb nc lc jj nd ne lg nf bi translated">步骤2:探索数据</h2><p id="0ae8" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">接下来，我们通过使用<strong class="is hj"> load_iris() </strong>方法从datasets包中加载数据来准备好数据。我们将数据分配给<strong class="is hj">虹膜</strong>变量。这个iris变量有两个键，一个是<strong class="is hj">数据</strong>键，其中显示所有输入，即萼片长度、萼片宽度、花瓣长度和花瓣宽度。在<strong class="is hj"> target </strong>键中，我们有花朵类型，其值为鸢尾、杂色鸢尾和海滨鸢尾。我们将这些分别加载到<strong class="is hj">特征</strong>和<strong class="is hj">目标</strong>变量中。</p><pre class="lq lr ls lt fd me mf mg mh aw mi bi"><span id="3673" class="mj kl hi mf b fi mk ml l mm mn">iris = datasets.load_iris()  <br/>features = iris.data  <br/>target = iris.target  <br/>print(features) <br/>print(target)</span><span id="034a" class="mj kl hi mf b fi ng ml l mm mn">Output:  <br/>[[5.1 3.5 1.4 0.2]  <br/>[4.9 3.  1.4 0.2]  <br/>[4.7 3.2 1.3 0.2]  <br/>[4.6 3.1 1.5 0.2]  <br/>[5.8 4.  1.2 0.2]  <br/>[5.7 4.4 1.5 0.4] <br/>. . . .  <br/>. . . . ] <br/>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]</span></pre><p id="15ab" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是我们的数据集的样子。</p><h2 id="d0ac" class="mj kl hi bd km mt mu mv kq mw mx my ku jb mz na ky jf nb nc lc jj nd ne lg nf bi translated">步骤3:创建决策树分类器对象</h2><p id="ddba" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">这里，我们将<strong class="is hj"> DecisionTreeClassifier </strong>加载到一个名为<strong class="is hj"> model </strong>的变量中，这个变量是之前从<strong class="is hj"> sklearn </strong>包中导入的。</p><pre class="lq lr ls lt fd me mf mg mh aw mi bi"><span id="acca" class="mj kl hi mf b fi mk ml l mm mn">decisiontree = DecisionTreeClassifier(random_state=0)</span></pre><h2 id="8dc3" class="mj kl hi bd km mt mu mv kq mw mx my ku jb mz na ky jf nb nc lc jj nd ne lg nf bi translated">步骤4:拟合模型</h2><p id="5360" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">这是训练过程的核心部分，通过对给定数据进行分割来构建决策树。我们用作为参数发送给<strong class="is hj"> fit() </strong>方法的<strong class="is hj">特征</strong>和<strong class="is hj">目标</strong>值来训练算法。这种方法是通过在特征和目标上训练模型来拟合数据。</p><pre class="lq lr ls lt fd me mf mg mh aw mi bi"><span id="69c5" class="mj kl hi mf b fi mk ml l mm mn">model = decisiontree.fit(features, target)</span></pre><h2 id="6317" class="mj kl hi bd km mt mu mv kq mw mx my ku jb mz na ky jf nb nc lc jj nd ne lg nf bi translated">第五步:做预测</h2><p id="72aa" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">在这一步中，我们进行样本观察并做出预测。我们创建了一个新的列表，包括花的萼片和花瓣的尺寸。此外，我们在经过训练的<strong class="is hj"> </strong>模型上使用<strong class="is hj"> predict() </strong>方法来检查它所属的类。我们还可以通过使用<strong class="is hj"> predict_proba </strong>方法来检查预测的概率(类概率)。</p><pre class="lq lr ls lt fd me mf mg mh aw mi bi"><span id="de1b" class="mj kl hi mf b fi mk ml l mm mn">observation = [[ 5, 4, 3, 2]] # Predict observation's class<br/>model.predict(observation)<br/>model.predict_proba(observation)</span><span id="64d3" class="mj kl hi mf b fi ng ml l mm mn">Output:<br/>array([1])<br/>array([[0., 1., 0.]])</span></pre><h2 id="7331" class="mj kl hi bd km mt mu mv kq mw mx my ku jb mz na ky jf nb nc lc jj nd ne lg nf bi translated"><strong class="ak">步骤6:用于预测的点数据</strong></h2><p id="adb8" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">在这一步中，我们以点格式(一种图形描述语言)导出训练好的模型。为了实现这一点，我们使用了可以从<strong class="is hj"> sklearn </strong>包导入的<strong class="is hj">树</strong>类。最重要的是，我们使用<strong class="is hj"> export_graphviz </strong>方法，将决策树、特性和目标变量作为参数。</p><pre class="lq lr ls lt fd me mf mg mh aw mi bi"><span id="dec1" class="mj kl hi mf b fi mk ml l mm mn">from sklearn import tree</span><span id="e5ba" class="mj kl hi mf b fi ng ml l mm mn">dot_data = tree.export_graphviz(decisiontree, out_file=None,<br/>feature_names=iris.feature_names, <br/>class_names=iris.target_names<br/>)</span></pre><h2 id="7037" class="mj kl hi bd km mt mu mv kq mw mx my ku jb mz na ky jf nb nc lc jj nd ne lg nf bi translated"><strong class="ak">第七步:绘制图形</strong></h2><p id="70ae" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">在最后一步，我们使用一个从<strong class="is hj"> IPython.display </strong>包导入的<strong class="is hj"> Image </strong>类来可视化决策树。</p><pre class="lq lr ls lt fd me mf mg mh aw mi bi"><span id="cab9" class="mj kl hi mf b fi mk ml l mm mn">from IPython.display import Image</span><span id="30c7" class="mj kl hi mf b fi ng ml l mm mn">graph = pydotplus.graph_from_dot_data(dot_data) # Show graph</span><span id="8dd4" class="mj kl hi mf b fi ng ml l mm mn">Image(graph.create_png())</span></pre><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/7601b1f4d003816d31d0a84aba43b01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8TZsdumiM01i_seoCnO9rA.png"/></div></div></figure></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="e3fe" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">优点和缺点</h1><p id="4f8e" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">决策树有一些优点和缺点。先说优点。与其他算法相比，决策树在处理数据时花费的时间非常少。可以跳过一些预处理步骤，如数据的标准化、转换和缩放。尽管数据集中存在缺失值，但模型的性能不会受到影响。决策树模型直观且易于向技术团队和利益相关者解释，并且可以跨多个组织实施。</p><p id="45d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">缺点来了。不确定树，数据的微小变化会导致决策树结构的巨大变化，进而导致不稳定。训练时间急剧增加，与数据集的大小成比例。在某些情况下，与其他传统算法相比，计算可能会变得复杂。</p></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="2161" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">总结和结论</h1><p id="05b7" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">在本文中，我们已经深入讨论了决策树算法。这是一种监督学习算法，可用于分类和回归。决策树的主要目标是根据一组规则和条件将数据集分割成一棵树。我们讨论了决策树的关键组成部分，如根节点、叶节点、子树、分裂和修剪。此外，我们已经看到了决策树是如何工作的，以及如何使用流行的算法如GINI、信息增益和卡方来执行战略分割。此外，我们使用scikit-learn在IRIS数据集上从头开始编码决策树。最后，我们讨论了使用决策树的优点和缺点。还有很多东西需要学习，本文将为您提供探索其他高级分类算法的快速入门。</p></div></div>    
</body>
</html>