<html>
<head>
<title>Unsupervised Learning with k-means part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k均值无监督学习第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/unsupervised-learning-with-k-means-part-1-32f19670e8cc?source=collection_archive---------15-----------------------#2019-12-28">https://medium.com/analytics-vidhya/unsupervised-learning-with-k-means-part-1-32f19670e8cc?source=collection_archive---------15-----------------------#2019-12-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7179" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在机器学习中，学习的类型可以大致分为3种类型:1 .监督学习，2。无监督学习和3。半监督学习。属于无监督学习家族的算法没有依赖于数据的预测变量。数据没有输出，只有输入，输入是描述数据的多个变量。这就是集群的由来。</p><p id="b757" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">聚类的任务是将一组对象组合在一起，使同一聚类中的对象彼此之间比其他聚类中的对象更相似。相似性是反映两个数据对象之间关系强度的度量。距离是用于测量相似性的最常见的度量。</p><p id="a6a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将介绍k-means。</p><p id="f2db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文中使用的数据集可从<a class="ae jd" href="https://archive.ics.uci.edu/" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu</a>获得</p><p id="2cf5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://archive.ics.uci.edu/ml/datasets/Activity+recognition+with+healthy+older+people+using+a+batteryless+wearable+sensor" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/datasets/Activity+recognition+with+healthy+older+people+using+a+无电池+可穿戴+传感器# </a></p><p id="7503" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">摘自Shinmoto Torres，R. L .，Ranasinghe，D. C .，Shi，q .，Sample，A. P. (2013年4月)。传感器支持的可穿戴RFID技术，用于降低在病床附近跌倒的风险。2013年IEEE RFID国际会议(第191-198页)。IEEE。</p><p id="aba8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的github repo中有这个笔记本</p><p id="7b8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://github.com/bentechno/datasciencecourses/blob/dev/1BlogPostMonthKmeans.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/bentechno/datasciencecourses/blob/dev/1 blogpostmonthkmeans . ipynb</a></p><p id="0a8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">k-means是商业世界中最常用的数据聚类(无监督学习)算法，这得益于它的主要优势:</p><ul class=""><li id="6cc5" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">良好的(或可接受的)准确性和性能。</li><li id="8656" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">其可解释性，集群可以很容易地向商业人士解释。</li></ul><p id="5294" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">K-means算法并不复杂。维基百科的定义是:<em class="js"> k </em> -means clustering旨在<a class="ae jd" href="https://en.wikipedia.org/wiki/Partition_of_a_set" rel="noopener ugc nofollow" target="_blank">将</a> <em class="js"> n </em>个观测值划分成<em class="js"> k </em>个簇，其中每个观测值都属于与<a class="ae jd" href="https://en.wikipedia.org/wiki/Mean" rel="noopener ugc nofollow" target="_blank">means</a>最近的<a class="ae jd" href="https://en.wikipedia.org/wiki/Cluster_(statistics)" rel="noopener ugc nofollow" target="_blank">个簇</a>，作为簇的原型。<em class="js"> k </em> -Means最小化类内方差(平方欧几里德距离)，但不是常规欧几里德距离，这将是更困难的:平均值优化平方误差，而只有几何中值最小化欧几里德距离。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es jt"><img src="../Images/4b9e019bf92ea4507de6c22bc51a6149.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*nnbofSbgDZ50YvmuHhn9bg.png"/></div></figure><p id="46bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<em class="js"> ml </em>是类Ci中元素的平均值。在这种情况下，聚类的数量是预先固定的，并且它代表算法的唯一参数，均值被递归地估计。</p><p id="4b12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，这种算法也有一些缺点:聚类数K必须预先固定，它收敛于局部极小值，不适合非凸结构和不同大小的类。但是一些预处理步骤(即特征工程)可以帮助解决这个问题。</p><p id="43d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">笔记本的第一段，在下面几行显示了如何加载数据。您的数据位于data/Datasets _ Healthy _ Older _ People/dataset文件夹</p><pre class="ju jv jw jx fd kb kc kd ke aw kf bi"><span id="549f" class="kg kh hi kc b fi ki kj l kk kl">import pandas as pd<br/>import glob, os</span><span id="d3dd" class="kg kh hi kc b fi km kj l kk kl">pathFiles = r’data/Datasets_Healthy_Older_People/dataset’<br/>all_files = glob.glob(os.path.join(pathFiles, “*”))<br/>li = []<br/>for filename in all_files:<br/>    df = pd.read_csv(filename, index_col=None, header= None, names=[‘timeSec’, ‘accFrontal’, ‘accVertical’, ‘accLateral’, ‘antennaID’, ‘rssi’, ‘phase’, ‘frequency’, ‘activity’])<br/>    li.append(df)<br/>dataAll = pd.concat(li, axis=0, ignore_index=True)</span></pre><p id="e526" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将使用scikit learn来执行kmeans，更多信息请访问<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn . org/stable/modules/generated/sk learn . cluster . k means . html</a></p><p id="4bfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你对从头实现kmeans感兴趣，你可以在这里找到一个帖子<a class="ae jd" rel="noopener" href="/machine-learning-algorithms-from-scratch/k-means-clustering-from-scratch-in-python-1675d38eee42">https://medium . com/machine-learning-algorithms-from-scratch/k-means-clustering-from-scratch-in-python-1675d 38 eee 42</a></p><p id="e38a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们将数据集分为一个训练集和一个测试集，并删除一些不相关的列，如antennaID、activity和frequency</p><pre class="ju jv jw jx fd kb kc kd ke aw kf bi"><span id="649b" class="kg kh hi kc b fi ki kj l kk kl">import sklearn<br/>dataAll1, dataAll2= sklearn.model_selection.train_test_split(dataAll, test_size=0.4, random_state=0)<br/>dataAllPart = dataAll2.drop(['antennaID','activity', 'frequency'], axis=1)</span></pre></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><p id="c0c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">聚类评估:剪影评分</strong><a class="ae jd" href="https://blog.floydhub.com/introduction-to-k-means-clustering-in-python-with-scikit-learn/" rel="noopener ugc nofollow" target="_blank">https://blog . floydhub . com/introduction-to-k-means-clustering-in-python-with-scikit-learn/</a></p><p id="c9ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在确定聚类的数量之前，我们可以尝试一系列的数字，以确保我们选择的是最佳的数字。这里有两件事需要考虑:</p><ol class=""><li id="1670" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc ku jk jl jm bi translated">如果我们有可用的数据点的基本事实标签(类别信息)(这里不是这种情况),那么我们可以利用外部方法，如<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html" rel="noopener ugc nofollow" target="_blank">同质性得分</a>、<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html#sklearn.metrics.completeness_score" rel="noopener ugc nofollow" target="_blank">完整性得分</a>等等。</li><li id="57d3" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc ku jk jl jm bi translated">但是，如果我们没有数据点的地面真实标签，我们将不得不使用固有方法，如基于剪影系数的<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html" rel="noopener ugc nofollow" target="_blank">剪影得分</a>。我们现在更详细地研究这个评估指标。</li></ol><p id="315d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从计算特定数据点的轮廓系数的等式开始:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kv"><img src="../Images/83c0e20995e89d2c6bf592a643cce06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*uYUnbsdWYX-jklF_IaK6_Q.png"/></div></figure><p id="d989" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里，</p><p id="44b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-<em class="js">s(</em><strong class="ih hj"><em class="js">o</em></strong><em class="js">)</em>为数据点<strong class="ih hj"> o </strong>的轮廓系数</p><p id="889f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-<em class="js">a(</em><strong class="ih hj"><em class="js">o</em></strong><em class="js">)</em>是<strong class="ih hj"> o </strong>与<strong class="ih hj"> o </strong>所属聚类中所有其他数据点之间的<strong class="ih hj">平均距离</strong></p><p id="3a09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-<em class="js">b(</em><strong class="ih hj"><em class="js">o</em></strong><em class="js">)</em>是从<strong class="ih hj"> o </strong>到<strong class="ih hj"> o </strong>不属于的所有聚类的<strong class="ih hj">最小平均距离</strong></p><p id="f3d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">轮廓系数的值在[-1，1]之间。分数1表示最好的含义，即数据点<strong class="ih hj"> o </strong>在其所属的聚类内非常紧凑，并且远离其他聚类。最差的值是-1。接近0的值表示重叠的簇。</p><p id="4195" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用scikit-learn，您可以使用一系列的聚类数(这里是2到7)非常容易地计算所有数据点的轮廓系数:</p><pre class="ju jv jw jx fd kb kc kd ke aw kf bi"><span id="ae4d" class="kg kh hi kc b fi ki kj l kk kl">from yellowbrick.cluster import KElbowVisualizer<br/>kmeans = KMeans(random_state=120)<br/># Instantiate the KElbowVisualizer with the number of clusters and the metric</span><span id="650f" class="kg kh hi kc b fi km kj l kk kl">visualizer = KElbowVisualizer(kmeans, k=(2,7), metric='silhouette')<br/>visualizer.fit(dataAllPart)<br/>visualizer.poof()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kw"><img src="../Images/84786a4cc7bb31288fd3784c7a859c95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*d-1sxs8Aubvpvmj-aPEZwg.png"/></div></figure><p id="8b4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">silouhette对于2个类更高，这意味着2个类是选择的最佳类数。现在，让我们用两个集群来构建我们的模型。</p><pre class="ju jv jw jx fd kb kc kd ke aw kf bi"><span id="8fe1" class="kg kh hi kc b fi ki kj l kk kl"># exploring the two classes<br/>kmeans = KMeans(n_clusters=2)<br/>dataAll21Class = kmeans.fit(dataAllPart)</span></pre><p id="c6f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们做一个散点图来检查是否有区别特征。</p><pre class="ju jv jw jx fd kb kc kd ke aw kf bi"><span id="eeb1" class="kg kh hi kc b fi ki kj l kk kl">import seaborn as sns<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from pylab import savefig</span><span id="caf3" class="kg kh hi kc b fi km kj l kk kl">sns.set()<br/>dataAllPart['label'] = dataAll21Class.labels_.astype(str)<br/>g = sns.PairGrid(dataAllPart, hue="label", hue_order=["0", "1"],<br/>palette=["b", "r"],<br/>hue_kws={"s": [20, 20], "marker": ["o", "o"]})<br/>g.map(plt.scatter, linewidth=1, edgecolor="w")<br/>g.add_legend()<br/>plt.savefig('scatterMatrix.PNG', dpi=700)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kx"><img src="../Images/696de407f1efe1b387246c95772a8cd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*6SvLSiaOXvIoy23H8Q9Fuw.png"/></div></figure><p id="ad41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，时间秒是最有区别的特征，因为无论散点图中的时间秒轴使用什么轴，都可以很好地区分这两个类。</p><p id="ebcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一种可用于评估聚类的方法是PCA。主成分分析(PCA)是一种统计过程，它使用正交变换将一组可能相关的变量(每个变量取不同数值的实体)的观察值转换为一组称为主成分的线性不相关变量的值。更多信息请访问<a class="ae jd" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Principal_component_analysis</a>和<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn . org/stable/modules/generated/sk learn . decomposition . PCA . html</a></p><p id="baa1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在2个第一主成分的绘图之后:</p><pre class="ju jv jw jx fd kb kc kd ke aw kf bi"><span id="d243" class="kg kh hi kc b fi ki kj l kk kl">from sklearn.decomposition import PCA<br/>import matplotlib.pyplot as plt</span><span id="1124" class="kg kh hi kc b fi km kj l kk kl">pcaGen = PCA(n_components=2).fit(dataAllPart)<br/>pcaGen2Cmp = pcaGen.transform(dataAllPart)<br/>plt.scatter(pcaGen2Cmp[:,0], pcaGen2Cmp[:,1], c=col, marker = ‘o’)<br/>plt.savefig(“pca2.png”)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ky"><img src="../Images/51a959b448806156fedc72e6949704fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*z2aJlLP8NWrAUnz7LPe3VQ.png"/></div></figure><p id="3a2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该图显示了根据类别很好地区分了这些点。这证明聚类工作良好。</p><p id="7586" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些群集可进一步用于其他目的，例如:</p><p id="614b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在监督学习中:我们可以使用我们的聚类结果将时间划分为两类(标签是聚类)，在带标签的数据上训练监督模型。</p><p id="8d0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章是对k-means的介绍，我介绍了在没有先验信息和标签的情况下，使用k-means构建聚类所需的关键概念和材料。我没有讨论特征工程，包括处理分类特征、缩放等。这不是我目前职位的目的。</p><p id="cb77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在以后的文章中，我们将讨论这些项目。</p></div></div>    
</body>
</html>