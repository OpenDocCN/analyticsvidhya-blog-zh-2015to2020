<html>
<head>
<title>Mathematics behind Random forest and XGBoost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林和XGBoost背后的数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mathematics-behind-random-forest-and-xgboost-ea8596657275?source=collection_archive---------1-----------------------#2019-10-09">https://medium.com/analytics-vidhya/mathematics-behind-random-forest-and-xgboost-ea8596657275?source=collection_archive---------1-----------------------#2019-10-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8f2194a019cfdb596c352fec775ed4ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sAefX-ePZFY4ZEH2.jpeg"/></div></div></figure><h1 id="1242" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是合奏？</h1><p id="b616" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">Ensemble的意思是集合或一组事物。</p><p id="b30c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">集成学习</strong>是一种机器学习技术，它结合了几个基本模型，以便产生一个最佳预测模型(强大的模型)。</p><p id="bf8b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">集成方法允许我们考虑决策树的样本，计算在每次分割时使用哪些特征或询问哪些问题，并基于样本决策树的聚合结果做出最终预测。</p><h1 id="4e81" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">集合方法的类型:</h1><ol class=""><li id="2e31" class="kr ks hi jq b jr js jv jw jz kt kd ku kh kv kl kw kx ky kz bi translated"><strong class="jq hj"><em class="la"/></strong>ing，或者<strong class="jq hj"><em class="la">B</em></strong>ootstrap<strong class="jq hj"><em class="la">AGG</em></strong>regating(随机森林)</li><li id="b293" class="kr ks hi jq b jr lb jv lc jz ld kd le kh lf kl kw kx ky kz bi translated">助推</li><li id="9ce9" class="kr ks hi jq b jr lb jv lc jz ld kd le kh lf kl kw kx ky kz bi translated">堆垛</li><li id="ea2c" class="kr ks hi jq b jr lb jv lc jz ld kd le kh lf kl kw kx ky kz bi translated">级联</li></ol><h1 id="4fd2" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> <em class="lg"> BAGG </em> </strong> ing，或<strong class="ak"><em class="lg">B</em></strong>oot strap<strong class="ak"><em class="lg">AGG</em></strong>regation:</h1><p id="b7ad" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">所有采样都是在替换采样时完成的。每个模型都是根据不同的数据子集构建的。如果模型随着训练数据的变化而变化很大，则该模型被称为具有高方差。因此bagging是一个在不影响偏差的情况下减少模型方差的概念。</p><p id="3767" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">装袋=DT +行取样</p><p id="e7fa" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">Bagging是Bootstrap过程在高方差机器学习算法(通常是决策树)中的应用。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/7ab8e064a3f07fc2e685850d37cf03ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*o5MR1DB_zpV8azm8.png"/></div></div></figure><p id="6c4a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">通过为每个模型获取一个小样本数据集，聚合模型不会有太大变化，因为它只影响数据集的一小部分。</p><p id="94a7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">典型的聚合运算是均值/中值。在回归的情况下是平均值/中值，在分类问题的情况下是多数投票。最终的聚集步骤是最终的模型(h)。</p><p id="c3c2" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">采用一组低偏差和高方差模型(h1、h2、h3、h4…)并将其与bagging结合。你会得到低偏差和减少方差的模型(h)。深度非常高的决策树。</p><p id="8940" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">当使用决策树进行打包时，我们不太担心个别树会过度拟合训练数据。出于这个原因并且为了效率，单独的决策树生长得更深(例如，在树的每个叶节点处很少训练样本),并且不修剪这些树。这些树将具有高方差和低偏差。当使用bagging组合预测时，这些是子模型的重要特征。</p><p id="b57c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">随机森林最流行的Bagging模型，现在每天用于低偏差和高方差数据集。</p><h2 id="3855" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated"><strong class="ak">随机森林:</strong></h2><p id="66ea" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">随机森林以决策树为基础进行行抽样和列抽样。模型h1、h2、h3、h4与仅装袋相比，差异更大，因为采用了柱取样。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/c3dd54d2512b777f596ad642b7cc73ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*Ny-N_sT3lKZdzGPcFVPxLw.png"/></div></figure><p id="585d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">随着基础学习者数量(k)的增加，方差会减小。当k减小时，方差增加。但是偏差在整个过程中保持不变。k可以通过交叉验证找到。</p><p id="6e2d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">随机森林= DT(基础学习者)+ bagging(带替换的行抽样)+特征bagging(列抽样)+聚合(平均值/中值，多数投票)</p><p id="6cfc" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这里我们希望我们的基础学习者是低偏差和高方差的。所以训练DT到全深度长度。我们不担心深度，我们让它们增长，因为最终方差会减少。</p><p id="a44e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对于模型h1，建模中未使用的(D-D′)数据集在包外数据集。他们用于h1模型的交叉验证。</p><p id="2ca4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><em class="la">让我们看看实现随机森林的步骤:</em></p><p id="19b6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 1 </strong>。假设在训练数据集中有N个观察值和M个特征。首先，从训练数据集中随机抽取一个样本进行替换。</p><p id="7a3d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 2 </strong>。随机选择M个特征的子集，并且使用给出最佳分裂的特征来迭代地分裂节点。</p><p id="486e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 3 </strong>。这棵树长到最大。</p><p id="96a1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 4 </strong>。重复上述步骤，并基于来自n棵树的预测的集合给出预测。</p><p id="bedb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">训练和运行时间复杂性</strong></p><p id="6e83" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">训练时间= O(log(nd)*k)</p><p id="e42e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">运行时间= O(深度*k)</p><p id="e156" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">Space = O(存储每个DT*K)</p><p id="d819" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">随着基础模型数量的增加，训练运行时间也会增加，因此请始终使用交叉验证来找到最佳超参数。</p><p id="3e05" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">代码:</strong></p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/514d9bdd66860cc07f7cdc24b4f554bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l-G20oJixXeJYxK74uJj7w.png"/></div></div></figure><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/defafff7b029eb461f6f6705e5041493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*I5pRnWnpZo9BK9fq1jtUbQ.png"/></div></figure><div class="mc md ez fb me mf"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">3.2.4.3.1.sk learn . ensemble . randomforestclassifier-sci kit-learn 0 . 21 . 3文档</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">class sk learn . ensemble . RandomForestClassifier(n _ estimators = ' warn '，criterion='gini '，max_depth=None…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">scikit-learn.org</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt io mf"/></div></div></a></div><h2 id="ecd8" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated"><strong class="ak">极度随机化树:</strong></h2><p id="7c71" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">极端随机化树= DT(基础学习者)+ bagging(带替换的行抽样)+特征bagging(列抽样)+聚合(均值/中值、多数投票)+选择阈值时的随机化</p><p id="b047" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">再增加一个级别以减少差异，但偏差可能会略有增加。但这种方法在现实生活中并不多见。</p><div class="mc md ez fb me mf"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">3.2.4.3.3.sk learn . ensemble . extractreesclassifier-sci kit-learn 0 . 21 . 3文档</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">class sk learn . ensemble . extractree classifier(n _ estimators = ' warn '，criterion='gini '，max_depth=None…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">scikit-learn.org</p></div></div><div class="mo l"><div class="mu l mq mr ms mo mt io mf"/></div></div></a></div><p id="3148" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">案例:</strong></p><ol class=""><li id="bb91" class="kr ks hi jq b jr km jv kn jz mv kd mw kh mx kl kw kx ky kz bi translated">除了在RF中，方差影响任何数量的基学习者(K)之外，所有情况下的决策树对于随机森林也是相同的。</li><li id="6158" class="kr ks hi jq b jr lb jv lc jz ld kd le kh lf kl kw kx ky kz bi translated">在决策树的情况下，特征重要性仅在一个模型上，但是在RF中，我们也考虑所有k个基本学习者模型(k)。</li></ol><p id="1ac6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">注</strong>:所有情况下的决策树同样适用于随机森林。</p><h1 id="bb44" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">增压:</h1><p id="fc4b" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj"> <em class="la">助推</em> </strong> <em class="la">是另一种创建预测值集合的集成技术。在这种技术中，学习者是按顺序学习的，早期的学习者将简单的模型与数据拟合，然后分析数据中的错误。换句话说，我们拟合连续的树(随机样本),并且在每一步，目标都是解决来自先前树的净误差。</em></p><p id="4b10" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这在高偏差和低方差基础模型和加法组合的情况下有效。我们将努力减少偏见。</p><p id="ba8c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">当一个输入被一个假设错误分类时，它的权重会增加，以便下一个假设更有可能将其正确分类。通过在最后组合整个集合，将弱学习者转换成更好执行的模型。</p><p id="2607" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">注意:</strong>在RF中，我们无法将损耗降至最低，因为我们没有使用它。但是在梯度推进中，我们可以最小化任何损失</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/ca09431a3f88c6b3391fe610215bd093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bdcyPnIsFBZZP9Myew5EJQ.png"/></div></div></figure><p id="39ef" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这里L(y，F(x))是损失函数(对数损失、线性损失、兴损失等)，M是基本模型的数量。所有都是按顺序完成的，因此GBDT没有并行化，所以甚至比RF需要更多的时间训练。</p><p id="f732" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">梯度提升是具有高偏差和低方差模型的基础学习器。这里我们将使用深度值非常低的梯度推进决策树。</p><h2 id="9926" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated"><a class="ae mz" href="https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3092/regularization-by-shrinkage/4/module-4-machine-learning-ii-supervised-learning-models" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">正规化靠收缩</strong> </a></h2><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es na"><img src="../Images/988ff6dc85eb32c5b045f27d44b9a654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*Nxh799kTvP4k5Tk1GvKMXA.png"/></div></figure><p id="8211" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">随着v的降低，模型的权重降低。如果v增加，你过度拟合的机会增加。务必对基础模型的数量(M)和收缩参数(v)进行交叉验证。</p><p id="fdba" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">随着M的增加，偏差当然会减少，但方差也会增加。所以用正规化。在收缩中，我们简单地将“v”乘以第二项。因此将“游戏”减少常数“v”。随着“v”减小，过度拟合的机会减少，因此方差减小。<strong class="jq hj">M和“v”都用于偏差-方差权衡。</strong></p><p id="4f1f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">训练和运行时间复杂度</strong></p><p id="d9a1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">训练时间= O(log(nd)*k)</p><p id="2037" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">运行时间= O(深度*k)</p><p id="c236" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">Space = O(存储每个DT* k)</p><h2 id="e378" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">XGBoost</h2><p id="e191" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">XGBoost:增强+随机化</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/8235e6ef60ea8a8e9962c8f5b79cfa07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*hzlvNi-hk11kDlsRhpHu7A.png"/></div></div></figure><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/0814a3545ae16ae51ce57b68a0ed42f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*wAOHeiUpHzJJzj8ECh4ZGw.png"/></div></figure><p id="0dc3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><a class="ae mz" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html" rel="noopener ugc nofollow" target="_blank">https://xgboost . readthedocs . io/en/latest/python/python _ API . html # module-xgboost . sk learn</a></p><p id="f90d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><a class="ae mz" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . gradientboostingclassifier . html</a></p><p id="04d8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><a class="ae mz" href="https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py" rel="noopener ugc nofollow" target="_blank">https://github . com/dmlc/xgboost/blob/master/demo/guide-python/sk learn _ examples . py</a></p><h2 id="47d5" class="lm ir hi bd is ln lo lp iw lq lr ls ja jz lt lu je kd lv lw ji kh lx ly jm lz bi translated">Adaboost:</h2><p id="4887" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">主要用于图像处理和人脸检测问题。在每一个阶段，它给那些被错误分类的点更多的权重。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nd"><img src="../Images/1e9df05bfe7835fc7532045143506f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g8j63jdOzA73Ek-lX3Lb2w.png"/></div></div></figure><div class="mc md ez fb me mf"><a href="https://alliance.seas.upenn.edu/~cis520/wiki/index.php?n=lectures.boosting" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">助推</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">有很多很多方法来解释boosting:作为指数损失最小化，作为一个特征选择算法，作为…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">alliance.seas.upenn.edu</p></div></div><div class="mo l"><div class="ne l mq mr ms mo mt io mf"/></div></div></a></div><p id="e085" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">您可以调整参数来优化算法的性能，我在下面提到了用于调整的关键参数:</p><ul class=""><li id="0616" class="kr ks hi jq b jr km jv kn jz mv kd mw kh mx kl nf kx ky kz bi translated"><strong class="jq hj"> n_estimators: </strong>它控制弱学习者的数量。</li><li id="580f" class="kr ks hi jq b jr lb jv lc jz ld kd le kh lf kl nf kx ky kz bi translated"><strong class="jq hj"> learning_rate: </strong>控制弱学习者在最终组合中的贡献。在learning_rate和n_estimators之间有一个折衷。</li><li id="7cf1" class="kr ks hi jq b jr lb jv lc jz ld kd le kh lf kl nf kx ky kz bi translated"><strong class="jq hj"> base_estimators </strong>:它有助于指定不同的ML算法。</li></ul><h1 id="dd96" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">堆叠模型:</h1><p id="dc56" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">第一次独立训练每个型号(C1、C2、C3……)</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/9c6c825fb28f65b464d19428496fe723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*tXJLcZHP0yaq6hndwhKESw.png"/></div></figure><p id="f990" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">元分类器可以是逻辑的、RF的或任何的。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/535e27cde2638128f6ce50d09855716f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*bLQTHN3DEtQmOwwtZMVxCw.png"/></div></figure><div class="mc md ez fb me mf"><a href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">堆叠分类器</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">用于堆叠的集成学习元分类器。分类器导入堆栈分类器堆栈是一个…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">rasbt.github.io</p></div></div></div></a></div><h1 id="2deb" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">级联分类器:</h1><p id="50b8" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这种模式主要用于错误发生的成本非常高的情况。广泛用于信用卡公司检测欺诈交易。或者医学领域来检测罕见问题以拯救生命。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ni"><img src="../Images/8814b85216f283eabf423fa55a393fce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bKD67O4Q1QOp9Lyj.jpeg"/></div></div></figure><p id="1c19" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">第二步中的数据集训练是第一步建模中未使用的(D-D1)数据集。</p><p id="8549" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">============大团圆结局=================</p><h1 id="684c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">参考:</h1><p id="0a6b" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">谷歌图片</p><p id="5902" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">应用人工智能</p><div class="mc md ez fb me mf"><a href="https://stats.stackexchange.com/questions/219241/gradient-for-logistic-loss-function" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">逻辑损失函数的梯度</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">我想问一个与此相关的问题。我在这里找到一个为xgboost写自定义损失函数的例子…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">stats.stackexchange.com</p></div></div><div class="mo l"><div class="nj l mq mr ms mo mt io mf"/></div></div></a></div></div></div>    
</body>
</html>