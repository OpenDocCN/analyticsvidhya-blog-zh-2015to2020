<html>
<head>
<title>Feature selection using Scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Scikit-learn进行功能选择</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-using-scikit-learn-5b4362e0c19b?source=collection_archive---------3-----------------------#2020-01-28">https://medium.com/analytics-vidhya/feature-selection-using-scikit-learn-5b4362e0c19b?source=collection_archive---------3-----------------------#2020-01-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/202f6e3c4ba8694bd7d5cd3dc17dd6f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z70gNH8GksIRoT-0"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">詹·西奥多在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="e1c8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">特征选择是机器学习中最重要的步骤之一。它是在不丢失全部信息的情况下缩小预测建模中要使用的特征子集的过程。有时，特征选择会被误认为是降维。这两种方法都倾向于减少数据集中的要素数量，但方式不同。降维通过创建新特征作为现有特征的组合来减少特征的数量。所有的功能结合起来，创造了一些独特的功能。另一方面，特征选择的工作原理是消除不相关的特征，只保留相关的特征。</p><p id="b9a4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是特征选择的主要优点:</p><ul class=""><li id="19b3" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">它提高了模型的性能:当你的数据中有不相关的特征时，这些特征就像噪音一样，这使得机器学习模型的性能很差。</li><li id="5b7a" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">它导致更快的机器学习模型。</li><li id="65bb" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">它避免了过度拟合，从而增加了模型的通用性。</li></ul><p id="e845" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">功能选择可以通过多种方式完成，我们将在这里看到一些<a class="ae iu" href="https://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>功能选择方法。</p><p id="d894" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">数据集</strong></p><p id="d3b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于这篇博客，我将使用<a class="ae iu" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29" rel="noopener ugc nofollow" target="_blank">乳腺癌威斯康星州(诊断)数据集</a>。它共有32个特征，包括id &amp;响应变量(诊断)。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="49e5" class="kq kr hi km b fi ks kt l ku kv">df = pd.read_csv (‘breast cancer data.csv’)<br/>y =df['diagnosis'].map({'B':0,'M':1})# target variable<br/>X= df.drop((['diagnosis','id']), axis=1) # features after dropping the  target (diagnosis) &amp; ID</span></pre><p id="fa25" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">多重共线性</strong></p><p id="acf5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在要素选择过程中，检查多重共线性是一个非常重要的步骤。多重共线性会显著降低模型的性能。移除多重共线特征将减少特征的数量并提高模型的性能。</p><p id="e62d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将数据运行到pandas profiling后，报告显示由于多重共线性，从要素列表中删除了10个要素。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="e9a1" class="kq kr hi km b fi ks kt l ku kv"><strong class="km hj">pandas_profiling.ProfileReport(df)</strong></span></pre><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/3499f0181c312e34a1009149ae6cce11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a6bZrV2QK34g0voXhcKFXg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">熊猫概况报告——多共线特征</figcaption></figure><p id="f505" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们从前面的列表中删除这些特性，然后继续下一个特性选择方法。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="7107" class="kq kr hi km b fi ks kt l ku kv"># variables rejected due to high correlation<br/><strong class="km hj">rejected_features= list(profile.get_rejected_variables())</strong> <br/>X_drop= X.drop(rejected_features,axis=1)<br/>X_drop.shape<br/></span></pre><p id="9b6e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 1。单变量特征选择</strong></p><p id="b706" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">单变量特征选择通过使用诸如卡方检验的单变量统计测试来选择最佳特征。它单独检查每个特征，以确定该特征与响应变量的关系强度。<a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> SelectKBest </strong> </a>是一种单变量方法，除了指定数量的最高得分特征之外，删除所有特征。</p><p id="727b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">拆分数据</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="050f" class="kq kr hi km b fi ks kt l ku kv"># split data train 70 % and test 30 %</span><span id="d9f5" class="kq kr hi km b fi kx kt l ku kv"><strong class="km hj">from sklearn.model_selection import train_test_split</strong><br/><strong class="km hj">x_train, x_test, y_train, y_test = train_test_split(X_drop, y, test_size=0.3, random_state=42)</strong></span></pre><p id="45a5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后运行SelectKbest来选择5个最佳特性</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="7718" class="kq kr hi km b fi ks kt l ku kv"><strong class="km hj">from sklearn.feature_selection import SelectKBest, chi2</strong><br/><strong class="km hj">X_5_best= SelectKBest(chi2, k=5).fit(x_train, y_train)</strong></span><span id="769d" class="kq kr hi km b fi kx kt l ku kv"><strong class="km hj">mask = X_5_best.get_support()</strong> #list of booleans for selected features<br/><strong class="km hj">new_feat = [] <br/>for bool, feature in zip(mask, x_train.columns):<br/> if bool:<br/> new_feat.append(feature)</strong><br/><strong class="km hj">print(‘The best features are:{}’.format(new_feat))</strong> # The list of your 5 best features</span></pre><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/06fbd198eb3385d55ea8ece44eaf6c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rgfRGpXFQwObk99lqxn2JA.png"/></div></div></figure><p id="786b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。递归特征消除(RFE) </strong></p><p id="98fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与单变量方法不同，<a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> RFE </strong> </a>首先对整个特征集拟合一个模型，并计算每个预测因子的重要性分数。然后移除最弱的特征，重新拟合模型，并再次计算重要性分数，直到使用了指定数量的特征。通过模型的coef_或feature_importances_ attributes，并通过递归地消除每个循环中的少量特征，对特征重要度进行排序。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="c441" class="kq kr hi km b fi ks kt l ku kv"><strong class="km hj">from sklearn.feature_selection import RFE</strong><br/><strong class="km hj">estimator = RandomForestClassifier(random_state = 42)</strong><br/><strong class="km hj">selector = RFE(estimator, 5, step=1)</strong><br/><strong class="km hj">selector = selector.fit(x_train, y_train)</strong></span><span id="5cee" class="kq kr hi km b fi kx kt l ku kv"><strong class="km hj">rfe_mask = selector.get_support()</strong> #list of booleans for selected features<br/><strong class="km hj">new_features = []</strong> </span><span id="7ddf" class="kq kr hi km b fi kx kt l ku kv"><strong class="km hj">for bool, feature in zip(rfe_mask, x_train.columns):<br/> if bool:<br/> new_features.append(feature)<br/>new_features</strong> # The list of your 5 best features</span></pre><p id="f01d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 3。交叉验证递归特征消除(RFECV) </strong></p><p id="d3ab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RFE要求保留指定数量的特征，但是通常事先并不知道有多少特征是有效的。为了找到最佳数量的特征，交叉验证与RFE一起用于对不同的特征子集进行评分，并选择最佳评分的特征集合。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="a194" class="kq kr hi km b fi ks kt l ku kv"><strong class="km hj">from sklearn.feature_selection import RFECV<br/>cv_estimator = RandomForestClassifier(random_state =42)<br/>X_train,X_test,Y_train,Y_test = train_test_split(X, y, test_size=0.3, random_state=42)<br/>cv_estimator.fit(X_train, Y_train)<br/>cv_selector = RFECV(cv_estimator,cv= 5, step=1,scoring=’accuracy’)<br/>cv_selector = cv_selector.fit(X_train, Y_train)<br/>rfecv_mask = cv_selector.get_support()</strong> #list of booleans</span><span id="7e1f" class="kq kr hi km b fi kx kt l ku kv"><strong class="km hj">rfecv_features = []</strong> </span><span id="f414" class="kq kr hi km b fi kx kt l ku kv"><strong class="km hj">for bool, feature in zip(rfecv_mask, X_train.columns):<br/> if bool:<br/> rfecv_features.append(feature)</strong></span><span id="2a2d" class="kq kr hi km b fi kx kt l ku kv"><strong class="km hj">print(‘Optimal number of features :’, cv_selector.n_features_)<br/>print(‘Best features :’, rfecv_features)<br/>n_features = X_train.shape[1]<br/>plt.figure(figsize=(8,8))<br/>plt.barh(range(n_features), cv_estimator.feature_importances_, align='center') <br/>plt.yticks(np.arange(n_features), X_train.columns.values) <br/>plt.xlabel('Feature importance')<br/>plt.ylabel('Feature')<br/>plt.show()</strong></span></pre><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/c55a76ae2119c5665db8b4a290cd3c34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N_zIcq8IuOE7fwAGpK70_w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">最佳特征数量</figcaption></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/6331bf2f5db79d74ac8a9e83b5ba0690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Asx074hSrK9IotLhhszcA.png"/></div></div></figure><p id="ee98" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，功能的数量减少到13个，我们有了这些功能的名称。下一步是使用这些特征来拟合模型并检查性能。</p><p id="6358" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢阅读！</p><p id="09b5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lb">参考文献:</em></p><p id="3463" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://bookdown.org/max/FES/recursive-feature-elimination.html" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> 11.3递归特征消除|特征工程和选择:预测模型的实用方法</em> </a></p><p id="322d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e" rel="noopener" target="_blank"><em class="lb">https://towards data science . com/feature-selection-techniques-in-machine-learning-with-python-f 24 e 7 da 3 f 36 e</em></a></p><p id="3a74" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" rel="noopener" href="/@contactsunny/what-is-feature-selection-and-why-do-we-need-it-in-machine-learning-28a28520607c"><em class="lb">https://medium . com/@ contact sunny/what-is-feature-selection-and-why-do-we-need-it-in-machine-learning-28a 28520607 c</em></a></p><p id="d931" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://towardsdatascience.com/feature-selection-using-python-for-classification-problem-b5f00a1c7028" rel="noopener" target="_blank">https://towards data science . com/feature-selection-using-python-for-class ification-problem-b 5 f 00 a1 c 7028</a></p></div></div>    
</body>
</html>