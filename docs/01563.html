<html>
<head>
<title>Finding Data Block Nirvana (a journey through the fastai data block API) — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">寻找数据块天堂(fastai数据块API之旅)——第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-part-2-9b23ea5d83ee?source=collection_archive---------11-----------------------#2019-10-31">https://medium.com/analytics-vidhya/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-part-2-9b23ea5d83ee?source=collection_archive---------11-----------------------#2019-10-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/cfecba7e31c0b8972249b249ea018921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XkSANUIJi-WY-smq0LvvGA.jpeg"/></div></div></figure><p id="a636" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文描述了如何训练我们在本系列的<a class="ae jo" href="https://blog.usejournal.com/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4" rel="noopener ugc nofollow" target="_blank">第1部分中构建的自定义fast.ai项目列表(和其他自定义数据块API位)。如果您还没有这样做，请确保您已经阅读了第一篇文章，并且可以运行yelp-00笔记本中位于</a><a class="ae jo" href="https://localhost:8889/tree/medium-finding-data-block-nirvana" rel="noopener ugc nofollow" target="_blank">这里</a>的配套代码。</p><p id="ff8d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第二部分的代码在这里是</strong><a class="ae jo" href="https://github.com/ohmeow/dl-experiments/tree/master/medium-finding-data-block-nirvana" rel="noopener ugc nofollow" target="_blank"><strong class="is hj"/></a><strong class="is hj">(见yelp-01笔记本)。</strong></p><p id="a319" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我在第一篇文章中所说的值得重复:</p><blockquote class="jp jq jr"><p id="78d6" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated">我坚信<em class="hi">通过首先阅读和运行相关代码，然后自己手工编写代码(不要复制&amp;粘贴),人们将会学到更多关于使用这个框架的知识。这并不意味着阅读文档，突出和强调重要的概念不重要(相信我，我做的比我应得的多)，只是为了让它在你的大脑中牢牢扎根，你必须<strong class="is hj">这样做</strong>。所以获取代码，运行代码，热爱代码。利用它和这篇文章的内容，通过自己编写代码来展示个人的理解。</em></p></blockquote><p id="9ad8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">说到这里，让我强调一下第2部分代码中一些更有趣的部分。</p></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="959a" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">保持干燥</h1><p id="5217" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">我将所有与数据块API相关的代码移到了<code class="du lg lh li lj b">utils.py</code>文件中，然后从笔记本顶部导入所有代码。因为我可能想在其他地方重用这些代码，所以最好记住编程的黄金准则之一:<strong class="is hj">D</strong>on t<strong class="is hj">R</strong>EPE at<strong class="is hj">Y</strong>yourself。</p><h1 id="bb97" class="kd ke hi bd kf kg lk ki kj kk ll km kn ko lm kq kr ks ln ku kv kw lo ky kz la bi translated">对数据块API位的修复</h1><p id="faf5" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">当我第一次尝试训练我的模型时，我注意到张量在<code class="du lg lh li lj b">mixed_tabular_pad_collate</code>函数中没有完全分组。我在第1部分的笔记本和<code class="du lg lh li lj b">utils.py</code>文件中修复了这个问题，但是<em class="js">我也在第1部分的文件中留下了旧的错误代码，这样你就可以查看输出应该是什么样子，不应该是什么样子。<strong class="is hj">我推荐</strong>你用修正版或者之前的版本运行<code class="du lg lh li lj b">yelp-00-custom-itemlist</code>笔记本，自己看看区别。</em></p><h1 id="3a70" class="kd ke hi bd kf kg lk ki kj kk ll km kn ko lm kq kr ks ln ku kv kw lo ky kz la bi translated">微调LM</h1><p id="a2b6" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">由于我们正在处理文本，我认为使用数据集中可用的目标文本对基于AWD·LSTM的<a class="ae jo" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank"> ULMFit模型</a>进行微调是有意义的。参见笔记本的<strong class="is hj"> LM微调</strong>部分。我说明了这样做所需的基本步骤，并且我确信这是可以改进的许多地方之一。</p><h1 id="7dbe" class="kd ke hi bd kf kg lk ki kj kk ll km kn ko lm kq kr ks ln ku kv kw lo ky kz la bi translated">构建混合表数据中心</h1><p id="d1fc" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">还记得我们在第1部分中编写所有代码吗？好吧，所有这些艰苦的工作使得在我们手头的建模任务中实际使用它变得如此简单。</p><pre class="lp lq lr ls fd lt lj lu lv aw lw bi"><span id="049c" class="lx ke hi lj b fi ly lz l ma mb">data_cls = (MixedTabularList.from_df(<br/>                            train_df, cat_cols, cont_cols, txt_cols,<br/>                            vocab=data_lm.train_ds.vocab, <br/>                            procs=procs, path=PATH)<br/>          .split_by_rand_pct(valid_pct=0.1, seed=42)<br/>          .label_from_df(dep_var)<br/>          .databunch(bs=32))</span></pre><p id="5222" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这对于任何使用fast.ai框架的人来说应该非常熟悉。注意我们是如何使用上面微调过的语言模型中的<code class="du lg lh li lj b">vocab</code>。</p><h1 id="a0a4" class="kd ke hi bd kf kg lk ki kj kk ll km kn ko lm kq kr ks ln ku kv kw lo ky kz la bi translated">目录文本</h1><p id="e743" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">我们如何使用这个<a class="ae jo" href="https://docs.fast.ai/basic_data.html#DataBunch" rel="noopener ugc nofollow" target="_blank">数据束</a>？我肯定有比我在这里提出的方法更好的方法，但是我仅仅通过利用从<code class="du lg lh li lj b">tabular_learner</code>和<code class="du lg lh li lj b">text_classifier_learner</code> fast.ai学习者创建的模型就能够得到不错的结果。我绝对相信这种方法至少是新颖的(至少我还没有在任何地方看到过),并且有可能得到改进。</p><p id="721f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">至于上面两个学习者需要的配置，我决定使用一个简单的字典来简化实验。参见在<code class="du lg lh li lj b">TabularTextNN</code>模块定义上方声明的各自的<code class="du lg lh li lj b">tabular_args</code>和<code class="du lg lh li lj b">text_args</code>变量。</p><p id="cd4a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">模块的<code class="du lg lh li lj b">init</code>是所有有趣事物的所在:</p><pre class="lp lq lr ls fd lt lj lu lv aw lw bi"><span id="6dbf" class="lx ke hi lj b fi ly lz l ma mb">def __init__(self, data, tab_layers, tab_args={}, text_args={}):<br/>        super().__init__()<br/>        <br/>        tab_learner = tabular_learner(data, tab_layers, **tab_args)<br/>        tab_learner.model.layers = tab_learner.model.layers[:-1]<br/>        self.tabular_model = tab_learner.model</span><span id="cbd7" class="lx ke hi lj b fi mc lz l ma mb">        text_class_learner = text_classifier_learner(data, AWD_LSTM, <br/>                                                     **text_args)<br/>        text_class_learner.load_encoder('lm_ft_enc')<br/>        self.text_enc_model = /         <br/>                        list(text_class_learner.model.children())[0]<br/>        <br/>        self.bn_concat = nn.BatchNorm1d(400*3+100)<br/>        <br/>        self.lin = nn.Linear(400*3+100, 50)<br/>        self.final_lin = nn.Linear(50, data.c)</span></pre><p id="ee50" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您查看这里的表格学习器返回的模型，您会看到最后一层是一个线性层，它输出模型需要预测的预期标签数。由于我们将在获取标签的概率之前将这个模型的输出与文本输出合并，我们只需将模型层设置为等于<code class="du lg lh li lj b">tabular_learner.model.layers[:-1]</code>就可以了。</p><p id="8355" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">类似地，我们在这里只需要文本分类学习器的编码器，因此我们通过<code class="du lg lh li lj b">list(text_class_learner.model.children())[0]</code>将<code class="du lg lh li lj b">PoolingClassifier</code>从其中移除。学习如何操作PyTorch模型对理解非常有帮助，我在下面提供了一些对我有指导意义的资源。</p><p id="942c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的<code class="du lg lh li lj b">forward()</code>函数的最后一步是连接两个模型的结果，通过一个批量标准化层和几个线性层运行它们，以获得我们的预测值。请注意，我还使用了fast.ai中使用的concat pooling技巧来利用文本编码器返回的所有信息。</p><h1 id="fac4" class="kd ke hi bd kf kg lk ki kj kk ll km kn ko lm kq kr ks ln ku kv kw lo ky kz la bi translated">培养</h1><p id="76a8" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">你猜怎么着？你就像训练其他fast.ai模型一样训练它。这意味着这里真的没有什么新东西可学。你可以把这个模型放在学习者身上:</p><pre class="lp lq lr ls fd lt lj lu lv aw lw bi"><span id="8548" class="lx ke hi lj b fi ly lz l ma mb">model = TabularTextNN(data_cls, tab_layers, tabular_args, text_args)<br/>learn = Learner(data_cls, model, metrics=[accuracy])</span></pre><p id="f2a6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">不错吧。</p></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="3ce4" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">后续步骤</h1><p id="12c6" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">你能打败我最好的准确度<code class="du lg lh li lj b">.673</code>吗？</p><p id="7fca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我很高兴接受任何和所有用你自己的笔记本提出的利用<code class="du lg lh li lj b">MixedTabular</code>项目列表来改进我的结果的请求(它们肯定可以被改进)。也许你们中的一些人想提交一个包含一些EDA工作的笔记本？或者是一台笔记本电脑，它展示了一种可靠的方法，可以根据功能的重要性来决定哪些功能应该包含，哪些不应该包含？或者，也许有人可以用特征工程和/或数据扩充的方式说明一些有帮助的东西，以帮助改进我的结果？</p><p id="c628" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">无论哪种方式，<strong class="is hj">我都希望看到从这些文章中受益的社区的一些工作</strong> …这些工作可以反过来使我自己和其他人受益。这是我对你的挑战。</p><p id="2648" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一如既往，我希望这两篇文章对你的工作有所帮助，并随时在twitter上关注我，地址是<a class="ae jo" href="https://twitter.com/waydegilliam" rel="noopener ugc nofollow" target="_blank"> @wgilliam </a>。在接下来的几个月里，我将发表一系列文章，向您展示如何使用即将发布的fast.ai框架第2版完成所有这些工作，敬请关注。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div class="er es md"><img src="../Images/c3e474afa948caeb7437b45ff4945561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QsTDODtzrfENg9qJ6DY0qQ.jpeg"/></div></figure></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h1 id="5f8e" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">资源</h1><p id="297f" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated"><strong class="is hj">数据块API</strong><br/><a class="ae jo" href="https://docs.fast.ai/data_block.html" rel="noopener ugc nofollow" target="_blank">fast . ai文档</a> <br/> <a class="ae jo" href="https://blog.usejournal.com/finding-data-block-nirvana-a-journey-through-the-fastai-data-block-api-c38210537fe4" rel="noopener ugc nofollow" target="_blank">寻找数据块涅槃—第1部分</a></p><p id="d717" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">修改PyTorch模型</strong> <br/> <a class="ae jo" href="https://discuss.pytorch.org/t/how-to-delete-layer-in-pretrained-model/17648" rel="noopener ugc nofollow" target="_blank">如何删除预训练模型中的图层？</a> <br/> <a class="ae jo" href="https://discuss.pytorch.org/t/how-to-replace-last-layer-in-sequential/14422" rel="noopener ugc nofollow" target="_blank">如何依次替换最后一层</a> <br/> <a class="ae jo" href="https://stackoverflow.com/questions/52548174/how-to-remove-the-last-fc-layer-from-a-resnet-model-in-pytorch" rel="noopener ugc nofollow" target="_blank">这篇stackoverflow文章有一些不错的信息</a> <br/> <a class="ae jo" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.add_module" rel="noopener ugc nofollow" target="_blank">参见官方文档中的add_module方法</a></p></div></div>    
</body>
</html>