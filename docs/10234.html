<html>
<head>
<title>PPO algorithm with custom RL environment made with Unity engine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Unity引擎定制RL环境的PPO算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ppo-algorithm-with-custom-rl-environment-made-with-unity-engine-effed6d98b9d?source=collection_archive---------6-----------------------#2020-10-10">https://medium.com/analytics-vidhya/ppo-algorithm-with-custom-rl-environment-made-with-unity-engine-effed6d98b9d?source=collection_archive---------6-----------------------#2020-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="48bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用ML-Agents Python低层API训练RL代理</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/631f5d3c235a54d0e3671114a9506d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04gZzkimeh-Jgo8fVwks0w.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="https://github.com/dhyeythumar/PPO-algo-with-custom-Unity-environment/blob/main/Environment_Details.md" rel="noopener ugc nofollow" target="_blank">我使用Unity的ML-Agents工具包训练RL代理的环境</a></figcaption></figure><p id="5f8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我写这篇文章是因为我发现几乎没有任何资源讨论如何将RL算法应用到用Unity引擎创建的定制环境中。因此，如果你在Unity中实现了一个定制环境，并使用ML-Agents工具包进行培训。但是现在，如果您想将PPO(或任何其他RL算法)应用到您的环境中(不使用ML-Agents inbuild trainer)，那么您就找对了地方。</p><p id="8013" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ML代理提供了两个附加功能:</p><ul class=""><li id="ca57" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><strong class="ih hj"> Python低级API</strong>，它允许我们直接与学习环境交互，这样我们就可以实现新的强化学习算法，并在我们的环境中测试它们。</li><li id="0d30" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><strong class="ih hj">健身房包装器</strong>因此我们可以将学习环境模拟成标准的健身房环境，并且它可以类似地用于其他健身房环境。</li></ul><p id="56ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我将解释如何使用python APIs通过PPO算法与您的学习环境进行交互。(但是对于其他RL算法可以遵循类似的步骤)</p><blockquote class="ki kj kk"><p id="ad04" class="if ig kl ih b ii ij ik il im in io ip km ir is it kn iv iw ix ko iz ja jb jc hb bi translated"><strong class="ih hj">注意:</strong>如果您期待实现PPO算法，那么请阅读这篇令人惊叹的文章<a class="ae jt" href="https://towardsdatascience.com/proximal-policy-optimization-tutorial-part-1-actor-critic-method-d53f9afffbf6" rel="noopener" target="_blank"> <strong class="ih hj">近端策略优化教程(第1/2部分:演员-评论家方法)</strong> </a></p></blockquote></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><p id="85cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤0:安装</strong></p><p id="36fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一步仅适用于您尚未安装ML-Agents工具包的情况。</p><pre class="je jf jg jh fd kw kx ky kz aw la bi"><span id="8470" class="lb lc hi kx b fi ld le l lf lg">$ git clone --branch release_1 <a class="ae jt" href="https://github.com/Unity-Technologies/ml-agents.git" rel="noopener ugc nofollow" target="_blank">https://github.com/Unity-Technologies/ml-agents.git</a><br/>$ python -m venv myvenv<br/>$ myvenv\Scripts\activate<br/>$ pip install -e ./ml-agents/ml-agents-envs</span></pre><p id="1ea7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kl">你只需要从克隆的回购中安装</em> <code class="du lh li lj kx b"><em class="kl">ml-agents-envs</em></code> <em class="kl">。</em></p></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><p id="5b29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第一步:加载环境</strong></p><p id="d6be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Python端的通信通过<code class="du lh li lj kx b"><strong class="ih hj">UnityEnvironment</strong></code>进行。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lk ll l"/></div></figure><p id="e628" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lh li lj kx b"><strong class="ih hj">file_name</strong></code> <em class="kl"> </em>是环境二进制的名称(提供二进制的正确路径)。如果您想与编辑器互动，当屏幕上显示消息<em class="kl">“按下Unity编辑器中的播放按钮开始训练”</em>时，使用<code class="du lh li lj kx b">file_name=None</code>并按下编辑器中的播放按钮。</p><p id="cc76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lh li lj kx b"><strong class="ih hj">seed</strong></code>表示在训练过程中生成随机数时使用的种子。</p><p id="c7da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lh li lj kx b"><strong class="ih hj">side_channels</strong></code> <strong class="ih hj"> <em class="kl"> </em> </strong>提供了一种与Unity仿真交换数据的方式，这种方式与强化学习循环无关。例如，我们正在设置屏幕尺寸和时间刻度。</p></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><p id="910b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第二步:获取环境细节</strong></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lk ll l"/></div></figure><p id="79b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lh li lj kx b"><strong class="ih hj">reset()</strong></code> <strong class="ih hj"> </strong>发送信号重置环境。</p><p id="3952" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lh li lj kx b"><strong class="ih hj">get_behavior_names()</strong></code>返回行为名称列表。对于单代理环境，只有一种行为。您可以有多个行为，这需要不同的模型(在PPO算法的情况下，您需要为每个行为有一个单独的参与者-批评家模型)</p><p id="868c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lh li lj kx b"><strong class="ih hj">get_behavior_spec()</strong></code>提供多个字段</p><ul class=""><li id="bd88" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><code class="du lh li lj kx b"><strong class="ih hj">action_size</strong></code>对应于您的代理期望的动作数量。(动作空间)</li><li id="211e" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><code class="du lh li lj kx b"><strong class="ih hj">observation_shapes</strong></code>返回元组列表。这取决于你使用了多少种不同的方法来收集观察值，比如光线投射法或使用<code class="du lh li lj kx b">VectorSensor</code>添加值。</li><li id="6688" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><code class="du lh li lj kx b"><strong class="ih hj">is_action_continuous()</strong></code>返回一个布尔值，它取决于您在编辑环境时如何在行为参数中定义您的动作空间。</li><li id="7bdf" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><code class="du lh li lj kx b"><strong class="ih hj">is_action_discrete()</strong></code> <strong class="ih hj"> <em class="kl"> </em> </strong>与上述方法相似。如果你使用离散动作空间，那么使用<code class="du lh li lj kx b"><strong class="ih hj">discrete_action_branches</strong></code>来获得一个动作选择元组。(例如，如果action_size = 2，则discrete_action_branches将返回(3，2，)这表示第一个分支有3个不同的值，第二个分支有2个不同的值。</li></ul></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><p id="e07c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第三步:收集初始观察结果</strong></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lk ll l"/></div></figure><p id="1b57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lh li lj kx b"><strong class="ih hj">get_steps()</strong></code> <strong class="ih hj"> <em class="kl"> </em> </strong>返回一组DecisionSteps，TerminalSteps(用于代理组)。我们还可以访问特定代理的决策步骤，使用其id作为step_result[0][agent_id]。并且对于终端步骤，其步骤结果[1][代理id]。</p><p id="bb2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">决策步骤</strong>包含以下字段:</p><ul class=""><li id="2716" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><code class="du lh li lj kx b"><strong class="ih hj">obs</strong></code> <strong class="ih hj"> <em class="kl"> </em> </strong>是代理收集的NumPy数组观察值的列表。(将它们累积在一个向量中)</li><li id="6dbd" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><code class="du lh li lj kx b"><strong class="ih hj">reward</strong></code>对应于代理从上一步开始收集的奖励。</li><li id="4975" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><code class="du lh li lj kx b"><strong class="ih hj">agent_id</strong></code> <em class="kl"> </em>是对应代理的唯一标识符。</li><li id="b4b0" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><code class="du lh li lj kx b"><strong class="ih hj">action_mask</strong></code>是一个可选的一维布尔数组列表。仅用于多离散动作空间类型。(如果为真，则在此模拟步骤中，该操作对代理不可用。)</li></ul><p id="8895" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">终端步骤</strong>为空，直到代理遇到结束剧集触发&amp;包含以下字段:</p><ul class=""><li id="7a49" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><code class="du lh li lj kx b"><strong class="ih hj">obs, reward, agent_id</strong></code> <strong class="ih hj"> <em class="kl"> </em> </strong>类似于决策步骤字段。</li><li id="86df" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><code class="du lh li lj kx b"><strong class="ih hj">max_step</strong></code>是一个布尔。如果代理在最后一个模拟步骤中达到其最大步骤数，则为True。</li></ul></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><p id="e3ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第四步:采取行动，改善环境</strong></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lk ll l"/></div></figure><p id="fa7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lh li lj kx b"><strong class="ih hj">set_actions()</strong></code>设置整个代理组的操作。它需要一个2D NumPy数组，其形状定义为<code class="du lh li lj kx b"><strong class="ih hj">(num_of_agents, n_actions, )</strong></code>。</p><p id="a8b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lh li lj kx b"><strong class="ih hj">set_action_for_agent(agent_group: str, agent_id: int, action: np.array)</strong></code> <strong class="ih hj"> </strong>设置代理组中特定代理的操作。agent_group是代理所属的组的名称。这里的动作是一个1D NumPy数组，其形状被定义为<code class="du lh li lj kx b"><strong class="ih hj">(n_actions, )</strong></code>。</p><p id="caa5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lh li lj kx b"><strong class="ih hj">step()</strong></code> <strong class="ih hj"> </strong>向步进环境发送信号。当调用<code class="du lh li lj kx b">step()</code>或<code class="du lh li lj kx b">reset()</code>时，Unity模拟将向前移动，直到模拟中的代理需要来自Python的输入才能动作。</p></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><p id="fb98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第五步:异常处理</strong></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lk ll l"/></div></figure><p id="476c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Unity ML-Agents还提供了定制的异常处理程序，当运行环境产生错误时可以调用(例如，如果您想要中断训练过程，那么通过使用这些异常，您可以安全地关闭Unity窗口并保存已训练的模型)。</p><p id="7842" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们来结束这篇文章。如果你有任何疑问、建议和改进，请告诉我。</p><p id="18ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想和上图中提到的环境互动，那么就来看看<a class="ae jt" href="https://github.com/dhyeythumar/PPO-algo-with-custom-Unity-environment/tree/main/rl_env_binary" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这个链接</strong> </a>。你也可以使用<a class="ae jt" href="https://github.com/dhyeythumar/PPO-algo-with-custom-Unity-environment/blob/main/env_driver.py" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这个</strong> </a>简单的脚本，它不使用任何算法，所以你可以更有效地探索ML-Agents API。</p><blockquote class="ki kj kk"><p id="6fc8" class="if ig kl ih b ii ij ik il im in io ip km ir is it kn iv iw ix ko iz ja jb jc hb bi translated">查看这个github repo，了解在使用Unity引擎定制的环境中PPO算法的完整实现。</p></blockquote><div class="lm ln ez fb lo lp"><a href="https://github.com/Dhyeythumar/PPO-algo-with-custom-Unity-environment" rel="noopener  ugc nofollow" target="_blank"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hj fi z dy lu ea eb lv ed ef hh bi translated">dhyethumar/PPO-algo-with-custom-Unity-environment</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">此报告包含在自定义的上使用Keras库的近似策略优化算法的实现…</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">github.com</p></div></div><div class="ly l"><div class="lz l ma mb mc ly md jn lp"/></div></div></a></div></div></div>    
</body>
</html>