<html>
<head>
<title>Understanding Polynomial Regression!!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解多项式回归！！！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-polynomial-regression-5ac25b970e18?source=collection_archive---------0-----------------------#2020-08-02">https://medium.com/analytics-vidhya/understanding-polynomial-regression-5ac25b970e18?source=collection_archive---------0-----------------------#2020-08-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/d4c09b31c27477a9da72e78a5ce31f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*o_dea2RJirf9FldCuE-pdA.png"/></div></figure><p id="af01" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在我之前的文章中，我们概述了<a class="ae jk" rel="noopener" href="/analytics-vidhya/understanding-the-linear-regression-808c1f6941c0"><strong class="io hj"/></a>和<a class="ae jk" rel="noopener" href="/analytics-vidhya/understanding-logistic-regression-b3c672deac04"> <strong class="io hj">逻辑回归</strong> </a>。<br/>再来看回归家族的另一个算法。</p><h1 id="2cd7" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">内容:</h1><ol class=""><li id="98c6" class="kj kk hi io b ip kl it km ix kn jb ko jf kp jj kq kr ks kt bi translated">什么是多项式回归？</li><li id="58c5" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">多项式回归的假设。</li><li id="1e31" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">为什么我们需要多项式回归？</li><li id="e8e8" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">如何求多项式方程的右次？</li><li id="4d31" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">多项式方程背后的数学。</li><li id="4cf9" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">多项式回归的代价函数。</li><li id="9cd8" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated">梯度下降多项式回归。</li></ol><h1 id="3052" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">什么是多项式回归？</h1><ul class=""><li id="ab32" class="kj kk hi io b ip kl it km ix kn jb ko jf kp jj kz kr ks kt bi translated"><strong class="io hj">多项式回归</strong>是回归分析的一种形式，其中自变量和因变量之间的关系用 n 次多项式建模。</li><li id="06e1" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kz kr ks kt bi translated"><strong class="io hj">多项式回归</strong>模型通常用最小二乘法拟合。<em class="la">最小二乘法最小化系数的方差，下</em> <a class="ae jk" href="https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem#:~:text=In%20statistics%2C%20the%20Gauss%E2%80%93Markov,and%20expectation%20value%20of%20zero." rel="noopener ugc nofollow" target="_blank"> <strong class="io hj"> <em class="la">高斯马尔可夫定理</em> </strong> </a> <em class="la">。</em></li><li id="ef84" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kz kr ks kt bi translated">多项式回归是线性回归的一种特殊情况，我们将<strong class="io hj">多项式方程</strong>拟合到因变量和自变量之间具有曲线关系的数据上。</li></ul><blockquote class="lb lc ld"><p id="3756" class="im in la io b ip iq ir is it iu iv iw le iy iz ja lf jc jd je lg jg jh ji jj hb bi translated">二次方程是二次多项式方程。但是，这个度数可以增加到第 n 个值。</p></blockquote><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/dc980accde7ba0f37b3e3ef160bb282d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*fU8Q4gAN-MHK-LAsWSDjjQ.jpeg"/></div></figure><p id="ff78" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，你知道什么是多项式回归了。让我们看一下假设列表，因为每个回归分析都有自己的假设。</p><h1 id="9b67" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">多项式回归的假设:</h1><ul class=""><li id="cc80" class="kj kk hi io b ip kl it km ix kn jb ko jf kp jj kz kr ks kt bi translated">因变量的行为可以用因变量和一组<em class="la"> k </em>自变量<em class="la"> (xi，i=1 到 k)之间的线性或曲线加性关系来解释。</em></li><li id="1865" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kz kr ks kt bi translated">因变量和任何自变量之间的关系是线性的或曲线的(特别是多项式)。</li><li id="ac54" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kz kr ks kt bi translated">自变量是相互独立的。</li><li id="dc7e" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kz kr ks kt bi translated">误差是独立的，正态分布，均值为零，方差恒定(<em class="la"> OLS </em>)。</li></ul><h1 id="750e" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">为什么我们需要多项式回归？</h1><p id="4a69" class="pw-post-body-paragraph im in hi io b ip kl ir is it km iv iw ix lm iz ja jb ln jd je jf lo jh ji jj hb bi translated">我们来考虑一个<a class="ae jk" rel="noopener" href="/analytics-vidhya/understanding-the-linear-regression-808c1f6941c0"> <strong class="io hj"> <em class="la">简单线性回归</em> </strong> </a>的情况。</p><ul class=""><li id="28dd" class="kj kk hi io b ip iq it iu ix lp jb lq jf lr jj kz kr ks kt bi translated">我们制作了模型，发现它表现很差，</li><li id="5c4b" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kz kr ks kt bi translated">我们观察实际值和我们预测的最佳拟合线之间的情况，看起来实际值在图中有某种曲线，我们的线没有接近切割点的平均值。</li><li id="1471" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kz kr ks kt bi translated">这就是多项式回归发挥作用的地方，它预测遵循数据模式(曲线)的最佳拟合线，如下图所示:</li></ul><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/f72029603ce46d42e40d35bd124e1944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*_UaCxPswsCxkj9JzYXCiWg.png"/></div></figure><ul class=""><li id="78c2" class="kj kk hi io b ip iq it iu ix lp jb lq jf lr jj kz kr ks kt bi translated"><strong class="io hj">多项式回归不要求数据集中的自变量和因变量之间的关系是线性的，</strong>这也是线性回归和多项式回归的主要区别之一。</li><li id="8ff4" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kz kr ks kt bi translated">当数据中的点没有被线性回归模型捕获，并且<a class="ae jk" rel="noopener" href="/analytics-vidhya/understanding-the-linear-regression-808c1f6941c0">线性回归</a>不能清楚地描述最佳结果时，通常使用多项式回归。</li></ul><blockquote class="lt"><p id="1b1e" class="lu lv hi bd lw lx ly lz ma mb mc jj dx translated">当我们增加模型中的次数时，往往会增加模型的性能。然而，增加模型的次数也会增加数据过度拟合和欠拟合的风险<a class="ae jk" rel="noopener" href="/analytics-vidhya/over-fitted-and-under-fitted-models-f5c96e9ac581">。</a></p></blockquote><h1 id="f4aa" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw md jy jz ka me kc kd ke mf kg kh ki bi translated">如何找到方程的正确次数？</h1><p id="41a2" class="pw-post-body-paragraph im in hi io b ip kl ir is it km iv iw ix lm iz ja jb ln jd je jf lo jh ji jj hb bi translated">为了找到模型的正确程度以防止<a class="ae jk" rel="noopener" href="/analytics-vidhya/over-fitted-and-under-fitted-models-f5c96e9ac581">过度装配或</a>装配不足，我们可以使用:</p><ol class=""><li id="9605" class="kj kk hi io b ip iq it iu ix lp jb lq jf lr jj kq kr ks kt bi translated"><strong class="io hj">前向选择:<br/> </strong>该方法增加度数，直到其足够显著，以定义最佳可能模型。</li><li id="da64" class="kj kk hi io b ip ku it kv ix kw jb kx jf ky jj kq kr ks kt bi translated"><strong class="io hj">后向选择:<br/> </strong>该方法降低程度，直到它足够显著以定义最佳可能模型。</li></ol><h1 id="7752" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">多项式回归背后的数学！</h1><p id="9876" class="pw-post-body-paragraph im in hi io b ip kl ir is it km iv iw ix lm iz ja jb ln jd je jf lo jh ji jj hb bi translated">如果你知道什么是线性回归，那么你可能也会理解多项式回归背后的数学原理。<br/>线性回归基本上是一次多项式。<br/>我希望下面的图片能说明问题。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mg"><img src="../Images/dd8704f7408fbc118cce9cda70477713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ZnDb5b6UtBZ41Wy90MTkw.jpeg"/></div></div></figure><p id="58e7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在用矩阵乘法求出 b 的值。<br/>对于多变量，矩阵计算通过以下方式完成:</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/fbae168e6f9b3ff8dccd8a4b4ea9d77a.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*coks7fQGg5f5KqRlxWLyvA.jpeg"/></div></figure><blockquote class="lt"><p id="13f3" class="lu lv hi bd lw lx ly lz ma mb mc jj dx translated">为了更好地理解背后的数学，我建议你参考这个<a class="ae jk" href="http://polynomialregression.drque.net/math.html" rel="noopener ugc nofollow" target="_blank">链接</a>，它清楚地解释了数学。</p></blockquote><h1 id="c23f" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw md jy jz ka me kc kd ke mf kg kh ki bi translated">多项式回归的成本函数</h1><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es mm"><img src="../Images/32ccf52a7d691b690d46d8ebd8bf9323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Wg6lnFkfyBmCTfI4"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">Pepi Stojanovski 在<a class="ae jk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="0989" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">成本函数</strong>是<strong class="io hj">针对给定数据衡量机器学习模型</strong>性能的函数。<br/>成本函数基本上是预测值和期望值之间的误差的计算，<strong class="io hj">以单个实数的形式呈现</strong>。<br/>许多人混淆了<strong class="io hj">成本函数</strong>和<strong class="io hj">损失函数</strong>，<br/>简单来说<strong class="io hj">成本函数</strong>是数据中 n 个样本的平均误差，而<strong class="io hj">损失函数</strong>是单个数据点的误差。换句话说，<strong class="io hj">损失函数</strong>是针对一个训练样本的，<strong class="io hj">成本函数</strong>是针对整个训练集的。</p><p id="a2eb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以，当我们清楚什么是成本函数时，让我们继续。</p><blockquote class="lb lc ld"><p id="24bf" class="im in la io b ip iq ir is it iu iv iw le iy iz ja lf jc jd je lg jg jh ji jj hb bi translated">如果你读过我关于<a class="ae jk" rel="noopener" href="/analytics-vidhya/understanding-the-linear-regression-808c1f6941c0">线性回归</a>的文章，你就会知道线性回归的成本函数。</p></blockquote><ul class=""><li id="e48c" class="kj kk hi io b ip iq it iu ix lp jb lq jf lr jj kz kr ks kt bi translated">多项式回归的成本函数也可以取为<em class="la">均方误差</em>，但是方程会有微小的变化。</li></ul><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/2d6ebaa61b18ba8dacb2f13a346a2df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/0*ChRI-B3qQlfQWEFx.jpeg"/></div></figure><pre class="li lj lk ll fd ms mt mu mv aw mw bi"><span id="b175" class="mx jm hi mt b fi my mz l na nb">#Cost Function of Linear Regression<br/>J = 1/n*sum(square(pred - y))</span><span id="2606" class="mx jm hi mt b fi nc mz l na nb">Which, can also be written as :<br/>J = 1/n*sum(square(pred-(b0 + b1x1)))  i.e, y = mx+b</span><span id="f919" class="mx jm hi mt b fi nc mz l na nb">#Cost Function of Polynomial Ression<br/>J = 1/n*sum(square(pred - y))<br/>However,here the eqaution of y will change.So,the equation can also be written as:<br/>J = 1/n*sum(square(pred - (b0 + b1x + b2x^2 + b3x^3.....))</span></pre><ul class=""><li id="7589" class="kj kk hi io b ip iq it iu ix lp jb lq jf lr jj kz kr ks kt bi translated">多项式回归可以减少成本函数返回的成本。它给你的回归线一个曲线形状，使它更适合你的基础数据。通过应用高阶多项式，您可以更精确地拟合数据的回归线。</li></ul><p id="14a8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们知道成本函数的理想值是 0 或接近 0 的某个值。<br/>为了得到理想的成本函数，我们可以执行梯度下降，更新权重，从而使误差最小化。</p><h1 id="e1c8" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">多项式回归的梯度下降</h1><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/a5eea9c041e5f41bbc67a22aace4a456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*mPJjojuDRrLODKnBnt4LvQ.png"/></div></figure><p id="3a94" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">梯度下降是一种优化算法，用于找到使成本函数(成本)最小化的函数的参数(系数)值。</p><blockquote class="lb lc ld"><p id="4b8f" class="im in la io b ip iq ir is it iu iv iw le iy iz ja lf jc jd je lg jg jh ji jj hb bi translated">为了更多地了解它，并对梯度下降有一个完美的理解，我建议阅读杰森·布朗利的博客。</p></blockquote><p id="5673" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要更新 m 和 b 值以降低成本函数(最小化 MSE 值)并获得最佳拟合线，您可以使用梯度下降。想法是从随机的 m 和 b 值开始，然后迭代地更新这些值，达到最小成本。</p><p id="8c8f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">梯度下降后获得较低成本函数的步骤:</strong></p><p id="b148" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">→最初，m 和 b 的值将为 0，学习率(α)将被引入函数。<br/>学习率(α)的值取得很小，大约在 0.01 或 0.0001 之间。</p><blockquote class="lb lc ld"><p id="dfc4" class="im in la io b ip iq ir is it iu iv iw le iy iz ja lf jc jd je lg jg jh ji jj hb bi translated"><strong class="io hj">学习率是优化算法中的调整参数，该优化算法在向成本函数的最小值移动时确定每次迭代的步长。</strong></p></blockquote><p id="e184" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">→然后根据斜率(m)计算成本函数方程的偏导数，并计算截距(b)的导数。</p><blockquote class="lb lc ld"><p id="4a01" class="im in la io b ip iq ir is it iu iv iw le iy iz ja lf jc jd je lg jg jh ji jj hb bi translated"><em class="hi">熟悉微积分的人会理解导数是如何产生的。</em></p><p id="6983" class="im in la io b ip iq ir is it iu iv iw le iy iz ja lf jc jd je lg jg jh ji jj hb bi translated"><em class="hi">如果你不知道微积分，不要担心，只要理解它是如何工作的，就足够直观地思考幕后发生的事情了。</em></p></blockquote><p id="ffab" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">→计算导数后，斜率(m)和截距(b)在以下等式的帮助下更新。<br/> m = m 的 m-α*导数<br/> b = b 的 b-α*导数<br/>上面计算了 m 和 b 的导数，α是学习率。</p><blockquote class="lt"><p id="2284" class="lu lv hi bd lw lx ne nf ng nh ni jj dx translated"><em class="nj">如果你浏览过杰森·布朗利的博客</em><a class="ae jk" href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener ugc nofollow" target="_blank"><em class="nj"/></a><em class="nj">你可能已经理解了梯度下降背后的直觉，以及它如何试图达到全局最优(最低成本函数值)。</em></p></blockquote><blockquote class="lb lc ld"><p id="b838" class="im in la io b ip nk ir is it nl iv iw le nm iz ja lf nn jd je lg no jh ji jj hb bi translated"><strong class="io hj"> <em class="hi">为什么要用导数减去权重(m 和 b)？</em> </strong> <em class="hi"> <br/>梯度给我们损失函数的最陡上升的方向，最陡下降的方向与梯度相反，这就是为什么我们从权重(m 和 b)中减去梯度</em></p></blockquote><p id="5832" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">→更新 m 和 b 的值的过程继续，直到成本函数达到理想值 0 或接近 0。<br/>m 和 b 的值现在将是描述最佳拟合线的最佳值。</p><blockquote class="lt"><p id="a8a6" class="lu lv hi bd lw lx ne nf ng nh ni jj dx translated">我希望以上内容对你有意义。</p></blockquote><p id="b84d" class="pw-post-body-paragraph im in hi io b ip nk ir is it nl iv iw ix nm iz ja jb nn jd je jf no jh ji jj hb bi translated">快乐学习！！！！！</p></div><div class="ab cl np nq gp nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="hb hc hd he hf"><p id="9148" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">喜欢我的文章？请为我鼓掌并分享它，因为这将增强我的信心。此外，我每周日都会发布新文章，所以请保持联系，以了解数据科学和机器学习基础系列的未来文章。</p><p id="f3e7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">还有，一定要在 LinkedIn 上和我联系。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="er es nw"><img src="../Images/d2497b3760c07ae02d7e004bf000957c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hrcustka3ieYh0yx"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated"><a class="ae jk" href="https://unsplash.com/@alx_andru?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Alex </a>在<a class="ae jk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div></div>    
</body>
</html>