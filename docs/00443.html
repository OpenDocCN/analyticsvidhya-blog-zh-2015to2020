<html>
<head>
<title>How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚在NLP中是如何工作的？最新最先进型号指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-do-transformers-work-in-nlp-a-guide-to-the-latest-state-of-the-art-models-52424082c132?source=collection_archive---------1-----------------------#2019-06-19">https://medium.com/analytics-vidhya/how-do-transformers-work-in-nlp-a-guide-to-the-latest-state-of-the-art-models-52424082c132?source=collection_archive---------1-----------------------#2019-06-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/03077c3c6d76f17f56e874a0130a1f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQgJI5CYDPjWSEofAtpcxg.jpeg"/></div></div></figure><h1 id="e580" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">介绍</h1><p id="cd0a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">我喜欢现在作为一名数据科学家在自然语言处理部门工作。突破和发展正以前所未有的速度发生。从超高效的ULMFiT框架到谷歌的BERT，NLP真正处于黄金时代。</p><p id="3f65" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">这场革命的核心是变压器的概念。这改变了我们数据科学家处理文本数据的方式——您将很快在本文中看到这一点。</p><p id="a959" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">想知道Transformer有多有用吗？看看下面这段话:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/85f2fcb78878fe097fbd803a87e8a70f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*935a3MNDXjUEAqO7.png"/></div></div></figure><p id="e7df" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">突出显示的单词指的是同一个人——格里兹曼，一个受欢迎的足球运动员。对我们来说，找出文本中这些词之间的关系并不困难。然而，对于一台机器来说，这是一项艰巨的任务。</p><p id="7075" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">捕捉句子中的这种关系和单词序列对于机器理解自然语言至关重要。<strong class="jq hj">这就是变压器概念发挥重要作用的地方。</strong></p><p id="898f" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><em class="kx">注意:本文假设对一些深度学习概念有基本的理解:</em></p><ul class=""><li id="b6f4" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl ld le lf lg bi translated"><a class="ae km" href="https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/?utm_source=medium&amp;utm_medium=understanding-transformers-nlp-state-of-the-art-models" rel="noopener ugc nofollow" target="_blank"> <em class="kx">深度学习精要——序列对序列建模注意</em> </a></li><li id="9d7e" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><em class="kx"> F </em> <a class="ae km" href="https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/?utm_source=medium&amp;utm_medium=understanding-transformers-nlp-state-of-the-art-models" rel="noopener ugc nofollow" target="_blank"> <em class="kx">深度学习基础—递归神经网络简介</em> </a></li><li id="9f15" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae km" href="https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/?utm_source=medium&amp;utm_medium=understanding-transformers-nlp-state-of-the-art-models" rel="noopener ugc nofollow" target="_blank"><em class="kx">Python中使用深度学习的文本摘要综合指南</em> </a></li></ul><h1 id="5798" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">目录</h1><ol class=""><li id="9f85" class="ky kz hi jq b jr js jv jw jz lm kd ln kh lo kl lp le lf lg bi translated">序列间模型—背景<br/>—基于RNN的序列间模型<br/>—挑战</li><li id="eaa0" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl lp le lf lg bi translated">NLP中的Transformer介绍<br/> -了解模型架构<br/> -掌握自关注的诀窍<br/> -自关注的计算<br/>-Transformer的局限性</li><li id="8e1e" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl lp le lf lg bi translated">了解Transformer-XL <br/> -使用Transformer进行语言建模<br/> -使用Transformer-XL进行语言建模</li><li id="28e0" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl lp le lf lg bi translated">NLP中的新感觉:Google的BERT <br/> -模型架构<br/> - BERT预训练任务</li></ol><h1 id="b4fc" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">序列间模型——背景</h1><p id="5765" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj">序列到序列(seq 2 seq)</strong>NLP中的模型用于将类型A的序列转换为类型b的序列。例如，将英语句子翻译为德语句子是序列到序列的任务。</p><p id="b821" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">基于递归神经网络(RNN)的序列对序列模型</strong>自2014年推出以来，已经获得了大量的关注。当前世界中的大多数数据都是以序列的形式存在的——它可以是数字序列、文本序列、视频帧序列或音频序列。</p><p id="d035" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">这些seq2seq型号的性能在2015年增加了<strong class="jq hj">注意机制</strong>后得到了进一步增强。在过去的5年中，NLP的发展速度是如此之快，令人难以置信！</p><p id="b673" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">这些序列到序列模型非常通用，可以用于各种NLP任务，例如:</p><ul class=""><li id="e4eb" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl ld le lf lg bi translated">机器翻译</li><li id="677e" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">文本摘要</li><li id="7fc1" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">语音识别</li><li id="b48f" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">问答系统等等</li></ul><h1 id="fc20" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">基于RNN的序列间模型</h1><p id="04f9" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">让我们举一个简单的序列到序列模型的例子。查看下图:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/2c68e84be63152e280a252a1b1a5e378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*2YMtHziIJyEUzxM4.gif"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">使用序列到序列模型的德语到英语翻译</figcaption></figure><p id="d3f8" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">上面的seq2seq模型将德语短语转换成英语短语。让我们来分解一下:</p><ul class=""><li id="dd76" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl ld le lf lg bi translated"><strong class="jq hj">编码器</strong>和<strong class="jq hj">解码器</strong>都是rnn</li><li id="e26d" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">在编码器中的每个时间步长，RNN从输入序列中提取一个字向量(xi ),并从前一个时间步长中提取一个隐藏状态(Hi)</li><li id="9ed4" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">隐藏状态在每个时间步更新</li><li id="af7c" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">来自最后一个单元的隐藏状态被称为<strong class="jq hj">上下文向量。</strong>这包含关于输入序列的信息</li><li id="7a33" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">该上下文向量随后被传递给解码器，然后用于生成目标序列(英语短语)</li><li id="c452" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">如果我们使用<strong class="jq hj">注意机制</strong>，那么隐藏状态的加权和作为上下文向量被传递给解码器</li></ul><h1 id="1289" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">挑战</h1><p id="e66e" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">尽管seq-2-seq模型在这方面做得非常好，但仍有一定的局限性，请注意:</p><ul class=""><li id="6152" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl ld le lf lg bi translated">处理长期依赖仍然具有挑战性</li><li id="d868" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">模型架构的顺序本质阻止了并行化。谷歌大脑的变压器概念解决了这些挑战</li></ul><h1 id="ac76" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">变压器介绍</h1><p id="1aa4" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">NLP中的Transformer是一种新颖的体系结构，旨在解决序列到序列的任务，同时轻松处理长距离依赖性。论文中提出的变压器<a class="ae km" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">你所需要的就是注意力</a>。推荐任何对NLP感兴趣的人阅读。</p><p id="908d" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">引用报纸上的话:</p><blockquote class="lv lw lx"><p id="1db6" class="jo jp kx jq b jr kn jt ju jv ko jx jy ly kp kb kc lz kq kf kg ma kr kj kk kl hb bi translated">“Transformer是第一个完全依靠自我关注来计算其输入和输出的表示，而不使用序列对齐的RNNs或卷积的转导模型。”</p></blockquote><p id="661c" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">这里，“转导”是指将输入序列转换成输出序列。Transformer背后的想法是用<strong class="jq hj">注意力</strong>和完全递归来处理输入和输出之间的依赖关系。</p><p id="cf72" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">下面我们来看看变压器的架构。这看起来可能有点吓人，但是不要担心，我们会一点一点地分解和理解它。</p><h1 id="e821" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">了解Transformer的模型架构</h1><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/1a5120d25cde9e8b18e7112a7de44138.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/0*f_D_v8xhsIQOOnI0.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">《变形金刚》——模型建筑<br/>(来源:<a class="ae km" href="https://arxiv.org/abs/1706.03762)" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.03762)</a></figcaption></figure><p id="f311" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">上图是Transformer架构的绝佳展示。让我们首先只关注<strong class="jq hj">编码器</strong>和<strong class="jq hj">解码器</strong>部分。</p><p id="74f6" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">现在关注下图。编码器模块有一层<strong class="jq hj">多头注意力</strong>，后面是另一层<strong class="jq hj">前馈神经网络</strong>。另一方面，解码器有一个额外的<strong class="jq hj">屏蔽</strong> <strong class="jq hj">多头注意力。</strong></p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/39705da4f2f5dd75261437b4b532216a.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/0*Nyhh1MR8rlJH0t4h.png"/></div></figure><p id="804a" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">编码器和解码器模块实际上是多个相同的编码器和解码器堆叠在一起。</strong>编码器堆栈和解码器堆栈具有相同数量的单元。</p><p id="793d" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">编码器和解码器单元的数量是一个超参数。本文使用了6种编码器和解码器。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es md"><img src="../Images/0ed665504c54277d131b500084c18fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/0*fmQdx3JOphxWNeom.png"/></div></figure><p id="31c2" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">让我们看看编码器和解码器堆栈的设置是如何工作的:</p><ul class=""><li id="ad6a" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl ld le lf lg bi translated">输入序列的字嵌入被传递到第一编码器</li><li id="c67f" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">这些然后被转换并传播到下一个编码器</li><li id="ff25" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">编码器堆栈中最后一个编码器的输出被传递给解码器堆栈中的所有解码器，如下图所示:</li></ul><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es me"><img src="../Images/3dbf2c8f06d337219fdbaf845a6d01d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/0*Cox-B1vdqY8lLHOx.png"/></div></figure><p id="831d" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">这里需要注意一件重要的事情——除了<strong class="jq hj">自关注</strong>和前馈层，解码器还有一层编码器-解码器关注层。这有助于解码器关注输入序列的适当部分。</p><p id="1b30" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">你可能会想——这个“自我关注”层在《变形金刚》中到底做了什么？问得好！这可以说是整个设置中最关键的组件，所以让我们来理解这个概念。</p><h1 id="6a36" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">获得自我关注的诀窍</h1><p id="7527" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">根据该文件:</p><blockquote class="lv lw lx"><p id="8859" class="jo jp kx jq b jr kn jt ju jv ko jx jy ly kp kb kc lz kq kf kg ma kr kj kk kl hb bi translated">自我注意，有时称为内部注意，是一种为了计算序列的表示而将单个序列的不同位置联系起来的注意机制</p></blockquote><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/30574d4590dd945e03f6900127344740.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/0*yw7w-htFOSHpoU_o.png"/></div></figure><p id="fb97" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">看一下上面的图片。你能猜出这个句子中的术语“它”指的是什么吗？</p><p id="d5b9" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">它指的是街道还是动物？对我们来说这是个简单的问题，但对算法来说不是。当模型在处理“它”这个词时，自我注意试图将“它”与同一个句子中的<strong class="jq hj">“动物”</strong>联系起来。</p><p id="d69d" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">自我注意允许模型查看输入序列中的其他单词，以更好地理解序列中的某个单词。现在，让我们看看如何计算自我关注度。</p><h1 id="b5be" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">计算自我注意力</h1><p id="383a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">为了便于理解，我将这一部分分成了不同的步骤。</p><ol class=""><li id="fd1d" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl lp le lf lg bi translated">首先，我们需要从编码器的每个输入向量中创建三个向量:<br/> -查询向量<br/> -关键向量<br/> -值向量。<br/>(这些向量在训练过程中被训练和更新。完成这一部分后，我们将对他们的角色有更多了解)</li><li id="2822" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl lp le lf lg bi translated">接下来，我们将为输入序列中的每个单词计算自我注意</li><li id="5f55" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl lp le lf lg bi translated">想想这句话——“行动得到结果”。为了计算第一个词“行动”的自我关注度，我们将计算短语中所有词相对于“行动”的得分。当我们对输入序列中的某个单词进行编码时，这个分数决定了其他单词的重要性。</li></ol><p id="db91" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">通过取查询向量(q1)与所有单词的关键字向量(k1，k2，k3)的点积来计算第一个单词的分数:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/70050d9df7f9b1d998337668d346c8ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/0*WpxNAIcZxnTN4TQB.png"/></div></figure><p id="f23e" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">然后，这些分数除以8，8是关键向量维数的平方根:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/28ab0b982859a6b01eef9f9b91178588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/0*bJDMm-FgHFi5lWzR.png"/></div></figure><p id="1ca0" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">接下来，使用softmax激活函数对这些分数进行归一化:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/b1dc43e90658aaee41be8efe2b546a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/0*bATgCJd28eo4GrA5.png"/></div></figure><p id="0004" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">然后，将这些归一化的分数乘以值向量(v1，v2，v3 ),并将结果向量相加，以得到最终向量(z1)。这是自我关注层的输出。然后，它被传递到前馈网络作为输入:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/70979861830e6278fe0ef535a51412d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_Y_PfkPp3miuXCbB.png"/></div></figure><p id="088b" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">因此，z1是输入序列“动作得到结果”的第一个单词的自我注意向量。我们可以用同样的方式得到输入序列中剩余单词的向量:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/2c1595253da78245f4ab75d96977a095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3VULdVWgGxlW0und.png"/></div></div></figure><p id="de1b" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">在Transformer的架构中，自我关注不是一次而是多次计算，并行且独立。因此被称为<strong class="jq hj">多头关注</strong>。输出被连接并进行线性转换，如下图所示:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/2ec38711af0e002dc5288bec594e176c.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/0*JhuJReMjq0HnT5Tj.png"/></div></figure><p id="733a" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">根据“注意力是你所需要的一切”这篇论文:</p><blockquote class="lv lw lx"><p id="3f40" class="jo jp kx jq b jr kn jt ju jv ko jx jy ly kp kb kc lz kq kf kg ma kr kj kk kl hb bi translated"><em class="hi">“多头注意力允许模型在不同位置共同注意来自不同表征子空间的信息。”</em></p></blockquote><p id="f893" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">你可以在这里访问实现Transformer <a class="ae km" href="https://paperswithcode.com/paper/attention-is-all-you-need" rel="noopener ugc nofollow" target="_blank">的代码。</a></p><h1 id="ecee" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">变压器的局限性</h1><p id="84e8" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">与基于RNN的seq2seq模型相比，Transformer无疑是一个巨大的改进。但是它也有自己的局限性:</p><ul class=""><li id="848d" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl ld le lf lg bi translated">注意只能处理定长文本串。在作为输入输入到系统之前，文本必须被分割成一定数量的段或块</li><li id="344e" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">这种文本分块导致<strong class="jq hj">上下文碎片</strong>。例如，如果一个句子从中间分开，那么大量的上下文就丢失了。换句话说，在不考虑句子或任何其他语义边界的情况下分割文本</li></ul><p id="6fbe" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">那么我们如何处理这些重大问题呢？这是和Transformer一起工作的人问的问题。由此诞生了Transformer-XL。</p><h1 id="5a75" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">了解变压器-XL</h1><p id="bb20" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">变压器架构可以学习长期依赖性。但是，由于使用了固定长度的上下文(输入文本段)，它们不能超出某个级别。论文中提出了一种新的架构来克服这个缺点——<a class="ae km" href="https://arxiv.org/pdf/1901.02860.pdf" rel="noopener ugc nofollow" target="_blank">Transformer-XL:固定长度上下文之外的注意力语言模型</a>。</p><p id="b31c" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">在这种架构中，在先前片段中获得的隐藏状态被重新用作当前片段的信息源。由于信息可以从一个部分流向下一个部分，因此它可以对长期依赖关系进行建模。</p><h1 id="4c69" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">使用转换器进行语言建模</h1><blockquote class="lv lw lx"><p id="66a6" class="jo jp kx jq b jr kn jt ju jv ko jx jy ly kp kb kc lz kq kf kg ma kr kj kk kl hb bi translated"><em class="hi">把语言建模想象成一个在给定前一个单词的情况下估计下一个单词的概率的过程。</em></p></blockquote><p id="b968" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1808.04444" rel="noopener ugc nofollow" target="_blank"> Al-Rfou等人(2018) </a>提出了<strong class="jq hj">应用Transformer模型进行语言建模</strong>的思想。根据这篇论文，整个语料库可以被分割成大小可控的固定长度的片段。然后，我们独立地在分段上训练变压器模型，忽略来自先前分段的所有上下文信息:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/d43c26bdb48461b838d5a0109edc4233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gb947GhvaPRvhcgM.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">分段长度为4的变压器模型(来源:<a class="ae km" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1901.02860</a></figcaption></figure><p id="408f" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">这种架构没有渐变消失的问题。但语境碎片化限制了其长期依存学习。在评估阶段，线段仅向右移动一个位置。新的段必须完全从头开始处理。不幸的是，这种评估方法是相当计算密集型的。</p><h1 id="4589" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">使用Transformer-XL进行语言建模</h1><p id="d49b" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在Transformer-XL的训练阶段，为先前状态计算的隐藏状态被用作当前段的附加上下文。Transformer-XL的这种递归机制考虑到了使用固定长度上下文的局限性。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/88af23fc7023377ab272d2c60624600a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TZcAhL7s7KY7ZKgK.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated"><em class="mo">分段长度为4的变压器XL型号</em></figcaption></figure><p id="b15f" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">在评估阶段，可以重用来自先前片段的表示，而不是从头开始计算(如变压器模型的情况)。这当然提高了计算速度。</p><p id="90f9" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">你可以在这里访问实现Transformer-XL <a class="ae km" href="https://paperswithcode.com/paper/transformer-xl-attentive-language-models" rel="noopener ugc nofollow" target="_blank">的代码。</a></p><h1 id="af47" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">NLP的新感觉:Google的BERT(变形金刚的双向编码器表示)</h1><p id="e40d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">我们都知道<strong class="jq hj">转移学习</strong>在计算机视觉领域的重要性。例如，预先训练的深度学习模型可以针对ImageNet数据集上的新任务进行微调，并且仍然可以在相对较小的标记数据集上给出不错的结果。</p><p id="5065" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">语言模型预训练同样对于改进许多自然语言处理任务非常有效:</p><ul class=""><li id="be32" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl ld le lf lg bi translated"><a class="ae km" href="https://openai.com/blog/language-unsupervised/" rel="noopener ugc nofollow" target="_blank">https://papers with code . com/paper/transformer-XL-attention-language-models</a></li><li id="773c" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae km" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">https://papers with code . com/paper/transformer-XL-attention-language-models</a>。</li></ul><p id="4db4" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">来自谷歌人工智能的新语言表示模型BERT framework使用预训练和微调来创建用于广泛任务的最先进的模型。这些任务包括问答系统、情感分析和语言推理。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/027f4b288a5112ee6d63dd5c84571b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/0*os2Csce-PF01JAGn.png"/></div></figure><h1 id="1c29" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">伯特的模型建筑</h1><p id="490f" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">BERT使用多层双向变压器编码器。它的自我关注层在两个方向上执行自我关注。谷歌发布了这种模式的两种变体:</p><ol class=""><li id="cee3" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl lp le lf lg bi translated"><strong class="jq hj"> BERT Base </strong>:变压器层数= 12层，总参数= 110M</li><li id="2d4c" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl lp le lf lg bi translated"><strong class="jq hj"> BERT Large </strong>:变压器层数= 24层，总参数= 340米</li></ol><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/7b9151a259e512984d1ae0c0d26b6d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/0*jq-5D-FBN7JItuzJ.png"/></div></figure><p id="f37c" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">BERT通过对几项任务进行预训练来使用双向性，这些任务包括<strong class="jq hj">屏蔽语言模型</strong>和<strong class="jq hj">下一句预测</strong>。让我们详细讨论这两个任务。</p><h1 id="e25f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">BERT培训前任务</h1><p id="d702" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">使用以下两个无监督预测任务对BERT进行预训练。</p><h2 id="bc48" class="mr ir hi bd is ms mt mu iw mv mw mx ja jz my mz je kd na nb ji kh nc nd jm ne bi translated">1.掩蔽语言建模(MLM)</h2><p id="5e6c" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">根据<a class="ae km" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>:</p><blockquote class="lv lw lx"><p id="72e2" class="jo jp kx jq b jr kn jt ju jv ko jx jy ly kp kb kc lz kq kf kg ma kr kj kk kl hb bi translated"><em class="hi">“屏蔽语言模型随机屏蔽输入中的一些标记，目标是仅基于其上下文预测屏蔽单词的原始词汇id。与从左到右的语言模型预训练不同，MLM目标允许表示融合左右上下文，这允许我们预训练深度双向转换器。”</em></p></blockquote><p id="3fa2" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">谷歌人工智能研究人员随机屏蔽了每个序列中15%的单词。任务？来预测这些蒙面文字。这里有一个警告—被屏蔽的单词并不总是被屏蔽的标记[MASK]替换，因为[MASK]标记在微调过程中永远不会出现。</p><p id="5445" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">因此，研究人员使用了以下技术:</p><ul class=""><li id="8a66" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl ld le lf lg bi translated">80%的情况下，单词被替换为掩码标记[MASK]</li><li id="44aa" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">10%的时候，这些单词被随机的单词替换</li><li id="50cf" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">10%的时间单词保持不变</li></ul><h2 id="45b6" class="mr ir hi bd is ms mt mu iw mv mw mx ja jz my mz je kd na nb ji kh nc nd jm ne bi translated">2.下一句预测</h2><p id="e07e" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">通常，语言模型不能捕捉连续句子之间的关系。伯特也预先接受了这项任务的训练。</p><p id="be4d" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">对于语言模型预训练，BERT使用句子对作为其训练数据。每一对的选句都挺有意思的。我们试着借助一个例子来理解一下。</p><p id="fd89" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">假设我们有一个包含100，000个句子的文本数据集，我们希望使用该数据集预先训练一个BERT语言模型。因此，将有50，000个训练示例或句子对作为训练数据。</p><ul class=""><li id="7c6d" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl ld le lf lg bi translated">对于50%的配对，第二句实际上是第一句的下一句</li><li id="db7d" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">对于剩余的50%的对，第二个句子将是来自语料库的随机句子</li><li id="834f" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">第一种情况的标签是<strong class="jq hj"><em class="kx">‘是下一个’</em></strong>，第二种情况的标签是<strong class="jq hj"><em class="kx">‘不是下一个’</em></strong></li></ul><p id="9dc9" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">像BERT这样的架构证明了无监督学习(预训练和微调)将成为许多语言理解系统中的关键元素。低资源任务尤其可以从这些深度<em class="kx">双向</em>架构中获得巨大收益。</p><p id="c193" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">下面是BERT在其中扮演重要角色的几个NLP任务的快照:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/d461988a75bd5624edb8f5ebed827407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/0*HjOe6JEBIQomUthr.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated"><em class="mo">来源:</em><a class="ae km" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"><em class="mo">https://arxiv.org/abs/1810.04805</em></a></figcaption></figure><h1 id="8aeb" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结束注释</h1><p id="b51b" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">我们真的应该认为自己很幸运，因为这么多最先进的进步正在以如此快的速度在NLP中发生。像Transformers和BERT这样的架构为未来几年更高级的突破铺平了道路。</p><p id="8386" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">我鼓励你实现这些模型，并在下面的评论部分分享你的工作。如果您对本文有任何反馈或任何疑问/疑问，请告诉我，我会尽快回复您。</p><p id="b603" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">您也可以参加以下课程来学习或提高您的NLP技能:</p><ul class=""><li id="b915" class="ky kz hi jq b jr kn jv ko jz la kd lb kh lc kl ld le lf lg bi translated"><a class="ae km" href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp??utm_source=medium&amp;utm_medium=understanding-transformers-nlp-state-of-the-art-models" rel="noopener ugc nofollow" target="_blank">使用Python的自然语言处理(NLP)</a></li></ul><p id="67b8" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><em class="kx">原载于2019年6月19日https://www.analyticsvidhya.com</em><em class="kx">T21</em><a class="ae km" href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>