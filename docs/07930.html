<html>
<head>
<title>Understanding Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-linear-regression-in-depth-intuition-6c9f3b1cbb51?source=collection_archive---------8-----------------------#2020-07-12">https://medium.com/analytics-vidhya/understanding-linear-regression-in-depth-intuition-6c9f3b1cbb51?source=collection_archive---------8-----------------------#2020-07-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="d4f1" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="cbca" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">本文试图成为你在<strong class="jf hj">理解使用<em class="kb">梯度下降</em> </strong>的<em class="kb">线性回归</em>算法时需要的参考。虽然这个算法很简单，但只有少数人真正理解它的数学和基本原理。</p><p id="4a43" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">我希望这篇文章能找到你的书签！现在，让我们深入研究一下吧！</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/9a370a9d6cf4c878b03f65abe3b90e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zJVLxD-6hPQAPZuzn07gMQ.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">介绍</figcaption></figure><h1 id="e86c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">目录</h1><ul class=""><li id="f0ff" class="kx ky hi jf b jg jh jk jl jo kz js la jw lb ka lc ld le lf bi translated">什么是线性回归？</li><li id="70d5" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated">处理简单线性回归</li><li id="1eae" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated">什么是损失函数？</li><li id="c04f" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated">什么是梯度下降？</li><li id="29a9" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated">多元线性回归概述</li><li id="df09" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated">线性回归的假设</li></ul><h1 id="ab3f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">什么是线性回归？</h1><p id="8f7a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">线性</strong>表示在特定行中，而<strong class="jf hj">回归</strong>表示关系的度量，因此线性回归是数据<em class="kb">(自变量)</em><em class="kb">(目标变量)</em>的<strong class="jf hj"><em class="kb"/></strong>线性关系。</p><h2 id="c62d" class="ll ig hi bd ih lm ln lo il lp lq lr ip jo ls lt it js lu lv ix jw lw lx jb ly bi translated">线性回归的类型</h2><ul class=""><li id="4422" class="kx ky hi jf b jg jh jk jl jo kz js la jw lb ka lc ld le lf bi translated">简单线性回归</li><li id="78a2" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated">多元线性回归</li></ul><h1 id="8530" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">处理简单线性回归</h1><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es lz"><img src="../Images/ffaedd0edc7dc74a08ae90922cbb78bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SgIj3njMLxAKMlCmlNvSvw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图1:简单的线性回归</figcaption></figure><p id="066d" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated"><em class="kb">简单线性回归</em>用于寻找两个连续变量之间的关系，即寻找自变量(预测值)和因变量(响应值)之间的关系。</p><p id="da6c" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">简单线性回归算法的<strong class="jf hj"> <em class="kb">难点</em>是获得最符合数据的直线。这是通过最小化损失函数来实现的。损失函数是什么？将在本博客稍后讨论。</strong></p><p id="df59" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">图2显示了回归线的方程式。<em class="kb"> </em>其中<em class="kb"> c </em>为y轴截距<em class="kb">，m </em>为直线相对于独立特征<em class="kb"> x </em>的斜率，y为预测值(也称为<em class="kb"> ŷ </em>)。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ma"><img src="../Images/1fe230a62c52beff0c11f5e335b85a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*SVzsonjOuuYnNdOfteBoqg.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图2:简单线性回归的回归线方程</figcaption></figure><h2 id="6b74" class="ll ig hi bd ih lm ln lo il lp lq lr ip jo ls lt it js lu lv ix jw lw lx jb ly bi translated">“m”表示什么？</h2><ul class=""><li id="0b2e" class="kx ky hi jf b jg jh jk jl jo kz js la jw lb ka lc ld le lf bi translated"><strong class="jf hj">如果m &gt; 0 </strong>，那么<em class="kb"> X </em>(预测值)和<em class="kb"> Y </em>(目标值)有一个<strong class="jf hj">正关系</strong>。这意味着Y的<em class="kb">值将随着X </em>值的增加而增加。</li><li id="1bc4" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">如果m &lt; 0 </strong>，那么<em class="kb"> X </em>(预测值)和<em class="kb"> Y </em>(目标值)有一个<strong class="jf hj">负关系</strong>。这意味着Y的<em class="kb">值将随着X </em>值的增加而减少。</li></ul><h2 id="b48e" class="ll ig hi bd ih lm ln lo il lp lq lr ip jo ls lt it js lu lv ix jw lw lx jb ly bi translated"><strong class="ak">c表示什么</strong>？</h2><ul class=""><li id="f1af" class="kx ky hi jf b jg jh jk jl jo kz js la jw lb ka lc ld le lf bi translated">是<em class="kb"> X </em> =0时<em class="kb"> Y </em>的值。假设，如果我们绘制一个图，其中X轴由经验年限(独立特征)组成，Y轴由工资组成(非独立特征)。工作年限= 0工资是多少，这是用c表示的。</li></ul><p id="98ce" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">现在你已经理解了回归线的理论，让我们讨论如何使用损失函数为特定模型选择最佳的回归线。</p><h2 id="05c2" class="ll ig hi bd ih lm ln lo il lp lq lr ip jo ls lt it js lu lv ix jw lw lx jb ly bi translated">什么是损失函数？</h2><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mb"><img src="../Images/1911de987774e60ff6574313d8fc8437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZXDNR8Y2vr9FFaABJcygJw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图3:有损失的简单线性回归</figcaption></figure><p id="e3c1" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated"><strong class="jf hj">损失函数</strong>是计算算法的当前输出和预期输出之间的距离的函数。这是一种评估算法如何对数据建模的方法。它可以分为两类。一个用于分类(离散值，0，1，2…)，另一个用于回归(连续值)。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mc"><img src="../Images/db5a214601f9c0d44c1a06dc7b4b60f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*RwRtwlQii2ir2hWPzSl-yw.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">损失函数的例子</figcaption></figure><p id="ac15" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">术语<strong class="jf hj">成本</strong>和<strong class="jf hj">损失函数</strong>几乎指的是同一个东西。<strong class="jf hj">成本函数</strong>计算为<strong class="jf hj">损失函数</strong>的平均值。<strong class="jf hj">损失函数</strong>是在每个实例中计算的值。因此，对于单个训练周期,<strong class="jf hj">损失</strong>被计算多次，而<strong class="jf hj">成本函数</strong>只被计算一次。</p><p id="d263" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated"><strong class="jf hj"> <em class="kb">注:对于依赖梯度下降优化模型参数的算法，每个选择的损失函数必须是可微的。</em>T19】</strong></p><p id="3428" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">在我们的例子中，考虑损失函数是均方误差(MSE)。图4显示了该损失函数的公式，其中<em class="kb"> n </em>是数据集中的样本数，<em class="kb"> Y </em>是实际值，<em class="kb">ŷ</em>是iᵗʰ数据点的相应预测值。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es md"><img src="../Images/6f864a24f3832a8fab365c936920b1a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jgV3BZJ58Ca627_M7olp0A.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图4:均方误差公式</figcaption></figure><h2 id="a687" class="ll ig hi bd ih lm ln lo il lp lq lr ip jo ls lt it js lu lv ix jw lw lx jb ly bi translated">什么是梯度下降？</h2><p id="e480" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">梯度下降是一种寻找函数最小值的迭代优化算法。这里，那个函数指的是损失函数。</p><p id="762d" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">同样在使用梯度下降时，术语学习率也出现了。<strong class="jf hj">学习速率</strong>由<strong class="jf hj"> α </strong>表示，该参数控制m和c的值在每次迭代/步骤后应该改变多少。选择合适的α值也很重要，如图5所示。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es me"><img src="../Images/98a0975b88fcf01d6a7840ec4a8be046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*An4tZEyQAYgPAZl396JzWg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图5:学习率案例</figcaption></figure><p id="8d63" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">从m和c的初始值<strong class="jf hj">为0.0 </strong>开始，并为学习率设置一个小值，例如<strong class="jf hj"> α=0.001 </strong>，对于该值，我们使用损失函数计算误差值。对于不同的m和c值<em class="kb">、</em>，我们将得到不同的误差值，如图6所示。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mf"><img src="../Images/3dffb343355896070b9a31824264b196.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/1*fEgZAEP4Mykk-3gES4wu8Q.gif"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图6:梯度下降工作(简单线性回归)</figcaption></figure><p id="fb03" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">一旦选择了初始值，我们就可以通过应用链式法则找到损失函数的偏导数。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mg"><img src="../Images/f3a67fb0d516c59b2a7865043d1d4b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nd6O-Bq5e6BwuFqv-SC5BA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图7:损失函数w . r . t . m和c的偏导数(斜率)</figcaption></figure><p id="c7f2" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">计算斜率后，我们现在使用图8所示的公式更新m和c的值。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mh"><img src="../Images/aa5ecb296838d8cbf378260ef2cab2be.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*bf0JevcIBPt6R49480ZU7Q.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图8:更新m和c的公式</figcaption></figure><ul class=""><li id="a320" class="kx ky hi jf b jg kc jk kd jo mi js mj jw mk ka lc ld le lf bi translated">如果特定点的斜率为负，则m和c的值增加，该点向右移动一小段距离，如图6所示。</li><li id="8f23" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated">如果特定点的斜率为<em class="kb">正</em>，则m和c的值减小，该点向左侧移动一小段距离。</li></ul><p id="8a0b" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">梯度下降算法不断改变m和c的值，直到损失变得非常小或变成0(理想情况下)，这就是我们如何找到最佳拟合回归线。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ml"><img src="../Images/7a2dec797ba855d1e97491c38745f367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*jpoqGLXvnsrBA-gYQuCP_A.gif"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">渐变下降的图形表示|| <a class="ae mm" href="https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931" rel="noopener" target="_blank">来源</a></figcaption></figure></div><div class="ab cl mn mo gp mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="hb hc hd he hf"><p id="2939" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">既然你已经理解了<em class="kb">简单线性回归</em>算法的完整工作原理，让我们看看<em class="kb">多元线性回归</em>算法是如何工作的。</p><h1 id="27fb" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">多元线性回归概述</h1><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mu"><img src="../Images/e84e7d4a3a531af48b2bfd9900d0ac09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dToo8pNrhBmYfwmPLp6WrQ.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图9:多元线性回归(图表)</figcaption></figure><p id="dab4" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">在现实生活中，永远不会有一个单一的特征来预测目标。因此，我们简单地执行多元线性回归。</p><p id="b21f" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">图10所示的方程与简单线性回归方程非常相似；简单地将独立特征/预测器的数量和它们相应的系数相加。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mv"><img src="../Images/3abab11500f554529e689bf143f19971.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*XJNAbOn6IDZOprbkvFVkrA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图10:多元线性回归(回归平面方程)</figcaption></figure><p id="d86f" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated"><em class="kb">注意:算法的工作方式保持不变，唯一变化的是梯度下降图。在简单线性回归中，梯度下降图呈2D形式，但随着独立特征/预测因子数量的增加，梯度下降图的维数也不断增加。</em></p><p id="a86f" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">图11显示了3D格式的梯度下降图，其中<em class="kb"> A </em>是初始权重/起点，而<em class="kb"> B </em>是全局最小值。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mw"><img src="../Images/0dd0ba8a7880f51476c7ac4df4477b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*pgB8rSfZ_zU30jkxoApEFQ.jpeg"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图11:梯度下降(多元线性回归)</figcaption></figure><p id="f30b" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">图12显示了从<em class="kb"> A </em>到<em class="kb">b</em>的3D格式梯度下降的完整工作</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mx"><img src="../Images/f5989d49ae0c8c9907dea7b3ecf124ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/1*dJHi8v2L2ksun53QqB5JKg.gif"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图12:梯度下降工作(多元线性回归)</figcaption></figure></div><div class="ab cl mn mo gp mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="hb hc hd he hf"><h1 id="1044" class="if ig hi bd ih ii my ik il im mz io ip iq na is it iu nb iw ix iy nc ja jb jc bi translated">线性回归的假设</h1><p id="300e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">以下是线性回归的基本假设，可用于回答我们是否可以对特定数据集使用线性回归算法的问题？</p><ul class=""><li id="75ed" class="kx ky hi jf b jg kc jk kd jo mi js mj jw mk ka lc ld le lf bi translated"><strong class="jf hj">特征和目标变量之间的线性关系:<br/> </strong>线性回归假设独立特征和目标之间的关系是线性的。它不支持任何其他内容。您可能需要转换数据以使关系呈线性(例如，指数关系的对数转换)。</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es nd"><img src="../Images/d436eebc39f728b4881835926c2eaff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntD6W_ZZ4iqyDrCmBfBEtA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">线性关系示例</figcaption></figure><ul class=""><li id="3e0e" class="kx ky hi jf b jg kc jk kd jo mi js mj jw mk ka lc ld le lf bi translated"><strong class="jf hj">要素间很少或没有多重共线性:<br/> </strong>当发现独立变量中度或高度相关时，多重共线性存在。在具有相关变量的模型中，找出预测值与目标变量的真实关系是一项艰巨的任务。换句话说，很难找出哪个变量实际上有助于预测响应变量。</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ne"><img src="../Images/550500ce9b279c2b5cc11a3bfd50fe16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*C0WH5zkTfoVzJ5oLrpTLtg.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">多重共线性示例</figcaption></figure><ul class=""><li id="345c" class="kx ky hi jf b jg kc jk kd jo mi js mj jw mk ka lc ld le lf bi translated"><strong class="jf hj">残差中很少或没有自相关:<br/> </strong>误差项中相关性的存在大大降低了模型的准确性。这通常发生在下一个时刻依赖于前一个时刻的时间序列模型中。如果误差项相关，估计的标准误差往往会低估真实的标准误差。</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es nf"><img src="../Images/75366e20039d79cfc2bccab5ac6944af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*XgL5uMccVwKDVrSba-gCfg.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">自相关的例子</figcaption></figure><ul class=""><li id="afd8" class="kx ky hi jf b jg kc jk kd jo mi js mj jw mk ka lc ld le lf bi translated"><strong class="jf hj">无异方差:<br/> </strong>误差项中非恒定方差的存在导致异方差。一般来说，非常数方差出现在异常值的情况下。看起来，这些值得到了太多的权重，从而不成比例地影响了模型的性能。</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ng"><img src="../Images/a00a6cf73e2c91fc8eba6f3b5658b854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eyelwAxJM1dSdvlUWDQmAg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">异方差的例子</figcaption></figure><ul class=""><li id="4580" class="kx ky hi jf b jg kc jk kd jo mi js mj jw mk ka lc ld le lf bi translated"><strong class="jf hj">误差项的正态分布:<br/> </strong>如果误差项不是正态分布，置信区间可能会变得过宽或过窄。一旦置信区间变得不稳定，就会导致基于最小二乘法估计系数的困难。非正态分布的存在表明存在一些不寻常的数据点，必须仔细研究这些数据点才能建立更好的模型。</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es nh"><img src="../Images/a2dbb37303103f895331a022e27f7fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1UxnNwijFkqNZUTWcGu8xQ.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">误差项的正态分布示例</figcaption></figure></div><div class="ab cl mn mo gp mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="hb hc hd he hf"><h1 id="319c" class="if ig hi bd ih ii my ik il im mz io ip iq na is it iu nb iw ix iy nc ja jb jc bi translated">参考</h1><ul class=""><li id="8896" class="kx ky hi jf b jg jh jk jl jo kz js la jw lb ka lc ld le lf bi translated"><a class="ae mm" href="https://www.youtube.com/watch?v=nk2CQITm_eo&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=9" rel="noopener ugc nofollow" target="_blank">Josh star mer的stat quest</a></li><li id="2613" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><a class="ae mm" href="https://machinelearningmastery.com/linear-regression-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">机器学习掌握度</a></li><li id="6f74" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><a class="ae mm" href="https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931" rel="noopener" target="_blank">阿达什·梅农的线性回归</a></li></ul><h1 id="ab99" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">帮我接通</h1><ul class=""><li id="90b7" class="kx ky hi jf b jg jh jk jl jo kz js la jw lb ka lc ld le lf bi translated"><a class="ae mm" href="https://www.linkedin.com/in/anujkvyas" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="3519" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated">GitHub </li><li id="46c6" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><a class="ae mm" href="https://www.kaggle.com/anujvyas" rel="noopener ugc nofollow" target="_blank">卡格尔</a></li></ul><p id="8b5f" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">那都是乡亲们！<a class="ae mm" rel="noopener" href="/analytics-vidhya/understanding-logistic-regression-in-depth-intuition-99ad14724464"> <strong class="jf hj">点击这里</strong> </a>阅读我关于<strong class="jf hj">逻辑回归</strong>的博客。</p><p id="95c8" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">如果你从这个博客中学到了什么，一定要给它一个👏🏼会在其他博客上和你见面，直到和平✌🏼</p></div></div>    
</body>
</html>