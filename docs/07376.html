<html>
<head>
<title>Why direction of steepest descent is always opposite to the gradient of loss function?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么最速下降的方向总是与损失函数的梯度相反？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-direction-of-steepest-descent-is-always-opposite-to-the-gradient-of-loss-function-dddc995a816e?source=collection_archive---------19-----------------------#2020-06-23">https://medium.com/analytics-vidhya/why-direction-of-steepest-descent-is-always-opposite-to-the-gradient-of-loss-function-dddc995a816e?source=collection_archive---------19-----------------------#2020-06-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9827" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们都听说过梯度下降算法，以及它是如何在每次迭代中以最小化损失函数的方式更新参数的。并且，最陡的下降是当损失函数最小时。但是你知道为什么最陡下降总是和损失函数的梯度相反吗？或者为什么我们称算法为‘梯度下降’？让我们来了解一下！</p><p id="af7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> w(t) = w(t-1) -α(∇L) </strong></p><p id="4d76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个更新等式中，- <strong class="ih hj"> ∇L </strong>是“相反”方向。但是为什么呢？</p><h2 id="007f" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">都在泰勒级数里！</h2><p id="8b2f" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">让我们考虑一下<strong class="ih hj"> w </strong>的一个小变化。更新后的权重将变成，<strong class="ih hj"> w(新)= w+ξδw</strong>。(记住w是矢量，所以<strong class="ih hj">δw</strong>是方向的变化，<strong class="ih hj"> η </strong>是变化的幅度)。</p><p id="2c16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让<strong class="ih hj">δw = u</strong>和<strong class="ih hj">转置(u) = v </strong>。新的损失函数w.r.t <strong class="ih hj"> w(new) </strong>将是<strong class="ih hj"> L( w + ηu) </strong>。</p><p id="3e64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设是<strong class="ih hj"> w </strong>的一个小变化，我们可以通过泰勒展开写出新的损失函数如下:</p><p id="21cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> L(w + ηu) = L(w) + (η)。v∇L(w) + (η /2！).v∇大学……</strong></p><p id="5781" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，η非常小。所以，我们可以忽略包含η的项，η和后面的项。所以，我们的新方程变成了，</p><p id="b1cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> L(w + ηu)-L(w) = (η)。v∇L(w) </strong></p><p id="ea40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所知，我们的目标是在每一步中最小化损失函数。因此，</p><p id="d146" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">L(w + ηu)-L(w) &lt; 0 即新损失小于旧损失。从上面的等式我们可以说，<strong class="ih hj"> v.∇L(w) &lt; 0 </strong></p><p id="fcec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> v.∇L(w) </strong>是点积。设<strong class="ih hj"> β </strong>为<strong class="ih hj"> v </strong>与<strong class="ih hj">∇l(w</strong>之间的角度</p><p id="b5cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">cos(<strong class="ih hj">β)=(v.∇l(w))/|v||∇l(w)|.</strong>为简单起见，让<strong class="ih hj"> |v||∇L(w)|=k </strong>。</p><p id="3475" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，cos( <strong class="ih hj"> β) = (v.∇L(w))/k，</strong>其中<strong class="ih hj"> -1≤ </strong> cos( <strong class="ih hj"> β)≤1。</strong></p><p id="970a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们希望<strong class="ih hj">【v.∇l(w】</strong>尽可能低/负(我们希望我们的新损失尽可能小于旧损失)。因此，我们希望cos( <strong class="ih hj"> β) </strong>尽可能低。cos( <strong class="ih hj"> β) </strong>可以取的最小值是-1。这种情况下，<strong class="ih hj"> β = 180度。</strong></p><p id="d806" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，为了最大程度地最小化损失函数，算法总是转向与损失函数的梯度相反的方向，<strong class="ih hj"> ∇L </strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kd"><img src="../Images/f51b407cf220ce54942e5dc3f39e7c5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/0*LFAZnvPvs5aS_Vwh.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><a class="ae kp" href="https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io/en/latest/_ images/gradient _ descent _ de mysticed</a></figcaption></figure><p id="4b41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kq">注:本文的概念基于NPTEL在线教授的课程</em> <a class="ae kp" href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="noopener ugc nofollow" target="_blank"> <em class="kq"> CS7015:深度学习</em> </a> <em class="kq">的视频。</em></p><h2 id="0768" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">资源:</h2><ol class=""><li id="6d5f" class="kr ks hi ih b ii jy im jz iq kt iu ku iy kv jc kw kx ky kz bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Taylor_series#:~:text=In%20mathematics%2C%20the%20Taylor%20series,are%20equal%20near%20this%20point." rel="noopener ugc nofollow" target="_blank">泰勒级数</a></li><li id="3710" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">梯度</li></ol></div></div>    
</body>
</html>