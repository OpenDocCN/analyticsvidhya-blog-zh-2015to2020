<html>
<head>
<title>Mathematical Prerequisites For Understanding Autoencoders and Variational Autoencoders (VAEs): Beginner Friendly, Intermediate Exciting, and Expert Refreshing.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解自动编码器和变分自动编码器(VAEs)的数学先决条件:初学者友好，中级兴奋，专家刷新。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mathematical-prerequisites-for-understanding-autoencoders-and-variational-autoencoders-vaes-8f854025390e?source=collection_archive---------2-----------------------#2020-05-28">https://medium.com/analytics-vidhya/mathematical-prerequisites-for-understanding-autoencoders-and-variational-autoencoders-vaes-8f854025390e?source=collection_archive---------2-----------------------#2020-05-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="36e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">几天前，我弟弟走进我的房间，看到我正在写代码和学习数学教程，他带着困惑的表情问道，你现在是在学习数学还是在建立人工智能模型？我笑着回答说，人工智能是数学！！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/df101b41aaa492515cc266cd027c0df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-hbU2nTW7wJjIyGe"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae jt" href="https://unsplash.com/@franckinjapan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Franck V. </a>拍摄的照片</figcaption></figure><p id="10d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，本质上不完全是数学，但要真正打下基础，一个人需要很好地掌握数学基础，因为AI不仅仅是用Pytorch、Tensorflow或Keras训练ConvNets或Transformer模型。事实上，阅读、理解和复制一篇研究论文至少需要对一些数学概念有相当的理解，包括但不限于线性代数、微积分、概率论和统计学。</p><p id="b0fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人工智能对每个人都是免费的，所以无论你是否来自计算机科学、数学或工程背景都没有多大关系，事实上，这根本不重要，因为互联网现在是一个信息过剩的时代，特别是随着MOOCs的出现，你可以自己学习所有这些基础知识，并做得更好。我知道这一点，因为在过去的5年里，作为一名学习城市规划的本科生，我做过的最复杂的数学是人口均值、偏度、峰度和矩。那是在我的第一年！我现在正打算四舍五入，我从来没有上过一门需要数学的课程。我最后一次认真的数学是在高中，现在我知道的其他事情都是自学的，我只是在途中学会的。</p><p id="30c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我知道很多人都是这种情况，你想深入研究人工智能，但你觉得你没有合适的背景，所以你留在原地或进入网络开发。如果你属于这一类，没有什么可以限制你！！！</p><p id="91c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我们将讲述理解自动编码器、变分自动编码器和矢量量化变分自动编码器(VQ-变分自动编码器)所需的一些基本数学知识。具体来说，我们将着眼于:</p><ol class=""><li id="bf6a" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">自动编码器综述</li><li id="2231" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">概率基础</li><li id="59c3" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">期望最大化</li><li id="66cf" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">库尔巴克·莱布勒散度及其意义</li></ol><p id="9e23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自动编码器本质上是一种神经网络，其被设计成以无监督的方式学习同一性函数，使得它可以压缩和重建原始输入，并且通过这样做，它发现原始输入数据的更有效和压缩的表示。值得注意的是，这个想法起源于20世纪80年代，后来由<a class="ae jt" href="https://pdfs.semanticscholar.org/c50d/ca78e97e335d362d6b991ae0e1448914e9a3.pdf" rel="noopener ugc nofollow" target="_blank">辛顿和萨拉胡特迪诺夫在2006年</a>的一篇开创性论文中提出。</p><p id="a6e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自动编码器广泛用于图像压缩和重建。图像重建基本上意味着自动编码器网络试图生成我们在输入阶段传递给它的任何图像。随着我们的进步，我们很快就会理解这是如何工作的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ki"><img src="../Images/1deaaf4b7a8971e80da42a4825a12c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_K4yRm8ILDZ02vPq.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">自动编码器架构的图示。来源<a class="ae jt" href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#td-vae" rel="noopener ugc nofollow" target="_blank">:李连文的博客</a></figcaption></figure><p id="8946" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图描绘了一个典型的自动编码器网络架构，为了直观地理解其工作原理，我们应该首先了解自动编码器网络包括:</p><p id="d230" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kj">省略输入和输出(重构输入)</em></p><ol class=""><li id="649c" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">由Gφ表示的编码器模块</li><li id="7d46" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">瓶颈用z表示</li><li id="fcf5" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">由Fθ表示的解码器模块</li></ol><p id="09e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">编码器模块接收图像的输入向量，并将其作为压缩向量“z”传递给瓶颈，然后解码器模块尝试从压缩表示中重建该输入图像。为了更好地理解，假设我们有一个大小为(28 x 28)的输入图像，按照惯例，这个图像在输入到神经网络之前必须被展平。该图像的展平表示将是(784，)并且这被传递到编码器(第一个块)。编码器的输出然后被馈送到瓶颈或潜在空间，该潜在空间应该是缩减的版本，例如，如果潜在空间中的节点数量是8、或16、或甚至任何数量，这仅仅意味着我们已经成功地将大小为784的图像压缩到仅仅8个节点、或16个节点、或任何数量。然后，解码器网络试图从瓶颈中的压缩状态重建原始(28×28)输入图像。事情就是这样的。</p><p id="af07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦图像被重建，您就可以将重建的图像与原始图像进行比较，计算差异，并计算可以最小化的损失。</p><p id="71c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">损失的计算方法如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kk"><img src="../Images/ed30a8091bfdb881a89d9ba7f0d6a1f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*18u4JC9BqTDjf4rXOsfxzA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">自动编码器损失函数</figcaption></figure><p id="e44c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不要对损失函数感到惊慌，我将分解它。</p><p id="9d67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，损失函数取决于“theta”和“phi ”,它们是定义编码器和解码器的参数。如前所述，根据上面的自动编码器图像，编码器由Gφ表示，而解码器由Fθ表示，它们只是表示神经网络的权重和偏差。</p><p id="3551" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在等式中，我们对原始图像x '和重建图像Fθ(gφ(x `))之间的差异进行求和。</p><p id="907f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种数学表示也显示了从编码器到解码器的数据流(即输入到输出)</p><h1 id="a692" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">变型自动编码器</strong></h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ki"><img src="../Images/7f5386cd23d3c9976a10e5dae6648b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ndg6glTk5l_ouLON.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">VAE模型的图解。来源:<a class="ae jt" href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#td-vae" rel="noopener ugc nofollow" target="_blank">李连文的博客</a></figcaption></figure><p id="d569" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由<a class="ae jt" href="https://arxiv.org/abs/1312.6114" rel="noopener ugc nofollow" target="_blank">金玛等人在2013年</a>提出的VAE背后的基本思想是，不是将输入映射到固定向量，而是将输入映射到分布。自动编码器和变分自动编码器在许多方面是相似的，事实上，自动编码器和变分自动编码器之间唯一的根本区别是VAE的瓶颈是连续的，并被两个独立的向量所取代；一个代表分布的平均值，另一个代表分布的标准偏差。</p><p id="ab5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">VAE的损失函数由两个项定义，重建损失和正则项，正则项本质上是编码器分布和潜在空间之间的KL散度。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lj"><img src="../Images/cccc1ef3442c741904185e09624484e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1SRJjMDV1FMYO9oC0HQGGA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">VAE损失函数，基本上是重建损失+ KL散度</figcaption></figure><p id="dc13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">向前看，理解自动编码器和VAE网络的更深层次的数学基础，如它们的损失函数，需要对一些概念有一个公平的理解，如期望最大化、条件概率、最大似然估计和Kulback Leibler散度，这是本文的关键。</p><p id="3616" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理解VAE的第一个先决条件是概率论，我不会讲得太详细，但会尽可能多地解释理解这些概念所需要的。</p><p id="363d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可能会更频繁地遇到以下术语:</p><p id="f788" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">P(x):这定义了随机变量X的概率</p><p id="70f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">P(x|y):被称为条件概率，它提供了一个随机变量x的概率，假设y已经发生。给定y，它读作x的P。</p><p id="cc24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个概率概念也可以写成:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lk"><img src="../Images/4cb4d525293cfbbe5063bac174720f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*Yf6tx0o19qibRK1fKa2ATQ.png"/></div></figure><p id="26ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的表示来自于贝叶定理，其中；</p><p id="08aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">P(y|x)是后验概率</p><p id="be7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">p(y)是先验概率</p><p id="e1a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">p(x|y) / p(x)是似然比</p><p id="0bdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还需要理解这样的全概率定理；</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ll"><img src="../Images/a083897b647d629bec0d98c453ae5534.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aghk_SWQwyaOliX5H_gWIQ.jpeg"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/02441a4ad123de70e069e16c2de2d24a.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*GlI4K9zw5F6uKdd7khirzw.png"/></div></figure><p id="e6ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">互斥事件也称为不相交，意味着它们之间没有重叠，这就是它们的交集等于零的原因。当我们进入更复杂的东西时，我们将会看到更多这样的东西，所以最好现在就解决它。</p><h2 id="7dbd" class="ln km hi bd kn lo lp lq kr lr ls lt kv iq lu lv kz iu lw lx ld iy ly lz lh ma bi translated"><strong class="ak">随机变量X的期望，即E(X) </strong></h2><p id="e321" class="pw-post-body-paragraph if ig hi ih b ii mb ik il im mc io ip iq md is it iu me iw ix iy mf ja jb jc hb bi translated">随机变量的期望值是X的所有可能值的加权平均值，其中每个值根据该事件的概率进行加权，其定义为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/1d4d0e5ddad9b0668c531e610ce14b91.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*WTixH1R9GN4E7sAlZQByTw.png"/></div></figure><p id="3447" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可能有点太复杂了，但是你应该知道这个概念类似于数学平均值。为了更好的直觉，你应该<a class="ae jt" href="https://www.youtube.com/watch?v=j__Kredt7vY" rel="noopener ugc nofollow" target="_blank">看看这个</a></p><p id="f152" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看一些简单的例子:</p><p id="aa45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Q1。</strong>掷骰子一次，得到三的概率是多少？</p><p id="f52b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">回答:</strong>你猜对了！给定样本空间= {1，2，3，4，5，6}</p><p id="73e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">P(x) = P(3) = 1/6。</p><p id="76fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Q2。在掷骰子时，如果掷出的骰子是奇数，出现3的概率是多少？</strong></p><p id="09f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">答案:</strong>这是一个条件概率即P(x|y)，我们追求的是得到一个x=3的概率，给定那个y =奇数。也就是P(3|y为奇数)。</p><p id="cd29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然条件是投掷是“奇数”，那么样本空间肯定要从6减少到3，因为如果你从1-6计数，只有3个奇数。因此，{1，2，3，4，5，6} ==&gt; {1，3，5}。因此，在这个缩减的样本空间中出现3的概率是1/3，其中3是我们在缩减的样本空间中的样本总数。这是我能找到的解释条件概率的最简单的例子。</p><p id="2be6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们观察正确，在第一个例子中，有一个3即P(3)的概率是1/6，但在第二个例子中，概率变成了1/3。这说明了什么？</p><p id="5bad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">再次，你猜对了！说明以Y为条件，即“奇数样本”时，得到3的概率增加。因此，知道p(x)和p(x|y)可以有不同的解释是很重要的。这就是条件概率的概念，因为“x”的概率会受到“y”的很大影响，如例子所示。</p><h1 id="2d89" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak"> KL发散</strong></h1><p id="1426" class="pw-post-body-paragraph if ig hi ih b ii mb ik il im mc io ip iq md is it iu me iw ix iy mf ja jb jc hb bi translated">Kulback-Leibler散度(简称D_KL)是一个概率分布与另一个概率分布如何不同的度量。对于离散概率分布P和Q，P和Q之间的KL散度定义为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mh"><img src="../Images/3db3f8eea1d01e615dfd7ec2bd403605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*3CHUM11FqvEm2o6nJ0ZSpA.png"/></div></figure><p id="d11d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">例如:</strong></p><p id="f05f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有两个概率分布，P &amp; Q。我们想找出两个概率之间的差异，我们可以简单地应用KL散度，如下所示。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mi"><img src="../Images/e736bf3c3091647f5db289455e5fd5ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e2AgsIoUlKYgf7EFLQfHaA.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mj"><img src="../Images/d816506637e07dfa1577d358b7382ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*CW-UzYVOHHvuoWrOIfIMsA.jpeg"/></div></figure><p id="8bc0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Medium实际上不支持数学符号，这就是为什么方程必须在别处求解，然后我把它作为图像粘贴在这里。为了解释上面的计算，我们有一个均匀概率为1/3的分布Q，即<strong class="ih hj"> ≈ </strong> 0.333，还有一个随机变量x等于0时概率为0.36的分布P，随机变量x等于1时概率为0.48，随机变量x等于2时概率为0.16。</p><p id="2e11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这两种分布之间的差异可以使用KL散度来计算，因此我们要做的是像上面一样代入KL散度方程中的值，并求解0.09673 nats的答案。</p><p id="a4a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“nats”只是通过使用自然对数(ln(x))获得的信息单位。</p><p id="6321" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理解KL散度对于理解VAE损失函数是不可或缺的，因为它扮演着一个关键的角色——作为一个正则项。这也是为什么我额外用图形和数学来说明它。</p><p id="1537" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经讨论了一些数学前提条件，这些条件将在下一步中派上用场，你应该看一看<a class="ae jt" rel="noopener" href="/retina-ai-health-inc/variational-inference-derivation-of-the-variational-autoencoder-vae-loss-function-a-true-story-3543a3dc67ee"> <strong class="ih hj">这篇由<a class="mk ml ge" href="https://medium.com/u/f3b4e4defa62?source=post_page-----8f854025390e--------------------------------" rel="noopener" target="_blank">Stephen Odaibo</a>博士撰写的关于完全从零开始推导VAE损失函数的说明性文章</strong> </a>。</p><p id="d2ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数学很有趣！</p></div></div>    
</body>
</html>