<html>
<head>
<title>Simple Linear Regression with an example using NumPy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用NumPy的简单线性回归示例</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/simple-linear-regression-with-example-using-numpy-e7b984f0d15e?source=collection_archive---------1-----------------------#2019-11-12">https://medium.com/analytics-vidhya/simple-linear-regression-with-example-using-numpy-e7b984f0d15e?source=collection_archive---------1-----------------------#2019-11-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f97844fec49e8526c1bb257e1f5cf0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yr6V96fEzRzdbQoE4wU2Ug.jpeg"/></div></div></figure><p id="4761" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇文章中，我将用一个关于酒精和幸福之间关系的例子来解释机器学习中的一个基本算法。</p><p id="7560" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们开始吧。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h2 id="40d8" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">什么是线性回归？</h2><p id="4cd7" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">线性回归是根据过去的数据推测未来输出的数学技术。</p><p id="e180" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，假设你正在观看你最喜欢的球员在今天的比赛中踢足球，他在与对手的比赛中有着非常好的记录，平均每场比赛有2个进球，根据你头脑中的这个简单计算，你可能希望他至少得分2分或更多，所以你的大脑所做的是计算简单的平均值。</p><p id="83f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="kv">平均值=与对手球队的总得分/与对手的比赛次数</em> </strong></p><p id="dc6d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归也与此类似，但我们不是取平均值，而是使用输入变量(x)和目标变量(y)之间的线性关系进行更好的统计猜测。</p><p id="3524" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注:线性回归仅适用于连续变量，如降雨量与湿度、心率与跑步速度等。</p><p id="1e68" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们通过在流行的数值计算python包NumPy中实现它来看看它是如何工作的。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h2 id="c154" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">使用NumPy的线性回归</h2><p id="2906" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated"><strong class="is hj">第一步</strong>:导入所有计算需要用到的包。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="4921" class="jv jw hi lb b fi lf lg l lh li">import pandas as pd<br/>import numpy as np</span></pre><p id="c5f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第二步</strong>:使用pandas库读取输入文件。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="6351" class="jv jw hi lb b fi lf lg l lh li">data = pd.read_csv('/Users/arunramji/Downloads/Sourcefiles/Alchol_vs_Happiness.csv',',',<br/>                  usecols=['Country','Alcohol','Happiness Score'])</span></pre><p id="3a39" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们看一下导入的数据是什么样子的，</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="8f4f" class="jv jw hi lb b fi lf lg l lh li">data.head()</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/e6299357075d78909c4f89da29c315fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*nNg-9bZbaLQwOqv0YwDGCw.png"/></div></figure><p id="64b6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">步骤3 </strong>:只过滤需要的变量</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="e821" class="jv jw hi lb b fi lf lg l lh li">A = data[['Alcohol','Happiness Score']]<br/>A.tail()</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/6d85805782b47645836b335c60187bf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*uTICGYckCYp6xJvwTlhBRA.png"/></div></figure><p id="6891" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">步骤4 </strong>:将熊猫数据帧转换成numpy数组。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="65ce" class="jv jw hi lb b fi lf lg l lh li">matrix = np.array(A.values,'float')<br/>matrix[0:5,:]    #first 5 rows of data</span><span id="642b" class="jv jw hi lb b fi lk lg l lh li">array([[194.33333333,   7.526     ],<br/>       [188.33333333,   7.509     ],<br/>       [124.        ,   7.501     ],<br/>       [123.        ,   7.498     ],<br/>       [164.33333333,   7.413     ]])</span></pre><p id="a220" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">步骤5 </strong>:让我们为进一步的计算分配输入和目标变量，x和y。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="1016" class="jv jw hi lb b fi lf lg l lh li">#Assign input and target variable</span><span id="b1ea" class="jv jw hi lb b fi lk lg l lh li">X = matrix[:,0]<br/>y = matrix[:,1]</span></pre><p id="7f9d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">步骤6 </strong> : <em class="kv">特征</em> <em class="kv">归一化</em>-这是许多ML模型的重要步骤之一，我们实际上做的是将所有的输入变量压缩到更小且相似的量级，以便后面的计算更快更有效。下面我们有一个特征标准化技术，使输入变量x的大小相似。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="44c8" class="jv jw hi lb b fi lf lg l lh li">#feature normalization</span><span id="f522" class="jv jw hi lb b fi lk lg l lh li"># input variable divided by maximum value among input values in X</span><span id="a70b" class="jv jw hi lb b fi lk lg l lh li">X = X/(np.max(X)) <br/></span></pre><p id="0e37" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第七步</strong>:由于它是一个输入变量和一个输出变量，我们可以绘制2d图，看看它是如何分布的。这将有助于我们更好地理解数据和问题。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="a121" class="jv jw hi lb b fi lf lg l lh li">import matplotlib.pyplot as plt</span><span id="1a9c" class="jv jw hi lb b fi lk lg l lh li">plt.plot(X,y,'bo')<br/>plt.ylabel('Happiness Score')<br/>plt.xlabel('Alcohol consumption')<br/>plt.legend(['Happiness Score'])<br/>plt.title('Alcohol_Vs_Happiness')<br/>plt.grid()<br/>plt.show()</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/2ad67c22e25e5d5136ebcf548c6959ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*Xs5mkYcYnuh_pr-M_XMx7Q.png"/></div></figure><p id="7023" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在很明显，饮酒量和幸福指数之间存在某种关联，这意味着我们可以看到，饮酒量越大的国家越幸福！！</p><p id="bffb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们开始计算假设。</p><blockquote class="ll lm ln"><p id="45f6" class="iq ir kv is b it iu iv iw ix iy iz ja lo jc jd je lp jg jh ji lq jk jl jm jn hb bi translated">假设是用于定义给定训练样本的近似目标值(y)的术语，它将由我们的ML模型计算。</p></blockquote></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h2 id="739b" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">假设</h2><p id="65ae" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">我们需要通过评估X和y之间的线性关系来计算假设，这里是酒精消耗量对幸福指数。</p><p id="eff4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么，对于给定的输入(x ),我们如何计算假设或近似输出值(y )?</p><blockquote class="lr"><p id="00ce" class="ls lt hi bd lu lv lw lx ly lz ma jn dx translated">一个想法是，如果我们在数据上绘制一条简单的线，它与实际值的偏差或误差较小，那么它可以用来预测未来值，误差非常小。</p></blockquote><p id="bb02" class="pw-post-body-paragraph iq ir hi is b it mb iv iw ix mc iz ja jb md jd je jf me jh ji jj mf jl jm jn hb bi translated">所以我们的目标是找到最佳线，下面是我们需要计算的线方程。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/8bfd69710ac734aadec8728e24e73326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5mlab7EPIjL9nwJ077Pzcg.jpeg"/></div></div></figure><p id="b162" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，我们从样本数据中知道x，y的值，使用该值，我们必须计算最佳θ0和θ1，这具有最小的误差成本来绘制线性拟合。</p><blockquote class="ll lm ln"><p id="3b66" class="iq ir kv is b it iu iv iw ix iy iz ja lo jc jd je lp jg jh ji lq jk jl jm jn hb bi translated">成本或SSE(误差平方和)是我们的假设和实际数据点之间的差异</p></blockquote><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/55cd498d50d29451e473bbdf96c50f21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7X_jFIxR0lV_ywWWhzkzw.jpeg"/></div></div></figure><p id="749a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第8步</strong>:让我们定义计算成本或SSE的函数。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="5860" class="jv jw hi lb b fi lf lg l lh li">def computecost(x,y,theta):<br/>    <br/>    a = 1/(2*m)<br/>    b = np.sum(((x@theta)-y)**2)<br/>    j = (a)*(b)<br/>    return j</span></pre><p id="acd5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第9步</strong>:在我们现有的矩阵X中附加一个x0项，为了数学上的方便，x0的值应该是‘1’。</p><p id="9ca6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">并将一些初始θ指定为0。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="391b" class="jv jw hi lb b fi lf lg l lh li">#initialising parameter</span><span id="2463" class="jv jw hi lb b fi lk lg l lh li">m = np.size(y)<br/>X = X.reshape([122,1])<br/>x = np.hstack([np.ones_like(X),X])</span><span id="3c75" class="jv jw hi lb b fi lk lg l lh li">theta = np.zeros([2,1])</span><span id="0ac2" class="jv jw hi lb b fi lk lg l lh li">print(theta,'\n',m)</span><span id="e716" class="jv jw hi lb b fi lk lg l lh li">[[0.]<br/> [0.]] <br/> 122</span></pre><p id="0fae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们计算一下如果θ为0，成本是多少，</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="b36c" class="jv jw hi lb b fi lf lg l lh li">print(computecost(x,y,theta))</span><span id="df41" class="jv jw hi lb b fi lk lg l lh li">1941.7825705000002</span></pre><p id="30a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的目标是进一步降低这个成本J(theta)值，以便我们可以实现数据的最佳线性拟合。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h2 id="f8b4" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">梯度下降</h2><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/cf07e3ef802db937f7865e9f5862ea3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*15DDrvDIlV0otIZ5J375cw.gif"/></div></figure><p id="eeb0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">梯度下降是一种这样的算法，用于使用给定的参数找到最佳参数“θ”,</p><p id="a51b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">x —输入值</p><p id="59c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">y-输出值</p><p id="b3ec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Initial_theta —大多数情况下为空theta</p><p id="f69d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">alpha —渐变指针下降到最佳值的速率</p><p id="e936" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">迭代——设置应该进行多少次迭代</p><p id="8f14" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">理解“Gradinet Desecnd”可能需要一点微积分，但没有必要实现和使用它来解决ML问题。了解上述参数的作用对于实施来说通常就足够了。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/a119fe52a65f06c421313c6b631fa944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oP5t82nztvWP14p4nplWYQ.jpeg"/></div></div></figure><p id="7f46" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第十步</strong>:定义梯度下降算法的函数。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="020f" class="jv jw hi lb b fi lf lg l lh li">def gradient(x,y,theta):<br/>    <br/>    alpha = 0.00001<br/>    iteration = 2000</span><span id="7d19" class="jv jw hi lb b fi lk lg l lh li">#gradient descend algorithm<br/>    J_history = np.zeros([iteration, 1]);</span><span id="2a5e" class="jv jw hi lb b fi lk lg l lh li">for iter in range(0,2000):<br/>        <br/>        error = (x @ theta) -y<br/>        temp0 = theta[0] - ((alpha/m) * np.sum(error*x[:,0]))<br/>        temp1 = theta[1] - ((alpha/m) * np.sum(error*x[:,1]))<br/>        theta = np.array([temp0,temp1]).reshape(2,1)<br/>        J_history[iter] = (1 / (2*m) ) * (np.sum(((x @ theta)-y)**2))   #compute J value for each iteration <br/>    return theta, J_history</span></pre><p id="0c67" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们对数据使用梯度函数，</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="e973" class="jv jw hi lb b fi lf lg l lh li">theta , J = gradient(x,y,theta)<br/>print(theta)</span><span id="8070" class="jv jw hi lb b fi lk lg l lh li">[[4.22499706]<br/> [2.38031097]]</span></pre><p id="9615" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们已经得到了用梯度下降法计算的最优θ，但是我们怎么能确定这就是最优θ呢，使用computecost函数我们可以看到它。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="63cb" class="jv jw hi lb b fi lf lg l lh li">theta , J = gradient(x,y,theta)<br/>print(J)<br/></span><span id="0d6d" class="jv jw hi lb b fi lk lg l lh li">array([[1936.24274283],<br/>       [1930.71941566],<br/>       [1925.21254022],<br/>       ...,<br/>       [ 115.49668262],<br/>       [ 115.49460323],<br/>       <strong class="lb hj">[ 115.49255932]</strong>])</span></pre><p id="4d69" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">成本或SSE值为115.42，比θ= 0时计算的1941.78好得多</p><p id="9e41" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第11步:现在让我们在数据上画线，看看它与数据的吻合程度。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="9faf" class="jv jw hi lb b fi lf lg l lh li">#plot linear fit for our theta<br/>plt.plot(X,y,'bo')<br/>plt.plot(X,x@theta,'-')<br/>plt.axis([0,1,3,7])<br/>plt.ylabel('Happiness Score')<br/>plt.xlabel('Alcohol consumption')<br/>plt.legend(['HAPPY','LinearFit'])<br/>plt.title('Alcohol_Vs_Happiness')<br/>plt.grid()<br/>plt.show()</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/add828abaac3748d162072d6b90143b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*3C6aaC7oUfAQ9mKL7-ENGw.png"/></div></figure><p id="b265" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于给定的数据样本，这似乎是合理的，让我们使用这个线性拟合来计算新的未知输入值x。</p><p id="eef2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">步骤12 </strong>:让我们预测新的输入值😃</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="5582" class="jv jw hi lb b fi lf lg l lh li">predict1 = [1,(164/np.max(matrix[:,0]))] @ theta #normalising the input value, 1 is for intercept term so not need to normalise</span><span id="c8a8" class="jv jw hi lb b fi lk lg l lh li">print(predict1)</span><span id="2dd6" class="jv jw hi lb b fi lk lg l lh li">[<strong class="lb hj">5.98606924</strong>]</span></pre></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h2 id="bfff" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated"><strong class="ak">奖励说明</strong>:</h2><p id="88e5" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">还有一些其他方法可以确定梯度下降是否有效，其中一种是绘制每次迭代的J(theta ),并查看值如何变化，如果J值在每次迭代中减少，这很好，但如果它增加，那么我们的算法或数据一定有问题。</p><p id="c664" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看看它是如何工作的。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="2d15" class="jv jw hi lb b fi lf lg l lh li">#visualising J (theta0 , theta1)</span><span id="0939" class="jv jw hi lb b fi lk lg l lh li">theta0_vals = np.linspace(-5,10,100).reshape(1,100)<br/>theta1_vals = np.linspace(-5,10,100).reshape(1,100)</span><span id="f26e" class="jv jw hi lb b fi lk lg l lh li">#initialise J value to matrix of 0<br/>J_vals = np.zeros([np.size(theta0_vals),np.size(theta1_vals)])</span><span id="c05c" class="jv jw hi lb b fi lk lg l lh li">#fill J_vals<br/>for i in range(0,np.size(theta0_vals)):<br/>    for j in range(0,np.size(theta1_vals)):<br/>        t = np.array([theta0_vals[:,i],theta1_vals[:,j]])<br/>        J_vals[i,j] = computecost(x,y,t)</span><span id="cb28" class="jv jw hi lb b fi lk lg l lh li"># Because of the way meshgrids work in the surf command, we need to<br/># transpose J_vals before calling surf, or else the axes will be flipped<br/>J_vals = J_vals.T</span></pre><p id="23d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们来出图。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="0d98" class="jv jw hi lb b fi lf lg l lh li">#surface plot for covergence<br/>from mpl_toolkits.mplot3d import Axes3D <br/>fig = plt.figure(figsize=[12.0,8.0])<br/>ax = fig.add_subplot(111,projection ='3d')<br/>ax.plot_surface(theta0_vals,theta1_vals,J_vals)<br/>ax.set_xlabel('theta0')<br/>ax.set_ylabel('theta1')<br/>ax.set_zlabel('J_vals')</span><span id="54cb" class="jv jw hi lb b fi lk lg l lh li">plt.show()</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/2b1c87a2fe578d00b882dd6bbc2dfc40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QezLtM7JIi_Vd6mF9CTsrQ.png"/></div></div></figure><p id="caa2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上图中，我们可以看到曲面在θ0→4和θ1→2附近收敛，因此我们可以说计算的参数是合理的。</p><h2 id="20bd" class="jv jw hi bd jx jy jz ka kb kc kd ke kf jb kg kh ki jf kj kk kl jj km kn ko kp bi translated">结论</h2><p id="2bf8" class="pw-post-body-paragraph iq ir hi is b it kq iv iw ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn hb bi translated">所解释的线性回归技术是预测连续变量的常用建模技术，那么它对各种数据都有效吗？这一点我们无法确定，但只要我们了解数据和问题，线性回归肯定会为未知和新的输入值提供良好的统计猜测。</p><blockquote class="lr"><p id="657a" class="ls lt hi bd lu lv lw lx ly lz ma jn dx translated">为了更好的未来继续努力！！</p></blockquote></div></div>    
</body>
</html>