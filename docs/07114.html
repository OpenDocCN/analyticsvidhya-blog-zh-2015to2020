<html>
<head>
<title>Revealing BART : A denoising objective for pretraining</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭示BART:预处理的去噪目标</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/revealing-bart-a-denoising-objective-for-pretraining-c6e8f8009564?source=collection_archive---------7-----------------------#2020-06-14">https://medium.com/analytics-vidhya/revealing-bart-a-denoising-objective-for-pretraining-c6e8f8009564?source=collection_archive---------7-----------------------#2020-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2f17d6606a697aa0225826a716e39b0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lEERaIW1uWxyRUDy"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">乔治·特罗瓦托在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><blockquote class="jc jd je"><p id="e194" class="jf jg jh ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="ji hj"/>论文真实作者:、、刘、纳曼·戈亚尔、马尔扬·加兹维尼贾德、阿卜杜勒拉赫曼·穆罕默德、奥迈尔·利维、维斯·斯托扬诺夫、卢克·泽特勒莫耶</p></blockquote></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="4c87" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated"><strong class="ji hj">简介:</strong></p><p id="a1b6" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">本文讨论了自然语言生成中序列对序列模型的预处理去噪方法。我试图以一种清晰的方式解释我的研究中的一切，希望每个读者都能理解这篇文章，并从中受益。巴特(虽然听起来像你已经知道的伯特，但不要在这里停下来，因为这是写下这篇文章的动机，因为它将帮助你理解巴特的确切文献)。</p><p id="83e8" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">目前，用于预训练的自监督学习已经在各种下游自然语言处理任务中取得了巨大的成功，例如，Word2Vec、ELMO、BERT、spanBert、XLNet等。都是基于自我监督的学习。但在所有这些中，自我监督的掩蔽语言模型以其卓有成效的SOTA性能真正撼动了NLP的领域。同样，巴特是另一个成功。</p><p id="ac28" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">问题来了“巴特是什么？”它是一种自监督自动编码器，首先使用添加了噪声的源文本(通过破坏源文本中的一些标记或使用本文稍后讨论的任何合适的噪声方案)作为输入，然后使用LM(语言模型)通过预测被破坏标记的真实替换来重建原始文本。当用于自然语言生成(NLG)任务时，该模型在性能方面是最好的，但对于理解任务也是值得称赞的。</p><p id="ddba" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated"><strong class="ji hj">架构:</strong></p><figure class="ki kj kk kl fd ij er es paragraph-image"><div class="er es kh"><img src="../Images/f093e80f95d5ea72d3dd021512c581d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*tqZRjw9a33eEXSP-SGO-lQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">本文作者创建的图像</figcaption></figure><p id="14ed" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">正如作者在论文中所建议的，这是一种基于转换器的Seq2Seq模型，它使用损坏的源文本，然后通过从解码器重新生成原始文本来尝试对源文本进行去噪，并且解码器的每一层都关注编码器的最终隐藏层。它可以被看作是一个Seq2Seq模型，修改后作为一个自动编码器。该架构的一个显著特点是使用GELU代替RELU激活层。与BERT相比，它不像BERT那样在顶部使用前馈网络进行单词预测。此外，与同等的基于BERT的架构相比，BART仅多使用10%的参数，并在语言生成任务方面实现了更好的性能。架构中涉及的参数初始化为正态分布~ N(0.00，0.02)。作者谈到了根据用户要求提供两种不同的预训练模型:</p><ol class=""><li id="cdac" class="km kn hi ji b jj jk jn jo ke ko kf kp kg kq kd kr ks kt ku bi translated">基础案例模型(6层架构)</li><li id="b190" class="km kn hi ji b jj kv jn kw ke kx kf ky kg kz kd kr ks kt ku bi translated">大型模型(12层架构)</li></ol><p id="1922" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">为了准备用于预训练的模型，首先，来自输入/源文本的一些标记被随机破坏(添加噪声方案),并且在训练时，使用输出和解码器输出之间的交叉熵损失来优化再生损失。与现有的去噪自动编码器不同，BART允许我们应用任何类型的文档损坏。在极端情况下，关于源的所有信息都丢失了，BART相当于一个语言模型。让我们也来看看可以在源文本上的BART中使用的噪声方案/变换:</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div class="er es la"><img src="../Images/014bcc8a1aa640b530309136874f132a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*Gv8H6ns4RXo5OpnaPUrfAQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">BART paper(<a class="ae iu" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1910.13461.pdf</a>)的真实作者(开头提到过)的图片</figcaption></figure><p id="d2ea" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">1.令牌屏蔽:随机令牌被采样并用[MASK]令牌屏蔽。</p><p id="ffa3" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">2.令牌删除:随机令牌被采样和删除(类似于屏蔽)，模型在它们的位置添加新的令牌。</p><p id="fdaf" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">3.标记填充:从泊松分布中抽取多个文本区间(一组连续的标记)，并且每个区间由屏蔽的标记[MASK]代替。</p><p id="ddc5" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">4.句子排列:文档句子的随机排列。</p><p id="5747" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">5.文档旋转:随机统一选择一个标记，文档围绕该标记旋转，以便文档从该标记开始。</p><p id="1f08" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated"><strong class="ji hj">为什么巴特如此富于表现力而又高高在上？</strong></p><p id="27fc" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">让我们通过假设一种情况来理解这一点，在这种情况下，我们只有BERT作为我们感兴趣用于NLG任务的语言模型。你能看出任何限制吗？好吧，让我给你一个提示:“BERT使用源文本，其中一些标记被<em class="jh">屏蔽</em>，它试图预测实际上可以替换那些<em class="jh">屏蔽</em>标记的单词。”如果我的暗示听起来对你没有帮助，这并不意味着你不擅长做事情，因为我的暗示可能很糟糕😔。</p><p id="7abf" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">好吧，让我解释一下，如果你已经知道屏蔽令牌是由BERT独立预测的，并且声称对于基于NLG的任务来说是一个“不太好”的体系结构，尽管BERT本质上是双向的，那么我在上面的提示的最后两行中给单词“屏蔽”加下划线的原因就很容易理解了。但是像GPT这样的架构(读取从左到右的上下文)由于其自回归性质而在NLG任务中是健壮的，然而，它仅包括从左到右的上下文读取，并且不是深度双向的。</p><p id="cdc8" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">因此，为了实现这种任务的良好性能，建议通过结合用于编码被破坏的源文本的BERT和用于通过预测被屏蔽的标记来生成原始文本的GPT来获得两个世界的最佳效果。这激发了对基于编码器-解码器(seq2seq)的架构BART的需求，以提高下游NLG和理解任务的性能。</p><p id="38c4" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated"><strong class="ji hj">应用:</strong></p><p id="b363" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">BART可以进行微调，以便在各种下游任务中表现出色:</p><ol class=""><li id="d408" class="km kn hi ji b jj jk jn jo ke ko kf kp kg kq kd kr ks kt ku bi translated">序列分类:使用预训练的BART，并且解码器输出的最终表示(解码器的顶部隐藏状态)被用作序列的有意义的输入表示，并且被用于新的多类分类器中。</li><li id="620e" class="km kn hi ji b jj kv jn kw ke kx kf ky kg kz kd kr ks kt ku bi translated">令牌分类:使用预训练的BART，解码器输出的最终表示(解码器的顶部隐藏状态)提供每个单词的有意义的表示，并用于令牌的分类。</li><li id="4499" class="km kn hi ji b jj kv jn kw ke kx kf ky kg kz kd kr ks kt ku bi translated">序列生成:它类似于去噪预训练目标，因为解码器输出具有从原始输入序列复制的信息的序列。它可以用于摘要和问答任务中的序列生成。</li><li id="a784" class="km kn hi ji b jj kv jn kw ke kx kf ky kg kz kd kr ks kt ku bi translated">机器翻译:整个预训练的编码器-解码器被假定为将生成目标序列的解码器，并且引入一个新的编码器，该编码器将源序列作为输入。在训练时，预训练结构的参数被冻结，在编码器试图学习源和目标序列之间的比对的第一步中，仅学习新编码器的参数。第二步，通过较少的迭代学习整个架构。</li></ol><p id="5cce" class="pw-post-body-paragraph jf jg hi ji b jj jk jl jm jn jo jp jq ke js jt ju kf jw jx jy kg ka kb kc kd hb bi translated">我希望我的文章对所有的读者都是有用的、有成效的和有益的。我还要感谢所有好奇的读者，他们花时间阅读了我的第一篇文章。我也对那些认为事情可以用更清晰的方式解释的人感到抱歉，并要求他们一定要给予必要的反馈，我下次一定会考虑你的建议。此外，我完全接受和回答您的问题，疑虑和与本文相关的反馈。结束，保持健康，保持动力！</p></div></div>    
</body>
</html>