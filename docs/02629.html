<html>
<head>
<title>Implementing Petabyte Scale Cloud Data Migration</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实施Pb级云数据迁移</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/petabyte-scale-cloud-data-migration-e37594958f3b?source=collection_archive---------6-----------------------#2019-12-26">https://medium.com/analytics-vidhya/petabyte-scale-cloud-data-migration-e37594958f3b?source=collection_archive---------6-----------------------#2019-12-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/61398aac48cfec319e0abe4f0f5acb24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmsoC2dHG_k2lqj0TKWkDg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">云数据迁移</figcaption></figure><p id="7aff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一家非常大的第一食品零售连锁店正在将其数据平台迁移到AWS云。目前，他们的内部数据中心有IBM DataStage作为结构化ETL工具，Teradata和Microsoft SQL Server作为OLAP数据库，Tableau作为报告层，Hortonworks Hadoop发行版。在新平台中，他们打算使用S3、红移作为OLAP数据库、Talend用于数据集成以及rewire tableau到红移以服务于报告层来构建数据湖。委托一个10节点r 4.2x大型长期运行EMR集群处理半结构化数据，如用于点击流、厨房视频系统等的XML和JSON。数据处理—这是对内部HortonWorks Hadoop集群的替代。</p><p id="be55" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这种迁移的本质是“提升和转移”。我不会在这里讨论战略/管理考虑，但会讨论实现部分。要完成这一迁移，需要实施四项主要任务</p><ul class=""><li id="4e3d" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">在Talend中重写所有必要的DataStage数据集成作业。</li><li id="8281" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">将现有数据从内部迁移到云—迁移到S3，然后拷贝到Redshift。</li><li id="6b54" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">设置迁移ETL管道以在prem和云平台上运行，并在关闭prem管道之前验证一段时间的数据。</li><li id="97d3" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">根据需要将报告层重新连接到新的数据源。</li></ul><p id="e24f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">虽然必须有广泛的迁移计划，但我将特别讨论上述列表中的第二项任务，以及我们面临的挑战和我们如何创建一个漂亮的通用解决方案，该解决方案节省了我们大量的时间/精力，并有助于顺利完成任务。</p><h1 id="7182" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">问题:</h1><p id="4593" class="pw-post-body-paragraph iu iv hi iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr hb bi translated">Pb级的数据被锁在Teradata &amp; SQL Server中，必须按分区提取、压缩并复制到S3。下一步，来自S3的数据将被加载到Redshift中——整个过程中应该有验证步骤，以确保数据在整个过程中正确传输。最初，团队开始一次做一个表，并使用互联网通过VPC私有/公共子网复制数据，使用特定的元数据、分区键、验证逻辑等为每个单独的表创建DataStage ETL作业。这需要太多的时间来为每个表创建管道，缓慢的数据传输和从头到尾照看整个过程。我们没有在市场上找到一个好的通用解决方案来满足我们的特定需求，所以我们需要一个定制的高度可配置的解决方案来无缝地完成这项任务，它可以使用配置的值自行运行，进行验证，并在任务完成后发送自动报告。我们将我们的需求分类如下:</p><ul class=""><li id="1a97" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">需要创建一个高度可配置的通用流程来完成端到端任务。</li><li id="4ce4" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">需要使用批量数据传输方法，像AWS滚雪球一样随着使用公共互联网。</li></ul><h1 id="ceca" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">解决方案:</h1><p id="f96c" class="pw-post-body-paragraph iu iv hi iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr hb bi translated">我们主要使用unix和python开发了一个软件。我将在下面讨论如何使用它来完成每个单独的任务:</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/3533f8ef034e616c09581754c047532a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SZwMYTMlt_Pl52A26A4nQA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">数据迁移—任务流</figcaption></figure><h1 id="8ade" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">数据提取到文件:</h1><p id="9d32" class="pw-post-body-paragraph iu iv hi iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr hb bi translated">主要的数据库供应商提供了向/从他们的数据库批量导入/导出数据的工具。Teradata和SQL Server也有批量导出数据的实用程序——分别是FastExport和bcp。我们的大部分数据存储在Teradata中，一小部分存储在SQL Server中，因此选择Teradata来解释该过程，但是可以根据需要使用“bcp”配置或任何其他数据库批量导出实用程序来运行相同的过程。</p><p id="61f7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Fastexport从Teradata表中导出64K块的数据，这对于提取大量数据非常有用。该实用程序需要提供“varchar”格式的特定列名，并且不能与“select *”一起使用。这不是问题，因为可以从系统表中获取列名，并且在配置文件中可以省略或添加派生列或添加过滤器。另一个问题是导出数据中的默认分隔符是' \t '，这可能不是我们一直想要的，因为数据本身可能包含' \t '，这使得很难区分数据和分隔符。幸运的是，Teradata支持用户编写的过时例程，这些例程可用于在写入文件之前对数据进行预处理，因此这可用于根据需要替换分隔符。我找到了一个用C写的例程，编译成dlmt_vchar.so哪个做的工作。</p><p id="0c72" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这个项目的源代码可以在下面的github链接中找到:</p><p id="d696" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae lo" href="https://github.com/sandipayan/Onprem2Aws" rel="noopener ugc nofollow" target="_blank">https://github.com/sandipayan/Onprem2Aws</a></p><p id="84fe" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该流程的主要组成部分包括:</p><p id="181c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> snowball.ini: </strong>这是主配置文件，包含源数据库、不同路径、分隔符、zip选项、列排除、自定义sql、复制到snowball或通过互联网(对于小表，因为snowball是一个物理设备，需要手动运送，稍后将详细介绍)等的凭证。</p><p id="a028" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> input_list: </strong>每行表格列表，用逗号分隔过滤条件(如果有)。</p><p id="031c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> snowball_main.ksh: </strong>这是运行流程时要调用的主脚本。这会读取“input_list”文件中的表名(以及其他给定的附加信息),并遍历列表。开始时，它创建一个主日志文件，并写入每个表的摘要信息，如开始时间、结束时间、花费的时间、原始文件大小、压缩大小、文件路径等。在每次迭代中，它从dbc.columns中找到表的元数据，使用Varchar中的列形成select查询，添加过滤器信息(如果有的话)或接受要提取数据的定制sql查询。</p><p id="051e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> snowball_cs_trfm.ksh: </strong>该脚本帮助格式化从dbc.columns系统表中检索的元数据，并创建一个中间的{variable}.cs.dat表，该表在主脚本中使用。</p><p id="7bc1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> snowball_cs.fe: </strong>它从snowball.ini中获取上面创建的sql文件和输入，并创建一个中间文件snowball _ cs _ seded { FE _ LOG _ SUFFIX }。带有所有参数替换的fe，这些参数将在下一步中用于从表中运行Fastexport提取过程。</p><p id="36d3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> snowball_main.fe: </strong>这是最终的Fastexport脚本，它使用前面过程的输出并实际生成数据提取文件。</p><p id="5a87" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> dlmt_vchar.so: </strong>这是一个共享对象(编译的C库)，在上面的脚本中使用它来帮助预处理提取的数据，比如改变分隔符。这与环境变量FEXP_DELIMITER一起使用，该变量在。ini”文件。</p><p id="d6aa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">基于定义的配置值，该脚本压缩提取的数据文件，并通过公共互联网或写入雪球设备移动到派生的s3路径。</p><h2 id="1000" class="lp kh hi bd ki lq lr ls km lt lu lv kq jf lw lx ku jj ly lz ky jn ma mb lc mc bi translated">挑战:</h2><p id="8272" class="pw-post-body-paragraph iu iv hi iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr hb bi translated">虽然这一过程对于95%以上的数据来说工作顺利，但是有一小部分数据主要在SQL server中，这些数据具有像“comment”或其他自由格式文本列这样的列，很难使用可靠的分隔符。对于这些情况，我们需要以模式和数据一起提供的格式导出/导入数据。在这些情况下，我们使用Sqoop进行数据提取，并使用Avro格式。其余的过程照常进行。</p><h1 id="b53a" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">雪球设置:</h1><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/8ba44133c06162b3e58a9f64d2d59438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KUpNWCBxMFYjGFwqeUeA8A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">雪球装置</figcaption></figure><p id="081f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用公共互联网传输大量数据可能需要数天/数周时间，而且非常不可靠。对于小桌子，我们使用公共互联网传输数据，对于大容量，我们根据需要订购雪球设备。订购雪球设备时，需要提供磁盘空间(如:50/80 GB)和s3存储桶名称。该设备本身是一个坚固的盒子，有自己的运输标签。一旦收到设备发货，它需要在数据中心连接，然后需要配置<strong class="iw hj">雪球客户端</strong>并用于将数据写入设备。由于现在连接是在本地数据中心进行的，因此写入速度非常快。一旦写入完成，设备就可以分离并运回AWS，它们会将数据移动到写入雪球设备的同一s3存储桶和密钥中。这是一个简单的过程，在“AWS雪球开发者指南”中有很好的记录。</p><h1 id="7fd7" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">加载到红移:</h1><p id="d5d8" class="pw-post-body-paragraph iu iv hi iw b ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn li jp jq jr hb bi translated">现在数据在S3，这需要加载到相应的红移表。需要有一定的考虑因素才能装载到S3:</p><ul class=""><li id="bdec" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">在Redshift中需要一个相应的表(DDL ),数据将被复制到这个表中。</li><li id="5cab" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">当加载到红移时，需要提到各个列，因此列被显式映射以避免无意的错误数据加载/错误。</li><li id="f3f7" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">需要有能力删除和重新加载数据。</li><li id="eefc" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">记录活动，包括成功的记录计数加载。</li></ul><p id="46dd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">红移表的创建混合了自动脚本和手动编辑，以根据需要包含一些额外的列和默认值。以下是此过程中使用的脚本:</p><p id="102e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> input_list: </strong>要加载的表格列表。</p><p id="aaa7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">Redshift . conf:</strong>JSON格式的红移凭证信息。</p><p id="edbd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">s3 . conf:</strong>Key&amp;JSON格式的秘密访问密钥，从这里S3数据被读取。</p><p id="afbc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> getcols.sh: </strong>从提取的数据中读取标题信息，并准备要在复制命令中传递的列列表。</p><p id="c550" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> Redshift_Loader.py: </strong>这是使用‘psycopg 2’库的主脚本，使用上述文件将数据迭代加载到input_list中的相应表中。这还会生成用于最终验证的日志。</p><p id="0943" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有时我们需要使用不同的S3路径来加载特定的红移表，在这种情况下，创建了一个<strong class="iw hj">清单</strong>文件来提及所有S3路径(支持通配符)，用于红移表加载。要加载到Redshift中的数据应该分布在许多文件中，而不是只有一个巨大的文件。这将有助于红移从许多不同的文件同时加载数据，并大大加快加载过程。为了充分利用最大程度的并行性，文件数应该是Redshift中可用片数的倍数。我们不希望文件太小，因为这会增加开销，也不希望文件太大，因为会增加处理时间。文件大小大约应该接近128 MB，并且数据尽可能均匀地分布在文件中。</p><p id="6f84" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="me">自动状态报告&amp;验证:</em> </strong></p><p id="5a6e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有两个主要步骤，我们希望状态报告生成，保存和电子邮件。两个步骤，因为数据提取和加载发生在两个不同的时间(或天)。</p><p id="0325" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于第一步，报告的一些关键指标有:表名、分区键、提取开始ts、提取结束ts、提取文件大小、提取行数、S3文件路径、S3的文件大小等。此阶段的示例报告如下所示:</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="mf mg l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">提取报告样本</figcaption></figure><p id="23fc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于第二步，同时加载相同的数据到S3，我们需要类似的记录计数匹配等基本验证日志。要跟踪的一些日志包括:表名、S3/清单路径、开始和结束时间、持续时间、插入的记录数、开始记录数、结束记录数等。此阶段的示例报告如下所示。</p><figure class="lk ll lm ln fd ij"><div class="bz dy l di"><div class="mf mg l"/></div></figure><p id="75bc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">可以根据需要实现基于表的更具体的标准，例如，为两个表生成和匹配“R”样式的表摘要。但是我们发现，在时间推移之前，最好对单个表/表集进行手动验证，也就是说，当我们正式完成历史数据迁移，并开启云数据管道和本地数据管道的同步运行时。在prem流程结束之前，需要对这两个流程进行一段时间的监控。</p><p id="9116" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">您是如何将数据平台迁移到云的，是使用专有工具还是像我们一样开发内部解决方案？很想听听你的经历。</p></div></div>    
</body>
</html>