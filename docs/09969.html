<html>
<head>
<title>Speech Emotion Recognition using Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于机器学习的语音情感识别</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/speech-emotion-recognition-using-machine-learning-df31f6fa8404?source=collection_archive---------9-----------------------#2020-09-28">https://medium.com/analytics-vidhya/speech-emotion-recognition-using-machine-learning-df31f6fa8404?source=collection_archive---------9-----------------------#2020-09-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/df7dfae1624d570585906fe7d5c9243a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZ4OUd5qJeodR-Z2Dh0uAg.jpeg"/></div></div></figure><blockquote class="iq ir is"><p id="c33d" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">作为人类，语言是表达自己最自然的方式之一。我们如此依赖表情符号，以至于在求助于电子邮件和短信等其他交流形式时，我们认识到了它的重要性，我们经常使用表情符号来表达与信息相关的情绪。由于情感在交流中起着至关重要的作用，因此对情感的检测和分析在当今远程交流的数字世界中至关重要。情绪检测是一项具有挑战性的任务，因为情绪是主观的。对于如何对它们进行衡量或分类，还没有达成共识。</p></blockquote><p id="7b78" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">语音情感识别(Speech Emotion Recognition)，缩写为<strong class="iw hj"> SER </strong>，是试图从语音中识别人类情感和情感状态的行为。这种系统可以在各种各样的应用领域中找到用途，例如基于交互式语音的助理或呼叫者代理对话分析。</p><h1 id="6d74" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">SER -目标</h1><p id="8092" class="pw-post-body-paragraph it iu hi iw b ix kt iz ja jb ku jd je js kv jh ji jt kw jl jm ju kx jp jq jr hb bi translated">使用python的<strong class="iw hj"> librosa </strong>库构建一个从语音中识别情绪的模型。</p><h1 id="9b9d" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">数据源</h1><p id="b37b" class="pw-post-body-paragraph it iu hi iw b ix kt iz ja jb ku jd je js kv jh ji jt kw jl jm ju kx jp jq jr hb bi translated">对于这个项目，我们将使用<strong class="iw hj"> RAVDESS </strong>数据集，它是瑞尔森情感语音和歌曲数据集的视听数据库的缩写形式。这个数据集有<strong class="iw hj"> 7356 </strong>个文件，由<strong class="iw hj"> 247 </strong>个人对情感有效性、强度和真实性进行了10次评级。</p><h1 id="c307" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">使用的库</h1><p id="e158" class="pw-post-body-paragraph it iu hi iw b ix kt iz ja jb ku jd je js kv jh ji jt kw jl jm ju kx jp jq jr hb bi translated">我们将使用的主要库是<strong class="iw hj"> Librosa。</strong>除此之外，我们还将使用<strong class="iw hj">声音文件</strong>和<strong class="iw hj"> Pyaudio。Librosa </strong>是一个用于分析音频和音乐的Python库。它具有更扁平的封装布局、标准化的接口和名称、向后兼容性、模块化功能和可读代码。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/5b3dda9197e9da26aa18b17ddb46432d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*IGdKOxaj4aTuS5yidPt6eg.png"/></div></figure><h1 id="df29" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">本研究中使用的特征</strong></h1><p id="dc0b" class="pw-post-body-paragraph it iu hi iw b ix kt iz ja jb ku jd je js kv jh ji jt kw jl jm ju kx jp jq jr hb bi translated">从音频数据中我们提取了<strong class="iw hj">三个</strong>关键特征用于本研究，即<strong class="iw hj"> MFCC </strong>(梅尔频率倒谱系数)<strong class="iw hj">梅尔声谱图</strong>和<strong class="iw hj">色度</strong>。Librosa用于他们的提取。</p><p id="0084" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj"> MFCC : </strong>到目前为止，MFCC是该数据集中研究和利用最多的要素。它代表声音的短期功率谱。</p><p id="ab8f" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj"> Mel光谱图:</strong>这只是一个描述振幅的光谱图，该振幅映射在Mel标度上。</p><p id="2a11" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">色度:</strong>色度矢量通常是12元素的特征矢量，指示在标准色度标度中每个音调类有多少能量存在于信号中。</p><h1 id="0627" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">提取特征</h1><p id="4ff4" class="pw-post-body-paragraph it iu hi iw b ix kt iz ja jb ku jd je js kv jh ji jt kw jl jm ju kx jp jq jr hb bi translated">我们只需定义一个函数，从声音文件中提取MFCC、色度和Mel特征。这个函数有4个参数——文件名和三个布尔参数。用声音文件库打开声音文件。从中读取并调用它<strong class="iw hj"> X </strong>。另外，获取采样率。如果色度为真，得到<strong class="iw hj"> X </strong>的<strong class="iw hj">短时傅立叶变换</strong>。</p><p id="fed6" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">假设结果是一个空的numpy数组。现在，对于这三个特性中的每一个，如果存在的话，从librosa.feature中调用相应的函数，得到平均值。用结果和特征值从numpy调用函数hstack()，并将它存储在result中。然后，返回结果。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/2b82ebf87ae64a216666ba1bcf743dae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IhjuOrbMHqBrYcF-sNNyLg.png"/></div></div></figure><h1 id="3d6b" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">遵循的流程</h1><p id="18f5" class="pw-post-body-paragraph it iu hi iw b ix kt iz ja jb ku jd je js kv jh ji jt kw jl jm ju kx jp jq jr hb bi translated">我们定义了一个字典来保存RAVDESS数据集中可用的数字和情绪，以及一个列表来保存我们想要的东西——平静、快乐、恐惧和厌恶。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/3239a91a677a2cef7b5cd3a528206c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TAPrNKm8idHb2z3LV_zFIg.png"/></div></div></figure><p id="7b26" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">加载数据</strong></p><p id="718b" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我们用一个函数加载数据，该函数将测试集的相对大小作为参数。x和y是空列表。我们将使用来自<strong class="iw hj"> glob模块</strong>的<strong class="iw hj"> glob() </strong>函数来获取数据集中声音文件的所有路径名。</p><p id="714a" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">因此，对于每一个这样的路径，获得文件的基本名称，通过将名称分割在“-”周围并提取第三个值来获得情感。</p><p id="a1ae" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">使用我们的<strong class="iw hj">情感字典</strong>，这个数字被转换成一种情感，我们的函数检查这个情感是否在我们的<strong class="iw hj">观察到的情感列表中，</strong>如果不在，它继续到下一个文件。它调用<strong class="iw hj">提取特征</strong>，并将返回的内容存储在“特征”中。然后，它将特征附加到x，将情感附加到y。因此，列表x保存特征，而y保存情感。我们用这些调用函数train-test-split，测试大小，和一个随机状态值，并返回它。</p><p id="3403" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">列车测试分割</strong></p><p id="6455" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我们将数据集分为训练和测试数据，保持测试规模为总数据的25% 。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/21aeded96b0ed3a70e6e814c677712d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOk3ZQfwt7xmw9zR8zy3lQ.png"/></div></div></figure><p id="b6b1" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">分类任务</strong></p><p id="bc84" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">对于我们的语音情感识别系统，我们将使用<strong class="iw hj">MLP分类器</strong>。这是多层感知器分类器，它使用随机梯度下降优化对数损失函数。MLP分类器具有用于分类的内部神经网络。这是一个前馈人工神经网络模型。</p><p id="c6d7" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">然后，我们训练我们的模型，并从中获得预测。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/f0cd111c502c5ca3d0f6365776bdd6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c2_oDlNnIrpWBOMpQoQlNQ.png"/></div></div></figure><p id="2e3a" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">模型评估</strong></p><p id="9b10" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">为了计算我们模型的精确度，我们将调用从<a class="ae lh" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> sklearn </a>导入的<strong class="iw hj"> accuracy_score() </strong>函数。我们还将使用<strong class="iw hj">混淆矩阵</strong>来更好地理解我们的模型预测。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/6475b0f2707164492fce0f0dfa64c0fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p0sNr_Y5EkL9T53YuTa2yw.png"/></div></div></figure><p id="bf27" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">从代码输出可以清楚地看出，该模型获得了<strong class="iw hj"> 68.75% </strong>的准确度分数</p><p id="eba1" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">完整的代码可以在我的GitHub资源库中找到:</p><div class="lj lk ez fb ll lm"><a href="https://github.com/Umair-1119/Speech-Emotion-Recognition" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab dw"><div class="lo ab lp cl cj lq"><h2 class="bd hj fi z dy lr ea eb ls ed ef hh bi translated">umair-1119/语音情感识别</h2><div class="lt l"><h3 class="bd b fi z dy lr ea eb ls ed ef dx translated">在GitHub上创建一个帐户，为Umair-1119/语音情感识别的发展做出贡献。</h3></div><div class="lu l"><p class="bd b fp z dy lr ea eb ls ed ef dx translated">github.com</p></div></div><div class="lv l"><div class="lw l lx ly lz lv ma io lm"/></div></div></a></div><p id="fe2c" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">欢迎在<a class="ae lh" href="https://www.linkedin.com/in/umair-ayub-/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我！</p></div></div>    
</body>
</html>