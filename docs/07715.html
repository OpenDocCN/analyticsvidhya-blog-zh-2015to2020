<html>
<head>
<title>Image Captioning with Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Tensorflow的图像字幕</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/image-captioning-with-tensorflow-2d72a1d9ffea?source=collection_archive---------7-----------------------#2020-07-05">https://medium.com/analytics-vidhya/image-captioning-with-tensorflow-2d72a1d9ffea?source=collection_archive---------7-----------------------#2020-07-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="8629" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">一张图片胜过千言万语，它说…</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/72c609333cc57f9d9b7daf209af13051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U8KhTUg4PU2CzPSK"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">照片由<a class="ae jn" href="https://unsplash.com/@danielcgold?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">丹金</a>在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="def9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当你感到怀旧时，你可能想在你的图库或Instagram feed中滚动你收集的图片，因为它捕捉了你在那些美好的日子里在做什么。或者，如果你需要解释某事，你可能需要图表或图像来更好地解释。所以，是的，图片可以帮助我们更好地交流。</p><p id="81c9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">通常，我们可以很容易地解释一幅图片，但在某些情况下，图片需要由机器来解释，例如当向有困难的人提供描述时。这一次，我和我的朋友合作了一个项目，教机器如何在给定一张图片时创建说明。</p><h1 id="15dd" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">数据概述和预处理</h1><h2 id="d89e" class="lc kl hi bd km ld le lf kq lg lh li ku jx lj lk kw kb ll lm ky kf ln lo la lp bi translated">数据集概述</h2><p id="00f1" class="pw-post-body-paragraph jo jp hi jq b jr lq ij jt ju lr im jw jx ls jz ka kb lt kd ke kf lu kh ki kj hb bi translated">本项目使用的数据集取自Kaggle的Flickr8K。该数据集包含8000幅图像，每幅图像提供5个标题。</p><p id="6535" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">数据集链接:【https://www.kaggle.com/shadabhussain/flickr8k T4】</p><div class="iy iz ja jb fd ab cb"><figure class="lv jc lw lx ly lz ma paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/ee1decc5da3c646c202f4f351396048f.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*DqqtHcOjnrhKLBcgVNtpnw.png"/></div></figure><figure class="lv jc mb lx ly lz ma paragraph-image"><img src="../Images/6681a674d7b0610f38ed27bd73e622a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*x_4nBLIxOtmqjtGNfuYAOg.png"/></figure></div><h2 id="d5b4" class="lc kl hi bd km ld le lf kq lg lh li ku jx lj lk kw kb ll lm ky kf ln lo la lp bi translated">图像特征提取</h2><p id="a2aa" class="pw-post-body-paragraph jo jp hi jq b jr lq ij jt ju lr im jw jx ls jz ka kb lt kd ke kf lu kh ki kj hb bi translated">在图像处理方面，我们应用ResNet50提取图像特征。我们还省略了最后一层(softmax层)，因为我们只需要提取特征，而不需要对图像进行分类。此外，我们还使用ImageNet中的权重进行迁移学习。ImageNet已经训练了超过1400万张图像，并且已经被分组到特定的类别和子类别中。我们希望它可以帮助模型识别对象，以便更好地添加字幕。</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="8ac7" class="lc kl hi md b fi mh mi l mj mk">import tensorflow as tf<br/>from keras.preprocessing import image<br/>from tensorflow.keras.applications.resnet50 import ResNet50<br/>from tensorflow.keras.applications.resnet50 import preprocess_input<br/>import os</span><span id="117a" class="lc kl hi md b fi ml mi l mj mk"><em class="mm"># Get all filenames of the images<br/></em>folder = "../input/flickr8k/Flickr_Data/Flickr_Data/Images/"<br/>images = os.listdir(folder)</span><span id="a818" class="lc kl hi md b fi ml mi l mj mk"><em class="mm">#</em> <em class="mm">Load the CNN Architecture with Imagenet as weights<br/></em>image_model = ResNet50(weights='imagenet')<br/>model_new = tf.keras.Model(image_model.input,image_model.layers[-2].output)</span><span id="b7b1" class="lc kl hi md b fi ml mi l mj mk"><em class="mm"># Store image features in dictionary<br/></em>img_features = dict()  <br/>for img <strong class="md hj">in</strong> images: <br/>    img1 = image.load_img(folder + img, target_size=(224, 224))<br/>    x = image.img_to_array(img1)<br/>    x = np.expand_dims(x, axis=0)<br/>    x = preprocess_input(x)<br/>    <br/>    fea_x = model_new.predict(x)<br/>    fea_x1 = np.reshape(fea_x , fea_x.shape[1])<br/>    img_features[img] = fea_x1</span></pre><p id="ee15" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">执行代码后，输出将是每个图像的1x2048矢量，包含图像的特征。</p><p id="7138" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在进行这项工作时，我和我的朋友也尝试了简单或基本的CNN来从这些图像中提取特征。我们想知道在使用基本CNN模型时会有多好。该模型还生成1x2048矢量。这是我们创建的模型。</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="605d" class="lc kl hi md b fi mh mi l mj mk">model = tf.keras.models.Sequential([<br/> tf.keras.layers.Conv2D(64, (3, 3), activation=’relu’, <br/>          input_shape=  (224, 224, 3)),<br/> tf.keras.layers.MaxPooling2D(2, 2),<br/> tf.keras.layers.Conv2D(64, (3, 3), activation=’relu’),<br/> tf.keras.layers.MaxPooling2D(2, 2),<br/> tf.keras.layers.Flatten(),<br/> tf.keras.layers.Dense(128, activation=tf.nn.relu),<br/> tf.keras.layers.Dense(2048, activation=tf.nn.softmax)<br/>])</span></pre><p id="59ab" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">稍后，我们将比较ResNet50和这个基本CNN模型的结果…</p><h2 id="7d88" class="lc kl hi bd km ld le lf kq lg lh li ku jx lj lk kw kb ll lm ky kf ln lo la lp bi translated">字幕预处理</h2><p id="adc8" class="pw-post-body-paragraph jo jp hi jq b jr lq ij jt ju lr im jw jx ls jz ka kb lt kd ke kf lu kh ki kj hb bi translated">我们要做的第一件事是收集Flickr8k中的所有字幕。Token.txt，并通过一个键(即文件名)将它们分组。之后，我们根据Flickr_8k.trainImages.txt、Flickr _ 8k . devi images . txt和Flickr_8k.testImages.txt为训练、验证和测试集拆分标题。这三个文件只包含各自数据集的文件名。在根据这些文件分割标题时，我们还在每个标题的开头添加了“Startseq ”,在句子的结尾添加了“Endseq”。这是为了在训练阶段告知模型何时应该开始书写或停止预测下一个单词。</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="9a89" class="lc kl hi md b fi mh mi l mj mk"><em class="mm"># Get All Captions</em><br/>fn = "../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt"<br/>f = open(fn, 'r')<br/>capts = f.read()<br/><em class="mm">#Group all captions by filename, for references</em><br/>captions = dict()<br/>i = 0<br/><br/>try:<br/>    for line <strong class="md hj">in</strong> capts.split("<strong class="md hj">\n</strong>"):<br/>        txt = line.split('<strong class="md hj">\t</strong>')<br/>        fn = txt[0].split('#')[0]<br/>        if fn <strong class="md hj">not</strong> <strong class="md hj">in</strong> captions.keys():<br/>            captions[fn] = [txt[1]]<br/>        else:<br/>            captions[fn].append(txt[1])<br/>        i += 1<br/>except:<br/>    passModel </span><span id="f063" class="lc kl hi md b fi ml mi l mj mk">def getCaptions(path):<br/>    <br/>    f = open(path, 'r')<br/>    capts = f.read()<br/>    desc = dict()<br/><br/>    try:<br/>        for line <strong class="md hj">in</strong> capts.split("<strong class="md hj">\n</strong>"):<br/>            image_id = line<br/>            image_descs = captions[image_id]<br/><br/>            for des <strong class="md hj">in</strong> image_descs:<br/>                ws = des.split(" ")<br/>                w = [word for word <strong class="md hj">in</strong> ws if word.isalpha()]<br/>                des = "startseq " + " ".join(w) + " endseq"<br/>                if image_id <strong class="md hj">not</strong> <strong class="md hj">in</strong> desc:<br/>                    desc[image_id] = list()<br/>                desc[image_id].append(des)<br/>    except:<br/>        pass<br/>    <br/>    return desc</span><span id="92f2" class="lc kl hi md b fi ml mi l mj mk"><em class="mm"># Split captions</em><br/>train_caps = getCaptions("../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt")<br/>val_caps = getCaptions("../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt")</span></pre><p id="44b7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">之后，我们必须对来自训练集的标题进行标记，并从中获得单词索引和索引单词字典。单词索引字典的目的是将标题表示为输入到模型中的数字，而索引单词是将下一个单词/预测转换为单词形式，正如我们所知。</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="6ff5" class="lc kl hi md b fi mh mi l mj mk">from keras.preprocessing.text import Tokenizer</span><span id="4bbf" class="lc kl hi md b fi ml mi l mj mk"><em class="mm"># Preparing to make word-index and index-word</em><br/>train_captions = []<br/>for key, desc_list <strong class="md hj">in</strong> train_caps.items():<br/>    for i <strong class="md hj">in</strong> range(len(desc_list)):<br/>        train_captions.append(desc_list[i])<br/><br/><em class="mm"># Tokenize top 5000 words in Train Captions</em><br/>tokenizer = Tokenizer(num_words=5000,<br/>                      oov_token="&lt;unk&gt;",<br/>                      filters='!"#$%&amp;()*+.,-/:;=?@[\]^_`{|}~ ')<br/>tokenizer.fit_on_texts(train_captions)<br/>word_index = tokenizer.word_index<br/>index_word = tokenizer.index_word</span></pre><p id="4943" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">此外，不要忘记将提取的图像特征分割成训练集、开发集和测试集。</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="70f2" class="lc kl hi md b fi mh mi l mj mk">train_fns = list(train_caps.keys())<br/>train_set = dict((k, img_fea[k]) for k <strong class="md hj">in</strong> train_fns)<br/>val_fns = list(val_caps.keys())<br/>val_set = dict((k, img_fea[k]) for k <strong class="md hj">in</strong> val_fns)</span><span id="351e" class="lc kl hi md b fi ml mi l mj mk">fn_test = "../input/flickr8k/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.testImages.txt"<br/>f = open(fn_test, 'r')<br/>t = f.read()<br/><br/>test_fns= t.split("<strong class="md hj">\n</strong>")<br/>test_set = dict((k, img_fea[k]) for k <strong class="md hj">in</strong> list(test_fns[:-1]))</span></pre><h1 id="2e7d" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">训练数据</h1><h2 id="3f5d" class="lc kl hi bd km ld le lf kq lg lh li ku jx lj lk kw kb ll lm ky kf ln lo la lp bi translated">模型的数据表示</h2><p id="5d81" class="pw-post-body-paragraph jo jp hi jq b jr lq ij jt ju lr im jw jx ls jz ka kb lt kd ke kf lu kh ki kj hb bi translated">复杂的部分来了。首先，既然有两种类型的数据(图像和标题)，我们如何向模型提供这些数据呢？</p><p id="5fe1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">回想图像特征提取，现在图像都变成了长度为2048的向量。字幕也被符号化了。提取的图像特征将被附加到已经切片的字幕上。也许，用例子来解释更好…比如，我们有这张图片和标题:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mn"><img src="../Images/d845f32823c780c5fefc2fb9a34674b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*whxoJ_pmC3aYIShoQlh8aA.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">这张图片的说明:五个人在跑步</figcaption></figure><p id="c41e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">标题必须附加“startseq”和“endseq ”,并进行标记化。假设这是已经生成的词到索引词典。</p><ul class=""><li id="fa12" class="mo mp hi jq b jr js ju jv jx mq kb mr kf ms kj mt mu mv mw bi translated">起始序列:1</li><li id="68fd" class="mo mp hi jq b jr mx ju my jx mz kb na kf nb kj mt mu mv mw bi translated">结束序列:2</li><li id="ae89" class="mo mp hi jq b jr mx ju my jx mz kb na kf nb kj mt mu mv mw bi translated">五点十分</li><li id="ba7f" class="mo mp hi jq b jr mx ju my jx mz kb na kf nb kj mt mu mv mw bi translated">人:3</li><li id="da53" class="mo mp hi jq b jr mx ju my jx mz kb na kf nb kj mt mu mv mw bi translated">分别是:5</li><li id="1030" class="mo mp hi jq b jr mx ju my jx mz kb na kf nb kj mt mu mv mw bi translated">跑步:90</li></ul><p id="f0b5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">输入机器的数据应该是这样的:</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="2e90" class="lc kl hi md b fi mh mi l mj mk">X1 : [1X2048 Image feature],[1, 0, 0, … , 0] (1 is for 'startseq')<br/>y1 : 10 (10 is for 'five')</span><span id="cecd" class="lc kl hi md b fi ml mi l mj mk">X2 : [1X2048 Image feature],[1, 10, 0, … , 0] (append last output/next word in the caption to the input)<br/>y2 : 3 (3 is for 'people')</span><span id="e075" class="lc kl hi md b fi ml mi l mj mk">X3 : [1X2048 Image feature],[1, 10, 3, 0, … , 0] (again, append last output/next word in the caption to the input)<br/>y3 : 5(5 is for 'are')</span><span id="e7f4" class="lc kl hi md b fi ml mi l mj mk">X4 : [1X2048 Image feature],[1, 10, 3, 5, 0, … , 0] (again, append last output/next word in the caption to the input)<br/>y4 : 90(90 is for 'running')</span><span id="5799" class="lc kl hi md b fi ml mi l mj mk">X3 : [1X2048 Image feature],[1, 10, 3, 5, 90, 0, … , 0] (again, append last output/next word in the caption to the input)<br/>y3 : 2(2 is for 'endseq')</span></pre><p id="7a3d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">简而言之，将有两个向量输入。一个是提取的图像特征，另一个是用数字表示的切片字幕。输出，即标题中的下一个单词，也用数字表示。</p><p id="cf9f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">字幕可以有不同的长度，即使是相同的图像。为了处理这个问题，分片的字幕必须用零填充，达到训练集中最长字幕的长度。</p><h2 id="10cb" class="lc kl hi bd km ld le lf kq lg lh li ku jx lj lk kw kb ll lm ky kf ln lo la lp bi translated">单词嵌入</h2><p id="bc09" class="pw-post-body-paragraph jo jp hi jq b jr lq ij jt ju lr im jw jx ls jz ka kb lt kd ke kf lu kh ki kj hb bi translated">对于这个项目，我们使用了手套字嵌入字幕。字典中的每个单词都将从预训练的手套模型映射到一个向量中。我们用的手套尺寸是200。</p><p id="0dba" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">根据GloVe网站的报道，GloVe是根据一个单词在另一个单词之后出现的频率来建模的。因此，我们希望预测的字幕可以得到改善。</p><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="6bed" class="lc kl hi md b fi mh mi l mj mk"><em class="mm"># Load Glove vectors</em><br/>embeddings_index = {} <em class="mm"># empty dictionary</em><br/>f = open("../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt", encoding="utf-8")<br/><br/>for line <strong class="md hj">in</strong> f:<br/>    values = line.split()<br/>    word = values[0]<br/>    coefs = np.asarray(values[1:], dtype='float32')<br/>    embeddings_index[word] = coefs<br/>f.close()<br/>print('Found <strong class="md hj">%s</strong> word vectors.' % len(embeddings_index))</span><span id="23d4" class="lc kl hi md b fi ml mi l mj mk"><em class="mm"># Get 200-dim dense vector for each of the 10000 words in out vocabulary</em><br/>vocab_size = len(word_index) + 1<br/>embedding_dim = 200<br/>embedding_matrix = np.zeros((vocab_size, embedding_dim))<br/><br/>for word, i <strong class="md hj">in</strong> word_index.items():<br/>    <em class="mm">#if i &lt; max_words:</em><br/>    embedding_vector = embeddings_index.get(word)<br/>    if embedding_vector <strong class="md hj">is</strong> <strong class="md hj">not</strong> None:<br/>        <em class="mm"># Words not found in the embedding index will be all zeros</em><br/>        embedding_matrix[i] = embedding_vector</span></pre><p id="823d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，我们收集GloVe中的所有单词及其系数。然后，我们将以前字典中的所有单词映射到一个向量，并将它们收集到一个矩阵中(嵌入矩阵)</p><h2 id="55cc" class="lc kl hi bd km ld le lf kq lg lh li ku jx lj lk kw kb ll lm ky kf ln lo la lp bi translated">模型架构</h2><pre class="iy iz ja jb fd mc md me mf aw mg bi"><span id="f614" class="lc kl hi md b fi mh mi l mj mk">max_length = 34<br/>image_model = Sequential()<br/><br/>image_model.add(Dense(embedding_dim, input_shape=(2048,), activation='relu'))<br/>image_model.add(RepeatVector(max_length))<br/><br/>language_model = Sequential()<br/><br/>language_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))<br/>language_model.add(LSTM(256, return_sequences=True))<br/>language_model.add(TimeDistributed(Dense(embedding_dim)))<br/><br/>conca = Concatenate()([image_model.output, language_model.output])<br/>x = LSTM(128, return_sequences=True)(conca)<br/>x = LSTM(512, return_sequences=False)(x)<br/>x = Dense(vocab_size)(x)<br/>out = Activation('softmax')(x)<br/>model_1 = Model(inputs=[image_model.input, language_model.input], outputs = out)<br/><br/>model_1.layers[2].set_weights([embedding_matrix])<br/>model_1.layers[2].trainable = False<br/><br/>model_1.compile(loss='categorical_crossentropy', optimizer = Adam(learning_rate = 0.0001), metrics=['accuracy'])</span></pre><p id="bfcb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于模型架构，开头有两个部分。第一个是处理图像特征(image_model)，它由密集层和重复向量组成。第二部分是处理标题的部分(language_model)。在语言模型的第一层，有一个嵌入层，其权重将由我们之前在嵌入矩阵中收集的手套系数来分配。我们还将该层设置为不可训练，因为它之前已经过预训练。</p><p id="67cf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">接下来，连接image_model和language_model来预测下一个单词(输出)。对于下一个单词预测部分，有2层LSTM，接着是用于分类的密集和软最大层(因为这种情况无论如何是多类分类)。</p><h1 id="b7a3" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">结果</h1><p id="a65e" class="pw-post-body-paragraph jo jp hi jq b jr lq ij jt ju lr im jw jx ls jz ka kb lt kd ke kf lu kh ki kj hb bi translated">我们正在比较没有单词嵌入的基本CNN架构与高级CNN架构&amp; GloVe之间的结果。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/87bf2678b3d2c0d5c464ea8e9f39815b.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*7miUdNEqSk3ovqSp-SpdcQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">建模的结果</figcaption></figure><p id="ce74" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">除了基本的CNN和ResNet50，我们还用InceptionV3提取图像。根据结果，使用更先进的CNN架构和单词嵌入，性能更好，在训练集和验证集中损失下降1个点。在训练集中，准确率提高了8-14%，而在验证集中，准确率仅提高了3%。</p><p id="8c61" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这里引入了一个称为“BLEU”的指标。什么是BLEU评分？</p><p id="95ae" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">BLEU代表双语评估替角，用一些参考资料来评估候选翻译。虽然最初用于翻译，但现在该分数通常用于其他NLP目的。对于完全匹配，分数为1，对于完全不匹配，分数为0。</p><p id="691c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于上面的结果，使用InceptionV3时BLEU评分更好。简而言之，有了更先进的CNN，BLEU分数增加了2-4%。</p><h2 id="a356" class="lc kl hi bd km ld le lf kq lg lh li ku jx lj lk kw kb ll lm ky kf ln lo la lp bi translated">图像测试</h2><p id="f725" class="pw-post-body-paragraph jo jp hi jq b jr lq ij jt ju lr im jw jx ls jz ka kb lt kd ke kf lu kh ki kj hb bi translated">所以，让我们在图像上测试！下面，图片是用ResNet50 &amp; GloVe测试的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/7dcba388d8690ade4b6733d7cabca41e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tOIlzrk_e5XTYOHqfaV6Zg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:个人图片。预测说明:一群人正在摆好姿势拍照</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/f01aa9ee64afa86768a27037de214870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pyV_gt1GYqwAOmp29yKMhQ.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:Youtube。预测描述:身穿蓝色制服的足球运动员拿着足球奔跑</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nf"><img src="../Images/a7105ae6f164aad43942dbf298f09303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXYf51jE0Wfp9Z6s4RLnag.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:个人图片。预测说明:人们站在一座城市的城市景观前</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/966c0c5dafb30ec39d506e653bc83adf.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*ljy-DfywGGDmUnvYHpDvTg.jpeg"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">资料来源:Unsplash。预测描述:狗正在草丛中奔跑</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nh"><img src="../Images/5aeb6d0a55c6cb0606334f9491a59a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*toVoiC0uLOD4KphOJ089og.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:Freepik。预测描述:穿红衬衫和蓝衬衫的男孩正在吃零食</figcaption></figure><h1 id="8c30" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">改进的余地</h1><p id="c3bf" class="pw-post-body-paragraph jo jp hi jq b jr lq ij jt ju lr im jw jx ls jz ka kb lt kd ke kf lu kh ki kj hb bi translated">在我们处理这个案例时，我们认为有一些想法可以提高性能:</p><h2 id="88a9" class="lc kl hi bd km ld le lf kq lg lh li ku jx lj lk kw kb ll lm ky kf ln lo la lp bi translated">向数据集添加品种</h2><p id="6b32" class="pw-post-body-paragraph jo jp hi jq b jr lq ij jt ju lr im jw jx ls jz ka kb lt kd ke kf lu kh ki kj hb bi translated">我们注意到数据集中出现的大多数对象不是人就是狗。一旦我们在猫身上做了测试，它就会在狗身上预测这个标题。我们很好奇，发现字幕里猫只出现了61次，而狗出现了1000次！</p><p id="9400" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们还注意到标题总是以人/动物开头，后面跟着动词。有一次，我们对一张从更高楼层拍摄的高楼照片进行了测试，预测从“人”开始，而照片上没有人。我们认为也许字幕应该更多样化。</p><p id="3ebd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这种情况下预测的动词实际上并不多种多样(也有可能我们没有在更多的图片上测试它)。出现最多的动词是:站、跑、坐、玩、爬。当我们提交一个睡在床上的人时，它说坐在床上。也许，应该首先评估单词在词典中的分布。</p><h2 id="6899" class="lc kl hi bd km ld le lf kq lg lh li ku jx lj lk kw kb ll lm ky kf ln lo la lp bi translated">更好地预测颜色和性别</h2><p id="bf1b" class="pw-post-body-paragraph jo jp hi jq b jr lq ij jt ju lr im jw jx ls jz ka kb lt kd ke kf lu kh ki kj hb bi translated">颜色经常被错误地分类。当实际图片是一件绿色衬衫时，预测说是另一种颜色。在其他情况下，男孩被预测为女孩，女人被预测为男孩，但这可能取决于照片中的姿势或位置。</p></div><div class="ab cl ni nj gp nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="hb hc hd he hf"><p id="5c6f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于本项目，我们参考以下链接:</p><div class="np nq ez fb nr ns"><a href="https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8" rel="noopener follow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hj fi z dy nx ea eb ny ed ef hh bi translated">使用Keras的图像字幕——“教计算机描述图片”</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">目录:</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">towardsdatascience.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og jh ns"/></div></div></a></div><div class="np nq ez fb nr ns"><a href="https://machinelearningmastery.com/develop-a-deep-learning-caption-generation-model-in-python/" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hj fi z dy nx ea eb ny ed ef hh bi translated">如何从零开始开发深度学习照片字幕生成器-机器学习掌握</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">开发一个深度学习模型，用Keras一步一步地用Python自动描述照片。标题…</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">machinelearningmastery.com</p></div></div><div class="ob l"><div class="oh l od oe of ob og jh ns"/></div></div></a></div><div class="np nq ez fb nr ns"><a rel="noopener follow" target="_blank" href="/@raman.shinde15/image-captioning-with-flickr8k-dataset-bleu-4bcba0b52926"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hj fi z dy nx ea eb ny ed ef hh bi translated">使用Flickr8k数据集和BLEU的图像字幕</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">目录:</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">medium.com</p></div></div><div class="ob l"><div class="oi l od oe of ob og jh ns"/></div></div></a></div><div class="np nq ez fb nr ns"><a href="https://www.kaggle.com/shadabhussain/flickr8k" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hj fi z dy nx ea eb ny ed ef hh bi translated">Flickr8K</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">Kaggle是世界上最大的数据科学社区，拥有强大的工具和资源来帮助您实现您的数据…</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">www.kaggle.com</p></div></div><div class="ob l"><div class="oj l od oe of ob og jh ns"/></div></div></a></div><div class="np nq ez fb nr ns"><a href="https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hj fi z dy nx ea eb ny ed ef hh bi translated">GloVe:单词表示的全局向量</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">来自维基百科2014 + Gigaword 5的预训练单词向量</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">www.kaggle.com</p></div></div><div class="ob l"><div class="ok l od oe of ob og jh ns"/></div></div></a></div></div></div>    
</body>
</html>