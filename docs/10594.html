<html>
<head>
<title>Something-More about KNN Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于 KNN 分类的更多信息</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/something-more-about-knn-classification-bedbc2d383ab?source=collection_archive---------20-----------------------#2020-10-25">https://medium.com/analytics-vidhya/something-more-about-knn-classification-bedbc2d383ab?source=collection_archive---------20-----------------------#2020-10-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="40ff" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果是数据科学，你不能错过 KNN</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es jk"><img src="../Images/be9e2715468453726c27bf08c8523f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*YQdNufGhTGKSKmVAsxa3iw.jpeg"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">我可爱的邻居！</figcaption></figure><h1 id="a2e9" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">介绍</h1><p id="f16c" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated">用于分类的最基本算法之一(当然你对机器学习很熟悉)。该算法是在机器学习中使用的更简单的技术之一，并且由于其易用性和低计算时间而受到业内许多人的青睐。KNN 是一种根据最相似的点对数据点进行分类的模型。它使用特征相似性来预测新点将落入的聚类。</p><blockquote class="kz la lb"><p id="fc82" class="im in lc io b ip iq ir is it iu iv iw ld iy iz ja le jc jd je lf jg jh ji jj hb bi translated">KNN 是一种被认为<strong class="io hj">非参数化</strong>的算法，也是<strong class="io hj">懒惰学习</strong>的一个例子。现在让我们谈一谈这个问题。</p></blockquote><ul class=""><li id="0094" class="lg lh hi io b ip iq it iu ix li jb lj jf lk jj ll lm ln lo bi translated"><strong class="io hj">非参数化</strong>由于没有对潜在的数据分布模式做出假设，该模型完全由提供给它的数据组成，但没有假设数据的结构是正常的。</li><li id="78c4" class="lg lh hi io b ip lp it lq ix lr jb ls jf lt jj ll lm ln lo bi translated"><strong class="io hj">懒惰学习</strong> KNN 没有训练步骤，数据点仅在预测时使用。由于没有训练步骤，预测步骤的成本很高。热切的学习者算法在训练步骤中热切地学习。</li></ul><h1 id="6a38" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">算法的预感</strong></h1><p id="48c5" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated">与 KNN 一样，输出被分配给其<em class="lc"> K 个最近邻中最常见的一个类，</em>K 表示所考虑的邻的数量。现在，下一个问题是如何确定 K 的值，因为基于训练数据的模型是经过 wrt 测试数据验证的，并基于该数据计算误差，所以基本上迭代过程是通过改变 K(通常从<em class="lc"> 1 到所有记录计数的平方根</em>)进行的，并且在图中绘制每个 K 值的相应误差值，然后可以基于最低误差选择 K 值，由于其形状，该图有时也被称为肘形图。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lu"><img src="../Images/c743f5fda5b2c7904bc3ea33924483d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lRAmWAikCPc_SK8ENRDUg.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">验证图(肘形图)</figcaption></figure><p id="bfd8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在验证图中，错误率最初降低并达到最小值。在最小值点之后，它随 K 的增加而增加。误差达到最小值时的 K 值应用于所有预测。</p><h1 id="160a" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">现在如何决定我的邻居？</h1><p id="af80" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated">基于新点和每个训练点之间的距离来决定邻居。有各种方法来计算这个距离，其中最常见的方法是欧几里德距离、曼哈顿距离(用于连续距离)和汉明距离(用于分类距离)。</p><p id="ac1e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">欧几里德距离:</strong>欧几里德距离计算为新点(x)和现有点(y)的平方差之和的平方根。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lz"><img src="../Images/1df88ad8151a5c5fce3751a392865f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*1HWAX5TKUnKBB3KSSLux0w.jpeg"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><strong class="bd jy">欧几里德距离</strong></figcaption></figure><p id="a7db" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">曼哈顿距离:</strong>两个向量(或点)a 和 b 之间的曼哈顿距离定义为∑<em class="lc">I</em>|<em class="lc">ai</em>-<em class="lc">bi</em>|在向量的维数上。</p><p id="f285" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果我们需要计算网格状路径中两个数据点之间的距离，我们使用曼哈顿距离，也称为<strong class="io hj">城市街区距离</strong>，或<strong class="io hj">出租车几何形状</strong>。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ma"><img src="../Images/989bf67d0a962cd356422dca70f85e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*CH1OGSMnZseUrIaEn-xChA.jpeg"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">对曼哈顿和欧氏距离的简单理解</figcaption></figure><p id="1b6f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">汉明距离:</strong>汉明距离衡量两个相同长度的字符串之间的相似性。相同长度的两个字符串之间的汉明距离是对应字符不同的位置的数量。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mb"><img src="../Images/99624390fe63456b49fa0b5c711f0de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qWsg--lzaJCpfriyerNK9A.jpeg"/></div></div></figure><p id="76be" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">汉明距离根据位置比较两个字符串中的每个字母。因此，单词 1 的第一个字母与单词 2 的第一个字母进行比较，然后根据位置比较两个字符串中的每个字母。</p><h1 id="e82a" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">摘要</h1><p id="3443" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated">k-最近邻(KNN)算法是一种简单的监督机器学习算法，可用于解决分类和回归问题。它易于实现和理解，但有一个主要缺点，即随着所用数据的增长，速度会明显变慢。</p><p id="9e83" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">虽然本文中没有讨论回归，但请记住回归的一般概念，KNN 通过查找查询和数据中所有示例之间的距离，选择指定数量的最接近查询的示例(K)，然后投票选择最频繁的标签(在分类的情况下)或平均标签(在回归的情况下)。</p><h2 id="8ca1" class="mc jx hi bd jy md me mf kc mg mh mi kg ix mj mk kk jb ml mm ko jf mn mo ks mp bi translated">如果你学到了新的东西或者喜欢阅读这篇文章，请鼓掌👏并分享给其他人看。也可以随意发表评论。</h2></div></div>    
</body>
</html>