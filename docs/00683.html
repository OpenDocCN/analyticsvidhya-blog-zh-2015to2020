<html>
<head>
<title>The Math and Intuition behind Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归背后的数学和直觉</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-complete-understanding-of-how-the-logistic-regression-can-perform-classification-a8e951d31c76?source=collection_archive---------0-----------------------#2019-08-26">https://medium.com/analytics-vidhya/a-complete-understanding-of-how-the-logistic-regression-can-perform-classification-a8e951d31c76?source=collection_archive---------0-----------------------#2019-08-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a4b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated"><span class="l je jf jg bm jh ji jj jk jl di"> L </span> <em class="jm">逻辑回归(LR)是机器学习(ML)中最流行的分类算法之一。顾名思义，它是回归，但仍被用作分类，当应用程序具有低延迟需求时，它被广泛使用，并且它还以其良好的功能可解释性而闻名。与KNN、随机森林、XGBoost不同，LR不需要很高的计算资源。然而，当假设失败时，性能可能会更差。让我们深入了解LR背后的数学原理。</em></p></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><h1 id="2415" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">我们先来了解一个人类是如何分类的？</h1><p id="cc93" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">看看下面这些漂亮的人像，分别在A和b两个不同的位置。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es kx"><img src="../Images/013dc9d6c1a2c55330c993b47960843b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wbowrQCtEdMpXzUbJ-iQLw.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">来源是<a class="ae ln" href="https://www.setaswall.com/landscape-wallpapers/bright-day-light-wallpaper-1920x1200/" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae ln" href="http://miriadna.com/preview/green-field,-tree-and-storm" rel="noopener ugc nofollow" target="_blank">这里</a></figcaption></figure><p id="7992" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果有人让你预测1小时后哪个位置会有更高的降雨量呢？看着这两个位置，你可以明确地告诉在“位置B<strong class="ih hj">”</strong>将有更高的降雨机会。</p><p id="5848" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在你自己解释一下，你是如何判断b地点有很高的降雨几率的。</p><p id="69fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是的，当然，你可能会说，比如在位置B有<strong class="ih hj">乌云</strong>，乌云的出现是降雨发生的一个很好的迹象。因此，答案是位置B，这是绝对完美的。但是你是怎么知道“乌云的出现是降雨的好征兆”这一事实的呢？这个事实是我们从过去观察许多类似情况的经验中学到的。</p><p id="0365" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我们(人类)可以通过应用我们从经验中学到的知识来判断一些简单的(如上)未来行动。每种分类/回归算法也以类似的方式判断/预测未来的结果，一旦它从大量数据中学习到隐藏的模式，就像(在上面的例子中)我们已经知道乌云的存在是降雨的一个很好的迹象，但是这些算法可以利用非常复杂的数据来更有效地预测。</p><p id="ac8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看看下面的图片。如果我们想要预测哪个位置在未来1小时内最有可能降雨，只基于单一特征“<strong class="ih hj">存在乌云</strong>”。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lo"><img src="../Images/cc7bbf860dcf549261f5c50b88e3779a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6vZt4rwO6aKGEYZQU-timg.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">来源是<a class="ae ln" href="http://miriadna.com/preview/green-field,-tree-and-storm" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae ln" href="https://thewallpaper.co/nature-rain-windowsstorm-clouds-landscape-sky-landscape-mobile-wallpaper-weatherandroid/" rel="noopener ugc nofollow" target="_blank">这里</a></figcaption></figure><p id="5a58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，在这种情况下，这是一个有点复杂的预测任务，因为这两个地方有类似的多云气候。在上述两个例子中，我们只有一个变量来判断/预测降雨量，即“乌云的存在”。假设，对于每个位置，您都提供了一些类似于<br/>的记录信息。风速<br/>。湿度<br/>。温度<br/>。大气压<br/>。离海岸的距离<br/>。存在乌云等。</p><p id="0cee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到所有这些变量，对一个人来说很难判断，但对一个ML类来说，只有当我们有足够大的数据时，这才是一个非常简单的任务。我们可以训练分类器模型，分类器比人类更有效地从数据中学习，并根据给定的当前条件预测结果。</p><h1 id="d5c9" class="ju jv hi bd jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr bi translated">ML分类器如何工作的高级概述？</h1><p id="d50d" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">将以下伪数据视为过去几年收集的历史数据，其中可能包含几千个观察值，如下所示。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lu"><img src="../Images/a98597d17d2d234e25180f320a0ff331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qpIhk-jnbU7MRReSmzNz4w.jpeg"/></div></div></figure><p id="1eb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们有6个变量和一个依赖类标签的数据。为了建立一个ML分类器，大约70-80%的数据可以用来训练模型，其余的30-20%的数据可以用来测试它。一旦达到最佳结果，我们就可以根据当前条件使用该模型进行未来预测。下面是描述同样事情的流程图。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es lv"><img src="../Images/46cddf043bba0df0f267fb61ed072c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*mx_-see9KyUE064ag1oaMw.png"/></div></figure><h1 id="16c5" class="ju jv hi bd jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr bi translated">逻辑回归分类器</h1><p id="4095" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">是的，名字看起来模棱两可但不是它的概念。在了解它之前，我们先了解一下什么是回归和分类。</p><ul class=""><li id="d2ce" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated"><strong class="ih hj">分类</strong>:生成输出的模型，该输出将被限制为某个有限的离散值集合。</li></ul><p id="543c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:考虑我们正在进行的例子，其中的任务是找到在未来一个小时内是否会下雨？这里输出只能限制为“1”或“0”。</p><p id="2213" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1:是的</p><p id="03c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">0:否</p><ul class=""><li id="8246" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated"><strong class="ih hj">回归:</strong>模型，其输出可以是连续值，不能局限于某些设定的离散值</li></ul><p id="2bec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:假设任务是找到未来1小时内的降雨量，这里我们不能将降雨量限制在某组数字。这可以是任何浮点数。</p><p id="40b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于分类和回归，都需要类别标签，因此这些算法属于被称为监督学习的类别，换句话说，这些算法需要包含类别标签的先前记录的数据。</p><p id="0afb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">甚至名称“回归”也包含在LR中，在其机制结束时，它充当分类器。</p><p id="2619" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不仅仅是降雨量预测，这里还有一些我们可以使用分类算法的场景，比如LR。</p><ul class=""><li id="d052" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated">假设您想在您的企业中开发一个医疗保健移动应用程序，该应用程序可以根据个人目前的症状预测3年后中暑发作的几率</li><li id="7a09" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">假设，在您的电子商务业务中，您需要从他的评论中了解客户满意度，您可以使用LR构建一个系统，告诉您客户是非常高兴、满意还是失望。</li><li id="e4aa" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">对于某些类型的疾病诊断，如癌症，医生将投入大量的精力和时间来分析多项测试结果，以确认最终结果。在这种情况下，可以开发一个基于LR的模型来预测结果，并且在短时间内提供结果的原因。</li></ul><p id="8601" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，我们不能一概而论，这些只是应用的最佳情况，LR只有在其假设为真时才是最佳选择，LR的优点在于，它不仅预测结果，而且提供预测结果的置信度，并且它在很短的时间内提供结果的原因。</p><h1 id="d9ec" class="ju jv hi bd jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr bi translated">LR如何做决定</h1><p id="0eed" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">现在让我们考虑一个新的例子，假设我们有一些新的数据，如果我把它画在二维空间，它可能看起来像下图所示。</p><p id="1c5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地理解，假设绘制的数据属于“<strong class="ih hj">两个</strong>类(是/否)的点，属于一个人的集合，该集合指示该人在5年后是否会有心脏病发作的机会，仅基于2个变量(例如可以是“年龄”、“胆固醇水平”)。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mk"><img src="../Images/4a3e30ed91169ba8a21cdc633a5cf5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*Pd5tb1gwS04BcWbKg6_v6w.png"/></div></figure><p id="1b1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过观察上面的散点图，我们可以说所有的阳性诊断点在2d空间的上侧，而阴性诊断点在该空间的下侧。同样的事情，让我通过引用一个完美的例子来更有效地总结。</p><p id="cfbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">想象一下，如果我找到分离红点和绿点的最佳直线。所以我可以说，属于线的一边的点是一种，属于线的另一边的点是另一种。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ml"><img src="../Images/9b17cdc64277d345e74016b4d4f04ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*MtTf9FPc5Q1Psm3nfnlTMw.png"/></div></figure><p id="43fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就是这样，LR的主要作用是找到几乎能把两个不同类分开的最佳线性曲面(直线/平面/超平面)，也叫决策曲面。找最佳直线无非是找那条直线的方程。直线方程的一般形式是<em class="jm"> ax+by+c=0。</em>在本例中，寻找划分等级的直线方程，这在数学上意味着我们必须找到[a b c]的值，其中<strong class="ih hj"> x </strong>和<strong class="ih hj"> y </strong>取“胆固醇水平”和“年龄”的值。</p><p id="d6ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我在上面的例子中绘制的数据只使用了两个变量，因此它完全适合二维空间。在实时中，我们可以有几千个这样的变量，其中线/平面不足以在这样的高维空间中分离。为了使它通用，让我们坚持表示d维线性曲面的符号，它是超平面的一个方程</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mm"><img src="../Images/966b0bf117b10b7eea64707562c277bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*XO2NGiT8l5LXlEEOHhdKmA.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">超平面方程</figcaption></figure><p id="c414" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用线性代数，我们可以重写同样的东西如下。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mn"><img src="../Images/b1bad687e50c23461a7025431196e65f.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*IILT5lF06M1d2Z_mIsTi7w.png"/></div></figure><p id="dd01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mo"><img src="../Images/f93f4901a259b1c0d284491be2a82756.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*Zg9NWFeZWGJ71fWtwVseyg.png"/></div></figure><p id="8963" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要找到最佳超平面，就要找到常数<strong class="ih hj"> C </strong>和<strong class="ih hj"> W </strong>向量的值，也称为权向量，还有w1，w2，w3，…。，wd称为飞机的重量。在LR模型的训练阶段，它试图找到并学习几乎可以分离两种不同点的最佳权重值。由于LR决策完全基于线性表面的参考，当且仅当数据几乎都是线性可分的时，它表现良好。因此，在应用LR之前，我们采用这个假设。</p><p id="50f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了计算权重，我们需要一些目标函数，这也有助于减少每次迭代中的损失。</p><h1 id="d13d" class="ju jv hi bd jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr bi translated">让我们了解如何获得LR的目标函数</h1><p id="b185" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">为了导出目标函数，我们借助于几何学，因为LR在更高维度空间中被推广，在更高维度(&gt; 3-d)中成像几何学对于人类来说是不可能的。因此，我们试图在二维空间中形象化整个概念，并将同样的理解扩展到更高维度的空间。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mp"><img src="../Images/e3bff47b41662a1d4cb6003f2419f5d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*-1W79Mty61BdcurXUiq_Ug.png"/></div></figure><p id="d2fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">想象这是一个d维空间，有一个超平面，其权重向量<strong class="ih hj">‘W’</strong>将两个不同的点分开。</p><ul class=""><li id="6aac" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated">既然是高维空间我们就不能简单的判断超平面上面的点属于一类或者超平面下面的点属于另一类，这在d维空间不一定总是成立的。概括的思想是考虑向量的方向，并得出结论:点位于与W相同的方向，则它是正的点，而点位于与W相反的方向，则它是负的点。</li></ul><p id="ca83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设超平面通过原点，权重向量是单位向量。那么超平面的方程将如下。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mq"><img src="../Images/be372f1ff1017e73168e5ea17eb28421.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*S0k--_QccTt1pjGw8C4JpA.png"/></div></figure><p id="f7ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设“<strong class="ih hj">Xi”</strong>为任意一点，使得该点与超平面之间的距离由下式给出</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mr"><img src="../Images/5a69d0eeb5ce82ddbcb291544eb394d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*kAa1U4G5vOaXhXQ1tetgFw.png"/></div></figure><p id="8a1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为w是单位矢量</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ms"><img src="../Images/c998b38641dd597b6cca53f59bb65aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:164/format:webp/1*PqA_tlLMw5lApfYNCadN-w.png"/></div></figure><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mt"><img src="../Images/5095c2b9358d30ae2e271ee24756b62a.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*TdnpAtBEIfJvt-JUaQwGTQ.png"/></div></figure><p id="054a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果<em class="jm">距离</em>距离<strong class="ih hj"> xi </strong>为正，这意味着指向<strong class="ih hj"> W </strong>的方向，那么其对应的预测为“<strong class="ih hj">易</strong>”=+1。如果<em class="jm">距离</em>到“<strong class="ih hj"> xi </strong>的距离为负，则该点与<strong class="ih hj"> W </strong>的方向相反，则“<strong class="ih hj">yi</strong>”=-1。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mu"><img src="../Images/f857d64379eee20406b15bdfcf57a88e.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*UPGrZAz7nPdcU7L7XxzeFQ.png"/></div></figure><p id="dfea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我想检查模型上的一个查询点<strong class="ih hj"> "xq" </strong>，因为我们已经知道这个'<strong class="ih hj"> xq </strong>'的实际'<strong class="ih hj"> yq </strong>，所以我们有</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mv"><img src="../Images/ff9dc00e4d5ddbb041f0140c1ad312c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*uz8E30gtWqYqYvVtUoWAWQ.png"/></div></figure><p id="5aeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设，如果我们得到从"<strong class="ih hj"> xq </strong>"到超平面的距离为任意正值，那么预测值将为+1，并且我们得到实际值为"<strong class="ih hj"> yq"=1，</strong>因此</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es mw"><img src="../Images/a28a2912c2d28272a9c8e7f16f9db5bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*L5YtsOP2T6Edh4eDfvsbfA.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">1 x 1 = 1</figcaption></figure><p id="e060" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们在另一种情况下考虑这一点，现在假设我们有"<strong class="ih hj"> yq </strong> "= -1作为实际值。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mx"><img src="../Images/f18bf94e3b6b790bc44687b63249d6d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*oWT7vjReA8MZ7MubgSlPvw.png"/></div></figure><p id="82c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们得到从“<strong class="ih hj">xq”</strong>到超平面的负距离值，那么预测的输出将是-1，这被模型正确地分类。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es my"><img src="../Images/561f5cfb773d22518d87352a908aff62.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*-I0544xlcEHLGHQuYLPt5g.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">-1 x -1 = +1</figcaption></figure><p id="6746" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，对于正确分类的点，距离和实际值的乘积总是正的。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mr"><img src="../Images/342329e61ec0ceeb537e0e1149252391.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*VTlRGMTpMcI9xcGIFfac7w.png"/></div></figure><p id="c947" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于分类不正确的点，总是以同样的方式</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es mz"><img src="../Images/31be2d390aab14fe3b279121676b78d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*-G7N_kdxNLa4nYUZoIM1gQ.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">分类不正确+1 x -1 = -1</figcaption></figure><p id="2fff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的主要目标是找到一个超平面，使得它能够尽可能正确地分离这些点。对于一个可观的模型，需要最大化正确分类数，减少误分类数。把同样的东西写进数学。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es na"><img src="../Images/668a2c8efb6624608e07d23ff8b6350e.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*zGyUGz1uM_MVqU4Unl_qsA.png"/></div></figure><p id="8005" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的方程被称为优化问题，它将在每次迭代中改变其值时找到最佳的<strong class="ih hj"> W </strong>。直观地说，这意味着它试图找到<strong class="ih hj"> W </strong>的最佳值，这样它将产生从列车数据中的所有<strong class="ih hj"> n </strong>点获得的上述总和的最大值。</p><p id="8f94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的问题是，理想情况下，求和值越大，误分类就越少。因此，我们正在寻找最佳的"<strong class="ih hj"> W" </strong>，它将使求和达到最大值。</p><p id="c9ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里对于LR，我们不考虑上面的等式，因为它有一些实际的原因，看看下面的陈述。</p><ul class=""><li id="06a1" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated">到目前为止，我们正在计算从一个点到超平面的距离，根据获得的符号(忽略其大小)来判断其类别，大小值也不能给你查询的点是+ve还是-ve。</li><li id="8003" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">在任何类型的数据中都有较高的机会出现异常点，对于这些异常点来说，距离的量值相对非常高，即使很少的异常点也会对找到最优超平面产生更大的影响。</li><li id="2134" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">点到平面的距离可以是(- ∞，∞)之间的任意值。作为一个输出，它给出真实值，在这个阶段LR就像一个回归量，我们需要通过考虑一些阈值来限制这个输出范围到某个水平，以实现最终的分类任务。</li></ul><p id="a694" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了满足上述所有要求，我们应该挤压价值观。对于压缩值，我们使用<strong class="ih hj"> Sigmoid函数</strong>，我们之所以只使用Sigmoid函数，是因为它会给出非常好的概率解释，换句话说，对于任何作为输入的数字，Sigmoid总是返回范围在[0，1]之间的值。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es nb"><img src="../Images/564605a3773fd6d8b0c1cb9f4f2059da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t1rz2JgSIly63pfiFUnKPg.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated"><strong class="bd jw">Sigmoid函数图</strong>(来源为<a class="ae ln" href="https://deepai.org/machine-learning-glossary-and-terms/sigmoidal-nonlinearity" rel="noopener ugc nofollow" target="_blank">此处为</a>)</figcaption></figure><p id="adb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es nc"><img src="../Images/0baf068ad497612b49c8eb257d006dac.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*X1xg2KQd0gPlh61pcFBizw.png"/></div></figure><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es nd"><img src="../Images/9b37e3331587ac68067fd7f0d7528427.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*JLLD_5VoaP0Eib4k-hyOJA.png"/></div></figure><p id="ea97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于sigmoid输出，决定如下</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ne"><img src="../Images/21efd299be0403b414b4a69d69a05e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*BKAL123nkwLXpzJdxoeotQ.png"/></div></figure><p id="9a31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，应用sigmoid后的优化问题如下。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es nf"><img src="../Images/571c102ecdd3dc16a50967440c68200a.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*0jXIsY_HCwG9A5eTXz0r9A.png"/></div></figure><p id="ec6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在将<strong class="ih hj"> log( ) </strong>应用到优化问题中。由于<strong class="ih hj"> log( ) </strong>是单调函数，不会影响整体优化，<strong class="ih hj"> log( ) </strong>具有良好的性质，可以将乘法转换为加法，将分数转换为减法。</p><p id="329c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们应用<strong class="ih hj"> log( ) </strong>的原因是，这里我们的主要目标是通过优化上述等式找到最佳的<strong class="ih hj"> W </strong>，已经说过，在应用sigmoid后，我们会得到介于[0，1]之间的值，这将更有可能得到非常小的十进制值，当我们将所有数据点的值相加时，可能会产生数值不稳定。当我们应用<strong class="ih hj"> log() </strong>时，它会在优化过程中处理数值计算，而不会影响优化目标。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ng"><img src="../Images/d6ef6b3eff3be0a17bd4ee88216ff5de.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*eRAq0YIhLYHTSeF2J6TdRQ.png"/></div></figure><p id="e22b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过使用<strong class="ih hj">对数</strong>属性，同样的等式可以写成如下。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es nh"><img src="../Images/6b0a51bafc639299627e7f44092f4e0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*QCZq_TJJ8O_QPLY0uYGUJA.png"/></div></figure><p id="b977" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个方程可以被投影为一个优化问题，可以通过使用梯度下降类算法(如SGD)来解决，当我们使用上述优化来训练具有大数据的模型时，我们可以获得最佳的<strong class="ih hj"> W </strong>，这意味着由<strong class="ih hj"> W </strong>形成的方程可以很好地分离所有可能包括异常值和噪声点的训练数据。但是对于未来看不到的数据点，它可能不太适用，这也称为过拟合。为了控制这种影响，我们将在上面的方程中加入一些正则项。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ni"><img src="../Images/a6c727dddf9a38880cc8a420a2387ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*yzVM2bR44jNufzxcb_ErgA.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">这是LR的最终目标函数</figcaption></figure><p id="5faf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里<strong class="ih hj"> λ </strong>被称为超参数，这意味着我们可以使用它来控制正则化的影响。</p><p id="2901" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果<strong class="ih hj"> λ </strong>增加得更多，那么模型也倾向于欠拟合，这意味着训练数据本身的性能更差。</p><p id="c4ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果<strong class="ih hj"> λ </strong>减小，则模型倾向于过度拟合，因此选择最佳<strong class="ih hj"> λ </strong>非常重要，这可以通过使用交叉验证技术调整超参数来确定。</p><p id="ba97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在正则化项中，我们使用了<strong class="ih hj"> W </strong>的<strong class="ih hj"> λ </strong>和<strong class="ih hj"> L2范数</strong>的乘积，这里我们也可以尝试使用<strong class="ih hj"> W </strong>的<strong class="ih hj"> L1范数</strong>，但是<strong class="ih hj"> L1范数</strong>将为<strong class="ih hj"> W </strong>向量中所有不太重要的特征生成零，这意味着它比<strong class="ih hj"> L2范数</strong>创建了更多的稀疏度。</p><p id="9742" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是LR的最终目标函数，现在将使用SGD算法求解。通过观察这个方程，我们可以知道，它是一个损失项和一个正则项之和。这种损失也称为逻辑损失，是0-1损失函数的近似值(见下图)。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es nj"><img src="../Images/c4c2981873622d99b90fa78a2812ed24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*_KElZyFLT4own_flvIhnFA.png"/></div></figure><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es nk"><img src="../Images/a63586ab71e2fa8c95f8be064d0a9798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TK-7rI2K6C7PxqY9stf18g.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">显示逻辑损失和0–1损失的图表</figcaption></figure><h1 id="eec4" class="ju jv hi bd jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr bi translated">当我们训练和测试LR时，内部会发生什么</h1><p id="db16" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">嗯，当我们用数据训练LR时，目标函数在内部试图最小化log-loss，同时它将不断更新每次迭代的权重值，直到它满足收敛。这是一个图解，显示了在LR的训练阶段通常会发生什么。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es nl"><img src="../Images/53ff219c1bbd77f44c58de76dc21cb09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Dd32-kxnFwBXBizBB04Zig.gif"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">在训练阶段，对每次迭代的逻辑损失减少的简单图形解释</figcaption></figure><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es nl"><img src="../Images/8fd2af20e1def30131b1522b2b99215f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*V6LFVo3UOosfXR7mYl3lQA.gif"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">LR如何在训练期间从每次迭代中学习最佳权重的简单想法</figcaption></figure><p id="55d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们完成了训练阶段，我们将得到<strong class="ih hj"> </strong>最终<strong class="ih hj"> W </strong>值，该值可以根据其类别分离几乎所有的数据。当我们通过任何要测试的点时，该点将被代入由<strong class="ih hj"> W </strong>形成的平面的方程中，并且获得的值将被传递给sigmoid函数，该函数将提供最终的分类结果。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es nm"><img src="../Images/8b2eea278e290794871fba77c6a58c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*gvw-cHeqHfGTud2dR4K3Ew.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">预测每个测试点的类别值的数学计算</figcaption></figure><h1 id="edad" class="ju jv hi bd jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr bi translated">时间和空间的复杂性</h1><p id="8b33" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在训练阶段，将通过利用训练数据中的每一点来使用目标函数优化。假设我们在具有'<em class="jm"> d </em>'特征的列车数据中有'<em class="jm"> n </em>个点。那么列车时间复杂度为<strong class="ih hj"> O( <em class="jm"> nd </em> ) </strong></p><p id="94ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在测试期间，我们只需要存储一个最终的权重向量，它是一个长度为'<em class="jm"> d </em>'的数组。因此测试空间的复杂度是O(d)。对于单个测试点，我们将进行“<em class="jm"> d </em>”乘法和一次加法，之后我们将传递给sigmoid函数。因此时间复杂度为O(d)。</p><p id="387c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于当'<em class="jm"> d </em>'小时，运行时间和空间复杂度非常小，所以LR可以非常容易地实现低延迟要求。</p><h1 id="c39a" class="ju jv hi bd jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr bi translated"><strong class="ak">了解Sklearn的LR </strong></h1><p id="b8ce" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">Sklearn是一个python开源库，它为建模ML分类、回归和聚类提供了完整的功能支持，这意味着大多数算法都已经预定义好了，并且在sklearn中可用。</p><p id="bf61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了应用LR，我们通常不喜欢显式地编写整个工作算法。我们简单地从sklearn包中实例化LR类，并对其参数进行处理，以便它能够为当前数据提供最佳拟合。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es nn"><img src="../Images/e75ff5e2d4b45ee045436710199c2787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*9mSmuK9Znt12tbXrXOiHrg.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">LR的Sklearn实现；所有粗略加粗的<strong class="bd jw"><em class="no"/></strong>均为参数名称，所有标有<strong class="bd jw"> <em class="no">绿色</em> </strong>的数值均为默认值。</figcaption></figure><p id="abc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，这里我解释的是sklearn的<strong class="ih hj"> 0.21.3版本</strong>。如果你正在寻找更高版本或更老版本，请在这里浏览sklearn的文档<a class="ae ln" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="edbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地理解这一点，我正在考虑一个名为心脏病UCI的简单数据集，它包含13个特征和一个目标变量，将为患者提供他/她是否患有心脏病的信息。这个数据是从这个<a class="ae ln" href="https://www.kaggle.com/ronitf/heart-disease-uci" rel="noopener ugc nofollow" target="_blank"> kaggle链接</a>中获取的。要了解更多数据，请<a class="ae ln" href="https://www.kaggle.com/ronitf/heart-disease-uci" rel="noopener ugc nofollow" target="_blank">点击此处</a>。完成了所有需要的探索性数据分析和数据预处理，并将训练数据、测试数据分别存储在变量X_train和X_test中。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es nj"><img src="../Images/904b63fcadb401b670e1eada0ce80df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*heS-m-vhx79QTPMcXKCAXQ.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">逻辑回归的目标函数。</figcaption></figure><p id="6492" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用上述函数，我们需要获得<strong class="ih hj">‘W’</strong>的最佳值，这样这些值就可以对数据进行分类(别忘了我们的目标)。正如我们所讨论的，为了在任何数据上获得最佳的<strong class="ih hj">‘W’</strong>，有必要调整参数，这些参数可以是λ、正则化中的范数类型、最大迭代次数、收敛迭代的容差等。令人欣慰的是，sklearn使我们能够完成所有这些操作，所以我在这里解释sklearn的LR和目标函数是如何相关的，并且让我们看看这些参数将如何影响我们的模型效率。</p><p id="bd88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为了操作‘λ’值，我们在sklearn的LR中有‘C’</strong></p><p id="0262" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们对“λ”的详细讨论，为了在低偏差和高方差之间获得完美的平衡，其值应该是适当的，该变量在sklearn中作为“C”提供，保持关系<strong class="ih hj">λ= 1/C。</strong>让我们尝试一些值，观察它如何影响模型欠拟合/过拟合。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="e6e9" class="nu jv hi nq b fi nv nw l nx ny">#case 1:<br/># 'λ' = 0.001 checking with small value<br/>Lambda = 0.001<br/>clf = LogisticRegression(C=1/Lambda ) #instantiating LR into "clf"<br/>clf.fit(X_train, y_train)<br/>evaluate_this_model(clf)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es nz"><img src="../Images/247e295c7a2b769ed707be40f687b32a.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*BK2MK_NbvSzFG9PRl_Z-rw.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">λ = 0.001时列车和试验数据的损失</figcaption></figure><p id="a67f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们认为λ = 0.001是相对较小的值，因此当您观察测试和训练数据集的对数损失时，我们可以实现很小的过拟合。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="306f" class="nu jv hi nq b fi nv nw l nx ny">#Case 2:<br/># 'λ' = 100000 checking with large value<br/>Lambda = 100000<br/>clf = LogisticRegression(C=1/Lambda) #instantiating LR into "clf" <br/>clf.fit(X_train, y_train)<br/>evaluate_this_model(clf)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es oa"><img src="../Images/af5060ffde6f2145501cc15d1f446849.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*AbS8D43lzn7Umio5gA1-Iw.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">λ = 100000时列车和试验数据的损失</figcaption></figure><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ob"><img src="../Images/8df8b764eb5430086fc52a866dddca94.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*l29vwDgeN-VwynwPmT39ZA.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">λ = 100000时的特征权重</figcaption></figure><p id="d8b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我为“λ”选择了一个非常大的值，以显示它实际上是如何欠拟合的，我们可以看到测试集和训练集的损失都非常高，还可以观察到权重向量几乎所有的值都非常接近于零。</p><p id="dc48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">选择权重向量的范数</strong></p><p id="7caa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在sklearn中，默认情况下正则项<strong class="ih hj">‘λ’</strong>将按比例增加<strong class="ih hj"> W </strong>的L2范数，但当您设置<strong class="ih hj">LogisticRegression(penalty =‘L1’)</strong>时，这也可以选择为L1范数，然后正则项将如下。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es oc"><img src="../Images/846e8248051861af8ed9db67bc2939fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*Z11V3GSwqrDWDgEe_g1Ymw.png"/></div></figure><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es od"><img src="../Images/9a3322bcb45877930c293fb49ee39fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*Z6mUAuO2VioSFqnmZ_j1Og.png"/></div></figure><p id="4eae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看这些规范如何影响最终的预测权重向量<strong class="ih hj">‘W’</strong>。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="e755" class="nu jv hi nq b fi nv nw l nx ny">Norm = "l2"  #("l2" is defult norm value)<br/>Lambda = 100<br/>clf = LogisticRegression(penalty=Norm) <br/>clf.fit(X_train, y_train)<br/>evaluate_this_model(clf)</span></pre><p id="00ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，通过保持所有其他参数不变，将罚值改为L1范数，让我们观察两种情况下的结果向量。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="2d84" class="nu jv hi nq b fi nv nw l nx ny">Norm = "l1" <br/>Lambda = 100<br/>clf = LogisticRegression(penalty=Norm,C=1/Lambda) <br/>clf.fit(X_train, y_train)<br/>evaluate_this_model(clf)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es oe"><img src="../Images/46ecc9d016a6c24dfc0247ec740e1723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uhIWernFtORT7l19fIsWGg.png"/></div></div></figure><p id="809f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看着上面两个向量，我们可以意识到当penalty= 'l1 '时有更多的零。这将证明L1范数将通过使所有不太重要的特征的权重为零而导致更多的稀疏。</p><h2 id="f984" class="nu jv hi bd jw of og oh ka oi oj ok ke iq ol om ki iu on oo km iy op oq kq or bi translated"><strong class="ak">选择算法求解器的类型</strong></h2><p id="15a5" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">Sklearn可以用不同的方法求解目标函数。它可以使用不同的算法进行相同的优化。Sklearn允许通过操作<strong class="ih hj">“solver”</strong>参数来选择算法的类型，<strong class="ih hj">“solver”</strong>参数可以采用<strong class="ih hj"><em class="jm">‘Newton-CG’，‘lbfgs’，‘liblinear’，‘sag’，‘saga’，</em> </strong> <em class="jm">这些不同的</em>算法风格来优化目标函数。默认求解器是'<strong class="ih hj"> <em class="jm"> liblinear </em> </strong>'。</p><p id="15a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管目标是相同的，但这些算法在增加惩罚、在小数据集上执行和在多类分类上执行方面不同。但是一个直接的问题是何时使用哪个解算器，这里是从sklearn文档中捕获的很好的总结点，描述了哪个解算器具有哪个优势。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es os"><img src="../Images/e5dd8cf5902e5f61524d8cc123b45e60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*faROyBPk6hDyQ08eJrov4A.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">摘自<a class="ae ln" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank"> sklearn的文档</a> s</figcaption></figure><p id="cfed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设如果我们使用<strong class="ih hj"> solver = 'sag' </strong>它使我们能够惩罚'<strong class="ih hj"> elasticnet </strong>'正则化，这意味着我们可以用<strong class="ih hj"> L1 </strong>和<strong class="ih hj"> L2 </strong>正则化的组合将惩罚添加到损失项中。</p><p id="9284" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该组合可与另一个参数操作，即<strong class="ih hj">“L1 _ ratio”</strong>，其默认值设置为<strong class="ih hj">无。</strong>如果我们设置<strong class="ih hj"> l1_ratio =1 </strong>，那么它相当于设置<strong class="ih hj">罚值= 'l1' </strong>，<strong class="ih hj"> </strong>如果我们设置<strong class="ih hj"> l1_ratio =0 </strong>，那么它类似于<strong class="ih hj">罚值= 'l2 '，</strong>如果<strong class="ih hj"> l1_ratio =0.8 </strong>，我们可以设置(0，1)之间的任何值，这意味着l1规范将有80%的影响，其余20%让我们看一个案例。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="518e" class="nu jv hi nq b fi nv nw l nx ny">Norm = "elasticnet" <br/>Lambda = 100<br/>algo_style="saga"</span><span id="3ce0" class="nu jv hi nq b fi ot nw l nx ny">clf = LogisticRegression(penalty=Norm,C=1/Lambda,l1_ratio=0.3,solver = algo_style)</span><span id="0f42" class="nu jv hi nq b fi ot nw l nx ny">clf.fit(X_train, y_train)<br/>evaluate_this_model(clf)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ou"><img src="../Images/a7bfd62ada14e8519b2a07474033ca4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*9NikYe88iPUJJZUq1LEgWQ.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated"><strong class="bd jw"> L1- </strong>范数在L1+L2组合中贡献30%时的权重向量。</figcaption></figure><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="fd60" class="nu jv hi nq b fi nv nw l nx ny">Norm = "elasticnet" <br/>Lambda = 100<br/>algo_style="saga"</span><span id="ea98" class="nu jv hi nq b fi ot nw l nx ny">clf = LogisticRegression(penalty=Norm,C=1/Lambda,l1_ratio=0.8,solver = algo_style)</span><span id="a061" class="nu jv hi nq b fi ot nw l nx ny">clf.fit(X_train, y_train)<br/>evaluate_this_model(clf)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ov"><img src="../Images/4b08ce6c64665e9d7a09fc160feaf080.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*HQkdYwRz4UOMGj_kpTIGPQ.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">当<strong class="bd jw"> L1- </strong>范数在L1+L2组合中贡献80%时的权重向量。</figcaption></figure><p id="5ffd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着我们增加<strong class="ih hj"> l1_ratio，</strong>稀疏度将增加，因为l1产生的影响-范数将随着<strong class="ih hj"> l1_ratio增加。</strong></p><p id="d852" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">选择截距项</strong></p><p id="06bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">【fit _ intercept】</strong>和<strong class="ih hj">【intercept _ scaling】</strong>在您有任何特定偏好时使用，以增加/减少截距值的影响。当您设置<strong class="ih hj"> fit_intercept = False </strong>时，我们将获得通过原点的方程的权重，这意味着超平面方程中的截距值将为零。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="fa60" class="nu jv hi nq b fi nv nw l nx ny">Lambda = 100<br/>clf =    LogisticRegression(C=1/Lambda,<br/>intercept_scaling=0,fit_intercept=False) <br/>clf.fit(X_train, y_train)<br/>#evaluate_this_model(clf)</span><span id="31ad" class="nu jv hi nq b fi ot nw l nx ny">print("Intercept value is: {} ".format(clf.intercept_))<br/>print("\nAnd weights vaector is : ")<br/>(clf.coef_[0])</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ow"><img src="../Images/8947d76da238e9f54101e8eda9fa8beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*z05osFhoUfxSe04RY4mKkA.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">设置<strong class="bd jw"> fit_intercept=False </strong>时的权重和截距值</figcaption></figure><p id="2a27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">默认情况下，sklearn设置<strong class="ih hj"> fit_intercept = True，</strong>因此，我们将得到一个截距项，注意，我们已经将(L1/L2)正则化为所需的权重，因此，如果我们想增加截距项在最终预测中的影响，我们可以通过使用<strong class="ih hj">“截距_缩放”来放大截距项，这将在内部减少正则化对权重的影响，从而返回减少的权重值。</strong></p><p id="5b17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，当我们增加<strong class="ih hj">截距缩放</strong>值时，截距的影响会增加，从而间接减少权重的影响。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="c17d" class="nu jv hi nq b fi nv nw l nx ny">Lambda = 100<br/>clf = LogisticRegression(C=1/Lambda,intercept_scaling=100,fit_intercept=True) <br/>clf.fit(X_train, y_train)<br/>#evaluate_this_model(clf)</span><span id="5cf1" class="nu jv hi nq b fi ot nw l nx ny">print("Intercept value is: {} ".format(clf.intercept_))<br/>print("\nAnd weights vaector is : ")<br/>(clf.coef_[0])</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ox"><img src="../Images/40b98a67a2a27a3c320f3c9b46080aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*uuGHA8u6S918ZzQ-g6zGKw.png"/></div></figure><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="399f" class="nu jv hi nq b fi nv nw l nx ny">Lambda = 100<br/>clf = LogisticRegression(C=1/Lambda,intercept_scaling=10000000,fit_intercept=True) <br/>clf.fit(X_train, y_train)<br/>#evaluate_this_model(clf)</span><span id="2866" class="nu jv hi nq b fi ot nw l nx ny">print("Intercept value is: {} ".format(clf.intercept_))<br/>print("\nAnd weights vaector is : ")<br/>(clf.coef_[0])</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es oy"><img src="../Images/60ff76936b4d76dd522eefa7b976f7bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*sbbYm4FRlfDGFQfAaYRErQ.png"/></div></figure><p id="403b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看看上面的两种情况，当我们将<strong class="ih hj"> intercept_scaling </strong>值更改为更高的值时，权重向量中的值减少了，这意味着截距项对减少权重的影响增加了。</p><p id="3310" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">控制迭代</strong></p><p id="fc7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练过程中，该算法试图使损失最小化。它总是在计算当前迭代与其前一次迭代的损失之间的差异时检查其收敛性，该残值被称为容差。</p><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es oz"><img src="../Images/b0904c61f29e7cbd196214348194e6a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*-QJQSwO-2sw1WVK37kW9mQ.png"/></div></figure><p id="0962" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果算法满足给定的容差值，则它停止在该特定迭代中训练。在sklearn中，公差的默认值由<strong class="ih hj"> tol=0.0001、</strong>给出，如果我们给出大的<strong class="ih hj">‘tol’</strong>值，它将提前停止，因此可能导致错误的分类，如果我们选择很小的值，那么算法将花费更多的时间来收敛。我们通常使用公差的默认值。</p><p id="745b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以使用<strong class="ih hj"> max_iter </strong>参数选择最大迭代次数，当我们拥有大量训练数据时，我们通常会增加迭代次数，默认值为<strong class="ih hj"> max_iter =100。</strong></p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="21a1" class="nu jv hi nq b fi nv nw l nx ny">Lambda = 100<br/>clf = LogisticRegression(C=1/Lambda,max_iter =1000, tol=1e-3) <br/>clf.fit(X_train, y_train)<br/>evaluate_this_model(clf)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es pa"><img src="../Images/d49358f135aab759464ffea9d9b27e19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*vB46nV3SX3X2S73AGITL2A.png"/></div></figure><p id="1d72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的例子中，我们给出了<strong class="ih hj"> max_iter =1000 </strong>和<strong class="ih hj"> tol=1e-3，</strong>这意味着算法将有机会达到1000次迭代以获得给定的收敛值。</p><p id="8581" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看如果我给出更高的公差值会发生什么</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="cf43" class="nu jv hi nq b fi nv nw l nx ny">Lambda = 100<br/>clf = LogisticRegression(C=1/Lambda,max_iter =1000, tol=3 ) <br/>clf.fit(X_train, y_train)<br/>evaluate_this_model(clf)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es pb"><img src="../Images/57fae6b38476362cf4b3f834e5de6afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*zJXBIXDzICu-j2YhW9_nTg.png"/></div></div></figure><p id="f093" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">观察损失和权重向量值，这里我给了<strong class="ih hj"> tol = 3，</strong>这是一个非常大的值，因此这在第一次迭代中就停止了算法，从而导致更高的损失值和零权重。</p><p id="309d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">处理不平衡数据集</strong></p><p id="4b07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常，当我们有不平衡的数据时，我们需要通过应用过采样/欠采样等技术来处理它，当我们使用sklearn库进行建模时，我们可以使用<strong class="ih hj"> class_weight </strong>参数开发相同的平衡效果。</p><p id="3aed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当数据存在不平衡类时，我们将设置<strong class="ih hj">class _ weight</strong>= '<strong class="ih hj"><em class="jm">' balanced '</em></strong>。因此模型将假设它符合平衡数据。该参数也接受dict格式的输入<strong class="ih hj">class _ weight =</strong>{ class _ label:weight },在这里我们可以明确定义类的平衡比例。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="78f2" class="nu jv hi nq b fi nv nw l nx ny">clf = LogisticRegression(class_weight = 'balanced') <br/>clf.fit(X_train, y_train)<br/>evaluate_this_model(clf)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es pc"><img src="../Images/f5f5519567615c7ddc09086d3d1d271e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*C_8ztul42KoRiSCg8hLcNw.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">当<strong class="bd jw">class _ weight =‘平衡’时的模型重量和损失</strong></figcaption></figure><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="010c" class="nu jv hi nq b fi nv nw l nx ny">clf = LogisticRegression(class_weight = None) <br/>clf.fit(X_train, y_train)<br/>evaluate_this_model(clf)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es pd"><img src="../Images/9c8a291709f0fcbd0874e5abffb99f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*ZRj6vQ70pcUzaKaiCZG8Zw.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">当<strong class="bd jw"> class_weight = None </strong>时的模型重量和损失</figcaption></figure><p id="507b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们无法观察到上述两种情况之间的任何显著差异，因为幸运的是，我们的数据集已经是平衡的。如果没有，我们可以使用<strong class="ih hj"> class_weight来增加弱职业的力量。</strong></p><p id="7cbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">sk learn LR中的其他参数</strong></p><p id="5318" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">双重:</strong></p><p id="9315" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们看到的目标函数称为原始公式，还有另一个使用<strong class="ih hj">拉格朗日乘数</strong>的LR目标函数公式，也称为<strong class="ih hj">对偶公式</strong>。在sklearn中，通过使用<strong class="ih hj">“dual”</strong>，我们可以方便地使用对偶和原始公式，这也是一个函数参数。通过设置“<strong class="ih hj"> dual = True </strong>”，该算法求解对偶公式，默认情况下为<strong class="ih hj"> False </strong>，这意味着它使用原始公式。通常，当样本数量&gt;特征数量时，我们更喜欢<strong class="ih hj"> dual=False </strong>。请注意，对偶公式仅在<strong class="ih hj">惩罚=‘L2’</strong>和<strong class="ih hj">求解器=‘lib linear’</strong>时实施</p><p id="ea5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> n_jobs </strong>:</p><p id="3940" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该参数提供了并行运行拟合作业的功能。如果您选择<strong class="ih hj"> n_jobs = 2 </strong>，那么您系统中的两个内核将并行执行相同的任务。当您选择<strong class="ih hj"> n_jobs = -1 </strong>时，系统中的所有内核将并行工作<strong class="ih hj"> </strong>，从而有助于减少计算时间。</p><p id="03a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机_状态:</strong></p><p id="b770" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这确保了算法控制随机性，我们给<strong class="ih hj"> random_state </strong>的值被用作随机数生成器的种子。这将确保算法中涉及的所有随机性都以相同的顺序生成。</p><p id="c996" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多_类</strong>:</p><p id="58de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们有一个二进制类标签，那么sklearn会自动用one vs rest(ovr)策略拟合数据。如果我们的数据中有多标签，那么我们选择“<strong class="ih hj"> <em class="jm">【多项式】</em> </strong>选项，在内部尝试减少多项式对数损失。</p><p id="3e9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">啰嗦:</strong></p><p id="acc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此参数用于获取算法的详细程度。它有助于显示优化过程中生成的消息。我们可以传递一个整数值给它，如果我们选择大的整数值，我们会看到更多的消息。</p><p id="8edf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">热启动</strong></p><p id="48dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们之前讨论的那样，为了确定最佳模型，我们需要通过使用sklearn的网格搜索cv来试验用多个超参数值和正则化值进行拟合，它对不同值的相同数据集重复拟合估计量，因此，如果我们想要在当前学习中重复使用以前的模型学习会怎样。默认情况下，当您设置<strong class="ih hj"> warm_start = True </strong>时，它可能被设置为<strong class="ih hj"> False。</strong></p><p id="c9c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，逐一试验所有这些参数确实是一项很大的任务。因此，我们选择sklearn提供的任何CV技术，我们将一次性给出一组值。该CV算法将从提供的值中返回最佳拟合。看看下面的代码。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="4921" class="nu jv hi nq b fi nv nw l nx ny">parameters={'C':[10**-6,10**-5,10**-3,10**-4, 10**-2, 10**-1,10**0, 10**2, 10**3,10**4,10**5,10**6],<br/>            'penalty':['l1','l2'],<br/>            'tol':[0.0001,1e-4,1e-5,0.01],<br/>            'fit_intercept':[True,False],<br/>            'intercept_scaling':[0.1,0.01,1,10],<br/>            'warm_start': [True,False]<br/>            } #Setting all parameters in a single pipeline</span><span id="5c6b" class="nu jv hi nq b fi ot nw l nx ny">clf_log = LogisticRegression(n_jobs=-1)</span><span id="ba34" class="nu jv hi nq b fi ot nw l nx ny">clf = GridSearchCV(clf_log, parameters, cv=5, scoring='neg_log_loss',return_train_score =True,n_jobs=-1,verbose=5)<br/>clf.fit(X_train, y_train)</span><span id="2c6a" class="nu jv hi nq b fi ot nw l nx ny">train_loss= clf.cv_results_['mean_train_score']<br/>cv_loss = clf.cv_results_['mean_test_score']</span></pre><p id="ad57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在交叉验证之后，GridSearchCV返回所有提供的参数中的最佳匹配。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="810f" class="nu jv hi nq b fi nv nw l nx ny">clf = clf.best_estimator_<br/>clf</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es pe"><img src="../Images/27819598ab81a9d3274b77f151490c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*ZxL5H82famDtecQzix2AqA.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">来自GridSearchCV的最佳拟合</figcaption></figure><p id="e0ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型再次用相同的参数值训练，并且也用看不见的数据测试。</p><p id="c133" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">详细代码请参考<a class="ae ln" href="https://github.com/pothabattulasantosh/sklearnLR_on_heartDisease_dataset" rel="noopener ugc nofollow" target="_blank">这个</a> github链接，我在这个例子中已经解释过了，也请参考<a class="ae ln" href="https://scikit-learn.org/stable/user_guide.html" rel="noopener ugc nofollow" target="_blank">这里</a>访问sklearn的官方文档。</p><h1 id="9398" class="ju jv hi bd jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn lt kp kq kr bi translated">现在让我们将LR应用于真实的降雨数据，看看它是如何工作的</h1><p id="f8c5" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在我开始这篇博客时，我解释了一些假降雨的例子，我想以一个类似的降雨例子来结束。</p><p id="13e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们处理真实的降雨数据，训练和测试LR模型，并使用相同的python库实现对预测的全面解释。这里我在考虑澳大利亚的降雨记录数据。</p><p id="5dbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个数据取自这个卡格尔<a class="ae ln" href="https://www.kaggle.com/jsphyg/weather-dataset-rattle-package" rel="noopener ugc nofollow" target="_blank"> <em class="jm">环节</em> </a> <em class="jm">。</em>该数据集包含来自众多澳大利亚气象站的24个变量的每日天气观测。这里我们的目标是预测明天是否会下雨？</p><p id="5d88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在这个<a class="ae ln" href="https://www.kaggle.com/jsphyg/weather-dataset-rattle-package/version/2" rel="noopener ugc nofollow" target="_blank">链接</a>上探索关于数据的更多细节</p><p id="05eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在应用任何模型之前，通过了解数据的大小、其具有的特征类型、数据是否平衡/不平衡、任何异常值或缺失值、需要的任何特征缩放/特征变换等来分析数据以获得更好的理解是非常重要的。，所有这些提到的步骤通常涵盖当您执行探索性数据分析时，数据预处理，这是建模数据之前非常重要的阶段。我对所有数据进行了预处理，并分成训练集和测试集。下面是使用sklearn的Gridsearchcv为逻辑回归进行超参数调整的代码</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="f376" class="nu jv hi nq b fi nv nw l nx ny">#taking different set of values for C where C = 1/λ<br/>parameters={'C':[10**-6,10**-5,10**-4, 10**-2, 10**0, 10**2, 10**3]}</span><span id="da02" class="nu jv hi nq b fi ot nw l nx ny">#for plotting<br/>log_c = list(map(<strong class="nq hj">lambda</strong> x : float(math.log(x)),parameters['C']))</span><span id="7295" class="nu jv hi nq b fi ot nw l nx ny">#using sklearn's LogisticRegression classifier with L2- norm<br/>clf_log = LogisticRegression(penalty='l2') </span><span id="db8f" class="nu jv hi nq b fi ot nw l nx ny"># hyperparametertunig with 5 fold CV using grid search<br/>clf = GridSearchCV(clf_log, parameters, cv=5,scoring='neg_log_loss',return_train_score =<strong class="nq hj">True</strong>)<br/>clf.fit(X_train, y_train)<br/><br/>train_loss= clf.cv_results_['mean_train_score']<br/>cv_loss = clf.cv_results_['mean_test_score'] <br/></span><span id="c579" class="nu jv hi nq b fi ot nw l nx ny">#A function defined for plotting cv and trian errors <br/>plotErrors(k=log_c,train=train_loss,cv=cv_loss)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es pf"><img src="../Images/4afbcf1826827b7b3f8e93ad276cce75.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*eANNB4gcdBErkxUwf2omUA.png"/></div></figure><p id="eb9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过查看图表，我们可以观察到负对数损耗在超参数调谐期间是如何增加的。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="f234" class="nu jv hi nq b fi nv nw l nx ny">clf = clf.best_estimator_<br/><em class="jm">#Trainig the model with the best value of C</em><br/>clf.fit(X_train, y_train)<br/></span></pre><p id="c095" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从GridsearchCV中，考虑具有更好的偏差和方差权衡的模型，并训练该最优模型。</p><p id="3ea3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在训练部分已经完成，让我们检查测试数据上的模型性能。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="afb6" class="nu jv hi nq b fi nv nw l nx ny"><em class="jm">#Printing the log-loss for both trian and test data</em><br/>train_loss = log_loss(y_train, clf.predict_proba(X_train)[:,1])<br/>test_loss  =log_loss(y_test, clf.predict_proba(X_test)[:,1])<br/><br/><br/>print("Log_loss on train data is :<strong class="nq hj">{}</strong>".format(train_loss))<br/>print("Log_loss on test data is :<strong class="nq hj">{}</strong>".format(test_loss))</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es pg"><img src="../Images/a1cda2334921cb937383fcacb8279ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*pN9h4xvjidGCmD4ilhBDqw.png"/></div></figure><p id="8738" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过查看上述对数损失值，我们可以知道模型没有出现低基数/高方差，但是使用对数损失我们无法知道模型有多好，因此使用AUC指标进行检查</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="b949" class="nu jv hi nq b fi nv nw l nx ny"><em class="jm">#Plotting AUC </em> <br/>train_fpr, train_tpr, thresholds = roc_curve(y_train,clf.predict_proba(X_train)[:,1]) </span><span id="37ed" class="nu jv hi nq b fi ot nw l nx ny">test_fpr, test_tpr, thresholds = <br/>roc_curve(y_test, clf.predict_proba(X_test)[:,1]) </span><span id="9411" class="nu jv hi nq b fi ot nw l nx ny">plt.plot(train_fpr, train_tpr, label="trainAUC="+str(auc(train_fpr,train_tpr)))<br/> <br/>plt.plot(test_fpr, test_tpr, label="test AUC ="+str(auc(test_fpr, test_tpr))) <br/>plt.legend() <br/>plt.xlabel("FPR") <br/>plt.ylabel("TPR") <br/>plt.title("ROC for Train and Test data with best_fit") plt.grid() plt.show()</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es ph"><img src="../Images/23fd38b5f8db291710f4604afe654d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*ocICqdah7f7HLDltvr7phw.png"/></div></figure><p id="ec28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在查看test-AUC时，我们可以理解，对于新的查询点，模型有87.42%的机会能够预测其原始值。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="8fe7" class="nu jv hi nq b fi nv nw l nx ny">clf.coef_[0]</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es pi"><img src="../Images/0ec821b738fb0e59a07375239c41e19e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*m-6RLrJsjlrbj_6n2DMh7A.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">权重向量数组。</figcaption></figure><p id="1c2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的数组表示训练阶段完成后获得的最终权重向量。</p><p id="515f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些值将帮助我们解释特征的重要性，这意味着权重值越大，在分类任务中的影响就越大。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="e263" class="nu jv hi nq b fi nv nw l nx ny">feature_weights =sorted(zip(clf.coef_[0],column_names),reverse=<strong class="nq hj">True</strong>)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div class="er es pj"><img src="../Images/6ff451409e3fc6d8d059258b543a863e.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*bkJ3oL_QpcYDEBLp9ETXsA.png"/></div></figure><p id="ff73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面是根据权重值排序的要素数组。权重值越高，意味着该特征越重要。</p><p id="3b38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在只需通过发送一个具有以下特征值的新查询点来解释模型结果。</p><pre class="ky kz la lb fd np nq nr ns aw nt bi"><span id="ac17" class="nu jv hi nq b fi nv nw l nx ny"><em class="jm">#Giving one query point here</em>  <br/>MinTemp   = 26.2 <br/>MaxTemp   = 31.7 <br/>Rainfall   = 2.8 <br/>Evaporation   = 5.4 <br/>Sunshine   = 3.5 <br/>WindGustDir   = "NNW" <br/>WindGustSpeed   = 57 <br/>WindDir9am   = "NNW" <br/>WindDir3pm   = "NNW" <br/>WindSpeed9am   = 20 <br/>WindSpeed3pm   = 13 <br/>Humidity9am   = 81 <br/>Humidity3pm   = 95 <br/>Pressure9am   = 1007.2 <br/>Pressure3pm   = 1006.1 <br/>Cloud9am   = 7 <br/>Cloud3pm   = 8 <br/>Temp9am   = 28.8 <br/>Temp3pm   = 25.4 <br/>RainToday   ="Yes"</span><span id="e4fc" class="nu jv hi nq b fi ot nw l nx ny">point = [MinTemp,MaxTemp,Rainfall,<br/>         Evaporation,Sunshine,WindGustDir,<br/>         WindGustSpeed,WindDir9am,WindDir3pm,<br/>         WindSpeed9am,WindSpeed3pm,Humidity9am,<br/>         Humidity3pm,Pressure9am,Pressure3pm,<br/>         Cloud9am,Cloud3pm,Temp9am,Temp3pm,RainToday]<br/><br/>xq=dict()<br/><strong class="nq hj">for</strong> i,name <strong class="nq hj">in</strong> enumerate(column_names):<br/>    xq[name]=point[i]</span><span id="f9b7" class="nu jv hi nq b fi ot nw l nx ny">"""<strong class="nq hj">will_rain_fall_for_this_conditions</strong> is function defined to do all pre-processing steps and to predict output from classifier"""</span><span id="aeb8" class="nu jv hi nq b fi ot nw l nx ny">will_rain_fall_for_this_conditions(xq)</span></pre><figure class="ky kz la lb fd lc er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es pk"><img src="../Images/7952593d6d2fbcf7c50982c5acc0343f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Wwh2XB3lL_lWeyU5tnaiA.png"/></div></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">打印带有置信度值的分类器结果，并基于特征重要性解释结果。</figcaption></figure><p id="c867" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用特征权重值，我们可以将结果打印给最终用户，如上所示。</p><p id="2db2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ln" href="https://github.com/pothabattulasantosh/Rainfall-prediction-in-Australia/blob/master/Rainfall_analysis.ipynb" rel="noopener ugc nofollow" target="_blank">单击此处</a>查看我的GitHub档案中该示例的完整源代码，其中包含探索性数据分析、数据预处理和建模的所有代码。</p><p id="fe3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ol class=""><li id="996c" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc pl mc md me bi translated"><a class="ae ln" href="https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/</a></li><li id="11a8" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc pl mc md me bi translated">https://www.youtube.com/watch?v=yIYKR4sgzI8&amp;list = plblh 5 jkoolukxzep 5 ha 2d-Li 7 ijkhfxse</li><li id="393a" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc pl mc md me bi translated">【https://scikit-learn.org/stable/user_guide.html T4】</li></ol></div></div>    
</body>
</html>