<html>
<head>
<title>Gradient descent: Why and How ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降:为什么和如何？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-why-and-how-e369950ae7d3?source=collection_archive---------2-----------------------#2019-09-30">https://medium.com/analytics-vidhya/gradient-descent-why-and-how-e369950ae7d3?source=collection_archive---------2-----------------------#2019-09-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="bc1d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">梯度下降及其变体的理论方法</h2></div><p id="1c53" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最小化代价函数在机器学习模型中非常重要。这里梯度下降是用于最小化成本函数的迭代技术。成本函数是预测误差(Y_Actual-Y_Predicted)</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/2aea2486ba41ec40811620c448f66673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4x7dwg5RCHhJoAI4sLlvlQ.jpeg"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">梯度下降</figcaption></figure><p id="2273" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你看到上面的图片，这个人在山顶或半山腰，他想用最少的步数下山。这个人反复地(一小步一小步地)这样做，直到到达山的底部。</p><p id="d73e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度下降也一样。这里，山是数据的维度，步骤是学习率，底部(目标点)是误差最小的成本函数。</p><p id="d51f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">学习率:- </strong></p><p id="d15a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的例子中，达到目标点(最佳点)所采取的步骤被称为学习率</p><p id="b9b4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">如何选择学习率？</strong></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kj"><img src="../Images/ff0c76b50b0e7d2be7f7ceaf2c6f1a98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*OfYVPAl2ZEBeBHlTfq_glA.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">学习率比较</figcaption></figure><p id="f1cc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果学习率太高，那么它会跳至凸函数，但不会有效地达到局部最小值</p><p id="0476" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果学习率太小，那么它会达到局部最小值，但会花费太多时间</p><p id="b9f9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们必须选择学习率，使得成本函数同时降低。在此绘制成本函数与间隔的关系图，以检查正确的学习率。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kj"><img src="../Images/6087a5126a995be0ec2664e93493713a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*LhuI0ysvsRGfO8wy2iEOQA.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">在迭代中选择好的学习率</figcaption></figure><blockquote class="kk kl km"><p id="9d93" class="ix iy kn iz b ja jb ij jc jd je im jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated">在梯度下降中，我们计算所有数据点的成本。并在每次迭代中更新权重。例如，如果我们在训练中有10，000个点，那么我们仅在一次迭代中计算W(权重)和成本。</p></blockquote><p id="932b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度下降的问题是，假设山有两个最小点，第一个到达的最小点被称为局部最小值，第二个(最后一个)最小点被称为全局最小值。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kr"><img src="../Images/8d4fa03a824cdd4adfdf538f936eca1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*Z-xJ5TAVGXRHK-4R29Pbfw.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">积分的最小值和最大值[图片来源:<a class="ae ks" href="https://www.datasciencecentral.com/profiles/blogs/optimization-techniques-finding-maxima-and-minima" rel="noopener ugc nofollow" target="_blank">点击_此处</a> ]</figcaption></figure><p id="e0bb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">优化成本函数使其达到全局最小值总是更好的。为此，我们必须使用另一种梯度下降技术。这些措施如下</p><ol class=""><li id="300a" class="kt ku hi iz b ja jb jd je jg kv jk kw jo kx js ky kz la lb bi translated"><strong class="iz hj">批量梯度下降</strong></li><li id="fd72" class="kt ku hi iz b ja lc jd ld jg le jk lf jo lg js ky kz la lb bi translated"><strong class="iz hj">随机梯度下降</strong></li><li id="8f94" class="kt ku hi iz b ja lc jd ld jg le jk lf jo lg js ky kz la lb bi translated"><strong class="iz hj">小批量梯度下降</strong></li></ol><h1 id="5c3a" class="lh li hi bd lj lk ll lm ln lo lp lq lr io ls ip lt ir lu is lv iu lw iv lx ly bi translated"><strong class="ak"> 1。批量梯度下降:- </strong></h1><p id="717c" class="pw-post-body-paragraph ix iy hi iz b ja lz ij jc jd ma im jf jg mb ji jj jk mc jm jn jo md jq jr js hb bi translated">它也被称为香草梯度下降技术。在这种技术中，我们计算每个观测值的误差，但是仅当所有观测值都完成评估时才执行更新</p><p id="4930" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">批量梯度下降技术是计算昂贵的技术。它非常慢，而且大型数据集无法加载到内存中</p><h1 id="9c60" class="lh li hi bd lj lk ll lm ln lo lp lq lr io ls ip lt ir lu is lv iu lw iv lx ly bi translated"><strong class="ak"> 2。随机梯度下降:- </strong></h1><p id="bc96" class="pw-post-body-paragraph ix iy hi iz b ja lz ij jc jd ma im jf jg mb ji jj jk mc jm jn jo md jq jr js hb bi translated">SGD在每次迭代中对每个观察值执行权重更新。在这里，我们首先需要打乱数据集，以便我们得到完全随机的数据集。</p><p id="6db6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于我们的数据集是随机的数据集，并且针对每个示例更新了误差和权重，梯度权重达到了全局最小值，而不是停留在局部最小值。</p><p id="d1d0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">SGD比批量梯度技术更快。这种技术在计算上也是多余的，因为它一次计算一个例子</p><h1 id="61c7" class="lh li hi bd lj lk ll lm ln lo lp lq lr io ls ip lt ir lu is lv iu lw iv lx ly bi translated">3.小批量梯度下降:-</h1><p id="a219" class="pw-post-body-paragraph ix iy hi iz b ja lz ij jc jd ma im jf jg mb ji jj jk mc jm jn jo md jq jr js hb bi translated">该技术是sgd和批量梯度技术的结合。首先，它将数据集分成小批，然后对每批执行更新。这里，当我们在批量样本中获取数据时，它消除了噪声，即权重更新的方差。与分批法和sgd法相比，这是一种快速的技术。</p><blockquote class="kk kl km"><p id="3960" class="ix iy kn iz b ja jb ij jc jd je im jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated"><em class="hi">同时查看下一部分，即该技术的编程方法</em></p></blockquote></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="343d" class="lh li hi bd lj lk ml lm ln lo mm lq lr io mn ip lt ir mo is lv iu mp iv lx ly bi translated">参考资料:-</h1><div class="mq mr ez fb ms mt"><a href="https://www.appliedaicourse.com" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hj fi z dy my ea eb mz ed ef hh bi translated">应用课程</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">我们知道转行是多么具有挑战性。我们的应用人工智能/机器学习课程被设计为整体学习…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">www.appliedaicourse.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh kd mt"/></div></div></a></div><div class="mq mr ez fb ms mt"><a rel="noopener follow" target="_blank" href="/mindorks/an-introduction-to-gradient-descent-7b0c6d9e49f6"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hj fi z dy my ea eb mz ed ef hh bi translated">梯度下降导论</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">梯度下降</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">梯度Descentmedium.com</p></div></div><div class="nc l"><div class="ni l ne nf ng nc nh kd mt"/></div></div></a></div><div class="mq mr ez fb ms mt"><a rel="noopener follow" target="_blank" href="/@montjoile/an-introduction-to-gradient-descent-algorithm-34cf3cee752b"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hj fi z dy my ea eb mz ed ef hh bi translated">梯度下降算法简介</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">梯度下降是机器学习和深度学习中使用最多的算法之一。</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">medium.com</p></div></div><div class="nc l"><div class="nj l ne nf ng nc nh kd mt"/></div></div></a></div><div class="mq mr ez fb ms mt"><a href="https://developers.google.com/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hj fi z dy my ea eb mz ed ef hh bi translated">减少损失:随机梯度下降</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">预计时间:3分钟在梯度下降，一批是总数量的例子，你用来计算…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">developers.google.com</p></div></div><div class="nc l"><div class="nk l ne nf ng nc nh kd mt"/></div></div></a></div></div></div>    
</body>
</html>