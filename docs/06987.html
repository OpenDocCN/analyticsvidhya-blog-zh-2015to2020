<html>
<head>
<title>Neural Networks Part I: One Neuron</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络第一部分:一个神经元</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-networks-part-i-one-neuron-473d1ac06207?source=collection_archive---------19-----------------------#2020-06-09">https://medium.com/analytics-vidhya/neural-networks-part-i-one-neuron-473d1ac06207?source=collection_archive---------19-----------------------#2020-06-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1ba5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经试着摆弄神经网络有一段时间了。问题是，我们喜欢把神经网络想象成一个黑匣子，在这个黑匣子里会发生一些神奇的事情，你会得到一些很酷的答案。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/261036175a62b80a41c4dcaf810b8783.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5g-073NIm6rkO9qO5nE9tQ.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">神奇的神经网络接管世界…</figcaption></figure><p id="9dfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是有些东西在那个盒子里，有些东西可能会被跳过，以便得到复杂的，理论上的东西。但是这些“东西”对于理解神经网络如何工作或不工作以及为什么工作是必不可少的。</p><p id="1266" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本上，神经网络的工作有点像人脑。我们的神经元相互连接，相互发出指令，最终产生输出。从这个角度来看，神经网络是一样的，但它们使用感知机。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jt"><img src="../Images/8cdc788a310aba3df312554e51bc10f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K8qfq2C2pzu-c3CB.jpg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片由<a class="ae ju" href="https://www.learndatasci.com/" rel="noopener ugc nofollow" target="_blank"> LearnDataSci </a>通过<a class="ae ju" href="https://en.wikipedia.org/wiki/File:Artificial_Neural_Network.jpg" rel="noopener ugc nofollow" target="_blank">维基共享</a> (CC BY-SA 4.0)</figcaption></figure><p id="cfcf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们来预测一下一个超级简单的水瓶的成本。唯一真正改变的是水瓶的大小；这家公司决定不管颜色如何，成本都保持不变。这是一张与瓶子大小相关的成本表:</p><pre class="je jf jg jh fd jv jw jx jy aw jz bi"><span id="ea6a" class="ka kb hi jw b fi kc kd l ke kf">Size(oz) | Cost($)</span><span id="3c7e" class="ka kb hi jw b fi kg kd l ke kf">5        | 2</span><span id="7cf9" class="ka kb hi jw b fi kg kd l ke kf">3        | 1</span><span id="7f6b" class="ka kb hi jw b fi kg kd l ke kf">24       | 30</span></pre><p id="a6fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一张图表:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kh"><img src="../Images/ba3f59d3787315b52c6b708378095cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*LmPUPj9Fk1vGhIrJvVs9rQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">成本与规模</figcaption></figure><p id="d2d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">仅从3个数据点中很难得出任何准确的结论，但总体思路是存在的。</p><p id="90d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本质上，一个感知器的工作方式是它遵循一个函数，通常是一个非常简单的函数，如线性函数:</p><pre class="je jf jg jh fd jv jw jx jy aw jz bi"><span id="298b" class="ka kb hi jw b fi kc kd l ke kf">y = mx + b</span></pre><p id="686a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么x是我们的输入，y是感知机预测的输出。第一步，我们将简单地让一个感知器工作，然后我们可以继续对多个感知器应用相同的概念。</p><p id="d449" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我们有一个遵循线性函数的感知器，和三个点。最终，我们的目标是让这个非常小的神经网络能够输入和输出瓶子的大小，为了做到这一点，我们需要找出“m”(斜率)和“b”(y轴截距)是什么。我们可以这样做。</p><h1 id="05e8" class="ki kb hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">1.得出m和b的随机值</h1><p id="717e" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">随便取个值，大概在1到0之间。这将是我们(非常小的)神经网络的起点。</p><pre class="je jf jg jh fd jv jw jx jy aw jz bi"><span id="4ed7" class="ka kb hi jw b fi kc kd l ke kf">import numpy as np # For random numbers and simple matrix operations<br/></span><span id="acfc" class="ka kb hi jw b fi kg kd l ke kf">class Neuron:</span><span id="0879" class="ka kb hi jw b fi kg kd l ke kf">   def __init__(self):</span><span id="b662" class="ka kb hi jw b fi kg kd l ke kf">      # Initializing.</span><span id="89c7" class="ka kb hi jw b fi kg kd l ke kf">      self.m = np.random.rand()</span><span id="aea7" class="ka kb hi jw b fi kg kd l ke kf">      self.b = np.random.rand()</span><span id="e81f" class="ka kb hi jw b fi kg kd l ke kf">      # Created random values for m and b.</span><span id="8153" class="ka kb hi jw b fi kg kd l ke kf">NN = Neuron()</span></pre><h1 id="ccdd" class="ki kb hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">2.运行神经网络</h1><p id="e423" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">现在，我们将在类中创建另一个函数，它接收一组输入，将它们传递给我们的神经元，然后返回一个输出。这又回到了整个y = mx + b的思想。</p><pre class="je jf jg jh fd jv jw jx jy aw jz bi"><span id="2953" class="ka kb hi jw b fi kc kd l ke kf">import numpy as np # For random numbers and simple matrix operations</span><span id="c561" class="ka kb hi jw b fi kg kd l ke kf">class Neuron:</span><span id="7f00" class="ka kb hi jw b fi kg kd l ke kf">   def __init__(self):</span><span id="7f1f" class="ka kb hi jw b fi kg kd l ke kf">      # Initializing.</span><span id="affc" class="ka kb hi jw b fi kg kd l ke kf">      self.m = np.random.rand()</span><span id="d0a4" class="ka kb hi jw b fi kg kd l ke kf">      self.b = np.random.rand()</span><span id="7246" class="ka kb hi jw b fi kg kd l ke kf">      # Created random values for m and b.</span><span id="8a3d" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   def runThrough(self, inputs):</strong></span><span id="70b1" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      return [self.m * i + self.b for i in inputs]</strong></span><span id="3512" class="ka kb hi jw b fi kg kd l ke kf">data = np.array([</span><span id="c994" class="ka kb hi jw b fi kg kd l ke kf">[5, 2],</span><span id="0d14" class="ka kb hi jw b fi kg kd l ke kf">[3, 1],</span><span id="b963" class="ka kb hi jw b fi kg kd l ke kf">[24, 30]</span><span id="024c" class="ka kb hi jw b fi kg kd l ke kf">])</span><span id="1cd6" class="ka kb hi jw b fi kg kd l ke kf">NN = Neuron()</span><span id="d99e" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">print(NN.runThrough(data[:,0])) # Quick test run</strong></span></pre><h1 id="67a9" class="ki kb hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">3.找出我们做错了什么</h1><p id="e368" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">现在我们需要回溯一下，看看我们错在哪里。这使用了一个误差函数，在这种情况下，我们将使用均方误差。</p><p id="4c55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">均方差(MSE)的基本工作原理是计算每个不同点之间差异的平方，然后计算它们的平均值。用一种奇特的方式来说:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lk"><img src="../Images/2b318b86300e331b66d77563cc8f945b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IbnY2ECPuCSKA6a7x5aVNA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">均方误差(mean square error)</figcaption></figure><p id="a769" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们在代码中实现这一点:</p><pre class="je jf jg jh fd jv jw jx jy aw jz bi"><span id="3856" class="ka kb hi jw b fi kc kd l ke kf">import numpy as np # For random numbers and simple matrix operations</span><span id="e7c8" class="ka kb hi jw b fi kg kd l ke kf">class Neuron:</span><span id="76af" class="ka kb hi jw b fi kg kd l ke kf">   def __init__(self):</span><span id="cb32" class="ka kb hi jw b fi kg kd l ke kf">      # Initializing.</span><span id="db63" class="ka kb hi jw b fi kg kd l ke kf">      self.m = np.random.rand()</span><span id="d4a6" class="ka kb hi jw b fi kg kd l ke kf">      self.b = np.random.rand()</span><span id="863e" class="ka kb hi jw b fi kg kd l ke kf">      # Created random values for m and b.</span><span id="7422" class="ka kb hi jw b fi kg kd l ke kf">   def runThrough(self, inputs):</span><span id="5d96" class="ka kb hi jw b fi kg kd l ke kf">      # Given inputs, output the NN's guess</span><span id="0f5c" class="ka kb hi jw b fi kg kd l ke kf">      return [self.m * i + self.b for i in inputs]</span><span id="4d7f" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   def MSE(self, inputs, outputs):</strong></span><span id="7839" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      # Given inputs and outputs, provide the MSE</strong></span><span id="9b6c" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      results = self.runThrough(inputs)</strong></span><span id="85bb" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      MeanSquaredErr = 0</strong></span><span id="55ed" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      for i in range(len(results)):</strong></span><span id="ab53" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">         # Compare to outputs</strong></span><span id="43df" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">         MeanSquaredErr += (results[i] - outputs[i])**2</strong></span><span id="809e" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      MeanSquaredErr /= len(results)</strong></span><span id="2d54" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      return MeanSquaredErr</strong></span><span id="2882" class="ka kb hi jw b fi kg kd l ke kf">data = np.array([</span><span id="7623" class="ka kb hi jw b fi kg kd l ke kf">[5, 2],</span><span id="bb05" class="ka kb hi jw b fi kg kd l ke kf">[3, 1],</span><span id="6c4a" class="ka kb hi jw b fi kg kd l ke kf">[24, 30]</span><span id="5f50" class="ka kb hi jw b fi kg kd l ke kf">])</span><span id="0049" class="ka kb hi jw b fi kg kd l ke kf">NN = Neuron()</span><span id="fdff" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">print(NN.MSE(data[:,0], data[:,1])) # Testing MSE function</strong></span></pre><p id="767d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">酷！现在我们已经初始化了一个感知器，正在运行，并使用MSE查看它做错了什么。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ll"><img src="../Images/070f7a0057dbb2760b84658e9f02a63c.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*ZA7tbAjLvhXh6oVkr7u_6A.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">第一次训练！</figcaption></figure><h1 id="8801" class="ki kb hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">4.使用衍生品反向传播</h1><p id="1296" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">现在，这是我可能会回到Tensorflow并收工的部分。但我们不会那么做。</p><p id="1e3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们要扩展我们的MSE方程。我们知道我们的神经网络输出等于:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/3bd0846d1a4085ba4310e0a59abeca5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*C6Tb6NmofwkKBV8y1d5G1g.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">我们神经网络的输出</figcaption></figure><p id="5882" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们把它代入等式。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ln"><img src="../Images/5ef0968d9189266c582cda0ad5ec0dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H2IdNvjBINeZfBSUjfbhXg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">扩展MSE</figcaption></figure><p id="f114" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，关于偏导数，需要记住的最重要的事情是，它们基本上找到了瞬时斜率。有很多不同的规则和复杂的方法可以使用它们，但主要的想法是，给定一个函数中的变量，你可以找到该变量如何影响整体函数。</p><p id="1145" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们使用以下规则来推导均方误差函数:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lo"><img src="../Images/63c1ce3a5404f4b41737d7405fd8308e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Nz-wYwwGtF2rqMnc.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">链式法则</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/70986ebb97adb58116e015ade9d1dd89.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/0*gkecROMJXRIqwVEC.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">权力规则</figcaption></figure><p id="ba6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当取偏导数时，每隔一个变量就被认为是一个常数，而常数在求导时是0。所以我们要做的是，用这个来找出变量m和b如何影响我们的MSE函数，并找出它们的偏导数。</p><p id="6f1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们用链式法则去掉外部函数；在这种情况下，这是均方误差的平方，对于任何x，幂法则表明导数是2x。现在我们会有:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lq"><img src="../Images/fc5630bae41eae51452eae4bf2af2e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pn3uSv__yYl9IyjL7hZ0g.png"/></div></div></figure><p id="e791" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">鉴于:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lr"><img src="../Images/4718edb555031894ed77786c07e84c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tBZvh-z6bSbH8EhpULuIsA.png"/></div></div></figure><p id="9aea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好吧。现在，我们需要为m导出g(x)(我们马上就要做b)。假设y和b都是常数和0，我们剩下-mx，而且，因为m的导数是1，那么g(x)对m的偏导数留给我们-x，耶！</p><p id="8792" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以对b做类似的事情，我们知道mx和y都将被视为常数，所以它们被视为0。b的导数是1，所以我们最终剩下-1作为g(x)对g的偏导数。</p><p id="48d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于m:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ls"><img src="../Images/fc520f12b01fb0e72e57c490bd2bae28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*er3igzW6y_9DQrcg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">m的导数</figcaption></figure><p id="e8f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于b:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lt"><img src="../Images/2be9ab23baad4ac308b5d226a36dfe0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*khmZeAgUtIt1mGFr.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">b的导数</figcaption></figure><p id="162c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这具体地告诉神经网络m和b都必须改变多少。然而，由于过拟合问题和导数只计算瞬时斜率，我们需要一个学习率。学习率乘以我们的导数，就像在检查神经网络是否正常之前，朝着正确的方向迈出了一小步。你不会在没有考虑你是否在正确的方向上(如果你想走出迷宫的话)就在迷宫中冲刺。</p><p id="bec7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们编码这个。注意:simpleError只输出g(x ),所以函数MSE被修改，以减少在求导时的运算次数。修改部分以粗体显示。</p><pre class="je jf jg jh fd jv jw jx jy aw jz bi"><span id="cf3d" class="ka kb hi jw b fi kc kd l ke kf">import numpy as np # For random numbers and simple matrix operations</span><span id="2ef6" class="ka kb hi jw b fi kg kd l ke kf">class Neuron:</span><span id="ae43" class="ka kb hi jw b fi kg kd l ke kf">   def __init__(self):</span><span id="68cf" class="ka kb hi jw b fi kg kd l ke kf">      # Initializing.</span><span id="f065" class="ka kb hi jw b fi kg kd l ke kf">      self.m = np.random.rand()</span><span id="f370" class="ka kb hi jw b fi kg kd l ke kf">      self.b = np.random.rand()</span><span id="8fb1" class="ka kb hi jw b fi kg kd l ke kf">      # Created random values for m and b.</span><span id="994e" class="ka kb hi jw b fi kg kd l ke kf">   def runThrough(self, inputs):</span><span id="9c4b" class="ka kb hi jw b fi kg kd l ke kf">      # Given inputs, output the NN's guess</span><span id="8c84" class="ka kb hi jw b fi kg kd l ke kf">      return [self.m * i + self.b for i in inputs]</span><span id="3b46" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   def simpleError(self, outputE, outputI):</strong></span><span id="14ff" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      # Given one output and one expected output, return</strong></span><span id="1ed9" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      # y - runThorough(inputI)</strong></span><span id="b35f" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      return outputE - outputI</strong></span><span id="78d9" class="ka kb hi jw b fi kg kd l ke kf">   def MSE(self, inputs, outputs):</span><span id="d9c0" class="ka kb hi jw b fi kg kd l ke kf">      # Given inputs and outputs, provide the MSE</span><span id="50c5" class="ka kb hi jw b fi kg kd l ke kf">      results = self.runThrough(inputs)</span><span id="3fd7" class="ka kb hi jw b fi kg kd l ke kf">      MeanSquaredErr = 0</span><span id="dfcf" class="ka kb hi jw b fi kg kd l ke kf">      for i in range(len(results)):</span><span id="e76c" class="ka kb hi jw b fi kg kd l ke kf">         # Compare to outputs</span><span id="45f6" class="ka kb hi jw b fi kg kd l ke kf">         <strong class="jw hj">MeanSquaredErr += (self.simpleError(outputs[i], results[i]))**2</strong></span><span id="35f8" class="ka kb hi jw b fi kg kd l ke kf">MeanSquaredErr /= len(results)</span><span id="53d2" class="ka kb hi jw b fi kg kd l ke kf">      return MeanSquaredErr</span><span id="5c85" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   def derivMB(self, inputs, outputs):</strong></span><span id="7fb2" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      # Similar to MSE, iterate through the inputs and outputs and perform the partial derivative of MSE in terms of m and b.</strong></span><span id="42bf" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      changeM = 0</strong></span><span id="30ee" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      changeB = 0</strong></span><span id="eb50" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      res = self.runThrough(inputs)</strong></span><span id="b808" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      for i in range(len(res)):</strong></span><span id="0c5b" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">         # Add to changeM the derivative</strong></span><span id="606d" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">         changeM += (-2*inputs[i])*self.simpleError(outputs[i], res[i])</strong></span><span id="aca3" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">         changeB += (-2)*self.simpleError(outputs[i], res[i])</strong></span><span id="17a4" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">         changeB /= len(res)</strong></span><span id="0380" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">         changeM /= len(res)</strong></span><span id="1e9a" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      return changeM, changeB</strong></span><span id="88c3" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   def learn(self, inputs, outputs, learningRate=0.001):</strong></span><span id="595c" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      changeM, changeB = self.derivMB(inputs, outputs) # Call the derivative function. This tells us how much to change m and b.</strong></span><span id="8e3d" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      changeM *= learningRate # Making tiny steps towards the solution</strong></span><span id="773d" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      changeB *= learningRate</strong></span><span id="f00c" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      # Now modify m and b by this</strong></span><span id="359c" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      self.m -= changeM</strong></span><span id="c7d5" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">      self.b -= changeB</strong><br/></span><span id="40b4" class="ka kb hi jw b fi kg kd l ke kf">data = np.array([</span><span id="78dd" class="ka kb hi jw b fi kg kd l ke kf">[3, 1],</span><span id="754e" class="ka kb hi jw b fi kg kd l ke kf">[5, 2],</span><span id="f4b8" class="ka kb hi jw b fi kg kd l ke kf">[24, 30]</span><span id="e07f" class="ka kb hi jw b fi kg kd l ke kf">])</span><span id="1d6d" class="ka kb hi jw b fi kg kd l ke kf">NN = Neuron()</span><span id="757f" class="ka kb hi jw b fi kg kd l ke kf">print(NN.runThrough(data[:,0]), data[:,0])</span><span id="72f9" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">import matplotlib.pyplot as plt</strong></span><span id="3415" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">for i in range(100):</strong></span><span id="fc39" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   plt.ion() # Creates interactive plt interface</strong></span><span id="f397" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   plt.cla() # Clear graph</strong></span><span id="d15a" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   NN.learn(data[:,0], data[:,1]) # Initiate learning function</strong></span><span id="8832" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   plt.plot(data[:,0], data[:,1], "ro") # Plot the points</strong></span><span id="4a8e" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   plt.plot(data[:,0], NN.runThrough(data[:,0]), "m-") # Plot the predicted values of those numbers</strong></span><span id="58c3" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   plt.show()</strong></span><span id="ef24" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   print(NN.MSE(data[:,0], data[:,1])) # Print how it's doing, to check whether the MSE is decreasing.</strong></span><span id="db8a" class="ka kb hi jw b fi kg kd l ke kf"><strong class="jw hj">   plt.pause(0.1) # Pause is necessary for interactive interface</strong></span></pre><p id="4791" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以现在，如果你运行上面的代码，你可以看到神经网络试图学习如何适应这些值。您可以尝试学习速率、误差函数，或者根据自己的意愿改变它从for循环中学习的数量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/13d1cdabfe95e93d248f4eaccf7c322a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*-gpc7JXsJSx2kYJhb82gFA.gif"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">NN培训的GIF</figcaption></figure><p id="3b61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们得到了一个简单的感知器(具体来说，我们使用梯度下降执行线性回归)。其中一个已经很好地预测了一个水瓶的价格——数百、数千甚至数百万个这样的人一起工作的力量是难以想象的。</p><p id="7e86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除非另有说明，所有照片均由作者拍摄/制作。</p></div></div>    
</body>
</html>