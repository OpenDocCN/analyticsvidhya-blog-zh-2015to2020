<html>
<head>
<title>Building a Distributed Hadoop Cluster with HBase on Amazon EC2’s from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Amazon EC2上使用HBase从头开始构建分布式Hadoop集群</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-a-distributed-hadoop-cluster-with-hbase-on-amazon-ec2s-from-scratch-84d503cf36b8?source=collection_archive---------9-----------------------#2020-04-02">https://medium.com/analytics-vidhya/building-a-distributed-hadoop-cluster-with-hbase-on-amazon-ec2s-from-scratch-84d503cf36b8?source=collection_archive---------9-----------------------#2020-04-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/30aa73c034aeb453295ef2cfebc89ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DJvGajoZpUGKsSSEFyzwwQ.jpeg"/></div></div></figure><p id="a156" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你想在AWS EC2上用HBase构建一个分布式Hadoop集群，那么最好的选择就是使用AWS EMR。但是如果你像我一样，想从头开始构建集群，那么你就来对地方了。建立你的集群可能有很多原因，对我来说，是为了理解主从之间的联系是如何发生的，并更深入地挖掘系统。或者，如果您只想调整EMR中不可用的come配置或代码，并运行生产负载。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="923f" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">Hadoop先决条件</h1><p id="b000" class="pw-post-body-paragraph iq ir hi is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">首先，让我们启动4个EC2实例(1个主实例和3个从实例)。为了简单起见，我没有使用bastion主机，启用了公共IP，并且将安全组限制为仅我的IP。一旦实例准备就绪，就将它们标记为master、slave1、slave2和slave3。我用过ubuntu 18.04 AMI，下面是我的<code class="du ky kz la lb b">~/.ssh/config</code>文件</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="ab35" class="lk jw hi lb b fi ll lm l ln lo">#OnLocal</span><span id="01ac" class="lk jw hi lb b fi lp lm l ln lo">Host master<br/>  User ubuntu<br/>  IdentityFile ~/.aws/key.pem<br/>  ProxyCommand ssh -q -W %h:%p ubuntu@ec2-x-x-x-x.us-west-2.compute.amazonaws.com -i ~/.aws/key.pem<br/>  StrictHostKeyChecking no<br/>  UserKnownHostsFile=/dev/null<br/>  HostName 10.0.x.x</span><span id="46e7" class="lk jw hi lb b fi lp lm l ln lo">Host slave1<br/>  User ubuntu<br/>  IdentityFile ~/.aws/bds.pem<br/>  ProxyCommand ssh -q -W %h:%p ubuntu@x-x-x-x.us-west-2.compute.amazonaws.com -i ~/.aws/key.pem<br/>  StrictHostKeyChecking no<br/>  UserKnownHostsFile=/dev/null<br/>  HostName 10.0.x.x</span><span id="9dcf" class="lk jw hi lb b fi lp lm l ln lo">Host slave2<br/>  User ubuntu<br/>  IdentityFile ~/.aws/bds.pem<br/>  ProxyCommand ssh -q -W %h:%p ubuntu@x-x-x-x.us-west-2.compute.amazonaws.com -i ~/.aws/key.pem<br/>  StrictHostKeyChecking no<br/>  UserKnownHostsFile=/dev/null<br/>  HostName 10.0.x.x</span><span id="734e" class="lk jw hi lb b fi lp lm l ln lo">Host slave3<br/>  User ubuntu<br/>  IdentityFile ~/.aws/key.pem<br/>  ProxyCommand ssh -q -W %h:%p ubuntu@x-x-x-x.compute.amazonaws.com -i ~/.aws/bds.pem<br/>  StrictHostKeyChecking no<br/>  UserKnownHostsFile=/dev/null<br/>  HostName 10.0.x.x</span></pre><p id="69f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">记住用ec2实例的实际公共DNS替换ubuntu@x-x-x-x.us-west-2.compute.amazonaws.com，并将主机名更改为私有IPv4。</p><p id="3d28" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在配置好了<code class="du ky kz la lb b">~/.ssh/config</code>,我可以使用<code class="du ky kz la lb b">ssh master</code>或<code class="du ky kz la lb b">ssh slave1</code>等简单地ssh到机器中。</p><p id="f38f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们在继续安装Hadoop之前更新一下。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="b615" class="lk jw hi lb b fi ll lm l ln lo">#OnAll</span><span id="4722" class="lk jw hi lb b fi lp lm l ln lo">sudo apt update &amp;&amp; sudo apt dist-upgrade -y</span></pre><p id="1bb1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，设置主服务器的静态主机名。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="3cb9" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="f289" class="lk jw hi lb b fi lp lm l ln lo">sudo hostnamectl set-hostname --static master</span></pre><p id="6ad9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">奴隶也是如此。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="041a" class="lk jw hi lb b fi ll lm l ln lo">#OnSlaves</span><span id="bd65" class="lk jw hi lb b fi lp lm l ln lo">sudo hostnamectl set-hostname --static slave1<br/>sudo hostnamectl set-hostname --static slave2<br/>sudo hostnamectl set-hostname --static slave3</span></pre><p id="e84d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在打开文件<code class="du ky kz la lb b">sudo vim /etc/cloud/cloud.cfg</code>并配置下面的属性。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="4826" class="lk jw hi lb b fi ll lm l ln lo">#OnAll</span><span id="b07e" class="lk jw hi lb b fi lp lm l ln lo">preserve_hostname=true</span></pre><p id="82b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在用主实例和从实例的私有IP更新<code class="du ky kz la lb b">/etc/hosts</code>文件。这是我的IP列表。记住从<code class="du ky kz la lb b">/etc/hosts</code>文件中删除第一行<code class="du ky kz la lb b">127.0.0.1 localhost</code>。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="a853" class="lk jw hi lb b fi ll lm l ln lo">#OnAll</span><span id="5d5e" class="lk jw hi lb b fi lp lm l ln lo">sudo vim /etc/hosts</span><span id="4435" class="lk jw hi lb b fi lp lm l ln lo">10.0.6.80     master<br/>10.0.6.174    slave1<br/>10.0.6.252    slave2<br/>10.0.6.35     slave3</span></pre><p id="6623" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，让我们在实例上安装OpenJDK 8</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="7c83" class="lk jw hi lb b fi ll lm l ln lo">#OnAll</span><span id="0286" class="lk jw hi lb b fi lp lm l ln lo">sudo apt install openjdk-8-jdk openjdk-8-jre -y</span></pre><p id="1f18" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在将重新启动实例</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="d607" class="lk jw hi lb b fi ll lm l ln lo">#OnAll</span><span id="62fb" class="lk jw hi lb b fi lp lm l ln lo">sudo reboot</span></pre><p id="d3a9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们为实例启用并设置密码。这样做是为了更容易在主节点和从节点之间传输文件。我们可以在以后禁用它。</p><p id="c15c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在所有节点上进行以下更改</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="fbf3" class="lk jw hi lb b fi ll lm l ln lo">#OnAll</span><span id="ee37" class="lk jw hi lb b fi lp lm l ln lo">sudo vim /etc/ssh/sshd_config</span><span id="fec0" class="lk jw hi lb b fi lp lm l ln lo"># Set the below value in the file<br/>PasswordAuthentication yes</span></pre><p id="98b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后重启<code class="du ky kz la lb b">sshd</code>并为<code class="du ky kz la lb b">ubuntu</code>用户设置密码。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="f2ff" class="lk jw hi lb b fi ll lm l ln lo">#OnAll</span><span id="c12b" class="lk jw hi lb b fi lp lm l ln lo">sudo service ssh restart<br/>sudo passwd ubuntu<br/># Enter the password and remember it for future use</span></pre><p id="020a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在只在主节点上执行下一条指令。我们将生成公共/私有ssh密钥，并将其从主设备复制到所有从设备。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="0196" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="01ba" class="lk jw hi lb b fi lp lm l ln lo">ssh-keygen -b 4096</span><span id="199d" class="lk jw hi lb b fi lp lm l ln lo">ssh-copy-id -i $HOME/.ssh/id_rsa.pub ubuntu@master<br/>ssh-copy-id -i $HOME/.ssh/id_rsa.pub ubuntu@slave1<br/>ssh-copy-id -i $HOME/.ssh/id_rsa.pub ubuntu@slave2<br/>ssh-copy-id -i $HOME/.ssh/id_rsa.pub ubuntu@slave3</span></pre><p id="1d87" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意，如果上面的命令<code class="du ky kz la lb b">ssh-copy-id</code>被冻结，那么您需要查看一下安全组，以确保您可以从主设备ssh到从设备。一种方法是，将一个公共安全组(SG)附加到所有实例，并将该SG添加到自身。</p><h1 id="5368" class="jv jw hi bd jx jy lq ka kb kc lr ke kf kg ls ki kj kk lt km kn ko lu kq kr ks bi translated">让我们安装和设置Hadoop！</h1><p id="37a9" class="pw-post-body-paragraph iq ir hi is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">在主节点上，</p><p id="1843" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">运行以下命令将Hadoop下载、解压缩并复制到<code class="du ky kz la lb b">/usr/local/hadoop</code>。让我们也给同一个目录许可。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="79c4" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="51b8" class="lk jw hi lb b fi lp lm l ln lo">wget <a class="ae lv" href="https://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/hadoop-2.7.3.tar.gz" rel="noopener ugc nofollow" target="_blank">https://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/hadoop-2.7.3.tar.gz</a><br/>sudo mkdir /usr/local/hadoop<br/>tar -xzf hadoop-2.7.3.tar.gz<br/>sudo mv hadoop-2.7.3/* /usr/local/hadoop/<br/>ls -ltr /usr/local/hadoop/<br/>sudo chown -R ubuntu:ubuntu /usr/local/hadoop</span></pre><p id="b627" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们添加一些到<code class="du ky kz la lb b">~/.bashrc</code>文件的路径，使它更容易向前导航。设置路径总是会使它更容易。将下面的配置附加到您的<code class="du ky kz la lb b">~/.bashrc</code>文件中。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="24f4" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="1f2e" class="lk jw hi lb b fi lp lm l ln lo"># Java<br/>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64<br/>export JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre</span><span id="168b" class="lk jw hi lb b fi lp lm l ln lo"># Hadoop<br/>export HADOOP_INSTALL=/usr/local/hadoop<br/>export PATH=$PATH:$HADOOP_INSTALL/bin<br/>export PATH=$PATH:$HADOOP_INSTALL/sbin<br/>export HADOOP_MAPRED_HOME=$HADOOP_INSTALL<br/>export HADOOP_COMMON_HOME=$HADOOP_INSTALL<br/>export HADOOP_HDFS_HOME=$HADOOP_INSTALL<br/>export YARN_HOME=$HADOOP_INSTALL<br/>export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native<br/>export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"</span></pre><p id="78d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦保存，运行命令<code class="du ky kz la lb b">source ~/.bashrc</code>在当前会话中更新它。</p><p id="b651" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们对Hadoop环境进行更改，使其支持分布式模式。接下来会有一长串的变化。</p><p id="89c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，让我们进入<code class="du ky kz la lb b">cd /usr/local/hadoop/etc/hadoop</code>目录。</p><ol class=""><li id="52d2" class="lw lx hi is b it iu ix iy jb ly jf lz jj ma jn mb mc md me bi translated">将<code class="du ky kz la lb b">JAVA_HOME</code>设置在<code class="du ky kz la lb b">hadoop-env.sh</code>中。</li></ol><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="00d8" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="fb5b" class="lk jw hi lb b fi lp lm l ln lo">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span></pre><p id="7dba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.在<code class="du ky kz la lb b">yarn-site.xml</code>中添加纱线配置。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="5e8e" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="c4ff" class="lk jw hi lb b fi lp lm l ln lo">&lt;property&gt;<br/>    &lt;name&gt;yarn.acl.enable&lt;/name&gt;<br/>    &lt;value&gt;0&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br/>    &lt;value&gt;master&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br/>    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;<br/>    &lt;value&gt;2826&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;<br/>    &lt;value&gt;2726&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;<br/>    &lt;value&gt;128&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;<br/>    &lt;value&gt;false&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;<br/>    &lt;value&gt;2&lt;/value&gt;<br/>&lt;/property&gt;</span></pre><p id="165e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">确保在<code class="du ky kz la lb b">&lt;configuration&gt;</code>和<code class="du ky kz la lb b">&lt;/configuration&gt;</code>之间添加上述属性。</p><p id="64e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.将以下配置添加到<code class="du ky kz la lb b">core-site.xml</code>。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="f0d0" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="148d" class="lk jw hi lb b fi lp lm l ln lo">&lt;property&gt;<br/>    &lt;name&gt;fs.default.name&lt;/name&gt;<br/>    &lt;value&gt;hdfs://master:9000&lt;/value&gt;<br/>&lt;/property&gt;</span></pre><p id="6083" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.将HDFS配置添加到<code class="du ky kz la lb b">hdfs-site.xml</code>。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="7416" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="c589" class="lk jw hi lb b fi lp lm l ln lo">&lt;property&gt;<br/>    &lt;name&gt;dfs.replication&lt;/name&gt;<br/>    &lt;value&gt;3&lt;/value&gt;<br/>    &lt;description&gt;<br/>        Default block replication.<br/>        The actual number of replications can be specified when the file is created.<br/>        The default is used if replication is not specified in create time.<br/>    &lt;/description&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br/>    &lt;value&gt;file:/usr/local/hadoop_store/hdfs/namenode&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br/>    &lt;value&gt;file:/usr/local/hadoop_store/hdfs/datanode&lt;/value&gt;<br/>&lt;/property&gt;</span></pre><p id="c020" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.现在，让我们将Map-Reduce配置添加到<code class="du ky kz la lb b">mapred-site.xml</code>。但是首先，我们需要复制模板。这可以通过以下方式实现</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="8a67" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="5942" class="lk jw hi lb b fi lp lm l ln lo">cp mapred-site.xml.template mapred-site.xml</span></pre><p id="9456" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后打开文件<code class="du ky kz la lb b">mapred-site.xml</code>并添加以下配置</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="10cc" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="a53b" class="lk jw hi lb b fi lp lm l ln lo">&lt;property&gt;<br/>    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br/>    &lt;value&gt;yarn&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>        &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt;<br/>        &lt;value&gt;1024&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>        &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;<br/>        &lt;value&gt;1024&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>        &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;<br/>        &lt;value&gt;1024&lt;/value&gt;<br/>&lt;/property&gt;</span></pre><p id="9c9a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你有兴趣了解每个属性是做什么的，那么请参考Hadoop 的<a class="ae lv" href="https://hadoop.apache.org/docs/r2.7.3/" rel="noopener ugc nofollow" target="_blank">官方文档。</a></p><p id="089b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">6.让我们为HDFS商店创建一个目录。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="1eb0" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="dcf3" class="lk jw hi lb b fi lp lm l ln lo">sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode<br/>sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode<br/>sudo chown -R ubuntu:ubuntu /usr/local/hadoop_store/</span></pre><p id="3434" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">7.现在我们已经在主节点上完成了所有需要的配置，让我们在从节点上进行更改。但是首先，让我们将<code class="du ky kz la lb b">hadoop-2.7.3.tar.gz</code>复制到从节点。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="0be0" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="0146" class="lk jw hi lb b fi lp lm l ln lo">cd ~<br/>scp hadoop-2.7.3.tar.gz ubuntu@slave1:/home/ubuntu/<br/>scp hadoop-2.7.3.tar.gz ubuntu@slave2:/home/ubuntu/<br/>scp hadoop-2.7.3.tar.gz ubuntu@slave3:/home/ubuntu/</span></pre><p id="5201" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦我们将文件复制到从属节点，就应该在那里进行类似的更改了。</p><p id="0db7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在奴隶身上，</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="cdd2" class="lk jw hi lb b fi ll lm l ln lo">#OnSlaves</span><span id="1d72" class="lk jw hi lb b fi lp lm l ln lo">sudo mkdir /usr/local/hadoop<br/>tar -xzf hadoop-2.7.3.tar.gz<br/>sudo mv hadoop-2.7.3/* /usr/local/hadoop/<br/>ls -ltr /usr/local/hadoop/<br/>sudo chown -R ubuntu:ubuntu /usr/local/hadoop</span></pre><p id="d7d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们也在slaves中添加一些到<code class="du ky kz la lb b">~/.bashrc</code>文件的路径。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="364f" class="lk jw hi lb b fi ll lm l ln lo">#OnSlaves</span><span id="524c" class="lk jw hi lb b fi lp lm l ln lo"># Java<br/>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64<br/>export JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre</span><span id="a86b" class="lk jw hi lb b fi lp lm l ln lo"># Hadoop<br/>export HADOOP_INSTALL=/usr/local/hadoop<br/>export PATH=$PATH:$HADOOP_INSTALL/bin<br/>export PATH=$PATH:$HADOOP_INSTALL/sbin<br/>export HADOOP_MAPRED_HOME=$HADOOP_INSTALL<br/>export HADOOP_COMMON_HOME=$HADOOP_INSTALL<br/>export HADOOP_HDFS_HOME=$HADOOP_INSTALL<br/>export YARN_HOME=$HADOOP_INSTALL<br/>export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native<br/>export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"</span></pre><p id="82ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦保存，运行命令<code class="du ky kz la lb b">source ~/.bashrc</code>在当前会话中更新它。</p><p id="fe81" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们在从服务器中为HDFS商店创建一个目录。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="2516" class="lk jw hi lb b fi ll lm l ln lo">#OnSlaves</span><span id="11fb" class="lk jw hi lb b fi lp lm l ln lo">sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode<br/>sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode<br/>sudo chown -R ubuntu:ubuntu /usr/local/hadoop_store/</span></pre><p id="57d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们已经配置了从节点，让我们回到主节点。</p><p id="e4d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在Master上，</p><p id="5d11" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们将对Hadoop配置所做的所有更改也复制到从服务器。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="e80e" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="29c1" class="lk jw hi lb b fi lp lm l ln lo">scp /usr/local/hadoop/etc/hadoop/* ubuntu@slave1:/usr/local/hadoop/etc/hadoop/</span><span id="01a7" class="lk jw hi lb b fi lp lm l ln lo">scp /usr/local/hadoop/etc/hadoop/* ubuntu@slave2:/usr/local/hadoop/etc/hadoop/</span><span id="5b3e" class="lk jw hi lb b fi lp lm l ln lo">scp /usr/local/hadoop/etc/hadoop/* ubuntu@slave3:/usr/local/hadoop/etc/hadoop/</span></pre><p id="d1ad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们将从属名称添加到<code class="du ky kz la lb b">/usr/local/hadoop/etc/hadoop/slaves</code>文件中。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="f79d" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="9c98" class="lk jw hi lb b fi lp lm l ln lo">slave1<br/>slave2<br/>slave3</span></pre><p id="6f9a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们已经完成了Hadoop设置。让我们启动Hadoop并进行测试！</p><p id="2d74" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">又在主人身上，</p><p id="7d8c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们格式化<code class="du ky kz la lb b">namenode</code>。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="ecd2" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="ddc2" class="lk jw hi lb b fi lp lm l ln lo">hdfs namenode -format</span></pre><p id="5f07" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后我们将启动<strong class="is hj"> DFS </strong></p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="8d1c" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="4c14" class="lk jw hi lb b fi lp lm l ln lo">start-dfs.sh</span></pre><p id="7795" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了检查所有的配置是否正确，让我们检查一下<code class="du ky kz la lb b">jps</code></p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="fa87" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="c954" class="lk jw hi lb b fi lp lm l ln lo">jps</span></pre><p id="188b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在主节点上，输出应该是</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="1f10" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="f99a" class="lk jw hi lb b fi lp lm l ln lo">4448 SecondaryNameNode<br/>4572 Jps<br/>4175 NameNode</span></pre><p id="d410" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在从节点上，</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="b9e8" class="lk jw hi lb b fi ll lm l ln lo">#OnSlaves</span><span id="59f7" class="lk jw hi lb b fi lp lm l ln lo">2632 DataNode<br/>2713 Jps</span></pre><p id="e5b6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当然，PID会有所不同。</p><p id="32f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们也开始纺纱。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="2ebd" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="428a" class="lk jw hi lb b fi lp lm l ln lo">start-yarn.sh</span></pre><p id="484d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，当我们运行<code class="du ky kz la lb b">jps</code>时，我们应该看到ResourceManager被添加到主节点的列表中，NodeManager被添加到从节点的列表中。</p><p id="5a8c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在主人身上</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="c8a7" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="029e" class="lk jw hi lb b fi lp lm l ln lo">4448 SecondaryNameNode<br/>4631 ResourceManager<br/>4895 Jps<br/>4175 NameNode</span></pre><p id="5e1e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在奴隶身上</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="2697" class="lk jw hi lb b fi ll lm l ln lo">#OnSlaves</span><span id="93fc" class="lk jw hi lb b fi lp lm l ln lo">2832 NodeManager<br/>2632 DataNode<br/>2943 Jps</span></pre><p id="48b8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，您还可以看到UI是否一切运行正常。首先，找出主节点的公共IP。DFS UI在端口50070上，YARN UI在8088上。</p><h2 id="7f8e" class="lk jw hi bd jx mf mg mh kb mi mj mk kf jb ml mm kj jf mn mo kn jj mp mq kr mr bi translated">DFS: http:// <master_ip> :50070</master_ip></h2><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/b992396abfd0aaf69825824b48fab8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UuZO84zQ-RFGDtQOIxmN-A.png"/></div></div></figure><h2 id="642c" class="lk jw hi bd jx mf mg mh kb mi mj mk kf jb ml mm kj jf mn mo kn jj mp mq kr mr bi translated">纱线:http://<master_ip>:8088/簇</master_ip></h2><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/fd632a292033a10e89fa87ab3a0d0b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TyPIqy1aH1KPvzGsLWLkxQ.png"/></div></div></figure><p id="7c65" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们可以运行一个小的Map Reduce来计算文本中的单词。在主机中执行以下命令(当然)来运行作业。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="4350" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="72fd" class="lk jw hi lb b fi lp lm l ln lo">cd ~<br/>mkdir sample_data<br/>cd sample_data/<br/>wget -O alice.txt <a class="ae lv" href="https://www.gutenberg.org/files/11/11-0.txt" rel="noopener ugc nofollow" target="_blank">https://www.gutenberg.org/files/11/11-0.txt</a><br/>cd ~</span><span id="5028" class="lk jw hi lb b fi lp lm l ln lo">hadoop fs -copyFromLocal sample_data/alice.txt hdfs://master:9000/<br/>hdfs dfs -ls hdfs://master:9000/<br/>yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount "hdfs://master:9000/alice.txt" hdfs://master:9000/output/</span></pre><p id="55d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当作业正在运行时，您还可以访问YARN UI来跟踪作业。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="39e2" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">HBase</h1><ol class=""><li id="c7a9" class="lw lx hi is b it kt ix ku jb mu jf mv jj mw jn mb mc md me bi translated">让我们下载HBase，然后解压并复制文件到<code class="du ky kz la lb b">/usr/local/hbase</code>目录。下载HBase的官方链接可以在<a class="ae lv" href="https://hbase.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。从那里你必须找到镜像并下载它。请根据您的位置随意替换链接。还有，我用HBase版本<code class="du ky kz la lb b">1.4.13</code>测试过。</li></ol><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="2d2a" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="22c9" class="lk jw hi lb b fi lp lm l ln lo">wget <a class="ae lv" href="https://downloads.apache.org/hbase/1.4.13/hbase-1.4.13-bin.tar.gz" rel="noopener ugc nofollow" target="_blank">https://downloads.apache.org/hbase/1.4.13/hbase-1.4.13-bin.tar.gz</a><br/>tar -zxvf hbase-1.4.13-bin.tar.gz<br/>sudo mv hbase-1.4.13 /usr/local/hbase<br/>sudo chown -R ubuntu:ubuntu /usr/local/hbase</span></pre><p id="60d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.将以下命令添加到<code class="du ky kz la lb b">~/.bashrc</code>文件中</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="aa9f" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="e035" class="lk jw hi lb b fi lp lm l ln lo">export HBASE_HOME=/usr/local/hbase<br/>export PATH=$PATH:$HBASE_HOME/bin</span></pre><p id="94e5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，像往常一样源文件。<code class="du ky kz la lb b">source ~/.bashrc</code>。</p><p id="2dbc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.更新HBase配置文件</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="b256" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="9cd3" class="lk jw hi lb b fi lp lm l ln lo">cd /usr/local/hbase/conf/</span></pre><p id="ca9d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在<code class="du ky kz la lb b">hbase-env.sh</code>文件中设置<code class="du ky kz la lb b">JAVA_HOME</code>。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="95d2" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="6dbf" class="lk jw hi lb b fi lp lm l ln lo">export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64</span></pre><p id="b554" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，继续将属性添加到<code class="du ky kz la lb b">hbase-site.xml</code>文件。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="1293" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="bc4e" class="lk jw hi lb b fi lp lm l ln lo">&lt;property&gt;<br/>      &lt;name&gt;hbase.rootdir&lt;/name&gt;<br/>      &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt;<br/>   &lt;/property&gt;</span><span id="7514" class="lk jw hi lb b fi lp lm l ln lo">&lt;property&gt;<br/>      &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;<br/>      &lt;value&gt;hdfs://master:9000/zookeeper&lt;/value&gt;<br/>   &lt;/property&gt;</span><span id="b883" class="lk jw hi lb b fi lp lm l ln lo">&lt;property&gt;<br/>     &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;<br/>     &lt;value&gt;true&lt;/value&gt;<br/>   &lt;/property&gt;<br/>&lt;property&gt;<br/>    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;<br/>    &lt;value&gt;slave1,slave2,slave3&lt;/value&gt;<br/>  &lt;/property&gt;</span></pre><p id="a688" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后将从机名称添加到<code class="du ky kz la lb b">regionservers</code></p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="6545" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="2ea4" class="lk jw hi lb b fi lp lm l ln lo">slave1<br/>slave2<br/>slave3</span></pre><p id="9229" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在从节点上，创建<code class="du ky kz la lb b">/usr/local/hbase</code>目录并授予权限，以便我们可以将文件从主节点复制到从节点。在<strong class="is hj">从动装置上进行以下更改。</strong></p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="3ccd" class="lk jw hi lb b fi ll lm l ln lo">#OnSlaves</span><span id="3fa4" class="lk jw hi lb b fi lp lm l ln lo">sudo mkdir -p /usr/local/hbase<br/>sudo chown -R ubuntu:ubuntu /usr/local/hbase</span></pre><p id="bb85" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们需要从主机将HBase文件复制到从机。运行下面的命令来完成。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="5b7f" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="f30e" class="lk jw hi lb b fi lp lm l ln lo">scp -rp /usr/local/hbase/* ubuntu@slave1:/usr/local/hbase/<br/>scp -rp /usr/local/hbase/* ubuntu@slave2:/usr/local/hbase/<br/>scp -rp /usr/local/hbase/* ubuntu@slave3:/usr/local/hbase/</span></pre><p id="1004" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在是启动HBase的时候了。在启动HBase之前，确保Hadoop正在运行。</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="1eaf" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="3796" class="lk jw hi lb b fi lp lm l ln lo">start-hbase.sh</span></pre><p id="f63a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当你运行<code class="du ky kz la lb b">jps</code>命令时，</p><p id="2b0f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在主人身上你看到了</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="6fba" class="lk jw hi lb b fi ll lm l ln lo">#OnMaster</span><span id="a9ff" class="lk jw hi lb b fi lp lm l ln lo">7616 NameNode<br/>7891 SecondaryNameNode<br/>8851 Jps<br/>8581 HMaster<br/>8056 ResourceManager</span></pre><p id="6c19" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在奴隶身上</p><pre class="lc ld le lf fd lg lb lh li aw lj bi"><span id="22dd" class="lk jw hi lb b fi ll lm l ln lo">#OnSlave</span><span id="cd06" class="lk jw hi lb b fi lp lm l ln lo">4741 DataNode<br/>5270 HQuorumPeer<br/>5614 Jps<br/>5438 HRegionServer<br/>4927 NodeManager</span></pre><p id="87cc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">HBase还有一个UI来查看RegionServers信息以及其他信息。</p><h2 id="5916" class="lk jw hi bd jx mf mg mh kb mi mj mk kf jb ml mm kj jf mn mo kn jj mp mq kr mr bi translated">h base UI:http://<master_ip>:16010/master-status</master_ip></h2><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/8a3a487c7c1802fe15f772f4616a023d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dXd2RF0mNCiNauzyrjKiFQ.png"/></div></div></figure><p id="20c2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Hadoop和HBase在分布式模式下启动并运行，AWS EC2实例上的复制因子为3！</strong></p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h2 id="ed72" class="lk jw hi bd jx mf mg mh kb mi mj mk kf jb ml mm kj jf mn mo kn jj mp mq kr mr bi translated">参考资料:</h2><ol class=""><li id="83db" class="lw lx hi is b it kt ix ku jb mu jf mv jj mw jn mb mc md me bi translated"><a class="ae lv" rel="noopener" href="/@zaman.nuces/setting-up-fully-distributed-hadoop-cluster-series-2-of-3-27b9831c25ae">https://medium . com/@ zaman . nuces/setting-up-full-distributed-Hadoop-cluster-series-2-of-3-27b 9831 c 25 AE</a></li><li id="79d8" class="lw lx hi is b it my ix mz jb na jf nb jj nc jn mb mc md me bi translated"><a class="ae lv" href="https://aws.amazon.com/premiumsupport/knowledge-center/linux-static-hostname-rhel7-centos7/" rel="noopener ugc nofollow" target="_blank">https://AWS . Amazon . com/premium support/knowledge-center/Linux-static-hostname-rhel 7-centos 7/</a></li><li id="ff35" class="lw lx hi is b it my ix mz jb na jf nb jj nc jn mb mc md me bi translated"><a class="ae lv" href="https://www.digitalocean.com/community/questions/ssh-copy-id-not-working-permission-denied-publickey" rel="noopener ugc nofollow" target="_blank">https://www . digital ocean . com/community/questions/ssh-copy-id-not-work-permission-denied-public key</a></li><li id="c2f3" class="lw lx hi is b it my ix mz jb na jf nb jj nc jn mb mc md me bi translated"><a class="ae lv" href="https://aws.amazon.com/premiumsupport/knowledge-center/ec2-password-login/" rel="noopener ugc nofollow" target="_blank">https://AWS . Amazon . com/premium support/knowledge-center/ec2-password-log in/</a></li><li id="2aa7" class="lw lx hi is b it my ix mz jb na jf nb jj nc jn mb mc md me bi translated"><a class="ae lv" rel="noopener" href="/@yzhong.cs/hbase-installation-step-by-step-guide-cb73381a7a4c">https://medium . com/@ yzhong . cs/h base-安装-分步指南-cb73381a7a4c </a></li><li id="8702" class="lw lx hi is b it my ix mz jb na jf nb jj nc jn mb mc md me bi translated"><a class="ae lv" href="https://unsplash.com/photos/gpjvRZyavZc" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/gpjvRZyavZc</a></li><li id="20ff" class="lw lx hi is b it my ix mz jb na jf nb jj nc jn mb mc md me bi translated"><a class="ae lv" href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html" rel="noopener ugc nofollow" target="_blank">https://Hadoop . Apache . org/docs/stable/Hadoop-project-dist/Hadoop-common/cluster setup . html</a></li><li id="e3b1" class="lw lx hi is b it my ix mz jb na jf nb jj nc jn mb mc md me bi translated"><a class="ae lv" href="https://hbase.apache.org/book.html#configuration" rel="noopener ugc nofollow" target="_blank">https://hbase.apache.org/book.html#configuration</a></li></ol></div></div>    
</body>
</html>