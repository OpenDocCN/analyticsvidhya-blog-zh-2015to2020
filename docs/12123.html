<html>
<head>
<title>Text-Based Communication Analysis with Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于文本的通信分析与机器学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-based-communication-analysis-with-machine-learning-17138c0a4a4e?source=collection_archive---------13-----------------------#2020-12-31">https://medium.com/analytics-vidhya/text-based-communication-analysis-with-machine-learning-17138c0a4a4e?source=collection_archive---------13-----------------------#2020-12-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ffe8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这项研究通过应用各种机器学习技术对历史文本消息进行了更深入的研究。该研究的目的是应用自然语言处理(NLP)技术来确定沟通趋势，评估现有流程的有效性，并从历史数据中提供任何见解。</p><h1 id="8452" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">方法学</h1><p id="c497" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">为了分析数据，我使用 Python 作为编程语言，同时使用了许多软件库，如 pandas、math、numpy、sklearn、matplotlib 等。此外，我还使用了自然语言处理(NLP)，这是一种机器学习(ML)技术，有助于使用人工智能来理解、解释和操纵人类语言。</p><h1 id="b042" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">数据探索</h1><p id="5fe0" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">这里的重点是探索大量的非结构化数据，并发现任何初始模式、特征和兴趣点。在这一步中，并不期望揭示数据集包含的每一点信息，而是帮助创建重要趋势和主要点的大致情况，以进行更详细的研究。</p><p id="cece" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">寻找春夏进出信息趋势</strong></p><p id="c917" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当前数据集包含有限数量的列。为了按月获取消息计数，我们需要从时间字段中提取年和月。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="bbfd" class="kp je hi kl b fi kq kr l ks kt">df_text=pd.read_csv('../Data/clean_text.csv')</span><span id="7da9" class="kp je hi kl b fi ku kr l ks kt">df_text.info()</span><span id="1737" class="kp je hi kl b fi ku kr l ks kt">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 62513 entries, 0 to 62512<br/>Data columns (total 7 columns):<br/>message_type    62513 non-null object<br/>text            62513 non-null object<br/>time            62513 non-null object<br/>ID              62513 non-null object<br/>group_list      62513 non-null object<br/>term            62513 non-null object<br/>sent_by         62513 non-null object<br/>dtypes: object(7)<br/>memory usage: 3.3+ MB</span><span id="c188" class="kp je hi kl b fi ku kr l ks kt">#Extracting year and month from 'time' column and append the dataframe with new columns.</span><span id="9074" class="kp je hi kl b fi ku kr l ks kt">df_text['year'] = pd.DatetimeIndex(df_text['time']).year<br/>df_text['month'] = pd.DatetimeIndex(df_text['time']).month<br/>df_text.head()<br/></span></pre><p id="b3ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以按月统计入站和出站消息的数量。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="68f2" class="kp je hi kl b fi kq kr l ks kt"># Getting the message count by message type and month<br/># reset_index() gives a column for counting, after groupby uses year and category<br/>df_cnt = (df_text.reset_index()<br/>          .groupby(['month','message_type'], as_index=False)<br/>          .count()<br/>          # rename isn't strictly necessary here, it's just for readability<br/>          .rename(columns={'index':'count'})<br/>       )<br/>#sorting the values by month<br/>df_cnt.sort_values(by = 'month', ascending = True)</span></pre><p id="21aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在是时候使用<em class="kv"> matplotlib </em>来呈现数据了。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="68f6" class="kp je hi kl b fi kq kr l ks kt">import matplotlib.pyplot as plt</span><span id="b077" class="kp je hi kl b fi ku kr l ks kt">df_all_cnt =df_cnt</span><span id="1293" class="kp je hi kl b fi ku kr l ks kt">#adding month name for better presentation<br/>df_all_cnt['month_t'] = df_all_cnt['month'].apply(lambda x: calendar.month_abbr[x])</span><span id="2e12" class="kp je hi kl b fi ku kr l ks kt">fig, ax = plt.subplots(figsize=(5, 3), dpi=150)</span><span id="2ee5" class="kp je hi kl b fi ku kr l ks kt"># key gives the group name (i.e. category), data gives the actual values<br/>for key, data in df_all_cnt.groupby('message_type'):<br/>    data.plot(x='month_t', y='count', ax=ax, label=key)<br/>    <br/># Hide the right and top spines<br/>ax.spines['right'].set_visible(False)<br/>ax.spines['top'].set_visible(False)</span><span id="90d6" class="kp je hi kl b fi ku kr l ks kt">#draw the grid<br/>ax.grid( linestyle='-', linewidth=0.2)     <br/>ax.legend()<br/>ax.set_xlabel('Month')<br/>ax.set_ylabel('Nu. Of Messages')<br/>ax.set_title('In and Out Messages By Month - Both Programs')</span></pre><figure class="kg kh ki kj fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es kw"><img src="../Images/0bee6b877f6d1c22b330d86238e7f5fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*12YJ4W8eyDddTmYaJh24OQ.png"/></div></div></figure><p id="eb19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，我们可以使用数据集中的其他列作为 x 轴和 y 轴来填充不同类型的图形，以显示同一数据集的各个方面。</p><h1 id="d5dd" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">使用自然语言处理的文本分析</h1><p id="50ba" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">NLP 是一种分析和解释文本数据(如学生回答)的优秀方法。这是一种机器用来理解人类语言(如文本、语音、表情符号)的 ML 技术，目前在行业中广泛使用(即 Siri 和 Alexa)。</p><h1 id="3bdb" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">清洁、去除停用词和堵塞</h1><p id="0915" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">通过删除所有不必要的单词、符号和任何对您进行有意义的分析没有帮助的文本来清理您的数据是非常重要的。下面的函数将帮助我们清理，删除停用词，但我们也需要一些库来帮助。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="58e4" class="kp je hi kl b fi kq kr l ks kt">from nltk.corpus import stopwords<br/>nltk.download('stopwords')<br/>nltk.download('punkt')<br/>from gensim.parsing.preprocessing import STOPWORDS<br/>from gensim.parsing.preprocessing import remove_stopwords<br/>set(stopwords.words('english'))<br/>from nltk.tokenize import word_tokenize<br/>from contractions import contractions_dict<br/>import unicodedata</span><span id="63ee" class="kp je hi kl b fi ku kr l ks kt">def remove_punctuation(text):<br/>    text = ''.join([i for i in text if not i.isdigit()])<br/>    return re.sub("[!@#$+%*:()/|,;:'-]", ' ', text)</span><span id="ba8f" class="kp je hi kl b fi ku kr l ks kt">def removebrackets(text):<br/>    return re.sub('[\(\[].*?[\)\]]', ' ', text)</span><span id="2e46" class="kp je hi kl b fi ku kr l ks kt">def remove_accented_chars(text):<br/>    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')</span><span id="2ad8" class="kp je hi kl b fi ku kr l ks kt">def remove_special_chars(text, remove_digits=False):<br/>    pattern = r'[^a-zA-Z0-9\s]' if not remove_digits else r'[^a-zA-Z\s]'<br/>    return re.sub(pattern, '', text)</span><span id="c5f3" class="kp je hi kl b fi ku kr l ks kt">def remove_stopwords(text):<br/>   # text_tokens = word_tokenize(text)<br/>   # text = remove_stopwords(text)<br/>      <br/>    from gensim.parsing.preprocessing import STOPWORDS</span><span id="f838" class="kp je hi kl b fi ku kr l ks kt">all_stopwords_gensim = STOPWORDS.union(set(['thank', 'need', 'yes', 'okay']))</span><span id="e409" class="kp je hi kl b fi ku kr l ks kt">text_tokens = word_tokenize(text)<br/>    tokens_without_sw = ' '.join([word for word in text_tokens if not word in all_stopwords_gensim])<br/>    <br/>    return tokens_without_sw</span><span id="23f9" class="kp je hi kl b fi ku kr l ks kt">def stemming (text):<br/>    <br/>   ps = nltk.porter.PorterStemmer()<br/>   return ' '.join([ps.stem(word) for word in text.split()])<br/>    <br/>  stopword_list = stopwords.words('english')<br/>  tokens = nltk.word_tokenize(text)<br/>  tokens = [token.strip() for token in tokens]<br/>  return ' '.join([token for token in tokens if token not in stopword_list])</span><span id="904f" class="kp je hi kl b fi ku kr l ks kt">def lemmatize(text):<br/>    text = nlp(text)<br/>    return ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])</span><span id="d553" class="kp je hi kl b fi ku kr l ks kt">def expand_contractions(text, contraction_mapping=contractions_dict):<br/>    <br/>    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), <br/>                                      flags=re.IGNORECASE|re.DOTALL)<br/>    def expand_match(contraction):<br/>        match = contraction.group(0)<br/>        first_char = match[0]<br/>        expanded_contraction = contraction_mapping.get(match)\<br/>                                if contraction_mapping.get(match)\<br/>                                else contraction_mapping.get(match.lower())                       <br/>        expanded_contraction = first_char+expanded_contraction[1:]<br/>        return expanded_contraction<br/>        <br/>    expanded_text = contractions_pattern.sub(expand_match, text)<br/>    return re.sub("'", "", expanded_text)</span></pre><p id="acb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">清理完数据后，我们可以填充单词 cloud 进行分析。单词云是表示这些数据的最有效的方式之一，它通过大小、颜色的深度和单词的粗体来指示文本的重要性。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="ab37" class="kp je hi kl b fi kq kr l ks kt">from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator</span><span id="aea2" class="kp je hi kl b fi ku kr l ks kt">wc = WordCloud(stopwords=STOPWORDS,max_font_size=200, max_words=1000000, background_color="white", width=1000, height=1000).generate(' '.join(df_filtered['text_clean']))<br/>plt.figure(figsize=(20,20))<br/>plt.imshow(wc)<br/>plt.axis("off")<br/>plt.show()</span></pre><figure class="kg kh ki kj fd kx er es paragraph-image"><div class="er es le"><img src="../Images/6a81b429a8a57f87c48045d69f2da700.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*XM8geg0jo3z8JE0iZwsRdg.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated"><em class="lj">字云基于历史数据分析</em></figcaption></figure><p id="7922" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在填充了单词 cloud 之后，您可以识别出您的分析不需要的任何其他文本。您可以简单地通过添加那些标识的单词来扩展当前的停用词库，如下所示。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="5f56" class="kp je hi kl b fi kq kr l ks kt">def clean_text_again(text):</span><span id="369e" class="kp je hi kl b fi ku kr l ks kt">from gensim.parsing.preprocessing import STOPWORDS</span><span id="91b3" class="kp je hi kl b fi ku kr l ks kt">all_stopwords_gensim = STOPWORDS.union(set(['thank', 'need', 'yes', 'okay','ok','thanks','morning','hello','sure','hi', 'know', 'got','yesterday']))</span><span id="b5cd" class="kp je hi kl b fi ku kr l ks kt">text_tokens = word_tokenize(text)<br/>    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]</span><span id="b882" class="kp je hi kl b fi ku kr l ks kt">return ' '.join([word for word in text_tokens if not word in all_stopwords_gensim])</span></pre><h1 id="1f22" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">字云扩展— Ngram 探索</strong></h1><p id="1e02" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">Ngrams 只是 n 个字的连续序列。查看最频繁的 n-grams 可以让我们更好地理解上下文。</p><p id="ddd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的函数在条形图中显示了最频繁出现的不间断单词。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="c5f8" class="kp je hi kl b fi kq kr l ks kt">import seaborn as sns</span><span id="3aba" class="kp je hi kl b fi ku kr l ks kt">def plot_top_non_stopwords_barchart(text):<br/>     <br/>    new= text.str.split()<br/>    new=new.values.tolist()<br/>    corpus=[word for i in new for word in i]</span><span id="1c80" class="kp je hi kl b fi ku kr l ks kt">counter=Counter(corpus)<br/>    most=counter.most_common()<br/>    x, y=[], []<br/>    for word,count in most[:40]:<br/>        if (word not in stop):<br/>            x.append(word)<br/>            y.append(count)<br/>      <br/>    plt.figure(figsize=(15,8))<br/>    sns.barplot(x=y,y=x,palette="colorblind")</span></pre><figure class="kg kh ki kj fd kx er es paragraph-image"><div class="er es lk"><img src="../Images/06d472408eef9b1a6f0359cb02ccc176.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*LBa5FyeuoLww353JU10UrA.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated"><em class="lj">最常出现的不间断单词</em></figcaption></figure><p id="0a48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，为了理解不间断单词的上下文及其关系；我们需要用紧随其后的单词填充图表。我修改了上面的函数，使用 3 个直接的单词来绘制<em class="kv">三元模型</em>，如下所示。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="ff17" class="kp je hi kl b fi kq kr l ks kt">def plot_top_ngrams_barchart(text, n=3): #n is the number of immediate words you need.<br/>      <br/>    new= text.str.split()<br/>    new=new.values.tolist()<br/>    corpus=[word for i in new for word in i]</span><span id="0013" class="kp je hi kl b fi ku kr l ks kt">def _get_top_ngram(corpus, n=None):<br/>        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)<br/>        bag_of_words = vec.transform(corpus)<br/>        sum_words = bag_of_words.sum(axis=0) <br/>        words_freq = [(word, sum_words[0, idx]) <br/>                      for word, idx in vec.vocabulary_.items()]<br/>        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)<br/>        return words_freq[:10]</span><span id="829c" class="kp je hi kl b fi ku kr l ks kt">top_n_bigrams=_get_top_ngram(text,n)[:10]<br/>    x,y=map(list,zip(*top_n_bigrams))<br/>    sns.barplot(x=y,y=x)</span></pre><figure class="kg kh ki kj fd kx er es paragraph-image"><div class="er es ll"><img src="../Images/ec09ab690a4fcdba087f27604cada9fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*scuJ9ckIhX4TQFI_x5Mtqw.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated"><em class="lj">出现频率最高的三元组</em></figcaption></figure><h1 id="b071" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">情感分析</strong></h1><p id="fdf4" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">情感分析是一种广泛使用的工具，具有 NLP 工具包，可以自动对文本数据(如学生回答)的情感进行分类。首先，我们可以使用极性直方图找到情感的分布。极性是介于-1 到 1 之间的浮点数，其中 1 表示肯定的陈述，而-1 表示否定的陈述。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="9692" class="kp je hi kl b fi kq kr l ks kt">from textblob import TextBlob<br/>    <br/>def plot_polarity_histogram(text):<br/>    <br/>    def polarity(text):<br/>        return TextBlob(text).sentiment.polarity<br/>    <br/>    df_x = pd.DataFrame() <br/>    polarity_score =text.apply(lambda x : polarity(x))<br/>    df_filtered['polarity_score']=df_filtered['text'].\<br/>       apply(lambda x : polarity(x))<br/>    polarity_score.hist()</span></pre><figure class="kg kh ki kj fd kx er es paragraph-image"><div class="er es lm"><img src="../Images/6d08dfcd1dab370916f13203e8fd769a.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*s9aGjZ0LLhHompsV2z2nMA.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated"><em class="lj">整体情绪极性</em></figcaption></figure><p id="19bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以进一步分析情绪，以查看积极、消极和中性情绪的分布。下面的函数对情绪进行分类，并显示一个条形图。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="cfb4" class="kp je hi kl b fi kq kr l ks kt">from textblob import TextBlob<br/>import matplotlib.pyplot as plt<br/>from nltk.sentiment.vader import SentimentIntensityAnalyzer<br/>import nltk</span><span id="aae3" class="kp je hi kl b fi ku kr l ks kt">def sentiment_vader(text, sid):<br/>    ss = sid.polarity_scores(text)<br/>    ss.pop('compound')<br/>    return max(ss, key=ss.get)</span><span id="7ebd" class="kp je hi kl b fi ku kr l ks kt">def sentiment_textblob(text):<br/>        x = TextBlob(text).sentiment.polarity<br/>        <br/>        if x&lt;0:<br/>            return 'Negative'<br/>        elif x==0:<br/>            return 'Neutral'<br/>        else:<br/>            return 'Positive'<br/>        <br/>def sentiment_x(x):<br/>    if x&lt;0:<br/>        return 'Negative'<br/>    elif x==0:<br/>        return 'Neutral'<br/>    else:<br/>        return 'Positive' <br/>        <br/>def plot_sentiment_barchart(text, method):<br/>    if method == 'TextBlob':<br/>        sentiment = text.map(lambda x: sentiment_textblob(x))<br/>    elif method == 'Vader':<br/>        nltk.download('vader_lexicon')<br/>        sid = SentimentIntensityAnalyzer()<br/>        sentiment = text.map(lambda x: sentiment_vader(x, sid=sid))<br/>    else:<br/>        raise ValueError('Textblob or Vader')<br/>    <br/>    df_filtered['polarity']=df_filtered['polarity_score'].\<br/>       apply(lambda x : sentiment_x(x))<br/>  <br/>    plt.figure(figsize=(5, 2), dpi=100)<br/>    plt.bar(sentiment.value_counts().index,<br/>            sentiment.value_counts())</span></pre><figure class="kg kh ki kj fd kx er es paragraph-image"><div class="er es ln"><img src="../Images/8de824438efe94ebe711a2aed9dd385a.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*D7tSWpjNMnW7xUZO6Sj6wA.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated"><em class="lj">整体情感分类</em></figcaption></figure><p id="19ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">情感分析技术还处于起步阶段。因此，预测应该在人工干预下进行相应的评估。但是，有了大量干净的和标记的数据，足够大的神经网络将越来越准确，从长远来看，将是一个伟大的工具。</p></div></div>    
</body>
</html>