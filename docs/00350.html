<html>
<head>
<title>K-Neighbors Regression Analysis in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的k近邻回归分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-neighbors-regression-analysis-in-python-61532d56d8e4?source=collection_archive---------1-----------------------#2019-04-20">https://medium.com/analytics-vidhya/k-neighbors-regression-analysis-in-python-61532d56d8e4?source=collection_archive---------1-----------------------#2019-04-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2cb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">k最近邻是一种简单的算法，它存储所有可用的案例，并基于相似性度量(例如，距离函数)来预测数字目标。KNN作为一种非参数技术，早在20世纪70年代就已经被用于统计估计和模式识别。<strong class="ih hj">算法</strong>KNN回归的一个简单实现是计算K个最近邻的数值目标的平均值。另一种方法使用K个最近邻居的反距离加权平均值。KNN回归使用与KNN分类相同的距离函数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/5d8b241486c859830969f838792d3ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/0*M_DPxG7ORoUisdPG.png"/></div></figure><p id="4a4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上三种距离度量只对连续变量有效。在分类变量的情况下，您必须使用汉明距离，这是一种对相同长度的两个字符串中相应符号不同的实例数量的度量。</p><p id="9323" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用单个邻居的预测只是最近邻居的目标值。</p><p id="bb21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们开始动手操作，在本文中我使用来自mglearn的数据集，第一步如果您的笔记本中没有包，请在cmd/anaconda提示符下安装..</p><pre class="je jf jg jh fd jl jm jn jo aw jp bi"><span id="26a4" class="jq jr hi jm b fi js jt l ju jv">pip install mglearn</span></pre><p id="9695" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之后，您可以绘制n_neighbors = 1的k-neighbors回归。</p><pre class="je jf jg jh fd jl jm jn jo aw jp bi"><span id="a5de" class="jq jr hi jm b fi js jt l ju jv">import mglearn <br/>import matplotlib.pyplot as plt</span><span id="c8a3" class="jq jr hi jm b fi jw jt l ju jv">mglearn.plots.plot_knn_regression(n_neighbors=1)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es jx"><img src="../Images/5eb0fbc9e55705f6988d30c4dd07c9b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDCp9usTtL_d2SfIguPeiQ.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">图一。通过最近邻回归对波浪数据集进行预测</figcaption></figure><p id="0db6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，这种k-neighbors回归只使用1 n_neighbors，可以使用多个单个最近邻进行回归，并且预测的是相关邻的平均值或均值。让我们看看…</p><pre class="je jf jg jh fd jl jm jn jo aw jp bi"><span id="6809" class="jq jr hi jm b fi js jt l ju jv">mglearn.plots.plot_knn_regression(n_neighbors=3)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es jx"><img src="../Images/e4adc3ed983532ae2dd1ffdcd8f92aee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TiuvdKWf7Pgy_vVFImtpPA.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">图二。通过对波浪数据集的三最近邻回归进行预测</figcaption></figure><p id="e045" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以使用knn回归对测试数据进行预测，n_neightbors = 3</p><pre class="je jf jg jh fd jl jm jn jo aw jp bi"><span id="b456" class="jq jr hi jm b fi js jt l ju jv">from sklearn.neighbors import KNeighborsRegressor<br/>X, y = mglearn.datasets.make_wave(n_samples=40)</span><span id="ff23" class="jq jr hi jm b fi jw jt l ju jv"># split the wave dataset into a training and a test set<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span><span id="0642" class="jq jr hi jm b fi jw jt l ju jv"># instantiate the model and set the number of neighbors to consider to 3<br/>reg = KNeighborsRegressor(n_neighbors=3)</span><span id="c31c" class="jq jr hi jm b fi jw jt l ju jv"># fit the model using the training data and training targets<br/>reg.fit(X_train, y_train)</span></pre><p id="bc8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你已经完成了以上工作，你就可以在测试数据上使用你的模型了</p><pre class="je jf jg jh fd jl jm jn jo aw jp bi"><span id="a805" class="jq jr hi jm b fi js jt l ju jv">print(reg.score(reg.score(X_test, y_test)))</span></pre><p id="fdbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出:0.83</p><p id="48df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">分析近邻回归量</strong></p><p id="5a24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以分析精度如何受到n_neighbors的影响:我们可以使用不同的n_neighbors值，并解释模型的n_neighbors值。</p><pre class="je jf jg jh fd jl jm jn jo aw jp bi"><span id="6996" class="jq jr hi jm b fi js jt l ju jv">fig, axes = plt.subplots(1, 3, figsize=(15, 4))<br/># create 1,000 data points, evenly spaced between -3 and 3<br/>line = np.linspace(-3, 3, 1000).reshape(-1, 1)<br/>for n_neighbors, ax in zip([1, 3, 9], axes):<br/>    # make predictions using 1, 3, or 9 neighbors<br/>    reg = KNeighborsRegressor(n_neighbors=n_neighbors)<br/>    reg.fit(X_train, y_train)<br/>    ax.plot(line, reg.predict(line))<br/>    ax.plot(X_train, y_train, '^', c=mglearn.cm2(0),   <br/>             markersize=8)<br/>    ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize=8)<br/>    ax.set_title("{} neighbor(s)\n train score: {:.2f} test  <br/>              score: {:.2f}".format(n_neighbors,    <br/>              reg.score(X_train, y_train),reg.score(X_test, <br/>              y_test)))<br/>    ax.set_xlabel("Feature")<br/>    ax.set_ylabel("Target")<br/>axes[0].legend(["Model predictions", "Training data/target","Test   <br/>    data/target"], loc="best")</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es jx"><img src="../Images/baca9b3cb8b60a7e190653eb82bc713c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-KaKtDqGeHSdqV4i7WyGyA.png"/></div></div></figure><p id="6e2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们从图中看到的，仅使用单个邻居，训练集中的每个点对预测都有明显的影响，并且预测值贯穿所有数据点。这导致了非常不稳定的预测。考虑更多的邻居导致更平滑的预测，但是这些也不符合训练数据。</p><p id="0646" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考:安德烈亚斯·C·穆勒和萨拉·圭多。2017.pyhton机器学习简介</p></div></div>    
</body>
</html>