<html>
<head>
<title>The motivation behind Residual Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">残差神经网络背后的动机</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-residual-neural-networks-8af5b7c4afd4?source=collection_archive---------4-----------------------#2019-11-05">https://medium.com/analytics-vidhya/introduction-to-residual-neural-networks-8af5b7c4afd4?source=collection_archive---------4-----------------------#2019-11-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/1efe57480dc87e8682d183e6667b6b05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hb4l_paTxs7mFuLENeR3HQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">残差神经网络架构(<a class="ae iu" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="3d4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇文章并不是对剩余神经网络的技术解释。我敢肯定，许多教程和书籍已经存在，并在这方面做得更好。这篇文章旨在介绍ResNets背后的动机，以及为什么它们工作得很好。</p><h1 id="8c00" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">介绍</h1><p id="a676" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">所有深度学习模型都由几个堆叠的层组成，这允许模型学习我们输入的特征，并对其性质做出决定。但是网络如何检测数据中的特征呢？</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/7749ae6e61b721d6a3a1c458619400f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*Yb3Q72UevDCv_4ftViBL0A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">神经网络特征的例子(<a class="ae iu" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="a999" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">神经网络是通用函数逼近器，这意味着给定训练数据，我们的模型会尝试学习正确的参数，这些参数密切代表给出正确输出的函数。添加层增加了参数，这允许复杂非线性函数的映射。但是，如果我们遵循这个规则，堆叠更多的层不就意味着更好的性能吗？嗯，有一个条件。在实践中，我们注意到向我们的模型添加更多的层会产生更好的性能，直到我们达到某个数字，我们注意到精度不仅停滞不前，而且实际上开始下降。</p><h1 id="0a6f" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">理解大量层次的问题</strong></h1><p id="d6dd" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">首先，我们需要了解模型如何从训练数据中学习，这是通过将每个输入传递给模型(前馈)然后再传递回来(反向传播)来实现的。在反向传播过程中，我们根据模型对输入的分类程度来更新模型的权重，这由我们定义的损失函数来量化。更新包括从先前的权重值中减去损失函数相对于权重的梯度。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/1da48774451bc894276e8c5fbdd4c521.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*kWPgeDnujt6eYpoSIbG4mA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">更新权重的公式。(<a class="ae iu" rel="noopener" href="/analytics-vidhya/this-blog-post-aims-at-explaining-the-behavior-of-different-algorithms-for-optimizing-gradient-46159a97a8c1">来源</a>)</figcaption></figure><p id="c798" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在问题开始出现了。为了计算损失函数的梯度，我们必须使用<a class="ae iu" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">链式法则</a>，这意味着为了计算每一层的梯度，我们要将之前各层计算的值相乘，因为每个值都已经很小了，所以将它们相乘会得到非常接近于零的值。许多层意味着网络起点的层不会发生任何变化，这被称为消失梯度问题。虽然消失梯度是最普遍的问题，但在实践中，我们也发现可能会出现相反的情况。如果梯度值真的很大，乘以多个大数量将导致爆炸梯度问题。</p><h1 id="7c28" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">解决渐变消失问题</strong></h1><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/d808224576226bb3fbd1f925d4530439.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bPMKpBiUG2m7Qa8tt67Wvw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">一个残块(<a class="ae iu" href="https://arxiv.org/abs/1512.03385v1" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="bac6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">实际上，剩余网络是这个问题的解决方案。在残差网络中，我们添加跳过2或3层的快捷方式，该快捷方式将改变我们计算每层梯度的方式。简而言之，将输入传递到输出允许阻止一些层改变梯度的值，这意味着我们跳过对一些层的学习。这是解释梯度如何在网络中回流的最佳表示。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/c4b6fd3a9cf53d14da5eee55a9b7e5da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*iT0buqMEA8Rzo8Wz9jm7yg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">剩余块图。(<a class="ae iu" href="https://stats.stackexchange.com/questions/268820/gradient-backpropagation-through-resnet-skip-connections/268824#268824" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="e44c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们训练时，模型学习哪些层是值得保留的，那些没有帮助的层将被转向身份映射。这是残差网络成功的另一个因素，使层映射到单位函数非常容易，就像使F(x)=0一样简单。</p><p id="ab2f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以“隐藏”没有帮助的层，这一事实转化为动态的层数，我们从所有层开始，然后决定保留哪些层。</p><p id="f008" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看看这些精彩的文章:</p><p id="93f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec" rel="noopener" target="_blank">剩余块ResNet的构建块</a></p><p id="9069" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" rel="noopener" href="/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c">反向传播非常简单。谁让它变得复杂了？</a></p><p id="a214" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://stats.stackexchange.com/questions/268820/gradient-backpropagation-through-resnet-skip-connections" rel="noopener ugc nofollow" target="_blank">梯度如何流经残差网络？</a></p></div></div>    
</body>
</html>