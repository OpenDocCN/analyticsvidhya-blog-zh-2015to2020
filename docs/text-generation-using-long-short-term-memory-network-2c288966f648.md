# ä½¿ç”¨é•¿çŸ­æœŸè®°å¿†ç½‘ç»œçš„æ–‡æœ¬ç”Ÿæˆ

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/text-generation-using-long-short-term-memory-network-2c288966f648?source=collection_archive---------24----------------------->

![](img/a3072ca57310660f0b7a68cfa3d02e0f.png)

æˆ‘ä»¬å°†åœ¨æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ª LSTM ç½‘ç»œï¼Œå®ƒè‡ªå·±å­¦ä¹ ç”Ÿæˆä¸è®­ç»ƒææ–™å½¢å¼ç›¸åŒçš„æ–°æ–‡æœ¬ã€‚å¦‚æœä½ åœ¨æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒä½ çš„ LSTMï¼Œå®ƒä¼šå­¦ä¹ äº§ç”Ÿæ–°çš„å•è¯ï¼Œç±»ä¼¼äºæˆ‘ä»¬è®­ç»ƒçš„å•è¯ã€‚LSTM é€šå¸¸ä¼šä»æºæ•°æ®ä¸­å­¦ä¹ äººç±»è¯­æ³•ã€‚å½“ç”¨æˆ·åƒèŠå¤©æœºå™¨äººä¸€æ ·è¾“å…¥æ–‡æœ¬æ—¶ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ç±»ä¼¼çš„æŠ€æœ¯æ¥å®Œæˆå¥å­ã€‚

ä½¿ç”¨ *tensorflow 2.x* å¯¼å…¥æˆ‘ä»¬çš„ä¾èµ–é¡¹â€”

```
*import string
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,LSTM,Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences*
```

*è¯»å–æ•°æ®*

```
*file=open('t8.shakespeare.txt','r+')
data=file.read()*
```

## *æ–‡æœ¬æ¸…ç†*

*è·å¾—æ–‡æœ¬æ•°æ®åï¼Œæ¸…ç†æ–‡æœ¬æ•°æ®çš„ç¬¬ä¸€æ­¥æ˜¯å¯¹ä½ è¦è¾¾åˆ°çš„ç›®æ ‡æœ‰ä¸€ä¸ªæ¸…æ™°çš„è®¤è¯†ï¼Œå¹¶åœ¨è¿™ç§èƒŒæ™¯ä¸‹å›é¡¾ä½ çš„æ–‡æœ¬ï¼Œçœ‹çœ‹åˆ°åº•ä»€ä¹ˆä¼šæœ‰å¸®åŠ©ã€‚*

*åœ¨æ•°æ®ä¸­æœ‰è®¸å¤šæ ‡ç‚¹ç¬¦å·å’Œæ•°å­—å­—ç¬¦ï¼Œä»¥æ¶ˆé™¤å®ƒ*

```
*data=data.split('\n') 
data=data[253:]
data=' '.join(data)*
```

*cleaner å‡½æ•°æœ‰åŠ©äºåˆ é™¤æ•°æ®ä¸­çš„æ ‡ç‚¹å’Œæ•°å­—ï¼Œå¹¶å°†ä¸­çš„æ‰€æœ‰å­—ç¬¦è½¬æ¢ä¸ºå°å†™*

```
*def cleaner(data):
    token=data.split()
    table=str.maketrans('','',string.punctuation)
    token=[w.translate(table) for w in token]
    token=[word for word in token if word.isalpha()]
    token=[word.lower() for word in token]
    return tokenwords=cleaner(data=data)*
```

## *åˆ›é€ ä¸€ä¸ªå•è¯åºåˆ—*

*seed_length æ˜¯ 50ï¼Œè¿™æ„å‘³ç€å‰ 50 ä¸ªå•è¯å°†æ˜¯æˆ‘çš„è¾“å…¥ï¼Œä¸‹ä¸€ä¸ªå•è¯å°†æ˜¯æˆ‘çš„è¾“å‡ºã€‚å®ƒéœ€è¦å¤§é‡çš„è®¡ç®—èƒ½åŠ›å’Œå†…å­˜æ¥å¤„ç†æ‰€æœ‰æ•°æ®ã€‚æ‰€ä»¥æˆ‘åªç”¨å‰ 10 ä¸‡ä¸ªå•è¯æ¥è®­ç»ƒæˆ‘çš„ç¥ç»ç½‘ç»œã€‚*

```
*seed_length=50+1
sentence=list()
for i in range(seed_length,len(words)):
    sequence=words[i-seed_length:i]
    line=' '.join(sequence)
    sentence.append(line)
    if i >100000:
        break*
```

*ç¥ç»ç½‘ç»œè¦æ±‚å¯¹è¾“å…¥æ•°æ®è¿›è¡Œæ•´æ•°ç¼–ç ï¼Œè¿™æ ·æ¯ä¸ªå•è¯éƒ½ç”±ä¸€ä¸ªå”¯ä¸€çš„æ•´æ•°è¡¨ç¤ºã€‚ç¼–ç åå°†æ•´æ•°è½¬æ¢æˆæ•´æ•°åºåˆ—*

```
*tokenizer=Tokenizer()
tokenizer.fit_on_texts(sentence)
sequence=tokenizer.texts_to_sequences(sentence)
sequence=np.array(sequence)*
```

*åˆ†ç¦»è‡ªå˜é‡å’Œç›®æ ‡å˜é‡*

```
*X,y=sequence[:,:-1],sequence[:,-1]
vocab_size=len(tokenizer.word_index)+1
y=to_categorical(y,num_classes=vocab_size)*
```

## *åˆ›å»º LSTM ç½‘ç»œ*

*åµŒå…¥å±‚è¢«å®šä¹‰ä¸ºç½‘ç»œçš„ç¬¬ä¸€ä¸ªéšè—å±‚ã€‚å®ƒå¿…é¡»éœ€è¦ 3 ä¸ªå‚æ•°*

1.  *vocab_size â€”æ–‡æœ¬æ•°æ®ä¸­è¯æ±‡çš„å¤§å°ã€‚*
2.  *output_dim â€”å•è¯å°†åµŒå…¥å…¶ä¸­çš„å‘é‡çš„å¤§å°ã€‚*
3.  *è¾“å…¥é•¿åº¦â€”è¾“å…¥åºåˆ—çš„é•¿åº¦ã€‚*

```
*model=Sequential()
model.add(Embedding(vocab_size,50,input_length=50))
model.add(LSTM(100,return_sequences=True))
model.add(LSTM(100))
model.add(Dense(100,activation='relu'))
model.add(Dense(vocab_size,activation='softmax'))
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
model.summary()*
```

## *è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹*

*è®­ç»ƒä½ çš„æ¨¡å‹æ›´å¤šçš„æ—¶ä»£ï¼Œæˆ‘ä»¬çš„ç½‘ç»œå°†èƒ½å¤Ÿå­¦ä¹ å¦‚ä½•ç”Ÿæˆå•è¯ã€‚*

```
*model.fit(X,y,batch_size=256,epochs=1000)*
```

*generate å‡½æ•°å¸®åŠ©æˆ‘ä»¬ç”Ÿæˆ 50 ä¸ªå•è¯ä¹‹åçš„å•è¯ï¼Œä½œä¸ºæ¨¡å‹çš„è¾“å…¥*

```
*def generate(text,n_words):
    text_q=[]
    for _ in range(n_words):
        encoded=tokenizer.texts_to_sequences(text)[0]
        encoded=pad_sequences([encoded],maxlen=sequence_length,truncating='pre')
        prediction=model.predict_classes(encoded)
        for word , index in tokenizer.word_index.items():
            if index==prediction:
                predicted_word=word
                break
        text=text+' '+predicted_word
        text_q.append(predicted_word)
    return ' '.join(text_q)*
```

*ä½¿ç”¨å‡½æ•°å¹¶ç”Ÿæˆæ¥ä¸‹æ¥çš„ 100 ä¸ªå•è¯*

```
*input = sentence[0]
generate(input,100)*
```

*æ„Ÿè°¢é˜…è¯»ï¼æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« æ˜¯æœ‰å¸®åŠ©çš„ã€‚*

*ä½ ä»¬çš„è¯„è®ºå’ŒæŒå£°è®©æˆ‘æœ‰åŠ¨åŠ›åˆ›ä½œæ›´å¤šçš„ææ–™ã€‚æˆ‘å¾ˆæ¬£èµä½ ï¼ğŸ˜Š*