<html>
<head>
<title>Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-225904ee7bcc?source=collection_archive---------9-----------------------#2019-11-01">https://medium.com/analytics-vidhya/linear-regression-225904ee7bcc?source=collection_archive---------9-----------------------#2019-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5d925342420d243d525f2fbc123a3ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*S8GxFVcy_vlJM2sZ.png"/></div></div></figure><h1 id="da2e" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">什么是回归？</strong></h1><p id="04c8" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">回归分析是一种强大的统计方法，允许您检查两个或更多感兴趣的变量之间的关系。回归分析是确定哪些变量对感兴趣的主题有影响的可靠方法。执行回归的过程允许您自信地确定哪些因素最重要，哪些因素可以忽略，以及这些因素如何相互影响。</p><h2 id="d743" class="km ir hi bd is kn ko kp iw kq kr ks ja jz kt ku je kd kv kw ji kh kx ky jm kz bi translated"><strong class="ak">线性回归</strong></h2><p id="6f4d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">它是研究<em class="la">变量之间的线性</em>、<em class="la">加法</em>关系。线性回归模型的因变量是连续变量，而自变量可以采用任何形式(连续、离散或指示变量)。</p><p id="8f1e" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">为了全面理解回归分析，有必要理解以下术语:</p><ul class=""><li id="ee00" class="lg lh hi jq b jr lb jv lc jz li kd lj kh lk kl ll lm ln lo bi translated">因变量:这是你试图理解或预测的主要因素。</li><li id="4a7e" class="lg lh hi jq b jr lp jv lq jz lr kd ls kh lt kl ll lm ln lo bi translated"><strong class="jq hj">自变量:</strong>这些是你假设对因变量有影响的因素。</li></ul><h2 id="30b4" class="km ir hi bd is kn ko kp iw kq kr ks ja jz kt ku je kd kv kw ji kh kx ky jm kz bi translated"><strong class="ak">简单线性回归</strong>:</h2><p id="34fd" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">简单线性回归模型只有一个自变量，而多元线性回归模型有两个或更多自变量。对于寻找两个连续变量之间的线性关系(直线关系)很有用。(因果关系)</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/2520510f3b3aaf7b864ea30df469529c.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*SLcKnbqA_YmC-Ty0YDml-g.png"/></div></figure><p id="44af" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">必须选择b0和b1值，以使误差最小。</p><p id="7e6e" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">在线性回归中，<br/>预测因子- &gt; X - &gt;特性。<br/>预测- &gt; Y - &gt;目标变量。<br/>系数- &gt; b0，b1。</p><h2 id="b880" class="km ir hi bd is kn ko kp iw kq kr ks ja jz kt ku je kd kv kw ji kh kx ky jm kz bi translated"><strong class="ak">误差平方和</strong>:</h2><p id="dc5a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">它被认为是度量，然后目标是获得减少误差的最佳线。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/d2d5ed76531dc4131ad56833ab746c9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*yhe2EDsk6u03fGGCMCLlhg.png"/></div></figure><p id="5d34" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">如果我们不平方误差，那么正负点就会互相抵消。</p><p id="6ffb" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">X的单位变化影响Y的速率(预测)</p><p id="4b0a" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">使用OLS(普通最小二乘回归)技术计算系数。OLS技术捕捉最佳可能直线，给出预测(Y)和预测(X)变量之间更好的关系。是尽可能接近多点的线。计算每一点离直线的距离。我们从y轴得到的距离，有些是正的，有些是负的。平方然后求和。距离最小的线是可能的最佳线。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/9258644fbb6fd19db6442ec84c499734.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*rxqwxUiPVhC9prEg3BTzPQ.png"/></div></figure><p id="c50a" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">因此选择具有最小均方误差的线。它也被称为剩余平方和。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/c601fee3ca2e05dbac655c9ae6b47672.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*Ka62JJVVNt5phmqTeemtJQ.png"/></div></figure><p id="a676" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">Yi =训练集目标值。(W.Xi+b)=使用模型预测的目标值。</p><h2 id="f938" class="km ir hi bd is kn ko kp iw kq kr ks ja jz kt ku je kd kv kw ji kh kx ky jm kz bi translated"><strong class="ak"> python实现</strong></h2><p id="ad29" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">让我们以波士顿房价数据为例，</p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="91ca" class="km ir hi md b fi mh mi l mj mk">import pandas as pd<br/>import numpy as np</span><span id="b6f4" class="km ir hi md b fi ml mi l mj mk">from sklearn.datasets import load_boston<br/>boston = load_boston()<br/>print(boston.data.shape)<br/>boston.keys()<br/></span></pre><p id="d1c2" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">数据</strong></p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="8bb1" class="km ir hi md b fi mh mi l mj mk">pd.DataFrame(boston.data).head()</span></pre><p id="f02e" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">目标</strong></p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="a2db" class="km ir hi md b fi mh mi l mj mk">pd.DataFrame(boston.target).head()</span></pre><p id="3c41" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">功能名称和描述</strong></p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="5f44" class="km ir hi md b fi mh mi l mj mk">print(boston.feature_names)<br/>print(boston.DESCR)</span></pre><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/cc8b2ba1a554cf6debf0662c53bf9272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EoDK2_LWsEUt_970ROa9YQ.png"/></div></div></figure><p id="2a2d" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">创建数据帧</strong></p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="7d5e" class="km ir hi md b fi mh mi l mj mk">boston_data = pd.DataFrame(boston.data)<br/>boston_data.columns = boston.feature_names<br/>boston_data['MEDV']=boston.target<br/>print(boston_data.head())</span></pre><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/c5a6795bf4588019a236e64283487565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BueIts1bADm_3fRUOLmM7A.png"/></div></div></figure><p id="cd7c" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">可视化</strong></p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="83ac" class="km ir hi md b fi mh mi l mj mk">import matplotlib.pyplot as plt<br/>plt.hist(boston.target)<br/>plt.title('Boston Housing Prices and Count Histogram')<br/>plt.xlabel('price ($1000s)')<br/>plt.ylabel('count')<br/>plt.show()</span></pre><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/22bfe79ac3804f97d1830557a5692052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*qOTi5VqxRdwAfkB9wpt1cg.png"/></div></figure><p id="40c7" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">下面将创建一个散点图，显示每个要素名称与波士顿房价的关系。</p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="ef22" class="km ir hi md b fi mh mi l mj mk">import seaborn as sns</span><span id="b421" class="km ir hi md b fi ml mi l mj mk">sns.pairplot(boston_data,x_vars=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'], y_vars=["MEDV"])</span></pre><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/5ccdde63fbc2abe1874a6f3df4267be3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpBTscjc3s1wGx4xzQ8m7A.png"/></div></div></figure><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="7d64" class="km ir hi md b fi mh mi l mj mk">#separating the dependent and independent variables<br/>X = boston_data.drop('MEDV', axis = 1)<br/>Y = boston_data['MEDV']</span></pre><blockquote class="mq mr ms"><p id="4416" class="jo jp la jq b jr lb jt ju jv lc jx jy mt ld kb kc mu le kf kg mv lf kj kk kl hb bi translated"><strong class="jq hj">sci kit-learn中的最小二乘线性回归:</strong></p></blockquote><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="5098" class="km ir hi md b fi mh mi l mj mk">x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 0)</span><span id="9999" class="km ir hi md b fi ml mi l mj mk">Linreg = LinearRegression().fit(x_train, y_train)</span><span id="54fd" class="km ir hi md b fi ml mi l mj mk">print("Linear model intercept(b):{}".format(Linreg.intercept_))<br/>print("Linear model coeff (w):{}".format(Linreg.coef_))<br/>print("R-squared score(training): {:.3f}".format(Linreg.score(x_train,y_train)))<br/>print("R-squared score(test): {:.3f}".format(Linreg.score(x_test,y_test)))</span></pre><p id="c920" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">Y=W0。X0+b <br/> W0 = Linreg.coef_，b=linreg.intercept_</p><p id="cac5" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj"> <em class="la"> linreg.coef_ </em> </strong>和<strong class="jq hj"><em class="la">linreg . intercept _</em></strong>:-下划线表示来自训练数据的量，与用户设置相反。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/8719899ede37728e113d1367aab67372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dH2arW0bMIY07aaba5BSTQ.png"/></div></div></figure><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="eeb1" class="km ir hi md b fi mh mi l mj mk">y_pred = Linreg.predict(x_test)</span><span id="2e66" class="km ir hi md b fi ml mi l mj mk">plt.scatter(y_test, y_pred)<br/>plt.xlabel("Prices: $Y_i$")<br/>plt.ylabel("Predicted prices: $\hat{Y}_i$")<br/>plt.title("Prices vs Predicted prices: $Y_i$ vs $\hat{Y}_i$")</span></pre><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/55021d15f6b0ce73572973ec35d88bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*XLjMnbHo_O9mTv2NGnC7kA.png"/></div></figure><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="a6c5" class="km ir hi md b fi mh mi l mj mk">from sklearn.metrics import mean_squared_error<br/>mse = mean_squared_error(y_test, y_pred)<br/>print(mse)</span></pre><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es my"><img src="../Images/ba37d70c31b88004a6d9e148be5afc72.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*xw6pq3x8ZSAC07PcTBveQg.png"/></div></figure><p id="2ab3" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">这意味着这个模型不是一个真正伟大的线性模型</p><p id="b233" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">现在让我们借助于正则化来改善这个“R分数”。</p><h2 id="25f6" class="km ir hi bd is kn ko kp iw kq kr ks ja jz kt ku je kd kv kw ji kh kx ky jm kz bi translated"><strong class="ak">线性回归中的变化:</strong></h2><h2 id="10e9" class="km ir hi bd is kn ko kp iw kq kr ks ja jz kt ku je kd kv kw ji kh kx ky jm kz bi translated"><strong class="ak"> <em class="mz"> 1。</em>岭线性回归</strong>:</h2><p id="dda4" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">它使用相同的“最小平方准则”,但是增加了对“W”参数(b1)的大变化的惩罚，以避免过度拟合和复杂的模型。<br/>罚参数称为正则化<br/>岭回归采用- &gt; L2正则化</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es na"><img src="../Images/0d07154c374a3d323dd4a0fb9cd8c8b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*wSHRcREcQW5N_bnyGVyk4w.png"/></div></figure><p id="d9c6" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">具有较大特征权重(W)的模型将为目标函数的总体值增加更多。<br/>因为我们的目标是最小化整体目标函数，正则化项充当具有大量大特征权重值的模型的惩罚。<br/>·更高的Alpha意味着更规范。</p><h2 id="caa4" class="km ir hi bd is kn ko kp iw kq kr ks ja jz kt ku je kd kv kw ji kh kx ky jm kz bi translated"><strong class="ak">特征归一化的需要:</strong></h2><p id="973d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">当输入变量、特征具有非常不同的尺度时，那么当正则化系数时，具有不同尺度的输入变量将对该L2惩罚具有不同的贡献。<br/>因此，所有输入特征必须在相同的比例上。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es nb"><img src="../Images/332aeaaca3c193a358ab0bc17cd60e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*PCRSgC0IchRxcatMEL_blg.png"/></div></figure><p id="1b7d" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">使用标量对象:拟合和变换方法</strong></p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="5b50" class="km ir hi md b fi mh mi l mj mk">from sklearn.preprocessing import MinMaxScaler<br/>Scaler = MinMaxScaler()<br/>X_train_Scaled = Scaler.fit_transform(x_train)<br/>X_test_Scaled = Scaler.transform(x_test)<br/>clf = Ridge().fit(X_train_Scaled , y_train)<br/>R2_score = clf.score(X_test_Scaled, y_test)</span></pre><p id="26ae" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">正则化参数为α的岭回归</strong></p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="1a71" class="km ir hi md b fi mh mi l mj mk">from sklearn.linear_model import Ridge</span><span id="ed95" class="km ir hi md b fi ml mi l mj mk">print('Ridge regression : effect of alpha regularization parameter \n')<br/>for this_alpha in [0,1,10,20,50,100,1000]:<br/>    linridge = Ridge(alpha = this_alpha).fit(X_train_Scaled,y_train)<br/>    r2_train = linridge.score(X_train_Scaled, y_train)<br/>    r2_test = linridge.score(X_test_Scaled, y_test)<br/>    num_coeff_bigger = np.sum(abs(linridge.coef_)&gt;1.0)<br/>    print('Alpha = {:.2f}\n numabs(coeff) &gt;1.0:{}, r-&gt;squared training: {:.2f}, r_squared test: {:.2f}\n'.format(this_alpha,num_coeff_bigger,r2_train,r2_test))</span></pre><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/b7eb7744adab895ef47821fd6f2daaf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n7yCAtyoMCnbqRJbKK1N_w.png"/></div></div></figure><h2 id="820c" class="km ir hi bd is kn ko kp iw kq kr ks ja jz kt ku je kd kv kw ji kh kx ky jm kz bi translated">岭回归在做什么？</h2><p id="cf9d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">它通过对“w”系数的大小施加平方和惩罚来正则化线性回归。所以α的作用是将w系数收缩到零，并相互靠近。</p><h2 id="7341" class="km ir hi bd is kn ko kp iw kq kr ks ja jz kt ku je kd kv kw ji kh kx ky jm kz bi translated"><strong class="ak"> 2。<em class="mz">拉索回归</em> : </strong></h2><p id="c2ec" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这是另一种形式的正则化线性回归，使用L1正则化。<br/> L1惩罚:最小化系数的绝对值之和。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/64fd787960655bf749e16268bff74d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*4Ixeexo8c-f_l6xeHeyf4Q.png"/></div></figure><p id="d434" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">这具有将“w”中的参数权重设置为零的效果。影响最小的变量。这就是所谓的稀疏解。这是一种特征选择。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/d2f4dca58aa319031a31cf955572cf0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*JqYBraJ7p_zX-hScWCyOWA.png"/></div></figure><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="8297" class="km ir hi md b fi mh mi l mj mk">from sklearn.liner_model import Lasso<br/>from sklearn.preprocessing import MinMaxScaler</span><span id="2eaa" class="km ir hi md b fi ml mi l mj mk">Scaler = MinMaxScaler()<br/>X_train_Scaled = Scaler.fit_transform(x_train)<br/>X_test_Scaled = Scaler.transform(x_test)<br/>clf = Ridge().fit(X_train_Scaled , y_train)<br/>R2_score = clf.score(X_test_Scaled, y_test)</span><span id="e30f" class="km ir hi md b fi ml mi l mj mk">linlasso = Lasso(alpha=0.0001, max_iter=10e5).fit(X_train_Scaled,y_train)<br/>print('lasso regression linear model intercept:{}'.format(linlasso.intercept_))<br/>print('lasso regression linear model coeff:\n{}'.format(linlasso.coef_))<br/>print('Non-zero feature :{}'.format(np.sum(linlasso.coef_!=0)))<br/>print('R-Squared score(training):{:.3f}'.format(linlasso.score(X_train_Scaled, y_train)))<br/>print('R-Squared score(test):{:.3f}\n'.format(linlasso.score(X_test_Scaled,y_test)))</span></pre><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nf"><img src="../Images/6bced113a04eebd4d7f265c73a0ac373.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JLQPGItmrgvK-mvfhVRKcA.png"/></div></div></figure><p id="1f6f" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">什么时候使用山脊vs套索回归？</strong> <br/>许多小型/中型效果:使用山脊<br/>只有少数变量具有中型/大型效果:使用套索</p><h2 id="3dc3" class="km ir hi bd is kn ko kp iw kq kr ks ja jz kt ku je kd kv kw ji kh kx ky jm kz bi translated"><strong class="ak"> 3。<em class="mz">线性回归多项式特征</em> : </strong></h2><p id="a682" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">生成由原始两个要素(X0，X1)的所有多项式组合组成的新要素。<br/>多项式的次数指定了每次有多少变量参与每个新特征。(我们的eg: degree 2) <br/>这仍然是特征的加权线性组合，因此它仍然是线性模型，并且可以对“w”和“b”(系数)使用相同的最小二乘消除方法。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/31754f3716bff0908813d0aadceed8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*Wtuswyzl4A9GP_fCZFO_Bg.png"/></div></figure><p id="5fca" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">我们为什么要转换数据？</strong> <br/>通过将原始特征作为特征添加到线性模型中来捕捉它们之间的相互作用。<br/>·简化分类问题。</p><p id="a5e5" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj"> <em class="la">例如:</em> </strong>作为一个理论上的例子，房价可能是作为房屋坐落的土地价格和为该房产支付的税款的二次函数而变化的。<br/>简单的线性模型无法捕捉这种非线性关系，但通过在线性回归模型中添加多项式等非线性特征，我们可以捕捉这种非线性关系。</p><p id="aff0" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">小心高次多项式特征扩展，因为这会导致过度拟合的复杂模型。<br/>因此，多项式特征扩展通常与岭回归等正则化学习方法相结合。</p><p id="c8d0" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">现在同时运行所有三个并检查:</strong></p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="e465" class="km ir hi md b fi mh mi l mj mk">from sklear.linear_model import LinearRgression<br/>from sklearn.linear_model import Ridge<br/>from sklearn.preprocessing import PolynomialFeatures</span><span id="9aac" class="km ir hi md b fi ml mi l mj mk">Poly = PolynomialFeatures(degree =2)<br/>X_F1_poly = Poly.fit_transform(X)<br/>x_poly_train, x_poly_test, y_train, y_test = train_test_split(X_F1_poly, Y, random_state = 0)</span><span id="dae0" class="km ir hi md b fi ml mi l mj mk">linreg = LinearRegression().fit(x_poly_train, y_train)<br/>print('(poly deg 2) Linear model coeff(W):\n{}'.format(linreg.coef_))<br/>print('(poly deg 2) Linear model intercept(b):{:.3f}'.format(linreg.intercept_))<br/>print('(poly deg 2) R-Squared score (training):{:.3f}'.format(linreg.score(x_poly_train, y_train)))<br/>print('(poly deg 2) R-Squared score (test):{:.3f}'.format(linreg.score(x_poly_test, y_test)))</span></pre><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/9891a1f4bdb5fdc4a532c8340bdec0fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*Jaeawim27MtxTgHDaVb5nA.png"/></div></figure><p id="7e82" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">添加许多多项式特征通常会导致过度拟合，因此我们经常将多项式特征与具有正则化惩罚的回归(如岭回归)结合使用。</p><p id="904e" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">岭回归避免过拟合:</strong></p><pre class="lv lw lx ly fd mc md me mf aw mg bi"><span id="8d44" class="km ir hi md b fi mh mi l mj mk">Poly = PolynomialFeatures(degree =2)<br/>X_F1_poly1 = Poly.fit_transform(X)<br/>x_poly_train1, x_poly_test1, y_train, y_test = train_test_split(X_F1_poly1, Y, test_size=0.4, random_state = 0)</span><span id="854b" class="km ir hi md b fi ml mi l mj mk">linrid = Ridge(alpha = 0.001).fit(x_poly_train1, y_train)<br/>print('(poly deg 2) Linear model coeff(W):\n{}'.format(linrid.coef_))<br/>print('(poly deg 2) Linear model intercept(b):{:.3f}'.format(linrid.intercept_))<br/>print('(poly deg 2) R-Squared score (training):{:.3f}'.format(linrid.score(x_poly_train1, y_train)))<br/>print('(poly deg 2) R-Squared score (test):{:.3f}'.format(linrid.score(x_poly_test1, y_test))))</span></pre><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/ba254c407ac73da8724ed80572538be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-W0Rzt32fYle995TnzJEiQ.png"/></div></div></figure><p id="639f" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">正如我们从上面的输出中所看到的，带有岭回归的多项式特征在没有过度拟合的情况下给出了最佳的可能输出。</p><p id="53e5" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj"> <em class="la"> 1。第一次回归</em> </strong>:只使用最小二乘回归，没有多项式特征变换。<br/> <strong class="jq hj"> 2。</strong> <strong class="jq hj"> <em class="la">第二次回归</em> </strong>:创建次数设置为2的多项式特征对象，然后调用多项式特征的‘fit _ transform’方法。然后代码调用普通最小二乘线性回归。<br/>我们可以在这个扩展的特征表示上看到过度拟合的迹象，因为训练集上的模型R平方分数接近1，但是在测试集上低得多。<br/> <strong class="jq hj"> 3。<em class="la">第三次回归</em> </strong>:训练和测试集基本相同。正则化多项式回归的测试分数在所有三个回归中表现最好。</p><p id="b440" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">还有其他评估指标。但在本文中，我主要集中在R平方得分和MSE(均方误差)来解释线性回归和线性回归的变化及其对准确性的影响，并避免过度拟合。</p><p id="5d01" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">实施代码请参见“<a class="ae ni" href="https://github.com/rohithramesh1991/Linear-Regression" rel="noopener ugc nofollow" target="_blank">https://github.com/rohithramesh1991/Linear-Regression</a></p><p id="ffc1" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">感谢您的阅读…</p></div></div>    
</body>
</html>