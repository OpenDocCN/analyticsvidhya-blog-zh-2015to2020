<html>
<head>
<title>Faster R-CNN : Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更快的R-CNN:目标检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/faster-r-cnn-object-detection-7e48e5b9a906?source=collection_archive---------7-----------------------#2020-10-12">https://medium.com/analytics-vidhya/faster-r-cnn-object-detection-7e48e5b9a906?source=collection_archive---------7-----------------------#2020-10-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f7ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">简介</strong></p><p id="ce35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对象检测涉及图像的识别或分类及其分割。分割是通过在感兴趣的对象上绘制边界框来实现的。对象检测通常用于定位图像中的对象。用于检测对象的最流行的方法采用R-CNN或YOLO架构。</p><p id="cf5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最初设计的R-CNN经过多年的发展，已经发展到快速R-CNN，然后发展到更快的R-CNN架构。就运行速度和原始性能而言，每一款都比前一款有所改进。本文的目标是介绍更快的R-CNN架构的工作原理和主要组件。</p><p id="299e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更快的R-CNN论文最早发表于2015年。快速R-CNN是对其前身快速R-CNN的改进。快速R-CNN使用选择性搜索算法来建议可以找到对象的区域。但在更快的R-CNN中，建议是作为卷积运算本身的一部分生成的。这提高了它在执行目标检测的整体任务方面的效率和速度。</p><p id="f75b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更快的R-CNN的主要组件是区域提议网络和ROI池，以及分类器和回归器头，以获得预测的类别标签和位置。</p><p id="a842" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将探索更快的R-CNN的每个组件。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/9754cddd326bb2e220657bb353a8cc1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*DWVDN2U4nTYiqgdNgT_ulA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图1:更快的RCNN架构</figcaption></figure><p id="62a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">主播</strong></p><p id="7f4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">锚点是可以检测到对象的潜在边界框候选。它们是在训练开始之前根据长宽比和比例的组合预先定义的，并放置在整个图像中。更快的R-CNN利用3种纵横比和3种比例生成3*3 = 9种组合。</p><p id="cc73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑0.5的纵横比和[8，16，32]的比例组合。在将图像通过conv池VGG网络之后，最终的下采样图像步幅大小是16。现在，与上面的结果组合将是:-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jp"><img src="../Images/9a6edd0c41481c56d5fafb055427c42a.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*S-0--pfK-uDU0X_kHuJEoA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图2:锚</figcaption></figure><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jq jr l"/></div></figure><p id="57a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们得到3个宽度大于高度的矩形。类似地，考虑比率为1，我们得到正方形的3种变体，比率为2，将有高度大于宽度的矩形的3种变体。这形成了总共有9种变化的基础锚。</p><p id="fb14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们已经获得了基础锚点，下一步就是为图像生成所有可能的锚点。</p><p id="e653" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果通过VGGNet传递图像后，我们得到的高度、宽度、步幅为H，W，F，那么原始图像的尺寸为(H x F，W x F)。</p><p id="5fff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基础锚定中心可以位于所有这些位置。[(0，0)，(F，0)，(0，F)，(F，F)..(H x F，W x F)]这导致H x W x 9个图像锚点的总数。</p><p id="569e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个锚点由(x1，y1，x0，y0)表示，其中x1，y1是框的左上角，x0，y0是框的右下角。</p><p id="f909" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">地区提案网</strong></p><p id="5f49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RPN负责从上一步生成的图像锚点中返回一组好的建议。RPN使用由图像特征提取器生成的特征图，并将其通过两个平行的卷积层，以给出两组输出。</p><p id="c770" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一个卷积层给出边界框回归器输出。这些输出的目的不是精确定位边界框本身的直接位置，而是预测偏移和比例，这些将应用于图像锚点以改进预测。</p><p id="524e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RPN还输出分类输出，指示边界框是前景还是背景的概率。由于有许多图像锚点，因此需要一种方法来选择最有可能检测到对象的框，并丢弃剩余的框。利用许多锚彼此重叠的事实，进行非最大抑制来实现这一点。</p><p id="f13a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">非最大抑制</strong></p><p id="5f6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非最大抑制(NMS)应用于预测的边界框，使用它们的预测分数作为过滤标准。在将其传递给NMS之前，感兴趣区域(ROI)通过剪裁和移除高度、宽度超过阈值的区域来进行预处理。仅取最高的ROI，并按其置信度得分进行排序。</p><p id="05b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NMS的工作方式是一个接一个地选取所有感兴趣区域，并与其他所有感兴趣区域进行比较。如果该比较的IOU值大于预定阈值，则从列表中弹出后一个ROI。这确保了不会有太多多余的盒子塞满图像。重复这个过程，直到没有剩余的盒子。</p><p id="9fc2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终列表中的最高ROI被单独传递到下一步。</p><p id="7fa8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">投资回报池和VGG-Head </strong></p><p id="21ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来在这些选定的ROI上进行ROI汇集，以产生固定大小的特征图。ROI Pooling将每个特征映射分割成多个区域，并对它们应用最大池。这样做主要是为了确保所有建议可以使用相同的特征图，从而便于在下一步中将整个图像直接传递给VGG头。</p><p id="1fb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">VGG头预测将应用于它们的边界框偏移/比例，用于进一步细化，以及它们的相应分数，指示每个预定义类别标签的概率。</p><p id="5d5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">亏损</strong></p><p id="42eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总共有四组损失供模型训练。这4个损失从2层获得:- RPN和ROI池/头部层。</p><p id="bccf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更快的R-CNN对回归变量使用平滑L1损失，对分类器使用交叉熵来计算损失。来自这四层的损失被累积并反向传播以训练整个模型。</p><p id="a45c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> RPN </strong></p><p id="fa22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算基础真实边界框(GTBb)和图像锚之间的IOU。</p><ol class=""><li id="7541" class="js jt hi ih b ii ij im in iq ju iu jv iy jw jc jx jy jz ka bi translated"><strong class="ih hj">边界框偏移/比例:- <br/> </strong>预测的偏移/比例用图像锚点解码得到最终的边界框。同样，图像锚以其最大IOU GTBb进行编码，以获得失调/比例差异。</li><li id="151b" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated"><strong class="ih hj">包围盒置信度得分:- <br/> </strong>预测的置信度得分指示对象的存在以及它是背景还是前景。对于基本事实标签，那些有借据的，高于正阈值的用标签1标记，低于负阈值的用标签0标记。其余的标记为-1，表示可以忽略。</li></ol><p id="5123" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ROI </strong></p><p id="0f3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算地面真实边界框(GTBb)和所选ROI之间的IOU。</p><ol class=""><li id="9060" class="js jt hi ih b ii ij im in iq ju iu jv iy jw jc jx jy jz ka bi translated"><strong class="ih hj">边界框偏移/比例:<br/></strong>VGG头预测所选感兴趣区域的偏移/比例。为了获得地面真实偏移/比例差异，所选ROI用其相应的最大IOU GTBb编码。</li><li id="65a7" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated"><strong class="ih hj">边界框标签:<br/></strong>VGG头预测所选感兴趣区域的标签。必须为每个选定的ROI分配一个基础事实类别标签，用于计算损失。每个GTBb已经被分配了一个标签。使用相同的方法，计算每个所选ROI对应的最大IOU GTBb，并且该GTBb的标签与ROI相关联。</li></ol><p id="9adc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结论</strong></p><p id="d93b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文到此结束，重点介绍了更快的R-CNN架构的关键组件。继续进行<a class="ae kg" href="https://github.com/Sai-Venky/FasterRCNN" rel="noopener ugc nofollow" target="_blank">https://github.com/Sai-Venky/FasterRCNN</a>中的代码将为该架构的整体实现提供坚实的基础。</p><p id="cf42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快乐阅读…</p><p id="eb35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献</strong></p><ol class=""><li id="4bd6" class="js jt hi ih b ii ij im in iq ju iu jv iy jw jc jx jy jz ka bi translated"><a class="ae kg" href="https://www.alegion.com/faster-r-cnn" rel="noopener ugc nofollow" target="_blank">https://www.alegion.com/faster-r-cnn</a></li><li id="fea7" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated"><a class="ae kg" href="https://leimao.github.io/blog/Bounding-Box-Encoding-Decoding/" rel="noopener ugc nofollow" target="_blank">https://lei Mao . github . io/blog/Bounding-Box-Encoding-Decoding/</a></li><li id="0454" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated"><a class="ae kg" rel="noopener" href="/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8">https://medium . com/@ smallfishbigsea/faster-r-CNN-explained-864d 4 FB 7 E3 f 8</a></li></ol></div></div>    
</body>
</html>