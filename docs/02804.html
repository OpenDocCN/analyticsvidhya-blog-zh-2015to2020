<html>
<head>
<title>Understanding Convolution Neural Networks -Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解卷积神经网络-第二部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-convolution-neural-networks-part-ii-10ddfd6b8bcd?source=collection_archive---------23-----------------------#2020-01-02">https://medium.com/analytics-vidhya/understanding-convolution-neural-networks-part-ii-10ddfd6b8bcd?source=collection_archive---------23-----------------------#2020-01-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="dab9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文是<a class="ae jd" rel="noopener" href="/@mustufain.abbas/understanding-convolution-neural-networks-part-i-e86c14a34be3">第一部分</a>的延续。如果你没有读过《T2》第一部，我强烈建议你去读一读</p><p id="2003" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第二部分，我们将建立如图1.1所示的网络</p><p id="1676" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们定义一些我们将在本文中使用的术语。</p><p id="77d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f =过滤器尺寸</p><p id="bbc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">n_filers =过滤器数量</p><p id="4c89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">p =填充</p><p id="35b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">s =步幅</p><p id="cfb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">m =培训示例的数量</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/6436a3434572ff31ef2e6aa1666de328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e5IrvlQBLvpDnmkM5wV_Gw.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.1卷积神经网络</figcaption></figure><p id="1ec8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据</strong></p><p id="4204" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用包含64 x 64 x 3形状图像的数字符号数据集。训练集由1080幅图像组成，测试集由120幅图像组成。类别数为6，包含数字符号0、1、2、3、4、5。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ju"><img src="../Images/c86e43089773e02f7fd9e4198eb74f5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:128/format:webp/1*563puhwnen2wv1c9ADq9fA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.2显示4位数字符号的图像。</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ju"><img src="../Images/a716a07ed54fc758cfc78d007498a783.png" data-original-src="https://miro.medium.com/v2/resize:fit:128/format:webp/1*aU2i-aYfdlYG8K1v2sT8rQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.3显示4位数字符号的图像。</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ju"><img src="../Images/3af003d53ea83cdaf156cf9b5c65e3d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:128/format:webp/1*2StHunfliGyNAPSBNsgkMg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.4显示5位数标志的图像。</figcaption></figure><p id="1de2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将利用我们在<a class="ae jd" href="https://medium.com/p/e86c14a34be3/edit" rel="noopener">第一部分</a>中所学的概念，从头开始构建一个卷积神经网络。</p><p id="ee49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">卷积神经网络</strong></p><p id="7857" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图1.1中描述的网络由<em class="jv">卷积层</em>，r <em class="jv"> elu层</em>，<em class="jv"> max pool层</em>，<em class="jv">平坦层</em>，<em class="jv">密集层</em>组成，后面是<em class="jv"> softmax激活函数</em>，因为我们有不止一个类。我们将利用<em class="jv"> Adam优化器</em>来训练网络。在所有层中，使用he初始化[He <em class="jv"> et初始化权重。艾尔。随着网络被重新激活。</em></p><h1 id="7de8" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">正向传播</strong></h1><h2 id="f2dc" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated"><strong class="ak">卷积层</strong></h2><p id="0d63" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">卷积层的超参数是:</p><ul class=""><li id="ae35" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated"><em class="jv"> p </em>是2</li><li id="f806" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><em class="jv"> s </em>是2</li><li id="cec9" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><em class="jv"> f </em>是3×3</li><li id="fbb0" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><em class="jv"> n_filters </em>是10</li></ul><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.5卷积的单步</figcaption></figure><p id="634f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">卷积层中的正向传播包括三个步骤:</p><ul class=""><li id="c6f2" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">用数量<em class="jv"> p </em>为图像填充零</li></ul><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="6787" class="ku jx hi me b fi mi mj l mk ml">def zero_pad(self, X, pad):<br/>    <em class="jv">"""<br/>    Set padding to the image X.<br/><br/>    Pads with zeros all images of the dataset X.<br/>    Zeros are added around the border of an image.<br/><br/>    Parameters:<br/>    X -- Image -- numpy array of shape (m, n_H, n_W, n_C)<br/>    pad -- padding amount -- int<br/><br/>    Returns:<br/>    X_pad -- Image padded with zeros around width and height. -- numpy array of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)<br/><br/>    """<br/>    </em>X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant')<br/>    return X_pad</span></pre><ul class=""><li id="0b11" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">基于<em class="jv"> s </em>获取图像窗口</li></ul><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="189a" class="ku jx hi me b fi mi mj l mk ml">def get_corners(self, height, width, filter_size, stride):<br/>    <em class="jv">"""<br/>    Get corners of the image relative to stride.<br/><br/>    Parameters:<br/>    height -- height of an image -- int<br/>    width -- width of an image -- int<br/>    filter_size -- size of filter -- int<br/>    stride -- amount by which the filter shifts -- int<br/><br/>    Returns:<br/>    vert_start -- a scalar value, upper left corner of the box.<br/>    vert_end -- a scalar value, upper right corner of the box.<br/>    horiz_start -- a scalar value, lower left corner of the box.<br/>    horiz_end -- a scalar value, lower right corner of the box.<br/><br/>    """<br/>    </em>vert_start = height * stride<br/>    vert_end = vert_start + filter_size<br/>    horiz_start = width * stride<br/>    horiz_end = horiz_start + filter_size<br/>    return vert_start, vert_end, horiz_start, horiz_end</span></pre><ul class=""><li id="587e" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">应用卷积运算，用<em class="jv"> f. </em>做图像窗口的逐元素乘积</li></ul><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="b3b0" class="ku jx hi me b fi mi mj l mk ml">def convolve(self, image_slice, W, b):<br/>    <em class="jv">"""<br/>    Apply a filter defined by W on a single slice of an image.<br/><br/>    Parameters:<br/>    image_slice -- slice of input data -- numpy array of shape (f, f, n_C_prev)<br/>    W -- Weight parameters contained in a window - numpy array of shape (f, f, n_C_prev)<br/>    b -- Bias parameters contained in a window - numpy array of shape (1, 1, 1)<br/><br/>    Returns:<br/>    Z -- a scalar value, result of convolving the sliding window (W, b) on image_slice<br/><br/>    """<br/>    </em>s = np.multiply(image_slice, W)<br/>    z = np.sum(s)<br/>    Z = z + float(b)<br/>    return Z</span></pre><p id="fe59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">综上所述，卷积层中的前向传播将是以下完整的训练数据。</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="2096" class="ku jx hi me b fi mi mj l mk ml">def forward(self, A_prev):<br/>    <em class="jv">"""<br/>    Forward proporgation for convolution.<br/><br/>    This takes activations from previous layer and then convolve it<br/>    with a filter defined by W with bias b.<br/><br/>    Parameters:<br/>    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)<br/><br/>    Returns:<br/>    Z -- convolution output, numpy array of shape (m, n_H, n_W, n_C)<br/><br/>    """<br/>    </em>np.random.seed(self.seed)<br/>    self.A_prev = A_prev<br/>    filter_size, filter_size, n_C_prev, n_C = self.params[0].shape<br/>    m, n_H_prev, n_W_prev, n_C_prev = self.A_prev.shape<br/>    Z = np.zeros((m, self.n_H, self.n_W, self.n_C))<br/>    A_prev_pad = self.zero_pad(self.A_prev, self.pad)<br/><br/>    for i in range(m):<br/>        a_prev_pad = A_prev_pad[i, :, :, :]<br/>        for h in range(self.n_H):<br/>            for w in range(self.n_W):<br/>                for c in range(n_C):<br/>                    vert_start, vert_end, horiz_start, horiz_end = self.get_corners(h, w, self.filter_size, self.stride)<br/>                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]<br/>                    Z[i, h, w, c] = self.convolve(<br/>                            a_slice_prev, self.params[0][:, :, :, c], self.params[1][:, :, :, c])<br/>    assert (Z.shape == (m, self.n_H, self.n_W, self.n_C))<br/>    return Z</span></pre><p id="ad6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">卷积层的输出将是形状(m，33，33，10)。计算卷积层输出高度和宽度的一般公式:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mm"><img src="../Images/9a7c01591a936691c21c421410e337eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*yJ1WK_E_juSw87kP_lb4QA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.6卷积层的输出高度和宽度。</figcaption></figure><p id="0e32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中n是输入尺寸。这有助于在实现过程中检查矩阵形状。然后输出形状变为(m，高度，宽度，通道数)，其中通道数等于n_filters。</p><h2 id="fdab" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated"><strong class="ak">热卢层</strong></h2><p id="2d5c" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">ReLU中的正向传播</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="0a12" class="ku jx hi me b fi mi mj l mk ml">def forward(self, Z):<br/>    <em class="jv">"""<br/>    Forward propogation of relu layer.<br/><br/>    Parameters:<br/>    Z -- Input data -- numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)<br/><br/>    Returns:<br/>    A -- Activations of relu layer-- numpy array of shape m, n_H_prev, n_W_prev, n_C_prev)<br/><br/>    """<br/>    </em>self.Z = Z<br/>    A = np.maximum(0, Z)  # element-wise<br/>    return A</span></pre><p id="c378" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">relu层的输入是卷积层的输出。relu layer不会改变矩阵尺寸，因此输出形状保持不变。</p><h2 id="29ed" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated">最大池层</h2><p id="444f" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">最大池的超级参数是:</p><ul class=""><li id="df58" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated"><em class="jv"> f </em>是2×2</li><li id="94a0" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><em class="jv"> s </em>是1</li></ul><p id="4056" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">maxpool层中的前向传播包括两个步骤:</p><ul class=""><li id="b9b4" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">根据<em class="jv"> s </em>得到输入窗口</li></ul><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="f81d" class="ku jx hi me b fi mi mj l mk ml">def get_corners(self, height, width, filter_size, stride):<br/>    <em class="jv">"""<br/>    Get corners of the image relative to stride.<br/><br/>    Parameters:<br/>    height -- height of an image -- int<br/>    width -- width of an image -- int<br/>    filter_size -- size of filter -- int<br/>    stride -- amount by which the filter shifts -- int<br/><br/>    Returns:<br/>    vert_start -- a scalar value, upper left corner of the box.<br/>    vert_end -- a scalar value, upper right corner of the box.<br/>    horiz_start -- a scalar value, lower left corner of the box.<br/>    horiz_end -- a scalar value, lower right corner of the box.<br/><br/>    """<br/>    </em>vert_start = height * stride<br/>    vert_end = vert_start + filter_size<br/>    horiz_start = width * stride<br/>    horiz_end = horiz_start + filter_size<br/>    return vert_start, vert_end, horiz_start, horiz_end</span></pre><ul class=""><li id="5392" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">在输入窗口上应用Maxpool操作，并在完整的训练数据上向前传播。</li></ul><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="5b71" class="ku jx hi me b fi mi mj l mk ml">def forward(self, A_prev):<br/>    <em class="jv">"""<br/>    Forward prpogation of the pooling layer.<br/><br/>    Arguments:<br/>    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)<br/><br/>    Returns:<br/>    Z -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)<br/><br/>    """<br/>    </em>self.A_prev = A_prev<br/>    m, n_H_prev, n_W_prev, n_C_prev = self.A_prev.shape<br/>    Z = np.empty((m, self.n_H, self.n_W, n_C_prev))<br/>    for i in range(m):<br/>        a_prev = self.A_prev[i]<br/>        for h in range(self.n_H):<br/>            for w in range(self.n_W):<br/>                for c in range(self.n_C):<br/>                    vert_start, vert_end, horiz_start, horiz_end = self.get_corners(<br/>                        h, w, self.filter_size, self.stride)<br/>                    #if horiz_end &lt;= a_prev.shape[1] and vert_end &lt;= a_prev.shape[0]:<br/>                    a_slice_prev = a_prev[<br/>                            vert_start:vert_end, horiz_start:horiz_end, c]<br/>                    Z[i, h, w, c] = np.max(a_slice_prev)<br/>    assert(Z.shape == (m, self.n_H, self.n_W, n_C_prev))<br/>    return Z</span></pre><p id="9653" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Maxpool层的输出将是(m，32，32，10)的形状。计算maxpool层输出高度和宽度的通用公式如图1.6所示。然后，输出形状变为(m，高度，宽度，通道数)，其中通道数是输入维度的最后一个轴。</p><h2 id="e8c7" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated">展平图层</h2><p id="fda6" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">平坦层中正向传播</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="f46c" class="ku jx hi me b fi mi mj l mk ml">def forward(self, A_prev):<br/>    <em class="jv">"""<br/>    Forward propogation of flatten layer.<br/><br/>    Parameters:<br/>    A_prev -- input data -- numpy of array shape (m, n_H_prev, n_W_prev, n_C_prev)<br/><br/>    Returns:<br/>    Z -- flatten numpy array of shape (m, n_H_prev * n_W_prev * n_C_prev)<br/><br/>    """<br/>    </em>np.random.seed(self.seed)<br/>    self.A_prev = A_prev<br/>    output = np.prod(self.A_prev.shape[1:])<br/>    m = self.A_prev.shape[0]<br/>    self.out_shape = (self.A_prev.shape[0], -1)<br/>    Z = self.A_prev.ravel().reshape(self.out_shape)<br/>    assert (Z.shape == (m, output))<br/>    return Z</span></pre><p id="dba5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出形状为(m，10240)。</p><h2 id="6590" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated">致密层</h2><p id="86c9" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">这是一个全连接的神经网络层。</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="456f" class="ku jx hi me b fi mi mj l mk ml">def forward(self, A_prev):<br/>    <em class="jv">"""<br/>    Forward propogation of Dense layer.<br/><br/>    Parameters:<br/>    A_prev -- input data -- numpy of array shape (m, input_dim)<br/><br/>    Returns:<br/>    Z -- flatten numpy array of shape (m, output_dim)<br/><br/>    """<br/>    </em>np.random.seed(self.seed)<br/>    m = A_prev.shape[0]<br/>    self.A_prev = A_prev<br/>    Z = np.dot(self.A_prev, self.params[0]) + self.params[1]<br/>    assert (Z.shape == (m, self.output_dim))<br/>    return Z</span></pre><p id="ddfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这主要用在损失函数之前。因为类的数量是6，所以这一层的输出形状将是类的数量。</p><h2 id="d206" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated"><strong class="ak"> Softmax损失</strong></h2><p id="d990" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">因为这是一个多类分类问题，所以我们将使用softmax损失函数，也称为类别交叉熵损失。我们将使用softmax激活函数为每个单独的类生成概率，所有概率之和为1，如图1.6所示。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/c6d5db437bf905d6296d022fc1de236f.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*aWdymANNua9F81y0_oJv0A.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.6软最大概率</figcaption></figure><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="a54e" class="ku jx hi me b fi mi mj l mk ml">def softmax(z):<br/>    <em class="jv">"""<br/><br/>    </em><strong class="me hj"><em class="jv">:param</em></strong><em class="jv"> Z: output of previous layer of shape (m, 6)<br/>    </em><strong class="me hj"><em class="jv">:return</em></strong><em class="jv">: probabilties of shape (m, 6)<br/>    """<br/><br/>    </em># numerical stability<br/>    z = z - np.expand_dims(np.max(z, axis=1), 1)<br/>    z = np.exp(z)<br/>    ax_sum = np.expand_dims(np.sum(z, axis=1), 1)<br/><br/>    # finally: divide elementwise<br/>    A = z / ax_sum<br/>    return A</span></pre><p id="5d82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Softmax函数容易出现两个问题:<strong class="ih hj">溢出</strong>和<strong class="ih hj">下溢</strong></p><p id="0c8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">溢出</strong>:这意味着在爆炸梯度的情况下，权重会显著增加，这使得概率变得无用。</p><p id="87bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">下溢</strong>:它发生在消失梯度的情况下，权重可能接近于零，因此具有相同的概率。</p><p id="155c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了在进行softmax计算时解决这些问题，一个常见的技巧是通过从所有元素中减去最大元素来移动输入向量。对于输入向量z，定义z如下:</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="8494" class="ku jx hi me b fi mi mj l mk ml">z = z - np.expand_dims(np.max(z, axis=1), 1)</span></pre><p id="f52f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jv"> m </em>训练数据的损失函数定义为:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mo"><img src="../Images/9c7b8096201f69aea6483356b21d8662.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*jfnfyPgfenn9n1qkXeKpIw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.7 Softmax损失函数</figcaption></figure><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="bf37" class="ku jx hi me b fi mi mj l mk ml">def softmaxloss(x, labels):<br/>    <em class="jv">"""<br/>    </em><strong class="me hj"><em class="jv">:param</em></strong><em class="jv"> x: output of previous layer of shape (m, 6)<br/>    </em><strong class="me hj"><em class="jv">:param</em></strong><em class="jv"> labels: class labels of shape (1, m)<br/>    </em><strong class="me hj"><em class="jv">:return</em></strong><em class="jv">:<br/>    """<br/><br/>    </em>one_hot_labels = convert_to_one_hot(labels, 6)<br/>    predictions = softmax(x)<br/>    epsilon = 1e-12<br/>    predictions = np.clip(predictions, epsilon, 1. - epsilon)<br/>    N = predictions.shape[0]<br/>    loss = -np.sum(one_hot_labels * np.log(predictions + 1e-9)) / N<br/>    grad = predictions.copy()<br/>    grad[range(N), labels] -= 1<br/>    grad /= N<br/>    return loss, grad</span></pre><p id="8d83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在损失函数中，我们通过<em class="jv">ε</em>对softmax预测进行了剪裁，以防止可能导致数值不稳定的过大值。</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="4a60" class="ku jx hi me b fi mi mj l mk ml">epsilon = 1e-12<br/>predictions = np.clip(predictions, epsilon, 1. - epsilon)</span></pre><h1 id="80e0" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">反向传播</strong></h1><h2 id="b08b" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated">软最大损失</h2><p id="2a86" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">现在，我们将传播我们的梯度回到第一层。首先，我们将使用softmax计算相对于密集层的交叉熵损失的导数(输入到softmax)。</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="7a70" class="ku jx hi me b fi mi mj l mk ml">grad = predictions.copy()<br/>grad[range(N), labels] -= 1<br/>grad /= N</span></pre><h2 id="eb44" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated">致密层</h2><p id="7f15" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">在致密层中，我们接收相对于致密层的交叉熵损失的梯度作为输入。然后，我们计算dW、db和dA_prev。<em class="jv">注意，我们会互换使用损失或成本函数</em>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mp"><img src="../Images/2016f4a7175539603a514fa94d7d8e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*E1J5NvWXyH75N7JIUSHf7w.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.8相对于成本函数的权重梯度。</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mq"><img src="../Images/d2fbcedd6ae7c5f50d76aa58cc41595f.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*6UFfmKRx4wBFggzw8xskPQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.9相对于成本函数的偏差梯度。</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mr"><img src="../Images/3ff38ee9be3cb36954de07ee5a795a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*WCnIay2_JOE_8xXr5r-wSQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1.10相对于密集层输入的成本梯度。</figcaption></figure><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="057c" class="ku jx hi me b fi mi mj l mk ml">def backward(self, dA):<br/>    <em class="jv">"""<br/>    Backward propogation for Dense layer.<br/><br/>    Parameters:<br/>    dA -- gradient of cost with respect to the output of the Dense            layer, same shape as Z<br/><br/>    Returns:<br/>    dA_prev -- gradient of cost with respect to the input of the .                Dense layer, same shape as A_prev<br/><br/>    """<br/><br/>    </em>np.random.seed(self.seed)<br/>    m = self.A_prev.shape[0]<br/>    dW = np.dot(self.A_prev.T, dA)<br/>    db = np.sum(dA, axis=0, keepdims=True)<br/>    dA_prev = np.dot(dA, self.W.T)<br/>    assert (dA_prev.shape == self.A_prev.shape)<br/>    assert (dW.shape == self.params[0].shape)<br/>    assert (db.shape == self.params[1].shape)<br/><br/>    return dA_prev, [dW, db]</span></pre><h2 id="f01b" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated">展平图层</h2><p id="8f83" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">展平层没有参数来训练，所以我们不会计算dW和db。对于向后传播梯度，它只是将dA整形为A_prev，即(m，10240)。</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="41ee" class="ku jx hi me b fi mi mj l mk ml">def backward(self, dA):<br/>    <em class="jv">"""<br/>    Backward propogation of flatten layer.<br/><br/>    Parameters:<br/>    dA -- gradient of cost with respect to the output of the flatten layer, same shape as Z<br/><br/>    Returns:<br/>    dA_prev -- gradient of cost with respect to the input of the flatten layer, same shape as A_prev<br/><br/>    """<br/>    </em>np.random.seed(self.seed)<br/>    dA_prev = dA.reshape(self.A_prev.shape)<br/>    assert (dA_prev.shape == self.A_prev.shape)<br/>    return dA_prev, []</span></pre><h2 id="9230" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated">最大池层</h2><p id="f8e6" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">在进行反向传播之前，我们将创建一个函数来跟踪矩阵的最大值在哪里。True (1)表示矩阵中最大值的位置，其他条目为False (0)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ms"><img src="../Images/6244e96fafa5aee0250c25fcc0a09b74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*GhQEzqaA-FRGjO8lMYlaww.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图2.1寻找矩阵中最大值的掩码</figcaption></figure><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="82d4" class="ku jx hi me b fi mi mj l mk ml">def create_mask_from_window(self, image_slice):<br/>    <em class="jv">"""<br/>    Get  mask from a image_slice to identify the max entry.<br/><br/>    Parameters:<br/>    image_slice -- numpy array of shape (f, f, n_C_prev)<br/><br/>    Returns:<br/>    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of image_slice.<br/><br/>    """<br/>    </em>mask = np.max(image_slice)<br/>    mask = (image_slice == mask)<br/>    return mask</span></pre><p id="37f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们跟踪矩阵中的最大值，因为这是最终影响输出的输入值，因此也影响成本。反向投影计算的是相对于成本的梯度，因此任何影响最终成本的因素都应该有一个非零的梯度。因此，反向传播会将梯度“传播”回影响成本的特定输入值。</p><p id="d358" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为Maxpool没有参数，所以我们不会计算dW和db。</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="5093" class="ku jx hi me b fi mi mj l mk ml">def backward(self, dA):<br/>    <em class="jv">"""<br/>    Backward propogation of the pooling layer.<br/><br/>    Parameters:<br/>    dA -- gradient of cost with respect to the output of the pooling layer,<br/>          same shape as Z<br/><br/>    Returns:<br/>    dA_prev -- gradient of cost with respect to the input of the pooling layer,<br/>               same shape as A_prev<br/><br/>    """<br/>    </em>m, n_H_prev, n_W_prev, n_C_prev = self.A_prev.shape<br/>    m, n_H, n_W, n_C = dA.shape<br/>    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))<br/>    for i in range(m):<br/>        a_prev = self.A_prev[i]<br/>        for h in range(n_H):<br/>            for w in range(n_W):<br/>                for c in range(n_C):<br/>                    vert_start, vert_end, horiz_start, horiz_end = self.get_corners(h, w, self.filter_size, self.stride)<br/>                    a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]<br/>                    mask =self.create_mask_from_window(a_prev_slice)<br/>                    dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i, h, w, c]<br/>    assert(dA_prev.shape == self.A_prev.shape)<br/>    return dA_prev, []</span></pre><h2 id="c144" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated">Relu层</h2><p id="4ea1" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">relu中的反向传播如图2.2所示</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mt"><img src="../Images/d1c5332965d8af6d2c5fb094030db07c.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*8Vr2LkIekw7BkaURehazBg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图2.2相对于relu输入的成本梯度。</figcaption></figure><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="75d7" class="ku jx hi me b fi mi mj l mk ml">def backward(self, dA):<br/>    <em class="jv">"""<br/>    Backward propogation of relu layer.<br/><br/>    f′(x) = {1 if x &gt; 0}<br/>            {0 otherwise}<br/><br/>    Parameters:<br/>    dA -- gradient of cost with respect to the output of the relu layer, same shape as A<br/><br/>    Returns:<br/>    dZ -- gradient of cost with respect to the input of the relu layer, same shape as Z<br/><br/>    """<br/>    </em>Z = self.Z<br/>    dZ = np.array(dA, copy=True)<br/>    dZ[Z &lt;= 0] = 0<br/>    assert (dZ.shape == self.Z.shape)<br/>    return dZ, []</span></pre><h2 id="23e6" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated">卷积层</h2><p id="4bca" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">在卷积层，我们将计算三个梯度，dA，dW，db。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mu"><img src="../Images/3dcd5f49aae591da58ff5db0da1b6cf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*QyhwiWxu_mMVidnd3OsY4g.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图2.3 dA关于某一滤波器的成本。</figcaption></figure><p id="f362" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图2.3中，<em class="jv"> Wc </em>是一个滤波器，<em class="jv"> dZhw </em>是一个标量，对应于第h行第w列卷积层<em class="jv"> Z </em>输出的成本梯度(对应于第I步向左和第j步向下的点积)。注意，每次更新<em class="jv"> dA </em>时，我们都将同一个滤波器<em class="jv"> Wc </em>乘以不同的<em class="jv"> dZ </em>。我们这样做主要是因为当计算正向传播时，每个滤波器由不同的a_slice点状排列和求和。因此，当计算<em class="jv"> dA </em>的后投影时，我们只是将所有a_slices的梯度相加。图2.3中的公式转化为反向传播中的以下代码:</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="83cd" class="ku jx hi me b fi mi mj l mk ml">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += self.params[0][:, :, :, c] * dZ[i, h, w, c]</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mv"><img src="../Images/74b1bce5edc36486239722238583e793.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*4L_0QduA7MbDAFVpgsLR-A.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图2.4一个滤波器相对于损耗的梯度。</figcaption></figure><p id="46a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">aslice对应于用于生成激活Zij的切片。因此，这最终给出了W相对于该切片的梯度。因为是同一个W，我们就把所有这样的梯度加起来得到dW。</p><p id="c074" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在反向传播中，图2.4中的公式转化为下面的代码。</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="bdc3" class="ku jx hi me b fi mi mj l mk ml">dW[:, :, :, c] += a_slice_prev * dZ[i, h, w, c]</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mw"><img src="../Images/73e6f9a79e451425c09d1ca970e9bc5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*mH90NYemIYmpoXwmqHK1uw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图2.5特定滤波器相对于成本的偏差梯度。</figcaption></figure><p id="e499" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在反向传播中，图2.4中的公式转化为下面的代码。</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="9799" class="ku jx hi me b fi mi mj l mk ml">db[:, :, :, c] += dZ[i, h, w, c]</span></pre><p id="75d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">综上所述，卷积层的反向传播如下所示:</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="6da8" class="ku jx hi me b fi mi mj l mk ml">def backward(self, dZ):<br/>    <em class="jv">"""<br/>    Backward propagation for convolution.<br/><br/>    Parameters:<br/>    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)<br/><br/>    Returns:<br/>    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev), numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)<br/>    dW -- gradient of the cost with respect to the weights of the conv layer (W) numpy array of shape (f, f, n_C_prev, n_C)<br/>    db -- gradient of the cost with respect to the biases of the conv layer (b) numpy array of shape (1, 1, 1, n_C)<br/><br/>    """<br/>    </em>np.random.seed(self.seed)<br/>    m, n_H_prev, n_W_prev, n_C_prev = self.A_prev.shape<br/>    f, f, n_C_prev, n_C = self.params[0].shape<br/>    m, n_H, n_W, n_C = dZ.shape<br/>    dA_prev = np.zeros(self.A_prev.shape)<br/>    dW = np.zeros(self.params[0].shape)<br/>    db = np.zeros(self.params[1].shape)<br/>    # Pad A_prev and dA_prev<br/>    A_prev_pad = self.zero_pad(self.A_prev, self.pad)<br/>    dA_prev_pad = self.zero_pad(dA_prev, self.pad)<br/>    for i in range(m):<br/>        a_prev_pad = A_prev_pad[i, :, :, :]<br/>        da_prev_pad = dA_prev_pad[i, :, :, :]<br/>        for h in range(n_H):<br/>            for w in range(n_W):<br/>                for c in range(n_C):<br/>                    vert_start, vert_end, horiz_start, horiz_end = self.get_corners(h, w, self.filter_size, self.stride)<br/>                    a_slice_prev = a_prev_pad[<br/>                    vert_start:vert_end, horiz_start:horiz_end, :]<br/>                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += self.params[0][:, :, :, c] * dZ[i, h, w, c]<br/>                    dW[:, :, :, c] += a_slice_prev * dZ[i, h, w, c]<br/>                    db[:, :, :, c] += dZ[i, h, w, c]        dA_prev[i, :, :, :] = da_prev_pad[self.pad:-self.pad, self.pad:-self.pad, :]<br/>    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))<br/>    return dA_prev, [dW, db]</span></pre><h2 id="d99d" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated"><strong class="ak">梯度检查</strong></h2><p id="8a7f" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">梯度检查在验证反向传播和正确计算梯度时非常有用。它使用双侧差分在数值上近似梯度。我们将从训练数据中随机选择2个数据点，并对其进行梯度检查。注意:因为梯度检查非常慢，所以不要在训练中使用它。</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="1aad" class="ku jx hi me b fi mi mj l mk ml">def grad_check():<br/><br/>    train_set_x, train_set_y, test_set_x, test_set_y, n_class = load_data()<br/>    # select randomly 2 data points from training data<br/>    n = 2<br/>    index = np.random.choice(train_set_x.shape[0], n)<br/>    train_set_x = train_set_x[index]<br/>    train_set_y = train_set_y[:, index]<br/>    cnn = make_model(train_set_x, n_class)<br/>    print (cnn.layers)<br/>    A = cnn.forward(train_set_x)<br/>    loss, dA = softmaxloss(A, train_set_y)<br/>    assert (A.shape == dA.shape)<br/>    grads = cnn.backward(dA)<br/>    grads_values = grads_to_vector(grads)<br/>    initial_params = cnn.params<br/>    parameters_values = params_to_vector(initial_params) # initial parameters<br/>    num_parameters = parameters_values.shape[0]<br/>    J_plus = np.zeros((num_parameters, 1))<br/>    J_minus = np.zeros((num_parameters, 1))<br/>    gradapprox = np.zeros((num_parameters, 1))<br/>    print ('number of parameters: ', num_parameters)<br/>    epsilon = 1e-7<br/>    assert (len(grads_values) == len(parameters_values))<br/>    for i in tqdm(range(0, num_parameters)):<br/><br/>        thetaplus = copy.deepcopy(parameters_values)<br/>        thetaplus[i][0] = thetaplus[i][0] + epsilon # parameters<br/>        new_param = vector_to_param(thetaplus, initial_params)<br/>        difference = compare(new_param, initial_params)<br/>        # make sure only one parameter is changed<br/>        assert ( difference == 1) <br/>        cnn.params = new_param<br/>        A = cnn.forward(train_set_x)<br/>        J_plus[i], _ = softmaxloss(A, train_set_y)<br/><br/>        thetaminus = copy.deepcopy(parameters_values)<br/>        thetaminus[i][0] = thetaminus[i][0] - epsilon<br/>        new_param = vector_to_param(thetaminus, initial_params)<br/>        difference = compare(new_param, initial_params)<br/>        # make sure only one parameter is changed<br/>        assert (difference == 1)  <br/>        cnn.params = new_param<br/>        A = cnn.forward(train_set_x)<br/>        J_minus[i], _ = softmaxloss(A, train_set_y)<br/><br/>        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)<br/><br/>    numerator = np.linalg.norm(gradapprox - grads_values)<br/>    denominator = np.linalg.norm(grads_values) + np.linalg.norm(gradapprox)<br/>    difference = numerator / denominator<br/><br/>    if difference &gt; 2e-7:<br/>        print("\033[93m" + "There is a mistake in the backward propagation! difference = " + str(<br/>                difference) + "\033[0m")<br/>    else:<br/>        print("\033[92m" + "Your backward propagation works perfectly fine! difference = " + str(<br/>                difference) + "\033[0m")<br/><br/>    return difference</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mx"><img src="../Images/2abe4b82dc80ec9bc105db4ce62c0ecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cLHPP-DjXTKmwUCvmuBrvw.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图2.6梯度检查结果。</figcaption></figure><p id="c564" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您的反向传播有效，它将输出如图2.6所示的消息。如果在你的反向传播中有一些错误，一件事就是比较近似梯度和原始梯度的单个值，检查差异大的地方，并寻找那些梯度的实现。还有一点要注意的是，我们可能会遇到<em class="jv">扭结</em>，这可能是无法通过等级检查的不准确来源。纽结是指目标函数的不可微部分，由ReLU <strong class="ih hj"> <em class="jv"> (max(0，x)) </em> </strong> <em class="jv">等函数引入。</em>例如，考虑在<em class="jv"> x = -1e-8处进行梯度检查。</em>您可能还记得图2.2中的ReLU反向传播，因为x &lt;为0，所以它会计算一个零梯度。然而，当计算两侧差(x+ε)时，ε作为<em class="jv"> 1e-7 </em>计算<em class="jv"> 9e-08 </em>，这将引入非零梯度。</p><h2 id="bcd0" class="ku jx hi bd jy kv kw kx kc ky kz la kg iq lb lc kk iu ld le ko iy lf lg ks lh bi translated">Adam优化器</h2><p id="8d01" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">Adam optimizer被用作最小化损失函数的优化算法。众所周知，它可以很好地处理各种问题。Adam的Hyper参数有:<em class="jv">学习率</em>，<em class="jv">β1</em>，<em class="jv">β2</em>，<em class="jv">ε</em>。<em class="jv"> beta1 </em>的默认选择是0.9，而<em class="jv"> beta2 </em>的默认选择是0.999。<em class="jv">ε</em>的选择关系不大，设置为1e-08。一般来说，所有其他超级参数都使用默认值，学习率是可调的。<em class="jv">β1</em>计算导数的平均值，称为一阶矩，而<em class="jv">β2</em>用于计算平方的指数加权平均值。也就是所谓的二阶矩。Adam具有相对较低的内存需求，即使除了学习速率之外的超参数调整很少，也通常工作良好。[金马<em class="jv">等</em> 2014]</p><pre class="jf jg jh ji fd md me mf mg aw mh bi"><span id="00a6" class="ku jx hi me b fi mi mj l mk ml">class Adam(object):<br/><br/>    def __init__(self, model, X_train, y_train,<br/>                 learning_rate, epoch, minibatch_size, X_test, y_test):<br/>        self.model = model<br/>        self.X_train = X_train<br/>        self.y_train = y_train<br/>        self.learning_rate = learning_rate<br/>        self.beta1 = 0.9<br/>        self.beta2 = 0.999<br/>        self.epsilon = 1e-08<br/>        self.epoch = epoch<br/>        self.X_test = X_test<br/>        self.y_test = y_test<br/>        self.num_layer = len(self.model.layers)<br/>        self.minibatch_size = minibatch_size<br/><br/>    def initialize_adam(self):<br/>        VdW, Vdb, SdW, Sdb = [], [], [], []<br/>        for param_layer in self.model.params:</span><span id="3eb7" class="ku jx hi me b fi my mj l mk ml">            # layers which has no learning<br/>            if len(param_layer) is not 2:</span><span id="8082" class="ku jx hi me b fi my mj l mk ml">                VdW.append(np.zeros_like([]))<br/>                Vdb.append(np.zeros_like([]))<br/>                SdW.append(np.zeros_like([]))<br/>                Sdb.append(np.zeros_like([]))<br/>            else:<br/>                VdW.append(np.zeros_like(param_layer[0]))<br/>                Vdb.append(np.zeros_like(param_layer[1]))<br/>                SdW.append(np.zeros_like(param_layer[0]))<br/>                Sdb.append(np.zeros_like(param_layer[1]))<br/><br/>        assert len(VdW) == self.num_layer<br/>        assert len(Vdb) == self.num_layer<br/>        assert len(SdW) == self.num_layer<br/>        assert len(Sdb) == self.num_layer<br/><br/>        return VdW, Vdb, SdW, Sdb<br/><br/>    def update_parameters(self, VdW, Vdb, SdW, Sdb, grads, t):<br/><br/>        VdW_corrected = [np.zeros_like(v) for v in VdW]<br/>        Vdb_corrected = [np.zeros_like(v) for v in Vdb]<br/>        SdW_corrected = [np.zeros_like(s) for s in SdW]<br/>        Sdb_corrected = [np.zeros_like(s) for s in Sdb]<br/><br/>        # compute dW, db using current mini batch<br/><br/>        grads = list(reversed(grads))<br/>        for i in range(len(grads)):</span><span id="f0d3" class="ku jx hi me b fi my mj l mk ml">            # layer which contains weights and biases<br/>            if len(grads[i]) is not 0:   <br/>                # Moving average of the gradients (Momentum)<br/><br/>                a = self.beta1 * VdW[i]<br/>                b = (1 - self.beta1) * grads[i][0]<br/>                VdW[i] = np.add(a, b)<br/><br/>                a = self.beta1 * Vdb[i]<br/>                b = (1 - self.beta1) * grads[i][1]<br/>                Vdb[i] = np.add(a, b)<br/><br/>                # Moving average of the squared gradients. (RMSprop)<br/>                a = self.beta2 * SdW[i]<br/>                b = (1-self.beta2) * np.power(grads[i][0], 2)<br/>                SdW[i] = np.add(a, b)<br/><br/>                a = self.beta2 * Sdb[i]<br/>                b = (1-self.beta2) * np.power(grads[i][1], 2)<br/>                Sdb[i] = np.add(a, b)<br/><br/>                # Compute bias-corrected first moment estimate<br/><br/>                den = (1-(self.beta1 ** t))<br/>                VdW_corrected[i] = np.divide(VdW[i], den)<br/>                Vdb_corrected[i] = np.divide(Vdb[i], den)<br/><br/>                # Compute bias-corrected second raw moment estimate<br/>                den = 1-(self.beta2 ** t)<br/>                SdW_corrected[i] = np.divide(SdW[i], den)<br/>                Sdb_corrected[i] = np.divide(Sdb[i], den)<br/><br/>                # weight update<br/>                den = np.sqrt(SdW_corrected[i]) + self.epsilon<br/>                self.model.params[i][0] = self.model.params[i][0] - self.learning_rate * np.divide(VdW_corrected[i], den)<br/><br/>                # bias update<br/>                den = np.sqrt(Sdb_corrected[i]) + self.epsilon<br/>                self.model.params[i][1] = self.model.params[i][1] - self.learning_rate * np.divide(Vdb_corrected[i], den)<br/><br/>    def minimize(self):<br/>        costs = []<br/>        t = 0<br/>        np.random.seed(1)<br/>        VdW, Vdb, SdW, Sdb = self.initialize_adam()<br/>        for i in tqdm(range(self.epoch)):<br/>            start = time.time()<br/>            loss = 0<br/>            minibatches = get_minibatches(self.X_train,<br/>                                          self.y_train,<br/>                                          self.minibatch_size)<br/>            for minibatch in tqdm(minibatches):<br/>                # Select a minibatch<br/>                (minibatch_X, minibatch_Y) = minibatch<br/>                # forward and backward propogation<br/>                loss, grads = self.model.fit(minibatch_X, minibatch_Y)<br/>                loss += loss<br/>                t = t + 1  # Adam counter<br/>                # weight update<br/>                self.update_parameters(VdW, Vdb, SdW, Sdb, grads, t)<br/><br/>            # Print the cost every epoch<br/>            end = time.time()<br/>            epoch_time = end - start<br/>            train_acc = accuracy(self.model.predict(self.X_train),<br/>                                 self.y_train)<br/>            val_acc = accuracy(self.model.predict(self.X_test),<br/>                               self.y_test)<br/>            print ("Cost after epoch %i: %f" % (i, loss),<br/>                   'time (s):', epoch_time,<br/>                   'train_acc:', train_acc,<br/>                   'val_acc:', val_acc)<br/>            costs.append(loss)<br/>        print ('total_cost', costs)<br/><br/>        return self.model, costs</span></pre><p id="f654" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经到了文章的结尾，希望你已经跟上了。如果你喜欢它，别忘了给它一个大拇指:)</p><p id="0f89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以在LinkedIn上加我:<a class="ae jd" href="https://www.linkedin.com/in/mustufain-abbas/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/mustufain-abbas/</a></p><p id="4a4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该文章的代码可以在:<a class="ae jd" href="https://github.com/Mustufain/Convolution-Neural-Network-" rel="noopener ugc nofollow" target="_blank">https://github.com/Mustufain/Convolution-Neural-Network-</a>找到</p><h1 id="39ad" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">参考</h1><p id="2ca2" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">coursera上的吴恩达课程:<a class="ae jd" href="https://www.coursera.org/learn/convolutional-neural-networks-tensorflow" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/convolutionary-neural-networks-tensor flow</a></p><p id="44ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深入研究整流器:在imagenet分类上超越人类水平的表现IEEE计算机视觉国际会议论文集。2015</p><p id="6e0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">金玛，迪德里克p .和吉米巴。"亚当:随机最优化的方法."<em class="jv"> arXiv预印本arXiv:1412.6980 </em> (2014)。</p><p id="d66c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">泽勒，马修d和罗布弗格斯。"可视化和理解卷积网络."<em class="jv">欧洲计算机视觉会议</em>。施普林格，查姆，2014年。</p></div></div>    
</body>
</html>