<html>
<head>
<title>Types of regularization and when to use them.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化的类型以及何时使用它们。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/types-of-regularization-and-when-to-use-them-f0350ca651a7?source=collection_archive---------16-----------------------#2020-12-16">https://medium.com/analytics-vidhya/types-of-regularization-and-when-to-use-them-f0350ca651a7?source=collection_archive---------16-----------------------#2020-12-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/9fd66b41f4ef2eff9d45b6935cb7160e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SABnYfT-DtAuwGOgkqs79g.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">约书亚·索蒂诺在<a class="ae iu" href="https://unsplash.com/s/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="2a93" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本文将解释3种类型的正则化，以及在哪里以及如何使用Scikit-Learn来使用它们。</p><h1 id="a279" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">为什么要用正规化？</h1><p id="43bd" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">首先我们需要明白为什么我们要正规化。正则化主要用于使模型不会过度拟合数据。多项式模型是最常见的正则化模型，因为它可能具有会导致模型过度拟合的高阶特征，正则化所做的是减少多项式次数，从而使模型不会过度拟合数据。</p><p id="d44a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与减少训练数据中的特征数量等其他方法相比，使用正则化的优势在于，当移除特征时，我们会在训练时丢失有价值的信息。这就是为什么正则化更好，因为它减少了假设参数<strong class="ix hj"> (θ)的影响。</strong></p><p id="dd4a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，我将解释3种正规化的方法。</p><p id="f4db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我们将要处理的虚拟数据。正如我们所见，它非常分散，多项式模型最适合此数据。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/79cedfa88499b43315b2a62eb447f89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*c70Qr-QYH7aIyEQxz80mAQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图1:虚拟数据</figcaption></figure><h1 id="2df7" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">里脊回归</h1><p id="b19b" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><em class="lb">岭回归</em>(也称为<em class="lb">吉洪诺夫正则化)</em>是线性回归的正则化版本，其正则项等于:</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/d2f4cc3c87263da93fbc9653508af889.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*u3qZ0TQYfH5mTgAMGwSlAQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">岭回归正则项。</figcaption></figure><p id="f7f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当将这一项添加到成本函数中时，迫使学习算法不仅拟合数据，而且保持模型权重尽可能小。</p><blockquote class="ld le lf"><p id="51ab" class="iv iw lb ix b iy iz ja jb jc jd je jf lg jh ji jj lh jl jm jn li jp jq jr js hb bi translated">这一项只应在训练时添加到成本函数中。</p></blockquote><p id="35e3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">超参数<strong class="ix hj"> α </strong>控制你想要正则化模型的程度。现在，如果<strong class="ix hj"> α </strong> = 0，则该模型是没有任何正则化的正常线性回归模型。如果<strong class="ix hj"> α </strong>非常大，那么所有的权重将非常接近于0，并且模型将只是一条直线。</p><p id="ccbd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lb">岭回归成本函数:</em></p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/c43e52da5f9b19fbaa485c00f0917b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/1*-YTK8_HIQSefQ6hXSuO1yw.gif"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><em class="lk">岭回归成本函数</em></figcaption></figure><blockquote class="ld le lf"><p id="9497" class="iv iw lb ix b iy iz ja jb jc jd je jf lg jh ji jj lh jl jm jn li jp jq jr js hb bi translated">这里MSE指的是“均方误差”</p></blockquote><p id="0c8d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们将<strong class="ix hj"> w </strong>定义为特征权重的向量。然后脊回归使用向量<strong class="ix hj"> w </strong>的<strong class="ix hj"> l₂ </strong>范数<strong class="ix hj"> </strong>。</p><p id="a20e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lb">对岭回归的解进行封闭:</em></p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/2c4dde8943da1465968fd0247f7a3fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/1*FNFDKMxEm5G94NJbzmBNAQ.gif"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><em class="lk">岭回归的封闭形式解</em></figcaption></figure><blockquote class="ld le lf"><p id="cbf8" class="iv iw lb ix b iy iz ja jb jc jd je jf lg jh ji jj lh jl jm jn li jp jq jr js hb bi translated">其中<strong class="ix hj"> A </strong>是左上角为0的(n + 1) × (n + 1)单位矩阵，对应<a class="ae iu" rel="noopener" href="/nerd-for-tech/what-exactly-a-bias-term-is-d177164b1956#:~:text=Bias%20term%20is%20an%20educated,in%20other%20fields%20of%20Engineering.">偏差</a>项。</p></blockquote><p id="a170" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lb">使用Scikit-Learn实现岭回归:</em></p><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="c082" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们来看看在一些线性虚拟数据上训练的不同岭模型。在左翼，使用简单的山脊模型进行线性预测。左侧是具有扩展多项式特征的数据，然后对数据应用岭回归。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/369ff0c70e05a98d7cf3748d0630cab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*SX_IBq0CvJOzMFJD-DdMag.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图2:不同α值的岭回归。</figcaption></figure><p id="6086" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从<em class="lb">图-2 </em>中我们可以看出，增加<strong class="ix hj"> α </strong>值会导致更平坦的预测，从而减少模型<a class="ae iu" href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/#:~:text=Variance%20is%20the%20amount%20that,algorithm%20to%20have%20some%20variance." rel="noopener ugc nofollow" target="_blank">方差并增加偏差</a>。</p><h1 id="282c" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">套索回归</h1><p id="ecd1" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><em class="lb">最小绝对收缩和选择算子回归</em>是线性回归的另一个正则化版本，就像岭回归一样，它在成本函数中添加了一个正则化项，但它使用了权重向量<strong class="ix hj"> w </strong>的<strong class="ix hj"> l </strong> ₁范数。</p><p id="b6fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lb">拉索回归成本函数:</em></p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/1957f2551ed982bdb7b0509e6dc6d497.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/1*dk6-QXKHyksOKm9hauttVA.gif"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><em class="lk">拉索回归成本函数</em></figcaption></figure><p id="3019" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lb">拉索回归成本函数:</em></p><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="63cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看一些根据虚拟数据训练的套索模型:</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/cd9924b44980113231c517f161424a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*AY49ZhVm2J-2mQFhKRhsaA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图3:不同α值的套索回归</figcaption></figure><p id="5fe7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Lasso回归的一个重要特征是，它倾向于消除最不重要的要素的权重，例如，我们可以看到图<em class="lb">3</em>中的右图，其中<strong class="ix hj"> α </strong> = 1模型几乎移除了所有要素，因为$ \α$值太高，看起来更像线性模型而不是多项式模型。随着<strong class="ix hj"> α </strong>值的降低，我们看到模型变得更加多项式化，并且更好地拟合数据，但是当<strong class="ix hj"> α </strong> = 0时，模型过度拟合数据，因此没有应用套索回归，因此最佳点似乎是<strong class="ix hj"> α </strong> = 1e-07。换句话说，Lasso回归自动执行要素缩放并输出稀疏模型(即，具有很少的非零要素权重)。</p><h1 id="a3ef" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">弹性网</h1><p id="4ac9" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">弹性网是岭回归和套索回归的中间地带。这是山脊和套索调整的混合，你可以使用混合比例<strong class="ix hj"> r </strong>来控制两者的混合。当<strong class="ix hj"> r </strong> = 0时，弹性网相当于岭回归，当<strong class="ix hj"> r </strong> = 1时，等于套索回归。</p><p id="5200" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lb">弹性净成本函数:</em></p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/d3c8d6cd66da3d3d9fc6a94b8f35896d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*qaqLR8yxInmEFs503h8NUw.gif"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><em class="lk">弹性净成本函数</em></figcaption></figure><p id="e1ac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lb">使用Scikit-Learn实现弹性网络:</em></p><figure class="kx ky kz la fd ij"><div class="bz dy l di"><div class="lm ln l"/></div></figure><blockquote class="ld le lf"><p id="1ceb" class="iv iw lb ix b iy iz ja jb jc jd je jf lg jh ji jj lh jl jm jn li jp jq jr js hb bi translated"><strong class="ix hj"> l1_ratio </strong>对应混合比<strong class="ix hj"> r </strong>。</p></blockquote><h1 id="02a6" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">那么什么时候用哪个呢？</h1><p id="1835" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">几乎总是建议使用某种正则化，所以大多数时候应该避免简单的线性回归。那么应该用哪一个呢？</p><p id="2eb9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">岭回归是很好的，如果想保留所有的特征，并避免权重爆炸，这是一个很好的默认设置。但是，如果您认为只有少数几个要素有用，那么最好使用套索回归或弹性网，因为它会将不太有用的要素的权重设置为0。</p><p id="0bee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通常，弹性网优于套索，因为当要素数量大于训练实例数量或许多要素高度相关时，弹性网可能会表现不稳定。</p><h1 id="88d5" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="6d17" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在这篇短文中，我们学习了三种不同类型的正则化方法，以及它们如何有助于防止模型过度拟合，以及每种正则化类型的用途。</p><p id="bb7d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，继续努力，并尝试在您的下一个模型中实现它们！</p></div></div>    
</body>
</html>