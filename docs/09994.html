<html>
<head>
<title>Review: High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:基于多尺度神经面片合成的高分辨率图像修复</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/review-high-resolution-image-inpainting-using-multi-scale-neural-patch-synthesis-4bbda21aa5bc?source=collection_archive---------2-----------------------#2020-09-29">https://medium.com/analytics-vidhya/review-high-resolution-image-inpainting-using-multi-scale-neural-patch-synthesis-4bbda21aa5bc?source=collection_archive---------2-----------------------#2020-09-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7481" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗨，伙计们！在<a class="ae jd" rel="noopener" href="/@ronct/introduction-to-generative-models-for-image-inpainting-and-review-context-encoders-13e48df30244">之前的文章</a>中，我们已经介绍了图像修复和第一个基于 GAN 的修复算法<a class="ae jd" href="https://arxiv.org/abs/1604.07379" rel="noopener ugc nofollow" target="_blank">上下文编码器</a>。如果你还没有看过之前的帖子，我强烈推荐你先快速浏览一下！这一次，我们将深入研究另一种修补方法，它可以被视为上下文编码器的改进版本。开始吧！</p><h1 id="8efc" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">回忆</h1><p id="bbdc" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在这里，我简单回忆一下我们在<a class="ae jd" rel="noopener" href="/@ronct/introduction-to-generative-models-for-image-inpainting-and-review-context-encoders-13e48df30244">上一篇</a>中学到的东西。</p><ul class=""><li id="81db" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">对图像或图像上下文的深层语义理解对于修复任务是重要的，并且(通道式)全连通层是捕捉图像上下文的一种方式。</li><li id="fb62" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">对于图像修复，填充图像的视觉质量比逐像素重建精度更重要。更具体地说，由于生成的像素没有模型答案(我们在现实世界的情况下没有地面真相)，我们只想要看起来逼真的填充图像。</li></ul></div><div class="ab cl kv kw gp kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="hb hc hd he hf"><h1 id="7b99" class="je jf hi bd jg jh lc jj jk jl ld jn jo jp le jr js jt lf jv jw jx lg jz ka kb bi translated">动机</h1><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es lh"><img src="../Images/d24bc32f8b9fad1db3bdebfbc4ea7fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vY6BekdpD4GcuDQlQ0hEWw.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图一。修复任务的定性比较[1]。</figcaption></figure><ul class=""><li id="b794" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">由于内存限制和高分辨率图像的训练困难，现有的修复算法只能处理低分辨率图像。</li><li id="fa23" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">虽然最先进的修补方法，即上下文编码器，可以成功地回归(预测)缺失部分，并具有一定程度的语义正确性，但在预测像素的纹理和细节方面仍有改进的空间，如图 1 所示。</li></ul><h1 id="2729" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">介绍</h1><ul class=""><li id="851f" class="kh ki hi ih b ii kc im kd iq lx iu ly iy lz jc km kn ko kp bi translated">上下文编码器不完善。I)可以进一步改善所生成像素的纹理细节。ii)不能处理高分辨率图像。</li><li id="7b50" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">与此同时，<a class="ae jd" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">神经风格转移</a>是一个热门话题，我们想把一个图像(风格图像)的风格转移到另一个具有相同内容(内容图像)的图像上，如下图 2 所示。</li></ul><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es ma"><img src="../Images/b7f424fc5f0a91d856eced42b9487641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D0MxplMaePxQPikEjO6btg.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图二。举例说明风格转移的任务[2]</figcaption></figure><ul class=""><li id="b898" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">注意，纹理和颜色可以视为一种风格。本文作者采用风格转移算法来增强生成像素的纹理细节。</li></ul><h1 id="19d9" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">解决办法</h1><ul class=""><li id="6bd9" class="kh ki hi ih b ii kc im kd iq lx iu ly iy lz jc km kn ko kp bi translated">作者使用上下文编码器来预测丢失部分并得到预测像素。</li><li id="5265" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">然后，对预测像素和有效像素采用风格转移算法。主要思想是将最相似的有效像素的样式转移到预测像素，以增强纹理细节。</li><li id="80ab" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">在他们的公式中，他们假设测试图像的尺寸总是 512×512，并且缺少一个 256×256 的中心孔。他们使用三级金字塔的方式来处理这个高分辨率的修复问题。输入首先被调整为 128x128，具有 64x64 的中心孔，用于低分辨率重建。之后，填充的图像被上采样到 256×256，具有 128×128 的粗填充孔，用于第二次重建。最后，填充的图像再次被上采样到 512×512，其中 256×256 的填充孔用于最后的重建(或者可以称之为细化)。</li></ul><h1 id="064e" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">贡献</h1><ul class=""><li id="bb83" class="kh ki hi ih b ii kc im kd iq lx iu ly iy lz jc km kn ko kp bi translated">提出一个结合上下文编码器和神经风格转移技术的框架。</li><li id="0938" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">建议多尺度方式处理高分辨率图像。</li><li id="508c" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">实验表明，风格转移技术可用于增强所生成像素的纹理细节。</li></ul><h1 id="6830" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">方法</h1><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mb"><img src="../Images/eaf7b7bddc891f51ba5e8f829d9e5ebe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FdZUwlYIAI_sEziOZS-Qjg.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图 3。框架概述(下)和内容网络的结构(上)[1]</figcaption></figure><p id="a1d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图 3 显示了提出的框架，实际上并不难理解。内容网络是稍微修改的上下文编码器，而纹理网络是 ImageNet 上预先训练的<a class="ae jd" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> VGG-19 </a>网络。对我来说，这是一个早期版本的粗到细的网络，可以在多尺度上运行。本文的主要观点是他们如何优化模型(即损失函数的设计)。</p><h1 id="c66f" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">内容网络</h1><ul class=""><li id="38b1" class="kh ki hi ih b ii kc im kd iq lx iu ly iy lz jc km kn ko kp bi translated">如前所述，内容网络是上下文编码器。他们首先独立训练内容网络。然后，训练内容网络的输出将用于优化整个提议的框架。</li><li id="0156" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">参考图 3 中内容网络的结构，与原始上下文编码器有两个不同之处。<strong class="ih hj"> <em class="mc"> i) </em> </strong>中间的通道式全连接层被标准全连接层代替。<strong class="ih hj"> <em class="mc"> ii) </em> </strong>所有的 ReLU 或漏 ReLU 激活功能层都被 eLU 层代替。作者声称，与 ReLU 和 Leaky ReLU 相比，ELU 可以更好地处理大的负面神经反应。请注意，ReLU 只允许肯定的响应通过。</li><li id="ace7" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">它们使用与上下文编码器相同的方式训练内容网络。L2 损失和对抗性损失的组合。具体可以参考<a class="ae jd" rel="noopener" href="/analytics-vidhya/introduction-to-generative-models-for-image-inpainting-and-review-context-encoders-13e48df30244">我之前的帖子</a>。</li></ul><h1 id="3867" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">纹理网络</h1><p id="dcb5" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">我将尝试在这里解释更多关于纹理网络的内容，因为它与神经类型转移的主题有关。感兴趣的读者可以谷歌一下了解更多细节。</p><ul class=""><li id="ca7e" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">纹理网络的目标是确保生成像素的精细细节与有效像素的细节相似(即，我们希望图像具有一致的风格/纹理)</li><li id="35eb" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">简单地说，作者利用了[2]中的发现。在某种程度上，网络内不同层的特征图代表了图像风格。换句话说，给定一个训练好的网络，如果两个图像在网络内部具有相似的特征图，我们可以声称这两个图像具有相似的图像风格。说实话，这是一个过于简化的说法。在[2]中，作者在 ImageNet 上使用了一个<a class="ae jd" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank">预训练的 VGG 网络</a>作为特征提取器进行分类。他们建议在 VGG 的每一个图层上使用特征地图的 Gram 矩阵(也称为自相关矩阵)。如果两幅图像具有相似的 Gram 矩阵，则它们具有相似的图像风格，例如纹理和颜色。回到修复论文，作者也使用预训练的 VGG 网络作为他们的纹理网络，如图 3 所示。他们试图强制在 VGG 的几个层上，中心孔区域 内的特征映射<strong class="ih hj"> <em class="mc">的响应与中心孔区域</em> </strong>外的特征映射<strong class="ih hj"> <em class="mc">的响应相似。他们说他们使用<em class="mc"> relu3_1 </em>和<em class="mc"> relu4_1 </em>层进行计算。</em></strong></li></ul><h1 id="885c" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">损失函数</h1><p id="19af" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">总损失函数由三项组成，即内容损失(L2 损失)、纹理损失和 TV 损失(总变化损失)。</p><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es md"><img src="../Images/92572399ea45184f5e942839f098b1e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7Gh4Ia7pl_1BgKK1V9E8w.png"/></div></div></figure><ul class=""><li id="fd37" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">以上是他们想要最小化的联合损失函数。请注意，<em class="mc"> i </em>是电子秤的数量，如前所述，他们在这项工作中使用了 3 台电子秤。<em class="mc"> x </em>为地面真实图像(即图像完好无缺失)。<em class="mc"> h </em> ( <em class="mc"> x_i </em>，<em class="mc"> R </em>)返回孔区域<em class="mc"> R </em>内<em class="mc"> x_i </em>的颜色含量。<em class="mc"> phi_t </em> ( <em class="mc"> x </em>)返回给定输入<em class="mc"> x </em>时网络<em class="mc"> t </em>计算的特征图。<em class="mc"> R^phi </em>表示特征图中对应的空洞区域。最后一项是总变化损失项，它通常用于图像处理以确保图像的平滑度。α和β是平衡损失项的权重。</li></ul><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/d3cdc2a9c1159dee06f7dec382a1aeed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jLlAnMoje-mQW9EA0WQwDQ.png"/></div></div></figure><ul class=""><li id="8dd6" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">对于内容损失项，非常容易理解，只需计算 L2 损失以确保逐像素重建精度。</li></ul><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/18c23fcc65948f7e008d4a70090a2ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ETeUJHB7d443YJirgFnrCw.png"/></div></div></figure><ul class=""><li id="d364" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">对于纹理损失术语，它看起来有点复杂，但也很容易理解。</li><li id="6739" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">首先，他们将图像馈送给预训练的 VGG-19 网络，以获得<em class="mc"> relu3_1 </em>和<em class="mc"> relu4_1 </em>层(中间层)的特征图。然后，他们将特征图分成两组，一组用于洞区域(<em class="mc"> R^phi </em>)，另一组用于外部(即有效区域)。每个局部特征块<em class="mc"> P </em>的尺寸为<em class="mc">s</em>x<em class="mc">s</em>x<em class="mc">c</em>(<em class="mc">s</em>为空间尺寸，<em class="mc"> c </em>为特征图的数量)在孔洞区域内。他们所做的是找到洞区域外最相似的小块，然后计算每个局部小块与其最近邻的平均 L2 距离。</li><li id="0072" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">在 Eq 中。3、| <em class="mc"> R^phi </em> |是在<em class="mc"> R^phi </em>区域采样的总斑块数，<em class="mc"> P_i </em>是以位置<em class="mc"> i </em>为中心的局部斑块，<em class="mc"> nn </em> ( <em class="mc"> i </em>)计算为:</li></ul><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mf"><img src="../Images/61aa5da996b68a2a9eef672929c7cc02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VDtFDGUVu09O5VSkx9Z-UA.png"/></div></div></figure><ul class=""><li id="746a" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">情商。4 用于搜索每个局部面片的最近邻居<em class="mc"> P_i </em>。</li><li id="afb9" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">最后，电视损耗计算如下:</li></ul><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mg"><img src="../Images/b6a843fbcabf8500595ba12387830b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CXmKXtvwwqn8VRLn2XK6ow.png"/></div></div></figure><ul class=""><li id="45a4" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">同样，这通常用于图像处理，以确保图像的平滑度。</li></ul><h1 id="4a74" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">实验结果</h1><ul class=""><li id="b57e" class="kh ki hi ih b ii kc im kd iq lx iu ly iy lz jc km kn ko kp bi translated">与上下文编码器相同，两个数据集用于评估，Paris StreetView [3]和 ImageNet [4]数据集。巴黎街景由 14900 张训练图像和 100 张测试图像组成；ImageNet 包含 1.26M 的训练图像，从验证集中随机选择 200 张测试图像。</li></ul><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mh"><img src="../Images/0642d1f7ce0077cb7ef0f2f07e309ae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3g7_473UW8fiVgXKTi5Gbg.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">表 1。巴黎街景数据集上的定量比较。PNSR 越高越好。[1]</figcaption></figure><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mi"><img src="../Images/9afb524593e06e4896b05b5fea23bddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1e9UGpyDfZBliIYuchoYw.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图 4。不同方法的视觉比较[1]</figcaption></figure><ul class=""><li id="9b5f" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">表 1 显示了不同方法的定量结果。更高的 PNSR 意味着更好的性能。显然，本文提出的方法提供了最高的 PNSR。</li><li id="6af0" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">作者还声称定量评价(如 PSNR、L1 误差等。)对于图像修补任务可能不是最有效的度量，因为目标是生成看起来逼真的填充图像。</li><li id="fcf9" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">图 4 是几种方法的直观比较。从(d)和(e)的放大版本中，我们可以看到，与最先进的方法(上下文编码器)相比，所提出的方法可以生成更清晰的纹理细节。</li></ul><h1 id="d2fa" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">内容和纹理网络的效果</h1><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mj"><img src="../Images/5a190eb07e21c5387b69571478c54bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gONvl27EtE68TqC7zYIsA.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图 5。(a)输入图像(b)没有内容损失项的结果(c)来自所提出的方法[1]的结果</figcaption></figure><ul class=""><li id="c2fe" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">作者提供了损失项的烧蚀研究。图 5 显示了不使用内容损失项的结果。很明显，如果没有内容损失项，修复结果的结构是完全不正确的。</li></ul><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mi"><img src="../Images/c7b4c8e60d0fc2689631e0155c50ccc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7WbpQ5YzIeIj5wGVp2zEg.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图 6。使用不同纹理权重α的效果。[1]</figcaption></figure><ul class=""><li id="a4f6" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">除了显示内容损失条款是必要的。作者还指出了纹理损失项的重要性。图 6 显示了等式中不同纹理权重α的效果。1.显然，纹理损失项越多，结果越清晰，但它可能会影响整体图像结构，如图 6(d)所示。</li></ul><h1 id="4978" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">对抗损失的影响</h1><p id="2b6c" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">如上所述，作者使用与上下文编码器相同的方式来训练内容网络。它们显示了仅使用 L2 损失和同时使用 L2 和对抗性损失的效果。</p><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mk"><img src="../Images/a5e0a4c790361f927f169de1bde273c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A4BcvENDL2cF6tVS3tMrvg.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图 7。(a)仅用 L2 损失训练的内容网络的输出。(b)使用(a)的拟议方法的最终结果。(c)用 L2 +对抗性损失训练的内容网络的输出。(d)使用(c)的拟议方法的最终结果[1]</figcaption></figure><ul class=""><li id="d09d" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">从图 7 中，我们可以清楚地看到，内容网络的输出质量对最终结果很重要。研究表明，利用 L2 损失和对抗性损失来训练内容网络更好。</li></ul><h1 id="c10f" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">高分辨率图像修复</h1><p id="a590" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">如前所述，作者提出了一种处理高分辨率图像的多尺度方法。结果如下所示，</p><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es ml"><img src="../Images/f9d76f7368539fd530cf1795c0527a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFW-aVeBfCOBGVID3ViWkg.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图 8。ImageNet 结果的视觉比较[1]。从上到下:输入、内容感知填充、上下文编码器、建议方法。</figcaption></figure><ul class=""><li id="5b62" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">图 8 显示了高分辨率图像修复结果。对于上下文编码器，它仅适用于 128x128 输入图像。因此，使用双线性插值将结果上采样到 512x512。对于所提出的方法，输入将以三种尺度通过网络三次，以完成重构。显然，与其他方法相比，所提出的方法提供了最佳的视觉质量。然而，由于高分辨率图像修复的多尺度方式，所提出的方法用 Titan X GPU 填充 512×512 图像的 256×256 孔洞需要大约 1 分钟，这是所提出的方法的主要缺点(即低效率)。</li></ul><h1 id="3626" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">真实场景(对象移除)</h1><ul class=""><li id="43ee" class="kh ki hi ih b ii kc im kd iq lx iu ly iy lz jc km kn ko kp bi translated">作者进一步扩展了提出的方法来处理不规则形状的孔。简单地说，他们首先将不规则的孔修改为边界矩形孔。然后，他们执行裁剪和填充，将孔定位在中心。通过这样做，他们可以处理带有不规则孔洞的图像。下面显示了一些例子，</li></ul><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mm"><img src="../Images/843b556ba2141a0764a7ba8c5247919b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJYsmr7IFhhFUWCfOxlTzw.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图 9。任意对象移除[1]。从左至右:输入、对象遮罩、内容感知填充结果、建议的方法</figcaption></figure><h1 id="62e5" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">结论</h1><ul class=""><li id="ccec" class="kh ki hi ih b ii kc im kd iq lx iu ly iy lz jc km kn ko kp bi translated">这是上下文编码器的一个明显的改进版本。作者采用神经风格转移技术，通过上下文编码器进一步增强生成像素的纹理细节。因此，我们离逼真的填充图像更近了一步。</li><li id="b6e4" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">然而，作者也指出了一些未来的改进方向。<strong class="ih hj"> <em class="mc"> i) </em> </strong>在如图 10 所示的复杂场景下，仍然很难填充缺失的部分。<strong class="ih hj"> <em class="mc"> ii) </em> </strong>速度是一个问题，因为它无法实现实时性能。</li></ul><figure class="li lj lk ll fd lm er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mn"><img src="../Images/ba7ce51b478d4bf0675865e1cbc373f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hr51WFp4MYDr1SsIcm_69A.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图 10。建议方法的失败案例[1]</figcaption></figure><h1 id="f751" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">外卖食品</h1><p id="6ff6" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">同样，我想在这里强调一些要点，这些要点对以后的帖子很有用。</p><ul class=""><li id="0b0a" class="kh ki hi ih b ii ij im in iq kj iu kk iy kl jc km kn ko kp bi translated">这项工作是粗到细网络(也称为两级网络)的早期版本。我们首先重建丢失的部分，并且重建的部分应该具有一定的逐像素重建精度(即，确保结构正确)。然后，我们细化重建部分的纹理细节，使得填充后的图像具有良好的视觉质量。</li><li id="9a6e" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc km kn ko kp bi translated">纹理损失的概念在后来的图像修复论文中起着重要的作用。通过利用这种损失，我们可以产生更清晰的图像。之后，我们通常通过使用感知损失和/或风格损失来实现清晰的生成图像。我们将很快覆盖它们！</li></ul><h1 id="aedf" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">下一步是什么？</h1><ul class=""><li id="e765" class="kh ki hi ih b ii kc im kd iq lx iu ly iy lz jc km kn ko kp bi translated">下一次，我们将深入研究基于深度学习的图像修复算法的另一个里程碑。我必须说，这么多的修图论文都是基于他们的网络架构！希望你喜欢这篇文章:)</li></ul><h1 id="6d6e" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">参考</h1><ol class=""><li id="2cd8" class="kh ki hi ih b ii kc im kd iq lx iu ly iy lz jc mo kn ko kp bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1611.09969.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.09969.pdf</a></li><li id="25b0" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc mo kn ko kp bi translated">莱昂·A·加蒂丝等，《艺术风格的神经算法》，<a class="ae jd" href="https://arxiv.org/pdf/1508.06576.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1508.06576.pdf</a></li><li id="513d" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc mo kn ko kp bi translated">C.Doersch，S. Singh，A. Gupta，J. Sivic 和 A. Efros。是什么让巴黎看起来像巴黎？<em class="mc">2012 年美国计算机学会图形汇刊</em>。</li><li id="cd03" class="kh ki hi ih b ii kq im kr iq ks iu kt iy ku jc mo kn ko kp bi translated">ImageNet 大规模视觉识别挑战。<em class="mc"> IJCV </em>，2015。</li></ol><p id="c1cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">再次感谢。感谢你花时间写这篇文章。如有任何问题，欢迎随时留下评论:)下次见！</p></div></div>    
</body>
</html>