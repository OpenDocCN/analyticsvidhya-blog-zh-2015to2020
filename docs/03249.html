<html>
<head>
<title>Predicting user churn with PySpark (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PySpark预测用户流失(下)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/predicting-user-churn-with-pyspark-part-2-90874e6807bd?source=collection_archive---------31-----------------------#2020-01-21">https://medium.com/analytics-vidhya/predicting-user-churn-with-pyspark-part-2-90874e6807bd?source=collection_archive---------31-----------------------#2020-01-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0387" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这是三部分系列的第二部分，我们制作预测特征，并将数据输入到受监督的机器学习模型中</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a92a08fca47b533eb37bcb046d2af257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zHsRLXWzeNYHtanEDeY3YQ.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">虚拟流媒体平台Sparkify的徽标</figcaption></figure><h1 id="a090" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍</h1><p id="b832" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">这是由三部分组成的系列文章的第二部分，其中我描述了一种使用<a class="ae lb" href="https://pypi.org/project/pyspark/" rel="noopener ugc nofollow" target="_blank"> pyspark </a>预测用户流失的方法。点击这里找到这个系列的第一部分<a class="ae lb" rel="noopener" href="/@jcm.orlando/predicting-user-churn-with-pyspark-part-1-f13befbf04c3">。</a></p><p id="4116" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">在本文中，我们使用在探索阶段获得的知识来制作一些预测特征，并将数据输入到受监督的机器学习模型中。</p><h1 id="43b7" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">工程特征</h1><p id="b828" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">在我们制作一些特性之前，最好先了解一下用户的属性，以及哪些用户没有。下面我拆分了数据集，并显示了每个数据集的一般统计数据:</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="0408" class="lm jo hi li b fi ln lo l lp lq">def print_relevant_stats(df, total_rows, total_users):<br/>    <em class="lr">""""Print high level statistics for the dataframe"""</em><br/>    nrows = df.count()<br/>    nusers = df.select('userId').dropDuplicates().count()<br/>    npaid = df.groupby('userId', 'level').count().where(<br/>        'level == "paid"').count()<br/>    print(f'Proportion of the total rows in this group: '<br/>          f'{nrows / total_rows:.2f}')<br/>    print(f'Proportion of the total users in this group: '<br/>          f'{nusers / total_users:.2f}')<br/>    print(f'Proportion of users that paid for the service '<br/>          f'at some point: {npaid / nusers:.2f}')<br/>    print(f'Proportion of users that have never paid for the '<br/>          f'service: {(nusers - npaid) / nusers:.2f}')<br/>    <br/>    <em class="lr"># The average amount of songs played can be found by grabbing <br/>    # the rows with a song defined and then just grouping by userId   <br/>    # to get counts for the calculation</em><br/>    avg_agg = df.where('song is not null') \<br/>        .groupby('userId') \<br/>        .count() \<br/>        .select(<br/>            sqlF.mean(sqlF.col('count')).alias('avg_songs_played')<br/>        ).head(1)[0]<br/>    print(f'Avg amount of songs played per user:'       <br/>          f'{avg_agg.avg_songs_played:.2f}')<br/>    <br/>    <em class="lr"># The average amount of sessions can be calculated from getting <br/>    # unique duplicate pairs of user and session ids and then <br/>    # grouping by user to get the counts for the average calculation</em><br/>    avg_agg = df.select('userId', 'sessionId') \<br/>        .dropDuplicates() \<br/>        .groupby('userId') \<br/>        .count() \<br/>        .select(<br/>            sqlF.mean(sqlF.col('count')).alias('avg_sessions')<br/>        ).head(1)[0]<br/>    print(f'Avg number of sessions per user:' <br/>          f'{avg_agg.avg_sessions:.2f}')<br/>    <br/>    <em class="lr"># Getting the average actions and lengths per session requires <br/>    # us to find the particular value for each one, which we can get  <br/>    # from the max in each group. Then we can proceed to get avg per <br/>    # user, and then a final average across all users</em><br/>    current_window = Window.partitionBy(<br/>        'userId', 'sessionId').orderBy('ts')<br/>    avg_agg = df.withColumn(<br/>            'sessionLen', <br/>            (sqlF.col('ts') - sqlF.first(sqlF.col('ts')) \<br/>                                  .over(current_window)) / 1000.0<br/>        ).groupby('userId', 'sessionId') \<br/>        .max() \<br/>        .groupby('userId') \<br/>        .avg() \<br/>        .select(<br/>            sqlF.mean(<br/>                sqlF.col('avg(max(itemInSession))'<br/>            )).alias('avg_user_actions'),                <br/>            sqlF.mean(<br/>                sqlF.col('avg(max(sessionLen))'<br/>            )).alias('avg_session_len'),<br/>        ).head(1)[0]<br/>    print(f'Avg number of actions per session:' <br/>          f'{avg_agg.avg_user_actions:.2f}')<br/>    print(f'Avg duration of sessions (in hours):'<br/>          f'{avg_agg.avg_session_len / (60 * 60):.2f}')<br/>    <br/>    <em class="lr"># Getting the average amount of seconds since the first show of <br/>    # users is similar to the above, but this time we don't group by   <br/>    # sessionId</em><br/>    current_window = Window.partitionBy('userId').orderBy('ts')<br/>    avg_agg = df.withColumn('secondsSinceFirstShow', <br/>        (sqlF.col('ts') -   sqlF.first(sqlF.col('ts')) \<br/>                                .over(current_window)) / 1000.0<br/>        ).groupby('userId') \<br/>        .max() \<br/>        .select(<br/>            sqlF.mean(sqlF.col('max(secondsSinceFirstShow)')) \<br/>                          .alias('avg_first_show'),<br/>        ).head(1)[0]<br/>    print(f'Avg number of hours since first show:'<br/>          f'{avg_agg.avg_first_show / (60 * 60):.2f}')<br/>    <br/>    # Calculate not only page visit counts but also percentage of <br/>    # the total for each<br/>    pages_agg = df.groupby('page').count()<br/>    npages = pages_agg.select(<br/>        sqlF.sum(sqlF.col('count')).alias('npages'),<br/>    ).head(1)[0].npages<br/>    per_page_agg = pages_agg.sort(sqlF.desc('count')) \<br/>        .withColumn('proportion', sqlF.col('count') / npages)          <br/>    print('Stats per page:')<br/>    per_page_agg.show()</span><span id="5936" class="lm jo hi li b fi ls lo l lp lq"><em class="lr"># Get some counts for next steps</em><br/>total_rows = df.count()<br/>total_users = df.select('userID').dropDuplicates().count()<br/>print(f'Number of rows and users in dataset: '<br/>      f'{total_rows}, {total_users}')</span><span id="071e" class="lm jo hi li b fi ls lo l lp lq"><strong class="li hj"><em class="lr">Number of rows and users in dataset: 286500, 226</em></strong></span><span id="36c4" class="lm jo hi li b fi ls lo l lp lq">print('Stats for set of non-churned users:')<br/>print_relevant_stats(df.where('churned=0'), total_rows, total_users)</span><span id="f60b" class="lm jo hi li b fi ls lo l lp lq"><strong class="li hj"><em class="lr">Stats for set of non-churned users:<br/>Proportion of the total rows in this group: 0.84<br/>Proportion of the total users in this group: 0.77<br/>Proportion of users that paid for the service at some point: 0.75<br/>Proportion of users that have never paid for the service: 0.25<br/>Avg amount of songs played per user: 1108.17<br/>Avg number of sessions per user: 24.53<br/>Avg number of actions per session: 88.50<br/>Avg duration of sessions (in hours): 5.48<br/>Avg number of hours since first show: 1129.71<br/>Stats per page:<br/>+-------------------+------+--------------------+<br/>|               page| count|          proportion|<br/>+-------------------+------+--------------------+<br/>|           NextSong|191714|  0.7933999900677051|<br/>|               Home| 12785|0.052910162393020904|<br/>|          Thumbs Up| 10692| 0.04424837358671721|<br/>|    Add to Playlist|  5488|0.022711847572381597|<br/>|         Add Friend|  3641|0.015068118988892383|<br/>|              Login|  3241|0.013412736512771277|<br/>|        Roll Advert|  2966|0.012274661060438015|<br/>|             Logout|  2673|0.011062093396679303|<br/>|        Thumbs Down|  2050|0.008483835190120678|<br/>|          Downgrade|  1718|0.007109867734940158|<br/>|               Help|  1487|0.006153884354980218|<br/>|           Settings|  1244|0.005148239500736...|<br/>|              About|   868|0.003592179973182804|<br/>|            Upgrade|   387|0.001601582545647...|<br/>|      Save Settings|   252|0.001042890959956298|<br/>|              Error|   226|9.352910990084259E-4|<br/>|     Submit Upgrade|   127|5.255839361684517E-4|<br/>|   Submit Downgrade|    54|2.234766342763495...|<br/>|           Register|    18|7.449221142544985E-5|<br/>|Submit Registration|     5|2.069228095151385E-5|<br/>+-------------------+------+--------------------+</em></strong></span><span id="f2e2" class="lm jo hi li b fi ls lo l lp lq">print('Stats for set of churned users:')<br/>print_relevant_stats(df.where('churned=1'), total_rows, total_users)</span><span id="523e" class="lm jo hi li b fi ls lo l lp lq"><strong class="li hj"><em class="lr">Stats for set of churned users:<br/>Proportion of the total rows in this group: 0.16<br/>Proportion of the total users in this group: 0.23<br/>Proportion of users that paid for the service at some point: 0.69<br/>Proportion of users that have never paid for the service: 0.31<br/>Avg amount of songs played per user: 699.88<br/>Avg number of sessions per user: 10.33<br/>Avg number of actions per session: 78.94<br/>Avg duration of sessions (in hours): 4.33<br/>Avg number of hours since first show: 564.35<br/>Stats per page:<br/>+--------------------+-----+--------------------+<br/>|                page|count|          proportion|<br/>+--------------------+-----+--------------------+<br/>|            NextSong|36394|  0.8112072039942939|<br/>|           Thumbs Up| 1859| 0.04143634094151213|<br/>|                Home| 1672|0.037268188302425106|<br/>|     Add to Playlist| 1038|0.023136590584878745|<br/>|         Roll Advert|  967|0.021554029957203995|<br/>|          Add Friend|  636|0.014176176890156919|<br/>|              Logout|  553| 0.01232614122681883|<br/>|         Thumbs Down|  496|0.011055634807417974|<br/>|           Downgrade|  337|0.007511590584878745|<br/>|            Settings|  270|0.006018188302425107|<br/>|                Help|  239|0.005327211126961484|<br/>|             Upgrade|  112|0.002496433666191...|<br/>|       Save Settings|   58|0.001292796005706134|<br/>|               About|   56|0.001248216833095...|<br/>|              Cancel|   52|0.001159058487874...|<br/>|Cancellation Conf...|   52|0.001159058487874...|<br/>|               Error|   32|7.132667617689016E-4|<br/>|      Submit Upgrade|   32|7.132667617689016E-4|<br/>|    Submit Downgrade|    9|2.006062767475035...|<br/>+--------------------+-----+--------------------+</em></strong></span></pre><p id="5359" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">从上面得到的结果信息非常丰富！从高层次来看，用户在平台上花费的时间越多，用户在不久的将来流失的可能性就越小。</p><p id="944f" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">基于上面的学习，我认为下面是一组很好的用户特性:</p><ul class=""><li id="0582" class="lt lu hi kh b ki lc kl ld ko lv ks lw kw lx la ly lz ma mb bi translated"><code class="du mc md me li b">number_sessions</code>:会话总数</li><li id="04f5" class="lt lu hi kh b ki mf kl mg ko mh ks mi kw mj la ly lz ma mb bi translated"><code class="du mc md me li b">seconds_since_genesis</code>:第一次出现后的总秒数</li><li id="233e" class="lt lu hi kh b ki mf kl mg ko mh ks mi kw mj la ly lz ma mb bi translated"><code class="du mc md me li b">avg_actions_per_session</code>:每次会话的平均操作量</li><li id="1746" class="lt lu hi kh b ki mf kl mg ko mh ks mi kw mj la ly lz ma mb bi translated"><code class="du mc md me li b">avg_seconds_per_session</code>:每次会话花费的平均秒数</li></ul><p id="1c35" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">目前，每个用户可以有多行，因为它们代表了特定时间点的单一操作。但是我们真正想要的是，除了我们想要预测的<code class="du mc md me li b">churned</code>标签(我们已经在<a class="ae lb" rel="noopener" href="/@jcm.orlando/predicting-user-churn-with-pyspark-part-1-f13befbf04c3#c5e7">上一篇文章</a>中添加了这个专栏)之外，每个用户还有一行提到的特性。</p><p id="cf87" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">在接下来的小节中，我将集中精力创建正确的函数来为每个用户生成每个特性，然后我将所有的部分放在一起。</p><h2 id="01b5" class="lm jo hi bd jp mk ml mm jt mn mo mp jx ko mq mr jz ks ms mt kb kw mu mv kd mw bi translated">会话总数</h2><p id="ef42" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">这很容易，我们只需要获得不同的用户和会话id对，然后按用户id分组，这样我们就可以提取我们需要的计数:</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="192c" class="lm jo hi li b fi ln lo l lp lq">def add_feature_number_sessions(df):<br/>    <em class="lr">"""Add `number_sessions`: amount of sessions per user"""</em><br/>    counts_df = df.select('userId', 'sessionId') \<br/>        .dropDuplicates() \<br/>        .groupby('userId') \<br/>        .count() \<br/>        .withColumnRenamed('count', 'number_sessions')<br/>    return df.join(counts_df, ['userId'])</span></pre><h2 id="7391" class="lm jo hi bd jp mk ml mm jt mn mo mp jx ko mq mr jz ks ms mt kb kw mu mv kd mw bi translated">自第一次出现以来的总秒数</h2><p id="a6b4" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">我们可以获得从第一个用户使用窗口函数开始到第一次按用户分组时观察到的时间戳之间的总秒数。从那里，我们只获取每个分区获得的最大值作为最终值:</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="fb64" class="lm jo hi li b fi ln lo l lp lq">def add_feature_seconds_since_genesis(df):<br/>    <em class="lr">"""Add `seconds_since_genesis`: seconds since first appearance"""</em><br/>    current_window = Window.partitionBy('userId').orderBy('ts')<br/>    genesis_df = df.withColumn(<br/>        'seconds_since_genesis', <br/>        (sqlF.col('ts') - sqlF.first(sqlF.col('ts'))<br/>                              .over(current_window)) / 1000.0)<br/>    genesis_df = genesis_df.groupby('userId') \<br/>        .max() \<br/>        .withColumnRenamed(<br/>            'max(seconds_since_genesis)', 'seconds_since_genesis'<br/>        ).select('userId', 'seconds_since_genesis')<br/>    return df.join(genesis_df, ['userId'])</span></pre><h2 id="f9f5" class="lm jo hi bd jp mk ml mm jt mn mo mp jx ko mq mr jz ks ms mt kb kw mu mv kd mw bi translated">每个会话的平均操作量</h2><p id="52a9" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">为了计算每个用户的平均会话操作，我们需要首先按照用户和会话id进行分组。从那里，max <code class="du mc md me li b">itemInSession</code>列将报告每个会话的动作总量。最后，我们可以取这些值的平均值，我们将得到我们想要的值:</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="96d3" class="lm jo hi li b fi ln lo l lp lq">def add_feature_avg_actions_per_session(df):<br/>    <em class="lr">"""Add `avg_actions_per_session`: average actions per session"""</em><br/>    current_window = Window.partitionBy('userId').orderBy('ts')<br/>    avg_df = df.groupby('userId', 'sessionId') \<br/>        .max() \<br/>        .groupby('userId') \<br/>        .avg() \<br/>        .withColumnRenamed(<br/>            'avg(max(itemInSession))', 'avg_actions_per_session'<br/>        ).select('userId', 'avg_actions_per_session')<br/>    return df.join(avg_df, ['userId'])</span></pre><h2 id="9fab" class="lm jo hi bd jp mk ml mm jt mn mo mp jx ko mq mr jz ks ms mt kb kw mu mv kd mw bi translated">每个会话花费的平均秒数</h2><p id="b780" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">为了计算每个用户的平均会话持续时间，我们做了一些与前一个案例类似的事情，我们只需要在流程中构造一个中间列，这样我们就可以计算正在运行的会话持续时间:</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="fc04" class="lm jo hi li b fi ln lo l lp lq">def add_feature_avg_seconds_per_session(df):<br/>    """Add `avg_seconds_per_session`: average session duration"""<br/>    current_window = Window.partitionBy(<br/>        'userId', 'sessionId').orderBy('ts')<br/>    avg_df = df.withColumn(<br/>            'sessionLen', <br/>            (sqlF.col('ts') - sqlF.first(sqlF.col('ts')) \<br/>                                  .over(current_window)) / 1000.0<br/>        ).groupby('userId', 'sessionId') \<br/>        .max() \<br/>        .groupby('userId') \<br/>        .avg() \<br/>        .withColumnRenamed(<br/>            'avg(max(sessionLen))', 'avg_seconds_per_session'<br/>        ).select('userId', 'avg_seconds_per_session')<br/>    return df.join(avg_df, ['userId'])</span></pre><h2 id="b2bb" class="lm jo hi bd jp mk ml mm jt mn mo mp jx ko mq mr jz ks ms mt kb kw mu mv kd mw bi translated">把所有的放在一起</h2><p id="4801" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">现在，我们已经具备了创建函数的所有要素，该函数将加载数据集、清理数据集、添加要素和标注，并将其简化为一种形式，在这种形式中，每个用户保留一行，仅包含要素和预测标注:</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="2bfa" class="lm jo hi li b fi ln lo l lp lq">def load_df_for_ml(json_filepath):<br/>    <em class="lr">"""Load json, then cleans/transform it for modeling"""</em><br/>    df = spark.read.json(json_filepath)<br/>    df_clean_v1 = df.filter('userId != ""')<br/>    df_with_features = add_feature_number_sessions(df_clean_v1)<br/>    df_with_features = add_feature_seconds_since_genesis(<br/>        df_with_features)                   <br/>    df_with_features = add_feature_avg_actions_per_session(<br/>        df_with_features)                   <br/>    df_with_features = add_feature_avg_seconds_per_session(<br/>        df_with_features)                   <br/>    df_with_features = add_label_churned(<br/>        df_with_features)<br/>    <br/>    features = [<br/>        'number_sessions', 'seconds_since_genesis', <br/>        'avg_actions_per_session', 'avg_seconds_per_session',<br/>    ]<br/>    return df_with_features.select(<br/>        ['userId', 'churned'] + features).dropDuplicates()</span><span id="8934" class="lm jo hi li b fi ls lo l lp lq">file_path = './mini_sparkify_event_data.json'<br/>final_df = load_df_for_ml(file_path)<br/>final_df.select(<br/>    'userId', 'churned', 'number_sessions').sort('userId').show()</span><span id="c7de" class="lm jo hi li b fi ls lo l lp lq"><strong class="li hj"><em class="lr">------+-------+---------------+<br/>|userId|churned|number_sessions|<br/>+------+-------+---------------+<br/>|    10|      0|              6|<br/>|   100|      0|             35|<br/>|100001|      1|              4|<br/>|100002|      0|              4|<br/>|100003|      1|              2|<br/>|100004|      0|             21|<br/>|100005|      1|              5|<br/>|100006|      1|              1|<br/>|100007|      1|              9|<br/>|100008|      0|              6|<br/>|100009|      1|             10|<br/>|100010|      0|              7|<br/>|100011|      1|              1|<br/>|100012|      1|              7|<br/>|100013|      1|             14|<br/>|100014|      1|              6|<br/>|100015|      1|             12|<br/>|100016|      0|              8|<br/>|100017|      1|              1|<br/>|100018|      0|             21|<br/>+------+-------+---------------+<br/>only showing top 20 rows</em></strong></span></pre><h1 id="dbf0" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">建筑模型</h1><p id="6d07" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">我们终于准备好在数据集上运行监督机器学习模型了。我认为我们应该以尽可能多召回的模型为目标，考虑到搅动的集合是如此之小(只有16%的行来自搅动的用户)，还因为我认为假阴性(预测某人不会搅动，但实际上他们会)比假阳性(预测某人会搅动，但实际上他们不打算搅动)更具破坏性。</p><p id="2a6f" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">让我们建立一个物流回收渠道:</p><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="fd2e" class="lm jo hi li b fi ln lo l lp lq">def get_ml_pipeline(clf):<br/>    <em class="lr">"""Constructs a pipeline to transform data before running clf"""</em><br/>    features = [<br/>        'number_sessions', 'seconds_since_genesis',<br/>        'avg_actions_per_session', 'avg_seconds_per_session',<br/>    ]<br/>    assembler = VectorAssembler(<br/>        inputCols=features, outputCol="features")<br/>    return Pipeline(stages=[assembler, clf])</span><span id="4804" class="lm jo hi li b fi ls lo l lp lq">def eval_model(model, validation_df):<br/>    <em class="lr">"""Runs a model against test set and prints performance stats"""</em><br/>    results = model.transform(validation_df)    <br/>    predictionAndLabels = results.rdd.map(<br/>        lambda row: (float(row.prediction), float(row.label)))<br/>    metrics = MulticlassMetrics(predictionAndLabels)<br/>    print('Performance Stats')<br/>    print(f'Accuracy: {metrics.accuracy:.4f}')<br/>    print(f'Precision = {metrics.precision(1.0):.4f}')<br/>    print(f'Recall = {metrics.recall(1.0):.4f}')<br/>    print(f'F1 Score = {metrics.fMeasure(1.0):.4f}')</span><span id="7b9e" class="lm jo hi li b fi ls lo l lp lq"><em class="lr"># Grab a random 80% of the dataset for the train set and the rest <br/># for validation</em><br/>train_df, validation_df = final_df.withColumnRenamed(<br/>    'churned', 'label').randomSplit([0.8, 0.2], seed=42)</span><span id="586f" class="lm jo hi li b fi ls lo l lp lq"><em class="lr"># Fit pipeline to the training dataset</em><br/>pipeline = get_ml_pipeline(LogisticRegression(standardization=<strong class="li hj">True</strong>))<br/>model = pipeline.fit(train_df)</span><span id="2dee" class="lm jo hi li b fi ls lo l lp lq"><em class="lr"># Run model against the validation dataset and print performance <br/># statistics</em><br/>eval_model(model, validation_df)</span><span id="ad4e" class="lm jo hi li b fi ls lo l lp lq"><strong class="li hj"><em class="lr">Performance Stats<br/>Accuracy: 0.7941<br/>Precision = 0.6667<br/>Recall = 0.2500<br/>F1 Score = 0.3636</em></strong></span></pre><p id="8659" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">我们在准确性和精确性方面做得很好，但在回忆方面做得不太好(这意味着F1分数较低)。让我们尝试改进我们的模型。</p><h1 id="404c" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">精炼模型</h1><p id="8ef2" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">为了改进我们的模型，我们应该考虑尝试逻辑回归分类器的超参数值的组合。PySpark提供了CrossValidator，它使用k-fold交叉验证来评估每个参数组合。</p><p id="df09" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">我将重点尝试以下逻辑回归参数的组合:</p><ul class=""><li id="9d4e" class="lt lu hi kh b ki lc kl ld ko lv ks lw kw lx la ly lz ma mb bi translated"><strong class="kh hj"> regParam </strong>:模型正则化参数(&gt; = 0)。(默认值:0.0)</li><li id="a91b" class="lt lu hi kh b ki mf kl mg ko mh ks mi kw mj la ly lz ma mb bi translated"><strong class="kh hj">aggregation depth</strong>:tree aggregate的建议深度(大于等于2)。如果特征的尺寸或分区的数量很大，该参数可以调整到更大的尺寸。(默认值:2.0)</li><li id="088f" class="lt lu hi kh b ki mf kl mg ko mh ks mi kw mj la ly lz ma mb bi translated"><strong class="kh hj">elastic net参数</strong> : <a class="ae lb" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html" rel="noopener ugc nofollow" target="_blank"> ElasticNet </a>混合参数。对于0，它引入了L2惩罚。对于1，这是一个L1的惩罚。(默认值:0.0)</li><li id="e7a7" class="lt lu hi kh b ki mf kl mg ko mh ks mi kw mj la ly lz ma mb bi translated"><strong class="kh hj"> maxIter </strong>:最大拟合迭代次数。(默认值:100.0)</li></ul><pre class="iy iz ja jb fd lh li lj lk aw ll bi"><span id="c4f4" class="lm jo hi li b fi ln lo l lp lq">def build_cross_validator(numFolds=3):<br/>    """Build CrossValidator for tuning a LogisticRegression model"""<br/>    lr = LogisticRegression(standardization=True)<br/>    pipeline = get_ml_pipeline(lr)<br/>    paramGrid = ParamGridBuilder() \<br/>        .addGrid(lr.regParam, [0.0, 0.5]) \<br/>        .addGrid(lr.aggregationDepth, [2, 4]) \<br/>        .addGrid(lr.elasticNetParam, [0.0, 1.0]) \<br/>        .addGrid(lr.maxIter, [10, 100]) \<br/>        .build()<br/>    evaluator = MulticlassClassificationEvaluator()    <br/>    return CrossValidator(estimator=pipeline,<br/>                          estimatorParamMaps=paramGrid,<br/>                          evaluator=evaluator,<br/>                          numFolds=numFolds)</span><span id="f708" class="lm jo hi li b fi ls lo l lp lq"><em class="lr"># Tune the mode using a k-fold cross-validation based approach to <br/># try to find better hyperparameters </em><br/>cv = build_cross_validator()<br/>cv_model = cv.fit(train_df)</span><span id="60eb" class="lm jo hi li b fi ls lo l lp lq"><em class="lr"># Evaluate the performance of the cross-validated model</em><br/>eval_model(cv_model, validation_df)</span><span id="8782" class="lm jo hi li b fi ls lo l lp lq"><strong class="li hj"><em class="lr">Performance Stats<br/>Accuracy: 0.7941<br/>Precision = 0.6667<br/>Recall = 0.2500<br/>F1 Score = 0.3636</em></strong></span></pre><p id="7cc1" class="pw-post-body-paragraph kf kg hi kh b ki lc ij kk kl ld im kn ko le kq kr ks lf ku kv kw lg ky kz la hb bi translated">它并没有真正改善事情，所以默认值已经在当前的训练测试分割中工作得很好。中型数据集只有225个不同的用户，所以我相信问题的一部分是样本大小。我们可以尝试其他采样方法(因为搅拌集很小)，或者其他机器学习模型或设计更多功能，但我认为25%的召回率足以转移到更大的数据集。</p><h1 id="7ad0" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">在下一篇文章中…</h1><p id="794d" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">现在我们已经有了一个工作的预测管道，在本系列的下一篇(也是最后一篇)文章中，我将带您完成建立一个AWS EMR集群的步骤，以便用12GB数据集训练和评估我们的模型。然后，我会分享我对结果的想法，并提供可能的后续步骤。</p><blockquote class="mx my mz"><p id="93e5" class="kf kg lr kh b ki lc ij kk kl ld im kn na le kq kr nb lf ku kv nc lg ky kz la hb bi translated">如果你对可用于复制所有工作结果的实际代码感兴趣，请访问我的<a class="ae lb" href="https://github.com/ojcastillo/sparkify" rel="noopener ugc nofollow" target="_blank"> github repo </a>。</p></blockquote></div></div>    
</body>
</html>