<html>
<head>
<title>Advanced Regression Techniques to Predict Home Prices With Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 预测房价的高级回归技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/advanced-regression-techniques-to-predict-home-prices-with-python-86caa9e0861d?source=collection_archive---------5-----------------------#2020-12-21">https://medium.com/analytics-vidhya/advanced-regression-techniques-to-predict-home-prices-with-python-86caa9e0861d?source=collection_archive---------5-----------------------#2020-12-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f325" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这是一个教程，面向那些对 Python 有一些经验的数据科学学生和房地产经纪人，他们希望了解你所在地区的房价。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/7a23fbb6243980532b4f3824f3ccec28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4LBW31mD3wjFoPb6.jpg"/></div></div></figure><p id="b91b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">房地产经纪人可能需要一辈子的经验才能感觉到房子会卖多少钱。很多时候，经验不足的卖家或买家可能会错过一些对老手来说显而易见的东西。</p><p id="186f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在这个练习中，我将尝试从描述住宅各个方面的 79 个解释变量中梳理出几个在预测房屋最终销售价格时最重要的变量。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kf"><img src="../Images/acb9bb78fb4045ffb8d6a1075b2b721d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lM22CYwdmzre_JNzB5ULlA.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">我们的最终产品。销售价格最重要的特点。</figcaption></figure><p id="9dec" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在很大程度上，结果正如我们所料，较大的新房子或最近装修的房子卖得更多。但是如果有太多的卧室，或者有一个以上的壁炉有什么关系呢？了解最重要的特征和它们的限制对房地产市场中的任何人都很重要，通过遵循这些步骤，你将能够分析你的财产，看看它应该落在哪里，以避免要求或花费太多。</p><h1 id="6a74" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">数据集</h1><p id="47f6" class="pw-post-body-paragraph jj jk hi jl b jm ld ij jo jp le im jr js lf ju jv jw lg jy jz ka lh kc kd ke hb bi translated">这个数据集来自爱荷华州<a class="ae li" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" rel="noopener ugc nofollow" target="_blank">Kaggle Ames</a>竞赛，但是所有的技术同样适用于任何数量的观察或特征。</p><h1 id="8775" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">I .加载和清理数据</h1><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="c49d" class="lo km hi lk b fi lp lq l lr ls"># Import packages and load the full dataset</span><span id="8e84" class="lo km hi lk b fi lt lq l lr ls">%matplotlib inline<br/>import warnings<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sea<br/>import matplotlib.pyplot as plt<br/>from sklearn.impute import SimpleImputer<br/>from sklearn import datasets, linear_model<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split, GridSearchCV<br/>from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet<br/>from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor<br/>from sklearn.neighbors import KNeighborsRegressor</span><span id="e870" class="lo km hi lk b fi lt lq l lr ls"># Ignore the warnings<br/>warnings.filterwarnings('ignore')</span><span id="18e1" class="lo km hi lk b fi lt lq l lr ls"># Load the data<br/>train = pd.read_csv('train.csv')</span></pre><h1 id="56ea" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">检查丢失的数据</h1><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="4d46" class="lo km hi lk b fi lp lq l lr ls"># Check missingness:<br/>missingData = train.isnull().mean(axis=0)</span><span id="552b" class="lo km hi lk b fi lt lq l lr ls"># remove is greater than 30%<br/># index and gives the column names<br/>missingIndex = missingData[missingData&gt;0.3].index<br/>missingIndex</span><span id="2af1" class="lo km hi lk b fi lt lq l lr ls"># Make a working copy of the data<br/>workingDf = train.copy()<br/>workingDf.isna().sum().loc[workingDf.isna().sum()&gt;0].sort_values()</span></pre><p id="9f3a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">将输出任何缺失数据的要素。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="03fc" class="lo km hi lk b fi lp lq l lr ls">Index(['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], dtype='object')</span><span id="830d" class="lo km hi lk b fi lt lq l lr ls">Electrical         1<br/>MasVnrType         8<br/>MasVnrArea         8<br/>BsmtQual          37<br/>BsmtCond          37<br/>BsmtFinType1      37<br/>BsmtExposure      38<br/>BsmtFinType2      38<br/>GarageCond        81<br/>GarageQual        81<br/>GarageFinish      81<br/>GarageType        81<br/>GarageYrBlt       81<br/>LotFrontage      259<br/>FireplaceQu      690<br/>Fence           1179<br/>Alley           1369<br/>MiscFeature     1406<br/>PoolQC          1453<br/>dtype: int64</span></pre><p id="0fa9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">许多丢失的数据往往只是因为没有游泳池或壁炉或其他什么东西，所以我们可以用 0 或“没有游泳池”或其他任何特性来替换 NULL。对每个特性都这样做，如下所示:</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="e04f" class="lo km hi lk b fi lp lq l lr ls"><em class="kk">#Remove NA from PoolQC</em></span><span id="9cdd" class="lo km hi lk b fi lt lq l lr ls">workingDf.loc[pd.Series(workingDf.PoolQC.isna()), 'PoolQC'] <strong class="lk hj">=</strong> 'NoPool'</span></pre><p id="a0c4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">某些要素可能高度相关，并具有缺失值。用散点图之类的东西检查与您可能怀疑的特征的相关性。例如，LotFrontage 缺少很多数据，但我猜它与总 LotArea 相关。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="5ccd" class="lo km hi lk b fi lp lq l lr ls"><em class="kk"># Compare frontage to lot area!</em></span><span id="f02b" class="lo km hi lk b fi lt lq l lr ls">lotFrontageByArea <strong class="lk hj">=</strong> workingDf[['LotFrontage', 'LotArea']]</span><span id="c631" class="lo km hi lk b fi lt lq l lr ls">plt.scatter(np.log(workingDf['LotArea']), np.log(workingDf['LotFrontage']))</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lu"><img src="../Images/57151ed285f5708235407578dea11117.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*bHJ8o_nA6C_PMwovMrWmuw.png"/></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">看起来高度相关</figcaption></figure><p id="7e6b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，要填充相关要素中的缺失数据，我们可以制作两者的模型，将它们分为缺失和非缺失，然后预测缺失值并重新组合！</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="2aa9" class="lo km hi lk b fi lp lq l lr ls"><strong class="lk hj">from</strong> sklearn.impute <strong class="lk hj">import</strong> SimpleImputer<br/><strong class="lk hj">from</strong> sklearn.linear_model <strong class="lk hj">import</strong> LinearRegression, Ridge, Lasso, ElasticNet</span><span id="90d5" class="lo km hi lk b fi lt lq l lr ls">lotByAreaModel <strong class="lk hj">=</strong> linear_model.LinearRegression()<br/>lotByAreaModel.fit(lotFrontageNoNa[['LotArea']], lotFrontageNoNa.LotFrontage)</span><span id="a69e" class="lo km hi lk b fi lt lq l lr ls"><em class="kk">#Spint into missing and not missing<br/></em>workingDfFrontageNas <strong class="lk hj">=</strong> workingDf[workingDf.LotFrontage.isna()]<br/>workingDfFrontageNoNas <strong class="lk hj">=</strong> workingDf[<strong class="lk hj">~</strong>workingDf.LotFrontage.isna()]</span><span id="0be0" class="lo km hi lk b fi lt lq l lr ls"><em class="kk"># Must use Data frame<br/></em>workingDfFrontageNas.LotFrontage <strong class="lk hj">=</strong> lotByAreaModel.predict(workingDfFrontageNas[['LotArea']])</span><span id="4aea" class="lo km hi lk b fi lt lq l lr ls"><em class="kk"># Must concat a list!!!<br/></em>workingDfImputedFrontage <strong class="lk hj">=</strong> pd.concat([workingDfFrontageNas, workingDfFrontageNoNas], axis <strong class="lk hj">=</strong> 0)</span></pre><p id="2dee" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">接下来，您需要对分类特征进行虚拟化。在某些情况下，这可能会使我们的数据集非常“宽”，但对于我们将使用的大多数回归来说，这是可以的。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="cd46" class="lo km hi lk b fi lp lq l lr ls"><em class="kk"># Now Dummify to, workingDummies<br/></em>workingDummies <strong class="lk hj">=</strong> workingClean.copy()<br/>workingDummies <strong class="lk hj">=</strong> pd.get_dummies(workingDummies)<br/>print(workingDummies.shape)<br/>workingDummies.head()</span><span id="3ddb" class="lo km hi lk b fi lt lq l lr ls">print(workingDummies.isna().sum().loc[workingDummies.isna().sum()<strong class="lk hj">&gt;</strong>0].sort_values(ascending<strong class="lk hj">=False</strong>))</span></pre><p id="f590" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">确保您没有 NA 值。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="78a6" class="lo km hi lk b fi lp lq l lr ls"><em class="kk"># Replace NAs in Dummies Set with 0</em></span><span id="8475" class="lo km hi lk b fi lt lq l lr ls">print(workingDummies.isna().sum().loc[workingDummies.isna().sum()<strong class="lk hj">&gt;</strong>0].sort_values(ascending<strong class="lk hj">=False</strong>))</span></pre><p id="7a67" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">将数据分为训练集和测试集:</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="b88f" class="lo km hi lk b fi lp lq l lr ls"><em class="kk">#split feature and salePrice<br/></em>salePriceClean <strong class="lk hj">=</strong> workingClean.SalePrice<br/>homeFeaturesClean <strong class="lk hj">=</strong> workingClean.copy().drop("SalePrice",axis<strong class="lk hj">=</strong>1)<br/>salePriceDummies <strong class="lk hj">=</strong> workingDummies.SalePrice<br/>homeFeaturesDummies<strong class="lk hj">=</strong>workingDummies.copy().drop("SalePrice",axis<strong class="lk hj">=</strong>1)</span></pre><p id="d00e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，对于 EDA，我们将分别可视化连续和分类特征。首先，这里是如何为所有数值制作一个直方图网格。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="9ece" class="lo km hi lk b fi lp lq l lr ls"><em class="kk"># Split into contious and Categorical</em></span><span id="02f2" class="lo km hi lk b fi lt lq l lr ls">workingNumeric <strong class="lk hj">=</strong> workingClean[['GarageYrBlt', 'LotFrontage', 'LotArea', 'OverallCond', 'YearBuilt', 'YearRemodAdd',  'MoSold', 'GarageArea', 'TotRmsAbvGrd', 'GrLivArea', 'BsmtUnfSF', 'MSSubClass', 'YrSold', 'MiscVal', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'BsmtFullBath','BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'Fireplaces', 'GarageCars', 'WoodDeckSF', 'OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'OverallQual', 'MasVnrArea']]<br/>workingNumeric['SalePrice'] <strong class="lk hj">=</strong> salePriceClean</span></pre><p id="0cc3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，用 plt 绘图。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="2fdf" class="lo km hi lk b fi lp lq l lr ls">​fig <strong class="lk hj">=</strong> plt.figure(figsize<strong class="lk hj">=</strong>[20,10])</span><span id="52d9" class="lo km hi lk b fi lt lq l lr ls"><em class="kk"># get current axis = gca<br/></em>ax <strong class="lk hj">=</strong> fig.gca()</span><span id="02ca" class="lo km hi lk b fi lt lq l lr ls"><em class="kk"># We here will apply to the last one described...<br/></em>workingNumeric.hist(ax <strong class="lk hj">=</strong> ax)</span><span id="ab13" class="lo km hi lk b fi lt lq l lr ls">plt.subplots_adjust(hspace<strong class="lk hj">=</strong>0.5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lv"><img src="../Images/35a2d10095ffc867f8e5b24c302aa238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-pvXsDvG9LYVcrA43LZ7Dg.png"/></div></div></figure><p id="56a0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">然后，我们可以查看这些相同特征的关联热图:</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="08c3" class="lo km hi lk b fi lp lq l lr ls">rs <strong class="lk hj">=</strong> workingNumeric<br/>d <strong class="lk hj">=</strong> pd.DataFrame(data<strong class="lk hj">=</strong>workingNumeric, columns<strong class="lk hj">=</strong>list(workingNumeric.columns))</span><span id="515e" class="lo km hi lk b fi lt lq l lr ls"><em class="kk"># Compute the correlation matrix<br/></em>corr <strong class="lk hj">=</strong> d.corr()</span><span id="7694" class="lo km hi lk b fi lt lq l lr ls"><em class="kk"># Generate a mask for the upper triangle<br/></em>mask <strong class="lk hj">=</strong> np.triu(np.ones_like(corr, dtype<strong class="lk hj">=</strong>bool))</span><span id="853d" class="lo km hi lk b fi lt lq l lr ls"><em class="kk"># Set up the matplotlib figure<br/></em>f, ax <strong class="lk hj">=</strong> plt.subplots(figsize<strong class="lk hj">=</strong>(11, 9))</span><span id="1f5d" class="lo km hi lk b fi lt lq l lr ls"><em class="kk"># Generate a custom diverging colormap<br/></em>cmap <strong class="lk hj">=</strong> sea.diverging_palette(230, 20, as_cmap<strong class="lk hj">=True</strong>)</span><span id="06ee" class="lo km hi lk b fi lt lq l lr ls"><em class="kk"># Draw the heatmap with the mask and correct aspect ratio<br/></em>sea.heatmap(corr, mask<strong class="lk hj">=</strong>mask, cmap<strong class="lk hj">=</strong>cmap, vmax<strong class="lk hj">=</strong>.3, center<strong class="lk hj">=</strong>0,<br/>square<strong class="lk hj">=True</strong>, linewidths<strong class="lk hj">=</strong>.5, cbar_kws<strong class="lk hj">=</strong>{"shrink": .5})</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/e99a168e0bd0e295a3941db05de696e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*xJGJruBb7RIeskE8qaig3A.png"/></div></figure><p id="6e21" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">接下来，我们用 Python 为所有分类特征生成一个条形图网格。不幸的是，我们不像 boxplot 代码，我们需要单独生成每个图，并将它们放在图中，我在这里只显示前几个图的代码。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="9d4f" class="lo km hi lk b fi lp lq l lr ls">fig, axes <strong class="lk hj">=</strong> plt.subplots(13, 3, figsize<strong class="lk hj">=</strong>(18, 55))</span><span id="35a3" class="lo km hi lk b fi lt lq l lr ls">sea.boxplot(ax<strong class="lk hj">=</strong>axes[0, 0], data<strong class="lk hj">=</strong>workingCategorical, x<strong class="lk hj">=</strong>'MSZoning', y<strong class="lk hj">=</strong>'SalePrice')</span><span id="456a" class="lo km hi lk b fi lt lq l lr ls">sea.boxplot(ax<strong class="lk hj">=</strong>axes[0, 1], data<strong class="lk hj">=</strong>workingCategorical, x<strong class="lk hj">=</strong>'SaleType', y<strong class="lk hj">=</strong>'SalePrice')</span><span id="72e2" class="lo km hi lk b fi lt lq l lr ls">sea.boxplot(ax<strong class="lk hj">=</strong>axes[0, 2], data<strong class="lk hj">=</strong>workingCategorical, x<strong class="lk hj">=</strong>'GarageType', y<strong class="lk hj">=</strong>'SalePrice')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/0a53d2ae5cdad2abb8ad3d19150d3bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qxhTSGxIPpdQxAWaHXPQA.png"/></div></div></figure><p id="413d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">最后，到我们的模型建筑。在这里，我将展示如何用虚拟数据集做几个回归模型，哪一个的结果是最好的，以及我们如何使用这些信息。</p><p id="6a59" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">第一，KNN 模型使用网格搜索寻找最佳参数(α):</p><p id="b99c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi">1</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="912e" class="lo km hi lk b fi lp lq l lr ls"><em class="kk">#KNN<br/></em>knn_model <strong class="lk hj">=</strong> KNeighborsRegressor()<br/>param_grid <strong class="lk hj">=</strong> {'n_neighbors':np.arange(5, 200, 5)}<br/>gsModelTrain <strong class="lk hj">=</strong> GridSearchCV(estimator <strong class="lk hj">=</strong> knn_model, param_grid <strong class="lk hj">=</strong> param_grid, cv<strong class="lk hj">=</strong>2)<br/>gsModelTrain.fit(featuresDummiesTrain, priceDummiesTrain)<br/>knn_model.set_params(<strong class="lk hj">**</strong>gsModelTrain.best_params_)</span><span id="c1bb" class="lo km hi lk b fi lt lq l lr ls"><em class="kk">#fit to train data<br/></em>knn_model.fit(featuresDummiesTrain, priceDummiesTrain)</span><span id="616f" class="lo km hi lk b fi lt lq l lr ls"><em class="kk">#Get scores comparing real house prices and predicted house prices from the test dataset.<br/></em>print("r2 Test score:", r2_score(priceDummiesTest, knn_model.predict(featuresDummiesTest)))</span><span id="ec66" class="lo km hi lk b fi lt lq l lr ls">print("r2 Train score:", r2_score(priceDummiesTrain, knn_model.predict(featuresDummiesTrain)))</span><span id="5e80" class="lo km hi lk b fi lt lq l lr ls">trainRMSE <strong class="lk hj">=</strong> np.sqrt(mean_squared_error(y_true<strong class="lk hj">=</strong>priceDummiesTrain, y_pred<strong class="lk hj">=</strong>knn_model.predict(featuresDummiesTrain)))</span><span id="3df9" class="lo km hi lk b fi lt lq l lr ls">testRMSE <strong class="lk hj">=</strong> np.sqrt(mean_squared_error(y_true<strong class="lk hj">=</strong>priceDummiesTest, y_pred<strong class="lk hj">=</strong>knn_model.predict(featuresDummiesTest)))</span><span id="f20f" class="lo km hi lk b fi lt lq l lr ls">print("Train RMSE:", trainRMSE)<br/>print("Test RMSE:", testRMSE)</span></pre><p id="69f1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">结果！</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="e18e" class="lo km hi lk b fi lp lq l lr ls">​r2 Test score: 0.6307760069201898<br/>r2 Train score: 0.7662448448900541<br/>Train RMSE: 37918.53459536937<br/>Test RMSE: 48967.481488778656</span></pre><p id="48a1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，对 Ridge()、Lasso、RandomForrest 做同样的事情，但是改变优化的参数。以下是我的最终结果:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/6c9e279dc8b55286bf62ac6bae32601d.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*JG6jK0r_qubmwuxbUI0zYw.png"/></div></figure><p id="9493" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们看到随机森林或 boosting 模型是最好的，因此接下来我们从该模型中获取最重要的特征，如下所示:</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="564d" class="lo km hi lk b fi lp lq l lr ls">rfModel.feature_importances_<br/>feature_importances <strong class="lk hj">=</strong> pd.DataFrame(rfModel.feature_importances_,<br/>index <strong class="lk hj">=</strong> featuresDummiesTrain.columns,<br/>columns<strong class="lk hj">=</strong>['importance']).sort_values('importance',ascending<strong class="lk hj">=False</strong>)</span><span id="04c7" class="lo km hi lk b fi lt lq l lr ls">pd.set_option("display.max_rows",<strong class="lk hj">None</strong>,"display.max_columns",<strong class="lk hj">None</strong>)</span><span id="2196" class="lo km hi lk b fi lt lq l lr ls">print(feature_importances)</span><span id="d4e3" class="lo km hi lk b fi lt lq l lr ls">feature_importances.index[:10]</span></pre><p id="76fa" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这就是我们如何使用 Python 获得该模型中最重要特性的列表！</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="082f" class="lo km hi lk b fi lp lq l lr ls">​importance<br/>OverallQual              5.559609e-01<br/>GrLivArea                9.929112e-02<br/>TotalBsmtSF              4.050054e-02<br/>1stFlrSF                 3.604577e-02<br/>TotRmsAbvGrd             2.772576e-02<br/>FullBath                 2.693519e-02<br/>BsmtFinSF1               2.064975e-02<br/>GarageCars               1.915783e-02<br/>2ndFlrSF                 1.753713e-02<br/>GarageArea               1.748563e-02<br/>LotArea                  1.219073e-02<br/>YearBuilt                1.075511e-02<br/>LotFrontage              7.270100e-03<br/>YearRemodAdd             7.038709e-03<br/>BsmtQual_Ex              5.726935e-03<br/>OpenPorchSF              4.677578e-03<br/>BsmtUnfSF                4.245650e-03<br/>MoSold                   3.397142e-03<br/>OverallCond              3.180477e-03<br/>WoodDeckSF               2.865491e-03<br/>KitchenQual_Gd           2.692117e-03<br/>ExterQual_Ex             2.253200e-03<br/>GarageType_Detchd        1.832978e-03<br/>MSSubClass               1.808349e-03<br/>BsmtFullBath             1.791505e-03<br/>MSZoning_RM              1.781576e-03<br/>ScreenPorch              1.679301e-03<br/>YrSold                   1.664580e-03<br/>BsmtExposure_No          1.533721e-03<br/>GarageFinish_Unf         1.514469e-03<br/>MasVnrArea_1170.0        1.431316e-03</span></pre><p id="ca00" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">所以现在我们知道房子的整体质量和大小是非常重要的，但是我们基本上可以忽略一些东西，比如一个完工的车库，或者它是否在门廊上安装了纱窗！</p><p id="cbcd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">您可以使用同一系统评估具有任意数量特征的任何本国数据。我们在这里的大收获是，列表底部的所有特征都可以安全地忽略，所以如果你在出售或定价房屋，大多数时候甚至没有理由考虑它们。</p><blockquote class="lz"><p id="30e4" class="ma mb hi bd mc md me mf mg mh mi ke dx translated">快乐找房子，谢谢你！</p></blockquote></div></div>    
</body>
</html>