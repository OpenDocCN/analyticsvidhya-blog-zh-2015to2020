<html>
<head>
<title>Transformers — Let’s Dive Deeeep!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚——让我们继续沉睡吧！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/transformers-lets-dive-deeeep-7784bdb20807?source=collection_archive---------2-----------------------#2020-12-01">https://medium.com/analytics-vidhya/transformers-lets-dive-deeeep-7784bdb20807?source=collection_archive---------2-----------------------#2020-12-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/e50d902a0af104461d344d74c2f35279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tl9K1t1edRZfwzzPVQJSvw.jpeg"/></div></figure><p id="f1a3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">警告</strong>！！</p><p id="a5e2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个<strong class="io hj">博客</strong>是<strong class="io hj">而不是</strong>给那些<strong class="io hj">害怕</strong>数学的人看的！</p><p id="7b1d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">(但是如果你被冒犯了，一定要读一读:P)</p><h1 id="561f" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">那么，什么是变形金刚呢？</h1><p id="8232" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">现在，你一定在想美国科幻动作片<strong class="io hj">中<strong class="io hj">迈克尔·贝</strong>系列的机器人</strong>！</p><p id="8dd4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">不，这是不同的东西。</p><p id="c12b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi kn translated"><span class="l ko kp kq bm kr ks kt ku kv di">T</span>he<strong class="io hj">Transformer</strong>是在<strong class="io hj"> 2017 </strong>中推出的一款<strong class="io hj"> </strong> <a class="ae kw" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj">深度学习</strong> </a>模型，以其<strong class="io hj">架构</strong>在论文中提出的<strong class="io hj"/><a class="ae kw" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">注意力就是你所需要的</strong></a><strong class="io hj"/>为基础的一种<strong class="io hj">自我注意机制</strong>而使用</p><h1 id="3bb8" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">是什么让《变形金刚》成为“语言理解的新型神经网络架构”？</h1><p id="9fc4" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">与<strong class="io hj">rnn</strong>不同，<strong class="io hj">变压器</strong>不要求顺序数据按照顺序进行<strong class="io hj">处理。例如，如果输入数据是自然语言语句，则转换器<strong class="io hj">不需要在结束</strong>之前处理它的开始</strong>。由于这个特性，<strong class="io hj">转换器比 RNNs 更适合于</strong> <a class="ae kw" href="https://en.wikipedia.org/wiki/Parallel_computing" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj">并行化</strong> </a> <strong class="io hj">，因此减少了训练次数。</strong></p><p id="dbb6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当我去研究提出这个模型(我上面提到的那个)的研究论文时，我太笨了，甚至连一个句子或其中包含的复杂计算都不懂。因此，在花了几个晚上在各种参考博客上之后，我终于成功地破解了 NLP 这个庞然大物！</p><blockquote class="kx ky kz"><p id="412f" class="im in la io b ip iq ir is it iu iv iw lb iy iz ja lc jc jd je ld jg jh ji jj hb bi translated">在这里，在这个博客中，我将尽我所能简化文章中的东西，你需要知道这些来获得一个关于 Transformer 架构的像样的知识！相信我，你会喜欢的！</p></blockquote><h2 id="d182" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">所以，我们开始吧！</h2><h2 id="9bdf" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">1.过于简单的表情(为了不吓到你) :</h2><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es ls"><img src="../Images/2496c07a85a9f45675e04d333c708e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3QN9vmVyPb1vBTYOiE96Fw.png"/></div></div></figure><p id="771f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们不要一开始就把事情复杂化，看看巨大的<strong class="io hj">艺术模型</strong>就像一个简单的<strong class="io hj">黑盒</strong>，它只需用一种语言(这里说我们用英语)输入的<strong class="io hj">句子，<strong class="io hj">将其翻译成另一种语言</strong>作为输出(这里说它用德语输出)。</strong></p><h2 id="1a0c" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">2.是时候打开野兽了！(但是很慢:“3”</h2><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mb"><img src="../Images/453e5bb1b4e9a3fb53df5360e35573a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ssAoid8gjQh-eDQBzZyB_Q.png"/></div></div></figure><p id="814c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以，在<strong class="io hj">将</strong>分解后，我们看到<strong class="io hj">里面有两个主要组件</strong>。左侧为<strong class="io hj">编码器</strong>组件，右侧为<strong class="io hj">解码器</strong>组件。在继续之前，让我们向您介绍一下编码器和解码器各自的作用。</p><p id="0efe" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">编码器</strong>—<strong class="io hj">编码器</strong>将一个输入序列映射到一个抽象的连续表示中，该表示保存了该输入的所有学习信息。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/a08d1e2fb347669b51a22b2b023def96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*inMSIkB5mb0Ukk8jB94pXQ.gif"/></div></figure><p id="eafb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">解码器</strong>—<strong class="io hj">解码器</strong>然后获取该连续表示，并逐步生成单个输出，同时也被馈送先前的输出。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/38b9451c7badfdc77f634f60318f0cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*ZvQQqedAY2LYRkp7BzjTJA.gif"/></div></figure><p id="8fc1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在提出的论文中，研究人员已经将编码器作为一个由 6 个编码器组成的<strong class="io hj">堆栈，解码器组件也是由一个相同编号</strong>的解码器<strong class="io hj">堆栈组成。这里必须提到的是，他们已经用<strong class="io hj">尝试了编码器和解码器</strong>数量的许多变化，其中采用<strong class="io hj"> 6 作为超参数，获得了最好的结果，</strong>因此他们在论文中提出了 6。</strong></p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es md"><img src="../Images/05664200d85ef77de677234dfde47116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*yNSNU686RvLSvwH4_8gHvg.png"/></div></figure><h2 id="5697" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">3.现在，让我们深入研究编码器架构！</h2><p id="032a" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated"><strong class="io hj">编码器</strong>共用<strong class="io hj">相同的结构</strong>，其中<strong class="io hj">各由两个子层</strong>、<strong class="io hj">自关注层</strong>和<strong class="io hj">前馈神经网络层</strong>组成。看下图。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es me"><img src="../Images/85e700a5443ca56a5afeb9dd1acd9bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HaGTuYfNHWg45GZbTBnVSA.png"/></div></div></figure><p id="8849" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">自我关注层</strong>:自我关注层使模型能够学习当前单词与句子前一部分的<strong class="io hj">相关性。</strong></p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mf"><img src="../Images/188995c223d8bfc91eb91d055e9e394f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LMxtoaou2EAkehmdyJjxaA.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">红色表示当前单词，蓝色表示相关激活级别。</figcaption></figure><p id="667e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">简而言之，自我关注层所做的就是，它允许<strong class="io hj">编码器</strong>到<strong class="io hj">查看输入句子中的其他单词</strong>，<strong class="io hj">将它们与</strong>，<strong class="io hj">相关联，捕捉它们的重要性，同时它对特定的单词进行编码。</strong></p><p id="a28b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">考虑下面的句子:</p><p id="2563" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个男孩因为受伤而无法参加比赛。</p><p id="df8e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这句话中的“<strong class="io hj">他</strong>指的是<strong class="io hj">男孩</strong>，而不是<strong class="io hj">“受伤”</strong>这个词的直觉，一个<strong class="io hj">人类</strong>可以立刻理解，但对于一个学习算法来说，这绝对是一个<strong class="io hj">更难的任务。所以，当模型在处理“他”这个词的时候，<strong class="io hj">自我注意允许它把“他”和“男孩”联系起来</strong>。</strong></p><p id="a065" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">自我注意力计算</strong>仍然需要非常深刻的<strong class="io hj">理解，因为其中有许多复杂的<strong class="io hj">步骤</strong>，需要仔细观察。我们一个一个来看。(这个真的需要耐心:0)</strong></p><h2 id="5c0e" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">步骤 1:创建查询向量(q1，q2)、关键向量(k1，k2)和值向量(v1，v2)。</h2><p id="a695" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated"><strong class="io hj">键/值/查询</strong>概念通常来自<strong class="io hj">信息检索</strong>系统。您可以简单地将'<strong class="io hj">查询</strong>映射到一组相关的'<strong class="io hj">键</strong>'，以检索给定'<strong class="io hj">查询</strong>'的最佳'<strong class="io hj">值</strong>。这些向量通过<strong class="io hj">将嵌入乘以在训练期间训练的三个矩阵</strong>来创建。</p><p id="376e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，如果<strong class="io hj"> X1 和 X2 </strong>是我们的嵌入输入，并且<strong class="io hj"> W(Q)，W(K)和 W(V) </strong>是权重矩阵、w.r.t 查询、键和值，那么我们的查询、键和值向量可以通过以下公式计算</p><p id="3c56" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> X1 x W(Q) = q1，X2 x W(Q) = q2 </strong></p><p id="de56" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> X1 x W(K) = k1，X2 x W(k) = k2 </strong></p><p id="aa4c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> X1 x W(V) = v1，X2 x W(V) = v2 </strong></p><h2 id="c3d8" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">第二步:计算每个单词的分数</h2><p id="c741" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">现在我们需要计算输入句子中每个单词相对于我们一次考虑的单词的分数。这个过程非常关键，因为它<strong class="io hj">建立了一个注意力分数</strong>，或者基本上是<strong class="io hj">当在某个位置对一个单词进行编码时，与其他单词</strong>相比，对该特定单词<strong class="io hj">给予</strong>多少注意力。</p><p id="b7a3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">分数的计算方法是—</p><ul class=""><li id="3ad9" class="mk ml hi io b ip iq it iu ix mm jb mn jf mo jj mp mq mr ms bi translated"><strong class="io hj">计算查询(q1，q2)向量与相应单词的关键向量(k1，k2)的点积。</strong></li></ul><p id="e8e8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> q1。k1</strong>=位置 1 的单词的分数</p><p id="4036" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> q2。k2 </strong> =位置 2 的单词得分…以此类推。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mt"><img src="../Images/776d7b43e264930cf03f4fcadece3010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Tm6NnavwUycmGZP8I4EjQw.gif"/></div></div></figure><p id="fd18" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里需要注意的一点是，查询、键和值向量的<strong class="io hj">维度比嵌入向量</strong>要小得多。在所提出的论文中，嵌入向量的维数被取为<strong class="io hj"> 512 </strong>，而 q、k 和 v 向量据说具有<strong class="io hj"> 64 </strong>的维数。</p><h2 id="9c7d" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">步骤 3:将获得的分数除以关键向量(d(k))的维数的平方根，并对其应用 softmax 激活。</h2><p id="049f" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated"><strong class="io hj">第三步</strong>包括将先前获得的分数除以 8(因为 root(d(k))= root(64)=8)。</p><p id="cbe6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">请注意,( 1/√dk)是比例因子，为模型提供更稳定的梯度。然而，点积的大小往往会随着查询和关键向量的维数而增长，因此转换器会重新调整点积的大小，以防止它爆炸成巨大的值。</strong></p><p id="7de0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，这一步的一般公式如下:</p><p id="dc57" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">分数/根(d(k)) = (q1.k1)/8，(q1.k2)/8，…诸如此类。</strong></p><p id="eb20" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们将<strong class="io hj"> softmax 激活</strong>应用于获得的值，这<strong class="io hj">将</strong>分数归一化，因此它们可以<strong class="io hj">相加为 1.0 </strong>。</p><h2 id="3493" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">步骤 4:计算缩放的点积关注度</h2><p id="0f50" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">这个步骤过于简单，我们可以说这个步骤仅仅包括<strong class="io hj">将每个值向量</strong>乘以之前获得的<strong class="io hj"> softmax 得分</strong>。做这一步的原因基本上是<strong class="io hj">保留我们想要关注的单词的值</strong>，而<strong class="io hj">剔除不相关的单词</strong>。在这之后，我们简单地对加权值向量求和，以获得一个新的向量，比如 z</p><p id="f7e1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，计算自我注意力的整个过程的公式是这样的</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/256af46bc628d257291b0e84301c13a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*Q5LoFPhezSsT_1QRHGCT-Q.png"/></div></figure><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mv"><img src="../Images/d42337c5118f94e28227df8837edd512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSwckeG028obZPWafgJrmw.png"/></div></div></figure><p id="a251" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这就是<strong class="io hj">自我关注度的计算方法</strong>。希望方程式看起来不那么迷人<strong class="io hj">现在</strong><strong class="io hj">:p .</strong></p><h2 id="b26a" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">得到的向量是我们可以发送给前馈神经网络的向量！！</h2><p id="6b75" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated"><strong class="io hj">前馈神经网络</strong>——自关注层产生的输出然后<strong class="io hj">传递</strong>到<strong class="io hj">前馈神经网络</strong>，在那里相同的 FFNN 层被<strong class="io hj">独立地应用到每个位置</strong>。在编码器中，每个位置的字都遵循它们自己的路径<strong class="io hj">。自我关注层中的这些路径之间存在依赖关系，但是前馈层<strong class="io hj">不</strong>具有这些依赖关系。因此，不同的路径可以<strong class="io hj">并行</strong>运行，同时穿过前馈层。</strong></p><p id="5f3d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">希望现在你已经对<strong class="io hj">编码器</strong>的主要组件有了一些直觉。</p><h2 id="bd60" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">所以，现在让我们更深入，让向量进入框架！</h2><p id="7b8e" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">如果你熟悉文本预处理的<strong class="io hj">基本 NLP 技术，你一定知道这个任务包括<strong class="io hj">使用<strong class="io hj">单词嵌入技术</strong>(带有<strong class="io hj"> Word2Vec </strong>、<strong class="io hj"> tfidf 编码、潜在语义分析编码、二进制编码等)将每个输入单词转换成一个矢量</strong>。</strong>)。</strong></p><p id="1949" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">嵌入</strong>发生在<strong class="io hj">最下面的编码器</strong>中。编码器接收每个维度为<strong class="io hj"> 512 </strong>的向量列表。尺寸仅仅是一个<strong class="io hj">超参数，</strong>我们需要用它来<strong class="io hj">微调</strong>。通常，这将是我们训练数据中最长句子的<strong class="io hj">长度。</strong></p><p id="2d6e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">将单词嵌入我们的输入序列后，每个单词都流经上面指定的编码器的两层中的每一层，最后，FFNN 的输出被传递到下一个编码器层。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es mw"><img src="../Images/316039549121ed96a862dfcefb8e1d23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RBzykEkO7ZQn4UEriYFUkA.jpeg"/></div></div></figure><p id="2344" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">既然你头脑中已经有了关于编码过程如何执行的清晰画面，让我们进入研究人员开发的奇妙概念“多头注意力”。</p><h2 id="a4c5" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">4.多头野兽</h2><p id="7a03" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">Transformer 中的注意机制被解释为基于一些<strong class="io hj">键</strong>和<strong class="io hj">查询</strong>来计算一组<strong class="io hj">值</strong>(信息)的相关性的方式。</p><p id="61e9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果我们只计算值的单个注意力加权和，将很难捕捉输入的各种不同方面。为了解决这个问题，研究人员提出了<strong class="io hj">多头注意力</strong>模块。</p><p id="d460" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这样计算出<strong class="io hj"> <em class="la">多个</em>注意力</strong>的加权和，而不是一个<strong class="io hj"> <em class="la">单个</em>注意力</strong>传递过来的值——因此得名<strong class="io hj">“多头”注意力。</strong></p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/ac15c60bbfc6488dd557ec173340c5ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*N3v4hKmmLRUC5fCTPWHSFQ.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">多头注意力将<strong class="bd jm">不同的线性变换</strong>应用于每个注意力“头”的值、键和查询。</figcaption></figure><h1 id="cf4b" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">不要把你对“多头注意力”架构的想法复杂化，而是像这样想—</h1><p id="d871" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated"><strong class="io hj">我们上面讨论的<strong class="io hj">的自我关注计算</strong>只需要用不同的权重矩阵</strong>重复 8 次，其中<strong class="io hj">权重被随机初始化</strong>，从而<strong class="io hj">创建多个自我关注模型。</strong></p><h2 id="73a0" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">所以，现在我们有八个不同的 Z 矩阵(Z0，Z1，Z2，…..Z7)，其中 Z0 =注意力头部矩阵 0，Z1 =注意力头部矩阵 1..诸如此类。</h2><h1 id="5431" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">万岁！！</h1><p id="181d" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated"><strong class="io hj">等一下</strong>！它还没有完成。我们需要记住，<strong class="io hj">前馈神经网络只接受一个矩阵</strong>(每个单词的向量)。所以我们必须<strong class="io hj">将所有这 8 个矩阵</strong>连接成一个<strong class="io hj">单一矩阵。</strong></p><h2 id="aecc" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">怎么会？</h2><p id="2f04" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">嗯，没什么大不了的。</p><p id="542d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">我们连接这些矩阵，然后将它们乘以一个额外的权重矩阵 W(O ),与模型一起训练。</strong></p><h2 id="6dbe" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">那么我们现在得到了什么？</h2><p id="eca0" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">我们得到了一个<strong class="io hj"> Z 矩阵，它存储了来自所有 8 个注意力头的相关信息，现在可以发送到 FFNN。</strong></p><h1 id="9238" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">耶！！</h1><p id="544c" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated"><strong class="io hj">到目前为止我们讨论的所有内容的直观描述— </strong></p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es my"><img src="../Images/d9467e0ecb2dd4d67e905cd1df27f788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNdtPg2KINh9-lhrVswa0w.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">来源—<a class="ae kw" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-transformer/</a></figcaption></figure><h1 id="ee6c" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">位置编码—一个重要的概念</h1><p id="964f" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">与<strong class="io hj">递归神经网络和 LSTM-RNNs </strong>不同，<strong class="io hj">多头注意力网络不能自然地利用单词在输入序列中的位置。</strong>如果没有位置编码，多头注意力网络的<strong class="io hj">输出</strong>对于句子<strong class="io hj">“我喜欢鸡多过鱼”</strong>和<strong class="io hj">“我喜欢鱼多过鸡”，将是同样的<strong class="io hj"/>。</strong>位置编码<strong class="io hj">将输入的相对/绝对位置</strong>明确编码为矢量，然后添加到<strong class="io hj">输入嵌入中。</strong></p><h2 id="aa94" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">什么时候执行位置编码？</h2><p id="e79c" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">位置编码是<strong class="io hj">至关重要的步骤</strong>，在将字转换为向量之后，在将其传递给嵌入之前执行<strong class="io hj">。</strong></p><h2 id="f7b8" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">使用位置编码的好处-</h2><ol class=""><li id="046b" class="mk ml hi io b ip ki it kj ix mz jb na jf nb jj nc mq mr ms bi translated">在嵌入向量投影到 Q、K、V 向量时，在嵌入向量之间提供有意义的距离。</li><li id="ae25" class="mk ml hi io b ip nd it ne ix nf jb ng jf nh jj nc mq mr ms bi translated">它提供了模式，<strong class="io hj">对单词</strong>的顺序感，从而遵循了<strong class="io hj">单词的特定模式</strong>。</li></ol><p id="9fe3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">直觉</strong> —使用这种技巧是因为<strong class="io hj">没有语序的概念</strong>(第一个词，第二个词，..)在建议的架构中。输入序列的所有字都被馈送到网络，没有特殊的顺序或位置(不像常见的 RNN 或康文网络架构)，因此，模型<strong class="io hj">不知道这些字是如何排序的</strong>。因此，依赖于位置的<strong class="io hj">信号</strong>被添加到每个单词嵌入中，以帮助<strong class="io hj">模型合并单词的顺序</strong>。<strong class="io hj">根据实验，这种添加不仅避免了破坏嵌入信息，而且添加了重要的位置信息。</strong></p><p id="bc37" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">注意</strong> —位置编码器的作用是借助 sin(x)和 cos(x)函数的循环特性，返回单词在句子中的位置信息。</p><p id="c16c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">本文使用以下等式<strong class="io hj">计算位置编码</strong>:</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/9ad8032fbeb3a26b40cb95ff7031b549.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*tQ4d6ObsUFKYJewNqlPIKQ.png"/></div></figure><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/8eaf967307ceeebb4ff4e1cf4c0ef868.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*VDfVRb6JnfxEd88wXJv5LQ.png"/></div></figure><p id="80b6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">其中，<strong class="io hj"> pos </strong>代表<strong class="io hj">位置</strong>，<strong class="io hj"> i </strong>代表<strong class="io hj">尺寸</strong>，原文中<strong class="io hj"> d(型号)=512，(因此 I∈【0，255】)</strong>。对于<strong class="io hj">输入向量</strong>上的每个<strong class="io hj">奇数索引</strong>，使用 cos 函数创建一个<strong class="io hj">向量。对于每个<strong class="io hj">偶数索引</strong>，使用正弦函数</strong>创建一个<strong class="io hj">向量。然后<strong class="io hj">将这些向量添加到它们相应的输入嵌入中。</strong>这成功地给出了关于每个矢量位置的网络信息。</strong></p><p id="022d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">剩余漏失</strong></p><p id="0333" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">作者在将每个子层添加到原始输入之前，对其应用了<strong class="io hj">丢弃。他们还将 dropout 应用于嵌入的总和以及<strong class="io hj">位置编码。</strong>辍学率默认为 0.1。</strong></p><p id="c11f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">每个编码器中的每个子层(自关注，FFNN)都有一个围绕它的残差连接，然后是一个<a class="ae kw" href="https://arxiv.org/abs/1607.06450" rel="noopener ugc nofollow" target="_blank">层归一化</a>步骤。</p><h2 id="791b" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">这背后的基本概念是跳过自我关注层，如果不需要的话，通过层规范化(类似于放弃)。</h2><p id="dec7" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">它<strong class="io hj">将 X 和 Z 矩阵</strong>相加，<strong class="io hj">以特定比率(文中使用 0.1)</strong>将其归一化。这也适用于解码器的子层。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/e0284408db16ce85519f0bd9cf16202b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*pRVYB4dwaibtW2YZdl0gcg.gif"/></div></figure><h1 id="2c14" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">因此，我们需要了解的变压器编码器模块的所有内容都结束了！</h1><blockquote class="nk"><p id="b535" class="nl nm hi bd nn no np nq nr ns nt jj dx translated">等等…，你需要在喝咖啡前把这个喝完！</p></blockquote><h2 id="0814" class="le jl hi bd jm lf nu lh jq li nv lk ju ix nw lm jy jb nx lo kc jf ny lq kg lr bi translated">解码器模块仍有待发现！让我们跳进来吧！</h2><h1 id="da47" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">解码器</h1><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/4687901f72b718d36d0cda635d78e111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*xzHBM_-KKv4SGejktFEAhw.gif"/></div></figure><p id="5377" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">解码器架构类似于编码器</strong>，除了一个额外的“<strong class="io hj">屏蔽多头注意力</strong>层，帮助<strong class="io hj">解码器将更多注意力放在输入序列中的相关位置。</strong></p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es nz"><img src="../Images/d3f0ffd4b63fc913803efe352c3da04a.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*h2wKOHxpgOSBc8qXkiqwSw.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">解码器</figcaption></figure><p id="0f6c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">记住，解码器通常被训练成基于当前单词之前的所有单词来预测句子。</p><h2 id="e143" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">这一层覆盖了先前的解码器输入，因此起到了与解码器隐藏状态相似的作用，将解码器的输入与未来的时间步骤屏蔽开。</h2><p id="af56" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">干得好！</p><h2 id="61c7" class="le jl hi bd jm lf lg lh jq li lj lk ju ix ll lm jy jb ln lo kc jf lp lq kg lr bi translated">所以，为了总结编码和解码的工作原理，让我们创建几个简单的步骤-</h2><ol class=""><li id="c54f" class="mk ml hi io b ip ki it kj ix mz jb na jf nb jj nc mq mr ms bi translated">编码器通过<strong class="io hj">处理输入序列</strong>启动。</li><li id="4227" class="mk ml hi io b ip nd it ne ix nf jb ng jf nh jj nc mq mr ms bi translated">输入通过嵌入层和位置编码层得到<strong class="io hj">位置嵌入</strong>。</li><li id="909c" class="mk ml hi io b ip nd it ne ix nf jb ng jf nh jj nc mq mr ms bi translated">这些被馈送到解码器的<strong class="io hj">第一个"<em class="la">被屏蔽的</em>多头注意力层"</strong>以屏蔽来自<strong class="io hj">未来时间步</strong>的解码器输入，以便它能够<strong class="io hj">计算解码器输入</strong>的注意力分数。</li><li id="bed8" class="mk ml hi io b ip nd it ne ix nf jb ng jf nh jj nc mq mr ms bi translated"><strong class="io hj">第一个多头关注层输出</strong>为<strong class="io hj">值</strong>。<strong class="io hj">第二多头关注层的输出</strong>是<strong class="io hj">查询和关键字。</strong>该步骤将<strong class="io hj">编码器的输入匹配到解码器的输入，</strong>允许<strong class="io hj">解码器决定关注哪个编码器输入。</strong></li><li id="68b3" class="mk ml hi io b ip nd it ne ix nf jb ng jf nh jj nc mq mr ms bi translated">第二多头注意力的<strong class="io hj">输出经过<strong class="io hj">逐点前馈层进一步处理</strong>。每一步的输出在下一个时间步被馈送到<strong class="io hj">底部解码器。</strong></strong></li><li id="df0c" class="mk ml hi io b ip nd it ne ix nf jb ng jf nh jj nc mq mr ms bi translated">这些步骤<strong class="io hj">继续运行</strong>直到<strong class="io hj">序列结束</strong>，或者串结束<strong class="io hj"> &lt; eos &gt; </strong>信号到达<strong class="io hj">指示变压器解码器已经完成其输出。</strong></li></ol><h1 id="9209" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">最终层-线性分类器和 Softmax 层</h1><p id="d383" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">在<strong class="io hj">解码器层</strong>将输出作为向量进行处理之后，<strong class="io hj">最终逐点前馈层的输出通过最终线性层</strong>，其充当<strong class="io hj">分类器</strong>。它有<strong class="io hj">个单元，其大小与您通过让您的模型从训练数据集学习而获得的输出词汇的大小一样大。</strong></p><p id="786b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个巨大的输出然后被馈送到一个<strong class="io hj"> Softmax 层，以生成范围在 0 到 1 之间的概率值。</strong></p><p id="69d1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以，为了简化你的事情，</p><p id="3f0f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">最高概率得分指数(argmax) =我们预测的单词。</strong></p><h1 id="8d56" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">这就是全部！</h1><blockquote class="kx ky kz"><p id="b5c6" class="im in la io b ip iq ir is it iu iv iw lb iy iz ja lc jc jd je ld jg jh ji jj hb bi translated">希望这个博客能帮助你理解这篇研究论文，并节省你在阅读时花在理解上的时间。</p></blockquote><p id="3347" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">也试着通过运行<a class="ae kw" href="https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj"> Tensor2Tensor 笔记本</strong> </a>让你的<strong class="io hj">之手变脏</strong>，在那里你可以<strong class="io hj">加载一个 Transformer 模型、</strong>并用一些有用的<strong class="io hj">交互式可视化来玩代码。</strong></p><blockquote class="kx ky kz"><p id="bd4f" class="im in la io b ip iq ir is it iu iv iw lb iy iz ja lc jc jd je ld jg jh ji jj hb bi translated"><strong class="io hj">如果你是数据科学和机器学习的初学者，并对数据科学/ML-AI、向数据科学的职业过渡指导、面试/简历准备有一些具体的疑问，甚至想在你的 D-Day 之前获得模拟面试，请随时在这里</strong>  <strong class="io hj">预约 1:1 电话</strong> <a class="ae kw" href="https://topmate.io/sukannya" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj">。我很乐意帮忙！</strong></a></p></blockquote><p id="14ed" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">快乐学习！</p><p id="be40" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下次见！</p></div></div>    
</body>
</html>