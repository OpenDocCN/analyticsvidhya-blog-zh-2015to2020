<html>
<head>
<title>Paper review — ADGAN:Controllable Person Image Synthesis With Attribute-Decomposed GAN(CVPR 2020)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文综述——ADGAN:利用属性分解的GAN进行可控的人物图像合成(CVPR 2020)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/paper-review-adgan-controllable-person-image-synthesis-with-attribute-decomposed-gan-1c45bddbe00a?source=collection_archive---------13-----------------------#2020-07-02">https://medium.com/analytics-vidhya/paper-review-adgan-controllable-person-image-synthesis-with-attribute-decomposed-gan-1c45bddbe00a?source=collection_archive---------13-----------------------#2020-07-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="3981" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">内容</h1><ol class=""><li id="3d4c" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">摘要</li><li id="80a8" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">方法</li><li id="056d" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">结果</li></ol><h1 id="2b78" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">1.摘要</h1><p id="a3bb" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">*所有图片来自纸张*</p><p id="515d" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><a class="ae ku" href="https://arxiv.org/pdf/2003.12267.pdf" rel="noopener ugc nofollow" target="_blank">本文</a>被(2020，口头陈述)接受，介绍了一个使用GAN进行姿态传递和组件属性传递的模型。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es kv"><img src="../Images/62b692975ae39481e08c59b9a2852274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*335UWoyujR6uOdbW9SaPaw.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图一。方法概述</figcaption></figure><p id="4b1c" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">姿态属性由姿态编码器编码，组件属性通过分解组件编码器(DCE)编码为样式代码。后面将详细描述上述两种编码器。</p><p id="bd17" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">目前，本文被实现为Pytorch。在<a class="ae ku" href="https://github.com/menyifang/ADGAN" rel="noopener ugc nofollow" target="_blank"> Github </a>上，姿态传递和组件属性传递的过程是以gif文件的形式上传的，如果你有兴趣的话，推荐你看看。</p><h1 id="9f83" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">2.方法</h1><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/a75e7a6c67364e5aec72fae951607670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4AaqVCLlSqhbBh-4zpITw.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图二。全型号。</figcaption></figure><p id="96a5" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">目标姿态表示使用<a class="ae ku" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> Openpose </a>从源图像中提取姿态关键点生成的热图(H*W*18)。因此，该模型不需要任何注释，因为它可以从图像源中提取</p><h2 id="a3be" class="lm ig hi bd ih ln lo lp il lq lr ls ip jk lt lu it jm lv lw ix jo lx ly jb lz bi translated">2–1姿态编码器</h2><p id="e7e6" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">姿态编码器使用N下采样卷积层对目标姿态(Pt)进行编码。在这种情况下，N = 2，并且据说使用了编码器的常规配置。</p><h2 id="a122" class="lm ig hi bd ih ln lo lp il lq lr ls ip jk lt lu it jm lv lw ix jo lx ly jb lz bi translated">我想知道，</h2><p id="f8af" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">不知道什么是常规配置。有没有正式形式的下采样？</p><h2 id="8888" class="lm ig hi bd ih ln lo lp il lq lr ls ip jk lt lu it jm lv lw ix jo lx ly jb lz bi translated">2–2分解组件编码(DCE)</h2><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ma"><img src="../Images/9e63ef2f1ec66ac3772890da9f5354a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rfjYB39gQQhAvifiO6gJVA.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图3。数据通信设备(Data Communications Equipment)</figcaption></figure><p id="b07e" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">在本文中，分割图可以通过<a class="ae ku" href="https://github.com/Engineering-Course/LIP_SSL" rel="noopener ugc nofollow" target="_blank"> LIP_SSL </a>提取，它有20个标签，然后合并为8个标签(即背景、头发、脸、上衣、裤子、裙子、手臂和腿)<br/>具有分量I的分解的人物图像可以生成如下。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es mb"><img src="../Images/7d9b7ae1f56a09a11306aa680ff259ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*0ivU73ZwRWiwqDott7wnBw.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图4分解的人物形象方程</figcaption></figure><p id="0037" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">其中⊙表示元素间的乘积。</p><p id="3b39" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">VGG编码器(VGG)使用COCO数据集进行预训练。注意，VGG是不可训练的。并且，VGG功能与可学习的编码器连接。</p><p id="7af7" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">通过每个模型的输出被连接起来。在连接特征之后，通过平均池提取样式代码。</p><p id="2172" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">根据本文，区分可学习编码器和固定VGG的原因是可学习编码器可以提取复杂的特征，而固定VGG可以提取全局纹理</p><p id="f965" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">作者认为DCE效应提高了生成图像的质量，模型的收敛性得到了改善。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mc"><img src="../Images/c34fdd4b81433525ce98e315e6a0cfd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BhxVPso-HZhQTM3C5xxj5g.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图5。DCE效应1</figcaption></figure><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es md"><img src="../Images/cebbf7cff17b9c5c2ed8f6782b569b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ScfnPTcx6IlD5n0W2V56Og.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图6。DCE效应2</figcaption></figure><h2 id="61e9" class="lm ig hi bd ih ln lo lp il lq lr ls ip jk lt lu it jm lv lw ix jo lx ly jb lz bi translated">我想知道，</h2><p id="9a3e" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">首先，它没有规定如何实现可学习编码器。我不知道这是VGG还是使用定制的编码器。</p><p id="07ad" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">而且，他们说串联VGG的特征，我不知道它是连接VGG的所有特征还是有选择地连接。</p><h2 id="76a9" class="lm ig hi bd ih ln lo lp il lq lr ls ip jk lt lu it jm lv lw ix jo lx ly jb lz bi translated">根据<a class="ae ku" href="https://github.com/menyifang/ADGAN/blob/4dd70649ad136829b92dd6a1a823af7594a0220f/models/model_adgen.py#L84" rel="noopener ugc nofollow" target="_blank"> github </a>..</h2><ul class=""><li id="6fa2" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq me js jt ju bi translated">定制编码器由4个卷积层组成。</li><li id="5b25" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq me js jt ju bi translated">从每个第一conv块中提取VGG特征，并连接到定制编码器</li></ul><h2 id="d481" class="lm ig hi bd ih ln lo lp il lq lr ls ip jk lt lu it jm lv lw ix jo lx ly jb lz bi translated">2–3纹理风格转移</h2><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es mf"><img src="../Images/6bdcf978929598d7f9e41b3fc893b5b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*v-eNM57nGtHFzvfDjhQU_g.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图7。风格编码-&gt;融合模块</figcaption></figure><p id="a20a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">如图6所示，在连接通过DCE提取的样式代码后，将它们放入融合模型。<br/>此时，A是通过融合模型(FM)提取的<a class="ae ku" href="https://arxiv.org/pdf/1703.06868.pdf" rel="noopener ugc nofollow" target="_blank"> AdaiN </a>层的仿射变换参数(尺度和平移σ)。</p><p id="8381" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">该论文介绍了由3个完全连接的层组成的FM，前两层允许网络通过线性重组灵活地选择期望的特征，最后一层提供所需维度的参数。</p><p id="f35c" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">这款FM的图像质量也得到了提升。(图7。)</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mg"><img src="../Images/1d42c72240f40ef99e2b608023e5c836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iv10iNb5vq3T_35b2aMp4A.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图8。融合模型的效果</figcaption></figure><h2 id="e3c7" class="lm ig hi bd ih ln lo lp il lq lr ls ip jk lt lu it jm lv lw ix jo lx ly jb lz bi translated">我想知道，</h2><p id="2c00" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">这可能是因为我没有详细看过AdaiN的论文，但我不确定FM是如何用代码实现的。<br/>最后输出的是两个参数吗？并且，<br/>我不清楚是否所有的样式代码都用于提取参数。也不清楚这是适用于所有样式块还是每个样式块。</p><h2 id="861b" class="lm ig hi bd ih ln lo lp il lq lr ls ip jk lt lu it jm lv lw ix jo lx ly jb lz bi translated">2-4人图像重建</h2><p id="05b7" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">像2–1的姿态编码器一样，使用了N个常规解码器配置，N=2。常规配置也不在此描述。</p><h1 id="cc39" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">3.结果</h1><p id="826e" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">总之……它展示了我迄今为止读过的论文中最好的结果。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mh"><img src="../Images/32e62573246d95a7c7ea34db501ccf3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vK9aAAwR46ehLDVyrCmMMQ.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图9: <br/>根据任意姿态生成的图像。</figcaption></figure><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mi"><img src="../Images/1a62f1c922245c3cb4863ea5eb9cb4b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7XUTVS3YGXA5ckyKLOZlQ.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图10。与其他论文生成的图像进行比较</figcaption></figure><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mj"><img src="../Images/afcd056b3fb8a637851c494101ff1fb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8qHEcVWeokVGmBm_i_veQ.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图11。属性转移的结果</figcaption></figure><p id="fb9b" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">如果数据包含许多偏差，也有失败的案例。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mk"><img src="../Images/864be67af301ff39ebb21ce7082ed7ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P057AhUvfdC929eRSq_IAw.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图12。失败案例</figcaption></figure><h1 id="bf73" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">我对纸的看法。</h1><p id="6c21" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">在我看来，这篇论文的长处在于不需要数据标注。在没有数据标注的自定义数据中，可以使用Openpose和LIP_SSL进行提取，所以可以说是健壮的。</p><p id="db96" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">但换句话说，我认为姿态提取和分割图的提取会极大地影响模型的性能。</p></div></div>    
</body>
</html>