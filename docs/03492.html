<html>
<head>
<title>An insight to BERT Attentions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对伯特注意的一点看法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-insight-to-bert-attentions-2e106f004dc0?source=collection_archive---------8-----------------------#2020-02-04">https://medium.com/analytics-vidhya/an-insight-to-bert-attentions-2e106f004dc0?source=collection_archive---------8-----------------------#2020-02-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1977" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当有人被要求从一本书里寻找某个问题的答案时，我们会有计划地搜索直接跳转回答，而不是阅读整本书。这可能会节省时间。机器中的类似方案有助于提高短语的定位和精确度，但需要耗费资源和时间。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es jk"><img src="../Images/34c8e15925b78b1e8e8812d2cb972603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*OSTaf7pWJuEC0xnWfzx50Q.jpeg"/></div></figure><p id="2103" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意机制现在普遍用于许多NLP任务中。注意力主要是指像人类这样的机器对数据的专注。就其对机器学习尤其是自然语言处理的贡献而言，2017年非常关键。transformer模型以不同的方式改变了放置NLP任务的方式。以前，对于像摘要和问题回答这样的NLP任务，常见的实现是基于点的注意(平面注意)。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es js"><img src="../Images/f4fed3090b4ce4150cf1e1ecd124716c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*UmnvyiWV1Eq__CMnactgIA.png"/></div></figure><p id="53f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT(双向编码器-解码器转换器)的所有12层都有多头注意力模块。我们有12个注意力中心，显示一个词对句子中其他词的注意力。这种分析不仅仅限于当前令牌，还包括特殊令牌{[CLS]，[SEP]}。</p><p id="4ba1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下图显示了注意力评分在热图中的应用。这个图是通过<a class="ae jt" href="https://github.com/cbaziotis/neat-vision" rel="noopener ugc nofollow" target="_blank"> <em class="ju">工整的视觉</em> </a> <em class="ju">映射出来的。</em>用硬颜色表示重要性较高的文本，反之亦然。</p><p id="2f41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jv translated"><span class="l jw jx jy bm jz ka kb kc kd di"> T </span> he transformer库有很多预先训练好的模型，可以从<a class="ae jt" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> <em class="ju"> huggingface </em> </a>库中找到。这些模型可用于多项任务，并可进一步针对任何下游任务进行微调。考虑到BERT复杂的结构，可视化学习到的重量是一场噩梦。幸运的是，我们有一个像Tensor2Tensor这样的工具，它是由<em class="ju"> Jesse Vig </em>到<a class="ae jt" href="https://github.com/jessevig/bertviz" rel="noopener ugc nofollow" target="_blank"> <em class="ju"> BertViz </em> </a>进一步开发的。它用不同的颜色和粗细将每个单词的注意力线从左到右可视化。研究者可以选择一个层和注意力头来观察模型。我们将进一步探索和理解下面的各种模式。</p><h1 id="5942" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated"><strong class="ak">伯特维兹:</strong></h1><pre class="jl jm jn jo fd lc ld le lf aw lg bi"><span id="e2af" class="lh kf hi ld b fi li lj l lk ll">pip install regex<br/>pip install transformers</span><span id="6e4c" class="lh kf hi ld b fi lm lj l lk ll">from bertviz.transformers_neuron_view import BertModel,BertTokenizer<br/>model_version = 'bert-base-uncased'<br/>model = BertModel.from_pretrained(model_version)<br/>tokenizer = BertTokenizer.from_pretrained(model_version)</span></pre><p id="e8f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 1。神经元视图</strong></p><p id="e0dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它用线条粗细显示了一个句子中的每个单词对其他每个单词的关注。该图描述了如何根据句子中的所有其他标记来评估注意力。这里粗线比细线代表更高的重量。在下图中可以看到进一步的提取，其中以前的查询和关键向量用于提取这些分数。</p><div class="jl jm jn jo fd ab cb"><figure class="ln jp lo lp lq lr ls paragraph-image"><img src="../Images/e43ac3c13c4895d3cd7570c67b1aa599.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*XpvALnPXENYD4S0z7CF_wg.png"/></figure><figure class="ln jp lt lp lq lr ls paragraph-image"><img src="../Images/50c658ad22014ab36ce9e438d08aecea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/1*ousy4p-aNQSR-eqpjA2HFw.gif"/></figure></div><pre class="jl jm jn jo fd lc ld le lf aw lg bi"><span id="01d1" class="lh kf hi ld b fi li lj l lk ll">import sys</span><span id="d19d" class="lh kf hi ld b fi lm lj l lk ll">!test -d bertviz_repo &amp;&amp; echo “FYI: bertviz_repo directory already exists, to pull latest version uncomment this line: !rm -r bertviz_repo”</span><span id="c505" class="lh kf hi ld b fi lm lj l lk ll">!test -d bertviz_repo || git clone <a class="ae jt" href="https://github.com/jessevig/bertviz" rel="noopener ugc nofollow" target="_blank">https://github.com/jessevig/bertviz</a> bertviz_repo</span><span id="2b18" class="lh kf hi ld b fi lm lj l lk ll">if not ‘bertviz_repo’ in sys.path:<br/>sys.path += [‘bertviz_repo’]</span><span id="e20b" class="lh kf hi ld b fi lm lj l lk ll">def call_html():<br/>import IPython<br/>display(IPython.core.display.HTML(‘’’<br/>&lt;script src=”/static/components/requirejs/require.js”&gt;&lt;/script&gt;<br/>&lt;script&gt;<br/>requirejs.config({<br/>paths: {<br/>base: ‘/static/base’,<br/>“d3”: “https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min",<br/>jquery: ‘//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min’,<br/>},<br/>});<br/>&lt;/script&gt;<br/>‘’’))</span><span id="4994" class="lh kf hi ld b fi lm lj l lk ll">from bertviz.neuron_view import show<br/>sentence="This is a testing sentence"<br/>call_html()<br/>show(model, 'bert', tokenizer, sentence)</span></pre><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lu"><img src="../Images/ac0cbeec90c4229983c78bd82f6ac283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Arf2Fe8A-jqwIIp8XVdWfQ.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">神经元可视化中的深层结构视图</figcaption></figure><p id="15e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。模型视图</strong></p><p id="63ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它为所有层和注意力头呈现一个图形。如果你仔细观察，你会发现在这个视图的最后一行中有一个和神经元视图中的图形一样的线条模式。</p><pre class="jl jm jn jo fd lc ld le lf aw lg bi"><span id="3ecf" class="lh kf hi ld b fi li lj l lk ll">from bertviz import model_view</span><span id="5f25" class="lh kf hi ld b fi lm lj l lk ll">def call_html():<br/>import IPython<br/>display(IPython.core.display.HTML(‘’’<br/>&lt;script src=”/static/components/requirejs/require.js”&gt;&lt;/script&gt;<br/>&lt;script&gt;<br/>requirejs.config({<br/>paths: {<br/>base: ‘/static/base’,<br/>“d3”: “https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min",jquery: ‘//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min’,},});&lt;/script&gt;<br/>‘’’))</span><span id="3453" class="lh kf hi ld b fi lm lj l lk ll">call_html()<br/>model_view(attentions, tokens)</span></pre><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es md"><img src="../Images/90976f2a9ac4cbf079c5da716e5f91c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uKy3Z60GvptV8cQTlzX60w.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">来自BertViz的模型视图</figcaption></figure><p id="4a03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。头部视图</strong></p><p id="008e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个有点复杂的可视化层的所有注意力头在同一时间。对于每一个令牌，可以看到我们都有对应头部的颜色。另一方面，我们有一个带有选择性标记的颜色条的可视化。</p><pre class="jl jm jn jo fd lc ld le lf aw lg bi"><span id="4692" class="lh kf hi ld b fi li lj l lk ll">input_ =tokenizer.encode_plus(sentence,add_special_tokens=True,return_tensors=’pt’)</span><span id="311a" class="lh kf hi ld b fi lm lj l lk ll">output, loss, attentions = model(input_[‘input_ids’], token_type_ids = input_[‘token_type_ids’], attention_mask = input_[‘attention_mask’])</span><span id="0650" class="lh kf hi ld b fi lm lj l lk ll">tokens = tokenizer.convert_ids_to_tokens(input_[‘input_ids’][0])<br/>call_html()<br/>head_view(attentions, tokens)</span></pre><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es me"><img src="../Images/a0905d85d81dc72ba0b439cc1950fb97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*bZyY3U61bgEeqJ1BO4OXBw.png"/></div></figure><p id="c01d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献:</strong></p><p id="6a04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有代码都可以在官方资源库<a class="ae jt" href="https://github.com/jessevig/bertviz" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div></div>    
</body>
</html>