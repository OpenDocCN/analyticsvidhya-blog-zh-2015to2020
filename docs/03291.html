<html>
<head>
<title>BERT — REST Inference from the fine-tuned model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">来自微调模型的BERT-REST推断</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bert-rest-inference-from-the-fine-tuned-model-4b1f31151f97?source=collection_archive---------10-----------------------#2020-01-24">https://medium.com/analytics-vidhya/bert-rest-inference-from-the-fine-tuned-model-4b1f31151f97?source=collection_archive---------10-----------------------#2020-01-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a06c978562501f2796d442c72898d86c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZvG-wIy7H53MFf-Y"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">由<a class="ae iu" href="https://unsplash.com/@katergaris?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Aris Sfakianakis </a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="c21d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">伯特已经一年多了。每个人都在使用Bert执行各种NLP任务。有些只是使用预训练的模型，重新训练预训练的模型进行句子嵌入，来执行句子相似性任务。其中一个句子嵌入的开源项目是由腾讯人工智能实验室的晓寒完成的，这个项目是bert-as-service。自从Bert-as-service发布以来，我已经使用它将近一年了。我真的很欣赏那个项目，我现在还在用。</p><p id="07c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">除了预先训练的模型之外，我们大多数人都希望使用微调的模型来解决NLP任务。NLP研究人员正在尝试解决几个NLP任务，如GLUE和SQuaD，GLUE官方网站中的排行榜经常以更好的准确性弹出。可能我们大多数人都使用过BERT的开源代码来达到某种目的。但是，为了预训练和微调的目的，需要做一些修改来构建您自己的数据预处理器。</p><p id="2b7a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个例子中，为了简单起见，我将使用二元分类，但是这同样适用于多类分类。我们将使用来自谷歌研究的BERT GitHub回购协议。此外，我将添加一些功能，用于将经过训练的检查点模型和元图导出到proto buffer (Pb)文件格式。</p><p id="9f2f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本节中，我将跳过微调模型的培训，因为大部分内容已经在BERT repo中陈述过了。现在，让我们沿着“run _ cal class ier . py”文件添加一个将在服务期间使用的服务函数。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="6c73" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">def </strong>serving_input_fn():<br/>    label_ids = tf.placeholder(tf.int32, [<strong class="jy hj">None</strong>], name=<strong class="jy hj">'label_ids'</strong>)<br/>    input_ids = tf.placeholder(tf.int32, [<strong class="jy hj">None</strong>, FLAGS.max_seq_length], name=<strong class="jy hj">'input_ids'</strong>)<br/>    input_mask = tf.placeholder(tf.int32, [<strong class="jy hj">None</strong>, FLAGS.max_seq_length], name=<strong class="jy hj">'input_mask'</strong>)<br/>    segment_ids = tf.placeholder(tf.int32, [<strong class="jy hj">None</strong>, FLAGS.max_seq_length], name=<strong class="jy hj">'segment_ids'</strong>)<br/>    input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn({<br/>        <strong class="jy hj">'label_ids'</strong>: label_ids,<br/>        <strong class="jy hj">'input_ids'</strong>: input_ids,<br/>        <strong class="jy hj">'input_mask'</strong>: input_mask,<br/>        <strong class="jy hj">'segment_ids'</strong>: segment_ids,<br/>    })()<br/>    <strong class="jy hj">return </strong>input_fn</span></pre><p id="2d0b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在主函数中，我们会添加几行用于导出模型的代码，类似于<code class="du ki kj kk jy b">do_train</code>、<code class="du ki kj kk jy b">do_predict</code>和<code class="du ki kj kk jy b">do_eval</code>。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="034e" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">if </strong>FLAGS.do_export:<br/>    estimator._export_to_tpu = <strong class="jy hj">False<br/>    </strong>estimator.export_savedmodel(FLAGS.export_dir, serving_input_fn)</span></pre><p id="e29b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们有了这个，我们就可以将检查点模型导出到pb模型。类似于训练模型，我们将使用相同的run_classifier.py命令来启用<code class="du ki kj kk jy b">do_export=True</code>，并给出一个导出目录为<code class="du ki kj kk jy b">export_dir</code>的路径，这将产生pb模型。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/59cc035891ae74f515c672a09c3178e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*vjuXVTE6RCFbWXIJBPQHFA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图:导出的模型文件</figcaption></figure><p id="2573" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们想要确认导出模式的签名，以查看到我们先前定义的模型的输入和输出映射。我们将使用<code class="du ki kj kk jy b">saved_model_cli</code>来查看签名定义。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="1dfb" class="kc kd hi jy b fi ke kf l kg kh">$ saved_model_cli show --dir /home/dsdev/gitrepo/bert_serving/exported/1579422100/ --all<br/>......<br/>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:</span><span id="ecff" class="kc kd hi jy b fi km kf l kg kh">signature_def['serving_default']:<br/>  The given SavedModel SignatureDef contains the following input(s):<br/>    inputs['input_ids'] tensor_info:<br/>        dtype: DT_INT32<br/>        shape: (-1, 256)<br/>        name: input_ids_1:0<br/>    inputs['input_mask'] tensor_info:<br/>        dtype: DT_INT32<br/>        shape: (-1, 256)<br/>        name: input_mask_1:0<br/>    inputs['label_ids'] tensor_info:<br/>        dtype: DT_INT32<br/>        shape: (-1)<br/>        name: label_ids_1:0<br/>    inputs['segment_ids'] tensor_info:<br/>        dtype: DT_INT32<br/>        shape: (-1, 256)<br/>        name: segment_ids_1:0<br/>  The given SavedModel SignatureDef contains the following output(s):<br/>    outputs['output'] tensor_info:<br/>        dtype: DT_FLOAT<br/>        shape: (-1, 2)<br/>        name: loss/Softmax:0<br/>  Method name is: tensorflow/serving/predict</span></pre><p id="f905" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，一切看起来都很好，我们将使用tensorflow-serving来服务模型。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="dd58" class="kc kd hi jy b fi ke kf l kg kh">$ tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=myclassify --model_base_path=/home/path/to/bert/exported/<br/>.................<br/>2020-01-24 06:35:39.441288: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: myclassify model_base_path: /home/path/to/bert/exported/<br/>2020-01-24 06:35:39.441484: I tensorflow_serving/model_servers/server_core.cc:461] Adding/updating models.<br/>2020-01-24 06:35:39.441524: I tensorflow_serving/model_servers/server_core.cc:558]  (Re-)adding model: myclassify<br/>2020-01-24 06:35:39.541891: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: myclassify version: 1579422100}<br/>2020-01-24 06:35:39.541938: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: myclassify version: 1579422100}<br/>2020-01-24 06:35:39.541960: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: myclassify version: 1579422100}<br/>2020-01-24 06:35:39.541988: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /home/path/to/bert/exported/1579422100<br/>2020-01-24 06:35:39.542006: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /home/path/to/bert/exported/1579422100<br/>2020-01-24 06:35:39.558353: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }<br/>2020-01-24 06:35:39.576158: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA<br/>2020-01-24 06:35:39.633978: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:162] Restoring SavedModel bundle.<br/>2020-01-24 06:35:40.033231: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:138] Running MainOp with key saved_model_main_op on SavedModel bundle.<br/>2020-01-24 06:35:40.062124: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:259] SavedModel load for tags { serve }; Status: success. Took 520097 microseconds.<br/>2020-01-24 06:35:40.062210: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:83] No warmup data file found at /home/path/to/bert/exported/1579422100/assets.extra/tf_serving_warmup_requests<br/>2020-01-24 06:35:40.062324: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: myclassify version: 1579422100}<br/>2020-01-24 06:35:40.064403: I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ...<br/>[evhttp_server.cc : 237] RAW: Entering the event loop ...<br/>2020-01-24 06:35:40.065865: I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...</span></pre><p id="31c2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型被提供了，现在我们需要编写一个客户端来从提供的模型进行推理，客户端应该接受字符串作为输入。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="2aa0" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import </strong>json<br/><strong class="jy hj">import </strong>os<br/><strong class="jy hj">import </strong>requests<br/><strong class="jy hj">import </strong>tokenization</span><span id="df83" class="kc kd hi jy b fi km kf l kg kh">endpoints = <strong class="jy hj">"http://localhost:8501/v1/models/myclassify:predict"<br/></strong>headers = {<strong class="jy hj">"content-type"</strong>:<strong class="jy hj">"application-json"</strong>}</span><span id="5e31" class="kc kd hi jy b fi km kf l kg kh">example = <strong class="jy hj">"This is the input string"</strong></span><span id="8baf" class="kc kd hi jy b fi km kf l kg kh">tokenizer = tokenization.FullTokenizer(vocab_file=<strong class="jy hj">"/home/path/to/bert/vocabfile/vocab.txt"</strong>,do_lower_case=<strong class="jy hj">True</strong>)<br/>token_a = tokenizer.tokenize(example)</span><span id="924a" class="kc kd hi jy b fi km kf l kg kh">tokens = []<br/>segments_ids = []<br/>tokens.append(<strong class="jy hj">"[CLS]"</strong>)<br/>segment_ids = []<br/>segment_ids.append(0)<br/><strong class="jy hj">for </strong>token <strong class="jy hj">in </strong>token_a:<br/>    tokens.append(token)<br/>    segment_ids.append(0)</span><span id="a653" class="kc kd hi jy b fi km kf l kg kh">tokens.append(<strong class="jy hj">'[SEP]'</strong>)<br/>segment_ids.append(0)</span><span id="76fb" class="kc kd hi jy b fi km kf l kg kh">input_ids = tokenizer.convert_tokens_to_ids(tokens)<br/>input_mask = [1] * len(input_ids)<br/>max_seq_length = 256<br/><strong class="jy hj">while </strong>len(input_ids) &lt; max_seq_length:<br/>    input_ids.append(0)<br/>    input_mask.append(0)<br/>    segment_ids.append(0)<br/></span><span id="9c0e" class="kc kd hi jy b fi km kf l kg kh">label_id = 0</span><span id="4edc" class="kc kd hi jy b fi km kf l kg kh">instances = [{<strong class="jy hj">"input_ids"</strong>:input_ids, <strong class="jy hj">"input_mask"</strong>:input_mask, <strong class="jy hj">"segment_ids"</strong>:segment_ids, <strong class="jy hj">"label_ids"</strong>:label_id}]</span><span id="aa88" class="kc kd hi jy b fi km kf l kg kh">data = json.dumps({<strong class="jy hj">"signature_name"</strong>:<strong class="jy hj">"serving_default"</strong>, <strong class="jy hj">"instances"</strong>:instances})</span><span id="927e" class="kc kd hi jy b fi km kf l kg kh">response = requests.post(endpoints, data=data, headers=headers)<br/>prediction = json.loads(response.text)[<strong class="jy hj">'predictions'</strong>]<br/>print(prediction)</span><span id="f247" class="kc kd hi jy b fi km kf l kg kh">&gt;&gt; [[0.999996, 4.28649e-06]]  //Result</span></pre></div></div>    
</body>
</html>