<html>
<head>
<title>Generalised Method For Initializing Weights in CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CNN中初始化权重的一般化方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/generalised-method-for-initializing-weights-in-cnn-24b183db51e3?source=collection_archive---------23-----------------------#2020-09-10">https://medium.com/analytics-vidhya/generalised-method-for-initializing-weights-in-cnn-24b183db51e3?source=collection_archive---------23-----------------------#2020-09-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0176" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用正确的值初始化参数是从神经网络获得精确结果的最重要的条件之一。</p><h2 id="ba9b" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">所有值为零的权重初始化</strong></h2><p id="03cc" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">如果所有的权重都用<strong class="ih hj">零</strong>初始化，那么对于<strong class="ih hj"> W[l]中的每个W，关于损失函数的导数是相同的，其中W[l]是神经网络层l</strong>中的权重，因此所有的权重在后续迭代中具有相同的值。这使得隐藏单元对称并持续所有n次迭代，即设置权重为<strong class="ih hj">零</strong>并不会使其比线性模型更好，因此我们不应该用零初始化它。</p><h2 id="d6b5" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">用过大或过小的值初始化权重(随机权重)</strong></h2><p id="c274" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">如果权重太大，那么我们有一个问题叫做<strong class="ih hj">爆炸梯度</strong>问题，它导致偏离最小损失。</p><p id="e0ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果权重太小，那么我们有一个问题叫做<strong class="ih hj">消失梯度</strong>问题，它导致在达到最小值之前收敛。</p><p id="6d64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如需深入了解，请参考以下内容:</p><div class="kd ke ez fb kf kg"><a rel="noopener follow" target="_blank" href="/hackernoon/exploding-and-vanishing-gradient-problem-math-behind-the-truth-6bd008df6e25"><div class="kh ab dw"><div class="ki ab kj cl cj kk"><h2 class="bd hj fi z dy kl ea eb km ed ef hh bi translated">爆炸和消失的梯度问题:真相背后的数学</h2><div class="kn l"><h3 class="bd b fi z dy kl ea eb km ed ef dx translated">你好星尘！今天我们将看到爆炸和消失梯度问题背后的数学原因，但首先让我们…</h3></div><div class="ko l"><p class="bd b fp z dy kl ea eb km ed ef dx translated">medium.com</p></div></div><div class="kp l"><div class="kq l kr ks kt kp ku kv kg"/></div></div></a></div><h2 id="fb0e" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">如何初始化重量</strong></h2><p id="faa9" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">必须对重量进行初始化，以便:</p><ul class=""><li id="6360" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">每层激活的<strong class="ih hj">平均值</strong>接近于<strong class="ih hj">零</strong>。</li><li id="f96d" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">每层激活的<strong class="ih hj">方差</strong>接近<strong class="ih hj">1</strong>。</li></ul><p id="fbce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一种方法是我们在神经网络的每一层附加一个<strong class="ih hj"> Pytorch钩子</strong>。<strong class="ih hj"> Pytorch hook </strong>基本上就是一个函数，有非常具体的签名。当我们说一个钩子被执行时，实际上，我们说的是这个函数被执行。挂钩将层的输入和层的输出作为参数，我们可以获取这些激活的平均值和标准偏差，用于调试模型，并查看为什么模型不能正确工作。</p><pre class="lk ll lm ln fd lo lp lq lr aw ls bi"><span id="3b43" class="jd je hi lp b fi lt lu l lv lw"><strong class="lp hj">class Hook():</strong><br/><strong class="lp hj">def __init__(self, m, f):</strong> <br/>self.hook =m.register_forward_hook(partial(f, self))</span><span id="a24f" class="jd je hi lp b fi lx lu l lv lw"><strong class="lp hj">def remove(self): </strong><br/>self.hook.remove()</span><span id="deeb" class="jd je hi lp b fi lx lu l lv lw"><strong class="lp hj">def __del__(self):</strong> <br/>self.remove()</span><span id="214c" class="jd je hi lp b fi lx lu l lv lw"><strong class="lp hj">def append_stat(hook, mod, inp, outp):<br/></strong>d = outp.data<br/>hook.mean,hook.std = d.mean().item(),d.std().item()</span></pre><p id="ce28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码片段创建了一个钩子，它是一个将被附加到层<strong class="ih hj"> m </strong>的函数，该函数作为初始化参数被传递。此处的函数计算该层激活的平均值和标准差。</p><pre class="lk ll lm ln fd lo lp lq lr aw ls bi"><span id="a5ef" class="jd je hi lp b fi lt lu l lv lw"><strong class="lp hj"><br/>def </strong>children(m):<strong class="lp hj"> </strong><br/>    <strong class="lp hj">return</strong> list(m.children())</span><span id="b7dd" class="jd je hi lp b fi lx lu l lv lw">hooks = [Hook(l, append_stat) for l in children(learn.model)]<br/></span></pre><p id="55ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码片段将钩子连接到神经网络的每一层。</p><p id="5d02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，当我们在神经网络上进行正向传递时，计算每层激活的所有平均值和标准偏差。</p><pre class="lk ll lm ln fd lo lp lq lr aw ls bi"><span id="7d73" class="jd je hi lp b fi lt lu l lv lw"><strong class="lp hj">def </strong>lsuv_module(m, xb):<br/>    h = Hook(m, append_stat)</span><span id="6b27" class="jd je hi lp b fi lx lu l lv lw"><strong class="lp hj">    while</strong> learn.model(xb) is not None and abs(h.mean)  &gt; 1e-3: <br/>        m.bias = nn.Parameter(m.bias-h.mean)</span><span id="4af1" class="jd je hi lp b fi lx lu l lv lw"><strong class="lp hj">    while</strong> learn.model(xb) is not None and abs(h.std-1) &gt; 1e-3:                m.weight.data = nn.Parameter(m.weight.data/h.std)</span><span id="d380" class="jd je hi lp b fi lx lu l lv lw"><strong class="lp hj">    learn.model(xb)</strong><br/>    h.remove()<br/>    <strong class="lp hj">return</strong> "mean :"+str(h.mean),"std :"+str(h.std)</span><span id="709b" class="jd je hi lp b fi lx lu l lv lw"><strong class="lp hj">for m in models:</strong> <br/>    print(lsuv_module(m, torch.tensor(xb)))</span><span id="3f6e" class="jd je hi lp b fi lx lu l lv lw"><strong class="lp hj">#xb is batch of input data</strong></span></pre><p id="738c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码片段一个接一个地遍历模型的所有层。</p><p id="ea0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于模型中的每一层l:</p><ul class=""><li id="a486" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">计算所有激活的平均值和标准偏差。</li><li id="cba5" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">如果该层的所有激活的平均值大于abs(1e-3 ),那么我们从该层中所有神经元的偏差中减去该平均值，使得在该步骤之后，新的平均值下降到接近于<strong class="ih hj">零</strong>。</li><li id="02f6" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">类似地，如果我们将该层的所有权重除以该层激活的标准偏差，新的标准偏差将接近于<strong class="ih hj">1</strong>。</li></ul><h2 id="d49d" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">结果</h2><figure class="lk ll lm ln fd ly er es paragraph-image"><div class="ab fe cl lz"><img src="../Images/aa9e714e567bb82f1208abc3fd040d8a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*O0rw8BZ8t5s-OJnXLwl_0A.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated">使用明凯统一初始化的第4层网络的均值和标准差</figcaption></figure><p id="4785" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些是在使用<strong class="ih hj">明凯统一初始化</strong>后得到的结果，随着网络的深入，这些结果往往会表现得更差。这里的均值和标准差分别远离<strong class="ih hj">零点</strong>和<strong class="ih hj">一个</strong>。因此，它会导致不太准确的结果。</p><p id="1974" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用一般化方法后获得的结果如下:</p><figure class="lk ll lm ln fd ly er es paragraph-image"><div class="er es mf"><img src="../Images/74764921a52f583f63b85417a97d94bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*xs7gUp2BcQusmGkqD1NHcQ.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated">使用通用初始化的第4层网络的均值和标准差</figcaption></figure><p id="bcac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，每一层的均值和标准差分别接近于<strong class="ih hj">零</strong>和<strong class="ih hj">一</strong>，因此我们可以防止消失和爆炸梯度问题。因此，这种初始化对神经网络更有效。</p></div></div>    
</body>
</html>