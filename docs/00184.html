<html>
<head>
<title>Improving Neural Networks — Hyperparameter Tuning, Regularization, and More (deeplearning.ai Course #2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">改进神经网络——超参数调整、正则化等(deeplearning.ai课程#2)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/improving-neural-networks-hyperparameter-tuning-regularization-and-more-deeplearning-ai-7621fca60c1?source=collection_archive---------0-----------------------#2018-11-12">https://medium.com/analytics-vidhya/improving-neural-networks-hyperparameter-tuning-regularization-and-more-deeplearning-ai-7621fca60c1?source=collection_archive---------0-----------------------#2018-11-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c7a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">建立第一个模型——这不正是我们在深度学习领域努力的方向吗？当我们看到我们的模型成功运行时，那种兴奋的感觉是无与伦比的。但是责任并不止于此。</p><p id="9000" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如何才能提高模型的准确性？有什么方法可以加快训练进度？无论你是在黑客马拉松环境中还是在做客户项目，这些都是需要问的关键问题。当你建立了一个深度神经网络时，这些方面会变得更加突出。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/14e0097ccf1b3166cf6f1b974d2eaf66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*sZWFepzfT4y0Cjsq.jpg"/></div></figure><p id="670b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像超参数调整、正则化、批量归一化等功能。在这个过程中脱颖而出。这是由伟大的<a class="jl jm ge" href="https://medium.com/u/592ce2a67248?source=post_page-----7621fca60c1--------------------------------" rel="noopener" target="_blank">吴恩达</a>教授的deeplearning.ai课程(深度学习专业化)的第二部分。我们在第1部分中看到了神经网络的<a class="ae jn" href="https://www.analyticsvidhya.com/blog/2018/10/introduction-neural-networks-deep-learning/" rel="noopener ugc nofollow" target="_blank">基础知识以及如何实现它们，如果你需要快速复习，我建议你浏览一下。</a></p><p id="6277" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将探索这些神经网络的内部工作方式，包括研究如何提高它们的性能并减少总的训练时间。这些技术已经帮助数据科学家登上了机器学习竞赛排行榜(以及其他)，并赢得了最高荣誉。是的，这些概念是无价的！</p><h1 id="23b2" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">目录</h1><ol class=""><li id="4a66" class="km kn hi ih b ii ko im kp iq kq iu kr iy ks jc kt ku kv kw bi translated">课程结构</li><li id="08af" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">课程2:改进深度神经网络:超参数调整，正则化和优化<br/> 1。模块1:深度学习的实践方面<br/> a .设置你的机器学习应用<br/> b .正则化你的神经网络<br/> c .设置你的优化问题</li><li id="4130" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">模块2:优化算法</li><li id="b268" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">模块3:超参数调优、批处理规范化和编程框架<br/> a .超参数调优<br/> b .批处理规范化<br/> c .多类分类<br/> d .编程框架介绍</li></ol><h1 id="de8d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">1.课程结构</h1><p id="510e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">本系列的第1部分涵盖了浅层和深层神经网络如何工作、如何在单个和多个训练示例上实现前向和后向传播等概念。现在的问题是，如何调整这些神经网络，以便从中提取最大的准确性。</p><p id="a7e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将在本文中看到的课程2包括三个模块:</p><ol class=""><li id="3cf3" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">在模块1中，我们将涉及<strong class="ih hj">深度学习的实践方面</strong>。我们将看到如何从给定的数据中分离出训练集、验证集和测试集。我们还将涉及正规化、辍学、正常化等主题。这有助于我们提高模型的效率。</li><li id="8dd9" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">在模块2中，我们将讨论小批量梯度下降的概念以及更多优化器，如Momentum、RMSprop和ADAM。</li><li id="3104" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">在最后一个模块中，我们将了解如何调整不同的超参数来提高模型的效率。我们还将介绍批处理规范化的概念，以及如何解决多类分类问题。</li></ol><h1 id="a6ad" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">课程2:改进深度神经网络:超参数调整、正则化和优化</h1><p id="4a37" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">既然我们已经知道了这篇综合文章将要涵盖的内容，那就让我们开始吧！</p><h1 id="b4bf" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">模块1:深度学习的实践方面</h1><p id="20ee" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">以下几点总结了我们对本模块的期望:</p><ul class=""><li id="92c8" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">回想不同类型的初始化会导致不同的结果</li><li id="bb37" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">认识到复杂神经网络中初始化的重要性</li><li id="fe49" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">认识训练/验证/测试集之间的差异</li><li id="8dae" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">诊断模型中的偏差和方差问题</li><li id="a394" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">学习何时以及如何使用正则化方法，如辍学或L2正则化</li><li id="09c8" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">理解深度学习中的实验问题，例如消失或爆炸梯度以及如何处理它们</li><li id="4646" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">使用梯度检验来验证反向传播实现的正确性</li></ul><p id="5ffa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本模块相当全面，因此进一步分为三个部分:</p><ul class=""><li id="74c1" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">第1部分:设置机器学习应用程序</li><li id="5de2" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">第二部分:调整你的神经网络</li><li id="a273" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">第三部分:设置您的优化问题</li></ul><p id="abec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们详细了解每一部分。</p><h1 id="0bb2" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">第1部分:设置机器学习应用程序</h1><h1 id="58b3" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">培训/开发/测试设备</h1><p id="e225" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在训练深度神经网络时，我们需要对以下超参数做出许多决定:</p><ol class=""><li id="73dc" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">网络中的隐藏层数</li><li id="147a" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">每个隐藏层的隐藏单元数</li><li id="4894" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">学习率</li><li id="0fa0" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">不同层的激活功能等。</li></ol><p id="5258" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">没有指定或预定义的方法来选择这些超参数。以下是我们通常遵循的内容:</p><ol class=""><li id="3306" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">以一个<strong class="ih hj">想法</strong>开始，即以一定的隐层数、一定的学习率等开始。</li><li id="eeb4" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">试试这个想法通过<strong class="ih hj">编码</strong>吧</li><li id="8078" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated"><strong class="ih hj">实验</strong>这个主意效果如何</li><li id="cb64" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">提炼这个想法并重复这个过程</li></ol><p id="cfb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们如何确定这个想法是否可行？这就是培训/开发/测试集发挥作用的地方。假设我们有整个数据集:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lj"><img src="../Images/5901882208d474563b58170be1255d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/0*dlfdr0ve44ZuWhQo.png"/></div></figure><p id="9ded" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以将该数据集分为三个不同的集合，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lk"><img src="../Images/dce4a0138cff584a5f62683235d6b885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/0*vDLnwPyH0YBOVnsG.png"/></div></div></figure><ol class=""><li id="4c57" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated"><strong class="ih hj">训练集:</strong>我们在训练数据上训练模型。</li><li id="9d3a" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated"><strong class="ih hj">开发集:</strong>在训练模型之后，我们检查它在开发集上的表现如何。</li><li id="65db" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated"><strong class="ih hj">测试集:</strong>当我们有了最终模型(即，在训练集和开发集上都表现良好的模型)时，我们在测试集上对其进行评估，以便获得我们的算法表现如何的无偏估计。</li></ol><p id="6d0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这之后还有一个问题— <strong class="ih hj">这些培训、开发和测试集的长度应该是多少？</strong>这实际上是任何机器学习项目的一个非常关键的方面，最终将在决定模型表现如何方面发挥重要作用。让我们来看看专家们在决定每组长度时遵循的一些传统准则:</p><ul class=""><li id="5d09" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">在以前的时代，当我们有小数据集时，不同数据集的分布是:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/1cc61145f3f9790e07a492805443269a.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/0*ubycHXYqYEfuRWgH.png"/></div></figure><p id="af52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">或者只是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lq"><img src="../Images/a34c36784416f261b7f0843f283faf2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/0*tFOxIPTF0tgbNayv.png"/></div></div></figure><ul class=""><li id="3af5" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">随着近年来数据可用性的增加，我们可以使用大量数据来训练模型:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/0631521334e875bb05afcc545bd05bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/0*-xWbTSJNmQCO1sPY.png"/></div></figure><p id="1da3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这当然是决定这些不同组的长度的一种方式。这在大多数情况下工作正常，但是请允许我考虑以下场景:</p><p id="03dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ls">假设我们从不同的网站上搜集了多张猫的图片，并用自己的相机点击了几张。这两种类型图像的分布是不同的，对吗？现在，我们以这样一种方式分割数据，即训练集包含所有抓取的图像，而开发和测试集包含所有相机图像。在这种情况下，训练集的分布将不同于开发和测试集，因此，我们很有可能得不到好的结果。</em></p><p id="c367" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下(不同的发行版)，我们可以遵循以下准则:</p><ol class=""><li id="0969" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">划分训练集、开发集和测试集，使它们的分布相似</li><li id="23f6" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">跳过测试集，只使用开发集来验证模型</li></ol><p id="8a92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以使用这些集合来查看模型的偏差和方差，这有助于我们确定模型的拟合和执行情况。</p><h1 id="5c77" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">偏差/方差</h1><p id="d4a0" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">考虑一个数据集，它给出了下图:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/4f8bcb2f1e79ba85daca36e6900ccd75.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/0*6p9y9zCh-bsaiKU2.png"/></div></figure><p id="1f3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们用一条直线把这些点分成不同的类，会发生什么？该模型将欠拟合并且具有高偏差。另一方面，如果我们完美地拟合数据，即，将所有点分类到它们各自的类中，我们将具有高方差(和过度拟合)。合适的模型通常位于这两个极端之间:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lu"><img src="../Images/e3e8d5091cb1dfca87887d0f6822a675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*A-6EXKR5LFuolWM5.png"/></div></div></figure><p id="6981" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们希望我们的模型恰到好处，这意味着具有低偏差和低方差。我们可以通过检查训练集和偏差集误差来决定模型应该具有高偏差还是高方差。一般来说，我们可以将其定义为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lv"><img src="../Images/4d2fc3d562c47085ca5b0e263a6882a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/0*FXVQbEP6Bdc2uOQQ.png"/></div></div></figure><ul class=""><li id="1ad2" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">如果开发集误差比训练集误差大得多，则模型是过度拟合的，并且具有高方差</li><li id="5214" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">当训练集和开发集误差都很高时，模型拟合不足，偏差很大</li><li id="d299" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">如果训练集误差很高，而偏差集误差甚至更差，则模型具有高偏差和高方差</li><li id="8976" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">当训练集和开发集误差都很小时，模型合理地拟合数据，具有低偏差和低方差</li></ul><h1 id="bcaa" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">机器学习的基本方法</h1><p id="a7c5" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">我有一个非常简单的方法来处理我在机器学习中面临的某些问题。问一系列问题，然后逐一找出答案。事实证明，它在我的旅程中对我非常有帮助，并且通常会提高模型的性能。这些问题列举如下:</p><p id="fb35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">问题1:模型是否有较高的偏倚？</strong></p><p id="09d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解决方案</strong>:我们可以通过查看训练集误差来判断模型是否存在高偏差。高训练误差导致高偏差。在这种情况下，我们可以<strong class="ih hj">尝试更大的网络</strong>、<strong class="ih hj">训练更长时间的模型</strong>，或者<strong class="ih hj">尝试不同的神经网络架构</strong>。</p><p id="e8d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">问题2:模型方差高吗？</strong></p><p id="aa67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解:</strong>如果dev集误差高，我们可以说模型方差高。为了减少方差，我们可以<strong class="ih hj">获得更多数据</strong>，<strong class="ih hj">使用正则化，</strong>或<strong class="ih hj">尝试不同的神经网络架构</strong>。</p><p id="54e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">减少方差最流行的技术之一叫做正则化。让我们在第二部分看看这个概念以及它如何应用于神经网络。</p><h1 id="666b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">第二部分:调整你的神经网络</h1><p id="c3b0" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">我们可以通过增加数据量来减少方差。但那真的是每次都可行的选择吗？也许没有其他可用的数据，即使有，对您的项目来说也太贵了。这是一个相当普遍的问题。这就是为什么正则化的概念在防止过度拟合中起着重要的作用。</p><h1 id="5e8d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">正规化</h1><p id="ac76" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">让我们以逻辑回归为例。我们试图最小化损失函数:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/7e0ec148285f2942d2e1e308e3d40c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/0*coeztQMB9fuNs9HQ.png"/></div></figure><p id="62aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，如果我们将正则化添加到这个成本函数中，它将看起来像:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/8d6928622e65d5139d21216a0a753832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/0*9xUls00uGn1p7-kt.png"/></div></figure><p id="de02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是所谓的<strong class="ih hj"> L2正规化</strong>。ƛ是我们在训练模型时可以调整的正则化参数。现在，让我们看看如何为神经网络使用正则化。神经网络的成本函数可以写成:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/b29131e528d91e50df2890512eb8ffb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/0*0VYtl8rkL9mzgTxc.png"/></div></figure><p id="70e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以在这个成本函数中添加一个正则项(就像我们在逻辑回归方程中所做的一样):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/f3b9fbec877c7402b14a21d36e76dbd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/0*GJ3AjWm0SLt_Rnfk.png"/></div></figure><p id="6a50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，让我们看看正则化是如何为梯度下降算法工作的:</p><ul class=""><li id="c065" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">没有正则化更新方程由下式给出:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ma"><img src="../Images/bf2dd9f424bd69ceb4f5605b814f13d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/0*ftVJE6Oef6jJXBR4.png"/></div></figure><ul class=""><li id="acc8" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">这些更新方程的正则化形式将是:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/970d1fa4a0a2ec76d978d72d55165e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/0*3UBx_BWXcugjCJvP.png"/></div></figure><p id="7c6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您可以从上面的等式中推测的那样，在正则化的情况下，权重的减少会更多(因为我们从权重中添加了更高的量)。这就是L2正则化也被称为权重衰减的原因。</p><p id="7a32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这一点上，你一定想知道正则化是如何防止模型过度拟合的？下一节我们来试着理解一下。</p><h1 id="15c8" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">正则化如何减少过拟合？</h1><p id="0124" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">过度拟合发生的主要原因是因为模型学习了数据中存在的即使是最微小的细节。因此，在学习了它可以找到的所有可能的模式之后，该模型往往在训练集上表现得非常好，但在开发和测试集上却无法产生好的结果。当面对以前看不见的数据时，它就崩溃了。</p><p id="a6ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">防止过度拟合的一种方法是降低模型的复杂性。这正是正规化所做的！如果我们将正则化参数(ƛ)设置为较大的值，则在梯度下降更新期间权重的衰减将会更多。因此，大多数隐藏单元的权重将接近于零。</p><p id="643e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为权重可以忽略不计，所以模型不会从这些单元中学习太多。这将最终使网络更简单，从而减少过度拟合:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/0b22f1e8edc22676e377158ed9810e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*qfjSBdFxjMOyfrgm.png"/></div></figure><p id="d098" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们再通过一个例子来理解这个概念。假设我们使用<em class="ls"> tanh </em>激活函数:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/bc68c026f64dd68c8fb899d624d11993.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/0*gKN9XDtUK14WkNFb.png"/></div></figure><p id="f707" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，如果我们把ƛ设置成一个大的值，单位w[l]的重量会变小。为了计算z[l]值，我们将使用以下公式:</p><p id="f98e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">z[l]= w[l]a[l-1]+ b[l]</p><p id="9294" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，z值会更小。如果我们使用<em class="ls"> tanh </em>激活函数，z[l]的这些低值将位于原点附近:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es me"><img src="../Images/b25b8fd83610f3678ee2f22f76bf7eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/0*JuFDs-FKi9vb-hJx.png"/></div></figure><p id="57cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们只使用了<em class="ls">双曲正切</em>函数的线性区域。这将使网络中的每一层大致呈线性，即，我们将获得分隔数据的线性边界，从而防止过度拟合。</p><h1 id="84a8" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">辍学正规化</h1><p id="9273" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">还有一种技术可以用来执行正则化。假设您正在构建如下所示的神经网络:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/8af1696b0c3ebc07a1f4ab36b2e43362.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/0*vP5eoiWfwTGVcVuU.png"/></div></figure><p id="55b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个神经网络对训练数据过度拟合。假设我们给所有这些图像加上0.5的漏失。该模型将从每层中随机移除50%的单元，我们最终得到一个更简单的网络:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/d93c591c750d05823e66dd1607e01898.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/0*aYZMOVssw4p9j7w6.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mh"><img src="../Images/9903428d571e53f78e5218c627eccc96.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/0*kO1WzPKejxc9QD2N.png"/></div></figure><p id="4ce9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这已被证明是一种非常有效的正则化技术。怎么才能自己落实呢？我们去看看吧！</p><p id="3d5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将致力于这个例子，我们有三个隐藏层。现在，我们将考虑第三层，l=3。第三隐藏层的丢失向量d可以写成:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="af5f" class="mn jp hi mj b fi mo mp l mq mr">d3= np.random.rand(a3.shape[0], a3.shape[1]) &lt; keep_prob</span></pre><p id="5a19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，<em class="ls"> keep_prob </em>是保持一个单位的概率。现在，我们将计算所选设备的激活量:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="b619" class="mn jp hi mj b fi mo mp l mq mr">a3= np.multiply(a3,d3)</span></pre><p id="33fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个a3值将减少(1-keep_probs)倍。为了得到a3的期望值，我们将该值除以:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="519b" class="mn jp hi mj b fi mo mp l mq mr">a3/= keep_dims</span></pre><p id="0f6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用一个例子来理解辍学的概念:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="543a" class="mn jp hi mj b fi mo mp l mq mr">Number of units in the layer = 50 </span><span id="0b2d" class="mn jp hi mj b fi ms mp l mq mr">keep_prob = 0.8</span></pre><p id="3121" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，20%的总单位(即10)将被随机关闭。</p><p id="61e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每次训练迭代中，不同组的隐藏层被随机丢弃。请注意，退出仅在训练模型时进行(而不是在测试阶段)。这样做的原因是因为:</p><ul class=""><li id="9a3e" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">我们不希望我们的输出是随机的</li><li id="633e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">丢失给预测增加了噪声</li></ul><h1 id="3c09" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">其他正则化方法</h1><p id="f8aa" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">除了L2正则化和剔除，还有一些其他技术可以用来减少过拟合。</p><ol class=""><li id="0bb6" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated"><strong class="ih hj">数据扩充:</strong>假设我们正在建立一个图像分类模型，由于各种原因缺少必要的数据。在这种情况下，我们可以使用数据增强，即应用一些更改，如翻转图像、随机裁剪图像、随机旋转图像等。这些可以潜在地帮助我们获得更多的训练数据，从而减少过度拟合。</li><li id="989f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated"><strong class="ih hj">提前停止:</strong>为了理解这一点，考虑下面的例子:</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mt"><img src="../Images/5b2a957bf9ceb7d751f442484193a3bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Nb2n8e5CLRkeroel.png"/></div></div></figure><p id="4911" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，训练误差相对于时间不断减小。另一方面，dev set误差在几次迭代后增加之前先减小。我们可以在dev set误差开始增加时停止训练。简单地说，这就是所谓的提前停止。</p><p id="bc5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就正则化技术而言，这是一个总结！</p><h1 id="b8f0" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">第三部分:设置您的优化问题</h1><p id="9131" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在本模块中，我们将讨论可用于加快培训过程的不同技术。</p><h1 id="a34f" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">标准化输入</h1><p id="b293" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">假设我们有2个输入要素，它们的散点图如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mu"><img src="../Images/072f3800e27e5eaab9bbf48d103eb1fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*HZuiBrk519-mIH2l.png"/></div></figure><p id="dfc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我们将输入表示为向量的方式:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mv"><img src="../Images/45e3e69a0eb4aa3c277df07b663ff3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/format:webp/0*08BaCDjjY_59R1hY.png"/></div></figure><p id="8daf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将遵循以下步骤来规范化输入:</p><ol class=""><li id="3378" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">从输入要素中减去平均值:</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/26f23ccb94f2f4bced4d4b0dba105c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/0*VCH3AAGreh8FMnrh.png"/></div></figure><p id="1805" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将使散点图从上图所示变为(请注意，该图中变量的方差更大):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mx"><img src="../Images/0db0a637e1d934c954682023962234c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*0DpK3v4w5z4GE-BNt7eVxg.png"/></div></figure><p id="2ebf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.接下来，我们将方差归一化:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es my"><img src="../Images/edcb98e89f42f9459c99fef31e3e6451.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/0*OyMDFGKMJmN5tL-i.png"/></div></figure><p id="93c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们用方差来划分特征。这将使输入看起来像:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/97e9bceeff5f8147b8c1a6ccb8108aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*ZO8Fy22sSEjzi3Hi.png"/></div></figure><p id="a4db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要注意的一个关键点是，我们也使用相同的均值和方差来标准化测试集。我们应该这样做，因为我们希望在训练和测试数据上发生相同的转换。</p><p id="aed1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是<strong class="ih hj">为什么归一化数据会让算法更快呢？</strong></p><p id="e1e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在非标准化数据的情况下，特征的规模会变化，因此每个特征的学习参数会有变化。这将使成本函数不对称:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mz"><img src="../Images/14b0c46af4304c1719cc45c1faf8c594.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/0*MLDHW7eDWvTQXFR6.png"/></div></figure><p id="b9ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，在归一化数据的情况下，尺度将是相同的，并且成本函数也将是对称的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es na"><img src="../Images/6f89083664be282a1c8222cd8f37ef8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/0*9_PPnGgSXp5rOtWp.png"/></div></figure><p id="d887" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">归一化输入使得成本函数对称。这使得梯度下降算法更容易更快地找到全局最小值。这反过来使得算法运行得更快。</p><h1 id="6e39" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">消失/爆炸渐变</h1><p id="7f95" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在训练深度神经网络时，有时导数(斜率)可能变得非常大或非常小。这会使训练阶段变得相当困难。这就是消失/爆炸渐变的问题。假设我们使用具有两个输入特征的“l”层的神经网络，并且我们初始化了大的权重:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nb"><img src="../Images/1d88f4c6fc661820c6e7b40ff694b6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/0*Juu4vtuA0vk6tqSq.png"/></div></figure><p id="59af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第<em class="ls">1</em>层的最终输出将是(假设我们使用的是线性激活函数):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nc"><img src="../Images/eb5b71591b46f277d4f18b9b1c7c24a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/0*3fqhJihzM6Cg015g.png"/></div></figure><p id="21bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于更深的网络，L会很大，使得梯度非常大，学习过程很慢。同样，使用小权重会使梯度变得非常小，结果是学习会很慢。为了减少训练时间，我们必须处理这个问题。那么<strong class="ih hj">权重应该如何初始化呢？</strong></p><h1 id="8eca" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">深度网络的权重初始化</h1><p id="f44f" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">这个问题的一个潜在解决方案是随机初始化。考虑如下所示的单个神经元:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nd"><img src="../Images/09b95448df39c1442bbc8e93efb46034.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/0*6n4J2XEp6kF0wpgD.png"/></div></figure><p id="7584" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于本例，我们可以将权重初始化为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ne"><img src="../Images/75c6dd5140a26570d1f4fa0b0ffd92a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/0*W6-CRkY-tkr3SIjT.png"/></div></figure><p id="c267" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机初始化权重的主要原因是为了打破对称性。我们希望确保不同的隐藏单元学习不同的模式。还有一种技术可以帮助确保我们的实现是正确的，并且可以快速运行。</p><h1 id="7b0e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">梯度检查</h1><p id="3636" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">梯度检查用于发现反向传播实现中的缺陷(如果有的话)。考虑下图:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nf"><img src="../Images/b40c9c48dc3c3301f99cfac01539d66e.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/0*ornVSe0GXfc0eNZy.png"/></div></figure><p id="8b1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">函数w.r .对θ的导数最好表示为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ng"><img src="../Images/869b26f9126b45dedde9288d6f9ff1b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/0*NJ2HYiV3EA1cdIfI.png"/></div></figure><p id="69ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，ε是我们向θ左右移动的小步。确保上面计算的导数几乎等于函数的实际导数。以下是我们进行梯度检查的步骤:</p><ul class=""><li id="5fc0" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">取W[1]，b[1]，…。，w[L]，b[L]并将其重塑为一个大向量θ:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nh"><img src="../Images/0ca46d2412d4226c1545b8582df8cbf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/0*23CS9CDd6rmZUkQK.png"/></div></figure><ul class=""><li id="6cd9" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">我们还计算dW[1]，db[1]，…，dw[L]，db[L]并将其整形为一个大向量dθ</li><li id="1f0f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">最后，我们检查dθ是否是J(θ)的梯度</li></ul><p id="4c2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个I，我们计算:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ni"><img src="../Images/cd5babd48736b7d983d4696262562bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/0*12A66X0Aosl4BJBT.png"/></div></figure><p id="9328" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用欧几里德距离(ε)来衡量这两项是否相等:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nj"><img src="../Images/809312b26655ec2dc908815c285173b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*yDhuKvQ1gD2__k1_.png"/></div></figure><p id="d33b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们希望这个值尽可能小。因此，如果ε是10–7，我们说这是一个很好的近似值，如果ε是10–5，这是可以接受的，如果ε在10–3的范围内，我们必须改变近似值并重新计算权重。</p><p id="d0c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模块1到此结束！</p><h1 id="7f65" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">模块2:优化算法</h1><p id="d866" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">本模块的目标是:</p><ul class=""><li id="c990" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">学习不同的优化方法，如(随机)梯度下降、动量、RMSProp和Adam</li><li id="f9a5" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">使用随机小批量加速收敛和改善优化</li><li id="d0b6" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">了解学习率衰减的好处，并将其应用于优化</li></ul><h1 id="9806" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">小批量梯度下降</h1><p id="dc5c" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">我们在课程1中看到了矢量化如何帮助我们有效地处理“m”训练示例。我们可以去掉显式for循环，使训练阶段更快。因此，我们将训练示例视为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es nk"><img src="../Images/d81fd17c6354fd57af41c00bff6a9ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5jSYH33iW3x9Z3sr.png"/></div></div></figure><p id="ecc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中X是矢量化输入，Y是相应的输出。但是如果我们有一个大的训练集，比如说m = 5，000，000，会发生什么呢？如果我们在每个训练迭代中处理所有这些训练示例，梯度下降更新将花费大量时间。相反，我们可以使用小批量的训练样本，并基于它们更新权重。</p><p id="3b4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们做一个小批量，每个包含1000个样品。这意味着我们有5000个批次，训练集将如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es nl"><img src="../Images/00afef0e801657d46064991b848cc440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zDYmvlZhMsUDMAsI.png"/></div></div></figure><p id="2c56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，X{t}，Y{t}表示第<em class="ls">个第</em>个小批量输入和输出。现在，让我们看看如何实现小批量梯度下降:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="314b" class="mn jp hi mj b fi mo mp l mq mr">for t = 1:No_of_batches                 # this is called an epoch<br/>    A[L], Z[L] = forward_prop(X{t}, Y{t})<br/>    cost = compute_cost(A[L], Y{t})<br/>    grads = backward_prop(A[L], caches)<br/>    update_parameters(grads)</span></pre><p id="4f45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这相当于1个时期(1个时期=单次通过训练集)。注意，小批量的成本函数如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nm"><img src="../Images/bcd10b9d1857f1ad429d1918d0695277.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/0*CDOnxAugR4euApqx.png"/></div></figure><p id="2b7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中1000是我们在上面的例子中看到的小批量的数量。让我们更深入地了解小批量梯度下降的细节。</p><h1 id="7ebf" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">了解小批量梯度下降</h1><p id="8a1c" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在批量梯度下降中，我们的成本函数应该在每次迭代中减少:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nn"><img src="../Images/a8b4701c30f73fe1c5d5449be82991e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/0*BAV7BeeD6OMZdfkc.png"/></div></figure><p id="8514" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在小批量梯度下降的情况下，我们只使用一组指定的训练样本。结果，对于一些迭代，成本函数可以降低:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es no"><img src="../Images/b6994d025fb34c15d3f00d52b89fa5fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/0*-DBLfQKdBJTLgcsR.png"/></div></figure><p id="9018" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们如何选择小批量？</strong>让我们看看各种情况:</p><ol class=""><li id="2c54" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">如果mini-batch size = m: <br/>这是一个批次梯度下降，其中所有的训练样本都在每次迭代中使用。每次迭代花费太多时间。</li><li id="46f4" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">如果小批量= 1: <br/>称为随机梯度下降，其中每个训练样本都是自己的小批量。因为在每一次迭代中，我们只取一个例子，它会变得非常嘈杂，需要更多的时间来达到全局最小值。</li><li id="262f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">如果小批量大小在1到m: <br/>之间，则为小批量梯度下降。小批量的大小不应该太大或太小。</li></ol><p id="c9d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是在决定小批量时要记住的一些一般准则:</p><ol class=""><li id="5e12" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">如果训练集很小，我们可以选择m&lt;2000</li><li id="73f1" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">For a larger training set, typical mini-batch sizes are: 64, 128, 256, 512</li><li id="962d" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">Make sure that the mini-batch size fits your CPU/GPU memory</li></ol><h1 id="9045" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Exponentially weighted averages</h1><p id="ade1" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">Below is a sample of hypothetical temperature data collected for an entire year:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="de0e" class="mn jp hi mj b fi mo mp l mq mr">Θ1 = 40 F<br/>Θ2 = 49 F<br/>Θ3 = 45 F<br/>.<br/>.<br/>Θ180 = 60 F<br/>Θ181 = 56 F<br/>.<br/>.</span></pre><p id="f934" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">The below plot neatly summarizes this temperature data for us:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es np"><img src="../Images/e057315ca1893030311bbed7a6b0859f.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/0*_wPjg9u8eeYGNSLc.png"/></div></figure><p id="befc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Exponentially weighted average, or exponentially weighted moving average, computes the trends. We will first initialize a term as 0:</p><p id="c8fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V0 = 0</p><p id="ffc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Now, all the further terms will be calculated as the weighted sum of V0 and the temperature of that day:</p><p id="f90a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V1 = 0.9 * V0 + 0.1 * Θ1</p><p id="8b2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V2 = 0.9 * V1 + 0.1 * Θ2</p><p id="a47d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">And so on. A more generalized form of exponentially weighted average can be written as:</p><p id="675d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="ls">Vt =β* V(t-1)+(1—β)*θt</em></strong>的小批量</p><p id="0e0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用这个趋势方程，数据将被概括为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nq"><img src="../Images/2661d3d57a0f0ea8a376a9951fcfac8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/0*eIYLmygaLJ6Fiy_m.png"/></div></figure><p id="0f53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本例中的β值为0.9，这意味着Vt是1/(1-β)天平均值的近似值，即1/(1–0.9)= 10天温度。增加β值将导致更多天的近似值，即取更多天的平均温度。如果β值很小，即我们仅使用1天的数据进行近似，预测会变得更加嘈杂:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nr"><img src="../Images/402cd70505e012943951a1171f201976.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/0*mw64IlGZLROspELp.png"/></div></figure><p id="7192" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，绿线是β = 0.98时的近似值(使用50天)，黄线是β = 0.5时的近似值(使用2天)。可以看出，使用小β会导致有噪声的预测。</p><p id="642a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">指数加权平均值的公式如下:</p><p id="4154" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="ls">Vt =β* V(t-1)+(1—β)*θt</em></strong></p><p id="0527" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看如何实现这一点:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="6d8b" class="mn jp hi mj b fi mo mp l mq mr">Initialize VΘ = 0<br/>Repeat<br/>{<br/>get next Θt<br/>VΘ = β * VΘ + (1 - β) * Θt<br/>}</span></pre><p id="6d1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一步占用的内存要少得多，因为我们正在覆盖以前的值。因此，这是一个计算性的和内存高效的过程。</p><h1 id="48aa" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">指数加权平均值中的偏差校正</h1><p id="2120" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">我们初始化V0 = 0，因此在计算V1值时，它将只等于(1—β)*θ1。它不能很好地概括实际值。我们需要使用偏差校正来克服这一挑战。</p><p id="b9d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代替使用前面的等式，即，</p><p id="2033" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="ls">Vt =β* V(t-1)+(1—β)*θt</em></strong></p><p id="d572" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们包括一个偏差校正项:</p><p id="1ac9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="ls">Vt =[β* V(t-1)+(1—β)*θt]/(1—βt)</em></strong></p><p id="7819" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当t较小时，βt会较大，导致(1-βt)的值较小。这将使Vt值更大，确保我们的预测是准确的。</p><h1 id="57fc" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">动量梯度下降</h1><p id="0e84" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">具有动量的梯度下降的基本思想是计算梯度的指数加权平均值，并使用它们来更新权重。假设我们有一个成本函数，其轮廓如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es ns"><img src="../Images/cf0003ccae0031594dfa0e5e42c32bb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KSP6lqH2aBY7kCY9.png"/></div></div></figure><p id="e698" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">红点是全局最小值，我们想要到达那个点。使用梯度下降，更新将如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es nt"><img src="../Images/2bb1280fbfa02c2d9dfa9f061abc5690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2DMvDRj-LhcbdfxJ.png"/></div></div></figure><p id="cebb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一种方法是使用更大的学习率。但这可能导致大的升级步骤，我们可能无法达到全局最小值。此外，学习率太小会使梯度下降更慢。<strong class="ih hj">我们希望在垂直方向上进行较慢的学习，在水平方向上进行较快的学习，这将帮助我们更快地达到全局最小值。</strong></p><p id="d06b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看如何利用动力实现它:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="c322" class="mn jp hi mj b fi mo mp l mq mr">On iteration t:<br/><em class="ls">Compute dW, dB on current mini-batch using momentum</em><br/>VdW = β * VdW + (1 - β) * dW<br/>Vdb = β * Vdb + (1 - β) * db<br/><em class="ls">Update weights</em><br/>W = W - α * VdW<br/>b = b - α * Vdb</span></pre><p id="93bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们有两个超参数，即α和β。上式中dW和db的作用是提供动量，VdW和Vdb提供速度，β作为摩擦力，防止超速。考虑一个滚落的球——VdW和Vdb为球提供速度，使它移动得更快。我们不希望我们的球加速太快，以至于错过了全局最小值，因此β作为摩擦力。</p><p id="794b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">再给大家介绍几个优化算法。</p><h1 id="6384" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">RMSprop</h1><p id="b9e8" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">考虑简单梯度下降的例子:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es nu"><img src="../Images/4237a7ad43acd450cc4c1f6d91ba3068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NnyWhzRKZZKHYI_X.png"/></div></div></figure><p id="9996" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有两个参数w和b，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nv"><img src="../Images/54fec5a2826eb033bce2cf39682dc74b.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/format:webp/0*T-tpUUoMisSo53xY.png"/></div></figure><p id="0adc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看上面显示的轮廓和参数图。我们要减缓b方向即垂直方向的学习，加快w方向即水平方向的学习。RMSprop中遵循的步骤可以总结为:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="38eb" class="mn jp hi mj b fi mo mp l mq mr">On iteration t:<br/><em class="ls">Compute dW, dB on current mini-batch</em><br/>SdW = β2 * SdW + (1 - β2) * dW2<br/>Vdb = β2 * Sdb + (1 - β2) * db2<br/><br/><em class="ls">Update weights</em><br/>W = W - α * (dW/SdW)<br/>b = b - α * (db/Sdb)</span></pre><p id="b5f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">The slope in the vertical direction (db in our case) is steeper, resulting in a large value of Sdb. As we want slow learning in the vertical direction, dividing db with <em class="ls">Sdbin</em> update step will result in a smaller change in b. Hence, learning in the vertical direction will be less. Similarly, a small value of SdW will result in faster learning in the horizontal direction, thus making the algorithm faster.</p><h1 id="20d5" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Adam optimization algorithm</h1><p id="3026" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">Adam is essentially a combination of momentum and RMSprop. Let’s see how we can implement it:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="54b0" class="mn jp hi mj b fi mo mp l mq mr">VdW = 0, SdW = 0, Vdb = 0, Sdb = 0<br/>On iteration t:<br/><em class="ls">Compute dW, dB on current mini-batch using momentum and RMSprop</em><br/>VdW = β1 * VdW + (1 - β1) * dW<br/>Vdb = β1 * Vdb + (1 - β1) * db<br/><br/>SdW = β2 * SdW + (1 - β2) * dW2<br/>Sdb = β2 * Sdb + (1 - β2) * db2<br/><br/><em class="ls">Apply bias correction</em><br/>VdWcorrected = VdW / (1 - β1t)<br/>Vdbcorrected = Vdb / (1 - β1t)<br/><br/>SdWcorrected = SdW / (1 - β2t)<br/>Sdbcorrected = Sdb / (1 - β2t)<br/><br/><em class="ls">Update weights</em><br/>W = W - α * (VdWcorrected / SdWcorrected + ε)<br/>b = b - α * (Vdbcorrected / Sdbcorrected + ε)</span></pre><p id="d5ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">There are a range of hyperparameters used in Adam and some of the common ones are:</p><ul class=""><li id="87ce" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated"><strong class="ih hj">Learning rate α:</strong> needs to be tuned</li><li id="8417" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated"><strong class="ih hj">Momentum term β1:</strong> common choice is 0.9</li><li id="484e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated"><strong class="ih hj">RMSprop term β2:</strong> common choice is 0.999</li><li id="7e2c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated"><strong class="ih hj">ε:</strong> 10–8</li></ul><p id="a326" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Adam helps to train a neural network model much more quickly than the techniques we have seen earlier.</p><h1 id="ce7c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Learning Rate Decay</h1><p id="ebce" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">If we slowly reduce the learning rate over time, we might speed up the learning process. This process is called learning rate decay.</p><p id="acd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Initially, when the learning rate is not very small, training will be faster. If we slowly reduce the learning rate, there is a higher chance of coming close to the global minima.</p><p id="5f1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Learning rate decay can be given as:</p><p id="ac45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">α = [1 / (1 + decay_rate * epoch_number)] * α0</p><p id="ec77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Let’s understand it with an example. Consider:</p><ul class=""><li id="da4f" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi">α0 = 0.2</li><li id="ca82" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">decay_rate = 1</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nw"><img src="../Images/8fb690510b2ff7d7ea86b8cf83675980.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*TQ82nDKMNjt64Jap5sTdhQ.png"/></div></figure><p id="c760" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">This is how, after each epoch, there is a decay in the learning rate which helps us reach the global minima much more quickly. There are a few more learning rate decay methods:</p><ol class=""><li id="4d85" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated"><strong class="ih hj">Exponential decay:</strong> α = (0.95)epoch_number * α0</li><li id="03ba" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">α = k / epochnumber1/2* α0</li><li id="c003" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">α = k / t1/2* α0</li></ol><p id="0be3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Here, t is the mini-batch number.</p><p id="bc00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">This was all about optimization algorithms and module 2! Take a deep breath, we are about to enter the final module of this article.</p><h1 id="7492" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Module 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks</h1><p id="2459" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">The primary objectives of module 3 are:</p><ul class=""><li id="525b" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">To master the process of hyperparameter tuning</li><li id="6010" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">To familiarize yourself with the concept of Batch Normalization</li></ul><p id="281a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Much like the first module, this is further divided into three sections:</p><ul class=""><li id="04b2" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">Part I: Hyperparameter tuning</li><li id="9874" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">Part II: Batch Normalization</li><li id="6e4c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">Part III: Multi-class classification</li></ul><h1 id="eaa7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Part I: Hyperparameter tuning</h1><h1 id="2185" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Tuning process</h1><p id="7f12" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">Hyperparameters. We see this term popularly being bandied about in data science competitions and hackathons. But how important is it in the overall scheme of things?</p><p id="65bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Tuning these hyperparameters effectively can lead to a massive improvement in your position on the leaderboard. Following are a few common hyperparameters we frequently work with in a deep neural network:</p><ul class=""><li id="b081" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">Learning rate — α</li><li id="f5ae" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">Momentum — β</li><li id="cbdc" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">Adam’s hyperparameter — β1, β2, ε</li><li id="e435" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">Number of hidden layers</li><li id="e1c0" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">Number of hidden units for different layers</li><li id="91c3" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">Learning rate decay</li><li id="b54d" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">Mini-batch size</li></ul><p id="499f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习速度通常被证明是上述因素中最重要的。其次是隐单元数、动量、小批量、隐层数，然后是学习率衰减。</p><p id="9a04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，假设我们有两个超参数。我们对网格中的点进行采样，然后系统地探索这些值。考虑一个5x 5的网格:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nx"><img src="../Images/326942b048edf06c605a417d92639f1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/0*NfBf6ySEmlIbmkob.png"/></div></figure><p id="0ccc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们检查所有25个值，并选择最佳超参数。除了使用这些网格，我们还可以尝试随机值。为什么？因为我们不知道哪个超参数值可能是重要的，并且在网格中我们只定义特定的值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nx"><img src="../Images/2d28ae4b608892a544fd945194a1c897.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/0*vzDlmEExngTLSvhN.png"/></div></figure><p id="a3f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本小节的主要内容是使用随机抽样和充分搜索。</p><h1 id="cccd" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">使用合适的标度选择超参数</h1><p id="ea75" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">为了理解这一点，考虑隐藏单元的数量超参数。我们感兴趣的范围是从50到100。我们可以使用包含50到100之间的值的网格，并使用该网格来查找最佳值:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ny"><img src="../Images/74c26e7e8d0c05ad7bef6471d933fd9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/0*bIOqszo3N0Hx2p9t.png"/></div></figure><p id="e97e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在考虑范围在0.0001和1之间的学习率。如果我们用这些极值画一条数线，并随机均匀地采样这些值，大约90%的值将落在0.1到1之间。换句话说，我们使用90%的资源来搜索0.1到1之间的值，只使用10%的资源来搜索0.0001到0.1之间的值。这看起来不正确！相反，我们可以使用对数标度来选择值:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nz"><img src="../Images/b23bdb9a3c13eafdea5eb6fac1cdb44a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/0*R2bNP4fzysujb3pG.png"/></div></figure><p id="f8e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将学习一种技术，这种技术使我们的神经网络对超参数的选择更加稳健，并且使训练阶段更加快速。</p><h1 id="638c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">第二部分:批处理规范化</h1><h1 id="3a3b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">标准化网络中的激活</h1><p id="9dce" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">让我们回忆一下逻辑回归是什么样子的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es oa"><img src="../Images/a1a4f5f0e5b8d81510f1e54467bcf1ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/0*UFsl00LOh8a-K4Qe.png"/></div></figure><p id="cc79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经看到了在这种情况下规范化输入是如何加快学习过程的。在深度神经网络的情况下，我们有许多隐藏层，这导致许多激活:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ob"><img src="../Images/bdacebde14374d74c5b7840cfebad5e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/0*UAf2sWLGPBMhgKiO.png"/></div></figure><p id="d915" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如果我们能把这些激活(a[2])的均值和方差归一化，以便让w[3]，b[3]的训练更有效，岂不是很棒？</strong>批量标准化就是这样工作的。我们将隐藏层的激活标准化，以便下一层的权重可以更快地更新。从技术上讲，我们将z[2]的值归一化，然后使用归一化值的激活函数。下面是我们实现批处理规范化的方法:</p><p id="6cae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定NN Z(1)中的一些中间值，…，Z(米):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es oc"><img src="../Images/f85eb622af4372d950fc3b7e37f31b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/0*e9r-2dprsOjrYHLp.png"/></div></figure><p id="5337" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，ɣ和β是可学习的参数。</p><h1 id="2d7b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">将批量定额拟合到神经网络中</h1><p id="3377" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">考虑如下所示的神经网络:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es od"><img src="../Images/18785218654c3d4c546879278fe58204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*5JIYg4Hi_Mabo0h5.png"/></div></div></figure><p id="ea05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经网络的每个单元计算两件事。它首先计算Z，然后在Z上应用激活函数来计算a。如果我们在每一层应用批范数，计算将如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es oe"><img src="../Images/0dcf3ae65a08280a8247651411eb337a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*k_OqcMYLLq7JCocF.png"/></div></figure><p id="4abc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算Z值后，我们应用批量范数，然后在其上应用激活函数。这种情况下的参数是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es of"><img src="../Images/fd304e8ea59f9732779197c0b4450444.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/0*ZYJ1fUXOZoQrwdVq.png"/></div></figure><p id="a220" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，让我们看看如何使用批量范数应用梯度下降:</p><pre class="je jf jg jh fd mi mj mk ml aw mm bi"><span id="c8df" class="mn jp hi mj b fi mo mp l mq mr">For t=1, ….., number of batches:<br/>    Compute forward propagation on X{t}<br/>    In each hidden layer, use batch normalization<br/>    Use backpropagation to compute dW[l], db[l], dβ[l] and dƔ[l]<br/>    Update the parameters:<br/>    W[l] = W[l] - α*dW[l]<br/>    β[l] = β[l] - α*dβ[l]</span></pre><p id="fb04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，这也适用于momentum、RMSprop和Adam。</p><h1 id="2813" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">批量定额是如何工作的？</h1><p id="2c39" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在逻辑回归的情况下，我们现在知道标准化输入如何有助于加速学习。批处理规范的工作方式大致相同。让我们再举一个用例来更好地理解它。考虑二进制分类问题的训练集:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es og"><img src="../Images/5f14b563e2aefa83e713a717a337196e.png" data-original-src="https://miro.medium.com/v2/resize:fit:244/format:webp/0*SbVcHN32_AdqGPOZ.png"/></div></figure><p id="f939" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是当我们试图将其推广到具有不同分布的数据集时，比如说:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es oh"><img src="../Images/6afcef42220eddffc67ec287d08ea781.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/0*Rzgh0MA8G2kG8Q27.png"/></div></figure><p id="1b0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">两种情况下的决策界限可能是相同的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es oi"><img src="../Images/598ac797be349ef3a832f64384b9e6a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/0*Actj3UrNvJ8xirHj.png"/></div></div></figure><p id="efa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是模型将不能发现这个绿色决策边界。因此，随着输入分布的变化，我们可能需要再次训练模型。考虑一个深度神经网络:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es oj"><img src="../Images/1d4d9a36cf932b571b2abe9e04563d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZHJXH9CDKm1CcH2R.png"/></div></div></figure><p id="c410" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">而且我们只考虑第三层的学习。它将来自第二层的激活作为其输入:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ok"><img src="../Images/115bfc8407afdc729b0708b179ece0fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/0*hKYOHLcor5IyQhPA.png"/></div></figure><p id="1e84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第三个隐藏层的目的是获取这些激活并将它们映射到输出。这些激活随着先前层的参数的改变而改变。因此，我们看到激活值有很大的变化。批量定额减少了这些隐藏单元值的分布的移动量。</p><p id="cdfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，批处理规范也有正则化效果:</p><ul class=""><li id="6216" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">每个小批量使用仅在该小批量上计算的平均值/方差进行标准化</li><li id="b82d" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">这给小批量中的z[l]值增加了一些噪声，类似于丢失的影响</li><li id="2d5d" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">因此，这也有轻微的正则化效果</li></ul><p id="c181" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要注意的一点是，在进行预测时，我们使用批处理规范化的方式略有不同。</p><h1 id="2e44" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">测试时的批量定额</h1><p id="8eed" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在对测试数据进行预测时，我们需要一次处理一个例子。在训练期内，批量定额的步骤可以写成:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mx"><img src="../Images/d566dea0927e13b9c81a66d161a2be6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/0*-4ywehkqg1Lhpkjp.png"/></div></figure><p id="6acc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们首先计算该小批量的平均值和方差，并使用它来归一化z值。我们将使用整个小批量来计算平均值和标准差。我们分别处理每幅图像，因此取单幅图像的平均值和标准差是没有意义的。</p><p id="4369" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用指数加权平均值来计算不同小批量的平均值和方差。最后，我们使用这些值来调整测试数据。</p><h1 id="c0c7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">第三部分:多类分类</h1><h1 id="0c68" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Softmax回归</h1><p id="829d" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">二元分类意味着处理两个类。但是当我们在一个问题中有两个以上的类时，那就叫做多类分类。假设我们必须在一组图像中识别猫、狗和老虎。有多少种类型的课程？4 —猫、狗、老虎，一个都没有。如果你说三，那就再想想！</p><p id="9c19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决这类问题，我们使用softmax回归。在输出层，不是只有一个单元，而是有等于类总数的单元(在我们的例子中是4个)。每个单元告诉我们图像落在不同类别的概率。因为它告诉概率，来自每个单元的值的总和总是等于1。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ol"><img src="../Images/7cafdd8440979a42021a9d2adbc9a9c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/0*NtpiQ6rXsLYr0v8k.png"/></div></figure><p id="5a73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是用于多类分类的神经网络的样子。因此，对于层L，输出将是:</p><p id="cffa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Z[L] = W[L]*a[L-1] + b[L]</p><p id="6937" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激活功能将是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es om"><img src="../Images/64a8524aee86f956776e70c1229f8df6.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/0*odRbXAf0YoD5MEPI.png"/></div></figure><p id="0fd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们用一个例子来理解这个。考虑最后一个隐藏层的输出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es on"><img src="../Images/b817d16832510bad8d84bcd32198123c.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/0*8zCoKZ2zNGbCXzt1.png"/></div></figure><p id="6c40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们使用上面给出的公式计算t:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es oo"><img src="../Images/338c4ba16b981c064763e63e40383646.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/0*MiVmtSpeWxUZwz5E.png"/></div></figure><p id="fd8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们计算激活量:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es op"><img src="../Images/db9104c9176d87dd7dcf13ad8a024871.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/0*wFJhR9W3OkPZuVrq.png"/></div></figure><p id="5132" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我们如何使用softmax激活函数解决多类分类问题。课程2到此结束！</p><h1 id="db1c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结束注释</h1><p id="39e2" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">恭喜你！我们已经完成了深度学习专业化的第二个课程。写这篇文章是一项相当紧张的工作，在这个过程中，它确实有助于巩固我自己的概念。总结一下我们在这里讨论的内容:</p><ul class=""><li id="6465" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">我们现在知道如何使用各种技术来提高神经网络的性能</li><li id="2c2f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">我们首先看到了决定培训/开发/测试分割如何帮助我们决定哪一个模型表现最好</li><li id="556f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">然后我们看到了正则化是如何帮助我们处理过度拟合的</li><li id="31b8" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">我们讲述了如何设置一个优化问题</li><li id="3ea4" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">我们还研究了各种优化算法，如momentum、RMSprop、Adam，它们帮助我们更快地达到成本函数的全局最小值，从而减少学习时间</li><li id="a1aa" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">我们学习了如何调整神经网络模型中的各种超参数，以及缩放如何帮助我们实现这一点</li><li id="5eb4" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">最后，我们介绍了批处理规范化技术，我们可以用它来进一步加快训练时间</li></ul><p id="bd8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您对本文有任何反馈或有任何疑问/问题，请在下面的评论区分享。</p></div><div class="ab cl oq or gp os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="hb hc hd he hf"><p id="dfa4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ls">原载于2018年11月12日</em><a class="ae jn" href="https://www.analyticsvidhya.com/blog/2018/11/neural-networks-hyperparameter-tuning-regularization-deeplearning/" rel="noopener ugc nofollow" target="_blank"><em class="ls">【www.analyticsvidhya.com】</em></a><em class="ls">。</em></p></div></div>    
</body>
</html>