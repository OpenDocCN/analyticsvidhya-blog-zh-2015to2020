<html>
<head>
<title>Understanding Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解递归神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-rnns-652b7d77500e?source=collection_archive---------14-----------------------#2020-08-25">https://medium.com/analytics-vidhya/understanding-rnns-652b7d77500e?source=collection_archive---------14-----------------------#2020-08-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="2ed6" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><div class=""><h2 id="3e72" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">递归神经网络导论</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/fe7dc50c0f1798d633c64dcd64e7d0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ggFZwn7TExHCBy8ijTsCxQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">折叠和展开模型中的递归神经网络(RNN)。<a class="ae jw" href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="noopener ugc nofollow" target="_blank">来源</a> [1]。</figcaption></figure><h1 id="f69e" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">什么是递归神经网络(RNN)？</h1><p id="8694" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">第一次碰到RNNs，完全被难倒了。一个网络怎么可能记得事情？自20世纪80年代末首次出现以来，递归神经网络已被证明是处理序列数据的有效和受欢迎的方法。</p><p id="a4c7" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">递归神经网络是从传统的前馈神经网络中衍生出来的。它们有所谓的记忆元件，帮助网络记住以前的输出。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lq"><img src="../Images/aa20e321a2225a1a3f5aab62cb828b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*qKfFJAgdpUlSGfKqjzEgmQ.jpeg"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图片来源<a class="ae jw" href="https://images.unsplash.com/photo-1533279443086-d1c19a186416?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1489&amp;q=80" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>。</figcaption></figure><h1 id="cee6" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">那么，为什么用循环这个词呢？</h1><p id="cad9" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">它们是循环的，因为它们对序列中的每个元素重复执行相同的任务，输出依赖于先前的计算。</p><p id="bc4d" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">递归神经网络(RNNs)是对传统神经网络的巨大改进。典型的普通神经网络计算当前输入的输出，并以预定的固定输入大小为限制进行加权。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lr"><img src="../Images/dba66e706ebcb2fad3cf215969bcf961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*X7uX7TaYP-J1BHCVoZ6Qgw.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">香草神经网络:前馈神经网络。来源<a class="ae jw" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">NNDL</a>【2】。</figcaption></figure><p id="7e6b" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">在本文中，我们将以Elman网络为例，用足够的数学知识来研究RNNs的体系结构。</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><h1 id="2863" class="jx jy hi bd jz ka lz kc kd ke ma kg kh ix mb iy kj ja mc jb kl jd md je kn ko bi translated">为什么是RNNs？</h1><p id="60da" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">在典型的神经网络中，输出仅基于当前输入。生成当前输出时，不考虑任何先前的输出。没有记忆元素。在我们需要相同的情况下，rnn是有用的。</p><p id="a10b" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">rnn被设计成接受一系列输入，没有预先确定的大小限制。</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><h1 id="4e0c" class="jx jy hi bd jz ka lz kc kd ke ma kg kh ix mb iy kj ja mc jb kl jd md je kn ko bi translated">为什么是过去的产出？</h1><p id="f783" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">大多数应用程序都有时间依赖性。这意味着生成的输出不仅取决于当前的输入，还取决于先前的输出。</p><p id="c2a2" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">rnn在语音识别(Alexa，google assistant等)中很有用。)、时间序列预测(股市、天气预报)、自然语言处理(NLP)等。</p><p id="13db" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">rnn能够捕捉随时间推移的时间依赖性。</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><h1 id="994a" class="jx jy hi bd jz ka lz kc kd ke ma kg kh ix mb iy kj ja mc jb kl jd md je kn ko bi translated">深入研究RNNs</h1><p id="04b8" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">你可能会想，好吧，但是这些网络是如何做到记忆的呢？好吧，现在让我们讨论同样的问题。</p><p id="4335" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">rnn在训练阶段将序列作为输入，并且具有基本上是隐藏层输出的存储元素。这些所谓的记忆元素在下一个训练步骤中用作输入。</p><p id="1f7f" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><em class="me"> Elman网络</em>是最基本的三层神经网络，反馈作为记忆输入。不要被这些符号弄得不知所措。我们过一会儿再看。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mf"><img src="../Images/f7dbbf21951de5f4ceb965248211e499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*xkettaJKKiel3_OQ3DFDVQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">埃尔曼网络。来源<a class="ae jw" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">维基百科</a>【3】。</figcaption></figure><p id="e5a7" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><strong class="kr hs"> <em class="me">在FFNN(前馈神经网络)</em> </strong>时刻t的输出，是当前输入和权值的函数。这可以很容易地表达如下:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mg"><img src="../Images/83f7bc4be424c1b5ec93aaaf7c2333f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*MttmoCrLYBbjXc4WUNd_Sw.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">FFNN的输出。来源Udacity。[5]</figcaption></figure><p id="0ae1" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">隐藏层输出可以用激活函数φ表示如下:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mh"><img src="../Images/771bac247951409165e2e27323322211.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*dO-dZQV0SjP2WYVnhZR8sQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">FFNN中带有激活函数的隐层输出。来源Udacity。[5]</figcaption></figure><p id="e7bf" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">当谈到激活功能时，以下是RNNs最常用的功能:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mi"><img src="../Images/ec3ba3f964d91e40ad2e1d67c4705806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EkxUDA4D-wNIuWPD4EvO3A.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">激活功能。<a class="ae jw" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank">来源</a>【4】</figcaption></figure><p id="17d8" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">然而，在<strong class="kr hs"> <em class="me"> RNN(递归神经网络)中，</em> </strong>在时间t的输出是当前输入、权重以及先前输入的函数。这可以很容易地表达如下:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mj"><img src="../Images/95eddf012e1a2396d9e86fbc29dff86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*r0waDbItMo9PXO66Gzjqfg.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">RNN的产量。来源Udacity。[5]</figcaption></figure><h2 id="efd5" class="mk jy hi bd jz ml mm mn kd mo mp mq kh ky mr ms kj lc mt mu kl lg mv mw kn ho bi translated">RNN折叠和展开模型</h2><p id="d544" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">让我们了解这些网络背后的架构和数学。在RNN，我们有输入层、状态层和输出层。这些状态层类似于FFNN中的隐藏层，但是它们具有捕捉时间依赖性或者说网络的先前输入的能力。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mx"><img src="../Images/030e167c0c06436e66c5dc837fa0313d.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*DJYP1vRFtmzOJlxWBzuMwg.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">RNN折叠模型。来源Udacity。[5]</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es my"><img src="../Images/126db1689f57012b6e6b5db7b0afded4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-j9zVo6xXVjLZZ0a19YvHg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">RNN展开模型。来源Udacity。[5]</figcaption></figure><p id="9f56" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><strong class="kr hs">展开模型</strong>通常是我们在处理RNNs时使用的。</p><p id="baef" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">上图中，<em class="me"> x </em> (x条)表示输入向量，<em class="me"> y </em> (y条)表示输出向量，<em class="me"> s </em> (s条)表示状态向量。</p><p id="ec12" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><em class="me"> Wx </em>是将输入连接到状态层的权重矩阵。</p><p id="9e3d" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><em class="me"> Wy </em>是连接状态层和输出层的权重矩阵。</p><p id="87d9" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><em class="me"> Ws </em>表示连接前一时间步的状态和当前时间步的状态的权重矩阵。</p><p id="140c" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">所谓的状态层输出可以给出为:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mz"><img src="../Images/61b6e98ac34592d15a005a854cb83c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*E80FqBwGvcOmGLvGL9UyGQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">状态层的输出。来源Udacity。[5]</figcaption></figure><p id="1bc9" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">输出层(带softmax函数)可由下式给出:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es na"><img src="../Images/ea3224147bf291e96cff21902d9d32cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*_6mlS4qEx7SYzCO2mvLVSg.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">RNN的产量。来源Udacity。[5]</figcaption></figure><p id="5724" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">通常使用展开模型的原因是我们可以很容易地将其可视化，以便更好地理解。让我们看看折叠和展开的Elman网络。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nb"><img src="../Images/b8cfe13a59eed2ab647ab0608f7d490d.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*oxF9OJ0PzDVUHrRiC0GAHw.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">埃尔曼网络折叠模型在时间t。[5]</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nc"><img src="../Images/a8da7f72179bbcbd72bc6bdde170a296.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*9Kvj3zVdx4fHg2Z-oxq41Q.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">Elman网络在时间t展开模型。[5]</figcaption></figure><p id="9812" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">时间t的折叠Elman网络，输出<em class="me"> y1，y2。</em></p><p id="e9fd" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">存储元件由状态层表示。折叠模型的真正问题是我们不能一次可视化一个以上的时间实例。</p><p id="f827" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">展开的模型给出了输入序列、状态和输出层在时间T(零)到时间Tn(一段时间内)的清晰图像。比如Y <em class="me"> t+2 </em>由<em class="me"> Wy </em>和<em class="me"> St+1 </em>和<em class="me"> Xt+2 </em>确定，对应权重<em class="me"> Wy </em>和<em class="me"> Wx。</em></p><h2 id="6917" class="mk jy hi bd jz ml mm mn kd mo mp mq kh ky mr ms kj lc mt mu kl lg mv mw kn ho bi translated">穿越时间的反向传播(BPTT)</h2><p id="3b6b" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">我们现在可以看看网络是如何学习的。它类似于FFNN，除了我们需要考虑以前的时间步骤，因为系统有记忆。rnn使用穿越时间的反向传播(BPTT)。</p><p id="77b9" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">为了简化，让我们考虑如下损失函数:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nd"><img src="../Images/97290818416f4bc3a40260d813ef188b.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*J9VjhTKUAwMw_LLDIkEz-g.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">损失或误差函数。来源Udacity。[5]</figcaption></figure><p id="b5b5" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><em class="me"> Et </em>表示时间t的输出误差</p><p id="63f0" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><em class="me"> dt </em>表示时间t的期望输出</p><p id="ac3e" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><em class="me"> yt </em>表示时间t时的计算输出</p><p id="0793" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">在BPTT，我们计算梯度来优化<em class="me"> Wy、Ws和Wx的权重。</em></p><p id="c858" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">对于<em class="me"> Wy，</em>在时间N的重量变化，可以一步计算如下:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ne"><img src="../Images/2bf8cb3585997513effc065b08efc201.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*srQEzBR-p59DxDOxfuTPRw.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">错误w.r.t <strong class="bd jz"> y，</strong>为时间n .源Udacity。[5]</figcaption></figure><p id="e328" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">对于<em class="me"> Ws </em>梯度是<em class="me">每一状态随时间累积的。因此，假设在时间t=3，我们考虑从t=1到t=3的梯度，并应用考虑到<em class="me"> s </em> 1 (s1杆)到<em class="me"> s </em> 2 (s2杆)和<em class="me"> s </em> 3 (s3杆)的链式法则如下:</em></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nf"><img src="../Images/60f3505d3233bb0167dd7a80d9258309.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*5al6Jv-oTSm9QTKms9pGCA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">时间t=3时的误差w.r.t <strong class="bd jz"> S，</strong>。来源Udacity。[5]</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mz"><img src="../Images/d6d8938bc57415a4276ff1578d2c1851.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*eb65aHJbJZYbPPCmBXMd0g.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">错误w.r.t <strong class="bd jz"> S，</strong>为时间n .源Udacity。[5]</figcaption></figure><p id="19fc" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">类似地计算重量引起的误差<em class="me"> Wx </em>。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ng"><img src="../Images/59c9f2293ace514829eafb1bc8fea24c.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*26UDZ9z9ffxUcr3BuE2jLQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">时间t=3时的误差w.r.t <strong class="bd jz"> X，</strong>。来源Udacity。[5]</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nh"><img src="../Images/3122dbcf204a8cdb4345ac1b9b4fa55e.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*YiZeADAfCZpzxCUqAI5bUw.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">错误w.r.t <strong class="bd jz"> X，</strong>为时间n .源Udacity。[5]</figcaption></figure><h2 id="e89f" class="mk jy hi bd jz ml mm mn kd mo mp mq kh ky mr ms kj lc mt mu kl lg mv mw kn ho bi translated">RNN的缺点</h2><p id="b554" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">如果我们反向传播超过10个时间步长，梯度就会变得太小。这种现象被称为<strong class="kr hs">消失梯度问题。</strong>因此，跨越许多时间步长的时间依赖性将被网络有效地丢弃。发生这种情况的原因是，由于倍增梯度可能相对于层数呈指数下降/上升，因此很难捕捉长期相关性。</p><p id="5a42" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">在RNNs中，我们也可能遇到相反的问题，称为<strong class="kr hs">爆炸梯度</strong>问题，其中梯度的值不受控制地增长。爆炸渐变问题的一个简单解决方案是<strong class="kr hs">渐变裁剪</strong>。通过限制梯度的最大值，这种现象在实践中得到控制。</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><h1 id="b365" class="jx jy hi bd jz ka lz kc kd ke ma kg kh ix mb iy kj ja mc jb kl jd md je kn ko bi translated">结论</h1><p id="f6f7" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">我希望您对RNNs的工作原理有了基本的了解。所谓的<strong class="kr hs">长短期记忆细胞(</strong> LSTM)进一步改进了rnn，作为消失梯度问题的解决方案，帮助我们捕捉10个时间步长甚至1000个时间步长的时间依赖性！<strong class="kr hs"> LSTM </strong>单元格稍微复杂一点，同样的内容将在另一篇文章中讨论。</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><h1 id="8f63" class="jx jy hi bd jz ka lz kc kd ke ma kg kh ix mb iy kj ja mc jb kl jd md je kn ko bi translated">参考</h1><ol class=""><li id="f045" class="ni nj hi kr b ks kt kv kw ky nk lc nl lg nm lk nn no np nq bi translated"><a class="ae jw" href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="noopener ugc nofollow" target="_blank">丹尼·布里兹的wild ml RNNs</a></li><li id="68bc" class="ni nj hi kr b ks nr kv ns ky nt lc nu lg nv lk nn no np nq bi translated"><a class="ae jw" href="http://neuralnetworksanddeeplearning.com/chap1.html" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习，作者迈克尔·尼尔森</a></li><li id="f73a" class="ni nj hi kr b ks nr kv ns ky nt lc nu lg nv lk nn no np nq bi translated"><a class="ae jw" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络维基百科</a></li><li id="870a" class="ni nj hi kr b ks nr kv ns ky nt lc nu lg nv lk nn no np nq bi translated"><a class="ae jw" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank"> CS230斯坦福深度学习</a></li><li id="559e" class="ni nj hi kr b ks nr kv ns ky nt lc nu lg nv lk nn no np nq bi translated"><a class="ae jw" href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="noopener ugc nofollow" target="_blank"> Udacity深度学习纳米学位计划</a></li></ol></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><blockquote class="nw nx ny"><p id="a659" class="kp kq me kr b ks ll is ku kv lm iv kx nz ln la lb oa lo le lf ob lp li lj lk hb bi translated">嘿，如果你喜欢这篇文章，请点击拍手按钮，分享这篇文章，以示你的支持。关注我，获取更多关于机器学习、深度学习和数据科学的文章。 </p></blockquote><h1 id="c144" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">在网络上找到我</h1><p id="286f" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated"><a class="ae jw" href="https://github.com/NvsYashwanth" rel="noopener ugc nofollow" target="_blank"> <strong class="kr hs"> GitHub简介:</strong>这就是我叉</a>的地方</p><p id="fa9c" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><a class="ae jw" href="https://www.linkedin.com/in/nvsyashwanth/" rel="noopener ugc nofollow" target="_blank"> <strong class="kr hs"> LinkedIn简介:</strong>联系分享职业动态</a></p><p id="4acb" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><a class="ae jw" href="https://twitter.com/YashwanthNvs" rel="noopener ugc nofollow" target="_blank"> <strong class="kr hs">推特:</strong>分享科技推特</a></p><h1 id="a2be" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">谢谢:)</h1></div></div>    
</body>
</html>