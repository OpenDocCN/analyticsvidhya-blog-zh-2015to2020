<html>
<head>
<title>Building a Neural Network from scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头开始构建神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-a-neural-network-from-scratch-in-python-edc74ae63761?source=collection_archive---------6-----------------------#2020-03-13">https://medium.com/analytics-vidhya/building-a-neural-network-from-scratch-in-python-edc74ae63761?source=collection_archive---------6-----------------------#2020-03-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="2d3d" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">一种理解神经网络工作背后的内在数学的方法</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jh"><img src="../Images/88f7fd54ff2e8a4059f032a0ee703a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gr0sK3Uo6wvb6BQUAfi8tw.jpeg"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">来源:谷歌图片</figcaption></figure><p id="dce1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">我始终相信，要成为任何领域的大师，我们都应该了解那个特定领域的核心工作领域。为了开发我们自己的库，我们应该理解神经网络工作背后的内部数学。所以我决定从头开始建立一个神经网络，而不需要任何深度学习库。</p><p id="4265" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">在这篇文章中，我们将从头开始学习神经网络和Python代码背后的数学。我们建立一个具有各种层(完全连接)的神经网络。最终，我们将能够以模块化的方式创建网络。</p><p id="24e2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">我假设你已经有了一些关于神经网络的T2知识。这里的目的不是解释神经网络的概念或者我们为什么要做这些模型，而是向<strong class="il hj">展示如何做一个合适的实现</strong>。</p><h1 id="12a3" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak"> <em class="ky">神经网络的组成</em> </strong></h1><p id="a967" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jx lb iw ix jy lc ja jb jz ld je jf jg hb bi translated">神经网络由以下组件组成</p><ul class=""><li id="c442" class="le lf hi il b im in iq ir jx lg jy lh jz li jg lj lk ll lm bi translated">一个<strong class="il hj">输入层</strong>，<strong class="il hj"> X </strong></li><li id="de1f" class="le lf hi il b im ln iq lo jx lp jy lq jz lr jg lj lk ll lm bi translated">任意数量的<strong class="il hj">隐藏层</strong></li><li id="2686" class="le lf hi il b im ln iq lo jx lp jy lq jz lr jg lj lk ll lm bi translated">一个<strong class="il hj">输出层</strong>，<strong class="il hj">，<em class="ik"> ŷ </em>，</strong></li><li id="b347" class="le lf hi il b im ln iq lo jx lp jy lq jz lr jg lj lk ll lm bi translated">一组<strong class="il hj">权重</strong>和<strong class="il hj">在各层之间偏向</strong>、<strong class="il hj">、<em class="ik"> W和b </em>、</strong></li><li id="dccb" class="le lf hi il b im ln iq lo jx lp jy lq jz lr jg lj lk ll lm bi translated">一个选择<strong class="il hj">激活功能</strong>用于每个隐藏层，<strong class="il hj">一个</strong>。在本教程中，我们将使用一个<strong class="il hj"> Sigmoid </strong>激活函数。</li></ul><p id="c11c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">下图显示了2层神经网络的架构(<em class="ik">注意，在计算神经网络的层数时，输入层通常被排除在外</em></p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es ls"><img src="../Images/48ba6cbb8def3cfe4d067473656ad8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*2zVy-_ssBjJFuAgc9qnlxg.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">两层神经网络</figcaption></figure><h1 id="cbb7" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">构建我们算法的各个部分</h1><p id="4950" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jx lb iw ix jy lc ja jb jz ld je jf jg hb bi translated">构建神经网络的主要步骤是:</p><ol class=""><li id="0f2a" class="le lf hi il b im in iq ir jx lg jy lh jz li jg lt lk ll lm bi translated">定义模型结构(如输入要素和隐藏图层的数量)</li><li id="905e" class="le lf hi il b im ln iq lo jx lp jy lq jz lr jg lt lk ll lm bi translated">初始化模型的参数。</li><li id="baa8" class="le lf hi il b im ln iq lo jx lp jy lq jz lr jg lt lk ll lm bi translated">循环直到成本函数的最小值。</li></ol><ul class=""><li id="d168" class="le lf hi il b im in iq ir jx lg jy lh jz li jg lj lk ll lm bi translated">计算电流损耗(<strong class="il hj"> <em class="ik">正向传播</em> </strong>)</li><li id="4f5b" class="le lf hi il b im ln iq lo jx lp jy lq jz lr jg lj lk ll lm bi translated">计算当前梯度(<strong class="il hj"> <em class="ik">反向传播</em> </strong>)</li><li id="45a4" class="le lf hi il b im ln iq lo jx lp jy lq jz lr jg lj lk ll lm bi translated">更新参数(<strong class="il hj"> <em class="ik">渐变下降</em> </strong>)</li></ul><p id="443a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">你经常分别构建1-3，然后将它们整合成一个函数，我们称之为<code class="du lu lv lw lx b">model()</code></p><h1 id="780d" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">实现</strong></h1><p id="8d2c" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jx lb iw ix jy lc ja jb jz ld je jf jg hb bi translated">所以，我们现在知道了神经网络背后的主要思想。让我们开始在代码中实现这些想法。正如我提到的，我们不会使用任何深度学习库。因此，我们将主要使用numpy来高效地执行数学计算。</p><pre class="ji jj jk jl fd ly lx lz ma aw mb bi"><span id="29c3" class="mc kb hi lx b fi md me l mf mg">import numpy as np</span></pre><p id="82e5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">构建神经网络的第一步是在初始化参数之前定义我们的模型结构(layer_dim)。我们需要为每层中的每个神经元初始化两个参数:1) <em class="ik">权重</em>和2) <em class="ik">偏差</em>。</p><p id="14b2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">这些权重和偏差以<strong class="il hj"> <em class="ik">矢量化</em> </strong>的形式声明。这意味着我们不是为每一层中的每个神经元初始化权重和偏差，而是为每一层创建一个权重向量(或矩阵)和另一个偏差向量。(注意<em class="ik">矢量化实现可以被视为矩阵运算，通常比标准循环更有效)</em></p><p id="254a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">这些<em class="ik">权重</em>和<em class="ik">偏置</em>矢量将与层的输入相结合。然后，我们将对该组合应用sigmoid函数，并将其作为输入发送到下一层。</p><p id="3e9a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated"><strong class="il hj"> layer_dim </strong> <em class="ik"> </em>保存每一层的尺寸。我们将把这些层的尺寸传递给<strong class="il hj"> init_parms </strong> <em class="ik"> </em>函数，该函数将使用它们来初始化参数。这些参数将存储在名为<strong class="il hj"> params </strong>的字典中。因此在params字典<strong class="il hj">中，params['W1'] </strong> <em class="ik"> </em>将表示层1的权重矩阵。</p><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="f095" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">酷！我们已经初始化了权重和偏差，现在我们将定义<em class="ik"> sigmoid函数</em>。它将为任何给定值<strong class="il hj"> Z </strong>计算sigmoid函数的值，并将该值存储为缓存。我们将存储缓存值，因为我们需要它们来实现反向传播。这里的<strong class="il hj"> Z </strong>就是<em class="ik">线性假设</em>。这里，sigmoid函数是<em class="ik">激活函数，不同类型的<em class="ik">激活函数</em>可用于获得更好的性能，但为了简单起见，我们将坚持使用sigmoid(<em class="ik">注意，Relu是使用最广泛的激活函数)</em></em></p><blockquote class="if ig ih"><p id="1c32" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">激活函数的工作是调整神经元的输出。其目的是将线性输出转换为非线性输出。</p></blockquote><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="mh mi l"/></div></figure><h2 id="dcdd" class="mc kb hi bd kc mj mk ml kg mm mn mo kk jx mp mq ko jy mr ms ks jz mt mu kw mv bi translated"><strong class="ak">正向传播</strong></h2><p id="8c72" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jx lb iw ix jy lc ja jb jz ld je jf jg hb bi translated">现在，我们开始编写向前传播的代码。我们知道<em class="ik">正向</em> <em class="ik">传播</em>将从上一层获取值，并将其作为输入提供给下一层。下面的函数将把<em class="ik">训练数据</em>和<em class="ik">参数</em>作为输入，并将生成一层的输出，然后将该输出馈送到下一层，依此类推。</p><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="06fc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated"><strong class="il hj"> A_prev </strong> <em class="ik"> i </em> s输入到第一层。我们将遍历网络的所有层，并计算线性假设。之后，它将获取<strong class="il hj"> Z </strong>(线性假设)的值，并将其提供给sigmoid激活函数。高速缓存值沿途存储，并在<strong class="il hj">高速缓存</strong>中累积。最后，该函数将返回生成的值和存储的缓存。</p><p id="2c6c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">现在是时候定义我们的<strong class="il hj">成本函数</strong>来检查我们的模型性能了。</p><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="9e30" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">随着代价函数值的减小，我们的模型的性能变得更好。可以通过更新神经网络中每一层的参数值来最小化成本函数值。诸如<strong class="il hj"> <em class="ik">梯度下降</em> </strong>的算法被用于以最小化成本函数的方式更新这些值。</p><h2 id="bb40" class="mc kb hi bd kc mj mk ml kg mm mn mo kk jx mp mq ko jy mr ms ks jz mt mu kw mv bi translated"><strong class="ak">反向传播</strong></h2><p id="ccfe" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jx lb iw ix jy lc ja jb jz ld je jf jg hb bi translated">既然我们已经测量了我们预测的误差(损失)，我们需要找到一种方法来<strong class="il hj">传播</strong>误差回来，并更新我们的权重和偏差。为了知道调整权重和偏差的适当量，我们需要知道损失函数相对于权重和偏差的<strong class="il hj">导数。</strong></p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mw"><img src="../Images/a4ee635214a9c28eb563256b1f8812d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PcKVZkdAxDE0qxebzS1sjw.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">梯度下降算法</figcaption></figure><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="975b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">上面的代码运行一个单层的反向传播步骤。它使用我们之前存储的缓存值来计算一个图层的sigmoid单元的梯度值。在激活缓存中，我们已经存储了该层的<strong class="il hj"> Z </strong>的值。使用这个值，我们将计算<strong class="il hj"> dZ </strong>，它是成本函数相对于给定神经元的线性输出的导数。</p><p id="edc5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">现在我们可以计算出<strong class="il hj"> dW </strong>、<strong class="il hj"> db </strong>和<strong class="il hj"> dA_prev、</strong>，它们分别是成本函数关于权重、偏差和先前激活的导数。我直接使用了代码中的公式。如果你不熟悉微积分，那么它可能看起来太复杂了(注意<em class="ik">关于反向传播的数学非常复杂)</em></p><p id="ea3a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">现在我们将实现整个神经网络的反向传播。函数<strong class="il hj"> backprop </strong>实现了相应的代码。这里，我们创建了一个字典，用于将渐变映射到每个层。我们将反向遍历模型并计算梯度。</p><p id="3d68" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">一旦我们循环了所有的层并计算了梯度，我们将把这些值存储在<strong class="il hj"> grads </strong>字典中并返回。</p><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="15ff" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">太好了！我们已经用numpy手动实现了所有层的反向传播。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es mx"><img src="../Images/ab4fe1b872145da58b44e304b5a24f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*3qepEjj4hypG_B9AztSOng.gif"/></div></figure><h2 id="8d3e" class="mc kb hi bd kc mj mk ml kg mm mn mo kk jx mp mq ko jy mr ms ks jz mt mu kw mv bi translated">更新参数</h2><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es my"><img src="../Images/1da48774451bc894276e8c5fbdd4c521.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*kWPgeDnujt6eYpoSIbG4mA.png"/></div></figure><p id="8823" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">最后，使用这些梯度值，我们将更新每个层的参数。函数<strong class="il hj"> update_parameters </strong>遍历所有层，更新参数并返回。</p><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="73e6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">最后，是时候把它们放在一起了。我们将创建一个名为<strong class="il hj">模型</strong>的函数来训练我们的神经网络。</p><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="mh mi l"/></div></figure><p id="abd0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">终于搞定了！我们从零开始构建了一个神经网络，没有使用任何深度学习库。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es mz"><img src="../Images/9933769b87a60ccdbb681eff01246318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*8vrj38tlT0YYULIBe-YPxg.gif"/></div></figure><h1 id="b831" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">结论</strong></h1><p id="0bd1" class="pw-post-body-paragraph ii ij hi il b im kz io ip iq la is it jx lb iw ix jy lc ja jb jz ld je jf jg hb bi translated">在这篇文章中，我展示了如何从头开始正确实现一个神经网络(<em class="ik">记住你知道神经网络的基础知识</em>)。网上有大量资源可以学习神经网络的重要部分，如<strong class="il hj">矢量化实现、反向传播、梯度下降和微积分</strong>。去探索它们吧。</p><p id="37b7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jx iv iw ix jy iz ja jb jz jd je jf jg hb bi translated">快乐深度学习！</p></div></div>    
</body>
</html>