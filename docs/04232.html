<html>
<head>
<title>Understanding K nearest neighbors.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解K个最近邻。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-k-nearest-neighbors-962f16b62a9f?source=collection_archive---------20-----------------------#2020-03-10">https://medium.com/analytics-vidhya/understanding-k-nearest-neighbors-962f16b62a9f?source=collection_archive---------20-----------------------#2020-03-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if"><p id="f6c6" class="ig ih hi bd ii ij ik il im in io ip dx translated">数据科学是讲述故事的艺术。人们需要将数据可视化来讲述它。你越形象化，故事(输出)就越好。</p></blockquote><figure class="ir is it iu iv iw er es paragraph-image"><div role="button" tabindex="0" class="ix iy di iz bf ja"><div class="er es iq"><img src="../Images/4e8dae6213c8563d86afe55b4cdf9fbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mLv09sf5jzJaUzoU"/></div></div><figcaption class="jd je et er es jf jg bd b be z dx translated">在<a class="ae jh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae jh" href="https://unsplash.com/@frostroomhead?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Rodion Kutsaev </a>拍照</figcaption></figure><p id="a10e" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">k最近邻用于分类和回归问题。想象一下你的数据的散点图，每个点明显被许多其他点包围着。因此，KNN所做的是，根据邻点，它会采取他们的统计模式，并预测你的测试值。</p><blockquote class="kf kg kh"><p id="06af" class="ji jj ki jk b jl jm jn jo jp jq jr js kj ju jv jw kk jy jz ka kl kc kd ke ip hb bi translated">对于分类，它采用模式；但是在回归的情况下，它取这些点的平均值，并预测你的结果。</p></blockquote><figure class="kn ko kp kq fd iw er es paragraph-image"><div class="er es km"><img src="../Images/a90563549db4d2085bf477a04a339b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*sA4bWreGrcRCnbOCLC7VkA.png"/></div><figcaption class="jd je et er es jf jg bd b be z dx translated">来源:<a class="ae jh" href="https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/03/introduction-k-neighbors-algorithm-clustering/</a></figcaption></figure><p id="5943" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">KNN是一个非参数模型。KNN被称为懒惰的学习者，因为它没有从输入中学到任何东西。相反，它会记住整个列车数据，当您放置一个测试点/数据时，它会根据相邻列车数据的位置预测其值。</p><p id="2336" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">两点之间的距离由“欧几里德”距离给出。欧几里德距离是通过(x2-x1)的平方+ (y2-y1)的平方根来计算的。</p><p id="fcd8" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">它是如何工作的？</p><p id="e9c7" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">K-NN工作可以在高层次上解释如下:</p><ul class=""><li id="3961" class="kr ks hi jk b jl jm jp jq jt kt jx ku kb kv ip kw kx ky kz bi translated">选择邻居的数量。</li><li id="c4de" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip kw kx ky kz bi translated">计算欧几里德距离。</li><li id="5793" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip kw kx ky kz bi translated">在近邻中，统计每个类别中需要分类的数据点的数量。</li><li id="760c" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip kw kx ky kz bi translated">将新数据点分配到特定的分类类别，对于该类别，相邻点的数量是最大的。</li></ul><p id="85ff" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated"><strong class="jk hj">KNN的利与弊:</strong></p><p id="7c3b" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated"><strong class="jk hj">优点:</strong></p><ul class=""><li id="963f" class="kr ks hi jk b jl jm jp jq jt kt jx ku kb kv ip kw kx ky kz bi translated">好用。</li><li id="ba9b" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip kw kx ky kz bi translated">更少的计算时间。</li><li id="cd65" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip kw kx ky kz bi translated">没有关于数据的假设。</li></ul><p id="de0a" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated"><strong class="jk hj">缺点:</strong></p><ul class=""><li id="39d7" class="kr ks hi jk b jl jm jp jq jt kt jx ku kb kv ip kw kx ky kz bi translated">准确性取决于数据的质量。</li><li id="22ee" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip kw kx ky kz bi translated">必须找到一个最佳k值。</li><li id="0618" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip kw kx ky kz bi translated">对边界处的数据点进行分类可能很困难。</li></ul><p id="c69d" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated"><strong class="jk hj"> KNN？可以用在哪里？</strong></p><p id="4543" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">KNN主要用于推荐系统和图像识别应用。</p><p id="d698" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">网飞和亚马逊Prime使用这种算法来推荐你的下一次购买，电影或书籍。</p><h1 id="c433" class="lf lg hi bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">使用哪种距离度量？是否有多个指标？</h1><p id="5791" class="pw-post-body-paragraph ji jj hi jk b jl md jn jo jp me jr js jt mf jv jw jx mg jz ka kb mh kd ke ip hb bi translated">最常用的距离度量是:</p><ul class=""><li id="c39d" class="kr ks hi jk b jl jm jp jq jt kt jx ku kb kv ip kw kx ky kz bi translated"><em class="ki">欧几里德距离(最流行)</em></li><li id="86f3" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip kw kx ky kz bi translated"><em class="ki">曼哈顿距离</em></li><li id="04aa" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip kw kx ky kz bi translated"><em class="ki">切比雪夫距离</em></li></ul><p id="8445" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated"><strong class="jk hj">欧几里德距离:</strong></p><p id="f949" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">一维中两点间的<strong class="jk hj">距离</strong>就是它们坐标差的绝对值。换句话说，它只是n维特征空间中一对样本p和q之间的<strong class="jk hj">距离</strong>度量。</p><p id="d04b" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated"><strong class="jk hj"> <em class="ki">曼哈顿距离:</em> </strong></p><p id="52ff" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">两个向量(或点)a和b之间的曼哈顿距离定义为向量维数上的∑<em class="ki">I</em>|<em class="ki">ai</em>bi|。它被称为曼哈顿距离，因为这是一辆汽车在城市中行驶的距离，在这个城市中，建筑物呈正方形，笔直的街道以直角相交。</p><p id="2501" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated"><strong class="jk hj"> <em class="ki">切比雪夫距离:</em> </strong></p><p id="0d71" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">在<a class="ae jh" href="https://en.wikipedia.org/wiki/Mathematics" rel="noopener ugc nofollow" target="_blank">数学</a>中，切比雪夫距离(或切比雪夫距离)、最大度量或<a class="ae jh" href="https://en.wikipedia.org/wiki/Lp_space" rel="noopener ugc nofollow" target="_blank"> L∞度量</a>是在<a class="ae jh" href="https://en.wikipedia.org/wiki/Vector_space" rel="noopener ugc nofollow" target="_blank">向量空间</a>上定义的<a class="ae jh" href="https://en.wikipedia.org/wiki/Metric_(mathematics)" rel="noopener ugc nofollow" target="_blank">度量</a>，其中两个<a class="ae jh" href="https://en.wikipedia.org/wiki/Coordinate_vector" rel="noopener ugc nofollow" target="_blank">向量</a>之间的<a class="ae jh" href="https://en.wikipedia.org/wiki/Distance" rel="noopener ugc nofollow" target="_blank">距离</a>是它们沿任意坐标维度的最大差异。<a class="ae jh" href="https://en.wikipedia.org/wiki/Chebyshev_distance#cite_note-2" rel="noopener ugc nofollow" target="_blank">【2】</a>因<a class="ae jh" href="https://en.wikipedia.org/wiki/Pafnuty_Chebyshev" rel="noopener ugc nofollow" target="_blank">帕夫纳蒂·切比雪夫</a>而得名。(维基百科)。</p><pre class="kn ko kp kq fd mi mj mk ml aw mm bi"><span id="54aa" class="mn lg hi mj b fi mo mp l mq mr">import pandas as pd<br/>import numpy as np<br/>import os<br/>import math</span></pre><p id="5bd7" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">加载数据并将数据分成X和y，然后进行训练和测试。稍后，我们将构建一个knn模型。</p><pre class="kn ko kp kq fd mi mj mk ml aw mm bi"><span id="17a1" class="mn lg hi mj b fi mo mp l mq mr">from sklearn import datasets<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.metrics import accuracy_score</span><span id="2814" class="mn lg hi mj b fi ms mp l mq mr">data = datasets.load_iris()<br/>X = data.data[:, :2]<br/>y = data.target</span><span id="a60f" class="mn lg hi mj b fi ms mp l mq mr">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)<br/>print(‘Shape of X_train: ‘, X_train.shape, ‘Shape of X_test: ‘, X_test.shape, ‘Shape of y_train: ‘, y_train.shape, ‘Shape of y_test: ‘, y_test.shape)<br/></span><span id="286d" class="mn lg hi mj b fi ms mp l mq mr">knn = KNeighborsClassifier(n_neighbors = 5)<br/>knn_model = knn.fit(X_train, y_train)<br/>knn_pred = knn_model.predict(X_test)<br/>knn_acc = accuracy_score(knn_pred, y_test)</span></pre><p id="2dab" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">这就是如何使用sklearn构建knn模型。需要进行预处理，这是一个完全不同的主题，我们将在我接下来的文章中讨论。</p><blockquote class="kf kg kh"><p id="2112" class="ji jj ki jk b jl jm jn jo jp jq jr js kj ju jv jw kk jy jz ka kl kc kd ke ip hb bi translated">希望你在读的时候学到了一些东西，就像我在写的时候学到的一样。</p></blockquote><p id="d902" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">请关注我即将发表的文章:</p><p id="7b28" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">系列:</p><ol class=""><li id="0a10" class="kr ks hi jk b jl jm jp jq jt kt jx ku kb kv ip mt kx ky kz bi translated">从不同类型的文件中提取文本。</li><li id="9752" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip mt kx ky kz bi translated">文本挖掘和文本清洗。</li><li id="f57b" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip mt kx ky kz bi translated">在提取的文本中进行单词搜索。</li></ol><p id="5824" class="pw-post-body-paragraph ji jj hi jk b jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke ip hb bi translated">文章:</p><ol class=""><li id="47e8" class="kr ks hi jk b jl jm jp jq jt kt jx ku kb kv ip mt kx ky kz bi translated">主题建模。</li><li id="e151" class="kr ks hi jk b jl la jp lb jt lc jx ld kb le ip mt kx ky kz bi translated">关键词提取。</li></ol></div></div>    
</body>
</html>