<html>
<head>
<title>Word Embeddings in NLP | Word2Vec | GloVe | fastText</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 中的单词嵌入| Word2Vec | GloVe | fastText</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/word-embeddings-in-nlp-word2vec-glove-fasttext-24d4d4286a73?source=collection_archive---------0-----------------------#2020-08-30">https://medium.com/analytics-vidhya/word-embeddings-in-nlp-word2vec-glove-fasttext-24d4d4286a73?source=collection_archive---------0-----------------------#2020-08-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/62b64d7ec0bd9139a486b391e0cfc486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*LdviucnshWgIIcQvhTTF-g.png"/></div></figure><p id="4173" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">单词嵌入是单词向量表示，其中具有相似意思的单词具有相似的表示。单词向量是表示单词的最有效方式之一。</p><p id="797e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在以前的博客(<a class="ae jk" rel="noopener" href="/analytics-vidhya/natural-language-processing-using-spacy-in-python-part-1-ac1bc4ad2b9c">第一部分</a>，<a class="ae jk" rel="noopener" href="/@AravindR07/nlp-using-spacy-and-topic-modeling-using-gensim-python-42c4574830d">第二部分</a>)中，我们已经讨论了向量，以及它们如何被用来以数学形式表示文本数据，并且所有 ML 算法的基础都依赖于这些表示。所以，让我们向前迈出一步，使用 ML 技术生成单词的向量表示，更好地<strong class="io hj">封装单词的</strong>含义。</p><p id="2f3b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">ML 在文本分析中最重要的应用之一。顾名思义，它根据我们使用的语料库创建单词的向量表示。</p><h1 id="e264" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">带有 Word2Vec 的单词向量</h1><p id="041e" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">使用深度学习来 NLP 任务已经被证明表现非常好。核心概念是将人类可读的句子输入<strong class="io hj">神经网络</strong>，以便模型能够<strong class="io hj">从中提取</strong>某种<strong class="io hj">信息</strong>。</p><p id="c335" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">理解如何<strong class="io hj">为神经网络</strong>预处理文本数据很重要。神经网络可以接受数字作为输入，但不能接受原始文本。因此，我们需要将这些单词转换成数字格式。</p><h1 id="904d" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">词向量</h1><p id="9c62" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">单词向量是比<strong class="io hj"><em class="ko">one hot encoded vectors</em></strong>更好的表示单词的方式(随着词汇量的增加，在表示文本向量时会导致大量的内存使用)。分配给每个单词的索引不包含任何语义。在一个热编码向量中，“狗”和“猫”的向量就像“狗”和“计算机”一样彼此接近，因此神经网络必须非常努力地理解每个单词，因为它们被视为完全孤立的实体。词向量的使用旨在解决这两个问题。</p><blockquote class="kp"><p id="2b4c" class="kq kr hi bd ks kt ku kv kw kx ky jj dx translated"><em class="kz">“单词向量比一个热编码向量消耗更少的空间，并且它们还保持单词的语义表示”。</em></p></blockquote><p id="3158" class="pw-post-body-paragraph im in hi io b ip la ir is it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj hb bi translated">在直接进入单词向量之前，一个重要的方面是，相似的单词比不相似的单词更频繁地出现在一起。</p><p id="64b2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">仅仅因为一个词出现在另一个词的附近，并不总是意味着它们有相似的意思，但是当我们考虑被发现在一起的词的频率时，我们发现相似意思的词被发现在一起。</p><h1 id="290c" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">Word2Vec</h1><p id="6cd8" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">在 word2vec 中有两种架构<strong class="io hj"> CBOW </strong>(连续字包)和<strong class="io hj"> Skip Gram </strong>。</p><p id="4c03" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">首先要做的是收集单词共现数据。我们需要一组数据来告诉我们哪些单词出现在某个单词附近。我们将使用称为 <strong class="io hj"> <em class="ko">的上下文窗口</em> </strong> <em class="ko">来做这件事。</em></p><p id="4e22" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">考虑一下，“深度学习很辛苦，也很好玩”。我们需要设置一个叫做<strong class="io hj"> <em class="ko">的窗口大小</em> </strong>。假设这种情况下是 2。我们所做的是迭代给定数据中的所有单词，在这种情况下，这只是一个句子，然后<strong class="io hj">考虑一个包围它的 word 窗口</strong>。这里，因为我们的窗口大小是 2，我们将考虑单词后面的 2 个单词和单词后面的 2 个单词，因此每个单词将得到 4 个与之相关的单词。我们将对数据中的每个单词都这样做，并收集单词对。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/1ef96c5e3dfa7e7520ccac11f63e7a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/0*i4l0ve-UbhaNmmfX.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">上下文单词示例</figcaption></figure><p id="ebdc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当我们<strong class="io hj"> <em class="ko">通过文本数据传递上下文窗口</em> </strong>时，我们找到所有<strong class="io hj">对目标和上下文</strong>单词，以形成目标单词和上下文单词格式的数据集。对于上面的句子，它看起来像这样:</p><p id="f30e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第一个窗口对:</strong>(深，学)，(深，是)</p><p id="cea9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第 2 个窗口对:</strong>(学，深)，(学，是)，(学，非常)</p><p id="d8dc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第 3 个窗口对:</strong>(是，深)，(是，学)，(是，很)，(是，硬)</p><p id="3cb4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">诸如此类。最后，我们的<strong class="io hj">目标单词与上下文单词数据集</strong>将看起来像这样:</p><blockquote class="lo lp lq"><p id="e415" class="im in ko io b ip iq ir is it iu iv iw lr iy iz ja ls jc jd je lt jg jh ji jj hb bi translated"><em class="hi"/></p></blockquote><p id="c020" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这可以认为是我们对于 word2vec 的<strong class="io hj"><em class="ko"/></strong>。</p><blockquote class="lo lp lq"><p id="0b07" class="im in ko io b ip iq ir is it iu iv iw lr iy iz ja ls jc jd je lt jg jh ji jj hb bi translated">在 skipgram 模型中，我们试图预测给定目标词的每个上下文词。</p></blockquote><p id="c2d2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们使用神经网络来完成这项预测任务。<strong class="io hj">神经网络的输入是上下文单词</strong>的一个热编码版本。因此，输入和输出层的大小是<strong class="io hj"> V </strong>(词汇计数)。这个神经网络只有一层在中间，隐藏层的<strong class="io hj">大小决定了我们希望在最后拥有的单词向量的大小。</strong></p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/672861c22a522c9af0d5883fd0698bc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*vm1_SO4Z6mp4tE4d.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">Word2Vec 中的 Skip Gram 架构</figcaption></figure><p id="fae8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于该神经网络总共有 3 层，因此该网络只有 2 个权重矩阵，<strong class="io hj"> W1 </strong>和<strong class="io hj"> W2 </strong>。<strong class="io hj"> W1 </strong>的尺寸为 10000*300，而<strong class="io hj"> W2 </strong>的尺寸为 300*10000。这两个权重矩阵将在计算单词向量时发挥重要作用。</p><p id="db50" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于我们从原始文本数据中收集的整个数据集，我们将把每一对传递给神经网络并对其进行训练。这里的神经网络试图猜测给定一个目标单词可以出现哪些上下文单词。训练好神经网络后，如果我们向神经网络中输入任何一个目标词，它都会给出一个向量输出，这个向量输出代表在给定词附近出现概率较高的词。</p><p id="499c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">对于 CBOW 来说，唯一的区别是我们试图在给定上下文单词</strong>的情况下预测目标单词，本质上我们只是反转跳过 gram 模型来获得 CBOW 模型。看起来是这样的:</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/4f48fd1c5c0938fa43591c1ef1c3cc9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/0*eLn2x-fdSsE4-Qno.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:<a class="ae jk" href="https://arxiv.org/pdf/1309.4168v1.pdf" rel="noopener ugc nofollow" target="_blank">利用语言间的相似性进行机器翻译</a>论文。</figcaption></figure><p id="01b2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里，当我们给出一组上下文单词的矢量表示时，我们将得到<strong class="io hj">最合适的目标单词，它将在那些单词</strong>的附近。</p><p id="f81b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">例如，如果我们给定句子:Deep _____ is very hard，其中[“Deep”、“is”、“very”、“hard”]代表上下文单词，神经网络应该有希望给出“Learning”作为<strong class="io hj">输出目标单词</strong>。在 CBOW 的情况下，这是神经网络试图训练的核心任务。</p><p id="fe3b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">单词向量有助于表示单词的语义— </strong>这是什么意思？</p><p id="0922" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这意味着我们可以对单词使用向量推理最著名的例子之一来自<a class="ae jk" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj"> Mikolov 的论文</strong> </a>，其中我们看到，如果我们使用单词 vectors 并执行(这里，我们使用<strong class="io hj"> V(word) </strong>来表示单词的向量表示)V(国王)-V(男人)+ V(女人)，得到的向量最接近 V(女王)。很容易看出为什么这是显著的——我们对这些单词的直观理解反映在单词的学习向量表示中。</p><p id="eda6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这使我们能够在我们的文本分析管道中添加更多的功能——拥有向量的直观语义表示将会不止一次地派上用场。</p><p id="a797" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> Word2Vec </strong>将大文本语料库作为其输入，并产生向量空间，通常具有数百维，语料库中的每个唯一单词被分配给空间中的相应向量。单词向量被定位到向量空间，使得语料库中共享共同上下文的单词在空间中彼此非常接近</p><h1 id="9eb4" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">履行</h1><p id="dccf" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated"><strong class="io hj"> Word2Vec </strong>模型使用<strong class="io hj">分级 softmax 进行训练</strong>，并将拥有 200 个特征，这意味着它具有分级输出，并在其最终层使用 softmax 函数。</p><pre class="lg lh li lj fd lw lx ly lz aw ma bi"><span id="9707" class="mb jm hi lx b fi mc md l me mf">Extract the data set in your colab or ipython notebook<br/>!wget <a class="ae jk" href="http://mattmahoney.net/dc/text8.zip" rel="noopener ugc nofollow" target="_blank">http://mattmahoney.net/dc/text8.zip</a></span><span id="7dcb" class="mb jm hi lx b fi mg md l me mf"># Unzip the data from the path you extracted the data to<br/>!unzip /content/text8.zip</span></pre><ul class=""><li id="3eff" class="mh mi hi io b ip iq it iu ix mj jb mk jf ml jj mm mn mo mp bi translated">对于<strong class="io hj"> <em class="ko"> word2vec 有一个庞大的参数列表。word 2 vec</em>T7】类。它们可以在文档页面<a class="ae jk" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</strong></li></ul><pre class="lg lh li lj fd lw lx ly lz aw ma bi"><span id="50d3" class="mb jm hi lx b fi mc md l me mf">from gensim.models import word2vec<br/>sentences = word2vec.Text8Corpus('/content/text8')<br/>model = word2vec.Word2Vec(sentences, size=200, hs=1) <br/># hs: If hirearchical softmax used for model training</span></pre><ul class=""><li id="8ead" class="mh mi hi io b ip iq it iu ix mj jb mk jf ml jj mm mn mo mp bi translated">打印模型:</li></ul><pre class="lg lh li lj fd lw lx ly lz aw ma bi"><span id="798b" class="mb jm hi lx b fi mc md l me mf">print(model)<br/>Output: Word2Vec(vocab=71290, size=200, alpha=0.025)</span></pre><ul class=""><li id="ae07" class="mh mi hi io b ip iq it iu ix mj jb mk jf ml jj mm mn mo mp bi translated">让我们试一个例子:</li></ul><pre class="lg lh li lj fd lw lx ly lz aw ma bi"><span id="bb33" class="mb jm hi lx b fi mc md l me mf">model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)[0]</span><span id="7af1" class="mb jm hi lx b fi mg md l me mf">o/p: ('queen', 0.5473929643630981)</span></pre><p id="7e65" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因为这是一个概率过程，所以获得的结果可能会有变化。例如，像“王座”或“帝国”这样的词可能会出现。</p><ul class=""><li id="1fc3" class="mh mi hi io b ip iq it iu ix mj jb mk jf ml jj mm mn mo mp bi translated">我们也可以使用<strong class="io hj"> most_similar_cosmul </strong>方法-</li></ul><pre class="lg lh li lj fd lw lx ly lz aw ma bi"><span id="62fa" class="mb jm hi lx b fi mc md l me mf">model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])</span></pre><ul class=""><li id="c31a" class="mh mi hi io b ip iq it iu ix mj jb mk jf ml jj mm mn mo mp bi translated">如果我们希望查找一个单词的矢量表示，我们需要做的就是:</li></ul><pre class="lg lh li lj fd lw lx ly lz aw ma bi"><span id="7115" class="mb jm hi lx b fi mc md l me mf">model.wv['computer']</span></pre><ul class=""><li id="46da" class="mh mi hi io b ip iq it iu ix mj jb mk jf ml jj mm mn mo mp bi translated">如果您想要将您的模型保存到磁盘并再次使用它，我们可以使用保存和加载功能来实现。</li></ul><pre class="lg lh li lj fd lw lx ly lz aw ma bi"><span id="0ace" class="mb jm hi lx b fi mc md l me mf">model.save('text8_model')<br/>model = word2vec.Word2Vec.load('text8_model')</span></pre><ul class=""><li id="df01" class="mh mi hi io b ip iq it iu ix mj jb mk jf ml jj mm mn mo mp bi translated">使用单词向量，我们可以确定列表中哪个单词离其他单词最远。Gensim 通过<strong class="io hj"> dosent_match </strong>方法实现该功能。</li></ul><pre class="lg lh li lj fd lw lx ly lz aw ma bi"><span id="5835" class="mb jm hi lx b fi mc md l me mf">model.wv.doesnt_match("breakfast cereal dinner lunch".split())</span><span id="14f0" class="mb jm hi lx b fi mg md l me mf">Output: “cereal” <br/># One word which dosent match the other in the list is picked out.</span></pre><ul class=""><li id="0d77" class="mh mi hi io b ip iq it iu ix mj jb mk jf ml jj mm mn mo mp bi translated">我们还可以使用该模型来了解语料库中的单词有多相似或不同:</li></ul><pre class="lg lh li lj fd lw lx ly lz aw ma bi"><span id="cba2" class="mb jm hi lx b fi mc md l me mf">model.wv.distance(‘man’, ‘woman’) <br/><strong class="lx hj">0.35839658414569464 # the result is self explanatory</strong></span></pre><ul class=""><li id="1d1c" class="mh mi hi io b ip iq it iu ix mj jb mk jf ml jj mm mn mo mp bi translated"><strong class="io hj"> CBOW </strong>速度更快，对更多<strong class="io hj">常用词有<strong class="io hj">更好的表现</strong>。</strong></li><li id="4bb2" class="mh mi hi io b ip mq it mr ix ms jb mt jf mu jj mm mn mo mp bi translated">Skipgram 适用于少量数据，并且被发现可以很好地表示罕见的单词。</li></ul><h1 id="d71e" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">手套</h1><p id="b986" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">Glove 基于单词上下文矩阵上的<strong class="io hj">矩阵分解技术。它首先<strong class="io hj">构造</strong>(单词 x 上下文)共现信息 ie 的<strong class="io hj">大矩阵</strong>。对于每个单词，你要计算我们在大型语料库中的某些上下文中看到这些单词的频率。</strong></p><p id="3b14" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了理解手套的工作原理，我们需要了解手套的两种主要制作方法</p><ol class=""><li id="59af" class="mh mi hi io b ip iq it iu ix mj jb mk jf ml jj mv mn mo mp bi translated"><strong class="io hj">全局矩阵分解</strong></li></ol><p id="5895" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在自然语言处理中，全局矩阵分解是使用线性代数的矩阵分解来减少大项频率矩阵的过程。这些矩阵通常表示文档中单词的出现或缺失。</p><p id="8875" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.<strong class="io hj">本地上下文窗口。</strong></p><p id="96d7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">局部上下文窗口方法是<strong class="io hj"> CBOW </strong>和<strong class="io hj"> Skip-Gram </strong>，上面已经解释过了。</p><p id="3622" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="ko"> Glove 是一种单词向量表示方法，其中对来自语料库</em> </strong>的聚集的全局单词-单词共现统计进行训练。这意味着像 word2vec 一样，它使用上下文来理解和创建单词表示。描述该方法的研究论文名为<em class="ko"> GloVe: </em> <a class="ae jk" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ko">单词表示的全局向量</em> </a>，非常值得一读，因为它在描述他们自己的方法之前描述了 LSA 和 Word2Vec 的一些缺点。</p><p id="b3b7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该论文的作者提到，与其学习<strong class="io hj">原始出现概率，</strong>不如学习这些同时出现概率的<strong class="io hj">比率</strong>更有用。嵌入被优化，因此<strong class="io hj">2 个向量的点积等于 2 个字彼此接近出现的次数的对数。</strong></p><p id="cc89" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">例如，如果两个单词“猫”和“狗”在彼此的上下文中出现，比如在文档语料库中的 10 个单词的窗口中出现 20 次，那么—</p><p id="b108" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">载体(猫)。向量(狗)=对数(10) </strong></p><p id="c95a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这就迫使模型在一个更加全球化的环境中对出现在它们附近的词的频率分布进行编码。</p><h1 id="4e81" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">快速文本</h1><p id="d99a" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">FastText 是 facebook AI research 开发的一种矢量表示技术。正如它的名字暗示了它执行相同任务的<strong class="io hj">快速而有效的方法</strong>并且因为它训练方法的本质，它最终也学习了<strong class="io hj">形态学</strong>的细节。</p><p id="bfe4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">FastText 是独一无二的，因为它可以<strong class="io hj">为未知单词或词汇表外的单词导出单词向量</strong>——这是因为通过考虑单词的形态特征，它可以<em class="ko">为未知单词创建</em>单词向量。由于<strong class="io hj">形态学指的是单词</strong>的结构或句法，FastText 倾向于更好地执行此类任务，<strong class="io hj"> word2vec </strong>更好地执行<strong class="io hj">语义任务</strong>。</p><p id="e287" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> FastText 对生僻字很好用</strong>。因此，即使一个单词在训练中没有被看到，它也可以被分解成 n 元语法来获得它的嵌入。</p><p id="4e53" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="ko"> Word2vec 和 GloVe 都无法为不在模型词典中的单词提供任何矢量表示</em>。这是这种方法的一个巨大优势。</p><p id="8354" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">脸书的实现可以在他们的<a class="ae jk" href="https://github.com/facebookresearch/fastText" rel="noopener ugc nofollow" target="_blank"> GitHub </a>库中找到。</p><h1 id="8773" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">应用程序</h1><ul class=""><li id="a965" class="mh mi hi io b ip kj it kk ix mw jb mx jf my jj mm mn mo mp bi translated">分析调查反馈。</li><li id="91d5" class="mh mi hi io b ip mq it mr ix ms jb mt jf mu jj mm mn mo mp bi translated">分析逐字评论。</li><li id="3574" class="mh mi hi io b ip mq it mr ix ms jb mt jf mu jj mm mn mo mp bi translated">音乐/视频推荐系统。</li></ul><h1 id="19e3" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">摘要</h1><p id="783e" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">在这篇文章中，文本分析的<strong class="io hj">主要创新之一，单词嵌入或单词向量被探究。这不仅有助于代表我们的<strong class="io hj">单词</strong>和<strong class="io hj">单词</strong>，也是看待我们单词的一种新方式。博客的一些主题参考了 deeplearning 的分层页面和 Bhargav Srinivasa-Desikan 的书<a class="ae jk" href="https://www.packtpub.com/product/natural-language-processing-and-computational-linguistics/9781788838535" rel="noopener ugc nofollow" target="_blank"> NLP 和 CL </a>。word2vec 的成功导致了各种单词嵌入方法的爆发，每种方法都有自己的优缺点。</strong></p><p id="5bd7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你喜欢这篇文章，请给它鼓掌。感谢您的阅读。</p><p id="3c0a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">继续学习</strong> ………..</p><h1 id="1938" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">谢谢你，谢谢你</h1></div></div>    
</body>
</html>