<html>
<head>
<title>Machine Learning: Support Vector Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:支持向量回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-support-vector-regression-181aea35bedf?source=collection_archive---------5-----------------------#2020-06-05">https://medium.com/analytics-vidhya/machine-learning-support-vector-regression-181aea35bedf?source=collection_archive---------5-----------------------#2020-06-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/1d5a0190677e2a2bb4a351aa01917a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*HE9W_o111Kmij-K8KJPzbQ.png"/></div></figure><p id="e714" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jk">我们即将讨论SVR(支持向量回归)。那么，我们来讨论一下回归实际上是什么？在统计建模中，</em> <strong class="io hj"> <em class="jk">回归分析</em> </strong> <em class="jk">是一组统计过程，用于估计因变量(通常称为“结果变量”)和一个或多个自变量(通常称为“预测变量”、“协变量”或“特征”)之间的关系。回归问题是分类问题的一般化，其中模型返回连续值输出，而不是来自有限集的输出。换句话说，回归模型估计连续值的多元函数。</em></p><blockquote class="jl jm jn"><p id="9c7c" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated"><strong class="io hj">支持向量机:</strong></p><p id="97bc" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated">如上所述，回归问题是分类问题的推广。因此，让我们来看一下支持向量机(SVMs ),它是一种分类算法，通过凸优化问题来解决二进制分类问题。优化问题处理寻找分离超平面的最大间隔，同时正确地分类尽可能多的训练点。支持向量机代表超平面和支持向量。SVM的分散解和良好的推广使它们适合于回归问题。</p></blockquote><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es jr"><img src="../Images/47489ec382644c1bfeb47c58756b7ebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*GohMl86hnRYjR00sPt4mqw.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated"><strong class="bd ka">支持向量机。来源:谷歌图片。</strong></figcaption></figure><blockquote class="jl jm jn"><p id="d22f" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated"><strong class="io hj">支持向量回归:</strong></p><p id="b0f6" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated">SVR(支持向量回归)没有SVM(支持向量机)流行。但是，支持向量回归已被证明是实值函数估计的有效工具。作为一种监督学习方法，SVR使用对称损失函数进行训练，该函数同等地惩罚高和低错误估计。</p><p id="6c9b" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated">如同在分类中一样，SVR的特征在于使用核、稀疏解、VC对支持向量的余量和数量的控制。</p><p id="1056" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated">SVM推广到SVR是通过在函数周围引入一个ɛ-insensitive区域来实现的，称为<strong class="io hj">ɛ-tube</strong>。该管道将优化问题转化为寻找最接近连续值函数的管道。试管试图平衡模型的<strong class="io hj">复杂度</strong>和<strong class="io hj">预测误差</strong>。更具体地说，关于管的创建，通过首先定义要最小化的凸ɛ-insensitive损失函数并找到包含大多数训练实例的最平坦的管，SVR被公式化为优化问题。超平面用支持向量表示，支持向量是位于管边界之外的训练样本。如在SVM，SVR中的支持向量是影响管子形状的最有影响力的实例，并且<strong class="io hj">训练</strong>和<strong class="io hj">测试</strong>数据被假定为<strong class="io hj">独立且同分布</strong>。</p></blockquote><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/b89fbba6e1b15bd56a6988d38e90c7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*2eqnnvoNU6xazK4J_jhi2w.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated"><strong class="bd ka">支持向量回归。来源:谷歌图片。</strong></figcaption></figure><blockquote class="jl jm jn"><p id="7b13" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated"><strong class="io hj"> <em class="hi"> SVR:数学和图形表示:</em> </strong></p><p id="44d0" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated">SVR问题的公式化通常最好从几何的角度来推导。处理两种类型的数据:</p><p id="d756" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated"><strong class="io hj">一维数据:</strong>被逼近的连续值函数的数学实现可以写成下面的等式</p></blockquote><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es kc"><img src="../Images/b2a9bf3a5991106a6223fb8650a67942.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*ZpPUTOq8PWeWEIhwhW-7Ag.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated"><strong class="bd ka">表示一维公式的方程。</strong></figcaption></figure><figure class="js jt ju jv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kd"><img src="../Images/a2408cb0c62198f3f35955474de04002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aHQJYOwPPsRSouhb1t0Tmw.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated"><strong class="bd ka">一维线性支持向量回归机。来源:谷歌图片。</strong></figcaption></figure><blockquote class="jl jm jn"><p id="3eaa" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated">SVR将这个函数近似问题公式化为一个优化问题，该优化问题试图找到以表面为中心的最窄的管，同时最小化预测误差，即预测输出和期望输出之间的距离。前一个条件产生等式中的目标函数，其中||w||是被逼近表面的法向量的大小:</p></blockquote><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es ki"><img src="../Images/1da962077095f82e5bde6b68f1dca235.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*oSYvVTeYdlIS_i9WmQSQOg.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated"><strong class="bd ka">最小化距离的公式。</strong></figcaption></figure><blockquote class="jl jm jn"><p id="8544" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated"><strong class="io hj">多维数据:</strong>我们将x增加1，并将b包含在w向量中，以简化数学符号，并获得以下等式中的多元回归-</p></blockquote><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/45ed6260af8e8f3179ff0f93fb01d863.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*4kdPpJZG98X4EO7jnhM_dA.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated"><strong class="bd ka">表示多维公式的方程。</strong></figcaption></figure><blockquote class="jl jm jn"><p id="f6a7" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated">这里，M是用于逼近函数的多项式的阶。随着向量w的大小增加，越来越多的wi是非零的，从而导致更高阶的解。水平线是0阶多项式解，与所需输出有很大偏差，因此误差较大。线性函数(一阶多项式)对一部分数据产生了更好的近似，但仍不足以拟合训练数据。6阶解决方案在函数平坦度和预测误差之间取得了最佳平衡。最高阶的解决方案没有误差，但是复杂度很高，并且很可能会使解决方案过度适应尚未看到的数据。w的大小充当正则项，并提供对解的平坦性的优化问题控制。</p></blockquote><figure class="js jt ju jv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kk"><img src="../Images/9d5f039ea1760bd4c11a746c57ab28a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gQ0dbo83nucoBwGyyqlaw.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated"><strong class="bd ka">解各种命令。来源:谷歌图片。</strong></figcaption></figure><blockquote class="jl jm jn"><p id="9552" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated"><strong class="io hj">损失函数:</strong>如果e减小，管的边界向内移动。因此，边界周围的数据点越多，表示支持向量越多。同样，增加ɛ将导致边界周围的点减少。因为它对噪声输入不太敏感，所以e不敏感区域使模型更健壮。可以采用几种损失函数，包括线性、二次和休伯ɛ.</p></blockquote><figure class="js jt ju jv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kl"><img src="../Images/873779a329e04836fde82ad778180373.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DeGMp4FrGeuqPHbAjo9YkQ.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated"><strong class="bd ka">损失函数:a)线性b)二次和c)胡伯。来源:谷歌图片。</strong></figcaption></figure><blockquote class="jl jm jn"><p id="c605" class="im in jk io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated"><strong class="io hj">不对称损失函数:</strong>采用类似于SVM中采用的软裕度方法，可以添加松弛变量ɛ、ɛ*以防止异常值。这些变量决定了管外可以容忍多少点。c是一个正则化参数——因此，对于这个多目标优化问题，它是一个可调参数，为最小化平坦度或误差提供了更大的权重。</p></blockquote><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es km"><img src="../Images/9d66d397785182da1913933c66d3c37d.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*kQ1V0DiJdY8iaq_a6PrKbw.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated"><strong class="bd ka">优化配方。</strong></figcaption></figure><figure class="js jt ju jv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kn"><img src="../Images/409ae14f695ec43053d2b0379208769d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5q7z--bdCbYWIGPYVIl9A.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated"><strong class="bd ka">支持向量回归。来源:谷歌图片。</strong></figcaption></figure><p id="8291" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="jk">结论:</em></strong><em class="jk">SVR的主要优点是它的计算复杂度不依赖于输入空间的维数。此外，它还具有很好的泛化能力和很高的预测精度。修正的SVR可以应用于对函数进行估计或严格限制低估是非常必要的情况。</em></p><blockquote class="ko"><p id="a703" class="kp kq hi bd kr ks kt ku kv kw kx jj dx translated">"感谢您在宝贵的时间阅读这个关于机器学习算法之一的博客."</p></blockquote></div></div>    
</body>
</html>