<html>
<head>
<title>Machine Learning: k-NN Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:k-NN算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-knn-algorithm-4130f799d697?source=collection_archive---------16-----------------------#2020-06-10">https://medium.com/analytics-vidhya/machine-learning-knn-algorithm-4130f799d697?source=collection_archive---------16-----------------------#2020-06-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b7069129d08ff7497f6d8283b2ba1343.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0cB_5tDxKVULg86jJCD3hw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">k-最近邻居。来源:谷歌图片。</strong></figcaption></figure><p id="2cda" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">k-最近邻(k-NN)算法简单且易于实现。它是一种监督学习技术，可以解决分类和回归问题。所以，出现的第一个问题是</p><p id="555b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">什么是监督学习？</em>T5】</strong></p><p id="998e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">监督学习说— </em> <strong class="ix hj"> <em class="jt">【教我】</em> </strong> <em class="jt">。这意味着学习通过标记的数据集。</em></p><blockquote class="ju jv jw"><p id="d62a" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">把模特想象成家庭中的孩子。这个孩子被教导猫、狗等的区别。之后儿童能够区分他/她看到的动物。类似地，用标记数据来教导模型，这有助于学习可用数据类别的特征之间的差异，并预测新观察的结果。</p></blockquote><p id="469b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="jt">【k-近邻】</em> </strong></p><blockquote class="ju jv jw"><p id="8228" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">k-最近邻算法(k-NN)一种用于分类和回归的非参数方法。在这两种情况下，输入由特征空间中k个最接近的训练数据组成。输出取决于<em class="hi"> k </em> -NN是用于分类还是回归:</p><p id="daed" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><strong class="ix hj">分类:</strong>在k-NN分类中，输出是一个类成员。一个对象通过其邻居的多个投票被分类，该对象被分配到其k个最近邻居中最常见的类别(k是正整数)，如果k= 1，则该对象被简单地分配到该单个最近邻居的类别。</p><p id="8184" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><strong class="ix hj">回归:</strong>在<em class="hi"> k-NN回归</em>中，输出为对象的值。该值是<em class="hi"> k个</em>最近邻居值的平均值。</p><p id="18e9" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">在我们学习机器学习技术的过程中，我们一定都听说过“<strong class="ix hj">CART”</strong>这个词。CART(分类和回归树)决策树算法可用于构建分类树(对分类响应变量进行分类)和回归树(预测连续响应变量)。k-最近邻方法也可以创建分类和回归模型。</p></blockquote><p id="9125" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="jt">k-最近邻(k-NN)分类:</em> </strong></p><blockquote class="ju jv jw"><p id="3dcc" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">在模式识别中，k-NN算法是一种基于特征空间中最接近的训练样本对对象进行分类的方法。k-NN是一种基于实例的学习，或称懒惰学习。当很少或没有关于数据分布的先验知识时，k-NN是基本和最简单的分类技术。该规则简单地在学习期间保留整个训练集，并向每个查询分配一个由训练集中其k个最近邻的多数标签表示的类。在这种方法中，每个样品都应与其周围的样品进行类似的分类。因此，如果样本的分类未知，则可以通过考虑其最近邻样本的分类来预测。</p></blockquote><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/f9a837983d88cce3211228523f76f271.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*n3BJiJFIvCOJpKfgsyUjMw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">1-NN决策规则:要点？被分配给左边的类。</figcaption></figure><p id="a1c9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">举例:- </em> </strong></p><p id="7012" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">“对于分成2类的一组样本，K=1时的k-NN判决规则。最近邻规则(NN)是K= 1时k-NN的最简单形式。”-仅使用一个已知样本对未知样本进行分类。这在图中清晰可见。</em></p><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es kf"><img src="../Images/5dbcfc6fd65c1ebc64cffaafc821516e.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*L7oa21ngIiZ1y4Bl1KUECw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">K-NN规则判定规则，其中K= 4: the point？也被分配到左边的类</strong></figcaption></figure><p id="f485" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">举例:- </em> </strong></p><p id="c81b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">“对于分成两类的一组样本，K=4时的k-NN判决规则。”-通过使用空间中一个以上的已知样本对未知样本进行分类。</em> K <em class="jt">设置为4，以便考虑最接近的四个样本对未知样本进行分类。他们中的三个属于同一个阶层，而只有一个属于另一个阶层。这在图中清晰可见。</em></p><blockquote class="ju jv jw"><p id="efa7" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">k-NN分类器的性能主要取决于K的选择以及所应用的距离度量。该估计受到邻域大小K的选择的敏感性的影响，因为局部区域的半径由第K个最近邻到查询的距离来确定，并且不同的K产生不同的条件类概率。如果K很小，由于数据稀疏和噪声、模糊或错误标记的点，局部估计往往很差。为了进一步平滑估计，我们可以增加K并考虑查询周围的大区域。不幸的是，K的大值容易使估计过度平滑，并且分类性能随着来自其他类的离群值的引入而降低。</p></blockquote><p id="9091" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="jt">k-近邻回归:</em> </strong></p><blockquote class="ju jv jw"><p id="b397" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">上述方法可用于回归，只需将对象的值指定为其K个最近邻居值的平均值。对邻居的贡献进行加权是有用的，这样较近的邻居比较远的邻居对平均值的贡献更大。回归问题涉及在给定一组自变量的情况下预测因变量的结果。</p></blockquote><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kg"><img src="../Images/5131bfa4fdf9e851d1eb0f4f9c1bafef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28Su7N4t7sfZWHsBLQnXpQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">回归的k-NN决策规则。来源:谷歌图片。</strong></figcaption></figure><blockquote class="ju jv jw"><p id="f427" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><strong class="ix hj">举例- </strong></p><p id="c877" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">让我们考虑上图，从1-最近邻开始。在这种情况下，我们搜索示例集(绿色方块)并定位最接近新观察值x的一个。对于这种特殊情况，这恰好是x4。于是，x4的结果(即y4)被认为是X的结果(即Y)的答案。因此，对于1-最近邻，我们可以写成:Y = y4。</p><p id="7098" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">现在考虑2-最近邻法。在这种情况下，我们定位最接近X的前两个点，这两个点恰好是y3和y4。取其结果的平均值，则Y的解为:</p></blockquote><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es kh"><img src="../Images/52866375edfa8f23811bf4c34c10f489.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*4teMo_TzIW6Ss4ZELoFwoA.png"/></div></figure><blockquote class="ju jv jw"><p id="54bb" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">类似地，如果K的值增加，则预测值是所有最近邻的平均值。</p></blockquote><blockquote class="ki"><p id="f4de" class="kj kk hi bd kl km kn ko kp kq kr js dx translated"><strong class="ak">结论:</strong>在了解了k-NN算法之后，让我们来讨论一下它的优缺点。</p></blockquote><blockquote class="ju jv jw"><p id="f2ab" class="iv iw jt ix b iy ks ja jb jc kt je jf jx ku ji jj jy kv jm jn jz kw jq jr js hb bi translated">优点: k-NN有几个主要优点:简单、有效、直观，在很多领域都有很好的分类性能。它对有噪声的训练数据是鲁棒的，并且如果训练数据很大，它是有效的。</p><p id="2ab4" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><strong class="ix hj">缺点:</strong>尽管有上述优点，k-N还是有一些局限性。当训练集很大时，KNN的运行时性能会很差。它对不相关或冗余的特征非常敏感，因为所有特征都有助于相似性，从而有助于分类。通过仔细的特征选择或特征加权，这是可以避免的。该方法的另外两个缺点是:</p><p id="eb09" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">1.基于距离的学习不清楚使用哪种类型的距离和使用哪种属性来产生最佳结果。</p><p id="804f" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">2.计算成本相当高，因为我们需要计算每个查询实例到所有训练样本的距离。</p></blockquote><blockquote class="ki"><p id="6a1d" class="kj kk hi bd kl km kn ko kp kq kr js dx translated">"感谢您抽出宝贵的时间来阅读这篇关于机器学习算法的博客."</p></blockquote></div></div>    
</body>
</html>