<html>
<head>
<title>Neural Machine Translation using Bahdanau Attention Mechanism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Bahdanau注意机制的神经机器翻译</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-machine-translation-using-bahdanau-attention-mechanism-d496c9be30c3?source=collection_archive---------0-----------------------#2020-04-24">https://medium.com/analytics-vidhya/neural-machine-translation-using-bahdanau-attention-mechanism-d496c9be30c3?source=collection_archive---------0-----------------------#2020-04-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="3f88" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">目录</h1><ol class=""><li id="5c72" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">介绍</li><li id="8f64" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">神经机器翻译</li><li id="646e" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">NMT使用Seq2Seq模型时没有注意</li><li id="ac5d" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">注意机制</li><li id="1423" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">巴赫达瑙注意机制</li><li id="9d8f" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">结果</li><li id="a0cf" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">参考</li></ol><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es ka"><img src="../Images/c078ebaad8380c31ccd16faa5cec9f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TBAccQp0ktGuXkz2Ih_wFQ.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">来源- <a class="ae kq" href="https://www.google.com/search?q=machine+translation&amp;tbm=isch&amp;sxsrf=ALeKk00cIM7hcz3bU25VVJT94fQccxFYPg:1587228333785&amp;source=lnms&amp;sa=X&amp;ved=0ahUKEwigxoyotvLoAhWk7nMBHSV4DtoQ_AUIECgD&amp;biw=1536&amp;bih=754&amp;dpr=1.25#imgrc=My1KhSBjNzxv9M&amp;imgdii=5jybvdZEugIQxM" rel="noopener ugc nofollow" target="_blank">页面</a></figcaption></figure><h1 id="19c0" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="4433" class="pw-post-body-paragraph kr ks hi jf b jg jh kt ku ji jj kv kw jk kx ky kz jm la lb lc jo ld le lf jq hb bi translated">机器翻译概念的起源可以追溯到20世纪30年代，当时Peter Troyanskii提出了第一台用于从一种语言翻译到另一种语言时选择和打印单词的机器。从那以后，机器翻译领域有了很多发展。从自动将60个俄语句子翻译成英语的IBM 701计算机到可以将几乎任何句子翻译成任何语言的Google Translate，我们走过了漫长的道路。下面是1950年到2015年机器翻译从基于规则的机器翻译到神经机器翻译的演变图。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lg"><img src="../Images/71208e0765ced539c51b527cc0a88636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZXv7fBGpG2au_RcJ34A4Q.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">来源— <a class="ae kq" href="https://www.freecodecamp.org/news/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5/" rel="noopener ugc nofollow" target="_blank">页面</a></figcaption></figure><h1 id="f726" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">神经机器翻译(NMT)</h1><p id="4aaf" class="pw-post-body-paragraph kr ks hi jf b jg jh kt ku ji jj kv kw jk kx ky kz jm la lb lc jo ld le lf jq hb bi translated">神经机器翻译是最近提出的机器翻译方法。与传统的统计机器翻译不同，神经机器翻译旨在建立一个单一的神经网络，可以联合调整以最大限度地提高翻译性能。最近提出的神经机器翻译模型通常属于编码器-解码器家族，并将源句子编码为固定长度的向量，解码器从该向量生成翻译。</p><h1 id="40df" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">NMT利用序列对序列模型不加注意</h1><p id="9183" class="pw-post-body-paragraph kr ks hi jf b jg jh kt ku ji jj kv kw jk kx ky kz jm la lb lc jo ld le lf jq hb bi translated">具有递归神经网络的编码器-解码器结构成为神经机器翻译(NMT)的有效方法。该方法的主要优点是能够直接在源和目标句子上训练单个端到端模型，并且能够处理可变长度的文本输入和输出序列。</p><p id="ef17" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">下图是基于RNN的编码器-解码器架构的NMT。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es lm"><img src="../Images/d6db9a6cc517b9c49e02dc60ca7197b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*loMi_H0MsT0GqcjkOS9l3g.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">编码器-解码器架构来源- <a class="ae kq" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/" rel="noopener ugc nofollow" target="_blank">页面</a></figcaption></figure><h2 id="9b8e" class="ln ig hi bd ih lo lp lq il lr ls lt ip jk lu lv it jm lw lx ix jo ly lz jb ma bi translated">编码器</h2><p id="3362" class="pw-post-body-paragraph kr ks hi jf b jg jh kt ku ji jj kv kw jk kx ky kz jm la lb lc jo ld le lf jq hb bi translated">编码器从源语言中读取输入的句子，并将这些信息编码到称为隐藏状态的向量中。我们只取最后一个RNN的隐藏状态，并丢弃编码器输出。</p><h2 id="7802" class="ln ig hi bd ih lo lp lq il lr ls lt ip jk lu lv it jm lw lx ix jo ly lz jb ma bi translated">解码器</h2><p id="7a7b" class="pw-post-body-paragraph kr ks hi jf b jg jh kt ku ji jj kv kw jk kx ky kz jm la lb lc jo ld le lf jq hb bi translated">解码器将最后一个编码器RNN单元的隐藏状态作为其第一个RNN单元的初始状态，并将<start>标记作为初始输入，以产生输出序列。我们使用教师强制来更快更有效地训练解码器。教师强制是一种快速有效地训练递归神经网络模型的方法，该模型使用来自先前时间步骤的基础事实作为输入。在这种方法中，给出正确的答案作为训练的开始，这样模型将快速有效地进行训练。</start></p><p id="6298" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">你可以从这个<a class="ae kq" href="https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7" rel="noopener" target="_blank">博客</a>中读到这个模型的详细解释。对于LSTM和GRU等RNN变体，我建议查看<a class="ae kq" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解LSTM网络</a>和<a class="ae kq" href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be" rel="noopener" target="_blank">了解GRU网络</a></p><h2 id="a945" class="ln ig hi bd ih lo lp lq il lr ls lt ip jk lu lv it jm lw lx ix jo ly lz jb ma bi translated">这种模式有什么问题？</h2><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es mb"><img src="../Images/f03c73829b6b58f2306118603b2e492c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mz6jmCQA0OsX0yrVpOEVyg.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">Seq2Seq模型的缺点来源— <a class="ae kq" href="https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/" rel="noopener ugc nofollow" target="_blank">页面</a></figcaption></figure><p id="46b3" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">来自编码器的信息将变得越来越不相关，因为编码器的状态仅被传递到解码器的第一个RNN单元。在这个模型中，输出序列很大程度上取决于编码器最后一个RNN单元的隐藏状态。这使得模型很难处理长序列。所以，这个系统的明显缺陷是它不能记住更长的序列。输出序列严重依赖于编码器最终输出中隐藏状态定义的上下文，使得模型处理长句具有挑战性。</p><h1 id="e7d9" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">注意机制</h1><p id="8742" class="pw-post-body-paragraph kr ks hi jf b jg jh kt ku ji jj kv kw jk kx ky kz jm la lb lc jo ld le lf jq hb bi translated">英语单词“Attention”的意思是注意到某人或某事；认为某人或某事令人兴奋或重要。注意机制是基于这一确切的概念，即在预测序列模型的输出时，将注意力集中在重要的因素上。</p><p id="1a8b" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">现在，让我们来看看这个帖子。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es mc"><img src="../Images/5802d7ff72fd79fea6f0c9296c27b618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*fj4gGQJenQfh0tXz58Oz_Q.jpeg"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">来源- <a class="ae kq" href="https://imgflip.com/tag/you+read+that+wrong" rel="noopener ugc nofollow" target="_blank">页面</a></figcaption></figure><p id="f7fa" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">我们大多数人(不包括一些聪明的人)在生活中的某个时候都曾被这种迷因所迷惑。这种模因基于一个简单的心理学事实，即当人们阅读一个句子时，我们一起而不是单独地解释单词和句子，也就是说，我们不是顺序阅读，而是在阅读时一起关注两个或更多的单词。(<a class="ae kq" href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0" rel="noopener" target="_blank">来源</a>)</p><p id="f5c9" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">在无注意的序列模型中，我们顺序地处理和预测句子。然而，在NMT，从一种语言到另一种语言的单词预测可能依赖于句子中该特定单词之前或之后的单词，这是可能的并且是非常可能的。</p><p id="de7d" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">下图显示了单词的预测如何依赖于句子中的两个或多个单词。在下面的gif中，细的链接对单词预测的贡献较低，而粗的链接的贡献较高。我们可以观察到，目标序列中的大部分预测单词都依赖于源序列中相应单词前后的单词。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es md"><img src="../Images/316f94c16521ddab4d13d5ac1a562da6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/1*SmAeISN5_u_gjiLPUA8qIA.gif"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">注意机制(来源— <a class="ae kq" href="https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/" rel="noopener ugc nofollow" target="_blank">页面</a></figcaption></figure><p id="bc96" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">所以，我想我们现在明白了为什么我们需要关注。</p><p id="f199" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">现在，让我们来理解Bahdanau注意机制。</p><p id="c6c6" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">我们将同时实现每一步的代码。你可以从这个<a class="ae kq" href="http://www.manythings.org/anki/ita-eng.zip" rel="noopener ugc nofollow" target="_blank">链接</a>下载数据集。该数据集包含意大利语-英语翻译的336614个数据点。</p><p id="07a5" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">数据准备可参考此<a class="ae kq" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention#translate" rel="noopener ugc nofollow" target="_blank">页面</a>。</p><h1 id="f57f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">巴赫达瑙注意机制</h1><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es me"><img src="../Images/b16c444ab1895c59b78129bb63a3d830.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gHyQZ4bFEoDkTbMZkptuYA.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">Bahdanau注意机制(来源- <a class="ae kq" href="https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a" rel="noopener" target="_blank">页面</a></figcaption></figure><p id="72b3" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">Bahdanau注意力也被称为附加注意力，因为它执行编码器状态和解码器状态的线性组合。现在，让我们来理解巴赫达瑙提出的机制。</p><h2 id="98d1" class="ln ig hi bd ih lo lp lq il lr ls lt ip jk lu lv it jm lw lx ix jo ly lz jb ma bi translated">伪代码:</h2><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="2e35" class="ln ig hi mg b fi mk ml l mm mn">Notations:</span><span id="03cd" class="ln ig hi mg b fi mo ml l mm mn"><strong class="mg hj">FC</strong> = Fully connected (dense) layer,</span><span id="3920" class="ln ig hi mg b fi mo ml l mm mn"><strong class="mg hj">EO</strong> = Encoder output,</span><span id="0b62" class="ln ig hi mg b fi mo ml l mm mn"><strong class="mg hj">H</strong> = hidden state,</span><span id="0c19" class="ln ig hi mg b fi mo ml l mm mn"><strong class="mg hj">X</strong> = input to the decoder.</span><span id="1dcf" class="ln ig hi mg b fi mo ml l mm mn"><strong class="mg hj">* score = FC(tanh(FC(EO) + FC(H)))</strong></span><span id="a89e" class="ln ig hi mg b fi mo ml l mm mn"><strong class="mg hj">* attention weights = softmax(score, axis = 1)</strong>. Softmax by default is applied on the last axis but here we want to apply it on the <em class="mp">1st axis</em>, since the shape of score is <em class="mp">(batch_size, max_length, hidden_size)</em>. <!-- -->Max_length<!-- --> is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.</span><span id="6740" class="ln ig hi mg b fi mo ml l mm mn"><strong class="mg hj">* context vector = sum(attention weights * EO, axis = 1)</strong>. Same reason as above for choosing axis as 1.</span><span id="7bac" class="ln ig hi mg b fi mo ml l mm mn"><strong class="mg hj">* embedding output</strong><strong class="mg hj"> = The input to the decoder</strong> X is passed through an embedding layer.</span><span id="8815" class="ln ig hi mg b fi mo ml l mm mn"><strong class="mg hj">* merged vector = concat(embedding output, context vector)</strong></span><span id="b437" class="ln ig hi mg b fi mo ml l mm mn">This merged vector is then given to the GRU (Source - <a class="ae kq" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">Page</a>)</span></pre><p id="1307" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated"><strong class="jf hj">步骤1:生成编码器隐藏状态</strong></p><p id="78c0" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">我们可以使用RNN的任何变体，如LSTM或GRU来编码输入序列。对于传递的每个输入，每个单元格都将产生一个隐藏状态。现在，与序列到序列模型不同，我们将所有RNN单元产生的所有隐藏状态传递到下一步。</p><p id="d52a" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">可以使用以下代码在Tensorflow中构建编码器。</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="4235" class="ln ig hi mg b fi mk ml l mm mn">class Encoder(tf.keras.Model):<br/>    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):<br/>        super(Encoder, self).__init__()<br/>        self.batch_sz = batch_sz<br/>        self.enc_units = enc_units<br/>        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)<br/>        self.gru = tf.keras.layers.GRU(self.enc_units,<br/>                                       return_sequences=True,<br/>                                       return_state=True,                                    recurrent_initializer='glorot_uniform')</span><span id="8cfa" class="ln ig hi mg b fi mo ml l mm mn">def call(self, x, hidden):<br/>        x = self.embedding(x)<br/>        output, state = self.gru(x, initial_state = hidden)<br/>        return output, state</span><span id="200f" class="ln ig hi mg b fi mo ml l mm mn">def initialize_hidden_state(self):<br/>        return tf.zeros((self.batch_sz, self.enc_units))</span></pre><p id="1aaa" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">如果我们查看Bahdanau注意机制的解码器的图表，我们可以看到所有的编码器隐藏状态以及解码器隐藏状态都用于生成上下文向量。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es mq"><img src="../Images/f45390650c44f4e1048d7f7cfda73ce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-2Lg8RuKBGo8aK_zrJk7g.png"/></div></div></figure><p id="4fc6" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated"><strong class="jf hj">步骤2:计算对齐向量</strong></p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es mr"><img src="../Images/94c089eaad23b02452e714123efc8758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M4R5MeUZINDXtycT6qMbKw.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">Bahdanau注意力的得分函数</figcaption></figure><p id="7daf" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">现在，我们必须计算比对分数。它是在前一个解码器隐藏状态和每个编码器隐藏状态之间计算的。每个编码器隐藏状态的校准分数被组合并表示在一个向量中，然后被软最大化<strong class="jf hj">。</strong>比对向量是与源序列具有相同长度的向量。它的每个值都是源序列中相应单词的得分(或概率)。对齐向量对编码器的输出进行加权。有了这些权重，解码器就可以决定在每个时间步长关注什么。</p><p id="fc59" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated"><strong class="jf hj">第三步:计算上下文向量</strong></p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es ms"><img src="../Images/0cc23fbbc585da1b45b212f8d9eee6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1t5kzdM5UWc2DJJ8rtlqlw.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">巴哈马注意力方程式</figcaption></figure><p id="4c2c" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">编码器隐藏状态和它们各自的对齐分数(上述等式中的注意力权重)相乘以形成上下文向量。上下文向量用于计算解码器的最终输出。</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="5404" class="ln ig hi mg b fi mk ml l mm mn">class BahdanauAttention(tf.keras.layers.Layer):<br/>    def __init__(self, units):<br/>        super(BahdanauAttention, self).__init__()<br/>        self.W1 = tf.keras.layers.Dense(units)<br/>        self.W2 = tf.keras.layers.Dense(units)<br/>        self.V = tf.keras.layers.Dense(1)</span><span id="2a1d" class="ln ig hi mg b fi mo ml l mm mn">def call(self, query, values):<br/>        query_with_time_axis = tf.expand_dims(query, 1)<br/>        score = self.V(tf.nn.tanh(<br/>            self.W1(query_with_time_axis) + self.W2(values)))<br/>        attention_weights = tf.nn.softmax(score, axis=1)<br/>        context_vector = attention_weights * values<br/>        context_vector = tf.reduce_sum(context_vector, axis=1)<br/>        return context_vector, attention_weights</span></pre><p id="6e35" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated"><strong class="jf hj">步骤4:解码输出</strong></p><p id="8004" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">在前一步骤中获得的上下文向量与前一解码器输出连接，并馈入解码器RNN单元，以产生新的隐藏状态。然后，这个过程从步骤2开始再次重复。对于解码器的每个时间步长，该过程重复进行，直到产生一个“<end>”标记或输出超过指定的最大长度。时间步长的最终输出是通过将新的隐藏状态传递给线性层来获得的，该线性层充当分类器来给出下一个预测单词的概率分数。</end></p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="d394" class="ln ig hi mg b fi mk ml l mm mn">class Decoder(tf.keras.Model):<br/>    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):<br/>        super(Decoder, self).__init__()<br/>        self.batch_sz = batch_sz<br/>        self.dec_units = dec_units<br/>        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)<br/>        self.gru = tf.keras.layers.GRU(self.dec_units,<br/>                                       return_sequences=True,<br/>                                       return_state=True,<br/>                                       recurrent_initializer='glorot_uniform')<br/>        self.fc = tf.keras.layers.Dense(vocab_size)<br/>        # used for attention<br/>        self.attention = BahdanauAttention(self.dec_units)</span><span id="4299" class="ln ig hi mg b fi mo ml l mm mn">def call(self, x, hidden, enc_output):<br/>        context_vector, attention_weights = <br/>                                 self.attention(hidden, enc_output)<br/>        x = self.embedding(x)<br/>        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)<br/>        output, state = self.gru(x)<br/>        output = tf.reshape(output, (-1, output.shape[2]))<br/>        x = self.fc(output)<br/>        return x, state, attention_weights</span></pre><p id="c17b" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated"><strong class="jf hj">步骤5:使用编码器-解码器模型训练数据集</strong></p><p id="a62f" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">首先，我们将定义一个优化器和一个损失函数。</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="2c24" class="ln ig hi mg b fi mk ml l mm mn">optimizer = tf.keras.optimizers.Adam()<br/>loss_object = tf.keras.losses.SparseCategoricalCrossentropy(<br/>    from_logits=True, reduction='none')</span><span id="2ba6" class="ln ig hi mg b fi mo ml l mm mn">def loss_function(real, pred):<br/>    mask = tf.math.logical_not(tf.math.equal(real, 0))<br/>    loss_ = loss_object(real, pred)<br/>    mask = tf.cast(mask, dtype=loss_.dtype)<br/>    loss_ *= mask<br/>    return tf.reduce_mean(loss_)</span></pre><p id="cb02" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">现在，为了进行培训，我们将实现以下内容。</p><p id="304b" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">通过编码器传递输入和初始隐藏状态，编码器将返回编码器输出序列和编码器隐藏状态。编码器隐藏状态、编码器输出和解码器输入被传递给解码器。在第一个时间步长，解码器将'<start>'作为输入。解码器返回解码器隐藏状态和预测字作为输出。我们使用教师强制进行训练，在每一个时间步将实际单词传递给解码器。然后，计算梯度下降，将其应用到优化器和反向传播。(来源- <a class="ae kq" href="https://towardsdatascience.com/implementing-neural-machine-translation-with-attention-using-tensorflow-fc9c6f26155f" rel="noopener" target="_blank">第</a>页)</start></p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="8996" class="ln ig hi mg b fi mk ml l mm mn">def train_step(inp, targ, enc_hidden):<br/>    loss = 0<br/>    with tf.GradientTape() as tape:<br/>        enc_output, enc_hidden = encoder(inp, enc_hidden)<br/>        dec_hidden = enc_hidden<br/>        dec_input = tf.expand_dims([targ_lang.word_index['&lt;start&gt;']] * BATCH_SIZE, 1)<br/>        # Teacher forcing - feeding the target as the next input<br/>        for t in range(1, targ.shape[1]):<br/>            # passing enc_output to the decoder<br/>            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)<br/>            loss += loss_function(targ[:, t], predictions)<br/>            # using teacher forcing<br/>            dec_input = tf.expand_dims(targ[:, t], 1)<br/>    batch_loss = (loss / int(targ.shape[1]))<br/>    variables = encoder.trainable_variables + decoder.trainable_variables<br/>    gradients = tape.gradient(loss, variables)<br/>    optimizer.apply_gradients(zip(gradients, variables))<br/>    return batch_loss</span></pre><p id="cae8" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">多纪元训练。</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="d720" class="ln ig hi mg b fi mk ml l mm mn">EPOCHS = 4<br/>for epoch in range(EPOCHS):<br/>    start = time.time()<br/>    enc_hidden = encoder.initialize_hidden_state()<br/>    total_loss = 0<br/>    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):<br/>        batch_loss = train_step(inp, targ, enc_hidden)<br/>        total_loss += batch_loss<br/>        if batch % 1000 == 0:<br/>            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,<br/>                                                       batch,<br/>                                                      batch_loss.numpy()))<br/>    # saving (checkpoint) the model every 2 epochs<br/>    if (epoch + 1) % 2 == 0:<br/>        checkpoint.save(file_prefix = checkpoint_prefix)<br/>    print('Epoch {} Loss {:.4f}'.format(epoch + 1,<br/>                                      total_loss / steps_per_epoch))<br/>    print('Time taken for 1 epoch {} sec\n'.format(time.time() - start))</span></pre><p id="5a40" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">该模型在由Google Colab提供的Tesla K80 GPU上进行训练。训练4个纪元大概用了2400秒。在第4个时期结束时，损失为0.0837。</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es mt"><img src="../Images/1bacd7f1a653fc30e79aab17ce163613.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sFcjqTJjJwQIcceNioIwPg.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">编解码器模型的训练</figcaption></figure><p id="6e7c" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated"><strong class="jf hj">第六步:预测</strong></p><p id="c56e" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">在这个阶段，我们不使用老师强迫。相反，我们将前一时间步的预测字作为输入传递给解码器。我们还存储了注意力权重，以便它们可以用于绘制注意力图。</p><p id="1d87" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">对于评估，首先，我们对句子进行预处理。然后，我们使用在数据准备期间创建的tokenizer对象创建令牌。在传递和创建输入张量之后，我们初始化一个隐藏状态，它被初始化为零，并与输入向量一起传递给编码器。在此之后，编码器隐藏状态和'<start>'标记被传递给解码器。然后，我们使用解码器输入、隐藏状态和上下文向量找到具有最大概率的预测id，并且我们存储注意力权重。现在，我们将predicted_id转换为word，并将其附加到结果字符串中。这一直持续到遇到'<end>'标签或达到最大目标序列。</end></start></p><p id="38a2" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">函数来评估一个句子。</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="de50" class="ln ig hi mg b fi mk ml l mm mn">def evaluate(sentence):<br/>    attention_plot = np.zeros((max_length_targ, max_length_inp))<br/>    sentence = preprocess_sentence(sentence)<br/>    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]<br/>    inputs =  tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp,padding='post')    <br/>    inputs = tf.convert_to_tensor(inputs)<br/>    result = ''<br/>    hidden = [tf.zeros((1, units))]<br/>    enc_out, enc_hidden = encoder(inputs, hidden)<br/>    dec_hidden = enc_hidden<br/>    dec_input = tf.expand_dims([targ_lang.word_index['&lt;start&gt;']], 0)<br/>    for t in range(max_length_targ):<br/>        predictions, dec_hidden, attention_weights =     decoder(dec_input,dec_hidden,enc_out)<br/>        # storing the attention weights to plot later on<br/>        attention_weights = tf.reshape(attention_weights, (-1, ))<br/>        attention_plot[t] = attention_weights.numpy()<br/>        predicted_id = tf.argmax(predictions[0]).numpy()<br/>        result += targ_lang.index_word[predicted_id] + ' '<br/>        if targ_lang.index_word[predicted_id] == '&lt;end&gt;':<br/>            return result, sentence, attention_plot<br/>        # the predicted ID is fed back into the model<br/>        dec_input = tf.expand_dims([predicted_id], 0)<br/>    return result, sentence, attention_plot</span></pre><p id="f3d7" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">用于绘制注意力权重的函数。</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="77e0" class="ln ig hi mg b fi mk ml l mm mn"># function for plotting the attention weights<br/>def plot_attention(attention, sentence, predicted_sentence):<br/>    fig = plt.figure(figsize=(10,10))<br/>    ax = fig.add_subplot(1, 1, 1)<br/>    ax.matshow(attention, cmap='viridis')<br/>    fontdict = {'fontsize': 14}<br/>    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)<br/>    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)  <br/>    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))<br/>    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))<br/>    plt.show()</span></pre><p id="1530" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">翻译句子的功能</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="4752" class="ln ig hi mg b fi mk ml l mm mn">def translate(sentence):<br/>    result, sentence, attention_plot = evaluate(sentence)<br/>    print('Input: %s' % (sentence))<br/>    print('Predicted translation: {}'.format(result))<br/>    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]<br/>    plot_attention(attention_plot, sentence.split(' '),    result.split(' '))<br/>    return result</span></pre><h1 id="a38a" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结果</h1><p id="058c" class="pw-post-body-paragraph kr ks hi jf b jg jh kt ku ji jj kv kw jk kx ky kz jm la lb lc jo ld le lf jq hb bi translated"><strong class="jf hj">来自训练模型的预测:</strong></p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es mu"><img src="../Images/667cee3c36286c1e9f3f71986cf08b3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tcm0x69QN5eAm0Xoq9oj6g.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">来自编码器-解码器模型的预测结果</figcaption></figure><p id="4ebb" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated"><strong class="jf hj">谷歌翻译结果:</strong></p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es mv"><img src="../Images/4092b6cd3095a9de03c6cc7fcfce8b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4HILbn20Z3Q0SMtjmu8oKQ.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">谷歌翻译从意大利语到英语的测试句子。</figcaption></figure><p id="06fd" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">除了‘它’被预测为‘他’和‘我’被预测为‘你’之外，模型预测的大多数单词都是对的。</p><p id="f047" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated"><strong class="jf hj">关注剧情:</strong></p><figure class="kb kc kd ke fd kf er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es mw"><img src="../Images/c91534af5880183ebfc7b230544f9542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mrORIQ1GU-DHFQeKGnWc6w.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">注意力图</figcaption></figure><p id="05ac" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">黄色和绿色阴影表示在预测目标序列的单词时，对源序列中的相应单词给予更高的关注权重。</p><p id="b8af" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated"><strong class="jf hj"> Bleu评分:</strong></p><p id="171e" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">双语评估替角得分，简称BLEU，是一种将生成的句子与参考句子进行评估的指标。完全匹配的得分为1.0，而完全不匹配的得分为0.0。</p><p id="ce04" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">Bleu分数可以计算如下</p><pre class="kb kc kd ke fd mf mg mh mi aw mj bi"><span id="3348" class="ln ig hi mg b fi mk ml l mm mn">import nltk.translate.bleu_score as bleu<br/>reference = 'if you don t believe me , go and see it for yourself .'<br/>print('BLEU score: {}'.format(bleu.sentence_bleu(reference, result)))</span></pre><p id="2ff9" class="pw-post-body-paragraph kr ks hi jf b jg lh kt ku ji li kv kw jk lj ky kz jm lk lb lc jo ll le lf jq hb bi translated">输出</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es mx"><img src="../Images/0487d20614e5c6ed211bd455b08643f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*CXKsp-ZzKmRSjuWfiL6Scg.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">Bleu评分</figcaption></figure><h1 id="fb2f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">参考</h1><ol class=""><li id="e71c" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><a class="ae kq" href="https://blog.floydhub.com/attention-mechanism/" rel="noopener ugc nofollow" target="_blank">https://blog.floydhub.com/attention-mechanism/</a></li><li id="dc71" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated"><a class="ae kq" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention#translate" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/tutorials/text/NMT _ with _ attention # translate</a></li><li id="dc6d" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated"><a class="ae kq" href="https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/" rel="noopener ugc nofollow" target="_blank">https://machine talk . org/2019/03/29/neural-machine-translation-with-attention-mechanism/</a></li><li id="f8bb" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated"><a class="ae kq" href="https://towardsdatascience.com/implementing-neural-machine-translation-with-attention-using-tensorflow-fc9c6f26155f" rel="noopener" target="_blank">https://towards data science . com/implementing-neural-machine-translation-with-attention-using-tensor flow-fc 9 c6f 26155 f</a></li><li id="94f8" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated"><a class="ae kq" href="https://towardsdatascience.com/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a" rel="noopener" target="_blank">https://towards data science . com/sequence-2-sequence-model-with-attention-mechanism-9e 9 ca 2 a 613 a</a></li></ol></div></div>    
</body>
</html>