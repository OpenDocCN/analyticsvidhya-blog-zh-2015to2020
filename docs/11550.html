<html>
<head>
<title>Deep One-Class Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度一类分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/paper-summary-deep-one-class-classification-doc-adc4368af75c?source=collection_archive---------7-----------------------#2020-12-08">https://medium.com/analytics-vidhya/paper-summary-deep-one-class-classification-doc-adc4368af75c?source=collection_archive---------7-----------------------#2020-12-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b502" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">许多应用对一类分类的高需求的可行解决方案</h2></div><p id="2baa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个故事中，介绍了约翰·霍普金斯大学为一类分类学习深度特征。这是作为IEEE图像处理汇刊(IEEE TIP)的期刊文章发表的。在本文中，一种新的基于深度学习的方法应用于单类迁移学习，其中来自不相关任务的标记数据用于单类分类中的特征学习。在各种数据集上的实验表明，所提出的深度一类分类(DOC)方法比现有的分类方法取得了显著的改进。</p><p id="8e53" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">他们甚至在他们的Github上向所有人公开代码！<br/>让我们看看他们是如何做到的。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es ju"><img src="../Images/f311723a813d7acc8c6a47e79f30a151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DiHSbVlJmNPanue6WZ2x9Q.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">图1 <strong class="bd kg">一级分类</strong></figcaption></figure></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h1 id="11f2" class="ko kp hi bd kg kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">概述</h1><ol class=""><li id="165a" class="lf lg hi iz b ja lh jd li jg lj jk lk jo ll js lm ln lo lp bi translated"><strong class="iz hj">什么是一级分类</strong></li><li id="da9b" class="lf lg hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated"><strong class="iz hj">深度一类分类(DOC)算法</strong></li><li id="2c65" class="lf lg hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated"><strong class="iz hj">实验结果</strong></li></ol></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h1 id="db35" class="ko kp hi bd kg kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated"><strong class="ak"> 1。一级分类是什么</strong></h1><p id="1f64" class="pw-post-body-paragraph ix iy hi iz b ja lh ij jc jd li im jf jg lv ji jj jk lw jm jn jo lx jq jr js hb bi translated">一类分类训练分类器在给定单个类样本时能够识别类外对象[1]。一类分类在许多真实世界的计算机视觉应用中遇到[2，3，4]，包括新奇检测、异常检测、医学成像和移动主动认证。</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h1 id="e1b8" class="ko kp hi bd kg kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated"><strong class="ak"> 2。深度一类分类(DOC)算法</strong></h1><h2 id="1161" class="ly kp hi bd kg lz ma mb kt mc md me kx jg mf mg kz jk mh mi lb jo mj mk ld ml bi translated">这是什么纸？研究人员到底做了什么？</h2><p id="66c8" class="pw-post-body-paragraph ix iy hi iz b ja lh ij jc jd li im jf jg lv ji jj jk lw jm jn jo lx jq jr js hb bi translated">尽管单类分类对许多应用是有希望的，但是现有的单类分类方案的方法仅仅训练在给定的概念上，不能在真实数据集上产生有希望的结果。作者认为，如果我们在考虑描述性的同时考虑紧凑性，我们可以学会更有效地表达自己。</p><p id="3773" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本研究从迁移学习的角度优化一类分类问题。作者通过设计优化单类分类任务的深度功能来解决这一特定问题，他们将其命名为单类迁移学习。</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h2 id="c8a4" class="ly kp hi bd kg lz ma mb kt mc md me kx jg mf mg kz jk mh mi lb jo mj mk ld ml bi translated"><strong class="ak">论文关于DOC算法的介绍</strong></h2><p id="20d0" class="pw-post-body-paragraph ix iy hi iz b ja lh ij jc jd li im jf jg lv ji jj jk lw jm jn jo lx jq jr js hb bi translated">所提出的方法在选择的卷积神经网络(CNN)之上操作，并产生描述性特征，同时在给定类的特征空间中保持低的类内方差。为此，提出了两个损失函数，紧凑性损失(<em class="mm"> lC </em>)和描述性损失(<em class="mm"> lD </em>)以及并行CNN架构。</p><p id="45e8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所提出的方法(图2)冻结了来自预训练深度模型的初始特征<em class="mm"> g_s </em>，并学习<em class="mm"> gl </em>和<em class="mm"> hc </em>。<em class="mm"> gl </em>指特征提取网络<em class="mm"> h_c </em>指分类子网络。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mn"><img src="../Images/db5e4e0ccd99d7e1f0f6489eb3d6d5db.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*EHfqvZaPSBjLjtmGZc1pDA.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">图2单类特征学习方法</figcaption></figure><p id="0444" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基于分类子网络(<em class="mm"> hc </em>)的输出，针对紧凑性损失(<em class="mm"> lC </em>)和描述性损失(<em class="mm"> lD </em>)两种损失进行优化。因此，整体损失函数引入紧凑性损失(<em class="mm"> lC </em>)作为正则项，同时最小化描述性损失(<em class="mm"> lD </em>)。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es mo"><img src="../Images/8e07dbb8288190829beb389a46cb48f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*srHEiC-qGtdK_VzGB1o3iQ.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">损失函数</figcaption></figure><p id="de0a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所提出的方法通过反向传播从总损失函数中学习<em class="mm"> g_l </em>和<em class="mm"> h_c </em>的权重。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mt"><img src="../Images/94542c5d22fde56888322e877ee0152d.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*vQ0aIhGyJoGr45y4gnAD-g.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">图3建议方法的概述</figcaption></figure><p id="e86a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"/>紧密度损失(<em class="mm"> lC </em>)使用给定的单类数据集，增加同一类内不同图像特征的相似度。损失函数由特定批次的成分样本之间的平均相似性表示。</p><p id="c182" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"/>描述性损失(<em class="mm"> lD </em>)是一种多类分类损失函数，它使用外部多类参考数据集在很大程度上分离每个类的不同要素表示。损失函数由ImageNet数据集的交叉熵表示。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es mu"><img src="../Images/8feb515139c7c107e1c38ca3c005065d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ah2Q6qhaaz8eekFBT5TVTA.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">图4提议的架构:(a)训练，以及(b)提议的DOC方法的测试框架。</figcaption></figure><p id="9bba" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了训练DOC算法，图4(a)中所示的架构用于训练DOC算法，而图4(b)用于测试。设共同特征提取子架构为<em class="mm"> g(⋅) </em>，共同分类子架构为<em class="mm"> hc(⋅) </em>。对于参考和二级网络，测试的是AlexNet和VGG16。</p><p id="dead" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">训练网络模型由两个CNN处理，一个参考网络和一个次级网络，具有目标和参考数据输入。参考网络和次级网络的权重在每个对应的对应物上绑定。基于两个损失(描述性损失和紧凑性损失)的学习，使用在参考和次级网络中提取的特征。</p><p id="ca79" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">测试模型包括两个阶段:模板生成和匹配。在训练期间学习的激励图被用作从一小组样本生成模板的特征。然后，根据保存的模板，用一类SVM、SVDD、k-最近邻分类器等进行分类。</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h2 id="a44d" class="ly kp hi bd kg lz ma mb kt mc md me kx jg mf mg kz jk mh mi lb jo mj mk ld ml bi translated">他们是如何做到的？</h2><p id="a172" class="pw-post-body-paragraph ix iy hi iz b ja lh ij jc jd li im jf jg lv ji jj jk lw jm jn jo lx jq jr js hb bi translated">作者有三种使用深度学习进行一类分类的策略来比较DOC的性能。使用的数据集是椅子的正常和异常图像。总之，这三种策略都不能产生一个紧凑的描述性特征。</p><ol class=""><li id="f720" class="lf lg hi iz b ja jb jd je jg mv jk mw jo mx js lm ln lo lp bi translated"><strong class="iz hj">提取深层特征</strong></li><li id="5b42" class="lf lg hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated"><strong class="iz hj">使用外部数据集微调两类分类器</strong></li><li id="64c9" class="lf lg hi iz b ja lq jd lr jg ls jk lt jo lu js lm ln lo lp bi translated"><strong class="iz hj">使用单一类别数据进行微调</strong></li></ol><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es ju"><img src="../Images/128591238d8503be9b3a008a5f4ea540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*CduyBi37lwDjFc20PVaBnw.png"/></div></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">图5异常图像检测中一类分类的可能策略。(a)正常和异常图像样本。(b)使用AlexNet功能获得的功能空间。(c)通过使用由ImageNet数据样本表示的外来物体来训练两类CNN而获得的特征空间。(d)仅使用正常对象通过微调获得的特征空间。(e)使用所提出的方法获得的特征空间。</figcaption></figure><p id="4e97" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">图5(b)使用预先训练的训练数据，通过策略1从现有CNN架构中提取深层特征。在策略1中，因为深度特征是描述性的，所以期望相同类别的样本在提取的特征空间中聚集在一起。策略2将图5(c)中的特征空间分类为两类图像，一类来自正常的椅子，另一类来自ImageNet数据集。图5(d)由策略3在仅使用常规椅子类别的预训练AlexNet网络上进行微调。图5(e)是使用DOC方法获得的特征空间。</p><p id="0292" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在图5(b)和5(c)中，正常和异常图像的样本没有被充分分离。在图5(b)中，现有的CNN体系结构不具有在现有的CNN体系结构中将正常和异常椅子的类别充分分离为不同簇的能力。在图5(c)中，在正常和异常椅子的图像之间存在细微差异，但是它们比其他ImageNet对象图像更相似。这阻止了正常和异常图像被充分地分离为不同的组。在图5(d)中，因为所有类别标签都是相同的，所以当显示异常椅子对象时没有辨别能力，并且正常和异常样本都被投射到同一点。</p><p id="70bd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">图5(e)显示了正常和异常样品的相对分离。所提出的方法表明，通过添加损失函数来表示描述性和紧凑性，可以学习策略1的方法的有效表示。</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h1 id="e3a4" class="ko kp hi bd kg kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">3.实验结果</h1><p id="dd47" class="pw-post-body-paragraph ix iy hi iz b ja lh ij jc jd li im jf jg lv ji jj jk lw jm jn jo lx jq jr js hb bi translated">在使用1001异常对象数据集的异常图像检测中，异常的性质是先验未知的，因此在单个类中执行训练。随着所提出的框架的引入，AlexNet的性能提高了大约14%,并且所提出的方法在该数据集上表现最佳。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es my"><img src="../Images/f05efb9c4caaa25e0975f5e287a2c842.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*d0fElN4X0FN8UZmi8ZCrPQ.png"/></div></figure><p id="8c2a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，在一类新颖性检测中，Caltech256数据集用于基于先前观察到的样本来评估新样本的新颖性。很明显，与现有方法相比，DOC方法产生了显著的改进。DOC方法将AlexNet的性能提高了约13%。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mz"><img src="../Images/60eb30973d577ac8afdb9a9b1e7ac7c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*a-wLem7rPp2U4qWeJyO2Ig.png"/></div></figure><p id="fcdd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1类新奇检测问题中的错误分类的性质非常类似于基于多类CNN的分类。大多数假阴性情况表明美国国旗在图像的背景中或者离图像太近而不能清楚地识别其特征。假阳性图像通常是美国国旗的颜色或挥舞旗帜的纹理。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es na"><img src="../Images/71d11527818c5d04157f1ff0c3b348d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*jzMFPbwE_tPvcMLvRAf00w.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">图6新颖性检测的一类问题的错误检测样本</figcaption></figure></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h1 id="8a91" class="ko kp hi bd kg kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">参考</h1><p id="0d5b" class="pw-post-body-paragraph ix iy hi iz b ja lh ij jc jd li im jf jg lv ji jj jk lw jm jn jo lx jq jr js hb bi translated">[1] H .何和y .马。《不平衡学习:基础、算法和应用》，Wiley-IEEE出版社，第一版，2013年。</p><p id="204a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[2]马尔科和辛格。“新颖性检测:综述—第1部分:统计方法”，信号处理，83(12):2481–2497，2003。</p><p id="ed7e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[3]帕特尔、切拉普、钱德拉和巴尔贝罗。“移动设备上的连续用户认证:最新进展和剩余挑战”，IEEE信号处理杂志，33(4):49–61，2016年7月。</p><p id="7f2f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[4]佩雷拉和帕特尔“移动主动认证中入侵者的高效和低延迟检测”，IEEE信息取证与安全汇刊，13(6):1392–1405，2018。</p><p id="f91e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[<a class="ae jt" href="https://arxiv.org/pdf/1801.05365.pdf" rel="noopener ugc nofollow" target="_blank">ArXiv</a>][<a class="ae jt" href="https://github.com/PramuPerera/DeepOneClass" rel="noopener ugc nofollow" target="_blank">Github</a>]学习一类分类的深度特征</p></div><div class="ab cl kh ki gp kj" role="separator"><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km kn"/><span class="kk bw bk kl km"/></div><div class="hb hc hd he hf"><h1 id="1417" class="ko kp hi bd kg kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">过去论文摘要列表</h1><h2 id="aceb" class="ly kp hi bd kg lz ma mb kt mc md me kx jg mf mg kz jk mh mi lb jo mj mk ld ml bi translated">数据不确定性学习</h2><p id="1fd2" class="pw-post-body-paragraph ix iy hi iz b ja lh ij jc jd li im jf jg lv ji jj jk lw jm jn jo lx jq jr js hb bi translated"><strong class="iz hj"> 2020: </strong> [ <a class="ae jt" href="https://mako95.medium.com/cvpr2020-paper-summary-data-uncertainty-in-face-recognition-1f17547473a2" rel="noopener"> DUL </a> ]</p><h2 id="8d7b" class="ly kp hi bd kg lz ma mb kt mc md me kx jg mf mg kz jk mh mi lb jo mj mk ld ml bi translated">一级分类</h2><p id="fa7c" class="pw-post-body-paragraph ix iy hi iz b ja lh ij jc jd li im jf jg lv ji jj jk lw jm jn jo lx jq jr js hb bi translated"><strong class="iz hj"> 2019: </strong> [ <a class="ae jt" rel="noopener" href="/swlh/paper-summary-deep-one-class-classification-doc-adc4368af75c"> DOC </a> ]</p><p id="4e60" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2020年: [ <a class="ae jt" rel="noopener" href="/the-shadow/exploring-important-feature-repressions-in-deep-one-class-classification-droc-d04a59558f9e"> DROC </a> ]</p><h2 id="f07a" class="ly kp hi bd kg lz ma mb kt mc md me kx jg mf mg kz jk mh mi lb jo mj mk ld ml bi translated">生物医学图像分割</h2><p id="ed21" class="pw-post-body-paragraph ix iy hi iz b ja lh ij jc jd li im jf jg lv ji jj jk lw jm jn jo lx jq jr js hb bi translated"><strong class="iz hj">2018:</strong><a class="ae jt" rel="noopener" href="/swlh/paper-summary-biomedical-image-segmentation-and-object-detection-uolo-c1175ba5c8c4">【UOLO】</a></p><h2 id="b84f" class="ly kp hi bd kg lz ma mb kt mc md me kx jg mf mg kz jk mh mi lb jo mj mk ld ml bi translated">图像聚类</h2><p id="a66d" class="pw-post-body-paragraph ix iy hi iz b ja lh ij jc jd li im jf jg lv ji jj jk lw jm jn jo lx jq jr js hb bi translated"><strong class="iz hj">2020:</strong><a class="ae jt" rel="noopener" href="/swlh/paper-deep-transfer-clustering-dtc-learning-to-discover-novel-visual-categories-ec5a26aea075">【DTC】</a></p></div></div>    
</body>
</html>