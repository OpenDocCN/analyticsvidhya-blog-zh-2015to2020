<html>
<head>
<title>Revise your PySpark knowledge</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习你的 PySpark 知识</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/revise-your-pyspark-knowledge-b3d6bb545ba4?source=collection_archive---------17-----------------------#2020-12-21">https://medium.com/analytics-vidhya/revise-your-pyspark-knowledge-b3d6bb545ba4?source=collection_archive---------17-----------------------#2020-12-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/93d01e0d674d4d6528c2c1e236fb1dca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-AR1bMixw4b_WSLh"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">麦克斯韦·尼尔森在 T2 的照片</figcaption></figure><p id="3813" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作为一名数据工程师，在一段时间后，你可能不得不在不同的编程/脚本语言之间切换。根据我的经验，我通常需要根据项目需求在纯 SQL 查询、Java、Shell 脚本、Scala 和 Python 之间切换。</p><p id="b821" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些直接的转换可能需要一些时间才能让我们上手，有时我们需要寻找直接函数是否可用于该编程语言以及语法。所以我写下了一些关键点，或者我们可以称之为 PySpark 的“作弊代码”,这可能会在你长时间休息后过渡到 PySpark 时有所帮助。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><blockquote class="ka kb kc"><p id="acb3" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj">T5】初始化 spark sessionT7】</strong></p></blockquote><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="dbf2" class="kq kr hi km b fi ks kt l ku kv">from pyspark.sql import SparkSession <br/>spark = SparkSession \<br/>        .builder \<br/>        .appName("PySpark Example") \<br/>        .config("spark.some.config.option", "your-value") \<br/>        .getOrCreate()</span></pre><blockquote class="ka kb kc"><p id="97a4" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">创建数据帧</em> </strong></p></blockquote><p id="7274" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">来自 RDDs </strong></p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="3599" class="kq kr hi km b fi ks kt l ku kv">from pyspark.sql.types import *</span></pre><p id="629e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kd">推断模式</em></p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="9881" class="kq kr hi km b fi ks kt l ku kv">sc = spark.sparkContext <br/>lines = sc.textFile("E:\products.csv") <br/>parts = lines.map(lambda l: l.split(",")) <br/>products = parts.map(lambda p: Row(prod_id=int(p[0]), \<br/>        product_name=p[1], price=int(p[2]))) <br/>products_df = spark.createDataFrame(products)</span></pre><p id="d286" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kd">指定模式</em></p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="b478" class="kq kr hi km b fi ks kt l ku kv">schemaString = "prod_id product_name price" <br/>fields = [StructField(field_name, StringType(), True) \<br/>        for field_name in schemaString.split()] <br/>schema = StructType(fields) <br/>products_df = spark.createDataFrame(products, schema)</span></pre><p id="37eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">来自火花数据源</strong></p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="b8ea" class="kq kr hi km b fi ks kt l ku kv">#TEXT files <br/>df = spark.read.text("sales.txt")</span><span id="12cb" class="kq kr hi km b fi kw kt l ku kv">#JSON files<br/>df1 = spark.read.json("customer.json") <br/>df2 = spark.read.load("students.json", format="json")</span><span id="c385" class="kq kr hi km b fi kw kt l ku kv">#Parquet files <br/>df3 = spark.read.load("transactions.parquet")</span></pre><blockquote class="ka kb kc"><p id="00f5" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">向数据帧添加新列</em> </strong></p></blockquote><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="6979" class="kq kr hi km b fi ks kt l ku kv">df = df.withColumn('name',df.customer.name) \<br/>        .withColumn('mailId',df.customer.mailId) \<br/>        .withColumn('address',df.customer.address) \<br/>        .withColumn('PhoneNumber',explode(df.contactNo.number))</span></pre><blockquote class="ka kb kc"><p id="4edb" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">更新数据帧的列名</em> </strong></p></blockquote><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="8883" class="kq kr hi km b fi ks kt l ku kv">df = df.withColumnRenamed('mailId', 'Email')</span></pre><blockquote class="ka kb kc"><p id="694a" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">从数据帧</em> </strong>中删除列</p></blockquote><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="1d2f" class="kq kr hi km b fi ks kt l ku kv">df = df.drop("Email", "address")<br/>df = df.drop(df.Email).drop(df.address)</span></pre><blockquote class="ka kb kc"><p id="047c" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">检查你的数据帧</em> </strong></p></blockquote><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="167a" class="kq kr hi km b fi ks kt l ku kv">df.show()             <br/>#Display the content of df</span><span id="2d9d" class="kq kr hi km b fi kw kt l ku kv">df.head(3)                 <br/>#Return first n rows</span><span id="f12d" class="kq kr hi km b fi kw kt l ku kv">df.tail(5)                 <br/>#Return last n rows</span><span id="b194" class="kq kr hi km b fi kw kt l ku kv">df.first()                 <br/>#Return first row</span><span id="6532" class="kq kr hi km b fi kw kt l ku kv">df.take(2)                <br/>#Return the first n rows</span><span id="8733" class="kq kr hi km b fi kw kt l ku kv">df.schema                 <br/>#Return the schema of df</span><span id="a668" class="kq kr hi km b fi kw kt l ku kv">df.printSchema()        <br/>#Print the schema of df</span><span id="be82" class="kq kr hi km b fi kw kt l ku kv">df.columns              <br/>#Return the columns of df</span><span id="904d" class="kq kr hi km b fi kw kt l ku kv">df.dtypes                 <br/>#Return df column names and data types</span><span id="e95c" class="kq kr hi km b fi kw kt l ku kv">df.describe().show()    <br/>#Compute summary statistics</span><span id="b2a6" class="kq kr hi km b fi kw kt l ku kv">df.count()              <br/>#Count the number of rows in df</span><span id="e62d" class="kq kr hi km b fi kw kt l ku kv">df.distinct().count()   <br/>#Count the number of distinct rows in df</span></pre><blockquote class="ka kb kc"><p id="a684" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">在数据帧上写查询</em> </strong></p></blockquote><p id="b578" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">选择查询</strong></p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="503c" class="kq kr hi km b fi ks kt l ku kv">df.select("StudentID").show()           <br/>#Show all records in StudentID column</span><span id="fe2f" class="kq kr hi km b fi kw kt l ku kv">df.select("StudentID",explode("contactInfo").alias("PhoneNumber")) \<br/>        .select("PhoneNumber.type","StudentID","age").show()<br/>#Show all records in  StudentID, age and PhoneNumber type</span><span id="2acf" class="kq kr hi km b fi kw kt l ku kv">df.select(df["StudentID"],df["age"]+ 1).show()<br/>#Show all records in StudentID and age with 1 added to the entries of age</span><span id="5f5b" class="kq kr hi km b fi kw kt l ku kv">df.select(df.address.substr(1,4).alias("street")).show() <br/>#Return substrings of  address</span></pre><p id="4673" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">条件查询</strong></p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="2323" class="kq kr hi km b fi ks kt l ku kv">df.select(df["age"] &gt; 15).show()       <br/>#Show all records where age &gt; 15</span><span id="a8b8" class="kq kr hi km b fi kw kt l ku kv">df.filter(df["age"] &gt; 15).show()<br/>#Show all records where age &gt; 15</span><span id="a592" class="kq kr hi km b fi kw kt l ku kv">df[df.StudentID.isin(1234,1235)].show()   <br/>#Show StudentID if in the given values</span><span id="4c84" class="kq kr hi km b fi kw kt l ku kv">df.select("FirstName",df.department.like("%D4%") \<br/>        .alias("D4_dept")).show()<br/>#Show FirstName and D4_dept as TRUE if department is like D4</span><span id="86d2" class="kq kr hi km b fi kw kt l ku kv">df.select("FirstName",df.department.startswith("Civil") \<br/>        .alias("Civilian")).show() <br/>#Show FirstName and Civilian as TRUE if department starts with Civil</span><span id="4b6c" class="kq kr hi km b fi kw kt l ku kv">df.select("StudentID",df.department.endswith("Engineering") \<br/>        .alias("Engineer")).show()<br/>#Show StudentID and Engineer as TRUE if department ends with Engineering</span></pre><p id="8177" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">按查询分组</strong></p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="d505" class="kq kr hi km b fi ks kt l ku kv">df.groupBy("department").count().show()<br/>#Return count of members for each department group</span></pre><p id="8532" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">分类数据帧</strong></p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="7399" class="kq kr hi km b fi ks kt l ku kv">df.sort(df.age.desc()).show() <br/>df.sort("age", ascending=False).show() <br/>df.orderBy(["age","city"],ascending=[0,1]).collect()</span></pre><blockquote class="ka kb kc"><p id="378f" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">修改数据帧中的值</em> </strong></p></blockquote><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="d1b0" class="kq kr hi km b fi ks kt l ku kv">df.na.fill(50).show() <br/>#Replace null values</span><span id="9725" class="kq kr hi km b fi kw kt l ku kv">df.na.replace(10, 20).show()<br/>#Replace one value with another and return a new df</span><span id="e503" class="kq kr hi km b fi kw kt l ku kv">df.na.drop().show()<br/>#Drop rows with null values and return a new df</span><span id="1eca" class="kq kr hi km b fi kw kt l ku kv">df = df.dropDuplicates()<br/>#Drop duplicated rows and return a new df</span></pre><blockquote class="ka kb kc"><p id="4455" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">修改数据帧的分区数量</em> </strong></p></blockquote><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="ff2a" class="kq kr hi km b fi ks kt l ku kv">df.repartition(10).rdd.getNumPartitions()<br/>#Can increase or decrease the number of partitions</span><span id="31ba" class="kq kr hi km b fi kw kt l ku kv">df.coalesce(1).rdd.getNumPartitions()<br/>#Can decrease the number of partitions</span></pre><blockquote class="ka kb kc"><p id="3ed6" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">将数据帧注册为视图</em> </strong></p></blockquote><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="509c" class="kq kr hi km b fi ks kt l ku kv">customer_df.createTempView("customer")<br/>product_df.createOrReplaceTempView("product")<br/>student_df.createGlobalTempView("student")</span><span id="c7d4" class="kq kr hi km b fi kw kt l ku kv">#Now we can write spark.sql queries</span><span id="efac" class="kq kr hi km b fi kw kt l ku kv">spark.sql("select * from product").show()</span></pre><blockquote class="ka kb kc"><p id="91a6" class="iv iw kd ix b iy iz ja jb jc jd je jf ke jh ji jj kf jl jm jn kg jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">获取数据帧的输出</em> </strong></p></blockquote><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="ff1e" class="kq kr hi km b fi ks kt l ku kv">new_rdd = df.rdd<br/>#Convert df into RDD</span><span id="1c33" class="kq kr hi km b fi kw kt l ku kv">df.toJSON().collect()<br/>#Return df into JSON format</span><span id="2d4d" class="kq kr hi km b fi kw kt l ku kv">df.toPandas()<br/>#Return df as Pandas DataFrame</span><span id="36d6" class="kq kr hi km b fi kw kt l ku kv">df.select("StudentID","department").write.save("E:\f1.parquet")<br/>#Write content of df in f1.parquet file</span><span id="d754" class="kq kr hi km b fi kw kt l ku kv">df.select("StudentID","department").write. \<br/>        save("E:\f2.json", format="json")<br/>#Write content of df in f2.json file</span></pre></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><p id="ac1e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我希望所有这些命令和函数列表将有助于解决您在开发中的障碍，如果您觉得缺少什么，也可以在评论中随意添加。</p></div></div>    
</body>
</html>