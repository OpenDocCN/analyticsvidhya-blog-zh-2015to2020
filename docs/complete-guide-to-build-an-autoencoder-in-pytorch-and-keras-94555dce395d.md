# åœ¨ Pytorch å’Œ Keras ä¸­æ„å»ºè‡ªåŠ¨ç¼–ç å™¨çš„å®Œæ•´æŒ‡å—

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/complete-guide-to-build-an-autoencoder-in-pytorch-and-keras-94555dce395d?source=collection_archive---------8----------------------->

è¿™ç¯‡æ–‡ç« æ˜¯æˆ‘ä¹‹å‰çš„[æ–‡ç« ](/analytics-vidhya/complete-guide-to-build-cnn-in-pytorch-and-keras-abc9ed8b8160)çš„å»¶ç»­ï¼Œè¿™ç¯‡æ–‡ç« æ˜¯ä½¿ç”¨ pytorch å’Œ keras æ„å»º CNN çš„å®Œæ•´æŒ‡å—ã€‚

ä»æ ‡å‡†æ•°æ®é›†æˆ–è‡ªå®šä¹‰æ•°æ®é›†è·å–è¾“å…¥å·²ç»åœ¨[ä½¿ç”¨ pytorch å’Œ keras çš„ CNN å®Œå…¨æŒ‡å—](/analytics-vidhya/complete-guide-to-build-cnn-in-pytorch-and-keras-abc9ed8b8160)ä¸­æåˆ°ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä»å¯¹è‡ªåŠ¨ç¼–ç å™¨çš„å¿…è¦ä»‹ç»å¼€å§‹ï¼Œç„¶åå®ç°ä¸€ä¸ªã€‚

## è‡ªåŠ¨ç¼–ç å™¨

è‡ªåŠ¨ç¼–ç å™¨æ˜¯ä¸€ç§ç¥ç»ç½‘ç»œï¼Œå®ƒå­¦ä¹ ä»¥æœ€å°çš„ä¿¡æ¯æŸå¤±å¯¹æ•°æ®è¿›è¡Œç¼–ç ã€‚

![](img/4c06d6623ca8552a7d249af453aa4b4d.png)

è‡ªåŠ¨ç¼–ç å™¨

ä¸Šè¿°ç½‘ç»œæœ‰è®¸å¤šå˜ä½“ã€‚å…¶ä¸­ä¸€äº›æ˜¯:

## ç¨€ç–è‡ªåŠ¨ç¼–ç å™¨

è¿™ç§è‡ªåŠ¨ç¼–ç å™¨é€šè¿‡æ­£åˆ™åŒ–æ¿€æ´»å‡½æ•°éšè—èŠ‚ç‚¹æ¥å‡å°‘è¿‡æ‹Ÿåˆã€‚

## é™å™ªè‡ªåŠ¨ç¼–ç å™¨

è¿™ä¸ªè‡ªåŠ¨ç¼–ç å™¨æ˜¯é€šè¿‡åœ¨è¾“å…¥ä¸­åŠ å…¥å™ªå£°æ¥è®­ç»ƒçš„ã€‚è¿™å°†æ¶ˆé™¤è¯„ä¼°æ—¶è¾“å…¥çš„å™ªå£°ã€‚

## å˜ä½“è‡ªåŠ¨ç¼–ç å™¨

è¿™æ˜¯ä¸€ç§æ·±åº¦ç”Ÿæˆç¥ç»ç½‘ç»œã€‚è‡ªåŠ¨ç¼–ç å™¨çš„ä¸»è¦æŒ‘æˆ˜æ˜¯å®ƒä»¬æ€»æ˜¯è¯•å›¾æœ€å°åŒ–é‡å»ºè¯¯å·®ï¼Œå¹¶ä¸”ä»ä¸å…³å¿ƒæ½œåœ¨çš„è¡¨ç¤ºã€‚

ä¸€ä¸ªå¥½çš„æ½œåœ¨è¡¨ç¤ºåº”è¯¥æ€»æ˜¯æœ‰æ„ä¹‰çš„ï¼Œä»¥ä¾¿å®ƒå¯ä»¥ç”¨äºåƒ GAN è¿™æ ·çš„ç”Ÿæˆç¥ç»ç½‘ç»œã€‚æœ‰æ„ä¹‰æ˜¯æŒ‡å®‰æ’ã€‚æ¥è‡ªåŒä¸€ç±»çš„æ•°æ®ç‚¹åˆ†ç»„æ›´è¿‘ï¼Œæ¥è‡ªä¸åŒç±»çš„æ•°æ®ç‚¹åˆ†ç»„ç¨è¿œã€‚

![](img/160b20692089a71a9a579caadfc3c187.png)

ã€https://blog.keras.io/building-autoencoders-in-keras.html 

è¿™ç§æ½œåœ¨è¡¨ç¤ºå¯ä»¥é€šè¿‡å¦‚ä¸‹æ”¹å˜ç¥ç»ç½‘ç»œçš„ç»“æ„æ¥å®ç°:

![](img/cad657f3565d83359a697b16776f3037.png)

VAE

ä¸å…¶ä»–è‡ªåŠ¨ç¼–ç å™¨ä¸åŒï¼Œæˆ‘ä»¬æ­£åœ¨ç”Ÿæˆä¸€ä¸ªå…·æœ‰å‡å€¼å’Œæ ‡å‡†å·®çš„æ½œåœ¨åˆ†å¸ƒï¼Œè€Œä¸æ˜¯å•ä¸€çš„æ½œåœ¨å‘é‡ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ä»æ½œåœ¨åˆ†å¸ƒä¸­å–æ ·ä»¥é‡å»ºè¾“å…¥ã€‚

å…³äºå˜ä½“è‡ªåŠ¨ç¼–ç å™¨çš„ä¸¤ä»¶é‡è¦äº‹æƒ…æ˜¯:

åœ¨é‡‡æ ·æ—¶ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨é‡æ–°å‚æ•°åŒ–æŠ€å·§æ¥å¤„ç†èŠ‚ç‚¹çš„éšæœºæ€§ï¼Œå› ä¸ºèŠ‚ç‚¹çš„éšæœºæ€§å¯èƒ½ä¼šåœæ­¢åå‘ä¼ æ’­ã€‚

> Î¼,ğ›”â‰ˆÎ¼+ğ›”*n(0,1)

è¿™ç§é‡æ–°å‚æ•°åŒ–çš„æŠ€å·§ä¸ä¼šæ”¹å˜åˆ†å¸ƒã€‚ä½†æ˜¯å®ƒå°†è°ƒæ•´å‚æ•°ä»¥å…è®¸åå‘ä¼ æ’­ã€‚

å˜åŒ–è‡ªåŠ¨ç¼–ç å™¨ä½¿ç”¨ä¸‹é¢çš„ç­‰å¼æ­£åˆ™åŒ–æˆæœ¬å‡½æ•°ã€‚

> æ­£åˆ™åŒ–æˆæœ¬å‡½æ•°= Loss+KL(N(Î¼,ğ›”),N(0,1))

è¿™å°†å¼ºåˆ¶æ½œåœ¨åˆ†å¸ƒéµå¾ªæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œä»è€Œæ‰©å±•å…¶åœ¨æ·±åº¦ç”Ÿæˆæ¨¡å‹ä¸­çš„ä½¿ç”¨ã€‚

ä½ å¯ä»¥åœ¨è¿™ç¯‡[æ–‡ç« ](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73)ä¸­è¯»åˆ°æ›´å¤šå…³äº VAE çš„ä¿¡æ¯ï¼Œåœ¨è¿™é‡Œä½ å¯ä»¥è¯»åˆ°æ›´å¤šå…³äºå„ç§ç±»å‹çš„è‡ªåŠ¨ç¼–ç å™¨çš„ä¿¡æ¯ã€‚æˆ‘ä»¬å°†åœ¨æœ¬æ–‡ä¸­å®ç° VAEã€‚

# å±¥è¡Œ

ä»»ä½•è‡ªåŠ¨ç¼–ç å™¨éƒ½åŒ…æ‹¬ä¸¤ä¸ªç½‘ç»œç¼–ç å™¨å’Œè§£ç å™¨ã€‚å¦‚å‰æ‰€è¿°ï¼ŒVAE ä¹Ÿä½¿ç”¨è§„åˆ™åŒ–æˆæœ¬å‡½æ•°ã€‚

## ç¼–ç å™¨

ç¼–ç å™¨æ¥å—è¾“å…¥å¹¶è¿”å›æ½œåœ¨åˆ†å¸ƒçš„å¹³å‡å€¼å’Œæ ‡å‡†åå·®ã€‚

```
#**Pytorch****class** **VAE**(nn.Module):
    **def** __init__(self, x, h1, h2, z):
        super(VAE, self).__init__()

        self.fc1 = nn.Linear(x, h)
        self.fc2 = nn.Linear(h1, h2)
        self.fc_mean = nn.Linear(h2, z)
        self.fc_sd = nn.Linear(h2, z)

    **def** encoder(self, x):
        h1 = F.relu(self.fc1(x))
        h2 = F.relu(self.fc2(h1))
        **return** self.fc_mean(h2), self.fc_sd(h2) *# mu, log_var*#**Keras**x = Input(batch_shape=(batch_size, original_dim))
h = Dense(intermediate_dim, activation='relu')(x)
z_mean = Dense(latent_dim)(h)
z_log_sigma = Dense(latent_dim)(h) 
```

## æŠ½æ ·

æ ¹æ®ä»ç¼–ç å™¨è·å¾—çš„å¹³å‡å€¼å’Œæ ‡å‡†åå·®ï¼Œæˆ‘ä»¬å°†é€šè¿‡é‡‡æ ·ç”Ÿæˆè§£ç å™¨çš„è¾“å…¥ã€‚ä¸Šé¢æåˆ°çš„é‡æ–°å‚æ•°åŒ–æŠ€å·§å‡ºç°åœ¨è¿™é‡Œã€‚

```
**#Pytorch**def sampling(self, mu, log_var):
        std = torch.exp(0.5*log_var)
        eps = torch.randn_like(std)
        **return** eps.mul(std).add_(mu) #**Keras**def sampling(args):
    z_mean, z_log_sigma = args
    epsilon = K.random_normal(shape=(batch_size, latent_dim),
                              mean=0., std=epsilon_std)
    return z_mean + K.exp(z_log_sigma) * epsilon
```

## è§£ç å™¨

è§£ç å™¨è·å–é‡‡æ ·å‡½æ•°çš„è¾“å‡ºï¼Œå¹¶å°è¯•é‡å»ºåŸå§‹è¾“å…¥ã€‚

```
**#Pytorch****class** **VAE**(nn.Module):
    **def** __init__(self, x, h1, h2, z):
        super(VAE, self).__init__()
        self.fc1 = nn.Linear(x, h1)
        self.fc2 = nn.Linear(h1, h2)
        self.fc_mean = nn.Linear(h2, z)
        self.fc_sd = nn.Linear(h2, z)
        *# decoder* 
        self.fc4 = nn.Linear(z, h2)
        self.fc5 = nn.Linear(h2, h1)
        self.fc6 = nn.Linear(h1, x)

    **def** decoder(self, z):
        h1 = F.relu(self.fc4(z))
        h2 = F.relu(self.fc5(h1))
        **return** F.sigmoid(self.fc6(h2))#**Keras**decoder_h = Dense(intermediate_dim, activation='relu')
decoder_mean = Dense(original_dim, activation='sigmoid')
h_decoded = decoder_h(z)
x_decoded_mean = decoder_mean(h_decoded)
```

## æŸå¤±å‡½æ•°

å¦‚å‰æ‰€è¿°ï¼ŒVAE ä½¿ç”¨æ­£åˆ™åŒ–æŸå¤±å‡½æ•°ï¼Œ

å…·æœ‰å‡å€¼Î¼i å’Œæ ‡å‡†åå·®ğ›”i çš„åˆ†å¸ƒçš„ KL æ•£åº¦å…·æœ‰æ ‡å‡†æ­£æ€åˆ†å¸ƒ(KL(N(Î¼i,ğœI),N(0,1))æ˜¯

![](img/2f9c6fad3459b31d22e7397213724b43.png)

```
**#Pytorch****def** loss_function(reconstructed_x, x, mu, log_var):
    loss = F.binary_cross_entropy(reconstructed_x, x.view(-1, 784),      
                       reduction='sum')
    regularized_term = -0.5 * torch.sum(1 + log_var - mu.pow(2) -   
                      log_var.exp())

    **return** loss + regularized_term**#Keras**def vae_loss(x, x_decoded_mean):
    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)
    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - 
                 K.exp(z_log_sigma), axis=-1)
    return xent_loss + kl_loss
```

## æ•°æ®æµåŠ¨

æ•°æ®ä»ç¼–ç å™¨ã€é‡‡æ ·å¼€å§‹ï¼Œç„¶åæ˜¯è§£ç å™¨ã€‚

```
**#Pytorch**def forward(self, x):
        mu, log_var = self.encoder(x.view(-1, 784))
        z = self.sampling(mu, log_var)
        **return** self.decoder(z), mu, log_var
```

åœ¨ keras ä¸­ï¼Œä¸éœ€è¦è½¬å‘å‡½æ•°ã€‚æ•°æ®å°†æŒ‰ç…§ä½ å»ºç«‹ç½‘ç»œæ¨¡å‹çš„é¡ºåºæµåŠ¨ã€‚

ç”¨æŸå¤±å‡½æ•°ç¼–åˆ¶ç½‘ç»œã€‚

```
#**Pytorch**vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)
latent, mu, log_var = vae(data)
loss = loss_function(latent, data, mu, log_var)

loss.backward()

optimizer.step()**#Keras**vae = Model(x, x_decoded_mean)vae.compile(optimizer='rmsprop', loss=vae_loss)
```

æˆ‘ä»¬è¿˜å°†åœ¨ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­æ‰“åŒ… pytorch å’Œ keras ä¸­ GAN çš„å®ç°ã€‚

æ„Ÿè°¢é˜…è¯»:)

## å‚è€ƒ

[](https://blog.keras.io/building-autoencoders-in-keras.html) [## åœ¨ Keras ä¸­æ„å»ºè‡ªåŠ¨ç¼–ç å™¨

### åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†å›ç­”ä¸€äº›å…³äºè‡ªåŠ¨ç¼–ç å™¨çš„å¸¸è§é—®é¢˜ï¼Œæˆ‘ä»¬å°†æ¶µç›–ä»£ç çš„ä¾‹å­â€¦

blog.keras.io](https://blog.keras.io/building-autoencoders-in-keras.html)