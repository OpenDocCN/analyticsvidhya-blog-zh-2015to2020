<html>
<head>
<title>The story of BERT-ian era</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特-伊恩时代的故事</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-story-of-bert-ian-era-4fc455f0cfc?source=collection_archive---------13-----------------------#2020-03-08">https://medium.com/analytics-vidhya/the-story-of-bert-ian-era-4fc455f0cfc?source=collection_archive---------13-----------------------#2020-03-08</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><div class=""/><figure class="ev ex im in io ip er es paragraph-image"><div role="button" tabindex="0" class="iq ir di is bf it"><div class="er es il"><img src="../Images/77af02593bf714d186d04c6dfb2776fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fWU5jbgKARBNlRSd"/></div></div><figcaption class="iw ix et er es iy iz bd b be z dx translated">丹·迪莫克在<a class="ae ja" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="ac2c" class="pw-post-body-paragraph jb jc ho jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy hh bi translated">有许多NLP任务，例如文本摘要、问答、句子预测等等。完成这些任务的一种方法是使用预先训练好的模型。不是每次都使用数百万带注释的文本从头开始为NLP任务训练模型，而是通过在大量数据上训练模型来创建通用语言表示。这被称为预训练模型。这个预先训练好的模型会针对每个…</p></div></div>    
</body>
</html>