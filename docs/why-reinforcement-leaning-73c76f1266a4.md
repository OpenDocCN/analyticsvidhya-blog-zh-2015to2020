# 金融市场中的强化学习与监督学习

> 原文：<https://medium.com/analytics-vidhya/why-reinforcement-leaning-73c76f1266a4?source=collection_archive---------10----------------------->

![](img/511dafea0811f525666baf6cf7e898a0.png)

来自 Pexels 的 energepic.com

## 我对为什么在金融市场中强化学习优于监督学习的看法。

我们都同意，金融市场是我们现代经济的核心，毫无疑问，金融市场为债券、股票、外汇和衍生品等资产的买卖提供了重要渠道。然而，要从这样的市场中获利，投资者应该研究市场复杂多变的环境。让我们以股票市场为例，要从这样的市场中获利，你需要考虑所有可能影响市场运动的政治、经济甚至环境(如疫情)因素，这使得交易对于人类来说是一项非常困难的任务，这就是为什么在过去十年中，人们做出了许多努力，通过设计利用市场同时降低风险的适应性系统来自动生成金融资产交易中的成功交易。

# 监督模型有什么问题？

这些自适应系统中的大多数都依赖于*监督学习*，它本质上是根据历史数据训练一个预测模型来预测趋势方向，但是不管它们有多受欢迎，这些监督方法都有不同的局限性，从而导致次优结果。

金融市场中监督模型限制的主要原因是，交易金融资产不仅是像大多数监督模型那样预测未来价格的过程，而且还涉及许多其他应该考虑的方面，例如所涉及的风险，其中监督模型寻求预测误差的最小化(回报的最大化),而不管风险如何，这不符合投资者的利益，而且在大多数情况下根本没有考虑外部约束(例如，缺乏流动性和交易成本)。此外，金融市场数据非常嘈杂，因此在这样的环境中使用具有巨大学习能力的算法，如 ***神经网络*** 将主要导致过拟合。

*监督学习*的上述缺点可以通过使用*强化学习*来解决。**

# 那么什么是强化学习(RL)？

简而言之， **RL** 是**机器学习的一个领域，**关注软件代理应该如何在环境中采取行动。在 **RL** 设置中，一个由机器学习算法控制的自主代理在一个时间步长内观察其环境的状态。代理通过在当前状态采取动作来与环境交互，该动作将导致代理和环境都转换到新状态。*强化学习*将直接学习一种交易策略，该策略将预测价格和随后的投资组合构建整合在一个步骤中，以优化投资者的目标。

## MDP，秘密配料。

财务数据被描述为高度依赖于时间(时间的函数)的数据，使其非常适合*马尔可夫决策过程(MDP)* ，这是解决 **RL** 问题的核心过程。

MDP 捕获了整个过去的数据，并在代理的当前状态中定义了问题的整个历史，这在建立金融市场数据模型时极其重要。让我们假设一家历史繁荣的公司最近发布了亏损严重的年度报告，一个 S *监督学习*算法将根据该公司的良好历史预测下一年的良好表现，而忽略当前发布的报告。*强化学习，*另一方面，将给予当前事件(年度报告)更多的权重，代理决策将受到严重影响，无论公司的历史或商誉如何，该年度报告将影响从一个状态到另一个状态的转换。

最后，很高兴看到更多的注意力放在金融领域的 RL 上——特别是这些发现对一般的 RL 研究很有价值。