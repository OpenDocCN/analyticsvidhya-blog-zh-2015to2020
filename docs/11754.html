<html>
<head>
<title>Naïve Bayes Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朴素贝叶斯算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/na%C3%AFve-bayes-algorithm-5bf31e9032a2?source=collection_archive---------3-----------------------#2020-12-17">https://medium.com/analytics-vidhya/na%C3%AFve-bayes-algorithm-5bf31e9032a2?source=collection_archive---------3-----------------------#2020-12-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="276b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><strong class="ak"> <em class="ix">探索朴素贝叶斯:数学，它是如何工作的，优点&amp;缺点，以及应用</em> </strong></h2></div><h2 id="3a72" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">什么是朴素贝叶斯算法？</h2><p id="98e1" class="pw-post-body-paragraph jw jx hi jy b jz ka ij kb kc kd im ke jj kf kg kh jn ki kj kk jr kl km kn ko hb bi translated">朴素贝叶斯是一种基于贝叶斯定理的分类技术，它假设预测目标值的所有特征都是相互独立的。它会计算每个类别的概率，然后选择概率最高的类别。它已经被成功地用于许多目的，但是它在处理自然语言处理(NLP)问题时特别有效。</p><p id="001a" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">贝叶斯定理描述了一个事件发生的概率，基于可能与该事件相关的条件的先验知识。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ku"><img src="../Images/3607816b128df704c9496f46e4b98772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*aFhOj7TdBIZir4keHMgHOw.png"/></div></figure><h2 id="969b" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">是什么让朴素贝叶斯成为“幼稚”的算法？</h2><p id="9cb1" class="pw-post-body-paragraph jw jx hi jy b jz ka ij kb kc kd im ke jj kf kg kh jn ki kj kk jr kl km kn ko hb bi translated">朴素贝叶斯分类器假设我们用来预测目标的特征是独立的，互不影响。而在现实生活数据中，特征在确定目标时相互依赖，但朴素贝叶斯分类器忽略了这一点。</p><p id="3051" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">尽管独立性假设在真实世界的数据中从来都不正确，但在实践中却常常行之有效。所以它被称为<strong class="jy hj">【天真】</strong>。</p><h2 id="f734" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">朴素贝叶斯算法背后的数学</h2><p id="45e4" class="pw-post-body-paragraph jw jx hi jy b jz ka ij kb kc kd im ke jj kf kg kh jn ki kj kk jr kl km kn ko hb bi translated">给定一个特征向量X=(x1，x2，…，xn)和一个类别变量y，贝叶斯定理表明:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lc"><img src="../Images/f560e10ab208b36ee807eadcfb32de1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*xfvAipyVBfo1RPwkL9DmYQ.png"/></div></figure><p id="1ddc" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">我们感兴趣的是从可能性P(X | y)和先验概率P(y)，P(X)计算后验概率P(y | X)。</p><p id="9f05" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">使用链式法则，可能性P(X ∣ y)可以分解为:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ld"><img src="../Images/41fcc36c06f1eed6f6f6a17316da00be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*7E_s6icZgyWvf-Vm4OhtFg.png"/></div></figure><p id="b8e7" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">但是由于Naive的条件独立性假设，条件概率是相互独立的。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es le"><img src="../Images/87689cb22c6cc118195addbda0348de9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*n2LbIxG16jQFaSmBzL3oLQ.png"/></div></figure><p id="4ee0" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">因此，通过条件独立，我们有:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lf"><img src="../Images/25cbdd69c63cd5fb82682d224b3460f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bIZ1Jdn3k5deoeJxwNxRmA.png"/></div></div></figure><p id="16d1" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">由于所有值的分母保持不变，因此后验概率可以是:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lk"><img src="../Images/bc433d97aeb86eb33e2a384c31f0db0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*FFmYvf4oZ9MafJXOfWN1mA.png"/></div></div></figure><p id="9b47" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">朴素贝叶斯分类器将该模型与决策规则相结合。一个常见的规则是选择最有可能的假设；这被称为最大后验概率或MAP判决规则。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ll"><img src="../Images/b8b58a0bbf29e69de901a7e56b2790ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*eD4a7MigslH6n4QtOARsEw.png"/></div></figure><h2 id="cf77" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated"><strong class="ak">天真的海湾是如何工作的:</strong></h2><p id="2b50" class="pw-post-body-paragraph jw jx hi jy b jz ka ij kb kc kd im ke jj kf kg kh jn ki kj kk jr kl km kn ko hb bi translated">让我们用一个例子来解释它，让事情变得清楚:</p><p id="ce75" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">假设我们有一堆想要归类为垃圾邮件的电子邮件。</p><p id="3695" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">我们的数据集有<strong class="jy hj"> <em class="lm"> 15封非垃圾邮件</em> </strong>和<strong class="jy hj"> <em class="lm"> 10封垃圾邮件</em> </strong> <em class="lm"> </em>邮件。已经做了一些分析，每个词的频率被记录如下:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ln"><img src="../Images/bc953d23ad56448262a4c34911ecd2d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*G-g9VQJzCG0ksqPpHQ247g.png"/></div></figure><blockquote class="lo lp lq"><p id="dc50" class="jw jx lm jy b jz kp ij kb kc kq im ke lr kr kg kh ls ks kj kk lt kt km kn ko hb bi translated"><strong class="jy hj"> <em class="hi">注:</em> </strong>删除了" the "、" a "、" on "、" is "、" all "等停用词，因为它们没有重要意义，通常从文本中删除。同样的事情也适用于数字和标点。</p></blockquote><p id="50ef" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated"><strong class="jy hj">探索一些概率:</strong></p><ul class=""><li id="f79a" class="lu lv hi jy b jz kp kc kq jj lw jn lx jr ly ko lz ma mb mc bi translated">p(亲爱的|不是垃圾邮件)= 8/34</li><li id="f57f" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">p(访问|非垃圾邮件)= 2/34</li><li id="3196" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">p(亲爱的|垃圾邮件)= 3/47</li><li id="23fc" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">p(访问|垃圾邮件)= 6/47</li></ul><p id="dcc6" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">诸如此类。</p><p id="e2c2" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">现在假设我们有消息“<em class="lm">你好朋友</em>”，我们想知道它是否是垃圾邮件。</p><p id="d658" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">所以，利用贝叶斯定理</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mi"><img src="../Images/fb1713cd52952685165c16770555529b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PharzCCYWYPV-2jK9FBkKQ.png"/></div></div></figure><p id="b399" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">忽略分母</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mj"><img src="../Images/7fadd1a5a6f78a484f71813e4fd689f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P15GjWNcnQ6z4WtQn06Z8g.png"/></div></div></figure><p id="2ad2" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">但是，P(Hello friend | Not Spam) = 0，因为这种情况(Hello friend)在我们的数据集中不存在，也就是说，我们处理的是单个单词，而不是整个句子，P(Hello friend | Spam)的情况也将是零，这反过来将使成为垃圾邮件和不是垃圾邮件的概率都为零，这没有任何意义！！</p><blockquote class="lo lp lq"><p id="ae4f" class="jw jx lm jy b jz kp ij kb kc kq im ke lr kr kg kh ls ks kj kk lt kt km kn ko hb bi translated">但是等等！！我们说<em class="hi"/><em class="hi">朴素贝叶斯假设</em> <strong class="jy hj">`我们用来预测目标的特征是独立的`</strong>。</p></blockquote><p id="20d7" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">所以，</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mk"><img src="../Images/78ff95595742f2b6291a4a80d2d5305b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ogobemBLzfW7JnQs1u6kCw.png"/></div></div></figure><p id="08e9" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">现在，让我们使用相同的过程来计算成为垃圾邮件的概率:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es ml"><img src="../Images/d5503bdf8f2a55b95946b6c243dbbeae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jnn2iV_TmAvvvrZN1BUhmw.png"/></div></div></figure><p id="3a9b" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">所以，消息“<em class="lm">你好朋友”</em>不是垃圾邮件。</p><p id="3046" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated"><strong class="jy hj">现在，让我们试试另一个例子:</strong></p><p id="93db" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">假设消息<em class="lm">“亲爱的拜访晚餐钱钱钱”</em>。很明显这是一个垃圾邮件，但是让我们看看朴素贝叶斯会怎么说。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mm"><img src="../Images/f225116c2d3681a5a87a2f801cead2cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vvgrfid5BYTzha36CeZqJg.png"/></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mn"><img src="../Images/d0be56eeb86cd6ed7a8718deeeacc8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bpCJ8Y_PWyFlxObt4Xhbgg.png"/></div></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mo"><img src="../Images/f2ffd2131aff706791302c1b47ebdc6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q09-CyV_Zo04I0jLMbSjtA.png"/></div></div></figure><p id="c47d" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">哎呀！！Naive Bays说这个消息是<strong class="jy hj">不是垃圾邮件</strong>？！！！</p><p id="1051" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">发生这种情况是因为单词"<em class="lm">晚餐"</em>没有出现在<em class="lm">垃圾邮件</em>数据集中，所以P(晚餐|垃圾邮件)= 0，因此所有其他概率都没有影响。</p><p id="9587" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">这被称为<strong class="jy hj">零频率问题。</strong></p><p id="8864" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">为了解决这个问题，我们可以使用<strong class="jy hj">拉普拉斯平滑。</strong></p><blockquote class="lo lp lq"><p id="e014" class="jw jx lm jy b jz kp ij kb kc kq im ke lr kr kg kh ls ks kj kk lt kt km kn ko hb bi translated"><strong class="jy hj">拉普拉斯平滑</strong>是一种平滑分类数据的技术。小样本校正，或伪计数，将被纳入每一个概率估计。因此，任何概率都不会为零。这是一种正则化朴素贝叶斯的方法。</p><p id="a178" class="jw jx lm jy b jz kp ij kb kc kq im ke lr kr kg kh ls ks kj kk lt kt km kn ko hb bi translated">给定来自具有N次试验的多项式分布的观测值x = (x1，…，xd)和参数向量θ = (θ1，…，θd)，数据的“平滑”版本给出估计量:</p></blockquote><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mp"><img src="../Images/9c0e3fba97cfc6bcb1732a57b020e80a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Do1OmxJOvszQ9KCujg5fEg.png"/></div></div></figure><blockquote class="lo lp lq"><p id="004c" class="jw jx lm jy b jz kp ij kb kc kq im ke lr kr kg kh ls ks kj kk lt kt km kn ko hb bi translated">其中伪计数α &gt; 0是平滑参数(α = 0对应于没有平滑)。</p></blockquote><p id="97dc" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">在这里阅读更多关于<strong class="jy hj">加法平滑</strong> <a class="ae mq" href="https://en.wikipedia.org/wiki/Additive_smoothing" rel="noopener ugc nofollow" target="_blank">的内容。</a></p><p id="ba0f" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">回到我们的问题，我们将选择α = 1，而“d”是数据集中唯一单词的数量，在我们的例子中是10。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mp"><img src="../Images/7d5010736736b1588801e5f096ca94f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RvDH6YKDKDnn_EUZfuuZOQ.png"/></div></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mr"><img src="../Images/4563d9afdc4eb2d2f10856e13fe6ebd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4wm4r-4MRgyuXNC-Kq6l8Q.png"/></div></div></figure><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es ms"><img src="../Images/7e7b491bff6072de608cba06173d6b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bfohzjLKPUve_kui8EL53A.png"/></div></div></figure><p id="89df" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated">现在，它正确地将邮件分类为垃圾邮件。</p><h2 id="ada7" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated"><strong class="ak">朴素贝叶斯分类器的类型</strong></h2><ul class=""><li id="5d7e" class="lu lv hi jy b jz ka kc kd jj mt jn mu jr mv ko lz ma mb mc bi translated"><strong class="jy hj">多项式:</strong>特征向量表示通过多项式分布产生某些事件的频率。例如，计算每个单词在文档中出现的频率。这是通常用于文档分类的事件模型。</li><li id="c780" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated"><strong class="jy hj">伯努利:</strong>像多项式模型一样，该模型对于文档分类任务是流行的，其中使用二元术语出现(即，单词是否在文档中出现)特征，而不是术语频率(即，单词在文档中的频率)。</li><li id="d4bd" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated"><strong class="jy hj">高斯:</strong>用于分类，假设特征遵循正态分布。</li></ul><h2 id="d5c5" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated"><strong class="ak">朴素贝叶斯的利弊</strong></h2><p id="f7a9" class="pw-post-body-paragraph jw jx hi jy b jz ka ij kb kc kd im ke jj kf kg kh jn ki kj kk jr kl km kn ko hb bi translated"><strong class="jy hj">优点:</strong></p><ul class=""><li id="a7f3" class="lu lv hi jy b jz kp kc kq jj lw jn lx jr ly ko lz ma mb mc bi translated">需要少量的训练数据。所以训练花费的时间更少。</li><li id="381e" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">处理连续和离散数据，对无关特征不敏感。</li><li id="41ff" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">非常简单、快速、易于实现。</li><li id="4221" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">可用于二元和多类分类问题。</li><li id="2e32" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">高度可扩展，因为它随预测要素和数据点的数量线性扩展。</li><li id="821e" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">当朴素贝叶斯条件独立性假设成立时，它将比逻辑回归等判别模型收敛得更快。</li></ul><p id="593e" class="pw-post-body-paragraph jw jx hi jy b jz kp ij kb kc kq im ke jj kr kg kh jn ks kj kk jr kt km kn ko hb bi translated"><strong class="jy hj">缺点:</strong></p><ul class=""><li id="a6ca" class="lu lv hi jy b jz kp kc kq jj lw jn lx jr ly ko lz ma mb mc bi translated">独立预测因子/特征的假设。朴素贝叶斯隐含地假设所有的属性都是相互独立的，这在现实世界的数据中几乎是不可能找到的。</li><li id="555a" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">如果分类变量的某个值出现在测试数据集中，而没有在训练数据集中观察到，则模型将为其分配零概率，并且无法进行预测。这就是我们所说的<strong class="jy hj">“零频率问题”</strong>，可以使用平滑技术来解决。</li></ul><h2 id="f361" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated"><strong class="ak">朴素贝叶斯算法的应用</strong></h2><ul class=""><li id="436b" class="lu lv hi jy b jz ka kc kd jj mt jn mu jr mv ko lz ma mb mc bi translated">实时预测。</li><li id="eaf5" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">多类预测。</li><li id="dbd6" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">文本分类/垃圾邮件过滤/情感分析。</li><li id="a0c4" class="lu lv hi jy b jz md kc me jj mf jn mg jr mh ko lz ma mb mc bi translated">推荐系统。</li></ul></div></div>    
</body>
</html>