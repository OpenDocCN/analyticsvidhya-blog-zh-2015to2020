<html>
<head>
<title>Most widely used Optimization techniques: Optimizing Algorithms.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最广泛使用的优化技术:优化算法。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/most-widely-used-optimization-techniques-optimizing-algorithms-b81015ea377d?source=collection_archive---------8-----------------------#2020-01-04">https://medium.com/analytics-vidhya/most-widely-used-optimization-techniques-optimizing-algorithms-b81015ea377d?source=collection_archive---------8-----------------------#2020-01-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/861d19e8c3ec7d4ad80851cff311c4fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VHzmRqOxQH5ieys9.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图:梯度下降</figcaption></figure><p id="8bc9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面提到的是一些广泛使用的优化算法，我将在这篇文章中讨论</p><ol class=""><li id="0502" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated"><strong class="iw hj">香草渐变下降，</strong></li><li id="12d7" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hj">基于动量梯度下降，</strong></li><li id="d819" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hj">内斯特罗夫加速梯度下降，</strong></li><li id="f319" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hj">阿达格拉德，</strong></li><li id="519a" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hj"> RMSProp，</strong></li><li id="7f84" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hj">亚当</strong></li></ol><p id="5c34" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在进入这些技术之前，让我告诉你为什么我们想出这些技术，或者我们通常用于更新权重和偏差的梯度下降更新规则有什么问题，如下所示</p><p id="b048" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">w = w—eta * dw————-(1)</strong></p><p id="66cf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">其中，‘w’—与输入相关的权重，eta —学习率，dw —损失w.r.t权重的导数。让我们分析一下斜率的图形</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es kg"><img src="../Images/663c71980db86b155e84867679b7a9f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*pq2Ul_g5VP8fXuS2Thub-g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图1:斜率</figcaption></figure><p id="4c09" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上图可以看出，当斜率较陡时，导数较高，当斜率较缓时，导数较低。您可能想知道它是如何与我们的更新规则相关联的？。</p><p id="68e0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，当我们的初始化点在平坦区域时，这可能是个问题，因为算法移动得不够快。如果碰巧我们的‘w’和‘b’的随机初始化在平坦区域开始，那么该算法将需要运行许多时期来脱离平台，并且运行许多时期可能会花费我们更多的时间。</p><p id="fbc1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">梯度下降的一个主要问题是，由于这些区域中的梯度非常小，所以需要花费大量时间来导航缓坡区域。</p><blockquote class="kl km kn"><p id="ff4b" class="iu iv ko iw b ix iy iz ja jb jc jd je kp jg jh ji kq jk jl jm kr jo jp jq jr hb bi translated">好了，现在我们要知道为什么我们会想出新的技术，对吧！</p></blockquote><h2 id="436a" class="ks kt hi bd ku kv kw kx ky kz la lb lc jf ld le lf jj lg lh li jn lj lk ll lm bi translated"><strong class="ak">当有平坦区域时，我们可以考虑做什么？</strong></h2><p id="81e2" class="pw-post-body-paragraph iu iv hi iw b ix ln iz ja jb lo jd je jf lp jh ji jj lq jl jm jn lr jp jq jr hb bi translated">只是一个模糊的想法是迈出更大的步伐吧！。因此，一个直观的解决方案<strong class="iw hj"> </strong>将是，如果算法被反复要求向同一方向前进，那么它可能会获得一些信心，并开始在该方向上迈出更大的步伐。</p><p id="cacb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">比方说，当你去某个购物市场，被问到不知道路线时，有人会说直走大约1公里，你就可以找到商店了。既然这样说了，你就不会在100米后问每个人同样的路线，对吗？所以你有信心，移动1公里会让你到达你想要的目的地，这些算法也是如此。</p><h2 id="b27e" class="ks kt hi bd ku kv kw kx ky kz la lb lc jf ld le lf jj lg lh li jn lj lk ll lm bi translated">各种优化算法:</h2><p id="de63" class="pw-post-body-paragraph iu iv hi iw b ix ln iz ja jb lo jd je jf lp jh ji jj lq jl jm jn lr jp jq jr hb bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> V </span></p><p id="6b00" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">ωt+1 =ωt—η* ∇ωt———-(2)</strong></p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/6715f688cd6cd9ae43fb9bf2d9b7514d.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*CUta-9JfMnMpaFx5UKTPTA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图2:VGD代码</figcaption></figure><p id="4f3c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> M </span> <strong class="iw hj">基于网膜的梯度下降:</strong>现在不是直接更新权重，而是让在它移动的方向上加速一点，这样它在同一个方向上移动的步数就少了。</p><p id="d7cb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">这背后的直觉</strong>是，随着我们沿着一个系列或方向越走越远，随着我们沿着同一方向前进，我们可以越来越不重视后面的梯度，因为随着我们前进，“伽马”越来越强，这意味着它开始衰减，而对我们已经采取的最后一步越来越重要。</p><p id="f4fb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">υ<em class="ko">t</em>=γ*υ<em class="ko">t</em>1+η∇ω<em class="ko">t———-(3)</em></strong></p><p id="53d5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">ωt+1 =ωt—υ<em class="ko">t————-(4)</em>T17】</strong></p><p id="594a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">υt1-运动历史，γ-范围从0-1，η-学习率</p><p id="bfe4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里，我们取一个指数衰减的加权和，随着我们越来越深入这个序列，权重衰减得越来越多。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/f549b92f063027cd1925d079fa01aec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*q2soBgvPO6pCI96hFD0yNA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图3(a) : MBGD(29个时期)图3(b) : VGD(40个时期)</figcaption></figure><p id="d5bb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上面的图中我们可以看到，在图3(a)中，仅仅29个时期，即使在缓坡区域，MBGD也能够迈出大步，因为动量在继续。但是在图3(b)中，花了40个时期，也不能越过平台。</p><blockquote class="kl km kn"><p id="5124" class="iu iv ko iw b ix iy iz ja jb jc jd je kp jg jh ji kq jk jl jm kr jo jp jq jr hb bi translated">动作快总是好的吗？</p><p id="f9b4" class="iu iv ko iw b ix iy iz ja jb jc jd je kp jg jh ji kq jk jl jm kr jo jp jq jr hb bi translated">会不会出现一种情况，动力会促使我们跑过我们的目标？</p></blockquote><p id="8435" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">不，总是快速移动不是一个好主意，因为会出现超调和围绕最小值振荡的问题。因此，这是MBGD和VGD的高学习率超调的共同问题。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es md"><img src="../Images/bb626b99021d5e147d941c96e8b3d160.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*Mxi3IiGWbu9Winti5-UwpA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图3(c):代码_MBGD</figcaption></figure><p id="0ba8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">所以为了解决超调的问题，他们引入了所谓的内斯特罗夫加速梯度下降。</p><p id="bbf9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> N </span>我们在这里做的是，首先根据过去的历史，然后在向前移动之前，计算第二步在该点的导数，而在动量中，我们在迈出一步之前计算导数，然后根据历史和当前移动，这将导致超调问题。</p><p id="1170" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们后退一点，看看MBGD的方程——将方程(3)代入方程(4)</p><p id="9e54" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">ωt+1 =ωt—γ*υ<em class="ko">t</em>1—η∇ω<em class="ko">t———-(5)</em></strong></p><p id="d21b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上面的等式中我们可以看出，运动分两步进行</p><p id="a9d2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一、首先是历史项—γ*υ<em class="ko">t</em>—1</p><p id="b53d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">二。接下来是关于重量项——η∇ω<em class="ko">t</em></p><p id="4e77" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，每次移动两步时，都有可能超过两步之间的最小值。所以我们可以考虑先用历史项移动，然后从第一步(ωtemp)后我们所处的位置开始计算第二步。</p><p id="d4bd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">利用上述直觉，<strong class="iw hj">内斯特罗夫加速梯度下降</strong>解决了超调和多次振荡的问题。</p><p id="b619" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">ωtemp =ωt—γ*υt-1———-(6)；</strong>根据运动和历史进行计算</p><p id="7848" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">ωt+1 =ωtempη* ∇ωtemp—--(7)；</strong>向ωtemp的导数方向进一步移动。</p><p id="03c9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">υt =γ*υt-1+η* ∇ωtemp———-(8)；</strong>根据ωtemp的导数更新运动历史。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/d8c3ecbbaaed75f702ecd9297f35f640.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*5hqqkT-3E5Fb9SIt6a3Mxg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图4(a) : Code_NAG</figcaption></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/2fc478db10d37093503086cab9f3e475.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*EFCtpMp9xDfLkhjL_q8EEg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图4(b):图表_</figcaption></figure><p id="fd29" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">向前看有助于NAG比MBGD更快地纠正其路线。因此，振荡更小，逃离极小值谷的机会也更小。</p><p id="e056" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">A</span><strong class="iw hj">daGrad:</strong>adagrad的动机来自于数据集的稀疏和密集特征。当在特定特征中，如果大多数点是‘0’，那么更新权重没有意义，因为这意味着因为‘0’值。因此，我们可以做的是，对于1的数量少于0的情况，我们可以提高这些特征的学习率，降低稀疏特征的学习率。</p><p id="3912" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Adagrad的基本直觉是参数的学习速率与其更新规则成比例地衰减。</p><p id="a478" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">υt =υt1+(∇ωt)^2——-(9)</strong></p><p id="f795" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在密集特征的情况下，它在大多数迭代中递增，导致更大的υt值。</p><p id="6c60" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于稀疏特征，它不会增加太多，因为梯度值通常为0，导致较低的υt值。</p><p id="1150" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">ωt+1 =ωt(∇ωt)*η/√((υt)+ε)—-(10)</strong></p><p id="6876" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">根据上面的等式，我们可以说分母项‘υt’用于调节学习速率η，这就是为什么它被称为自适应学习速率。</p><p id="ecb6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">I .对于密集特征，υt越大，√(υt)越大，从而降低η。</p><p id="e455" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">二。对于稀疏特征，vt更小，√(υt)变得更小，从而将η降低到更小的程度。“ε”项被添加到分母√(υt ) + ε中，以防止在非常稀疏的特征的情况下出现被零除的误差，即在测量实例之前所有数据点都为零。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/dfd1f067c76a8f38d26920ea24e05bb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*9Af1fPaWzcKos2-h9SOlRw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图5(a):代码_阿达格勒</figcaption></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/7cfbfb5c7dc4f11b807cf91e3de13b6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*5JSAtGWaxCC2Xhsa6a5cDg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图5(b):图表_</figcaption></figure><p id="8c15" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">黑色-香草梯度下降，红色-基于动量的梯度下降，蓝色-内斯特罗夫加速梯度下降，绿色-阿达格拉德</p><p id="1470" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，从图中我们可以观察到，对于曲线(黑色、红色和蓝色)，我的“w”最初没有变化，只有“b”在变化。这就是这些算法的问题所在，问题是我的“w”是一个非常稀疏的特征，这就是为什么它的导数在大多数情况下为零，并且没有更新，而“b”是它正在更新的大部分密集特征。</p><p id="61e7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从绿色曲线中我们可以清楚地看到，对应于稀疏特征的参数得到了更好的更新。但是仍然缺少收敛，RMS Prop开始解决这个收敛问题。</p><p id="eb5a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">随着分母的增长，LR衰减非常剧烈(对于对应于密集特征的参数来说，这不是好事)</p><p id="0d9f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> R </span>所以RM prop的动机或直觉是，为什么不衰减分母，并防止它衰减？对！。</p><p id="e18f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里我们采用指数衰减增长来使曲线收敛到最小值。</p><p id="3a68" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">υt = beta *υt 1+(1—beta)*(∇ωt)^2——-(11)</strong></p><p id="0209" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">ωt+1 =ωt(∇ωt)*η/√((υt)+ε)———-(12)</strong></p><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/58a31d9a5b030c0700fb7e54c59c423e.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*_8M2NDwJiKWTAIgikf37xQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图6(a) : Code_RMSProp</figcaption></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/f9d89fc5d4d8dad90dcff3b1ff6cd70b.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*ipWAQEld4NnWw3-gHbrruQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图6(b):图形_，布朗均方根Prop</figcaption></figure><p id="f87d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上面的图中我们可以说，Adagrad在接近收敛时被卡住了(由于学习速率的衰减，它不再能够在垂直(b)方向上移动)。RMS prop通过减少衰减来解决这个问题。</p><p id="edc5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> A </span> <strong class="iw hj"> dam : </strong>我们知道动量和RMS prop都在使用历史。动量用的是梯度的历史，均方根用的是梯度平方的历史，对吧！。</p><p id="e55c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，在动量的情况下，该累积历史实际上有助于快速移出平坦表面，并用于更新当前权重。在rms prop中，历史被用来调整学习率，学习率被适当地用来收缩或增长。</p><blockquote class="kl km kn"><p id="68ce" class="iu iv ko iw b ix iy iz ja jb jc jd je kp jg jh ji kq jk jl jm kr jo jp jq jr hb bi translated">我们能把这两者结合起来吗？。</p></blockquote><p id="0da0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">是的，我们可以利用动量和均方根来加快移动速度，同时利用均方根来防止学习速度过快或无效。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/d2136d9f984236dbef56a41ebf24d390.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*7R7Hixov2s7G1SmATCj4NQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图7(a):代码_Adam</figcaption></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/6360082d092a8adc5651dc61b3a1a3bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*D44tXGtl4qeRHUw4lhFiOQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图7(b):图表_，青色-亚当</figcaption></figure><p id="c8a7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上面的图中我们可以看到，它超过了最小值，因为它使用了基于动量的gd。</p><p id="3756" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">mt =β1 * vt-1+(1—β1)(wt)——-(13)</strong></p><p id="78b4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">vt =β2 * vt—1+(1—β2)(wt)2—-(14)</strong></p><p id="9416" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">wt+1 = wt—mt *η/√((vt)+ε)——-(15)</strong></p><p id="b5ed" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">等式(13)利用了动量的优势，等式(14)利用了rms prop的优势。并且如(15)所示执行更新。adam还提供了所谓的偏差校正，以确保训练更加平滑，或者说输入不像奇怪的那种。使用下面提到的等式更新mt和vt。</p><p id="2aa4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">mt = mt/(1—beta1^t)———-(16)</strong></p><p id="742c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">vt = vt/(1-beta2^t)——————-(17)</strong></p><p id="e8b4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这些概念来自四分之一的实验室:</p><div class="mm mn ez fb mo mp"><a href="https://padhai.onefourthlabs.in/" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">IIT马德拉斯大学教授PadhAI - AI课程</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">经济实惠的数据科学和人工智能课程，由来自IIT马德拉斯的一些最好的老师授课</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">padhai.onefourthlabs.in</p></div></div><div class="my l"><div class="mz l na nb nc my nd io mp"/></div></div></a></div><p id="7982" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">#深度学习#机器学习#数据科学#优化#亚当</p></div></div>    
</body>
</html>