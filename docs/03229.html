<html>
<head>
<title>Fast(Ai)Bert: Solving Emotion Recognition in Conversations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Fast(Ai)Bert:解决对话中的情感识别</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fast-ai-bert-solving-emotion-recognition-in-conversations-63e6b644f94b?source=collection_archive---------11-----------------------#2020-01-21">https://medium.com/analytics-vidhya/fast-ai-bert-solving-emotion-recognition-in-conversations-63e6b644f94b?source=collection_archive---------11-----------------------#2020-01-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4ac5" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">根据情感识别的特殊需求定制fast.ai框架的教程(分离序列Bert)</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/f154a4b4f4bc4967d648b4f0759c9f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYB6JO6B6iKPy4tEbd4Rwg.jpeg"/></div></div></figure><p id="32f9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">去年自然语言处理(NLP)中最有前途的模型之一似乎是谷歌团队的模型<a class="ae kf" href="https://arxiv.org/pdf/1810.04805v2.pdf" rel="noopener ugc nofollow" target="_blank"> Bert </a>。</p><p id="61e5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">接下来，在杰瑞米·霍华德和他的团队的努力下，课程和框架<a class="ae kf" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>帮助了成千上万的人进入机器学习领域，并让他们接触到许多最先进的算法。</p><p id="3baf" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">然而，如果你想在fasti.ai的框架下使用Bert或任何基于变形金刚的模型，很长一段时间你都没有机会这样做。在人们定制API的倡议下，这成为可能:</p><ul class=""><li id="92a5" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke kl km kn ko bi translated">Keita Kurita的文章<a class="ae kf" href="https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/" rel="noopener ugc nofollow" target="_blank"> <em class="kp">使用fast.ai </em> </a>微调BERT的教程，这使得pytorch_pretrained_bert库与fastai兼容</li><li id="56cd" class="kg kh hi jl b jm kq jp kr js ks jw kt ka ku ke kl km kn ko bi translated">Dev Sharma的文章<a class="ae kf" rel="noopener" href="/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c"> <em class="kp">将RoBERTa与fast.ai一起用于NLP </em> </a>这使得pytorch_transformers库与fast.ai兼容</li><li id="906e" class="kg kh hi jl b jm kq jp kr js ks jw kt ka ku ke kl km kn ko bi translated">maximilien Roberti<a class="ae kf" href="https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2" rel="noopener" target="_blank">article</a>自定义fast.ai以使用huggingface变形金刚模型</li></ul><h1 id="bb83" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">更多定制工作</h1><blockquote class="ln lo lp"><p id="c7dc" class="jj jk kp jl b jm jn ij jo jp jq im jr lq jt ju jv lr jx jy jz ls kb kc kd ke hb bi translated"><strong class="jl hj">TL；DR: </strong>我们通过fast.ai使得在Bert中使用分离的序列成为可能，我们的笔记本包含了定制工作和一个在对话中进行情绪识别的SemEval任务的应用，排名在最高分:<a class="ae kf" href="https://github.com/PhilippMaxx/semeval2019_task3" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj"> Github </strong> </a> <strong class="jl hj">。</strong></p></blockquote><p id="f1ac" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这些解决方案集中在<strong class="jl hj">单序列入单/多类出</strong>方法上。所有这些解决方案都不能处理单独序列的问题。如果我们在预训练中使用Bert，在输入中使用两个独立的序列是必要的。如果你考虑问题-答案，那么在下游任务中进行分离是有意义的，Bert可以针对这些任务进行微调。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lt"><img src="../Images/95297845b41c51844cd8847d725f7ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tUeogmBp3RxKNhoFzCag4A.png"/></div></div></figure><p id="dbae" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们发现，特别是如果你有数据，将对话中来自不同人的序列作为独立的输入来处理是一个非常好的主意。</p><p id="5dab" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">由于我们有一个下游的任务与对话的必要性，我们非常积极地扩展上述工作。</p><p id="fd77" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我的笔记本建立在Maximilien Roberti的工作基础上，我用这里提供的新定制进行了扩展:<a class="ae kf" href="https://github.com/PhilippMaxx/semeval2019_task3/blob/master/00_fastbert.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Philip maxx/seme val 2019 _ task 3/blob/master/00 _ fastbert . ipynb</a>。</p><h1 id="fe20" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">SemEval-2019任务3:文本中的上下文情感检测</h1><p id="ff59" class="pw-post-body-paragraph jj jk hi jl b jm lu ij jo jp lv im jr js lw ju jv jw lx jy jz ka ly kc kd ke hb bi translated">我们在这里展示我们的定制工作，因为我们为SemEval-2019任务3提出了我们的基线模型。应用笔记本可以在这里找到:<a class="ae kf" href="https://github.com/PhilippMaxx/semeval2019_task3/blob/master/01_task3.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/PhilippMaxx/seme val 2019 _ task 3/blob/master/01 _ task 3 . ipynb</a>。</p><p id="1e96" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">任务定义可以在这里找到:【https://www.aclweb.org/anthology/S19-2005.pdf】的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/d3bdce53bf4d00502826383555a6c1fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qejTS-sWMKwoGD3EsNzjmw.jpeg"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">来自SemEval-2019任务3的对话示例</figcaption></figure><p id="0325" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">凭借fast.ai的广泛功能和Bert的强大功能，我们已经跻身于<strong class="jl hj">前3% </strong>(在最初的311个竞争对手中排名第8)。与其他模型相比，在这项挑战中投入研究的努力(Bert已经可用)，我们在模型命题上花费了很少的时间，表现优于大多数模型。</p><p id="504c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">公平地说，当对数据的不同部分进行训练时，我们甚至达到了3个Bert大模型的集合的<strong class="jl hj">前1% </strong>(在311中排名第3)。然而，这是一个很好的结果，当谈到计算能力时，我们喜欢我们的模型越小越好。除了堆积模型之外，还可以有更多智能路线(Bert是第一次尝试)。</p><h1 id="8955" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">定制—第1部分</h1><p id="ddda" class="pw-post-body-paragraph jj jk hi jl b jm lu ij jo jp lv im jr js lw ju jv jw lx jy jz ka ly kc kd ke hb bi translated">定制工作比之前的工作要复杂一些。我们要深入挖掘fast.ai源代码。</p><p id="94ac" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们再次回顾一下我们的特殊需求。我们必须对一个对话或一个<em class="kp"> List[turn] </em>元素进行标记化，其中<em class="kp"> turn </em>是字符串形式，并且希望返回一个<em class="kp"> List[tokens] </em>来表示分隔的格式。该任务定义了第三个回合的情感标签，因此前两个回合可以被视为情境回合。所以我们把这些串联起来，从第三个回合开始分开(Bert只能分开两个序列):</p><pre class="iy iz ja jb fd me mf mg mh aw mi bi"><span id="6834" class="mj kw hi mf b fi mk ml l mm mn"><strong class="mf hj">[CLS] + tok(turn_1) + tok(turn_2) + [SEP] + tok(turn_3) + [SEP]</strong></span><span id="dab9" class="mj kw hi mf b fi mo ml l mm mn"><strong class="mf hj"><em class="kp">tok </em></strong><em class="kp">:= Bert tokenizer</em><strong class="mf hj"><em class="kp"><br/>[SEP]</em></strong><em class="kp">:= Bert seperator token</em><strong class="mf hj"><em class="kp"><br/>[CLS]</em></strong><em class="kp">:= Bert classification token</em></span></pre><p id="66c9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">正如我们所知，fast.ai在为文本数据创建databunch时，使用a<strong class="jl hj"><em class="kp">tokenize processor</em></strong>对象执行标记化步骤。这个processer类再次获得一个<em class="kp">标记器</em>作为类对象<strong class="jl hj"> <em class="kp">标记器</em> </strong>。有趣的是，<strong class="jl hj"> <em class="kp">记号赋予者</em> </strong>再次从<strong class="jl hj"> <em class="kp">基础记号赋予者</em> </strong>获得记号化能力。我们终于可以开始了。类<strong class="jl hj"><em class="kp">base tokenizer</em></strong>得到一个对象<em class="kp"> _pretrained_tokenizer </em>，并拥有类函数<em class="kp"> tokenizer </em>，最终通过所有这些封装用于标记化。</p><p id="6981" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">所以我们从第一层开始，<strong class="jl hj"><em class="kp">BaseTokenizer</em></strong>。可以看到，与之前的定制工作相比，我们只是更改了最后几行。重点是让<em class="kp">记号赋予器</em>函数能够接受元素<em class="kp">列表【turn】</em>。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="88dd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在我们有了一个<strong class="jl hj"><em class="kp">base tokenizer</em></strong>，它能够对<em class="kp">List【turn】</em>元素进行单独的连接。完成后，我们可以创建一个<strong class="jl hj"> <em class="kp">记号赋予器</em> </strong>来处理这些元素。这里不需要做任何工作(只需要改变一种类型的注释—如果需要，查看<a class="ae kf" href="https://github.com/PhilippMaxx/semeval2019_task3/blob/master/00_fastbert.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj">笔记本</strong> </a>了解详情)。</p><p id="deae" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">下一个挑战是让<strong class="jl hj"><em class="kp">tokenize processor</em></strong>类能够使用<em class="kp">List【turn】</em>的新输入形式。在fast.ai中，<strong class="jl hj"><em class="kp">tokenize processor</em></strong>类将一列<em class="kp"> str </em>处理成一个串联，然后对整个文本使用<em class="kp"> tokenizer </em>。这里我们去掉了连接函数(<em class="kp"> _join_texts </em>)。</p><p id="1b06" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">以定制的形式使用<em class="kp"> _join_texts </em>函数，通过一个特殊的令牌连接转弯来完成我们的方法，会更优雅。这种方法的一个问题是，<strong class="jl hj"><em class="kp">bertokenizer</em></strong>不能区分文本和文本中的特殊标记。我们必须在标记化之后添加特殊标记。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="458e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">幸运的是，其他定制可以接管，所以我们已经得到了我们的数据束。我们可以转到下一个问题——是的😊</p><p id="747e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对于单独的Bert，我们不仅需要序列的<em class="kp">输入标识</em>，还需要<em class="kp">注意屏蔽</em>和<em class="kp">令牌类型标识</em>。这确保了模型可以正确地识别分离和填充。</p><p id="afcc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">当查看模型设置时，我们可以通过在正向传递中使用效用函数来做一个简单的技巧。因此，我们有两个实用函数从<em class="kp"> input_ids </em>批次中检索掩码。通过使用pytorch和numpy操作，我们得到了这个函数。请记住，我们正在运行中，必须管理GPU的VRAM。这就是为什么我们在CPU上做这些计算。尽管如此，这些都非常快，因为numpy和pytorch运行在一个<em class="kp"> C++ </em>后端，它不影响性能。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="c165" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，我们有了一个可以即时检索输入的<em class="kp"> token_type_ids </em>的函数，我们可以在模型的前向传递中使用它。</p><p id="db12" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="kp"> attention_mask </em>是one liner。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="5312" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">从现在开始，定制更多地与SemEval挑战的任务相关。</p><h1 id="a6a9" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">定制—第2部分</h1><p id="c973" class="pw-post-body-paragraph jj jk hi jl b jm lu ij jo jp lv im jr js lw ju jv jw lx jy jz ka ly kc kd ke hb bi translated">仍有一些未完成的任务。正如我们在挑战赛文章中看到的，我们对矩阵有特殊要求。此外，由于数据不平衡，重新加权损失函数将是一个好主意。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="2d28" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">请注意，我们正在尝试最大化三种情绪类别的微观平均F1分数— <strong class="jl hj">快乐、</strong>悲伤和<strong class="jl hj">愤怒</strong>。准确地说，定义的指标如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mr"><img src="../Images/9f8c37804639d7fc4aceb3a6c574285f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8SCQ0KiDc1DiXwJyCae96A.jpeg"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">TPᵢ、FPᵢ、FNᵢ是第一类<em class="ms">的真阳性、假阳性和假阴性</em></figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/6c484650d058f255eeb8ac6177e34546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfZR78HPMvbIObUwAIh7ug.jpeg"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">微观平均F1分数是调和平均值</figcaption></figure><p id="3035" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">因此，我们将编写微观平均F1分数作为定制指标(Credits:<a class="ae kf" href="https://github.com/juliusberner/emotion_transformer/blob/master/01_model.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Julius berner/emotion _ transformer/blob/master/01 _ model . ipynb</a>):</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mp mq l"/></div></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mp mq l"/></div></figure><h1 id="d78f" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">结论</h1><p id="4cfe" class="pw-post-body-paragraph jj jk hi jl b jm lu ij jo jp lv im jr js lw ju jv jw lx jy jz ka ly kc kd ke hb bi translated">在结合fast.ai和Bert模型的其他例子中，我们看到人们试图使用fast.ai版本，这也用于训练<em class="kp"> Ulmfit </em>。即逐层解冻模型。需要记住的是，这对于<em class="kp"> Ulmfit </em>来说是一个很好的实践，但对于Bert模型来说不一定。微调<em class="kp"> Ulmfit </em>时，有一个微调模型语言模型的第一阶段，以适应训练数据中的特定语言。然后在分类中进行微调。然而，在Bert中，语言和分类的微调是同时进行的。额外的实验可以测试当另一个预任务在之前完成并且因此语言模型已经被预训练时，该方法是否再次适用。</p><p id="e596" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">好吧，但是为什么把具有分离序列的Bert模型引入fast.ai的麻烦值得呢？嗯，如果我们考虑fast.ai的框架，仍然有很多可以尝试的地方。仅举几个例子:区别微调、逐步解冻、标签平滑、混合……还有许多任务将受益于这种输入使能。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/fb489d7528c0b69dec6342efef19a72b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YDZXi9kqJcF0MKlTq5_WoA.jpeg"/></div></div></figure><p id="8683" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果您通过使用我们的工作体验到了新的结果，请告诉我们😊</p></div></div>    
</body>
</html>