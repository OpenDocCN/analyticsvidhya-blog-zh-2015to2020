<html>
<head>
<title>The recent Queen of ML Algorithms: XGBoost, and it's future</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最近的ML算法女王:XGBoost及其未来</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-recent-queen-of-ai-algos-xgboost-and-its-future-22d6df3cd206?source=collection_archive---------8-----------------------#2019-10-13">https://medium.com/analytics-vidhya/the-recent-queen-of-ai-algos-xgboost-and-its-future-22d6df3cd206?source=collection_archive---------8-----------------------#2019-10-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="73a6" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">“有人称之为人工智能，但事实是这项技术将增强我们的能力。因此，我认为我们应该增强我们的智能，而不是人工智能。”—吉尼·罗梅蒂</p></blockquote></div><div class="ab cl jh ji gp jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="hb hc hd he hf"><p id="5b86" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">二叉树、随机森林和回归算法等算法统治ML世界已经有十年了，直到2014年推出了最新的XGBoostalgorithm，<strong class="il hj"> <em class="ik">陈天琦。</em>T3】</strong></p><p id="212a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated"><strong class="il hj"> XGBoost </strong>是一种基于决策树的集成机器学习算法，它使用了梯度推进框架。在涉及非结构化数据(图像、文本等)的预测问题中。)人工神经网络往往优于所有其他算法或框架。</p><p id="3428" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">XGBoost代表<strong class="il hj"> <em class="ik">极限渐变提升。</em> </strong>就像是‘类固醇’上的梯度推进机。它是梯度推进决策树算法的实现，有几个开发者的贡献，现在归属于组织<strong class="il hj">分布式机器学习社区或DMLC。</strong></p><p id="6692" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">那么两者的基本区别是什么呢？</p><p id="f656" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated"><strong class="il hj"><em class="ik">XGBoost这个名字实际上是指提升树算法的计算资源极限的工程目标。这也是很多人使用XGBoost的原因。</em>T15】</strong></p><p id="705a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated"><strong class="il hj"> <em class="ik"> —陈天齐</em> </strong></p></div><div class="ab cl jh ji gp jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="hb hc hd he hf"><h1 id="c4ca" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">了解XGBoost</h1><p id="080b" class="pw-post-body-paragraph ii ij hi il b im kp io ip iq kq is it jo kr iw ix jp ks ja jb jq kt je jf jg hb bi translated">要理解XGBoost，首先要理解梯度下降和梯度提升。</p><p id="fbcc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated"><strong class="il hj"> a)梯度下降:</strong></p><p id="39bf" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">成本函数衡量预测值与相应的实际值有多接近。理想情况下，我们希望预测值和实际值之间的差异尽可能小。因此，我们希望成本函数最小化。</p><p id="f35e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">与已训练模型相关联的权重使其预测出接近实际值的值。因此，与模型关联的权重越好，预测值就越准确，成本函数就越低。随着训练集中的记录越来越多，权重被学习然后被更新。</p><p id="22d7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">梯度下降是一种迭代优化算法。这是一种最小化具有多个变量的函数的方法。因此，梯度下降可用于最小化成本函数。它首先使用初始权重运行模型，然后通过多次迭代更新权重来寻求最小化成本函数。</p><p id="77da" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated"><strong class="il hj"> b)梯度增强:</strong></p><p id="cdd4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated"><strong class="il hj"><em class="ik"/></strong>:构建弱学习者的集合，其中错误分类的记录被赋予更大的权重(“提升”)，以在稍后的模型中正确预测它们。这些弱学习者随后被组合成一个强学习者。Boosting算法有AdaBoost、梯度Boosting、XGBoost等多种。该图描述了树集合模型。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/a04f5d534ae078675d299cc62af8b121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sbjDRNttpHAzyAaQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">预测给定用户是否喜欢电脑游戏的树集成模型。+2，+0.1，-1，+0.9，-0.9是每片叶子中的预测分数。给定用户的最终预测是来自每棵树的预测的总和。</figcaption></figure><p id="3c79" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">梯度推进将梯度下降和推进的原理带到监督学习中。梯度增强模型(GBM)是按顺序连续构建的树。在GBM中，我们取多个模型的加权和。</p><ul class=""><li id="2965" class="lk ll hi il b im in iq ir jo lm jp ln jq lo jg lp lq lr ls bi translated">每个新模型使用<strong class="il hj">梯度下降</strong>优化来更新/修正模型要学习的权重，以达到成本函数的局部最小值。</li><li id="aa21" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg lp lq lr ls bi translated">分配给每个模型的权重向量不是从先前模型的错误分类和分配给错误分类的结果增加的权重中导出的，而是从通过梯度下降优化以最小化成本函数的权重中导出的。梯度下降的结果是和开始一样的模型的函数，只是参数更好。</li><li id="8f9a" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg lp lq lr ls bi translated">梯度<strong class="il hj"> Boosting </strong>在每一步的现有函数上增加一个新的函数来预测输出。梯度增强的结果从一开始就是完全不同的函数，因为结果是多个函数的相加。</li></ul><p id="2474" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated"><strong class="il hj"> c) XGBoost: </strong></p><p id="325d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">XGBoost的构建是为了推动提升树的计算资源的极限。XGBoost是GBM的一个实现，有很大的改进。GBM按顺序构建树，但是XGBoost是并行的。这使得XGBoost速度更快。</p></div><div class="ab cl jh ji gp jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="hb hc hd he hf"><h1 id="d50d" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak">使其成为算法女王的一些主要特性</strong></h1><p id="8572" class="pw-post-body-paragraph ii ij hi il b im kp io ip iq kq is it jo kr iw ix jp ks ja jb jq kt je jf jg hb bi translated">在开发和设计XGBoost算法时，主要考虑的是<strong class="il hj">速度</strong>和<strong class="il hj">模型性能增强。</strong>此外，它支持scikit-learn的功能，如三种类型的梯度增强，即<strong class="il hj">梯度增强、正则化梯度增强和随机梯度增强。</strong>它还专注于支持一系列不同类型的计算，但这是由一系列不同的参数决定的，数据集的大小和结果的要求形式是其中的一部分。</p><ul class=""><li id="c24d" class="lk ll hi il b im in iq ir jo lm jp ln jq lo jg lp lq lr ls bi translated"><strong class="il hj">树构造的并行化</strong>在训练期间使用你所有的CPU核心。</li><li id="512a" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg lp lq lr ls bi translated"><strong class="il hj">分布式计算</strong>使用机器集群训练非常大的模型。</li><li id="8344" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg lp lq lr ls bi translated"><strong class="il hj">核外计算</strong>适用于内存容纳不下的超大型数据集。</li><li id="5f7b" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg lp lq lr ls bi translated"><strong class="il hj">数据结构和算法的缓存优化</strong>充分利用硬件。</li><li id="3baa" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg lp lq lr ls bi translated"><strong class="il hj">正规化学习目标:</strong></li></ul><p id="23d3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">在给定一组参数的情况下，要测量模型的性能，我们需要定义一个目标函数。一个目标函数必须总是包含两部分:训练损失和正则化。正则项降低了模型的复杂性。</p><p id="7e34" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated"><strong class="il hj">Obj(θ)= L(θ)+ω(θ)</strong></p><p id="c76d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">其中ω是大多数算法忘记包含在目标函数中的正则项。但是，XGBoost包括正则化，从而控制模型的复杂性，防止过拟合。</p><p id="ed2b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">其他一些区别于它的特征是-</p><ul class=""><li id="4171" class="lk ll hi il b im in iq ir jo lm jp ln jq lo jg lp lq lr ls bi translated"><strong class="il hj">应用广泛</strong>:可用于解决回归、分类和用户自定义预测问题。</li><li id="7618" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg lp lq lr ls bi translated"><strong class="il hj">可移植性</strong>:在Windows、Linux、MacOsX上运行流畅。</li><li id="8077" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg lp lq lr ls bi translated"><strong class="il hj">语言</strong>:支持所有主流编程语言，包括C++、Python、R、Java、Scala、Julia。</li><li id="9444" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg lp lq lr ls bi translated"><strong class="il hj">云集成</strong>:支持AWS、Azure和Yarn集群，与Flink、Spark和其他生态系统配合良好。</li></ul><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="ab fe cl ly"><img src="../Images/fd40ad249f8b3e2c64f679c723518099.png" data-original-src="https://miro.medium.com/v2/format:webp/0*tzKt_ABz5rRrsGoW.jpg"/></div></figure></div><div class="ab cl jh ji gp jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="hb hc hd he hf"><h1 id="6522" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">其卓越性能的一些证据</h1><p id="ffb1" class="pw-post-body-paragraph ii ij hi il b im kp io ip iq kq is it jo kr iw ix jp ks ja jb jq kt je jf jg hb bi translated">最近<strong class="il hj">Szilard Pafka</strong>ELTE布达佩斯tvö大学的首席数据科学家执行了一些关于XGBoost算法的最新基准测试，与梯度提升和袋装决策树进行了速度比较。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lz"><img src="../Images/d5bf79283072188f736c5bf1f8cb5c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*ew3VkVil1XlTYpcd.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">由<strong class="bd jt"> Szilard Pafka </strong>获得的结果</figcaption></figure></div><div class="ab cl jh ji gp jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="hb hc hd he hf"><h2 id="0c0e" class="ma js hi bd jt mb mc md jx me mf mg kb jo mh mi kf jp mj mk kj jq ml mm kn mn bi translated">XGBoost的构建使用了哪些算法，它是如何超越其他算法的？</h2><p id="29ba" class="pw-post-body-paragraph ii ij hi il b im kp io ip iq kq is it jo kr iw ix jp ks ja jb jq kt je jf jg hb bi translated">这种算法有许多不同的名字，如梯度推进、多重加法回归树、随机梯度推进或梯度推进机器。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mo"><img src="../Images/a6b3bc9661c9cc0526316dfa29aef1a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/0*bPQfYy-72PkpV6ck.png"/></div></figure><p id="35b7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">Boosting基本上是一种添加新模型来纠正现有模型所产生的错误的技术。模型按顺序添加，直到没有错误需要消除。一个流行的例子是Ada Boost算法，它对难以预测的数据点进行加权。</p><p id="657a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">梯度推进是一种方法，其中创建预测先前模型的误差的新模型，然后将这些新模型相加在一起以进行最终预测。它被称为梯度提升，因为它使用梯度下降算法来最小化添加新模型时的损失。</p><p id="1779" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">这种方法支持回归和分类预测建模问题。</p><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mp mq l"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">为了深入了解，一定要看一看</figcaption></figure><figure class="kv kw kx ky fd kz"><div class="bz dy l di"><div class="mp mq l"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">在Google Cloud上实现XGBoost</figcaption></figure></div><div class="ab cl jh ji gp jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="hb hc hd he hf"><h1 id="ddc7" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">现在，什么时候使用XGBoost，什么时候不使用</h1><h2 id="d94e" class="ma js hi bd jt mb mc md jx me mf mg kb jo mh mi kf jp mj mk kj jq ml mm kn mn bi translated">XGBoost是一个更好的选择，</h2><ol class=""><li id="b4f0" class="lk ll hi il b im kp iq kq jo mr jp ms jq mt jg mu lq lr ls bi translated"><strong class="il hj">你有大量的训练样本</strong></li></ol><p id="1c85" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">→超过1000个训练样本，不到100个特征。</p><p id="50be" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">→特征数量少于训练样本数量。</p><p id="eeda" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jo iv iw ix jp iz ja jb jq jd je jf jg hb bi translated">2.<strong class="il hj">你混合了分类和数字特征</strong></p><h2 id="0179" class="ma js hi bd jt mb mc md jx me mf mg kb jo mh mi kf jp mj mk kj jq ml mm kn mn bi translated">XGBoost不太适合</h2><ol class=""><li id="94a4" class="lk ll hi il b im kp iq kq jo mr jp ms jq mt jg mu lq lr ls bi translated">图像识别</li><li id="0858" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg mu lq lr ls bi translated">计算机视觉</li><li id="3644" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg mu lq lr ls bi translated">自然语言处理</li><li id="95be" class="lk ll hi il b im lt iq lu jo lv jp lw jq lx jg mu lq lr ls bi translated">当训练样本数量明显小于特征数量时。</li></ol></div><div class="ab cl jh ji gp jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="hb hc hd he hf"><h1 id="22a2" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">未来是什么样子的？</h1><p id="b611" class="pw-post-body-paragraph ii ij hi il b im kp io ip iq kq is it jo kr iw ix jp ks ja jb jq kt je jf jg hb bi translated">机器学习是一个非常活跃的研究领域，已经有几个可行的XGBoost替代方案。微软研究院最近发布了一个轻量级的GBM框架，显示了巨大的潜力。Yandex Technology开发的Cat Boost一直在提供令人印象深刻的基准测试结果。我们有一个更好的模型框架，在预测性能、灵活性、可解释性和实用性方面胜过XGBoost，这只是时间问题。然而，在一个强大的挑战者出现之前，XGBoost将继续统治机器学习世界！</p></div></div>    
</body>
</html>