<html>
<head>
<title>Introduction to Neural Networks and Activation Functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络和激活函数简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-neural-networks-and-activation-functions-58e26887c52d?source=collection_archive---------4-----------------------#2019-09-22">https://medium.com/analytics-vidhya/introduction-to-neural-networks-and-activation-functions-58e26887c52d?source=collection_archive---------4-----------------------#2019-09-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/8f51abb5089ef17c1b16658dbd130c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yrgbW7GvOcp94f-5HZcmyQ.jpeg"/></div></div></figure><div class=""/><p id="aaad" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在这篇博客中，我们将讨论:</p><ol class=""><li id="71e4" class="jp jq ht it b iu iv iy iz jc jr jg js jk jt jo ju jv jw jx bi translated">神经网络:简单的神经网络及其工作原理</li><li id="4cdc" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">激活函数:简介和三个广泛使用的激活函数:</li></ol><p id="81cb" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.a — Sigmoid函数</p><p id="9037" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.b — ReLu(整流线性单元)</p><p id="aa40" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.c —泄漏ReLu</p><p id="db82" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">那么，我们开始吧。</p><h2 id="f944" class="kd ke ht bd kf kg kh ki kj kk kl km kn jc ko kp kq jg kr ks kt jk ku kv kw kx bi translated"><em class="iq">神经网络:</em></h2><p id="c9aa" class="pw-post-body-paragraph ir is ht it b iu ky iw ix iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo hb bi translated">当对人脑进行分析时，发现它有数十亿个神经元以网状结构相互连接。那么，什么是神经元？神经元也可以被称为任何深度学习算法的构建模块。由于深度学习是我们试图复制的，所以简单来说，神经元也是任何深度学习算法的构建模块。</p><p id="130e" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这是一张人类大脑神经元的照片。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ld"><img src="../Images/d6d7810af2a4b5ce4989c642e16d881f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mQRHqkeYFWGPOHBUv-2-zg.jpeg"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated">参考:谷歌图片</figcaption></figure><p id="f5a5" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">人类神经元的这种结构具有作为输入的树突、作为处理区域的核和作为输出的轴突。正如我所说的，神经网络也受到了下图的启发。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es lm"><img src="../Images/a62c5b64414c9683a42dadf8b5432ded.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*S44K1nRJSuNz9CkxbVmnGA.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">参考:谷歌图片</figcaption></figure><p id="ccbb" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">神经元的基本结构包括:</p><ol class=""><li id="5d9b" class="jp jq ht it b iu iv iy iz jc jr jg js jk jt jo ju jv jw jx bi translated">一个或多个输入，即<em class="ln"> x </em></li><li id="be8b" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">一个加工区。</li><li id="3938" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">以及一个或多个输出，即<em class="ln"> y </em></li></ol><p id="a88d" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">单个神经元的描述如下:</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es lo"><img src="../Images/62a46119e34708129573d001e7e16e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*KVAcJcGSOc_2KxZg5fBSMA.png"/></div></figure><p id="daab" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这是一个包含以下元素的神经元:</p><ol class=""><li id="f033" class="jp jq ht it b iu iv iy iz jc jr jg js jk jt jo ju jv jw jx bi translated">输入:<em class="ln"> x1 </em>和<em class="ln"> x2 </em>，它们各自的权重为<em class="ln"> w1 </em>和<em class="ln"> w2 </em>。我们还有一个偏置值，即<em class="ln"> b </em></li><li id="0a5e" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">输出:<em class="ln"> a </em></li><li id="7923" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">处理:将所有带有偏差项<em class="ln"> b </em>的输入乘以各自的权重后相加。</li></ol><p id="dc0b" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这里我们还有两个权重项和有偏项<em class="ln"> b </em>为什么我们需要它们？在神经网络中，一些输入对于获得精确的输出更为重要。因此，我们使用权重来指定哪个输入应该具有更大的影响。首先我们随机初始化它们，然后更新它们以最小化误差。我们用有偏项来表示与输入无关的被激活的输出量。</p><p id="a09c" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">希望你了解一个神经元的基本结构。现在的问题是单个神经元是如何工作的？</p><p id="fb85" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在回答这个问题之前，我们应该熟悉激活函数。</p><p id="5949" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="ln">什么是激活功能？我们为什么要使用它们？</em></p><p id="2890" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">所以，不管你有多复杂的结构，工作都是相似的，为了简单起见，我用一个输入和一个输出的神经元。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es lp"><img src="../Images/f87241a531bb6dc8849cdba724208e02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Zwj2bPtKwfG0aTJz2_MaVA.png"/></div></figure><p id="a79c" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这里我们有输入<em class="ln"> x1 </em>具有权重<em class="ln"> w1 </em>和偏向项<em class="ln"> b </em>，当我们把所有的值放在一起，我们得到<em class="ln"> a </em> = 0.584</p><p id="cf1b" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这里我们有<em class="ln"> f(z): </em>它是激活函数，命名为sigmoid函数。我们以后再谈。但是首先我们要看看激活函数是什么意思。</p><h2 id="0301" class="kd ke ht bd kf kg kh ki kj kk kl km kn jc ko kp kq jg kr ks kt jk ku kv kw kx bi translated"><em class="iq">激活功能:</em></h2><p id="422a" class="pw-post-body-paragraph ir is ht it b iu ky iw ix iy kz ja jb jc la je jf jg lb ji jj jk lc jm jn jo hb bi translated">激活函数决定一个神经元是否应该被激活。换句话说，它用于决定神经元提供的输入是否相关。</p><p id="f280" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">注:没有激活函数的神经网络只是一个线性回归模型。</p><p id="c5d2" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">激活函数对输入执行线性变换，使神经网络能够学习和执行复杂的任务，如图像处理和语言翻译。</p><p id="dc05" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们已经看到了一个激活函数，即sigmoid函数，我们还有其他类型的激活函数，如下所列:</p><ol class=""><li id="a8de" class="jp jq ht it b iu iv iy iz jc jr jg js jk jt jo ju jv jw jx bi translated">二元阶跃函数</li><li id="aef2" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">线性函数</li><li id="7df1" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">Sigmoid函数</li><li id="f4d3" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">双曲正切函数</li><li id="2827" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">整流线性单位</li><li id="1adf" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">泄漏ReLu</li><li id="892c" class="jp jq ht it b iu jy iy jz jc ka jg kb jk kc jo ju jv jw jx bi translated">Softmax函数</li></ol><p id="79ac" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们将讨论一些常用的函数，如Sigmoid函数、ReLu和Leaky ReLu，而不是研究所有的函数。我们将简要讨论它们以及它们的优缺点。</p><p id="f887" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hu">乙状结肠功能:</strong></p><p id="f7e1" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">该函数给出0到1范围内的输出。所以，如果我们有非常大的正数，那么输出将接近于1。如果我们有非常大的负数，那么输出将接近于0。</p><p id="5269" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">数学上的sigmoid函数如下所示。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es lq"><img src="../Images/2a562f51d236f5ae8999444f8d1d52b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*MzLWfaWmmYqkJSYpNXMaRQ.png"/></div></figure><p id="71f7" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这里Z <em class="ln"> = ∑i=1到n xn.wn + bn </em></p><p id="d540" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">其中<em class="ln"> x </em>是输入<em class="ln"> w </em>是权重，<em class="ln"> b </em>是偏置项。</p><p id="c60b" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">sigmoid函数的图形表示如下。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es lr"><img src="../Images/39f8097e8049d8ccbf91d9c9c483f264.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*RHjEzZqVzEmWJTPc__DFsQ.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">参考:谷歌图片</figcaption></figure><p id="f161" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从上图可以看出，sigmoid激活函数使输出介于1和0之间。您还可以看到，当X轴在-4到4之间时，<em class="ln"> y </em>的值快速变化，这意味着<em class="ln"> X </em>从-4到4的微小变化反映了输出的重大变化(即<em class="ln"> Y </em>)。</p><p id="ae3c" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在线性激活函数中，输出可能从–INF到+inf，但这里的范围是0到1。</p><p id="74ba" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这听起来是很好的激活功能，但有其自身的局限性。在sigmoid函数中，我们现在有0到1之间的值，当不在-4到4之间时，<em class="ln"> y </em>的值大约为零。这意味着，当我们将这些输出相乘时，最终输出变得非常小，这个问题被称为消失梯度。这也是我们选择其他激活函数而不是像ReLu这样的sigmoid函数的主要原因。</p><p id="7c6e" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">那么，ReLu如何解决sigmoid函数出现的问题呢？为此，我们将深入研究ReLu激活函数。</p><p id="71d6" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hu"> ReLu(整流线性单元)激活功能:</strong></p><p id="e337" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">简而言之，这个函数的作用是把负值替换为零。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es ls"><img src="../Images/6ca89a5fb29020cc6a081906f6767961.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*aPFi733J8yJvooC8SWbvUA.jpeg"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">参考:谷歌图片</figcaption></figure><p id="e7a6" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从这个图中我们可以看到任何负值都变为零。这个函数和它的导数都是单调的。它的范围是[0，无穷大]。</p><p id="6820" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">该函数的数学表达式为:</p><p id="f93c" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="ln"> Y = max (0，x) </em></p><p id="7f55" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">但是它有一个小问题，因为它将所有负值转换为零。意思是如果你有很多负输出的神经元，那么relu会让它们变成死神经元，你会有很多死神经元。</p><p id="ed2c" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">也就是说，如果神经元在开始时没有被激活，你最终会得到一个永远不会学习的神经网络。你可能有很多不知道的死穴。但在CNN(卷积神经网络)中制作最大池层时是有用的。</p><p id="a460" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">为了克服这一点，我们有Leaky-ReLu来解决这个问题，并使神经网络更具响应性。</p><p id="5a89" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hu">漏ReLu激活功能:</strong></p><p id="9831" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在这种情况下，我们不是将负的神经元转化为零，而是给它们一个小的负值。如下图。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div class="er es lt"><img src="../Images/2b5266326f4a5c15d8a5b257376a76d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*QXwSKgTOAoE64qNQpIPIpg.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">参考:谷歌图片</figcaption></figure><p id="60fb" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在这里，如果值小于零，我们将把它改为<em class="ln"> 0.01x </em>，否则就保持原样。这样我们就克服了ReLu带来的问题，每个神经元都会在开始时学习。</p><p id="85bc" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我认为Leaky ReLU的主要缺点是你有另一个参数要调，斜率。</p><p id="cbe4" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hu">注:</strong></p><p id="67ed" class="pw-post-body-paragraph ir is ht it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们已经讨论了三个激活函数。由此产生的问题是，哪种激活函数最好用。激活函数的结果因情况而异。在某些情况下，乙状结肠在某些关系中是好的，等等…所以最终这取决于具体情况，哪种激活会产生好的结果。</p></div></div>    
</body>
</html>