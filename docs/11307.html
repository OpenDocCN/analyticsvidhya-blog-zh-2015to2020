<html>
<head>
<title>Unravelling Linear Regression-ML part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解开线性回归-ML 第 1 部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/unravelling-linear-regression-ml-part-1-99f7910a5867?source=collection_archive---------15-----------------------#2020-11-28">https://medium.com/analytics-vidhya/unravelling-linear-regression-ml-part-1-99f7910a5867?source=collection_archive---------15-----------------------#2020-11-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/6a47cd1a8641482250cea27e9d0b0fbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uuqIH1ag4Cqc4ePb.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><a class="ae hv" href="https://datascience-enthusiast.com/DL/Optimization_methods.html" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure><div class=""/><h1 id="fa7e" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">概观</h1><p id="4719" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">读者您好，希望大家身体健康。我写这篇博客是因为我看到机器学习文章的增加，这些文章解释了如何用 10 行代码实现一个模型，但很少有博客谈论它背后的数学。我决定重新开始我的博客，写一些关于机器学习背后的数学方法的文章，让你理解事情是如何工作的，而不是仅仅引入 sckit-learn 并加入激烈的竞争。我的第一篇博客涵盖了最流行和最简单的回归算法，即线性回归。</p><h1 id="76cb" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">观众</h1><p id="ca19" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">有人对进入<em class="kr">机器学习/数据科学</em>感兴趣吗</p><h1 id="118b" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">回归还是分类问题？</h1><p id="5e8d" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">数据科学领域的许多新手会混淆这两者。<br/>一个标准的定义是‘当我们试图预测的目标变量是连续的，我们称之为<strong class="jv hz">回归</strong>问题。当目标变量只能取少量离散值时，我们称之为<strong class="jv hz">分类</strong>问题。这里的连续和离散是什么？基于一些特征预测房子的价格将是连续的，因为输出可以取 0…无穷大的值，而给定房子的特征，我们需要确定房子是否处于可销售状态是一个分类问题。</p><h1 id="e0c6" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">线性回归</h1><p id="bcd4" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">假设输入变量(<em class="kr">特征</em>)和单输出(<em class="kr">目标</em>)变量之间存在<strong class="jv hz">线性</strong>关系的线性模型。以下等式将θj 描述为参数(也称为权重)，h(x)或 y 描述为假设值或预测值。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ks"><img src="../Images/3f793bf76fede74ac3fad3e5f5e893be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JjQFfzPEN3uyIMWrc14AYQ.png"/></div></div></figure><p id="2d32" class="pw-post-body-paragraph jt ju hy jv b jw kx jy jz ka ky kc kd ke kz kg kh ki la kk kl km lb ko kp kq hb bi translated">让我们举一个例子，在水平轴上使用单个特征数据，在垂直轴上使用目标，在绘图时，每个人在看到下图左边的图像时的基本直觉是一条斜线，它穿过图像右边所描绘的点。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lc"><img src="../Images/f7e4e4b0a34f017431e2698caf5a16c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtDLQV-qxMaUBcjMrhjCVw.png"/></div></div></figure><p id="a6da" class="pw-post-body-paragraph jt ju hy jv b jw kx jy jz ka ky kc kd ke kz kg kh ki la kk kl km lb ko kp kq hb bi translated">这正是我们想要的，一个预测函数，我们传递新的特征，它为我们提供非常接近实际值的预测值。那么，<strong class="jv hz">什么是最佳拟合线，如何预测该函数？</strong> <br/>单个特征的假设方程看起来像-</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div class="er es ld"><img src="../Images/13a49cd744370a97f7d11939d07b8cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*TRy5qB61e8g2BJLHZy94PA.png"/></div></figure><p id="e636" class="pw-post-body-paragraph jt ju hy jv b jw kx jy jz ka ky kc kd ke kz kg kh ki la kk kl km lb ko kp kq hb bi translated">其中 h(x)是训练数据的预测值。现在，为了检查模型的最佳表现，我们需要首先设置θj 的值，并计算误差函数。这里，我们通过<strong class="jv hz">最小化误差平方和(MSE)来拟合模型。</strong></p><blockquote class="le lf lg"><p id="b60e" class="jt ju kr jv b jw kx jy jz ka ky kc kd lh kz kg kh li la kk kl lj lb ko kp kq hb bi translated"><strong class="jv hz">为什么我们对 MSE 感兴趣，而不是计算绝对<br/>(实际—预测)误差。<br/> </strong>关于θ的 MSE 将是一个<strong class="jv hz">凸函数</strong>并且这里局部最小值和全局最小值将是相同的。因此，该算法将总是收敛到一个全局最小值。另一个原因是，我们想惩罚离预测线较远的点，而不是离预测线较近的点。</p></blockquote><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lk"><img src="../Images/5bbc12c75d78412f31e513eed9500900.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GThoQ-QJpnsg3cnOGZRshw.png"/></div></div></figure><p id="5c12" class="pw-post-body-paragraph jt ju hy jv b jw kx jy jz ka ky kc kd ke kz kg kh ki la kk kl km lb ko kp kq hb bi translated">这里 h(x)是预测值，y 是训练集中的实际目标，上标“I”表示数据行。我们取常数 1/2 只是为了简化计算，因为最小化常数不会有任何影响，因此 1/2 不会影响方程。<br/>为了最小化成本函数，我们将使用<strong class="jv hz">梯度下降</strong>。</p><h1 id="b43e" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">梯度下降</strong></h1><p id="26f8" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">梯度下降是一种寻找函数最小值的迭代优化算法。这个函数就是我们的成本函数。想象你正从山顶滑雪，目标是到达山脚。当坡度较陡(误差较大)时，您可以通过进行较大的跳跃来覆盖较大的距离，但是当坡度较缓(误差)时，向底部迈出较小的步伐，并在到达底部时停止(成本函数最小，理想情况下为 0)</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ll"><img src="../Images/01702edb1be76b05d8b68831bb6040b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fozSVRfNMO-608EamzZewA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图片来自:<a class="ae hv" href="https://blog.clairvoyantsoft.com/the-ascent-of-gradient-descent-23356390836f" rel="noopener ugc nofollow" target="_blank">https://blog . clairvoyantsoft . com/the-ascent-of-gradient-descent-23356390836 f</a></figcaption></figure><p id="1524" class="pw-post-body-paragraph jt ju hy jv b jw kx jy jz ka ky kc kd ke kz kg kh ki la kk kl km lb ko kp kq hb bi translated"><strong class="jv hz">梯度下降步骤:<br/> </strong>初始猜测θ，然后反复改变θ使 J(θ)变小，直到我们有希望收敛到使 J(θ)最小的θ值</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lm"><img src="../Images/c4d811949d93eac0be081f00bd86ec2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9UTyV0lkv5Zd69pjkonIyw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">谢谢你容忍我的笔迹</figcaption></figure><p id="affc" class="pw-post-body-paragraph jt ju hy jv b jw kx jy jz ka ky kc kd ke kz kg kh ki la kk kl km lb ko kp kq hb bi translated">将偏导数项放入(I)部分，我们得到<strong class="jv hz"> LMS 更新规则</strong> (LMS 代表“<strong class="jv hz">最小均方</strong>”)，也称为<strong class="jv hz"> Widrow-Hoff </strong>学习规则。下面给出的等式:</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ln"><img src="../Images/ec070124877b4f4cdd79801a00fdf84a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TvMvXS5EzLckfGJ9_GdVhw.png"/></div></div></figure><p id="5267" class="pw-post-body-paragraph jt ju hy jv b jw kx jy jz ka ky kc kd ke kz kg kh ki la kk kl km lb ko kp kq hb bi translated">更新幅度与误差项(y(I)hθ(x(I))成比例；因此，例如，如果我们遇到一个训练示例，在该示例中，我们的预测几乎与 y(i)的实际值相匹配，那么我们发现几乎不需要改变参数；相反，如果我们的预测 hθ(x(i))具有大的误差(即，如果它离实际数据非常远，即 y(i))，则将对参数进行较大的改变</p><h1 id="9f1b" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">算法结束</h1><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lo"><img src="../Images/de22caaf3a3b7d7b86f5ced6c0c5e331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G1dN9ngIqETPOccQpZNsBg.png"/></div></div></figure><p id="d454" class="pw-post-body-paragraph jt ju hy jv b jw kx jy jz ka ky kc kd ke kz kg kh ki la kk kl km lb ko kp kq hb bi translated">下面是一个动画，展示了如何在最小化误差/成本函数的同时得出最佳拟合线。这种方法在每一步都查看整个训练集中的每个示例，称为批量梯度下降</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div class="er es lp"><img src="../Images/ad0159994885b8f834b01f607bfe06da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*a-Gz9Ol2W2QUJoT4.gif"/></div></figure><h1 id="a902" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">学习率</h1><p id="3538" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">如果我们选择一个非常小的学习率值，算法会非常慢，需要多次迭代才能收敛(右图)。如果我们采用非常大的学习率值，它可能不会达到局部最小值，因为它可能会来回跳动(左图)。作为起点，简单地尝试保持学习率为 0.001，你可以改变它，看看它是否表现得更好。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lq"><img src="../Images/9ba71313617d117ed852c2dd34da86a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5PTQBpg6uO-vvCKfZ1uXxw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图片来自:<a class="ae hv" href="https://builtin.com/data-science/gradient-descent" rel="noopener ugc nofollow" target="_blank">https://builtin.com/data-science/gradient-descent</a></figcaption></figure><h1 id="89bf" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">结论</h1><p id="cf48" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们了解了线性回归的工作原理以及梯度下降的一些细节。<br/>在本系列的下一篇博客中，我们将在给定的数据集上建立一个线性回归模型，并围绕它进行优化。</p></div><div class="ab cl lr ls gp lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="hb hc hd he hf"><h1 id="5ed6" class="iv iw hy bd ix iy ly ja jb jc lz je jf jg ma ji jj jk mb jm jn jo mc jq jr js bi translated">参考</h1><p id="20ea" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><a class="ae hv" href="https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf" rel="noopener ugc nofollow" target="_blank">https://see . Stanford . edu/materials/aimlcs 229/cs 229-notes 1 . pdf</a><br/><a class="ae hv" href="https://builtin.com/data-science/gradient-descent" rel="noopener ugc nofollow" target="_blank">https://builtin.com/data-science/gradient-descent</a><br/><a class="ae hv" href="https://blog.clairvoyantsoft.com/the-ascent-of-gradient-descent-23356390836f" rel="noopener ugc nofollow" target="_blank">https://blog . clairvoyantsoft . com/the-ascent-of-gradient-descent-23356390836 f</a></p></div></div>    
</body>
</html>