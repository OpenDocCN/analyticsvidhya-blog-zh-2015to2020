<html>
<head>
<title>Finetuning an XLNet for sentiment extraction in tensorflow :</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调用于tensorflow中情感提取的XLNet</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/finetuning-a-xlnet-for-sentiment-extraction-9e75ded91f1c?source=collection_archive---------15-----------------------#2020-06-07">https://medium.com/analytics-vidhya/finetuning-a-xlnet-for-sentiment-extraction-9e75ded91f1c?source=collection_archive---------15-----------------------#2020-06-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/2b57b707f6113558fceb6b77e9f51333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*jHzNpL-KagnaHUSHzPTPkA.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">来源:KDnuggets</figcaption></figure><h1 id="0d12" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">内容:</h1><ol class=""><li id="8034" class="jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">XLNet简介</li><li id="acb7" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">什么是情感提取任务？</li><li id="f584" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">标记化</li><li id="2f51" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">建筑模型</li></ol><h1 id="3d32" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">XLNet简介:</h1><p id="7cc6" class="pw-post-body-paragraph kl km hi jq b jr js kn ko jt ju kp kq jv kr ks kt jx ku kv kw jz kx ky kz kb hb bi translated">XLNet是自然语言处理领域最先进的模型之一。它通过在20个任务上超越BERT模型接管了帝国，这些任务包括问题回答、自然语言推理、情感分析和文档排序。与BERT不同，XLNet利用了自回归(AR)语言建模和自动编码的预训练优势。</p><p id="a86a" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">好吧，等等！这些是什么？别担心，我们会弄清楚上面的话在下面是什么意思。</p><ol class=""><li id="bde2" class="jo jp hi jq b jr la jt lb jv lf jx lg jz lh kb kc kd ke kf bi translated">自回归语言建模；</li></ol><p id="6adb" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">这里，我们将要求模型逐步预测单词。让我举一个例子来说明这一点:</p><p id="a4db" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">让我们考虑“无监督表示学习在自然语言处理领域非常成功”这句话</p><p id="3441" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">现在，在训练期间，模型将被给予“无监督表示学习已经”，并被要求预测下一个单词，即这里的单词“已经”。它还没有完成，现在模型将再次被给予“无监督的表示学习已经”,并被要求预测下一个单词“高度”!这个过程还在继续。</p><blockquote class="li lj lk"><p id="ed5f" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi translated">输入1 →“无监督表示学习有”</p><p id="716e" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi translated">输出1→‘去过’</p><p id="e543" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi translated">输入2 →“无监督表示学习已经”</p><p id="b44c" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi translated">输出2 →“高度”</p><p id="a616" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi">.</p><p id="661f" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi">.</p><p id="1565" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi">.</p></blockquote><p id="a256" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">2.自动编码:</p><p id="6e6e" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">自动编码与我们之前讨论的有点不同。这里，句子中的一些单词将被屏蔽，现在模型应该预测这些被屏蔽的单词。让我用上面同样的例子来说明这一点。</p><p id="af3b" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">该模型将提供上述序列，但带有一些屏蔽的标记(如下所示)，并被要求预测这些单词:</p><blockquote class="li lj lk"><p id="3e8c" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi translated">无监督表示<mask>在自然语言处理领域已经高度<mask>化。</mask></mask></p></blockquote><p id="fa9c" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">BERT的预训练目标只是屏蔽语言建模(一种特殊的自动编码)，而不是自回归建模，而XLNet两者都用。</p><p id="ebb4" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">自回归建模的优点是，当模型试图预测一个单词时，它将可以访问所有单词，包括它之前预测的单词，这与自动编码不同，在自动编码中，它只能访问未屏蔽的标记。</p><p id="255c" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">我试图给出一个XLNet与BERT有什么不同的粗略概念。</p><p id="fa11" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">要了解更多信息，我强烈推荐这篇写得非常棒的<a class="ae lp" href="https://towardsdatascience.com/xlnet-explained-in-simple-terms-255b9fb2c97c" rel="noopener" target="_blank"> medium </a>文章。</p><h2 id="e9d4" class="lq ir hi bd is lr ls lt iw lu lv lw ja jv lx ly je jx lz ma ji jz mb mc jm md bi translated">什么是情感抽取？</h2><p id="d727" class="pw-post-body-paragraph kl km hi jq b jr js kn ko jt ju kp kq jv kr ks kt jx ku kv kw jz kx ky kz kb hb bi me translated">情感提取是一项自然语言处理任务，其中将给出一个句子及其相应的情感(无论是肯定句、否定句还是中性句),模型应该提取强烈支持给定情感的短语或词组。</p><p id="9bff" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">我们如何做到这一点？</p><p id="7db8" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">我们只是试图预测与给定情感非常一致的文本的开始和结束索引。</p><p id="20a3" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">现在，让我们通过学习代码来弄脏我们的手。</p><p id="e90b" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">注意:在这里，我没有特别使用任何数据来完成这项任务，我将提供基本的逻辑和代码，以便人们可以将此逻辑嵌入到他们自己的定制数据集的模型中。</p><p id="9152" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">从头开始编写像XLNet这样的模型并对它们进行预训练确实是一项艰巨的任务。非常感谢<a class="ae lp" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">拥抱脸变形金刚</strong> </a>，这个库让我们的任务变得更简单。它提供了预训练的转换器模型和标记器……它还为问答任务提供了XLNet模型(非常类似于情感提取任务),但我们将在这里微调我们自己的模型。</p><h1 id="29f0" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">符号化:</h1><p id="e3c2" class="pw-post-body-paragraph kl km hi jq b jr js kn ko jt ju kp kq jv kr ks kt jx ku kv kw jz kx ky kz kb hb bi translated">这是所有自然语言处理任务的第一步。我们需要对句子进行标记。</p><p id="de50" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">那么为什么要符号化呢？</p><p id="678e" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">因为不幸的是，计算机不擅长处理文字但幸运的是，它们擅长数字。</p><p id="dcc6" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">XLNet使用<a class="ae lp" href="https://arxiv.org/abs/1808.06226" rel="noopener ugc nofollow" target="_blank">句子片段</a>分词器对其文本进行分词，然而，在拥抱面部变形器中提供了一个用于XLNet的预训练分词器。</p><pre class="mn mo mp mq fd mr ms mt mu aw mv bi"><span id="ccb0" class="lq ir hi ms b fi mw mx l my mz">from transformers import *</span><span id="f3ed" class="lq ir hi ms b fi na mx l my mz">tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased'<br/> ,do_lower_case = True)</span></pre><p id="3960" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">上面这段代码用于导入transformers库和预训练的XLNet标记器。我们将要讨论的代码要求数据框中的训练集具有“文本”、“情感”和“选定文本”列。</p><p id="d062" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">现在，让我们看看如何标记一个句子并获得一个数据点，以便我们可以将相同的逻辑放入一个循环中来标记整个数据集。</p><p id="2bdf" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">XLNet需要三种类型的输入:</p><ol class=""><li id="7fd4" class="jo jp hi jq b jr la jt lb jv lf jx lg jz lh kb kc kd ke kf bi translated">input_ids:标记化的输入，必要时用<cls>和<sep>标记填充到最大长度。</sep></cls></li><li id="a03b" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">attention_mask:这有助于我们的XLNet模型只关注非填充令牌。</li><li id="3b75" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">token_type_ids:如果我们的输入有两种不同类型的序列，例如在我们的任务中，我们有两种不同的序列，一种是从中提取文本的句子，另一种是相应的情感。所以这两个可以通过在token_type_ids中给不同的值来区分。</li></ol><p id="29db" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">为了简单起见，在这篇文章中我将忽略令牌类型id，如果我们使用令牌类型id，性能可能会略有不同。</p><p id="efde" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">好吧！现在，让我们标记并获得句子的输入id和注意力屏蔽。</p><blockquote class="li lj lk"><p id="a580" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi translated">输入句子:“巴黎是个美妙的地方！”</p><p id="a53f" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi translated">情绪:“积极”</p></blockquote><p id="139b" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">大量代码！别担心，让我们一部分一部分地过一遍！</p><ol class=""><li id="dd0a" class="jo jp hi jq b jr la jt lb jv lf jx lg jz lh kb kc kd ke kf bi translated">第3行:我创建了一个字典，将特殊标记映射到它们的编号。这样我们就可以避免每次都调用tokenizer.encode()。</li><li id="fcba" class="jo jp hi jq b jr kg jt kh jv ki jx kj jz kk kb kc kd ke kf bi translated">第7，8行:默认情况下，tokenizer.encode()在末尾添加sep和cls标记，但是这里我们需要做更多的工作(比如查找偏移量),所以我们将它们排除在外。</li></ol><blockquote class="li lj lk"><p id="7a70" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi translated"><em class="hi">注意1 : tokenizer.encode()有时可能会将一个单词拆分成多个片段，例如它将paris拆分成pari &amp; s. </em></p><p id="76d7" class="kl km ll jq b jr la kn ko jt lb kp kq lm lc ks kt ln ld kv kw lo le ky kz kb hb bi translated">注意2 : Offsets帮助我们回到原来的句子，它存储了每个标记的开始和结束索引。</p></blockquote><p id="0824" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">3.第10、11、12行:我们创建了一个数组<em class="ll"> chars </em>，将1赋给对应于所选文本的位置，将0赋给剩余的位置。</p><p id="fa59" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">4.第15，16，17行:我们找到偏移量来知道每个令牌的位置，并把它存储在一个列表中。</p><p id="a3a6" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">5.第20、21、22行:这段代码帮助我们找出句子编码中的哪个标记也在我们选择的文本中(这样我们就可以据此计算开始和结束索引)。</p><p id="5aad" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">仅此而已！将上述逻辑放在for循环中，对整个数据集进行标记化。现在，让我们在XLNet上构建一个CNN头。</p><h1 id="4678" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">建筑模型:</h1><p id="2a9e" class="pw-post-body-paragraph kl km hi jq b jr js kn ko jt ju kp kq jv kr ks kt jx ku kv kw jz kx ky kz kb hb bi translated">上面的代码在XLNet上构建了一个简单的CNN头，它将input_ids和attention mask作为输入，将开始和结束索引作为输出。</p><p id="6e5f" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">就是这样！我们完了。</p><p id="20cf" class="pw-post-body-paragraph kl km hi jq b jr la kn ko jt lb kp kq jv lc ks kt jx ld kv kw jz le ky kz kb hb bi translated">现在可以通过选择优化器(可能是adam优化器)和损失函数(通常是分类交叉熵)来训练该模型。</p><h1 id="d42c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结论:</h1><p id="f404" class="pw-post-body-paragraph kl km hi jq b jr js kn ko jt ju kp kq jv kr ks kt jx ku kv kw jz kx ky kz kb hb bi translated">我希望你喜欢阅读这篇文章。如果您有任何疑问，请在此发布。要查看完整代码，请随意访问这个<a class="ae lp" href="https://github.com/mano3-1/Sentiment-Extraction-from-tweets" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>。</p></div></div>    
</body>
</html>