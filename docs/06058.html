<html>
<head>
<title>Contextual word embeddings — Part2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语境词嵌入—第二部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/contextual-word-embeddings-part2-bb4888310be1?source=collection_archive---------15-----------------------#2020-05-10">https://medium.com/analytics-vidhya/contextual-word-embeddings-part2-bb4888310be1?source=collection_archive---------15-----------------------#2020-05-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b58d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这个故事介绍了变压器架构和BERT。这是来自斯坦福大学深度学习NLP(第13讲)的简短总结。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/faa3971409d2f990efdecc0075c0adce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xQeU_wPy8-4LIPGKX8u6Hg.png"/></div></div></figure><p id="5f8e" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这个故事包含变压器架构和BERT。这是<a class="ae ix" rel="noopener" href="/@rachel_95942/contextual-word-embeddings-part1-20d84787c65">“语境词嵌入—第一部分”的后续。</a>它是<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">Stanford cs 224n:NLP with Deep Learning | Winter 2019 |第十三讲——语境词嵌入</a>的总结。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="9597" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">4.变压器架构</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es lf"><img src="../Images/9adfcb4976542c7ba0909c907513f6f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qdOJHCqhRcPJfe0LZdb-6A.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="cb81" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">ULMfit是一种你可以在1个GPU日内训练的东西，但OpenAI的人建立了一个预训练语言模型(GPT)，并在更大的计算量上对大量数据进行训练，使用了大约242个GPU日，它工作得更好。谷歌用256个TPU日训练了一个模型(BERT)，这意味着计算量增加了一倍左右。然后OpenAI再次变得更大，并为大约2000个TPU版本3天训练了一个模型(GPT-2)，它将能够再次做得更好。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es lk"><img src="../Images/30544f0b88cfa88e6777ca80c4a173f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b797ZY1gtP74ZQhHvQhY8Q.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="c643" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">GPT2模型印象非常深刻。当他们展示如果你在一个非常大的数据上建立一个非常非常庞大的语言模型，然后你说语言模型在这个特定的主题上产生一些文本，它实际上可以很好地产生文本。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es lm"><img src="../Images/f5942bf975cf6d020871b4d001d5a072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Awd7qqn68fhO2jZHxCCxw.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="f5a4" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj">故事的一部分是这些东西变得越来越大</strong>，但故事的另一部分是<strong class="jm hj"> <em class="ll">所有这3个都是使用变压器架构的系统</em> </strong> <em class="ll">。变压器架构不仅非常强大，而且在技术上允许扩展到更大的尺寸。</em></p><p id="44e2" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> <em class="ll">变形金刚的动机本质上是我们希望事情进行得更快，这样我们就可以建立更大的模型，</em> </strong> <em class="ll">这些LSTM或一般任何递归模型的问题是，它们是递归的，这意味着它们不能进行同一种并行计算。</em>但是GPU喜欢你可以做卷积神经网络之类的事情。此外，尽管LSTMs和GRUs之类的门控循环单元优于循环单元，<strong class="jm hj">它们在长序列长度内存在问题，这可以通过添加注意机制来改善</strong>。<em class="ll">既然注意力的作用如此之大，也许我们可以利用注意力，去掉模型中的循环部分，这就是变压器架构的用武之地。</em>尽管有GRUs和LSTMs，rnn仍然需要注意机制来处理长程相关性——否则，状态之间的路径长度会随着序列而增长。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="bcc1" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">4.变压器架构</h1><h2 id="5883" class="ln ko hi bd kp lo lp lq kt lr ls lt kx jt lu lv kz jx lw lx lb kb ly lz ld ma bi translated">4.1变压器架构</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mb"><img src="../Images/56b94f64653c188696ebe577963d6785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZhqoBhsy4a2eC7l84Idt1g.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">来源:<a class="ae ix" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">‘注意力是你需要的全部’</a>。</figcaption></figure><p id="6af4" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">关于变压器架构的原始论文名为<a class="ae ix" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">‘注意力是你需要的全部’</a>。所以在最初的工作中，他们专注地执行神经机器翻译。他们想<em class="ll">建立一个复杂的编码器和一个复杂的解码器，它们非递归地工作，</em>并且仍然能够通过利用大量的注意力分布来很好地翻译句子。现在:让我们定义变压器网络的基本构件:首先，新的关注层。<strong class="jm hj">基本思路是，他们要利用注意力无处不在来计算事情</strong>。有不同类型的注意:线性乘法注意和前馈网络加法注意。他们倾向于最简单的注意力，注意力只是两个事物之间的点积。出于各种目的，他们在两个事物之间做更复杂版本的点积，<em class="ll">键和值。</em> <em class="ll">所以对于点积注意力来说，输入是一个查询q，输出是一组ley-vale (k-v)对。查询、键、值和输出都是向量。</em> <em class="ll">输出是值的加权和，其中每个值的权重由查询和对应的键的内积计算</em>。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="aec5" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">4.变压器架构</h1><h2 id="39fb" class="ln ko hi bd kp lo lp lq kt lr ls lt kx jt lu lv kz jx lw lx lb kb ly lz ld ma bi translated">4.2多头缩放点积注意</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mc"><img src="../Images/da98b7bab19f18aee5e9c92d506691f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pb2_wDezIiuZs73d8-HpgA.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">点积注意力。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="4897" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">左侧显示了点积关注度，其中softmax应用于查询、关键相似度(查询和关键点之间的点积)以生成基于关注度的权重。权重应用于相应的值(vi)。<strong class="jm hj">实际上所有的查询关键字和值都是完全一样的，它们都是来自源语言的单词。</strong></p><div class="iz ja jb jc fd ab cb"><figure class="md jd me mf mg mh mi paragraph-image"><img src="../Images/5fd217657013dd3a9e755aab132be886.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*TcmtQp994dAF98dlKMOtUw.png"/></figure><figure class="md jd mj mf mg mh mi paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><img src="../Images/f47dc7434c2c503737473e46bec99a89.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*IaQNwfb1MHieOXvFmCJ5mw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx mk di ml mm translated">。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a>矩阵中的点积注意</figcaption></figure></div><p id="0c39" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">矩阵表示法中的点积注意是当我们有多个查询时，我们把它们堆在一个矩阵中。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mn"><img src="../Images/91bf7a0ea57564c896136a3aed9cbadd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*IIeUWYoWOU7Kukc-57quZA.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">成比例的点积注意力。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="9417" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><em class="ll">上述关注的问题是，随着d_{k}变大，q^{T}k的方差增加，这导致softmax内的一些值变大，这导致softmax变得非常尖峰，因此其梯度变小。</em> <strong class="jm hj"> <em class="ll">这个的解决方案是通过查询/关键向量来伸缩。</em>T11】</strong></p><p id="6745" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">输入单词向量是查询、键和值<em class="ll">。在顺序词中，词向量自己选择对方。</em>字向量栈等于Q等于K等于v我们会在解码器里看到为什么我们在定义里把它们分开。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mo"><img src="../Images/1047cb432db63ee26d2315ccbc6e2ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*ZGJBmJ4-HOOuaN5K6HXasg.png"/></div></figure><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mp"><img src="../Images/6af5ae1280bf02b897488a0cf573d665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vgOnXlYnUYGNTURHgErAqw.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">多头关注。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="68d5" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> <em class="ll">单纯的自我关注的问题在于，词语之间只有一种互动方式</em> </strong> <em class="ll">。</em> <strong class="jm hj"> <em class="ll">如果能从一个位置参加到各种事情就好了。</em> </strong>比如你训练依存解析器。如果你是一个单词，你可能想注意你的中心词，但你也可能想注意你的从属词。如果你碰巧是一个代词，你可能要注意这个代词指的是什么。你可能想得到更多的关注。<em class="ll">解此i </em> <strong class="jm hj"> <em class="ll"> s多头注意力，</em> </strong> <em class="ll">其中首先通过W矩阵将Q，K，V映射到h(等于文中的8)低维空间，然后你用其中的每一个来计算点积注意力，这样你就可以同时兼顾不同的。然后连接输出并通过线性层</em>。这种多头关注是变形金刚非常成功的想法之一，使他们成为一个更强大的架构。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="1056" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">4.变压器架构</h1><h2 id="512d" class="ln ko hi bd kp lo lp lq kt lr ls lt kx jt lu lv kz jx lw lx lb kb ly lz ld ma bi translated">4.3变压器组。</h2><p id="e4c7" class="pw-post-body-paragraph jk jl hi jm b jn mq ij jp jq mr im js jt ms jv jw jx mt jz ka kb mu kd ke kf hb bi translated">让我们完成变压器块。 <em class="ll">每个区块有2个“子层”:1)多线程关注；2)带ReLU的两层前馈神经网络。这两个步骤中的每一个也具有1)残留(短路)；2)连接和层名。</em></p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mv"><img src="../Images/7a5d28ad8b1d786c987999fc857c916f.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*LuLamI02s2LELvr-j1vfEw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">变压器块。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="cf10" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> <em class="ll">从我们的词向量开始，我们要对多个不同的事物进行关注，同时我们有一个围绕它们短路的剩余连接。将这两者相加后，归一化就完成了。所以块的输出是</em> </strong> <code class="du mw mx my mz b"><strong class="jm hj"><em class="ll">LayerNorm(x+Sublayer(x))</em></strong></code> <strong class="jm hj"> <em class="ll">。值得注意的是，这是层规范化，而不是批量规范化。</em></strong><a class="ae ix" href="https://arxiv.org/pdf/1607.06450.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jm hj"><em class="ll">layer norm</em></strong></a><strong class="jm hj"><em class="ll">将输入特征更改为每层均值为0，方差为1(并增加了两个参数)</em> </strong>。对于一个变压器模块，然后进行多头关注，通过一个前馈层，该层也有一个残差连接，对这些输出求和，然后再次进行另一层归一化。这是基本的变压器模块，你可以在任何地方使用。为了构建完整的架构，他们将开始堆叠这些变压器块，形成一个非常深的网络。从某种意义上来说，我们发现变形金刚表现得非常好。</p><p id="5ae4" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">但是没有免费的午餐。你现在不再得到重复出现的信息，实际上是被一个序列携带的。你在某个位置说了一个词，这可能会引起其他词的注意。所以，如果你想让信息在一条链中传递，你首先要走完这条链的第一步，然后你需要有另一个垂直层，它可以走完这条链的下一步，然后你需要有另一个垂直层，它可以走完这条链的下一步。因此，你正在摆脱序列中的循环，但是你正在替换一些深度以允许事情沿着多跳进行。  <em class="ll">然而，这在GPU架构中非常有利，因为它允许您使用并行化来同时计算每个深度的所有内容</em>。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es na"><img src="../Images/3704dcfd43870c316e3baee086c62c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*OSpMb-IOipUPaydmtTKz1g.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">位置编码</figcaption></figure><p id="1cdd" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">对于编码器输入，他们使用字节对编码(BPE)。但是如果你什么都不做，你只是把单词输入到这个单词向量中，你不知道你是在句子的开头还是结尾。虽然，<strong class="jm hj"> <em class="ll">他们有一种位置编码的方法，可以给你一些想法来定位你的单词在句子</em> </strong>中的位置。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="c1ed" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">4.变压器架构</h1><h2 id="4d79" class="ln ko hi bd kp lo lp lq kt lr ls lt kx jt lu lv kz jx lw lx lb kb ly lz ld ma bi translated">4.4编码器</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nb"><img src="../Images/919528ced8e7cc63d0dde337ebaa73d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*swb5lxZyR0QEUj0xjt2YAg.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">编码器。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="4e18" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这是一种编码系统。所以从单词来看，他们有一个初始单词嵌入。你加入他们的位置编码。你进入变压器组，然后重复n次。因此，这些变压器模块会垂直堆叠在一起。所以你会多次对句子的其他部分进行多头关注，计算值，前馈一个值，通过一个完全连接的层，然后你会对句子中的不同地方进行关注。获取你所有的信息，把它放入一个完全连接的层，然后向上，深入向上。编码器工作得很好，可能是因为你可以用你的多头注意力和句子中的各种其他地方，积累信息，把它推到下一层。如果你这样做6次(上面的N=6)，你就可以开始沿着序列向任一方向逐步推进信息，以计算感兴趣的值。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nc"><img src="../Images/2beb6aee284416211fd3752cdd890372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*23mdnTgc26KM3-vSLf-Uhg.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">第五层的注意力视觉化。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="32b4" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">有趣的是<em class="ll">这些模型在学习关注语言结构中的有趣事物方面表现得非常好。</em>这些只是一种提示性的图表，但这是查看变压器堆栈的第5层，并查看不同的注意力集中在哪些单词上。这些不同的颜色对应不同的注意头。这句话是<code class="du mw mx my mz b">in this spirt, that a majority of American governments have passed new laws since 2009 making the registration or voting process more difficult.</code>我们看到的是大部分注意力从“制造”转移到“更难”这个词上，这似乎很有用。其中一个注意力头似乎在看单词本身(“making”)，这可能没问题。然后，其他人开始关注“法律”和“2009”。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="46c3" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">4.变压器架构</h1><h2 id="91a9" class="ln ko hi bd kp lo lp lq kt lr ls lt kx jt lu lv kz jx lw lx lb kb ly lz ld ma bi translated">4.4解码器</h2><p id="1f06" class="pw-post-body-paragraph jk jl hi jm b jn mq ij jp jq mr im js jt ms jv jw jx mt jz ka kb mu kd ke kf hb bi translated"><strong class="jm hj"> <em class="ll">对于transformer解码器，解码器中的2个子层发生变化:1)屏蔽解码器对先前生成的输出的自我关注；2)在编码器-解码器关注中，查询来自先前的解码器层，而密钥和值来自编码器</em>T3的输出。</strong></p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="c007" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">5.伯特</h1><h2 id="e7b1" class="ln ko hi bd kp lo lp lq kt lr ls lt kx jt lu lv kz jx lw lx lb kb ly lz ld ma bi translated">5.1填银行字目标</h2><p id="4d9f" class="pw-post-body-paragraph jk jl hi jm b jn mq ij jp jq mr im js jt ms jv jw jx mt jz ka kb mu kd ke kf hb bi translated">帮助你完成任务的最新和最棒的上下文单词表示是这些<a class="ae ix" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT </a> vectors，<strong class="jm hj"> <em class="ll">，其中BERT是来自Transformers的双向编码器表示。</em> </strong>本质上，它使用的是变压器网络中的编码器。他们为网络架构做了几个选择。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nd"><img src="../Images/805ddfdedb1b72a8623f2132c0901300.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*tjHU5JpCNElhRyYU3N4wtg.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">伯特选择了双向性而不是方向性。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="a1db" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">首先，他们选择双向性而不是单向性。语言模型只使用左语境或右语境，但语言理解是双向的。<em class="ll">标准语言模型有</em> <strong class="jm hj"> <em class="ll">单向</em> </strong> <em class="ll">，</em>这很有用，因为<strong class="jm hj"> <em class="ll">它给出了一个语言模型</em> </strong>的概率分布。但这是不好的，因为你希望能够从两方面进行预测，以理解词义和上下文。</p><p id="3315" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">当您以两种方式整合信息时，您可以创建双向模型。但是这也有点问题，因为这样你就会有串音。假设<em class="ll">您运行一个BiLSTM，然后通过连接合并表示，然后将它们送入下一层。当你运行下一层的时候，前锋LSTM就已经从第一层得到了关于未来的信息。所以它以已经预见未来的词语结束</em>。所以你有这种复杂的非生成模型。不知何故，他们想做一点不同的事情，这样他们就可以有双向的上下文，而没有文字可以看到他们自己。<strong class="jm hj"><em class="ll">BERT的解决方案是屏蔽掉k%(通常是15%)的输入单词，然后预测被屏蔽的单词。</em> </strong> <em class="ll">对于输入，BERT </em>屏蔽掉句子中的一些单词。例如，句子<code class="du mw mx my mz b">the man went to the [mask] to buy a [mask] of milk'</code>被单词<code class="du mw mx my mz b">store</code>和<code class="du mw mx my mz b">gallon</code>屏蔽了。所以BERT中的LM不再是一个真正的LM，它产生一个句子的概率，这是标准的从左到右的工作，而是填充空白的目标。现在，你的训练目标是试着预测被屏蔽的单词是什么，你可以在一定程度上利用交叉熵损失做到这一点。所以你训练一个模型来填补这些空白(屏蔽词)。<strong class="jm hj"> <em class="ll">他们空白单词的比率基本上是7个单词中有1个。他们还讨论了这是一种怎样的权衡，因为如果你空白的单词太少，训练的成本就会很高。如果你漏掉了很多单词，你就漏掉了一个单词的大部分上下文，</em> </strong>，这意味着它对训练没什么用，他们发现大约七分之一的单词对他们来说似乎很有用。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ne"><img src="../Images/928ab74bcc8204e94d7ec0e3b52f33b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*ZLpXS7nGqwft6DSb_3bpvw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">埃尔莫，伯特和奥佩利BERT。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="efa2" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">论文还比较了GPT和埃尔莫。它指出<em class="ll">对于OpenAI的GPT来说，也是一个transformer模型，这是一种从左到右工作的经典语言模型，所以你只能得到左上下文。至于ELMo语言模型，虽然它从左到右和从右到左运行语言模型，这意味着在某种意义上它具有来自两侧的上下文，但这两种语言模型是完全独立训练的，然后您只是将它们的表示连接在一起。因此，我们实际上没有一种模型，在建立预先训练的上下文单词表示时，联合使用双方的上下文。</em>因此，作者希望在transformer模型中使用这种隐藏单词并使用整个上下文进行预测的技巧，这将允许他们使用双面上下文，并且更加有效。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="a466" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">5.伯特</h1><h2 id="b755" class="ln ko hi bd kp lo lp lq kt lr ls lt kx jt lu lv kz jx lw lx lb kb ly lz ld ma bi translated">5.2 <strong class="ak"> <em class="nf">学习句子之间的关系</em> </strong> <em class="nf"> s目的</em></h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ng"><img src="../Images/4154b68124d94c158823ba99949fe9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*ezkC6MjBmfwFZI-FIeuYWg.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">下一句预测。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="6a75" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">除了填充银行单词目标的语言建模目标之外，<strong class="jm hj"> <em class="ll"> BERT还有第二个目标来学习句子</em></strong><em class="ll"/><em class="ll">之间的关系，这对于像问题回答或自然语言推理任务这样的任务是有用的。让我们有两个句子，这些句子可能是课文中连续的两个句子，或者是一个句子后面跟着一个别处的随机句子。我们希望训练系统来预测你什么时候会看到正确的下一句话，而不是随机的一句话</em>。所以你也在训练一个基于下一句话预测任务的损失。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nh"><img src="../Images/da32b909f7ba761a5dfe625745a6a3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*V67n8bWh9_ENvAqIGS1snw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">句子对编码。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="4079" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">像<strong class="jm hj"> <em class="ll">这样的句子成对编码。</em> </strong>对于输入，他们会有一对句子:<code class="du mw mx my mz b">my dog is cute[separate] He likes playing</code>。单词被表示为单词片段。<em class="ll">所以每个词块都有标记嵌入。然后每个单词都有一个位置嵌入，这个位置嵌入会和记号嵌入相加。最后，每个单词片段都有一个片段嵌入，简单来说，它是来自分隔符之前或之后的第一个句子还是第二个句子。所以，你把这三样东西加在一起，得到了表征</em> <strong class="jm hj">。</strong>然后，您将在变压器模型中使用它们，在这种模型中，您的损耗会达到无法预测屏蔽词的程度。然后你的二元预测函数，关于是否有正确的下一个句子，这是训练架构。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="7f25" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">5.伯特</h1><h2 id="84cf" class="ln ko hi bd kp lo lp lq kt lr ls lt kx jt lu lv kz jx lw lx lb kb ly lz ld ma bi translated">5.3微调</h2><p id="d717" class="pw-post-body-paragraph jk jl hi jm b jn mq ij jp jq mr im js jt ms jv jw jx mt jz ka kb mu kd ke kf hb bi translated">有一些关于伯特的细节。<em class="ll">首先，它是用自我注意训练出来的，没有地点偏见，使远距离语境具有“平等机会”。每层一次乘法使其在GPU/TPU上高效运行。</em>伯特在维基百科和图书语料库上接受培训。它有两种型号规格:<em class="ll"> 1)伯特基</em> (12层，768隐，12头)；以及2) <em class="ll"> BERT-Large </em> (24层，1024隐藏，16头)。伯特在4x4(或8x8) TPU切片上接受了4天的训练。</p><p id="ebde" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我们可以利用<strong class="jm hj"> <em class="ll">对</em> </strong>这个预先训练好的BERT进行微调，它将对各种任务非常有用，比如命名实体识别、问题回答和自然语言推理等等。我们要做的方式有点像<em class="ll">做与ULMFit模型相同的事情，</em>并且它不像上下文单词表示(s.t ELMo)所做的那样。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ni"><img src="../Images/29b0e9ff505c11f0aa6f99b5e86a2f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WJVFwp7X9YF0O5extX-ycA.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">微调预训练伯特。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="7763" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">相反，我们只是继续使用我们作为语言模型训练的变压器网络，但针对特定任务对其进行微调。现在，您将运行这个转换器，计算特定任务的表示。我们要改变的是，我们要移除最顶层的预测。预测大众语言模型和下一句预测的位。我们将在上面替换一个适合任务的最终预测层。因此，如果我们的任务是小组问答，我们最终的预测层将预测跨度的开始和结束，类似于DrQA。如果我们在做NER任务，我们最终的预测层将会像标准的NER系统一样预测每个令牌的命名实体识别类。</p><p id="3bdb" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">所以他们建立了这个系统，并在一大堆数据集上进行测试。他们测试的主要内容之一是这个<strong class="jm hj"> <em class="ll">胶水数据集</em> </strong>，它有一大堆任务。GLUE benchmark以自然语言推理任务为主。例如，给你一个句子，比如<code class="du mw mx my mz b">Hills and mountains are sepeicall sancially sanctified in Janism.</code>，然后你可以在<code class="du mw mx my mz b">Janism hates nature</code>上写一个假设。你要做的是，<strong class="jm hj"> <em class="ll">假设是否从前提而来，与前提相矛盾，或者与前提无关。</em> </strong> <em class="ll">那是一个三路分类</em>。所以这与前提相矛盾。还有其他各种各样的任务，比如GLUE中的语言可接受性任务。实验表明，BERT在胶合任务上取得了良好的效果。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h2 id="a34b" class="ln ko hi bd kp lo lp lq kt lr ls lt kx jt lu lv kz jx lw lx lb kb ly lz ld ma bi translated">参考:</h2><ol class=""><li id="9191" class="nj nk hi jm b jn mq jq mr jt nl jx nm kb nn kf no np nq nr bi translated"><a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">斯坦福CS224N: NLP与深度学习| Winter 2019 |讲座13 —上下文单词嵌入</a></li></ol><p id="b9aa" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">2.<a class="ae ix" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a></p><p id="3b61" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">3.<a class="ae ix" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的一切</a></p></div></div>    
</body>
</html>