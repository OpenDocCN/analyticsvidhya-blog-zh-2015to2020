<html>
<head>
<title>Deploying AI at the Edge with Intel OpenVINO- Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用英特尔OpenVINO在边缘部署人工智能-第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deploying-ai-at-the-edge-with-intel-openvino-part-2-1f1a9faa514b?source=collection_archive---------5-----------------------#2020-02-13">https://medium.com/analytics-vidhya/deploying-ai-at-the-edge-with-intel-openvino-part-2-1f1a9faa514b?source=collection_archive---------5-----------------------#2020-02-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/794ed1377361f5347ab362537e94b013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zR4rEfJMnmBNlmQvkLCgeA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@drewpatrickmiller?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">德鲁·帕特里克·米勒</a>在<a class="ae iu" href="https://unsplash.com/s/photos/control-board?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="6a27" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在<a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-1-51a09752fb4e">第1部分</a>中，我谈到了如何下载一个已经为在openVINO toolkit中使用而优化的预训练模型。在这一部分中，我们将看到如何通过模型优化器来优化一个未优化的模型。这篇文章涉及的主题有:</p><ul class=""><li id="21b1" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">优化技术</li><li id="8bbd" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">支持的框架</li><li id="8916" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">中间表示</li><li id="d14c" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">转换ONNX模型</li><li id="4998" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">转换Caffe模型</li><li id="213b" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">转换张量流模型</li></ul><h1 id="bea2" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">优化是如何完成的</h1><p id="a64c" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">Model optimizer是一个命令行工具，它将使用受支持的框架制作的模型转换为可在推理引擎中使用的中间表示形式。模型优化器是一个名为<em class="lk"> "mo.py" </em>的python文件，可以在以下位置找到它:</p><blockquote class="ll"><p id="f168" class="lm ln hi bd lo lp lq lr ls lt lu js dx translated"><em class="lv"> &lt;安装_目录&gt;\部署_工具\模型\优化器\ </em></p></blockquote><p id="7353" class="pw-post-body-paragraph iv iw hi ix b iy lw ja jb jc lx je jf jg ly ji jj jk lz jm jn jo ma jq jr js hb bi translated">在转换过程中，它会稍微修改模型的性能。那么，模型优化器到底要做些什么来使模型在edge应用中使用起来更轻便呢？它可以做几件事。其中一些是量子化，冻结和融合。</p><h2 id="2602" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">量化</h2><p id="5d22" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">模型优化器可以将权重和偏差的精度从浮点值降低到整数或更低精度的浮点值。非常高的精度值对于训练是有用的，但是在推理中，精度可以降低到较低的精度，即从FP32到FP16或INT8，而不会显著损害模型的精度。这减少了计算时间和大小，但精度损失很小。这个过程叫做量子化。openVINO中的型号通常默认为FP32。在预训练模型中，int8和fp16精度可用。但是模型优化器本身目前不支持int8。</p><figure class="mq mr ms mt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/118549d484510c8a76e3ad78e3a0f9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vbpciTw7XA2I9IxsIXUHYQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">将权重从32位浮点减少到8位整数</figcaption></figure><h2 id="e69a" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">极冷的</h2><p id="1cb5" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">训练神经网络中的术语冻结意味着随机冻结一些层，使其不被训练，以便其他层可以被微调。在model optimizer中，冻结意味着删除一些元数据和操作，这些元数据和操作只对训练网络有用，对推理没有用。冻结仅专门用于张量流模型。例如，反向传播仅用于训练，但它在预测中没有用处。因此，在转换到中间表示的过程中，可以从模型中删除用于反向传播的步骤。</p><figure class="mq mr ms mt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/f3366a35a3d6b72e6b6794d9a2944c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dCekiCbkVH-MQnXLlWj2Qw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">左:训练中的模型，右:冷冻后的模型</figcaption></figure><h2 id="2c15" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">融合</h2><p id="6dc8" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">融合意味着将几层结合成一层。例如，批量标准化层、激活层和卷积层可以合并成一个单层。几个操作可能发生在独立的GPU内核中，但一个fussed层将在一个内核上运行，这消除了从一个内核切换到另一个内核的开销。</p><figure class="mq mr ms mt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/a17f5b8dd4a86e10988af3459140877c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nqlk4GpJc3TL3TUPJ4BFNg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">将多个层组合成一个单层</figcaption></figure><h1 id="729c" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">支持的框架</h1><p id="005b" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">模型优化器支持各种流行的深度学习框架。以下是受支持的深度学习框架列表</p><blockquote class="ll"><p id="3157" class="lm ln hi bd lo lp lq lr ls lt lu js dx translated"><a class="ae iu" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">张量流</a></p><p id="825c" class="lm ln hi bd lo lp lq lr ls lt lu js dx translated"><a class="ae iu" href="https://caffe.berkeleyvision.org/" rel="noopener ugc nofollow" target="_blank">咖啡馆</a></p><p id="043c" class="lm ln hi bd lo lp lq lr ls lt lu js dx translated"><a class="ae iu" href="https://mxnet.apache.org/" rel="noopener ugc nofollow" target="_blank"> MXNet </a></p><p id="eb39" class="lm ln hi bd lo lp lq lr ls lt lu js dx translated"><a class="ae iu" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>(包括PyTorch和Apple ML)</p><p id="34f2" class="lm ln hi bd lo lp lq lr ls lt lu js dx translated">卡尔迪</p></blockquote><p id="0d5f" class="pw-post-body-paragraph iv iw hi ix b iy lw ja jb jc lx je jf jg ly ji jj jk lz jm jn jo ma jq jr js hb bi translated">基于所使用的框架，在如何处理模型方面存在一些差异。但是大部分工作对于所有的模型都是相似的。我将在文章的后半部分讨论不同模型的转换。</p><h1 id="345a" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">中间表示</h1><p id="01fe" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">那么我们谈论了这么久，并试图将不同的模型转换成的这个中间表示是什么呢？这是神经网络的一种常见方言。那是什么意思！不同的框架对相同类型的操作使用不同的名称。例如，张量流中的“Conv2D”、caffe中的“卷积”和ONNX中的“Conv”在普通语言中被称为“卷积”，用于中间表示。如果你感兴趣，openVINO文档有所有不同图层名称的列表。当我们在前面的章节中谈到的不同的优化方法被应用到一个模型中时，这种名称的转换也会发生。IR可以被直接馈送到推理机中，然后推理机可以使用该模型来执行推理。这个表示由两个文件组成，一个<em class="lk"> *。xml </em>文件和一个<em class="lk"> *。bin </em>文件。<em class="lk"> *。xml </em>文件携带模型的架构和<em class="lk"> *。bin </em>文件携带模型的权重和偏差。</p><h1 id="6786" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">我们来转换一个模型吧！</h1><p id="a009" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">我们将从最简单的ONNX模型开始。然后，我们将了解如何转换Caffe模型，最后，我们将使用一个有点复杂的张量流模型。(确保您已经按照第0部分中提到的步骤8到14配置了您的模型优化器，并激活我们创建的虚拟环境。)</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="b611" class="mb ki hi mv b fi mz na l nb nc">openvinoenv\Scripts\activate</span></pre><h2 id="9c51" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">转换ONNX模型</h2><p id="6138" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">ONNX用于转换不同框架的模型格式。由于OpenVINO不直接支持PyTorch，因此可以首先将PyTorch模型转换为ONNX格式，然后使用模型优化器将转换后的模型非常容易地优化为IR格式。OpenVINO的<a class="ae iu" href="https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html" rel="noopener ugc nofollow" target="_blank">文档</a>中有ONNX型号列表。让我们试试第一个模型，<a class="ae iu" href="https://s3.amazonaws.com/download.onnx/models/opset_8/bvlc_alexnet.tar.gz" rel="noopener ugc nofollow" target="_blank"> bvlc_alexnet </a>。</p><ul class=""><li id="ae7f" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">打开命令提示符(windows)/终端(Linux，Mac)，将当前目录更改为保存<em class="lk"> "model.onnx" </em>文件的位置。</li><li id="6ef7" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">运行以下命令。</li></ul><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="2399" class="mb ki hi mv b fi mz na l nb nc">python "C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\mo.py" --input_model model.onnx</span></pre><p id="6a0b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lk">-input _ model "</em>参数是用于指示我们想要转换哪个模型的参数。(示例命令在windows中运行，并使用缺省安装目录OpenVINO。如果您的安装位置不同，请使用适当的路径指向<em class="lk"> "mo.py" </em>，<em class="lk"> " &lt;安装目录&gt;\ open vino \ deployment _ tools \ model _ optimizer \ mo . py "</em>)</p><ul class=""><li id="93db" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">您将得到一个打印行，告诉您创建的<em class="lk"> *的位置。xml </em>和<em class="lk"> *。bin </em>文件，这是我们期望的IR文件。</li></ul><h2 id="6156" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">转换Caffe模型</h2><p id="a5ad" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">将Caffe模型转换成IR也非常简单。ONNX模型优化的不同之处在于，在Caffe的情况下，模型优化器可以接受一些特定于Caffe模型的附加参数。您可以在相关的<a class="ae iu" href="https://docs.openvinotoolkit.org/2018_R5/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html" rel="noopener ugc nofollow" target="_blank">文档</a>中找到关于它们的详细信息。我会让事情变得简单。</p><p id="9e51" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们首先下载一个示例模型来使用。我们将以这个<a class="ae iu" href="https://github.com/forresti/SqueezeNet.git" rel="noopener ugc nofollow" target="_blank"> SqueezeNet </a>型号为例。下载完成后，在保存<em class="lk">“SqueezeNet_v1.1 . caffemodel”</em>文件的“SqueezeNet _ v 1.1”文件夹中更改你当前的目录。运行以下命令(示例适用于windows。针对您情况使用适当的位置和符号)，</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="e02e" class="mb ki hi mv b fi mz na l nb nc">python "C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\mo.py" --input_model squeezenet_v1.1.caffemodel --input_proto deploy.prototxt</span></pre><p id="39b4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们使用了一个额外的参数<em class="lk">-input _ proto "</em>。如果<em class="lk"> *这个论证是必要的。prototxt </em>与<em class="lk"> *名称不同。caffemodel </em>文件或者文件保存在不同的目录下。如果文件名为<em class="lk"> *。caffemodel </em>和<em class="lk"> *。prototxt </em>是相同的，这个论点是没有必要的。</p><p id="51ae" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您将得到一个输出，通知您新创建的<em class="lk"> *的目录。xml </em>和<em class="lk"> *。bin </em>文件。</p><h2 id="94a8" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">转换张量流模型</h2><p id="5788" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">OpenVINO中的TF模型动物园甚至可以进一步扩展预训练模型的范围。TF模型有冰冻和非冰冻两种类型，在模型动物园都有。根据您正在处理的对象，无论是冻结的还是解冻的，程序都会稍有变化。用于转换TF模型的专用<a class="ae iu" href="https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html#freeze-the-tensorflow-model" rel="noopener ugc nofollow" target="_blank">文档</a>页面解释了不同的步骤。</p><p id="b534" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您正在使用非冻结模型，则首先将其转换为冻结模型。这是在python中使用TensorFlow完成的。下面是冻结模型的代码。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="c5ea" class="mb ki hi mv b fi mz na l nb nc">import tensorflow as tf<br/>from tensorflow.python.framework import graph_io</span><span id="054b" class="mb ki hi mv b fi nd na l nb nc">frozen = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, ["name_of_the_output_node"])</span><span id="f7b4" class="mb ki hi mv b fi nd na l nb nc">graph_io.write_graph(frozen, '&lt;where/to/save/&gt;', '&lt;name_of_the_generated_graph&gt;.pb', as_text=False)</span></pre><p id="c32b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lk">“sess”</em>是定义网络拓扑的TensorFlow会话对象的实例。<em class="lk"> ["输出节点名称"] </em>是图形中输出节点名称的列表。冻结图将只包括原始<em class="lk"> "sees.graph_def" </em>中直接或间接用于计算给定输出节点的那些节点。</p><p id="f462" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">或者，您可以按照文档中的特定说明，在模型优化器中直接使用非冻结模型。</p><p id="33f7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作为一个例子，我将使用一个来自模型动物园的冻结图。我将在这里使用的型号是<em class="lk"> SSD MobileNet V2 COCO </em>。<a class="ae iu" href="http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz" rel="noopener ugc nofollow" target="_blank">如果你想自己按照步骤操作，下载</a>模型。</p><p id="3103" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，将当前目录更改为保存<em class="lk">“freezed _ inference _ graph . Pb”</em>文件的位置。</p><p id="9ce1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，在命令行中运行以下命令，</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="a66c" class="mb ki hi mv b fi mz na l nb nc">python <!-- -->"C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\mo.py"<!-- --> --input_model frozen_inference_graph.pb --tensorflow_object_detection_api_pipeline_config pipeline.config --reverse_input_channels --tensorflow_use_custom_operations_config <!-- -->"C:\Program Files (x86)\IntelSWTools\openvino\deployment_tools\model_optimizer\extensions\front\tf<!-- -->ssd_v2_support.json"</span></pre><p id="a8dd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们来分解一下。</p><p id="5f1f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">和往常一样，我们运行python文件“mo.py ”,它有几个参数，其中一些是特定于TF的。</p><ul class=""><li id="c44f" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated"><strong class="ix hj"> - input_model: </strong>获取我们正在转换的模型名称。在我们的例子中，它是<em class="lk">freezed _ inference _ graph . Pb</em>文件。</li><li id="4b7e" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj">-tensor flow _ object _ detection _ API _ pipeline _ config:</strong>这需要一个管道配置文件。</li><li id="3356" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj">-reverse _ input _ channels:</strong>tensor flow模型采用RGB(红绿蓝)颜色通道格式训练。OpenCV使用BGR(蓝绿色红色)格式。如果我们想使用BGR格式，这个参数是必要的。</li><li id="56ee" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj">-tensor flow _ use _ custom _ operations _ config:</strong>在某些情况下，由于存在模型优化器无法识别的自定义操作，模型优化器可能无法转换模型。或者模型优化器和模型使用的张量布局可能不匹配。在那些情况下，一个<em class="lk"> *。json </em>文件以优化器可识别的方式向模型优化器提供关于定制层的提示。</li></ul><p id="a0ef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果一切顺利，您将得到一个输出，显示新创建的<em class="lk"> *的位置。xml </em>和<em class="lk"> *。bin </em>文件。</p><h1 id="7fa3" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">下一步是什么</h1><p id="55f4" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">我们手里有一个经过训练的模型，要么直接从OpenVINO model zoo下载IR格式，要么使用模型优化器转换成IR格式。现在，下一步是在推理机中使用这个模型来执行实际的推理并得到结果。在<a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-3-final-part-664b92c75fde">第3部分</a>中，我们将使用推理机。那么，让我们进入下一部分。</p><p id="5230" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(<em class="lk">链接到所有零件:</em> <a class="ae iu" rel="noopener" href="/@ahsan.shihab2/ai-at-the-edge-an-introduction-to-intel-openvino-toolkit-a0c0594a731c"> <em class="lk">零件0</em></a><em class="lk">&gt;</em><a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-1-51a09752fb4e"><em class="lk">零件1</em></a><em class="lk">&gt;</em><a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-2-1f1a9faa514b"><em class="lk">零件2</em></a><em class="lk">&gt;</em><a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-3-final-part-664b92c75fde"><em class="lk">零件3 </em> </a> <em class="lk"> ) </em></p></div></div>    
</body>
</html>