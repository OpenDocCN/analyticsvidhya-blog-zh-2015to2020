<html>
<head>
<title>Smart Predictor with Attention Mechanism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有注意机制的智能预测器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/smart-composer-with-attention-mechanism-b67e798803b3?source=collection_archive---------6-----------------------#2020-02-05">https://medium.com/analytics-vidhya/smart-composer-with-attention-mechanism-b67e798803b3?source=collection_archive---------6-----------------------#2020-02-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/86c22ec706524904925a50d90bceb21b.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/1*3B6afRVwFOqNJR9eGlQl_w.gif"/></div></figure><p id="6fac" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">目录</strong></p><ol class=""><li id="462e" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated">注意机制导论。</li><li id="2ed1" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">问题陈述。</li><li id="0cdd" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">数据准备</li><li id="73c5" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">Seq2Seq建模，编解码，注意层。</li><li id="9ff3" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">结果</li><li id="fec0" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">未来的工作</li><li id="ea23" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">参考</li></ol><h1 id="070f" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak"> 1。注意力机制介绍</strong></h1><p id="6141" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">这里是我在<a class="ae lb" rel="noopener" href="/@amanvaryani1910/attention-mechanism-in-detail-42f4d9914d10">之前的博客</a>中对注意力机制的详细描述，它涵盖了注意力如何工作的所有细节。</p><p id="d10a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当我们想到英文单词“Attention”时，我们知道它的意思是将你的注意力集中在某件事情上，并给予更多的关注。深度学习中的注意力机制基于这种引导你的注意力的概念，在处理数据时，它更加关注某些因素。</p><p id="7974" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们用这个例子来理解，假设你开始读小说，由于某种原因，你在5-10天内不能读小说。当你在5-10天后继续你剩余的小说时，你将不会再读这部小说。你只需要注意你记得整个故事的情节。注意力就像预测下一句话一样，预测者不会记住整个输入。</p><h1 id="ea64" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">问题陈述</strong></h1><p id="fc7b" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">众所周知，当你给某人写信息时，你会得到下一个单词的预测。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/e272b99e796f55ed1578281a55e88229.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*o_9wmMXYfnoSQ47NLySPhg.jpeg"/></div></figure><p id="95a0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因为它最多只提供两三个单词的预测。</p><p id="c7b0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 2。谷歌自动完成</strong></p><p id="98cb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Google建议自动完成你的句子，这也是一个基于模型的解决方案，考虑到目前为止输入的搜索短语，并在趋势搜索中运行前缀搜索。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/c5e78261557d399214ac721d73106c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/1*FFG0pwBpHRn3l3N-1_e1NQ.gif"/></div></figure><p id="4f8d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 3。GMAIL智能撰写</strong></p><p id="382b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如你在上面的例子中所看到的，单词级预测最适合较短的句子，但是在电子邮件中，你的句子会根据情况而变化。因此，用户需要输入的内容越少，效率就越高。如果你仔细观察，你会发现用户重复很多句子，从问候到基本问题。</p><p id="f916" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">智能合成不同于其他单词级预测。它不仅使用当前文本，还使用电子邮件的主题和您发送给该人的前一封电子邮件。它还可以记录时间，所以在写问候的时候，它也会自动完成你的问候。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es li"><img src="../Images/ddc67cde7b08beb07d4f06c2899a6805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*sX8thmPULgCS-IUvCWGvGw.jpeg"/></div></figure><p id="258f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以我试着做了一个智能写作的原型，它通过用户输入的前一个单词来预测下一个句子。由于它不是s <a class="ae lb" href="https://arxiv.org/abs/1906.00080" rel="noopener ugc nofollow" target="_blank"> mart compose </a>的精确复制品，开发者使用了许多东西来预测下一句话和多年的研究。这个项目背后的主要动机是让人们清楚地了解注意力是如何工作的，并尽可能得到最好的结果。</p><p id="0465" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在深入研究技术代码之前，有一个小提示:如果你对LSTM、RNN、Seq2Seq和Attention有基本的了解，你会对代码有清晰的理解。顺便说一句，我保证尽可能简单地交付它。</p><h1 id="8c46" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">数据准备</strong></h1><p id="0ae6" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">我使用了kaggle上的<a class="ae lb" href="https://www.kaggle.com/wcukierski/enron-email-dataset" rel="noopener ugc nofollow" target="_blank">安然电子邮件数据集</a>。安然电子邮件数据集包含大约500，000封由安然公司员工生成的电子邮件。这是联邦能源管理委员会在调查安然公司倒闭时获得的。</p><p id="47fd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">或者你可以通过python中的<a class="ae lb" href="https://tobler-optik.ch/blbjq/python-extract-data-from-email.html" rel="noopener ugc nofollow" target="_blank"> API </a>来使用你自己的电子邮件数据集。</p><p id="b406" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用安然电子邮件数据集时，第一步是将所有信息压缩成两列。这里是<a class="ae lb" href="https://github.com/amanv1906/SMART-COMPOSER-WITH-ATTENTION-MECHANISM/blob/master/EXTRACTING_DATA.ipynb" rel="noopener ugc nofollow" target="_blank">提取信息</a>的链接。</p><p id="ad68" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">提取数据后，我们只需要包含电子邮件消息部分的消息正文，如下所示。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/39c87e666ba8506f8a4d6474c599b812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*1Dtq1Sv9xVrsJhzzMsZ0ow.jpeg"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">msg _安然电子邮件正文</figcaption></figure><p id="b641" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们要做的第一件事是预处理数据，因为你可以看到数据非常脏，它包含许多不必要的符号。</p><p id="065f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">最近，在语言模型的开发中使用神经网络已经变得非常流行，以至于它现在可能是优选的方法。</p><p id="fd4e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在语言建模中使用神经网络通常被称为神经语言建模，或简称为NLM。</p><p id="6901" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">无论是在独立的语言模型上，还是在语音识别和机器翻译等具有挑战性的任务上，神经网络方法都比经典方法取得了更好的结果。</p><p id="11e5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">具体而言，采用单词嵌入，其使用实值向量来表示投影向量空间中的每个单词。这种基于单词用法的单词学习表示允许具有相似意思的单词具有相似的表示。</p><p id="20a4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你可以把基于单词的语言模型想成这样:</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lo"><img src="../Images/2705879c85988cc672380657f2ff5112.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0KMOJ-q6lrBZ47WbdFDjlg.jpeg"/></div></div></figure><p id="01ab" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">简单地说，语言模型的目标是分解文本语料库，并为文本序列分配概率，通常是一次一个单词。(但也有其他变种)。</p><p id="c641" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我使用了一个字符一个字符的模型，而不是逐字逐句。</p><p id="7cbd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对语言建模的理解超出了本博客的范围，我们使用语言建模来生成seq。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/a0740c587053f66dfed74c2be674985f.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*jlc1LcNVOhPBjvYOgKlhwg.jpeg"/></div></figure><p id="9fad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">\n和\t只是句首和句尾的前缀和后缀。我附上这个是因为在预测给定输入的句子时，我们的模型知道什么时候开始，什么时候结束。如果你开始输入<strong class="io hj"><em class="lu">【w】</em></strong><em class="lu"/>它就会预测到<strong class="io hj"> <em class="lu">【如你日】</em> </strong> <em class="lu">。</em>所以我们可能需要一个比单词级的LM更细粒度的LM。对于机器学习和深度学习任务，给定的输入可以是任何形式，因此应该转换为数字数据。对于这个任务，我们的输入是句子，所以我们必须对输入进行标记，并制作包含特定单词索引映射的字典。</p><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="3087" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上述函数逐个字符地标记句子并填充序列。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lx"><img src="../Images/980cef8c0da8b7dbf0378f71ccda8235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vEAWo1qrhN5IfCAD5VKb2A.jpeg"/></div></div></figure><p id="4144" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们的输入将如下所示。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es ly"><img src="../Images/69753df2583e08c6dac08046a07bb036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mnTnDaGtO16x_oYV0V-ueA.jpeg"/></div></div></figure><h1 id="7678" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">Seq2Seq建模，编解码，注意层。</h1><p id="1ab7" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">顾名思义，seq2seq将一个单词序列(一个或多个句子)作为输入，并生成一个单词输出序列。这是通过使用递归神经网络(RNN)来实现的。虽然很少使用RNN的普通版本，但它的更高级版本，即LSTM或GRU，会被使用。这是因为RNN面临着梯度消失的问题。它通过在每个时间点获取2个输入来发展单词的上下文。一个来自用户，另一个来自之前的输出，因此称为递归(输出作为输入)。</p><p id="eaef" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下面提到seq2seq模型有许多变体。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lz"><img src="../Images/8cfcacb5e46a3cf6fa0cc65cc7d4105b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NcUC6v4Lbm0siGjsqyIANQ.jpeg"/></div></div></figure><p id="164e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于我的任务，我使用了多对多，因为我不知道我的输出句子长度会是多少。</p><p id="e4b2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">它主要由两部分组成，即<em class="lu">编码器</em>和<em class="lu">解码器</em>，因此有时被称为<strong class="io hj">编解码网络</strong>。</p><p id="c89d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">编码器:</strong>使用深度神经网络层，将输入的单词转换成相应的隐藏向量。每个向量代表当前单词和单词的上下文。</p><p id="7e54" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">解码器:</strong>类似于编码器。它将编码器生成的隐向量、自身的隐状态和当前单词作为输入，产生下一个隐向量，并最终预测下一个单词。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es ma"><img src="../Images/d8fa9f31f9d931cbdb4ff94a5440bfde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JWElloUd8ttb6xx8.png"/></div></div></figure><p id="d05b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们开始:</p><p id="8b5f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你可以在最后提供的github链接中找到所有必要的代码</p><p id="dc5b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">建筑:- </strong></p><h2 id="b663" class="mb jz hi bd ka mc md me ke mf mg mh ki ix mi mj km jb mk ml kq jf mm mn ku mo bi translated">编码器</h2><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="9742" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第一步编码器部分</strong></p><p id="f143" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">编码器是简单的LSTM层，在每个时间步<br/>接受单一输入。返回状态将为真，即先前时间步<br/>的隐藏状态将输入当前时间步的LSTM状态。</p><p id="00c3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一个数据序列，即一个句子，被传递到编码器，<br/>记住，在将该序列数据传递到编码器或<br/>解码器之前，必须对该序列应用填充和标记<br/>，与传递到LSTM模型的序列数据相同。</p><p id="2ee6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">注意:</strong></p><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="153b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第二步:注意部分</strong></p><p id="fb62" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在前一层(编码器)中生成的权重向量和输出被传递给关注层<br/>。在注意层中，隐藏状态和输出通过密集层，以便它们可以用反向传播<br/>来训练，在此基础上应用双曲正切激活函数，然后与可训练向量相乘。<br/>之后，应用softmax，整个向量的和变为1，向量中的每个值在0和1之间变化<br/>，从而知道每个时间步长的每个输入的权重。</p><h1 id="6d74" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">注意机制</h1><ol class=""><li id="5c8c" class="jk jl hi io b ip kw it kx ix mp jb mq jf mr jj jp jq jr js bi translated">隐藏形状== (Batch_Size，隐藏单位)</li><li id="bb97" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">y_hidden_states =(批量大小x 1 x隐藏单位)</li><li id="1d4e" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">我们这样做是为了执行加法来计算分数</li><li id="9411" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">并将密集层的两个输出相加</li><li id="d925" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">现在，来自密集层的输出的总和与编码器输出的输出相加，并且与tanh和单个单位密集层相加</li><li id="90f0" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">分数形状==(批处理大小，序列长度，1)</li><li id="8948" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">我们在最后一个轴上得到1，因为我们对self应用了分数。V</li><li id="69d5" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">应用self之前张量的形状。v是(批量大小，最大长度，单位)</li><li id="245c" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">我们将softmax应用于分数，以获得0和1之间的值，值的总和= 1。应用axis = 1的原因是因为默认情况下softmax仅应用于最后一个轴，但我们必须应用于Tx状态(Tx是超参数)</li><li id="e489" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">上下文向量形状=(批量大小，隐藏单位)</li></ol><p id="73bd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">解码器:</strong></p><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="2a2c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第三步。解码器部分</strong></p><p id="4ccc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">到解码器层的输入也与在前一层中生成的上下文向量连接(注意)<br/>，并且该连接的向量作为输入被传递到解码层，如图所示。解码器将在每个时间步长生成输出。将这些生成的输出与目标值进行比较，并反向传播所有层以获得最佳权重。</p><h1 id="6d3a" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">培训:-</h1><p id="f325" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">定义优化器、损失函数和精度函数。</p><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="fa71" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用急切执行，我们为10个时期训练网络。要阅读更多关于渴望执行的内容，请参考官方文档<a class="ae lb" href="https://www.tensorflow.org/guide/eager" rel="noopener ugc nofollow" target="_blank"> <em class="lu">此处</em> </a>。</p><p id="5f8d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">输出:</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es ms"><img src="../Images/34e8d27bd5fdea47e1fe22f54a1a8efd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VQ6hmBvNgkZAGLOpJshQvw.jpeg"/></div></div></figure><h2 id="9caf" class="mb jz hi bd ka mc md me ke mf mg mh ki ix mi mj km jb mk ml kq jf mm mn ku mo bi translated">绘图的可视化:</h2><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/b2520ff897877d09cf7411dcb98723f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*dCDwGYrGN3jkBRhXOGaYgQ.jpeg"/></div></figure><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/278fd03bda52a299b7938bb618acc576.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*uIaTd04cl5cLxpxUniRouQ.jpeg"/></div></figure><h2 id="b0d6" class="mb jz hi bd ka mc md me ke mf mg mh ki ix mi mj km jb mk ml kq jf mm mn ku mo bi translated">测试:</h2><p id="8a30" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">对于测试，我们使用bleu评分。BLEU score计算句子的相似度，我们通过预测句子和实际句子，检查我们的预测输出是否与实际相同。</p><p id="753c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">例如:</p><p id="e42d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">输入序列是我的模型所采用的，预测序列是当输入序列通过模型时，我们得到的预测输出。<a class="ae lb" href="https://machinelearningmastery.com/calculate-bleu-score-for-text-python/" rel="noopener ugc nofollow" target="_blank"> BLEU SCORE </a>是当预测输出包含与原始句子中相同的单词时，输出为一个eg。</p><p id="e9f5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">original = [['this '，' is '，' small '，' test']]</p><p id="b7e9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">预测= ['this '，' is '，' a '，' test']</p><p id="e50c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">BLEU评分:0.75(因为我原来的句子里没有a。</p><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="bc36" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当t在那里时我们停止，这意味着句子在那里结束。</p><p id="0c98" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">测试句子的可视化:</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mv"><img src="../Images/0483849bb6d49fe36c2aa83563ed34d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SUmhXiZcRoNet2Ty-vKoWQ.jpeg"/></div></div></figure><h2 id="0d85" class="mb jz hi bd ka mc md me ke mf mg mh ki ix mi mj km jb mk ml kq jf mm mn ku mo bi translated">未来工作:-</h2><p id="9bf5" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">尝试用flask做一个web app。</p><p id="32e1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用变压器</p><h1 id="27b3" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">参考:-</h1><ol class=""><li id="4900" class="jk jl hi io b ip kw it kx ix mp jb mq jf mr jj jp jq jr js bi translated"><a class="ae lb" href="https://www.coursera.org/lecture/nlp-sequence-models/attention-model-lSwVa" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/lecture/NLP-sequence-models/Attention-model-lSwVa</a>(吴恩达关于注意力的解释)</li><li id="0556" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">https://arxiv.org/abs/1409.3215<a class="ae lb" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank"/></li><li id="d699" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">【https://keras.io/examples/lstm_seq2seq/ T4】</li><li id="8ffd" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated"><a class="ae lb" href="https://arxiv.org/abs/1406.1078" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1406.1078</a></li><li id="cc28" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated"><a class="ae lb" href="https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/" rel="noopener ugc nofollow" target="_blank">https://machine talk . org/2019/03/29/neural-machine-translation-with-attention-mechanism/</a></li><li id="77b7" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated"><a class="ae lb" href="https://www.appliedaicourse.com/course/11/Applied-Machine-learning-course" rel="noopener ugc nofollow" target="_blank">https://www . Applied ai course . com/course/11/Applied-Machine-learning-course</a></li></ol><h2 id="b7d9" class="mb jz hi bd ka mc md me ke mf mg mh ki ix mi mj km jb mk ml kq jf mm mn ku mo bi translated">代码:-</h2><p id="17ec" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">你可以在github repo <a class="ae lb" href="https://github.com/amanv1906/SMART-COMPOSER-WITH-ATTENTION-MECHANISM" rel="noopener ugc nofollow" target="_blank">这里</a>获得我的完整代码</p><p id="2e59" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae lb" href="https://www.linkedin.com/in/aman-varyani-885725181/" rel="noopener ugc nofollow" target="_blank">领英</a></p></div></div>    
</body>
</html>