<html>
<head>
<title>Gradient Boosting — A Bird’s eye view into one of the most widely used ML Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度增强——最广泛使用的最大似然算法之一的鸟瞰图</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-boosting-a-birds-eye-view-into-widely-used-ml-algorithm-a95db080d256?source=collection_archive---------19-----------------------#2019-10-08">https://medium.com/analytics-vidhya/gradient-boosting-a-birds-eye-view-into-widely-used-ml-algorithm-a95db080d256?source=collection_archive---------19-----------------------#2019-10-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ede312a41e40b3d5e3793fc48bf52d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DDbqqAC3Nk2sE6f9"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">杰佛森·桑多斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="bf21" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度推进是最广泛使用集成方法。它是用于解决许多复杂问题的许多机器学习算法之一。广义上讲，集成方法可以分为两类，即boosting和bagging。顾名思义，梯度推进属于第一类，即推进。</p><p id="6fe1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度增强背后的直观想法很简单。根据响应值拟合第一个模型，然后根据各自的负梯度拟合后续模型，然后将其集成以获得最终预测值。</p><p id="d649" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据上述直觉，很容易认为梯度推进算法背后模型与梯度下降非常相似，其中:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/fbec1817b9e294f6b505e18ca15ffc6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*NU82JzlMyjkFAMUACRZh4w.png"/></div></figure><p id="9454" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">L是损失函数，L(fn(x)，y)是集合n个基本模型后的损失。</p><p id="5590" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">也就是说，我们首先在响应变量(y)上拟合模型，该响应变量由f0(x)预测，然后<strong class="ix hj"> <em class="jy">在gradient -d[L(f0(x)，y)]/d[fn(X)]</em>【T7]上拟合模型，并集合它以获得f1(X)并迭代地进行该过程。虽然从广义上讲这就是梯度推进算法的工作方式<strong class="ix hj"> <em class="jy">但这并不完全正确</em> </strong>。</strong></p><p id="c330" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了更深入地理解梯度增强，我们首先要理解<strong class="ix hj"> <em class="jy">“在gradient -d[L(f0(x)，y)]/d[fn(x)]”</em></strong>实际上是什么意思<strong class="ix hj"> <em class="jy">。这里我们将考虑一个场景，其中我们的基础模型是决策树。也就是说，我们在响应值/负梯度上拟合决策树，然后集成它。</em></strong></p><p id="3570" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">任何机器学习算法背后的思想都是识别一个适当的损失函数，然后将其最小化。对于回归任务，损失函数可以是均方误差(MSE)或平均绝对误差(MAE ),对于分类，损失函数可以是分类交叉熵、铰链损失等。</p><p id="6afd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，我们考虑损失函数为L(fn(x)，y)的一般情况，其中，</p><p id="ce6b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">y是观察到的响应值。</p><p id="4396" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">fn(x)是n个决策树集成后的预测值</p><p id="f1ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">l是损失函数。</p><p id="5190" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设我们的特征向量空间是m维的，也就是说，</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jz"><img src="../Images/590e68f3aed4d9280ac25feb159fe341.png" data-original-src="https://miro.medium.com/v2/resize:fit:168/format:webp/1*n_MRtExJfoF-oEp5Wfrrjg.png"/></div></figure><p id="a181" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在梯度推进算法中，我们主要解决两个优化算法</p><p id="4cb4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第一种优化算法将m维特征向量空间x分成不同的最佳区域，每个区域具有m维(树节点的分裂)。</p><p id="18b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第二种优化算法评估每个区域的最佳预测值(评估每个叶子的预测值)。</p><p id="388a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">寻找最佳区域</strong></p><p id="9540" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第一个优化问题的目标函数是</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/3abff8881993d3f999277a8b87770167.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*DAbbI53HeVJkaUjzTz53zA.png"/></div></figure><p id="a690" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在哪里</p><p id="a405" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> gi= d[L(fn(x)，y)]/d[fn(x)] </strong>对于i=1，2，3，4……..p，p是训练集中数据点的数量。</p><p id="b215" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> T(x,ʘ) </strong>是该地区ʘ.的预测值</p><p id="4baf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以基本上上面的目标函数<strong class="ix hj"> <em class="jy">实现的是找到区域(ʘ) </em> </strong>，最小化目标函数<strong class="ix hj"> A. </strong></p><p id="fd88" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意目标函数是(A)是平方损失。这被称为<strong class="ix hj"> <em class="jy">弗里德曼损失函数</em> </strong>。</p><p id="e557" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jy">注意，我们解决的是分类问题还是回归(梯度推进分类或梯度推进回归)问题并不重要，使用这个弗里德曼损失函数总能找到最佳区域。</em> </strong></p><p id="48ef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是所谓的<strong class="ix hj"> <em class="jy">【渐变上拟合模型】</em> </strong></p><p id="3714" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在标准决策树中，当我们解决类似于(a)的优化问题时，我们也已经获得了该区域(ʘ)的预测值T(x,ʘ，在这种情况下，不需要解决第二个优化问题。</p><p id="427c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，在梯度推进算法中，很酷的事情正在发生。这里，一个区域被识别，第二优化算法被优化以评估预测值。</p><p id="fec0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">获得预测值</strong></p><p id="9f24" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第二个优化问题的目标函数是</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/b7f4ec3da0dfbd5a98e9a48c596b30a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*dBWrhvsMhuViA3u2GNXglA.png"/></div></figure><p id="8fea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在哪里</p><p id="764d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">j= 1，2，3，4……q(树的总区域/末端叶)</p><p id="630f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Rj是第j个末端叶片。</p><p id="5557" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一.在Rj地区的观察</p><p id="de3b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">fni (x)是在n个基础模型(在这种情况下是树)被集合用于第I次观察之后的预测值</p><p id="b033" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">求解优化问题(B)我们在n+1个基础模型被集成之后得到预测值，并且它被给出为:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kc"><img src="../Images/282a9241638c5d7c61ece4a9bc10cbc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*Dx4BB51ntPqtXqblFqK7cw.png"/></div></figure><p id="3cc6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在哪里</p><p id="a27e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">γj通过优化区域j的目标函数B来获得</p><p id="3a3b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">I(i ͼ Rj)是指示函数，如果i ͼ Rj为1，否则为0。</p><p id="24f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">j= 1，2，3，4……q(树的总区域/末端叶)。</p><p id="66e6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">求解这两个算法完整的解释了梯度提升算法。为了更好地理解，需要强调梯度增强的某些特征。</p><p id="7182" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1.无论我们的任务是什么，为了找出最优区域(决策树的叶子),我们总是使用弗里德曼损失函数，这是一个平方损失函数。我们之所以用这个，是因为我们有很多不同的方法来优化平方损失函数。</p><p id="d912" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.然而，为了评估预测值，我们使用损失函数<strong class="ix hj"> L </strong>，损失函数的选择取决于我们使用的是分类问题还是回归问题。这样做的好处是，它在某种程度上也正规化了。</p><p id="9fc8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以上文章试图简要解释梯度推进算法如何工作。这是一种广泛用于解决许多ML问题的算法，并且经常通过超参数调整给出好的结果。我会在下一篇文章分享用python中的<em class="jy"> sklearn包</em>实现梯度提升算法。</p></div></div>    
</body>
</html>