<html>
<head>
<title>PCA — Demystified.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA——去神秘化。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pca-demystified-3f38a9e78cd2?source=collection_archive---------17-----------------------#2020-10-03">https://medium.com/analytics-vidhya/pca-demystified-3f38a9e78cd2?source=collection_archive---------17-----------------------#2020-10-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d2d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常在机器学习中，数据集有许多用来进行预测的特征。<strong class="ih hj">主成分分析(PCA) </strong>是一种用于降低维度的技术。它经常被误解为特征选择技术，但它是特征提取。在这篇文章中，我会给你一个主成分分析的概念，所涉及的数学，我们为什么以及什么时候使用主成分分析？</p><h2 id="3891" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">介绍</h2><p id="2e52" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">维数灾难是指数据的属性相对于观察值的数量更多的情况。机器学习模型享有大量的特征，但这导致了数据点的稀疏性。为了说明相关的问题，让我们考虑直线上 0 和 1 之间的距离，即 1。现在，如果我们添加另一个维度，点现在变成(0，0)和(1，1)，距离增加到 2 的平方根。在三维空间中，距离增加到 3 的平方根。如果它们比这个范围(0，1)中的点的数量多，则相似性或差异可以被放大，但是如果它们比点的数量少，则这些点变得稀疏和更远。潜在的陈述是“特征的质量比用于训练模型的特征的数量更重要”</p><p id="3171" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据经验，每个特征应该至少有 5 个观察值，这也取决于数据是否能捕捉到各种可能的组合。简单来说，<strong class="ih hj">数据框不应该更宽，而应该更高。</strong>如果数据维数很高，特征空间将会有空白，数据点将会分散。删除特征不是一个明智的想法，因为它可能包含重要的信息。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kd"><img src="../Images/5f4a44162485ee40a80c52f1b09a5128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Hctqnz4xZGmf94lDbicGXA.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">左边的表容易出现维数灾难，可以将其转换为右边的表来捕获不同的实例。(<a class="ae kp" href="https://stackoverflow.com/questions/29844056/how-to-transform-data-frame-with-different-column-names-from-wide-to-long-with" rel="noopener ugc nofollow" target="_blank"> Sourc </a> e)</figcaption></figure><p id="4c4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">领域知识可以帮助将数据框架转换到较低的维度，如上例所示。大多数情况下不会出现这种情况，对此，五氯苯甲醚会出手相救。<strong class="ih hj">PCA 的目标是捕捉与数据集相关的最大方差，但在更少的维度中。</strong></p><h2 id="dbe2" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">感性艺术类比</h2><p id="a1b0" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">让我们看一个例子来理解 PCA 是如何工作的。考虑一个像下图这样的感知艺术，只有从一个特定的角度来看，艺术才会有一个已知的形式。在所有其他视图中，它看起来像是散布在空间各处的随机艺术作品(维度的诅咒)。最好的视图是将艺术转化为 2D 图像，并且所有的白色空间都被省略的视图。同样，主成分分析试图找到能够降低特征空间稀疏性的最佳方向。在整篇文章中，我将使用这个类比。因此，像“最佳视图”这样的短语表示数据集的理想特征。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es kq"><img src="../Images/6d94541e715941dc85eb0c611f8db80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6rJ9GPuN4Xe4L1C2mtdmAQ.jpeg"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><a class="ae kp" href="https://awesomebyte.com/perceptual-art-of-michael-murphy/" rel="noopener ugc nofollow" target="_blank">迈克尔·墨菲的感知艺术</a></figcaption></figure><p id="65a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们深入研究与 PCA 相关的数学，并了解它如何找到最佳观察方向。下面是代码片段，其中创建了两个随机相关变量 x1 和 x2。从数据集中取一个随机数据点，并重复乘以(点积)协方差矩阵。点积的合成向量收敛于相同的斜率(尝试 x1 和 x2 的任意值)。实际上，协方差矩阵试图将数据点投影到一条线上。</p><figure class="ke kf kg kh fd ki"><div class="bz dy l di"><div class="kv kw l"/></div></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kx"><img src="../Images/1f7b2dd368c873c282adb1911a4a9940.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*fVFtd_LpFxsdIQmo9l7p1A.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">投影到协方差矩阵上的向量的收敛斜率。</figcaption></figure><p id="7401" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，如果向量(随机选择的数据点)具有相同的收敛斜率会怎样？由点积得到的矢量与初始矢量方向相同，但大小不同。假设‘e’是向量,[cov]是协方差矩阵，下面的等式代表标准。λ是一个标量值，它决定了幅度的变化。该方程是标准的特征向量方程。向量' e '称为特征向量，λ称为特征值。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ky"><img src="../Images/26c2547636b0df08286d687e58738bf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/0*mCY6_skcYMzYKkSR"/></div></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kz"><img src="../Images/05e46710643720f948a9b986aceef46a.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*qGU1bzONHBqLLOu4NGNYzw.gif"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">线性变换后，本征向量的幅度变化而方向不变。(<a class="ae kp" href="https://thumbs.gfycat.com/FluffyMiniatureBackswimmer-max-1mb.gif" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="d4ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这看起来不错，但是这对降维有什么帮助呢？为了回答，在这个二维数据中，如果我们将所有点投影到一条线(主轴)上，同时保留数据中的最大方差(组合)。可以实现降维。我们将让这条线捕捉数据点的最大可变性(分布),将 2D 数据转换为 1D 数据。从感性艺术的解读中，我们试图看到艺术的 2D 形象，这是一种低维度的艺术。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es la"><img src="../Images/5361a0bc72c3c329aeb563eb45740a17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*O17hRGlt1IFqbn6OAehVRg.gif"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">可能的最佳方向。(<a class="ae kp" href="https://www.kdnuggets.com/2020/05/dimensionality-reduction-principal-component-analysis.html" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><h2 id="589f" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">为什么特征向量是最好的观点？</h2><p id="771a" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">最佳向量的标准是它应该捕捉最大方差。设“d”代表数据点，“d1”是“d”在任意向量‘j’上的投影。最大化 d1 的方差将产生最佳‘j’。因为焦点在方向上，所以矢量 J 可以具有可变的幅度。标准化向量将限制“j”。拉格朗日乘数用于归一化。</p><p id="1b57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将方差(d1)的导数等于零，我们得到一个表示 d1 的协方差矩阵的特征向量的方程。这意味着最佳视图实际上是特征向量。我保持数学简单，但为了更深入的理解，<strong class="ih hj">你可以参考链接</strong> <a class="ae kp" href="https://www.cs.toronto.edu/~urtasun/courses/CSC411/tutorial8.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">【最大方差】</strong> </a> <strong class="ih hj">。</strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lb"><img src="../Images/ed264237cf587b4ee8a2c54b4ee9472f.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/0*gNt9MyLtPwnWmA8v"/></div></figure><h2 id="0830" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">特征向量的正交性</strong></h2><p id="bf01" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">特征向量的一个性质是，当矩阵对称时，对应的特征向量彼此正交。考虑 n 乘 n 的矩阵 C，其中 n 乘 1 的特征向量 e1、1 乘 n 的特征向量 e2 以及 l1 和 l2 是它们各自的特征值。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lc"><img src="../Images/eca0afc86c3c9583673c5900fcf54774.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/0*96ucg8DmvOKhk4r_"/></div></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ld"><img src="../Images/55b957d93999a69f528bfd547dd98afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/0*WjUyev3vWoEO-OmU"/></div></figure><p id="5d61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于协方差矩阵 C 是对称的，我们可以有下面的等式</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es le"><img src="../Images/fcd4b7768a528ee1fb7ef2ec9c80229b.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/0*WdFg4XpH284Ivu-7"/></div></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es le"><img src="../Images/e0b70953d22daad6dbc79d57a79f058c.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/0*ebS2BKWB5zH1vUIU"/></div></figure><p id="f10a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面两个等式，我们得到</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lf"><img src="../Images/bf45bad5bbc87f97f298f63556c3969c.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/0*ziTaYWQ9fh5hB4QM"/></div></figure><p id="8f39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果 L1 和 L2 是唯一的，e2 和 e1 的点积必须为零。这表明特征向量是正交的。如果它们不正交，新的特征空间将具有不同于 90 度的坐标轴。因此，这些向量成为新的坐标系。</p><h2 id="37ad" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">主成分分析的流程。</h2><ul class=""><li id="f6b6" class="lg lh hi ih b ii jy im jz iq li iu lj iy lk jc ll lm ln lo bi translated">提取自变量。请注意，目标属性不应包含在 PCA 转换中。</li><li id="8c79" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">将数据标准化，使数值以原点为中心。</li><li id="8453" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">从协方差矩阵中提取特征向量和特征值。</li><li id="0682" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">对特征值进行排序，选择顶部的特征向量(主轴),其捕获了期望的方差量(通常为 95%)。</li><li id="abb5" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">将数据点投影到选定的特征向量上，创建新的维度。点积返回转换后的坐标。</li><li id="ce4d" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">在这些新的维度上构建模型。</li></ul><p id="015e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">形成的新维度将是每个旧属性的组成部分。新维度‘D’可以被视为原始维度的线性组合。β值表示特定特征对新提取特征的影响。像这样，直到 D(n)创建了“n”个新维度。在这之前，维度没有减少。在分析新特征捕获的差异后，不需要的尺寸将被删除。从而实现降维。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es lu"><img src="../Images/329d353ea7796c6b1085d9ae7b6ed8fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/0*L_lnOlNPaqVCDYNC"/></div></figure><p id="9d6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是一个 python 实现，来看看 PCA 是如何工作的。</p><div class="lv lw ez fb lx ly"><a href="https://github.com/uknwho/MachineLearning_-DataSets_solution/blob/master/PCA_Example/PCA_Example.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hj fi z dy md ea eb me ed ef hh bi translated">uknwho/machine learning _-DataSets _ solution</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">permalink dissolve GitHub 是超过 5000 万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">github.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm kj ly"/></div></div></a></div><p id="c360" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在该示例中，转换后的 10 个 PCA 维度捕获了总方差的 96%。数据帧减少了 3 个特征。</p><h2 id="e4af" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">什么时候使用 PCA？</strong></h2><p id="282d" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">要记住的一点是，只有当大多数数据表现出良好的相关性时，PCA 才会有效。要理解这一点，请看下图。当存在强相关性时，可以获得最佳拟合线，这导致较少的稀疏性。第三张图中的最佳拟合线的数据点将远离该线。需要更多的维度来捕捉良好的方差。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es mn"><img src="../Images/bb54f5d8922c2d3d8bd7ddc0c822da04.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*wg2TKE36AOmbpLPrc95_bQ.jpeg"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">相关数据(T2 来源)</figcaption></figure><p id="63c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">提取的新维度没有任何意义。如果这不是一个要求，那么五氯苯甲醚将是一个很好的选择。在图像和视频中，PCA 技术将非常有效，因为特征的含义是不相关的。除了 PCA，探索 SVD 另一种广泛使用的降维技术。</p></div></div>    
</body>
</html>