<html>
<head>
<title>Transfer Learning — Powering up with Pretrained Models (Read: BERT)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">迁移学习—使用预训练模型启动(阅读:BERT)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/transfer-learning-powering-up-with-pretrained-models-read-bert-ce7da68f3e08?source=collection_archive---------8-----------------------#2020-01-06">https://medium.com/analytics-vidhya/transfer-learning-powering-up-with-pretrained-models-read-bert-ce7da68f3e08?source=collection_archive---------8-----------------------#2020-01-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="97e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">奖励:在互联网关闭的情况下使用变形金刚</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/0b10d134da8197bd8e76a7ca4ae16ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bpzk6a-DGwymRaiK"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">由<a class="ae jt" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马库斯·斯皮斯克</a>在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="c0a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi ju translated">关于这些预训练模型已经说了很多，但是，围绕迁移学习意义的重要概念，我们如何使用它需要一些挖掘。例如，如果您使用了BERT的tfhub版本。有人可能会问，这一层的“可训练”是什么？</p><p id="456e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">事实证明，我们有三种方法可以进行预训练模型的迁移学习:</p><ul class=""><li id="e6a3" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated">特征提取:预训练层仅用于提取特征，如使用BatchNormalization将权重转换为0到1之间的范围，平均值为0。在这种方法中，权重在反向传播期间不更新。这是在模型摘要中标记为不可培训的内容</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es km"><img src="../Images/84d0a3f36206b8f6e1196e97223ec1ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rIFiyUEg_H2dBP1-SWkihQ.png"/></div></div></figure><p id="3eb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并且在tfhub中为可训练=假。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kn"><img src="../Images/01bb3ba75ff1df211b5addc9bafe3a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4o2ENPFejy27-ARV6pZQw.png"/></div></div></figure><p id="5d5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想手动设置。然后，像这样选择图层:</p><blockquote class="ko"><p id="db7b" class="kp kq hi bd kr ks kt ku kv kw kx jc dx translated">model.add(dense(100，可训练=False))</p></blockquote><ul class=""><li id="0f01" class="kd ke hi ih b ii ky im kz iq la iu lb iy lc jc ki kj kk kl bi translated">微调:这就是整个比赛的意义所在。BERT是这项任务的理想选择，因为它接受过问答训练。因此，我们只需微调模型以适应我们的目的。含义-该图层已针对常规数据集进行了训练。我们需要重新培训，以优化我们的具体任务。同样，在TFHUB中，这是可训练的=真。(见上文—以绿色突出显示)。在Keras模型总结中，它被称为可训练参数(见上文-以绿色突出显示)。对于具有多层的预训练模型，通常这将是巨大的。Albert是轻型版本，包含1100万个参数。</li><li id="4dff" class="kd ke hi ih b ii ld im le iq lf iu lg iy lh jc ki kj kk kl bi translated">提取层:在这种方法中，我们只提取任务所需的那些层。例如，我们可能希望只提取BERT中较低级别的层，以执行POS、情感分析等任务，其中只提取单词级特征就足够了，不需要太多的上下文或序列匹配。下面是一个例子</li></ul><pre class="je jf jg jh fd li lj lk ll aw lm bi"><span id="4949" class="ln lo hi lj b fi lp lq l lr ls">import tensorflow as tf<br/>from transformers import BertTokenizer, TFBertModel</span><span id="f268" class="ln lo hi lj b fi lt lq l lr ls">tokenizer = BertTokenizer.from_pretrained(‘bert-base-uncased’)<br/>model = TFBertModel.from_pretrained(‘bert-base-uncased’)<br/>input_ids = tf.constant(tokenizer.encode(“Hello, my dog is cute”, add_special_tokens=True))[None, :] # Batch size 1<br/>outputs = model(input_ids)<br/>last_hidden_states = outputs[0] # The last hidden-state is the first element of the output tuple</span></pre><p id="1185" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lu"> BERT —重要提示</em> </strong> <br/>鉴于对参数的狂热，采用BERT论文推荐:</p><ul class=""><li id="fc7a" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated">纪元-范围在3，4之间</li><li id="1ebe" class="kd ke hi ih b ii ld im le iq lf iu lg iy lh jc ki kj kk kl bi translated">Batch_size — 4，8，16(如果您针对特定群体进行训练，比如较小的样本，那么可能是32)</li><li id="9f97" class="kd ke hi ih b ii ld im le iq lf iu lg iy lh jc ki kj kk kl bi translated">层-除了输出和平均/最大池以外，您可能不需要添加任何额外的层来根据您的要求重新调整bert输出。因为伯特或等同物已经为我们优化了层和隐藏单元。我尝试添加一个消失梯度层——这些深度信念网络可以将权重缩小到几乎为零，所以我添加了一个泄漏的relu层来减缓权重恶化。模型运行时间疯狂上涨。</li></ul></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="2c26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">奖金</strong></p><p id="0422" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你是一个人，正在寻找一个可行的解决方案，但被一家公司的防火墙挡住了，你需要在本地实现这些功能。这是一个喘息的机会，至少可以节省一个星期的时间去尝试从tfhub，google-research，huggingface等的不同版本中找出相同的模型。我们将使用Huggingface，因为他们可以很容易地在不同型号之间切换，并且兼容pytorch和keras。</p><ol class=""><li id="311a" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc mc kj kk kl bi translated">进入huggingface transformers的github页面—来源—链接—<a class="ae jt" href="https://github.com/huggingface/transformers/tree/master/src/transformers" rel="noopener ugc nofollow" target="_blank">https://github . com/hugging face/transformers/tree/master/src/transformers</a></li><li id="479f" class="kd ke hi ih b ii ld im le iq lf iu lg iy lh jc mc kj kk kl bi translated">打开相关的配置、建模或标记化。在这种情况下，例如，它将是configuration_albert.py</li><li id="5891" class="kd ke hi ih b ii ld im le iq lf iu lg iy lh jc mc kj kk kl bi translated">在Config_Archive_Map列表下，你会找到aws source它下载所需的文件</li><li id="30ad" class="kd ke hi ih b ii ld im le iq lf iu lg iy lh jc mc kj kk kl bi translated">你将需要，一个config.json，spiece.model(如果句子片段被使用)或vocab.txt/vocab和tf_model.h5(对于keras)</li><li id="550f" class="kd ke hi ih b ii ld im le iq lf iu lg iy lh jc mc kj kk kl bi translated">瞧啊。下载文件并将其作为数据集添加到您的内核中。</li></ol><p id="de43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有些模型有点复杂，有些模型需要使用sentencepiece，而有些模型可能需要不同的vocab文件。为了使它更容易，我已经为他们中的一些人这样做了。你可以从以下网站下载这些数据集—</p><p id="d4c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">https://www.kaggle.com/stitch/albertlargev2huggingface<a class="ae jt" href="https://www.kaggle.com/stitch/albertlargev2huggingface" rel="noopener ugc nofollow" target="_blank"/></p><p id="d1c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jt" href="https://www.kaggle.com/stitch/bertcasecasedhuggingface2" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/stitch/bertcasecasedhuggingface2</a></p><p id="d7a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jt" href="https://www.kaggle.com/stitch/distilrobertabasehuggingface" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/stitch/distilrobertabasehuggingface</a></p><p id="d611" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jt" href="https://www.kaggle.com/stitch/robertalargehuggingface" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/stitch/robertalargehuggingface</a></p></div></div>    
</body>
</html>