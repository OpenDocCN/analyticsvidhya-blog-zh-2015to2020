<html>
<head>
<title>Reinforcement Learning Guide: Solving the Multi-Armed Bandit Problem from Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习指南:用Python从零开始解决多臂土匪问题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-guide-solving-the-multi-armed-bandit-problem-from-scratch-in-python-cecd1f093a02?source=collection_archive---------1-----------------------#2018-09-24">https://medium.com/analytics-vidhya/reinforcement-learning-guide-solving-the-multi-armed-bandit-problem-from-scratch-in-python-cecd1f093a02?source=collection_archive---------1-----------------------#2018-09-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8ad57efd5c6b832cf653c8dbb4ed16f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*81O8scTmcJKEupxN.jpg"/></div></div></figure><p id="06dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你在城镇里有最喜欢的咖啡店吗？当你想喝咖啡时，你可能会去这个地方，因为你几乎可以肯定你会得到最好的咖啡。但这意味着你错过了这个地方的同城竞争对手提供的咖啡。</p><p id="4e93" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你一个接一个地尝试所有的咖啡场所，品尝到你一生中最糟糕的咖啡的概率会相当高！但是话说回来，你有机会找到一个更好的咖啡冲泡器。但是所有这些和强化学习有什么关系呢？</p><p id="831a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">很高兴你这么问。</p><p id="2bae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们品尝咖啡实验中的困境来自于不完整的信息。换句话说，我们需要收集足够的信息来制定最佳的总体战略，然后探索新的行动。这将最终导致整体不良经历的最小化。</p><p id="9928" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">多臂土匪是这个比喻的简化形式</strong>。它被用来表示类似的问题，找到一个好的策略来解决这些问题已经帮助了很多行业。</p><p id="a3da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本文中，我们将首先了解什么是多臂土匪问题，它在现实世界中的各种用例，然后探讨一些如何解决它的策略。然后，我将向您展示如何使用点击率优化数据集在Python中解决这一挑战。</p><h1 id="ede4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">什么是多武装匪徒问题(MABP)？</h1><p id="5da0" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">强盗被定义为偷你钱的人。独臂强盗是一个简单的吃角子老虎机，你将一枚硬币投入机器，拉动杠杆，立即获得奖励。但是为什么叫土匪呢？原来所有的赌场都以这样一种方式配置这些老虎机，所有的赌徒最后都会输钱！</p><p id="c2be" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">多臂强盗是一种复杂的吃角子老虎机，其中不是一个，而是有几个杠杆，游戏者可以拉，每个杠杆给出不同的回报。对应于每个级别的奖励的概率分布是不同的，并且对于游戏者来说是未知的。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/bcbcbebee87317e39fd43e87e950732c.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/0*5mJwzv2UhMJFEfaj.png"/></div></figure><p id="3afe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">任务是在给定的一组试验后，确定拉哪个杠杆以获得最大的回报。这个问题陈述就像一个单步马尔可夫决策过程，我在<a class="ae kw" href="https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>中讨论过。选择的每一只手臂都相当于一个动作，而这个动作会立即带来回报。</p><p id="d7a7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">伯努利MABP背景下的勘探开发</strong></p><p id="204e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下表显示了具有标记为1、2、3、4和5的臂的5臂Bernoulli bandit的样本结果:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/ea64a189538a2b9d8fbb134d8b39c29f.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/0*w6ETv7slX9l0Eunp.png"/></div></figure><p id="c1a9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这被称为伯努利，因为返回的回报不是1就是0。在这个例子中，看起来3号手臂给出了最大回报，因此一个想法是继续玩这个手臂，以获得最大回报(纯粹的剥削)。</p><p id="738b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">仅根据给定样本的知识，5可能看起来像是一个不好的手臂，但我们需要记住，我们只玩过这个手臂一次，也许我们应该多玩几次(探索)才能更有信心。只有这样，我们才应该决定使用哪只手臂(剥削)。</p><h1 id="391b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">用例</h1><p id="d2dd" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">Bandit算法正被用于行业内的许多研究项目中。我在这一节中列出了它们的一些用例。</p><h1 id="f9c4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">临床试验</h1><p id="c108" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">临床试验期间患者的健康与研究的实际结果同样重要。在这里，探索相当于确定最佳治疗方法，而开发则是在试验过程中尽可能有效地治疗病人。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/3cc9bec2043856160c528824f6336b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/0*1F1IGl1qVWYo7t-J.jpg"/></div></figure><h1 id="5e33" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">网络路由</h1><p id="aaa9" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">路由是为网络(如电话网络或计算机网络(互联网))中的流量选择路径的过程。将信道分配给正确的用户，使得总吞吐量最大化，这可以用MABP来表示。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/1974218b4d4e763f238cce245896e692.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/0*Ryv6Fqt8mea6zqFB.jpg"/></div></figure><h1 id="241d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">在线广告</h1><p id="2908" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">广告活动的目标是从展示广告中获得最大收益。每当网络用户点击一个报价，广告商就获得收入。与MABP类似，在探索和利用之间也有一个权衡，探索的目标是利用点击率收集广告表现的信息，而利用则是我们坚持使用迄今为止表现最好的广告。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es la"><img src="../Images/35d9aa411f75c38b7dd0b027c5efe7c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*h_MjU_uQKfuDUXAc.jpg"/></div></figure><h1 id="f5e1" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">游戏设计</h1><p id="84a3" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">打造一款热门游戏极具挑战性。MABP可以用来测试游戏/界面的实验性变化，并利用这些变化为玩家带来积极的体验。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lb"><img src="../Images/c3da8e00293825eb8338e2e01c53e70e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y9vDN5zjW8P9Ma2g.jpg"/></div></div></figure><h1 id="78be" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">解决方案策略</h1><p id="bb6b" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在这一节中，我们将讨论一些解决多臂土匪问题的策略。但在此之前，让我们先熟悉一下从现在开始我们将使用的几个术语。</p><h1 id="41df" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">动作值函数</h1><p id="08ba" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">预期收益或预期回报也可以称为行动价值函数。它用q(a)表示，定义了在时间t时每个行为的平均回报。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/91b9ce730aeba3bba387de9901ad3e29.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/0*gqKf1hz6wKaQjYzS.jpg"/></div></figure><p id="45fb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设一个K臂强盗的奖励概率由{P1，P2，P3 …… Pk}给出。如果在时间t选择了具有臂的<em class="ld">，则Qt(a) = Pi。</em></p><p id="1515" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">问题是，我们如何决定一个给定的策略是否优于其他策略？一个直接的方法是比较我们在<em class="ld"> n </em>次试验后每种策略得到的总回报或平均回报。如果我们已经知道对于给定的土匪问题的最佳行动，那么一个有趣的方式来看这是后悔的概念。</p><h1 id="63d5" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">遗憾</h1><p id="1571" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">让我们说，我们已经知道对于给定的土匪问题的最佳手臂拉。如果我们不断重复拉动这条手臂，我们将获得一个最大的预期回报，可以用一条水平线来表示(如下图所示):</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/59d2148b117b8dbc4e8aef4636cc1c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fiDy6RraUbqhvgsq.jpg"/></div></div></figure><p id="443c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是在一个真正的问题陈述中，我们需要通过拉不同的手臂进行重复的试验，直到我们大致确定拉哪只手臂在某个时间<em class="ld"> t </em>获得最大的平均回报。<strong class="is hj">我们因学习花费的时间/轮次而招致的损失叫做后悔。</strong>换句话说，即使在学习阶段，我们也想让我的回报最大化。后悔的名字非常恰当，因为它准确地量化了你后悔没有选择最佳手臂的程度。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/d52eecf98f940dc867373d7dca4a57e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/0*jC91TlneKZC9j-hY.jpg"/></div></figure><p id="0a34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，有人可能会好奇，如果我们遵循的方法没有进行足够的探索，并最终利用了一个次优的分支，后悔会如何改变。最初可能后悔程度较低，但总的来说，我们远低于给定问题的最大可实现奖励，如下图中绿色曲线所示。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/7455f4b80d2a4f05faa4c5ebceefeced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*C5BmAe5SxWUEiK35.jpg"/></div></div></figure><p id="4a84" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据勘探的方式，有几种方法可以求解MABP。接下来，我们将讨论一些可能的解决策略。</p><h1 id="18d4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">没有探索(贪婪的方法)</h1><p id="9055" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">一种简单的方法是在每个时间步计算所有臂的q或动作值函数。从该点开始，选择一个给出最大q值的动作。每个动作的动作值将由以下函数在每个时间步长存储:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/e3eaff9bbbcddc6deeeee4304cc8a616.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/0*JEwAYklpxzeiDyIJ.jpg"/></div></figure><p id="4657" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，它在每个时间步选择最大化上述表达式的动作，由下式给出:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es li"><img src="../Images/df9f68317da0f56bd079cadb4dbed27a.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/0*Jwal6cQAtxZTNO2I.jpg"/></div></figure><p id="74dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，为了在每个时间t评估这个表达式，我们需要计算整个奖励的历史。我们可以通过运行求和来避免这种情况。因此，在每个时间t，每个动作的q值可以使用奖励来计算:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/80a9f95881dfda608c3c9ebb38079299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/0*hAdkoA-PvJKP3Yee.jpg"/></div></figure><p id="e64d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里的问题是，这种方法只是利用，因为它总是选择相同的行动，而不担心探索可能会带来更好回报的其他行动。为了找到最佳的手臂，一些探索是必要的，否则我们可能会永远拉着次优的手臂。</p><h1 id="e97c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">ε贪婪方法</h1><p id="2fbd" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">一个可能的解决方案是现在，然后我们可以探索新的行动，以确保我们不会错过更好的选择。有了ε概率，我们会选择一个随机动作(探索)，选择一个qt(a)最大的动作，概率为1-ε。</p><p id="edae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ld">以概率1-ε—我们选择最大值的动作(argmaxa Qt(a)) </em> </strong></p><p id="fd7a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ld">概率为ε—我们从一组所有动作A </em> </strong>中随机选择一个动作</p><p id="b6b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，如果我们对两个操作(A和B)有问题，则ε贪婪算法的工作方式如下所示:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/bb502d63f631abcdea4edfc9fb47a9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NXW7i--z8Al_FRDm.jpg"/></div></div></figure><p id="7792" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这比贪婪的方法好得多，因为我们在这里有一个探索的元素。然而，如果两个动作的q值相差很小，那么即使这个算法也只会选择概率比其他动作高的那个动作。</p><h1 id="7804" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Softmax探索</h1><p id="e4d0" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">解决方案是使选择动作的概率与q成比例。这可以使用softmax函数来完成，其中在每一步选择动作<em class="ld"> a </em>的概率由以下表达式给出:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/9eb35020d287bb3f0470c6e7ff44bc92.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/0*sXzWvFZdl64DH-jX.jpg"/></div></figure><h1 id="e50b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">腐朽的贪婪的ε</h1><p id="f226" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">ε的值对于决定ε贪婪算法对于给定问题的效果非常重要。我们可以通过保持ε依赖于时间来避免设置这个值。例如，ε可以保持等于<em class="ld"> 1/log(t+0.00001) </em>。它将不断减少久而久之，直到我们开始探索越来越少，因为我们对最佳行动或手臂越来越有信心。</p><p id="a852" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随机选择动作的问题是，在足够的时间步长后，即使我们知道某个arm是坏的，该算法也会以概率<em class="ld">ε/n</em>继续选择。本质上，我们正在探索一个听起来不是很有效的坏行为。解决这一问题的办法可能是支持开发潜力大的武器，以获得最佳价值。</p><h1 id="2afa" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">置信上限</h1><p id="d172" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">置信上限(UCB)是多臂土匪问题最广泛使用的解决方法。这种算法基于面对不确定性时的乐观原则。</p><p id="a8c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">换句话说，我们对一条手臂越不确定，探索这条手臂就变得越重要。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/1988dcf663cccc0dfde60f90045b34af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sEBlQ1t-365zSfrH.jpg"/></div></div></figure><ul class=""><li id="9a14" class="ln lo hi is b it iu ix iy jb lp jf lq jj lr jn ls lt lu lv bi translated">经过几次试验后，3个不同臂a1、a2和a3的作用值函数分布如上图所示。该分布显示a1的行动值具有最高的方差，因此具有最大的不确定性。</li><li id="b0f6" class="ln lo hi is b it lw ix lx jb ly jf lz jj ma jn ls lt lu lv bi translated">UCB说，我们应该选择手臂a1，并接受奖励，让我们对它的行动价值不那么确定。对于下一个试验/时间步，如果我们仍然对a1非常不确定，我们将再次选择它，直到不确定性降低到阈值以下。</li></ul><p id="20f8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这样做的直观原因是，当以这种方式乐观行事时，会发生以下两种情况之一:</p><ul class=""><li id="78df" class="ln lo hi is b it iu ix iy jb lp jf lq jj lr jn ls lt lu lv bi translated">乐观是合理的，我们会得到积极的回报，这是最终的目标</li><li id="aa55" class="ln lo hi is b it lw ix lx jb ly jf lz jj ma jn ls lt lu lv bi translated">这种乐观是没有道理的。在这种情况下，我们玩一手牌，我们认为这手牌可能会给我们很大的奖励，但实际上并没有。如果这种情况发生得足够频繁，那么我们将会知道这种行为的真正收益是什么，而不会在未来选择它。</li></ul><p id="ca60" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">UCB实际上是一个算法家族。在这里，我们将讨论UCB1。</p><p id="0b32" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">UC B1涉及的步骤:</strong></p><ul class=""><li id="bb72" class="ln lo hi is b it iu ix iy jb lp jf lq jj lr jn ls lt lu lv bi translated"><em class="ld">将K个动作中的每个动作玩一次，在</em>给出每个动作对应的平均奖励的初始值</li><li id="f38d" class="ln lo hi is b it lw ix lx jb ly jf lz jj ma jn ls lt lu lv bi translated"><em class="ld">对于每一轮t = K: </em></li><li id="69c7" class="ln lo hi is b it lw ix lx jb ly jf lz jj ma jn ls lt lu lv bi translated"><em class="ld">让Nt(a)表示到目前为止动作a被播放的次数</em></li><li id="bc46" class="ln lo hi is b it lw ix lx jb ly jf lz jj ma jn ls lt lu lv bi translated"><em class="ld">扮演最大化以下表达式的动作:</em></li></ul><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/1e15b66ecda90d8ed4cdf7a5bfba21b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/0*7FdAso_HGBOI_A_c.jpg"/></div></figure><ul class=""><li id="4651" class="ln lo hi is b it iu ix iy jb lp jf lq jj lr jn ls lt lu lv bi translated"><em class="ld">观察奖励并更新所选行动的平均奖励或预期回报</em></li></ul><p id="c675" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将不深入研究UCB的数学证明。然而，理解与我们选择的动作相对应的表达式是很重要的。记住，在随机探索中，我们只有Q(a)最大化，而这里我们有两个项。第一个是动作值函数，第二个是置信项。</p><ul class=""><li id="4f01" class="ln lo hi is b it iu ix iy jb lp jf lq jj lr jn ls lt lu lv bi translated">每次选择<em class="ld"> a </em>时，不确定性大概会减少:Nt(a)增加，并且，当它出现在分母中时，不确定性项减少。</li></ul><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/9c4c7c2c231282b00e11ee7b1c7a4e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/0*-1kVuFuFu42m5Qv3.jpg"/></div></figure><ul class=""><li id="2ab9" class="ln lo hi is b it iu ix iy jb lp jf lq jj lr jn ls lt lu lv bi translated">另一方面，每次选择除了<em class="ld"> a </em>之外的动作时，t增加，但是Nt(a)不增加；因为t出现在分子中，不确定性估计增加。</li></ul><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es md"><img src="../Images/b0b8d6d403f33c938b6bdca015f86e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/0*RtPb60hfIZWz__M0.jpg"/></div></figure><ul class=""><li id="3103" class="ln lo hi is b it iu ix iy jb lp jf lq jj lr jn ls lt lu lv bi translated">自然对数的使用意味着随着时间的推移，增量变小；最终将选择所有动作，但是具有较低估计值的动作，或者已经被频繁选择的动作，将随着时间的推移以降低的频率被选择。</li><li id="f2c0" class="ln lo hi is b it lw ix lx jb ly jf lz jj ma jn ls lt lu lv bi translated">这将最终导致最优行动最终被反复选择。</li></ul><p id="ad7f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">后悔对比</strong></p><p id="34c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本文给出的所有算法中，只有UCB算法提供了一种后悔随着log(t)增加的策略，而在其他算法中，我们得到了不同斜率的线性后悔。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/da30df2245ff532ee4422d4d92a293ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6vBgr_jN0Qpk8RvR.jpg"/></div></div></figure><h1 id="2617" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">非平稳Bandit问题</h1><p id="a9c8" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我们在这里做的一个重要假设是，我们与同一个bandit一起工作，并且在每个时间步中被采样的奖励分布保持不变。这就是所谓的稳态问题。再用一个例子来解释，比如说你每抛一次硬币就获得1的奖励，结果是正面。假设在1000次硬币投掷后，由于磨损和撕裂，硬币变得有偏差，那么这将成为一个不稳定的问题。</p><p id="1b31" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了解决一个非平稳问题，更新的样本将是重要的，因此我们可以使用一个常数贴现因子α，我们可以将更新方程改写如下:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/ea45aa5e5c6e324e2a7c111465778dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/0*ufPwvu7dnBP-KdKS.jpg"/></div></figure><p id="4630" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意，我们在这里用常数α代替了Nt(at ),这确保了最近的样本被赋予更高的权重，并且增量更多地由这些最近的样本决定。还有其他技术，提供不同的解决方案，强盗与非固定奖励。你可以在这篇<a class="ae kw" href="https://arxiv.org/abs/0805.3415" rel="noopener ugc nofollow" target="_blank">论文</a>中读到更多关于他们的内容。</p><h1 id="63f4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">从头开始为广告中心优化实施Python</h1><p id="842b" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">如用例部分所述，MABP在在线广告领域有很多应用。</p><p id="c758" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设一家广告公司在一个网页上投放了10个不同的广告，目标是相似的人群。我们有用户点击广告的结果<a class="ae kw" href="https://drive.google.com/open?id=1whkIInL4FKeHg2IfdcbT1j18L26fg9aF" rel="noopener ugc nofollow" target="_blank">这里</a>。每个列索引代表一个不同的广告。如果广告被用户点击，我们得到1，如果没有被点击，我们得到0。原始数据集中的一个示例如下所示:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/4d1b2fd99aa74757eeb55166c4caa423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/0*UnzdxHrNHMy2qBnv.jpg"/></div></figure><p id="601f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一个模拟数据集，它将Ad #5作为给出最大奖励的数据集。</p><p id="8309" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们将尝试一种随机选择技术，我们随机选择任意一个广告并展示给用户。如果用户点击广告，我们得到报酬，如果没有，就没有利润。</p><pre class="ks kt ku kv fd mh mi mj mk aw ml bi"><span id="a0b9" class="mm jp hi mi b fi mn mo l mp mq"># Random Selection<br/><br/># Importing the libraries<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/><br/># Importing the dataset<br/>dataset = pd.read_csv('Ads_Optimisation.csv')<br/><br/># Implementing Random Selection<br/>import random<br/>N = 10000<br/>d = 10<br/>ads_selected = []<br/>total_reward = 0<br/>for n in range(0, N):<br/>    ad = random.randrange(d)<br/>    ads_selected.append(ad)<br/>    reward = dataset.values[n, ad]<br/>    total_reward = total_reward + reward</span></pre><p id="a912" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随机选择算法的总奖励是1170。由于这个算法没有学习任何东西，它不会聪明地选择任何给出最大回报的广告。因此，即使我们查看过去的1000次试验，也无法找到最佳广告。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/b30708c13d5272834826b6ed530be17e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CxYJEsFpwXlE3QFd.jpg"/></div></div></figure><p id="eb36" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们尝试置信上限算法来做同样的事情:</p><pre class="ks kt ku kv fd mh mi mj mk aw ml bi"><span id="cef8" class="mm jp hi mi b fi mn mo l mp mq"># Implementing UCB<br/>import math<br/>N = 10000<br/>d = 10<br/>ads_selected = []<br/>numbers_of_selections = [0] * d<br/>sums_of_reward = [0] * d<br/>total_reward = 0<br/><br/>for n in range(0, N):<br/>    ad = 0<br/>    max_upper_bound = 0<br/>    for i in range(0, d):<br/>        if (numbers_of_selections[i] &gt; 0):<br/>            average_reward = sums_of_reward[i] / numbers_of_selections[i]<br/>            delta_i = math.sqrt(2 * math.log(n+1) / numbers_of_selections[i])<br/>            upper_bound = average_reward + delta_i<br/>        else:<br/>            upper_bound = 1e400<br/>        if upper_bound &gt; max_upper_bound:<br/>            max_upper_bound = upper_bound<br/>            ad = i<br/>    ads_selected.append(ad)<br/>    numbers_of_selections[ad] += 1<br/>    reward = dataset.values[n, ad]<br/>    sums_of_reward[ad] += reward<br/>    total_reward += reward</span></pre><p id="ead3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">UCB的<em class="ld">总奖励</em>是2125。显然，这比随机选择好得多，而且确实是一种聪明的探索技术，可以显著改善我们解决MABP的策略。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/54484b8568cbc21e45d00e0e8dc19c9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1XslKWweyaMwE3Wn.jpg"/></div></div></figure><p id="5d31" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">仅仅经过1500次试验，UCB就已经倾向于广告#5(索引4 ),它恰好是最佳广告，并且对于给定的问题获得了最大的回报。</p><h1 id="5912" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结束注释</h1><p id="4a18" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">作为一个活跃的研究领域，MABP将渗透到行业的其他各个领域。这些算法如此简单而强大，以至于越来越多的小型科技公司都在使用它们，因为它们所需的计算资源通常很低。</p><p id="e335" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">展望未来，还有其他基于概率模型的技术，如巴拉拉曼教授在这个令人惊叹的<a class="ae kw" href="https://www.youtube.com/watch?v=H2OWTxdauqA" rel="noopener ugc nofollow" target="_blank">视频</a>中解释的汤普森抽样。</p><p id="3191" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你也可以在班加罗尔举行的DataHack Summit 2018上参加他关于强化学习的备受期待且极其有用的演讲！更多详情请访问<a class="ae kw" href="https://www.analyticsvidhya.com/datahack-summit-2018/" rel="noopener ugc nofollow" target="_blank">https://www.analyticsvidhya.com/datahack-summit-2018/</a>。</p><p id="abb2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ld">原载于2018年9月24日</em><a class="ae kw" href="https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/" rel="noopener ugc nofollow" target="_blank"><em class="ld">www.analyticsvidhya.com</em></a><em class="ld">。</em></p></div></div>    
</body>
</html>