<html>
<head>
<title>Transforming Spark Datasets using Scala transformation functions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Scala转换函数转换Spark数据集</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/transforming-spark-datasets-using-scala-transformation-functions-36007a7ac57f?source=collection_archive---------8-----------------------#2019-10-01">https://medium.com/analytics-vidhya/transforming-spark-datasets-using-scala-transformation-functions-36007a7ac57f?source=collection_archive---------8-----------------------#2019-10-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e9b122a5c579bd11e440842eb16bc2f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OvMy342xIwuiII3aS6b6yg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">转换Spark数据集</figcaption></figure><p id="e1b4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在读写拼花、csv或xml文件时，我很少选择Spark作为ETL工具，因为它很容易使用。</p><p id="fdc1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">阅读这些文件格式中的任何一种都像一行spark代码一样简单(当然是在您确保拥有所需的依赖项之后)</p><h2 id="8db2" class="js jt hi bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">目的</h2><p id="aafc" class="pw-post-body-paragraph iu iv hi iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr hb bi translated">大多数在线可获得的转换数据集的参考资料都指向调用<code class="du ks kt ku kv b">createOrReplaceTempView()</code>并将<code class="du ks kt ku kv b">Dataset/Dataframe</code>注册为一个表，然后对其执行<code class="du ks kt ku kv b">SQL</code>操作。</p><p id="e585" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">虽然这对于大多数用例来说可能没问题，但有时使用Spark的Scala转换函数会感觉更自然，尤其是如果您已经用<code class="du ks kt ku kv b">Scala</code>或<code class="du ks kt ku kv b">ReactiveX</code>甚至<code class="du ks kt ku kv b">Java 8</code>编写了一些预先存在的转换逻辑。</p><p id="1f8b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果这听起来像是你想做的事情，那么请继续读下去。</p><h2 id="0e98" class="js jt hi bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">入门指南</h2><p id="a584" class="pw-post-body-paragraph iu iv hi iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr hb bi translated">假设我们想要使用数据集进行以下转换。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/61c09b04bf53c07b78bec8a98292013b.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*s3NSjJ28uxS4Efkh8tKJXg.png"/></div></figure><h2 id="d314" class="js jt hi bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">领域对象和转换函数</h2><p id="05f0" class="pw-post-body-paragraph iu iv hi iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr hb bi translated">我们假设我们有下面的域对象和转换函数，它将一个<code class="du ks kt ku kv b">FlightSchedule</code>对象转换成一个<code class="du ks kt ku kv b">FlightInfo</code>对象。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lb"><img src="../Images/7f5e8e877e61b5a1fae56e4ec4d26b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1T9eghS7kcdqVtTyKx4Gkg.png"/></div></div></figure><h2 id="6869" class="js jt hi bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">创建spark会话并读取输入数据集</h2><p id="f547" class="pw-post-body-paragraph iu iv hi iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr hb bi translated">为了简洁，创建输入<code class="du ks kt ku kv b">Dataset</code>很简单。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/d4435b64d556665813b598398008b3f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rlAHfXjKus9j-upE16sTVA.png"/></div></div></figure><h2 id="596b" class="js jt hi bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">定义编码器和火花变换</h2><p id="246e" class="pw-post-body-paragraph iu iv hi iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr hb bi translated">这就是事情开始变得有趣的地方。为了将一个<code class="du ks kt ku kv b">Dataset[FlightSchedule]</code>转化为一个<code class="du ks kt ku kv b">Dataset[FlightInfo]</code>，Spark需要知道如何<em class="ld">“编码”</em>你的<code class="du ks kt ku kv b">case class</code>。省略这一步将会导致下面可怕的编译时错误。</p><pre class="kx ky kz la fd le kv lf lg aw lh bi"><span id="13db" class="js jt hi kv b fi li lj l lk ll">Error:(34, 18) Unable to find encoder for type stored in a Dataset.  Primitive types (Int, String, etc) and Product types (case classes) are supported by importing spark.implicits._</span><span id="3fbd" class="js jt hi kv b fi lm lj l lk ll">Support for serializing other types will be added in future releases.<br/>schedules.map(s =&gt; existingTransformationFunction(s))</span></pre><p id="3e13" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><code class="du ks kt ku kv b">Encoders[T]</code>用于将任何类型<code class="du ks kt ku kv b">T</code>的JVM对象或原语与Spark SQL的<a class="ae ln" href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-InternalRow.html" rel="noopener ugc nofollow" target="_blank"> InternalRow </a>表示相互转换。由于<code class="du ks kt ku kv b">Dataset.map()</code>方法要求将编码器作为隐式参数传递，我们将定义一个<code class="du ks kt ku kv b">implicit</code>变量。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/d3438fba16c9ed4acf78243a19c07efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_3G18ZpKGRCO2rcsVxsWtw.png"/></div></div></figure><h2 id="58f8" class="js jt hi bd ju jv jw jx jy jz ka kb kc jf kd ke kf jj kg kh ki jn kj kk kl km bi translated">转换数据集</h2><p id="04c8" class="pw-post-body-paragraph iu iv hi iw b ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn kr jp jq jr hb bi translated">现在剩下唯一要做的事情就是调用输入<code class="du ks kt ku kv b">Dataset</code>上的<code class="du ks kt ku kv b">transform</code>方法。我将在这里包含整个代码以及对<code class="du ks kt ku kv b">show()</code>的调用，这样我们就可以看到我们的结果。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/ed108f9ed8dd925877ab78bb2cee3415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cAnroRGNw7H0owmck3-iVw.png"/></div></div></figure><h1 id="d014" class="lq jt hi bd ju lr ls lt jy lu lv lw kc lx ly lz kf ma mb mc ki md me mf kl mg bi translated">参考</h1><ol class=""><li id="d7f1" class="mh mi hi iw b ix kn jb ko jf mj jj mk jn ml jr mm mn mo mp bi translated"><a class="ae ln" href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Encoder.html" rel="noopener ugc nofollow" target="_blank"> Spark SQL编码器</a></li></ol><p id="4f86" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="ld">也贴在:</em><a class="ae ln" href="https://olivermascarenhas.com/2019-09-25-how-to-transform-a-spark-dataset/" rel="noopener ugc nofollow" target="_blank"><em class="ld">https://olivermascarenhas . com/2019-09-25-how-to-transform-a-spark-dataset/</em></a></p></div></div>    
</body>
</html>