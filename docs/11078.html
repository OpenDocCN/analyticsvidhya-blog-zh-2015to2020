<html>
<head>
<title>Decision Trees made simple, Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树变得简单，第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-trees-made-simple-part-i-57673d898705?source=collection_archive---------24-----------------------#2020-11-16">https://medium.com/analytics-vidhya/decision-trees-made-simple-part-i-57673d898705?source=collection_archive---------24-----------------------#2020-11-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ab3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">机器学习基础的第一个条目</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/b84e4e133c0fdce1945208f952eb5567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WQ-gju_7P0HMPHlY7YvhgQ.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">Arnaud Mesureur 在<a class="ae ju" href="https://unsplash.com/@tbzr" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的图片</figcaption></figure><p id="d0c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是最常见和最广泛使用的机器学习算法之一，对于预测分类(非数字)变量特别有用，使它们成为预测分析的强大工具。</p><p id="57ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树不依赖于决策表，即使是小的玩具数据集也可能运行得难以管理，而是依赖于信息熵的概念，并通过不断选择我们可以获得最多信息的路径来构建其预测路径。</p><h2 id="8944" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">这意味着什么呢？</h2><p id="dfea" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">决策树背后的思想是逐步选择最能区分具有不同分类值的观察值的描述性特征。为了实现这一点，通过分析每组观察值的大小和概率分布，通过估计每组实例相对于我们试图预测的目标变量的纯度，来估计变量的区分能力。这种纯度的衡量标准是香农的熵模型。</p><h2 id="9796" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">香农的什么？</h2><p id="5aff" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">香农熵模型是一个集合中元素纯度的度量。在这种情况下,“纯度”指的是数据集中元素的同质性，理解为当你从数据集中随机选择时，猜测结果的不确定性。考虑一组卡片，如果你能看到所有的卡片，而且它们都是一样的，那么选择其中一张的不确定性是零，因此熵也是零。然而，如果所有的卡片都不同，并且其中一些被覆盖，那么你将有非常高的不确定性，导致非常高的熵。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kv"><img src="../Images/29515b9bf962f7bb7f6573368aa3f0a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*AyoFIxGGEAblbLGalXQo4w.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">卡片集中的熵(来源:预测分析的机器学习基础)</figcaption></figure><p id="5957" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常，具有大概率的结果将映射到较低的熵值，而具有小概率的结果将映射到较高的熵值。</p><p id="3e32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">香农的熵模型，只不过是当我们从一个集合中随机选择时，每个可能结果的概率的对数的加权和:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kw"><img src="../Images/d6972b728a3a18fd8dbbc6bce2f1ba53.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*wX6KIP_CGoDFI4I8gGUzRw.png"/></div></figure><p id="0338" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中 P(t=i)是随机选择元素 t 的结果是类型 I 的概率，l 是集合中不同类型事物的数量，s 是任意对数底数。对于熵，我们将始终使用 2 作为基数，这意味着我们用<strong class="ih hj">位</strong>来度量熵</p><h2 id="9ac3" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated"><strong class="ak">信息增益</strong></h2><p id="4c1a" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">根据 Shannon 熵模型，我们可以构建一系列测试，将训练集中的数据分成相对于目标特征值逐渐更纯的集合，然后我们可以通过将相同的测试序列应用于查询并使用目标特征值对其进行标记来标记查询。因此，该机器学习模型包括两个主要步骤:</p><ol class=""><li id="fc3e" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated"><strong class="ih hj">构建决策树</strong>:我们将训练数据集分成一系列规则，这些规则将逐渐创建更纯(更低熵)的集合，直到我们从原始训练数据中获得最纯、最同质的集合</li><li id="4761" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated"><strong class="ih hj">然后在树中运行测试查询:</strong>根据查询的描述性特征来评估查询，并且算法基于查询的描述性特征来决定它属于哪个目标特征值。</li></ol><p id="3a23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">算法决定如何在决策树中前进的方式是通过使用称为信息增益的概念。信息增益是对一组实例的总体熵减少的度量，这是通过测试我们正在考虑的查询的一个特定描述性特征来实现的。为了计算信息增益，我们:</p><ol class=""><li id="35d2" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated">我们计算整个数据集的熵，记住数据集的熵定义如下:</li></ol><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ll"><img src="../Images/d51814b2a998442c877f123f563b98d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*S7vnNGuqNXGpl1-qM6VryQ.png"/></div></figure><p id="05b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.对于每个描述性特征，我们使用每个特征的值域创建分区数据集，然后计算每个分区的熵值。这为我们提供了在使用描述性特征分割实例后，将实例组织到纯集合中所需的信息。</p><p id="8a21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.从集合的总熵中减去剩余的熵值，获得信息增益(IG)</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lm"><img src="../Images/997b2c491979661033e919558181961a.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*Km1NE125dqLAAquAUFvOSw.png"/></div></figure><p id="8fbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们使用描述性特征<em class="jd"> d </em>对数据集<em class="jd"> D </em>进行分区时，会为<em class="jd"> d 的每个值创建一个新分区，rem(d，D) </em>是我们测试了每个单独的<em class="jd"> d </em>的熵之后剩余的熵，定义为每个单独分区的熵的加权和:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ln"><img src="../Images/900fdc2749f02e5407b01d80ef98c191.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*GUp1LFQ0pofY7RYlVuxkJw.png"/></div></figure><h2 id="3836" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">ID3(迭代二分法 3)</h2><p id="a0c8" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">实现基于熵的决策树的最常见算法是迭代二分法 3 (ID3)。ID3 以递归的深度优先方式构建决策树，从根节点开始向下到叶节点。该算法首先选择要测试的最佳描述性特征(计算训练数据集中描述性特征的信息增益)。</p><p id="6344" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在选择了特征之后，使用该特征分割训练数据集，并且向树中添加节点，为每个可能的测试结果创建一个分区，该分区包含每个结果的训练实例。然后，重复选择最佳描述性特征的过程，直到分区中的所有实例都具有相同的目标级别，此时，创建叶节点并用该级别进行标记。</p><p id="c81d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管算法背后的思想相对简单，但所有关于特征级别和描述性特征的讨论都会令人困惑，这就是为什么实现算法对于真正理解它非常重要，在下一篇文章中，我们将介绍一个简单的 ID3 和一个玩具数据集的实现，目前，主要的要点是:</p><ol class=""><li id="d207" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated">决策树最适合用于有限且可测量值范围的分类数据。其他算法更适合数值型连续数据类型。</li><li id="1e6a" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated">熵是对数据集中纯度水平的一种度量，换句话说，在数据集中随机选择一个观察值，我们对结果有多大把握。</li><li id="c9f7" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated">信息增益是对较大数据集的给定分区的总熵减少的度量</li><li id="57cc" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated">ID3 利用信息增益来创建决策树，该决策树基于从给定分区产生最高信息增益的最具描述性的特征</li></ol></div></div>    
</body>
</html>