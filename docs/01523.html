<html>
<head>
<title>Introduction to PySpark — Take your First Steps into Big Data Analytics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark简介—迈出大数据分析的第一步</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-pyspark-take-your-first-steps-into-big-data-analytics-b98cf7dce8b3?source=collection_archive---------10-----------------------#2019-10-28">https://medium.com/analytics-vidhya/introduction-to-pyspark-take-your-first-steps-into-big-data-analytics-b98cf7dce8b3?source=collection_archive---------10-----------------------#2019-10-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="51bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们正以前所未有的速度生成数据。老实说，我跟不上世界各地庞大的数据量！我敢肯定，您已经看到了对正在产生的数据量的估计——麦肯锡、Gartner、IBM等。都提供了自己的数字。</p><p id="0190" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有一些令人难以置信的数字供您参考——一天之内发送了超过5亿条推文、900亿封电子邮件、6500万条WhatsApp消息！仅脸书在24小时内就产生4pb的数据。太不可思议了！</p><p id="d50b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，这本身也带来了挑战。数据科学团队如何获取如此大量的数据？你如何处理它并<a class="ae jd" href="https://courses.analyticsvidhya.com/courses/applied-machine-learning-beginner-to-professional?utm_source=blog&amp;utm_medium=pyspark-for-beginners-first-steps-big-data-analysis" rel="noopener ugc nofollow" target="_blank">从中建立机器学习模型</a>？如果你是数据科学家或数据工程师，这些都是令人兴奋的问题。</p><blockquote class="je jf jg"><p id="cda8" class="if ig jh ih b ii ij ik il im in io ip ji ir is it jj iv iw ix jk iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi">这就是火花出现的地方。Spark是用Scala编写的，它提供了与Scala、JAVA、Python和r协同工作的API。</em> </strong></p></blockquote><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/3011bf7c287a6db93acd3ea24e7ae597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IXTsrnKQZiaTfMOh.png"/></div></div></figure><p id="ad80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">处理大数据的一种传统方式是使用Hadoop等分布式框架，但这些框架需要在硬盘上进行大量读写操作，这在时间和速度方面都非常昂贵。计算能力是一个重大障碍。</p><p id="a1b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PySpark以一种高效且易于理解的方式处理这个问题。所以在这篇文章中，我们将开始了解这一切。我们将了解什么是Spark，如何在您的机器上安装它，然后我们将深入了解不同的Spark组件。这里也有一大堆代码，让我们开心一下吧！</p><p id="5b8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jh">这里有一个</em> <a class="ae jd" href="https://www.analyticsvidhya.com/big-data/?utm_source=blog&amp;utm_medium=pyspark-for-beginners-first-steps-big-data-analysis" rel="noopener ugc nofollow" target="_blank"> <em class="jh">大数据世界的快速介绍</em> </a> <em class="jh">，以防你需要复习。请记住，这些数字已经远远超出了那里显示的数字——而且距离我们发表那篇文章只有3年时间！</em></p><h1 id="d141" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">什么是火花？</h1><blockquote class="je jf jg"><p id="c75b" class="if ig jh ih b ii ij ik il im in io ip ji ir is it jj iv iw ix jk iz ja jb jc hb bi translated"><em class="hi"> Apache Spark是一个开源的分布式集群计算框架，用于快速处理、查询和分析大数据。</em></p></blockquote><p id="aac1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是当今企业中最有效的数据处理框架。诚然，Spark的成本很高，因为它需要大量RAM来进行内存计算，但它仍然是数据科学家和大数据工程师的最爱。您将在本文中看到为什么会这样。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es kv"><img src="../Images/264b2386e1e30ab6bf519786b5007911.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*Pq59Bsw2Z9zUvipc.png"/></div></div></figure><p id="eaa7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常依赖类似Map Reduce的框架的组织现在正在转向Apache Spark框架。Spark不仅执行内存计算，而且比Hadoop等Map Reduce框架快100倍。Spark在数据科学家中大受欢迎，因为它在内存中分发和缓存数据，并帮助他们在大数据上优化<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?utm_source=blog&amp;utm_medium=pyspark-for-beginners-first-steps-big-data-analysis" rel="noopener ugc nofollow" target="_blank">机器学习算法</a>。</p><p id="5189" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我推荐查看Spark的官方页面<a class="ae jd" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">这里</a>了解更多细节。它有大量的文档，是关于Spark的很好的参考指南。</p><h1 id="f04f" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">在您的机器上安装Apache Spark</h1><h2 id="9176" class="kw jy hi bd jz kx ky kz kd la lb lc kh iq ld le kl iu lf lg kp iy lh li kt lj bi translated">1.下载Apache Spark</h2><p id="44f2" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">安装Spark的一个简单方法是通过pip。但是根据Spark的官方文档，这不是推荐的方法，因为Spark的Python包并不打算取代所有其他用例。</p><p id="69ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实现基本功能时，您很可能会遇到许多错误。它仅适用于与现有集群交互(无论是独立的Spark、YARN还是Mesos)。</p><p id="0b9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，第一步是从<a class="ae jd" href="http://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">这里</a>下载最新版本的Apache Spark。解压缩并移动压缩文件:</p><pre class="jm jn jo jp fd lp lq lr ls aw lt bi"><span id="8b9a" class="kw jy hi lq b fi lu lv l lw lx">tar xzvf spark-2.4.4-bin-hadoop2.7.tgz mv spark-2.4.4-bin-hadoop2.7 spark sudo mv spark/ /usr/lib/</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es ly"><img src="../Images/9245bba32e817f579810a970c0ef9b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0Kq6HFKKN4_v9iQ2.png"/></div></div></figure><h2 id="3bea" class="kw jy hi bd jz kx ky kz kd la lb lc kh iq ld le kl iu lf lg kp iy lh li kt lj bi translated">2.安装JAVA</h2><p id="abaf" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">确保您的系统中安装了JAVA。我强烈推荐JAVA 8，因为众所周知，Spark版本2在JAVA 9及更高版本中存在问题:</p><pre class="jm jn jo jp fd lp lq lr ls aw lt bi"><span id="103c" class="kw jy hi lq b fi lu lv l lw lx">sudo apt install default-jre sudo apt install openjdk-8-jdk</span></pre><h2 id="1a46" class="kw jy hi bd jz kx ky kz kd la lb lc kh iq ld le kl iu lf lg kp iy lh li kt lj bi translated">3.安装Scala构建工具(SBT)</h2><p id="b8cb" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">当您在处理一个包含很少源代码文件的小项目时，手动编译它们会更容易。但是如果您正在处理一个有数百个源代码文件的更大的项目呢？在这种情况下，您需要使用构建工具。</p><blockquote class="je jf jg"><p id="84fc" class="if ig jh ih b ii ij ik il im in io ip ji ir is it jj iv iw ix jk iz ja jb jc hb bi translated">SBT，Scala Build Tool的缩写，管理你的Spark项目和你在代码中使用的库的依赖关系。</p></blockquote><p id="62b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">请记住，如果您使用的是PySpark，就不需要安装这个。但是如果你使用JAVA或者Scala来构建Spark应用，那么你需要在你的机器上安装SBT。运行以下命令安装SBT:</strong></p><pre class="jm jn jo jp fd lp lq lr ls aw lt bi"><span id="201b" class="kw jy hi lq b fi lu lv l lw lx">echo "deb <a class="ae jd" href="https://dl.bintray.com/sbt/debian" rel="noopener ugc nofollow" target="_blank">https://dl.bintray.com/sbt/debian</a> /" | sudo tee -a /etc/apt/sources.list.d/sbt.list curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&amp;search=0x2EE0EA64E40A89B84B2DF73499E82A75642AC823" | sudo apt-key add sudo apt-get update sudo apt-get install sbt</span></pre><h2 id="1978" class="kw jy hi bd jz kx ky kz kd la lb lc kh iq ld le kl iu lf lg kp iy lh li kt lj bi translated">4.配置火花</h2><p id="3f61" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">接下来，打开Spark的配置目录，并复制默认的Spark环境模板。这已经作为<strong class="ih hj"> spark-env.sh.template出现在那里。</strong>使用编辑器打开它:</p><pre class="jm jn jo jp fd lp lq lr ls aw lt bi"><span id="396a" class="kw jy hi lq b fi lu lv l lw lx">cd /usr/lib/spark/conf/ <br/>cp spark-env.sh.template spark-env.sh <br/>sudo gedit spark-env.sh</span></pre><p id="15d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，在文件<strong class="ih hj"> spark-env.sh </strong>中，添加<strong class="ih hj"> JAVA_HOME </strong>路径，并将内存限制分配给<strong class="ih hj"> SPARK_WORKER_MEMORY </strong>。在这里，我将其分配为4GB:</p><pre class="jm jn jo jp fd lp lq lr ls aw lt bi"><span id="d4b7" class="kw jy hi lq b fi lu lv l lw lx">## add variables <br/>JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 <br/>SPARK_WORKER_MEMORY=4g</span></pre><h2 id="f200" class="kw jy hi bd jz kx ky kz kd la lb lc kh iq ld le kl iu lf lg kp iy lh li kt lj bi translated">5.设置火花环境变量</h2><p id="ecb8" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">使用下面的命令打开并编辑<strong class="ih hj"> bashrc </strong>文件。这个<strong class="ih hj"> bashrc </strong>文件是一个脚本，每当您启动一个新的终端会话时都会执行:</p><pre class="jm jn jo jp fd lp lq lr ls aw lt bi"><span id="b399" class="kw jy hi lq b fi lu lv l lw lx">## open bashrc file <br/>sudo gedit ~/bashrc</span></pre><p id="5d5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在文件中添加以下环境变量:</p><pre class="jm jn jo jp fd lp lq lr ls aw lt bi"><span id="0d19" class="kw jy hi lq b fi lu lv l lw lx">## add following variables <br/>export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 <br/>export SBT_HOME=/usr/share/sbt/bin/sbt-launch.jar <br/>export SPARK_HOME=/usr/lib/spark <br/>export PATH=$PATH:$JAVA_HOME/bin <br/>export PATH=$PATH:$SBT_HOME/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin export PYSPARK_DRIVER_PYTHON=jupyter <br/>export PYSPARK_DRIVER_PYTHON_OPTS='notebook' <br/>export PYSPARK_PYTHON=python3 <br/>export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH</span></pre><p id="2b44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，找到<strong class="ih hj"> bashrc </strong>文件。这将使用更新的脚本重新启动终端会话:</p><pre class="jm jn jo jp fd lp lq lr ls aw lt bi"><span id="3ccd" class="kw jy hi lq b fi lu lv l lw lx">## source bashrc file <br/>source ~/.bashrc</span></pre><p id="d83a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，在终端中键入<strong class="ih hj"> pyspark </strong>，它将在默认浏览器中打开<strong class="ih hj"> Jupyter </strong>，并且spark上下文(它是Spark服务的入口点)将自动初始化为变量名<strong class="ih hj"> sc </strong>:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es lz"><img src="../Images/be4a462c748f938c241a17694957948f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ztFILY9j3sPycExU.png"/></div></div></figure><h1 id="b563" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">什么是Spark应用？</h1><p id="19f8" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">Spark应用程序是Spark上下文的一个实例。它由一个驱动程序进程和一组执行程序进程组成。</p><p id="cf5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">驱动进程负责维护关于Spark应用程序的信息，响应代码，跨执行器分配和调度工作。</strong>驱动程序进程是绝对必要的——它是Spark应用程序的核心，在应用程序的生命周期中维护所有相关信息</p><p id="3a56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">执行者负责实际执行司机分配给他们的工作。</strong>所以，每个遗嘱执行人只负责两件事:</p><ul class=""><li id="a9d5" class="ma mb hi ih b ii ij im in iq mc iu md iy me jc mf mg mh mi bi translated">执行由驱动程序分配给它的代码，以及</li><li id="78cb" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated">将该执行器上的计算状态报告回驱动程序节点</li></ul><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es mo"><img src="../Images/0c321dea93c008a0cecc3fc1a5ac03ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CLqLSyZMHBidIEjc.png"/></div></div></figure><h1 id="0274" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">那什么是火花会？</h1><p id="b9e6" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">我们知道驱动程序进程控制Spark应用程序。<strong class="ih hj">驱动程序进程将自身作为一个名为Spark会话的对象提供给用户。</strong></p><p id="539e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark会话实例是Spark在集群中执行用户定义的操作的方式。在Scala和Python中，当您启动控制台时，Spark会话变量可用作<strong class="ih hj"> spark </strong>:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es mp"><img src="../Images/a46bf43f860a19b5db1a24d09602fa95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AL83JMrqrOP6MSw9.png"/></div></div></figure><h1 id="231e" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">Spark中的分区</h1><blockquote class="je jf jg"><p id="bef3" class="if ig jh ih b ii ij ik il im in io ip ji ir is it jj iv iw ix jk iz ja jb jc hb bi translated">分区意味着完整的数据不在一个地方。它被分成多个块，这些块被放置在不同的节点上。</p></blockquote><p id="1d19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你有一个分区，Spark将只有一个并行度，即使你有成千上万个执行程序。此外，如果您有许多分区，但只有一个执行器，Spark仍然只有一个并行度，因为只有一个计算资源。</p><p id="c7fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Spark中，底层API允许我们定义分区的数量。</p><p id="8e6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们举一个简单的例子来理解分区如何帮助我们更快地得到结果。我们将创建一个10到1000之间的2000万个随机数的列表，并将计算大于200的数字。</p><p id="0f71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看仅用一个分区我们能做到多快:</p><pre class="jm jn jo jp fd lp lq lr ls aw lt bi"><span id="0a6a" class="kw jy hi lq b fi lu lv l lw lx">from random import randint</span><span id="4327" class="kw jy hi lq b fi mq lv l lw lx"># create a list of random numbers between 10 to 1000<br/>my_large_list = [randint(10,1000) for x in range(0,20000000)]</span><span id="3a79" class="kw jy hi lq b fi mq lv l lw lx"># create one partition of the list  <br/>my_large_list_one_partition = sc.parallelize(my_large_list,numSlices=1)</span><span id="8f8d" class="kw jy hi lq b fi mq lv l lw lx"># check number of partitions<br/>print(my_large_list_one_partition.getNumPartitions())<br/># &gt;&gt; 1</span><span id="5a39" class="kw jy hi lq b fi mq lv l lw lx"># filter numbers greater than equal to 200<br/>my_large_list_one_partition = my_large_list_one_partition.filter(lambda x : x &gt;= 200)</span><span id="f192" class="kw jy hi lq b fi mq lv l lw lx"># code was run in a jupyter notebook <br/># to calculate the time taken to execute the following command<br/>%%time</span><span id="79fb" class="kw jy hi lq b fi mq lv l lw lx"># count the number of elements in filtered list<br/>print(my_large_list_one_partition.count())<br/># &gt;&gt; 16162207</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es mr"><img src="../Images/187ee26b32efa1322cf232e76122f5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qNY26Apo7k0dwhF5.png"/></div></div></figure><p id="67c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用一个分区过滤结果需要34.5毫秒:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es ms"><img src="../Images/fdb50e6c51d527541f8018b21f6bf6b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OAlbiVVEXvloqbB7.png"/></div></div></figure><p id="1abe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们将分区的数量增加到5，并检查我们是否在执行时间上有所改进:</p><pre class="jm jn jo jp fd lp lq lr ls aw lt bi"><span id="1178" class="kw jy hi lq b fi lu lv l lw lx"># create five partitions of the list<br/>my_large_list_with_five_partition = sc.parallelize(my_large_list, numSlices=5)</span><span id="6438" class="kw jy hi lq b fi mq lv l lw lx"># filter numbers greater than equal to 200<br/>my_large_list_with_five_partition = my_large_list_with_five_partition.filter(lambda x : x &gt;= 200)</span><span id="47c7" class="kw jy hi lq b fi mq lv l lw lx">%%time</span><span id="9147" class="kw jy hi lq b fi mq lv l lw lx"># count the number of elements in the filtered list<br/>print(my_large_list_with_five_partition.count())<br/># &gt;&gt; 16162207</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es mt"><img src="../Images/ac4c2f26d7e9390e1c849e5e437f769d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wOK8wgIF-_3hYso6.png"/></div></div></figure><p id="f7c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用五个分区过滤结果花费了11.1毫秒:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es mu"><img src="../Images/11d6cc955aab03992336ee3de678e51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vesw2xruD0Z5HRzL.png"/></div></div></figure><h1 id="4028" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">火花中的转变</h1><p id="284c" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">数据结构在Spark中是不可变的。这意味着它们一旦创建就不能更改。但是如果我们不能改变它，我们应该如何使用它呢？</p><blockquote class="je jf jg"><p id="c2bd" class="if ig jh ih b ii ij ik il im in io ip ji ir is it jj iv iw ix jk iz ja jb jc hb bi translated">因此，为了做出任何改变，我们需要指示Spark我们希望如何修改我们的数据。这些指令被称为转换。</p></blockquote><p id="5682" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回想一下我们在上面看到的例子。我们要求Spark过滤大于200的数字——这基本上是一种转换。Spark中有两种类型的转换:</p><ul class=""><li id="23e9" class="ma mb hi ih b ii ij im in iq mc iu md iy me jc mf mg mh mi bi translated"><strong class="ih hj">窄转换:</strong>在窄转换中，计算单个分区的结果所需的所有元素都位于父RDD的单个分区中。例如，如果您想过滤小于100的数字，您可以分别在每个分区上执行此操作。转换后的新分区仅依赖于一个分区来计算结果</li></ul><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es mv"><img src="../Images/681d9982b5421fd6a9b59f2e479baf26.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/0*bqiFQMxmZ2Rq7HVr.png"/></div></figure><ul class=""><li id="b9a0" class="ma mb hi ih b ii ij im in iq mc iu md iy me jc mf mg mh mi bi translated"><strong class="ih hj">宽转换:</strong>在宽转换中，计算单个分区的结果所需的所有元素可能位于父RDD的多个分区中。例如，如果您想要计算字数，那么您的转换依赖于所有分区来计算最终结果。</li></ul><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es mw"><img src="../Images/61cbf289668b1e60d77f0d9b807d3382.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/0*hne8WbuuRniDVd4u.png"/></div></figure></div><div class="ab cl mx my gp mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="hb hc hd he hf"><p id="c855" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jh">原载于2019年10月28日</em><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/10/pyspark-for-beginners-first-steps-big-data-analysis/?utm_source=av&amp;utm_medium=feed-articles&amp;utm_campaign=feed" rel="noopener ugc nofollow" target="_blank"><em class="jh">https://www.analyticsvidhya.com</em></a><em class="jh">。</em></p></div></div>    
</body>
</html>