<html>
<head>
<title>All about YOLOs — Part5 — How to Code it up</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于YOLOs的一切——第五部分——如何编写代码</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/all-about-yolos-part5-how-to-code-it-up-937f05cc9ae9?source=collection_archive---------9-----------------------#2020-01-20">https://medium.com/analytics-vidhya/all-about-yolos-part5-how-to-code-it-up-937f05cc9ae9?source=collection_archive---------9-----------------------#2020-01-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/d3622f08e4e2574fcb14e07bc812d1c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MKQ84cP2dmXbfQRG.jpg"/></div></div></figure><div class=""/><p id="bfcb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇文章中，我将解释如何使用YOLOv3的普通版本来检测COCO数据集中的对象，以及如何为自己的用例定制自己的数据集。</p><p id="3e4d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里的Yolo检测代码是基于<a class="ae jo" href="https://github.com/eriklindernoren/PyTorch-YOLOv3" rel="noopener ugc nofollow" target="_blank">埃里克·林德诺伦</a>对<a class="ae jo" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" rel="noopener ugc nofollow" target="_blank">约瑟夫·雷德蒙和阿里·法尔哈迪论文</a>的实现。</p><p id="eafd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下是该系列的链接。</p><p id="ea9b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" rel="noopener" href="/@rehan_ahmad/all-about-yolos-part1-a-bit-of-history-a995bad5ac57"> <strong class="is hu">关于YOLOs — Part1 —一点历史</strong> </a></p><p id="3103" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" rel="noopener" href="/@rehan_ahmad/all-about-yolos-part2-the-first-yolo-2b5db7d78411"> <strong class="is hu">关于YOLOs — Part2 —第一个YOLO </strong> </a></p><p id="99e3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" rel="noopener" href="/@rehan_ahmad/all-about-yolos-part3-the-better-faster-and-stronger-yolov2-9c0cf9de9758"> <strong class="is hu">关于YOLOs的一切——第三部分——更好更快更强YOLOv2 </strong> </a></p><p id="cc3b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" rel="noopener" href="/@rehan_ahmad/all-about-yolos-part4-yolov3-an-incremental-improvement-36b1eee463a2"> <strong class="is hu">关于YOLOs — Part4 — YOLOv3的一切，一个增量的改进</strong> </a></p><p id="356c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">关于YOLOs的一切——第5部分——启动和运行</strong></p><p id="7e25" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请在我的<a class="ae jo" href="https://github.com/gotorehanahmad/yolov3-pytorch" rel="noopener ugc nofollow" target="_blank"> gitrepo </a>中找到项目文件夹。</p><p id="f4cd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在项目文件夹中，您会发现一个名为<strong class="is hu"> config </strong>的子文件夹，其中包含配置文件、类名和环境变量，还有包含数据集的<strong class="is hu"> data </strong>文件夹和包含一些有用的python函数的<strong class="is hu"> utils </strong>文件夹。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es jp"><img src="../Images/8f6cbb28eb7763c1298f1f0c7b4c6d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*T07apuFsGQGAXO1C6R0ZIQ.png"/></div></figure><p id="a66c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，下载YOLOv3权重文件，并通过运行以下命令将其放在项目的config文件夹中。(我已经在回购中添加了一个. sh文件来完成此操作)</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="fb11" class="jz ka ht jv b fi kb kc l kd ke">wget <a class="ae jo" href="https://pjreddie.com/media/files/yolov3.weights" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/media/files/yolov3.weights</a></span></pre><p id="4769" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下载后，config文件夹的内容应该如下所示。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kf"><img src="../Images/c86c70b160462c85e809aab3ade86c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*HAOkwFub0kVXDP0SFmIq4w.png"/></div></div></figure><h1 id="0908" class="kg ka ht bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">如何让YOLOv3启动并运行以检测COCO对象？</h1><p id="f45b" class="pw-post-body-paragraph iq ir ht is b it ld iv iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">让我们看看推理COCO对象的香草YOLO的实现。</p><ul class=""><li id="eb41" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">导入所需的模块。</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="a09b" class="jz ka ht jv b fi kb kc l kd ke">from models import *<br/>from utils import *import os, sys, time, datetime, random<br/>import torch<br/>from torch.utils.data import DataLoader<br/>from torchvision import datasets, transforms<br/>from torch.autograd import Variableimport matplotlib.pyplot as plt<br/>import matplotlib.patches as patches<br/>from PIL import Image</span></pre><ul class=""><li id="3438" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">加载预训练的配置和权重，以及对<a class="ae jo" href="https://github.com/pjreddie/darknet" rel="noopener ugc nofollow" target="_blank"> <strong class="is hu"> Darknet </strong> </a>模型进行训练的COCO数据集的类名。img_size、conf_thres和num_thresold是可以基于用例进行调整的参数。</li></ul><blockquote class="lr ls lt"><p id="1240" class="iq ir lu is b it iu iv iw ix iy iz ja lv jc jd je lw jg jh ji lx jk jl jm jn hb bi translated">注意:将模型设置为eval模式以进行推理。</p></blockquote><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="9d3e" class="jz ka ht jv b fi kb kc l kd ke">config_path='config/yolov3.cfg'<br/>weights_path='config/yolov3.weights'<br/>class_path='config/coco.names'<br/>img_size=416<br/>conf_thres=0.8<br/>nms_thres=0.4</span><span id="be99" class="jz ka ht jv b fi ly kc l kd ke"># Load model and weights<br/>model = Darknet(config_path, img_size=img_size)<br/>model.load_weights(weights_path)<br/>model.cuda()<br/>model.eval()<br/>classes = utils.load_classes(class_path)<br/>Tensor = torch.cuda.FloatTensor</span></pre><ul class=""><li id="8c85" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">写一个函数来执行给定图像的基本检测。请查看注释了解代码的作用。主要是对图像进行预处理。</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="d937" class="jz ka ht jv b fi kb kc l kd ke">def detect_image(img):<br/>    # scale and pad image<br/>    ratio = min(img_size/img.size[0], img_size/img.size[1])<br/>    imw = round(img.size[0] * ratio)<br/>    imh = round(img.size[1] * ratio)<br/>    img_transforms=transforms.Compose([transforms.Resize((imh,imw)),<br/>         transforms.Pad((max(int((imh-imw)/2),0), <br/>              max(int((imw-imh)/2),0), max(int((imh-imw)/2),0),<br/>              max(int((imw-imh)/2),0)), (128,128,128)),<br/>         transforms.ToTensor(),<br/>         ])<br/>    # convert image to Tensor<br/>    image_tensor = img_transforms(img).float()<br/>    image_tensor = image_tensor.unsqueeze_(0)<br/>    input_img = Variable(image_tensor.type(Tensor))<br/>    # run inference on the model and get detections<br/>    with torch.no_grad():<br/>        detections = model(input_img)<br/>        detections = utils.non_max_suppression(detections, 80, <br/>                        conf_thres, nms_thres)<br/>    return detections[0]</span></pre><ul class=""><li id="daef" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">现在是使用这个函数得到推论的代码。这适用于COCO数据集中的任何对象。同样，大部分代码处理图像的预处理和边界框的绘制。</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="9e57" class="jz ka ht jv b fi kb kc l kd ke"># load image and get detections<br/>img_path = "images/blueangels.jpg"<br/>prev_time = time.time()<br/>img = Image.open(img_path)<br/>detections = detect_image(img)<br/>inference_time = datetime.timedelta(seconds=time.time() - prev_time)<br/>print ('Inference Time: %s' % (inference_time))# Get bounding-box colors<br/>cmap = plt.get_cmap('tab20b')<br/>colors = [cmap(i) for i in np.linspace(0, 1, 20)]img = np.array(img)<br/>plt.figure()<br/>fig, ax = plt.subplots(1, figsize=(12,9))<br/>ax.imshow(img)pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))<br/>pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))<br/>unpad_h = img_size - pad_y<br/>unpad_w = img_size - pad_xif detections is not None:<br/>    unique_labels = detections[:, -1].cpu().unique()<br/>    n_cls_preds = len(unique_labels)<br/>    bbox_colors = random.sample(colors, n_cls_preds)<br/>    # browse detections and draw bounding boxes<br/>    for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:<br/>        box_h = ((y2 - y1) / unpad_h) * img.shape[0]<br/>        box_w = ((x2 - x1) / unpad_w) * img.shape[1]<br/>        y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]<br/>        x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]<br/>        color = bbox_colors[int(np.where(<br/>             unique_labels == int(cls_pred))[0])]<br/>        bbox = patches.Rectangle((x1, y1), box_w, box_h,<br/>             linewidth=2, edgecolor=color, facecolor='none')<br/>        ax.add_patch(bbox)<br/>        plt.text(x1, y1, s=classes[int(cls_pred)], <br/>                color='white', verticalalignment='top',<br/>                bbox={'color': color, 'pad': 0})<br/>plt.axis('off')<br/># save image<br/>plt.savefig(img_path.replace(".jpg", "-det.jpg"),        <br/>                  bbox_inches='tight', pad_inches=0.0)<br/>plt.show()</span></pre><ul class=""><li id="143a" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">上面的脚本处理图像检测。现在让我们看看如何让它为<strong class="is hu">视频</strong>工作。</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="068d" class="jz ka ht jv b fi kb kc l kd ke">videopath = 'video/sample_video.mp4'</span><span id="1a01" class="jz ka ht jv b fi ly kc l kd ke">%pylab inline <br/>import cv2<br/>from IPython.display import clear_outputcmap = plt.get_cmap('tab20b')<br/>colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]<br/># initialize <br/>vid = cv2.VideoCapture(videopath)<br/>#while(True):<br/>for ii in range(40):<br/>    ret, frame = vid.read()<br/>    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)<br/>    pilimg = Image.fromarray(frame)<br/>    detections = detect_image(pilimg)    img = np.array(pilimg)<br/>    pad_x = max(img.shape[0] - img.shape[1], 0) * <br/>            (img_size / max(img.shape))<br/>    pad_y = max(img.shape[1] - img.shape[0], 0) * <br/>            (img_size / max(img.shape))<br/>    unpad_h = img_size - pad_y<br/>    unpad_w = img_size - pad_x<br/>    if detections is not None:<br/>        unique_labels = detections[:, -1].cpu().unique()<br/>        n_cls_preds = len(unique_labels)<br/>        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:<br/>            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])<br/>            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])<br/>            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])<br/>            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])                color = colors[int(cls_conf.item()) % len(colors)]<br/>            color = [i * 255 for i in color]<br/>            cls = classes[int(cls_pred)]<br/>            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h),<br/>                         color, 4)<br/>            cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60,<br/>                         y1), color, -1)<br/>            cv2.putText(frame, cls + "-" + str(cls_conf.item()), <br/>                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, <br/>                        1, (255,255,255), 3)    fig=figure(figsize=(12, 8))<br/>    title("Video Stream")<br/>    imshow(frame)<br/>    show()<br/>    clear_output(wait=True)</span></pre><blockquote class="lr ls lt"><p id="aceb" class="iq ir lu is b it iu iv iw ix iy iz ja lv jc jd je lw jg jh ji lx jk jl jm jn hb bi translated">注意:通过将视频路径更改为IP摄像机流，上述代码也可以用于实时流。</p></blockquote><h1 id="61db" class="kg ka ht bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">如何微调vanilla YOLOv3模型以适用于自定义对象？</strong></h1><p id="51c5" class="pw-post-body-paragraph iq ir ht is b it ld iv iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">让我们看看我们自己的用例的定制培训YOLOv3。他们有多种方法来实现这一点。但是下面的步骤解释了我在网上找到的最简单的方法。</p><h1 id="808c" class="kg ka ht bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">先决条件</h1><h2 id="cd28" class="jz ka ht bd kh lz ma mb kl mc md me kp jb mf mg kt jf mh mi kx jj mj mk lb ml bi translated">数据</h2><p id="bc60" class="pw-post-body-paragraph iq ir ht is b it ld iv iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">第一步是创建您的训练数据，即使用您希望检测发生的边界框和类别标签来标记图像。</p><p id="f0dd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有很多工具可以做到这一点。我发现最简单的方法是使用<a class="ae jo" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank">标签</a>。这是一个图形化的图像注释工具。您可以使用pip命令进行安装。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="c285" class="jz ka ht jv b fi kb kc l kd ke">pip install labelImg</span></pre><p id="73b4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">点击<a class="ae jo" href="https://pythonlearning.org/2019/12/02/labelimg-a-graphical-image-annotation-tool-and-label-object-bounding-boxes-in-images/" rel="noopener ugc nofollow" target="_blank">链接</a>找到一个关于如何使用它的很好的教程。</p><p id="2cdc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">给图像加标签很简单，就是给边界框坐标和类别加标签。因此，对于每个图像，生成的标签(。txt文件)将只有一行。这就是所谓的YOLO格式。</p><blockquote class="lr ls lt"><p id="df85" class="iq ir lu is b it iu iv iw ix iy iz ja lv jc jd je lw jg jh ji lx jk jl jm jn hb bi translated">注意:使用标签工具标记图像时，请确保选择YOLO格式。</p></blockquote><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="0222" class="jz ka ht jv b fi kb kc l kd ke">#class x y width height<br/>1 0.351466 0.427083 0.367168 0.570486</span></pre><p id="fe1d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦图像被加了标签。txt文件)生成后，运行脚本来拆分数据集以进行训练和验证。请只运行一次以下python代码来实现这一点。<strong class="is hu"> datasplit.py </strong>是<a class="ae jo" href="https://github.com/gotorehanahmad/yolov3-pytorch" rel="noopener ugc nofollow" target="_blank"> repo </a>中带有该代码的文件。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="aa36" class="jz ka ht jv b fi kb kc l kd ke">import glob<br/>import os<br/>import numpy as np<br/>import syscurrent_dir = "./data/artifacts/images"<br/>split_pct = 10  # 10% validation set<br/>file_train = open("data/artifacts/train.txt", "w")  <br/>file_val = open("data/artifacts/val.txt", "w")  <br/>counter = 1  <br/>index_test = round(100 / split_pct)  <br/>for fullpath in glob.iglob(os.path.join(current_dir, "*.JPG")):  <br/>  title, ext = os.path.splitext(os.path.basename(fullpath))<br/>  if counter == index_test:<br/>    counter = 1<br/>    file_val.write(current_dir + "/" + title + '.JPG' + "\n")<br/>  else:<br/>    file_train.write(current_dir + "/" + title + '.JPG' + "\n")<br/>    counter = counter + 1<br/>file_train.close()<br/>file_val.close()</span></pre><p id="0e5c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这将创建两个文件，train.txt和val.txt，其中包含所有图像的完整路径，90%在train中，10%在val中。</p><p id="697b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">完成后，请确保在以下文件夹结构中获取数据集。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="45c0" class="jz ka ht jv b fi kb kc l kd ke"><em class="lu">Main Folder</em><br/>--- <em class="lu">data</em><br/>    --- <em class="lu">dataset name</em><br/>        --- <em class="lu">images</em><br/>            --- img1.jpg<br/>            --- img2.jpg<br/>            ..........<br/>        --- <em class="lu">labels</em><br/>            --- img1.txt<br/>            --- img2.txt<br/>            ..........<br/>        --- train.txt<br/>        --- val.txt</span></pre><h2 id="d714" class="jz ka ht bd kh lz ma mb kl mc md me kp jb mf mg kt jf mh mi kx jj mj mk lb ml bi translated"><strong class="ak">配置</strong></h2><p id="dcaf" class="pw-post-body-paragraph iq ir ht is b it ld iv iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">现在来看看<em class="lu"> config/ </em>文件夹中的配置文件。首先，<strong class="is hu"> <em class="lu"> coco.data </em> </strong>大概是这样的:</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="2695" class="jz ka ht jv b fi kb kc l kd ke">classes = 80 # number of classes goes here<br/>train=data/alpha/train.txt # the path of the train.txt goes here<br/>valid=data/alpha/val.txt # the path of the val.txt goes here<br/>names=config/coco.names # edit the names file with class labels<br/>backup=backup/ # Keep this parameter as it is</span></pre><p id="6a7f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据您的自定义数据集编辑这些值。编辑“classes ”,以包含您的用例中要检测的类的数量。Train和valid分别保存到train.txt和val.txt的路径。编辑<strong class="is hu"><em class="lu">【coco . names】</em></strong>带有班级标签的文件。它应该列出类的名称，每行一个(对于注释文件，第一个对应于0，第二个对应于1，依此类推)</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="912f" class="jz ka ht jv b fi kb kc l kd ke">class1<br/>class2<br/>...</span></pre><blockquote class="lr ls lt"><p id="0b7f" class="iq ir lu is b it iu iv iw ix iy iz ja lv jc jd je lw jg jh ji lx jk jl jm jn hb bi translated">注意:没有使用backup参数，但它似乎是必需的。</p></blockquote><p id="a53b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在对于<strong class="is hu"> yolov3.cfg </strong>文件。这包含了YOLOv3算法的架构细节。</p><p id="71b4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在第一个<code class="du mm mn mo jv b">[net]</code>部分，调整<code class="du mm mn mo jv b">batch</code>值和<code class="du mm mn mo jv b">subdivisions</code>，以适应您的GPU内存。批量越大，训练越好，越快，但占用的内存也越多。您也可以在这里调整<code class="du mm mn mo jv b">learning_rate</code>。</p><blockquote class="lr ls lt"><p id="8aeb" class="iq ir lu is b it iu iv iw ix iy iz ja lv jc jd je lw jg jh ji lx jk jl jm jn hb bi translated">对于一个11Gb内存的Nvidia GPU来说，一批16和1细分就不错了。</p></blockquote><p id="b43f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">还有两个重要的东西需要改变，那就是<code class="du mm mn mo jv b">classes</code>和最终图层<code class="du mm mn mo jv b">filters</code>的值。而且你要在<strong class="is hu">文件中的三个</strong>不同的地方做。</p><p id="dc53" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你搜索这个文件，你会发现3个<code class="du mm mn mo jv b">[yolo]</code>部分。在这个部分中，将<code class="du mm mn mo jv b">classes</code>设置为模型中的类的数量。</p><p id="826f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您还必须将[yolo]正上方的[卷积]部分中的<code class="du mm mn mo jv b">filters</code>值更改为以下值。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="5463" class="jz ka ht jv b fi kb kc l kd ke">filters = (classes + 5) x 3</span></pre><p id="3305" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所有上述设置就绪后，您现在就可以开始训练模型了。</p></div><div class="ab cl mp mq gp mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="hb hc hd he hf"><h1 id="eee4" class="kg ka ht bd kh ki mw kk kl km mx ko kp kq my ks kt ku mz kw kx ky na la lb lc bi translated">密码</h1><h2 id="f4c4" class="jz ka ht bd kh lz ma mb kl mc md me kp jb mf mg kt jf mh mi kx jj mj mk lb ml bi translated">训练循环的实现</h2><ul class=""><li id="df41" class="li lj ht is b it ld ix le jb nb jf nc jj nd jn ln lo lp lq bi translated">导入库</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="3c3c" class="jz ka ht jv b fi kb kc l kd ke">from __future__ import division</span><span id="406d" class="jz ka ht jv b fi ly kc l kd ke">from models import *<br/>from utils.utils import *<br/>from utils.datasets import *<br/>from utils.parse_config import *</span><span id="bd2b" class="jz ka ht jv b fi ly kc l kd ke">import os<br/>import sys<br/>import time<br/>import datetime<br/>import argparse</span><span id="bc6e" class="jz ka ht jv b fi ly kc l kd ke">import torch<br/>from torch.utils.data import DataLoader<br/>from torchvision import datasets<br/>from torchvision import transforms<br/>from torch.autograd import Variable<br/>import torch.optim as optim</span></pre><ul class=""><li id="ea95" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">参数设置</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="4f59" class="jz ka ht jv b fi kb kc l kd ke">epochs = 20<br/>image_folder = "data/dataset/images"<br/>batch_size = 16<br/>model_config_path = "config/yolov3.cfg"<br/>data_config_path = "config/coco.data"<br/>weights_path = "config/yolov3.weights"<br/>class_path = "config/coco.names"<br/>conf_thres = 0.8<br/>nms_thres = 0.4<br/>n_cpu = 0<br/>img_size = 416<br/>checkpoint_interval = 1<br/>checkpoint_dir = 'checkpoints'<br/>use_cuda = True</span></pre><ul class=""><li id="98a2" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">如果可能，使用CUDA</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="16b6" class="jz ka ht jv b fi kb kc l kd ke">cuda = torch.cuda.is_available() and use_cuda</span></pre><ul class=""><li id="0513" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">将数据配置和参数存入内存</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="c0a3" class="jz ka ht jv b fi kb kc l kd ke"># Load classes<br/>os.makedirs("checkpoints", exist_ok=True)<br/>classes = load_classes(class_path)</span><span id="292e" class="jz ka ht jv b fi ly kc l kd ke"># Get data configuration<br/>data_config = parse_data_config(data_config_path)<br/>train_path = data_config["train"]</span><span id="6904" class="jz ka ht jv b fi ly kc l kd ke"># Get hyper parameters<br/>hyperparams = parse_model_config(model_config_path)[0]<br/>learning_rate = float(hyperparams["learning_rate"])<br/>momentum = float(hyperparams["momentum"])<br/>decay = float(hyperparams["decay"])<br/>burn_in = int(hyperparams["burn_in"])</span></pre><ul class=""><li id="e033" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">启动模型并开始训练。</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="d517" class="jz ka ht jv b fi kb kc l kd ke"># Initiate model<br/>model = Darknet(model_config_path)<br/>model.load_weights(weights_path)</span><span id="c971" class="jz ka ht jv b fi ly kc l kd ke">if cuda:<br/>    model = model.cuda()<br/>model.train()</span></pre><ul class=""><li id="19a8" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">获取数据加载器，并设置优化器</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="9234" class="jz ka ht jv b fi kb kc l kd ke"># Get dataloader<br/>dataloader = torch.utils.data.DataLoader(<br/>    ListDataset(train_path), batch_size=batch_size, shuffle=False, num_workers=n_cpu<br/>)</span><span id="4032" class="jz ka ht jv b fi ly kc l kd ke">Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor</span><span id="19b2" class="jz ka ht jv b fi ly kc l kd ke"># Get optimizer<br/>optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))</span></pre><ul class=""><li id="59f6" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">现在是主要的训练循环。</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="d33c" class="jz ka ht jv b fi kb kc l kd ke">for epoch in range(epochs):<br/>    for batch_i, (_, imgs, targets) in enumerate(dataloader):<br/>        imgs = Variable(imgs.type(Tensor))<br/>        targets = Variable(targets.type(Tensor), requires_grad=False)</span><span id="1d39" class="jz ka ht jv b fi ly kc l kd ke">optimizer.zero_grad()</span><span id="a5d1" class="jz ka ht jv b fi ly kc l kd ke">loss = model(imgs, targets)</span><span id="9e2f" class="jz ka ht jv b fi ly kc l kd ke">loss.backward()<br/>        optimizer.step()</span><span id="dcfe" class="jz ka ht jv b fi ly kc l kd ke">print(<br/>            "[Epoch %d/%d, Batch %d/%d] [Losses: x %f, y %f, w %f, h %f, conf %f, cls %f, total %f, recall: %.5f, precision: %.5f]"<br/>            % (<br/>                epoch,<br/>                epochs,<br/>                batch_i,<br/>                len(dataloader),<br/>                model.losses["x"],<br/>                model.losses["y"],<br/>                model.losses["w"],<br/>                model.losses["h"],<br/>                model.losses["conf"],<br/>                model.losses["cls"],<br/>                loss.item(),<br/>                model.losses["recall"],<br/>                model.losses["precision"],<br/>            )<br/>        )</span><span id="aeb8" class="jz ka ht jv b fi ly kc l kd ke">model.seen += imgs.size(0)</span><span id="6531" class="jz ka ht jv b fi ly kc l kd ke">if epoch % checkpoint_interval == 0:<br/>        print("saving")<br/>        model.save_weights("%s/%d.weights" % (checkpoint_dir, "latest"))</span></pre><p id="3e6e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的循环训练并将权重文件保存在模型的文件夹中，用于每个纪元，以纪元编号作为名称。它还打印一堆损失，以监控训练的进度。</p></div><div class="ab cl mp mq gp mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="hb hc hd he hf"><h2 id="1a88" class="jz ka ht bd kh lz ma mb kl mc md me kp jb mf mg kt jf mh mi kx jj mj mk lb ml bi translated">推理的实现</h2><ul class=""><li id="f631" class="li lj ht is b it ld ix le jb nb jf nc jj nd jn ln lo lp lq bi translated">导入库</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="4ace" class="jz ka ht jv b fi kb kc l kd ke">from models import *<br/>from utils import *<br/>import cv2<br/>import os, sys, time, datetime, random<br/>import torch<br/>from torch.utils.data import DataLoader<br/>from torchvision import datasets, transforms<br/>from torch.autograd import Variable</span><span id="01df" class="jz ka ht jv b fi ly kc l kd ke">import matplotlib.pyplot as plt<br/>import matplotlib.patches as patches<br/>from PIL import Image<br/>import imutils<br/>from imutils.video import WebcamVideoStream</span></pre><ul class=""><li id="00ff" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">设置参数。要使用新模型进行检测，请用config文件夹中的yolov3.weights文件替换在<strong class="is hu"> models </strong>文件夹中生成的最新权重文件。确保推理代码中的权重路径指向最新的权重路径。</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="a319" class="jz ka ht jv b fi kb kc l kd ke">num_classes = 1<br/>config_path='config/yolov3.cfg'<br/>weights_path='checkpoint_19.weights'<br/>class_path='config/coco.names'<br/>img_size=416<br/>conf_thres=0.95<br/>nms_thres=0.95</span></pre><ul class=""><li id="5c56" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">加载模型并设置为eval以进行推理</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="de8e" class="jz ka ht jv b fi kb kc l kd ke"># Load model and weights<br/>model = Darknet(config_path, img_size=img_size)<br/>model.load_weights(weights_path)<br/># model.cuda()<br/>model.eval()<br/>classes = load_classes(class_path)<br/>Tensor = torch.FloatTensor</span></pre><ul class=""><li id="7148" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">定义加载类和检测图像的函数。</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="7f02" class="jz ka ht jv b fi kb kc l kd ke">def load_classes(path):<br/>    """<br/>    Loads class labels at 'path'<br/>    """<br/>    fp = open(path, "r")<br/>    names = fp.read().split("\n")[:]<br/>    return names</span><span id="a90b" class="jz ka ht jv b fi ly kc l kd ke">def detect_image(img):<br/>    # scale and pad image<br/>    ratio = min(img_size/img.size[0], img_size/img.size[1])<br/>    imw = round(img.size[0] * ratio)<br/>    imh = round(img.size[1] * ratio)<br/>    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),<br/>         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),<br/>                        (128,128,128)),<br/>         transforms.ToTensor(),<br/>         ])<br/>    # convert image to Tensor<br/>    image_tensor = img_transforms(img).float()<br/>    image_tensor = image_tensor.unsqueeze_(0)<br/>    input_img = Variable(image_tensor.type(Tensor))<br/>    # run inference on the model and get detections<br/>    with torch.no_grad():<br/>        detections = model(input_img)<br/>        detections = utils.non_max_suppression(detections, num_classes, conf_thres, nms_thres)<br/>    return detections[0]</span></pre><ul class=""><li id="174e" class="li lj ht is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated">现在是推理循环。</li></ul><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="56d3" class="jz ka ht jv b fi kb kc l kd ke">videopath = 'video/sample_video.mp4'</span><span id="e259" class="jz ka ht jv b fi ly kc l kd ke">%pylab inline <br/>import cv2<br/>from IPython.display import clear_outputcmap = plt.get_cmap('tab20b')<br/>colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]<br/># initialize <br/>vid = cv2.VideoCapture(videopath)<br/>#while(True):<br/>for ii in range(40):<br/>    ret, frame = vid.read()<br/>    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)<br/>    pilimg = Image.fromarray(frame)<br/>    detections = detect_image(pilimg)    img = np.array(pilimg)<br/>    pad_x = max(img.shape[0] - img.shape[1], 0) * <br/>            (img_size / max(img.shape))<br/>    pad_y = max(img.shape[1] - img.shape[0], 0) * <br/>            (img_size / max(img.shape))<br/>    unpad_h = img_size - pad_y<br/>    unpad_w = img_size - pad_x<br/>    if detections is not None:<br/>        unique_labels = detections[:, -1].cpu().unique()<br/>        n_cls_preds = len(unique_labels)<br/>        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:<br/>            box_h = int(((y2 - y1) / unpad_h) * img.shape[0])<br/>            box_w = int(((x2 - x1) / unpad_w) * img.shape[1])<br/>            y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])<br/>            x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])                color = colors[int(cls_conf.item()) % len(colors)]<br/>            color = [i * 255 for i in color]<br/>            cls = classes[int(cls_pred)]<br/>            cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h),<br/>                         color, 4)<br/>            cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60,<br/>                         y1), color, -1)<br/>            cv2.putText(frame, cls + "-" + str(cls_conf.item()), <br/>                        (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, <br/>                        1, (255,255,255), 3)    fig=figure(figsize=(12, 8))<br/>    title("Video Stream")<br/>    imshow(frame)<br/>    show()<br/>    clear_output(wait=True)</span></pre><p id="233d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请在我的<a class="ae jo" href="https://github.com/gotorehanahmad/yolov3-pytorch" rel="noopener ugc nofollow" target="_blank"><strong class="is hu">git-repo</strong></a><strong class="is hu">中找到用于训练和推理的jupyter笔记本。</strong></p><p id="efcd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我希望本系列文章能让您清楚地了解关于YOLO的一切，并让您开始自己的实现。</p><p id="524e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你想看某个特定主题的博客，请在回复部分提出来。我会尽力的:)</p><p id="9097" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">资源:</strong></p><p id="997e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">https://arxiv.org/pdf/1506.02640.pdf</p><p id="036f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">YOLOv2和yolo 9000:<a class="ae jo" href="https://arxiv.org/pdf/1612.08242.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.08242.pdf</a></p><p id="fad1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">约洛夫3:【https://arxiv.org/pdf/1804.02767.pdf】T2</p><blockquote class="lr ls lt"><p id="a396" class="iq ir lu is b it iu iv iw ix iy iz ja lv jc jd je lw jg jh ji lx jk jl jm jn hb bi translated"><em class="ht">关于我</em></p></blockquote><p id="f8ff" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我是<a class="ae jo" href="https://wavelabs.ai/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Wavelabs.ai </a>的资深AI专家。我们Wavelabs帮助您利用人工智能(AI)来彻底改变用户体验并降低成本。我们使用人工智能独特地增强您的产品，以达到您的全部市场潜力。我们试图将尖端研究引入您的应用中。</p><p id="6668" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">欢迎访问<a class="ae jo" href="https://wavelabs.ai/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Wavelabs.ai </a>了解更多信息。</p></div><div class="ab cl mp mq gp mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="hb hc hd he hf"><p id="2ad5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嗯，这都是在这个职位。感谢阅读:)</p><p id="fe26" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">保持好奇！</p><p id="a954" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在<a class="ae jo" href="https://www.linkedin.com/in/rehan-a-18675296?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p></div></div>    
</body>
</html>