<html>
<head>
<title>Analysis of K-Means Algorithm Convergence Time</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-Means算法收敛时间分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/analysis-of-k-means-algorithm-convergence-time-using-mid-points-of-equally-divided-regions-between-2bca5b9d63a3?source=collection_archive---------9-----------------------#2019-10-07">https://medium.com/analytics-vidhya/analysis-of-k-means-algorithm-convergence-time-using-mid-points-of-equally-divided-regions-between-2bca5b9d63a3?source=collection_archive---------9-----------------------#2019-10-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="74b6" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用最小-最大值之间等分区域的中点</h2></div><p id="a779" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">K-Means是一种无监督的机器学习算法，用于将相似的数据点分组在一起。例如，假设我们有一个数据集，其中包含一个地区消费快餐的人数、该地区的气候条件(温度和降雨量)以及死亡率。现在，我们希望在数据集中找出这些变量之间具有相关性的组，例如一个组可能代表快餐消费较高的寒冷干燥地区的高死亡率。</p><p id="ab4e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">回答这个问题的一个显而易见的方法是过滤温度和降雨量的数据，并寻找死亡率与快餐消费的趋势。根据数据集的大小和数据分析人员的能力，大约需要10-15分钟。如果我们在这个数据集上使用K-Means，它会自己将数据分组到有意义的聚类中，然后可以手动或使用决策树进行聚类标记，这个过程最多需要5分钟！</p><p id="1148" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们的一次机器学习讲座中，当对一维数据点应用K-means算法时，我注意到聚类中心收敛到最小值和最大值之间的K个等距点。比如我们应用k均值算法，k=2，对于这些数据点0.1，0.3，0.4，0.8，0.9，我们得到的聚类中心是0.267，0.85。这些是在将聚类中心的初始猜测取为0.1和0.3之后运行K-Means算法4次之后获得的。</p><p id="412a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将上述数据点分成2个相等的空间(这里k=2)并取每个空间的中点，我们得到中点为0.3和0.7。考虑将这些点作为我们的K均值算法的初始猜测，我们在仅仅1次迭代中收敛到相同的点，0.267，0.85！这促使我尝试将这种技术应用于更大的k值和更高维度。</p><p id="bad5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于以下每个数据集，我比较了随机初始化和初始值初始化的K均值算法的收敛时间。我已经使用了sklearn的kmeans函数来执行这些任务。</p><p id="44c3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">距离芝加哥数据集</strong></p><p id="c940" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这包括主要城市及其与芝加哥的距离。在对具有随机初始化和k=4的该数据集应用K均值时，我们获得以下聚类中心:[ 378.8125，815.2174，1245.5455，1960.1111]。在提供初始值作为2148(最大值)和92(最小值)之间的等分区域的中点时，即[349，863，1377，1891]，我们获得相同的聚类中心，因此获得相同的聚类成员。当我们比较这些方法之间的收敛时间时，初始值初始化要快100%左右。</p><p id="073c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我尝试将k值增加到15，但是正如我们在下面看到的，聚类中心略有不同，因此聚类成员也略有不同。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/eb6794d250416ada4469b67f583623b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PY5IbHs4CHo0pJd3_JIDYA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">k=15的初始猜测值和聚类中心，距离芝加哥数据集</figcaption></figure><p id="9c14" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随机初始化的惯性值或类内平方和为47337.72619，初始值初始化为53456.6984。这告诉我们，初值法还没有完全收敛，或者收敛到局部极小值。然而，初始值方法仍然要快95%左右。</p><p id="fcff" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面，我们可以在美国地图上观察k=15的集群成员。每种颜色代表集群成员，红色的大点代表芝加哥。该图是使用python中的底图包获得的。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kj"><img src="../Images/d97b9cbf473d1b2ef3a45fc4a2f22473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PpQeB5t8vE8PszgNKSk86Q.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">使用底图包在美国地图上绘制城市</figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kk"><img src="../Images/7537e3c915c5dadaa76e00f0ff9003bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*NWdQt8kosYtsbHF4PkAlSQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">当我们增加距离芝加哥数据集的k值时，两种方法所用时间的比较</figcaption></figure><p id="c6d0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">二维数据集</strong></p><p id="6777" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对2D数据集进行类似的分析，我们获得了以下结果。</p><p id="400a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于k=4，我们看到这里的聚类中心略有不同，但是当我们查看聚类成员时，这种差异是不可见的。随机法和初值法的惯性值分别为138150387999424.77和138151320103050.34。初始值方法的惯性值也更大，但现在初始值方法快了67%。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kl"><img src="../Images/6da5e577a083478fff9262fb46902788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YjVE9F9eu8RzDef7zhVUUQ.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">两种方法中k=4的2D数据的聚类中心</figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es km"><img src="../Images/9781f0942625c2701aafac63b7e6bdfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3cH73vPAyvmCum5WbZZDPw.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">两种方法中k=4的2D数据的聚类隶属度</figcaption></figure><p id="3561" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对k=15的类似分析将为我们提供以下结果。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kn"><img src="../Images/e0b4f4aafa3459418725ad637b44f601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pr_PAa9cVx0XjaWuzhL0zQ.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">两种方法中k=15的2D数据的聚类成员。</figcaption></figure><p id="e271" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们再次看到集群成员是不同的。现在的惯性值相差很多，随机初始化8.9E12，初值法20E12。</p><p id="6ddb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于更高维度，可以得到类似的观察结果。n=32的结果可以在github repository上找到。</p><p id="c4f8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">与小批量K-means和K-means++播种方法的比较</strong></p><p id="bbbc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">小批量k-means是在随机选择的固定大小的小批量数据上运行的K-means算法。K-means++是选择初始值的另一种方法，其中随机选择第一个中心，同时选择连续的中心，使得它们离先前选择的所有中心最远。</p><p id="b283" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于下面的分析，批处理大小固定为10，kmeans算法使用python中的scikit learn包实现。这里使用的数据集是如上所用的二维数据集。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ko"><img src="../Images/76100244e4f68158cd7d8a56230a29f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G6kAgOdeEQUsBIudFyIt9Q.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">k=15时不同方法收敛时间的比较</figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kp"><img src="../Images/43b19ec928943949b684ba656d5c0091.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*19DXUzZteICUfSwEsR6CVg.png"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kq"><img src="../Images/f58a5bddf2ec92fa1d6e990e51b819f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M-ZUcr0Ygjtl0kNxT2E3Sw.png"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kr"><img src="../Images/b90d341602b227f2098810a76fd69894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-LUfP4ISkh1c6bgbS0l8A.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">k=15时不同方法的结果比较</figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ks"><img src="../Images/47d62890b42105def3b0e42b8c2e467e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*ZCxR2rPO91fK5yZ97zUupw.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">k值范围内不同方法的总体比较</figcaption></figure><p id="f543" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以清楚地看到，这里讨论的初值法是所有方法中收敛时间最短的，但其结果也与随机初始化法有很大的不同。</p><p id="0961" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">初始值方法并不总是收敛到全局最小值，因为它根据最小值和最大值平均分布数据。但是，随机初始化会将点随机放置在由相应的最小值和最大值限定的超矩形(或2D的矩形)内。因此，与初始值初始化相比，随机初始化使我们能够更准确地捕捉数据的分布。</p><p id="a3e4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">参考文献:</strong></p><ol class=""><li id="6823" class="kt ku hi iz b ja jb jd je jg kv jk kw jo kx js ky kz la lb bi translated">芝加哥伊利诺伊理工大学的机器学习讲座</li><li id="d262" class="kt ku hi iz b ja lc jd ld jg le jk lf jo lg js ky kz la lb bi translated">【http://cs.joensuu.fi/sipu/datasets/ T2】号</li><li id="496a" class="kt ku hi iz b ja lc jd ld jg le jk lf jo lg js ky kz la lb bi translated"><a class="ae lh" href="https://www.geeksforgeeks.org/ml-mini-batch-k-means-clustering-algorithm/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/ml-mini-batch-k-means-clustering-algorithm/</a></li></ol><p id="cda3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">源代码可以在:<a class="ae lh" href="https://github.com/nikhilpmiskin/Self-Projects/tree/master/KMeansConvergenceTime" rel="noopener ugc nofollow" target="_blank">https://github . com/nikhilpmiskin/Self-Projects/tree/master/KMeansConvergenceTime</a>找到</p><p id="9a31" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">关于我</strong>:我正在伊利诺伊理工大学攻读数据科学硕士学位，并在IIT孟买大学完成了学士学位。在处理一个涉及贸易金融中ML和NLP的自动化问题时，我对数据科学产生了兴趣。我对通过使用机器学习算法来理解数据中隐藏的意图感兴趣，这些算法可用于解决其他内在问题。<strong class="iz hj">领英:</strong><a class="ae lh" href="https://www.linkedin.com/in/nikhilpmiskin/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/nikhilpmiskin/</a></p></div></div>    
</body>
</html>