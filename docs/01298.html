<html>
<head>
<title>Reddit posts classification using NLP, Pandas and S3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Reddit发布了使用自然语言处理、熊猫和S3的分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reddit-posts-classification-using-nlp-pandas-and-s3-68045246a90e?source=collection_archive---------12-----------------------#2019-10-13">https://medium.com/analytics-vidhya/reddit-posts-classification-using-nlp-pandas-and-s3-68045246a90e?source=collection_archive---------12-----------------------#2019-10-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c389" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的上一篇<a class="ae jd" rel="noopener" href="/@tonya.chernyshova/loading-reddit-posts-using-aws-lambda-and-cloudwatch-events-7e58cbddd000">帖子</a>中，我使用AWS lambda和CloudWatch事件自动收集了S3的reddit帖子。在这篇文章中，我们将从两个子编辑中提取文章，并构建一个ML分类模型来识别给定文章属于哪个子编辑。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/85a4e2c51a959d3e1bfe5073c2366295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1oRk5XfC8x7kcn8sfB8XaA.png"/></div></div></figure><p id="8886" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个项目，我选择了高度相关的子编辑:</p><ul class=""><li id="a1fc" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">爱狗人士subreddit—【https://www.reddit.com/r/dogs/ T2】</li><li id="9865" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">憎恨狗的人subreddit—【https://www.reddit.com/r/Dogfree T4】</li></ul><p id="42d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们打开Jupyter笔记本，从加载两个子编辑的帖子开始。熊猫有一个从S3读取文件的便捷方法。</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="e96d" class="kj kk hi kf b fi kl km l kn ko"><strong class="kf hj">import</strong> pandas <strong class="kf hj">as</strong> pd<br/><strong class="kf hj">import</strong> numpy <strong class="kf hj">as</strong> np</span><span id="b4f2" class="kj kk hi kf b fi kp km l kn ko">#name of the bucket with reddit posts<br/>bucket_name = 'nlp-reddit-posts'</span><span id="bc0e" class="kj kk hi kf b fi kp km l kn ko">df_dogs = pd.<strong class="kf hj">read_csv</strong>('s3://' + bucket_name + '/dogs.csv')<br/>df_dogs = df_dogs[['subreddit', 'title', 'selftext']]<br/>df_dogs.<strong class="kf hj">head</strong>()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kq"><img src="../Images/8fde7d75428726c762e7e0a947c30016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BTipILSI5hwsyKrWItTu-g.png"/></div></div></figure><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="0489" class="kj kk hi kf b fi kl km l kn ko">df_dogfree = pd.<strong class="kf hj">read_csv</strong>('s3://' + bucket_name + '/Dogfree.csv')<br/>df_dogfree = df_dogfree[['subreddit','title','text']]<br/>df_dogfree.<strong class="kf hj">head</strong>()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kr"><img src="../Images/bbf031f375c928752680c0e3998c226b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4IFVyn25cx5PX-eXaogXNw.png"/></div></div></figure><p id="c203" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所看到的，有些帖子没有文字。这样的帖子由标题和一些外部资源的链接组成。让我们连接两个数据帧，并查看一些统计数据:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="b2bf" class="kj kk hi kf b fi kl km l kn ko">df = pd.<strong class="kf hj">concat</strong>([df_dogs, df_dogfree]) <br/>df.<strong class="kf hj">shape<br/></strong>(2245, 3)</span><span id="56f2" class="kj kk hi kf b fi kp km l kn ko">df.<strong class="kf hj">isnull</strong>().sum().sort_values(ascending=<strong class="kf hj">False</strong>)<br/>selftext     618<br/>title        0  <br/>subreddit    0  <br/>dtype: int64</span><span id="1c01" class="kj kk hi kf b fi kp km l kn ko">df['subreddit'].<strong class="kf hj">value_counts</strong>()<br/>dogs       1127<br/>Dogfree    1118<br/>Name: subreddit, dtype: int64</span></pre><p id="3724" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用空文本替换空值，并将标题和自文本连接起来。同样，让我们创建一个列subreddit_class，它将subreddit表示为二进制值0或1。</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="771a" class="kj kk hi kf b fi kl km l kn ko">df.<strong class="kf hj">replace</strong>(np.nan, '', inplace=<strong class="kf hj">True</strong>)<br/>df['text'] = df['title']  + ' ' + df['selftext']</span><span id="6a43" class="kj kk hi kf b fi kp km l kn ko">df['subreddit_class'] = [1 <strong class="kf hj">if</strong> i == 'dogs' <strong class="kf hj">else</strong> 0 <strong class="kf hj">for</strong> i <strong class="kf hj">in </strong>df['subreddit']]</span><span id="ce27" class="kj kk hi kf b fi kp km l kn ko">df.<!-- -->sample(frac=1)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ks"><img src="../Images/5a8893d8ba2a2d41cd8a024038ad9122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wd9EPbZumhQn7ZeLef86WA.png"/></div></div></figure><p id="4b7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们以一只狗和一只猫的形状创建漂亮的单词云:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="e9cd" class="kj kk hi kf b fi kl km l kn ko"><strong class="kf hj">from</strong> wordcloud <strong class="kf hj">import</strong> WordCloud, STOPWORDS, ImageColorGenerator<br/><strong class="kf hj">from</strong> nltk.corpus <strong class="kf hj">import</strong> stopwords</span><span id="4786" class="kj kk hi kf b fi kp km l kn ko"># exclude stop words from the cloud<br/>stop_words = stopwords.<strong class="kf hj">words</strong>('english') + ['help', 'discussion']</span><span id="0783" class="kj kk hi kf b fi kp km l kn ko"><strong class="kf hj">def</strong> world_cloud(text, filename, mask=<strong class="kf hj">None</strong>):<br/>    wordcloud = WordCloud(max_font_size=72, max_words=150,<br/>                      background_color="white",<br/>                      stopwords = stop_words,<br/>                      mask=mask).<strong class="kf hj">generate</strong>(text)<br/>    plt.<strong class="kf hj">figure</strong>(figsize=(10,10))<br/>    plt.<strong class="kf hj">imshow</strong>(wordcloud, interpolation="bilinear");<br/>    plt.<strong class="kf hj">axis</strong>("off");<br/>    plt.<strong class="kf hj">show</strong>();</span></pre><p id="5d23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">爱狗人士云可以这样创建:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="3077" class="kj kk hi kf b fi kl km l kn ko"># break down titles of "dogs" subreddit and concatenate them<br/>text_dogs = " ".<strong class="kf hj">join</strong>(text <strong class="kf hj">for</strong> text <strong class="kf hj">in</strong> df['title'][(df['subreddit']=='dogs')]) </span><span id="7f14" class="kj kk hi kf b fi kp km l kn ko"># image with a shape of a dog<br/>url_dog='https://render.fineartamerica.com/images/rendered/default/print/8.000/6.375/break/images-medium-5/golden-retriever-dog-vector-silhouette-helgamariah.jpg'</span><span id="195f" class="kj kk hi kf b fi kp km l kn ko">mask_dog=np.<strong class="kf hj">array</strong>(Image.<strong class="kf hj">open</strong>(requests.<strong class="kf hj">get</strong>(url_dog,stream=<strong class="kf hj">True</strong>).<strong class="kf hj">raw</strong>))world_cloud(text_dogs, 'dogs', mask_dog)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kt"><img src="../Images/ab406f64487f69bba5048d9b22ef31eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*04pq46TynW0uhk1-EheZrA.png"/></div></figure><p id="5878" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，憎恨狗的云可以这样创建:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="ca8b" class="kj kk hi kf b fi kl km l kn ko"># break down titles of "dogfree" subreddit and concatenate them<br/>text_dogfree = " ".<strong class="kf hj">join</strong>(text <strong class="kf hj">for</strong> text <strong class="kf hj">in</strong> df['title'][(df['subreddit'] == 'Dogfree')]) </span><span id="3934" class="kj kk hi kf b fi kp km l kn ko"># image with a shape of a cat<br/>url_cat='<a class="ae jd" href="https://images-na.ssl-images-amazon.com/images/I/31CaLdDFlKL.jpg'" rel="noopener ugc nofollow" target="_blank">https://images-na.ssl-images-amazon.com/images/I/31CaLdDFlKL.jpg'</a></span><span id="041d" class="kj kk hi kf b fi kp km l kn ko">mask_cat=np.<strong class="kf hj">array</strong>(Image.<strong class="kf hj">open</strong>(requests.<strong class="kf hj">get</strong>(url_cat,stream=True).<strong class="kf hj">raw</strong>))<br/>world_cloud(text_dogfree, 'dog_free_shape', mask_cat)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ku"><img src="../Images/ca0ced0fcc66f429fecec7aee2f48095.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*WOFYLsuCC9IhDrqDWJmcwg.png"/></div></figure><p id="27b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们使用<a class="ae jd" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">自然语言工具包</a>对帖子中的文本进行分词和词干处理:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="0a3d" class="kj kk hi kf b fi kl km l kn ko"><strong class="kf hj">from</strong> nltk.tokenize <strong class="kf hj">import</strong> RegexpTokenizer <br/><strong class="kf hj">from</strong> nltk.stem.porter <strong class="kf hj">import</strong> PorterStemmer</span><span id="df3d" class="kj kk hi kf b fi kp km l kn ko"><strong class="kf hj">def</strong> steam_text(text):    <br/>    stemmer = PorterStemmer()<br/>    tokenizer = RegexpTokenizer(r'\w+')<br/>    text_token = tokenizer.<strong class="kf hj">tokenize</strong>(text.<strong class="kf hj">lower</strong>())  <br/>    <strong class="kf hj">return</strong> ' '.<strong class="kf hj">join</strong>(map(<strong class="kf hj">lambda</strong> x: stemmer.<strong class="kf hj">stem</strong>(x), text_token))</span><span id="ae56" class="kj kk hi kf b fi kp km l kn ko">df['stemmed_text'] = df['text'].<strong class="kf hj">apply</strong>(steam_text)</span></pre><p id="deac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建我们的特征矩阵(X)和目标向量(y ),并分割成训练和测试数据集:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="d942" class="kj kk hi kf b fi kl km l kn ko">X = df['stemmed_text'] <br/>y = df['subreddit_class']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, <br/>                                                    stratify=y, <br/>                                                    random_state=42)</span></pre><p id="e217" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于基线估计量，让我们取具有最多观察值的类，并使用该类作为所有预测的结果。在这种情况下，我们的基线模型精度将为0.5018或50.18%:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="bdcf" class="kj kk hi kf b fi kl km l kn ko">y_test.<strong class="kf hj">value_counts</strong>(normalize=True)<br/>1    0.501779<br/>0    0.498221<br/>Name: subreddit_class, dtype: float64</span></pre><p id="dd7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们使用Gridsearch为超参数调整训练3个模型。首先是逻辑回归:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="96ea" class="kj kk hi kf b fi kl km l kn ko">pipe_lr_tf = Pipeline([<br/>    ('tf', TfidfVectorizer()),<br/>    ('lr', LogisticRegression())<br/>])<br/><br/>params_lr_tf = {<br/>    'tf__stop_words':   [None, 'english'],<br/>    'tf__max_features': [1000, 2000, 3000, 4000], <br/>    'tf__ngram_range':  [(1, 1), (1, 2), (1, 3)], <br/>    'lr__penalty':      ['l2', 'l1'],<br/>    'lr__C':            [.5, .01, 1],<br/>    'lr__random_state': [42] <br/>}<br/>gs_lr_tf = GridSearchCV(pipe_lr_tf, <br/>                        param_grid=params_lr_tf,<br/>                        cv=5,<br/>                        n_jobs=2,<br/>                        verbose=1)<br/><br/>gs_lr_tf.fit(X_train, y_train)<br/><br/><strong class="kf hj">print</strong>(f'GridSearch Best Params: <strong class="kf hj">{gs_lr_tf.best_params_}</strong>')<br/><strong class="kf hj">print</strong>('Train Score:', gs_lr_tf.<strong class="kf hj">score</strong>(X_train, y_train))<br/><strong class="kf hj">print</strong>('Test Score:', gs_lr_tf.<strong class="kf hj">score</strong>(X_test, y_test))<br/><br/>GridSearch Best Params: {'lr__C': 1, 'lr__penalty': 'l2', 'lr__random_state': 42, 'tf__max_features': 1000, 'tf__ngram_range': (1, 2), 'tf__stop_words': 'english'}<br/>Train Score: 0.976232917409388<br/>Test Score: 0.9537366548042705</span></pre><p id="8e12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来让我们试试朴素贝叶斯:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="c7c5" class="kj kk hi kf b fi kl km l kn ko">pipe_multinomial_nb_cvec = Pipeline([<br/>    ('cvec', CountVectorizer()),<br/>    ('multinomial_nb', MultinomialNB())<br/>])<br/><br/>params_multinomial_nb_cvec = {<br/>    'cvec__stop_words':           [None, 'english'],<br/>    'cvec__max_features':         [100, 500, 1000, 2000, 3000, 4000], <br/>    'cvec__ngram_range':          [(1, 1), (1, 2), (1, 3)],<br/>    'multinomial_nb__alpha':     [0], <em class="kv"># 2, 5, .5, 1</em><br/>    'multinomial_nb__fit_prior': [<strong class="kf hj">True</strong>, <strong class="kf hj">False</strong>]<br/>}<br/><br/>gs_multinomial_nb_cvec = GridSearchCV(pipe_multinomial_nb_cvec, <br/>                        param_grid=params_multinomial_nb_cvec,<br/>                        cv=5,<br/>                        n_jobs=2,<br/>                        verbose=1)<br/>    <br/>gs_multinomial_nb_cvec.<strong class="kf hj">fit</strong>(X_train, y_train)<br/><br/><strong class="kf hj">print</strong>(f'GridSearch Best Params:{gs_multinomial_nb_cvec.best_params_}')<br/><strong class="kf hj">print</strong>('Train Score:', gs_multinomial_nb_cvec.<strong class="kf hj">score</strong>(X_train, y_train))<br/><strong class="kf hj">print</strong>('Test Score:', gs_multinomial_nb_cvec.<strong class="kf hj">score</strong>(X_test, y_test))</span><span id="2bd6" class="kj kk hi kf b fi kp km l kn ko">GridSearch Best Params: {'cvec__max_features': 1000, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'multinomial_nb__alpha': 0, 'multinomial_nb__fit_prior': False}<br/>Train Score: 0.9477124183006536<br/>Test Score: 0.9234875444839857</span></pre><p id="554c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，随机森林:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="8b5a" class="kj kk hi kf b fi kl km l kn ko">pipe_rf_tf = Pipeline([<br/>    ('tf', TfidfVectorizer()),<br/>    ('rf', RandomForestClassifier())<br/>])<br/><br/>params_rf_tf={<br/>    'tf__stop_words':   [None, 'english'],<br/>    'tf__max_features': [1000, 2000, 3000],<br/>    'tf__ngram_range':  [(1,1), (1, 2)],<br/>    'tf__smooth_idf':   [<strong class="kf hj">True</strong>, <strong class="kf hj">False</strong>],<br/>    'rf__n_estimators': [25, 50, 100], <br/>    'rf__criterion':    ['gini'], <em class="kv">#'entropy',</em><br/>    'rf__max_depth':    [3, 5], <br/>    'rf__random_state': [42]<br/>}<br/>gs_rf_tf = GridSearchCV(pipe_rf_tf,<br/>                        param_grid=params_rf_tf, <br/>                        cv=5,<br/>                        n_jobs=2,<br/>                        verbose=1)<br/><br/>gs_rf_tf.<strong class="kf hj">fit</strong>(X_train, y_train)<br/><br/><strong class="kf hj">print</strong>(f'GridSearch Best Params: {gs_rf_tf.best_params_}')<br/><strong class="kf hj">print</strong>('Train Score:', gs_rf_tf.<strong class="kf hj">score</strong>(X_train, y_train))<br/><strong class="kf hj">print</strong>('Test Score:', gs_rf_tf.<strong class="kf hj">score</strong>(X_test, y_test))</span><span id="b64b" class="kj kk hi kf b fi kp km l kn ko">GridSearch Best Params: {'rf__criterion': 'gini', 'rf__max_depth': 5, 'rf__n_estimators': 100, 'rf__random_state': 42, 'tf__max_features': 1000, 'tf__ngram_range': (1, 1), 'tf__smooth_idf': False, 'tf__stop_words': 'english'}<br/>Train Score: 0.9548425430778372<br/>Test Score: 0.9199288256227758</span></pre><p id="621b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们使用以下方法评估我们的模型:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="20ee" class="kj kk hi kf b fi kl km l kn ko"><strong class="kf hj">def</strong> conf_matrix(model, X_test, y_test):<br/>    y_hat = model.predict(X_test) <br/>    <em class="kv"># get confusion matrix data</em><br/>    cm = confusion_matrix(y_test, y_hat)  <br/>    <em class="kv"># extract elements of confusion matrix</em><br/>    tn, fp, fn, tp = cm.ravel()           <br/>    <br/>    <em class="kv"># Convert Confusion Matrix to DataFrame</em><br/>    cm_df = pd.DataFrame(cm, columns = ['Predict Dog Haters (y=0)', 'Predict Dog Lovers (y=1)'], index = ['Actual Dog Haters (y=0)', 'Actual Dog Lovers (y=1)'])<br/>    <strong class="kf hj">return</strong> cm_df<br/>    <br/><strong class="kf hj">def</strong> conf_matrix_metrics(model, X_test, y_test):<br/>    y_hat = model.predict(X_test) <br/>    cm = confusion_matrix(y_test, y_hat)  <br/>    tn, fp, fn, tp = cm.ravel()           <br/>    <br/>    <em class="kv">#calculate metrics </em><br/>    accuracy = (tp + tn) / (tn + fp + fn + tp)<br/>    sensitivity = tp / (tp + fn)<br/>    specificity = tn/ (tn + fp)<br/>    precision = tp /(tp + fp)<br/>    <br/>    <strong class="kf hj">return</strong> {'accuracy': np.round(accuracy,3),<br/>            'sensitivity': np.round(sensitivity,3),<br/>            'specificity': np.round(specificity,3),<br/>            'precision': np.round(precision,3)}</span></pre><p id="0a81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="e6fb" class="kj kk hi kf b fi kl km l kn ko"><strong class="kf hj">print</strong>(conf_matrix_metrics(gs_lr_tf, X_test, y_test))<br/><strong class="kf hj">conf_matrix</strong>(gs_lr_tf, X_test, y_test)</span><span id="a92c" class="kj kk hi kf b fi kp km l kn ko">{'accuracy': 0.954, 'sensitivity': 0.961, 'specificity': 0.946, 'precision': 0.948}</span><span id="90d7" class="kj kk hi kf b fi kp km l kn ko">+-------------------+---------------------+--------------------+<br/>|                   | Predict Dog Haters  | Predict Dog Lovers |<br/>|                   | (y=0)               | (y=1)              |<br/>+-------------------+---------------------+--------------------+<br/>| Actual Dog Haters |                 265 |                 15 |(y=0)<br/>| (y=0)             |                     |                    |      <br/>| Actual Dog Lovers |                  11 |                271 |<br/>| (y=1)             |                     |                    |<br/>+-------------------+---------------------+--------------------+</span></pre><p id="daf6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">朴素贝叶斯:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="2f46" class="kj kk hi kf b fi kl km l kn ko"><strong class="kf hj">print</strong>(conf_matrix_metrics(gs_multinominal_nb_cvec, X_test, y_test))<br/><strong class="kf hj">conf_matrix</strong>(gs_multinominal_nb_cvec, X_test, y_test)</span><span id="5eca" class="kj kk hi kf b fi kp km l kn ko">{'accuracy': 0.923, 'sensitivity': 0.947, 'specificity': 0.9, 'precision': 0.905}</span><span id="31f1" class="kj kk hi kf b fi kp km l kn ko">+-------------------+---------------------+--------------------+<br/>|                   | Predict Dog Haters  | Predict Dog Lovers |<br/>|                   | (y=0)               | (y=1)              |<br/>+-------------------+---------------------+--------------------+<br/>| Actual Dog Haters |                 252 |                 28 |<br/>| (y=0)             |                     |                    |      <br/>| Actual Dog Lovers |                  15 |                267 |<br/>| (y=1)             |                     |                    |<br/>+-------------------+---------------------+--------------------+</span></pre><p id="af6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="d745" class="kj kk hi kf b fi kl km l kn ko"><strong class="kf hj">print</strong>(conf_matrix_metrics(gs_rf_tf, X_test, y_test))<br/><strong class="kf hj">conf_matrix</strong>(gs_rf_tf, X_test, y_test)</span><span id="18e2" class="kj kk hi kf b fi kp km l kn ko">{'accuracy': 0.92, 'sensitivity': 0.901, 'specificity': 0.939, 'precision': 0.937}</span><span id="13a8" class="kj kk hi kf b fi kp km l kn ko">+-------------------+---------------------+--------------------+<br/>|                   | Predict Dog Haters  | Predict Dog Lovers |<br/>|                   | (y=0)               | (y=1)              |<br/>+-------------------+---------------------+--------------------+<br/>| Actual Dog Haters |                 263 |                 17 |<br/>| (y=0)             |                     |                    |      <br/>| Actual Dog Lovers |                  28 |                254 |<br/>| (y=1)             |                     |                    |<br/>+-------------------+---------------------+--------------------+</span></pre><p id="3de8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管我选择了高度相关的子数据集，但所有三个模型(逻辑回归、朴素贝叶斯和随机森林)都显示出比基线至少高1.7倍的准确度。</p><p id="cebb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个实验中，我们用逻辑回归得到了最好的结果。我们可以以大约95.4%的准确率预测一个给定的帖子来自哪里。让我们看看模型的特征重要性。对于爱狗人士，我们将系数降序排列:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="1495" class="kj kk hi kf b fi kl km l kn ko">pd.DataFrame(<br/>list(zip(gs_lr_tf.best_estimator_[0].<strong class="kf hj">get_feature_names</strong>(),(gs_lr_tf.best_estimator_[1].coef_[0]))),<br/>columns=['word','coef']).<strong class="kf hj">sort_values</strong>('coef',ascending=<strong class="kf hj">False</strong>).<strong class="kf hj">head</strong>(5)</span><span id="ed04" class="kj kk hi kf b fi kp km l kn ko">+---------+----------+<br/>|  Word   |   Coef   |<br/>+---------+----------+<br/>| help    | 5.765303 |<br/>| parti   | 4.812514 |<br/>| discuss | 4.729457 |<br/>| vet     | 2.615551 |<br/>| ani     | 2.071909 |<br/>+---------+----------+</span></pre><p id="b403" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于讨厌狗的人，我们将系数升序排列:</p><pre class="jf jg jh ji fd ke kf kg kh aw ki bi"><span id="3c34" class="kj kk hi kf b fi kl km l kn ko">pd.DataFrame(<br/>list(zip(gs_lr_tf.best_estimator_[0].<strong class="kf hj">get_feature_names</strong>(),(gs_lr_tf.best_estimator_[1].coef_[0]))),<br/> columns=['word','coef']).sort_values('coef',ascending=<strong class="kf hj">True</strong>).<strong class="kf hj">head</strong>(5)</span><span id="890e" class="kj kk hi kf b fi kp km l kn ko">+--------+-----------+<br/>|  Word  |   Coef    |<br/>+--------+-----------+<br/>| attack | -2.942621 |<br/>| dog    | -2.639061 |<br/>| anim   | -2.497448 |<br/>| hate   | -2.313820 |<br/>| shit   | -1.950404 |<br/>+--------+-----------+</span></pre><p id="d43e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的顶级stem预测工具:</p><ul class=""><li id="0e93" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">对于爱狗人士:帮助，参与，讨论，兽医，肛门</li><li id="4826" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">对于讨厌狗的人:攻击，狗，动画，仇恨，狗屎</li></ul></div></div>    
</body>
</html>