<html>
<head>
<title>Teacher-Class Network: A Neural Network Compression Mechanism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">教师课堂网络:一种神经网络压缩机制</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/teacher-class-network-512b4ecf1df?source=collection_archive---------18-----------------------#2020-05-05">https://medium.com/analytics-vidhya/teacher-class-network-512b4ecf1df?source=collection_archive---------18-----------------------#2020-05-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7fc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大量训练数据的可用性与非常强大的图形处理单元和一系列先进的深度神经网络架构相结合，使深度学习领域能够不断提高其准确性。然而，这些现有技术的网络具有大量的参数并且是资源密集型的，因此在诸如移动电话的资源不足的设备上部署这样的网络几乎是不切实际的。例如，VGG-16的一次向前传递需要超过528MB的存储器和超过16 GFLOPs。随后，非常需要具有相当精度的紧凑深度模型。</p><p id="1337" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">知识提取也称为师生网络，是神经网络压缩领域中最流行的技术之一，知识提取最初由Geoffrey Hinton、Oriol Vinyals、Jeff Dean提出，使用大型预训练网络(教师)来训练小型模型(学生)。假设学生将能够学习教师所学的基本概念和知识，否则学生将无法学习，因为它的架构更简单，参数数量更少。这种知识转移是通过最小化教师和学生产生的软标签(在较高温度下由softmax产生的概率)之间的损失来实现的。由于这些软标签仅包含关于将输入分类到各个类别的知识，学习这些软目标将学生网络限制为解决单个特定问题，使其依赖于问题。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/f18b22cb92695c526cd1cb6488e10a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJgEIQQOY0uZ6XivLWJJqA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">Geoffrey Hinton、Oriol Vinyals和Jeff Dean提出的知识升华。提取神经网络中的知识。arXiv预印本arXiv:1503.02531，2015。来源:<a class="ae jt" rel="noopener" href="/neuralmachine/knowledge-distillation-dc241d7c2322">https://medium . com/neural machine/knowledge-distillation-DC 241 d7c 2322</a></figcaption></figure><h1 id="90aa" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">概观</h1><p id="4f26" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在这篇博客中，我们将一步一步地指导你实现教师级网络，这是一种新颖的神经网络压缩机制。当与现有的教师-学生压缩技术相比较时，这种架构具有两个关键区别，(I)这种架构不是只有一个学生，而是使用多个学生来学习相互排斥的知识块，以及(ii)这种架构不是在教师的软标签(由Geoffrey Hinton等人提出的softmax在更高温度下产生的概率)上训练学生，而是试图学习密集的特征表示，从而使解决问题独立。每个学生需要学习的组块的大小取决于学生的数量。在所有的学生都被独立训练后，每个学生所学的知识被合并，输出层被应用。这些层可以从教师网络借用预先训练的权重，也可以进行微调，以进一步改善向学生传授知识时发生的损失。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/328267eb7a74315f58168c6aca0bbb55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2gwCm_dxoT96qtVSI67lKQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">过程概述:教师网学习的稠密特征表示<strong class="bd jw"> d </strong>被分成组块d₁，d₂… dₙ.然后，每个组块由单个学生学习，最后，来自所有学生的知识被合并并馈送到输出层以供最终决策。</figcaption></figure><h1 id="f32d" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">第一步</h1><h2 id="7d14" class="kx jv hi bd jw ky kz la ka lb lc ld ke iq le lf ki iu lg lh km iy li lj kq lk bi translated">从教师提取密集表示</h2><p id="99c3" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">神经网络通常会产生密集的特征表示<strong class="ih hj"> d </strong>，在分类的情况下，这些特征表示会被输入到名为logits的特定于类的神经元中，我们希望从教师网络中提取这些表示。一个为我们想要解决的特定问题而训练有素的大型先进网络被视为一个教师，我们从这个教师网络创建一个新网络，为了创建这个新网络，我们将删除第一个密集层之后的所有问题特定层，因此简单地说，这个新网络将为我们通过的每个输入示例输出一个密集向量，而不是我们之前获得的概率向量(在分类的情况下)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ll"><img src="../Images/cc8d38b86791784647b2d06add528ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TrrQXNQQ_CmwSaoTMF8Ztw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">样本教师网络</figcaption></figure><p id="21a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们有了这个网络，我们就需要给这个网络提供完整的训练数据，这样我们就可以得到每个训练样本的密集表示。让我们假设我们正在为时尚MNIST数据集训练一个<em class="lm">教师级</em>网络，该数据集总共有60，000个形状(28，28，1)的示例，并且我们使用的教师网络产生大小为256的密集表示。通过来自我们的新网络的这60，000个例子，我们获得了我们的形状转移数据集(60000，256)。</p><pre class="je jf jg jh fd ln lo lp lq aw lr bi"><span id="0599" class="kx jv hi lo b fi ls lt l lu lv"><em class="lm">#Collect the dense vector from the previous layer and store <br/>#it in a different model</em><br/>WO_Softmax=Model(teacher.input, teacher.get_layer('dense_1').output)<br/><em class="lm">#Extracting dense representation from the teacher network</em><br/>train_dense = WO_Softmax.predict(X_train)<br/>test_dense = WO_Softmax.predict(X_val)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ll"><img src="../Images/affb92291d682cf0aac35f50ceaae0f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e1yU50Dt1rZEP8qhA9bLjg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">用于获得学生转移集的改进的教师网络</figcaption></figure><p id="faef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们将知识转移的目标重新定义为训练一个小的学生模型，以模仿一个大型的预先训练的教师网络的密集特征。换句话说，目标是最小化教师网络的密集特征向量和学生网络产生的密集特征向量之间的重构误差，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/cbc92acc9abd96340f58899c3302a46f.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*uoXxdvYHHfwA76CMwyVu2A.png"/></div></figure><p id="f708" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中m是训练样本总数，<strong class="ih hj"> d </strong>和<strong class="ih hj"> d-hat </strong>分别是教师和学生网络的密集特征表示。一旦学生学会了重构密集要素制图表达，就可以引入输出图层(针对多类分类问题的特定于类的logit和softmax)来获得所需的输出。该输出层可以简单地是具有预训练权重的教师输出层。同样的策略可以扩展到多个学生网络，其中密集特征表示<strong class="ih hj"> d </strong>可以被分成多个组块，每个组块可以由独立的学生模型学习，如下面的部分所讨论的。</p><h1 id="fe9e" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">第二步</h1><h2 id="3aa0" class="kx jv hi bd jw ky kz la ka lb lc ld ke iq le lf ki iu lg lh km iy li lj kq lk bi translated">使用<em class="lx"> n </em>个学生学习密集表示</h2><p id="d7ff" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">密集特征向量<strong class="ih hj"> d </strong>通过将密集向量分割成<em class="lm"> n </em>等份而被分割成组块(因为我们有<em class="lm"> n </em>个学生网络)。因此，我们简单地将<strong class="ih hj"> d </strong>向量分成<em class="lm"> n </em>部分，如<strong class="ih hj">d</strong>=【d₁，d₂，…，dₙ】，其中所有的dₖ可以连接起来重新生成密集向量<strong class="ih hj"> d </strong>，并且每个dₖ将由kᵗʰ学生独立学习。在我们创建了<em class="lm"> n </em>个学生之后，我们在从教师网络中提取的密集表示的<em class="lm"> 1/n </em>的完整输入数据集上独立地训练每个学生。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/12835db29969f6c36d1596357b7e4126.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*UwqDcPHaFw9DwID2FVWE3Q.gif"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">将从老师那里提取的密集表示拆分到“n”个学生中</figcaption></figure><pre class="je jf jg jh fd ln lo lp lq aw lr bi"><span id="0563" class="kx jv hi lo b fi ls lt l lu lv"><em class="lm">#Splitting the training dense vector <br/>#among N students(in this case 4)</em><br/>s1Train=train_dense[:,:64]<br/>s2Train=train_dense[:,64:128]<br/>s3Train=train_dense[:,128:192]<br/>s4Train=train_dense[:,192:]<br/><em class="lm">#Splitting the test dense vector <br/>#among N students(in this case 4)</em><br/>s1Test=test_dense[:,:64]<br/>s2Test=test_dense[:,64:128]<br/>s3Test=test_dense[:,128:192]<br/>s4Test=test_dense[:,192:]</span></pre><p id="813a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们，假设我们有一组学生Sⁿ，Sₖ是该组中的kᵗʰ学生。从数学上来说，将知识从老师传递给n个学生可以定义为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/dab5eeacfdd3c59f605f08e72463ea28.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*ufPi8DrPA35YtBIJl5d2Zw.png"/></div></figure><p id="2478" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，<strong class="ih hj"> d-hatₖ </strong>是kᵗʰ学生提炼出来的知识。请注意，每个学生基本上都试图学习密集表示<strong class="ih hj"> dₖ </strong>的一部分，这是一个实值向量。每个学生通过最小化均方误差来学习其知识块，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ma"><img src="../Images/afc038346d53651d2582b2a98769e54c.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*y_q8s8NZGz5x8eMUHAcogw.png"/></div></figure><p id="6837" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，Lₖ是生成密集表示的kᵗʰ学生的损失<strong class="ih hj">d-hatₖ</strong>,‘m’是训练实例的总数，<strong class="ih hj"> dₖ </strong>是学生Sₖ必须学习的从教师网络获得的密集向量的组块。</p><h1 id="321d" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">第三步</h1><h2 id="938c" class="kx jv hi bd jw ky kz la ka lb lc ld ke iq le lf ki iu lg lh km iy li lj kq lk bi translated">结合学到的知识</h2><p id="94a1" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在所有的<em class="lm"> n </em>个学生被独立训练后，学习到的知识块或密集向量块<strong class="ih hj"> dₖ </strong>被串联在一起，以估计所有的<em class="lm"> n </em>个学生学习到的知识<strong class="ih hj"> d-hat </strong>，并被定义为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/b312995cc7558ed442d00ef09b2a7bf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*eKwAuNscTpnPrxx6zDc05Q.png"/></div></figure><pre class="je jf jg jh fd ln lo lp lq aw lr bi"><span id="8d41" class="kx jv hi lo b fi ls lt l lu lv"><em class="lm">#Combining learned chunks of knowledge</em><br/>model_input = Input(shape=(28,28,1))<br/>o1=student1.output<br/>o2=student2.output<br/>o3=student3.output<br/>o4=student4.output<br/>output=concatenate([o1,o2,o3,o4])<br/>#Applying problem/output specific layers<br/>output2=Dropout(0.5)(output) <em class="lm"># For reguralization</em><br/>output3=Dense(10)(output2)<br/>output4=Activation('softmax')(output3)<br/>multi_model=Model([student1.input,student2.input,student3.input,student4.input],output4)<br/><br/>multi_model.summary()</span></pre><p id="ecd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中[…]是连接，X是输入数据，如图像或文本。理想情况下，向量<strong class="ih hj"> d-hat </strong>应该类似于从教师网络中提取的密集表示<strong class="ih hj"> d </strong>，并且可以用于解决教师正在解决的问题。在组合了知识块之后，我们需要添加我们从原始教师网络中移除的层，以创建密集向量生成教师网络，因为本质上，学生现在已经集体学习了教师的密集表示，并且在馈送到教师网络的输出层时，理论上应该给出与教师网络相同的结果。因此，在分类的情况下，softmax可以如下生成概率向量:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/c0421265f122ceffb8e8b31871cff959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*pfu4qLE1Y2bf2xhke_PDKA.png"/></div></figure><p id="bfc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，函数“g”是应用于来自所有预训练学生的输出的连接的输出层(特定于类的logit和softmax ),并且<strong class="ih hj"> θg </strong>是其权重，其也可以是从教师网络获得的预训练权重。老师<strong class="ih hj"> d </strong>和n个学生<strong class="ih hj"> d-hat </strong>所学的知识可能会有微小的错误。为了补偿这种误差并提高学生的整体准确性，可以在保持学生不可训练的同时对该输出层进行微调。因此，在分类的情况下，只有最后一个输出层可以使用交叉熵损失函数来优化，如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/5fdb2d52cda835fd43f848767e2656e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*R0aKV2kdTFfct0zJvuw6vA.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es md"><img src="../Images/53398ba14df8f743341b987b76cf8509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KbrKk8gmDRP7nP7YN6RzfQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">将问题/输出特定层应用于组合知识</figcaption></figure></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><blockquote class="ml mm mn"><p id="4c09" class="if ig lm ih b ii ij ik il im in io ip mo ir is it mp iv iw ix mq iz ja jb jc hb bi translated"><em class="hi">相关的python代码可以在我的GitHub资源库中找到</em>:<a class="ae jt" href="https://github.com/shaiqmalik/Teacher-Class.git" rel="noopener ugc nofollow" target="_blank">https://github.com/shaiqmalik/Teacher-Class.git</a></p></blockquote><blockquote class="mr"><p id="3b9a" class="ms mt hi bd mu mv mw mx my mz na jc dx translated">这篇博文遵循了论文<a class="ae jt" href="https://arxiv.org/abs/2004.03281" rel="noopener ugc nofollow" target="_blank">“教师课堂网络:一种神经网络压缩机制”</a>中提出的机制</p></blockquote><h2 id="2507" class="kx jv hi bd jw ky nb la ka lb nc ld ke iq nd lf ki iu ne lh km iy nf lj kq lk bi translated"><em class="lx">参考文献</em></h2><p id="f41f" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated"><a class="ae jt" href="https://arxiv.org/abs/1503.02531v1" rel="noopener ugc nofollow" target="_blank"> <em class="lm">提取神经网络中的知识</em> </a></p></div></div>    
</body>
</html>