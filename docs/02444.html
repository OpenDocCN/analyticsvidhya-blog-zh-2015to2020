<html>
<head>
<title>10 Questions you can expect in Spark Interview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Spark面试中你会遇到的10个问题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/10-questions-you-can-expect-in-spark-interview-24b89b807dfb?source=collection_archive---------1-----------------------#2019-12-17">https://medium.com/analytics-vidhya/10-questions-you-can-expect-in-spark-interview-24b89b807dfb?source=collection_archive---------1-----------------------#2019-12-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="83f6" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">Apache Spark访谈中常见的问题</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/ada7e8653423b9f2a675128583e305e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*Ia_HE0ajwzMizANGH_ZQrA.png"/></div></figure><p id="5f1f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">嘿伙计们，</p><p id="12e3" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">随着Apache Spark成为批处理和ETL的尖端技术，数据工程师的职位在最近非常吃香，了解它可以很容易地让你找到一份数据工程师的工作。因此，在本文中，我将展示Apache Spark访谈中的10个问题，请注意，我不会包括诸如“什么是Dataframe？”，“什么是火花RDD？”或者“如何读/写orc文件？”正如我所料，一个去Apache Spark面试的同事应该已经知道这些事情，再重复这些事情是没有意义的。</p><p id="08f9" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">所以这些都说了，还是跳到Q/A吧。</p><h2 id="4d0d" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jo km kn ko js kp kq kr jw ks kt ku kv bi translated">Spark比Hadoop好吗？为什么？</h2><p id="b0b5" class="pw-post-body-paragraph jf jg hi jh b ji kw ij jk jl kx im jn jo ky jq jr js kz ju jv jw la jy jz ka hb bi translated">是的，Spark明显优于Hadoop，主要原因之一是它比Hadoop更快，因为内存处理有助于减少读/写操作的延迟。基本上，当我们使用map reduce范式时，在完成每个任务时，都会在磁盘上写入数据，当需要再次使用数据时，会再次执行读取操作。但是，在Spark中，处理将在内存中完成，数据帧将被缓存以备将来使用，从而提高性能。此外，Spark附带了Spark ML、Spark SQL、Spark Streaming等库，这使它更加丰富。</p><h2 id="0a1e" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jo km kn ko js kp kq kr jw ks kt ku kv bi translated">联合和再分配的区别是什么？</h2><p id="bc37" class="pw-post-body-paragraph jf jg hi jh b ji kw ij jk jl kx im jn jo ky jq jr js kz ju jv jw la jy jz ka hb bi translated">当谈到优化你的spark工作时，这是讨论的热门话题。这两个函数基本上都允许我们操纵数据帧的分区数量，但是它们的用途是不同的。重新分区将对数据进行完全洗牌，因此我们可以增加或减少分区的数量，但联合只会将数据从一个分区转移到另一个分区，从而只会减少使用它的分区的数量。合并会更快，因为混洗会更少，但是如果分区的数量必须增加或者数据是倾斜的，并且我们希望通过重新混洗来减少分区的数量，那么我们应该使用重新分区方法。</p><h2 id="1a33" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jo km kn ko js kp kq kr jw ks kt ku kv bi translated">什么是广播加入？</h2><p id="5367" class="pw-post-body-paragraph jf jg hi jh b ji kw ij jk jl kx im jn jo ky jq jr js kz ju jv jw la jy jz ka hb bi translated">广播连接也用于优化Spark作业(尤其是连接)。当小数据帧与相对较大的数据帧连接时，我们可以广播小数据帧，这将向每个节点发送小数据帧的副本，这将导致更快的连接执行和更少的洗牌。下面给出了语法。</p><pre class="iy iz ja jb fd lb lc ld le aw lf bi"><span id="74eb" class="kb kc hi lc b fi lg lh l li lj">import pyspark.sql.functions as fn<br/>final = big.join(fn.broadcast(small),["common_id"])</span></pre><p id="9c0d" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当广播较小的数据帧时，我们可以将其分区减少到1以获得更好的性能(取决于您的使用情况)。</p><h2 id="2e5e" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jo km kn ko js kp kq kr jw ks kt ku kv bi translated">什么是懒评？</h2><p id="e06a" class="pw-post-body-paragraph jf jg hi jh b ji kw ij jk jl kx im jn jo ky jq jr js kz ju jv jw la jy jz ka hb bi translated">Apache Spark有两个重要的方面，一是行动，二是转化。Transformation包括filter、where、when等函数，调用这些函数时，Spark并不实际执行这些转换，而是堆叠起来，直到调用一个动作。当一个动作被调用时，所有的转换都在此时执行，这有助于Apache Spark优化作业的性能。操作示例有show()、count()、collect()。</p><h2 id="eed7" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jo km kn ko js kp kq kr jw ks kt ku kv bi translated">cache()和persist()有什么区别？</h2><p id="df8a" class="pw-post-body-paragraph jf jg hi jh b ji kw ij jk jl kx im jn jo ky jq jr js kz ju jv jw la jy jz ka hb bi translated">这两个Api都用于在不同级别的内存中持久存储数据帧，但在持久存储中，我们可以将存储级别指定为MEMORY_ONLY、MEMORY_AND_DISK、DISK_ONLY等，而在cache()中，我们不能指定存储级别，默认情况下被视为MEMORY_ONLY。</p><h2 id="3e85" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jo km kn ko js kp kq kr jw ks kt ku kv bi translated">秩和密秩的区别？</h2><p id="bba1" class="pw-post-body-paragraph jf jg hi jh b ji kw ij jk jl kx im jn jo ky jq jr js kz ju jv jw la jy jz ka hb bi translated">这是一个sql问题，但我把它包括在内，因为如果我们进入窗口分区部分，我们会遇到这个问题。假设我们有一个如下所示的数据集:</p><pre class="iy iz ja jb fd lb lc ld le aw lf bi"><span id="5a8c" class="kb kc hi lc b fi lg lh l li lj">Name  Salary  Rank  Dense_rank<br/>Abid  1000     1      1<br/>Ron   1500     2      2<br/>Joy   1500     2      2<br/>Aly   2000     4      3<br/>Raj   3000     5      4</span></pre><p id="19aa" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这里的薪水是递增的，我们得到的是数据集的rank()和dense_rank()。因为Ron和Joy有相同的薪水，所以他们得到相同的等级，但是rank()会留下一个洞并保持“3”为空，而dense_rank()会填充所有的空隙，即使遇到相同的值。</p><h2 id="3005" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jo km kn ko js kp kq kr jw ks kt ku kv bi translated">如何通过Spark SQL连接Hive？</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lk"><img src="../Images/7a94bee7ad4068d4bf6337c32e02f64f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*pLnszelEZuWaBOeUWjsXxg.png"/></div></figure><p id="029f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对此的解决方案是将hive-site.xml和core-site.xml复制到spark conf文件夹中，这将为spark job提供有关hive metastore的所有必需元数据，您必须启用Hive支持，并在配置中指定Hive的仓库目录位置，同时启动Spark会话，如下所示:</p><pre class="iy iz ja jb fd lb lc ld le aw lf bi"><span id="c90c" class="kb kc hi lc b fi lg lh l li lj">spark = SparkSession \<br/>    .builder \<br/>    .appName("Python Spark SQL Hive integration example") \<br/>    .config("spark.sql.warehouse.dir", warehouse_location) \<br/>    .enableHiveSupport() \<br/>    .getOrCreate()</span></pre><p id="ec9a" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">点击阅读关于此次访问的详细信息<a class="ae ll" href="https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html" rel="noopener ugc nofollow" target="_blank"/></p><h2 id="eb01" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jo km kn ko js kp kq kr jw ks kt ku kv bi translated">RDD比Dataframes好吗？</h2><p id="06bd" class="pw-post-body-paragraph jf jg hi jh b ji kw ij jk jl kx im jn jo ky jq jr js kz ju jv jw la jy jz ka hb bi translated">不，数据帧在执行上比rdd更快，在语法上也更容易。你可能会怀疑数据帧在后端被转换成RDD，那么RDD有多慢？答案是Dataframe使用Catalyst Optimizer，这使它比rdd运行得更快，另一方面，rdd在执行期间不使用任何优化器。这些Api之间的比较已经在2017 Spark Summit的<a class="ae ll" href="https://www.youtube.com/watch?v=Ofk7G3GD9jk" rel="noopener ugc nofollow" target="_blank">这个视频</a>中详细解释过了。</p><h2 id="3dd0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jo km kn ko js kp kq kr jw ks kt ku kv bi translated">如何在Spark中读取一个xml文件？</h2><p id="d1d4" class="pw-post-body-paragraph jf jg hi jh b ji kw ij jk jl kx im jn jo ky jq jr js kz ju jv jw la jy jz ka hb bi translated">这很简单，spark有<a class="ae ll" href="https://mvnrepository.com/artifact/com.databricks/spark-xml_2.10/0.2.0" rel="noopener ugc nofollow" target="_blank"> spark-xml </a>包，它允许我们将xml文件解析为数据帧，考虑下面给出的xml文件:</p><pre class="iy iz ja jb fd lb lc ld le aw lf bi"><span id="1a8a" class="kb kc hi lc b fi lg lh l li lj">&lt;person&gt;<br/>    &lt;name&gt;John&lt;/name&gt;<br/>    &lt;address&gt;Some more Data&lt;/address&gt;<br/>&lt;/person&gt;</span></pre><p id="19e1" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">然后，为了读取，我们可以在读取期间指定模式和根标签，如下所示:</p><pre class="iy iz ja jb fd lb lc ld le aw lf bi"><span id="b740" class="kb kc hi lc b fi lg lh l li lj">xmlDF = spark.read<br/>      .format("com.databricks.spark.xml")<br/>      .option("rootTag", "person")<br/>      .option("rowTag", "name")<br/>      .option("rowTag", "address")      <br/>      .xml("yourFile.xml")</span></pre><p id="9cdc" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这将为您提供以“姓名”和“地址”为列的数据框架。</p><h2 id="47e9" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jo km kn ko js kp kq kr jw ks kt ku kv bi translated">在纱线模式下运行Spark时，需要在纱线集群的所有节点上安装Spark吗？</h2><p id="f7a8" class="pw-post-body-paragraph jf jg hi jh b ji kw ij jk jl kx im jn jo ky jq jr js kz ju jv jw la jy jz ka hb bi translated">不，当通过YARN模式提交作业时，没有必要在所有节点上安装Spark，因为Spark运行在YARN之上，并使用YARN引擎来获得所有需要的资源，我们只需在一个节点上安装Spark。点击阅读更多关于纱线部署模式<a class="ae ll" href="https://spark.apache.org/docs/latest/running-on-yarn.html" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><p id="0f1d" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">所以，这就是所有人希望你发现我的文章有帮助。一定要看看我以前关于Spark Delta的文章，其中我解释了Spark上的酸性物质。直到那时再见！</p><div class="lm ln ez fb lo lp"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/spark-delta-lake-d05dd480287a"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hj fi z dy lu ea eb lv ed ef hh bi translated">火花三角洲湖</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">嘿伙计们，</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">medium.com</p></div></div><div class="ly l"><div class="lz l ma mb mc ly md jd lp"/></div></div></a></div></div></div>    
</body>
</html>