<html>
<head>
<title>Implementation of Stochastic Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机梯度下降的实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementation-of-stochastic-gradient-descent-1d36b6a0c013?source=collection_archive---------9-----------------------#2020-06-29">https://medium.com/analytics-vidhya/implementation-of-stochastic-gradient-descent-1d36b6a0c013?source=collection_archive---------9-----------------------#2020-06-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/3251f5860ebb3e704dc566a896a73af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/0*8v3dNTb2HAvq42i9.jpg"/></div></figure><p id="c84d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">写这篇文章的目的是为了理解梯度下降背后的数学原理。我们大多数人在机器学习中使用梯度下降，但我们需要理解它背后的数学原理。</p><p id="dd3c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">作为一个大一新生，我在学习随机梯度下降的时候，发现它有点复杂。在这里，我试图让那些想知道它是如何工作的人更简单。</p><p id="0a74" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我这篇文章的重点是展示梯度下降背后的数学原理。</p><p id="38df" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">快速复习一下什么是梯度下降？</p><p id="0a95" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">梯度下降:这是一种优化技术，用于寻找使输出误差最小化的函数系数。</p><p id="9424" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">梯度下降程序:</strong></p><ol class=""><li id="407c" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated">初始化系数的值(可以是0.0或小的随机值)</li><li id="f5ba" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">通过将系数代入函数来计算成本函数</li><li id="10ab" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">计算总误差相对于重量的偏导数</li><li id="add6" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">更新系数的值</li><li id="d7bb" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">重复上述过程，直到我们得到成本0.0或无法实现成本的进一步提高</li></ol><p id="aef7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在我们跳到随机梯度下降的实现之前，我们将看到梯度下降的方法。</p><figure class="jz ka kb kc fd ij er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es jy"><img src="../Images/384374f3d278a7b4b0e1270675e65c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DeSEDxtd68Zfhjv0"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">梯度下降法</figcaption></figure><ul class=""><li id="60b6" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj kl jq jr js bi translated"><strong class="io hj">批量梯度下降:</strong>在批量梯度下降中，系数是更新的，在计算训练集中每个训练样本的损失后，参数更新一次，即在所有训练样本都被评估后</li><li id="2141" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj kl jq jr js bi translated"><strong class="io hj">随机梯度下降:</strong>当我们有大量的数据时，我们可以使用梯度下降的一种变体，称为随机梯度下降。在SGD中，系数是为每个训练实例更新的，而不是在批处理实例结束时更新的。</li></ul></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><p id="868e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">充分利用梯度下降算法的几种方法:</strong></p><blockquote class="kt ku kv"><p id="f1d0" class="im in kw io b ip iq ir is it iu iv iw kx iy iz ja ky jc jd je kz jg jh ji jj hb bi translated"><strong class="io hj">绘制成本值:</strong>收集并绘制算法为每次迭代计算的成本值。如果每次迭代的成本没有降低，降低你的学习率</p><p id="6dd3" class="im in kw io b ip iq ir is it iu iv iw kx iy iz ja ky jc jd je kz jg jh ji jj hb bi translated"><strong class="io hj">学习率:</strong>学习率值应为0.1、0.001或0.0001等小值。尝试不同的值，看看哪一个效果最好</p><p id="34de" class="im in kw io b ip iq ir is it iu iv iw kx iy iz ja ky jc jd je kz jg jh ji jj hb bi translated"><strong class="io hj">重新调整输入:</strong>将所有输入变量重新调整到相同的范围，例如0到1之间</p></blockquote></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><p id="96ff" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">随机梯度下降是机器学习中广泛使用的算法。在这里，我演示了如何通过最小化训练数据集的误差，使用随机梯度下降来学习线性回归模型的系数。</p><p id="75c6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们看看下面的例子:</p><p id="70ad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">数据:</strong></p><figure class="jz ka kb kc fd ij er es paragraph-image"><div class="er es la"><img src="../Images/8d247cf837e488de1128329b24227d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/0*4Sjuj5vVbE-vyOyv"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">抽样资料</figcaption></figure><p id="0e58" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我用excel显示了计算结果。</p><ol class=""><li id="b974" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated"><em class="kw">将所有输入变量重新调整到相同的范围，例如0到1之间。这里，我们使用最小-最大标准化来重新调整数据。</em></li></ol><p id="8612" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">计算变量的最小值和最大值。</p><figure class="jz ka kb kc fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/4a9292a29bbc3d1e3ac570e0787ddc41.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/0*LB0fSe0nXKwSczqn"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">计算变量的最小值和最大值</figcaption></figure><p id="a1cd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用下面的公式将数据标准化。</p><figure class="jz ka kb kc fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/f615f1a09a06b53e61fa729b96c0e952.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/0*90eHcQSU07BN1s5x"/></div></figure><figure class="jz ka kb kc fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/8cc5a5d6dbd53edab2084b2748925faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/0*2phArJ5edA9Cnynv"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">对我们的数据应用最小-最大标准化</figcaption></figure><p id="ae94" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kw"> 2。让我们从初始化随机权重开始。</em> a= 0.45，b=0.75</p><figure class="jz ka kb kc fd ij er es paragraph-image"><div class="er es le"><img src="../Images/9e437a8e72696a063d7bd175cdea9c97.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/0*LAujJpV6Vao24lgf"/></div></figure><p id="4852" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kw"> 3。计算ŷ </em></p><p id="30bb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">考虑来自新缩放数据的第二个随机观察</p><p id="2e23" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> X新= 0.22222222，Y新= 0.097087379 </strong></p><p id="c063" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> ŷ = a + b*X新</strong></p><p id="269c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">ŷ= 0.45+0.75 * 0.22222222 = 0.61666</strong></p><p id="c1f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kw"> 4。计算损失</em></p><p id="20c9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">误差= ŷ- y纽= 0.6166666–0.0970873 = 0.519579</strong></p><p id="7be8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kw"> 5。计算偏微分</em></p><p id="f7a0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">计算总误差相对于重量的偏导数</p><p id="fc63" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">部分差异如下:</p><p id="8873" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">总误差w.r.t. a(权重)的偏导数</strong></p><p id="bf1b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"/></p><p id="e8f6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> = ∂(ŷ- y new)/∂a </strong></p><p id="fa38" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> = ∂(ŷ- (a+b*X new))/∂a </strong></p><p id="e705" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> = -(ŷ-y新)</strong></p><p id="7af5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> =-0.519579 </strong></p><p id="c19e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">总误差w.r.t. b(权重)的偏导数</strong></p><p id="8d4b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> = ∂E/∂b </strong></p><p id="97bc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> = ∂(ŷ- y new)/∂b </strong></p><p id="61f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> = ∂(ŷ- (a+b*X new))/∂b </strong></p><p id="ea5f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> =-(ŷ-y新)*X新</strong></p><p id="68f8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">=-0.519579 * 0.22222222</strong></p><p id="87a5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> = -0.115462064 </strong></p><p id="1ead" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kw"> 6。更新重量</em></p><p id="2a66" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">更新“a”</p><p id="e501" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用较小的学习率(alpha=0.01)</p><p id="febd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">新a =旧a —阿尔法* ∂E/∂a </strong></p><p id="8bd5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">= 0.45–0.01 *—0.519579</strong></p><p id="93b6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> = 0.455196 </strong></p><p id="fa9f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">更新“b”</p><p id="b515" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">新b =旧b — alpha*∂E/∂b </strong></p><p id="bfb0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">= 0.75–0.01 *-0.115462064</strong></p><p id="57da" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> = 0.75115 </strong></p><p id="4b61" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们刚刚完成了随机梯度下降的第一次迭代，我们已经将权重更新为<strong class="io hj"> a = </strong> <strong class="io hj"> 0.455196，b= 0.455196。</strong>对数据集中剩余的实例重复这个过程。</p><p id="9818" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kw"> 7。迭代次数:5 </em></p><p id="41f5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们跳到前面。以下是5次迭代中所有系数的更新值列表。</p><figure class="jz ka kb kc fd ij er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lf"><img src="../Images/95f4f03bba969a9e653316fab93c3bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MqRWXNBPRcSxNeE1"/></div></div></figure><p id="8fba" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上图中的黄框是每组系数的误差。它向我们展示了误差随着每次迭代而减少。我们可以在简单的线性回归模型中使用误差最小的系数来预测数据集中的每个点。</p><p id="24bc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">希望现在用SGD实现线性回归已经很清楚了。建议你自己试试上面的计算。</p><p id="0d02" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">感谢您的阅读。</p><p id="cbf2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">快乐学习！！！:)</p></div></div>    
</body>
</html>