<html>
<head>
<title>Polynomial Regression with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带Keras的多项式回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/polynomial-regression-with-keras-ef1797b39b88?source=collection_archive---------2-----------------------#2020-01-05">https://medium.com/analytics-vidhya/polynomial-regression-with-keras-ef1797b39b88?source=collection_archive---------2-----------------------#2020-01-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d98b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多项式回归是用于预测建模以及分类问题的机器学习和神经网络的基础。</p><h2 id="72a9" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">回归</h2><p id="44c4" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">回归就是寻找数据的趋势(变量之间的关系)。这使我们能够更好地理解数据分布，并预测新输入变量的值。</p><h2 id="a5fd" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">线性回归</h2><p id="411e" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">当数据中的趋势为<strong class="ih hj">线性、</strong>时，即其形式为<code class="du kd ke kf kg b">y = ax + b</code>时，使用线性回归。另一种更常用的回归形式是多项式回归。</p><h2 id="7a70" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">多项式回归</h2><p id="b640" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">当数据趋势是“n”次多项式时使用的多项式回归，即其形式为y = ax^n + bx^n-1+ … + n</p><p id="8f0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们进入使用Keras进行多项式回归建模的代码。第一个块显示了本教程所需的所有库。</p><pre class="kh ki kj kk fd kl kg km kn aw ko bi"><span id="b965" class="jd je hi kg b fi kp kq l kr ks">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from tensorflow.keras.layers import Input, Dense<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.optimizers import Adam<br/>from sklearn.preprocessing import PolynomialFeatures</span></pre><p id="70a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们生成数据。如你所见，我们在这里使用了一个三次方程<strong class="ih hj">。</strong>我们也在0-1的范围内重新调整数据。否则，y值以千计，导致训练时间过长。即使在500个时代之后，损失也是以千计。</p><pre class="kh ki kj kk fd kl kg km kn aw ko bi"><span id="600a" class="jd je hi kg b fi kp kq l kr ks"><em class="kt">def</em> generate_data():<br/>    X = np.arange(-30, 30, 1)<br/>    y = 9*X**3 + 5*X**2 + np.random.randn(60)*1000<br/>    return X, y</span><span id="b238" class="jd je hi kg b fi ku kq l kr ks">trX, trY = generate_data()<br/>trX = trX/max(trX)<br/>trY = trY/max(trY)<br/>#plot the data<br/>fig, ax = plt.subplots()<br/>ax.scatter(trX, trY)</span></pre><figure class="kh ki kj kk fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kv"><img src="../Images/9c63e1f5c92cdeb861e14cd017fc56d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4MSUG6V_RT_EetVFsXvWpw.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">生成的数据</figcaption></figure><p id="ce65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型期望输入的要素与曲线的阶数一样多。因此，我们必须从数据中生成更多的特征。Scikit-learn有这样一个方法。</p><pre class="kh ki kj kk fd kl kg km kn aw ko bi"><span id="fac4" class="jd je hi kg b fi kp kq l kr ks">n = 3<br/>trX_expanded = np.expand_dims(trX, axis=1)<br/>poly = PolynomialFeatures(n)</span><span id="a856" class="jd je hi kg b fi ku kq l kr ks"># returns: [1, x, x^2, x^3]<br/>trX_expanded = poly.fit_transform(trX_expanded)</span></pre><p id="e27d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们尝试使用一个线性模型。模型的形式是wx + b。</p><pre class="kh ki kj kk fd kl kg km kn aw ko bi"><span id="f6b8" class="jd je hi kg b fi kp kq l kr ks">inp = Input((1)) <br/>out = Dense(1)(inp)<br/>model_linear = Model(inputs=inp, outputs=out)<br/>model_linear.compile(<br/>optimizer=Adam(lr=1e-3), <br/>loss="mean_squared_error")</span></pre><p id="9151" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们训练模型，并绘制其曲线。</p><pre class="kh ki kj kk fd kl kg km kn aw ko bi"><span id="6f2c" class="jd je hi kg b fi kp kq l kr ks">model_linear.fit(trX, trY, epochs=500)<br/>ax.scatter(trX, trY)<br/>ax.plot(trX, model_linear.predict(trX), color="red")</span></pre><figure class="kh ki kj kk fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lh"><img src="../Images/c3ea04f05750bc18705a12dd1a0dd6d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XcZmDWaA_v-z1NQ7qGM3AQ.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">试图将曲线与数据拟合的线性模型</figcaption></figure><p id="7723" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们试试多项式回归。</p><pre class="kh ki kj kk fd kl kg km kn aw ko bi"><span id="a41b" class="jd je hi kg b fi kp kq l kr ks">inp = Input((n+1)) <br/>#since one of the features is 1, we need an extra input<br/>out = Dense(1)(inp)<br/>model = Model(inputs=inp, outputs=out)<br/>model.compile(optimizer=Adam(lr=1e-3), loss="mean_squared_error")</span></pre><p id="9e83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练模型，并绘制数据集的最终预测:</p><pre class="kh ki kj kk fd kl kg km kn aw ko bi"><span id="1b27" class="jd je hi kg b fi kp kq l kr ks">model.fit(trX_expanded, trY, epochs=500)<br/>ax.scatter(trX, trY)<br/>ax.plot(trX, model.predict(trX_expanded), color="red")</span></pre><figure class="kh ki kj kk fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es li"><img src="../Images/45b292e9a22b85967749a7a3e13b6af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*utTcpnsWs0F-Tm3M9TyfoA.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">多项式回归曲线</figcaption></figure><p id="5348" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所见，在这种情况下，多项式回归要好得多，因为数据趋势不是线性的。其他这样的情况可以是基于某些因素预测房子的价格等。然而，在某些情况下，线性回归更适合。例如，预测一次驾驶要消耗多少燃料，或者找出一个电器的年龄和它的价格之间的关系。</p><p id="fdb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要注意的一件事是过度拟合，当模型过于完美地拟合曲线时就会发生这种情况。这听起来可能是一件好事，但实际情况是模型太习惯于训练数据。当我们向它展示新数据时，特定于训练数据的曲线不会工作得太好。</p><h2 id="917b" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">真实世界的例子</h2><p id="b585" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated"><a class="ae lj" href="https://drive.google.com/file/d/1tNL4jxZEfpaP4oflfSn6pIHJX7Pachm9/view" rel="noopener ugc nofollow" target="_blank">这里的</a>是一个虚构的公司职位及其薪水的数据集。让我们阅读并绘制数据。</p><pre class="kh ki kj kk fd kl kg km kn aw ko bi"><span id="b0b5" class="jd je hi kg b fi kp kq l kr ks">import pandas as pd<br/>filepath = "path/to/dataset.csv"<br/>data_orig = pd.read_csv(filepath)<br/>x_data = data_orig["level"].values<br/>y_data = data_orig["salary"].values<br/>x_data = x_data/max(x_data)<br/>y_data = y_data/max(y_data)<br/>x_data_expanded = poly.fit_transform(np.expand_dims(x_data))<br/>fig2, ax2 = plt.subplots()<br/>ax2.scatter(x_data, y_data) #plot the data</span></pre><figure class="kh ki kj kk fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lk"><img src="../Images/9c2a6613c4cdfbf785ca07ec9d06bd6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AHoggvSimYHTAviVdzMtvg.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">给定数据集</figcaption></figure><p id="11dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们根据数据绘制出我们的模型预测的结果:</p><pre class="kh ki kj kk fd kl kg km kn aw ko bi"><span id="b8f5" class="jd je hi kg b fi kp kq l kr ks">ax2.plot(x_data, model.predict(x_data_expanded), color="red")</span></pre><figure class="kh ki kj kk fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ll"><img src="../Images/df90c1fabcb110bad59d26bff11fd71b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T0LPLtaP5HIa4lGAvir35Q.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">如果我们用这些数据重新训练模型，曲线很容易得到改善。</figcaption></figure><p id="01f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多项式回归教程到此结束。它教授机器学习背后的核心概念，是该领域的良好起点！</p></div></div>    
</body>
</html>