# 机器学习中的特征选择技术

> 原文：<https://medium.com/analytics-vidhya/feature-selection-techniques-in-machine-learning-3f28abe92451?source=collection_archive---------9----------------------->

改变模型性能的技术

![](img/c75ba61895044ba1d7735351b14b32ce.png)

照片由 KDnuggets 提供

许多人仍然认为数据科学/机器就是执行算法，你可以得到结果。是的，当然，你会得到结果，但这些结果够好吗？或者你有没有注意到，在任何数据科学/机器学习竞赛中，为什么只有少数人是获胜者？大家都在用的算法屈指可数，那么区别在哪里呢？

答案很简单，就是**数据**。你正在运行的机器学习算法的数据是有所不同的。

**有一个简单的 ML 规则——“垃圾进垃圾出”，这就是为什么人们需要非常关注被馈送到 ML 模型的数据。**这就是特征选择的由来。

此外，随着最近大数据的增长，我们可以访问大量数据，但大多数数据都是嘈杂的，包含一些完全不相关、无关紧要和不重要的特征。这不仅影响模型的精度，而且需要大量的计算资源。

因此，特征选择在任何机器学习管道中都是非常关键的，因为它将移除大多数不相关的、冗余的和有噪声的特征，并且仅保留相关的特征。**这一过程将有助于提高模型精度，减少计算资源，增加模型的互操作性。**

如果这个关于特征选择的简短介绍给了我们足够的理由来学习机器学习中的这项重要技术，那么让我们对此进行更深入的挖掘，探索更多的特征选择。

在本帖中，我们将介绍-

1.什么是特征选择？

2.特征选择的重要性

3.特征选择、特征工程和降维

4.特征选择方法

5.特征选择技术的稳定性

# 什么是特征选择？

**特征选择(也称为变量选择或属性选择)是一种预处理技术，通过移除不相关和冗余的特征来从数据集中选择重要的特征，以提高机器学习算法在准确性和建立模型的时间方面的性能。**

不相关或部分相关的特征会对模型性能产生负面影响。因此，特征选择的重点是从输入中选择变量的子集，该子集可以有效地描述输入数据，同时减少噪声或不相关变量的影响，并且仍然提供良好的预测结果。

# 特征选择的重要性

## 1.提高准确性

一旦您删除了不相关的和冗余的数据，并且只将最重要的特征输入到 ML 算法中，它就提高了准确性。

## 2.减少过度拟合

通过特征选择，数据的冗余性降低，这意味着模型基于噪声进行预测的机会减少。因此，这将减少模型过拟合，并且模型将很好地概括。

## 3.减少培训时间

当我们移除不相关和有噪声的特征时，这意味着我们在要馈入 ML 模型的数据集中具有较少的特征，并且这将减少模型的总体训练时间。

## 4.降低模型的复杂性

太多的变量通常会给模型增加噪声，而不是预测值。它还会使模型庞大、耗时，且在生产中更难实现。消除噪声和不相关的特征使模型更好地概括新的、看不见的数据，并使模型不那么复杂。

## 5.更好的互操作性

特征数量越少，就越容易理解它们对模型结果的影响。

# 特征选择、特征工程和降维

这些术语在特征空间中经常被误解。虽然这三种技术都是获得相关特征的方法，这些特征被输入到模型中以获得更好的准确性，但是它们仍然是不同的。

**特征工程**是通过聚合或组合多个特征，利用现有特征创建新特征的技术。创建新特征后，可以根据模型要求删除或保留原始特征。

而**特征选择**是一种技术，我们从原始特征列表中选择重要特征的子集(不改变它们),并丢弃不相关和冗余的特征。

**降维**中，特征被转换到一个更低维度的空间。它创建了一个全新的特征空间，但维度更小，这是不可逆的，因为在降低维度的过程中大多数信息都丢失了。

# 特征选择方法

特征选择分为四类，即过滤器、包装器、嵌入式和混合方法。

## 1.过滤方法

过滤方法选择特征，不受任何监督学习算法的影响。因此，它适用于任何分类算法，并且比包装器和嵌入式方法具有更低的计算复杂度，实现了更高的通用性。所以适用于高维空间。

## 2.包装方法

包装器方法结合了监督学习算法，用于使用搜索策略来验证生成的特征子集。它只对所采用的特定学习算法产生高分类精度。因此，它**不具有高通用性，并且计算复杂度高于嵌入式和滤波方法。**

## 3.嵌入式方法

嵌入式方法使用监督学习算法的一部分用于特征选择过程，并且它仅对于在选择过程中使用的学习算法产生更好的准确性。因此，它**不具有很高的通用性，并且它在计算上比过滤器昂贵，比包装器方法少。**

## 4.混合方法

包装器和过滤器方法的组合被称为混合方法。

# 特征选择技术的稳定性

特征选择算法的稳定性实际上是一个被忽视的问题。事实证明，一些特征选择策略在创建时可能表现得非常好，但在一段时间后进行测试时往往会失败，这意味着一些被选择的特征不稳定，可能在新数据上表现不佳。

特征稳定性，根据[特征选择算法的稳定性:综述](https://www.sciencedirect.com/science/article/pii/S1319157819304379)“**表示特征选择方法的再现能力。**

特征选择算法的稳定性可以被视为当新的训练样本被添加或者当一些训练样本被移除时，算法产生一致的特征子集的一致性。如果该算法为训练数据中的任何扰动产生不同的子集，那么该算法对于特征选择变得不可靠。

因此，当评估特征选择性能时，特征选择算法的高稳定性与高分类精度同等重要。随着时间的推移添加稳定性度量可以提供更好的反馈循环，以便在下一次迭代中执行更好的特征选择。

# 最后的想法

更多的时候，我们努力在我们的模型中获得更好的准确性，如果不进行特征选择，就无法获得好的准确性。我相信这篇文章已经给了你一个很好的选择特征的想法，来从你的机器学习模型中得到最好的东西。在我的下一篇文章中，我将详细介绍特征选择方法。

谢谢你的阅读。我希望你喜欢这篇文章！！如往常一样，请联系我们以获得任何问题/评论/反馈。