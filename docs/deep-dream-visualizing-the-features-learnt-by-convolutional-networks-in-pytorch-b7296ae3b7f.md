# æ·±åº¦æ¢¦:å¯è§†åŒ– PyTorch ä¸­å·ç§¯ç½‘ç»œå­¦ä¹ çš„ç‰¹å¾

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/deep-dream-visualizing-the-features-learnt-by-convolutional-networks-in-pytorch-b7296ae3b7f?source=collection_archive---------2----------------------->

å½“æ¶‰åŠåˆ°è®¡ç®—æœºè§†è§‰ç›¸å…³ä»»åŠ¡æ—¶ï¼Œå·ç§¯ç¥ç»ç½‘ç»œ(CNN)æ˜¯æœ€æœ‰æ•ˆçš„æœºå™¨å­¦ä¹ å·¥å…·ä¹‹ä¸€ã€‚ä»–ä»¬çš„æœ‰æ•ˆæ€§å¯ä»¥ä»ä»¥ä¸‹äº‹å®æ¥è¡¡é‡:å¤§å¤šæ•°è®¡ç®—æœºè§†è§‰ç«èµ›ï¼Œå¦‚ ILSVRCã€PASCAL VOC å’Œ COCOï¼Œå·²ç»è¢«ä½¿ç”¨åŸºäº CNN çš„åˆ›æ–°æ¶æ„æ¥å®ç°å…¶ç›®æ ‡çš„å‚èµ›ä½œå“æ‰€ä¸»å¯¼ã€‚

å› æ­¤ï¼Œè¯¢é—®â€œåœ¨ç»™å®šçš„ CNN ä¸­ï¼Œå„ç§è¿‡æ»¤å™¨å­¦ä¹ äº†ä»€ä¹ˆç‰¹å¾â€æ˜¯æœ‰è¶£çš„ã€‚è¿™ä¸ªé—®é¢˜ä¸ä»…ä»æ™®é€šçš„â€œå¥½å¥‡å¿ƒâ€çš„è§’åº¦æ¥çœ‹æ˜¯æœ‰è¶£çš„ï¼Œæ›´é‡è¦çš„æ˜¯çŸ¥é“è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆå¯ä»¥ç»™æˆ‘ä»¬éå¸¸æœ‰ç”¨çš„æ´å¯ŸåŠ›æ¥æ”¹å–„æˆ‘ä»¬ CNN çš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒILSVRC-2013 æŒ‘æˆ˜èµ›çš„è·å¥–ä½œå“(Clarifai)æ˜¯é€šè¿‡æ”¹è¿›ä¸Šä¸€å¹´çš„è·å¥–ä½œå“(AlexNet)è€Œè®¾è®¡çš„ã€‚è¿™äº›æ”¹è¿›æ˜¯é€šè¿‡åœ¨ AlexNet ä¸Šåº”ç”¨ç‰¹å¾å¯è§†åŒ–æŠ€æœ¯(Deconvnets)é€‰æ‹©çš„ã€‚å‚è§[è¿™ç¯‡](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf)è®ºæ–‡(ç”± ILSVRC-2013 è·å¥–è€…æ’°å†™)äº†è§£æ›´å¤šç»†èŠ‚ï¼Œæˆ–è€…[è¿™ç¯‡](/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103)åšå®¢è¿›è¡Œç²¾å½©è¯„è®ºã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ä¸€ç§å«åšâ€œæ¿€æ´»æœ€å¤§åŒ–â€çš„æŠ€æœ¯æ¥å¯è§†åŒ– CNN å­¦ä¹ çš„ç‰¹å¾ï¼Œè¿™ç§æŠ€æœ¯ä»ä¸€ä¸ªç”±éšæœºåˆå§‹åŒ–çš„åƒç´ ç»„æˆçš„å›¾åƒå¼€å§‹ï¼Œè¿™äº›åƒç´ çš„å€¼è¢«æ…¢æ…¢è°ƒæ•´ï¼Œä»¥æœ€å¤§åŒ–æˆ‘ä»¬å¸Œæœ›å¯è§†åŒ–çš„å±‚çš„è¾“å‡ºã€‚è¿™æ˜¯åœ¨[è¿™ç¯‡](https://www.researchgate.net/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network)è®ºæ–‡ä¸­é¦–æ¬¡ä»‹ç»çš„ï¼Œå¹¶åœ¨[è¿™ç¯‡](https://arxiv.org/abs/1312.6034)è®ºæ–‡ä¸­é¦–æ¬¡åº”ç”¨äº CNNã€‚ç„¶è€Œï¼Œå¯¹ CNN çš„æ¿€æ´»æœ€å¤§åŒ–çš„å¤©çœŸåº”ç”¨å€¾å‘äºäº§ç”Ÿéå¸¸é«˜é¢‘ç‡çš„å›¾åƒï¼Œè¿™äº›å›¾åƒçœ‹èµ·æ¥ä¸€ç‚¹ä¹Ÿä¸åƒäººä»¬æ¯å¤©é‡åˆ°çš„çœŸå®ä¸–ç•Œçš„è‡ªç„¶å›¾åƒã€‚ä¾‹å¦‚ï¼Œå‚è§[æ­¤å¤„](https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis)å¯¹è¯¥é—®é¢˜çš„è¯¦ç»†æè¿°ä»¥åŠè§£å†³è¿™äº›é—®é¢˜çš„å¸¸ç”¨æ–¹æ³•çš„è®¨è®ºã€‚åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†é™åˆ¶è‡ªå·±ä½¿ç”¨ä¸‰ç§ç®€å•çš„æ­£åˆ™åŒ–æŠ€æœ¯æ¥ä½¿å›¾åƒæ›´æœ‰æ„ä¹‰:

1.  ä»ä¸€å¼  28 x 28 çš„å°å›¾ç‰‡å¼€å§‹ï¼Œæ…¢æ…¢æ”¾å¤§åˆ°æƒ³è¦çš„å°ºå¯¸ï¼Œæ¯”å¦‚è¿™é‡Œçš„ã€‚
2.  æƒ©ç½šå¤§åƒç´ å€¼
3.  ä¸åˆ©äºå›¾åƒä¸­å¤§çš„åƒç´ æ¢¯åº¦ï¼Œå³ä¸åˆ©äºç›¸é‚»åƒç´ å€¼çš„ä»»ä½•æ€¥å‰§å˜åŒ–ã€‚

æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹å§ã€‚åŒ…å«æˆ‘å°è¯•è¿‡çš„å„ç§ä¸œè¥¿çš„å®Œæ•´ä»£ç å¯ä»¥åœ¨æˆ‘çš„ [Github](https://github.com/praritagarwal/Visualizing-CNN-Layers/blob/master/Activation%20Maximization.ipynb) ä¸Šæ‰¾åˆ°ã€‚è¿™ç¯‡æ–‡ç« æ˜¯åŸºäºç¬”è®°æœ¬ä¸­çš„è¯•éªŒ#6ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»ä¸€ä¸‹ä»£ç ã€‚é¡ºä¾¿æä¸€ä¸‹ï¼Œæˆ‘æœ€è¿‘è¿˜çœ‹åˆ°äº†ä¸€ä¸ªå¾ˆæ£’çš„ Kerasâ€”â€”Keras çš„åˆ›é€ è€… Francois Chollet å®ç°äº†åŒæ ·çš„æŠ€æœ¯ã€‚æˆ‘å¼ºçƒˆæ¨èå¤§å®¶çœ‹çœ‹ä»–çš„[å¸–å­](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)ã€‚

è®©æˆ‘ä»¬ä»åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å¼€å§‹:

```
import torch
from torchvision import models
model = models.googlenet(pretrained = True)
```

è™½ç„¶æˆ‘çœ‹åˆ°çš„å¤§å¤šæ•°å…³äºæ¿€æ´»æœ€å¤§åŒ–çš„åšå®¢éƒ½å€¾å‘äºä½¿ç”¨ VGG16 ä½œä¸ºä»–ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«çš„åŸå› ï¼Œé™¤äº†å°è¯•ä¸€äº›ä¸åŒçš„ä¸œè¥¿ï¼Œæˆ‘å°†ä½¿ç”¨ GoogLeNetã€‚è¿™ä¸ªåšå®¢ä¸­çš„å‡ ä¹æ‰€æœ‰ä»£ç éƒ½å¯ä»¥ç›´æ¥åº”ç”¨äºä»»ä½•å…¶ä»–ç»è¿‡è®­ç»ƒçš„ CNNã€‚

ç”±äºæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯å¯è§†åŒ–æ¨¡å‹å·²ç»å­¦ä¹ çš„å†…å®¹ï¼Œè€Œä¸æ˜¯é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬åº”è¯¥å†»ç»“æ¨¡å‹å‚æ•°ï¼Œä»¥ä¾¿å®ƒä»¬åœ¨åå‘ä¼ æ’­æœŸé—´ä¸ä¼šæ”¹å˜ã€‚

```
for param in model.parameters():
    param.requires_grad_(False)
```

è¯·æ³¨æ„ï¼Œæ¨¡å‹ä¸­çš„å„ä¸ªå±‚å¯ä»¥é€šè¿‡èµ‹äºˆå®ƒä»¬çš„å”¯ä¸€åç§°è½»æ¾è®¿é—®ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬åˆ—å‡ºæ¨¡å‹ä¸­ä¸åŒå±‚çš„åç§°:

```
list(map(lambda x: x[0], model.named_children()))
```

åœ¨ GoogLeNet ä¸Šï¼Œè¿™ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡º

```
['conv1',
 'maxpool1',
 'conv2',
 'conv3',
 'maxpool2',
 'inception3a',
 'inception3b',
 'maxpool3',
 'inception4a',
 'inception4b',
 'inception4c',
 'inception4d',
 'inception4e',
 'maxpool4',
 'inception5a',
 'inception5b',
 'avgpool',
 'dropout',
 'fc']
```

å‡ºäºæ¼”ç¤ºç›®çš„ï¼Œæˆ‘å°†(éšæœº)é€‰æ‹©åä¸ºâ€œinception4aâ€çš„å›¾å±‚ã€‚æˆ‘ä»¬ç°åœ¨å¿…é¡»ä¸ºè¿™ä¸€å±‚æ³¨å†Œä¸€ä¸ªå‘å‰çš„é’©å­ã€‚[æŒ‚é’©](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks)æä¾›å¯¹æ‰€éœ€å›¾å±‚çš„è¾“å‡ºå’Œ grad _ ouput çš„ç®€å•è®¿é—®ã€‚é¡¾åæ€ä¹‰ï¼Œå‘å‰é’©å­åœ¨å‘å‰ä¼ é€’æœŸé—´æ‰§è¡Œï¼Œå¹¶å…è®¸æˆ‘ä»¬æŸ¥çœ‹/ä¿®æ”¹å±‚çš„è¾“å‡ºã€‚ç±»ä¼¼åœ°ï¼Œåœ¨å‘åä¼ é€’æœŸé—´æ‰§è¡Œå‘åæŒ‚é’©ï¼Œå¹¶å…è®¸æˆ‘ä»¬æŸ¥çœ‹/ä¿®æ”¹å±‚çš„ grad _ ouputã€‚æŸ¥çœ‹è¿™ä¸ª[åšå®¢](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/)å’Œè¿™ä¸ª [kaggle å†…æ ¸](https://www.kaggle.com/sironghuang/understanding-pytorch-hooks/notebook)ä»¥è·å¾—æ›´å¤šå…³äºé’©å­çš„ä¿¡æ¯ã€‚è¿™é‡Œçš„å®ç°åŸºäº pytorch è®¨è®ºæ¿ä¸Šçš„è¿™ä¸ª[è®¨è®º](https://discuss.pytorch.org/t/visualize-feature-map/29597/2)ã€‚ä¸ºäº†æ³¨å†Œä¸€ä¸ªå‰å‘é’©å­ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸‹é¢çš„å·¥å‚å‡½æ•°ï¼Œå®ƒè¿”å›ä¸€ä¸ªå‡½æ•°å¯¹è±¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒä½œä¸ºé’©å­:

```
activation = {} # dictionary to store the activation of a layerdef create_hook(name):
 def hook(m, i, o):
   # copy the output of the given layer
   activation[name] = o

 return hook
```

æˆ‘ä»¬ç°åœ¨æ³¨å†ŒæŒ‚é’©:

```
# register a forward hook for layer inception4a
model.inception4a.register_forward_hook(create_hook(â€˜4aâ€™))
```

è¯·æ³¨æ„ï¼ŒPyTorch ä¸Šçš„é¢„è®­ç»ƒæ¨¡å‹è¦æ±‚è¾“å…¥å›¾åƒâ€œå¿…é¡»åŠ è½½åˆ°[0ï¼Œ1]çš„èŒƒå›´å†…ï¼Œç„¶åä½¿ç”¨`mean = [0.485, 0.456, 0.406]`å’Œ`std = [0.229, 0.224, 0.225]`è¿›è¡Œå½’ä¸€åŒ–â€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨å›¾åƒä¸Šå®šä¹‰ä»¥ä¸‹è½¬æ¢:

```
# normalize the input image to have appropriate mean and standard deviation as specified by pytorchfrom torchvision import transformsnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])# undo the above normalization if and when the need arises denormalize = transforms.Normalize(mean = [-0.485/0.229, -0.456/0.224, -0.406/0.225], std = [1/0.229, 1/0.224, 1/0.225] )
```

ç°åœ¨è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥ç”Ÿæˆç”±éšæœºåˆå§‹åŒ–çš„åƒç´ ç»„æˆçš„å›¾åƒã€‚ä¸ºäº†å…è®¸åœ¨åå‘ä¼ æ’­æœŸé—´è°ƒæ•´å›¾åƒï¼Œæˆ‘ä»¬å¿…é¡»å°†å›¾åƒçš„â€œrequires_grad_â€æ ‡å¿—è®¾ç½®ä¸ºçœŸã€‚

```
import numpy as npHeight = 28
Width = 28# generate a numpy array with random values
img = np.single(np.random.uniform(0,1, (3, Height, Width)))# convert to a torch tensor, normalize, set the requires_grad_ flag
im_tensor = normalize(torch.from_numpy(img)).requires_grad_(True)
```

è®©æˆ‘ä»¬è¿˜å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥åè§„æ ¼åŒ–å›¾åƒï¼Œå¹¶å°†é¢œè‰²é€šé“ç§»åŠ¨åˆ°æœ€åçš„ç»´åº¦ï¼Œä»¥ä¾¿ä½¿ç”¨ matplotlib çš„ imshow æ˜¾ç¤ºå®ƒã€‚è¿™åœ¨ä½¿ç”¨ open-cv çš„ resize æ–¹æ³•è°ƒæ•´å›¾åƒå¤§å°æ—¶ä¹Ÿå¾ˆæ–¹ä¾¿ã€‚

```
# function to massage img_tensor for using as input to plt.imshow()
def image_converter(im):

    # move the image to cpu
    im_copy = im.cpu()

    # for plt.imshow() the channel-dimension is the last
    # therefore use transpose to permute axes
    im_copy = denormalize(im_copy.clone().detach()).numpy()
    im_copy = im_copy.transpose(1,2,0)

    # clip negative values as plt.imshow() only accepts 
    # floating values in range [0,1] and integers in range [0,255]
    im_copy = im_copy.clip(0, 1) 

    return im_copy
```

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬å¸Œæœ›æƒ©ç½šå›¾åƒä¸­åƒç´ å€¼çš„ä»»ä½•æ€¥å‰§å˜åŒ–ï¼Œå³æˆ‘ä»¬å°†æƒ©ç½šå›¾åƒä¸­åƒç´ å€¼çš„ x å’Œ y å¯¼æ•°ã€‚è¿™å¯ä»¥é€šè¿‡ç”¨[ç´¢è´å°”æ»¤é•œ](https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/sobel_derivatives/sobel_derivatives.html)æˆ–[æ²™å°”æ»¤é•œ](https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=scharr#scharr)åˆ›å»ºå·ç§¯å±‚æ¥å®Œæˆã€‚æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªå·ç§¯å±‚ï¼Œå®ƒå¯ä»¥æ¥å—ä»¥ä¸‹ä»»ä½•ä¸€ç§æ»¤æ³¢å™¨:

```
import torch.nn as nn# class to compute image gradients in pytorch
class RGBgradients(nn.Module):
    def __init__(self, weight): # weight is a numpy array
        super().__init__()
        k_height, k_width = weight.shape[1:]
        # assuming that the height and width of the kernel are always odd numbers
        padding_x = int((k_height-1)/2)
        padding_y = int((k_width-1)/2)

        # convolutional layer with 3 in_channels and 6 out_channels 
        # the 3 in_channels are the color channels of the image
        # for each in_channel we have 2 out_channels corresponding to the x and the y gradients
        self.conv = nn.Conv2d(3, 6, (k_height, k_width), bias = False, 
                              padding = (padding_x, padding_y) )
        # initialize the weights of the convolutional layer to be the one provided
        # the weights correspond to the x/y filter for the channel in question and zeros for other channels
        weight1x = np.array([weight[0], 
                             np.zeros((k_height, k_width)), 
                             np.zeros((k_height, k_width))]) # x-derivative for 1st in_channel

        weight1y = np.array([weight[1], 
                             np.zeros((k_height, k_width)), 
                             np.zeros((k_height, k_width))]) # y-derivative for 1st in_channel

        weight2x = np.array([np.zeros((k_height, k_width)),
                             weight[0],
                             np.zeros((k_height, k_width))]) # x-derivative for 2nd in_channel

        weight2y = np.array([np.zeros((k_height, k_width)), 
                             weight[1],
                             np.zeros((k_height, k_width))]) # y-derivative for 2nd in_channel

        weight3x = np.array([np.zeros((k_height, k_width)),
                             np.zeros((k_height, k_width)),
                             weight[0]]) # x-derivative for 3rd in_channel

        weight3y = np.array([np.zeros((k_height, k_width)),
                             np.zeros((k_height, k_width)), 
                             weight[1]]) # y-derivative for 3rd in_channel

        weight_final = torch.from_numpy(np.array([          weight1x, weight1y, 
weight2x, weight2y,
weight3x, weight3y])).type(torch.FloatTensor)

        if self.conv.weight.shape == weight_final.shape:
            self.conv.weight = nn.Parameter(weight_final)
            self.conv.weight.requires_grad_(False)
        else:
            print('Error: The shape of the given weights is not correct')

    # Note that a second way to define the conv. layer here would be to pass group = 3 when calling torch.nn.Conv2d

    def forward(self, x):
        return self.conv(x)
```

äº‹å®è¯æ˜ï¼Œå¯¹äº 3 x 3 å†…æ ¸ï¼ŒScharr æ»¤æ³¢å™¨ä¼˜äº Sobel æ»¤æ³¢å™¨ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨ Scharr æ»¤æ³¢å™¨:

```
# Scharr Filtersfilter_x = np.array([[-3, 0, 3], 
                     [-10, 0, 10],
                     [-3, 0, 3]])filter_y = filter_x.T
grad_filters = np.array([filter_x, filter_y])
```

ç°åœ¨è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä¸Šé¢å®šä¹‰çš„å·ç§¯å±‚çš„å®ä¾‹ï¼ŒæŠŠå®ƒä¼ é€’ç»™ Scharr è¿‡æ»¤å™¨ã€‚

```
gradLayer = RGBgradients(grad_filters)
```

è®©æˆ‘ä»¬ä¹Ÿå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒä½¿ç”¨ä¸Šé¢å®šä¹‰çš„ gradLayer æ¥è®¡ç®—è¾“å…¥å›¾åƒçš„ x å’Œ y å¯¼æ•°ï¼Œå¹¶è¿”å›å®ƒä»¬çš„å‡æ–¹æ ¹å€¼ã€‚

```
# function to compute gradient loss of an image def grad_loss(img, beta = 1, device = 'cpu'):

    # move the gradLayer to cuda
    gradLayer.to(device) gradSq = gradLayer(img.unsqueeze(0))**2

    grad_loss = torch.pow(gradSq.mean(), beta/2)

    return grad_loss
```

æœ€åï¼Œè®©æˆ‘ä»¬æŠŠæ‰€æœ‰ä¸œè¥¿éƒ½æ¬åˆ° GPU ä¸Šã€‚å¦‚æœä½ æ²¡æœ‰ GPU æˆ–è€…ä½ æƒ³åœ¨ä½ çš„ cpu ä¸Šè¿›è¡Œè®¡ç®—ï¼Œä½ å¯ä»¥è·³è¿‡ä¸‹é¢çš„æ­¥éª¤ã€‚

```
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Calculations being executed on {}'.format(device))model.to(device)
img_tensor = im_tensor.to(device)
```

æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬ä¹Ÿä¼šæ…¢æ…¢æå‡å½¢è±¡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ opencv çš„ resize()æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹(å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨[torch vision . transforms . resize()](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Resize))ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¼å…¥ cv2ã€‚æˆ‘ä»¬è¿˜éœ€è¦ matplotlib.pyplot å’Œ torch.optimã€‚

```
import cv2
from torch import optim
import sys
import matplotlib.pyplot as plt
```

æˆ‘ä»¬ç°åœ¨å‡†å¤‡è°ƒæ•´æˆ‘ä»¬çš„éšæœºå›¾åƒï¼Œä½¿å…¶æœ€å¤§åŒ–æˆ‘ä»¬å·ç§¯å±‚æ‰€é€‰èŠ‚ç‚¹çš„è¾“å‡ºã€‚å‡ºäºæœ¬æ–‡çš„ç›®çš„ï¼Œè®©æˆ‘é€‰æ‹©ç´¢å¼•å€¼ä¸º 225 çš„èŠ‚ç‚¹ã€‚

æˆ‘å°†ä¼˜åŒ–å›¾åƒ 20 æ¬¡è¿­ä»£ï¼Œç„¶åä»¥ 1.05 çš„å› å­é‡æ–°ç¼©æ”¾ã€‚æˆ‘å°†é‡å¤è¿™ä¸ªå¾ªç¯ 45 æ¬¡ï¼Œä»¥å¾—åˆ°æœ€ç»ˆå°ºå¯¸ä¸º 249 x 249 çš„å›¾åƒã€‚

```
unit_idx = 225 # the neuron to visualize
act_wt = 0.5 # factor by which to weigh the activation relative to the regulizer termsupscaling_steps = 45 # no. of times to upscale
upscaling_factor = 1.05
optim_steps = 20# no. of times to optimize an input image before upscaling
```

æˆ‘ä»¬ç°åœ¨å°†è¿è¡Œä¸¤ä¸ªåµŒå¥—å¾ªç¯æ¥ä¼˜åŒ–æˆ‘ä»¬çš„å›¾åƒï¼Œç„¶åæŒ‰å¦‚ä¸‹æ–¹å¼æ”¾å¤§å®ƒ:

```
model.eval()
for mag_epoch in range(upscaling_steps+1):
    optimizer = optim.Adam([img_tensor], lr = 0.4)

    for opt_epoch in range(optim_steps):
        optimizer.zero_grad()
        model(img_tensor.unsqueeze(0))
        layer_out = activation['4a']
        rms = torch.pow((layer_out[0, unit_idx]**2).mean(), 0.5)
        # terminate if rms is nan
        if torch.isnan(rms):
            print('Error: rms was Nan; Terminating ...')
            sys.exit()

        # pixel intensity
        pxl_inty = torch.pow((img_tensor**2).mean(), 0.5)
        # terminate if pxl_inty is nan
        if torch.isnan(pxl_inty):
            print('Error: Pixel Intensity was Nan; Terminating ...')
            sys.exit()

        # image gradients
        im_grd = grad_loss(img_tensor, beta = 1, device = device)
        # terminate is im_grd is nan
        if torch.isnan(im_grd):
            print('Error: image gradients were Nan; Terminating ...')
            sys.exit()

        loss = -act_wt*rms + pxl_inty + im_grd        
        # print activation at the beginning of each mag_epoch
        if opt_epoch == 0:
            print('begin mag_epoch {}, activation: {}'.format(mag_epoch, rms))
        loss.backward()
        optimizer.step()

    # view the result of optimising the image
    print('end mag_epoch: {}, activation: {}'.format(mag_epoch, rms))
    img = image_converter(img_tensor)    
    plt.imshow(img)
    plt.title('image at the end of mag_epoch: {}'.format(mag_epoch))
    plt.show()

    img = cv2.resize(img, dsize = (0,0), 
                     fx = upscaling_factor, fy = upscaling_factor).transpose(2,0,1) # scale up and move the batch axis to be the first
    img_tensor = normalize(torch.from_numpy(img)).to(device).requires_grad_(True)
```

åœ¨ä¸Šé¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬ä¸ºæŸå¤±å‡½æ•°å®šä¹‰äº†ä¸‰ç§è´¡çŒ®:

1.  rms:è¿™æ˜¯æˆ‘ä»¬é€‰æ‹©çš„å·ç§¯å•å…ƒäº§ç”Ÿçš„è¾“å‡ºå¼ é‡ä¸­å…ƒç´ çš„å‡æ–¹æ ¹å€¼ã€‚æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è¿™ä¸€ç‚¹ã€‚
2.  pxl_inty:è¿™æ˜¯æˆ‘ä»¬å›¾åƒä¸­åƒç´ å€¼çš„å‡æ–¹æ ¹å€¼ã€‚å‡ºäºæ­£åˆ™åŒ–çš„ç›®çš„ï¼Œæˆ‘ä»¬å¸Œæœ›æƒ©ç½šå¤§åƒç´ å€¼ï¼Œä»è€Œä¿æŒ pxl_inty è¾ƒä½ã€‚
3.  im_grd:è¿™æ˜¯åƒç´ å€¼çš„ x å’Œ y å¯¼æ•°çš„å‡æ–¹æ ¹å€¼ã€‚é€šè¿‡ä¿æŒä½ç”µå¹³ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿åƒç´ å€¼ä¸ä¼šå‘ç”Ÿæ€¥å‰§å˜åŒ–ã€‚

å› æ­¤ï¼ŒæŸå¤±å‡½æ•°ç”±ä¸‹å¼ç»™å‡º

```
loss = -act_wt*rms + pxl_inty + im_grd
```

å…¶ä¸­â€˜act _ wtâ€™æ˜¯æˆ‘ä»¬åˆ†é…ç»™â€˜rmsâ€™ç›¸å¯¹äº pxl_inty å’Œ im_grd çš„æƒé‡ã€‚å› æ­¤ï¼Œä¸å›¾åƒä¸­çš„åƒç´ å¼ºåº¦å’Œæ¢¯åº¦ç›¸æ¯”ï¼Œæ”¹å˜ act_wt æ”¹å˜äº†å•å…ƒæ¿€æ´»çš„é‡è¦æ€§ã€‚æˆ‘ä»¬è¿˜æ£€æŸ¥åœ¨è¿­ä»£çš„ä»»ä½•ä¸€ç‚¹ï¼Œè¿™äº›æ˜¯å¦æˆä¸º nanï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ç»ˆæ­¢ä»£ç ã€‚

æ³¨æ„ï¼Œåœ¨å¤–éƒ¨å¾ªç¯ä¸­ï¼Œå³æ”¾å¤§å¾ªç¯ä¸­ï¼Œæ¯æ¬¡æˆ‘ä»¬è°ƒæ•´å›¾åƒå°ºå¯¸æ—¶ï¼Œæˆ‘ä»¬éƒ½ç”Ÿæˆæ–°çš„ img_tensorï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ¯ä¸ªæ”¾å¤§æ—¶æœŸçš„å¼€å§‹é‡å»ºæˆ‘ä»¬çš„ä¼˜åŒ–å™¨ã€‚

ç§å•Šã€‚æˆ‘ä»¬å®Œäº†ã€‚æ‰§è¡Œä¸Šé¢çš„å¾ªç¯ä¼šç”Ÿæˆä»¥ä¸‹å›¾åƒ:

![](img/edca18b03cf610fd5a6cbfb8b56bdecf.png)

å·¦ä¸Šè‡³å³ä¸‹:ç¬¬ 0ã€ç¬¬ 9ã€ç¬¬ 18ã€ç¬¬ 27ã€ç¬¬ 36 å’Œç¬¬ 45 å€æ”¾å¤§ç»“æŸæ—¶çš„å›¾åƒ

å¦‚æœæˆ‘æ²¡æœ‰åè§çš„è¯ï¼Œé‚£ä¹ˆè¿™æ ·äº§ç”Ÿçš„æœ€ç»ˆå›¾åƒä¼¼ä¹åŒ…å«äº†å¾ˆå¤šçœ¼ç›ä¸€æ ·çš„ç‰¹å¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­ï¼Œè¿™é‡Œè®¨è®ºçš„å·ç§¯å•å…ƒå¿…é¡»åœ¨è¾“å…¥å›¾åƒä¸­å¯»æ‰¾â€œçœ¼ç›â€ã€‚çœ‹çœ‹å…¶ä»–å·ç§¯èŠ‚ç‚¹æœ€ç»ˆå­¦åˆ°äº†ä»€ä¹ˆå°†ä¼šå¾ˆæœ‰è¶£ã€‚ä»¥ä¸‹æ˜¯æ¯å±‚ä¸­å‰ 10 ä¸ªæœ€æ´»è·ƒå•å…ƒçš„å¯è§†åŒ–æ•ˆæœ:

![](img/3da79c52ff8da65aef2eaf5a62f7e79a.png)![](img/cfa679328916796b3cd5991b1401acd9.png)![](img/033e9476f2e30be5826b0f471378e772.png)![](img/9f58eb18c522fce8ef17bf2bd0a8ba75.png)![](img/ee2ffa7ad53642f04edc4655eac171b9.png)![](img/d29d34b700ccf3fa9eb39aa5203ffaaf.png)![](img/9e2b7d98262bcbc6d423c60be8aeefb4.png)![](img/0d1914207e662d50326dcf1143b6e2ea.png)![](img/5912e2cac87bd0c26eb4af3cb7796153.png)![](img/7fd3d25003b1cdeb03b69053b2079549.png)![](img/3876ebbc9c50c51010502f227fd88c54.png)![](img/9e35269969fe4da27371dff7d511d47a.png)![](img/479659902efa186d84a9812a8a01ddc0.png)![](img/c5922c96e3ff9581c987f2e90f883610.png)![](img/9ea7c9a05965bc54cbac72af745a56ce.png)![](img/2994d678c86eb0cbdf95d742c17aded0.png)

çœ‹èµ·æ¥ CNN ä¸­çš„å¤§å¤šæ•°å•ä½æœ€ç»ˆéƒ½å­¦ä¹ äº†ä¸åŒç§ç±»çš„çº¹ç†ã€‚å¶å°”ï¼Œæœ‰ä¸€äº›å•ä½ä¼šå­¦ä¹ é¢éƒ¨ç‰¹å¾ï¼Œæ¯”å¦‚çœ¼ç›ç­‰ç­‰ã€‚æˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œå®ƒçœ‹èµ·æ¥åƒåœ¨ç¬¬ 4a å±‚-ç¬¬ 4e å±‚å’Œç¬¬ 5a å±‚çš„å•ä½æœ‰æœ€å¯è¾¨åˆ«çš„ç‰¹å¾ã€‚åœ¨è®¸å¤šåœ°æ–¹éƒ½æœ‰äººè®¤ä¸ºï¼ŒCNN çš„é«˜å±‚æœ€ç»ˆä¼šå­¦ä¹ ç”¨äºè®­ç»ƒçš„å›¾åƒçš„å†…å®¹ï¼Œè€Œä½å±‚åˆ™ç›¸åï¼Œå®ƒä»¬æœ€ç»ˆä¼šå­¦ä¹ å›¾åƒçš„çº¹ç†ã€‚ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘å¸Œæœ›æœ€åä¸€å±‚ï¼Œå³ inception5b èƒ½å¤Ÿäº§ç”ŸåŒ…å«é«˜åº¦æ˜æ˜¾çš„äººç±»å¯è§£é‡Šç»„ä»¶çš„å›¾åƒã€‚ç„¶è€Œï¼Œå¯¹äºä¸»è¦åŒ…å«éå¸¸é«˜é¢‘ç‡æ¨¡å¼çš„ç›¸åº”å›¾åƒæ¥è¯´ï¼Œæƒ…å†µä¼¼ä¹å¹¶éå¦‚æ­¤ã€‚ä¹Ÿè®¸ï¼Œæˆ‘åº”è¯¥å°è¯•ä¸€ä¸ªå¤§äº 3 x 3 æ»¤é•œçš„æ¸å˜å›¾å±‚ã€‚Mahendran å’Œ Vedaldi ä¹Ÿä¸»å¼ ä½¿ç”¨æŠ–åŠ¨æ¥è§„èŒƒè¿™äº›é«˜é¢‘æ¨¡å¼çš„å‡ºç°ã€‚è¿™æ˜¯æˆ‘æ²¡æœ‰åŒ…æ‹¬åœ¨å†…çš„ä¸œè¥¿ï¼Œä½†å°è¯•ä¸€ä¸‹ä¼šå¾ˆæœ‰è¶£ã€‚

å¸Œæœ›ä½ ä¼šå’Œæˆ‘ä¸€æ ·ç©å¾—å¼€å¿ƒã€‚ğŸ˜ƒ