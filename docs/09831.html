<html>
<head>
<title>Real-Time RNN Speech Noise Suppression on a MCU (STM32)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于单片机(STM32)的实时 RNN 语音噪声抑制</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/real-time-rnn-speech-noise-suppression-on-a-microcontroller-stm32-e17d8c3eac57?source=collection_archive---------0-----------------------#2020-09-22">https://medium.com/analytics-vidhya/real-time-rnn-speech-noise-suppression-on-a-microcontroller-stm32-e17d8c3eac57?source=collection_archive---------0-----------------------#2020-09-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="933a" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">基于众所周知的<a class="ae ix" href="https://jmvalin.ca/demo/rnnoise/" rel="noopener ugc nofollow" target="_blank"> RNNoise </a>提供的方法论。</h2></div><p id="f3fd" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">RNNoise 展示了传统信号处理和神经网络的良好结合，从而产生了一种低成本的噪声抑制方法。但是，由于 RNNoise 仍然使用浮点 NN 接口，并且是为 Cortex-A MPU 设计的。Cortex-M 级 MCU 处理信号的计算量仍然很大(约 40 Mflops)。</p><p id="8954" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">因此，通过使用 RNNoise 的方法论，我开发了一个 RNN 噪声抑制的 NNoM 实例，但是降低了系统复杂度，并且将 NN 触发器转换为定点算法(MAC ops)。</p><blockquote class="ju jv jw"><p id="b3b5" class="iy iz jx ja b jb jc ij jd je jf im jg jy ji jj jk jz jm jn jo ka jq jr js jt hb bi translated">对于不熟悉的人来说，<a class="ae ix" href="https://github.com/majianjia/nnom" rel="noopener ugc nofollow" target="_blank"> NNoM </a>是一个针对低资源平台(如 Cortex-M)的定点干扰 NN lib。它将复杂的 keras 模型转换成一个 C 头文件，并可以与优化的 CMSIS-NN 一起运行。</p></blockquote><p id="ef56" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">本教程的<strong class="ja hj">源代码</strong>可以在这里找到:<a class="ae ix" href="https://github.com/majianjia/nnom/tree/master/examples/rnn-denoise" rel="noopener ugc nofollow" target="_blank">https://github . com/majianjia/nnom/tree/master/examples/rnn-de noise</a></p><h1 id="482c" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">概观</h1><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/636d2edeee6174334add46f0c99aad31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J60Dga084yNvGsTh.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">系统概述。</figcaption></figure><p id="8363" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><a class="ae ix" href="https://jmvalin.ca/demo/rnnoise/" rel="noopener ugc nofollow" target="_blank"> RNNoise </a>已经解释了我们在这个项目中使用的方法。</p><p id="ca93" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">基本上，我们使用神经网络模型来控制非常高频率的音频均衡器(EQ ),从而抑制那些包含噪声的频带，同时保持增益包含语音。</p><p id="d5a8" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">与试图直接输出信号的传统神经网络不同，我们的神经网络模型输出均衡器的每个滤波器频带的增益。</p><p id="ea30" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">此示例使用 MFCC (Mel 标度)来确定增益，而不是 Opus 标度(RNNoise)或 Bark 标度。</p><p id="2aac" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">我们也没有实现 RNNoise 使用的音调滤波算法。该器件是由一组梳状滤波器组成的独立功能模块。</p><p id="9981" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">这里是我们可以实现的演示</strong></p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="ae5c" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">再次分享给你的朋友！</p><figure class="ku kv kw kx fd ky"><div class="bz dy l di"><div class="ll lk l"/></div></figure></div><div class="ab cl lm ln gp lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="hb hc hd he hf"><h1 id="fedf" class="kb kc hi bd kd ke lt kg kh ki lu kk kl io lv ip kn ir lw is kp iu lx iv kr ks bi translated">运行该示例的分步指南</h1><p id="56cf" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">本说明将为您提供在 PC 或 MCU 上运行模型和音频处理的必要信息。</p><p id="6a82" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">源代码可以在<a class="ae ix" href="https://github.com/majianjia/nnom/tree/master/examples/rnn-denoise" rel="noopener ugc nofollow" target="_blank"> NNoM 存储库</a>下获得。</p><h2 id="7ad0" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">1.得到嘈杂的演讲</h2><p id="2225" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">这个例子使用<a class="ae ix" href="https://github.com/microsoft/MS-SNSD" rel="noopener ugc nofollow" target="_blank">微软可扩展的噪声语音数据集</a> (MS-SNSD)。如果你想训练你自己的模型，你可以从上面的库中下载数据集，然后把它们放到文件夹<code class="du mr ms mt mu b">MS-SNSD/</code>中。</p><p id="ef15" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">在 MS-SNSD 被下载之后，你现在可以生成<code class="du mr ms mt mu b">clean speech</code>和它相应的<code class="du mr ms mt mu b">noisy speech</code>，混合了不同级别的噪声。</p><p id="f419" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">使用 MS-SNSD 的优势在于它是可扩展的。修改<code class="du mr ms mt mu b">noisyspeech_synthesizer.cfg</code>来配置如何生成语音。本示例的推荐配置如下:</p><pre class="ku kv kw kx fd mv mu mw mx aw my bi"><span id="8852" class="md kc hi mu b fi mz na l nb nc">sampling_rate: 16000<br/>audioformat: *.wav<br/>audio_length: 60 <br/>silence_length: 0.0<br/>total_hours: 15<br/>snr_lower: 0<br/>snr_upper: 20<br/>total_snrlevels: 3</span></pre><p id="f844" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">然后，运行<code class="du mr ms mt mu b">noisyspeech_synthesizer.py</code>来生成演讲。如果一切顺利，将会创建 3 个新文件夹<code class="du mr ms mt mu b">/Noise_training</code>、<code class="du mr ms mt mu b">/CleanSpeech_training</code>和<code class="du mr ms mt mu b">NoisySpeech_training</code>。我们只需要后面的两个文件夹。</p><h2 id="8fb7" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">2.生成数据集</h2><p id="55e2" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">现在我们有位于<code class="du mr ms mt mu b">MS-SNSD/CleanSpeech_training</code>和<code class="du mr ms mt mu b">MS-SNSD/NoisySpeech_training</code>的干净和嘈杂的语音。它们是原始 PCM 信号，但我们的神经网络将 MFCCs 及其导数作为输入，均衡器增益作为输出，因此我们需要处理它们以获得我们的训练数据集。</p><p id="6e28" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">现在我们需要运行<code class="du mr ms mt mu b">gen_dataset.py</code>来获得 MFCC 和增益。它将生成数据集文件<code class="du mr ms mt mu b">dataset.npz</code>，该文件可用于以后的神经网络训练。还有，</p><ul class=""><li id="7eb1" class="nd ne hi ja b jb jc je jf jh nf jl ng jp nh jt ni nj nk nl bi translated">您可以配置要在 RNN 中使用多少 MFCC 功能(这也是均衡器中滤波器波段的数量)。修改<code class="du mr ms mt mu b">num_filter = 20</code>；这个数字可以是从<code class="du mr ms mt mu b">10</code>到<code class="du mr ms mt mu b">26</code>。</li><li id="2fcd" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">该步骤将均衡器的滤波系数生成到文件<code class="du mr ms mt mu b">equalizer_coeff.h</code> ( <code class="du mr ms mt mu b">generate_filter_header(...)</code>)中，该文件将在 C 均衡器中使用。</li></ul><p id="4903" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">此外，<code class="du mr ms mt mu b">gen_dataset.py</code>还生成一个音频文件<code class="du mr ms mt mu b">_noisy_sample.wav</code>，它是原始的噪声语音，以及一个使用真实增益<code class="du mr ms mt mu b">_filtered_sample.wav</code>过滤的过滤文件。我建议播放这两个文件，看看使用这种均衡器抑制方法能得到什么样的最佳结果。</p><h2 id="721e" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">3.培养</h2><p id="954a" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">一旦我们准备好<code class="du mr ms mt mu b">dataset.npz</code>，运行<code class="du mr ms mt mu b">main.py</code>来训练模型。训练好的模型将被保存为<code class="du mr ms mt mu b">model.h5</code></p><p id="3944" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">我们用<code class="du mr ms mt mu b">stateful=True</code>和<code class="du mr ms mt mu b">timestamps=1</code>训练 RNN，这对于用反向传播训练不友好，所以我把<code class="du mr ms mt mu b">batchsize</code>设置得很大，让 BP 的日子好过些。</p><p id="65eb" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">在<code class="du mr ms mt mu b">main.py</code>的后期，它将重新加载<code class="du mr ms mt mu b">model.h5</code>，并尝试使用我们经过皮肉训练的 RNN <code class="du mr ms mt mu b"> filtered_sig = voice_denoise(...)</code>处理上述噪声文件<code class="du mr ms mt mu b">_noisy_sample.wav</code>，并将过滤后的信号保存到<code class="du mr ms mt mu b">_nn_filtered_sample.wav</code>。</p><p id="e367" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">此外，它将使用 NNoM 模型转换器脚本<code class="du mr ms mt mu b">generate_model(...)</code>来生成我们的 NNoM 模型<code class="du mr ms mt mu b">weights.h</code></p><h2 id="e52a" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">4.使用 NNoM 的推理</h2><p id="a691" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">这个例子提供了一个<code class="du mr ms mt mu b">SConstruct</code>，因此您可以在这个文件夹中运行<code class="du mr ms mt mu b">scons</code>来构建一个二进制可执行文件。</p><p id="ecd6" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">该可执行文件可以将<code class="du mr ms mt mu b">.wav</code>文件作为输入，输出过滤后的<code class="du mr ms mt mu b">.wav</code>文件。</p><blockquote class="ju jv jw"><p id="a5a4" class="iy iz jx ja b jb jc ij jd je jf im jg jy ji jj jk jz jm jn jo ka jq jr js jt hb bi translated"><strong class="ja hj">唯一支持的</strong>格式是<strong class="ja hj"> 16kHz </strong>、<strong class="ja hj"> 1CH </strong>。<code class="du mr ms mt mu b">main.c</code>不会解析 wav 文件，只会复制文件头并跳转到数据块。</p></blockquote><p id="b925" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">编译完成后，使用文件夹中的以下命令运行</p><ul class=""><li id="e2af" class="nd ne hi ja b jb jc je jf jh nf jl ng jp nh jt ni nj nk nl bi translated">Win powershell: <code class="du mr ms mt mu b">.\rnn-denoise [input_file] [output_file]</code>或将 wav 文件拖到可执行文件上。</li><li id="d3df" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">Linux:我不知道</li></ul><p id="b313" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">即运行<code class="du mr ms mt mu b">.\rnn-denoise _noisy_sample.wav nn_fixedpoit_filtered_sample.wav</code></p><p id="18c8" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">如果您遵循了指南，您会看到您的<code class="du mr ms mt mu b">rnn-denoise</code>文件夹中有以下文件。</p><pre class="ku kv kw kx fd mv mu mw mx aw my bi"><span id="f683" class="md kc hi mu b fi mz na l nb nc">_noisy_sample.wav  --&gt; original noisy speech<br/>_filtered_sample.wav  --&gt; denoised speech by truth gains<br/>_nn_filtered_sample.wav   --&gt; denoised speech by the NN gains from Keras<br/>_nn_fixedpoit_filtered_sample.wav   --&gt; denoised speech by NN gains from NNoM</span></pre><p id="8bce" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">从图形上看，结果如下:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es nr"><img src="../Images/223a28fa59718edc10cc7101740b857d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tvnIbP7hMpEH05S8.png"/></div></div></figure></div><div class="ab cl lm ln gp lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="hb hc hd he hf"><h1 id="ee97" class="kb kc hi bd kd ke lt kg kh ki lu kk kl io lv ip kn ir lw is kp iu lx iv kr ks bi translated">进一步的细节</h1><p id="d83b" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">总的来说，我建议你仔细阅读 python 代码，<code class="du mr ms mt mu b">gen_dataset.py</code>和<code class="du mr ms mt mu b">main.py</code>阅读代码中的注释，并用不同的配置进行测试。</p><h2 id="0460" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">培训用数据</h2><p id="0794" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated"><code class="du mr ms mt mu b">x_train</code>由 13 或 20 个 MFCC 系数和前 10 个系数的一阶和二阶导数组成，总共给出 33 或 40 个特征。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ns"><img src="../Images/f885bdee2cf13814108e568521e0b938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wpfsc0yGdKp3Ib5JH0ZoXg.png"/></div></div></figure><p id="f30f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><code class="du mr ms mt mu b">y_train</code>由两个数据组成，<code class="du mr ms mt mu b">gains</code>和<code class="du mr ms mt mu b">VAD</code>。</p><ul class=""><li id="52b3" class="nd ne hi ja b jb jc je jf jh nf jl ng jp nh jt ni nj nk nl bi translated">通过干净语音/有噪声语音的每个频带能量的平方根来计算增益。与 RNNoise 相同。</li><li id="af37" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">VAD 是由干净语音总能量高于阈值的地方产生的。对于一些样本，它被向前和向后扩展。</li></ul><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es nt"><img src="../Images/a28f0927642d2caaa7e353c0b3f22069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3pGT3ddgpsd73Yxm.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">用能量检测纯净语音中的周期</figcaption></figure><h2 id="1ad8" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">增益和语音活动检测</h2><p id="2ff2" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">在默认模式中，有一个只有一个神经的辅助输出，指示是否检测到<em class="jx">语音</em>。<code class="du mr ms mt mu b">1</code>检测到，<code class="du mr ms mt mu b">0</code>未检测到。在 MCU 示例<code class="du mr ms mt mu b">main_arm.c</code>中，这与一个板载 LED 相连，如果<code class="du mr ms mt mu b">VAD neural &gt; 0.5</code>出现，该 LED 就会亮起。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es nu"><img src="../Images/b8679137e2e0adbadf43fa80a8248e91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mX47tEyRc4y-QzEF.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">VAD 和 RNN 产量的前 10 个收益</figcaption></figure><h2 id="a134" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">均衡器</h2><p id="0b4b" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">该示例使用 20 个频段(默认)或 13 个频段(或您想要的任何数量的频段)来抑制噪声。每个频带由 1 阶(默认)或 2 阶(不稳定)IIR 带通滤波器滤波。两个相邻频带的-3dB(截止)是交叉的。频率响应如下所示:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es nv"><img src="../Images/08fbfbf46ae86cf4905c5df5b84893b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UAZCPzEPLioypDl6.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">均衡器中带通滤波器组的频率响应</figcaption></figure><p id="b1a1" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">信号将并行通过每个频带，然后叠加在一起。由于每个波段的重叠，信号可能听起来太响，会导致溢出(破裂噪声)。因此<code class="du mr ms mt mu b">0.6</code>的一个因子被乘以最终信号(不是一个数学计算的数字)。</p><h2 id="7425" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">模型结构</h2><p id="9d03" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">这个例子提供了两种不同的 RNN 模型。第一种是类似 RNNoise 结构，由许多单个 GRU 层之间的多个链接组成。这些链接使用连接来合并。除了均衡器的增益，它还输出语音活动检测(VAD)。该型号在<code class="du mr ms mt mu b">main.py</code>中是默认的，结构如下图所示。这个模型有大约<code class="du mr ms mt mu b">120k</code>的权重，比 RNNoise 大，因为它有一个额外的输入层。然而，模型的规模远远不够。你可以减少很多，但结果不会有任何明显的不同。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es nw"><img src="../Images/7dcbd2b601d6758bfe514c2a41cf7604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/0*b2l_EmWY8ln-tx-Q.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">RNN 整体结构</figcaption></figure><p id="3952" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">另一种是简单的堆叠 GRU。这将不提供 VAD 输出。令人惊讶是，它也工作得很好。为了测试这个，用<code class="du mr ms mt mu b">main.py</code>中的<code class="du mr ms mt mu b">train_simple(...)</code>替换线<code class="du mr ms mt mu b">history = train(...)</code></p><p id="6636" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">Keras 模型在<code class="du mr ms mt mu b">stateful=True</code>下训练，带<code class="du mr ms mt mu b">timestamp=1</code>不理想。<code class="du mr ms mt mu b">batch_size</code>等于实际的时间戳，因此我们需要使时间戳尽可能大，以使 BP 能够工作。我们用的是<code class="du mr ms mt mu b">batch_size &gt; 1024</code>。记得关掉<code class="du mr ms mt mu b">shuffle</code>。</p><blockquote class="ju jv jw"><p id="a044" class="iy iz jx ja b jb jc ij jd je jf im jg jy ji jj jk jz jm jn jo ka jq jr js jt hb bi translated">这个模型只需要 1 个时期就能过拟合…经常性的掉线是非常需要的。您还可以在每个 GRU 图层之间添加常规下降图层。</p></blockquote><h2 id="c806" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">MCU 示例</h2><p id="d64e" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">单片机例子是<code class="du mr ms mt mu b">main_arm.c</code>。这个例子运行在 STM32L476-Discovery 上。没有 RTOS 相关的依赖。</p><p id="958c" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">它使用来自板载麦克风的输入，并尝试过滤信号。目前，它只通过绿色 LED (PE8)输出一个 VAD，它<strong class="ja hj">不</strong>记录过滤后的语音或回放。然而，RNN 和均衡器是实现和信号滤波，人们可以实现自己的回放或记录现有的输出数据很容易。</p><p id="a222" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><code class="du mr ms mt mu b">main_arm.c</code>的功能部分与<code class="du mr ms mt mu b">main.c</code>相同</p><p id="99c2" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">如果你使用的是 ARM-Cortex M 芯片，开启下面的优化将有助于提高性能。</p><ul class=""><li id="1a90" class="nd ne hi ja b jb jc je jf jh nf jl ng jp nh jt ni nj nk nl bi translated">打开 CMSIS-NN 对 NNoM 的支持。参见<a class="ae ix" href="https://github.com/majianjia/nnom/blob/master/docs/Porting_and_Optimisation_Guide.md" rel="noopener ugc nofollow" target="_blank">移植和优化指南</a></li><li id="1885" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">在<code class="du mr ms mt mu b">mfcc.h</code>中，打开<code class="du mr ms mt mu b">PLATFORM_ARM</code>使用 ARM FFT</li></ul><h2 id="7a24" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">性能</h2><p id="114c" class="pw-post-body-paragraph iy iz hi ja b jb ly ij jd je lz im jg jh ma jj jk jl mb jn jo jp mc jr js jt hb bi translated">我们只关心 MCU 的性能。无论一个神经网络模型有多好，如果我们的 MCU 不能及时运行它，我们在这里所做的一切努力都将毫无意义。</p><p id="95d4" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">有 3 个计算量大的部分<em class="jx"> MFCC </em>、<em class="jx">神经网络</em>、<em class="jx">均衡器(EQ) </em>。所以我做了一个测试来评估这三个部分的耗时。</p><p id="e74f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">测试环境包括:</p><ul class=""><li id="56f3" class="nd ne hi ja b jb jc je jf jh nf jl ng jp nh jt ni nj nk nl bi translated">板卡:<a class="ae ix" href="https://www.st.com/en/evaluation-tools/32l476gdiscovery.html" rel="noopener ugc nofollow" target="_blank"> STM32L476-Discovery </a></li><li id="1296" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">MCU: STM32L476，超频@140MHz Cortex-M4</li><li id="6cb0" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">音频 src:嵌入式麦克风</li><li id="5133" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">音频输出:无(您可以将其连接到音频插孔)</li><li id="86d1" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">IDE:凯尔·MDK</li></ul><p id="5221" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">测试条件:</p><ul class=""><li id="f411" class="nd ne hi ja b jb jc je jf jh nf jl ng jp nh jt ni nj nk nl bi translated">NN 后端:CMSIS-NN 或 Loacl C 后端</li><li id="f4e2" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">FFT 库:<code class="du mr ms mt mu b">arm_rfft_fast_f32</code>或纯 C fft <a class="ae ix" href="https://github.com/lloydroc/arduino_fft" rel="noopener ugc nofollow" target="_blank"> arduino_fft </a></li><li id="ccc1" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">经过测试的编译器优化:<code class="du mr ms mt mu b">-O0/-O1</code>和<code class="du mr ms mt mu b">-O2</code></li><li id="1eff" class="nd ne hi ja b jb nm je nn jh no jl np jp nq jt ni nj nk nl bi translated">测试的均衡器频段:<code class="du mr ms mt mu b">13 band</code>和<code class="du mr ms mt mu b">20 band</code></li></ul><p id="f0b8" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">记住，我们的输入音频格式是<code class="du mr ms mt mu b">16kHz 1CH</code>，这意味着对于每个音频更新(<code class="du mr ms mt mu b">256</code>样本)，我们只有<code class="du mr ms mt mu b">256/16000 = 16ms</code>来完成整个工作。</p><p id="5fc4" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> 13 波段均衡器</strong></p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es nx"><img src="../Images/91d9f1956e22d12263bfc535a424e379.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZrSDvaGvpzuYmBrZEXVIyQ.png"/></div></div></figure><p id="600f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">测试结果令人印象深刻。使用优化程度最高的选项，总运行时间只有<code class="du mr ms mt mu b">6.18ms</code>，大约是总 CPU 负载的<code class="du mr ms mt mu b">38%</code>。纯 C 实现下(local+arduino_fft)，总运行时间还是在<code class="du mr ms mt mu b">16ms</code>之下。</p><p id="e144" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj"> 20 波段均衡器</strong></p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ny"><img src="../Images/18115bc9c729a09f9f73d6206e8172e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lPLHZXR4Y65Mp1R7dtrqHA.png"/></div></div></figure><p id="4a08" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">比较两者，<code class="du mr ms mt mu b">20</code>频段对均衡器的影响大于 NN。</p><h2 id="70e6" class="md kc hi bd kd me mf mg kh mh mi mj kl jh mk ml kn jl mm mn kp jp mo mp kr mq bi translated">NNoM 印刷的 RNN 模型信息</h2><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es nz"><img src="../Images/5dc0c5513a077edd36cdf96ee7fc5994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s2h2EGd2J_cPdkn5Zr6auA.png"/></div></div></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es oa"><img src="../Images/c7066af018d2037308991a0bbb5c6073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2lKx0lanVRYryvEC84k4sg.png"/></div></div></figure></div></div>    
</body>
</html>