<html>
<head>
<title>Principal Component Analysis(PCA) with code on MNIST dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MNIST数据集上的主成分分析(PCA)及其代码</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/principal-component-analysis-pca-with-code-on-mnist-dataset-da7de0d07c22?source=collection_archive---------0-----------------------#2019-09-03">https://medium.com/analytics-vidhya/principal-component-analysis-pca-with-code-on-mnist-dataset-da7de0d07c22?source=collection_archive---------0-----------------------#2019-09-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/87673d637e2e95017559b49db5536b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*PnqMbZEdnuL9yHuo.png"/></div></figure><p id="2ba6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">主成分分析被扩展用于高维数据可视化的降维。我们做降维，将高维数据集转换成n维数据，其中n <d. we="" usually="" set="" the="" threshold="" at="" d=""> 3。</d.></p><p id="7e97" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数据在一个轴上的分布非常大，但在另一个轴上的分布(方差)相对较小。传播只不过是方差或具有高信息，所以一般来说，<strong class="io hj">我们可以说高传播具有高信息。</strong>因此，我们可以跳过差异较小的维度，因为为了获得可视化效果，数据必须是列标准化的，因此信息较少。</p><h2 id="992b" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">二维到一维</h2><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es kf"><img src="../Images/80579b3c5de37656bd9f3846bbd3a20e.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/0*Jo1VWpTi6u6llQW9"/></div></figure><p id="a07a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们想找到v1变量方差最大的方向。为了做到这一点，在平面上旋转x轴。这里v1具有最大方差，v2具有最小方差，因此v1具有关于数据集的更多信息。因此最终具有(x，y)变量的2-D数据集可以被转换成v1方向上的1-D变量。</p><h2 id="910a" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">数学符号</h2><p id="47ea" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix km iz ja jb kn jd je jf ko jh ji jj hb bi translated">任务:找到u1，使得x(i)的投影和的方差最大。</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/cb80bc68ec731ea210f492cbf4770027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*8pW-sKN8FMgoFQ-Fsu1wWQ.png"/></div></figure><h2 id="c72c" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated"><strong class="ak">特征值和特征向量</strong></h2><p id="f027" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix km iz ja jb kn jd je jf ko jh ji jj hb bi translated">我们会找到协方差矩阵</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/148358a5fd263a462b967df382982ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/0*cgmVlr-0eTeJrudC.png"/></div></figure><p id="cc15" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于每个特征值，都有对应的特征向量。每一对特征向量都是相互垂直的。我们将按降序排列特征值。向量V1对应的最大特征值具有最大方差，意味着数据集的最大信息。同样，方差随着特征值的减小而减小。</p><p id="e708" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">将变量f1向V1方向投影，得到高方差向量。</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/9efbd6971667cb3c88dbbb196e11e562.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*X5QQmLTzK8aTB-40.jpg"/></div></figure><h2 id="e469" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">PCA的局限性:</h2><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/d5d4f5c7df00ae7bc3471bda60f2ba29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*X_Q55Ab2FUv6sF7XqAE0tw.png"/></div></figure><p id="1550" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果投影波形变形后，数据遵循某种波形结构。</p><h1 id="9f5a" class="kt jl hi bd jm ku kv kw jq kx ky kz ju la lb lc jx ld le lf ka lg lh li kd lj bi translated">带代码的MNIST数据集上的主成分分析</h1><p id="77c1" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix km iz ja jb kn jd je jf ko jh ji jj hb bi translated">这里有详细的代码:<a class="ae lk" href="https://github.com/ranasingh-gkp/PCA-TSNE-on-MNIST-dataset" rel="noopener ugc nofollow" target="_blank">https://github.com/ranasingh-gkp/PCA-TSNE-on-MNIST-dataset</a></p><p id="70e2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 1 —数据预处理</strong></p><p id="aaad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在应用PCA之前，必须将每个变量的均值转换为0，标准差转换为1。</p><pre class="kg kh ki kj fd ll lm ln lo aw lp bi"><span id="5b9a" class="jk jl hi lm b fi lq lr l ls lt"># Data-preprocessing: Standardizing the data<br/>#<a class="ae lk" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</a></span><span id="f130" class="jk jl hi lm b fi lu lr l ls lt"><strong class="lm hj">from sklearn.preprocessing import StandardScaler<br/>standardized_data = StandardScaler().fit_transform(data)<br/>print(standardized_data.shape)</strong></span></pre><p id="a190" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 2 —计算协方差矩阵</strong></p><pre class="kg kh ki kj fd ll lm ln lo aw lp bi"><span id="111e" class="jk jl hi lm b fi lq lr l ls lt">#find the co-variance matrix which is : A^T * A<br/><strong class="lm hj">sample_data = standardized_data</strong><br/># matrix multiplication using numpy<br/><strong class="lm hj">covar_matrix = np.matmul(sample_data.T , sample_data)</strong><br/>print ( “The shape of variance matrix = “, covar_matrix.shape)</span></pre><p id="65cc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 3 —计算特征值和特征向量</strong></p><pre class="kg kh ki kj fd ll lm ln lo aw lp bi"><span id="cbe6" class="jk jl hi lm b fi lq lr l ls lt"># finding the top two eigen-values and corresponding eigen-vectors <br/># for projecting onto a 2-Dim space.<br/>#<a class="ae lk" href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.eigh.html" rel="noopener ugc nofollow" target="_blank">https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.eigh.html</a></span><span id="2d86" class="jk jl hi lm b fi lu lr l ls lt"><strong class="lm hj">from scipy.linalg import eigh</strong></span><span id="0607" class="jk jl hi lm b fi lu lr l ls lt"># the parameter ‘eigvals’ is defined (low value to heigh value) <br/># eigh function will return the eigen values <strong class="lm hj">in asending order</strong><br/># this code generates only the top 2 <strong class="lm hj">(782 and 783)(index)</strong> eigenvalues.<br/><strong class="lm hj">values, vectors = eigh(covar_matrix, eigvals=(782,783))</strong></span><span id="52c7" class="jk jl hi lm b fi lu lr l ls lt"><strong class="lm hj">print(“Shape of eigen vectors = “,vectors.shape)</strong><br/># converting the eigen vectors into (2,d) shape for easyness of further computations<br/><strong class="lm hj">vectors = vectors.T</strong></span><span id="4473" class="jk jl hi lm b fi lu lr l ls lt"><strong class="lm hj">print(“Updated shape of eigen vectors = “,vectors.shape)</strong><br/># here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector<br/># here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector</span></pre><p id="e058" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">通过向量-向量乘法将原始数据样本投影到由两个主特征向量形成的平面上。</p><pre class="kg kh ki kj fd ll lm ln lo aw lp bi"><span id="9455" class="jk jl hi lm b fi lq lr l ls lt">import matplotlib.pyplot as plt<br/><strong class="lm hj">new_coordinates = np.matmul(vectors, sample_data.T)</strong></span></pre><p id="51a2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">将标注附加到2d投影数据(垂直堆栈)并创建新的数据框以绘制标注点。</p><pre class="kg kh ki kj fd ll lm ln lo aw lp bi"><span id="9035" class="jk jl hi lm b fi lq lr l ls lt">import pandas as pd</span><span id="6ce0" class="jk jl hi lm b fi lu lr l ls lt"><strong class="lm hj">new_coordinates = np.vstack((new_coordinates, labels)).T</strong><br/><strong class="lm hj">dataframe = pd.DataFrame(data=new_coordinates, columns=(“1st_principal”, “2nd_principal”, “label”))<br/>print(dataframe.head())</strong></span></pre><p id="84ea" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">(0，1，2，3，4是Xi，其他是主轴)</p><p id="e9e1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 4 —绘图</strong></p><pre class="kg kh ki kj fd ll lm ln lo aw lp bi"><span id="3b8e" class="jk jl hi lm b fi lq lr l ls lt"># plotting the 2d data points with seaborn<br/>import seaborn as sn<br/>sn.FacetGrid(dataframe, hue=”label”, size=6).map(plt.scatter, ‘1st_principal’, ‘2nd_principal’).add_legend()<br/>plt.show()</span></pre><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/cd8b7800c22c64c0763ec47f5753a40a.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*K3Z34xgx1oB5pp7daa4v3g.png"/></div></figure><p id="3ddd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">类别之间有很多重叠，这意味着PCA不太适合高维数据集。很少几个阶层可以分开，但大多数都是混合的。<strong class="io hj"> PCA主要用于降维</strong>，<strong class="io hj">不用于可视化</strong>。为了可视化高维数据，我们主要使用T-SNE(【https://github.com/ranasingh-gkp/PCA-TSNE-on-MNIST-dataset】和)</p><h2 id="d83d" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">5 —用于降维的主成分分析</h2><pre class="kg kh ki kj fd ll lm ln lo aw lp bi"><span id="ef6c" class="jk jl hi lm b fi lq lr l ls lt"># initializing the pca<br/><strong class="lm hj">from sklearn import decomposition<br/>pca = decomposition.PCA()</strong></span><span id="915c" class="jk jl hi lm b fi lu lr l ls lt"># PCA for dimensionality redcution (non-visualization)<br/><strong class="lm hj">pca.n_components = 784<br/>pca_data = pca.fit_transform(sample_data)</strong></span><span id="4550" class="jk jl hi lm b fi lu lr l ls lt"><strong class="lm hj">percentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_);<br/>cum_var_explained = np.cumsum(percentage_var_explained)</strong></span></pre><p id="e4a8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">测绘</p><pre class="kg kh ki kj fd ll lm ln lo aw lp bi"><span id="86ee" class="jk jl hi lm b fi lq lr l ls lt"># Plot the PCA spectrum<br/>plt.figure(1, figsize=(6, 4))<br/>plt.clf()<br/>plt.plot(cum_var_explained, linewidth=2)<br/>plt.axis(‘tight’)<br/>plt.grid()<br/>plt.xlabel(‘n_components’)<br/>plt.ylabel(‘Cumulative_explained_variance’)<br/>plt.show()</span></pre><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/549a4af48bb373d0c7d693f334a76305.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*w79AwxcOpEDGulotEJlTyA.png"/></div></figure><p id="791b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里我们画出了分量方差的累积和。这里300个成分解释了几乎90%的差异。所以我们可以根据要求的方差来降维。</p><p id="34e8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">=================谢谢===============</p><p id="0dd9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">参考:</strong></p><p id="d13e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">谷歌图片</p><p id="0ba0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae lk" href="https://colah.github.io/posts/2014-10-Visualizing-MNIST/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2014-10-Visualizing-MNIST/</a></p></div></div>    
</body>
</html>