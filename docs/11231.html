<html>
<head>
<title>A Review of Object Detection Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">目标检测模型综述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-review-of-object-detection-models-f575c515655c?source=collection_archive---------3-----------------------#2020-11-24">https://medium.com/analytics-vidhya/a-review-of-object-detection-models-f575c515655c?source=collection_archive---------3-----------------------#2020-11-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b9416c5d8cf9662ed9287ad7745dd643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6IOv24zYgd_Zt5iqYGridg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://unsplash.com/@agk42?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">亚历山大·奈特</a>在<a class="ae iu" href="https://unsplash.com/s/photos/ai-eye?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="52e8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">目标检测广泛应用于计算机视觉任务中。有许多基于对象检测的应用，例如自动驾驶汽车或对象跟踪。在这篇文章中，我将解释各种算法；R-CNN，快速R-CNN，更快R-CNN，SSD和YOLO。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="7344" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">R-CNN</h2><p id="3b18" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">R-CNN [1]之前使用了一些算法，如穷举搜索，但它们需要高计算性能和长时间才能找到图像上的最佳对象位置。为了解决这个问题，R-CNN提出了一种方法，我们使用选择性搜索从图像中提取大约2000个区域，称为区域建议。选择性搜索减少了考虑的地点数量。它解决了CNN的本地化问题，但还是太慢了。每张图像需要50秒的测试时间。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es la"><img src="../Images/18b0fc0f36acfd955026e60f5a4b3e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*ernO1b3iqSgd8oGYXHHDRw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd kc">【1】</strong></figcaption></figure><h2 id="0aed" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">快速R-CNN</h2><p id="c18e" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">快速R-CNN [2]被引入以解决R-CNN的缺点。不是将区域提议馈送给CNN，而是将整个输入图像馈送给CNN以生成特征图。从特征图中，我们识别区域提议，并将它们馈送到完全连接的层中。然后，我们使用Softmax层来预测类别和对象边界框的偏移值。Fast -RCNN比R-CNN快，因为每个图像只进行一次卷积运算。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/7fb3e49025e2aaa438dc527ed8bdfa0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*nqQwdehqqDLna1vuC6Pp_g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd kc">【2】</strong></figcaption></figure><h2 id="8739" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">更快的R-CNN</h2><p id="8df3" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">快速R-CNN [3]由提出区域的深度CNN和使用提出区域的快速R-CNN组成。更快的R-CNN比其他的要快得多，因为它使用了一种新的方法区域提议网络(RPN)来代替选择性搜索。RPN主要告诉快速R-CNN去哪里找。类似于快速R-CNN，单个CNN将整个图像作为输入，并产生特征图。在特征图上，RPN生成一组矩形对象提议，并以对象性分数作为输出。然后，使用RoI池对这些值进行整形，以预测边界框的类和偏移值。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/6f92d6a6041dcbbcc29046c2409d31f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*3mD7t5MGySouRJHl6Gejnw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd kc">【3】</strong></figcaption></figure><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es la"><img src="../Images/5d7552c6471da74def81302503849920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*HkGZyuYeLNG6UGazrH_3Tw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">R-CNN家族的比较</figcaption></figure><h2 id="f18c" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">YOLO</h2><p id="ef2b" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">第一版YOLO [4]是由约瑟夫·雷德蒙在2015年推出的。YOLO有一个单一的神经网络，在一次评估中直接从整个图像预测边界框和类别概率。R-CNN家族拥有复杂的管道，速度比YOLO慢。在这个模型中，“你只看一次”图像就可以预测物体的类别和它们的位置。YOLO将图像分成网格，每个网格单元预测边界框和这些框的置信度得分。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/2c5313322176c8d89e4bb49246df7eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*ZT-nijC3BpdViF_M8Roplw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx">[4]</figcaption></figure><p id="0023" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Yolo的第一个版本非常快，但也有缺点。网络与小对象斗争，这在后来的版本中得到了解决；约洛夫2，约洛夫3，约洛夫4。</p><h2 id="f321" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">单次发射探测器</h2><p id="408f" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">SSD [5]于2015年推出了单次多盒检测器。它比以前的对象检测模型(如YOLO)更快。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es li"><img src="../Images/a1f04039667779ab276ffc076d2a21c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*XjgxI3rLOSjc8bdONjARqA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd kc">【5】</strong></figcaption></figure><p id="e85b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将图像输入VGG-16，以提取不同比例的特征地图。此外，它通过使用卷积过滤器来预测对象类别和边界框位置的偏移。SSD为对象类实例的存在产生固定大小的边界框和分数的集合。则应用非最大抑制并产生最终检测。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es la"><img src="../Images/9c5beb950f5f53ea1f5161f565157b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*cJGXWcmAdhmicjMpJuCFmQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd kc"/></figcaption></figure><h2 id="654d" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">YOLOv4</h2><p id="9e40" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">yolov 4[6]是最先进的物体检测技术。它在实时检测应用中速度极快。Yolov4使用CSPDARKNET53型号作为主干。它是一个训练YOLO的框架，有29个卷积层和2760万个参数。此外，Yolov4由SPP和PAN作为颈部组成，YOLOv3作为头部。您可以使用预训练的Yolov4来训练您的自定义对象检测模型。访问<a class="ae iu" href="https://github.com/AlexeyAB/darknet" rel="noopener ugc nofollow" target="_blank">暗网回购</a>了解更多信息。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/3f6c166b8403427da704ac355c5fa23e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*SnjcWGpeClUN9XwxpOoJGg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd kc">【6】</strong></figcaption></figure></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="1192" class="lk kb hi bd kc ll lm ln kg lo lp lq kk lr ls lt kn lu lv lw kq lx ly lz kt ma bi translated">参考资料:</h1><p id="b266" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated"><strong class="ix hj">【1】</strong>用于精确对象检测和语义分割的丰富特征层次:<a class="ae iu" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1311.2524.pdf</a></p><p id="02cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">【2】</strong>快速R-CNN:<a class="ae iu" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1504.08083.pdf</a></p><p id="4006" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">【3】</strong>更快的R-CNN:利用区域提议网络实现实时目标检测:<a class="ae iu" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.01497.pdf</a></p><p id="7b98" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">【4】【https://arxiv.org/pdf/1506.02640.pdf】你只看一次:统一、实时的物体检测:<a class="ae iu" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank"/></strong></p><p id="5730" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">【5】</strong>SSD:单次多盒探测器:<a class="ae iu" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.02640.pdf</a></p><p id="866e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">【6】</strong>yolov 4:物体检测的最佳速度和精度:<a class="ae iu" href="https://arxiv.org/pdf/2004.10934.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2004.10934.pdf</a></p></div></div>    
</body>
</html>