<html>
<head>
<title>ML14: PyTorch — MLP on MNIST</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML14:py torch——MNIST 的 MLP</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml14-f03f75254934?source=collection_archive---------7-----------------------#2020-12-19">https://medium.com/analytics-vidhya/ml14-f03f75254934?source=collection_archive---------7-----------------------#2020-12-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="08ae" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">图像分类的第一步(98.13%的准确率)</h2></div><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="2b56" class="jg jh hi jc b fi ji jj l jk jl">Read time: 20 min</span><span id="8630" class="jg jh hi jc b fi jm jj l jk jl">Complete code on Colab: <a class="ae jn" href="https://bit.ly/34yjiod" rel="noopener ugc nofollow" target="_blank">https://bit.ly/34yjiod</a></span></pre><p id="9003" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">DL 领域有两大领域，即解决图像相关问题(计算机视觉)和解决文本相关问题(自然语言处理)。然而，DL 社区经常把焦点放在与图像相关的问题上，而不是与文本相关的问题上，因为前者更有利可图。</p><p id="05a3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">本文通过著名的数据集 MNIST 来说明神经网络模型的工作流程。在 MNIST MLP 的测试数据上，我们得到了 98.13%的准确率。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><blockquote class="kr ks kt"><p id="7001" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated"><strong class="jq hj"> <em class="hi">轮廓</em></strong><em class="hi"><br/>(1)</em><a class="ae jn" href="#c4cc" rel="noopener ugc nofollow"><em class="hi">MLP</em></a><em class="hi"><br/>(2)</em><a class="ae jn" href="#6f88" rel="noopener ugc nofollow"><em class="hi">准备数据</em></a><em class="hi"><br/>(3)</em><a class="ae jn" href="#6c34" rel="noopener ugc nofollow"><em class="hi">MNIST 的图像</em></a><em class="hi"><br/>(4)</em><a class="ae jn" href="#ea80" rel="noopener ugc nofollow"><em class="hi">初始化 MLP </em> </a> <em class="hi"> <br/></em></p></blockquote></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><p id="2b2a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们在<strong class="jq hj"> ML13 </strong>中尝试了 NN 简单线性回归，并浏览了<strong class="jq hj"> ML04 </strong>中的理论，现在我们继续到 MLP。</p><div class="ky kz ez fb la lb"><a href="https://merscliche.medium.com/ml13-e52e251d41c5" rel="noopener follow" target="_blank"><div class="lc ab dw"><div class="ld ab le cl cj lf"><h2 class="bd hj fi z dy lg ea eb lh ed ef hh bi translated">ML13: PyTorch —简单线性回归</h2><div class="li l"><h3 class="bd b fi z dy lg ea eb lh ed ef dx translated">使用 torch.nn 构建一个简单的用例</h3></div><div class="lj l"><p class="bd b fp z dy lg ea eb lh ed ef dx translated">merscliche.medium.com</p></div></div><div class="lk l"><div class="ll l lm ln lo lk lp lq lb"/></div></div></a></div><div class="ky kz ez fb la lb"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/ml04-ce0b172deb2b"><div class="lc ab dw"><div class="ld ab le cl cj lf"><h2 class="bd hj fi z dy lg ea eb lh ed ef hh bi translated">ML04:从 ML 到 DL 再到 NLP</h2><div class="li l"><h3 class="bd b fi z dy lg ea eb lh ed ef dx translated">简明概念图</h3></div><div class="lj l"><p class="bd b fp z dy lg ea eb lh ed ef dx translated">medium.com</p></div></div><div class="lk l"><div class="lr l lm ln lo lk lp lq lb"/></div></div></a></div></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="c4cc" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(1) MLP</h1><ul class=""><li id="7e52" class="mj mk hi jq b jr ml ju mm jx mn kb mo kf mp kj mq mr ms mt bi translated">神经元是神经网络的最小单位。感知器是单层神经网络。[1]</li><li id="7402" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">前馈神经网络(FNN)是一种人工神经网络，其中节点之间的连接不形成循环。因此，它不同于它的后代:递归神经网络(RNN) 。FNN 是第一个也是最简单的人工神经网络。在这个网络中，信息只在一个方向上移动——向前——从输入节点通过隐藏节点(如果有的话)到达输出节点。网络中没有循环或环路。[2]</li><li id="65d4" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">简言之，</li></ul><ol class=""><li id="d575" class="mj mk hi jq b jr js ju jv jx mz kb na kf nb kj nc mr ms mt bi translated">FNN: MLP(多层感知器，也称为“全连接”网络)，CNN(卷积神经网络)</li><li id="8c6d" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj nc mr ms mt bi translated">RNN: RNN(循环神经网络)，LSTM(长短期记忆)，GRU(门控循环单元)</li></ol><p id="4344" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">更多相关 NN 理论详细查看<a class="ae jn" href="https://becominghuman.ai/ml04-ce0b172deb2b" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> ML04 </strong> </a>。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="6f88" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated"><em class="nd"> (2)准备数据</em></h1><p id="346c" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx ne jz ka kb nf kd ke kf ng kh ki kj hb bi translated">MNIST 是 DL 舞台上一个值得注意的影像资料。我们输入 MNIST，并把它们分成训练和测试数据。训练数据有 60，000 个数据点(图像)，而测试数据有 10，000 个数据点。有几件事值得注意:</p><ul class=""><li id="b153" class="mj mk hi jq b jr js ju jv jx mz kb na kf nb kj mq mr ms mt bi translated">下载:如果为“真”，则将数据集下载到工作目录当数据集已经存在时，不执行任何操作。</li><li id="23ac" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">torch.utils.data.DataLoader:启用“shuffle”和“sample”这两个参数。</li><li id="5cea" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">batch_size = 100(我们必须在这里决定批量大小)</li></ul><figure class="ix iy iz ja fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nh"><img src="../Images/6ad6e3ec85eab5fb034451e87d7e01da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DGPyGXwNdJDSz7wqHc-5xA.png"/></div></div><figcaption class="no np et er es nq nr bd b be z dx translated">图 1: 60，000 张照片，每张 28 个单位 x 28 个单位。</figcaption></figure><p id="8fa4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">注意张量是 60，000 x 28 x 28。图像是 28 个单位× 28 个单位，所以我们必须设置<strong class="jq hj"> input_size = 784 = 28 x 28 </strong>。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="6c34" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(3)MNIST 的形象</h1><p id="a5b0" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx ne jz ka kb nf kd ke kf ng kh ki kj hb bi translated">MNIST 数据库(改进的国家标准和技术研究所数据库)是一个手写数字的大型数据库，通常用于训练各种图像处理系统。[3]</p><figure class="ix iy iz ja fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es ns"><img src="../Images/5d87c973c0c4d69f470b042e119a35f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pi1OPgrXJmvZT4hJq72KIA.png"/></div></div><figcaption class="no np et er es nq nr bd b be z dx translated">图 2:MNIST 的图像。</figcaption></figure></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="ea80" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(4)初始化 MLP</h1><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="7ee5" class="jg jh hi jc b fi ji jj l jk jl"># Neural Network Model (1 hidden layer)<br/>class Net(nn.Module):<br/>    def __init__(self, input_size, hidden_size, num_classes):<br/>        super(Net, self).__init__()<br/>        self.fc1 = nn.Linear(input_size, hidden_size) <br/>        self.relu = nn.ReLU()<br/>        self.fc2 = nn.Linear(hidden_size, num_classes)  <br/>    <br/>    def forward(self, x):<br/>        out = self.fc1(x)<br/>        out = self.relu(out)<br/>        out = self.fc2(out)<br/>        return out</span><span id="daa1" class="jg jh hi jc b fi jm jj l jk jl"># Neurons of each layer<br/>input_size = 784<br/>hidden_size = 500  <br/>num_classes = 10</span><span id="cc4b" class="jg jh hi jc b fi jm jj l jk jl"># Initialization<br/>MLP = Net(input_size, hidden_size, num_classes)</span><span id="d844" class="jg jh hi jc b fi jm jj l jk jl"># Hyperparameter<br/>num_epochs = 5<br/>batch_size = 100 # Recall that we set it before<br/>learning_rate = 0.001</span><span id="2c1e" class="jg jh hi jc b fi jm jj l jk jl"># Loss function and optimizer<br/>criterion = nn.CrossEntropyLoss()  <br/>optimizer = torch.optim.Adam(MLP.parameters(), lr=learning_rate)</span></pre><p id="5390" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们设置具有 784 个神经元的输入层(取决于图像的大小)，具有 500 个神经元的隐藏层，以及具有 10 个神经元的输出层(因为我们被给予 10 个类别的分类任务)。</p><h2 id="1eed" class="jg jh hi bd lt nt nu nv lx nw nx ny mb jx nz oa md kb ob oc mf kf od oe mh of bi translated">超参数</h2><ul class=""><li id="261f" class="mj mk hi jq b jr ml ju mm jx mn kb mo kf mp kj mq mr ms mt bi translated">输入大小= 784</li><li id="b39a" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">隐藏大小= 500</li><li id="3794" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">数量类= 10</li><li id="d204" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">次数= 5</li><li id="8430" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">batch_size = 100(回想一下我们之前设置的)</li><li id="5a82" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">学习率= 0.001</li></ul><h2 id="2c74" class="jg jh hi bd lt nt nu nv lx nw nx ny mb jx nz oa md kb ob oc mf kf od oe mh of bi translated">激活器、损失函数和优化器</h2><ul class=""><li id="3663" class="mj mk hi jq b jr ml ju mm jx mn kb mo kf mp kj mq mr ms mt bi translated">激活功能:ReLU</li><li id="09c6" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">损失函数:交叉熵</li><li id="0413" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">优化器:Adam</li></ul></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="5655" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(5)培训 MLP</h1><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="4c0d" class="jg jh hi jc b fi ji jj l jk jl">def rightness(predictions, labels):<br/>    '''<br/>    Calculate the prediction error rate<br/>    1. "predictions" gives a series of predictions, which is a  "batch_size" x "num_classes" matrix.<br/>    2. "labels" are correct answer<br/>    '''<br/>    pred = torch.max(predictions.data, 1)[1] <br/>    # For the first dimension of every row (every image), ouput the index of the biggest elements in every row.<br/>    rights = pred.eq(labels.data.view_as(pred)).sum() <br/>    # Compare the indexs with categories in "labels", and get the accumulated correct numbers.<br/>    return rights, len(labels) <br/>    # Return the correct numbers and all samples.</span><span id="1a6d" class="jg jh hi jc b fi jm jj l jk jl">record = [] # A container recording the training accuracies</span><span id="fef0" class="jg jh hi jc b fi jm jj l jk jl">for epoch in range(num_epochs):</span><span id="72c0" class="jg jh hi jc b fi jm jj l jk jl">train_rights = [] # Record the training accuracies</span><span id="0e48" class="jg jh hi jc b fi jm jj l jk jl">for i, (images, labels) in enumerate(train_loader):  <br/>        <br/>        # Convert torch tensor to Variable<br/>        images = Variable(images.view(-1, 28*28))<br/>        labels = Variable(labels)</span><span id="ee7e" class="jg jh hi jc b fi jm jj l jk jl"># Forward + Backward + Optimizer<br/>        optimizer.zero_grad()  # zero the gradient buffer<br/>        outputs = net(images)<br/>        loss = criterion(outputs, labels)<br/>        loss.backward()<br/>        optimizer.step()</span><span id="6f0e" class="jg jh hi jc b fi jm jj l jk jl">right = rightness(outputs, labels) # (outputs, labels) = (correct numbers, all samples)<br/>        train_rights.append(right)<br/></span><span id="43d3" class="jg jh hi jc b fi jm jj l jk jl">if (i+1) % 200 == 0:<br/>          <br/>          train_r = (sum([tup[0] for tup in train_rights]), sum([tup[1] for tup in train_rights]))<br/>          train_accuracy = 100. * train_r[0].numpy() / train_r[1]<br/>          total_step = len(train_dataset)//batch_size</span><span id="a64c" class="jg jh hi jc b fi jm jj l jk jl">print ('Epoch [{:d}/{:d}], Step [{:3d}/{:d}], Loss: {:.4f} | training accuracy: {:5.2f} %'.format( <br/>                 epoch+1, num_epochs, i+1, total_step, loss.data, train_accuracy))</span><span id="df42" class="jg jh hi jc b fi jm jj l jk jl">record.append(100 - 100. * train_r[0] / train_r[1])</span></pre><figure class="ix iy iz ja fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es og"><img src="../Images/8e539ea1246fc61164762e9baee53d86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDxaJQeKLJM7xdOIOfdIHA.png"/></div></div><figcaption class="no np et er es nq nr bd b be z dx translated">图 3:培训过程。</figcaption></figure></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="b186" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(6)训练和测试数据的准确性</h1><figure class="ix iy iz ja fd ni er es paragraph-image"><div class="er es oh"><img src="../Images/b91c8a6411318e80dadb4ce7ef94d9cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*tx6AGyuxv-koXXwuWzAVKQ.png"/></div><figcaption class="no np et er es nq nr bd b be z dx translated">图 4:训练错误率。</figcaption></figure><figure class="ix iy iz ja fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es oi"><img src="../Images/885334f89e5c207b592eca2b6180e7da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Abs74mu5YvMKZyrvPP3Zsw.png"/></div></div><figcaption class="no np et er es nq nr bd b be z dx translated">图 5:训练数据和测试数据的最终模型精度。</figcaption></figure><ul class=""><li id="0dd5" class="mj mk hi jq b jr js ju jv jx mz kb na kf nb kj mq mr ms mt bi translated">训练准确率:99.44 %</li><li id="5992" class="mj mk hi jq b jr mu ju mv jx mw kb mx kf my kj mq mr ms mt bi translated">测试准确度:<strong class="jq hj"> 98.13% </strong></li></ul></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="7ff3" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(7) <em class="nd">模型保存</em></h1><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="9a78" class="jg jh hi jc b fi ji jj l jk jl">torch.save(MLP.state_dict(), 'ML14_MLP_parameter.pkl') # Save the parameters<br/>torch.save(MLP, 'ML14_MLP_whole_model.pkl') # Save the whole model</span></pre><p id="4522" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在软件崩溃、硬件损坏或紧急断电的情况下，在训练神经网络模型时定期保存参数是至关重要的。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="c561" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated"><em class="nd"> (8)总结</em></h1><p id="2611" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx ne jz ka kb nf kd ke kf ng kh ki kj hb bi translated">在 MNIST MLP 的测试数据上，我们得到了 98.13%的准确率。</p><p id="d242" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">到目前为止，我们从:NN/DL 理论(<a class="ae jn" href="https://becominghuman.ai/ml04-ce0b172deb2b" rel="noopener ugc nofollow" target="_blank"> ML04 </a> ) = &gt;一个仅仅由 NumPy ( <a class="ae jn" href="https://becominghuman.ai/ml05-8771620a2023" rel="noopener ugc nofollow" target="_blank"> ML05 </a> ) = &gt;一个详细的 PyTorch 教程(<a class="ae jn" href="https://merscliche.medium.com/ml12-59d2a56737ac" rel="noopener"> ML12 </a> ) = &gt; NN 使用 PyTorch 的简单线性回归(<a class="ae jn" href="https://merscliche.medium.com/ml13-e52e251d41c5" rel="noopener"> ML13 </a> ) = &gt; MLP 在 MNIST 使用 PyTorch (ML14。这一个。).接下来，我们将在 ML15 中使用 PyTorch 转到 CNN 关于 MNIST 的报道。</p><div class="ky kz ez fb la lb"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/ml15-56c033cc00e9"><div class="lc ab dw"><div class="ld ab le cl cj lf"><h2 class="bd hj fi z dy lg ea eb lh ed ef hh bi translated">ML15:py torch——CNN 关于 MNIST 的报道</h2><div class="li l"><h3 class="bd b fi z dy lg ea eb lh ed ef dx translated">计算机视觉领域的资深人士(准确率 99.07%)</h3></div><div class="lj l"><p class="bd b fp z dy lg ea eb lh ed ef dx translated">medium.com</p></div></div><div class="lk l"><div class="oj l lm ln lo lk lp lq lb"/></div></div></a></div></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="7328" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(9)参考文献</h1><p id="fedf" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx ne jz ka kb nf kd ke kf ng kh ki kj hb bi translated">[1]维基百科(身份不明)。感知器。检索自<br/><a class="ae jn" href="https://en.wikipedia.org/wiki/Perceptron" rel="noopener ugc nofollow" target="_blank">【https://en.wikipedia.org/wiki/Perceptron】</a><br/>【2】维基百科(身份不明)。前馈神经网络。检索自【https://en.wikipedia.org/wiki/Feedforward_neural_network】<a class="ae jn" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank"/><br/>【3】维基百科(身份不明)。MNIST 数据库。检索自<br/><a class="ae jn" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/MNIST_database</a></p><h2 id="6753" class="jg jh hi bd lt nt nu nv lx nw nx ny mb jx nz oa md kb ob oc mf kf od oe mh of bi translated">(中文)</h2><p id="93b3" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx ne jz ka kb nf kd ke kf ng kh ki kj hb bi">[4] 張校捷 (2020)。深入淺出 PyTorch：從模型到源碼。北京，中國：電子工業。<br/>[5] 集智俱樂部 (2019)。深度學習原理與 PyTorch 實戰。北京，中國：人民郵電。<br/>[6] 邢夢來等人 (2018)。深度学习框架 PyTorch 快速开发与实战。北京，中國：電子工業。</p></div></div>    
</body>
</html>