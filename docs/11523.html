<html>
<head>
<title>A beginner’s guide to web scrapping using Beautiful Soup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用美丽的汤网报废的初学者指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-beginners-guide-to-web-scrapping-using-beautiful-soup-9f91a400c324?source=collection_archive---------4-----------------------#2020-12-07">https://medium.com/analytics-vidhya/a-beginners-guide-to-web-scrapping-using-beautiful-soup-9f91a400c324?source=collection_archive---------4-----------------------#2020-12-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/43474129d2d5bd4f844361fcab2c256d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6UIaApn54TOkhOQ607z-cw.jpeg"/></div></div></figure><p id="a1e7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个博客中，我们将学习使用请求和美丽的汤来清理网页。还可以学习如何从电子商务网站获取数据。</p><h1 id="5e0c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">什么是Web报废？</strong></h1><p id="6954" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">Web抓取是以自动方式收集结构化web数据的过程。<br/>一般来说，网络抓取本质上是提取大量公开可用的网络数据，以做出更明智的决策。<br/> <br/>如果你曾经从一个网站上复制粘贴过信息，那么你已经完成了和任何网页抓取器一样的功能，但是是手动的。与单调乏味、令人麻木的手动提取数据的过程不同，web抓取使用智能自动化从互联网看似无穷无尽的前沿检索数百、数百万甚至数十亿个数据点。</p><h1 id="6240" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">Python中使用的库</strong></h1><ul class=""><li id="a0c3" class="kr ks hi is b it km ix kn jb kt jf ku jj kv jn kw kx ky kz bi translated"><strong class="is hj">请求库:</strong><a class="ae la" href="http://docs.python-requests.org/en/master/" rel="noopener ugc nofollow" target="_blank">请求</a>库是在Python中发出请求的标准库。之后，向网站发出请求，网页的全部内容以可以分析的文本形式提供。</li><li id="7252" class="kr ks hi is b it lb ix lc jb ld jf le jj lf jn kw kx ky kz bi translated"><strong class="is hj">美汤:</strong> <a class="ae la" href="http://www.crummy.com/software/BeautifulSoup/" rel="noopener ugc nofollow" target="_blank">美汤</a>是一个从HTML和XML文件中抽取数据的Python库。它与一个解析器一起工作，用于导航、搜索和修改解析树。</li></ul><h1 id="1387" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">抓取数据的基本结构:</strong></h1><p id="cae0" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">以下代码是从维基百科抓取数据的概述。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="62b3" class="lp jp hi ll b fi lq lr l ls lt">url = "<a class="ae la" href="https://en.wikipedia.org/wiki/States_and_union_territories_of_India" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/States_and_union_territories_of_India</a>"</span><span id="cc99" class="lp jp hi ll b fi lu lr l ls lt">headers = {}  #To get header; google search 'what is my user agent'</span><span id="edf0" class="lp jp hi ll b fi lu lr l ls lt">search_response=requests.get(url,headers = headers)</span><span id="33cc" class="lp jp hi ll b fi lu lr l ls lt">if search_response.status_code == 200:<br/>    soup=BeautifulSoup(search_response.content,'html.parser')</span></pre><p id="3ced" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面详细解释了每个步骤:</p><ol class=""><li id="83fd" class="kr ks hi is b it iu ix iy jb lv jf lw jj lx jn ly kx ky kz bi translated"><strong class="is hj"> Get请求:</strong>Get方法表示您正在尝试从特定网站获取或检索数据。</li></ol><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="f97e" class="lp jp hi ll b fi lq lr l ls lt">search_response=requests.get(url,headers = headers)</span></pre><p id="ab81" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">参数:</p><p id="10b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">url —您要从中抓取数据的网站。</p><p id="ede0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">headers: Header帮助为请求设置一个客户用户代理。谷歌'什么是我的用户代理'，并得到你的标题。</p><p id="b42b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 2。检查状态代码:</strong>状态代码给出了请求的状态信息。200 OK状态意味着您的请求成功，而404 NOT FOUND状态意味着您要寻找的资源没有找到。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="7d42" class="lp jp hi ll b fi lq lr l ls lt">if search_response.status_code == 200:</span></pre><p id="3cf0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">始终检查状态代码是否为200。</p><p id="0ba1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3。</strong> <strong class="is hj">使用beautiful soup提取数据:</strong> Beautiful Soup帮助从响应接收的内容中解析数据。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="41af" class="lp jp hi ll b fi lq lr l ls lt">soup=BeautifulSoup(search_response.content,'html.parser')</span></pre><h1 id="9db1" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">使用美汤提取数据:</strong></h1><p id="4717" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">Beautiful Soup解析从request收到的内容。</p><ol class=""><li id="8228" class="kr ks hi is b it iu ix iy jb lv jf lw jj lx jn ly kx ky kz bi translated">首先，您必须将响应中的HTML文本转换成可以遍历和搜索的嵌套结构。</li></ol><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="3cda" class="lp jp hi ll b fi lq lr l ls lt">soup=BeautifulSoup(product_response.content,’html.parser’)</span></pre><p id="e3f7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">product_response.content获取页面内容。我们使用html解析器。</p><p id="366b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.现在使用其他各种漂亮的汤来获取数据。要想知道从哪里开始，你可以先了解HTML的结构。为此:</p><ul class=""><li id="49e8" class="kr ks hi is b it iu ix iy jb lv jf lw jj lx jn kw kx ky kz bi translated">访问要从中提取数据的网站。</li><li id="040c" class="kr ks hi is b it lb ix lc jb ld jf le jj lf jn kw kx ky kz bi translated">拾取要提取的对象。左键单击并选择“检查”。</li></ul><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/91f02df8758670e3951391df83af0db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*u9Y9c3MqhSY9S0a8IYN3MA.png"/></div></figure><ul class=""><li id="16d0" class="kr ks hi is b it iu ix iy jb lv jf lw jj lx jn kw kx ky kz bi translated">右手边，该部分的HTML结构将显示在右上角。</li></ul><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/a3e2bb0fd219dab8aa66f6b051adbe9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*SsZ9F9EyS2ZC2hYuGlIhpQ.png"/></div></figure><p id="8753" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.您现在可以使用相关标签来提取数据。在这种情况下是页面的标题。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="f18d" class="lp jp hi ll b fi lq lr l ls lt">soup.find('h1',{'id':'firstHeading'}).get_text()</span></pre><h1 id="41ff" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">从亚马逊抓取评论:</strong></h1><p id="5733" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">目的是提取网站中列出的各种earpods的评论。</p><ol class=""><li id="d657" class="kr ks hi is b it iu ix iy jb lv jf lw jj lx jn ly kx ky kz bi translated">亚马逊上列出的每个产品都有一个独一无二的ASIN编号，有助于识别产品。因此，首先要做的是获取各种产品的ASIN编号。</li></ol><ul class=""><li id="6e56" class="kr ks hi is b it iu ix iy jb lv jf lw jj lx jn kw kx ky kz bi translated">刮搜索页面为一个新的号码。</li><li id="4077" class="kr ks hi is b it lb ix lc jb ld jf le jj lf jn kw kx ky kz bi translated">一个搜索网址通常看起来像“www . Amazon . in/s？k = earpods”，earpods可以替换成你想搜索的任何产品。</li><li id="96b0" class="kr ks hi is b it lb ix lc jb ld jf le jj lf jn kw kx ky kz bi translated">要浏览搜索页面，请添加' &amp;page=2 '或您想点击的任何其他页面。最后，网址应该看起来像这样，'www.amazon.in/s？k=earpods&amp;page=2 '。</li><li id="a53a" class="kr ks hi is b it lb ix lc jb ld jf le jj lf jn kw kx ky kz bi translated">现在你可以刮一个号码。</li></ul><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="3d0b" class="lp jp hi ll b fi lq lr l ls lt">url = '<a class="ae la" href="https://www.amazon.in/s?k=earpods&amp;page=2'" rel="noopener ugc nofollow" target="_blank">https://www.amazon.in/s?k=earpods&amp;page=2'</a></span><span id="eade" class="lp jp hi ll b fi lu lr l ls lt">search_response=requests.get(url,headers = headers)</span><span id="0381" class="lp jp hi ll b fi lu lr l ls lt">if search_response.status_code == 200:<br/>    soup=BeautifulSoup(search_response.content,'html.parser')<br/>    print(soup.find('div',{'data-component-type':'s-search-result'})['data-asin'])</span></pre><p id="e293" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">报废的一个ASIN是' B08G1P26MJ '。</p><p id="f3f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.访问每个产品页面，获得所有评论的链接。</p><ul class=""><li id="d7cc" class="kr ks hi is b it iu ix iy jb lv jf lw jj lx jn kw kx ky kz bi translated">现在我们有了一个新的号码，我们可以点击产品的网址。</li><li id="107f" class="kr ks hi is b it lb ix lc jb ld jf le jj lf jn kw kx ky kz bi translated">一个产品的网址看起来像，'www.amazon.in/dp/B08G1P26MJ'。</li></ul><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="13d1" class="lp jp hi ll b fi lq lr l ls lt">url = '<a class="ae la" href="https://www.amazon.in/dp/B08G1P26MJ'" rel="noopener ugc nofollow" target="_blank">https://www.amazon.in/dp/B08G1P26MJ'</a></span><span id="e17d" class="lp jp hi ll b fi lu lr l ls lt">search_response=requests.get(url,headers = headers)</span><span id="9f47" class="lp jp hi ll b fi lu lr l ls lt">if search_response.status_code == 200:<br/>    soup=BeautifulSoup(search_response.content,'html.parser')<br/>    review_link = "<a class="ae la" href="https://www.amazon.in/" rel="noopener ugc nofollow" target="_blank">https://www.amazon.in/</a>"+soup.find('a',{'data-hook':'see-all-reviews-link-foot'})['href']<br/>    print(review_link)</span></pre><p id="a434" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.抓取产品详情和评论链接。</p><ul class=""><li id="ac36" class="kr ks hi is b it iu ix iy jb lv jf lw jj lx jn kw kx ky kz bi translated">访问评论链接和刮评论，星级等。</li></ul><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="571d" class="lp jp hi ll b fi lq lr l ls lt">search_response=requests.get(review_link,headers = headers)</span><span id="0c5b" class="lp jp hi ll b fi lu lr l ls lt">if search_response.status_code == 200:<br/>    soup=BeautifulSoup(search_response.content,'html.parser')<br/>    print('Star: ',soup.find('i',{'data-hook':'review-star-rating'}).get_text().strip()[0:3])<br/>    print('Title: ',soup.find('a',{'data-hook':'review-title'}).get_text())<br/>    print('Review: ',soup.find('span',{'data-hook':'review-body'}).get_text())</span></pre><h1 id="e97a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">结论:</strong></h1><p id="9939" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在这篇博客中，我试图帮助你建立一种直觉，如何从任何网站搜集数据。如果你想看我用来从亚马逊提取评论的代码，请点击<a class="ae la" href="https://github.com/SanjaPanda/Amazon-Review-Analysis/blob/main/Data%20Extraction%20using%20Beautiful%20Soup.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>查看。</p></div></div>    
</body>
</html>