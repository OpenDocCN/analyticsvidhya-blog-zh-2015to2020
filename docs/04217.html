<html>
<head>
<title>A review on Super-Resolution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超分辨率研究综述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-review-on-super-resolution-2c78cd77885a?source=collection_archive---------5-----------------------#2020-03-10">https://medium.com/analytics-vidhya/a-review-on-super-resolution-2c78cd77885a?source=collection_archive---------5-----------------------#2020-03-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6282" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从高分辨率图像到低分辨率图像的转换是传输、模糊、压缩以及过程中引入的不同伪像的结果。或者，在大多数情况下，图像是用低分辨率相机拍摄的，这意味着图像看起来模糊或有其他图像退化。超分辨率是从低分辨率图像到高分辨率图像的过程。低分辨率和高分辨率图像之间的关系由以下等式给出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/2558ec8ca47caf32b00c6b9b656514a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlnLBzk4LUSrFAh4_lYnAg.png"/></div></div></figure><p id="65a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> <em class="jp"> Ix </em> </strong>为退化图像或LR图像，而<strong class="ih hj"> <em class="jp"> Iy </em> </strong>为对应的HR图像。<strong class="ih hj"> <em class="jp"> D </em> </strong>是退化映射函数，而<strong class="ih hj"> <em class="jp"> δ </em> </strong>是退化过程的参数。一般情况下，我们没有给出退化函数<strong class="ih hj"> <em class="jp"> D，</em> </strong>这样就很难检索到<strong class="ih hj"> <em class="jp"> Iy。</em> </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jq"><img src="../Images/5bfbee0a77d0d952e32a7146099b4292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IMYB58LsCQA7I8ShNixG0A.png"/></div></div></figure><p id="6f0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是为什么检索到的图像将是<strong class="ih hj"><em class="jp">y .</em></strong>理想的情况是，如果<strong class="ih hj"><em class="jp">y</em></strong>和<strong class="ih hj"> <em class="jp"> Iy </em> </strong>相同，这意味着我们检索到了原始的HR图像。这里<strong class="ih hj"> <em class="jp"> F </em> </strong>是超分辨率函数<strong class="ih hj"> <em class="jp"> θ </em> </strong>表示<strong class="ih hj"> F </strong>的参数。</p><p id="81d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在继续之前，值得注意的是，函数<strong class="ih hj"> <em class="jp"> D </em> </strong>是一个未知过程，可能会因散焦、传感器噪声、压缩伪影等而异。为了在一个函数中映射这些过程，进行了一些概括，研究人员将退化建模为单个下采样映射。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jr"><img src="../Images/94ef23dcbcf11d3b95b83213ff7d44ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lM1qdI9GOzfl2EUqvK9TXQ.png"/></div></div></figure><p id="1866" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下采样表示为<strong class="ih hj"> <em class="jp"> ↓s，</em> </strong>其中<strong class="ih hj"> <em class="jp"> s </em> </strong>是采样比例因子。然而，也有其他研究使用更多的操作来模拟退化。</p><p id="2bfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据问题和可用数据，有三种超分辨率方法:(I)多图像超分辨率，(ii)基于示例的超分辨率，以及(iii)单幅图像超分辨率。</p><p id="2a1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">早在2014年，董等人就是第一批将深度学习作为解决超分辨率(SRCNN)方法的研究人员之一。他们给出了一种新的用于单幅图像超分辨率的深度学习方法，表明可以利用卷积神经网络(CNN)重新考虑传统的稀疏编码方法。这种方法学习低分辨率和高分辨率图像之间的端到端映射，很少进行预处理/后处理。这项工作的另一个贡献是使用所有三个颜色通道的可能性。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es js"><img src="../Images/6848211e8dc3d16b5cf956649a40cb2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vIiO6mcQJg4aYJhYKmM75Q.png"/></div></div></figure><p id="e690" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于上述论文带来的新颖性，许多研究人员正在努力将更多的深度学习方法应用到超分辨率问题中。正在进行的工作主要集中在使用更深层次的神经网络来解决这个问题。金等人利用视觉几何组(VGG)的工作启发非常深刻的CNN。他们的发现表明，增加网络深度可以显著提高精确度。它们使用分布在图像的较大区域中的上下文区域信息，因为在小块中不包含太多信息。考虑到对于非常深的网络，收敛问题变得更加相关，他们建议使用剩余学习CNN和高学习率。在这项工作的基础上，林和al.⁴提出了一个增强的深度超分辨率网络(EDSR)。该方法去除了传统剩余网络中不必要的模块。此外，他们采用残差缩放技术来稳定大型模型的训练过程。金和al.⁵也使用深度递归卷积网络(DRCN)，多达16递归，提出使用递归监督和跳过连接。</p><p id="8a6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">赖等人采用了另一种类似的方法，但是使用拉普拉斯金字塔来重建高分辨率图像的子带残差。他们建议使用粗分辨率特征地图作为每个金字塔等级的输入。此外，转置卷积用于向上采样到更细微的级别。值得考虑的是，虽然前面提到的作品在预处理过程中使用双三次插值，但这个作品没有使用它，而是使用学习转置卷积层。他们使用的损失函数是Charbonnier，这是一个更稳健的损失函数，最终优化了模型。</p><p id="2683" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">至于评估这些前馈卷积神经网络，主要使用地面实况图像和输出SR图像之间的每像素损失。相反，其他方法使用纯粹基于来自预训练网络的高级特征的感知损失函数。然而，另一个approach⁷提出使用两组特征用于感知损失函数，该函数可以在训练前馈网络期间使用。虽然对于优化问题，定性结果没有改善，但训练比最先进的技术更快。此外，他们表明，与每像素损失相比，使用感知损失会产生更视觉愉悦的结果。</p><p id="5519" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然在SR中使用CNN是一个突破，但是CNN没有解决一个主要问题，即恢复更精细的纹理细节和大的放大因子。大多数上述工作集中在最小化均方重建误差，这就是为什么它们导致高峰值信噪比，但仍然缺乏高频细节。这就是为什么使用生成性对抗性网络(GANs)⁸出现了。这种方法使用对抗过程来估计生成模型。同时训练生成模型和判别模型。第一种方法是生成与数据分布相匹配的数据，而第二种方法是估计样本来自真实训练数据而不是来自生成器的概率。它辨别得越多，生成模型将改进得越多，这是该技术的主要思想。</p><p id="8b33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">勒迪格和al.⁹ (SRGAN)首先在SR问题中引入GAN，以解决更精细的纹理细节问题。他们创建了一个框架，可以以4倍的放大系数输出逼真的图像。他们建议使用GANs来产生一个感知损失函数，该函数具有对抗性和内容损失。训练鉴别器来辨别由生成器生成的超分辨率图像和照片级逼真图像之间的差异。除此之外，内容损失着眼于感知相似性，而不是每像素相似性。这项工作带来了和SRCNN一样多的新奇感，并且它已经成为许多其他研究人员的基准。</p><p id="272a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这项工作的一个缺点是在处理过程中引入了不同的伪影。王、等人对这一问题作了进一步的探讨，并在斯尔干的架构、感性损失和对抗性损失三个方面作了进一步的调整。他们建议使用残差中的残差密集块(RRDB)而不进行批量归一化。他们让鉴别器预测相对真实值，而不是绝对真实值。为了保存亮度和纹理的一致性，在计算感知损失期间，所使用的特征是激活之前的特征。</p><p id="3416" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管上述方法在超分辨率方面带来了很大的改进，但仍有许多挑战需要解决。这就是这些网络的复杂性和在退化函数未知的退化图像中的有效性。此外，这些方法缺乏模型解释能力，这是大多数深度学习方法的情况。为什么这样的方法会产生如此好的表示，或者在某些情况下不会，这是当今DL中最激动人心和最有力的问题之一。此外，更准确和标准的评估技术仍然是SR中的一个问题。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><p id="31ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[1]董，李春春，何，唐，谢(2015).使用深度卷积网络的图像超分辨率。IEEE模式分析和机器智能汇刊，38(2)，295–307。</p><p id="65ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2] Kim，j .，Kwon Lee，j .，&amp; Mu Lee，K. (2016年)。使用非常深的卷积网络的精确图像超分辨率。IEEE计算机视觉和模式识别会议的会议记录(第1646-1654页)。</p><p id="369f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3] Simonyan，k .，&amp; Zisserman，A. (2014年)。用于大规模图像识别的非常深的卷积网络。<em class="jp"> arXiv预印本arXiv:1409.1556 </em>。</p><p id="d987" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4] Lim，b .，Son，s .，Kim，h .，Nah，s .，和Mu Lee，K. (2017年)。单幅图像超分辨率的增强型深度残差网络。在<em class="jp">IEEE计算机视觉和模式识别研讨会会议录</em>(第136–144页)。</p><p id="e211" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[5] Kim，j .，Kwon Lee，j .，&amp; Mu Lee，K. (2016年)。用于图像超分辨率的深度递归卷积网络。在<em class="jp">IEEE计算机视觉和模式识别会议论文集</em>(第1637-1645页)。</p><p id="835e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[6]赖伟生，黄建波，阿胡佳，杨明辉(2017)。快速精确超分辨率的深拉普拉斯金字塔网络。IEEE计算机视觉和模式识别会议论文集<em class="jp">(第624–632页)。</em></p><p id="77ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[7]约翰逊，j .，阿拉希，a .，，飞飞，L. (2016年10月)。实时风格转换和超分辨率的感知损失。在<em class="jp">欧洲计算机视觉会议</em>(第694–711页)。斯普林格，查姆。</p><p id="4018" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[8] Goodfellow，I .、Pouget-Abadie，j .、Mirza，m .、Xu，b .、Warde-Farley，d .、Ozair，s .、… &amp; Bengio，Y. (2014年)。生成对抗网络。在<em class="jp">神经信息处理系统的进展</em>(第2672-2680页)。</p><p id="e607" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[9] Ledig，c .，Theis，l .，Huszár，f .，Caballero，j .，Cunningham，a .，Acosta，a .，&amp; Shi，W. (2017)。使用生成对抗网络的照片级单幅图像超分辨率。在<em class="jp">IEEE计算机视觉和模式识别会议论文集</em>(第4681–4690页)中。</p><p id="5388" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[10]王，x，于，k，吴，s，顾，j，刘，y，董，c .，，&amp;变来，C. (2018)。Esrgan:增强的超分辨率生成对抗网络。在<em class="jp">欧洲计算机视觉会议(ECCV)会议录</em>(第0-0页)。</p></div></div>    
</body>
</html>