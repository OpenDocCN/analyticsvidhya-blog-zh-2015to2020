<html>
<head>
<title>Complete Guide to build an AutoEncoder in Pytorch and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Pytorch和Keras中构建自动编码器的完整指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/complete-guide-to-build-an-autoencoder-in-pytorch-and-keras-94555dce395d?source=collection_archive---------8-----------------------#2020-07-06">https://medium.com/analytics-vidhya/complete-guide-to-build-an-autoencoder-in-pytorch-and-keras-94555dce395d?source=collection_archive---------8-----------------------#2020-07-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="5d83" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这篇文章是我之前的<a class="ae jk" rel="noopener" href="/analytics-vidhya/complete-guide-to-build-cnn-in-pytorch-and-keras-abc9ed8b8160">文章</a>的延续，这篇文章是使用pytorch和keras构建CNN的完整指南。</p><p id="1982" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从标准数据集或自定义数据集获取输入已经在<a class="ae jk" rel="noopener" href="/analytics-vidhya/complete-guide-to-build-cnn-in-pytorch-and-keras-abc9ed8b8160">使用pytorch和keras的CNN完全指南</a>中提到。所以我们可以从对自动编码器的必要介绍开始，然后实现一个。</p><h2 id="2799" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">自动编码器</h2><p id="ddc1" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">自动编码器是一种神经网络，它学习以最小的信息损失对数据进行编码。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es kl"><img src="../Images/4c06d6623ca8552a7d249af453aa4b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cFvChxqTd39zvZYb_VDeuQ.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">自动编码器</figcaption></figure><p id="0218" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上述网络有许多变体。其中一些是:</p><h2 id="5d4a" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">稀疏自动编码器</h2><p id="7072" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">这种自动编码器通过正则化激活函数隐藏节点来减少过拟合。</p><h2 id="7785" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">降噪自动编码器</h2><p id="cdce" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">这个自动编码器是通过在输入中加入噪声来训练的。这将消除评估时输入的噪声。</p><h2 id="cd6d" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">变体自动编码器</h2><p id="6f34" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">这是一种深度生成神经网络。自动编码器的主要挑战是它们总是试图最小化重建误差，并且从不关心潜在的表示。</p><p id="7b20" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一个好的潜在表示应该总是有意义的，以便它可以用于像GAN这样的生成神经网络。有意义是指安排。来自同一类的数据点分组更近，来自不同类的数据点分组稍远。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lb"><img src="../Images/160b20692089a71a9a579caadfc3c187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*nNULTP48_LQgKSYLB-wV6Q.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">【https://blog.keras.io/building-autoencoders-in-keras.html T4】</figcaption></figure><p id="cc99" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这种潜在表示可以通过如下改变神经网络的结构来实现:</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lc"><img src="../Images/cad657f3565d83359a697b16776f3037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*drAHNZvlSr-bB4V5XAhzzw.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">VAE</figcaption></figure><p id="b9d7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">与其他自动编码器不同，我们正在生成一个具有均值和标准差的潜在分布，而不是单一的潜在向量。然后，我们将从潜在分布中取样以重建输入。</p><p id="a7c1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">关于变体自动编码器的两件重要事情是:</p><p id="9154" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在采样时，我们需要使用重新参数化技巧来处理节点的随机性，因为节点的随机性可能会停止反向传播。</p><blockquote class="ld le lf"><p id="55ce" class="im in lg io b ip iq ir is it iu iv iw lh iy iz ja li jc jd je lj jg jh ji jj hb bi translated">μ,𝛔≈μ+𝛔*n(0,1)</p></blockquote><p id="0ca9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这种重新参数化的技巧不会改变分布。但是它将调整参数以允许反向传播。</p><p id="2afe" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">变化自动编码器使用下面的等式正则化成本函数。</p><blockquote class="ld le lf"><p id="d00e" class="im in lg io b ip iq ir is it iu iv iw lh iy iz ja li jc jd je lj jg jh ji jj hb bi translated">正则化成本函数= Loss+KL(N(μ,𝛔),N(0,1))</p></blockquote><p id="ccad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这将强制潜在分布遵循标准正态分布，从而扩展其在深度生成模型中的使用。</p><p id="594b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你可以在这篇<a class="ae jk" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" rel="noopener" target="_blank">文章</a>中读到更多关于VAE的信息，在这里你可以读到更多关于各种类型的自动编码器的信息。我们将在本文中实现VAE。</p><h1 id="ced4" class="lk jm hi bd jn ll lm ln jr lo lp lq jv lr ls lt jy lu lv lw kb lx ly lz ke ma bi translated">履行</h1><p id="e53d" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">任何自动编码器都包括两个网络编码器和解码器。如前所述，VAE也使用规则化成本函数。</p><h2 id="4236" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">编码器</h2><p id="db29" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">编码器接受输入并返回潜在分布的平均值和标准偏差。</p><pre class="km kn ko kp fd mb mc md me aw mf bi"><span id="e96e" class="jl jm hi mc b fi mg mh l mi mj">#<strong class="mc hj">Pytorch</strong></span><span id="a7e1" class="jl jm hi mc b fi mk mh l mi mj"><strong class="mc hj">class</strong> <strong class="mc hj">VAE</strong>(nn.Module):<br/>    <strong class="mc hj">def</strong> __init__(self, x, h1, h2, z):<br/>        super(VAE, self).__init__()<br/>        <br/>        <br/>        self.fc1 = nn.Linear(x, h)<br/>        self.fc2 = nn.Linear(h1, h2)<br/>        self.fc_mean = nn.Linear(h2, z)<br/>        self.fc_sd = nn.Linear(h2, z)<br/>        <br/>      <br/>        <br/>    <strong class="mc hj">def</strong> encoder(self, x):<br/>        h1 = F.relu(self.fc1(x))<br/>        h2 = F.relu(self.fc2(h1))<br/>        <strong class="mc hj">return</strong> self.fc_mean(h2), self.fc_sd(h2) <em class="lg"># mu, log_var</em></span><span id="39f9" class="jl jm hi mc b fi mk mh l mi mj">#<strong class="mc hj">Keras</strong></span><span id="7da1" class="jl jm hi mc b fi mk mh l mi mj">x = Input(batch_shape=(batch_size, original_dim))<br/>h = Dense(intermediate_dim, activation='relu')(x)<br/>z_mean = Dense(latent_dim)(h)<br/>z_log_sigma = Dense(latent_dim)(h)<br/></span></pre><h2 id="5a19" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">抽样</h2><p id="09e5" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">根据从编码器获得的平均值和标准偏差，我们将通过采样生成解码器的输入。上面提到的重新参数化技巧出现在这里。</p><pre class="km kn ko kp fd mb mc md me aw mf bi"><span id="0681" class="jl jm hi mc b fi mg mh l mi mj"><strong class="mc hj">#Pytorch</strong></span><span id="be2a" class="jl jm hi mc b fi mk mh l mi mj">def sampling(self, mu, log_var):<br/>        std = torch.exp(0.5*log_var)<br/>        eps = torch.randn_like(std)<br/>        <strong class="mc hj">return</strong> eps.mul(std).add_(mu)</span><span id="7939" class="jl jm hi mc b fi mk mh l mi mj"><br/>#<strong class="mc hj">Keras</strong></span><span id="e115" class="jl jm hi mc b fi mk mh l mi mj">def sampling(args):<br/>    z_mean, z_log_sigma = args<br/>    epsilon = K.random_normal(shape=(batch_size, latent_dim),<br/>                              mean=0., std=epsilon_std)<br/>    return z_mean + K.exp(z_log_sigma) * epsilon</span></pre><h2 id="6240" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">解码器</h2><p id="2cd1" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">解码器获取采样函数的输出，并尝试重建原始输入。</p><pre class="km kn ko kp fd mb mc md me aw mf bi"><span id="3dfd" class="jl jm hi mc b fi mg mh l mi mj"><strong class="mc hj">#Pytorch</strong></span><span id="7199" class="jl jm hi mc b fi mk mh l mi mj"><strong class="mc hj">class</strong> <strong class="mc hj">VAE</strong>(nn.Module):<br/>    <strong class="mc hj">def</strong> __init__(self, x, h1, h2, z):<br/>        super(VAE, self).__init__()<br/>        self.fc1 = nn.Linear(x, h1)<br/>        self.fc2 = nn.Linear(h1, h2)<br/>        self.fc_mean = nn.Linear(h2, z)<br/>        self.fc_sd = nn.Linear(h2, z)<br/>        <em class="lg"># decoder </em><br/>        self.fc4 = nn.Linear(z, h2)<br/>        self.fc5 = nn.Linear(h2, h1)<br/>        self.fc6 = nn.Linear(h1, x)<br/>    <br/>    <strong class="mc hj">def</strong> decoder(self, z):<br/>        h1 = F.relu(self.fc4(z))<br/>        h2 = F.relu(self.fc5(h1))<br/>        <strong class="mc hj">return</strong> F.sigmoid(self.fc6(h2))</span><span id="7c5c" class="jl jm hi mc b fi mk mh l mi mj">#<strong class="mc hj">Keras</strong></span><span id="d231" class="jl jm hi mc b fi mk mh l mi mj">decoder_h = Dense(intermediate_dim, activation='relu')<br/>decoder_mean = Dense(original_dim, activation='sigmoid')<br/>h_decoded = decoder_h(z)<br/>x_decoded_mean = decoder_mean(h_decoded)</span></pre><h2 id="03ae" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">损失函数</h2><p id="958a" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">如前所述，VAE使用正则化损失函数，</p><p id="177b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">具有均值μi和标准偏差𝛔i的分布的KL散度具有标准正态分布(KL(N(μi,𝜎I),N(0,1))是</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es ml"><img src="../Images/2f9c6fad3459b31d22e7397213724b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*JzhXZdyuChm_jOCtSNv5gQ.png"/></div></figure><pre class="km kn ko kp fd mb mc md me aw mf bi"><span id="7f06" class="jl jm hi mc b fi mg mh l mi mj"><strong class="mc hj">#Pytorch</strong></span><span id="0f1a" class="jl jm hi mc b fi mk mh l mi mj"><strong class="mc hj">def</strong> loss_function(reconstructed_x, x, mu, log_var):<br/>    loss = F.binary_cross_entropy(reconstructed_x, x.view(-1, 784),      <br/>                       reduction='sum')<br/>    regularized_term = -0.5 * torch.sum(1 + log_var - mu.pow(2) -   <br/>                      log_var.exp())<br/>    <br/>    <strong class="mc hj">return</strong> loss + regularized_term</span><span id="70e5" class="jl jm hi mc b fi mk mh l mi mj"><strong class="mc hj">#Keras</strong></span><span id="7e54" class="jl jm hi mc b fi mk mh l mi mj">def vae_loss(x, x_decoded_mean):<br/>    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)<br/>    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - <br/>                 K.exp(z_log_sigma), axis=-1)<br/>    return xent_loss + kl_loss</span></pre><h2 id="fec8" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">数据流动</h2><p id="5880" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">数据从编码器、采样开始，然后是解码器。</p><pre class="km kn ko kp fd mb mc md me aw mf bi"><span id="7a56" class="jl jm hi mc b fi mg mh l mi mj"><strong class="mc hj">#Pytorch</strong></span><span id="a48b" class="jl jm hi mc b fi mk mh l mi mj">def forward(self, x):<br/>        mu, log_var = self.encoder(x.view(-1, 784))<br/>        z = self.sampling(mu, log_var)<br/>        <strong class="mc hj">return</strong> self.decoder(z), mu, log_var</span></pre><p id="da66" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在keras中，不需要转发函数。数据将按照你建立网络模型的顺序流动。</p><p id="4276" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">用损失函数编制网络。</p><pre class="km kn ko kp fd mb mc md me aw mf bi"><span id="81f8" class="jl jm hi mc b fi mg mh l mi mj">#<strong class="mc hj">Pytorch</strong></span><span id="3f6b" class="jl jm hi mc b fi mk mh l mi mj">vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)<br/>latent, mu, log_var = vae(data)<br/>loss = loss_function(latent, data, mu, log_var)<br/>        <br/>loss.backward()<br/> <br/>optimizer.step()</span><span id="f8de" class="jl jm hi mc b fi mk mh l mi mj"><strong class="mc hj">#Keras</strong></span><span id="7d62" class="jl jm hi mc b fi mk mh l mi mj">vae = Model(x, x_decoded_mean)</span><span id="8fc0" class="jl jm hi mc b fi mk mh l mi mj">vae.compile(optimizer='rmsprop', loss=vae_loss)</span></pre><p id="d557" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们还将在下一篇文章中打包pytorch和keras中GAN的实现。</p><p id="4071" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">感谢阅读:)</p><h2 id="40a7" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">参考</h2><div class="mm mn ez fb mo mp"><a href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">在Keras中构建自动编码器</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">在本教程中，我们将回答一些关于自动编码器的常见问题，我们将涵盖代码的例子…</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">blog.keras.io</p></div></div><div class="my l"><div class="mz l na nb nc my nd kv mp"/></div></div></a></div></div></div>    
</body>
</html>