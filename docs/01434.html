<html>
<head>
<title>Twitter Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推特情感分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/twitter-sentiment-analysis-8ef90df6579c?source=collection_archive---------2-----------------------#2019-10-22">https://medium.com/analytics-vidhya/twitter-sentiment-analysis-8ef90df6579c?source=collection_archive---------2-----------------------#2019-10-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/38610b76812ee572e712e7e4dee1dd1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q9Y5BWifNl_ZsJDM"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae iu" href="https://unsplash.com/@merakist?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Merakist </a>拍摄的照片</figcaption></figure><p id="5e76" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">注:</em> </strong> <em class="jt">你可以在这里</em>  <em class="jt">全面访问我的Python代码</em> <a class="ae iu" href="https://github.com/Shrneha/Twitter-sentiment-analysis.git" rel="noopener ugc nofollow" target="_blank"> <em class="jt">以及我在本帖中的全部分析。</em></a></p><p id="5982" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">简介:</strong></p><p id="9314" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">情感分析或观点挖掘指的是自然语言处理(NLP)。它是分析文本的过程，以确定它们所承载的情感基调。换句话说，就是用来确定作者对某件事的态度。</p><p id="2963" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">情感分析的应用非常强大。它正被大多数组织广泛采用，以从社交媒体数据中提取洞察力。数据的情感分析可以帮助公司获得人们如何谈论他们的品牌和竞争对手的信息。它让公司更好地了解他们的受众，发现行业的新趋势。公司可以获得人们如何谈论他们品牌的信息，并据此做出决定。它有助于组织在没有任何中介的情况下与他们的受众建立联系。</p><p id="6fc6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">问题陈述:</strong></p><p id="7bcb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这项任务的目标是检测推文中的仇恨言论。为了简单起见，如果一条推文带有种族主义或性别歧视的情绪，我们就说这条推文包含仇恨言论。所以，任务是将种族主义或性别歧视的推文从其他推文中分类。</p><p id="db0c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">执行自然语言处理的步骤:</strong></p><ol class=""><li id="6c72" class="ju jv hi ix b iy iz jc jd jg jw jk jx jo jy js jz ka kb kc bi translated">数据清理(包括删除不必要的字符、标记化、词条化等)</li></ol><p id="a767" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.数据可视化</p><p id="079f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.一个热门编码(一袋单词)</p><p id="d9d0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4.分类</p><p id="207d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">5.模型评估</p><p id="10d5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那么，让我们开始分析吧。我们将在需要时导入库。</p><p id="01a8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将数据导入pandas数据框架并浏览数据。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="e0d4" class="km kn hi ki b fi ko kp l kq kr">#load csv file<br/>Train = pd.read_csv("train_E6oV3lV.csv")<br/>data=Train<br/>data.head()</span></pre><p id="2b2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/bda19e86d1872beb805c0a94c3bf589c.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*4TLujjbxBdYiwb6ZgzsHeg.png"/></div></figure><p id="f81d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在给定的推文样本中，正面推文被标记为“0”，而负面推文被标记为“1”。</p><p id="8638" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一些观察结果:</p><ol class=""><li id="aa9a" class="ju jv hi ix b iy iz jc jd jg jw jk jx jo jy js jz ka kb kc bi translated">推文中有用户名，也有一些需要删除的特殊字符。</li><li id="2c80" class="ju jv hi ix b iy kt jc ku jg kv jk kw jo kx js jz ka kb kc bi translated">推文是句子格式的。我们需要把它分解成单词。</li></ol><p id="aa4f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">探索标签在数据中的分布。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="6322" class="km kn hi ki b fi ko kp l kq kr">data['label'].value_counts()</span></pre><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/4e702b92e782257fffa9c6682ab3959c.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*0PR0RGVItEGTM88chQKJqw.png"/></div></figure><p id="ca42" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到有29720条推文被标记为正面，2242条推文被标记为负面。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="4600" class="km kn hi ki b fi ko kp l kq kr">import seaborn as sns<br/>ax=sns.countplot(data.label)</span></pre><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/c9aa8da53e03c70018cb92a6c16bb186.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*QubOKjt9Zlblo_VYb4aPzg.png"/></div></figure><ol class=""><li id="4f76" class="ju jv hi ix b iy iz jc jd jg jw jk jx jo jy js jz ka kb kc bi translated"><strong class="ix hj">数据清理:</strong></li></ol><p id="dfcf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">a)从推特专栏删除推特用户名</p><p id="848d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Tweet包含伪装成“@user”的twitter句柄。需要删除它，因为它对数据没有太大影响。我们将创建名为“new_tweet”的新列，在其中存储我们处理过的推文。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="32f3" class="km kn hi ki b fi ko kp l kq kr"># removing usernames from tweet<br/>data['new_tweet'] = data.tweet.str.replace('<a class="ae iu" href="http://twitter.com/user" rel="noopener ugc nofollow" target="_blank">@user</a>', '')<br/>data.head()</span></pre><p id="fe97" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">b)删除标点、数字和特殊字符</p><p id="584a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">和用户名一样，标点符号，特殊字符没有太大帮助。所以我们会把它们从推文中删除。也删除标签。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="08be" class="km kn hi ki b fi ko kp l kq kr">#Removing Punctuations, Numbers, and Special Characters<br/>#[a-zA-Z] = Any single character in the range a-z or A-Z<br/># ^ = Start of line <br/># $ = End of line<br/>data[‘new_tweet’] = data[‘new_tweet’].str.replace(“[^a-zA-Z#]”, “ “)<br/>data[‘new_tweet’] = data[‘new_tweet’].str.replace(“#”, “”)<br/>data.head()</span></pre><p id="0392" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es la"><img src="../Images/7889731db19988ea0e9b4d22274c00d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*3Oj70JtpYjcEEp-ImpI6Qw.png"/></div></figure><p id="d55b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">c)标记化</p><p id="c5ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是一种将文本拆分成标记的方法。这些记号可以是段落、句子或单词。在这种情况下，推文将被转换为单独的单词。所以进一步加工会变得更容易。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="357c" class="km kn hi ki b fi ko kp l kq kr">#tokenization<br/>data[‘new_tweet’] = data[‘new_tweet’].apply(lambda x: x.split())<br/>data.head()</span></pre><p id="459f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/134a7eee9ba6d8167cd183da6c87ccfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*PBmZ8X-rB7aVKa0xYBYH8g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">标记化</figcaption></figure><p id="ac0f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">d)词干化和词汇化:</p><p id="1931" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">词干化和词汇化是<strong class="ix hj">自然语言处理</strong>领域中的<strong class="ix hj">文本规范化</strong>技术，用于为进一步处理准备文本、单词和文档。例如</p><p id="0053" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">“跑，跑”将被“跑”取代</p><p id="9d0b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">“am，is，are”将被单词“be”代替</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="eb09" class="km kn hi ki b fi ko kp l kq kr">#stemmer<br/>from nltk.stem.snowball import SnowballStemmer<br/>stemmer = SnowballStemmer(“english”)</span><span id="987f" class="km kn hi ki b fi lc kp l kq kr">data[‘new_tweet’]= data[‘new_tweet’].apply(lambda x: [stemmer.stem(i) for i in x])<br/>data.head()</span></pre><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/0b0d3f09bfaf33bcfe211fb249b5ed7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*bZsBUuvYkFBBTYS16s_Jeg.png"/></div></figure><p id="3f62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到使用词干分析器后单词是如何被缩减的。仍然有一些单词对分析没有太大的影响，比如“the”、“I”、“a”、“is”。这些被称为停用词。停用词是一个搜索引擎经常忽略的词。我们定义了一个名为“process”的函数来从数据中删除停用词。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="563b" class="km kn hi ki b fi ko kp l kq kr">from nltk.corpus import stopwords<br/>from nltk.tokenize import word_tokenize<br/>stopwords = nltk.corpus.stopwords.words(‘english’)</span><span id="bffd" class="km kn hi ki b fi lc kp l kq kr">import string</span><span id="b70a" class="km kn hi ki b fi lc kp l kq kr">def process(text):<br/>    # Check characters to see if they are in punctuation<br/>    nopunc = set(char for char in list(text) if char not in string.punctuation)<br/>    # Join the characters to form the string.<br/>    nopunc = " ".join(nopunc)<br/>    # remove any stopwords if present<br/>    return [word for word in nopunc.lower().split() if word.lower() not in stopwords]</span><span id="6036" class="km kn hi ki b fi lc kp l kq kr">data['new_tweet'] = data['new_tweet'].apply(process)</span></pre><p id="27dd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es le"><img src="../Images/f1ea709ffc4fe0fe8ad62710145dc93e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*F49wUOHg5u1UDZ4DAdQkmw.png"/></div></figure><p id="bc59" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">大部分不必要的单词都被删除了。现在，我们将使用当前的标准化数据进行进一步处理。我们将创建一个单词云，描述整个数据集中最常见的单词。</p><p id="89b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。数据可视化</strong></p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="b989" class="km kn hi ki b fi ko kp l kq kr">from wordcloud import WordCloud<br/>import matplotlib.pyplot as plt</span><span id="adb1" class="km kn hi ki b fi lc kp l kq kr"># get individual words<br/>words = []<br/>for line in data[‘new_tweet’]: <br/> words.extend(line)<br/> <br/># create a word frequency dictionary<br/>wordfreq = Counter(words)<br/># draw a Word Cloud with word frequencies<br/>wordcloud = WordCloud(<br/> background_color=’white’,<br/> max_words=2000,<br/> stopwords=stopwords<br/> ).generate_from_frequencies(wordfreq)<br/>plt.figure(figsize=(10,9))<br/>plt.imshow(wordcloud, interpolation=’bilinear’)<br/>plt.axis(“off”)<br/>plt.show()</span></pre><p id="920b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/2687b56a266bfdfcc42fda8289836ca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*HgiUyRSac4WRqQHy0vrJjA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">最常见单词的单词云</figcaption></figure><p id="667e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">大部分的话似乎是中性的。它没有给出任何关于种族主义/性别歧视的推文的想法。因此，我们将进一步将它分叉，找出积极和消极的推文。</p><p id="02de" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看，那些被标为正面的推文。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="1fc0" class="km kn hi ki b fi ko kp l kq kr">positive = [r for r in data['new_tweet'][data['label']==0]]<br/>pos = ''.join(positive)</span><span id="9ca1" class="km kn hi ki b fi lc kp l kq kr"># draw a Word Cloud with word frequencies<br/>wordcloud = WordCloud(<br/>    background_color='white',<br/>    max_words=2000,<br/>    stopwords=stopwords<br/>   ).generate(pos)<br/>plt.figure(figsize=(10,9))<br/>plt.imshow(wordcloud, interpolation='bilinear')<br/>plt.axis("off")<br/>plt.show()</span></pre><p id="8e21" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/f69b27be1d6c0f09ad698ab386806f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*edzSxK1d8Z3MK2iZxyPWog.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">正面推文的文字云</figcaption></figure><p id="16a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将创建带有负面推文的词云</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="2527" class="km kn hi ki b fi ko kp l kq kr">negative = [r for r in data['new_tweet'][data['label']==1]]<br/>neg = ''.join(negative)</span><span id="3280" class="km kn hi ki b fi lc kp l kq kr"># draw a Word Cloud with word frequencies<br/>wordcloud = WordCloud(<br/>    background_color='black',<br/>    max_words=2000,<br/>    stopwords=stopwords<br/>   ).generate(neg)<br/>plt.figure(figsize=(10,9))<br/>plt.imshow(wordcloud, interpolation='bilinear')<br/>plt.axis("off")<br/>plt.show()</span></pre><p id="8195" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/ec374dea53a0dab4246f96b5eecb0eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*nOOIp2U6Pt9k9uE2f0vu5Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">带有负面推文的词云</figcaption></figure><p id="861f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 3。一个热编码(一袋字):</strong></p><p id="f2ee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">词袋模型</strong> </a>是自然语言处理中使用的简化表示。在这个模型中，一个文本被表示为它的单词包，不考虑语法甚至词序，但保持多样性。</p><p id="1ba3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这种情况下，我们简单地计算数据集中的每个单词。我们创建矩阵，其中每行代表数据集中的一个文档，每列代表word。cell的值是该单词在文档中的出现频率。</p><p id="3a2d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一种流行的执行归一化的方法称为<strong class="ix hj">词频—逆文档频率</strong>或<a class="ae iu" href="https://en.wikipedia.org/wiki/Tf–idf" rel="noopener ugc nofollow" target="_blank">T3】TF-IDFT5。在这种加权方案中，术语计数首先被归一化为频率，然后除以文档数量。TfidfVectorizer根据出现在&amp;中的文档数量对每个术语计数进行加权，出现在许多文档中的术语权重较低。</a></p><p id="6bc3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用<strong class="ix hj"> SciKit </strong> Learn的<strong class="ix hj"> CountVectorizer </strong>函数，它将把一组文本文档转换成一个令牌计数矩阵。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="03b3" class="km kn hi ki b fi ko kp l kq kr">#Split data into training and testing sets </span><span id="5951" class="km kn hi ki b fi lc kp l kq kr">from sklearn.model_selection import train_test_split</span><span id="e18a" class="km kn hi ki b fi lc kp l kq kr">x_train, x_test, y_train, y_test =     train_test_split(data["new_tweet"], <br/>      data["label"], test_size = 0.2, random_state = 42)</span><span id="e6db" class="km kn hi ki b fi lc kp l kq kr">from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer</span><span id="084d" class="km kn hi ki b fi lc kp l kq kr">count_vect = CountVectorizer(stop_words='english')<br/>transformer = TfidfTransformer(norm='l2',sublinear_tf=True)</span><span id="ab90" class="km kn hi ki b fi lc kp l kq kr">x_train_counts = count_vect.fit_transform(x_train)<br/>x_train_tfidf = transformer.fit_transform(x_train_counts)<br/>print(x_train_counts.shape)<br/>print(x_train_tfidf.shape)<br/>#Output :(25569, 27304) (25569, 27304)</span><span id="ce38" class="km kn hi ki b fi lc kp l kq kr">x_test_counts = count_vect.transform(x_test)<br/>x_test_tfidf = transformer.transform(x_test_counts)<br/>print(x_test_counts.shape)<br/>print(x_test_tfidf.shape)<br/>#Output : (6393, 27304) (6393, 27304)</span></pre><p id="a400" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们已经完成了数据预处理。我们准备通过机器学习分类算法。</p><p id="3684" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 5。型号评估</strong>:</p><p id="522f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">随机森林分类器</strong></p><p id="a0d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">单个决策树可以学习相当复杂的函数。然而，在许多方面，它可能容易过度拟合。为了克服这一点，我们可以创建许多决策树，然后要求每棵树预测类值。我们可以采取多数投票，并用这个答案作为我们的总体预测。<a class="ae iu" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">随机森林</a>工作原理就是这样。总体过程中使用了一些参数:</p><p id="842b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt"> n_estimators </em>:这决定了应该建立多少棵决策树。值越大，运行时间越长，精度越高。</p><p id="070e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt"> oob_score </em> : <em class="jt"> </em>如果为真，则对不在为训练决策树而选择的随机子样本中的样本测试该方法。</p><p id="759c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt"> n_jobs </em>:指定并行训练决策树时使用的内核数量。</p><p id="a359" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，让我们开始用随机森林分类器模型训练我们的数据。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="521d" class="km kn hi ki b fi ko kp l kq kr">from sklearn.ensemble import RandomForestClassifier</span><span id="d556" class="km kn hi ki b fi lc kp l kq kr">model = RandomForestClassifier(n_estimators=200)<br/>model.fit(x_train_tfidf,y_train)<br/>predictions = model.predict(x_test_tfidf)</span><span id="1c5e" class="km kn hi ki b fi lc kp l kq kr">#Confusion Matrix <br/>from sklearn.metrics import confusion_matrix,f1_score<br/>confusion_matrix(y_test,predictions)</span><span id="4177" class="km kn hi ki b fi lc kp l kq kr">#Output: array([[5898,   39],        <br/>#               [ 207,  249]], dtype=int64)</span><span id="2621" class="km kn hi ki b fi lc kp l kq kr">#f1-score<br/>f1_score(y_test,predictions)<br/>#Output : 0.6693548387096774</span><span id="e67a" class="km kn hi ki b fi lc kp l kq kr">#Accuracy_score<br/>from sklearn.metrics import accuracy_score<br/>accuracy_score(y_test,predictions)*100<br/># output:96.15204129516658</span></pre><p id="75fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">评价:</em> </strong></p><p id="47e7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">选择评估指标时，考虑评估指标无用的情况总是很重要的。在许多情况下，准确性是很好的评估标准，因为它计算简单，易于理解。然而，它很容易被伪造。换句话说，在许多情况下，您可以创建精确度高但实用性差的算法。</p><p id="4a57" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> f1分数</strong>是基于每个类定义的，并且基于两个概念:精确度<em class="jt">和召回率</em>。<em class="jt">精度</em>是被预测为属于实际来自该类特定类的所有样本的百分比。而<em class="jt">召回</em>是数据集中在该类中并且实际上被标记为属于该类的样本的百分比。</p><p id="6dcf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">96%的情况下，模型对数据集中所有标签的预测都是正确的。因此，随机森林模型可用于评估目的。</p><p id="d506" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">注:</em> </strong> <em class="jt">您可以在这里</em>  <em class="jt">全面访问我的Python代码</em> <a class="ae iu" href="https://github.com/Shrneha/Twitter-sentiment-analysis.git" rel="noopener ugc nofollow" target="_blank"> <em class="jt">。</em></a></p></div></div>    
</body>
</html>