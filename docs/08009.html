<html>
<head>
<title>Building Neural network with Logistic Regression.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用逻辑回归建立神经网络。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-neural-network-with-logistic-regression-ef5fdd700810?source=collection_archive---------17-----------------------#2020-07-14">https://medium.com/analytics-vidhya/building-neural-network-with-logistic-regression-ef5fdd700810?source=collection_archive---------17-----------------------#2020-07-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f491" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归作为一个基本的神经网络，是的，我们将讨论如何实现逻辑回归的神经网络工作。我假设你知道一些神经网络的基础知识，当我们说建立神经网络时，我们必须定义如何建立神经网络的几个步骤，是的，在我们将神经网络实现为逻辑回归之前，有一些关于神经网络的基础知识，让我们看看如何实现？</p><p id="a36f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们看了正向传递，它包括输入和权重、产生输出的非线性函数，以及用于计算预测输出与实际输出相比的损失的成本函数</p><p id="5b51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二，我们来看看后向传递，包括使用梯度下降算法最小化损失函数，并通过计算梯度找出最终输出的理想权重，以将权重调整到实际输出，这称为优化。</p><p id="a001" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这两个基本要素是构建神经网络的基本构件，我们将在继续学习时讨论这一点，我们刚刚看了概述，我们将看到细节</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/59280481515b0e4fb9cb5a28345aecd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ESrPdqU8Ndq45uOCsVj_g.png"/></div></div></figure><h1 id="8f1c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">逻辑回归</h1><p id="8b34" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">逻辑回归是二元分类器</p><p id="63ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:给定一些狗或猫的图像，我们必须识别图像中是否有猫或狗，如果猫存在，我们标记为1，如果猫不存在，我们标记为0，就这么简单。那么我们如何表现这个形象呢？我们人类生来就有一些能力，我们可以识别它是否是猫，但是计算机如何识别它呢？为此，我们必须将数据呈现为计算机能够理解的形式，在此之前，这里的图片表示为像素，像素具有值，让我们来看看</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ks"><img src="../Images/0bd07d40409760a57b3a7b25fae46c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*xuoNxDI_Sva1bVTzjYUycg.jpeg"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">猫图像rgb</figcaption></figure><p id="f249" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面是猫的图像，下面是相应的像素值和图片本身的3个通道</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kx"><img src="../Images/03cda2d1f851a34ae27f9926f9bbf540.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*PSuclq09uXnYWdon1U9irQ.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">RGB通道</figcaption></figure><p id="6d8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">r-红色、G-绿色、B-蓝色这里我们有7*5的3个通道，也就是7*5*3像素</p><p id="bc51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您在上面看到的cat图像具有超过7*5*3的维度，但是在这里，例如我们在第二个图像中看到了7*5*3的维度，我们不能使用此图像来构建神经网络，我们必须将所有像素值取消排列为矢量化格式，例如:7*5*3=105个像素</p><p id="6e86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们将图片表示为nx *1向量的特征向量，其中nx =像素数(105)，这是一个列向量，这是一个图像，但实际上我们会有数千个图像，因此我们将所有这些图像向量垂直堆叠，如下所示。</p><p id="3d61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们需要输入特征向量来得到输出是否为猫</p><p id="f3fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里x1表示单个图像向量，即具有nx*1维列向量的特征向量</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ky"><img src="../Images/20e13d42bc9b65987d7c25d6ab211193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*CAQY4pXyJOGa29qxOgXFuA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">带有nx*m训练示例的矩阵</figcaption></figure><p id="4af5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将堆叠所有的输入图像向量，即来自x1、x2、x3、x4…xn的特征向量，并且我们将它们垂直堆叠，您可以参考上面的解释，每一列是作为特征向量的单个训练图像，并且在所有列上我们表示为大写X</p><p id="9150" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是神经网络输出</p><p id="62cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有输入向量x，我们想要预测的输出(即y^=p(y=1/x)</p><p id="c819" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中y是相对于输入X为1的概率</p><p id="1947" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，输出y^是x个输入的线性组合，权重为w，偏差为b</p><p id="c3ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那就是y^= wTx+b</p><p id="98d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以看到上面的输出产生了线性输出，这在本质上是连续的，我们在这里不需要，在逻辑回归中，我们预测0和1，所以我们必须将sigmoid函数应用于此输出，以获得0和1之间的值，其中sigmoid是1/1+e-z，z是y^的线性输出</p><p id="303e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们如何表示a = sigmoid(Z)，从而产生0和1之间的值，关于逻辑回归的更多内容可以参考在线资料。下图是sigmoid函数</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kz"><img src="../Images/e1d24d12fc2f5aa078cffc29eeb2a72b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*r5paakyHLob7t0oue-l8nQ.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">线性输出</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es la"><img src="../Images/44842bb090cf236b7ae20163e5cd8604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*0mi1Lb21xNqRVvcU7YFt9A.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">乙状结肠输出</figcaption></figure><h1 id="e393" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">价值函数</h1><p id="021f" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">这里，我们使用的成本函数就是我们的损失函数(对于单个训练示例，我们使用损失函数，但对于所有训练示例，我们使用成本函数)，成本函数的目的是计算所有训练示例相对于实际的损失，并输出所有训练图像的损失，以便为我们的模型找到正确的w权重偏差，因为我们首先已经定义了成本函数，一旦找到损失，我们就可以调整权重</p><p id="22c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于单个训练示例，我们将损失定义如下</p><p id="7b6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">l(y^,y)=-(y^+(1-y)日志(1-y^))</p><p id="90dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中y是实际输出，y^是预测输出，所以这里我们使用负号，因为我们必须最小化损失函数</p><p id="46ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果y=1，损失函数变成L(y^,y)=-log y^，因为-log y^必须尽可能小，然后y^将变成接近1的大值</p><p id="bdff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果y=0，损失函数变成L(y^,y)=-log(1- y^，对于对数(1- y^)，必须尽可能大，然后y^将变成接近0的小值</p><p id="9c9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于整体训练示例，我们使用成本函数来计算损失(成本)</p><p id="9154" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">即成本函数J(w，b)=1/m从I到m L(Y^(i),Y(i的总和)</p><p id="14d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代价函数:J(w，b)=-1/m sum I to m[(y(I)log y^(i)+(1-y(i)log(1-y^(i)))</p><p id="13be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，负号用于最小化损失以最大化概率，w和b是损失函数的参数，I到m是所有训练示例，我们必须对损失进行求和并求平均值，我们将所有训练示例的平均损失作为成本函数</p><p id="486a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经定义了我们的成本，接下来我们将看到如何找到w和b的最佳参数，以降低所有图像的成本函数</p><h1 id="a940" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">梯度下降</h1><p id="68a8" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们有计算图像损失的成本函数，现在要做的是减少损失，以使我们的预测输出与实际输出相匹配，我们在这里使用的方法是梯度下降算法，它为我们的图像预测找到最佳权重和偏差参数，让我们看看它在下图中是如何工作的</p><p id="6326" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们将看到下面的凸损失函数，以了解梯度下降</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lb"><img src="../Images/6e9111d90918d9ac13d7c262d31ccfb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*x2NRjBAOXcRDFNOmNxAMOQ.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">凸函数</figcaption></figure><p id="103a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">高于成本(损失)函数，其中曲线的高度和宽度代表整体权重和偏差，当随机初始化我们的权重时，当我们计算成本函数时，权重位于该曲线上的某个位置，梯度下降将做什么，它将采取步骤向下方向，其中损失最小，即全局最小值。梯度下降在最速根中采取步骤，直到找到全局最小值以减少损失。</p><p id="8160" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看下面</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/33da6820da94b56fbc077b005311529a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dji21sP5H5rC0cMLReriPg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">更新凸损失函数的权重</figcaption></figure><p id="ac8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的工作是找出位于你能看到的表面之下的最小损失，我们将在每次朝着最小损失采取措施时找到损失函数的导数，关于导数我们在这里不讨论，你可以找到在线材料，为了理解导数意味着斜率，我们必须找到损失的水平斜率，你可以参考上面的水平斜率</p><p id="5909" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面是成本函数，当我们初始化权重时，w将位于曲线的顶部，这意味着当w是大值时，我们必须将方向转向w的左侧，并从当前损失中减去，通过使用所谓的学习率，我们将发现学习率是决定在特定方向上采取多大步骤的一个因素，当w大时，我们采取相反的步骤</p><p id="17a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当w为负时，通过学习率，我们朝着正的方向前进，上瘾就会发生</p><p id="2fc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的最终目标是达到全局最小值，这就是梯度下降每次通过减少损失来更新权重并找到全局最小值</p><p id="5176" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的目标是找到最佳的w和b参数，因为我们不能在这里改变输入</p><p id="427b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">只有使用w和b，我们才能优化损失</p><h1 id="d617" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">计算图(向前传递和向后传递)</h1><p id="f887" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">前进传球</p><p id="8f2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经网络的计算包括两种方式，第一种是前向传递，其中我们将计算输出的成本函数，第二种是后向传递，其中我们计算成本函数的梯度。</p><p id="55da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一个函数j (a，b)=4(a+bc)</p><p id="3bf2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看向前传球的计算图</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/52e033c7dbeb440adaf4b355b15d1e94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pn_ifDTwnRsJrH538p9iQw.png"/></div></div></figure><p id="ff53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">前向传递或前向传播整体思想是计算函数j，即计算输出损耗，在示例中，我们采用简单函数计算前向传递计算图，我们可以先将b和c相乘，然后将w乘以a，最后将x乘以3，得到函数输出，以计算输出和前向传递损耗。</p><p id="60d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">偶数道次</p><p id="a4f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在后向传递中，我们必须找到相对于输出j和I的导数，即使用导数数学概念找到权重和偏差，我们现在不讨论导数，但让我们看看它是如何工作的，导数的意思是说，通过调整权重输入，我们可以通过频繁更新这些权重来将损失降至最低</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/67d2002e1311e78fd5d0278cd5cc1287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AehY6zmGu5bCeWCQHgu1Qg.png"/></div></div></figure><p id="855e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在上面的图像中看到反向传播看起来有多简单，我们必须找到所有输入相对于输出函数j的导数，为了做到这一点，我们使用导数的链式法则</p><p id="5843" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先我们计算输出相对于x的导数，即dj/dx，如果改变x，j的输出会改变多少，然后我们通过改变w i，e dx/dw，找到x的导数，通过改变a i，e dx/da，找到w的导数，分别改变b和c，即dw/db/，dw/dc</p><p id="4082" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是简单的反向传播，为此，你必须知道导数的链式法则，这里我们找到了输入a、b、c和偏差相对于反向传播中的损失或成本函数的权重</p><h1 id="0e93" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">逻辑回归梯度下降</h1><p id="ee3a" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">这里我们将讨论如何计算导数来实现逻辑回归的梯度下降，让我们用计算图来做这件事，我们现在将回忆逻辑回归</p><p id="8df3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">z = wTx+B—-相对于输入x的线性输出</p><h2 id="f3e5" class="lc jq hi bd jr ld le lf jv lg lh li jz iq lj lk kd iu ll lm kh iy ln lo kl lp bi translated">线性输入的Y^=a=sigmoid(z)-逻辑sigmoid输出</h2><p id="7300" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">L(a，y)=-(ylog(a)+(1-y)log(1-a))-单个训练示例的sigmoid输出的损失函数</p><p id="b6a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下图解释了单个训练示例</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/368ee5aa7bbd0d183b69560ce225e0f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOZi0hlm1X-wBIUUD1cOpQ.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">计算逻辑回归的导数</figcaption></figure><p id="ec7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们讨论关于计算单一训练样本的梯度</p><p id="cf4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面我们来讨论一下m训练的例子</p><p id="300f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们计算所有训练示例的成本函数</p><p id="f6a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">J(w，b)=1/m sum from i to m L(a(i)，y(i))</p><p id="6046" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中I到m是所有损失的总和，a/m是所有损失的平均值，a(i)或y(i)是指所有训练样本</p><p id="bbb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a(I)= y(I)= s形(Z(I))= s形(wTx(i)+b)</p><p id="3260" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经计算了sigmoid和成本函数j</p><p id="3ce0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看如何计算所有训练示例的总成本函数的导数</p><p id="d85f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">d/dw1*j(w，b)=1/m sum from i to m d/dw1*L(a(i)，y(i)</p><p id="1ac6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中dw1(i)相对于所有训练示例(x(i)，y(i)</p><p id="4445" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像这样，我们计算w2，b的权重和偏差</p><p id="8998" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">d/dw2*j(w，b)=1/m从I到m的和d/dw2*L(a(i)，y(i)</p><p id="4b9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">d/db*j(w，b)=1/m从I到m的和d/db*L(a(i)，y(i)</p><p id="92c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经知道如何为所有训练示例找到权重dw1(i)，dw2(i)，db(i)</p><p id="1317" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看看如何实现这个逻辑</p><p id="e455" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Z(i)=wTx(i)+b</p><p id="cabd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a(i)=sigmoid(z(i)</p><p id="75c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并且我们对所有训练示例循环成本函数是j+=[y(I)log a(I)+(1-y(I)log(1-a(I))</p><p id="2982" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并且z(i)=a(i)-y(i)对所有训练示例的损失a的导数</p><p id="e2c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dw1+=x1(i)dz(i)</p><p id="0dac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dw2+=x2(i)dz(i)</p><p id="c5b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">db+=dz(i)</p><p id="4194" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上是所有输入和所有训练样本的权重和偏差</p><p id="f055" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有成本函数j/m平均损失，其中m是训练示例的</p><p id="8e1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有训练样本m的最终权重为</p><p id="c52e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有训练示例的dw1/m、dw2/m/db/m</p><p id="81d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">权重更新如下:w1:= w1-αdw1，w2:= w2-αdw2</p><p id="f288" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b:= b-αdb，其中α指的是学习率</p><p id="7914" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经看到了如何为所有训练示例的逻辑回归实现梯度下降</p><p id="fb95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，对于所有训练示例，当您与没有平均的单个训练示例进行比较时，我们取平均损失1/m。</p><h1 id="e18c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">…向量化…</h1><p id="e713" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">矢量化是为了避免在代码中实现for循环，当我们在大型训练集上训练我们的模型时，由于for循环在迭代中进行，因此需要很长时间来训练，因此我们使用矢量化来避免这种情况</p><p id="1b3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看如何</p><p id="21bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当你计算z=wTx +b时，这里w和x是大的nx*1大的向量，为了计算这两个向量，我们必须对向量的每一行使用for循环。所以要避免</p><p id="5738" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们所能做的就是使用NumPy库，使用NumPy中的下面的函数来做这件事</p><p id="3f96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Z=np.dot(w，x)+b它等于wTx+b</p><p id="ee08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们将输入向量作为单个向量水平堆叠</p><p id="6b9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看下面如何做逻辑回归</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/ac230ccda286caaaea3960c9b24e3aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Okxjv6eu90u9Z2uEyd0FVw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">…向量化…</figcaption></figure><p id="bda7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面我们可以看到，通过使用NumPy库，我们可以将它水平堆叠起来，在NumPy中进行广播，这比循环更好，也更节省时间。</p><p id="eb0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们了解了如何使用所有必要的组件将逻辑回归作为基本的神经网络来实施，我们在这里进行了简单的讨论，但您可以探索更多有关使用在线资源的信息，这里的主要组件是计算成本的向前计算和通过实施梯度下降的向后计算，我们可以找到正确的权重和偏差，并减少损失。</p><p id="1c60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快乐学习！！！！！</p></div></div>    
</body>
</html>