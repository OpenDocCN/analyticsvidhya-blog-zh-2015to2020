<html>
<head>
<title>Best (fastest) ways to import CSV files in python for production environments (pandas, csv, dask)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在python中为生产环境导入csv文件的最佳(最快)方法(pandas、CSV、dask)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/best-fastest-ways-to-import-csv-files-in-python-for-production-environments-pandas-csv-dask-1407be42893e?source=collection_archive---------4-----------------------#2020-06-21">https://medium.com/analytics-vidhya/best-fastest-ways-to-import-csv-files-in-python-for-production-environments-pandas-csv-dask-1407be42893e?source=collection_archive---------4-----------------------#2020-06-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/a1dddc98da120bd2789906005d91b596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*oD9e75JjXMZ3QtHmzNK3jg.png"/></div></figure><p id="f451" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在处理数据的工程师的生活中，处理csv文件是一项日常任务。在处理分析任务时，我们有时会选择忽略导入数据所消耗的时间，但这在生产环境中变得相当重要。并且在处理大文件时会有些问题。事实证明，如果你面对较大的输入文件(比如几百MB或更多)，最好使用某种分区或并行处理来导入，这一点在本文后面会变得更加清楚。</p><p id="b5ed" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这篇文章不会关注超级精确的基准测试，而是关注如何执行每个CSV导入选项，以及如何自己运行一些简单的基准测试。基于这种偶然的分析，我们可以得出一些结论。这篇文章只关注CSV文件，因为这是表格数据的常用格式。</p><p id="412f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我将在这里介绍的选项有:<strong class="io hj"> csv。DictReader() </strong>，<strong class="io hj"> pandas.read_csv() </strong>，<strong class="io hj"> dask.dataframe.read_csv()。</strong>这绝不是CSV导入的所有方法的详尽列表。然而，这可能是一个最常用方法的详尽列表。</p><p id="b629" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">我如何衡量时间效率</strong></p><p id="0b87" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里我感兴趣的是使用各种方法导入一个CSV文件需要多少时间。这很简单。得到某个动作前后的时间，然后得到以秒为单位的差值。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jk"><img src="../Images/f7e74de84b2476d4f2bc71e159c17740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0pfOJ7sWmhveM6gsYLmZ0Q.png"/></div></div></figure><p id="877e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这种方法得出的结果可能非常不稳定。每一秒钟，不同的过程以不同的强度在计算机上运行，因此它们会干扰测量结果。出于本文的目的，我们可以忽略这种干扰，因为我们对相对时间效率比绝对时间效率更感兴趣。</p><p id="8ad8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们继续主要的比较。</p><p id="0df4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> csv。DictReader() </strong></p><p id="21b0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du jt ju jv jw b">DictReader</code>是一个Python类，它将读取的数据映射为一个字典，除非指定，否则它的键是CSV的第一行。后续行中的所有值都将是字典值，可以使用各自的字典键进行访问。但是，这些值将作为字符串导入。对于数据分析任务，我个人并不经常使用这种方法，但是在某些其他情况下(使用JSON格式或者通过终端处理CSV)这种方法很有价值。你可以这样做:</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jx"><img src="../Images/eb6101fddd31bd481ac91912df7aab81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ImpOaxU8UIlZlZbiI76UcQ.png"/></div></div></figure><p id="8a8f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上面的代码片段将创建CSV对象(数据)。</p><p id="f5fd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> pandas.read_csv() </strong></p><p id="2a92" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">pandas是一个流行的库，它的数据结构适合于包含异质类型的列(整数、浮点数、像我们已经创建的随机数据一样的字符串)以及时间序列的表格数据。对于数据分析任务，Pandas是我首选的数据结构，因为pandas非常容易操作和转换。在某种意义上，它们与r中的<code class="du jt ju jv jw b">data.frame</code>和<code class="du jt ju jv jw b">data.table</code>非常相似。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es jy"><img src="../Images/59b00771571b50a0e02ea5e2676f532e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*Y6tz9h8-dmix7Sr1unEIkQ.png"/></div></figure><p id="eef8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">根据我遇到的许多意见，pandas软件包已经过很好的优化，因此在许多任务中非常有效，包括数据导入等基本任务。</p><p id="b920" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里重要的一点是pandas.read_csv()可以用chunksize选项运行。这将把输入文件分成块，而不是把整个文件加载到内存中。这将减少大型输入文件的内存压力，并且通过反复试验找到最佳的块大小，可以显著提高效率。下面的代码将把输入文件分割成100 000行的块。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jz"><img src="../Images/c7ed38310bf74b74942fb4ed47af861d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b-hHc_ax-DMzhKKiDFQ72Q.png"/></div></div></figure><p id="fb62" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">熊猫的另一个重要功能是<strong class="io hj"> usecols </strong>参数。这允许您只读取选定的要读取的列，并跳过数据中不相关的列。这可以节省我们之前用来读取所有不必要的列的时间。</p><p id="f1ef" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我发现<a class="ae ka" href="http://gouthamanbalaraman.com/blog/distributed-processing-pandas.html" rel="noopener ugc nofollow" target="_blank">Gou tham balara man的这篇文章</a>非常有用，它也解释了如何将输入文件分成块来帮助加速进一步的处理。后来他编辑了他的帖子，说他发现dask.dataframe优于这种方法，这就把我们带到了下一种方法。</p><p id="ad14" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> dask.dataframe </strong></p><p id="871c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae ka" href="http://dask.pydata.org/en/latest/dataframe.html" rel="noopener ugc nofollow" target="_blank"> dask.dataframe </a>是由索引(用于识别数据的行标签)分割的较小的pandas数据帧的集合，可以在一台机器上或集群上的多台机器上并行处理。我是从埃里克·布朗的这篇有用的<a class="ae ka" href="http://pythondata.com/dask-large-csv-python/" rel="noopener ugc nofollow" target="_blank">帖子</a>中了解到dask的。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es kb"><img src="../Images/a9bd5b3357e20e655d802e0d16545b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rvSB04ABsMAUqHo2CsDxmg.png"/></div></div></figure><p id="d8c2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">显然，与带有dask的熊猫不同，数据没有完全加载到内存中，而是准备好进行处理。此外，某些操作可以再次执行，而无需将整个数据集加载到内存中。另一个优点是熊猫使用的大多数功能也可以用于dask。差异源于dask的并行性。</p><p id="904f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">标杆管理</strong></p><p id="6e62" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我用下面的代码对Python方法进行了比较，上面已经介绍了所有这些代码:</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es kc"><img src="../Images/3a2a863838a992d01eb41e9fb528f112.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHApwbcLRxQKwd63_Myo9g.png"/></div></div></figure><p id="74cd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上述代码使用的csv文件大约有500，000行，大小为420 MB。</p><p id="e479" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我得到的结果如下:</p><blockquote class="kd ke kf"><p id="2705" class="im in kg io b ip iq ir is it iu iv iw kh iy iz ja ki jc jd je kj jg jh ji jj hb bi translated"><strong class="io hj"> csv。DictReader </strong>耗时<strong class="io hj"> 0.000013709068298339844秒</strong><br/>T5】PD . read _ CSV耗时<strong class="io hj"> 11.0141019821167秒</strong><br/><strong class="io hj">PD . read _ CSV</strong>with<strong class="io hj">chunk size</strong>耗时<strong class="io hj"> 11.9807119369507秒【T15</strong></p></blockquote><p id="6053" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">csv。DictReader是迄今为止最快的，但是正如我前面指出的，它以字符串的形式导入所有内容，而其他方法试图分别猜测每一列的数据类型，并可能在导入时进行多种其他验证。这意味着如果你使用csv。数据分析任务的DictReader在分析内容之前，你可能需要对它进行一些转换。但是，如果您知道数据中的列都已经是字符串格式，那么可以使用csv。DictReader是值得一试的方法。</p><p id="166e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">忽略csv。DictReader，dask.dataframe是目前最快的方法。这是可以理解的，因为它不像pandas方法那样将整个数据集加载到内存中。</p><p id="2cb8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">总结</strong></p><p id="433e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Pandas在数据科学工作中非常受欢迎，并与许多其他库集成在一起。如果您正在处理一个大型数据集并且RAM不足，我建议您考虑chunksize选项。Dask是一个极大提高导入速度的选项。不过我在数据分析任务上没怎么用过，不过从我的理解来看应该和熊猫挺像的，很有前途。</p><p id="29e4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Csv。DictReader仍然是最快的选择，但是正如我前面提到的，我不推荐它用于数据分析任务。可能使用JSON格式就是这种方法非常有用的一个领域。在生产环境中，当您已经分析了数据，比如说使用pandas，并且只需要偶尔使用最终的预处理数据来重新训练模型，或者使用csv将更新的数据上传到搜索索引，如elasticsearch index。DictReader导入数据更有意义，因为它比其他选项更快。</p><p id="5aaf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是我最近工作时观察到的一个简短概述。如果你有任何建议，请联系我。</p><p id="473c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">干杯！</p></div></div>    
</body>
</html>