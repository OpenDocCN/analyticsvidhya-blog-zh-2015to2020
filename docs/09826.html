<html>
<head>
<title>Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-45bac8688203?source=collection_archive---------24-----------------------#2020-09-21">https://medium.com/analytics-vidhya/linear-regression-45bac8688203?source=collection_archive---------24-----------------------#2020-09-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="a66e" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">线性回归坚持认为，有一条(且只有一条)线可以描述趋势以及两个变量之间的关系。</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jh"><img src="../Images/c85dab95c20155d9dceb80dd3fd63a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uRAxAwTbYEBpiiDP.png"/></div></div></figure><h2 id="104b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">线性回归是一种机器学习算法，其中我们使用直线来解释因变量(Y)和一个或多个解释变量或自变量(X)之间的关系。</h2><p id="878f" class="pw-post-body-paragraph ii ij hi il b im kr io ip iq ks is it ke kt iw ix ki ku ja jb km kv je jf jg hb bi translated">这里，<strong class="il hj">目标类必须是连续特征</strong>并且影响目标类的特征可以是连续的或分类的。</p><p id="e7d2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">在直接应用线性回归之前，应该记住几件事。</p><h2 id="5d86" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">1)-线性</strong>:X和Y的关系是线性的。</h2><h2 id="d716" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">2)-同方差</strong>:残差的方差在x的所有值上都是相同的(残差或误差是期望值和预测值的差)。</h2><h2 id="856a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">3)-独立性</strong>:独立变量不应该相关，即没有多重共线性</h2><h2 id="a603" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">4)-正态性</strong>:残差项应正态分布。</h2><p id="3014" class="pw-post-body-paragraph ii ij hi il b im kr io ip iq ks is it ke kt iw ix ki ku ja jb km kv je jf jg hb bi translated">记住所有这些假设，我们可以建立一个非常有效的线性回归模型。</p><h1 id="a420" class="kw ju hi bd jv kx ky kz jz la lb lc kd ld le lf kh lg lh li kl lj lk ll kp lm bi translated">线性回归工作</h1><p id="ec01" class="pw-post-body-paragraph ii ij hi il b im kr io ip iq ks is it ke kt iw ix ki ku ja jb km kv je jf jg hb bi translated">线性回归试图找到最佳拟合线，通过一条直线来表达所有数据点，以便在不久的将来我们可以预测这些值。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es ln"><img src="../Images/ca3dc65d63305207d1128e2b45762636.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*KJ3jq4Ib0Xx7E3KSKtsOpw.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">最佳拟合线方程</figcaption></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es ls"><img src="../Images/8f3a8ba0eb83e4575fe8dfa0dab58030.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*CGqybK9LIXoRkoeXUnwngQ.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">残差或误差</figcaption></figure><p id="d3b5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">其中y_i=期望值</p><p id="85b2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">y_i(hat) =预测值</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es lt"><img src="../Images/d9af87ede207999e820a1ae62b352f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*-pGytyX2u_S4Kfjp4eKvog.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">残差平方和</figcaption></figure><blockquote class="if ig ih"><p id="6e5a" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">线性回归试图最小化残差平方和，提供最小误差的线将被视为<strong class="il hj">最佳拟合线</strong></p></blockquote><p id="d190" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">现在，问题出现了<em class="ik"> </em> <strong class="il hj"> <em class="ik">如何找到这条最佳拟合线</em> </strong> <em class="ik">。</em></p><p id="0ef9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">一种方法是创建通过数据点的多条线，然后找出所有线的最小残差平方和，并将其绘制为最佳拟合线。相反，更有效的方法是绘制对应于<strong class="il hj">不同斜率</strong> s的<strong class="il hj">成本函数</strong>，具有最小成本函数的斜率将被视为最佳拟合线的斜率。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es lu"><img src="../Images/5b0b52100dcf312d8bce2eddb9ae9ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*saM6wckP6iIVPfDH5piF4g.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">线性回归的成本函数(J)是预测y值(pred)和真实y值(y)之间的<strong class="bd jv">均方根误差(RMSE) </strong>。</figcaption></figure><p id="9af7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated"><strong class="il hj">如何实现具有最小代价函数的斜率？</strong>答案是<strong class="il hj">梯度下降。</strong></p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lv"><img src="../Images/dbeb1ecc7ea2f601fbc88b29f2a18f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9-auZVZV5cpl_jZyybAWg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">2D的梯度下降曲线[案例1和案例2]</figcaption></figure><p id="98e8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">这个想法是从随机w(斜率)值开始，然后迭代更新这些值，达到最小成本。</p><p id="d5b1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated"><strong class="il hj">梯度下降的收敛</strong></p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lw"><img src="../Images/7fa295c9b7e9c1193f25a757cdf7de8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KrQQezmWx7vpb6W6zhsVZg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">收敛定理</figcaption></figure><p id="024d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated"><strong class="il hj">在情况1中，导数是负的，如果我们从w中减去导数，它将增加新的斜率(w’)并更接近最小值</strong></p><p id="25f5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated"><strong class="il hj">在情况2中，导数是正的，如果我们从w中减去导数，它将减少w '值，并接近最小值。</strong></p><p id="23ed" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">随着独立要素数量的增加，梯度下降图将看起来像3D或4D图，这取决于要素的数量，并且所有要素都试图移动到全局最小点。</p><h2 id="0625" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">注意:学习率应该非常小，因为如果我们保持高学习率，它将导致高计算量，因为梯度下降将非常非常慢地收敛。</h2><p id="5686" class="pw-post-body-paragraph ii ij hi il b im kr io ip iq ks is it ke kt iw ix ki ku ja jb km kv je jf jg hb bi translated">这样我们就能找到合适的斜率。从而找到最佳拟合线的方程。</p><p id="0837" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">快乐学习。</p></div></div>    
</body>
</html>