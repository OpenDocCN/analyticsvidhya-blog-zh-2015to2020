<html>
<head>
<title>Part2:Sentiment Analysis in PyTorch ( transformers library)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第二部分:PyTorch(变形金刚库)中的情感分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/part2-sentiment-analysis-in-pytorch-transformers-library-ac7ae8ceae?source=collection_archive---------6-----------------------#2020-04-15">https://medium.com/analytics-vidhya/part2-sentiment-analysis-in-pytorch-transformers-library-ac7ae8ceae?source=collection_archive---------6-----------------------#2020-04-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/188a8c30f6876504db3bfedf56cf43b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bvgf0wmeqMw0jMCf"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://unsplash.com/@tengyart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">腾雅特</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="065a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在<a class="ae iu" rel="noopener" href="/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8">之前的文章</a>中，我们通过在PyTorch中创建定制的神经网络来探索情感分析。</p><p id="0837" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将探索更高级的语言模型。使用/实现语言模型有不同的方法。但是最近由HuggingFace开发的transformers library已经在NLP世界掀起了一阵风暴，因为它使用简单，并且几乎可以获得所有的艺术语言模型。</p><p id="6a46" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要了解BERT和不同的语言模型:<br/><a class="ae iu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-transformer/</a><br/><a class="ae iu" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-bert/</a><br/>这个博客是你能在互联网上找到的最好的解释。</p><p id="1d91" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">HuggingFace的Transformers库提供了许多预训练的语言模型，可以进一步用于/微调特定的NLP任务。<br/>更多信息:<a class="ae iu" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/transformers</a></p><p id="cecf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将通过以下两个步骤进行情感分析/句子分类:</p><ul class=""><li id="3b39" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">加载预训练的蒸馏模型架构，并使用逻辑回归对其进行进一步微调</li><li id="df61" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">加载预训练的DistilBert分类类，并使用PyTorch进一步微调它</li></ul><p id="110f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1之间的区别。蒸馏模型建筑和2。DistilBert分类类:<br/> DistilBert模型架构提供了模型中最后的隐藏状态。这些最后的隐藏状态可以被任何分类模型进一步使用。例如，我们将在逻辑回归中使用它们。<br/> DistilBert分类类使用这些最后的隐藏状态，并初始化分类任务的权重。这些重量需要进一步训练/微调。</p><p id="f305" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据集:<a class="ae iu" href="https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/nica potato/women-ecommerce-clothing-reviews</a></p><p id="c62f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于我们主要关注文本分析，我们将忽略数据集中的其他特性。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="50af" class="kq kr hi km b fi ks kt l ku kv">dataset.head()</span></pre><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/ec026a859335c94e5a9b2216ad66df93.png" data-original-src="https://miro.medium.com/v2/resize:fit:60/0*uAVlYuoVtSCY_m6j"/></div></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/0926d1b5aac1ef02cd1ea27c4a44c4c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*c_QjdA7Xt3IRBLub.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">资料组</figcaption></figure><ol class=""><li id="158d" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js ky jz ka kb bi translated"><strong class="ix hj">加载预训练蒸馏模型架构</strong></li></ol><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="ea29" class="kq kr hi km b fi ks kt l ku kv">!pip install transformers</span></pre><p id="790b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我使用了Google colab和GPU来实现，并且为了提高性能也减少了数据集的大小。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="2934" class="kq kr hi km b fi ks kt l ku kv">from transformers import DistilBertModel,DistilBertTokenizer</span><span id="30f1" class="kq kr hi km b fi kz kt l ku kv">#Tokenization:Convert words in 'review' column to numbers<br/>tokenized_reviews = dataset.review.apply(lambda x: tokenizer.encode(x,add_special_tokens=True))</span><span id="6b6c" class="kq kr hi km b fi kz kt l ku kv">#Padding:To make all sentences of same length.This is only required for batch creation<br/>'''First we need to find maximum length of senetence/review.'''</span><span id="b4b0" class="kq kr hi km b fi kz kt l ku kv">max_len = max(map(len,tokenized_reviews))</span><span id="9504" class="kq kr hi km b fi kz kt l ku kv">padded_reviews = np.array([ i+[0]*(max_len-len(i))  for i in tokenized_reviews])</span><span id="e3bf" class="kq kr hi km b fi kz kt l ku kv">#Masking:To tell DistilBert to ignore padding. This is called attention masking.Basically it'll put 1 where encoding is present and 0 for others.</span><span id="9de5" class="kq kr hi km b fi kz kt l ku kv">attention_masked_reviews = np.where(padded_reviews!=0,1,0)</span><span id="8567" class="kq kr hi km b fi kz kt l ku kv"># Get last hidden states</span><span id="fac7" class="kq kr hi km b fi kz kt l ku kv">input_ids = torch.tensor(padded_reviews).to(device)<br/>attention_mask = torch.tensor(attention_masked_reviews).to(device)</span><span id="b2aa" class="kq kr hi km b fi kz kt l ku kv">with torch.no_grad():<br/>  last_hidden_states =  model(input_ids,attention_mask=attention_mask)</span></pre><p id="5e59" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是我们不需要所有的隐藏状态。我们只需要每个句子的第一个标记“CLS”的最后一个隐藏状态。这个令牌是由BERT添加的，用于分类目的。这个“CLS”记号的向量可以被认为是句子编码。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="f7b9" class="kq kr hi km b fi ks kt l ku kv">'''Dimension of hidden state axbxc</span><span id="d163" class="kq kr hi km b fi kz kt l ku kv">a = number of reviews  <br/>b = number of tokens  <br/>c = number of hidden units '''</span><span id="7e6e" class="kq kr hi km b fi kz kt l ku kv">X = last_hidden_states[0][:,0,:].numpy()<br/>y = dataset.recommended</span></pre><p id="4351" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用简单的逻辑回归进行分类</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="84f5" class="kq kr hi km b fi ks kt l ku kv">from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn import metrics</span><span id="5545" class="kq kr hi km b fi kz kt l ku kv">#Logistic Regression</span><span id="84c5" class="kq kr hi km b fi kz kt l ku kv">X_train,X_test,y_train,y_test = train_test_split(X,y)<br/>log_model = LogisticRegression(max_iter=1500)<br/>log_model.fit(X_train,y_train)<br/>preds = log_model.predict(X_test)<br/>print(metrics.roc_auc_score(y_test, preds))<br/>--0.75</span></pre><p id="f9c7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。加载预训练蒸馏分类等级</strong></p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="c946" class="kq kr hi km b fi ks kt l ku kv">from transformers import DistilBertForSequenceClassification,DistilBertTokenizer</span><span id="c198" class="kq kr hi km b fi kz kt l ku kv">model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=1)</span><span id="7759" class="kq kr hi km b fi kz kt l ku kv">tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')</span></pre><p id="3dbb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将重复之前完成的一些步骤</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="bf68" class="kq kr hi km b fi ks kt l ku kv">tokenized_reviews = dataset.review.apply(lambda x: tokenizer.encode(x,add_special_tokens=True))</span><span id="f6fd" class="kq kr hi km b fi kz kt l ku kv">max_len = max(map(len,tokenized_reviews))</span><span id="31c4" class="kq kr hi km b fi kz kt l ku kv">padded_reviews = np.array([ i+[0]*(max_len-len(i))  for i in tokenized_reviews])</span><span id="168d" class="kq kr hi km b fi kz kt l ku kv">attention_masked_reviews = np.where(padded_reviews!=0,1,0)</span></pre><p id="e995" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将为训练创建数据集。我们已经在上一篇<a class="ae iu" rel="noopener" href="/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8">文章</a>中讨论了数据集创建的步骤。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="0b9e" class="kq kr hi km b fi ks kt l ku kv">#Dataset preparation</span><span id="85fa" class="kq kr hi km b fi kz kt l ku kv">from torch.utils.data import Dataset, TensorDataset,DataLoader<br/>from sklearn.model_selection import train_test_split</span><span id="85a9" class="kq kr hi km b fi kz kt l ku kv">X = torch.tensor(padded_reviews)<br/>X_attention = torch.tensor(attention_masked_reviews)<br/>y = torch.tensor(np.array(dataset.recommended.values)[:,np.newaxis], dtype=torch.float32)</span><span id="d7a8" class="kq kr hi km b fi kz kt l ku kv">X_train,X_test,y_train,y_test = X[:1500],X[1500:],y[:1500],y[1500:]</span><span id="971c" class="kq kr hi km b fi kz kt l ku kv">X_train_attention,X_test_attention = X_attention[:1500],X_attention[1500:]</span><span id="cdec" class="kq kr hi km b fi kz kt l ku kv">train_data = TensorDataset(X_train, X_train_attention, y_train)</span><span id="ee52" class="kq kr hi km b fi kz kt l ku kv">train_loader = DataLoader(train_data,batch_size=16, shuffle=True)</span></pre><p id="c1eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们将在数据集上进行训练</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="21d6" class="kq kr hi km b fi ks kt l ku kv">#Model training</span><span id="3ab1" class="kq kr hi km b fi kz kt l ku kv">NUM_EPOCHS = 1<br/>LEARNING_RATE = 0.01</span><span id="e05f" class="kq kr hi km b fi kz kt l ku kv">optimizer =torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)<br/>loss_fn = torch.nn.BCEWithLogitsLoss()</span><span id="368d" class="kq kr hi km b fi kz kt l ku kv">for i in range(NUM_EPOCHS):<br/>  model.train()<br/>  for X_batch,X_attention_batch,y_batch in train_loader:<br/>     output =   model(X_batch,attention_mask=X_attention_batch,labels=None)<br/>     y_pred = output[0]<br/>     loss = loss_fn(y_pred,y_batch)<br/>     loss.backward()<br/>     optimizer.step()<br/>     optimizer.zero_grad()</span></pre><p id="a636" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将在测试数据集上评估模型</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="45e1" class="kq kr hi km b fi ks kt l ku kv">#Evaluation</span><span id="bf21" class="kq kr hi km b fi kz kt l ku kv">test_dataset = TensorDataset(X_test, X_test_attention)<br/>test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)</span><span id="bde8" class="kq kr hi km b fi kz kt l ku kv">def sigmoid(x):<br/>   return 1 / (1 + np.exp(-x))</span><span id="db7e" class="kq kr hi km b fi kz kt l ku kv">preds = np.zeros([len(test_dataset), 1])<br/>model.eval()</span><span id="981c" class="kq kr hi km b fi kz kt l ku kv">for i, (x_batch, x_mask) in enumerate(test_loader):<br/>   outputs = model(x_batch.to(device),attention_mask=x_mask.to(device))<br/>   y_pred = sigmoid(outputs[0].detach().cpu().numpy())<br/>   preds[i*16:(i+1)*16, :] = y_pred</span><span id="18d0" class="kq kr hi km b fi kz kt l ku kv">print(metrics.roc_auc_score(y_test, preds))</span><span id="890f" class="kq kr hi km b fi kz kt l ku kv">--0.86</span></pre><p id="8ce0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">总结:</strong></p><p id="bf2b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在<a class="ae iu" href="https://github.com/sarang0909/Explore-PyTorch/blob/master/Part2_Pytorch_Sentiment_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> git-repo </a>可获得完整的笔记本。</p><p id="c016" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们已经看到了如何使用transformers库进行分类。我不能写更多关于语言模型如何工作的细节，因为我不可能写得比这篇<a class="ae iu" href="http://jalammar.github.io/" rel="noopener ugc nofollow" target="_blank">博客</a>更好。我强烈推荐阅读它。</p><p id="c806" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你喜欢这篇文章或有任何建议/意见，请在下面分享！</p><p id="2fa3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在<a class="ae iu" href="https://www.linkedin.com/in/sarang-mete-6797065a/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系讨论吧</p></div></div>    
</body>
</html>