<html>
<head>
<title>How does the BERT model work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特模型是如何工作的？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-does-the-bert-machine-learning-model-work-d8520a2791cb?source=collection_archive---------11-----------------------#2020-07-17">https://medium.com/analytics-vidhya/how-does-the-bert-machine-learning-model-work-d8520a2791cb?source=collection_archive---------11-----------------------#2020-07-17</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><div class=""/><p id="6af4" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hh bi translated">BERT已经在GitHub 上<a class="ae jj" href="http://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">开源，也上传到了</a><a class="ae jj" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank"> TF Hub </a>。</p><p id="9c8b" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hh bi translated">我认为理解它的最好方法是玩它的代码。GitHub 上的<a class="ae jj" href="https://github.com/google-research/bert/blob/master/README.md" rel="noopener ugc nofollow" target="_blank">自述文件详细描述了它是什么以及它是如何工作的:</a></p><p id="e068" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hh bi translated"><strong class="in hp">BERT—B</strong>I directional<strong class="in hp">E</strong>n coder<strong class="in hp">R</strong>presentations from<a class="ae jj" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank">T13】Ttransformers</a>是一种预先训练语言表示的方法，这意味着我们在大型文本语料库(如维基百科)上训练一个通用的“语言理解”模型，然后将该模型用于我们关心的下游NLP任务(如问答)。伯特胜过…</p></div></div>    
</body>
</html>