<html>
<head>
<title>Prudent development of Spark jobs in Scala</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Scala中Spark作业的谨慎开发</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/playing-with-spark-in-scala-warm-up-game-8bfbb7cfbcc4?source=collection_archive---------3-----------------------#2019-10-16">https://medium.com/analytics-vidhya/playing-with-spark-in-scala-warm-up-game-8bfbb7cfbcc4?source=collection_archive---------3-----------------------#2019-10-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ea1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不可否认，Apache Spark是目前最流行的数据处理工具之一。云平台，如AWS，将Spark打包在它们的无服务器(AWS Glue)和托管(AWS EMR)产品中，降低了与自行管理Spark集群相关的开销。好消息是，它降低了与管理Spark相关的操作复杂性，并降低了新用户的准入门槛。与此同时，这将Spark呈现为云中闪亮的收缩包装黑匣子，它无需我们了解其内部工作原理就能工作。但是如果我们试着把它从云上拿下来，打开它，在家里玩呢？</p><p id="a4d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark是一个分布式系统，这意味着它将需要完成的工作横向分布在多个执行器上。例如，Spark可以将输入数据分割成分区，并将分区分配给执行器，指示它们处理所分配分区中的数据。然而，这并不意味着每个Spark部署都必须有多个物理机。事实上，一个设计良好的分布式系统可以在您的笔记本电脑上运行，允许您在本地测试其功能。此外，这样的系统允许你在你最喜欢的IDE中运行它们，利用它所有好的特性，比如代码完成、动态编译和调试。</p><p id="2416" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我介绍了一名软件工程师尝试在Spark中构建一个简单的数据转换，并按照我们通常测试软件的方式进行测试。</p><blockquote class="jd je jf"><p id="668e" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">本文中开发的代码可以在<a class="ae jk" href="https://gitlab.com/softminded/transformer-spark-scala" rel="noopener ugc nofollow" target="_blank">https://gitlab.com/softminded/transformer-spark-scala</a>获得</p></blockquote><p id="7c30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文章的其余部分组织如下:</p><p id="49d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">- <a class="ae jk" href="#8387" rel="noopener ugc nofollow">数据处理用Apache Spark </a> <br/> - <a class="ae jk" href="#bf9f" rel="noopener ugc nofollow">设计对象模型</a> <br/> - <a class="ae jk" href="#f227" rel="noopener ugc nofollow">开发工作流程</a> <br/> - <a class="ae jk" href="#84bd" rel="noopener ugc nofollow">读写Spark中的数据</a>-<br/>-<a class="ae jk" href="#74e6" rel="noopener ugc nofollow">单元测试Spark作业</a>-<a class="ae jk" href="#7d4c" rel="noopener ugc nofollow">总结</a></p><h2 id="8387" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">用Apache Spark进行数据处理</h2><p id="8d2a" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">Apache Spark用于执行数据转换和数据查询。数据转换的一个例子是将数据从面向行的格式(如CSV)转换为列格式(如Parquet)。查询的一个例子是在本地硬盘上的CSV文件中存储的雇员目录中搜索具有给定姓氏的雇员。请记住，Spark是一个处理引擎，并不实现自己的持久存储。相反，Spark从外部存储器读取数据，执行数据转换和查询，并将结果写入外部数据存储器，该存储器可能不同于提供输入数据的存储器。这是一个Spark transformer作业的数据流图，该作业将数据从CSV格式转换为拼花格式。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es kl"><img src="../Images/f65a5bb1f1e7ed28b4ed92f55a0a123b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*9Q-tqrlYnzDwgzh4VfGPfQ.jpeg"/></div></figure><p id="f5ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们介绍了数据转换器的开发工作流，它将数据从一种格式转换为另一种格式。这里的主要焦点是探索当开发运行在Apache Spark上的代码时，我们如何利用软件工程工具、技术和过程。</p><p id="0380" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对任何工作来说，使用正确的工具都很重要。软件工程师使用交互式开发环境(ide)来编写代码。ide具有代码自动完成、动态代码编译、代码调试支持(如断点、表达式监视、资源利用监控等)等特性。对于这个项目，我使用了IntelliJ IDE。尽管我选择Scala作为这个练习的编程语言，但是本文中描述的实践也适用于Spark支持的其他语言，比如Python。</p><h2 id="bf9f" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">设计对象模型</h2><p id="c458" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">我们在本文中构建的Spark作业的一个用例是将数据从基于行的格式转换成列格式。这意味着作业需要能够读取初始数据、转换数据并将转换后的数据写入目标。Spark提供了几个编程抽象，如RDD、数据帧和数据集，来表示它所操作的数据。与rdd不同，数据帧和数据集有与之相关联的模式。然而，Dataset schema是强类型的，而DataFrame schema更通用，事实上，它只是Dataset[Row]的类型别名。由于该作业将生成一个列格式，它将有一个模式，因此我们希望使用DataFrame或Dataset来表示输出数据。因为作业应该能够转换不同种类的数据，例如客户、订单、行项目，所以模式在编译时可能是未知的，可能需要从配置文件中读取。由于这些原因，我发现DataFrame对于这个项目来说是一个合适抽象。</p><p id="fa5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是主要对象接口的列表:</p><ul class=""><li id="89e8" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated"><strong class="ih hj">data frame reader</strong>—<em class="jg">read():data frame</em>方法，将数据读入data frame；数据来源和数据格式是一个扩展类的实现细节；</li><li id="d983" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated"><strong class="ih hj">data frame transformer</strong>—<em class="jg">transform(input df:data frame)的特征:DataFrame </em>方法，从给定的数据帧转换数据并返回结果；</li><li id="d24e" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated"><strong class="ih hj"> DataFrameWriter </strong> —带有<em class="jg"> write(DataFrame)的特征:将给定数据帧写入其目的地的单元</em>方法；目的地和数据格式是扩展类的实现细节；</li><li id="db48" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated"><strong class="ih hj"> TableTransformerJob </strong> —一个具有<em class="jg"> run():Unit </em>方法的类，该方法协调读取、转换和写入转换后的数据，如下所示:</li></ul><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lh"><img src="../Images/1aeadea244b91a92d1b0298d0da2871b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*X5K7TO38nZKlSXWNZzlPzg.jpeg"/></div></figure><p id="54bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此时，我们可以开始考虑如何使用Spark读取、转换和写入数据。</p><h2 id="f227" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">开发工作流程</h2><p id="18e6" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">当我们准备编写第一行代码时，考虑一下代码组织和构建过程是个好主意。我使用<a class="ae jk" href="https://github.com/MrPowers/spark-sbt.g8" rel="noopener ugc nofollow" target="_blank"> spark-sbt.g8模板</a>为这个项目创建文件夹结构、示例代码和构建配置。这个项目使用SBT作为Scala构建工具，你可以在Git repo中查看<a class="ae jk" href="https://gitlab.com/softminded/transformer-spark-scala/blob/master/build.sbt" rel="noopener ugc nofollow" target="_blank"> SBT配置</a>。这是IDE中项目组织结构的样子:</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es li"><img src="../Images/e2e424cb38e645e64553ffcb51c20379.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*Yg1K_wMr5fCloWa9qaa31A.png"/></div></figure><p id="d754" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该项目使用GitLab进行源代码和CI/CD管道管理。这里的目标是使用一个单一的服务来托管源代码、运行测试、显示项目状态以及在出现问题时发送通知。GitLab CI/CD管道配置文件。gitlab-ci.yml驻留在源存储库的根目录中，允许我们使用标准Git工作流管理配置更改。该文件包含Docker映像的名称，该映像应该用于提供构建容器和安装必要的依赖项、运行测试、报告代码覆盖率等的脚本。</p><p id="acf2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">评估我们编写的代码的质量很重要。为了做到这一点，我们将编写单元和集成测试来验证代码是否如预期的那样运行并产生期望的结果。单元测试是白盒测试，它试图测试每一行代码，而不测试它与外部API的集成。“白盒测试”意味着测试作者知道确切的实现，并且能够针对其不同的代码分支。单元测试的有效性可以通过它们执行的代码部分来衡量。单元测试需要快速运行，让我们能够验证代码的质量并快速迭代。为此，单元测试模拟对外部数据源和其他第三方依赖项的调用，这会大大降低单元测试的速度。例如，在这个项目中，我使用优秀的<a class="ae jk" href="https://github.com/mockito/mockito-scala" rel="noopener ugc nofollow" target="_blank"> Mockito Scala </a>库来模拟SparkSession。你可以在马丁·福勒的文章中找到更多关于在测试中使用模拟的信息。</p><p id="8ec7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一方面，集成测试是黑盒测试，它检查集成点并从接口级别测试代码，允许代码进行所需的外部调用，例如从文件中读取数据。我使用术语“集成测试”来指代“狭义集成测试”，正如在<a class="ae jk" href="https://martinfowler.com/bliki/IntegrationTest.html" rel="noopener ugc nofollow" target="_blank">马丁·福勒的集成测试文章</a>中所描述的。我将“广泛集成测试”称为“端到端测试”。</p><p id="d4e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">单元和集成测试作为构建过程的一部分运行。英寸gitlab-ci.yml文件我配置了gitlab的CI/CD管道，将代码覆盖结果推送到CodeCov.io服务。这要求在CodeCov.io上生成一个身份验证令牌，并将CODECOV_TOKEN环境变量设置为令牌值。这可以通过在GitLab中的Settings-&gt;CI/CD-&gt;Variables页面上用TOKEN值创建一个CODECOV_TOKEN变量来轻松完成。本文使用的GitLab项目可以通过点击<a class="ae jk" href="https://gitlab.com/softminded/transformer-spark-scala" rel="noopener ugc nofollow" target="_blank">这个链接</a>来访问。</p><h2 id="84bd" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">在Spark中读写数据</h2><p id="0e5b" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">Spark集成了不同的<a class="ae jk" href="https://spark.apache.org/docs/latest/sql-data-sources.html" rel="noopener ugc nofollow" target="_blank">数据源</a>，提供了几个读取不同格式数据的API，比如<a class="ae jk" href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#textFile-java.lang.String-" rel="noopener ugc nofollow" target="_blank">data frame reader # textFile(String):Dataset&lt;String&gt;</a>从文本文件中读取数据。Spark还允许用Spark会话将数据帧注册为虚拟表或视图。一旦注册，该视图就可以在通过调用<a class="ae jk" href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/SparkSession.html#sql-java.lang.String-" rel="noopener ugc nofollow" target="_blank">spark session # SQL(String):Dataset&lt;Row&gt;</a>执行的SQL查询中使用。</p><p id="3628" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们的数据源已经被调用者用SparkSession注册为一个表。基于这一假设，我们实现DataFrameTableReader如下:</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="018c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">DataFrameTableReader类采用SparkSession和DataFrameTableReader。Config作为构造函数参数，并使用它们运行Spark SQL查询，从表中选择具有config中指定的名称的数据。我们采用类似的方法实现DataFrameTableWriter类，如下所示:</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="6b5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还实现了TableTransformerJob—ETL转换的主要入口点，它通过读取、转换和写入数据来编排ETL，如下所示:</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><h2 id="74e6" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">单元测试火花作业</h2><p id="3182" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">让我们编写一个单元测试，验证transformer job是否用正确的参数调用了读取器、转换器和写入器。为此，我们使用以下类签名在测试源树中创建一个TableTransformerJobSpec类:</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="6139" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">FunSuite 是ScalaTest库中的一个类，它为在Scala中编写测试提供支持。ScalaTest支持<a class="ae jk" href="http://www.scalatest.org/user_guide/selecting_a_style" rel="noopener ugc nofollow" target="_blank">不同的测试风格</a>，我选择了一个简单但足够描述性的风格，它允许我提供一个描述并用<code class="du ll lm ln lo b">test(“description”) {}</code>块注册一个测试。</p><p id="2de0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MockitoSugar是来自<a class="ae jk" href="https://github.com/mockito/mockito-scala" rel="noopener ugc nofollow" target="_blank"> Mockito Scala </a>库的特性，它在Mockito Java API的基础上提供了抽象，增加了对更像Scala的语法的支持，比如下面代码中使用的mock[DataFrameReader]。这也允许我们使用传统的带有<code class="du ll lm ln lo b">when(thisHappens).then(doThis)</code>和<code class="du ll lm ln lo b">verify(that).happened</code>调用的期望验证风格的Mockito测试。</p><p id="b30d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SparkSessionLender是一个特征，我通过将SparkSession生命周期管理责任从测试代码卸载到这个特征中，实现了<a class="ae jk" href="http://www.scalatest.org/user_guide/sharing_fixtures#loanFixtureMethods" rel="noopener ugc nofollow" target="_blank">贷款模式</a>。以下方法创建一个本地SparkSession，并将其传递给作为参数传递的测试函数。将此代码提取到一个单独的方法中，允许我们在一个地方调整SparkSession设置，例如，为了启用我们将在下面的集成测试中使用的Hive集成。</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="f930" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们在TableTransformerJobSpec中添加一个单元测试，模拟SparkSession、reader、transformer和writer，只关注测试数据是否正确地从reader传递到transformer以及从transformer传递到writer。我喜欢使用given-when-then注释风格来区分设置测试夹具的代码部分、调用被测代码的代码部分和验证后置条件的代码部分。</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="0a9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们将这些更改推送到GitLab，这将触发构建管道并更新项目的状态，如下所示。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lp"><img src="../Images/6378ce4022add80372e6e4bc675051f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*aCA4Bohew5qfUsoFhJ4-FA.png"/></div></figure><p id="15f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以理解的是，代码覆盖率相当低，因为我们模拟出了大部分代码的功能。期望的代码覆盖率水平是特定于应用程序的，不同的工程团队有不同的代码覆盖率标准，例如类、方法和代码行的覆盖率。在<a class="ae jk" href="https://www.martinfowler.com/bliki/TestCoverage.html" rel="noopener ugc nofollow" target="_blank">测试覆盖文章</a>中，Martin Fowler描述了代码覆盖的价值，并为建立这样的标准提供了一些基本原理。</p><p id="c907" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们添加更多的测试来增加代码覆盖率之前，让我们使我们的转换过程更加灵活一些。我们转换器工作的目的是将数据从一种格式转换成另一种格式。该工作应该能够处理不同类型的数据，如客户数据或来自订单处理系统的数据。我们引入一个DataFrameTransformerFactory类，它将创建一个transformer的实例，给定完全限定的transformer类名作为<code class="du ll lm ln lo b">transformerClass</code>参数。考虑到作业的<code class="du ll lm ln lo b">config.transformerClass</code>设置，我们还将TableTransformerJob更改为委托给DataFrameTransformerFactory进行转换器实例化。</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="dfd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们编写一个单元测试来测试这个新功能，它增加了对配置驱动的转换器实例化的支持。但是首先，让我们创建一个FakeDataFrameTransformer类，测试会将作业配置为委托给它，然后验证由假转换器返回的数据是否被传递给了编写器。我们实现了假的transformer来产生结果(<code class="du ll lm ln lo b">outputDF</code>)，我们可以在下面的测试中进行比较:</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="e4ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经准备好实现一个单元测试，验证作业是否正确地实例化并委托给FakeDataFrameTransformer。</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="1659" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">测试通过了，但是让我们再次检查它是否真的覆盖了我们期望它执行的行。IntelliJ允许我们通过点击下面截图中最右边的图标来运行测试。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es lq"><img src="../Images/c49da4859ec1fcb0bdb2f85ea6f4d3f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*xU7jmqfGglZRMeAzt9Do0Q.png"/></div></div></figure><p id="fd3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">测试完成后，我们可以检查transformer job和factory类，并观察测试所覆盖的每一行左边的绿色标记。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es lv"><img src="../Images/de4c2fa54a3e7fe843b540489fe77ee9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MnEFAibIxUgqHmKiBPGnoQ.png"/></div></div></figure><p id="c08c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这很酷，我们已经通过了测试，并运行了我们期望的代码行。那么，让我们运行所有的测试，并确认它们都仍然通过。我们可以在IntelliJ中或者通过执行<code class="du ll lm ln lo b">sbt clean coverage test coverageReport</code>命令来做到这一点，该命令正是我们的构建过程所运行的命令。现在，我们可以用一个描述性的Git注释来提交对Git的更改，比如“添加了一个单元测试来验证TableTransformerJob委托给了配置中设置的transformer类”并将其推送到GitLab，从而触发构建管道。完成后，我们在GitLab中的项目页面显示测试覆盖率上升，如下所示。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lw"><img src="../Images/b9b9383e319458b8a53c584397d83fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*-dIxCMj-IER8WDcm9AYfwA.png"/></div></figure><p id="6e8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好的，这很好，那么让我们通过点击codecov徽章来探索哪些代码还没有被测试覆盖。这将显示DataFrameTableReader和DataFrameTableWriter都没有被覆盖，因为我们所有的测试到目前为止都模拟了它们。</p><p id="2fa8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们添加一个测试，该测试在内存中生成一些测试数据，用SparkSession将其注册为一个表，并要求TableTransformerJob编排该数据的身份转换。</p><blockquote class="jd je jf"><p id="064d" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">来自<a class="ae jk" href="https://en.wikipedia.org/wiki/Identity_transform" rel="noopener ugc nofollow" target="_blank">维基百科</a>:身份转换是一种数据转换，它将源数据不加修改地复制到目标数据中。</p></blockquote><p id="95a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在不依赖于输入数据语义的情况下，身份转换非常有用。这种情况包括压缩以原始文本格式存储的输入数据，或者将大的输入文件分割成更小、更多的文件，以便提高并行处理大量文件的能力。下面显示了标识转换器的实现。</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="c621" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的测试中，我们不使用mocks，让transformer作业负责实例化reader、transformer和writer，就像在生产中一样。我们生成一些假的输入数据，并用SparkSession将其注册为一个表。输入数据应由DataFrameTableReader读取，并传递给IdentityDataFrameTransformer，后者将返回没有任何修改的数据。然后，数据将被提供给DataFrameTableWriter，以将其存储在用SparkSession注册的输出表中。但是，DataFrameTableWriter假定输出表已经存在，因此要求测试使用适当的模式创建该表。请记住，Spark没有自己的持久存储，而是委托其他技术进行持久存储管理。其中一项技术是<a class="ae jk" href="http://hive.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Hive </a>，它支持本地文件系统上的持久存储。要启用Hive支持，我们需要做两件事:</p><ul class=""><li id="56a3" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated">在build.sbt中添加spark-hive依赖:<br/> <code class="du ll lm ln lo b"><em class="jg">libraryDependencies </em>+= <strong class="ih hj">“org.apache.spark” </strong>%% <strong class="ih hj">“spark-hive” </strong>% <strong class="ih hj">“2.4.3” </strong>% <strong class="ih hj">“provided” </strong><em class="jg">// for integration testing</em></code></li><li id="d1d1" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">在SparkSessionLender # withLocalSparkContext:<br/><code class="du ll lm ln lo b"><strong class="ih hj">val </strong>spark = SparkSession.<em class="jg">builder</em>()<br/> .appName(<strong class="ih hj">“spark testing”</strong>)<br/> .master(<strong class="ih hj">“local”</strong>)<br/> <strong class="ih hj">.enableHiveSupport()</strong><br/> .getOrCreate()</code>内配置的SparkSession中启用配置单元支持</li></ul><p id="bbe8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们使用<a class="ae jk" href="http://www.scalatest.org/user_guide/sharing_fixtures#loanFixtureMethods" rel="noopener ugc nofollow" target="_blank"> Loan模式</a>来实现代码，该代码在给定表名和模式的情况下创建一个表，执行测试函数，并通过删除表来进行自我清理。</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="37c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经准备好编写一个集成测试，测试DataFrameTableReader、DataFrameTableWriter和IdentityDataFrameTransformer中的代码。该测试通过使用来自<a class="ae jk" href="https://github.com/MrPowers/spark-fast-tests" rel="noopener ugc nofollow" target="_blank"> spark-fast-tests </a>库的assertSmallDataFrameEquality(actualDF，expectedDF)断言输出表包含与输入表相同的数据来验证身份转换是否正常工作。我们使用<code class="du ll lm ln lo b">import spark.implicits._</code>引入一些Spark助手函数，使Spark数据帧的工作变得更简单，例如，通过调用RDD#toDF创建一个包含测试输入数据的数据帧。</p><figure class="km kn ko kp fd kq"><div class="bz dy l di"><div class="lj lk l"/></div></figure><p id="4a8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们通过在本地运行<code class="du ll lm ln lo b">sbt clean coverage test coverageReport</code>来验证我们所有的测试仍然通过。之后，我们可以用描述性的注释将我们的更改提交给Git，并将其推送到GitLab，从而触发构建管道。构建过程完成后，GitLab中的项目状态如下所示。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lx"><img src="../Images/0d9c703dbb56555aeaa57c1b805afe30.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*AVqYdIEJXPZJI0yTxeFENQ.png"/></div></figure><p id="3142" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这看起来相当不错——我们所有的测试都通过了，我们实现了100%的代码覆盖率。下面来自CodeCov.io的截图展示了我们的代码覆盖率是如何随时间变化的，以及当前的逐行覆盖率统计数据。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es ly"><img src="../Images/feca42a54d518fe499e8537c5c2bf30e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rXZmUqykI_Ijz7d09KQMxA.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">代码超龄趋势(CodeCove.io)</figcaption></figure><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es md"><img src="../Images/531aba928244124b85154ffa1716d5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2QIl78hAUc1GPxBJkTunoQ.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">项目顶层包(CodeCove.io)中包含的行的摘要</figcaption></figure><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es me"><img src="../Images/27a6218ac9b96dc556dd2fcb69a0aae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGNcZMSOm9VBOt5vVTRINg.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">每个类中包含的行的分解(CodeCove.io)</figcaption></figure><p id="92ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的代码状态非常好，所以让我们在Git中标记这个代码状态，以防我们将来想要检查这个代码状态。</p><pre class="km kn ko kp fd mf lo mg mh aw mi bi"><span id="5525" class="jl jm hi lo b fi mj mk l ml mm">git tag -a warmup-0.1 -m "identity transformation with tests and code coverage"<br/>git push --tags</span></pre><h2 id="7d4c" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">摘要</h2><p id="dc18" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">在这篇文章中，我们开始从软件工程的角度探索在Scala中使用Spark代码。我们在Git中创建了一个源代码库，并在GitLab中为它配置了一个CI/CD管道。我们集成了管道以将代码覆盖度量推送到CodeCov.io，并实现了单元和集成测试以实现高水平的覆盖。在我们的单元测试中，我们试验了对象模仿技术。在集成测试中，我们生成了一个样本数据集，并用SparkSession将其注册为一个表。我们启用了Spark与Hive的集成，以便允许测试将转换后的数据写入由本地文件系统支持的Hive表。在下一篇文章中，我们将通过实现一个实际用例的数据转换来继续这一探索。</p></div></div>    
</body>
</html>