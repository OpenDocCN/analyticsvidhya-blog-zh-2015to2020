<html>
<head>
<title>Paper Explained- Rethinking Attention with Performers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文解释-反思表演者的注意力</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/paper-explained-rethinking-attention-with-performers-b207f4bf4bc5?source=collection_archive---------6-----------------------#2020-10-31">https://medium.com/analytics-vidhya/paper-explained-rethinking-attention-with-performers-b207f4bf4bc5?source=collection_archive---------6-----------------------#2020-10-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/fb543442fd81bf598205cf39246faccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yt_fc-FMBOfEFikutf-_YQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">通过(随机)特征图近似常规注意机制AV(在D⁻重整化之前)。虚线框表示计算的顺序以及相应的时间复杂度。图片摘自<a class="ae iu" href="https://arxiv.org/pdf/2009.14794.pdf" rel="noopener ugc nofollow" target="_blank">论文。</a></figcaption></figure><h1 id="4a40" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">简介和概述</h1><p id="63a6" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">表演者是一类新的模特，他们近似于<a class="ae iu" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">的变形金刚</a>。他们这样做不会遇到经典的transformer瓶颈，即transformer中的注意力矩阵具有与输入大小成平方关系的空间和计算要求，这限制了您可以向模型中输入多少输入(文本或图像)。表演者通过一种叫做<strong class="jv hj">的技术通过正正交随机特征(FAVOR+)快速注意来解决这个问题。</strong> Performers是完全兼容常规变换器的线性架构，具有强大的理论保证:注意力矩阵的无偏或接近无偏估计，一致收敛，低估计方差。表演者；能够<strong class="jv hj">可证明</strong>准确且实用地估计正则(softmax)满秩注意力，但仅具有线性空间和时间复杂度，并且<strong class="jv hj">不依赖于任何先验</strong>如稀疏性或低秩性。表演者是第一个完全兼容常规变压器的线性架构。</p><h1 id="f40b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">常规注意机制与广义核心化注意</h1><p id="358f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">注意，X作为输入。x通过Wq变换成查询(Q ),通过Wk变换成键(K ),还通过Wv变换成值(V)。现在，通过将关键字(K)乘以查询(Q)来获得注意力图。我们应用softmax非线性来标准化地图。最后，我们将取值(V)并将其与地图相乘，以获得输出(Z)，称为自我关注矩阵。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/135f2ab4d20507c6379af4e2ac6f8e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FKe8Fs1Cwy2I5UMJ7REsqA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">文本数据的双向注意机制中的二次瓶颈。</figcaption></figure><p id="4ca3" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">这里l是输入符号序列的大小，d是隐藏维度(潜在表示的维度)，exp()按元素应用，1ₗ是长度为l的全1向量，diag()是以输入向量作为对角线的对角矩阵。</p><p id="c453" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">如果能<strong class="jv hj">把softmax分布</strong>分解到<strong class="jv hj">形成一个更低维的注意力矩阵(A) </strong>不是更简单吗？让我们假设，你可以先乘以KᵀV，然后乘以q，如果d比l小得多，这会节省很多计算</p><p id="e133" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">许多作者试图分解注意力矩阵(A ),但这是通过某些假设或添加太多偏见来完成的。例如，通过<strong class="jv hj">位置敏感散列(LSH </strong>)来利用稀疏性是许多技术中的一种。那么这篇论文提出了什么建议呢？作者使用了一个内核框架来描述双向注意力的<strong class="jv hj"> FAVOR+算法</strong>。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/fb543442fd81bf598205cf39246faccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yt_fc-FMBOfEFikutf-_YQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">通过(随机)特征图近似常规注意机制AV(在D⁻重整化之前)。虚线框表示计算的顺序以及相应的时间复杂度。图片摘自<a class="ae iu" href="https://arxiv.org/pdf/2009.14794.pdf" rel="noopener ugc nofollow" target="_blank">报。</a></figcaption></figure><p id="533e" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">在可内核化注意力中，一切都与注意力机制相同，但是，注意力矩阵(a)被分解成q’和(k’)<strong class="jv hj">、</strong>ᵀ，它们被称为质数，因为它们不是查询和关键字(查询和关键字相乘形成注意力矩阵)。</p><p id="568b" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">所以在概念上，q '×(k ')<strong class="jv hj">ᵀ</strong>≈exp[(q×k<strong class="jv hj">ᵀ</strong>)<strong class="jv hj">/√</strong>d]。但是这种分解是如何发生的呢？作者提出了一个通用的可内核化的注意机制，以显示他们如何分解注意矩阵。</p><p id="f875" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">作者概括了这个问题，并使用FAVOR+算法来解决它。他们说我们有A(i，j) = <strong class="jv hj"> K </strong> (qᵢ ᵀ，kⱼᵀ)形式的注意力矩阵a，其中qi/kj代表Q/K中的第I/j个查询/关键行向量，并且某个核函数K: Rᵈ× Rʳ→ R+被定义用于(通常是随机化的)映射:<strong class="jv hj">φ</strong>:rᵈ→rʳ+(对于某个r &gt; 0)如下:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/6119fd8d2f698012ab2e6123b7c50226.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*63nn-4uEaSf8tl-x9tRzXw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">近似公式。</figcaption></figure><p id="dc1c" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">现在，内核允许你在其他空间中表示内积。通过函数(<strong class="jv hj">φ</strong>)拉动时，两个输入的核函数(如上所示)将等于两个输入的内积之和。在上面的数学描述中，LHS是非线性softmax核，而RHS是线性函数。这使我们能够使Q’和K’矩阵线性，然后您可以简单地首先将K’乘以V，然后乘以Q’以减轻巨大的注意力矩阵a。那么，它们如何逼近softmax注意力内核(K)？</p><h1 id="9b6e" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">逼近Softmax内核</h1><p id="eae7" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">作者使用了随机傅立叶特征的数学概念。他们明确地没有为<q class="jv hj">ᵀ&gt;t21】≈exp[(q×k<strong class="jv hj">ᵀ</strong>)<strong class="jv hj">/√</strong>d]构造完全相等的高维空间，但是他们构造了一个近似，并且这个近似是任意好的。</q></p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/5c904bb88b50aace89aff9f46bd9427b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k2ssKAKxLPRVWPm00MM61g.jpeg"/></div></div></figure><p id="0f34" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">查询和键通过<strong class="jv hj">φ函数</strong>进行映射，该函数为我们提供Q’和K’。内积是在存在Q’和K’的任何维度空间中取的，其近似等于原始Q和K乘以softmax非线性。那么函数(<strong class="jv hj">φ</strong>)是什么样子的呢？</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/a3deb965972a8f114258448da39ea16f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lr9kNbXk6RVYLgirRqqcCg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd ix">φ函数。</strong>图像取自<a class="ae iu" href="https://arxiv.org/pdf/2009.14794.pdf" rel="noopener ugc nofollow" target="_blank">纸张。</a></figcaption></figure><p id="e805" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">这里h(x)是输入的确定性函数，<strong class="jv hj"> √ </strong> m是归一化因子，<strong class="jv hj"> ω </strong> ₁….ωₗ <strong class="jv hj">是从均匀高斯正态分布中提取的随机向量</strong>围绕平均值d各向同性，f₁… fₗ是确定性函数，其余的是映射到总维度空间的向量(细节在下面给出)。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/dec60e5421c28ce5e36804436500a9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kWDgy9ynLo5fugEmXyxLwQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd ix">对φ函数的深入直觉。</strong></figcaption></figure><p id="c4af" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">在向量内部，有L个不同的子向量，它们都是一个接一个地连接在一起的。在每个子向量中，你总是有相同的重复项。让我们进一步分析一下。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/5ebc9989183a4588ddd6a876b686b493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6EtjEXbOHJYMHT8GhAPBKg.jpeg"/></div></div></figure><p id="ade5" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">x是向量(Q或K)，ω是从各向同性均匀高斯正态分布D中提取的随机向量，但它们在整个算法中保持不变(有一种方法可以对它们进行重采样，但作者没有描述)。所以ω是一串随机向量。计算每个x与ω s '的内积，这给了我们实数，然后我们将有一个函数集合(在这种情况下是f1和F2；可以更多)。</p><p id="0d8b" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">请注意:f1=sin和f2=cos只是构型的例子，它可以是任何函数。</p><p id="32fa" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">我们将制作一个表格，将实数放入函数中，并将表格展平为一个向量。这个向量被称为随机特征向量。最值得注意的是，这是一个例子，我们并不总是将sin和cos作为f1和f2，这纯粹是一个例子。你甚至可以有一个功能或者多个功能。你也可以选择<strong class="jv hj">采样多少ω，这是这里的参数</strong>。</p><h1 id="e373" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">不同的选择，不同的内核</h1><p id="00cc" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">你有两个h和f的选择，它们是并行的。h和f的选择决定了φ(<strong class="jv hj">φ</strong>)函数是什么，随后，它决定了这个φ(<strong class="jv hj">φ</strong>)函数对应于哪个核函数(K)。因此，通过选择正确的函数，您可以告诉该函数您想要逼近哪个核，然后通过对ω(<strong class="jv hj">ω</strong>)进行采样，您采样的ω(<strong class="jv hj">ω</strong>)越多，就越能精确地逼近该核。定义常规注意力矩阵A的softmax-kernel被给出为:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/40a86020233765ca947e5b81063bd030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KTu5MsH9B5K2JmI3zbrvhQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">基于高斯核的内点积注意力矩阵近似。图片摘自<a class="ae iu" href="https://arxiv.org/pdf/2009.14794.pdf" rel="noopener ugc nofollow" target="_blank">论文。</a></figcaption></figure><p id="3aea" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated"><strong class="jv hj"> softmax内核通过使用高斯内核</strong>来近似。如果我们选择<strong class="jv hj"> h(x)=1，l=2，f1=sin，f2=cos </strong>，则分布为均值周围各向同性的正态分布，其中<strong class="jv hj"> D= N (0，I𝒹) </strong>。这导致高斯核(K𝓰ₐᵤₛₛ).</p><p id="ba9c" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">此后，我们通过使用三角函数获得softmax核的随机特征图无偏近似，其中:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/9c282fbc9e3d2c5bb61e0b9fe2318eef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*2DpYM52N4xNL9TRu0qhR0g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">SM(x，y)的随机特征映射无偏近似值的值。图片摘自<a class="ae iu" href="https://arxiv.org/pdf/2009.14794.pdf" rel="noopener ugc nofollow" target="_blank">论文。</a></figcaption></figure><p id="3f55" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">使用L=2，f1=sin，f2=cos得到高斯核，然后前面的因子h(x)得到softmax核。所以如果我们<strong class="jv hj"> h，l，</strong>和<strong class="jv hj"> f </strong> (f1，f2，…fn)如上所示我们称之为SMᵗʳᶦᵍ(x，y)。值得注意的是，当我们通过phi(<strong class="jv hj">φ</strong>)函数映射我们的查询和键，然后在它们之间生成内积，该内积将近似(取决于我们采样了多少omega(<strong class="jv hj">ω</strong>)结果，就好像我们首先将它们相乘，然后通过softmax函数处理它们一样。这就简单多了，因为我们可以通过φ(<strong class="jv hj">φ</strong>)函数独立地处理它们，然后这只是一个线性运算，允许我们用V乘以K，然后乘以Q，而不是反过来，这是我们在应用softmax时被迫要做的。我们省略√d-重整，因为我们可以等效地重整输入键和查询。</p><h1 id="9b8e" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">为什么天真的方法不起作用！</h1><p id="bb59" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">注意模块为每个标记构造值向量的凸组合，其系数作为相应的重正化核分数给出。这就是使用产生非负分数的核的原因。应用具有潜在负维度值(sin/cos)的随机特征映射会导致不稳定的行为，尤其是当核得分接近0(这是A的许多条目的情况，对应于不相关的表征)时，在这样的区域中由具有大方差的估计器来近似。这会导致异常行为，例如负对角线值重整器D1，从而完全阻止训练或导致次优模型。作者从经验上证明了这就是SMₘᵗʳᶦᵍ(x，y)的情况，并提供了详细的理论解释，表明SMₘᵗʳᶦᵍ(x，y)的方差随着近似值趋于0而变大。这是为什么从未提出用于近似常规softmax注意力的鲁棒随机特征映射机制的主要原因之一。</p><h1 id="0235" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">经由正面特征的更好的近似</h1><p id="676e" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">作者提出了一种不同的分解，一种对softmax核的不同近似。他们说我们也可以用下面的公式来近似softmax内核:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/a62a58b132783317cea3ac82fdfa66d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qb96hCVobmwgKfELNHZWxg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Softmax核的正随机特征。图片摘自<a class="ae iu" href="https://arxiv.org/pdf/2009.14794.pdf" rel="noopener ugc nofollow" target="_blank">论文。</a></figcaption></figure><p id="9de7" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">我们可以用两种方式选择h(x)，L，F(f1，f2，…)，D(如上图)。但是第二种选择更有意义，因为存在一点标准化因素。x和y可以被认为是查询和键，z= x+y，这只是一个不同于以前的近似值，这个近似值很酷的一点是，近似值本身只有正值。如果我们将<strong class="jv hj"> ω </strong>替换为<strong class="jv hj">√d(ω/| |ω|)</strong>，我们可以获得所谓的正则化softmax-kernel SMREG，我们可以以类似的方式近似它，只需将<strong class="jv hj"> D= N (0，I𝒹) </strong>变为D = Unif(√dSᵈ⁻)，一种对应于半径为√d的球面上的哈尔测度的分布(均匀分布)，获得估计量SMREGₘ⁺.这种随机特征也可以用来精确地近似正则softmax-kernel。</p><h1 id="d201" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">正交特征甚至更好</h1><p id="9ce2" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了进一步减少估计量的方差(以便我们可以使用更少数量的随机特征r)并更好地逼近，我们将不同的随机样本<strong class="jv hj"> ω </strong> ₁、…、<strong class="jv hj"> ω </strong> ₘ纠缠成完全正交。这可以在通过标准的Gram-Schmidt重正化过程使用各向同性分布D(即，特别是在我们迄今考虑的所有核中)的同时保持无偏性。正交随机特征(orf)是一种众所周知的方法，但事实证明，它与我们为softmax引入的正随机特征(PRF)一起工作得特别好。这导致了第一个理论结果，表明orf可以用于减少任何维数d的softmax/Gaussian核估计量的方差。</p><p id="151b" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated"><strong class="jv hj">如果你喜欢这篇文章并获得了真知灼见，请考虑</strong> <a class="ae iu" href="https://www.buymeacoffee.com/nakshatrasinghh" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">请我喝杯咖啡</strong> ☕️ <strong class="jv hj">点击这里</strong> </a> <strong class="jv hj"> :) </strong></p><h1 id="e1dd" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考</h1><ol class=""><li id="d2cb" class="lj lk hi jv b jw jx ka kb ke ll ki lm km ln kq lo lp lq lr bi translated"><a class="ae iu" href="https://arxiv.org/pdf/2009.14794.pdf" rel="noopener ugc nofollow" target="_blank">反思表演者的注意力。</a></li><li id="d2bf" class="lj lk hi jv b jw ls ka lt ke lu ki lv km lw kq lo lp lq lr bi translated">你所需要的只是注意力。</li></ol><p id="e21e" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">如果你喜欢这个帖子，请一定要鼓掌👏。💬连接？让我们来看看社会:<a class="ae iu" href="http://myurls.co/nakshatrasinghh" rel="noopener ugc nofollow" target="_blank"><strong class="jv hj"/></a><strong class="jv hj">。</strong></p></div></div>    
</body>
</html>