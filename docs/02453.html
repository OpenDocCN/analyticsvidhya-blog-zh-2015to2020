<html>
<head>
<title>Question Pair Similarity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">问题对相似度</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/question-pair-similarity-be19f3fee6a4?source=collection_archive---------10-----------------------#2019-12-17">https://medium.com/analytics-vidhya/question-pair-similarity-be19f3fee6a4?source=collection_archive---------10-----------------------#2019-12-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/78485ad3a3f3352970a01e40e511f9ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-COq9ZHW8x_KI_hpUj4gw.jpeg"/></div></div></figure><p id="51f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每月有超过1亿人访问Quora，所以很多人问类似措辞的问题也就不足为奇了。具有相同意图的多个问题会导致搜索者花费更多的时间来寻找问题的最佳答案，并使作者感到他们需要回答同一问题的多个版本。这对于作者和搜索者来说都是一个糟糕的用户体验，因为答案在同一问题的不同版本中变得支离破碎。其实这是一个在Stack Overflow等其他社交问答平台上非常明显的问题。一个现实世界的问题，求人工智能解决:)</p><p id="c383" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">问题陈述</strong></p><p id="09a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">识别Quora上被问到的问题是已经被问过的问题的重复。</p><p id="ab93" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">到目前为止，我们看到了业务问题陈述和约束，但是我们必须使用机器学习来解决问题，所以我们必须将业务问题陈述转换为机器学习问题陈述。所以我们想预测两个问题是否相似。</p><p id="9eca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以机器学习问题陈述如下:</p><blockquote class="jo jp jq"><p id="343f" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><em class="hi">“这是二元分类问题，对于给定的一对问题，我们需要用性能度量(对数损失)来预测它们是否重复”</em></p></blockquote><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es jv"><img src="../Images/ffc06430141b753b996817808eb79320.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*AaE0H4wpT-REWBEpKrH6LQ.jpeg"/></div></figure><p id="aa23" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们开始研究问题陈述。</p><h1 id="7c06" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">让我们开始吧。</h1><h1 id="b890" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">探索性数据分析</h1><p id="00fd" class="pw-post-body-paragraph iq ir hi is b it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn hb bi translated">这是任何机器学习问题的第一个重要部分。它让您对数据集有更深入的理解，并有助于根据问题陈述的要求添加一些新功能。</p><p id="2c9b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们首先通过加载数据集来启动EDA。</p><p id="824b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里我们给出了最少数量的数据字段，包括:</p><ul class=""><li id="2b6c" class="ld le hi is b it iu ix iy jb lf jf lg jj lh jn li lj lk ll bi translated">id:看起来像一个简单的行ID</li><li id="59c3" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">qid{1，2}:问题对中每个问题的唯一id</li><li id="ca4c" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">问题{1，2}:问题的实际文本内容。</li><li id="112c" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">is_duplicate:我们试图预测的标签—这两个问题是否彼此重复。</li></ul><p id="7efa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jr">输出类之间数据点的分布</em> </strong></p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/8c8c3e44014e5618f9fa58aed4319c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*ewIrqHQCUqPby8SEKqI6iQ.png"/></div></figure><p id="44f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们在上面看到的，大约63%的问题对是不重复的，36%的问题对是重复的。</p><p id="4afb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jr">每题出现次数</em> </strong></p><p id="f60c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">单个问题最多出现157次。</p><h1 id="3d9e" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">文本预处理</h1><ol class=""><li id="49a5" class="ld le hi is b it ky ix kz jb ls jf lt jj lu jn lv lj lk ll bi translated">删除html标签</li><li id="282f" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">删除标点符号</li><li id="4943" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">执行词干分析</li><li id="9c88" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">删除停用词</li></ol><p id="dffd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">停用词是指经常出现的词。为了删除停用词，我们使用nltk库。</p><p id="13c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">词干化是指将句子中的单词转换成不变部分的过程。在上面的例子中，这个词干是amus。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/7c6ccce168202cbc16a913efe95bff76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*89e0WU6VMUXuOvDL7LyG1w.png"/></div></div></figure><p id="8639" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种技术就像侦探夏洛克·霍姆斯。你必须成为一名侦探才能提取特征。有时领域专家会告诉你添加这些新特性，但是如果有新问题，那么你必须自己提取特性。对于这个问题，我参考了kaggle的观点，哪些特性是重要的，我将在下面解释。</p><p id="7cb2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">定义:</p><p id="cb89" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">令牌:你可以通过把句子分成一个空格得到一个令牌</p><p id="ca32" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Stop_Word:根据NLTK停止字。单词:不是停用词的标记</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/5888ab93fc8c39ee52afb3f2130e5e34.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*GtnCm2ADU4Yqekp7AfL37g.gif"/></div></figure><h1 id="a302" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">特点:</h1><ol class=""><li id="2b65" class="ld le hi is b it ky ix kz jb ls jf lt jj lu jn lv lj lk ll bi translated">CWC _ min:Q1和Q2常用字数与最小字数长度之比。</li><li id="995f" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">CWC _ max:Q1和Q2的常用字数与最大字数的比值。</li><li id="986f" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">csc_min:公共停靠点计数与Q1和Q2停靠点计数的最小长度之比。</li><li id="f3a9" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">CSC _ max:common _ stop _ count与Q1和Q2站点计数的最大长度之比。</li><li id="f29c" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">CTC _ min:common _ token _ count与Q1和Q2令牌计数的最小长度之比。</li><li id="4219" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">CTC _ max:common _ token _ count与Q1和Q2令牌计数最大长度的比值。</li><li id="9409" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">last_word_eq:检查两个问题的lastword是否相等。</li><li id="e1ab" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">first_word_eq:检查两个问题的第一个单词是否相等。</li><li id="a8ea" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">abs_len_diff : Abs。长度差。</li><li id="ea1e" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">mean_len:两个问题的平均令牌长度</li><li id="4ab6" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">longest_substr_ratio:最长公共子串的长度与Q1和Q2令牌计数的最小长度之比。</li><li id="6d99" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">FuzzyWuzzy是一个Python库，用于字符串匹配。模糊字符串匹配是查找与给定模式匹配的字符串的过程。</li></ol><p id="3686" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> WordCloud </strong></p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/2f68139819892b605de2861f5ffbb72c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*q_flGUPBBBPaWCkL5_Clfg.png"/></div></figure><p id="c3ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如你所见，越大的单词意味着发生的次数越多。</p><p id="1104" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">让我们绘制pairplot，看看我们的特征如何表现。</strong></p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/086c5ff39c099f2db3ea49c0b25df0da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UTosmHZ54JoAy-LzWJuJxw.png"/></div></div></figure><p id="f8d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到蓝色和橙色的点是分开的。有一些重叠，但我们可以认为这是一个很好的特征，并尝试随机模型，并提取重要的特征。</p><p id="14d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们有15个特征。我们使用降维技术T-SNE(<strong class="is hj">T</strong>-分布式随机邻居嵌入)。</p><p id="51ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">t-SNE -&gt;它是一种用于探索高维数据的非线性降维算法。它将多维数据映射到适合人类观察的两个或多个维度。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/b7f6499d402c159f9efcec6e86649c15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*wnOjj_Wk-45MkfZ_fYQgZg.png"/></div></figure><p id="7261" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们所见，在某些部分，红点和蓝点是可以分开的。所以我们得出结论，我们构建的特性适合建模。为了获得更好的性能，我们可以根据需要添加更多功能。</p><h2 id="b4da" class="mb kb hi bd kc mc md me kg mf mg mh kk jb mi mj ko jf mk ml ks jj mm mn kw mo bi translated">用TF-IDF加权词向量表征文本数据</h2><p id="56e2" class="pw-post-body-paragraph iq ir hi is b it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn hb bi translated">有多种技术可以将文本转换成矢量，如单词包、TF-IDF、avg w2v等。这里我们使用tfidf加权w2v。</p><p id="64f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我解释一下TF-IDF</p><p id="a65c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> TF-IDF </strong>代表“项频率—逆数据频率”。</p><p id="194d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">词频(tf) </strong>:给出这个词在语料库中每个文档中的出现频率。</p><p id="6861" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">逆数据频率(idf): </strong>用于计算语料库中所有文档中稀有词的权重。</p><p id="a89f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Word2vec是一种用于产生单词的分布式表示的算法，我们所说的<strong class="is hj">指的是</strong>单词类型；即词汇表中的任何给定单词，如get或grab或go，都有其自己的单词向量，并且这些向量被有效地存储在查找表或字典中。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/d3c67f3b236f9033acfa39dbbc849644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OEmWDt4eztOcm5pr2QbxfA.png"/></div></div></figure><p id="afaa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如你所见，语义词有接近的向量表示。</p><p id="66d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在真正的乐趣开始了…</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/2590758c402cd38aca60f287603324b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/1*NF01uqOgORrLaNQ3HNxxDA.gif"/></div></figure><p id="c8eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">机器学习模型</strong></p><p id="fac2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们尝试随机模型，以获得最坏情况下的日志丢失。</p><pre class="jw jx jy jz fd mr ms mt mu aw mv bi"><span id="fc84" class="mb kb hi ms b fi mw mx l my mz">predicted_y = np.zeros((test_len,2))<br/>for i in range(test_len):<br/>    rand_probs = np.random.rand(1,2)<br/>    predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])</span></pre><p id="2477" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们发现随机模型给出的对数损失为0.88。现在，我们尝试各种线性模型，并试图实现远小于0.88的对数损失。</p><p id="27f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们尝试用逻辑回归模型来解决分类问题</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/b7fa20a72a886ace640b581af1f6b2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-Sn5LfxHVyRrByuCYXA8A.png"/></div></div></figure><pre class="jw jx jy jz fd mr ms mt mu aw mv bi"><span id="d89c" class="mb kb hi ms b fi mw mx l my mz">For values of alpha = 1e-05 The log loss is: 0.592800211149<br/>For values of alpha = 0.0001 The log loss is: 0.532351700629<br/>For values of alpha = 0.001 The log loss is: 0.527562275995<br/>For values of alpha = 0.01 The log loss is: 0.534535408885<br/>For values of alpha = 0.1 The log loss is: 0.525117052926<br/>For values of alpha = 1 The log loss is: 0.520035530431<br/>For values of alpha = 10 The log loss is: 0.521097925307</span></pre><p id="5cfd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们看到的，对于α= 1，对数损失较小。</p><p id="1c8d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以我们选择阿尔法作为我们的<a class="ae nb" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">超参数</a>。</p><pre class="jw jx jy jz fd mr ms mt mu aw mv bi"><span id="d9d0" class="mb kb hi ms b fi mw mx l my mz">For values of best alpha = 1 The train log loss is: 0.513842874233<br/>For values of best alpha = 1 The test log loss is: 0.520035530431</span></pre><p id="3cd0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以看到，从0.88对数损失到0.52对数损失。</p><p id="2b9b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们将尝试线性支持向量机模型。它几乎表现类似，但损失是铰链线性SVM。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/8c3c3b818f306eac1e5feec9035c043b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HvPKpMXqpzL1slMXJKstCw.png"/></div></div></figure><pre class="jw jx jy jz fd mr ms mt mu aw mv bi"><span id="7e17" class="mb kb hi ms b fi mw mx l my mz">For values of alpha = 1e-05 The log loss is: 0.657611721261<br/>For values of alpha = 0.0001 The log loss is: 0.489669093534<br/>For values of alpha = 0.001 The log loss is: 0.521829068562<br/>For values of alpha = 0.01 The log loss is: 0.566295616914<br/>For values of alpha = 0.1 The log loss is: 0.599957866217<br/>For values of alpha = 1 The log loss is: 0.635059427016<br/>For values of alpha = 10 The log loss is: 0.654159467907</span></pre><p id="e178" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于α= 0.0001，我们得到最小对数损失。现在我们在这个aplha上建立模型。</p><p id="4a68" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们得到的结果是:</p><pre class="jw jx jy jz fd mr ms mt mu aw mv bi"><span id="74da" class="mb kb hi ms b fi mw mx l my mz">For values of best alpha = 0.0001 The train log loss is: 0.478054677285<br/>For values of best alpha = 0.0001 The test log loss is: 0.489669093534</span></pre><p id="d85e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们建立一些复杂的模型，比如GBDT。对于这些，我们将xgboost而不是sklearn，因为xgboost的实现比sklearn更好。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nd"><img src="../Images/6f57b1ef3cb9b1b764806a3a71f6c08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E8q8wpBL0B03SpCJk4WT-g.png"/></div></div></figure><p id="5cd4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随机模型— 0.88对数损失</p><p id="7575" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">逻辑回归— 0.52对数损失</p><p id="1449" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性SVM-0.489对数损耗</p><p id="92f5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">GBDT-0.357对数损失。</p><p id="a7f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">结论</p><ol class=""><li id="87ef" class="ld le hi is b it iu ix iy jb lf jf lg jj lh jn lv lj lk ll bi translated">我花了大部分时间分析和清理数据。当数据干净时，我们只需编写5-6行代码来尝试各种模型。</li><li id="773b" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated">通过检查性能指标或绘制ROC曲线，始终检查您的模型不会过拟合或欠拟合。</li></ol><p id="83fb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">参考链接:</p><ol class=""><li id="8dce" class="ld le hi is b it iu ix iy jb lf jf lg jj lh jn lv lj lk ll bi translated"><a class="ae nb" href="https://www.kaggle.com/c/quora-question-pairs" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/quora-question-pairs</a></li><li id="6f37" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated"><a class="ae nb" href="https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb/comments" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/anokas/data-analysis-xgboost-starter-0-35460-lb/comments</a></li><li id="827c" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated"><a class="ae nb" href="https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning" rel="noopener ugc nofollow" target="_blank">https://engineering . quora . com/Semantic-Question-Matching-with-Deep-Learning</a></li><li id="32a1" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn lv lj lk ll bi translated"><a class="ae nb" href="https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30" rel="noopener" target="_blank">https://towards data science . com/identifying-duplicate-questions-on-quora-top-12-on-ka ggle-4c 1 cf 93 f1 c 30</a>。</li></ol><p id="80f7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在Linkedin上联系我:</p><p id="ae75" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">https://www.linkedin.com/in/perul-jain-55845b154。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/a6ead93e0a81053a19adc9bc1ec0be41.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*8xyI0OdQwEyx0kEORcbXfA.jpeg"/></div></figure></div></div>    
</body>
</html>