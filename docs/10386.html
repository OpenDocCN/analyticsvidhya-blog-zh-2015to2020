<html>
<head>
<title>Paper Explained- Vision Transformers (Bye Bye Convolutions?)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文解释-视觉变形金刚(拜拜卷积？)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/vision-transformers-bye-bye-convolutions-e929d022e4ab?source=collection_archive---------1-----------------------#2020-10-17">https://medium.com/analytics-vidhya/vision-transformers-bye-bye-convolutions-e929d022e4ab?source=collection_archive---------1-----------------------#2020-10-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/dc367e887e12daf0578acb9592c1baa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQPLjibu2eq9P1RPezpZ4A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">模型概述。图片摘自<a class="ae iu" href="https://openreview.net/pdf?id=YicbFdNTTy" rel="noopener ugc nofollow" target="_blank">论文。</a></figcaption></figure><h1 id="d678" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">变形金刚对图像的限制</strong></h1><p id="07fb" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">变压器对于NLP来说工作得非常非常好，但是它们受到编码器模块中<strong class="jv hj">昂贵的二次注意力计算的内存和计算要求的限制。因此，图像对于变形金刚来说要困难得多，因为图像是像素的光栅，一个图像有许多许多像素。即使对于卷积神经网络，图像的光栅化本身也是一个问题。<strong class="jv hj">将一幅图像输入一个转换器</strong>每一个像素都必须关注其他每一个像素(就像注意力机制一样)，图像本身有255倍大，所以对一幅图像的注意力会让你付出255⁴的代价，这在当前的硬件中几乎是不可能的。因此，人们求助于其他技术，比如进行局部关注，甚至是全球关注。<strong class="jv hj">本文作者采用全球关注的改编方式。</strong></strong></p><h1 id="0964" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">视觉变压器架构</strong></h1><h2 id="982c" class="kr iw hi bd ix ks kt ku jb kv kw kx jf ke ky kz jj ki la lb jn km lc ld jr le bi translated"><strong class="ak">补丁嵌入</strong></h2><p id="f342" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">标准转换器接收作为1D令牌嵌入序列的输入。为了处理2D图像，我们将图像x∈R^{H×W×C}整形为一系列展平的2D片。</p><p id="0eab" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">其中，(H，W)是原始图像的分辨率，而(P，P)是每个图像块的分辨率。N = HW/P则为变压器的有效序列长度。图像被分割成固定大小的小块，在下图中，小块大小取为16×16。因此图像的尺寸将是48×48。</p><blockquote class="lk ll lm"><p id="7056" class="jt ju ln jv b jw lf jy jz ka lg kc kd lo lh kg kh lp li kk kl lq lj ko kp kq hb bi translated"><strong class="jv hj"> <em class="hi">注意:图像尺寸必须能被补丁尺寸整除。</em> </strong></p></blockquote><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/0d478a78c431a3d9b3b26f7c100b684c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L9kL-5knB9vQ772Azc8u7A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">重塑补丁嵌入的基本直觉</figcaption></figure><h2 id="e570" class="kr iw hi bd ix ks kt ku jb kv kw kx jf ke ky kz jj ki la lb jn km lc ld jr le bi translated"><strong class="ak">展平补片的线性投影</strong></h2><p id="a902" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在将补片传递到变压器模块之前，<a class="ae iu" href="https://openreview.net/pdf?id=YicbFdNTTy" rel="noopener ugc nofollow" target="_blank">论文的作者</a>发现首先将补片通过线性投影是有帮助的。所以有一个单一的矩阵，它被称为E，在这种情况下，“嵌入”，哈哈。他们将一个补丁展开成一个大向量，然后乘以嵌入矩阵，形成补丁嵌入，这就是与位置嵌入一起进入转换器的内容。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/1ae3b7fba0d5db7a425e1c2cc8d28620.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jd7iplC_C0xGufqPujnVuw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">编码器进给前线性投影块的直观性</figcaption></figure><h2 id="25ea" class="kr iw hi bd ix ks kt ku jb kv kw kx jf ke ky kz jj ki la lb jn km lc ld jr le bi translated"><strong class="ak">位置嵌入</strong></h2><p id="fef9" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">位置嵌入被添加到修补嵌入中，以保留位置信息。我们探索了位置嵌入的不同2D感知变体，而没有比标准的1D位置嵌入有任何显著的增益。<strong class="jv hj">关节嵌入作为变压器编码器的输入</strong>。</p><p id="324d" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">每个展开的面片(线性投影前)都有一个与之相关的数字序列，本文作者选择它为1，2，3，4…补丁数量。这些数字只不过是可学习的向量。每个向量都被参数化，并按行进行<strong class="jv hj">堆叠，以形成一个可学习的位置嵌入表。</strong></p><p id="2597" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">类似于BERT的<strong class="jv hj">【class】</strong>令牌，我们将一个可学习的嵌入添加到嵌入的补丁序列中，其在变换编码器(zₗ⁰)输出端的状态作为图像表示y。在预训练和微调期间，分类头都连接到zₗ⁰.</p><p id="e41b" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">最后，从表中提取与修补嵌入相关联的行号(最初的顺序号)(作为位置嵌入),连接，并馈送到变换器编码器块。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/d79b2647e528ca5042c14948badc7135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*appqGG9spAy-AD3cfSeDyg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">又名视觉模块，在编码器模块之前完成机械装置。</figcaption></figure><h2 id="e2dd" class="kr iw hi bd ix ks kt ku jb kv kw kx jf ke ky kz jj ki la lb jn km lc ld jr le bi translated"><strong class="ak">变压器编码器模块</strong></h2><p id="5200" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">变压器编码器由M个 <a class="ae iu" href="https://paperswithcode.com/method/graph-self-attention" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">多头自锁</strong> </a>和<strong class="jv hj"> MLP块</strong>的<strong class="jv hj">交替层组成。<a class="ae iu" href="https://paperswithcode.com/method/layer-normalization" rel="noopener ugc nofollow" target="_blank">图层规格化</a> ( <strong class="jv hj">图层规格化</strong>)在每个块之前应用，在每个块之后应用剩余连接。</strong></p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/a168fcd57648e72130480d0bfbf0ae87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*htfyQKxUXPJ0ZCasfmCggw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">编码器模块。图片摘自<a class="ae iu" href="https://openreview.net/pdf?id=YicbFdNTTy" rel="noopener ugc nofollow" target="_blank">论文。</a></figcaption></figure><h1 id="5e93" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">混合架构(类似的方法)</h1><p id="07be" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">作为将图像分成小块的替代方案，输入序列可以由ResNet的中间特征图形成。在这个混合模型中，面片嵌入投影E由ResNet的早期阶段代替。ResNet的一个中间2D特征映射被展平成一个序列，投影到Transformer维度，然后作为输入序列提供给Transformer。</p><h1 id="c81a" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">培训和微调</h1><p id="f07b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">作者使用β1 = 0.9，β2 = 0.999，批量大小为4096的<strong class="jv hj"> Adam </strong>训练所有模型，包括ResNets，并应用0.1的<strong class="jv hj">高权重衰减</strong>，他们发现这对于所有模型的转移都是有用的。作者使用了线性学习率——热身和衰退。为了进行微调，作者对所有模型使用了<strong class="jv hj">带动量</strong>的SGD，批量为512。</p><h1 id="d2ff" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">多层感知器头</h1><p id="95b2" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">输出端的全连接MLP头提供所需的类别预测。主模型可以在大型图像数据集上进行预训练，然后最终的MLP头部可以通过标准的迁移学习方法针对特定的任务进行微调。<strong class="jv hj">【MLP】包含两层带</strong> <a class="ae iu" href="https://paperswithcode.com/method/gelu" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">葛鲁非线性</strong> </a> <strong class="jv hj">。</strong></p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/f12fd1af22f8a3f1725764d94a5bda94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RA2JebIlklQF_P4n-ZMJzA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">PyTorch的精确实现足够快，因此这些近似可能是不必要的。图片摘自<a class="ae iu" href="https://paperswithcode.com/method/gelu" rel="noopener ugc nofollow" target="_blank">带代码的论文。</a></figcaption></figure><h1 id="5619" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">与SOTA的比较</h1><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/80ebd67798671c964d1e411cc4b1ef3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2b8dbAv7kd1-Dlp_IfUwyw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">VTAB在自然、专业和结构化任务组中的表现细分。图片摘自<a class="ae iu" href="https://openreview.net/pdf?id=YicbFdNTTy" rel="noopener ugc nofollow" target="_blank">论文。</a></figcaption></figure><p id="afcd" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated"><strong class="jv hj">如果你喜欢这篇文章并获得了真知灼见，可以考虑</strong> <a class="ae iu" href="https://www.buymeacoffee.com/nakshatrasinghh" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">请我喝杯咖啡</strong> ☕️ <strong class="jv hj">点击这里</strong> </a> <strong class="jv hj"> :) </strong></p><h1 id="0de4" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考</h1><ol class=""><li id="0821" class="mb mc hi jv b jw jx ka kb ke md ki me km mf kq mg mh mi mj bi translated"><a class="ae iu" href="https://openreview.net/pdf?id=YicbFdNTTy" rel="noopener ugc nofollow" target="_blank">一幅图像相当于16x16个字:大规模图像识别的变形金刚</a>，ICLR 2021。</li><li id="86cc" class="mb mc hi jv b jw mk ka ml ke mm ki mn km mo kq mg mh mi mj bi translated"><a class="ae iu" href="https://arxiv.org/pdf/2006.03677.pdf" rel="noopener ugc nofollow" target="_blank">视觉变形金刚</a>。</li><li id="e1da" class="mb mc hi jv b jw mk ka ml ke mm ki mn km mo kq mg mh mi mj bi translated">你所需要的只是注意力。</li></ol><p id="fc28" class="pw-post-body-paragraph jt ju hi jv b jw lf jy jz ka lg kc kd ke lh kg kh ki li kk kl km lj ko kp kq hb bi translated">如果你喜欢这个帖子，请一定要鼓掌👏。💬连接？让我们来看看社会:<a class="ae iu" href="http://myurls.co/nakshatrasinghh" rel="noopener ugc nofollow" target="_blank"><strong class="jv hj">http://myurls.co/nakshatrasinghh</strong></a><strong class="jv hj">。</strong></p></div></div>    
</body>
</html>