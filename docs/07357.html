<html>
<head>
<title>How to choose the size of the convolution filter or Kernel size for CNN?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CNN如何选择卷积滤波器的大小或核的大小？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-choose-the-size-of-the-convolution-filter-or-kernel-size-for-cnn-86a55a1e2d15?source=collection_archive---------0-----------------------#2020-06-23">https://medium.com/analytics-vidhya/how-to-choose-the-size-of-the-convolution-filter-or-kernel-size-for-cnn-86a55a1e2d15?source=collection_archive---------0-----------------------#2020-06-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/9460ff2db57fc4cfa47627a2fd249860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5DE2XPlqbKf0KPUemVWxEA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">深度学习的基础设施</figcaption></figure><p id="58db" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">卷积基本上是相同大小的图像(局部感受野)的核(或滤波器)和补丁的点积。卷积非常类似于相关性，并表现出平移等变的特性，这意味着如果我们移动或平移输入并对其应用卷积，它将与我们首先应用卷积然后平移图像的方式相同。</p><p id="e0c8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在CNN的学习过程中，你会在代码的不同位置发现不同的核大小，那么这个问题就会出现在你的脑海中<em class="js">是否有一个特定的方法来选择这样的维度或大小</em>。所以，答案是否定的。在当前的深度学习世界中，我们正在使用每个深度学习实践者都在使用的最受欢迎的选择，那就是3x3内核大小。现在，另一个问题出现在你的脑海中，为什么只有3x3，而不是1x1，2x2，4x4等等。继续读下去，你会在接下来的几分钟内找到这背后最清晰的原因！！</p><p id="ab41" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">基本上，我们将内核大小分为较小的和较大的。较小的内核大小由1x1、2x2、3x3和4x4组成，而较大的内核大小由5x5组成，以此类推，但是我们使用直到5x5来进行2D卷积。2012年，当<strong class="iw hj"> AlexNet </strong> CNN架构推出时，它使用了11x11，5x5这样更大的内核大小，花费了两到三周的训练时间。因此，由于训练时间过长和昂贵，我们不再使用如此大的内核。</p><p id="7430" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">与全连接网络相比，更喜欢小内核大小的原因之一是，它降低了计算成本和权重共享，最终导致反向传播的权重更小。因此，2015年出现了<strong class="iw hj"> VGG </strong>卷积神经网络，它用<strong class="iw hj"> 3x3 </strong>卷积层取代了如此大的卷积层，但使用了许多滤波器。从那以后，3x3大小的内核成为一种流行的选择。但是，<strong class="iw hj"> <em class="js">为什么不把1x1，2x2或者4x4作为更小的内核呢？</em>T11】</strong></p><ol class=""><li id="647d" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated"><strong class="iw hj"> 1x1 </strong>核大小仅用于旨在减少通道数量的降维。它仅在特征图的一个像素中捕获输入通道的交互。因此，消除了1x1，因为提取的特征将是细粒度的和局部的，也没有来自相邻像素的信息。</li><li id="34b4" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr jy jz ka kb bi translated"><strong class="iw hj"> 2x2 </strong>和<strong class="iw hj"> 4x4 </strong>通常<strong class="iw hj">不是优选的</strong>，因为奇数大小的过滤器对称地划分输出像素周围的前一层像素。如果这种对称性不存在，那么在使用偶数大小的内核(即2x2和4x4)时，各层之间会发生失真。所以，这就是我们不使用2x2和4x4内核大小的原因。</li></ol><p id="60a6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，<strong class="iw hj"> <em class="js"> 3x3是迄今为止修行者遵循的最优选择</em> </strong>。但它仍然是最昂贵的部分！</p><p id="4c73" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">奖励:</strong>进一步挖掘，我发现了另一种<strong class="iw hj">有趣的方法</strong>，这种方法在<em class="js"> Google </em>在<em class="js"> ImageNet识别挑战</em>期间推出的<strong class="iw hj"> Inception V3 CNN架构</strong>中使用，即<strong class="iw hj">用1x3层替换3x3卷积层，然后是3x1卷积层</strong>，这实际上是将3x3卷积分解成一系列一维卷积层。而且出来还挺划算的！！</p><p id="e645" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">谢谢你阅读它。我发现这是深度学习新手(包括我在内)最常提出的问题。；)，因为使用特定内核大小背后的清晰明了的原因在大多数学习课程中都没有涉及。这是我第一篇关于媒体的文章，所以如果你喜欢，别忘了鼓掌！！祝您愉快！！</p></div></div>    
</body>
</html>