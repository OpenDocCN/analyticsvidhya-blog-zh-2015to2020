<html>
<head>
<title>Explained: Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释:递归神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/explained-recurrent-neural-networks-2832ca147700?source=collection_archive---------35-----------------------#2020-04-06">https://medium.com/analytics-vidhya/explained-recurrent-neural-networks-2832ca147700?source=collection_archive---------35-----------------------#2020-04-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ce1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">递归神经网络是专门为序列形式的可用数据设计的专用神经网络。序列数据的几个例子可以是文本数据，如推文或评论、股票的每日收盘价、传感器读数等。在本文中，我们将主要关注作为RNNs输入的文本数据。在我们开始研究RNNs的技术细节之前，让我们先讨论一下为什么标准的神经网络不能处理序列数据。</p><h2 id="af71" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">为什么是RNNs？</strong></h2><p id="55aa" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">假设我们的目标是名称实体识别问题，并希望在给定的句子集中检测人名。如果我们使用一个标准的神经网络来解决这个问题，我们的架构可能看起来像下面这样。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/0f58f264bbd952165b88e29870728a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6fypUk9zCtq-rDOdfm6WmQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated"><em class="kt">NER标准神经网络</em></figcaption></figure><p id="eedf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们注意到这种架构的第一个问题是，我们固定了输入和输出的长度。因此，如果我们在测试数据中遇到一个比这个固定长度更长的句子，我们可能不得不截断它。同样，所有短于这个固定长度的句子都必须被填充。这并不理想，可能会导致意想不到的结果。关于输出，虽然在这种情况下，我们可能需要固定长度的输出，但在许多情况下，输出的长度是未知的。例如在语言翻译的情况下。这种情况不能由标准的神经网络架构来处理。</p><p id="0c97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二个问题是，在这种架构中学习到的特性不会在文本的不同位置之间共享。特别是，如果网络知道出现在句子第一个位置的‘Harry’是一个人名。然后，在预测时，如果“Harry”和类似的词出现在第一个位置，它可以将它们标记为人名。但是，如果“哈利”出现在句子的第三个位置，就不会出现同样的情况。这是使用这种架构的主要缺点。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/318b0bdebe1fb47faf424828017a4eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Ry1izmsucRXR__AJ3stRA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">用于文本处理的标准神经网络的输入大小计算</figcaption></figure><p id="ee0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个问题是输入层的大小。在上面的例子中，如果我们对输入单词使用独热向量，并且序列长度为100，那么输入大小将是用于生成独热向量的词汇大小的100倍。词汇量通常从10K开始，最高可达50万。这种表示将导致输入大小从1M到50M，这在任何情况下都是非常大的。即使我们使用单词嵌入，我们仍然会以序列长度乘以嵌入长度作为输入层的大小。这可以通过RNNs显著降低。</p><p id="b1dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">记住上面强调的问题，我们现在将讨论rnn，它有望以更好的方式处理序列数据。</p><h2 id="880c" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">RNNs基础</strong></h2><p id="3fb5" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在递归神经网络中，每个单词都经过一个隐藏层。这个隐藏层为每个单词产生两个向量——一个输出向量和一个激活向量。来自前一个单词的激活向量与当前单词结合使用。这两者结合起来，产生当前单词的输出向量和将与下一个单词一起使用的激活向量。这个过程一直持续到序列结束。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/b2227361510e2a2cde6bf8dd8709aa78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JhL58UYgnMXeXBkLhjPFQg.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated"><em class="kt">递归神经网络架构</em></figcaption></figure><p id="b2df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">递归神经网络从左到右扫描数据，并且参数是共享的，即相同的参数集用于序列的每个单词。因此，上面的RNN在对特定单词<em class="ku">、</em>进行预测时，不仅从该单词中获得信息，还从激活向量传递给它的先前单词中获得信息。</p><p id="3180" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用以下等式计算每个单词的激活向量和输出向量:</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kv"><img src="../Images/e95ef596109100874c051e13cea23281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_PrzjVfYAUyPbMowd3uatg.png"/></div></div></figure><p id="56c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上方程中，<em class="ku"> ga </em>和<em class="ku"> gy </em>为激活函数如tanh、ReLu和Sigmoid。对于激活向量<em class="ku">和</em>，通常使用的激活函数有tanh <em class="ku"> </em>和ReLu <em class="ku"> </em>两种，前者是最常用的选择。对于输出向量<em class="ku"> yn，</em>激活函数取决于所需的输出类型，例如，我们可以使用Sigmoid处理二分类问题，使用Softmax处理多分类问题。</p><p id="8d18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">基本RNN结构的局限性</strong></p><p id="1c3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述基本RNN结构的一个缺点是它遇到了消失梯度的问题。你可以在这里阅读关于渐变消失问题<a class="ae kw" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">的详细内容。简而言之，在非常深的神经网络中，来自后面层的输出的梯度可能很难传播回来以改变前面层的权重。</a></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kx"><img src="../Images/2018c2bf437ea4110acd3a59aa887f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y19ZcilW_7NXZDBzma0z5w.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated"><em class="kt">开头词影响后面词的句子</em></figcaption></figure><p id="ce3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们正在通过一个具有如上所示基本结构的RNN传递一个很长的句子。这个练习的目标是预测序列中的下一个单词。第一个输入将是单词“男孩”,在这个阶段的预期输出将是单词“谁”。类似地，在下一步，输入将是“who”以及上一步的激活向量，预期输出将是“got”。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/0dac4227ae39dcabc244debaf2115d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZVUg0WCuKZt9f4Vz7Sb3bQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated"><em class="kt"> RNN处理开头词影响后面词的句子</em></figcaption></figure><p id="75fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们到达预期输出为“was”的步骤时，我们将需要上一步的激活向量来包含名词是单数还是复数的信息。这一信息在句子的开头就被捕捉到了。因此，在该步骤的反向传播期间，来自输出的梯度将想要以这样的方式更新<em class="ku"> Wa </em>和<em class="ku"> Wx </em>，使得在处理“男孩”时可以捕获该信息。但是，由于这两个词之间有一个很长的间隔，梯度可能会消失，可能不会按要求更新参数。因此，遇到了渐变消失的问题。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/e22d68b5983985dfca6c61c9c1ff5d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmipcI9TbtPt3UnBx-lH5A.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated"><em class="kt">识别句子中的人名。“泰迪”仅在第二句中用作人名</em></figcaption></figure><p id="ba7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种基本结构的另一个问题是序列是从左到右处理的。在处理一个单词时，我们只能使用这个单词所传达的信息。然而，该单词右侧的信息可能对其处理至关重要，并可能影响该阶段的输出，如上图所示。</p><p id="2066" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将在本系列的下一篇博客中解决这些问题。具体来说，我们将讨论LSTMs、双向rnn和深度rnn。</p></div></div>    
</body>
</html>