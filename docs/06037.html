<html>
<head>
<title>Feedforward Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">前馈神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feedforward-neural-networks-13d26650e8c0?source=collection_archive---------17-----------------------#2020-05-09">https://medium.com/analytics-vidhya/feedforward-neural-networks-13d26650e8c0?source=collection_archive---------17-----------------------#2020-05-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/48d9f1885961bed0d2b3c4d0dae5381f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CVe8Vhdt3Ij1V-f8MXwPXw.jpeg"/></div></div></figure><p id="4318" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">引用说明——本文中的一些内容和例子摘自四分之一实验室的深度学习课程——pad hai</em></p><p id="ca03" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">前馈神经网络(也称为FNN神经网络)是一种简单类型的神经网络，其中数据从输入到输出仅沿一个方向流动，中间有一个或多个隐藏层。前一个隐藏层的输出作为前馈神经网络中下一层的输入。每层由一个或多个sigmoid神经元组成(可以是任何非线性函数，如tanh、ReLU、leaky ReLU等..在本文中，我们将考虑sigmoid函数，它构成了这种神经网络的构建模块。简单地说，FNN借助于位于多层的多个sigmoid神经元来确定输入‘x’和输出‘y’之间的关系，<em class="jo"> y = f(x) </em>。(层数和每层乙状结肠的数量可以变化，并被视为超参数，本文将不解释调整它们的方法)</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jp"><img src="../Images/92dde12201dd6afc67b9ef98d9b09dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LaEgAU-vdsR_pClMcgbikQ.jpeg"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图一。前馈神经网络</figcaption></figure><p id="fbc4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">好的。让我们从基础开始，了解两件事。</p><ol class=""><li id="aea3" class="jy jz hi is b it iu ix iy jb ka jf kb jj kc jn kd ke kf kg bi translated">需要深度神经网络/ FNN氏症</li><li id="0b92" class="jy jz hi is b it kh ix ki jb kj jf kk jj kl jn kd ke kf kg bi translated">什么是乙状结肠神经元，它与深度神经网络有什么关系。</li></ol><p id="8a43" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一般来说，任何机器学习模型都会学习最能确定输入x和输出y之间关系的函数<em class="jo"> f </em>。不幸的是，这种关系在大多数情况下是非线性的，因此我们需要复杂的数学函数来计算x和y之间的关系</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/82e4a2f17e4ca37da843fe017477530e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jp0W2Rlc361_csSAL-mkUQ@2x.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图二。线性可分数据</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kn"><img src="../Images/1c04f9982f061d5d1cc7a0bc2ab5ca27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MUFhpUcwA8o_JCq680bpZQ.jpeg"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图三。线性不可分数据</figcaption></figure><p id="191f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于人类几乎不可能尝试不同的数学运算并找出它们之间的关系，因此需要一个通用的范例来确定它们之间的关系。这就是深度神经网络的用武之地，它们被用来对一组输入和输出之间的几乎任何复杂关系进行建模。神经网络是一种模拟人脑活动的技术。(如需进一步阅读，请查看人类大脑中的视觉皮层是如何工作的——<a class="ae ko" href="https://www.neuroscientificallychallenged.com/blog/know-your-brain-primary-visual-cortex" rel="noopener ugc nofollow" target="_blank">https://www . neuroscifically challenged . com/blog/know-your-brain-primary-visual-cortex</a>)</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/75f676e1f7f008c78bf5580588c72536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*mzWqY5snL5pYrKo7r5-HZg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图4。乙状结肠神经元与脑神经元的比较</figcaption></figure><p id="161d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们了解一下，乙状结肠神经元是如何工作的。可用于解决二进制和多类分类问题的sigmoid神经元使用以下函数来模拟输入-输出关系。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/6abb4d2058a9531ad12f66349e62f501.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*KoAdBBACAKZ19QIVsIzIZA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">Sigmoid方程</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/7bb2a17698df77c6177f71ddd6742b8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:252/1*ldyxhrpt6yuz1kSyEiDpzA.gif"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图五。乙状结肠神经元</figcaption></figure><p id="7417" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<em class="jo"> x </em>为输入，<em class="jo"> p </em>为预测输出。一个sigmoid给出一个如下图所示的图形。可以看出，sigmoid不具有急剧的过渡(如在直线/平面中),并且在sigmoid方程中可以有两个我们可以控制的参数，<em class="jo"> w </em>和<em class="jo"> b </em>,它们分别控制陡度和过渡开始的点。对于给定的训练数据集，有几种学习算法可以帮助您计算出<em class="jo"> w </em>和<em class="jo"> b </em>的最佳值。(我不会深入讨论超参数，如学习率和迭代次数以及调整它们的方法)。在这一点上，您需要理解的是，单个sigmoid会给出一条类似于下图中的曲线，您可以使用该曲线来分离输出类。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/d7025d160f64e8637f516c7cb6687423.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*BwZmaMBaVW8f8lBAQUoytw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图六。二维sigmoid图</figcaption></figure><p id="c60c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，FNN在不同的层上组合了多个这样的s形曲线，因此通过连接多个这样的s形曲线，你几乎可以找出输入和输出之间的任何复杂关系。<a class="ae ko" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">通用近似定理</a>给出了多个sigmoids如何导致几乎任何关系的详细证明</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kt"><img src="../Images/bd9da1a70aec85ac13ee2301bda8d432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7HcxLdlIFFu0hzFgFDhyaA.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图7。FNN的各种层和sigmoids导致多个复杂的功能</figcaption></figure><p id="b462" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们做一些数学。FNN中的每个神经元包括两个功能，由<em class="jo"> a </em>表示的预激活功能和由<em class="jo"> h </em>表示的激活功能。让我们考虑一个神经网络，它有一个输入层(<em class="jo">有输入，x1，x2，x3 </em>)，一个输出层(下面用绿色标出)，2个隐藏层，每个隐藏层中有3个sigmoids。有3个输入<em class="jo"> x1、x2和x3 </em>，并且每一层将具有m*n个权重向量，其中m是来自前一层的输入的数量，n是当前层中神经元的数量(对于第一层，它将是9，因为在层1中有3个输入值和3个神经元)和1个大小为n的偏置向量</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/368782f26bf623fd2172c85f6cae658e.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*1Evi_lY9Icy40VcuBWqsvQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图7。深度神经网络</figcaption></figure><p id="5833" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们从第一层的第一个神经元开始(预激活为<em class="jo"> a1 </em>，激活为<em class="jo"> h1 </em>)。预激活函数是偏置和所有输入的总和乘以层中每个神经元各自的权重。因此，第一隐藏层中的第一神经元的激活函数将是</p><p id="cc70" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">a11 = w111 * x1+w121 * x2+w131 * x3+b11</em>其中，</p><p id="3faf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> a11 </em> =第一层第1个神经元的预激活功能，</p><p id="be5a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> w111 </em> =第一层(输入层)的权重，连接第一个输入和隐藏层中的第一个神经元，</p><p id="4844" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通常，权重表示为<em class="jo"> w(ijk) </em>其中，</p><p id="0bb3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> i </em>代表前一层编号(1代表输入层)，以及</p><p id="7bef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> j </em>表示当前层中的目标神经元数量</p><p id="dcbf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> k </em>代表前一层的源神经元/输入数。</p><p id="7dad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> b11 </em> =连接第一个神经元的第一层的偏置分量</p><p id="3b63" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">类似地，第二隐藏层中第一个神经元的预激活函数(预激活为<em class="jo"> a2 </em>，激活为<em class="jo"> h2 </em>)将为</p><p id="8153" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">a21 = w211 * x1+w221 * x2+w231 * x3+b21</em>(此处所有权重用w2表示..因为它们都是连接到第二层的权重)</p><p id="965e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们继续计算激活函数。激活函数可以是任何连续的非线性函数(它应该在所有点上都是可微分的，因为我们将计算导数来调整每次迭代的权重和偏差)。在一般的激活函数中，<em class="jo"> h(ij) </em>可以表示为:</p><p id="5a0c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> h(ij) = f(a (ij)) </em>其中，</p><p id="a969" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> i </em>表示层数，<em class="jo"> j </em>表示该层的神经元数。</p><p id="e113" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们认为函数<em class="jo"> f </em>是一个sigmoid函数，</p><p id="959b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">h(ij)= 1/(1+e ^-(a(ij))】</em></p><p id="4ae2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在概率模型的情况下，输出层可以使用<a class="ae ko" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax </a>函数作为激活函数</p><p id="f17b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用上述函数，我们将能够构建一个从输入层到输出层的完整的正向传递。在每次向前传递后，我们需要理解损失值(通过使用交叉熵或均方误差损失)并调整权重和偏差，以确保下一次迭代中的损失小于当前值。计算权重和偏差调整最常用的算法之一是梯度下降，我会在另一篇文章中解释，因为它太复杂了:)</p><p id="3b25" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">与前馈神经网络相关的进一步阅读主题</p><ol class=""><li id="31f4" class="jy jz hi is b it iu ix iy jb ka jf kb jj kc jn kd ke kf kg bi translated">梯度下降算法及其变体(随机、批量、小批量)。</li><li id="e485" class="jy jz hi is b it kh ix ki jb kj jf kk jj kl jn kd ke kf kg bi translated">深度学习中的优化算法</li></ol></div></div>    
</body>
</html>