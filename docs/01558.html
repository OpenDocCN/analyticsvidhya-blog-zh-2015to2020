<html>
<head>
<title>Machine learning at Scale using Pyspark &amp; deployment using AzureML/Flask</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pyspark进行大规模机器学习，使用AzureML/Flask进行部署</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-at-scale-using-pyspark-deployement-using-flask-3bad70e97165?source=collection_archive---------6-----------------------#2019-10-31">https://medium.com/analytics-vidhya/machine-learning-at-scale-using-pyspark-deployement-using-flask-3bad70e97165?source=collection_archive---------6-----------------------#2019-10-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/87539fd841285f2fb9a2533f4fcfd2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zz51K584W6q0EMP9"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@codestorm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">萨法尔·萨法罗夫</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="6607" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">大家好，在过去的几个月里，我一直在研究可扩展性&amp;生产机器学习算法。我在网上搜索了很多，得到的支持很少。公司仍在努力在这一领域获得更高的成功率。根据调查，只有4%的ML模型用于部署和生产环境，这是因为社区对此的支持较少。让我们从今天的话题开始，为社区做一点贡献。</p><h1 id="2979" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">在我们继续之前，需要了解一些事情:- </strong></h1><ol class=""><li id="8195" class="kr ks hi ix b iy kt jc ku jg kv jk kw jo kx js ky kz la lb bi translated">为什么选择Pyspark和databricks笔记本？</li><li id="949c" class="kr ks hi ix b iy lc jc ld jg le jk lf jo lg js ky kz la lb bi translated">什么是烧瓶，什么是替代品？</li></ol><p id="0fe5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我用过pyspark和Databricks notebook，因为它很好地定义了显示spark数据帧和图形的功能。Databricks还提供集群设置，因此我们可以在集群中使用任何机器配置来获得更高的计算能力。在我们的例子中，我使用了(3台机器42GB内存)。您可以查看【这里】(<a class="ae iu" href="https://databricks.com/" rel="noopener ugc nofollow" target="_blank">https://databricks.com/</a>)了解更多信息和使用方法。</p><p id="6e3a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里使用Flask进行部署。Flask是一个用于python的web服务器，它帮助创建公开服务请求的端点的服务器。这不是强制性的，我们必须去与烧瓶，有许多替代品目前在市场上。稍后我们将讨论更多关于这一点以及如何使用etc..更多信息请查看(this)[<a class="ae iu" href="http://flask.pocoo.org/" rel="noopener ugc nofollow" target="_blank">http://flask.pocoo.org/</a></p><p id="f323" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我选择非常基本的数据集，泰坦尼克号数据集在这里使用，但任何一个可以玩其他数据集。这里执行的一些基本预处理操作，如特征提取、插补、删除等..我将以函数的方式展示每一步。</p><p id="a6ae" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">步骤1 :-导入所有重要的库</strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="ce56" class="lq ju hi lm b fi lr ls l lt lu"><em class="lv">"""</em><br/><em class="lv">Loading important package of spark </em><br/><em class="lv">"""</em><br/><strong class="lm hj">from</strong> <strong class="lm hj">pyspark.sql</strong> <strong class="lm hj">import</strong> SparkSession<br/><strong class="lm hj">from</strong> <strong class="lm hj">pyspark.ml</strong> <strong class="lm hj">import</strong> Pipeline,PipelineModel<br/><strong class="lm hj">from</strong> <strong class="lm hj">pyspark.sql.functions</strong> <strong class="lm hj">import</strong> *<br/><strong class="lm hj">from</strong> <strong class="lm hj">pyspark.ml.pipeline</strong> <strong class="lm hj">import</strong> Transformer,Estimator<br/><strong class="lm hj">from</strong> <strong class="lm hj">pyspark.ml.feature</strong> <strong class="lm hj">import</strong> StringIndexer,VectorAssembler<br/><strong class="lm hj">from</strong> <strong class="lm hj">pyspark.ml.classification</strong> <strong class="lm hj">import</strong> LogisticRegression<br/><strong class="lm hj">from</strong> <strong class="lm hj">pyspark.ml.tuning</strong> <strong class="lm hj">import</strong> CrossValidator, ParamGridBuilder</span></pre><p id="7f6b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们用的是<code class="du lw lx ly lm b">pipeline</code>，基本上工作在类似顺序操作的阶段。在后面的代码中，你会看到我是如何为<code class="du lw lx ly lm b">StringIndexer</code>、<code class="du lw lx ly lm b">VectorAssembler</code>和<code class="du lw lx ly lm b">algorithm</code>使用管道的</p><p id="07c5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其他进口库请在spark官网查询。</p><p id="4489" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">步骤2:创建Spark会话</strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="e605" class="lq ju hi lm b fi lr ls l lt lu"><em class="lv">"""</em><br/><em class="lv">Spark session creater </em><br/><em class="lv">"""</em><br/><br/>st = SparkSession \<br/>        .builder \<br/>        .appName('Titanic') \<br/>        .getOrCreate()</span></pre><p id="b20b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您也可以为这个会话设置许多自定义内存选项，为了简单起见，我使用默认配置。</p><p id="1284" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">步骤3在Spark数据帧中加载数据集</strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="12ef" class="lq ju hi lm b fi lr ls l lt lu"><em class="lv">"""</em><br/><em class="lv">Load data function for loading data..</em><br/><em class="lv">@param - </em><br/><em class="lv">        path - path of file</em><br/><em class="lv">        header_value - header value, incase true first row will be header</em><br/><em class="lv">        </em><br/><em class="lv">@return - dataframe of loaded intended data.</em><br/><em class="lv">"""</em><br/><br/><strong class="lm hj">def</strong> load_data(path,header_value):<br/>  df = st.read.csv(path,inferSchema=<strong class="lm hj">True</strong>,header=header_value)<br/>  <strong class="lm hj">return</strong> df</span><span id="6c01" class="lq ju hi lm b fi lz ls l lt lu">df = load_data('/FileStore/tables/titanic_train.csv',<strong class="lm hj">True</strong>) <br/>df_test = load_data('/FileStore/tables/titanic_test.csv',<strong class="lm hj">True</strong>)</span></pre><p id="9285" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">加载数据文件，在这种情况下，训练和测试数据文件是分开的。为了方便起见，我创建了一个函数，这样每次想要加载数据时它都会调用函数<code class="du lw lx ly lm b">load_data</code>。</p><p id="2592" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">步骤4为预处理数据创建一个定制的转换器</strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="00b3" class="lq ju hi lm b fi lr ls l lt lu"><em class="lv">'''</em><br/><em class="lv">Custom Transformer class for tranformation implementation .</em><br/><br/><em class="lv">@param - </em><br/><em class="lv">       Transformer - Transformer class refrence </em><br/><em class="lv">       df - dataframe in which operation need to be carried ( passed through tranform function)</em><br/><em class="lv">       A - A class for variable sharing.</em><br/><br/><em class="lv">@return -</em><br/><em class="lv">       df - a dataframe which contains prediction value as well with featured value. </em><br/><br/><em class="lv">'''</em><br/><br/><strong class="lm hj">class preprocess_transform</strong>(Transformer):<br/>  <br/>    <strong class="lm hj">def</strong> _transform(self,df):<br/>      print("********************************  in Transform method ...************************************")<br/>      <br/>      <br/>      <em class="lv">"""</em><br/><em class="lv">      Generate feature column in dataframe based on specific logic</em><br/><br/><em class="lv">      @param - </em><br/><em class="lv">               df - dataframe for operation.</em><br/><br/><em class="lv">      @return - </em><br/><em class="lv">               df - dataframe with generated feature.</em><br/><em class="lv">      """</em><br/>      <br/>      <br/>      <strong class="lm hj">def</strong> feature_generation(self,df):<br/>        df = df.withColumn("Initial",regexp_extract(col("Name"),"([A-Za-z]+)\.",1))<br/>        df = df.replace(['Mlle','Mme', 'Ms', 'Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],<br/>                        ['Miss','Miss','Miss','Mr','Mr',  'Mrs',  'Mrs',  'Other',  'Other','Other','Mr','Mr','Mr'])<br/>        df = df.withColumn("Family_Size",col('SibSp')+col('Parch'))<br/>        df = df.withColumn('Alone',lit(0))<br/>        df = df.withColumn("Alone",when(df["Family_Size"] ==0, 1).otherwise(df["Alone"]))<br/>        <strong class="lm hj">return</strong> df<br/><br/><br/>      <em class="lv">"""</em><br/><em class="lv">      Impute Age based on Age mean of specific gender. ex for male mean is 46 update all null male row with 46, similarly for others</em><br/><br/><em class="lv">      @param - </em><br/><em class="lv">              df - dataframe for operation</em><br/><br/><em class="lv">      @return -</em><br/><em class="lv">             df - with imputed value</em><br/><br/><em class="lv">      """</em><br/>  <br/>      <strong class="lm hj">def</strong> Age_impute(self,df):<br/>        Age_mean = df.groupBy("Initial").avg('Age')<br/>        Age_mean = Age_mean.withColumnRenamed('avg(Age)','mean_age')<br/>        Initials_list = Age_mean.select("Initial").rdd.flatMap(<strong class="lm hj">lambda</strong> x: x).collect()<br/>        Mean_list = Age_mean.select("mean_age").rdd.flatMap(<strong class="lm hj">lambda</strong> x: x).collect()<br/>        <strong class="lm hj">for</strong> i,j <strong class="lm hj">in</strong> zip(Initials_list,Mean_list):<br/>            df = df.withColumn("Age",when((df["Initial"] == i) &amp; (df["Age"].isNull()), j).otherwise(df["Age"]))<br/><br/>        <strong class="lm hj">return</strong> df<br/>        <br/>        <br/>      <em class="lv">"""</em><br/><em class="lv">      Impute Embark based on mode of embark column</em><br/><em class="lv">      @param - </em><br/><em class="lv">              df - dataframe for operation</em><br/><br/><em class="lv">      @return -</em><br/><em class="lv">             df - with imputed value</em><br/><br/><em class="lv">      """</em><br/>      <strong class="lm hj">def</strong> Embark_impute(self,df):<br/>        mode_value = df.groupBy('Embarked').count().sort(col('count').desc()).collect()[0][0]<br/>        df = df.fillna({'Embarked':mode_value})<br/>        <strong class="lm hj">return</strong> df<br/>      <br/>      <br/>      <em class="lv">"""</em><br/><em class="lv">      Impute Fare based on the class which he/she had sat ex: class 3rd has mean fare 9 and null fare belong to 3rd class so fill 9</em><br/><em class="lv">      @param - </em><br/><em class="lv">              df - dataframe for operation</em><br/><br/><em class="lv">      @return -</em><br/><em class="lv">             df - with imputed value</em><br/><br/><em class="lv">      """</em><br/>      <strong class="lm hj">def</strong> Fare_impute(self,df):<br/>        Select_pclass = df.filter(col('Fare').isNull()).select('Pclass')<br/>        <strong class="lm hj">if</strong> Select_pclass.count() &gt; 0:<br/>          Pclass = Select_pclass.rdd.flatMap(<strong class="lm hj">lambda</strong> x: x).collect()<br/>          <strong class="lm hj">for</strong> i <strong class="lm hj">in</strong> Pclass:<br/>            mean_pclass_fare = df.groupBy('Pclass').mean().select('Pclass','avg(Fare)').filter(col('Pclass')== i).collect()[0][1]<br/>            df = df.withColumn("Fare",when((col('Fare').isNull()) &amp; (col('Pclass') == i),mean_pclass_fare).otherwise(col('Fare')))<br/>        <strong class="lm hj">return</strong> df<br/>      <br/>      <br/>      <em class="lv">'''</em><br/><em class="lv">      combining all column imputation together..</em><br/><br/><em class="lv">      @param - </em><br/><em class="lv">            df - a dataframe for operation.</em><br/><br/><em class="lv">      @return - </em><br/><em class="lv">            df - dataframe with imputed value.</em><br/><br/><em class="lv">      '''</em><br/>      <strong class="lm hj">def</strong> all_impute_together(df):<br/>        df = Age_impute(self,df)<br/>        df = Embark_impute(self,df)<br/>        df = Fare_impute(self,df)<br/>        <strong class="lm hj">return</strong> df<br/>      <br/>      <br/>      <em class="lv">'''</em><br/><em class="lv">      converting string to numeric values.</em><br/><br/><em class="lv">      @param - </em><br/><em class="lv">               df - dataframe contained all columns.</em><br/><em class="lv">               col_list - list of column need to be </em><br/><br/><em class="lv">      @return - </em><br/><em class="lv">              df - transformed dataframe.</em><br/><em class="lv">      '''</em><br/>      <strong class="lm hj">def</strong> stringToNumeric_conv(df,col_list):<br/>        indexer = [StringIndexer(inputCol=column,outputCol=column+"_index").fit(df) <strong class="lm hj">for</strong> column <strong class="lm hj">in</strong> col_list]<br/>        string_change_pipeline = Pipeline(stages=indexer)<br/>        df = string_change_pipeline.fit(df).transform(df)<br/>        <strong class="lm hj">return</strong> df<br/><br/>      <br/>      <em class="lv">"""</em><br/><em class="lv">      Drop column from dataframe</em><br/><em class="lv">      @param -</em><br/><em class="lv">             df - dataframe </em><br/><em class="lv">             col_name - name of column which need to be dropped.</em><br/><em class="lv">      @return -</em><br/><em class="lv">             df - a dataframe except dropped column</em><br/><em class="lv">      """</em><br/>      <strong class="lm hj">def</strong> drop_column(df,col_list):<br/>        <strong class="lm hj">for</strong> i <strong class="lm hj">in</strong> col_list:<br/>            df = df.drop(col(i))<br/>        <strong class="lm hj">return</strong> df<br/>      <br/>      <br/>      col_list = ["Sex","Embarked","Initial"]<br/>      dataset = feature_generation(self,df)<br/>      df_impute = all_impute_together(dataset)<br/>      df_numeric = stringToNumeric_conv(df_impute,col_list)<br/>      df_final = drop_column(df_numeric,['Cabin','Name','Ticket','Family_Size','SibSp','Parch','Sex','Embarked','Initial'])<br/>      <strong class="lm hj">return</strong> df_final</span></pre><p id="b8c9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在变压器类中存在各种方法，每种方法用于不同的操作。</p><p id="8afc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="lv">Feature _ generation()-从名称列生成标题。</em>T9】</strong></p><p id="e49b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="lv"> Age_impute() —根据年龄组平均值估算年龄</em> </strong>。</p><p id="5410" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="lv">登船_估算&amp;费用_估算—估算登船和费用</em> </strong></p><p id="4040" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="lv"> StringToNumeric()-字符串数据类型为数值</em> </strong></p><p id="f6b3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="lv"> Drop_col —从数据帧中删除不需要的列</em> </strong></p><p id="6e69" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">步骤5创建管道并提取模型</strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="84f7" class="lq ju hi lm b fi lr ls l lt lu"><strong class="lm hj">from</strong> <strong class="lm hj">pyspark.ml.classification</strong> <strong class="lm hj">import</strong> GBTClassifier <br/><strong class="lm hj">from</strong> <strong class="lm hj">pyspark.ml.classification</strong> <strong class="lm hj">import</strong> RandomForestClassifier <br/><strong class="lm hj">from</strong> <strong class="lm hj">pyspark.ml.evaluation</strong> <strong class="lm hj">import </strong>MulticlassClassificationEvaluator  </span><span id="d71c" class="lq ju hi lm b fi lz ls l lt lu"><em class="lv"><br/># initialization for pipeline setup</em> <br/>my_model = preprocess_transform() <br/>df = my_model.transform(df) </span><span id="1013" class="lq ju hi lm b fi lz ls l lt lu">feature = VectorAssembler(inputCols=['Pclass','Age','Fare','Alone','Sex_index','Embarked_index','Initial_index'],outputCol="features") <em class="lv"> </em></span><span id="88ac" class="lq ju hi lm b fi lz ls l lt lu"> rf = RandomForestClassifier(labelCol="Survived", featuresCol="features", numTrees=10) </span><span id="29b7" class="lq ju hi lm b fi lz ls l lt lu"><em class="lv">'''</em> <em class="lv">pipeline stages initilization , fit and transform.</em> <em class="lv">'''</em> <br/>pipeline = Pipeline(stages=[feature,rf])<em class="lv">  <br/>model = pipeline.fit(df)</em>  <br/>paramGrid = ParamGridBuilder().addGrid(rf.numTrees,[100,300]).build()  </span><span id="a1f1" class="lq ju hi lm b fi lz ls l lt lu">evaluator = MulticlassClassificationEvaluator(labelCol="Survived", predictionCol="prediction", metricName="accuracy")  </span><span id="b343" class="lq ju hi lm b fi lz ls l lt lu">crossval = CrossValidator(estimator=pipeline,                           estimatorParamMaps=paramGrid,                        evaluator=evaluator,numFolds=3)  <br/><em class="lv"># use 3+ folds in practice</em>  <em class="lv"># Run cross-validation, and choose the best set of parameters.</em> <br/>cvModel = crossval.fit(df) <br/>prediction = cvModel.transform(<em class="lv">df_test</em>)   <br/><em class="lv">mlflow.spark.log_model(model, "spark-model16")  mlflow.spark.save_model(model, "spark-model_test")</em> <br/></span></pre><p id="3d10" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这些步骤中，首先我们调用transformer类，并用我们创建的上述选项转换我们的数据帧。</p><p id="af0d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">变换后调用FeatureAssembler，它用于将所有输入特征绑定到一个矢量中。创建一个管道对象要经过两个阶段，第一个是特征组装器，第二个是估计器(分类器或回归器)。</p><p id="ddaf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你愿意，你可以创建一个交叉验证使用的超参数列表，paramGridBuilder使用列表值来分配各种超参数。</p><p id="c00e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为测量标准设置评估器，如多类分类评估器</p><p id="5ade" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">调用crossvalidator并传递管道模型、参数网格、赋值器和折叠数。</p><p id="a827" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦这些都完成了，将数据放入crossvalidator函数，记住它包含一个有模型的管道。所以它会用不同的参数组合训练模型，最后用<code class="du lw lx ly lm b">transform</code>的方法得到预测数据。</p><p id="a0db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://mlflow.org/" rel="noopener ugc nofollow" target="_blank"> MLFlow </a>是一个管理机器学习周期的平台。在预测之后，我们可以使用ml流的两个函数，即<code class="du lw lx ly lm b">log</code>和<code class="du lw lx ly lm b">save</code>。日志功能将在ml流门户中记录处理指标，保存功能将保存最佳ML模型。ML flow还有许多其他有用的功能，所以只需查看它们的官方文档。</p><p id="c15a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">使用Azure ML在Azure中进行第6步部署</strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="1830" class="lq ju hi lm b fi lr ls l lt lu"><strong class="lm hj">import</strong> mlflow.azureml<br/><strong class="lm hj">from</strong> azureml.core <strong class="lm hj">import</strong> Workspace<br/><strong class="lm hj">from</strong> azureml.core.webservice <strong class="lm hj">import</strong> AciWebservice, Webservice</span></pre><p id="c55b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在Azure ML中创建一个工作区。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="fe5c" class="lq ju hi lm b fi lr ls l lt lu">workspace_name = "MLServiceDMWS11"<br/>subscription_id = "xxxxxxxx-23ad-4272-xxxx-0d504b07d497"<br/>resource_group = "mlservice_ws"<br/>location = "xxx"<br/>azure_workspace = Workspace.create(name=workspace_name,<br/>                                   subscription_id=subscription_id,<br/>                                   resource_group=resource_group,<br/>                                   location=location,<br/>                                   create_resource_group=<strong class="lm hj">False</strong>,<br/>                                  )</span></pre><p id="5d6d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在工作空间中建立模型的图像，这基本上意味着我们只是在工作空间中保存模型对象。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="a748" class="lq ju hi lm b fi lr ls l lt lu">azure_image, azure_model = mlflow.azureml.build_image(model_uri="/dbfs/databricks/mlflow/my_test_ml_flow",<br/>                                                      workspace=azure_workspace,<br/>                                                      description="model_description",<br/>                                                      synchronous=<strong class="lm hj">True</strong>)</span></pre><p id="8f5b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用Azure web服务api会将模型公开为rest端点。传递模型图像、工作空间和配置设置。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="903f" class="lq ju hi lm b fi lr ls l lt lu">webservice_deployment_config = AciWebservice.deploy_configuration()<br/>webservice = Webservice.deploy_from_image(deployment_config=webservice_deployment_config,<br/>                                          image=azure_image, <br/>                                          workspace=azure_workspace, <br/>                                          name='mysvc')<br/>webservice.wait_for_deployment()<br/>print("Scoring URI is: %s", webservice.scoring_uri)</span></pre><p id="bd63" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦部署了模型，让我们检查一下我们创建的API是否工作。在列表中传递参数和相应的值，并点击post请求。一旦请求成功，它将通过响应进行确认。采用i/p和o/p的标准方式是json格式。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="5b58" class="lq ju hi lm b fi lr ls l lt lu"><strong class="lm hj">import</strong> requests<br/><strong class="lm hj">import</strong> json<br/><br/>sample_input = {<br/>    "columns": [<br/>        "col1",<br/>        "col2",<br/>        "col3",<br/>       "coln "<br/>    ],<br/>    "data": [<br/>        [val1, val2, val3,...... valn]<br/>    ]<br/>}<br/>response = requests.post(<br/>              url=webservice.scoring_uri, data=json.dumps(sample_input),<br/>              headers={"Content-type": "application/json"})<br/>response_json = json.loads(response.text)<br/>print(response_json)</span></pre><p id="7697" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这只是这篇文章的初稿，我会在不久的将来更新。你可以在下面我的git链接中查看全部代码。如果您有任何反馈或建议，我们将不胜感激。</p><div class="ma mb ez fb mc md"><a href="https://github.com/yug95/MachineLearning/tree/master/flask_app_deployment" rel="noopener  ugc nofollow" target="_blank"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hj fi z dy mi ea eb mj ed ef hh bi translated">yug 95/机器学习</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">使用PySpark Titanic Survival分类器使用flask web app部署，并公开一个rest端点。文件信息…</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">github.co</p></div></div></div></a></div><p id="92ec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢您的支持！我们下次再见:)</p></div></div>    
</body>
</html>