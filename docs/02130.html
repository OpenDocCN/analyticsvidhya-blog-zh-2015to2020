<html>
<head>
<title>The Dangers of Under-fitting and Over-fitting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">装配不足和过度装配的危险</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-dangers-of-under-fitting-and-over-fitting-495f9efa1847?source=collection_archive---------10-----------------------#2019-12-02">https://medium.com/analytics-vidhya/the-dangers-of-under-fitting-and-over-fitting-495f9efa1847?source=collection_archive---------10-----------------------#2019-12-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="100e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于数据科学家来说，机器学习模型是一个非常强大和有用的工具。构建模型时，重要的是要记住预测会带来预测误差。这些误差是由具有折衷关系的偏差和方差的组合造成的。理解这些基本原理只是建立精确模型和避免欠拟合和过拟合陷阱的第一步！</p><p id="fc79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">返璞归真！</strong></p><p id="fe74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对监督机器学习的核心理解是，我们正在逼近一个目标函数(f)，该目标函数将输入变量(X)映射到输出变量(Y)。该关系在数学上表示为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/4b2772dd63cbfa7bf0f43e7a9cc1a09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*i7ah9r2i14H-lJcUQaLF3w.png"/></div></figure><p id="8fab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中e代表总误差。总误差实际上可以进一步分为三部分:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jl"><img src="../Images/bc674b71b8d263db75c388fffd76440a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0-XUwVGnpkJTouW7ann8ww.png"/></div></div></figure><ul class=""><li id="80df" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated"><strong class="ih hj">的平方<em class="jz">偏向</em>的学习方法</strong></li><li id="e6f8" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">学习方法的<strong class="ih hj"> <em class="jz">方差</em> </strong></li><li id="1421" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">不可约误差</li></ul><p id="5256" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将详细讨论这些术语。随着我们对偏差和方差的深入研究，记住我们收集和分析的数据只是一个样本，因此是不完整的，而且有噪声，这一点很重要。模型如何对定型数据进行归纳对于确定模型如何对新数据做出反应非常重要。</p><p id="ddc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">那么什么是偏见呢？</strong></p><p id="3335" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jz">偏差</em> </strong>，或<strong class="ih hj"> <em class="jz">偏差误差</em> </strong>，可以定义为我们模型的预期预测值与我们试图预测的正确值之间的差值。高偏差会导致我们的模型错过我们的特征(X)和目标输出(Y)之间的重要关系，因此它无法学习训练数据或推广到新数据。这也被称为<strong class="ih hj"> <em class="jz">欠拟合</em> </strong>。拟合不足的模型被迫做出大量假设，这可能导致不准确的预测。</p><p id="975c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们对偏见的概念有了基本的了解…</p><p id="0483" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是方差？</strong></p><p id="96ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jz">方差</em> </strong> <em class="jz"> </em>是模型预测对于给定数据点的变异性。它是对训练数据中微小波动的敏感性误差。当存在高方差时，这可能导致随机噪声(e)而不是预期输出(Y)被引入到训练数据中。高方差又称<strong class="ih hj"> <em class="jz">过拟合</em> </strong>数据。当数据过度拟合时，模型本质上学习训练数据太好，因此不能推广到新数据。</p><p id="235c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后一个误差项是<strong class="ih hj"> <em class="jz">不可约误差</em> </strong>。不可约误差本质上是来自我们控制之外的因素的噪声量，无法消除。</p><p id="1a2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以通过查看最佳拟合 的<strong class="ih hj"> <em class="jz">线来识别欠拟合和过拟合。在下面的左图中，我们可以看到这条线很简单，没有遵循许多数据点，因此显示出高偏差。下面的右图显示了几乎每个数据点后面的一条线，即使是那些可能是噪声或异常值的数据点，也显示了很高的方差。我们的目标是在这两个极端之间找到一个平衡点，这样大部分数据点都可以用适当的噪声来解释。</em></strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kf"><img src="../Images/8920171ed18667749b72f5c3e0f85127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fh8o-02rCymx_s385JOKOg.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx translated"><a class="ae kk" rel="noopener" href="/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76">https://medium . com/grey atom/what-is-under fitting-and-over fitting-in-machine-learning-and-how-to-deal-it-6803 a 989 c76</a></figcaption></figure><p id="bc1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">偏差和方差之间的关系也可以很容易地使用目标示例来可视化。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kl"><img src="../Images/a5aa941e68daf6c527354e700bf1a8ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wh6XRy9K1Z8AKt69_WEtYQ.png"/></div></div></figure><p id="5ad2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目标的中心代表一个预测非常准确的模型。参数或线性机器学习模型(如线性回归和逻辑回归)通常具有较高的偏差，但方差较低(左下角)。非参数或非线性机器学习算法(如决策树和k-最近邻)通常具有低偏差但高方差(右上)。我们的目标是创建一个误差最小的低偏差和低方差(左上)的模型。</p><p id="d686" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">偏差-方差权衡</strong></p><p id="3783" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目的是使模型能够适当地对新输入进行归纳和分类。我们可以通过一个更复杂的模型来减少方差，但这样我们就有更大偏差的风险。相反，如果我们有一个复杂度较低的模型，偏差可能较低，但我们有较高方差的风险。偏差具有响应模型复杂性的负斜率，而方差具有正斜率。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es km"><img src="../Images/ede428a6b3090d4f8f0992563642d919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*MJazrHRKckRksAKPOSaVZw.png"/></div><figcaption class="kg kh et er es ki kj bd b be z dx translated"><a class="ae kk" href="https://djsaunde.wordpress.com/2017/07/17/the-bias-variance-tradeoff/" rel="noopener ugc nofollow" target="_blank">https://djsaunde . WordPress . com/2017/07/17/the-bias-variance-trade off/</a></figcaption></figure><p id="15a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要找到一个在偏差和方差之间折衷的“最佳点”,以便在训练数据中有足够的噪声来允许泛化，而不会对训练数据进行不准确的分类。这将允许我们找到“最适合”的模型</p><p id="9aef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">怎样才能防止欠拟合和过拟合？</strong></p><p id="f38f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于<strong class="ih hj"> <em class="jz">欠拟合</em> </strong>:</p><ul class=""><li id="b127" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">确保有足够的训练数据，以便误差/成本函数(例如MSE或SSE)充分最小化</li></ul><p id="211d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于<strong class="ih hj"> <em class="jz">过拟合</em> </strong>:</p><ul class=""><li id="8cb9" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">限制模型中特征或可调参数的数量。随着特征数量的增加，模型的复杂性也增加，从而产生更高的过拟合机会。</li><li id="bcec" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">缩短训练时间，使模型不会“过度学习”训练数据。</li><li id="6146" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">将某种形式的正则化项添加到误差/成本函数中，以鼓励更平滑的网络映射(岭回归或套索回归是常用的技术)</li><li id="9788" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">交叉验证以最小化均方误差</li></ul><p id="531b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您的阅读！</p><p id="9c8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献:</strong></p><ol class=""><li id="e05c" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc kn jw jx jy bi translated"><a class="ae kk" href="https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/over fitting-and-under fitting-with-machine-learning-algorithms/</a></li><li id="b025" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc kn jw jx jy bi translated">【http://scott.fortmann-roe.com/docs/BiasVariance.html T4】</li><li id="835a" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc kn jw jx jy bi translated"><a class="ae kk" href="https://elitedatascience.com/overfitting-in-machine-learning" rel="noopener ugc nofollow" target="_blank">https://elitedata science . com/over fitting-in-machine-learning</a></li><li id="a9ca" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc kn jw jx jy bi translated"><a class="ae kk" rel="noopener" href="/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76">https://medium . com/grey atom/what-is-under fitting-and-over fitting-in-machine-learning-and-how-to-deal-it-6803 a 989 c76</a></li><li id="3701" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc kn jw jx jy bi translated"><a class="ae kk" href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229" rel="noopener" target="_blank">https://towards data science . com/understanding-the-bias-variance-trade off-165 e 6942 b229</a></li><li id="1dc3" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc kn jw jx jy bi translated"><a class="ae kk" href="http://www.cs.bham.ac.uk/~jxb/INC/l9.pdf" rel="noopener ugc nofollow" target="_blank">http://www.cs.bham.ac.uk/~jxb/INC/l9.pdf</a></li><li id="5835" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc kn jw jx jy bi translated"><a class="ae kk" href="https://djsaunde.wordpress.com/2017/07/17/the-bias-variance-tradeoff/" rel="noopener ugc nofollow" target="_blank">https://djsaunde . WordPress . com/2017/07/17/the-bias-variance-trade off/</a></li><li id="f2c8" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc kn jw jx jy bi translated"><a class="ae kk" href="https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/under fitting-and-over fitting-in-machine-learning/</a></li></ol></div></div>    
</body>
</html>