<html>
<head>
<title>Neural Machine Translation using Simple Seq2Seq Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用简单Seq2Seq模型的神经机器翻译</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-machine-translation-using-simple-seq2seq-517b5ce1ae0f?source=collection_archive---------5-----------------------#2020-05-04">https://medium.com/analytics-vidhya/neural-machine-translation-using-simple-seq2seq-517b5ce1ae0f?source=collection_archive---------5-----------------------#2020-05-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/782ca555d9951dad7b235b66500a9b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*prLx5jgXucwWMQoz-Cb7rQ.jpeg"/></div></div></figure><p id="04b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在当今全球化时代，各国以某种方式相互联系，语言翻译起着至关重要的作用。尽管英语是被普遍接受的交流语言，仍然有许多国家更喜欢用他们的母语交流。如果我们暂时把全球视角放在一边，仍然有一些人用他们的母语更好地理解事物。因此，考虑到所有这些需求，从很长一段时间以来，语言翻译都是人类文明的一部分。在技术进步之前，有人类曾经在各民族之间充当翻译。但是技术发展后不久，我们创造了机器来做这种翻译。这种机器翻译不断发展久而久之，研究人员不断研究，使机器翻译更加准确和正确。随着深度学习和神经网络(如RNN、LSTM和更多基于序列的神经网络)的发展，构建一个简单的语言翻译器现在并不困难。</p><p id="f96d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以在这篇教程博客中，我将展示如何创建一个简单的神经机器翻译器，将英语句子翻译成马拉地语句子。我选择了一个马拉地语句子，因为它有助于我更好地理解翻译。此外，印地语数据集非常小。所以不浪费更多的时间，让我们开始吧。</p><h1 id="64d0" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">目录</h1><ol class=""><li id="0719" class="km kn hi is b it ko ix kp jb kq jf kr jj ks jn kt ku kv kw bi translated">介绍</li><li id="11c6" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">数据收集和清理</li><li id="9a07" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">编码器和解码器的输入创建</li><li id="dc0d" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">模型结构</li><li id="5613" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">推理模型构建</li><li id="934b" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">预言；预测；预告</li><li id="281a" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">模型性能</li><li id="a9f8" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">改进的范围</li><li id="92ac" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">结论</li><li id="3adc" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">参考</li><li id="d102" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">与我联系</li></ol><h1 id="3b82" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">1.介绍</h1><p id="f235" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">在这一部分，我将简单介绍一下什么是神经机器翻译，也称为NMT。所以，NMT是使用神经网络构建的，因此“N”代表“神经”,因为它使用机器来进行翻译，“M”代表“机器”,最终，它会进行翻译，因此“T”代表“翻译”。我希望你已经理解到这里，对于非机器学习和深度学习的读者，你们可能想知道这个“Seq2Seq模型”是什么，我将尝试为他们解释它。简而言之，在深度学习中，我们利用神经网络来训练我们的深度学习模型(进行预测)，所以有一些像RNN，LSTM，GRU这样的神经网络，它们以序列作为输入，就像我们例子中的英语句子序列一样。所以Seq2Seq模型就是这些特殊神经网络的代表。Seq2Seq模型有两个部分，一个称为编码器，另一个称为解码器，顾名思义，编码器对我们输入的英语句子进行编码，解码器将这些编码的英语句子解码成马拉地语句子。这是一个简单seq2seq模型的简单介绍，基于进一步的改进和研究，我们不断寻找新技术来改进输入序列的编码，但seq2seq模型的核心是编码器和解码器。关于编码器和解码器的更多信息，可以参考<a class="ae lf" href="https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/" rel="noopener ugc nofollow" target="_blank">这个</a>链接。</p><h1 id="dd3e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.数据收集和清理</h1><p id="2861" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">因此，在快速介绍了NMT和seq2seq模型之后，是时候让我们深入研究这个NMT的编码部分了。在本节中，我们将获取原始数据并执行一些基本的文本清理。要获取数据集，您可以参考<a class="ae lf" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank">这个</a>链接。</p><p id="c317" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在从我在上面段落中给出的链接加载数据之前，我们需要导入我们将在这个项目中使用的所有必要的库。</p><h2 id="ed4e" class="lg jp hi bd jq lh li lj ju lk ll lm jy jb ln lo kc jf lp lq kg jj lr ls kk lt bi translated">导入库</h2><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="50f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们已经导入了所有的库，我们可以继续加载我们的原始文本数据文件了。</p><pre class="lu lv lw lx fd ma mb mc md aw me bi"><span id="66f2" class="lg jp hi mb b fi mf mg l mh mi"><strong class="mb hj">with</strong> open('mar.txt','r') <strong class="mb hj">as</strong> f:<br/>  data = f.read()</span><span id="1e9b" class="lg jp hi mb b fi mj mg l mh mi"><em class="mk"># len(data)</em></span></pre><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="dbc0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以上要点显示了原始文本文件的内容，您可以看到我们需要清理该文件，因此让我们进入数据清理过程。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="07c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的代码中，我将原始数据文本的每一行都分离出来，并存储到名为uncleaned_data_list的列表中。之后，我将所有的英语和马拉地语句子分离出来，分别存储到英语单词和马拉地语单词列表中，最后，我打印出了英语句子和马拉地语句子的总数。</p><p id="a89c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">之后，我创建了一个熊猫数据框架，它有两列，分别命名为English和Marathi，并将其保存为CSV文件。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><pre class="lu lv lw lx fd ma mb mc md aw me bi"><span id="a1c8" class="lg jp hi mb b fi mf mg l mh mi">language_data.head()</span></pre><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/e1f56d22eb8e4ad74c4dcdd7b6e92199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9UjCV3wuqhfh1XeaPw6yzA.png"/></div></div></figure><pre class="lu lv lw lx fd ma mb mc md aw me bi"><span id="f8d0" class="lg jp hi mb b fi mf mg l mh mi">language_data.tail()</span></pre><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/f5406d26829054fa6bc44db0999234bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aESAVJz0LZyZKp6hz04wkQ.png"/></div></div></figure><p id="c95a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以看到我们新创建的pandas数据框架的头部和尾部数据。将数据表示为这种格式的原因是，它使数据处理更加有效，同时我们也可以利用panda的工具。现在让我们进一步进入我们的文本清理过程。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="a940" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我认为上面的代码部分是不言自明的，但我还是会简单地解释一下。在上面的代码部分，我做了文本预处理，包括一些基本的东西，如小写文本，删除标点符号，删除数字和空白。</p><pre class="lu lv lw lx fd ma mb mc md aw me bi"><span id="a94d" class="lg jp hi mb b fi mf mg l mh mi"><em class="mk"># Putting the start and end words in the marathi sentances</em><br/>marathi_text_ = ["start " + x + " end" <strong class="mb hj">for</strong> x <strong class="mb hj">in</strong> marathi_text_]</span></pre><p id="d9b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的代码部分看起来很简单，但它是使用seq2seq模型的神经机器翻译中最重要的一步。当我们要构建Seq2Seq模型时，我将解释在马拉地语句子中添加“start”和“end”标签的原因，直到请耐心等待，这样，我们就到了数据收集和数据清理部分的末尾。</p><h1 id="dab6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">编码器和解码器的输入创建</h1><p id="8039" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">在本节之前，我们已经有了干净且经过处理的文本数据集，在本节中，我将向您展示如何为您的NMT模型准备数据。</p><p id="0744" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们进入数据准备步骤之前，我们将把数据分成训练和测试，我在这里做的是90-10分割。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="ba98" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你要记住的第一件事是，seq2seq模型希望所有的输入序列长度相同，因此实现这一点的一种方法是，我们计算英语和马拉地语中每个句子的长度，然后我们选择最大长度，即英语和马拉地语中最长的句子。因此，最后，我们有两个max_length，一个用于英语句子，另一个用于马拉地语句子，你也可以在代码部分看到。在后面的章节中，我会告诉你如何使用这些最大长度来使所有的英语和马拉地语序列长度相等。在此之后，由于机器只能理解数字而不能理解文本，我们需要一种方法将输入的文本序列转换成数字，其中一种方法就是索引句子中的单词。我们可以使用Keras的“Tokenizer”方法进行索引。我们还需要获取英语和马拉地语语料库的词汇量，这是我们为模型训练创建输入数据时所需的。我现在解释的所有事情都在上面的代码块中完成了。</p><p id="a8eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的代码很长，但是不要害怕，我会完整地解释它。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="f96b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如你所见，我创建了一个名为“generator_batch”的函数，它接受三个参数“X”、“Y”、“batch_size”，你可以自己算出所有这些参数的含义。现在函数中的第一件事是一个“While True ”,即While循环，这个循环被设置为总是运行ok！好吧，我可以理解到这里，但在循环内发生了什么？当我第一次在我用来创建这个项目的引用博客上看到这段代码时，我想到了同样的问题。其实理解起来没那么复杂。我们正在运行一个从0到X(训练数据)长度的“for循环”,在本例中，这个循环的步长是batch_size，即128。在循环中，我有三个变量“编码器_数据_输入”、“解码器_数据_输入”、“解码器_目标_输入”。现在让我们更详细地看看每个变量。我已经创建了一个全零的度量，其行数等于“批处理_大小”，列数等于“最大_长度_英语”，类似地，我们对解码器输入也是如此，但您可以看到解码器目标数据是三维形状的，因为解码器输出是三维的。但是等等，我已经告诉过你们，解码器是把翻译的句子作为输出的东西，所以我们也创建了解码器输入。这是因为它使训练更快，这种方法被称为“教师强迫”技术。在教师强制中，我们将目标数据作为输入传递给解码器，例如，如果解码器要预测“the ”,那么我们也将“the”传递给解码器的输入，您可以将它与实际的师生学习过程联系起来。因此，它使学习过程更快。</p><p id="c6d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">到目前为止，我们已经创建了三个零矩阵。在嵌套的for循环中，我一个接一个地获取索引、X_train和Y_train数据，还有另一个for循环，它从外部for循环X_train语句中获取单词以及索引，然后， 我在我们的encoder_data_input zeros度量中填充了输入句子中单词的相应索引，这里，外部循环“I”是按行排列的，嵌套循环的“t”是按列排列的，正如您所观察到的，所有输入序列都将具有相同的长度，因为我已经将列设置为等于max_length_english，所以如果输入序列小于max _ length _ English，我们将为其其余列填充零。 在decoder_input_data的for循环中做了类似的事情，但在decoder_target_input中没有。请记住，我们已经在目标马拉地语句子中填充了“start”和“end”标签，因此在t = 0时，我们将“start”作为单词，我们不希望这个单词出现在目标输入序列中，因此我们从t&gt;0开始对decoder_target_input进行排序。</p><p id="b3ce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">到目前为止，我已经为seq2seq模型创建了输入和输出，并使用yield分批发送数据，而不是同时发送。在下一节中，我将告诉你在目标语句中添加“开始”和“结束”标签的原因，以及编码器和解码器的详细工作和模型代码。</p><h1 id="85dc" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">模型结构</h1><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/ada084a7969c84c38e993bee5634671d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDDhDYhipj5owgPKmuSs2Q.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">编码器和解码器</figcaption></figure><p id="7199" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图是一个简单的编码器和解码器的示意图。这里的横轴是时间戳，而‘A’、‘B’、‘C’是输入序列的单词。现在假设我们有10个输入句子，存储在我们的编码器_数据_输入指标中，指标中的每一行是一个句子，因此我们有10行和max_length_english列，比如X1，X2，…。，X10用来表示每个序列。</p><p id="1c6c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，</p><p id="bd22" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于X1，假设序列中有3个单词，</p><p id="5d5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在t = 0时，我们将w1传入编码器并获取其状态</p><p id="ad34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在t = 1时，我们将w2和t = 0时的状态一起传入编码器</p><p id="31f2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在t = 2时，我们将w3和t = 1时的状态一起传入编码器</p><p id="9b37" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在序列的末尾，我们将编码器的输出及其在t=2时的状态作为输入传递给解码器，然后将t=0时的解码器输出作为输入传递给t=1时的解码器，依此类推，直到没有到达字符串的末尾。</p><p id="b141" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在将“开始”和“结束”附加到解码器输入的原因是，“开始”标签用于启动解码器开始解码，“结束”标签用于通知解码器停止解码过程。如果我们不使用这一点，那么解码器将永远无法产生第一个字，因为我们知道，如果我们将该字作为输入传递给解码器，它将预测序列的下一个字，并且该预测将永远不会停止，因为它不知道何时停止。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="7bfb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的代码正是我之前解释过的，也显示在编码器-解码器图像中。参考上面的编码器-解码器图像，尝试理解代码。</p><pre class="lu lv lw lx fd ma mb mc md aw me bi"><span id="a0a8" class="lg jp hi mb b fi mf mg l mh mi">plot_model(model, to_file='train_model.png', show_shapes=<strong class="mb hj">True</strong>)</span></pre><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/8e3118f7ee56b5819cb9f21763ddd70a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHcQ-BMSjsud3LGaDhs6ow.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">培训模式</figcaption></figure><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="a084" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的代码部分中，我正在保存训练好的模型并加载它。请注意一件事，只像这样保存模型，而不是其他任何方式，因为我们必须创建推理编码器和解码器，以便稍后进行预测，并且该模型也使用训练好的模型层。</p><h1 id="fddd" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">推理模型构建</h1><p id="799f" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">以下部分是推理模型的代码，该模型用于进行预测。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><figure class="lu lv lw lx fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/151659fa41e82298576ea04710f6f813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*PICBrd2xSM2sdSaWkwbMvQ.png"/></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">编码器推理模型</figcaption></figure><figure class="lu lv lw lx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/36571673061199ed732355bd70e96dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hzkoXZlaffW25Bs9L4lL7A.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">解码器推理模型</figcaption></figure><p id="b5e8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码器推理模型类似于训练模型，不同之处在于解码器推理模型。如果您看到编码器-解码器图，我们可以看到前一个时间戳的状态被传递到下一个时间戳，因此我们需要找到一种方法来保存前一个时间戳的状态。解码器推理中的以下代码也在做同样的事情。</p><pre class="lu lv lw lx fd ma mb mc md aw me bi"><span id="f5da" class="lg jp hi mb b fi mf mg l mh mi"><em class="mk">#inference decoder</em><br/><em class="mk"># The following tensor will store the state of the previous timestep in the "starting the encoder final time step"</em><br/>decoder_state_h_input = Input(shape=(latent_dim,)) <em class="mk">#becase during training we have set the lstm unit to be of 50</em><br/>decoder_state_c_input = Input(shape=(latent_dim,))<br/>decoder_state_input = [decoder_state_h_input,decoder_state_c_input]</span></pre><p id="4b63" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，我们创建解码器推理模型。为了创建解码器模型，我们首先定义了输入层。这是经过训练的解码器模型的输入层。之后，我们创建与训练解码器模型中相同的层，但唯一的区别是解码器LSTM的初始状态被设置为先前时间戳的状态。正如我们在上面的编码器-解码器图表中看到的，推理模型的其余部分与训练解码器模型相似。</p><pre class="lu lv lw lx fd ma mb mc md aw me bi"><span id="2385" class="lg jp hi mb b fi mf mg l mh mi"><em class="mk"># inference decoder input</em><br/>decoder_input_inf = model_loaded.input[1] <em class="mk">#Trained decoder input layer</em><br/><em class="mk"># decoder_input_inf._name='decoder_input'</em><br/>decoder_emb_inf = model_loaded.layers[3](decoder_input_inf)<br/>decoder_lstm_inf = model_loaded.layers[5]<br/>decoder_output_inf, decoder_state_h_inf, decoder_state_c_inf = decoder_lstm_inf(decoder_emb_inf, initial_state =decoder_state_input)<br/>decoder_state_inf = [decoder_state_h_inf,decoder_state_c_inf]<br/><em class="mk">#inference dense layer</em><br/>dense_inf = model_loaded.layers[6]<br/>decoder_output_final = dense_inf(decoder_output_inf)<em class="mk"># A dense softmax layer to generate prob dist. over the target vocabulary</em><br/><br/>decoder_model = Model([decoder_input_inf]+decoder_state_input,[decoder_output_final]+decoder_state_inf)</span></pre><h1 id="73d8" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">预言；预测；预告</h1><p id="c684" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">现在，我们已经训练了seq2seq模型，并使用训练好的模型创建了用于进行预测的推理模型，因此是时候使用以下代码进行预测了。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="411f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的函数将翻译后的句子返回给我们，它首先对输入的句子进行编码，然后获取编码器最后一个时间戳的状态值。然后，我们创建了称为“target_seq”的单值度量，它保存下一个预测的马拉地语单词的索引，但是为了开始解码过程，我们需要传递第一个单词，即“start”。因此，在开始时，我们将把单词“start”的索引存储到“target_seq”中，然后，因为我们必须执行解码过程，直到我们到达“end”单词。因此，我们需要在一个循环中调用解码器推理模型。在循环中，我们将“target_seq”和编码器最后的时间戳“states”作为解码器推理模型的输入。推理解码器模型返回下一个单词索引和解码器状态，在此之后，我们将该索引转换回单词并将其添加到字符串变量“decoder_sentance ”,然后我们将新生成的单词索引放入“target_seq ”,并用解码器模型状态更新“state_value”。此过程继续，直到我们得到预测的单词“end”。因此，在while循环的末尾，我们得到了完整的翻译句子。以下是我得到的一些输出。</p><pre class="lu lv lw lx fd ma mb mc md aw me bi"><span id="204e" class="lg jp hi mb b fi mf mg l mh mi"><strong class="mb hj">for</strong> i <strong class="mb hj">in</strong> range(30):<br/>  sentance = X_test[i]<br/>  original_target = y_test[i]<br/>  input_seq = tokenizer_input.texts_to_sequences([sentance])<br/>  pad_sequence = pad_sequences(input_seq, maxlen= 30,padding='post')<br/>  <em class="mk"># print('input_sequence =&gt;',input_seq)</em><br/>  <em class="mk"># print("pad_seq=&gt;",pad_sequence)</em><br/>  predicted_target = decode_seq(pad_sequence)<br/>  print("Test sentance: ",i+1)<br/>  print("sentance: ",sentance)<br/>  print("origianl translate:",original_target[6:-4])<br/>  print("predicted Translate:",predicted_target[:-4])<br/>  print("=="*50)</span></pre><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><h1 id="ecb1" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">模型性能</h1><p id="cc22" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">与用于训练seq2seq模型的数据量相比，该模型的性能良好。对于在34825个数据集上训练了50个时期的模型，我已经获得了54.8%的准确度。简单的seq2seq模型的局限性在于，它不能有效地翻译一个长句子。所以这个模型也有这个局限性。</p><h1 id="ca99" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">改进的范围</h1><p id="7bb0" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">因为这只是一个简单的模型，向您展示如何创建一个神经机器翻译器，正如我们所知，没有解决方案是完美的，一切都随着时间的推移而不断改进。这个解决方案不是唯一的解决方案，您可以根据您的理解和技能进一步改进它。</p><p id="4185" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了进一步改进这一模式，我可以提出以下建议:</p><ol class=""><li id="53bc" class="km kn hi is b it iu ix iy jb mu jf mv jj mw jn kt ku kv kw bi translated">我们可以在一个有很多变化的大型数据集上训练这个模型。</li><li id="d273" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">对于它冗长的呈现限制，我们可以加入注意机制。</li><li id="8fbd" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">我们可以试着用LSTM代替GRU，检查模型的性能。</li></ol><h1 id="c1da" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="1e6c" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">这是博客的结论部分。这篇博客向你展示了神经机器翻译是怎么回事，这个世界对翻译的需求是什么，以及你如何利用深度学习自己创造一个NMT，你已经详细了解了seq2seq模型是什么。在下一篇博客中，我将向您展示如何部署这个NMT并使用Flask创建一个REST API，以便您可以通过任何应用程序访问您的模型。以下是我们将在下一篇博客中制作的最终产品的视频。所以请在媒体上关注我，这样你就可以在我的下一篇博客发表时收到通知。</p><figure class="lu lv lw lx fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/50fe7765023a646e26523c43f445a6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*H0TMzJgkcYjffviSBK4FYQ.gif"/></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">下一篇博客最终产品演示</figcaption></figure><h1 id="5c23" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">参考</h1><div class="my mz ez fb na nb"><a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">Keras中序列对序列学习的十分钟介绍</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">我经常看到这个问题——如何在Keras中实现RNN序列对序列学习？这里有一个简短的介绍…</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">blog.keras.io</p></div></div></div></a></div><div class="my mz ez fb na nb"><a href="https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq_restore.py" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">keras-team/keras</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">人类的深度学习。通过在GitHub上创建一个帐户，为keras-team/keras开发做出贡献。</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">github.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np io nb"/></div></div></a></div><div class="my mz ez fb na nb"><a href="https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">如何在Keras - Machine中开发用于序列到序列预测的编码器-解码器模型…</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">编码器-解码器模型提供了一种模式，使用递归神经网络来解决挑战性的…</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">machinelearningmastery.com</p></div></div><div class="nk l"><div class="nq l nm nn no nk np io nb"/></div></div></a></div><div class="my mz ez fb na nb"><a href="https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">编码器-解码器长短期记忆网络-机器学习掌握</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">使用示例Python代码对用于序列间预测的编码器-解码器LSTMs进行简单介绍。的…</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">machinelearningmastery.com</p></div></div><div class="nk l"><div class="nr l nm nn no nk np io nb"/></div></div></a></div><div class="my mz ez fb na nb"><a href="https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7" rel="noopener follow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">使用Seq2Seq编码器-解码器LSTM模型的单词级英语到马拉地语神经机器翻译</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">目录</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">towardsdatascience.com</p></div></div><div class="nk l"><div class="ns l nm nn no nk np io nb"/></div></div></a></div><h1 id="d629" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">与我联系</h1><p id="6c54" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">你可以在<a class="ae lf" href="http://linkedin.com/in/vashistnarayan-singh" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上和我联系。</p><p id="15b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在我的<a class="ae lf" href="https://github.com/vashist1994/Neural-machine-translation-seq2seq" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到这个博客的完整代码。</p></div></div>    
</body>
</html>