<html>
<head>
<title>Understanding FastAI v2 Training with a Computer Vision Example- Part 2: FastAI Optimizers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用计算机视觉例子理解FastAI v2训练-第2部分:FastAI优化器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-fastai-v2-training-with-a-computer-vision-example-part-2-fastai-optimizers-df65cb018604?source=collection_archive---------11-----------------------#2020-10-20">https://medium.com/analytics-vidhya/understanding-fastai-v2-training-with-a-computer-vision-example-part-2-fastai-optimizers-df65cb018604?source=collection_archive---------11-----------------------#2020-10-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/71def1c77ce3f6473f24e062b4b5f818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cXzglYO0mQlPVynnZKiIiA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图像来源:<a class="ae iu" href="https://arxiv.org/abs/1805.04829" rel="noopener ugc nofollow" target="_blank">端到端控制的空间不确定性采样</a></figcaption></figure><p id="da77" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是本系列的第二篇文章。本系列的目标读者是那些已经熟悉FastAI，并希望更深入地了解幕后发生的事情的人。该系列的总体结构如下:</p><ol class=""><li id="dc64" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated"><a class="ae iu" rel="noopener" href="/@rakesh.melezhath/understanding-fastai-v2-training-with-a-computer-vision-example-part-1-the-resnet-model-dd9270450bb8">研究resnet34模型架构，并使用普通Python &amp; PyTorch构建它。</a></li><li id="6948" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" rel="noopener" href="/@rakesh.melezhath/understanding-fastai-v2-training-with-a-computer-vision-example-part-2-fastai-optimizers-df65cb018604">深入研究FastAI优化器&amp;实现一个NAdam优化器。</a></li><li id="5fc5" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" rel="noopener" href="/@rakesh.melezhath/understanding-fastai-v2-training-with-a-computer-vision-example-part-3-fastai-learner-and-a3ea85c6fe78">学习FastAI学习器和回调&amp;用回调实现学习率查找器(lr_find方法)。</a></li></ol><p id="1cf6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将使用上一篇文章中构建的resnet模型来理解FastAI优化器。我们将使用Google Colab来运行我们的代码。你可以在这里找到这个系列<a class="ae iu" href="https://github.com/Rakeshsuku/Medium-Blog/tree/master/Understanding%20FastAI%20v2%20Training" rel="noopener ugc nofollow" target="_blank">的代码文件。我们直接开始吧。</a></p><p id="29f6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们将从上一篇文章中快速重建我们的模型(参考第一篇文章中的解释)。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kh"><img src="../Images/1a91328e602af868e1b24d3bbdf75395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oe-HyVnXfeTaS9Kpk1DHMw.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/43cf594cc56ec2716035a15c0bace65a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6_TozTnXTAItO_G8pmsYIg.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kn"><img src="../Images/2c1053b1c752319a1ecd32212cfc875e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u-RpZgxM1PEuLx55aGgiDw.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/bc169c1527a6e7fe32ffe043babe25a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6CSbDdVtfJKeSpgwRKs5pQ.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/0e5a9439dde7ef2063a3d4829ec4ad1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQV8wFyI0tOLntQQlLbtPg.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/8ba59f929302f3127d6529ec96d56013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Ue1L1Y0DyOXrBqpL1oojA.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/0c3e062388ba86c2e7a468def5c34165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mj1TJ1AZujgMU94WUnpBsg.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/4ba3ac811951d2a26486af39fca52ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*18gNI_AIaC7HWnweOcUOYw.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kh"><img src="../Images/1a2ea1ac7bd0802d9fd819d4be4bdac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WScOMuXCJaTdL_XRW9oOSQ.png"/></div></div></figure><p id="0002" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用于拟合模型的优化器保存在学习者对象的learn.opt属性中。在我们使用fit()方法或其任何变体(如fit_one_cycle)拟合模型之前，不会创建优化器对象。学习器构造函数的“opt_func”参数用于创建优化器。默认情况下，FastAI会添加一个Adam优化器。此外，默认情况下，模型的所有可训练参数(即未冻结的所有层的参数)在创建时作为单个列表传递给优化器。</p><p id="34f3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">FastAI优化器创建参数组，使我们能够为网络的不同部分设置不同的超参数值。默认行为是用模型的所有参数创建一个参数组。我们可以通过向优化器传递一个参数集合/生成器列表来创建多个参数组。我们将在第三篇文章中看到一个这样的例子，现在，让我们找到一个好的学习率，并训练一个时期的模型。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/6707ab03b10d243cc0be762f9a90335c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQ5LvMRBH8ZMqyeb9ssQmw.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/0338d724ea5ddbd9dc1ff98d7265a3f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VE41g91gaTY1lF2MapFToA.png"/></div></div></figure><p id="519b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">FastAI优化器有4个主要属性:</p><ol class=""><li id="0fde" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated"><strong class="ix hj"> param_list: </strong>参数列表的列表。每个内部列表形成一个参数组(稍后解释)。FastAI使用一个名为“L”的定制列表。</li><li id="ffce" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj"> hypers: </strong>超参数(学习率、权重衰减、动量等)字典列表。)每个参数组一个字典。</li><li id="5888" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj">状态:</strong>包含状态变量的字典，如平均梯度、平均平方梯度等。模型的所有参数。参数p的状态可以通过opt.state[p]来访问。状态变量用于实现Adam这样的自适应优化器。</li><li id="8e9b" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj">优化器回调:</strong> FastAI优化器在opt.step()操作期间使用回调函数来更新参数&amp;状态变量。我们将通过下面的例子来看看这是如何工作的。让我们检查一下FastAI添加到我们模型中的默认优化器。</li></ol><p id="86bc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> param_list: </strong>让我们先检查一下param_list。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/5db9c29df1acb10dd92a250e5e7db151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lJPLdfRa2eitwxK71HQBMA.png"/></div></div></figure><p id="0335" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们看到param_list是一个包含L个参数的L，即L(L(参数)，L(参数)，…)。每个内层L对应一个参数组。在我们的例子中，外部L中只有一个L，因为FastAI默认只创建一个参数组。内部L包含116项是我们xresnet34模型中参数张量的个数(checkout <a class="ae iu" rel="noopener" href="/@rakesh.melezhath/understanding-fastai-v2-training-with-a-computer-vision-example-part-1-the-resnet-model-dd9270450bb8">第一篇</a>看详情)。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kt"><img src="../Images/32cc58666b5a9de57fc63a3b0c9708d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_HW75QbkTmwHer3L6FvLpw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">xresnet34架构中参数张量的数量</figcaption></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/9a565a510c67f5e05f8c94629304c1a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hom4dkVjg1Wl8rc7YGq-SQ.png"/></div></div></figure><p id="85cb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">先来了解一下第一个参数“0.0.weight”。第一个“0”代表nn。在输入词干之后，第二个“0”表示输入词干的第一层，它是一个nn。Conv2d()。我们对这一层没有偏见，因为我们已经将bias=False传递给nn。Conv2d()。“0.0.weight”的形状应为[32，3，3，3]，对应于32个输出通道、3个输入通道和3×3内核。让我们检查一下这个。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ku"><img src="../Images/4b9fd9dc29d8c88655cb9c0dc171ca0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z3kHNSHuC-JsxlnDouwIaw.png"/></div></div></figure><p id="953c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型的其他参数可以用类似的方式识别。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kh"><img src="../Images/6864957b84a08162f10d4399560f0d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HrntfyQbFXts1p4O7YqjBg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">xresnet34模型上named_children方法的部分输出</figcaption></figure><p id="0e61" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> hypers: </strong>优化器的“hypers”<strong class="ix hj"/>属性存储超参数值。它是L个超参数字典，每个参数组有一个字典。因此，它允许我们为神经网络模型中的不同层(参数组)设置不同的超参数值(如微分学习率)。注意，我们这里只有一个参数组。让我们在上述1个时期的训练后检查超参数值。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kv"><img src="../Images/3933bfc8ca1327adbd424fe32f363e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MMbwqH9dXwa3edChrNmWIQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Adam优化器的超参数</figcaption></figure><p id="415d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面的“mom”和“sqr_mom”是Adam参数更新规则中的beta1和beta2。参考这篇文章<a class="ae iu" href="https://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank"> <em class="kw">梯度下降优化算法的概述</em> </a>如果你想复习不同的优化算法。有多少参数组，我们就有多少超参数字典。使用optimizer对象的set_hypers()方法更新超参数值。set_hypers()方法接受超参数名称&amp;值对作为其参数。超参数值可以用以下格式之一指定:</p><ul class=""><li id="9fef" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js kx jz ka kb bi translated">单一值:超参数将在所有参数组中设置为相同的值。</li><li id="6703" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kx jz ka kb bi translated">list like object:列表的长度必须等于参数组的数量。每个参数组中的超参数将被设置为列表中相应的值。</li><li id="16f7" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kx jz ka kb bi translated">切片对象:如果同时指定了“开始”和“停止”，参数组中的超参数将被设置为开始和停止之间的偶数倍，第一个参数组获得开始值，最后一个参数组获得停止值。如果没有指定“start”值(例如:slice(1e-3))，最后一个参数组中的超参数将获得“stop”值&amp;所有其他参数组将获得值“stop”/10。</li></ul><p id="e456" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">状态</strong>:优化器的状态属性存储每个参数的状态。状态值用于实现Adagrad、RMSprop、Adam等自适应优化器。让我们在1个时期的训练之后，探索我们的模型中的几个参数的状态。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/012e8acc39cadabcea1b071a44a6bbd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UZA9Mt6s308N4MK4sXf9Qw.png"/></div></div></figure><p id="b305" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可以看到state是一个长度为116的字典，参数作为键&amp;它们的状态作为值。</p><p id="5e63" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们模型中的最后一层是一个线性层，有512个输入激活和10个输出激活。让我们检查这一层的偏差参数，因为它只是一个长度为10的一维张量。优化器对象的<strong class="ix hj"> <em class="kw"> all_params() </em> </strong>方法为模型中的所有参数p返回L((p，pg，state，hyper)，…)，其中pg是p所属的参数组，state是p的状态，hyper是参数组pg的超参数值(字典)。让我们用这种方法来研究状态。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/6e164ce2aea984dcea54611ae29c648b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V4QrI-dLige887sm0z-sDw.png"/></div></div></figure><p id="8d62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面，我们可以看到状态跟踪Adam优化器所需的grad_avg &amp; sqr_avg。“步骤”存储优化器步骤(即参数更新)的数量，这也是Adam更新规则所需要的。由于我们刚刚训练了1个时期的模型，并且我们的训练数据集的大小是147批，所以步长值是147。</p><p id="89b9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">“do_wd”变量决定权重衰减是否应用于该参数。这由传递给学习者构造函数的参数“wd_bn_bias”(默认值:False)控制；默认值指定权重衰减不适用于所有偏移参数和所有batchnorm类型的参数(batch norm/实例规范/层规范)。学习者对象的create_opt()方法将值为False的变量“do_wd”插入到这些参数的状态字典中。如果参数在其状态中包含“do_wd”且值为False，则在step()操作期间不会应用权重衰减。请注意，create_opt()方法通过调用传递给学习者构造函数的“opt_func”方法来创建优化器对象。</p><p id="a656" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果学习器构造函数的“train_bn”参数设置为True(默认值)，create_opt()方法还会在批处理规范类型参数的状态字典中存储一个名为“force_train”的变量(值= True)。该变量导致批量定额参数被训练，即使它们属于转移学习设置中的冻结层。我们将在下一篇文章中看到这一点。</p><p id="3471" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们模型中倒数第三个参数是batchnorm层的权重张量。让我们检查该参数的状态:</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/26c4e65591da5c7f63e663b5910853f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QbQ6QvI-VSnCA2k64oTQIg.png"/></div></div></figure><p id="ffbf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">回调</strong> : FastAI优化器在opt.step()操作期间使用回调函数来更新参数&amp;状态变量。step()方法在模型的每个可训练参数上一个接一个地调用所有回调。这些回调可以更新参数值和/或可以返回新的状态值，然后将这些值保存到参数的状态中。让我们实现一个NAdam优化器来完成我们对FastAI优化器的研究。我无耻地从FastAI repo复制了下面的大部分代码。</p><p id="f7bf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还要注意，优化器回调可以具有在。默认属性。optimizer对象的__init__()方法将使用这些默认值来设置超参数值。但是，如果超参数也作为参数传入__init__()方法，则传入的值优先于回调默认值。</p><p id="2279" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">那达慕优化器</strong></p><p id="e19b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Nadam(内斯特罗夫加速自适应矩估计)是adam和NAG的组合。那达慕优化器的参数更新规则是:</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/3df016fbc40c99f861cd9bfbd052d51c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*LPtMMZOTSwqbfYawrbcZrw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">那达慕参数更新规则</figcaption></figure><p id="0348" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在哪里，</p><ul class=""><li id="8335" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js kx jz ka kb bi translated">θt+1和θt是时间步长t+1和t的参数值。</li><li id="a9cf" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kx jz ka kb bi translated">η是学习率。</li><li id="c60d" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kx jz ka kb bi translated">νt是过去平方梯度的指数衰减平均值，νt是其偏差校正估计值。</li><li id="482f" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kx jz ka kb bi translated">mt是过去梯度的指数衰减平均值，mt是其偏差校正估计值。</li><li id="e77c" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kx jz ka kb bi translated">gt是当前的graidient</li><li id="20a9" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kx jz ka kb bi translated">ϵ是一个平滑术语，以避免被0除</li><li id="2f0d" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kx jz ka kb bi translated">β1和β2在FastAI的说法中是mom和sqr_mom</li></ul><p id="12e9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们创建回调来实现Nadam参数更新规则:</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/e84be6fde006f34a05f8b9e44d298100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6BUNwD2szFx90wtINtQ5Pw.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/c305d9d7cbf7a7b902258aaef7f66329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Yg9YZLdvRaBekXHWb6GIA.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/6cd7da09de260b34502b9760535d1b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3oCLWb19UiHgrQtbIYxBg.png"/></div></div></figure><p id="3b16" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意，当damp = (1-mom)时，debias()函数只返回(1- mom**step)。所以，debias1 = (1-mom**step)，debias2 = (1-sqr_mom**step)。我们在nadam_step()中添加了set_trace()来检查我们的优化器。</p><p id="b9e8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面的<em class="kw"> num </em>计算<em class="kw">mom * grad _ avg+(1-mom)* p . grad</em>(即那达慕参数更新规则中的β1*mt + (1-β1)*gt)。请注意，grad_avg (debias1)的偏差校正不适用于此处'<em class="kw"> num </em>的计算。</p><p id="c7b4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，让我们创建一个函数，在添加所需的回调后返回一个Nadam优化器。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/b0994aa9c6b0d8b0433d274043422ec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-58F_qbGRqbFUvW-Rhc-Rg.png"/></div></div></figure><p id="bff1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们使用FastAI方式的测试参数来验证Nadam参数规则(查看FastAI repo中的优化器笔记本)。这个测试的excel实现可以在这里找到<a class="ae iu" href="https://github.com/Rakeshsuku/Medium-Blog/tree/master/Understanding%20FastAI%20v2%20Training" rel="noopener ugc nofollow" target="_blank"/>(文件名:Nadam step test example.xlsx)。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/42b452b25b78d0c80ee4297060b4fb03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTAUSaKK5GEh3o-HrPRWDw.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/8d8e9fa7a6dc8909694c7cf24329347e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ws69YTvJTZ3AwpOVrqcKw.png"/></div></div></figure><p id="ade6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意，权重衰减已经应用(p的值为[0.99，1.98，2.97])。grad_avg和sqr_avg都用0初始化，并且在第一个时间步长具有值(1-mom)*p.grad和(1-sqr_mom)*p.grad。debias1和debias2的值分别为0.1 ( = 1-mom**1)和0.01 ( = 1-sqr_mom**1)。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/f46910c757aef14950309ab8e580d094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjgoTA-5p07HwhG6v3ZxwQ.png"/></div></div></figure><p id="cc87" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面的<em class="kw"> num </em>计算<em class="kw">mom * grad _ avg+(1-mom)* p . grad</em>(即那达慕参数更新规则中的β1*mt + (1-β1)*gt)。让我们在opt.step()操作之后检查参数值。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/e50fca83d6d5ec9bb49b8ea227c334f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60aYPkAiokuSn4lGu0S92g.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/23f375752c4ceb435ec2abd5348de906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sY9TUubQ0wnzUMjWUuBrlA.png"/></div></div></figure><p id="8cd1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意，我们没有在步骤操作后将梯度归零。我们再来一轮吧。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/e297297934d81047667607b73625d5db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73S1eD9RT266tARWQtAC0Q.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/929da7be55ebedc757f75b0915416440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWu7nx7oymagdOKPbXJNcg.png"/></div></div></figure><p id="3799" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们用自己的Nadam优化器实现来拟合我们的模型。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/799aa5761dc1cd2698b7de7d7658b650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TUjCPGLYvu73z9HZTQYZ2Q.png"/></div></div></figure><p id="d8a4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们在下面重新定义了nadam_step()函数来注释掉set_trace()。</p><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/0d71ae1825afa3b0f1673759cce69757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zboUXmkGD-ndhLJlh3I43A.png"/></div></div></figure><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kn"><img src="../Images/7f1c0fbd3511f0f765ff7899c9728b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_13W4ka_SYe4QzyA4X2Ndg.png"/></div></div></figure><p id="65f9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们得到了80.96%的准确性在10个时代与我们的那达慕优化。</p><p id="28fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们学习了如何创建一个定制的FastAI优化器。但是，如果您想在FastAI中使用标准Pytorch优化器，您可以使用<strong class="ix hj"> OptimWrapper </strong>类来包装Pytorch优化器&amp;并在您的学习器中使用它。这里 找同样<a class="ae iu" href="https://docs.fast.ai/tutorial.imagenette#Changing-the-optimizer" rel="noopener ugc nofollow" target="_blank"> <em class="kw">的教程。</em></a></p><p id="e095" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下一篇文章中，我们将研究FastAI学习器&amp;回调&amp;构建我们自己的lr_find()方法的实现。你可以在这里找到这个系列<a class="ae iu" href="https://github.com/Rakeshsuku/Medium-Blog/tree/master/Understanding%20FastAI%20v2%20Training" rel="noopener ugc nofollow" target="_blank">的代码文件。</a></p><p id="0a65" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">本系列其他文章的链接:</strong></p><ul class=""><li id="65c2" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js kx jz ka kb bi translated"><a class="ae iu" rel="noopener" href="/@rakesh.melezhath/understanding-fastai-v2-training-with-a-computer-vision-example-part-1-the-resnet-model-dd9270450bb8"> <em class="kw">研究resnet34模型架构，使用普通Python &amp; PyTorch构建。</em>T25】</a></li><li id="ed7c" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js kx jz ka kb bi translated"><a class="ae iu" rel="noopener" href="/@rakesh.melezhath/understanding-fastai-v2-training-with-a-computer-vision-example-part-3-fastai-learner-and-a3ea85c6fe78"> <em class="kw">学习FastAI学习器和回调&amp;用回调实现学习率查找器(lr_find方法)。</em>T3】</a></li></ul><p id="b6b0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">参考文献:</strong></p><ol class=""><li id="3b4f" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated"><a class="ae iu" href="https://course.fast.ai/" rel="noopener ugc nofollow" target="_blank"> <em class="kw">程序员实用深度学习</em> </a></li><li id="b3f1" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://github.com/fastai/fastai" rel="noopener ugc nofollow" target="_blank"> <em class="kw"> FastAI GitHub回购</em> </a></li><li id="3985" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://github.com/fastai/fastbook" rel="noopener ugc nofollow" target="_blank"> <em class="kw">法泰书</em> </a></li><li id="3221" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://docs.fast.ai/index.html" rel="noopener ugc nofollow" target="_blank"> FastAI文档</a></li><li id="f994" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank"> <em class="kw">梯度下降优化算法概述</em> </a></li><li id="ac47" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> <em class="kw">亚当:一种随机优化的方法</em> </a></li><li id="706e" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://pytorch.org/docs/stable/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="kw"> Pytorch文档</em> </a></li></ol></div></div>    
</body>
</html>