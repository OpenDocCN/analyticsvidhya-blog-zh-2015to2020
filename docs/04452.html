<html>
<head>
<title>Automating Color Image generation from a given Sketch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从给定草图自动生成彩色图像</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/automating-color-image-generation-from-a-given-sketch-22243ae0a339?source=collection_archive---------6-----------------------#2020-03-20">https://medium.com/analytics-vidhya/automating-color-image-generation-from-a-given-sketch-22243ae0a339?source=collection_archive---------6-----------------------#2020-03-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/6baef8c2fe3c6620972e94d73b8c55b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M4sWwf0cLg7CBCJk9vDcmA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">草图和相应的彩色图像(取自Kaggle数据集)</figcaption></figure><div class=""/><p id="e01f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将尝试一些深度学习方法来生成彩色图像。</p><h1 id="e86a" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">目录:</h1><ol class=""><li id="1298" class="kq kr hx iw b ix ks jb kt jf ku jj kv jn kw jr kx ky kz la bi translated">商业问题</li><li id="2073" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">先决条件</li><li id="4d78" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">深度学习的使用</li><li id="2acc" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">现实世界/业务目标和约束</li><li id="ea1c" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">数据来源</li><li id="f687" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">现有方法</li><li id="4cb4" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">丰富</li><li id="db79" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">加载数据:读取图像文件</li><li id="0a9e" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">探索性数据分析</li><li id="dc9d" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">第一次切割溶液</li><li id="04c0" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">监督学习的性能度量</li><li id="7bf1" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">第一个模型-U-net</li><li id="1fef" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">第二种模式——生成对抗网络</li><li id="a889" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">第三种模式——带有“颜色暗示”的生成对抗网络</li><li id="f2ff" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">第四种模式——带有“颜色提示”的U-net</li><li id="4e73" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">模型比较</li><li id="bfa9" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">未来的工作</li><li id="a422" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr kx ky kz la bi translated">参考</li></ol><p id="f2ec" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 1。商业问题</strong></p><p id="e238" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">图像处理中的许多问题涉及将输入图像转换成相应的输出图像。</p><p id="946b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的任务是在<strong class="iw hy">一些深度学习模型的帮助下，从给定的<br/>黑白动漫草图中生成一个兼容的彩色动漫形象。</strong></p><p id="8bc1" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以下是给定草图和彩色图像的一些例子-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lg"><img src="../Images/09db1f783b4ffac85732329d11f7b5cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nPtnk9r6J_83R74kA26F1w.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图一</figcaption></figure><p id="c111" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 2。先决条件</strong></p><p id="33ad" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这篇文章假设读者对视觉任务的深度学习有一定的了解。</p><p id="c43d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 3。深度学习的用途</strong></p><p id="fa81" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将尝试使用U-net和GAN CNN模型生成彩色图像。</p><p id="0b06" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">模型的输入:-草图图像。</p><p id="6315" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">模型输出:-彩色图像</p><p id="f3aa" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 4。现实世界/业务目标和约束。</strong></p><ul class=""><li id="6694" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">没有低延迟要求。</li><li id="570b" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">生成的图像应具有与原始图像相同的颜色，不应模糊。</li></ul><p id="2415" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 5。数据来源</strong></p><p id="c2c3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们从下面的Kaggle链接下载了数据—</p><p id="8b66" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae lp" href="https://www.kaggle.com/wuhecong/danbooru-sketch-pair-128x" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/wuhecong/danbooru-sketch-pair-128x</a></p><p id="775a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 6。现有方法</strong></p><p id="2cbd" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用生成对抗网络和U-net模型解决了一些图像彩色化问题。</p><p id="4132" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 7。改进</strong></p><p id="2049" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我测试了图像生成模型像素级的不同误差指标</p><ul class=""><li id="7f43" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">绝对平均误差</li><li id="1246" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">均方误差</li><li id="328d" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">交叉熵</li></ul><p id="3ebb" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在所有这些损失函数中，在像素级使用平均绝对误差(L1距离)似乎产生了最好结果。</p><p id="24d1" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">并测试了这些方法</p><ul class=""><li id="f387" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">在0到+1的范围内归一化图像，并使用Relu作为图像生成模型的最后一层。</li><li id="4099" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">在-1到+1的范围内归一化图像，并使用Tanh作为图像生成模型的最后一层。</li></ul><p id="7442" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">第二种方法产生了更好的结果。</p><p id="326a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">还测试了通过颜色提示素描图像。</p><p id="3806" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 8。加载数据:读取图像文件</strong></p><p id="286b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">由于图像数据的大小是巨大的，我们分批读取它来训练模型。我们实现了以下逻辑来读取图像文件-</p><p id="f110" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)将所有图像文件的名称加载为“文件夹名+图像名”</p><figure class="lh li lj lk fd hk"><div class="bz dy l di"><div class="lq lr l"/></div></figure><p id="2407" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)然后创建训练、验证和测试集，前20000幅图像用于训练数据，接下来3000幅图像用于验证和测试数据。</p><pre class="lh li lj lk fd ls lt lu lv aw lw bi"><span id="15d6" class="lx jt hx lt b fi ly lz l ma mb">#Generate List of Train, test and validation images.<br/>img_tr  = img_name[0:20000]<br/>img_val = img_name[20000:23000]<br/>img_tst = img_name[23000:26000]</span></pre><p id="da10" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">显示了训练、测试和验证数据的形状</p><pre class="lh li lj lk fd ls lt lu lv aw lw bi"><span id="c761" class="lx jt hx lt b fi ly lz l ma mb">#Length of train, test and validation data.<br/>print('Number of images in train data:',len(img_tr))<br/>print('Number of images in validation data:',len(img_val))<br/>print('Number of images in test data:',len(img_tst))</span><span id="f5e7" class="lx jt hx lt b fi mc lz l ma mb">Number of images in train data: 20000<br/>Number of images in validation data: 3000<br/>Number of images in test data: 3000</span></pre><p id="5e6c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">c)我们编写了一个函数“generate_train_batch”来生成一批训练图像。它使用指针跟踪处理的图像数量。</p><p id="dc4b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">输入:-带有图像名称的数组，要生成的样本数。</p><p id="47e6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在我们实现的函数中</p><ul class=""><li id="6d99" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">我们使用cv2.imread来读取图像。</li><li id="7d75" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">然后使用cv2.cvtColor将图像转换为RGB格式。</li><li id="5c4a" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">还将像素规范化为-1到+1格式。</li></ul><figure class="lh li lj lk fd hk"><div class="bz dy l di"><div class="lq lr l"/></div></figure><p id="24f5" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们使用类似的函数从验证数据中生成一批图像。</p><p id="10e1" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 9。探索性数据分析</strong></p><p id="161a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们有火车设置提供草图和相应的彩色图像。每幅图像都是128 * 128像素。有三层颜色。</p><p id="e911" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们有两个文件夹:-</p><ul class=""><li id="5b14" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">草图—拥有草图图像</li><li id="675a" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">颜色——具有相应的彩色图像。</li></ul><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es md"><img src="../Images/e1a7d60c5fdc61fea036eda8fec6075f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*cOqAhNR4zUclO7RTvix9Qw.png"/></div></div></figure><p id="d9fe" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将从这些文件夹中挑选一些图像来训练模型，然后在模型看不到的一些对上进行测试。</p><p id="d5b0" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 10。第一次切割解决方案</strong></p><p id="cf86" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我已经按照以下步骤实施了解决方案:-</p><p id="8d37" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)首先，我仅采用GAN模型的生成器部分(U-net ),并尝试在像素级以平均绝对误差对其进行优化。并分析结果和检查从模型生成的图像。</p><p id="cc62" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)然后实施完整的GAN模型，并分析和查看生成的图像。</p><p id="ac8d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">c)然后在上面的模型中尝试传递颜色提示，看看它如何提高生成的彩色图像的质量。</p><p id="e889" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 11。监督学习的性能指标</strong></p><p id="54cd" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">模型产生的图像和我们想要产生的图像之间的差距就是损失函数。我们将通过最小化损失函数来训练CNN模型，以便它产生的图像与真实的艺术品相似。</p><p id="8164" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以下是我们用于模型的损失函数—</p><ul class=""><li id="b2f5" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">平均绝对误差/ L1损耗(每像素L1距离)</li><li id="54eb" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">二元交叉熵损失来预测生成的图像是真的还是假的(对于GAN模型)</li></ul><p id="1426" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">12。第一款- U-net </p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es me"><img src="../Images/5256f8d9e907fafed2d48c995a280068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wh04rfv1gLPf5Lu5hthM9g.png"/></div></div></figure><p id="286e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)模型输入:-草图图像(格式128*128*3)</p><p id="2f13" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)模型输出:-彩色图像(格式128*128*3)</p><p id="47e2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">c)建筑</p><p id="9c6e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">所使用的架构类似于编码器-解码器网络。输入通过一系列层，逐步向下采样，直到一个瓶颈层，在这一点上的过程是相反的。这样的网络要求所有信息流通过所有层，包括瓶颈。</p><p id="7393" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于图像彩色化问题，输入和输出图像共享显著边缘的位置。因此，将这些信息从模型的输入直接传送到输出是可取的。</p><p id="6dec" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了给模型提供一种绕过这种信息瓶颈的方法，我们按照“U-Net”的一般形状，在编码器-解码器模型中添加了skip连接。具体来说，我们在第I层和第n I层之间增加跳跃连接，其中n为总层数。每个skip连接只是将I层的所有通道与ni层的通道相连。</p><p id="9eaa" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">总而言之，模型由以下架构组成</p><p id="cc46" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)编码器-解码器:-</p><ul class=""><li id="ba92" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">在该模型中，最初我们在每一层中向下采样图像，直到它到达瓶颈。</li><li id="def7" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">然后再次开始上采样，直到它达到输出图像的大小。</li></ul><p id="2569" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)跳过连接:-</p><p id="3eee" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在每个上采样层，我们给出两个输入:-</p><ul class=""><li id="983f" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">Input_1 :-来自前一层的输出</li><li id="4dea" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">Input_2 :-从生成相同尺寸图像的下采样层输出</li></ul><p id="184a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">d)卷积层的参数:-</p><ul class=""><li id="9c38" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">内核大小:-我们使用5*5的每个内核</li><li id="b009" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">步幅:-我们使用步幅值2</li><li id="da3c" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">填充:-"相同"</li><li id="2c53" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">激活功能:-</li><li id="f642" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">a)下采样Layer-leaky relu(α= 0.2)</li><li id="e801" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">b)上采样层-Relu</li><li id="ce31" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">c)输出层-tanh</li></ul><p id="6187" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">*整流线性装置的泄漏型在装置不工作时允许小的梯度。</p><ul class=""><li id="7b1c" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">权重初始化:-我们用标准偏差为0.02的随机高斯变量初始化所有卷积层。</li><li id="41a6" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">批量归一化:-我们使用动量=0.9的批量归一化。</li><li id="39cc" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">优化器:- Adam(学习率=0.0002，beta_1=0.5)</li><li id="f154" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">损失函数:-我们优化了平均绝对误差/ L1损失(每像素L1距离)</li></ul><p id="5729" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请找到下面的代码:-</p><figure class="lh li lj lk fd hk"><div class="bz dy l di"><div class="lq lr l"/></div></figure><p id="689c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">培训损失与验证损失</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mf"><img src="../Images/40bb26ae92edb49db87f0f13e231ebb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y6up-3XofE0Y63ApHIW8Hw.png"/></div></div></figure><p id="50b5" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">每次迭代后，训练和验证损失都在减少。</p><p id="80d1" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下图显示了模型生成彩色图像在每20个时期后如何改进</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mg"><img src="../Images/d07370adc2411c7b9785d3c9868764f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8wKL9L9b4h3oQqf3b8RBKw.png"/></div></div></figure><p id="8504" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用列车上的最终模型和验证数据生成的彩色图像-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/c350a1d203ec7bcf905230db2681c1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i4K1V5-w8_UdHnrLlhnUAA.png"/></div></div></figure><p id="cf79" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">13。第二种模式——GAN</p><p id="bf4c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">生成对抗模型由两个神经网络组成:-生成模型G和鉴别器模型d。</p><p id="0f76" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">发生器模型被训练以产生输出，该输出不能被反向训练的鉴别器d从“真实”图像中区分出来。鉴别器网络努力区分从训练数据中提取的目标图像和从发生器中提取的目标图像。它产生一个概率值，即生成的图像是真实的训练样本，而不是从模型中抽取的假样本。</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/dca37af64e607e16fd9bd17527922c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hDnlMPR7GShUVkLTaTr3sg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源—<a class="ae lp" href="https://www.oreilly.com/library/view/generative-adversarial-networks/9781789139907/e3cbf7b1-fe09-465d-81ef-2d4fcf0db157.xhtml" rel="noopener ugc nofollow" target="_blank">https://www . oreilly . com/library/view/generative-adversarial-networks/9781789139907/E3 CBF 7 b 1-fe09-465d-81ef-2d 4 fcf 0 db 157 . XHTML</a></figcaption></figure><p id="1326" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">生成和鉴别器模型是卷积层、ReLU和批量归一化的组合。</p><p id="77b9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> a)发电机型号:- </strong></p><p id="90d6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该模型与我们在上一节中讨论的U-net模型相同。唯一的区别是我们不会编译这个单元。</p><p id="a9c8" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">它被训练生成彩色图像，这将欺骗鉴别器，它的真实彩色图像。</p><p id="c9be" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> b)鉴别器型号:- </strong></p><p id="ecc3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">与发生器相比，鉴别器只有编码器单元。它旨在分类输入的草图-图像对是“真”还是“假”。</p><p id="b8e9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">训练鉴别器模型来分类彩色图像是真实的还是生成的彩色图像。训练网络以最大化分类精度。</p><figure class="lh li lj lk fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/b242320cff0b8724620410d5d6adbcd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*boxsnKs2L6J0mpPxbuoIKA.png"/></div></figure><p id="9a9d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以下是鉴别器单元的元件—</p><p id="fd16" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)模型输入:-彩色图像(格式128*128*3)</p><p id="56fa" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)模型输出:-指示生成的彩色图像是真的还是假的概率。真实意味着来自目标域的图像是源图像的真实翻译。</p><p id="27c2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">c)架构:-我们使用了上述U-net/ Generator模型的下采样部分。</p><p id="56c5" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">卷积层的参数:-</p><ul class=""><li id="f161" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">内核大小:-我们使用5*5的每个内核</li><li id="de6d" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">步幅:-我们使用步幅值2</li><li id="c183" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">填充:-"相同"</li><li id="a292" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">激活函数:- LeakyRelu (alpha=0.2)</li><li id="91c5" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">输出图层-展平，然后是Sigmoid。</li><li id="2529" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">权重初始化:-我们用标准偏差为0.02的随机高斯变量初始化所有卷积层。</li><li id="7e47" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">批量归一化:-我们使用动量=0.9的批量归一化。</li><li id="67c6" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">优化器:- Adam(学习率=0.0002，beta_1=0.5)</li><li id="62d5" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">损失函数:-'二元交叉熵'</li></ul><figure class="lh li lj lk fd hk"><div class="bz dy l di"><div class="lq lr l"/></div></figure><p id="0197" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> c)甘单位:- </strong></p><p id="a244" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将上面详细描述的发生器和鉴别器结合起来形成一个GAN单元。</p><p id="03ef" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在GAN单元中，我们仅更新发电机模型的权重。因此，我们将discriminator的可训练参数设置为False，以便在训练该单元时不会更新其权重。</p><p id="97bb" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">discriminator.trainable = False</p><p id="5adc" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">发电机模型被训练来欺骗鉴别器。</p><ul class=""><li id="0219" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">输入:-这个模型有两个输入-</li></ul><p id="8800" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">I)草图图像(128*128*3)</p><p id="88f6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">ii)彩色图像(128 *128*3)</p><ul class=""><li id="6659" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">输出:-来自鉴别器模型的输出，指示彩色图像是真的还是假的概率</li><li id="0b57" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">优化器:- Adam(学习率=0.0002，beta_1=0.5)</li><li id="97c3" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">损失函数-</li></ul><p id="dd71" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">I)我们使用真实彩色图像和通过生成器生成的彩色图像之间的像素级L1距离作为损失函数。</p><p id="4258" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">L1损失会影响与输入匹配的地面真实输出(彩色图像)和可能不匹配的合成输出(生成的彩色图像)之间的距离。</p><figure class="lh li lj lk fd hk"><div class="bz dy l di"><div class="lq lr l"/></div></figure><p id="da2f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">ii)和二进制交叉熵，将图像视为真实的并从鉴别器模型输出。</p><p id="f6de" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">注意:-我们使用加权损失-100 *像素级L1损失+二进制交叉熵。</p><figure class="lh li lj lk fd hk"><div class="bz dy l di"><div class="lq lr l"/></div></figure><p id="524b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> d)模特培训:- </strong></p><p id="5001" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了训练模型，我们在鉴别器上交替一个梯度下降步骤，然后在发生器上交替一个步骤。我们使用小批量随机梯度下降并应用Adam优化器。</p><p id="00b2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，我们通过用真实和虚假图像单独训练鉴别器模型来确定鉴别器模型是否正确。然后，依次训练鉴别模型和对抗模型。</p><p id="710e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">生成模型G的训练目标是增加鉴别器模型D的错误率(即，通过制造看起来来自真实数据分布的新的合成实例来“愚弄”鉴别器)。</p><p id="a46e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在训练GAN模型时，我们在每个时期都做了以下工作</p><p id="6a25" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">I)我们使用了32号浴缸。</p><p id="8769" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">ii)我们仅在交替迭代中训练鉴别器模型。</p><p id="d090" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">iii)训练鉴别器模型-</p><ul class=""><li id="92b4" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">我们生成了一批图像来训练。</li><li id="085f" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">将目标图像视为真实图像来训练鉴别器。</li><li id="87c3" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">然后训练鉴别器，认为生成的图像是假的。</li><li id="1972" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">存储损耗和精确度</li></ul><p id="2c96" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">四)训练甘模型-</p><ul class=""><li id="cc71" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">我们生成了一批训练图像，并在此基础上训练了GAN模型。</li><li id="422a" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">发生器被训练来欺骗鉴别器，并且在L1意义上接近地面真实输出。</li></ul><p id="82ac" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">v)每20个时期后，我们绘制结果并打印生成的图像以跟踪进展。</p><figure class="lh li lj lk fd hk"><div class="bz dy l di"><div class="lq lr l"/></div></figure><p id="1766" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> e)模型结果:- </strong></p><p id="c298" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">发电机损耗-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/14f9683cf24e4c0ab3d0f9cb5dc33c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IbUA7-yQZ6SLGDAYXNv1YA.png"/></div></div></figure><p id="3892" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">训练和验证数据损失似乎都在改善。</p><p id="a4b7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">鉴频器损耗-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/9d9f512534d07ca3985bc891f8b56d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xOQ2A4CJhdXp8K0bMySU5A.png"/></div></div></figure><p id="efa9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">此外，鉴别器模型损耗随着时期的增加而改善。</p><p id="5c76" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">鉴频器精度-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mm"><img src="../Images/2c067958cd3fe00c743e18f00c791e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*Kr_VeCJHUEuAthAURDRzLg.png"/></div></div></figure><p id="5ff6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">随着GAN模型开始学习，鉴别器精度下降，但后来又开始提高。</p><p id="a125" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">发电机精度-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lg"><img src="../Images/56975e79e37fff2309d4ae23f0901bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x_KCBjdoEqcxNOPaiNUgCw.png"/></div></div></figure><p id="7e64" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一段时间后，验证数据的发电机精度开始下降，因为模型当时没有从验证数据中学到太多东西。</p><p id="c344" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下图显示了模型生成彩色图像在每20个时期后如何改进</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mn"><img src="../Images/465b4c03b4e8ff5f9d0617d3b303cfc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lu6qtTAiXxvHcQeHBZJ8FQ.png"/></div></div></figure><p id="ed08" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用经过训练的最终模型生成的图像</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mo"><img src="../Images/12a387ecd853ac7d62989c247ee15152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K2b4GUJrQZqrwP4IrZs5nw.png"/></div></div></figure><p id="a984" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 14。第三种模式——带有“颜色提示”的GAN(生成对抗网络)</strong></p><p id="23f7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于一个看不见的图像，仅仅从草图图像中看到轮廓是很难预测颜色的。因此，我们尝试在草图图像中沿着轮廓边缘传递颜色提示。</p><p id="85b4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以下是一些带有颜色提示的草图图像示例-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mp"><img src="../Images/a389c2d845442b3720b662f377375cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CjrgDrRj-y9_v3P41s7OVg.png"/></div></div></figure><p id="35fd" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了传递颜色提示，我们在目标彩色图像上应用了一个大32px半径的模糊</p><pre class="lh li lj lk fd ls lt lu lv aw lw bi"><span id="55c4" class="lx jt hx lt b fi ly lz l ma mb">X_sketches[i] = 0.5 *(X_sketches[i] + cv2.blur(X_color[i],(32,32)))</span></pre><p id="4b37" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">结果</strong></p><p id="88b3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">发电机损耗-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/9492cbebcc09a21bc3b64ab5e9254d92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*j4uZNlU9tFl4yuDAMFn-5A.png"/></div></figure><p id="d299" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">训练和验证数据损失似乎都在改善。此外，颜色提示提高了验证图像的性能。</p><p id="5295" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">鉴频器损耗-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/4fb9c0a045b5367dd2352ea775aaff02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*4JBk8pTVRAXVE9Hha4kHsQ.png"/></div></figure><p id="e3be" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">鉴别器模型损耗随着时代的增加而不断改善。</p><p id="faf6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">鉴频器精度-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/0940f0fed5d69deac10a06d074317ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*MsvXHelzU96D-vRB5TP8Ew.png"/></div></figure><p id="302b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">鉴别器模型的准确性最初下降，然后随着时期的增加而提高。</p><p id="f9f6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">发电机精度-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div class="er es ms"><img src="../Images/b14f80b350a2f4d0018751bac09841c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*3ouTL5FHPH-WmGH0-M6ftw.png"/></div></figure><p id="4221" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">随着训练和验证数据的出现，发生器的准确性得到提高。</p><p id="9cc3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下图显示了模型生成彩色图像在每20个时期后如何改进</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mt"><img src="../Images/d4c802e20c1f81e6c6ad4076b91469a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-RglGxXC_2HxR1eQ5BXPsw.png"/></div></div></figure><p id="9791" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用经过训练的最终模型生成的图像</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mu"><img src="../Images/b8ba4da27f61484330920cfd08b7068c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dqels77Wh1k5WnUuNQe1UQ.png"/></div></div></figure><p id="b253" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">15。第四种型号—带“颜色提示”的U-net</p><p id="0d6f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在之前的模型中，我们已经使用模糊图像为所有像素提供了颜色提示，这可能很困难。所以，我们试着给目标图像的一些像素颜色提示。</p><p id="149b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以下是一些带有颜色提示的草图图像示例-</p><figure class="lh li lj lk fd hk er es paragraph-image"><div class="er es mv"><img src="../Images/7787fb387e8edbbe7269d4ab7d924b3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*2fNJ1foiIQlx-Udsh7Uo4Q.png"/></div></figure><p id="3063" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们使用下面的代码传递了20个7*7像素的小块的提示</p><pre class="lh li lj lk fd ls lt lu lv aw lw bi"><span id="2bdf" class="lx jt hx lt b fi ly lz l ma mb">#Set of pixels to pass color hints from target image.</span><span id="1b55" class="lx jt hx lt b fi mc lz l ma mb">pix1=[64,32,96,32,96,32,64,64,96,16,32,16,16,64,16,96,112,112,96,80]</span><span id="8b98" class="lx jt hx lt b fi mc lz l ma mb">pix2=[64,32,96,96,32,64,32,96,64,16,16,32,64,16,96,16,112,96,112,80]</span><span id="c1df" class="lx jt hx lt b fi mc lz l ma mb">#Loop below to generate color hints for all images in sample.</span><span id="3e4d" class="lx jt hx lt b fi mc lz l ma mb">for i in range(n_samples):</span><span id="2de1" class="lx jt hx lt b fi mc lz l ma mb">   #Passing hints for 20 patches of 7*7</span><span id="6121" class="lx jt hx lt b fi mc lz l ma mb">   for l in range(20):</span><span id="034c" class="lx jt hx lt b fi mc lz l ma mb">      #Populating color hints for a patch of 7*7</span><span id="d4f2" class="lx jt hx lt b fi mc lz l ma mb">      for j in range(7):</span><span id="17cc" class="lx jt hx lt b fi mc lz l ma mb">         for k in range(7):</span><span id="fd1e" class="lx jt hx lt b fi mc lz l ma mb">             X_sketches[i,pix1[l]+j,pix2[l]+k] =  <br/>               X_color[i,pix1[l]+j,pix2[l]+k]</span></pre><p id="e73f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">结果</strong></p><p id="5f20" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">培训损失与验证损失</p><figure class="lh li lj lk fd hk er es paragraph-image"><div class="er es mv"><img src="../Images/8584bf2f1d94b7a17bc92dcfddc20071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*RyUKjF4VNcdJ_cjC8_4NTg.png"/></div></figure><p id="6888" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">验证损失在第40个纪元之前有所改善，之后一直停滞不前。此外，获得的损失更好的提示。</p><p id="cc8d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下图显示了模型生成彩色图像在每20个时期后如何改进</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mw"><img src="../Images/828d353a886c031912ee9426de1d9ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wy9fDTqOrtM-r2uQwZMTUQ.png"/></div></div></figure><p id="e8e6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用经过训练的最终模型生成的图像</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mx"><img src="../Images/7a638773778c3c1cada6895296e6df88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8KtL5IghIU25noLt_vgrCg.png"/></div></div></figure><p id="a614" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 16。型号对比</strong></p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es my"><img src="../Images/b7e86837d11aee9658d6dbd8a0c207ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xwzxqk_9dnJtvDGZVVA_JA.png"/></div></div></figure><p id="7728" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">通过颜色提示，验证图像的性能有了很大的提高。</p><p id="f75e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 17。未来工作</strong></p><p id="0e7a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们可以尝试在鉴别器模型中使用补丁。</p><p id="ccfd" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">18。参考文献</p><ul class=""><li id="ca49" class="kq kr hx iw b ix iy jb jc jf ll jj lm jn ln jr lo ky kz la bi translated">导师:-<a class="ae lp" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li><li id="c4ab" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated"><a class="ae lp" href="https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/a-gentle-introduction-to-pix 2 pix-generative-adversarial-network/</a></li><li id="4382" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated"><a class="ae lp" href="https://machinelearningmastery.com/upsampling-and-transpose-convolution-layers-for-generative-adversarial-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/up sampling-and-transpose-convolution-layers-for-generative-adversarial-networks/</a></li><li id="075e" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1611.07004v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.07004v1.pdf</a></li><li id="6678" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated"><a class="ae lp" href="https://machinelearningmastery.com/practical-guide-to-gan-failure-modes/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/practical-guide-to-gan-failure-modes/</a></li><li id="effd" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated"><a class="ae lp" href="https://www.kaggle.com/wuhecong/danbooru-sketch-pair-128x" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/wuhecong/danbooru-sketch-pair-128x</a></li><li id="dd5c" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated"><a class="ae lp" href="https://github.com/kvfrans/deepcolor/blob/master/main.py" rel="noopener ugc nofollow" target="_blank">https://github.com/kvfrans/deepcolor/blob/master/main.py</a></li><li id="6286" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated"><a class="ae lp" href="https://towardsdatascience.com/sketch-to-color-anime-translation-using-generative-adversarial-networks-gans-8f4f69594aeb" rel="noopener" target="_blank">https://towards data science . com/sketch-to-color-anime-translation-using-generative-adversarial-networks-gans-8f4f 69594 aeb</a></li><li id="5872" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1808.03240.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1808.03240.pdf</a></li><li id="f63e" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated"><a class="ae lp" href="http://kvfrans.com/coloring-and-shading-line-art-automatically-through-conditional-gans/" rel="noopener ugc nofollow" target="_blank">http://kvfrans . com/coloring-and-shading-line-art-automatically-through-conditional-gans/</a></li><li id="7c5d" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated"><a class="ae lp" href="https://arxiv.org/pdf/1705.01908v2.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1705.01908v2.pdf</a></li><li id="c80c" class="kq kr hx iw b ix lb jb lc jf ld jj le jn lf jr lo ky kz la bi translated">https://arxiv.org/pdf/1705.02999.pdf<a class="ae lp" href="https://arxiv.org/pdf/1705.01908v2.pdf" rel="noopener ugc nofollow" target="_blank"/></li></ul><p id="5e72" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="mz">你可以在</em> <a class="ae lp" href="https://github.com/shridharpr/Anime-Sketch-Colorization-Pair" rel="noopener ugc nofollow" target="_blank"> <em class="mz"> GitHub </em> </a> <em class="mz">上找到我的代码。也可以</em> <a class="ae lp" href="http://priyadarshi.cse@gmail.com" rel="noopener ugc nofollow" target="_blank"> <em class="mz">直接发邮件给我</em> </a> <em class="mz">或者</em> <a class="ae lp" href="http://www.linkedin.com/in/shridhar-priyadarshi-79b96312" rel="noopener ugc nofollow" target="_blank"> <em class="mz">在LinkedIn </em> </a> <em class="mz">上找我。</em></p></div></div>    
</body>
</html>