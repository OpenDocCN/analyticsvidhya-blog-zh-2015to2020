<html>
<head>
<title>Build a Lookalike Logistic Regression Model with SKlearn and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用SKlearn和Keras建立相似的逻辑回归模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/build-lookalike-logistic-regression-model-with-sklearn-and-keras-2b03c540cdd5?source=collection_archive---------5-----------------------#2019-11-05">https://medium.com/analytics-vidhya/build-lookalike-logistic-regression-model-with-sklearn-and-keras-2b03c540cdd5?source=collection_archive---------5-----------------------#2019-11-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/3f1d2ea841f1d187858e5e08c6cb29af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gL1Dv1y8I6NB_fKQ.jpg"/></div></div></figure><div class=""/><blockquote class="iq"><p id="25a8" class="ir is ht bd it iu iv iw ix iy iz ja dx translated"><code class="du jb jc jd je b"><em class="jf">~If you could attenuate to every strand of quivering data, the future would be entirely calculable.~Sherlock</em></code></p></blockquote><ul class=""><li id="7db0" class="jg jh ht ji b jj jk jl jm jn jo jp jq jr js ja jt ju jv jw bi translated">无隐层神经网络，输出层具有sigmoid激活函数。</li></ul><figure class="jy jz ka kb fd hk er es paragraph-image"><div class="er es jx"><img src="../Images/c8fba84cae9169d2ce213e51c8857a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*SPfVfDqNJlcQbqoyuQUuYg.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">具有一个输出层的神经网络</figcaption></figure><ul class=""><li id="1a19" class="jg jh ht ji b jj kg jl kh jn ki jp kj jr kk ja jt ju jv jw bi translated">Sklearn逻辑回归函数</li></ul></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><p id="ea01" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated">每当分类问题出现时，逻辑回归模型在其他分类模型中脱颖而出。逻辑回归是一种线性模型，它将概率分数映射到两个或多个类别。分类过程基于默认阈值0.5。它使用最大似然法给每个变量(系数估计)一个权重，以最大化似然函数。逻辑函数是几率函数对数的指数。</p><figure class="jy jz ka kb fd hk er es paragraph-image"><div class="er es lh"><img src="../Images/37ef56ad4b9960bcb188e695c6b6d8d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*u2S9lUcv7yrnx-fiqHa62A.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated"><strong class="bd li"> p(X) = Pr(Y = 1|X) </strong></figcaption></figure><p id="8115" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated">逻辑回归，可以用几种方法在python中实现，不同的包可以很好地完成这项工作。一种方法是通过使用著名的sklearn包，另一种是通过导入神经网络包Keras。为了在两种方法中获得相似的结果，我们应该改变两种模型中的超参数，以考虑迭代次数、优化技术和要使用的正则化方法。</p><h2 id="a8a0" class="lj lk ht bd li ll lm ln lo lp lq lr ls jn lt lu lv jp lw lx ly jr lz ma mb mc bi translated">参数调整</h2><p id="dc60" class="pw-post-body-paragraph ks kt ht ji b jj md ku kv jl me kw kx jn mf kz la jp mg lc ld jr mh lf lg ja hb bi translated"><strong class="ji hu"> <em class="mi">迭代次数/次数:</em> </strong></p><p id="9d8f" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated">在Keras中，传递给LogisticRegression()的历元数应该= SKlearn的max_iter。后者通常默认为100。</p><pre class="jy jz ka kb fd mj je mk ml aw mm bi"><span id="97f6" class="lj lk ht je b fi mn mo l mp mq">* Solution: nb_epochs = max_iter</span></pre><p id="c5e6" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated"><strong class="ji hu"> <em class="mi">优化器</em> </strong></p><p id="997b" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated">我们在Keras中使用“Adam”(Adaptive Moment estimation o)优化器，而LogisticRegression默认使用liblinear优化器。Sklearn称之为“解算器”。亚当运行梯度和梯度二阶矩的平均值。用简单的英语来说，梯度是为达到一个目标而采取的小步骤，我们的目标是最小化数据代表方程(目标函数)。</p><pre class="jy jz ka kb fd mj je mk ml aw mm bi"><span id="4dd7" class="lj lk ht je b fi mn mo l mp mq">* Solution: KERAS: Optimizer = 'sgd' (stochastic gradient descent) <br/>            SkLearn :Solver = 'sag' (stochastic average gradient descent)</span></pre><p id="8d15" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated">随机平均梯度下降(sag)是一种优化算法，它处理大型数据集，并处理l2(脊)损失或根本不处理损失。这种优化器快速收敛以解决数据的目标函数，只有当所有数据特征偏离相同尺度时才得到保证。这可以通过MinMaxscaler()或任何其他scaler函数获得。在任一模型中的缩放特征，对于在两种情况下获得稳健的相似模型是至关重要的。三个逻辑回归模型将被实例化，以表明如果数据没有缩放，模型的表现不如KERAS版本。</p><p id="4005" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated">随机梯度下降(sgd)是一种迭代优化技术。它是最适合数据集目标函数的梯度的近似值，而不是平均值，其中近似梯度是从整个数据的随机子集获得的。目标函数是尽可能接近描述所解释的基础数据集的通用函数的最佳拟合函数。</p><p id="d3ad" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated">这些算法适用于大型训练集，不存在简单的公式。它们减少了达到合适的最优解所需的计算负担和时间。</p><p id="637e" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated"><strong class="ji hu"> <em class="mi">正规化</em> </strong></p><p id="b731" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated">Sklearn的LogisticRegression默认使用惩罚= L2正则化，在Keras中不进行权重正则化。在Keras中，可以使用每个层的kernel _ regularizer或dropout正则化来正则化权重。</p><pre class="jy jz ka kb fd mj je mk ml aw mm bi"><span id="bc7b" class="lj lk ht je b fi mn mo l mp mq">* Solution: KERAS: kernel_regularizer=l2(0.) <br/>            SkLearn: penalty = l2</span></pre><blockquote class="mr ms mt"><p id="fd1c" class="ks kt mi ji b jj kg ku kv jl kh kw kx mu ky kz la mv lb lc ld mw le lf lg ja hb bi translated">模型数据集</p><p id="8775" class="ks kt mi ji b jj kg ku kv jl kh kw kx mu ky kz la mv lb lc ld mw le lf lg ja hb bi translated">来源:<a class="ae mx" href="https://www.kaggle.com/wendykan/lending-club-loan-data/download%E2%80%9D" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Wendy Kan/lending-club-loan-data/下载</a></p></blockquote><pre class="jy jz ka kb fd mj je mk ml aw mm bi"><span id="f294" class="lj lk ht je b fi mn mo l mp mq">X = model_data[features]<br/>Y = model_data[target]<br/>X_train_select,X_test_select,y_train_select,y_test_select=train_test_split(X,Y,test_size=0.3,random_state= 0)</span><span id="081e" class="lj lk ht je b fi my mo l mp mq"># Feature Scaling<br/>scaler = MinMaxScaler()<br/>scaled_train = scaler.fit_transform(X_train_select) #ONLY FIT to train data!!<br/>scaled_test = scaler.transform(X_test_select)</span></pre><p id="f65b" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated">为了演示使用sklearn和神经网络包Keras构建lookalike LR模型，Lending club的贷款数据用于此目的。在数据清理、空值插补和数据处理之后，使用随机改组对数据集进行分割，以进行训练和测试。</p><h2 id="fd60" class="lj lk ht bd li ll lm ln lo lp lq lr ls jn lt lu lv jp lw lx ly jr lz ma mb mc bi translated">KERAS Sigmoid输出图层(无隐藏图层)</h2><p id="99b7" class="pw-post-body-paragraph ks kt ht ji b jj md ku kv jl me kw kx jn mf kz la jp mg lc ld jr mh lf lg ja hb bi translated">没有隐藏层而只有一个输出层的神经网络简单地由该层中设置的激活函数来定义。例如，如果激活函数是“sigmoid ”,那么预测是基于概率的对数logit，这是与sklearn中线性回归相同的分配可变系数的方法。</p><pre class="jy jz ka kb fd mj je mk ml aw mm bi"><span id="f7aa" class="lj lk ht je b fi mn mo l mp mq">import keras<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.wrappers.scikit_learn import KerasClassifier<br/>from keras.regularizers import l2<br/>from keras.optimizers import SGD</span><span id="ca2b" class="lj lk ht je b fi my mo l mp mq">def build_logistic_regression_model():<br/>    model = Sequential()<br/>    model.add(Dense(units=1,kernel_initializer='glorot_uniform', <strong class="je hu">activation='sigmoid'</strong>,<strong class="je hu">kernel_regularizer=l2(0.)</strong>))<br/>    model.compile(<strong class="je hu">optimizer='sgd'</strong>,<br/>                  loss='binary_crossentropy',<br/>                  metrics=['accuracy'])<br/>    return model<br/>model_logit = KerasClassifier(build_fn=build_logistic_regression_model, batch_size = 10, <strong class="je hu">nb_epoch = 10</strong>)<br/>y_pred_logit = model_logit.predict(scaled_test)<br/>print(classification_report(y_test,y_pred_logit))<br/>print(model_logit.score(scaled_test,y_test)) #Returns the mean accuracy on the given test data and labels.</span><span id="c7b3" class="lj lk ht je b fi my mo l mp mq">*              precision    recall  f1-score   support<br/><br/>           0       0.71      0.34      0.46     85363<br/>           1       0.91      0.98      0.94    592829<br/><br/>    accuracy                           0.90    678192<br/>   macro avg       0.81      0.66      0.70    678192<br/>weighted avg       0.89      0.90      0.88    678192<br/><br/>678192/678192 [==============================] - 65s 95us/step<br/>*Accuracy Score: 0.8998543112427028</span></pre><h2 id="29a0" class="lj lk ht bd li ll lm ln lo lp lq lr ls jn lt lu lv jp lw lx ly jr lz ma mb mc bi translated">SKLearn逻辑回归</h2><p id="77ed" class="pw-post-body-paragraph ks kt ht ji b jj md ku kv jl me kw kx jn mf kz la jp mg lc ld jr mh lf lg ja hb bi translated">正则化包括在模型的不同参数上增加惩罚，以减少模型的自由度。因此，模型将不太可能适合训练数据的噪声，并且将提高模型的泛化能力。对于线性模型，通常有3种类型的正则化:</p><ul class=""><li id="ca6d" class="jg jh ht ji b jj kg jl kh jn ki jp kj jr kk ja jt ju jv jw bi translated">L1正则化(也称为套索):L1 /套索将一些参数缩小到零，因此允许消除特征。</li><li id="14c4" class="jg jh ht ji b jj mz jl na jn nb jp nc jr nd ja jt ju jv jw bi translated">L2正则化(也称为岭):对于l2 /岭，随着惩罚的增加，系数接近但不等于零，因此没有变量被排除！</li><li id="659a" class="jg jh ht ji b jj mz jl na jn nb jp nc jr nd ja jt ju jv jw bi translated">L1/L2正则化(也称为弹性网)</li></ul><p id="aa74" class="pw-post-body-paragraph ks kt ht ji b jj kg ku kv jl kh kw kx jn ky kz la jp lb lc ld jr le lf lg ja hb bi translated">下面，我将举例说明三个LR模型进行比较，并尝试获得尽可能接近Keras版本的精度分数。</p><pre class="jy jz ka kb fd mj je mk ml aw mm bi"><span id="a6da" class="lj lk ht je b fi mn mo l mp mq"><strong class="je hu"># Standard Logistic Regression Model</strong><br/>lrmodel = LogisticRegression()<br/>lrmodel.fit(X_train_select,y_train_select)<br/>predict = lrmodel.predict(X_test_select)<br/>print(classification_report(y_test_select,predict))<br/>print(lrmodel.score(X_test_select, y_test_select))</span><span id="c09f" class="lj lk ht je b fi my mo l mp mq">*               precision    recall  f1-score   support<br/><br/>           0       0.23      0.01      0.02     85363<br/>           1       0.87      1.00      0.93    592829<br/><br/>    accuracy                           0.87    678192<br/>   macro avg       0.55      0.50      0.47    678192<br/>weighted avg       0.79      0.87      0.82    678192<br/><br/>*Accuracy Score: 0.8715260575176351</span><span id="0b85" class="lj lk ht je b fi my mo l mp mq"><strong class="je hu"># Tuned Logistic Regression Model using original dataset</strong><br/>logmodel = LogisticRegression(penalty='l2', solver='sag', max_iter=10)<br/>logmodel.fit(X_train,y_train)<br/>predictions = logmodel.predict(X_test)<br/>print(classification_report(y_test,predictions))<br/>print(logmodel.score(X_test, y_test))</span><span id="408f" class="lj lk ht je b fi my mo l mp mq">*                precision    recall  f1-score   support<br/><br/>           0       0.21      0.01      0.03     85363<br/>           1       0.87      0.99      0.93    592829<br/><br/>    accuracy                           0.87    678192<br/>   macro avg       0.54      0.50      0.48    678192<br/>weighted avg       0.79      0.87      0.82    678192<br/><br/>*Accuracy Score: 0.8694986080637932</span><span id="c486" class="lj lk ht je b fi my mo l mp mq"><strong class="je hu"># Tuned Logistic Regression Model using scaled dataset used for Keras model:<br/></strong>logmodel_scaled = LogisticRegression(<strong class="je hu">penalty='l2', solver='sag', max_iter=10</strong>)<br/>logmodel_scaled.fit(scaled_train,y_train_select)<br/>predictions_scaled = logmodel_scaled.predict(scaled_test)<br/>print(classification_report(y_test_select,predictions_scaled))<br/>print(logmodel_scaled.score(scaled_test, y_test_select))</span><span id="8499" class="lj lk ht je b fi my mo l mp mq">*                precision    recall  f1-score   support<br/><br/>           0       0.75      0.34      0.47     85363<br/>           1       0.91      0.98      0.95    592829<br/><br/>    accuracy                           0.90    678192<br/>   macro avg       0.83      0.66      0.71    678192<br/>weighted avg       0.89      0.90      0.89    678192<br/><br/>*Accuracy Score: 0.9023816264420695</span></pre><blockquote class="mr ms mt"><p id="fb0c" class="ks kt mi ji b jj kg ku kv jl kh kw kx mu ky kz la mv lb lc ld mw le lf lg ja hb bi translated">KERAS准确度得分= 0.8998 VS SKLean准确度得分:0.9023</p><p id="529e" class="ks kt mi ji b jj kg ku kv jl kh kw kx mu ky kz la mv lb lc ld mw le lf lg ja hb bi translated">KERAS F1-得分:0.46/0.94 VS sk lean F1-得分:0.47/0.95</p></blockquote><h1 id="55cc" class="ne lk ht bd li nf ng nh lo ni nj nk ls nl nm nn lv no np nq ly nr ns nt mb nu bi translated">结论</h1><ul class=""><li id="259f" class="jg jh ht ji b jj md jl me jn nv jp nw jr nx ja jt ju jv jw bi translated">在分别调整max_iterations/nb_epochs、solver/optimizer和regulization方法后，sklearn逻辑模型具有与KERAS版本近似的精度和性能。</li><li id="f36c" class="jg jh ht ji b jj mz jl na jn nb jp nc jr nd ja jt ju jv jw bi translated">在KERAS中拟合和测试的缩放数据也应该被缩放以在SKLearn LR模型中拟合和测试。</li><li id="eefc" class="jg jh ht ji b jj mz jl na jn nb jp nc jr nd ja jt ju jv jw bi translated">对两个模型中的边际差异的解释可能是KERAS版本中的batch_size，因为它在SKLearn模型中没有被考虑。</li></ul></div></div>    
</body>
</html>