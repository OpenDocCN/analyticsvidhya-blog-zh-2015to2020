<html>
<head>
<title>Let’s activate your activation(function) in Deep learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们在深度学习中激活你的激活(功能)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/lets-activate-your-activation-function-in-deep-learning-c6f715bcbe57?source=collection_archive---------15-----------------------#2020-01-15">https://medium.com/analytics-vidhya/lets-activate-your-activation-function-in-deep-learning-c6f715bcbe57?source=collection_archive---------15-----------------------#2020-01-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/68c5f8c89fb2ee4e834756d95fe05f62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*_mXT5vgyTMKXh-fw"/></div></figure><p id="f4d7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这个技术时代，深度学习是解决自动化问题的一种非常好的方法。每天产生的数据量非常大。为了正确使用这些数据，我们可以使用机器学习和深度学习来解决许多重大问题。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jk"><img src="../Images/eba81ca536035a157722bd92d32745ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Juy52l8yzdgoTHE_"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">图片来源:<a class="ae jx" href="http://res.cloudinary.com/yumyoshojin/image/upload/v1/pdf/future-data-2019.pdf" rel="noopener ugc nofollow" target="_blank">http://RES . cloud inary . com/yumyoshojin/image/upload/v1/pdf/future-data-2019 . pdf</a></figcaption></figure><p id="b0ab" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">深度学习主要是利用人工神经网络来建立模型和解决问题。首先我们需要知道:</p><h1 id="2271" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">什么是人工神经元？？</h1><p id="e55a" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">人工神经元是<a class="ae jx" href="https://searchenterpriseai.techtarget.com/definition/neural-network" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>中的连接点。像人体的生物神经网络一样，人工神经网络具有分层的架构，每个<a class="ae jx" href="https://searchnetworking.techtarget.com/definition/node" rel="noopener ugc nofollow" target="_blank">网络节点</a>(连接点)都具有处理输入并将输出转发给网络中其他节点的能力。在人工和生物架构中，节点被称为神经元，连接由突触权重来表征，突触权重代表</p><p id="4e9b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这种联系。随着新数据的接收和处理，突触权重发生变化，这就是学习发生的方式。</p><figure class="jl jm jn jo fd ij"><div class="bz dy l di"><div class="lb lc l"/></div></figure><p id="5ebe" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们引入一个新概念，即激活函数，它有助于在给定一个或一组输入的情况下定义节点的输出。一个标准的计算机芯片电路可以被看作是一个由<strong class="io hj">激活功能</strong>组成的数字网络，激活功能可以根据输入“开”(1)或“关”(0)。</p><p id="a8c0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">激活功能就像一个电子开关，向另一个神经元发送响应。有许多类型的激活功能。我们讨论了在观想的深度学习问题中，你需要选择哪一个。</p><p id="6a6e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一个非常依赖激活函数的模型。如果你在一个非线性的问题上应用线性函数，你的模型将会很差并且不精确。</p><p id="ba33" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">激活功能的类型主要有两类:</p><p id="f778" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">1.线性函数</p><p id="5437" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.非线性函数</p><p id="9119" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里我们讨论各种激活函数，如relu，sigmoid，Linear，Tanh。</p><ul class=""><li id="1243" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj li lj lk ll bi translated"><strong class="io hj"> Relu(整流线性激活功能:</strong></li></ul><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/49d0b86f9cab2972a546bd0875192374.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/0*3mjwYTCA4tGuzNAV"/></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/4c2f5f096ad4d27982151cb8451d7e90.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/0*Eqmw6YePELDZ5JMt"/></div></figure><p id="9a88" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">整流线性激活函数是分段线性函数，如果为正，它将直接输出输入，否则，它将输出零。它已经成为许多类型的神经网络的默认激活函数，因为使用它的模型更容易训练，并且通常可以实现更好的性能。</p><ul class=""><li id="f8b4" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj li lj lk ll bi translated"><strong class="io hj">乙状结肠:</strong></li></ul><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/9247ca8329bfc056cf04e91336a31c74.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/0*x7KnkBY2ScLiGr7t"/></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/2cdb53fdcf93122ef171b34162c66bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/0*dPX-K-X4HQ4sCv9E"/></div></figure><p id="2f9d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">它也是一个非线性函数，具有S形曲线，称为sigmoid曲线，函数称为sigmoid函数。</p><p id="6c6c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">sigmoid函数的标准选择是第一张图中所示的逻辑函数，由公式定义。</p><p id="3927" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">它是一个为所有实输入值定义的有界、可微分的实函数，并且在每个点都有一个非负导数。sigmoid“函数”和sigmoid“曲线”指的是同一个对象。</p><ul class=""><li id="c93d" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj li lj lk ll bi translated"><strong class="io hj">双极:</strong></li></ul><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/ef956cc5c916d3300d0c7d2d64dcf51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/0*gYmry6x091mXtbgi"/></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/56f087679cc5bc0e2f6190dea4e21791.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/0*Y8qIxjV1EN4hK6Rg"/></div></figure><p id="7133" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">双极激活函数</strong>用于将一个单元(神经元)的<strong class="io hj">激活</strong>电平转换为输出信号。由于能够将输出信号的幅度范围压缩到某个有限值，它也被称为传递<strong class="io hj">功能</strong>或压缩<strong class="io hj">功能</strong>。</p><ul class=""><li id="86e8" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj li lj lk ll bi translated"><strong class="io hj">双曲正切(TanH): </strong></li></ul><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/b4cfe75cacc1b2ca76f0fc06ed175071.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/0*Yjp1vYHtGhkcsYsE"/></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/498a01e51e8892494e6a374afe2a90c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/0*HWjZ6xAlYZ11VVXg"/></div></figure><p id="2a65" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">双曲正切函数是DL中使用的另一种AF，它在DL应用中有一些变体。双曲正切函数称为双曲正切函数，是一种更平滑的零中心函数，其范围在-1到1之间。</p><ul class=""><li id="77a1" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj li lj lk ll bi translated"><strong class="io hj">指数线性单位，ELU </strong></li></ul><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/ee8cdccdda86a6dad5aceab8655308e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/0*yArm-Q3xrN2UlVk3"/></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/9bc0a897daa32638a1c50488b031ee84.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/0*-UyhaLxGgR-Q9yK6"/></div></figure><p id="fdf3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">指数线性单元</strong>或其广为人知名字<strong class="io hj"> ELU </strong>是一种倾向于更快地将成本收敛到零并产生更准确结果的函数。…对于非负输入，它们都是恒等函数形式。另一方面，<strong class="io hj"> ELU </strong>变得缓慢平滑，直到其输出等于-α，而RELU急剧平滑。</p><ul class=""><li id="9fb5" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj li lj lk ll bi translated">软加软件</li></ul><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/dd01a974fe87e3fdc18e302eea4a6f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/0*dU6vXdH3URZhkUb9"/></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/6b5e7d3000fbf7ec0cdcfde3890bacce.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/0*of92wYV--eKZnGAp"/></div></figure><p id="77b5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">softplus是比sigmoid和tanh更新的函数。这是2001年首次推出的。Softplus是传统函数的替代品，因为它是可微的，而且它的导数很容易证明。</p><figure class="jl jm jn jo fd ij"><div class="bz dy l di"><div class="lu lc l"/></div></figure><ul class=""><li id="7148" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj li lj lk ll bi translated">分段线性</li></ul><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/786fb2e0bf3bac0e70ba7abd9c81269b.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/0*_zuERhTPZ42d6cS0"/></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/67e593a1c50d1dac8c88e5d18794999c.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/0*PPJW9_XLKI47Gs1d"/></div></figure><p id="cde0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在数学和统计学中，<strong class="io hj">分段线性</strong>，PL或分段函数是定义在实数或其一段上的实值函数，其图形由直线段组成。它是一个<strong class="io hj">分段</strong>定义的函数，它的每一段都是一个仿射函数。</p><p id="85cd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这方面的总代码如下</p><figure class="jl jm jn jo fd ij"><div class="bz dy l di"><div class="lw lc l"/></div></figure><h1 id="b475" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论:</h1><p id="8118" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj hb bi translated">所以神经网络激活功能是深度学习的关键组成部分。激活函数决定了深度学习模型的输出，其准确性，以及训练模型的计算效率——这可以建立或破坏一个大规模的神经网络。激活函数对神经网络的收敛能力和收敛速度也有很大影响，或者在某些情况下，激活函数可能会首先阻止神经网络收敛。</p><p id="8aa6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，明智地选择您的激活:</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es if"><img src="../Images/84de2e57ae7aa97f0a71cecb3ae83eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*2ZDSHvvPio-fyp79"/></div></figure><p id="f431" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lx">来源我用:</em> </strong></p><ol class=""><li id="1b82" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj ly lj lk ll bi translated"><a class="ae jx" href="https://github.com/siebenrock/activation-functions" rel="noopener ugc nofollow" target="_blank">https://github.com/siebenrock/activation-functions</a></li><li id="f115" class="ld le hi io b ip lz it ma ix mb jb mc jf md jj ly lj lk ll bi translated">【https://arxiv.org/pdf/1811.03378.pdf T2】号</li><li id="7ac3" class="ld le hi io b ip lz it ma ix mb jb mc jf md jj ly lj lk ll bi translated">谷歌</li><li id="817b" class="ld le hi io b ip lz it ma ix mb jb mc jf md jj ly lj lk ll bi translated">达克达克戈</li></ol></div></div>    
</body>
</html>