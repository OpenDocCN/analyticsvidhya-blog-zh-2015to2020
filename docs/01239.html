<html>
<head>
<title>Neural Machine translation and the need for Attention Mechanism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经机器翻译和对注意机制的需求</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-machine-translation-and-the-need-for-attention-mechanism-60f9a39da9a?source=collection_archive---------5-----------------------#2019-10-10">https://medium.com/analytics-vidhya/neural-machine-translation-and-the-need-for-attention-mechanism-60f9a39da9a?source=collection_archive---------5-----------------------#2019-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="3d8b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">目录:</strong></h1><p id="fcd9" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">1-简介</p><p id="5ff3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">双编码器架构</p><p id="6a2c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">3解码器架构</p><p id="a579" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">4-推理阶段</p><p id="3f25" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">5-问题</p><p id="3209" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">6-什么是注意力？</p><p id="2d48" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">7-实施不同类型的注意机制</p><p id="93e6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">8-注意力的缺点</p><p id="40f5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">9-参考文献</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h2 id="7820" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">1.简介:</h2><p id="af69" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">序列对序列学习(Seq2Seq)完全是关于将序列作为输入并输出序列的模型。这方面有很多例子和应用，但今天我将集中讨论一个具体的应用，即机器语言翻译。例如从英语到印地语。</p><p id="e6d6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">有许多方法可以用来解决这个特殊的任务，但是最先进的技术是编码器-解码器方法(稍后将详细介绍)。</p><p id="ae78" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">从技术上讲，编码器是一个RNN单元，它将一个序列作为输入，并将其编码为一个状态向量，然后将该状态向量与一些输入一起馈送给解码器，以预测所需的输出。</p><p id="50cd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">直观地说，编码器将英语序列总结成一个特殊的向量，该向量被提供给解码器以将它们翻译成等效的印地语输出。</p><p id="ce2f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="lb">需要注意的一点:一般这个RNN单位是LSTM或者gru</em></p><p id="08a5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">所以我们最终的建筑看起来像这样。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lc"><img src="../Images/91727f48cdf5c43f6c28207537348284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u7DHZu5H9xkW-HXTTrDKCA.png"/></div></div></figure><p id="faa5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">与其在这里描述一切，不如让我们一步一步来:</p><h2 id="8551" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">2.编码器架构:</h2><p id="5d36" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我会选择一个LSTM单位来简化生活，但在现实世界中，会使用深层LSTM层，这样模型就可以用新的可能的多种方式学习。</p><p id="36bb" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">就上下文而言，LSTM单元接受三个输入并返回三个输出。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lc"><img src="../Images/3a31831db1bd45e8231c17c44051cc6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDMde46xfjjrX-l40WnPZg.png"/></div></div></figure><p id="e05e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里X、h0和c0是输入。y、ht和ct是输出。上图是LSTM单位的滚动版本。基本上，它一次接受一个单词/字符，并随着时间的推移而展开。下图将会清楚地说明这一点。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lc"><img src="../Images/19699f5a893b50b17c6830a49580c6c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LJRo8Kr65WjcjrvJs0tgSQ.png"/></div></div></figure><p id="1403" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">第一个输入是我们要翻译的序列，另外两个输入是两个向量，单元格状态和隐藏状态。</p><p id="7611" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">例如，让我们看看这个序列。<strong class="jf hj">“这是一部好手机”。</strong>该序列包含5个单词，因此我们的编码器lstm将以单个时间步长处理每个单词。</p><p id="229a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在上图中，x1是输入字“This”，h0、c0是输入状态向量，在开始时被随机初始化。它将输出三个向量y1(输出向量)、h1和c1(状态向量)。</p><p id="056e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">直观地，这里的h1和c1包含我们在时间步t0输入的‘This’单词的信息。现在，在时间步骤t1，LSTM将把h1和c1作为输入，序列中的下一个字<strong class="jf hj">是</strong>。向量h3，c3包括直到字3的信息，字3是<strong class="jf hj">‘a’</strong>。直到上一次步骤5，我们将得到h5和c5，它们包含了整个输入序列的信息。</p><p id="2ae1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">所以现在我们的输入序列<strong class="jf hj">“这是一个好手机”</strong>被转换成向量h5和c5。我们丢弃输出向量(y1到y5 ),因为这里不需要它们。我们只需要输出状态向量，因为它们将包含关于给定输入序列的信息。</p><p id="45bf" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们现在将用这些最终的编码器状态向量h5和c5初始化我们的解码器，而不是像我们用编码器LSTM那样随机初始化。从逻辑上讲，这也是有意义的，因为我们希望我们的解码器不只是随机启动，而是对输入序列有所了解。</p><h2 id="c31e" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">3.解码器架构:</h2><p id="9be9" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">解码器LSTM也将具有与编码器相同的架构，但具有不同的输入和输出。现在有两件事，训练阶段，和推理阶段。我们将在后面谈到推理阶段。首先，完成训练阶段。</p><p id="ca80" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们有LSTM编码器编码的矢量。解码器的h0和c0没有被初始化为随机的，而是用我们从编码器得到的h5和c5初始化。</p><p id="021c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">同样，为了使事情正常进行，我们在目标序列的开始和结束处添加_START_ symbol和_END_ symbol。现在最后的序列变成了<strong class="jf hj">' _ start _यहएकअच्छाफोनहै_ end _ '</strong></p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lc"><img src="../Images/bd985653374d7b791978b5cfe606b0e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wrMWze-_-5hhMo9UcJqTpA.png"/></div></div></figure><p id="72f1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里X1=_Start_和Y1=यह，编码器的h0=h5，编码器的c0=c5。这返回在下一时间步输入到解码器的状态向量h1、c1，并输出Y1，Y1作为基础事实输入到解码器。这种情况一直持续到模型遇到_END_符号。在最后一个时间步，我们忽略解码器的最终状态向量(h6，c6 ),因为这对我们没有用，我们只需要输出Y。</p><p id="5ab3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这种技术也被称为<strong class="jf hj">“老师强迫”。</strong>更多关于这个<a class="ae lo" href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="d2a6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">整个训练架构(编码器+解码器)可以总结在下图中:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lc"><img src="../Images/281cdb04a183b2f88a24a606527c04dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUN57st-P36gAhLJuKIJrg.png"/></div></div></figure><p id="39df" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">给定最终架构，我们现在可以预测每个时间步长的输出，然后将误差通过时间反向传播，以便更新整个网络的参数，然后计算我们的训练损失。</p><h2 id="35c7" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">4.推理阶段:</h2><p id="c014" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">应用经过训练的模型来生成翻译的任务被称为<strong class="jf hj">推理</strong>，或者更常见的是在机器翻译中解码序列。</p><p id="d036" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们有一个训练好的模型，现在我们可以根据给定的输入序列生成预测。这一步基本上被称为推理。你可以称之为测试阶段，而上述步骤是培训阶段。在这一步，我们只有学习的权重和要解码的输入序列。</p><p id="4dba" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">为训练定义的模型已经学习了该操作的权重，但是该模型的结构没有被设计为被递归调用以一次生成一个单词。为此，我们需要设计新的模型来测试我们训练好的模型。有许多方法来执行解码。</p><p id="8d96" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">推理阶段包含两个不同的编码器-解码器模型，它们将作为独立的模型用于各自的目的。</p><p id="9604" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">编码器模型很简单，因为它从训练模型中的编码器获取输入层，并输出隐藏和单元状态张量。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es lp"><img src="../Images/f33904070d90336c4069d9633fc5ba02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/0*-YR1AO_0V2Sbo2oF.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated"><a class="ae lo" href="https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/</a></figcaption></figure><p id="6284" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">解码器模型更复杂。它需要三个输入，来自编码器的隐藏和单元状态作为新定义的编码器模型的初始状态，以及到目前为止编码的翻译输出。对于每个单词，它都将被调用，也就是说，我们必须在一个循环中进行调用，一旦看到_END_符号，循环就会结束。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lu"><img src="../Images/51c44ff5a76fdc55b8e4bf5e9dc0f574.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kUq7DsyDizJeehpM.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated"><a class="ae lo" href="https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/</a></figcaption></figure><p id="7a44" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们将使用这个推理模型最终将序列从一种语言翻译成另一种语言。</p><h2 id="4d53" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">5.问题是:</h2><p id="c1ce" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">【关注】</strong>是深度学习社区最近的趋势之一。Ilya Sutskever是上述seq2seq机器翻译架构背后的人，他提到“注意力机制”是上述编码器-解码器方法中最令人兴奋的进步之一，并且它们将一直存在。但是这种方法背后的问题是什么，注意力解决了什么？</p><p id="11d0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">为了理解注意力能为我们做什么，让我们回顾一下上面同样的机器翻译问题。我们想把“这是一部好手机”这句话翻译给'यह एक अच्छा फोन है'.记得我们用LSTM编码器将英语句子映射成最终状态向量。让我们直观地看看:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lc"><img src="../Images/6e6d0bebc2aa9b8f6ebec47301fb084c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VFvsfcyfEavBa3Iq5rL0nw.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">图8</figcaption></figure><p id="e3c1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们可以看到，向量h5，c5必须编码我们需要知道的关于源句子的一切。它必须完全抓住它的意义。</p><p id="10a7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">但是有一个条件。</p><p id="58e2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">它基本上抓住了整个句子的意思，但这是在句子不长的时候。例如，我们采用的句子只有5个单词长，正常的编码器会公正地对待它，但当句子有50或100个单词长时，这个单一的最终向量将无法映射整个序列。这么看，如果句子长100个单词，那么源句的第一个单词很可能与目标句的第一个单词高度相关。但这意味着解码器必须考虑100步前的信息，并且这些信息需要以某种方式编码到矢量中。RNN有长期依赖的老问题。理论上，像LSTM这样的架构应该能够处理这个问题，但是在实践中，长期依赖仍然是个问题。有一些方法可以让事情变得更好，但它们不是原则性的解决方案。</p><p id="e7a2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="lb">这就是“注意力”的来源。</em></p><h2 id="169b" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated"><strong class="ak"> 6。什么是关注:</strong></h2><p id="0328" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">注意力是对前面方法的一点修改。我们不再试图将完整的源句子编码成最终的固定长度向量。相反，我们共同利用所有中间或局部向量信息，以便在解码目标句子时决定下一个序列。</p><p id="7610" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">例如，在图8中，我们将使用所有的h和c，而不是只使用h5和c5。所以现在，如果我们的解码器想要将“This”解码成'यह'，它可以直接访问第一状态向量h1，c1。这种想法被认为是给予当前单词更多的注意力，因此得名“注意力”。直觉上，你可以认为解码器在产生第一个印地语单词时会注意第一个英语单词，等等。</p><h2 id="716a" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">自定义Keras层的实现:</h2><p id="f53d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">为了实现我们自己的注意力机制，我们需要编写<a class="ae lo" href="https://keras.io/layers/writing-your-own-keras-layers/" rel="noopener ugc nofollow" target="_blank">自定义Keras层</a>。</p><p id="4eb3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">要在Keras中实现它，我们只需要编写三个简单的方法。</p><ul class=""><li id="7105" class="lv lw hi jf b jg kb jk kc jo lx js ly jw lz ka ma mb mc md bi translated">这是你定义体重的地方。这个方法必须在末尾设置<code class="du me mf mg mh b">self.built = True</code>，这可以通过调用<code class="du me mf mg mh b">super([Layer], self).build()</code>来实现。</li><li id="705b" class="lv lw hi jf b jg mi jk mj jo mk js ml jw mm ka ma mb mc md bi translated"><code class="du me mf mg mh b"><strong class="jf hj"><em class="lb">call(x)</em></strong></code>:这是层的逻辑所在。</li><li id="149a" class="lv lw hi jf b jg mi jk mj jo mk js ml jw mm ka ma mb mc md bi translated"><code class="du me mf mg mh b"><strong class="jf hj"><em class="lb">compute_output_shape(input_shape)</em></strong></code>:如果你的层修改了它的输入的形状，你应该在这里指定形状转换逻辑。</li></ul><h2 id="2e56" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">7.实施不同类型的注意机制:</h2><p id="a5c3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">以上解释是顶层观点。我们需要深入挖掘才能完全理解它。自2015年以来，注意力成为解决NLP问题的一个非常流行的概念和工具。近年来，许多进步和修改发生在简单的注意机制之上，导致不同类型的模型。尽管所有不同类型的注意力的基本原理是相同的，但它们的差异主要在于它们的结构和计算。下面我将讨论一些流行的方法，以及它们在Keras中的实现。为了简单起见，我们可以用GRU单位代替LSTM，因为GRU只包含一个隐藏态，而LSTM包含两个隐藏态。</p><p id="2e6d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 7.1-关注由</strong><a class="ae lo" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">T3】巴丹瑙T5】T6:</a></p><p id="bd46" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在我们有了所有编码的状态向量(h1)到(h5)。注意层将把这些编码向量和解码器在先前时间步长的内部状态作为输入。为了预测第一个字本身，解码器没有任何当前的内部状态。为此，我们将编码器的最后状态(即h5)视为先前的解码器状态。使用这些向量，将计算对齐分数，该分数随后将用于计算注意力权重。这将在下面详细讨论:</p><p id="f484" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">给定上下文向量c和先前预测的输出，解码器被训练来预测时间步长t的输出y。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mn"><img src="../Images/daeb46828414b180eae82ca8e8af0907.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*1vyVxTXQOtJJ0bDDUaLTxQ.jpeg"/></div></figure><p id="8571" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里y是先前预测的输出，c是上下文向量。换句话说，我们可以说，解码器正在试图找到上述条件概率。条件概率部分可以建模为:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mo"><img src="../Images/5aa8b029ebf84075af2b6f45268a62b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*-OCee9f3s8G-yBgdVLzQwA.jpeg"/></div></figure><p id="3003" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">其中g是一个非线性的、潜在的多层函数，它输出“yt”的概率，而“st”是GRU/LSTM的解码器隐藏状态，计算如下:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mp"><img src="../Images/2ab1a59f1ee8a9f2af23f232f461b74a.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*gipx2GTs6OBN7xP8nD5nhQ.jpeg"/></div></figure><p id="af2f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里你可以看到，概率取决于不同的向量c，它是每个目标单词的上下文向量。上下文向量ci取决于编码器将输入句子映射到的编码器输出序列(h1，h2，…hTx)。</p><p id="6945" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">然后，上下文向量ci被计算为这些状态向量的加权和。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mq"><img src="../Images/85e798b7fa49a2b98f76507fb864f475.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*ZIfwwcP7DZthx53wV2sgLg.jpeg"/></div></figure><p id="d027" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">如果我们展开上面的五个单词的例子的公式，我们将得到:</p><blockquote class="mr ms mt"><p id="9dd3" class="jd je lb jf b jg kb ji jj jk kc jm jn mu kd jq jr mv ke ju jv mw kf jy jz ka hb bi translated"><em class="hi">context _ vector =(</em>α<em class="hi">1 * h1+</em>α<em class="hi">2 * H2+</em>α<em class="hi">3 * H3+</em>α<em class="hi">4 * H4+</em>α<em class="hi">5 * H5)</em></p></blockquote><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mx"><img src="../Images/646212e4c168f1ab32196800af4092df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*N0EcRKVjkHsPQhbrfm-9QQ.jpeg"/></div></figure><p id="d1e7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">是一个对齐模型，它对位置j周围的输入和位置I处的输出的匹配程度进行评分。分数基于解码器GRU/LSTM隐藏状态si1和输入句子的第j个输出hj。这里的“a”是一个简单的前馈神经网络，它与所提出的系统的所有其他组件一起被联合训练。这些比对模型eij也被称为能量分数。</p><p id="262d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">为了计算energy _ score/allignment _ score t<a class="ae lo" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">,论文</a>引入了三个权重矩阵W_combined、W_decoder、W_encoder。下面的公式将使它变得清楚。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es my"><img src="../Images/b59e4c35d74ca0b9672cdc5e57eeeaca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xc3rFbbvFXfD13WfXA1wEw.jpeg"/></div></div></figure><p id="bc17" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们在AttentionLayer类的build函数中定义这些权重。</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="d089" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们已经定义了计算alignment_score所需的权重。现在让我们来定义这个类的调用函数，代码的实际计算和逻辑将驻留在这个类中。</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="34a2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">关注层最终会返回allignment_score和关注权重。然后，权重将用于计算最终的上下文向量。</p><pre class="ld le lf lg fd nb mh nc nd aw ne bi"><span id="4c5d" class="kn ig hi mh b fi nf ng l nh ni">context_vector = attention_wts * enc_output_seq</span></pre><p id="e91c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们最后的注意力层类定义如下:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="a9ab" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 2 -关注由</strong><a class="ae lo" href="https://arxiv.org/abs/1508.04025" rel="noopener ugc nofollow" target="_blank">T5】梁 </a> <strong class="jf hj"> : </strong></p><p id="9010" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这一机制是Thang Luong在Bahdanau机制之后于2015年推出的。它建立在以前的机制之上，有一些不同之处。</p><ol class=""><li id="e3c6" class="lv lw hi jf b jg kb jk kc jo lx js ly jw lz ka nj mb mc md bi translated">计算校准分数的方式。</li><li id="60c0" class="lv lw hi jf b jg mi jk mj jo mk js ml jw mm ka nj mb mc md bi translated">解码器中引入注意机制的位置。</li></ol><p id="e966" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在Bahdnau注意力中，注意力层将前一时间步的编码输入向量和解码器隐藏向量作为输入。然后使用它计算上下文向量。使用该上下文向量预测输出。但是在Luong的注意中，上下文向量仅在RNN/LSTM产生该时间步长的输出之后才被使用。在解码器级，先前的解码器隐藏状态和解码器输出通过<strong class="jf hj">解码器RNN </strong>传递，以生成该时间步长的<strong class="jf hj">新隐藏状态</strong>。使用新的解码器隐藏状态和编码器隐藏状态，计算<strong class="jf hj">对齐分数</strong>。</p><p id="0ef1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">与只有一种类型的Bahdanau的方法相比，共有三种类型的方法来计算比对分数。这些类型是:</p><h2 id="08c5" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">圆点:</h2><p id="1661" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这个函数中，我们只需要解码器的隐藏状态和编码器的隐藏状态来计算对齐分数。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nk"><img src="../Images/9cd6d5d5924e792a6c40755598348faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eoTa2FQjwMnBfCIoeZ1Ppw.jpeg"/></div></div></figure><h2 id="a215" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">常规:</h2><p id="0e22" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这类似于点函数，除了在点函数之后有一个权重矩阵。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es nl"><img src="../Images/191d6ad14fd344100176f5e2e8ba5f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*Y3GmBHlYJe9Q6i0hECACYg.jpeg"/></div></figure><h2 id="6087" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">连接:</h2><p id="8737" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">该函数以下列方式计算分数。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es nm"><img src="../Images/611f4b104e4f320fd365b06852dc669c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*tSFqwEhjpkDRXizJJwxKLQ.jpeg"/></div></figure><p id="02f4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里H_decoder是通过传递先前的隐藏状态和解码器输出而生成的新的隐藏状态。</p><p id="76b7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">下面是实施Luong注意事项:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="3204" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 8。注意力的缺点:</strong></p><p id="1c34" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">如上一段所述，它为每个单词计算context_vector，这是有计算成本的。序列越长，训练的时间就越长。同样，如果你看到人类的注意力是为了节省计算资源。专注于一件事，我们可能会忽略许多其他事情。但这并不是我们在上面的模型中所做的。在决定关注什么之前，我们本质上是在关注每件事情的细节。从直觉上来说，这相当于输出一个翻译的单词，然后回过头来检查文本的所有内存，以决定下一个单词是哪个。这似乎是一种浪费，根本不是人类正在做的事情。事实上，它更类似于内存访问，而不是注意力，在我看来这有点用词不当。尽管如此，这并没有阻止注意力机制变得非常流行，并在许多任务中表现出色。</p><h2 id="e7ac" class="kn ig hi bd ih ko kp kq il kr ks kt ip jo ku kv it js kw kx ix jw ky kz jb la bi translated">9.参考资料:</h2><p id="d2b1" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">1-<a class="ae lo" href="https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/</a></p><p id="c0fb" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">https://github.com/tensorflow/nmt2-<a class="ae lo" href="https://github.com/tensorflow/nmt" rel="noopener ugc nofollow" target="_blank"/></p><p id="127d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">3-<a class="ae lo" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/tutorials/text/NMT _ with _ attention</a></p><p id="73dc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">4-<a class="ae lo" href="https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/</a></p></div></div>    
</body>
</html>