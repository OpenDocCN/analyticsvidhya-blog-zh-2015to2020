<html>
<head>
<title>Implementing Word2Vec in Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Tensorflow中实现Word2Vec</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-word2vec-in-tensorflow-44f93cf2665f?source=collection_archive---------0-----------------------#2019-06-22">https://medium.com/analytics-vidhya/implementing-word2vec-in-tensorflow-44f93cf2665f?source=collection_archive---------0-----------------------#2019-06-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b890" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据维基百科的说法，“<strong class="ih hj"> Word2vec </strong>是一组相关的模型，用于产生<a class="ae jd" href="https://en.wikipedia.org/wiki/Word_embedding" rel="noopener ugc nofollow" target="_blank">单词嵌入</a>”。Word2vec是Google发布的一个非常强大的模型，用于在保持上下文关系的同时在特征空间中表示单词。</p><p id="cdbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“一个人可以通过他交的朋友来识别”，同样，一个单词可以通过经常使用的一组单词来识别，这就是Word2Vec所基于的思想。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/9ebff9f94edc8676078666e314b8a872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hELlVp9hmZbDZVFstS61pg.png"/></div></div></figure><p id="4e93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章需要对神经网络有透彻的理解，使用下面的文章快速回顾一下。</p><h2 id="a4be" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated"><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2018/10/introduction-neural-networks-deep-learning/" rel="noopener ugc nofollow" target="_blank">深度学习和神经网络入门指南</a></h2><h2 id="9eb3" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated"><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/" rel="noopener ugc nofollow" target="_blank">用Python和R从零开始理解和编码神经网络</a></h2><h2 id="2e10" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated"><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" rel="noopener ugc nofollow" target="_blank">对单词嵌入的直观理解:从计数向量到Word2Vec </a></h2><p id="1c27" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">目录:-</p><ol class=""><li id="31d4" class="kq kr hi ih b ii ij im in iq ks iu kt iy ku jc kv kw kx ky bi translated">什么是单词嵌入？</li><li id="35c3" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">连续词袋模型</li><li id="8088" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">跳过Gram模型</li><li id="9a28" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">履行</li><li id="bf53" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">形象化</li><li id="d0de" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">结论</li></ol><p id="0046" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是单词嵌入？</strong></p><p id="d15b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">任何处理文本数据的算法都需要一些数字形式的单词表示，因为计算机不能直接理解文本(截至目前)。因此，需要将输入的单词转换成算法可以理解的形式，最流行的方法之一是one hot编码，其中每个单词都被表示为在词汇表中其位置包含1的向量。</p><p id="e008" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，假设我们的语料库是一个单句“狐狸太懒了”。我们的词汇是['该'，'狐狸'，'是'，'太'，'懒']。现在最热门的单词编码是，</p><p id="19bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">fox -&gt; [0，1，0，0，0] the -&gt; [1，0，0，0，0] is -&gt;[0，0，1，0，0] too -&gt;[0，0，0，0，1，0] lazy-&gt;[0，0，0，0，0，1]</p><p id="9666" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种编码的问题是它们不能捕捉不同单词之间的关系，因为所有的向量都是独立的。任何两个热编码矢量之间的相似性(余弦)将总是0。此外，一个热编码可以显著增加数据集的维度，因为词汇表中的每个单词都被视为一个单独的特征。因此，我们需要一种表示法，使得相似的单词有相似的表示法，这就是Word2Vec出现的原因。直观地说，Word2Vec试图根据通常在其附近出现的其他单词来学习每个单词的表示，因此Word2Vec能够捕捉单词之间的上下文关系。</p><p id="c35e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">著名的国王王后男人的例子——如果我们考虑由Word2Vec产生的国王和王后的向量表示的差异，那么产生的向量非常类似于男人和女人的差异，这意味着嵌入包含关于性别的信息。</p><p id="3a17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Word2Vec有两个变体，一个基于Skip Gram模型，另一个基于连续单词包模型。</p><h1 id="ac4d" class="le jr hi bd js lf lg lh jw li lj lk ka ll lm ln kd lo lp lq kg lr ls lt kj lu bi translated"><strong class="ak">连续单词袋模型</strong></h1><p id="d637" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">在连续单词包模型中，我们尝试使用单词周围的单词(上下文单词)来预测单词，模型的输入是窗口大小内的上下文单词的一个热编码向量，窗口大小是一个超参数，指的是任一侧的上下文单词的数量(在当前单词之前和之后出现的单词)。)来预测它。</p><p id="7464" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">窗口大小</strong></p><p id="fae3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们举一个例子。</p><p id="3cb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“狐狸太懒了，什么都不会做。”。假设考虑中的单词是“lazy”，现在对于窗口大小为2，输入向量将在对应于单词“is”、“too”、“to”和“do”的位置具有1。</p><h1 id="ba0f" class="le jr hi bd js lf lg lh jw li lj lk ka ll lm ln kd lo lp lq kg lr ls lt kj lu bi translated">跳过Gram模型</h1><p id="7e74" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">在skip gram模型中，我们试图找出在所考虑的单词的窗口大小内出现的上下文单词。在下一节中，我们将尝试实现skip gram模型。</p><p id="d0dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">跳过gram模型如下进行</p><ol class=""><li id="34c2" class="kq kr hi ih b ii ij im in iq ks iu kt iy ku jc kv kw kx ky bi translated">实现3层神经网络(输入、隐藏和输出)</li><li id="0725" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">用于训练的输入数据是所考虑的单词的一个热编码向量，类似地，输出是落入窗口大小内的单词的一个热编码向量。例如，假设我们的语料库是句子，“敏捷的狐狸跳过了懒惰的狗。”。那么跳过gram模型的训练向量是</li></ol><p id="699e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">['狐狸'，'快']，['狐狸'，'跳了']，['狐狸'，'该']，['狐狸'，'过']等等。</p><p id="145c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.在训练之后，为了获得单词的表示，只需将单词的一个热编码向量作为输入，并且该表示是隐藏层的输出。</p><p id="47b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相同的模型可以用于训练跳格和连续单词袋模型，只有输入和输出训练向量被交换。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lv"><img src="../Images/b72f1afc4a1966505fa691071c911be8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D5TeVaDJb0m9_G3P7XVMdg.png"/></div></div></figure><h1 id="bcae" class="le jr hi bd js lf lg lh jw li lj lk ka ll lm ln kd lo lp lq kg lr ls lt kj lu bi translated">履行</h1><p id="1743" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">首先让我们导入必要的库</p><pre class="jf jg jh ji fd lw lx ly lz aw ma bi"><span id="87f4" class="jq jr hi lx b fi mb mc l md me">import numpy as np<br/>import tensorflow as tf<br/>import re<br/>import nltk<br/>import sys<br/>from collections import OrderedDict<br/>from sklearn.manifold import TSNE<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><p id="75e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们来阅读作为我们训练语料的文件，这里我们使用了《哈利波特与魔法石》。</p><pre class="jf jg jh ji fd lw lx ly lz aw ma bi"><span id="cdff" class="jq jr hi lx b fi mb mc l md me">file = open("J. K. Rowling - Harry Potter 1 - Sorcerer's Stone.txt",'r')<br/>raw_data_1 = file.read()<br/>file.close()</span></pre><p id="6c80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将把语料库拆分成单词，并执行一些清理步骤来删除数字和标点符号等。</p><pre class="jf jg jh ji fd lw lx ly lz aw ma bi"><span id="d75f" class="jq jr hi lx b fi mb mc l md me">words = raw_data_1.split()<br/>words = [ word.lower() for word in words if len(word)&gt;1 and word.isalpha()]<br/>vocab = set(words)<br/>char_to_int = dict((c,i) for i,c in enumerate(vocab))<br/>int_to_char = dict((i,c) for i,c in enumerate(vocab))</span></pre><p id="d2e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">char_to_int是一个字典，它将我们词汇表中的每个单词映射到一个唯一的数字，int_to_char实现了相反的映射。</p><pre class="jf jg jh ji fd lw lx ly lz aw ma bi"><span id="fc67" class="jq jr hi lx b fi mb mc l md me">X = []<br/>Y = []<br/>temp_dict = []<br/>window_size = 10<br/>for i in range(len(words)):<br/>    a = i-window_size<br/>    b= i+window_size<br/>    curr_word = words[i]<br/>    for z in range(a,i):<br/>        if z &gt;=0:<br/>            temp_dict.append((curr_word,words[z]))<br/>    for z in range(i+1,b):<br/>        if z&lt;len(vocab):<br/>            temp_dict.append((curr_word,words[z]))</span><span id="5e6c" class="jq jr hi lx b fi mf mc l md me">for pair in temp_dict:<br/>    tempx = np.zeros(len(vocab))<br/>    tempy = np.zeros(len(vocab))<br/>    tempx[char_to_int[pair[0]]] = 1<br/>    tempy[char_to_int[pair[1]]] = 1<br/>    X.append(tempx)<br/>    Y.append(tempy)</span></pre><p id="12d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">X和Y具有我们各自的训练输入和输出向量，X中的每个向量在所考虑的单词的位置处具有1，而Y在对应于上下文单词的位置处具有1，window_size用于调节要使用的上下文单词的数量。</p><pre class="jf jg jh ji fd lw lx ly lz aw ma bi"><span id="cdb6" class="jq jr hi lx b fi mb mc l md me">embedding_size = 1000<br/>batch_size = 64<br/>epochs = 32 <br/>n_batches = int(len(X)/batch_size)<br/>learning_rate= 0.001</span><span id="f3ad" class="jq jr hi lx b fi mf mc l md me">x = tf.placeholder(tf.float32,shape = (None,len(vocab)))<br/>y = tf.placeholder(tf.float32,shape = (None,len(vocab)))</span><span id="d414" class="jq jr hi lx b fi mf mc l md me">w1 = tf.Variable(tf.random_normal([len(vocab),embedding_size]),dtype = tf.float32)<br/>b1 = tf.Variable(tf.random_normal([embedding_size]),dtype = tf.float32)<br/>w2 = tf.Variable(tf.random_normal([embedding_size,len(vocab)]),dtype = tf.float32)<br/>b2 = tf.Variable(tf.random_normal([len(vocab)]),dtype = tf.float32)</span><span id="2aeb" class="jq jr hi lx b fi mf mc l md me">hidden_y = tf.matmul(x,w1) + b1<br/>_y = tf.matmul(hidden_y,w2) + b2</span><span id="de79" class="jq jr hi lx b fi mf mc l md me">#print(b.dtype)</span><span id="1d3d" class="jq jr hi lx b fi mf mc l md me">#_y = tf.matmul(x,w)</span><span id="dc42" class="jq jr hi lx b fi mf mc l md me">cost = tf.reduce_mean(tf.losses.mean_squared_error(_y,y))<br/>optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><span id="9912" class="jq jr hi lx b fi mf mc l md me">print(y.shape)<br/>print(_y.shape)</span><span id="4479" class="jq jr hi lx b fi mf mc l md me">init = tf.global_variables_initializer()<br/>init_l = tf.local_variables_initializer()<br/>saver = tf.train.Saver()<br/>gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.33)<br/>sess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options))<br/>sess.run(init)</span></pre><p id="e27e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们这里的嵌入长度是1000，即每个单词将被表示为长度为1000的向量。我们用一个隐藏层定义了我们的简单模型。</p><pre class="jf jg jh ji fd lw lx ly lz aw ma bi"><span id="1d42" class="jq jr hi lx b fi mb mc l md me">for epoch in range(5):<br/>    avg_cost = 0<br/>    for i in range(n_batches-1):<br/>        batch_x = X[i*batch_size:(i+1)*batch_size]<br/>        batch_y = Y[i*batch_size:(i+1)*batch_size]<br/>        #print(batch_x.shape)<br/>        _,c = sess.run([optimizer,cost],feed_dict = {x:batch_x,y:batch_y})<br/>        #print(test.shape)<br/>        <br/>        avg_cost += c/n_batches<br/>    print('Epoch',epoch,' - ',avg_cost)<br/>save_path = saver.save(sess,'/home/temp/w2v/word2vec_weights_all.ckpt')</span></pre><p id="5f86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型被训练5个时期，并且相应的权重也被保存。</p><pre class="jf jg jh ji fd lw lx ly lz aw ma bi"><span id="c0ac" class="jq jr hi lx b fi mb mc l md me">embeddings = dict()<br/>for i in vocab:<br/>    temp_a = np.zeros([1,len(vocab)])<br/>    temp_a[0][char_to_int[i]] = 1<br/>    temp_emb = sess.run([_y],feed_dict = {x:temp_a})<br/>    temp_emb = np.array(temp_emb)<br/>    #print(temp_emb.shape)<br/>    embeddings[i] = temp_emb.reshape([len(vocab)])<br/>    #print(embeddings[i].shape)</span></pre><p id="e975" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，嵌入字典将词汇表中的每个单词都映射到它的向量表示中。</p><pre class="jf jg jh ji fd lw lx ly lz aw ma bi"><span id="f351" class="jq jr hi lx b fi mb mc l md me"><br/>def closest(word,n):<br/>    distances = dict()<br/>    for w in embeddings.keys():<br/>        distances[w] = cosine_similarity(embeddings[w],embeddings[word])<br/>    d_sorted = OrderedDict(sorted(distances.items(),key = lambda x:x[1] ,reverse = True))<br/>    s_words = d_sorted.keys()<br/>    print(s_words[:n])</span></pre><p id="fe57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">closest函数将任意单词和n作为输入，查找n个最相似的单词。</p><p id="a450" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">可视化</strong></p><p id="62f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们用SNE霸王龙来想象我们的表现。</p><pre class="jf jg jh ji fd lw lx ly lz aw ma bi"><span id="8d87" class="jq jr hi lx b fi mb mc l md me">labels = []<br/>tokens = []<br/>for w in embeddings.keys():<br/>    labels.append(w)<br/>    tokens.append(embeddings[w])</span><span id="e08c" class="jq jr hi lx b fi mf mc l md me">tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)</span><span id="e022" class="jq jr hi lx b fi mf mc l md me">new_values = tsne_model.fit_transform(tokens)</span><span id="d661" class="jq jr hi lx b fi mf mc l md me">x = []<br/>y = []<br/>for value in new_values:<br/>    x.append(value[0])<br/>    y.append(value[1])<br/>    <br/>plt.figure(figsize=(16, 16)) <br/>for i in range(len(x)):<br/>    plt.scatter(x[i],y[i])<br/>    plt.annotate(labels[i],<br/>                     xy=(x[i], y[i]),<br/>                     xytext=(5, 2),<br/>                     textcoords='offset points',<br/>                     ha='right',<br/>                     va='bottom')<br/>plt.show()</span></pre><p id="5443" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将在特征空间中用它们之间的相应距离来绘制我们的单词。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mg"><img src="../Images/4307be021d7f17080ae208d80405c8b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XvnncJy2H6sWD184vSM1_A.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">词汇的t-SNE视觉化</figcaption></figure><p id="4248" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结论</strong></p><p id="47b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文的目的是提供对单词嵌入和用于生成它们的模型的直观理解。我希望你读完这篇文章后理解单词嵌入。您可以使用自己的语料库来创建嵌入长度和窗口大小不同值的嵌入和实验，将它们用于情感分类等任务。</p><p id="7470" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请务必阅读Word2Vec论文，并前往Github repo获取完整代码。</p><p id="3a9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文:-【https://arxiv.org/abs/1301.3781】T4</p></div></div>    
</body>
</html>