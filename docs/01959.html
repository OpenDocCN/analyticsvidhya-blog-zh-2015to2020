<html>
<head>
<title>MLPerf: Getting your feet wet with benchmarking ML workloads</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MLPerf:尝试ML工作负载基准测试</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mlperf-getting-your-feet-wet-with-benchmarking-ml-workloads-6ecd57620ac6?source=collection_archive---------2-----------------------#2019-11-24">https://medium.com/analytics-vidhya/mlperf-getting-your-feet-wet-with-benchmarking-ml-workloads-6ecd57620ac6?source=collection_archive---------2-----------------------#2019-11-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/59c4bd6777948a4aadbfb40afda7125b.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*ASQ4XjFGOIrA4wlXp8123A.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">用于测量ML硬件、软件和服务的训练和推理性能的基准套件</figcaption></figure><p id="8286" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文涵盖了设置和运行MLPerf培训基准之一所涉及的步骤。这将为读者提供一个基本的理解，即如何通过利用以前的提交者所做的工作来有效地开始使用MLPerf。MLPerf正在成为比较不同类型的专用基础设施或软件框架的有趣实验的事实上的ML工作负载。这篇文章还将使那些希望提交自己的MLPerf结果的人熟悉运行基准测试所涉及的步骤。无论最终目标是什么，让我们从如何开始在您的硬件上执行MLPerf基准开始:无论是在本地还是在云中。</p><p id="2901" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本系列的第一篇文章中，我们将介绍如何在GPU服务器上运行MLPerf训练基准之一。训练复杂的深度神经网络(DNN)模型需要大量的计算资源才能在合理的时间内完成。MLPerf培训基准套件、提交和审核流程的全面概述在MLPerf社区发布的文章中有所描述。</p><p id="5891" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">被测系统(硬件环境)</em>:我使用一台配备了4个NVIDIA Tesla V100 PCIe GPU加速器的GPU服务器来演示如何运行其中一个MLPerf训练基准。</p><p id="ca57" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">软件环境:</em> MLPerf要求所有基准提交使用容器来打包所有软件依赖项，并提供脚本来下载和准备数据集。这使得社区可以轻松地在其他系统上复制或运行基准测试。如果你不熟悉docker，有很多资源和教程可以帮助你开始学习，比如微软ML团队的<a class="ae jp" href="https://blogs.technet.microsoft.com/machinelearning/2018/03/15/demystifying-docker-for-data-scientists-a-docker-tutorial-for-your-deep-learning-projects/" rel="noopener ugc nofollow" target="_blank">教程</a>。</p><p id="5787" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基准测试可以通过以下步骤运行:</p><ol class=""><li id="956e" class="jq jr hi is b it iu ix iy jb js jf jt jj ju jn jv jw jx jy bi translated">设置docker和软件对被测系统的依赖性。网上有各种资源可以做这件事，对于GPU服务器我必须安装Docker和Nvidia-Docker。一些基准可能有额外的设置，在他们的阅读材料中提到。</li><li id="565e" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn jv jw jx jy bi translated">从ml perf GitHub repo:<a class="ae jp" href="https://github.com/mlperf" rel="noopener ugc nofollow" target="_blank">https://github.com/mlperf</a>下载基准测试软件库，其中包括运行基准测试所需的代码、脚本和文档</li><li id="2f69" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn jv jw jx jy bi translated">使用基准目录中提供的脚本下载并验证数据集。这是在docker之外的被测系统上运行的。</li><li id="b9ed" class="jq jr hi is b it jz ix ka jb kb jf kc jj kd jn jv jw jx jy bi translated">使用每个基准测试中包含的脚本和指令，构建并运行docker映像。每个基准将运行，直到达到目标质量，然后停止，打印计时结果和附加信息将被捕获在一个日志文件中。</li></ol><blockquote class="ke"><p id="d327" class="kf kg hi bd kh ki kj kk kl km kn jn dx translated">运行培训基准的逐步说明</p></blockquote><p id="2cc7" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">首先，克隆最新的MLPerf培训结果存储库，如下所示。对于训练基准，建议使用现有结果提交之一(<em class="jo"> training_results_v0.5 </em>或<em class="jo"> training_results_v0.6 </em>)，而不是mlperf/training存储库中提供的参考实现。这是因为参考代码是alpha版本，并不打算用于软件框架或硬件的实际性能测量。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="54c3" class="lc ld hi ky b fi le lf l lg lh"><strong class="ky hj">git clone </strong><a class="ae jp" href="https://github.com/mlperf/training_results_v0.6.git" rel="noopener ugc nofollow" target="_blank"><strong class="ky hj">https://github.com/mlperf/training_results_v0.6.git</strong></a></span></pre><p id="7b7d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，让我们研究下载的代码，并找到下载数据集、构建和运行docker容器等的脚本。在顶层，有每个供应商提交(谷歌，英特尔，英伟达等)的目录，其中包含用于生成他们提交的结果的代码和脚本。我们将重点关注NVIDIA提交，因为我们希望在NVIDIA GPUs上运行基准测试</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="d4b2" class="lc ld hi ky b fi le lf l lg lh">$/home/training_results_v0.6$<strong class="ky hj"> ls</strong><br/>Alibaba  CONTRIBUTING.md  Fujitsu  Google  Intel  LICENSE  NVIDIA  README.md</span><span id="e971" class="lc ld hi ky b fi li lf l lg lh">$/home/training_results_v0.6$ <strong class="ky hj">cd NVIDIA</strong></span><span id="e36f" class="lc ld hi ky b fi li lf l lg lh">$/home/training_results_v0.6/NVIDIA$ <strong class="ky hj">ls</strong><br/>benchmarks  LICENSE.md  README.md  results  systems</span><span id="4a90" class="lc ld hi ky b fi li lf l lg lh">$/home/training_results_v0.6/NVIDIA$ <strong class="ky hj">cd benchmarks; ls<br/></strong>gnmt  maskrcnn  minigo  resnet  ssd  transformer</span></pre><p id="1999" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在NVIDIA/基准目录中，我们可以看到6种不同的培训基准。让我们选择第一个基准“GNMT ”,它是一个递归神经网络模型，类似于Google的语言翻译模型。<a class="ae jp" href="https://github.com/mlperf/training_results_v0.6/tree/master/NVIDIA/benchmarks/gnmt/implementations/pytorch" rel="noopener ugc nofollow" target="_blank">NVIDIA提供了关于软件要求、数据集细节、执行的预处理以及在单节点和多节点系统上运行数据集的步骤的文档</a>。</p><p id="83a6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于我们对在单个节点上运行基准测试感兴趣，我们将挑选单个节点(<a class="ae jp" href="https://github.com/mlperf/training_results_v0.6/tree/master/NVIDIA/benchmarks/gnmt/dgx1_ngc19.05_pytorch" rel="noopener ugc nofollow" target="_blank"> NVIDIA DGX-1 </a>)的提交结果，并使用其文档在我们的系统上运行GNMT。</p><h2 id="598a" class="lc ld hi bd lj lk ll lm ln lo lp lq lr jb ls lt lu jf lv lw lx jj ly lz ma mb bi translated">下载并验证数据集</h2><p id="1861" class="pw-post-body-paragraph iq ir hi is b it mc iv iw ix md iz ja jb me jd je jf mf jh ji jj mg jl jm jn hb bi translated">下载和验证数据集的脚本可以在<em class="jo">实现</em>目录中获得。运行脚本以下载和准备数据集，根据您的网络连接，这需要大约90分钟，并且需要大约1.2GB的文件系统空间。通过执行第二个脚本，验证数据集是否已正确下载。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="64d9" class="lc ld hi ky b fi le lf l lg lh">$/home/training_results_v0.6/NVIDIA$ <strong class="ky hj">cd gnmt/implementations; ls</strong><br/>download_dataset.sh  pytorch  verify_dataset.sh</span><span id="b08a" class="lc ld hi ky b fi li lf l lg lh">$/home/training_results_v0.6/NVIDIA/benchmarks/gnmt/implementations$ <strong class="ky hj">bash download_dataset.sh</strong></span><span id="6300" class="lc ld hi ky b fi li lf l lg lh">$/home/training_results_v0.6/NVIDIA/benchmarks/gnmt/implementations$ <strong class="ky hj">bash verify_dataset.sh<br/></strong>OK: correct data/train.tok.clean.bpe.32000.en<br/>OK: correct data/train.tok.clean.bpe.32000.de<br/>OK: correct data/newstest_dev.tok.clean.bpe.32000.en<br/>OK: correct data/newstest_dev.tok.clean.bpe.32000.de<br/>OK: correct data/newstest2014.tok.bpe.32000.en<br/>OK: correct data/newstest2014.tok.bpe.32000.de<br/>OK: correct data/newstest2014.de</span></pre><h2 id="50b9" class="lc ld hi bd lj lk ll lm ln lo lp lq lr jb ls lt lu jf lv lw lx jj ly lz ma mb bi translated">启动培训工作</h2><p id="d0aa" class="pw-post-body-paragraph iq ir hi is b it mc iv iw ix md iz ja jb me jd je jf mf jh ji jj mg jl jm jn hb bi translated">执行培训作业的脚本和代码位于<em class="jo"> pytorch </em>目录中。让我们研究一下这个目录中的文件。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="7448" class="lc ld hi ky b fi le lf l lg lh">$/home/training_results_v0.6/NVIDIA/benchmarks/gnmt/implementations$ <strong class="ky hj">ls</strong><br/>data  download_dataset.sh  logs  pytorch  verify_dataset.sh<br/>$/home/training_results_v0.6/NVIDIA/benchmarks/gnmt/implementations$<strong class="ky hj">cd pytorch; ls -l</strong><br/>bind_launch.py<br/>config_DGX1_multi.sh<br/>config_DGX1.sh<br/>config_DGX2_multi_16x16x32.sh<br/>config_DGX2_multi.sh<br/>config_DGX2.sh<br/>Dockerfile<br/>LICENSE<br/>mlperf_log_utils.py<br/>preprocess_data.py<br/>README.md<br/>requirements.txt<br/>run_and_time.sh<br/>run.sub<br/>scripts<br/>seq2seq<br/>setup.py<br/>train.py<br/>translate.py</span></pre><p id="6040" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">配置_ &lt;系统&gt;。sh </strong>:由于我们在一个有4个GPU的系统上执行培训任务，我们必须创建一个新的配置文件来反映我们的系统配置。如果您的系统有8或16个GPU，您可以使用现有的config_DGX1.sh或config_DGX2.sh配置文件来启动培训作业。</p><p id="66f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我创建了一个新的配置文件config_SUT.sh(通过复制config_DGX1.sh)并编辑它以反映我的系统配置。在这种情况下，我只需要将GPU的数量从8个更改为4个。您可能需要更改CPU核心和插槽的数量，以反映系统上的可用CPU资源。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="b43d" class="lc ld hi ky b fi le lf l lg lh">$training_results_v0.6/NVIDIA/benchmarks/gnmt/implementations/pytorch$<strong class="ky hj">cp config_DGX1.sh config_SUT.sh</strong></span><span id="45c1" class="lc ld hi ky b fi li lf l lg lh">Edit <strong class="ky hj">config_SUT.sh </strong>to reflect your system config</span><span id="ad6d" class="lc ld hi ky b fi li lf l lg lh">## System config params<br/><strong class="ky hj">DGXNGPU=4</strong><br/>DGXSOCKETCORES=20<br/>DGXHT=2         # HT is on is 2, HT off is 1<br/>DGXIBDEVICES=''<br/>DGXNSOCKET=2<br/>BIND_LAUNCH=1</span></pre><p id="4907" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，您已经准备好构建docker容器并启动培训作业了。用docker hub注册名替换<docker>,这样就可以在其他系统或多节点运行中重用容器映像。</docker></p><p id="9c9e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Dockerfile </strong>这是docker容器的构建文件，将用于执行培训任务</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="8989" class="lc ld hi ky b fi le lf l lg lh"><strong class="ky hj">docker build -t &lt;docker/registry&gt;/mlperf-nvidia:rnn_translator .</strong></span><span id="2889" class="lc ld hi ky b fi li lf l lg lh"><strong class="ky hj">docker push &lt;docker/registry&gt;/mlperf-nvidia:rnn_translator</strong></span></pre><p id="4d98" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您没有docker hub帐户，您可以将其保存在本地系统上，并且不指定<docker/></p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="2a9f" class="lc ld hi ky b fi le lf l lg lh"><strong class="ky hj">docker build -t mlperf-nvidia:rnn_translator .</strong></span></pre><p id="1333" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">启动容器并查看其内容，验证它是否有config_SUT.sh文件等</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="bb3a" class="lc ld hi ky b fi le lf l lg lh"><strong class="ky hj">nvidia-docker run -it --rm mlperf-nvidia:rnn_translator<br/></strong>root@4e944d91164e:/workspace/rnn_translator# <strong class="ky hj">ls -l *.sh</strong><br/>config_DGX1.sh<br/>config_DGX1_multi.sh<br/>config_DGX2.sh<br/>config_DGX2_multi.sh<br/>config_DGX2_multi_16x16x32.sh<br/>config_SUT.sh<br/>run_and_time.sh</span></pre><p id="8083" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦您验证了正确的配置文件在新创建的docker容器中可用，我们现在就可以使用启动脚本<strong class="is hj">run . sub</strong>执行培训作业，并为数据集、日志文件和配置文件设置环境变量</p><p id="8c7a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">DATADIR =<path>LOGDIR =<path>PULL = 0 dgx system =<config file="">。/run.sub</config></path></path></p><p id="6bea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于我的测试，我将使用config_SUT.sh，因此将DGXSYTEM指定为SUT。我创建了一个新的目录“logs”来存储基准日志文件，并在启动基准运行时指定路径，如下所示:</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="aa6c" class="lc ld hi ky b fi le lf l lg lh">DATADIR=/home/training_results_v0.6/NVIDIA/benchmarks/gnmt/implementations/data LOGDIR=/home/training_results_v0.6/NVIDIA/benchmarks/gnmt/implementations/logs DGXSYSTEM=SUT PULL=0 ./run.sub</span></pre><p id="a4b8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果一切顺利，您应该可以开始比赛了，它将执行10次基准测试，并将日志文件存储在指定的目录中。由于我们在配置文件中指定了4个GPU，我们看到所有4个GPU都被用于训练GNMT模型。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mh"><img src="../Images/5f7aec62f3e593dcec15ca417bdc5e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U59GYvs3AGh51vB3bIvupQ.png"/></div></div></figure><p id="c59e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基准运行时间可以在日志文件中找到。我的跑步结果显示如下。运行的平均时间是90分钟，完成10次迭代需要将近15个小时。如果您不想运行所有10次迭代，可以修改run.sub来限制运行次数。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="0345" class="lc ld hi ky b fi le lf l lg lh">$/home/training_results_v0.6/NVIDIA/benchmarks/gnmt/implementations/logs$ <strong class="ky hj">grep RNN_TRANSLATOR *.log</strong><br/>1.log:RESULT,RNN_TRANSLATOR,,3795,nvidia,2019-11-22 09:25:23 PM<br/>2.log:RESULT,RNN_TRANSLATOR,,4683,nvidia,2019-11-22 10:28:56 PM<br/>3.log:RESULT,RNN_TRANSLATOR,,3807,nvidia,2019-11-22 11:47:17 PM<br/>4.log:RESULT,RNN_TRANSLATOR,,5594,nvidia,2019-11-23 12:51:02 AM<br/>5.log:RESULT,RNN_TRANSLATOR,,6473,nvidia,2019-11-23 02:24:33 AM<br/>6.log:RESULT,RNN_TRANSLATOR,,5576,nvidia,2019-11-23 04:12:43 AM<br/>7.log:RESULT,RNN_TRANSLATOR,,6484,nvidia,2019-11-23 05:45:57 AM<br/>8.log:RESULT,RNN_TRANSLATOR,,4683,nvidia,2019-11-23 07:34:19 AM<br/>9.log:RESULT,RNN_TRANSLATOR,,6481,nvidia,2019-11-23 08:52:40 AM<br/>10.log:RESULT,RNN_TRANSLATOR,,5580,nvidia,2019-11-23 10:40:59 AM</span></pre><p id="86dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其余目录(mask-rcnn、minigo、resnet、ssd和transformer)中的其他训练基准可以使用类似的步骤运行——下载数据集、构建和运行docker容器。您可以使用MLPerf培训基准来比较不同的GPU系统或评估不同的软件框架等。例如，您可以使用MLPerf评估存储子系统对ML工作负载的影响，或者评估如何改进MLPerf基准测试metrics⁴.</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="b7a9" class="lc ld hi ky b fi le lf l lg lh">[1] MLPerf Training Benchmark, Oct 2019 <a class="ae jp" href="https://arxiv.org/pdf/1910.01500.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1910.01500.pdf</a></span><span id="c22b" class="lc ld hi ky b fi li lf l lg lh">[2] Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, Oct 2016<br/><a class="ae jp" href="https://arxiv.org/abs/1609.08144" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1609.08144</a></span><span id="868a" class="lc ld hi ky b fi li lf l lg lh">[3]   Exploring the Impact of System Storage on AI &amp; ML Workloads via MLPerf Benchmark Suite,   Wes Vaske<br/>  <a class="ae jp" href="https://www.flashmemorysummit.com/Proceedings2019/08-08-Thursday/20190808_AIML-301-1_Vaske.pdf" rel="noopener ugc nofollow" target="_blank">https://www.flashmemorysummit.com/Proceedings2019/08-08-Thursday/20190808_AIML-301-1_Vaske.pdf</a></span><span id="b3d2" class="lc ld hi ky b fi li lf l lg lh">[4]  Metrics for Machine Learning Workload Benchmarking, Snehil Verma et al<br/>  <a class="ae jp" href="https://researcher.watson.ibm.com/researcher/files/us-ealtman/Snehil_Metrics_for_Machine_Learning_Workload_Benchmarking.pdf" rel="noopener ugc nofollow" target="_blank">https://researcher.watson.ibm.com/researcher/files/us-ealtman/Snehil_Metrics_for_Machine_Learning_Workload_Benchmarking.pdf</a></span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/235ac67ffdf4c1679cb00048f57db6ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wx4q0dXiTENdpqwrQu5_AQ.png"/></div></figure></div></div>    
</body>
</html>