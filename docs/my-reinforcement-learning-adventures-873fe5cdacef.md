# 我的强化学习冒险

> 原文：<https://medium.com/analytics-vidhya/my-reinforcement-learning-adventures-873fe5cdacef?source=collection_archive---------8----------------------->

![](img/4d1a032d4cd43de07800f1dfc805c17a.png)

我的一个实验运行的图像

你能想象在游戏里做军事策略会有多酷吗？拥有一个精通军事战术、战争艺术以及你可以在一天中的任何时候玩的任何东西的游戏怎么样？

2 年前，在读了一点热门漫画王国(神奇故事顺便说一句)后，我决定看看我能做些什么来制作这样的游戏。在这个游戏中，如果我把军队放在正确的位置，它会得出和 2250 多年前的将军们一样的结论。

对于下面的所有模型，我使用了近似策略优化模型。

# 版本 1

![](img/8d5862883b8b1c2de03a4ffb41dfba24.png)

版本 1 环境测试

对于我的第一个版本，我从国际象棋中汲取了一些灵感。

## 环境

*   行为空间

每次迭代，每个玩家(白点/蓝点)从四个方向中选择攻击其周围的 1 个方格。如果攻击方格中有敌方玩家，则该玩家获得+1 奖励。环境输出的另一个动作是代理移出 4 个方向的方向。所以，行动空间的维度是[玩家人数，8]。

*   了望处

保存 x 位置，y 位置，等级，侧面，在每个玩家的视野内为所有玩家活着。所以，观察空间的维度是[玩家数量，玩家可以看到的玩家数量，6]。我也有一个层次观察空间，但我不会在这里深入探讨。

*   基本模型解释

我进行了观察，并使用 LSTM 压缩成[玩家数量，256]，然后做了一些完全连接的层输出[玩家数量，8]。通过对前 4 个指标进行 softmax 选择动作，并选择攻击方向，对后 4 个指标做同样的事情，并选择移动方向。

我还做了它，使模型不仅对抗自己，而且对抗自己的旧版本，我认为这将有所帮助。

## 结果

![](img/4cad09c7c847cbf94641e67746f2b19c.png)![](img/3b32b612204950d1fa30dc0293f58776.png)

基本上，从剧情和动画中可以看出，他们似乎没有学到多少东西。这种环境的主要问题是

*   主要问题是球员之间似乎没有太多的配合
*   环境太简单，看不到军事战术。就像向左斜下方移动并不比向左下方移动快。
*   此外，策略和值在一段时间后停止学习。在我制作这个版本的时候，我认为环境太简单了，但现在我认为这是因为观察空间以如此复杂的方式编码。

# 版本 2

![](img/6865d51ede02e90dcfa4099d31e86868.png)

版本 2 环境测试

## 环境

*   行为空间

我现在改变它，使行动输出移动角度，移动长度，方向和攻击角度。方位和攻角基本上规定了攻击范围。范围越小，伤害越大。由于现在每个玩家都有 hp，我想这可能会导致一些玩家之间的决斗。上面的动画就是一个例子。

翅膀表示攻击范围。

*   了望处

与之前没有重大变化。

*   基本模型解释

主要变化是输出行动空间的对数标准差和平均值，并从分布中采样一个值。这就是我如何得到这些动作的值

## 结果

![](img/8f771984b85b3e241c9244871bb68f18.png)![](img/48215702ffba9a76e6eb1bd51fc9a61a.png)

虽然价值损失似乎有所改善，但保单损失似乎趋于平稳。但是，我注意到的一件事是，虽然两翼/进攻方向似乎有所改善，但球员之间的协调似乎没有出现。所以，主要问题是:

*   玩家可以在没有协调的情况下随意移动
*   翅膀的攻击机制太复杂了
*   像以前一样，观察空间太复杂了
*   最后，这也适用于版本 1，但动作空间有点太复杂，人类无法理解。

# 版本 3

![](img/c280cfd8dd3843cfa9031f598bee01dd.png)

版本 3 环境测试。彼此相邻的图像是每个模型所看到的。所以，左上角的视频是蓝色部分看到的，旁边的是红色部分看到的。

## 环境

*   行为空间

我把动作空间做成一个 4x 4 的向量场，这样就可以理解模型的走向。我还做了这个向量场来表示作用在玩家身上的力。

*   了望处

我把观察空间做成了上面渲染的简化版本，这样它更容易被人类理解，并且比以前更容易学习。现在，同时运行多个游戏成为可能！

*   QOL 的变化

我这样做是为了鼓励合作，让玩家们团结在一起，他们有一个体系，像这样绑着弹簧

![](img/644c1cec4006757ca6b08811ad6d7727.png)

弹簧牵引可视化

同样，对于攻击，我只是让速度是攻击的方向，就像这样，有一个三角形的攻击

![](img/f511d1ddc2e895345380c7c39dcfe783.png)

*   基本模型解释

我在媒体上写了一个单独的系列，名为[理解 OpenAI 基线源代码并让它进行自我播放](/analytics-vidhya/understanding-openai-baseline-source-code-and-making-it-do-self-play-part-1-9f30085a8c16)，但我基本上修改了 [OpenAI 基线](https://github.com/openai/baselines)，这样你就可以在其中进行自我播放了！我记得大概花了 1 个月，也许 2 个月，但是我对结果很满意。我这样做主要是因为我不确定问题是在我的模型还是环境中，而且因为我知道 Open AI 是最好的 AI 公司之一，我认为使用他们的代码不会出错。目前，除了 ppo2 之外，该代码仍然无法运行。

对于该模型，我只是使用了 cnn-lstm，其动作空间为 box(连续动作空间),这对我来说非常符合逻辑，因为观察空间是一个图像，输出只是连续的力。

## 结果

![](img/a5309613746ee0e17b1db88eb0b7e4fc.png)

保单损失

虽然该政策似乎正在改善，但当我查看视频时，我注意到在 100 个纪元的运行中，所做的事情没有显著变化。导致这种情况的一个问题是这个模型在和自己斗争。因此，在任何时候，一方持续赢另一方都没有多大意义。基本上，这个模型的主要问题是我不知道这个模型是否真的在学习。

# 版本 4

![](img/fdbeb2a6cc32f8c0138e96336bc478b4.png)

版本 4 环境测试运行。如你所见，进展并不顺利。

*   行为空间

对于动作空间，我更改了它，以便我可以指定我想要的大小。例如，上面的动画是一个 2x2 向量场。我还做了这样一个向量场，它不是一个加速度/力向量场，而是一个速度向量场，我只是施加力，使模型朝力的方向移动。我只是觉得这样更容易理解。

*   了望处

没有变化。

*   QOL 的变化

由于我不知道模型是否在学习，我决定采用最简单的策略:与静止的对手战斗。所以，如果这个模型能够以更快的速度击败敌人，我就会知道它正在改进。

我做的另一件事是我去掉了弹簧机械装置，因为如果可能的话，我想让模型学会自己粘在一起。

## 结果

![](img/ad6a1b800e86dd1f7321dd889e089feb.png)

不同模型的奖励图

![](img/e6c7ed4e05b3063ee9a5d4918e4a43f7.png)

不同模型下的保单损失图

正如你可能从视频中看到的，它并不顺利。上面的第一个情节基本上是奖励，所以在开始时，一个随机的政策，它工作了一段时间，但后来它就停止做任何有成效的事情，只是走到了一个角落。

在很长一段时间里，我对为什么会发生这种情况感到困惑，但事情很简单。对于我的动作空间，我输出一个形状为(2，2，2)的连续动作空间，其中动作空间[0][0]表示左上向量。问题是我使用了演员评论，它输出一个平均值和一个对数标准差。举例来说，模型了解到，对于标准差为 1 的所有操作，将平均值移动到-5 是一个很好的主意。这里发生的是，动作空间的所有部分都指向左下方，它会一直保持在那里，因为所有采样的数字都是负数。请注意，这个问题也发生在 1x1 动作空间。

保单看起来很好的原因是因为价值损失最初非常高，所以价值损失有所改善，最终只是预测为 0，这导致保单损失看起来有所改善。

# 第 5 版(当前版本)

![](img/3eff2ef50759684a4e2097dda52b602e.png)

版本 5 环境测试运行

*   行为空间

我决定只使用多离散动作空间，而不是之前使用的连续动作空间。然后，该模型只是做一个软最大值，并选择一个行动，而没有进入所有的均值变化问题。对于这个模型来说，它只是一个 1x1 的动作空间。目前，我开始在一个 2x2 的活动空间进行训练

*   了望处

没有变化。

## 结果

![](img/11145ad47553d1b2d74aa3b896f6beb0.png)

不同模型的奖励图

![](img/6f420c11ffe6b2229656b4b3b1b4bad6.png)

不同模型下的保单损失图

虽然模型似乎并没有随着所有模型的改进而改进，但我看到像 impala_cnn 和 cnn_lstm 这样的模型，模型奖励在增加。此外，这是一个有点吵，但问题的发现一个无效的政策似乎已经停止发生，我很高兴！此外，政策损失似乎是正常的。所以 2 年后我终于有所收获了！

# 后续步骤

因为我已经建立了一个自我游戏环境，所以我计划只让一个模型和它自己的旧版本战斗。例如，让一个模型学习 1 个纪元，然后让另一个模型与之对抗，看它如何改进。