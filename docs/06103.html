<html>
<head>
<title>Machine Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器翻译</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/translation-seq2seq-attention-4f8b23f33965?source=collection_archive---------29-----------------------#2020-05-11">https://medium.com/analytics-vidhya/translation-seq2seq-attention-4f8b23f33965?source=collection_archive---------29-----------------------#2020-05-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3c53" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这个故事涵盖了前神经机器翻译、神经机器翻译和注意力。这个故事是对课程的一个简短总结:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">斯坦福CS224N:带深度学习的| Winter 2019 |第八讲—翻译，Seq2Seq，注意</a>。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/8f8dec3b92b70bcf9c7405fa1631baf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fMdHcIpbYWff3oUZp1G2JA.png"/></div></div></figure><p id="1402" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这个故事包括前神经机器翻译，神经机器翻译，注意力。以下是课程总结:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank"> Stanford CS224N:带深度学习的| Winter 2019 |第八讲—翻译，Seq2Seq，注意</a>。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="6fba" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated"><strong class="ak">预神经机器翻译</strong></h1><p id="aee1" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated"><strong class="jm hj">机器翻译(MT) </strong>是将一种语言(源语言)的句子x翻译成另一种语言(目标语言)的句子y的任务。机器翻译研究始于20世纪50年代初，始于将俄语翻译成英语，这是冷战的推动。系统大多基于规则，使用双语词典将俄语单词映射到英语单词。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lk"><img src="../Images/54b42039567577ead1e7759aaa093ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*FoHExB0MVYNdDOPh0KXcnA.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">统计机器学习。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="4b5b" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">从20世纪90年代到2010年代，统计机器学习主导了MT，即从数据中学习概率模型。假设我们把法语翻译成英语。我们想找到最好的英语句子y，给定法语句子x。使用贝叶斯规则将其分解为两个部分，分别学习。如何学习翻译模型P(x|y)？首先，需要大量的并行数据(例如，成对的人工翻译的法语/英语句子)。那么如何从平行语料库中学习翻译模型P(x|y)？我们进一步分解它:我们实际上想要考虑:P(x，a|y)其中a是对齐，即法语句子x和英语句子y之间的单词级对应。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lp"><img src="../Images/31ffe17b0b0614040d9f62b0dcbcd241.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*iJnnWVYkw1Unhpb_spL0pA.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">对齐。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="fc42" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> <em class="lq">对齐是翻译句子对</em> </strong>中特定单词之间的对应关系。值得注意的是，有些词没有对应词。对齐是复杂的，可以是多对一、一对多和多对多。我们认为P(x，a|y)是许多因素的组合，包括1)特定单词对齐的概率(也取决于在句子中的位置)；2)特定单词具有特定生殖力的概率(对应单词的数量)</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es lr"><img src="../Images/8a871485ed309b8cf8494414eabdc80b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g0oGkl5CeTUg1jNqEcaoCQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">统计机器学习的解码。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="9131" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我们可以列举每一个可能的y并计算概率。但是太贵了！一种解决方案是使用启发式搜索算法来搜索最佳翻译，丢弃概率太低的假设。这个过程叫做解码。统计机器学习(SML)是一个巨大的研究领域。最好的系统极其复杂。它包含了数百个我们在这里没有提到的重要细节。系统有许多单独设计的子组件。它需要大量的特征工程。它需要设计一些功能来捕捉特定的语言现象。它需要编译和维护额外的资源。它就像等价短语的表格，需要大量的人力来维护。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="7696" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">2.神经机器翻译</h1><p id="a93c" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated">神经机器翻译(NMT) <strong class="jm hj">这是一种用单个神经网络进行机器翻译的方法。神经网络架构被称为<strong class="jm hj">序列到序列(又名seq2seq) </strong>，它涉及<strong class="jm hj">两个rnn</strong></strong></p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ls"><img src="../Images/137301d8e81a5a5eb86193e4071c28f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/1*f5XxoLhwnC1mGhI-a1FuVQ.gif"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">seq2seq NMT解码示例。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="0b9f" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">序列到序列(seq2seq)不仅仅适用于机器翻译。许多自然语言处理任务可以表述为序列到序列:摘要(从长文本到短文本)、对话(从前面的话语到下一个话语)、解析(从输入文本到输出解析为序列)和代码生成(从自然语言到python代码)。序列到序列模型是<strong class="jm hj">条件语言模型的一个例子。它是一个语言模型</strong>因为解码器正在预测目标句子y的下一个单词<strong class="jm hj">它是有条件的</strong>因为它的预测也是以源句子x为条件的NMT直接计算P(y|x)。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es lt"><img src="../Images/d990dd8aecd3617d3fdc167fe3364bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LkN1p54n_vBTY0i3QHcfPQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">NMT直接计算P(y|x)</figcaption></figure><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lu"><img src="../Images/cdf758c77497f0dbbf38b566a3dad88c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/1*FGTQzxPeK-qkssYegI0gvA.gif"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">NMT培训流程。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="59f2" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">得到一个大的平行语料库，我们可以训练一个NMT系统。<strong class="jm hj"> <em class="lq"> Seq2seq优化为单一系统，反向传播“端到端”操作</em> </strong>。</p><p id="c831" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我们看到了如何通过在解码器的每一步使用argmax来生成(或“解码”)目标句子。这是贪婪解码，在每一步中取最可能的单词)。贪婪解码的问题是它没有办法撤销决定。理想情况下，我们希望找到一个(长度为T)的平移y，使p(y|x)最大化。这就是<strong class="jm hj">穷举搜索解码</strong>算法。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es lv"><img src="../Images/5a746884e37d1fe294fcf13d2743113d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZFsZFgsd1vtg4T0Z-MmXA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">理想情况下，我们希望找到一个(长度为T)的平移y，使p(y|x)最大化。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="fba3" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><em class="lq">我们可以尝试计算所有可能的序列y。</em>这意味着在解码器的每一步t，我们都在跟踪Vt可能的部分翻译，其中V是vocab大小。这O(V^T)的复杂性是<strong class="jm hj">太昂贵了！</strong></p><p id="1c26" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我们可以使用波束搜索解码。在解码器的每一步，它跟踪K个最可能的部分平移(我们称之为假设)，其中K是光束大小(实际上大约为5到10)。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es lw"><img src="../Images/611140eec6be1d860b1773a4e60a7650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ga795AsvP0P7UrCYeT8Vw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">光束搜索得分。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="8406" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">一个假设y1，…yt有一个分数，这是它的对数概率。分数都是负数，分数越高越好。我们搜索高分假设，跟踪每一步的前k。波束搜索不能保证找到最优解，但它比穷举搜索效率高得多！</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lx"><img src="../Images/8d50ceeae09450702b326c1ebfc99e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/1*IcI7mVMW2kmxb05hHZCGUQ.gif"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">波束搜索解码示例。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="d8a2" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">先说解码中的停止准则。<strong class="jm hj"> <em class="lq">在贪婪解码中，我们通常会解码到model产生一个&lt; END &gt; token，</em> </strong>比如`&lt; START &gt;他用馅饼砸我&lt; END &gt;`。在波束搜索解码中，不同的假设可能在不同的时间步长上产生&lt;和&gt;记号。当一个假设产生&lt;结束&gt;时，那个假设就完成了。波束搜索把它放在一边，继续探索其他假设。<strong class="jm hj"> <em class="lq">我们通常继续波束搜索，直到到达时间步长T(其中T是某个预定义的截止值)，或者我们至少有n个完整的假设(其中n是预定义的截止值)</em> </strong>。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ly"><img src="../Images/b1883ff10b2343177ed8c2e3232e3edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S08EeUQSAIVZ1Q2x6blNXw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">我们列表中的每个假设y1，…yt都有一个分数</figcaption></figure><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lz"><img src="../Images/a9667f199be9e744a49330245e92659b.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*VVOiO3Qdt3nXpIqOKg9ygg.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">标准化射束搜索分数。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="40c5" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">现在我们有了完整的假设列表，但是如何选择得分最高的一个呢？我们列表中的每个假设y1，…yt都有一个分数。这样做的问题是，越长的假设得分越低。我们可以通过用长度t归一化分数来解决这个问题。</p><p id="6235" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">与SMT相比，NMT有许多优势:1)它有更好的表现:更流畅，更好地利用上下文和短语相似性；2)要端到端优化的单个神经网络:没有要单独优化的子组件；3)它需要更少的人工工程努力:不需要特征工程，并且对所有语言对使用相同的方法。与SMT相比，也有一些<strong class="jm hj">的缺点</strong>，比如NMT的可解释性较差，难以控制(s.t .不容易指定翻译的规则或指导方针)。</p><p id="b3cb" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我们用BLEU来评价ML。<a class="ae ix" href="https://www.aclweb.org/anthology/P02-1040.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jm hj"> <em class="lq"> BLEU </em> </strong> </a>代表双语评价替角。它将机器编写的翻译与一个或几个人类编写的翻译进行比较，然后<strong class="jm hj"> <em class="lq">根据以下因素计算相似性得分:n个字母的精度(通常是1、2、3和4个字母的精度)，加上对过短系统翻译的惩罚</em> </strong>。蓝色是有用的，但不完美。翻译一个句子有许多有效的方法。所以一个好的翻译可以得到一个很差的BLEU分数，因为它与人类翻译的n-gram重叠很低。</p><p id="8f06" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">NMT取得了不错的进展，<a class="ae ix" href="http://www.skynettoday.com/state_of_nmt" rel="noopener ugc nofollow" target="_blank">但是机器翻译还没有解决</a>。NMT是NLP深度学习的旗舰任务。NMT研究公司开创了NLP深度学习的许多最新创新。2019年，NMT研究继续蓬勃发展。研究人员已经发现了我们今天展示的“香草”seq2seq NMT系统的许多改进。但有一项改进是如此不可或缺，那就是新香草，<strong class="jm hj"><em class="lq"/></strong>注意。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="eb18" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">3.注意力</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ma"><img src="../Images/bee82a9b7899cf3ed6b105f8d77f4eb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3roDiOLHNB8f0ANnXnXDQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">seq2seq:瓶颈问题。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="3338" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">seq2seq的瓶颈问题是，瓶颈需要捕获关于源句子的所有信息。注意力为瓶颈问题提供了解决方案。T <strong class="jm hj"> <em class="lq">他的核心思想是，在解码器的每一步上，使用直接连接到编码器来聚焦源序列的特定部分</em> </strong>。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mb"><img src="../Images/6ccc90ca0129636104af7bd4cbce43a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/1*pZ69p8di49K_xggqYLYbJA.gif"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">注意从序列到序列。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="92e7" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><em class="lq">首先，我们计算&lt;开始&gt;和源句子中每个标记之间的点积。然后我们拿softmax把分数变成概率分布。从这个分布中，我们可以看到，在这个编码器时间步长上，我们主要关注第一个编码器隐藏状态“他”。然后，我们使用注意力分布对编码器隐藏状态进行加权求和。注意力输出主要包含来自受到高度关注的隐藏状态的信息。然后，我们将注意力输出与解码器隐藏状态连接起来，以计算第一个解码器输出，在本例中是“他”。有时我们从上一步中提取注意力输出，并将其输入到解码器中(和通常的解码器输入一起)</em>。最后，我们重复整个过程，一个单词一个单词地生成单词。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mc"><img src="../Images/d0c04d2426eafa0716ccbf14aec88c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MSznHmTqOGFDXgbytU81MQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">方程式中的注意。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="83a5" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">关注度很大。注意力通过允许解码器聚焦于源的某些部分而显著提高了NMT性能；注意力通过允许解码器直接查看源并绕过瓶颈来解决瓶颈问题；注意力有助于解决消失梯度问题，因为它提供了一条通往遥远状态的捷径。注意力也提供了一些可解释性。通过观察注意力分布，我们可以看到解码者关注的是什么。我们免费获得(软)校准。这很酷，因为我们从来没有明确训练一个对齐系统。网络只是自己学会了对齐。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es md"><img src="../Images/c1f29d19aafb5e352f141a315568cc93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m8m8cqKOmwwr_H7jDmuatQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">计算注意分数的注意变量。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="fc28" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">注意力在深度学习技术中是普遍的。我们已经看到，注意力是改进机器翻译的序列对序列模型的一个很好的方法。但是，你可以在很多架构(不仅仅是seq2seq)和很多任务(不仅仅是MT)中使用注意力。<strong class="jm hj"> <em class="lq">注意力的更一般定义是给定一组向量值，并进行向量查询，注意力是一种计算这些值的加权和的技术，依赖于查询。</em> </strong>我们有时会说<em class="lq">查询关注的是值。</em>例如，在seq2seq with attention模型中，每个解码器隐藏状态(查询)关注所有编码器隐藏状态(值)。<strong class="jm hj">直观关注的背后是，加权和是值中包含的信息的<em class="lq">选择性汇总</em>，其中查询确定关注哪些值</strong>。<em class="lq">注意力是一种获得任意一组表示</em>(值)的固定大小表示的方式，依赖于一些其他的表示(查询)。注意力总是包括3个步骤:1)计算注意力得分e，有多种方法可以做到这一点(如下图所示)；2)取softmax得到注意力分布α；3)利用注意力分布取权值之和。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="5cd4" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">参考:</h1><ol class=""><li id="0410" class="me mf hi jm b jn lf jq lg jt mg jx mh kb mi kf mj mk ml mm bi translated"><a class="ae ix" href="https://www.youtube.com/watch?v=XXtpJxZBa2c&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=8" rel="noopener ugc nofollow" target="_blank">斯坦福CS224N:具有深度学习的| Winter 2019 |第八讲—翻译，Seq2Seq，注意</a>。</li><li id="858a" class="me mf hi jm b jn mn jq mo jt mp jx mq kb mr kf mj mk ml mm bi translated"><a class="ae ix" href="https://www.aclweb.org/anthology/P02-1040.pdf" rel="noopener ugc nofollow" target="_blank"> BLEU:一种自动评估机器翻译的方法</a></li></ol><p id="8e92" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">3.<a class="ae ix" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">联合学习对齐和翻译的神经机器翻译</a></p></div></div>    
</body>
</html>