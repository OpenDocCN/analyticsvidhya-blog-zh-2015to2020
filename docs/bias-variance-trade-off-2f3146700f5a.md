# 偏差-方差权衡

> 原文：<https://medium.com/analytics-vidhya/bias-variance-trade-off-2f3146700f5a?source=collection_archive---------17----------------------->

![](img/7eada7e5870592f60471f5eec204e505.png)

照片由[德鲁·格拉汉姆](https://unsplash.com/@dizzyd718?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

[监督学习](https://en.wikipedia.org/wiki/Supervised_learning)可以通过偏差-方差权衡的帮助得到最好的理解。在监督学习下，任何模型的主要目的都是在输入变量的帮助下估计目标函数以预测输出。监督学习由机器学习算法组成，这些算法用于通过查看以前的结果来分析数据。每一个行动，都有它的结果或最终目标，这有助于它有用。监督学习借助于动作及其先前的结果来分析它并预测未来可能的结果。在监督学习中，每一个算法都是基于一些被标记的已知数据；此处标记表示给出了关于数据的所有信息。算法被反复训练在标签数据上，然后机器根据训练执行动作来预测结果。这些预测的结果或多或少与过去的结果非常相似。这有助于我们对尚未发生的行为做出决定。无论是天气预报、预测股票市场价格、房产价格、检测垃圾邮件、推荐系统、自动驾驶汽车、客户流失建模、产品销售等。，监督学习开始行动。在监督学习中，你监督学习过程，这意味着你在这里收集的数据被标记，所以你知道什么输入需要映射到什么输出。它是制作算法以学习将输入映射到特定输出的过程。这是通过使用您收集的带标签的数据集实现的。如果映射正确，则该算法已经成功学习。否则，您需要对算法进行必要的更改，以便它能够正确学习。监督学习算法可以帮助我们对未来获得的新的未知数据进行预测。这与师生情景相同。老师教学生从书中学习(标记为数据集)，学生从中学习，然后通过测试(算法预测)。如果学生未能通过[(过拟合或欠拟合)](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)，教师调整学生[(超参数调整)](https://towardsdatascience.com/hyperparameter-tuning-c5619e7e6624)以便稍后表现更好。但是他们在理想状态和实际可能状态之间还有很大的差距。因为没有学生(算法)或老师(数据集)在他们的工作中是 100%正确的。同样，每个模型和输入模型的数据都有许多优点和缺点。数据集可能不平衡，由许多缺失值组成，形状和大小不正确，可能包含许多异常值，这使得任何模型任务都难以执行。同样，每个模型都有其缺点，或者在绘制输出时会出错。我将讨论这些妨碍模型发挥最佳性能的错误，以及我们如何克服这些错误。

![](img/423357641bfec062445974ff65b3c494.png)

Max 陈在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

在进行模型训练之前，我们应该了解与之相关的误差[(偏差和方差)](/datadriveninvestor/bias-and-variance-in-machine-learning-51fdd38d1f86)。如果我们了解它，不仅可以帮助我们更好地进行模型训练，还可以帮助我们处理模型的欠拟合和过拟合。

这种预测误差有三种类型:

*1。偏置*

*2。差异*

*3。不可约误差*

先说偏见。

*1。* *偏置*

偏差是模型为使目标函数更容易学习而做出的简化假设。我们试图预测的是预测值和正确值之间的差异。当算法在理解结果的真正含义方面灵活性有限，当算法不能预测实际结果时，就会出现偏差。它来自算法做出的错误假设。

*维基百科声明*，“偏差是学习算法中错误假设的误差。高偏差会导致算法错过特征和目标输出之间的相关关系(欠拟合)。”

偏差实际上是由算法做出的预测的量度。如果一个算法做出了错误的预测，这意味着它有很高的偏差，预测越准确，偏差就越低。就当是对人的偏见判断吧。如果我们对任何人更有偏见，我们就更有可能对他们做出错误的假设，反之亦然。

根据福尔曼的文章:

*“偏差是算法由于没有考虑数据中的所有信息而不断学习错误的东西的趋势(欠拟合)。”*

一般来说，线性算法(参数算法的例子)具有较高的偏差，使得它们学习起来更快且更容易理解，但是通常不太灵活。反过来，它们在复杂问题上的预测性能较低，无法满足算法偏差的简化假设。参数算法或线性算法由固定大小的数据组成，与训练样本的数量无关。

偏见有两种类型:

> 高偏差-建议对目标变量进行更多假设。例如线性回归、逻辑回归、线性判别分析。
> 
> 低偏差—表明对目标变量的假设较少。例如 SVC、KNN、决策树。

![](img/8c28a2b9a3a2055769e92523dfb6ace4.png)

*图:*线性线拟合非线性数据。*来源* -精英数据科学

如果我们在非线性数据集中拟合线性回归模型，无论我们收集多少数据，线性线都不会对非线性曲线进行建模— ***欠拟合。***

*2。* *方差*

如果使用不同的训练数据，方差是目标函数估计值的变化量。就拿它来说吧，有各种不同口味的苹果取决于它生长的地方的变化。

*来自 EliteDataScience，方差是:“方差是指当算法从数据集学习真实信号的灵活性有限(过拟合)时，算法对训练集的特定集合的敏感度。”*

维基百科称，“……方差是对训练集中的小波动的敏感性产生的误差。高方差会导致算法模拟训练数据中的随机噪声，而不是预期的输出(过度拟合)。”

根据福尔曼的说法:

*——“方差是算法通过拟合高度灵活的模型来学习随机事物而不考虑真实信号的趋势，这些模型过于紧密地跟随数据中的误差/噪声(过度拟合)。”*

![](img/42bbb0f0cdb74ed7d5262796e8233492.png)

来源:EliteDataScience

在这里我们可以看到，模型试图连接每一个点或噪声，并试图与无约束和灵活的模型完美拟合，掩盖所有随机噪声— ***过拟合。***

差异有两种类型:

> 低方差-建议随着训练数据集的更改而对目标函数的估计值进行小的更改。例如线性回归、线性判别分析、逻辑回归。
> 
> 高方差-表示随着训练数据集的变化，目标函数的估计值会发生较大变化。例如 SVC、KNN、决策树。

在方差中，具有更大灵活性的算法试图掩盖每个随机噪声以进行更好的预测，因此，模型最适合训练集，但不能估计目标变量的准确性。它过度拟合了数据。

*3。* *不可约误差*

无论我们使用什么算法，都不能减少不可约误差。这种错误不是由于算法无法拟合数据而发生的，而是由于数据集中的未知变量而发生的。这可以通过适当的数据清理来减少。

***偏差-方差权衡***

我们人类知道我们只有一个地球，生活在地球上的每个人都是同类中的一员，也就是说，与其他任何人都不相同。但是，就像大多数科幻小说和系列一样，有多个宇宙和我们生活的宇宙是一样的。既然有多个宇宙，我们可以说有多个相同的地球，在上面生活着与我们相同的人。现在我们之间有了一个非常可爱的虚构人物“克拉克·肯特(超人)”。他勇敢、善良、善良，是我们的英雄。现在，很有可能每个地球上都有超人。我们的超人很伟大，是个好人，但是你能告诉我们其他地球上的超人也和我们一样好吗？也许也许-不。可能在某个地球上，克拉克·肯特不是超人只是一个记者，在某个地球上他在面包店工作，在某个地球上可能竞选总统候选人或者可能是一个坏的邪恶的超级超人(漫画或虚构电影中的黑人超人)。我们不能肯定地说，他可以是任何东西。如果我们假设他和我们一样是超人，我们就会对他有偏见，会做出错误的决定。如果我们假设，他在不同的地球上扮演不同的角色，并试图根据它来预测值，我们可能会使我们的算法更复杂，这将导致高方差或过度拟合。如果我们使用交叉验证技术，我们可以在许多集合上训练并平均预测，但是我们不能减少过度拟合。这就是偏差-方差权衡的由来。

![](img/67f6061d81a2754cac1996de23001ca8.png)

[廷杰伤害律师事务所](https://unsplash.com/@tingeyinjurylawfirm?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

为了获得更好的结果，每一个监督算法都必须遵循偏差-方差的折衷来做出好的预测。任何监督机器学习算法的目标都是实现低偏差和低方差。反过来，该算法应该实现良好的预测性能。

![](img/9844c6e043aba87f22bafc0ea9fd8842.png)

> *低偏差—高方差*

低偏差和高方差问题是过度拟合。给定各自的数据集，不同的数据集描述不同的见解。因此，这些模型会做出不同的预测。然而，如果平均结果，我们将有一个相当准确的预测。非线性机器学习模型具有低偏差和高方差。例如 SVC、KNN、决策树。

> *高偏置—低方差*

高偏差和低方差问题不适合。不同的算法会有相同的预测，但可能是不准确的。线性机器学习模型具有高偏差和低方差。例如线性回归、逻辑回归、线性判别分析。

![](img/1e63cae461cb24bf16a556cf506e9be2.png)

> 总误差=偏差+方差+不可约误差

处理*高方差*

> 增加训练数据；数据越大，预测就越准确。
> 
> 由于过度拟合，我们可以尝试使用较少的特征。
> 
> 通过增加λ来增加正则化，这将使模型更加正则化。
> 
> 通过进一步调整模型，例如在 KNN，我们可以增加 n_neighbors 值来进行更多预测，而在 SVC 中，我们可以增加 C 参数来控制训练数据中的边界违规，从而增加偏差。

处理*偏高*

> 我们可以尝试增加特征的数量，增加多项式特征也会使模型变得复杂。
> 
> 我们可以降低λ来降低模型的正则化程度，以便更好地拟合数据。

没有合适的方法可以避免这一点，通过调整模型和控制训练特征，我们可能能够减少偏差-方差权衡的影响。

![](img/6d32f48b0d1061df4fefb64a0a8873e8.png)

来源:EliteDataScience

通过优化偏差和方差，我们可以最终实现最适合数据的模型。这是每个机器学习模型的目标，以最佳地拟合数据并提高其性能。

—我是数据科学和机器学习的新手，如果我在这里遗漏了什么，或者写错了什么，请指导我。这对我帮助很大。