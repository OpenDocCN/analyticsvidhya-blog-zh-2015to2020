<html>
<head>
<title>Behind the scenes of MetaNets, from a newbie’s perspective</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从新手的角度看元网络的幕后</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/behind-the-scenes-with-metanets-from-a-newbies-perspective-d5a98507b0d5?source=collection_archive---------16-----------------------#2020-05-25">https://medium.com/analytics-vidhya/behind-the-scenes-with-metanets-from-a-newbies-perspective-d5a98507b0d5?source=collection_archive---------16-----------------------#2020-05-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="bb4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最近，我迷上了元学习的想法，随之而来的是深入研究的需要，我随机地从一篇文章跳到另一篇文章，以便更好地理解它。接下来的一周，我发现了这篇美丽的论文，标题是<strong class="ih hj">元网络</strong> [ <a class="ae jd" href="https://arxiv.org/abs/1703.00837" rel="noopener ugc nofollow" target="_blank"> 1 </a>，作者是于虹的Tsendsuren Munkhdalai。最初，我有点难以理解这个模型是如何工作的，因为它有很多活动部件，至少对我来说是这样。<br/>所以当我最终理解了它背后的理论时，我想我应该记录一个完全业余爱好者对此的看法，这样它可以作为我的业余爱好者的起点，也可以作为我将来需要快速修改时的个人笔记。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h1 id="f359" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">先决条件</h1><p id="38ce" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">我假设你对元学习有基本的了解。如果你没有，不用担心。现在，我将把你们带到一个非常好的系列讲座，即斯坦福大学助理教授切尔西·费恩的<em class="ko">斯坦福CS330:多任务和元学习</em> [ <a class="ae jd" href="https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5" rel="noopener ugc nofollow" target="_blank"> 2 </a>。我会说前三节课足以推进元网络理论。</p><p id="3ae5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我还假设我们已经清楚地了解了元学习范围内的神经网络是如何使用支持集和查询集来训练的，如果你想快速回顾一下，请参考[ <a class="ae jd" href="https://www.borealisai.com/en/blog/tutorial-2-few-shot-learning-and-meta-learning-i/" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]</p><p id="821a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">写这篇文章的目的是为MetaNetworks的论文[ <a class="ae jd" href="https://arxiv.org/abs/1703.00837" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]提供一个更容易的参考点，因此我强烈建议读者在继续阅读之前阅读原文。</p><p id="826e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是我的理解，如果我错了，或者我错过了一些重要的东西，或者有些东西需要更清楚的解释，请纠正我…干杯:)</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h1 id="2240" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">我们开始吧</h1><p id="355d" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">神经网络仍在试图克服用少量训练数据对新任务进行快速概括的挑战，同时保留以前在旧任务中学习到的特征。为了明确我所说的任务是什么意思，我们可以将任务视为一个机器学习问题，其中我们有一些数据和一些损失函数，我们希望对其进行优化，以产生一个合理的模型。<br/>众所周知，标准的神经网络缺乏持续学习的能力，或者在确保不扭曲先前学习的特征的同时，动态地增量学习新概念的能力。</p><p id="bf5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，在我们深入研究MetaNet如何解决这个问题之前，让我们先来看一下它的定义，这些定义可能现在还不清楚，但请记住我现在要说的话，以后会明白的。</p><p id="09a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">元网络(Meta Networks的缩写)跨任务学习元级知识，并通过由<strong class="ih hj">归纳转移</strong>提供的快速参数化来转移其<strong class="ih hj">归纳偏差</strong>。</p><blockquote class="kp kq kr"><p id="16f6" class="if ig ko ih b ii ij ik il im in io ip ks ir is it kt iv iw ix ku iz ja jb jc hb bi translated"><strong class="ih hj">归纳迁移</strong> [ <a class="ae jd" href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_401" rel="noopener ugc nofollow" target="_blank"> src </a> ]指的是学习机制在之前的任务中学习了不同但相关的概念或技能后，提高当前任务绩效的能力。</p><p id="08ea" class="if ig ko ih b ii ij ik il im in io ip ks ir is it kt iv iw ix ku iz ja jb jc hb bi translated">而学习算法的<strong class="ih hj">归纳偏差</strong> [ <a class="ae jd" href="https://en.wikipedia.org/wiki/Inductive_bias" rel="noopener ugc nofollow" target="_blank"> wiki </a> ] <strong class="ih hj"> </strong>则是<strong class="ih hj">学习器</strong>在没有遇到给定输入的情况下用来预测输出的一组假设。</p></blockquote><h2 id="4045" class="kv jm hi bd jn kw kx ky jr kz la lb jv iq lc ld jz iu le lf kd iy lg lh kh li bi translated">我的意思是…</h2><p id="3409" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">因此，根据元学习的正式定义，您在由{Ta，Tb，Tc}组成的某组任务Ts上训练一个模型，并且您期望该模型应该能够动态地消耗新任务{ Tx }的非常有限的训练数据，以获得预测或概括任务Tx本身的分布的能力，这样的例子是少数镜头学习。现在，为了对新任务Tx进行快速概括，<strong class="ih hj"> MetaNet </strong>依赖于一种叫做<em class="ko">快速权重</em>的东西。</p><p id="6f51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们都熟悉神经网络是如何训练的，我们通常在神经网络中有一组参数或权重，它们通过随机梯度下降来更新，通过缓慢改变这些权重来最小化一些损失函数。模型随时间更新以学习特征的权重被称为慢权重。</p><p id="a1a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为任何新任务产生所需最佳权重的一个更快的方法是用另一个神经网络为现有神经网络产生权重，这些产生的权重被称为<em class="ko">快速权重</em>。这是元学习的黑盒适应背后的核心概念。</p><p id="3997" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">元网络试图通过使用两个学习者(即元学习者和基础学习者)来捕捉元学习的本质，我这样说是因为元学习是用两级学习的问题来表述的:</p><ol class=""><li id="ca64" class="lj lk hi ih b ii ij im in iq ll iu lm iy ln jc lo lp lq lr bi translated">独立于任务执行的元级模型的缓慢学习，即元级学习者的目标是使用不同任务的底层共享结构来获取不同任务的一般知识。</li><li id="229e" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated">快速学习在每个任务中起作用的基础级模型，即，知识然后可以从元学习者转移到基础级学习者，以通过让元学习者向基础学习者提供任务特定的快速权重来提供新任务Tx的上下文中的概括。</li></ol><blockquote class="kp kq kr"><p id="84eb" class="if ig ko ih b ii ij ik il im in io ip ks ir is it kt iv iw ix ku iz ja jb jc hb bi translated">学习发生在两个不同的层次(即元空间和任务空间)。基础学习者在输入任务空间中执行，而元学习者在任务不可知的元空间中操作。</p><p id="c8fa" class="if ig ko ih b ii ij ik il im in io ip ks ir is it kt iv iw ix ku iz ja jb jc hb bi translated">-于虹Tsendsuren Munkhdalai</p></blockquote><p id="933f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回到定义，元网络试图在生成的快速权重的帮助下微调或更新基础学习者的假设或权重集，简而言之，通过元学习者执行的归纳转移机制来改变他们的归纳偏差(即他们所学特征的性质)。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="28c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们转向元网络体系结构之前，一些小细节将在后面的章节中派上用场。</p><h2 id="c5a8" class="kv jm hi bd jn kw kx ky jr kz la lb jv iq lc ld jz iu le lf kd iy lg lh kh li bi translated">层增强</h2><p id="acf5" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">标准神经网络符号，即在给定输入x和参数θ的情况下预测y，其中θ表示整个网络的权重。</p><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es lx"><img src="../Images/c1422b47d4f6ee0af4ca4b7aecfa7198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wuO__mTyjG4zQhAxwaJDlg.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">图2:具有3层的标准神经网络的可视化</figcaption></figure><p id="2c79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MetaNet架构中的神经网络通过慢速和快速权重来参数化，慢速和快速权重被组合以使用层增强过程来进行最终预测。</p><p id="2835" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">增强层的输入首先通过慢速和快速权重进行变换，然后通过非线性函数(即，在元网络情况下的Relu ),从而产生由与慢速权重或快速权重相关联的层创建的两个单独的激活向量。最终，激活向量通过逐元素相加来聚集。因此，给定的<em class="ko">y</em>x可以通过单独使用权重或组合使用权重来计算。</p><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mn"><img src="../Images/77dd5e6e0927b14dae2e5d7e5043f88c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_skPetgiRhqpGfE8ovH2hQ.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">图MetaNet中3层神经网络的可视化，具有慢速和快速权重</figcaption></figure><h2 id="7a53" class="kv jm hi bd jn kw kx ky jr kz la lb jv iq lc ld jz iu le lf kd iy lg lh kh li bi translated">接下来是排名损失</h2><p id="d9c5" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">它本身是一个独立的话题，如果你对它有一些基本的了解，它会很有用。因此，请务必通读[ <a class="ae jd" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener ugc nofollow" target="_blank"> 4 </a>，<a class="ae jd" href="https://gombru.github.io/2019/04/03/ranking_loss/" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]，这将帮助您理解什么是度量学习，它的用例是什么&amp;交叉熵损失[ <a class="ae jd" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]或对比损失[ <a class="ae jd" href="https://gombru.github.io/2019/04/03/ranking_loss/" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]意味着什么。在我们继续之前，您需要了解这些基本主题。</p><p id="9ca7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将需要在损失梯度中嵌入输入表示的度量学习，这将作为元信息消费，相信我，这将在以后更有意义。</p><h1 id="09da" class="jl jm hi bd jn jo mo jq jr js mp ju jv jw mq jy jz ka mr kc kd ke ms kg kh ki bi translated">建筑构成</h1><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mt"><img src="../Images/9a499d5df92fa12eaa3e68ba8af77695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0YsOEs-nqFfnQIieIW8-OA.jpeg"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">图4:扩展的MetaNet架构</figcaption></figure><p id="5014" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看一下元学习者和基础学习者的正式定义，稍后我会试着一步一步地给你解释。确保使用图4作为思维导图的参考。</p><p id="559a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">元网络由两个主要的学习组件组成，一个基础学习器和一个元学习器，并配有外部存储器。</p><h2 id="bcc1" class="kv jm hi bd jn kw kx ky jr kz la lb jv iq lc ld jz iu le lf kd iy lg lh kh li bi translated"><strong class="ak">元学习者</strong></h2><p id="271a" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">元学习器由三个神经网络组成:</p><ol class=""><li id="e5ab" class="lj lk hi ih b ii ij im in iq ll iu lm iy ln jc lo lp lq lr bi translated">动态表征学习神经网络<strong class="ih hj">T3】UT5】。与标准神经网络不同，<strong class="ih hj"><em class="ko"/></strong>由慢权值<em class="ko"> </em> <strong class="ih hj"> <em class="ko"> Q </em> </strong>和元级快速权值<strong class="ih hj"> <em class="ko"> Q* </em> </strong> <em class="ko">参数化。<br/></em><strong class="ih hj"><em class="ko"/></strong>将原始输入编码成特征向量<strong class="ih hj"> <em class="ko"> r </em> </strong>，这些特征嵌入被训练成可用于辨别两个输入使用代表性损失嵌入(即交叉熵或对比损失)的不同程度。</strong></li><li id="0822" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated">LSTM网<strong class="ih hj"> <em class="ko"> D </em> </strong>参数化<strong class="ih hj"> <em class="ko"> G </em> </strong>用于学习嵌入函数<strong class="ih hj"><em class="ko"/></strong>的快速权重<strong class="ih hj"><em class="ko">【Q *</em></strong>。它以<strong class="ih hj"><em class="ko"/></strong>的代表损失嵌入的损失梯度作为输入。</li><li id="59cd" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated">由<strong class="ih hj"> <em class="ko"> Z </em> </strong>参数化的神经网络<strong class="ih hj"> <em class="ko"> M </em> </strong>用于学习基学习器的快速权值，即由<strong class="ih hj"> <em class="ko"> W*表示。</em> </strong>快速权重<strong class="ih hj"><em class="ko">【W *】</em></strong>通过使用示例级别损失的梯度作为元信息来产生，该元信息是基础学习者在学习阶段使用支持数据集提供给元学习者的。</li></ol><h2 id="1486" class="kv jm hi bd jn kw kx ky jr kz la lb jv iq lc ld jz iu le lf kd iy lg lh kh li bi translated"><strong class="ak">基础</strong> <strong class="ak">学习者</strong></h2><p id="8dc5" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">与元学习器类似，基学习器由神经网络组成，由慢权重<em class="ko"/><strong class="ih hj"><em class="ko">W</em></strong><em class="ko"/>和任务级快速权重<strong class="ih hj"><em class="ko">【W *</em></strong>参数化。基础学习者是通过任务损失估计主要任务目标的学习者，表示为<strong class="ih hj"> <em class="ko"> B </em> </strong>。慢速权重在训练期间通过学习算法更新，而快速权重由元学习者的神经网络<strong class="ih hj"><em class="ko"/></strong>生成。</p><h2 id="ba3e" class="kv jm hi bd jn kw kx ky jr kz la lb jv iq lc ld jz iu le lf kd iy lg lh kh li bi translated">扩展内存</h2><p id="cc62" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">需要两个存储矩阵来存储每个任务的表示向量<strong class="ih hj"><em class="ko"/></strong>和通过消耗跨N路任务的K-shot支持数据集而产生的快速权重<strong class="ih hj"><em class="ko">【W *】</em></strong>。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h2 id="82a0" class="kv jm hi bd jn kw kx ky jr kz la lb jv iq lc ld jz iu le lf kd iy lg lh kh li bi translated">让我们找到一些清晰</h2><p id="71f1" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">为了更好地理解我们刚刚命名的每个组件的角色，我们可以将注意力集中在它们在MetaNet培训过程中的使用位置和它们完成的任务上，MetaNet培训过程主要包括:</p><ol class=""><li id="0a56" class="lj lk hi ih b ii ij im in iq ll iu lm iy ln jc lo lp lq lr bi translated">元信息的获取</li><li id="4c12" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated">快速权重的生成</li><li id="384c" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated">慢速权重的优化</li></ol><p id="a1b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最初，我们从支持集中随机抽取成对的样本，用于生成元学习者的快速权重。这些随机样本通过神经网络<strong class="ih hj"> <em class="ko"> U </em> </strong>，而仅使用其慢权值<strong class="ih hj"> <em class="ko"> Q </em> </strong>来产生表征损失嵌入，以捕获表征学习目标。</p><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mu"><img src="../Images/8905c235d23a13336eb19b19fee28331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9_pCkfybyz2QahV-FeYzcw.jpeg"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">图5:元学习者中的损失梯度生成</figcaption></figure><p id="7cbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">乍一看，交叉熵似乎与度量学习无关，因为它没有明确涉及成对距离公式。即便如此，仍有一些理论分析将交叉熵与几个成对损失公式联系起来。因此，作者的主张，建议它用于度量学习与表征损失嵌入函数是合理的。</p><p id="f033" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，LSTM神经网络<strong class="ih hj"><em class="ko"/></strong>观察对应于每个采样样本的损失梯度，并产生任务特定参数<strong class="ih hj"> <em class="ko"> Q* </em> </strong>。注意输入到LSTM <strong class="ih hj"> <em class="ko"> D </em> </strong>的顺序并不重要。或者，梯度的总和或平均值可以使用MLP。然而，在初步实验中，作者已经观察到后者导致较差的收敛性。</p><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mv"><img src="../Images/67651ae77e0c9a306976e93a5c415d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4PaJqAAzogaepZDgGa5VDw.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">图6:生成任务级快速权重的损失梯度消耗</figcaption></figure><p id="9372" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们浏览完整的支持集，以计算:</p><ol class=""><li id="9dcc" class="lj lk hi ih b ii ij im in iq ll iu lm iy ln jc lo lp lq lr bi translated">我们从神经网络<strong class="ih hj"> <em class="ko"> B. </em> </strong>得到的任务目标的损失梯度，这个损失梯度将被用作元信息，传递给每个数据点的元学习者。</li><li id="3d15" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated">使用损失梯度生成任务级快速权重，即<strong class="ih hj"> <em class="ko"> W* </em> </strong>。</li></ol><p id="75a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注</strong>:对于神经网络<strong class="ih hj"> <em class="ko"> B </em> </strong>，给定x的y的输出概率分布的损失函数即loss_task可以用交叉熵或MSE来计算。</p><p id="458d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于支持集中的每个数据点，我们计算如下:</p><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mw"><img src="../Images/1915634cbedb1c9c13d97d0b0711342c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zFyXEsoCRVD5wU-XErp5kg.jpeg"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">图7:示例级元信息和任务级特征向量的生成</figcaption></figure><p id="9692" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">稍后，我们将示例特定损失梯度馈送到神经网络<strong class="ih hj"> <em class="ko"> M </em> </strong>，以针对支持集S中的每个数据点产生任务级快速权重，即<strong class="ih hj"> <em class="ko"> W* </em> </strong>，其将被存储在存储器<strong class="ih hj"> M </strong>中的数据点特定索引处。类似于其对应的表示向量<strong class="ih hj"> <em class="ko"> r </em> </strong>如何保存在内存<strong class="ih hj"> R </strong>中。</p><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mx"><img src="../Images/47a2393fa33e2469d41f3f3d3561c5b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F971xGbafGUu1W_DQhjRKg.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">图8:用于生成示例级快速权重的损失梯度消耗</figcaption></figure><p id="f0a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，通过遍历查询/训练集中的所有数据点，我们构造训练损失L_train来优化所有慢权重{<strong class="ih hj"><em class="ko">Q</em></strong><strong class="ih hj"><em class="ko">W</em></strong><strong class="ih hj"><em class="ko">G</em></strong><strong class="ih hj"><em class="ko">Z</em></strong>}。</p><figure class="ly lz ma mb fd mc er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es my"><img src="../Images/d7c1c4eca852d9607836a63ef073ab85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hIFE22tpYT74j0Lkvw5UhQ.jpeg"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">图9:在训练阶段更新慢速权重</figcaption></figure><p id="99b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个单独的任务，重复整个过程。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="2e98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以找到MetaNet的一次性学习训练工作流程的算法视图，这与我们在第3节下提到的[ <a class="ae jd" href="https://arxiv.org/pdf/1703.00837.pdf" rel="noopener ugc nofollow" target="_blank">这里的</a> ]有关，即元网络。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="2d2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我肯定会推荐访问本文的结果部分，因为作者不仅在四组基准测试上运行了MetaNet w.r.t其他最新模型的变体实验，即Omniglot previous split、Mini-ImageNet、MNIST作为域外数据和Omniglot standard split。</p><p id="df2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们还进行了一些有趣的概括测试，例如:</p><ol class=""><li id="078a" class="lj lk hi ih b ii ij im in iq ll iu lm iy ln jc lo lp lq lr bi translated"><strong class="ih hj"> N路训练和K路测试:</strong>在本实验中，MetaNet在N路单射分类任务上进行训练，然后在K路单射任务上进行测试。其中训练类N和测试类K的数量是变化的。</li><li id="dfd8" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated"><strong class="ih hj">固定权重基学习器的快速参数化:</strong>在这个实验中，他们试图证明MetaNet可以学习有效地参数化一个固定权重的神经网络。通过显示跨基础学习者的准确性收敛，与具有固定权重的基础学习者相比，元学习者看到了更多的学习尝试。</li><li id="3fc7" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated"><strong class="ih hj">元级连续学习:</strong>元网在两级学习空间中运行，即示例级空间和元级(梯度)空间。元空间被认为是任务独立的，因此它应该支持终身学习。<br/>为了测试这一理论，作者最初在Omniglot数据集上训练和测试该模型，然后通过切换到MNIST数据集继续训练。在对大量MNIST一次性任务进行训练后，他们在原始Omniglot数据集上重新评估了该模型。<br/>准确性的提高表明元网络增强了逆向迁移学习和持续学习的能力。但只限于MNIST训练中的某一点。之后元权重开始忘记Omniglot信息。</li></ol></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="9d16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你对自己尝试这个模型感兴趣，那么作者已经将他们的代码公之于众。</p><div class="mz na ez fb nb nc"><a href="https://bitbucket.org/tsendeemts/metanet/src/master/" rel="noopener  ugc nofollow" target="_blank"><div class="nd ab dw"><div class="ne ab nf cl cj ng"><h2 class="bd hj fi z dy nh ea eb ni ed ef hh bi translated">比特桶</h2><div class="nj l"><h3 class="bd b fi z dy nh ea eb ni ed ef dx translated">元网络的链接码</h3></div><div class="nk l"><p class="bd b fp z dy nh ea eb ni ed ef dx translated">bitbucket.org</p></div></div></div></a></div></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h2 id="bba4" class="kv jm hi bd jn kw kx ky jr kz la lb jv iq lc ld jz iu le lf kd iy lg lh kh li bi translated">参考资料:</h2><ol class=""><li id="1e2a" class="lj lk hi ih b ii kj im kk iq nl iu nm iy nn jc lo lp lq lr bi translated"><a class="ae jd" href="https://arxiv.org/abs/1703.00837" rel="noopener ugc nofollow" target="_blank">元网【arXiv:1703.00837】，于虹Tsendsuren Munkhdalai</a></li><li id="2ba8" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated"><a class="ae jd" href="https://www.youtube.com/playlist?list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5" rel="noopener ugc nofollow" target="_blank">斯坦福CS330:多任务和元学习，2019年</a></li><li id="162a" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated"><a class="ae jd" href="https://www.borealisai.com/en/blog/tutorial-2-few-shot-learning-and-meta-learning-i/" rel="noopener ugc nofollow" target="_blank">教程-2:少数镜头学习和元学习</a></li><li id="d8af" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated"><a class="ae jd" href="https://gombru.github.io/2018/05/23/cross_entropy_loss/" rel="noopener ugc nofollow" target="_blank">了解分类交叉熵损失</a></li><li id="860b" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated"><a class="ae jd" href="https://gombru.github.io/2019/04/03/ranking_loss/" rel="noopener ugc nofollow" target="_blank">理解排名损失、对比损失、边际损失、三联损失、枢纽损失以及所有这些容易混淆的名称</a></li><li id="f470" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated"><a class="ae jd" href="https://arxiv.org/abs/2003.08983" rel="noopener ugc nofollow" target="_blank">度量学习:交叉熵与成对损失</a></li><li id="7952" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated"><a class="ae jd" href="https://bitbucket.org/tsendeemts/metanet/src/master/" rel="noopener ugc nofollow" target="_blank">元网官方链接代码</a></li><li id="6664" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated"><a class="ae jd" href="https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html" rel="noopener ugc nofollow" target="_blank">元学习:学会快速学习</a>【元学习文献综述】</li></ol></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="8712" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你认为我们是志同道合的人，应该联系，那么你可以在LinkedIn 上找到我，或者发电子邮件到vamshikdshetty@gmail.com找我。如果您有任何想法、问题或反馈，请在下面随意评论，我很乐意收到您的来信。</p></div></div>    
</body>
</html>