<html>
<head>
<title>K-Nearest Neighbor(k-NN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-最近邻</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbor-k-nn-f4418a55b74f?source=collection_archive---------23-----------------------#2020-04-11">https://medium.com/analytics-vidhya/k-nearest-neighbor-k-nn-f4418a55b74f?source=collection_archive---------23-----------------------#2020-04-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/08f71a574319038e3ec928d24ad77f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vppGqsrwph9Yq0Mk2S4bLw.jpeg"/></div></div></figure><p id="06c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">K-最近邻(K-NN)算法是一种简单、易于实现的有监督机器学习算法。k-NN中的“K”指的是它在做出决策时将考虑的最近邻居的数量。它通常被用作分类算法，但是我们已经看到了k-NN被用作回归算法并且优于许多回归算法的情况。</p><p id="3f1f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">K-NN也被称为懒惰学习器，因为它不从训练数据中学习任何东西，而是记忆整个训练数据集，并且在分类时，它对数据集执行操作。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="5898" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">K-NN算法本身相当简单，可以总结为以下步骤:</p><p id="f567" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.选择k的数量和距离度量。距离度量可以是欧几里德、曼哈顿、汉明或闵可夫斯基距离。</p><p id="10ec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.找出我们要分类的样本的k个最近邻。</p><p id="1e0d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.通过多数投票分配类别标签。</p><p id="7a0a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基于所选择的距离度量，k-NN算法在训练数据集中找到与我们想要分类的点最接近(最相似)的k个样本。然后，新数据点的类别标签由其k个最近邻居中的多数投票来确定。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="c651" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们通过scikit-learn页面上列出的一个简单示例来了解k-NN的工作原理。</p><div class="jv jw ez fb jx jy"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="jz ab dw"><div class="ka ab kb cl cj kc"><h2 class="bd hj fi z dy kd ea eb ke ed ef hh bi translated">sk learn . neighbors . kneighborsclassifier-scikit-learn 0 . 22 . 2文档</h2><div class="kf l"><h3 class="bd b fi z dy kd ea eb ke ed ef dx translated">实现k-最近邻投票的分类器。了解更多信息。参数n_neighborsint，可选(默认值=…</h3></div><div class="kg l"><p class="bd b fp z dy kd ea eb ke ed ef dx translated">scikit-learn.org</p></div></div><div class="kh l"><div class="ki l kj kk kl kh km io jy"/></div></div></a></div><pre class="kn ko kp kq fd kr ks kt ku aw kv bi"><span id="4973" class="kw kx hi ks b fi ky kz l la lb">X = [[0], [1], [2], [3]] <br/>y = [0, 0, 1, 1] #class-labels<br/>from sklearn.neighbors import KNeighborsClassifier #import<br/>neigh = KNeighborsClassifier(n_neighbors=3, <em class="lc">metric='</em>Euclidean<em class="lc">'</em>)#k=3<br/>neigh.fit(X, y) #training <br/>print(neigh.predict([[1.1]])) #new or unknown data.<br/>[0] #output</span></pre><p id="2d48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们训练我们的k-nn模型，数据为0，1，2和3，类别标签或输出分别为0，0，1，1。取k的值为3，距离度量为欧几里得。</p><p id="1e2e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在训练我们的模型之后，我们提供了测试数据为1.1的or模型，并且机器计算所有输入数据的欧几里德距离，然后输出结果为0。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="8856" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看看k-NN算法的内部工作原理。欧几里德距离由公式给出。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/053591c16fbb2def4c45e37dfad292cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*AsY9lVJwcnL0OaWp-b5DdQ.jpeg"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">如果p和q是感兴趣的点，那么p和q之间的欧几里德距离</figcaption></figure><p id="da8a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们将计算测试数据(1.1)和训练数据之间的距离，并选择最近的三个距离。</p><pre class="kn ko kp kq fd kr ks kt ku aw kv bi"><span id="4346" class="kw kx hi ks b fi ky kz l la lb">Euclidean Distance  Class-Label</span><span id="0458" class="kw kx hi ks b fi li kz l la lb">(1.1–0)² = 1.21         0</span><span id="a304" class="kw kx hi ks b fi li kz l la lb">(1.1–1)² = 0.01         0</span><span id="7381" class="kw kx hi ks b fi li kz l la lb">(1.1–2)² = 0.81         1</span><span id="d843" class="kw kx hi ks b fi li kz l la lb">(1.1–3)² = 3.61         1</span></pre><p id="60bf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，最接近新点1.1的最小3距离是1.21、0.01和0.81，它们的类别标签分别为0、0、1、1。因此，在多数情况下，新点的类别标签为0。</p><h1 id="f291" class="lj kx hi bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">k-NN的优势</strong></h1><ol class=""><li id="1098" class="mg mh hi is b it mi ix mj jb mk jf ml jj mm jn mn mo mp mq bi translated">简单的</li><li id="3260" class="mg mh hi is b it mr ix ms jb mt jf mu jj mv jn mn mo mp mq bi translated">只有一个要优化的超参数(k)</li></ol><h1 id="cac1" class="lj kx hi bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">k-NN的缺点</h1><ol class=""><li id="a494" class="mg mh hi is b it mi ix mj jb mk jf ml jj mm jn mn mo mp mq bi translated">慢速算法</li><li id="bf1d" class="mg mh hi is b it mr ix ms jb mt jf mu jj mv jn mn mo mp mq bi translated">维度的诅咒</li><li id="eeab" class="mg mh hi is b it mr ix ms jb mt jf mu jj mv jn mn mo mp mq bi translated">对异常值、缺失数据等敏感。</li></ol><h1 id="2d32" class="lj kx hi bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">应用程序</h1><ol class=""><li id="cb45" class="mg mh hi is b it mi ix mj jb mk jf ml jj mm jn mn mo mp mq bi translated">文本挖掘</li><li id="8a76" class="mg mh hi is b it mr ix ms jb mt jf mu jj mv jn mn mo mp mq bi translated">农业</li><li id="2a47" class="mg mh hi is b it mr ix ms jb mt jf mu jj mv jn mn mo mp mq bi translated">金融</li></ol><h1 id="fdff" class="lj kx hi bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">条款预警:</strong></h1><ol class=""><li id="6e84" class="mg mh hi is b it mi ix mj jb mk jf ml jj mm jn mn mo mp mq bi translated"><strong class="is hj">有监督的机器学习</strong>:这是一种向机器提供输入数据和类别标签的学习，机器从这些数据中学习输入和输出之间的关系，并预测未知数据的结果。其他类型的学习包括无监督和强化学习。</li><li id="37ef" class="mg mh hi is b it mr ix ms jb mt jf mu jj mv jn mn mo mp mq bi translated"><strong class="is hj">曼哈顿距离:</strong>沿直角轴测量的两点间的距离。在p1在(x1，y1)且p2在(x2，y2)的平面中，它是|x1 — x2| + |y1 — y2|。</li><li id="c6bf" class="mg mh hi is b it mr ix ms jb mt jf mu jj mv jn mn mo mp mq bi translated"><strong class="is hj">汉明距离:</strong>两个二进制字符串之间不同的位数。更正式地说，两个字符串A和B之间的距离是∑ | Ai — Bi |。</li></ol><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/276b15e8b20e1c436aa0f1e4829a55b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kMVneEWgU9YUAfXRROpNbA.jpeg"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">距离度量</figcaption></figure><p id="b489" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">建议总是受欢迎的。别忘了看看我的其他文章。</p><p id="d598" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae mx" href="https://ak95ml.blogspot.com/2020/06/linear-regression.html" rel="noopener ugc nofollow" target="_blank">从零开始线性回归。</a></p><p id="ac84" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae mx" href="https://www.blogger.com/blog/post/edit/1004924421828631592/5881650886724527591#" rel="noopener ugc nofollow" target="_blank">美汤刮痧</a></p><p id="f37c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae mx" href="https://www.blogger.com/blog/post/edit/1004924421828631592/5881650886724527591#" rel="noopener ugc nofollow" target="_blank">我如何开始我作为机器学习爱好者的旅程</a>。</p></div></div>    
</body>
</html>