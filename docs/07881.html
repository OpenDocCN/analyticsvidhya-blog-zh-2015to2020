<html>
<head>
<title>Logistic Regression — Part III — Titanic Disaster Survival Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归——第三部分——泰坦尼克号灾难生存预测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-part-iii-titanic-disaster-survival-prediction-8473c83e465d?source=collection_archive---------18-----------------------#2020-07-10">https://medium.com/analytics-vidhya/logistic-regression-part-iii-titanic-disaster-survival-prediction-8473c83e465d?source=collection_archive---------18-----------------------#2020-07-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/7475bfe86d5635a4ff6d9dbcc8eb19a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWevRPLcX3ZXwXzXFAmXYQ.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:照片由马克·丹顿在Unsplash上拍摄</figcaption></figure><div class=""/><p id="cdd3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在本文中，我们将使用逻辑回归和分类度量来研究Titanic数据集。</p><p id="2fab" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们看看如何使用来自sklearn的Python-LogisticRegression()进行逻辑回归。</p><p id="d0d4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我从Kaggle那里拿到了泰坦尼克号的数据集。<a class="ae js" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/titanic/data</a></p><figure class="jt ju jv jw fd hk"><div class="bz dy l di"><div class="jx jy l"/></div></figure><figure class="jt ju jv jw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es jz"><img src="../Images/6acc4d7e4ba914f3dce0816cd72ce7b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q545W2cFe6-KXb2vg6yjtA.png"/></div></div></figure><p id="fb1e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里，除了编码，我跳过了数据处理部分。我会提出一个新的职位专门为数据预处理。</p><p id="3e93" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">#1= &gt;删除了Cabin，因为它看起来是大量的空列，并且对于具有高操纵值的列不能接收太多信息。</p><p id="57b4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">#2= &gt;对类别列进行编码。我还将完成验证数据集(test.csv)的所有编码。</p><p id="7710" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">#3= &gt;删除原始列并连接编码的列。</p><figure class="jt ju jv jw fd hk"><div class="bz dy l di"><div class="jx jy l"/></div></figure><figure class="jt ju jv jw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ka"><img src="../Images/e41af646e0a4230ea0aa12e2d400e276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9zlnjvMWuNlkRJcgGKV-vQ.png"/></div></div></figure><p id="d380" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">模型—使用逻辑回归:</p><figure class="jt ju jv jw fd hk"><div class="bz dy l di"><div class="jx jy l"/></div></figure><figure class="jt ju jv jw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kb"><img src="../Images/36e05c5ebea758c8a513b61662c56c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A3FScq8npnMWVLEnqzTxMg.png"/></div></div></figure><p id="c70b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最后，我们使用predict()方法预测了测试数据的幸存值。</p><h1 id="5661" class="kc kd hx bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">韵律学</h1><p id="38c6" class="pw-post-body-paragraph iu iv hx iw b ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn le jp jq jr hb bi translated">要为错误度量导入的包:</p><pre class="jt ju jv jw fd lf lg lh li aw lj bi"><span id="b3a4" class="lk kd hx lg b fi ll lm l ln lo">from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,roc_auc_score</span></pre><h2 id="cd21" class="lk kd hx bd ke lp lq lr ki ls lt lu km jf lv lw kq jj lx ly ku jn lz ma ky mb bi translated">1.混淆矩阵</h2><p id="8999" class="pw-post-body-paragraph iu iv hx iw b ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn le jp jq jr hb bi translated">这是正确预测的清晰表示。所有正确的预测都按对角线顺序排列。</p><figure class="jt ju jv jw fd hk er es paragraph-image"><div class="er es mc"><img src="../Images/355740283206309c4b1379643cee12dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*ZyTzNCROQ2hkDeM4rDebqg.png"/></div></figure><h2 id="8a1e" class="lk kd hx bd ke lp lq lr ki ls lt lu km jf lv lw kq jj lx ly ku jn lz ma ky mb bi translated">2.分类准确度</h2><p id="f16a" class="pw-post-body-paragraph iu iv hx iw b ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn le jp jq jr hb bi translated">此指标衡量正确预测占预测总数的比率。对于更高的精度，该模型给出最好的结果。</p><figure class="jt ju jv jw fd hk"><div class="bz dy l di"><div class="jx jy l"/></div></figure><p id="66e9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">输出:0.804674368685</p><h2 id="f730" class="lk kd hx bd ke lp lq lr ki ls lt lu km jf lv lw kq jj lx ly ku jn lz ma ky mb bi translated">3.ROC曲线和AUC分数</h2><p id="d612" class="pw-post-body-paragraph iu iv hx iw b ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn le jp jq jr hb bi translated"><em class="md"> ROC(受试者工作特征)曲线是假阳性率(x轴)和真阳性率(y轴)的可视化。</em></p><p id="7b7f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">predict_proba(…)在数组中提供概率。pred_prob[:，1]意味着我们只取正值。</p><figure class="jt ju jv jw fd hk"><div class="bz dy l di"><div class="jx jy l"/></div></figure><p id="c1c4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">AUC分数:0.88</p><figure class="jt ju jv jw fd hk er es paragraph-image"><div class="er es me"><img src="../Images/af2fd181b742a874cc7682fdeb9aa170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*AU1p02wE9g94HvIxK_RYdA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">受试者工作特征曲线</figcaption></figure><p id="b1d9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们可以看到ROC曲线还不错。通过更多的预处理，我们可以提高AUC分数。要了解更多ROC曲线，请访问<a class="ae js" rel="noopener" href="/@aasha01/logistic-regression-part-ii-cost-function-error-metrics-bbffbe93eb36?source=your_stories_page---------------------------">逻辑回归第二部分—成本函数&amp;误差度量</a>。</p><h2 id="6170" class="lk kd hx bd ke lp lq lr ki ls lt lu km jf lv lw kq jj lx ly ku jn lz ma ky mb bi translated">4.分类报告</h2><p id="99d3" class="pw-post-body-paragraph iu iv hx iw b ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn le jp jq jr hb bi translated">这是每个类的指标摘要。</p><figure class="jt ju jv jw fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/4608e2d48e3b85c3e47460910a6d65f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*0QWnI9-JrAS9R8paj7_Z0g.png"/></div></figure><p id="f8ec" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在上面的报告中，我们分别给出了0类和1类的精度、召回率和F1值。</p><h1 id="0cb2" class="kc kd hx bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">结论:</h1><p id="565c" class="pw-post-body-paragraph iu iv hx iw b ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn le jp jq jr hb bi translated">在本文中，我们已经了解了如何使用逻辑回归来预测离散值。</p><p id="7b8b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">要了解更多关于逻辑回归的信息:</p><p id="3bcf" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae js" rel="noopener" href="/analytics-vidhya/logistic-regression-part-i-transformation-of-linear-to-logistic-395cb539038b?source=your_stories_page---------------------------">逻辑回归第一部分——线性到逻辑的转换</a></p><p id="5bd6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae js" rel="noopener" href="/@aasha01/logistic-regression-part-ii-cost-function-error-metrics-bbffbe93eb36?source=your_stories_page---------------------------">逻辑回归第二部分—成本函数&amp;误差指标</a>。</p><p id="6606" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请尝试逻辑回归并在此留下您的评论。</p><p id="9ab8" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">谢谢大家！👍</p><p id="019e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">喜欢支持？只需点击拍手图标👏想吃多少就吃多少。</p><p id="ab53" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">编程快乐！🎈</p></div></div>    
</body>
</html>