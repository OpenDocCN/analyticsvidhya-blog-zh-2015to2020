# AWS 中的 ETL 数据管道

> 原文：<https://medium.com/analytics-vidhya/etl-data-pipeline-in-aws-150acd6fee60?source=collection_archive---------1----------------------->

ETL(提取、转换和加载)是所有 IT 行业中的一个新兴话题。行业经常寻找一些简单的解决方案和开源工具和技术来对他们有价值的数据进行 ETL，而无需在其他事情上花费太多精力。

这里有 AWS 胶水，它是 Amazon Web Services 的一个特性，可以创建一个简单的 ETL 管道。

![](img/34753ae2bbb952a1985dcc106afd5314.png)

## AWS 胶水简介

AWS Glue 是 AWS 的另一个产品，是一个在云上的无服务器 ETL(提取、转换和加载)服务。这是一项全面管理、经济高效的服务，可对您的数据进行分类、清理和丰富，并最终将其从源系统移动到目标系统。

AWS Glue 由称为 Glue catalog 的集中式元数据存储库组成，Glue catalog 是一个 ETL 引擎，用于为 ETL 作业生成 Scala 或 Python 代码，并执行作业监控、调度和元数据管理。无需管理任何基础架构，aws 已经很好地管理了这些基础架构。

AWS Glue 非常适合结构化和半结构化数据，它有一个直观的控制台来发现、转换和查询数据。您还可以使用控制台来修改生成的 ETL 脚本，并实时执行它们。

## AWS 胶水的成分

1.  数据目录:它是存储元数据和数据结构的集中目录。
2.  数据库:该选项用于创建数据库，以便将数据从源移动和存储到目标。
3.  Table:该选项允许您在数据库中创建可供源和目标使用的表。
4.  爬虫和分类器:爬虫是 AWS Glue 提供的一个突出特性。
5.  作业:作业是执行 ETL 任务的应用程序。在内部，它使用 Scala 或 Python 作为编程语言，并使用 EMR/EC2 在集群上执行这些应用程序。
6.  触发器:触发器按需或在特定时间启动 ETL 作业执行。
7.  开发端点:开发环境由一个处理 ETL 操作的集群组成。这是一个 EMR 集群，可以连接到笔记本电脑或执行工作。
8.  Notebook: Jupyter notebook 是一个在 web IDE 上开发和运行用于开发和测试的 Scala 或 Python 程序的工具。

## AWS 胶水的主要特点

1.  AWS Glue 在配置作业后自动生成执行 ETL 的代码结构。
2.  您可以修改代码并添加想要对数据执行的额外功能/转换。
3.  AWS crawler 连接到数据源，它自动映射模式并将它们存储在表和目录中。

## 用 AWS 胶水构建 ETL 管道

先决条件:

1.  AWS 帐户
2.  需要对数据和 ETL 过程有基本的了解。

AWS Glue 是对源数据执行 ETL(提取、转换、加载)以移动到目标的完美工具。

在 AWS Glue 中创建 ETL 管道的步骤:

1.  创建一个爬虫
2.  查看表格
3.  配置作业

## 创建 S3 时段并在 S3 时段中上传数据源。

![](img/86aa2b412205e35c64148795ea456e9f.png)

创建存储桶

![](img/f79f5fdcdd28415625ba84fac7916e44.png)![](img/a04503beedd6d077d51a17215b0071e2.png)![](img/04a521ba3ab8e0523b4475acf24af23d.png)![](img/64e4588e5dd0cf1a946a220f7a186705.png)

在创建的存储桶中上传数据源 CSV 文件。

## 创建一个爬虫

1.  登录 AWS 控制台，从搜索选项中搜索 AWS Glue，然后单击打开 AWS Glue 页面。
2.  转到控制台添加爬虫。

![](img/bfdc4389d341f8f1971a7e72793840a0.png)

3.单击添加 crawler 后，请指定 Crawler 名称。

![](img/44b0c9870414744107f0c75778b644da.png)

4.指定名称后，点击**下一个**，在下一个屏幕上，选择数据源，点击**下一个**。

![](img/9b13b052f97404ad5e8f465eb7b9518b.png)

5.在下一个屏幕上，选择数据源为 S3，并指定数据的路径。

![](img/d25e541c7f38086603135e4a649c0272.png)

6.填写完所有信息后，单击 Next，在下一部分中，当要求添加另一个数据源时，选择 No。

![](img/53182671732c62e2b0e536a99bf3370e.png)

7.单击“下一步”后，它会要求您创建一个 IAM 角色来访问 S3 并运行作业。提供角色的名称，然后单击下一步。

![](img/9e53eff219352a0c0a81b99bf66fc4a1.png)

8.一旦您提供了 IAM 角色，它将询问您希望如何调度您的爬虫。

![](img/987c3d624fccef35f38ae131e4868170.png)

9.选择调度程序后，单击 next 并创建输出数据库。

![](img/a28d6663f0222593550684bd8bc6ebbb.png)![](img/f84a8d756562e449c8a0ca86ddb9e674.png)

10.创建数据库后，将会打开 review 页面。检查所有设置，然后单击完成。

![](img/23027544636a97d9145b08c688f137e1.png)![](img/ffc061f09c48234f077a835e45a15f57.png)

11.单击 finish 后，crawler 将立即创建，并且可以运行了。点击运行爬虫，开始执行。

![](img/2af6ca863248d599dfab284544eedfb6.png)

## 查看表格

一旦 crawler 成功执行，您就可以看到在定义的 DB 中创建的表及其元数据。浏览创建的表的步骤—

1.  在 Glue 教程的底部，点击 Explore table。
2.  这将引导您进入表部分，并单击在 flights-db 数据库中创建的 CSV 表。
3.  单击表格，然后单击查看详细信息。

![](img/5a07d40f3791780756919740e28633ca.png)

4.您可以看到该表的所有信息/属性。

![](img/a1b3c2f362aa9d056c07d30023fe3520.png)

5.如果您继续向下滚动到同一个页面，您可以看到从文件中自动提取的表的元数据。

![](img/ce12e584699431b7de31d8bdbbf963af.png)

## 配置作业

1.  在该部分中，您必须配置作业，通过使用 crawler 将数据从 S3 移动到表中。

![](img/e3fea881baf0924c278ce3eb33d79462.png)

2.单击“添加作业”后，将会打开一个作业配置页面。填写所需的详细信息，如作业名称、选择 IAM 角色、执行类型和其他参数。

![](img/02b5c4ff74c572c4b64e8b55fd2c35d7.png)

3.配置完成后，单击下一步。在下一页上，选择数据源“csv ”,然后单击 next。

![](img/cd5dbeee6c83870981b586229325a68b.png)

4.选择转换类型-更改模式。

![](img/ddea790b63a32f33626dee4da39b8237.png)

5.选择数据目标，并选择上面创建的表“csv”。

![](img/2e6bccb89899410707af609481a05656.png)

6.在下一页，您将看到源到目标的映射信息。添加/删除您想要的列。我们建议保留默认映射，然后单击下一步。

![](img/40b2b7f1d9810bb24fc4b43cce03a71b.png)

7.单击 save job，在下一页上，您可以看到该作业的流程图，并可以编辑生成的脚本。

![](img/b84752b58cf9eff6bd16f189d963c7ec.png)

8.您可以使用 AWS Glue 的内置转换特性向数据添加一些预定义的转换。

![](img/957bb2c0471d5d93a4bed0add5e44ab4.png)

9.检查作业的状态，一旦完成，就前往表部分查看数据。

![](img/ef9a233d92bd68bbe39a170518bcce25.png)

10.点击查看数据，它将打开 Athena，并从数据中预览一些记录。

## 结论

在这篇博文中，我们解释了 AWS Glue，以及如何不用任何编码就能创建一个简单的 ETL 管道。