<html>
<head>
<title>Simple Neural Network with BCELoss for Binary classification for a custom Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于定制数据集的二进制分类的具有BCELoss的简单神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/simple-neural-network-with-bceloss-for-binary-classification-for-a-custom-dataset-8d5c69ffffee?source=collection_archive---------1-----------------------#2019-09-17">https://medium.com/analytics-vidhya/simple-neural-network-with-bceloss-for-binary-classification-for-a-custom-dataset-8d5c69ffffee?source=collection_archive---------1-----------------------#2019-09-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/fcde688484c79ff5c7e90f327a17ddbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7YjN0bdWXj-xtjBSCjA1RQ.png"/></div></div></figure><p id="446f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们之前的博客<a class="ae jo" rel="noopener" href="/@bhuvana.kundumani/writing-a-custom-dataloader-for-a-simple-neural-network-in-pytorch-a310bea680af">用Pytorch </a>为自定义数据集(神经网络)编写数据加载器中，我们看到了如何为数据集编写自定义数据加载器。我们将<a class="ae jo" href="https://github.com/bhuvanakundumani/pytorch_Dataloader/tree/master/data" rel="noopener ugc nofollow" target="_blank"> Boston数据集</a>转化为一个分类问题，生成数据批次(数据加载器),准备好输入pytorch神经网络架构。在这篇博客中，我们将关注如何在Pytorch中使用BCELoss来构建一个简单的神经网络。</p><p id="b393" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">预处理后的数据集有12个特征和1个目标变量。我们将有一个具有128个神经元的1个隐藏层的神经网络。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="ea28" class="jy jz hi ju b fi ka kb l kc kd"># Simple Neural network </span><span id="cfe2" class="jy jz hi ju b fi ke kb l kc kd">input_size = 12<br/>hidden_size = 128<br/>num_classes = 1 <br/>num_epochs = 5<br/>learning_rate = 0.001<br/>BATCH_SIZE_1 = 101 #train_loader as it has 404 observations<br/>BATCH_SIZE_2 = 51 #test_loader as it has 102 observations</span></pre><p id="a243" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将使用BCELoss作为损失函数。BCELoss创建了一个衡量目标和输出之间的二元交叉熵的标准。你可以在这里阅读更多关于BCELoss <a class="ae jo" href="https://pytorch.org/docs/stable/nn.html#bceloss" rel="noopener ugc nofollow" target="_blank">的内容</a>。如果我们使用BCELoss函数，我们需要在网络中有一个sigmoid层。sigmoid函数是一种激活函数，更具体地定义为挤压函数。挤压功能将输出限制在0和1之间的范围内。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="169d" class="jy jz hi ju b fi ka kb l kc kd"># Neural Network should have a sigmoid activation function if you are using BCELoss()</span><span id="522a" class="jy jz hi ju b fi ke kb l kc kd">class LinearModel(nn.Module):<br/>    def __init__(self, input_size, hidden_size, num_classes):<br/>        super(LinearModel, self).__init__()<br/>        self.fc1 = nn.Linear(input_size, hidden_size)<br/>        self.fc2 = nn.Linear(hidden_size, num_classes)<br/>        self.relu = nn.ReLU()<br/>                           <br/>    def get_weights(self):<br/>        return self.weight<br/>    <br/>    def forward(self,x):<br/>        out = self.fc1(x)<br/>        out = self.relu(out)<br/>        out = F.sigmoid(self.fc2(out)) #sigmoid as we use BCELoss<br/>        return out</span></pre><p id="303c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们已经有了我们的神经网络架构，让我们看看我们的训练和测试功能。</p><p id="a570" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">列车功能</strong></p><p id="b092" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练函数具有输入参数——模型、设备、训练加载器和优化器。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="1d9b" class="jy jz hi ju b fi ka kb l kc kd">#TRAINING THE NETWORK</span><span id="421e" class="jy jz hi ju b fi ke kb l kc kd">def train(model, device, train_loader, optimizer):<br/>    model.train()<br/>    y_true = []<br/>    y_pred = []</span><span id="b7e3" class="jy jz hi ju b fi ke kb l kc kd">    for i in train_loader:<br/>        <br/>        #LOADING THE DATA IN A BATCH<br/>        data, target = i<br/> <br/>        #MOVING THE TENSORS TO THE CONFIGURED DEVICE<br/>        data, target = data.to(device), target.to(device)<br/>       <br/>        #FORWARD PASS<br/>        output = model(data.float())<br/>        loss = criterion(output, target.unsqueeze(1)) <br/>        <br/>        #BACKWARD AND OPTIMIZE<br/>        optimizer.zero_grad()<br/>        loss.backward()<br/>        optimizer.step()<br/>        <br/>        # PREDICTIONS <br/>        pred = np.round(output.detach())<br/>        target = np.round(target.detach()             <br/>        y_pred.extend(pred.tolist())<br/>        y_true.extend(target.tolist())<br/>        <br/>    print("Accuracy on training set is" ,         <br/>    accuracy_score(y_true,y_pred))</span></pre><p id="c56b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">model.train()告诉程序模型处于训练模式。当我们的网络中有漏失层时，这就变得很重要。默认情况下，PyTorch神经网络模型处于train()模式。只要网络中没有掉线层(或者批量归一化)，就不需要担心train()模式对eval()模式。但是，我通常在我的训练功能中默认使用它。我们将在测试函数中使用model.eval()。</p><p id="61d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们从train_loader加载数据，然后使用<code class="du kf kg kh ju b">output = model(data.float())</code>创建模型</p><p id="75cc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于train_loader的批量大小是101，因此数据和目标将是shapes -torch。尺寸([101，12])手电筒。大小([101])。输出为火炬形状。大小([101，1])。因此，当我们计算损失时，我们必须使用target.unsqueeze(1)将目标的形状改为torch。大小([101，1])</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="e346" class="jy jz hi ju b fi ka kb l kc kd">loss = criterion(output, target.unsqueeze(1))</span></pre><p id="4e9d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们不使用unsqueeze，我们将得到下面的错误- <code class="du kf kg kh ju b">ValueError: Target size (torch.Size([101])) must be the same as input size (torch.Size([101, 1]))</code></p><p id="03c0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们已经计算了损失函数。然后，我们使用下面的命令进行反向传播和优化。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="6e4a" class="jy jz hi ju b fi ka kb l kc kd">#BACKWARD AND OPTIMIZE<br/>optimizer.zero_grad()<br/>loss.backward()<br/>optimizer.step()</span></pre><p id="34cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们必须对训练数据集进行预测，以计算训练集的准确性。我们在这里使用准确性作为性能指标。请注意，我们的数据集存在类别不平衡(简单来说，我们对类别0有422个观察值，对类别1有84个观察值)，因此这不是本例中的最佳度量。我会在博客上发布一篇关于性能指标的详细文章。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="ca21" class="jy jz hi ju b fi ka kb l kc kd"># PREDICTIONS<br/>pred = np.round(output.detach().numpy())<br/>target = target.float()<br/>y_true.extend(target.tolist()) <br/>y_pred.extend(pred.reshape(-1).tolist())</span></pre><p id="89f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在进行预测时，我们需要使用output.detach()。numpy()，否则我们得到下面的错误- <code class="du kf kg kh ju b"><em class="ki">RuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.</em></code></p><p id="59c4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为我们在最后一层有一个sigmoid层，所以我们的输出在0到1的范围内。我们使用Python中的round函数将这些值四舍五入为0.0或1.0。</p><p id="7144" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在<code class="du kf kg kh ju b"><em class="ki">y_pred.extend(pred.reshape(-1).tolist())</em></code>中，shape(-1)用于将pred的大小从(101，1)转换为(101)，tolist()将数组转换为列表，extend用于将列表追加到y_pred。</p><p id="f152" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">类似地，在<code class="du kf kg kh ju b"><em class="ki">y_true.extend(target.tolist()),</em></code> <em class="ki"> </em>中，这里不需要整形(-1)，因为我们在形状[101]中已经有了目标。tolist()将torch张量转换为列表，extend用于将列表追加到y_true。</p><p id="caea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">测试功能</strong></p><p id="c60d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">测试函数有输入参数——模型、设备和测试加载器。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="ad21" class="jy jz hi ju b fi ka kb l kc kd">#TESTING THE MODEL<br/>def test(model, device, test_loader):<br/>    #model in eval mode skips Dropout etc<br/>    model.eval()<br/>    y_true = []<br/>    y_pred = []<br/>    <br/>    # set the requires_grad flag to false as we are in the test mode<br/>    with torch.no_grad():<br/>        for i in test_loader:<br/>            <br/>            #LOAD THE DATA IN A BATCH<br/>            data,target = i<br/>            <br/>            # moving the tensors to the configured device<br/>            data, target = data.to(device), target.to(device)<br/>            <br/>            # the model on the data<br/>            output = model(data.float())<br/>                       <br/>            #PREDICTIONS<br/>            pred = np.round(output)<br/>            target = target.float()<br/>            y_true.extend(target.tolist()) <br/>            y_pred.extend(pred.reshape(-1).tolist())<br/>    <br/>            <br/>    print("Accuracy on test set is" , accuracy_score(y_true,y_pred))<br/>    print("***********************************************************")</span></pre><p id="078a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du kf kg kh ju b"><em class="ki">with torch.no_grad()</em></code>停用亲笔签名。因为在测试模式下，我们不会使用backprop。这也有助于减少内存使用和加快计算速度。</p><p id="96da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们从test_loader加载数据和目标。使用<code class="du kf kg kh ju b"><em class="ki">output = model(data.float())</em></code>创建数据模型</p><p id="9cb9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们必须对测试数据集进行预测，以计算测试数据集的准确性。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="ceb3" class="jy jz hi ju b fi ka kb l kc kd">#PREDICTIONS<br/>pred = np.round(output)<br/>target = target.float()<br/>y_true.extend(target.tolist()) <br/>y_pred.extend(pred.reshape(-1).tolist())</span></pre><p id="b383" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请注意，在测试函数中，我们没有使用<code class="du kf kg kh ju b"><em class="ki">output.detach().numpy()</em></code> <em class="ki"> </em>，因为我们处于评估模式，我们在测试函数中使用了<code class="du kf kg kh ju b"><em class="ki">with torch.no_grad()</em></code>。</p><p id="252b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建模型并设置损失和优化器:</p><p id="b0b4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下命令创建一个模型，设置损失到BCELoss，并使用Adam优化器。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="11b1" class="jy jz hi ju b fi ka kb l kc kd"># Creating model and setting loss and optimizer.<br/>model = LinearModel(input_size, hidden_size, num_classes).to(device)<br/>criterion = nn.BCELoss()<br/>optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span></pre><p id="2032" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们调用5个时期的训练和测试函数，看看我们的模型如何表现。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="eaa1" class="jy jz hi ju b fi ka kb l kc kd">for epoch in range(num_epochs):<br/>        train(model,device,train_loader,optimizer)<br/>        test(model,device,test_loader)</span></pre><p id="4ed6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面给出了执行上述命令时的输出。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kj"><img src="../Images/5271e3500e73af66bb6cbfa416ebc695.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4HAVTwyNcVO7EMJ5Dir3yQ.png"/></div></div></figure><p id="f0f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码可从github repo获得—<a class="ae jo" href="https://github.com/bhuvanakundumani/BCEvsBCEWithLogitsloss.git" rel="noopener ugc nofollow" target="_blank">https://github . com/bhuvanakundumani/bcevsbcewithlogitsloss . git</a></p></div></div>    
</body>
</html>