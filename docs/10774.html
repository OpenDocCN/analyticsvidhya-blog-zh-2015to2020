<html>
<head>
<title>Tweet Sentiment Extraction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推特情感提取</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tweet-sentiment-extraction-e5d242ff93c2?source=collection_archive---------4-----------------------#2020-11-02">https://medium.com/analytics-vidhya/tweet-sentiment-extraction-e5d242ff93c2?source=collection_archive---------4-----------------------#2020-11-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5be53d503a97e8c7988e137cf7b5dab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JQLxFn-FJhShDtHL.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://monkeylearn.com/sentiment-analysis/#:~:text=Sentiment%20analysis%20(or%20opinion%20mining,brand%20reputation%2C%20and%20understand%20customers." rel="noopener ugc nofollow" target="_blank">https://monkey learn . com/情操-分析/#:~:text =情操%20分析% 20(或% 20意见% 20挖掘，品牌% 20信誉% 2C % 20了解% 20客户。</a></figcaption></figure><h1 id="8f6b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">目录</strong></h1><ol class=""><li id="5a09" class="jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">介绍</li><li id="35e4" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">使用ML/DL解决此问题</li><li id="3b0e" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">数据概述</li><li id="342e" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">绩效指标</li><li id="0a7c" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">探索性数据分析</li><li id="ecf1" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">使用深度学习模型来解决这个问题</li><li id="a2f5" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">基础模型</li><li id="63a0" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">改进的基础模型</li><li id="4a64" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">变形金刚和伯特</li><li id="74ab" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">TFRoBERTa问答模型</li><li id="1da2" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">可以做的进一步改进</li><li id="fa9b" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">模型的部署</li><li id="fe29" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">GitHub中的代码</li><li id="83e0" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">参考</li></ol><h2 id="c088" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">介绍</h2><p id="4621" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated">情感分析可以定义为分析文本数据并将其分类为积极、消极或中性情感的过程。情感分析在很多情况下都会用到，比如社交媒体监控、客户服务、品牌监控、政治活动等。通过分析社交媒体对话、产品评论和调查反馈等客户反馈，公司可以更好地了解客户的情绪，这对于满足他们的需求变得越来越重要。</p><h2 id="37a6" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated"><strong class="ak">此问题ML/DL的用法</strong></h2><p id="753e" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated">手动整理成千上万的社交媒体对话、客户评论和调查几乎是不可能的。因此，我们必须使用ML/DL来构建一个分析文本数据并执行所需操作的模型。我在这里试图解决的问题是这场<a class="ae iu" href="https://www.kaggle.com/c/tweet-sentiment-extraction" rel="noopener ugc nofollow" target="_blank"> Kaggle竞赛</a>的一部分。在这个问题中，我们得到了一些文本数据以及他们的情绪(正面/负面/中性)，我们需要找到最能支持这种情绪的短语/单词。</p><h2 id="ebe6" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated"><strong class="ak">数据概述</strong></h2><p id="1b44" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated">这里使用的数据集来自Kaggle竞赛<a class="ae iu" href="https://www.kaggle.com/c/tweet-sentiment-extraction" rel="noopener ugc nofollow" target="_blank">推文情感提取</a>。本次比赛中使用的数据集来自<a class="ae iu" href="https://www.figure-eight.com/data-for-everyone/" rel="noopener ugc nofollow" target="_blank">Figure Figure 8的Data for Everyone platform </a>中的短语。它由两个数据文件train.csv和test.csv组成，其中训练数据有27481行，测试数据有3534行。</p><p id="17ed" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><em class="ly">数据集中的列列表</em></p><p id="79f1" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj"> textID </strong>:每行数据的唯一ID</p><p id="03c8" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj"> text: </strong>该列包含推文的文本数据。</p><p id="9d79" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">感悟:</strong>文本的感悟(正面/负面/中性)</p><p id="f5aa" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj"> selected_text: </strong>文本中最能支持观点的短语/单词</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/16fe026b3882cb62e6d98c3b53fc73f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qt-XaudO0Zod3BB_y-bLlg.png"/></div></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div class="er es me"><img src="../Images/ae0f36318fd8a2e4b6a1cecc312d7dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*ZkNvuXBgstnVwWATwZnCeQ.png"/></div></figure><h2 id="424d" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">绩效指标</h2><p id="371f" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated">这个问题中使用的性能指标是<strong class="jv hj">单词级Jaccard得分</strong>。Jaccard得分或Jaccard相似性是用于理解两个集合之间的相似性的统计数据之一。</p><figure class="ma mb mc md fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/42ed87d6c5b4fb045a7cb770988f0373.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*7uyE7V-urtUJIapXOQHEBA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">https://en.wikipedia.org/wiki/Jaccard_index<a class="ae iu" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="ea41" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">文本数据的Jaccard得分示例:</p><p id="a9e9" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">第一句:</strong>人工智能是我们的朋友，它一直很友好<br/> <strong class="jv hj">第二句:</strong>人工智能和人类一直很友好</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/00eb5bc3bc736f7af2ac062b32152cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UEVUlMAcyFJwg2REZG0uwA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50" rel="noopener" target="_blank">https://towards data science . com/overview-of-text-similarity-metrics-3397 c 4601 f 50</a></figcaption></figure><h2 id="6001" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">探索性数据分析</h2><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/638f9da342323d003d927b2d8e837357.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MoDdBkRAvOOrHzR_W4GoBg.png"/></div></div></figure><p id="20f6" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">从上面的图中，我们可以得出结论，训练数据和测试数据都包含关于情感的类似分布，因为大多数点属于<strong class="jv hj">中性的</strong>，随后是正面和负面文本。</p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/bb3f7aea66b8440649af9585477c587c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_kipBnmsGTBXxqbKc6-B6Q.png"/></div></div></figure><p id="cb39" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">以上可用于推断文本的长度(字数),因为所有情感都在计数<strong class="jv hj">0–20</strong>之间，只有少数点大于25个单词</p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/8debbe4f91e831a793053f7d2ee9642f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*urj2C_7JO7O5DS8eoru3PQ.png"/></div></div></figure><p id="753a" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">对于不同的情绪，<strong class="jv hj"> selected_text </strong>栏的字数会有一定的差异。我们可以在上面三个图中的两个图中看到一个<strong class="jv hj">明显的尖峰</strong>，这表明大多数选定文本短语的长度在<strong class="jv hj">0–10</strong>之间，只有少数句子的长度大于10个单词。<br/>对于<strong class="jv hj">中性</strong>情感，当与正面/负面情感标签相比时，<strong class="jv hj">大多数所选文本短语更长</strong>，因为大多数短语位于0–20的<strong class="jv hj">长度之间。</strong></p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/f4cf704e4c8d96add641a53b016dbb10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3iDYDAq9F8z2K7oZNXcY7Q.png"/></div></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/d979c8c55bc75e58fb1efb3c49d8ec63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gktYTGPRLxHkcrT6QUcXKA.png"/></div></div></figure><p id="f1fb" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">上图显示了不同情感在text和selected_text列中最常见的单词。</p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/dd4942d88e102daa3204646d167c9ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4VpcR-iprn1idnrK0d03gQ.png"/></div></div></figure><p id="e70b" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">对于中性情感，在1.0附近有一个<strong class="jv hj">巨大的峰值</strong>，这意味着大多数中性情感标签的selected_text短语是文本句子本身，即对于大多数中性情感数据，text和selected_text值都是相同的<strong class="jv hj"/>。对于积极和消极情绪标签，我们可以在图表中看到两个峰值，一个在0.1或0.15左右，另一个在1.0左右。0.1/0.15附近的尖峰表明文本和selected_text之间的<strong class="jv hj">相似度</strong>非常低<strong class="jv hj">。即给定一个文本，只有几个单词/短语被认为是selected_text。</strong></p><h2 id="d608" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">使用深度学习模型来解决这个问题</h2><p id="56bc" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated">如果输入数据是序列或上下文数据，最广泛使用的神经网络之一是<strong class="jv hj">递归神经网络(RNN)。</strong>RNN提供的优于普通前馈神经网络的主要优势之一是，RNN不仅考虑当前时间步长的输入，还考虑先前的值来预测当前输出。</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/1dd7655689c19b1c6096574648fb029b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hoi8Uxd3paudwVJJ5G5Lww.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://towardsdatascience.com/understanding-rnn-and-lstm-f7cdf6dfc14e" rel="noopener" target="_blank">https://towards data science . com/understanding-rnn-and-lstm-f 7 CDF 6 DFC 14 e</a></figcaption></figure><p id="dc80" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">从上图中可以看出，在第一个时间步，X⁰作为输入被传递给模型以获得H⁰.在下一个时间步中，x和H⁰都被作为输入传递给模型，该模型给出h作为输出。与普通的神经网络不同，所有的输入都是相互关联的。最常用的RNN网络是<strong class="jv hj">、LSTM(长短期记忆)和GRU(门控循环单元)。</strong></p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/d0bb64f3f1fea375627fa99b7ecb87b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6xl38Fz-_BnsPEf3k8UNg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://towardsdatascience.com/rnn-simplified-a-beginners-guide-cf3ae1a8895b" rel="noopener" target="_blank">https://towards data science . com/rnn-simplified-a-初学者指南-cf3ae1a8895b </a></figcaption></figure><p id="d048" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">我们可能不会在这里深入LSTM和GRU的内部，但我会给出这些RNN网络的一个广泛的概述。</p><p id="74e4" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj"> LSTM: </strong></p><p id="cb33" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">LSTM网络有3个主要入口<br/> 1。忘记门<br/> 2。输入门<br/> 3。输出门</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/e116a09fb9a7fb3cf20f6404a000e71c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFBELMp78VvuRetZlnAzkg.png"/></div></div></figure><p id="5839" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj"> C_(t-1) </strong>:旧单元状态<br/> <strong class="jv hj"> c_t: </strong>当前单元状态<br/> <strong class="jv hj"> h_(t-1): </strong>从前一状态输出<br/> <strong class="jv hj"> h_t </strong> =当前状态输出</p><p id="4575" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">遗忘门</strong>决定需要从先前状态保留多少信息以及可以忽略多少信息<br/> <strong class="jv hj">输入门</strong>决定哪些新信息将被添加到单元状态<br/> <strong class="jv hj">输出门</strong>将决定在下一个实例中什么信息将被传递到网络</p><p id="4670" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">GRU: </p><p id="b3f6" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">LSTM和GRU的主要区别在于GRU只有两个门<strong class="jv hj">更新门和复位门。</strong></p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/269d537ee061915155eff1a82c32c08d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQEQ-1vOJe87N0RF7BsWrQ.png"/></div></div></figure><p id="39fc" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">更新门</strong>是输入门和遗忘门的组合。它决定保留什么信息和添加什么信息。<br/> <strong class="jv hj">复位门</strong>决定在下一个实例中什么信息需要被传递到网络。</p><p id="91c9" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">与LSTM相比，GRU的城门数量较少，因此在计算上比LSTM更便宜更快。</p><h2 id="9df0" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">基本型号:</h2><p id="6409" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated">在这里，我们试图使用LSTM/GRU构建一个基本RNN模型，该模型将<strong class="jv hj">文本</strong>和<strong class="jv hj">情感</strong>作为输入，将<strong class="jv hj">选择文本</strong>作为输出。<br/>我们有需要转换成数字数据的文本格式的输入，这样我们就可以将它传递给模型。此外，我们需要执行<strong class="jv hj">数据清理和预处理</strong>步骤，以便数据是干净的，并为进一步的操作做好准备。</p><p id="53a1" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj"> <em class="ly">将文本转换成整数并填充到相同长度</em> </strong></p><p id="46d4" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">我们有文本栏，包含来自各种推文的文本。每个数据点将具有不同的长度。因此，我们必须将这些文本数据转换成数字，并填充所有的点，以便所有的输入都具有相同的长度。在TensorFlow中，我们有<strong class="jv hj">标记器</strong>和<strong class="jv hj"> pad_sequences </strong>模块可以用来执行这些操作。</p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="0aad" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">嵌入层:</strong></p><p id="39b0" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">嵌入层由嵌入矩阵组成，该矩阵包含训练数据中存在的特定单词的高维表示。</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/291a07d780cfc169e12d6e238c9c2048.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cwflOuHMKnhgORIFguuw3g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" rel="noopener" href="/@Petuum/embeddings-a-matrix-of-meaning-4de877c9aa27">https://medium . com/@ PE tuum/embeddings-a-matrix-of-meaning-4de 877 C9 aa 27</a></figcaption></figure><p id="0696" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">通常，我们使用预训练的嵌入向量，因为这些向量是通过训练大量数据获得的，我们可以简单地下载它们并在我们的嵌入层中使用它们。</p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/0cb88d1bede226a24a30f0fd54f0595e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*qr8_iOJoowem0-l59GjuyQ.png"/></div></figure><p id="34c8" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">以上是基础模型的模型架构，其中模型的输入是文本和情感值。这些值被传递到GRU层，最后，我们有一个密集层，它预测开始和结束索引，以从文本中获取selected_text值。</p><p id="86bd" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">例如:</p><p id="a56d" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">正文</strong>:我今天很开心，因为我买了一部新手机<br/> <strong class="jv hj">情绪</strong>:积极<br/> <strong class="jv hj">选择_正文</strong>:我很开心<br/>开始指数:0 <br/>结束指数:3 <br/>所以如果我给我的模型提供这个输入数据，预测的<strong class="jv hj">输出应该是0，3 </strong>(开始和结束位置)，这对应于正文“我很开心”</p><p id="1d01" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">基本型号的性能</strong></p><p id="e625" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">使用这个基本模型，我们得到了大约0.5的Jaccard分数，当然，模型的性能可以比这提高得多。</p><h2 id="aa2b" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">改进的基础模型:</h2><p id="a210" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated">由于我们的基本模型没有像预期的那样表现良好，我们需要修改我们的基本模型来提高性能。这里我们不使用普通的LSTM/GRU，而是使用<strong class="jv hj">双向LSTM/GRU </strong>。使用双向LSTM优于常规方法的优点在于，双向LSTMs允许网络在每个时间步中都具有关于序列的前向和后向信息。<br/>双向RNNs会以两种方式运行你的输入，一种是从第一个到最后一个，另一种是从最后到第一个。双向rnn有助于保存来自后面的输入状态的信息，而单向rnn仅有助于保存关于前面的输入的信息。</p><figure class="ma mb mc md fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/b8b1810c9fff9d5b3b2bf9ba55525e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*xqHgoQgOEUC4H9cZvbE-EQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://valiancesolutions.com/bi-lstm-explained/#:~:text=Using%20bidirectional%20will%20run%20your,any%20point%20in%20time%20to" rel="noopener ugc nofollow" target="_blank">https://valiance solutions . com/bi-lstm-explained/#:~:text =使用% 20 bidirectional % 20 will % 20 run % 20 your，any % 20 point % 20 in % 20 time % 20 to</a></figcaption></figure><p id="60ce" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">此外，在这个模型中，输出格式将与基本模型不同。这里输出的是len MAX_LEN(max。仪表板的长度)。作为selected_text的一部分的单词将被赋予值1，而其他单词将被赋予值0</p><p id="c758" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">比如:<br/> <strong class="jv hj">正文</strong>:今天好开心因为买了新手机<br/> <strong class="jv hj">感悟</strong>:正面<br/> <strong class="jv hj">选定_正文</strong>:好开心<br/> <strong class="jv hj">输出向量</strong>:1 1 1 0 0 0 0 0</p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/32e9eb132db55b3211912b77c6282922.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*3Se0RhOWGPl5FX15YJ_Dag.png"/></div></figure><p id="208f" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">模型的性能</strong></p><p id="650a" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">这个改进的Seq2Seq模型比基本模型表现得更好，因为我们得到了0.6的Jaccard分数。在分析误差时，我们可以注意到，该模型在中性数据上表现良好，但在正负数据点上表现不佳。</p><h2 id="cabc" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">变形金刚和伯特</h2><p id="f53b" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated">变压器是解决大多数NLP任务的最先进的模型。变压器使用一种叫做<strong class="jv hj">注意</strong>的概念来处理输入和输出之间的依赖关系。注意力机制帮助模型查看输入的句子，并在每个时间步决定序列的其他部分是重要的。更重要的一点是，《变形金刚<strong class="jv hj">在模型中没有使用任何rnn。</strong>这是一个序列到序列架构，即输入被送入编码器，编码器处理输入并传递给预测输出的解码器。</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/6f17692cfddfb8ef3a7c6edc6e0b57de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XheWawPigtzmIUX-4euoXg.png"/></div></div></figure><p id="31a8" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">实际上，变压器是由一堆编码器和解码器组成的。即它包括6个编码器和6个解码器。进一步编码器有<strong class="jv hj">两个子层:多头自关注层和全连接前馈神经网络。解码器</strong>包含三个子层:<strong class="jv hj">自关注层、编码器-解码器关注层和前馈神经网络</strong>。</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/04f4fb9d8c511997f7097f0d074597bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dP6-D63DYWgOIn2HKC19TA.png"/></div></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/d383f67b2db06ba13ee9cfb023499d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_x7ken-CZCSnvPa5DFfGA.png"/></div></div></figure><p id="850e" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">编码器中的<strong class="jv hj">自我关注层</strong>在对特定单词进行编码时，有助于查看输入句子中的其他单词。</p><p id="a74a" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">解码器中的<strong class="jv hj">编码器-解码器注意力层</strong>帮助它关注输入句子的相关部分</p><h2 id="244d" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">关于注意力的简短说明:</h2><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/4fecdb13910bb683ca57e90af777bd40.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*b9dCEtxSfQvWYYi-oY9eeQ.png"/></div></div></figure><p id="5cbe" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">给定一个类似上图中的句子，单词<strong class="jv hj">它</strong>指的是什么？它是指动物、街道还是其他什么东西？人类很容易理解这一点，但这正是注意力层帮助模型理解特定单词如何与文本中的其他单词相关的地方。</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/a29e63a13322a8484b8011b35257ef48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tka4liRy5Yq5YiEGVOR8SQ.png"/></div></div></figure><p id="4b26" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">q是包含查询的矩阵(序列中一个单词的向量表示)，K表示所有的键(序列中所有单词的向量表示)，V是值，也是序列中所有单词的向量表示。<br/>在变压器的架构中，自我关注不是一次而是多次计算，并行且独立。因此被称为<strong class="jv hj">多头关注</strong>。</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/89324c6b1d14e18d246e2dd3cd79af02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0JcIw2zlwWRstXWz9-1FA.png"/></div></div></figure><h2 id="960b" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">变压器双向编码器表示法</h2><p id="5ca3" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated">BERT(来自变压器的双向编码器表示)是谷歌人工智能语言研究人员最近发表的一篇论文。BERT在各种NLP任务中提供了最先进的结果，包括问题回答(SQuAD v1.1)、自然语言推理(MNLI)等。</p><p id="87d9" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">转换器编码器一次读取整个单词序列。因此，它被认为是双向的，虽然说它是非定向的会更准确。这一特性允许模型基于单词的所有周围环境(单词的左侧和右侧)来学习单词的上下文。</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nd"><img src="../Images/91172de0dc7f872ae37c8997ddc6f078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l1hyUQCmLYmkhyXypwslMw.png"/></div></div></figure><p id="bd14" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">第一个输入令牌由特殊的[CLS]提供，其中CLS代表分类。BERT模型中的编码器单元与我们之前在Transformer模型中看到的类似。<br/> BERT将一系列单词作为输入，这些单词继续流经编码器堆栈，每个编码器的输出被传递到下一个编码器模型。<br/>每个位置输出一个大小为(768伯特)的向量。</p><p id="b85b" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">BERT模型还可以针对各种任务进行微调，如序列分类、句子对分类、问答任务、命名实体识别等。自从BERT发明以来，已经提出了几种方法来改进BERT的性能度量或计算速度。下表列出了各种型号的详细信息。</p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/f6b2537425c296348e9f2b4ab4eac514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KqdHxAqkr_qd0smFnAqjbA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8" rel="noopener" target="_blank">https://towards data science . com/Bert-Roberta-distil Bert-xlnet-one-to-use-3d 5 ab 82 ba 5 f 8</a></figcaption></figure><p id="13b9" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">对于这个问题，我使用了RoBERTa(<strong class="jv hj">Ro</strong>bustly optimized<strong class="jv hj">BERT</strong><strong class="jv hj">A</strong>pproach)模型来解决这个问题。由于RoBERTa是基于BERT开发的，所以有很多相同的配置。以下是罗伯塔和伯特之间的一些不同之处。</p><ul class=""><li id="a207" class="jt ju hi jv b jw lt jy lu ka nf kc ng ke nh kg ni ki kj kk bi translated">保留标记:BERT使用<code class="du nj nk nl nm b">[CLS]</code>和<code class="du nj nk nl nm b">[SEP]</code>分别作为起始标记和分隔符标记，而RoBERTa使用<code class="du nj nk nl nm b">&lt;s&gt;</code>和<code class="du nj nk nl nm b">&lt;/s&gt;</code>来转换句子。</li><li id="aa22" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg ni ki kj kk bi translated">子词的大小:BERT有大约30k的子词，而RoBERTa有大约50k的子词。</li><li id="93d6" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg ni ki kj kk bi translated">更大的训练数据(16G对161G)</li><li id="1db9" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg ni ki kj kk bi translated">较长序列的训练</li></ul><p id="d45d" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">BERT使用哪种标记化策略？</strong></p><p id="de2a" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">BERT使用<a class="ae iu" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf" rel="noopener ugc nofollow" target="_blank">词块标记化</a>。用语言中的所有单个字符初始化词汇表，然后迭代地添加词汇表中现有单词的最频繁/最可能的组合。</p><p id="4593" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">伯特如何处理OOV的话？</p><p id="a1f9" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">任何不在词汇表中出现的单词都被贪婪地分解成子单词。例如，如果<strong class="jv hj"> play </strong>、<strong class="jv hj"> ##ing </strong>和<strong class="jv hj"> ##ed </strong>出现在词汇表中，但是<strong class="jv hj">play</strong>和<strong class="jv hj"> played </strong>是OOV单词，那么它们将被分别分解为<strong class="jv hj"> play + ##ing </strong>和<strong class="jv hj"> play + ##ed </strong>。(<strong class="jv hj"> ## </strong>用于表示子词)。</p><h2 id="7579" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">TFRoBERTa问答模型</h2><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nn"><img src="../Images/7ff0560be0f1420f5d3df84ba2fbe887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TP2hYeR87Y2le1AnTO0XpA.png"/></div></div></figure><p id="e9d8" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">这个模型使用一个RoBERTa记号赋予器，它是从GPT-2记号赋予器派生出来的，使用字节级字节对编码。</p><p id="8781" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">如何将我们的问题公式化为问答任务？</strong></p><p id="7df8" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">假设我们的问题是从给定文本中获取selected_text，那么这个问题可以转化为一个问答任务，其中<strong class="jv hj">问题</strong>是情感，<strong class="jv hj">上下文</strong>是文本，<strong class="jv hj">答案</strong>是selected_text</p><p id="7c4f" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">例如:</p><p id="e0ac" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">问题</strong>:正</p><p id="a1d8" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">语境</strong>:我很开心，因为我今天买了一部新手机</p><p id="c74d" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">回答</strong>:我好开心</p><p id="8187" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">在建立模型之前，我们需要了解某些参数。</p><p id="53b3" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj"> input_ids </strong> —词汇表中输入序列标记的索引。</p><p id="3a0c" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">输入id通常是作为输入传递给模型的唯一必需的参数。它们是记号索引，记号的数字表示构成将被模型用作输入的序列。</p><p id="df61" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj"> attention_mask </strong> —避免对填充令牌索引执行注意的掩码。在[0，1]中选择的掩码值:</p><p id="36cf" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">1表示没有被屏蔽的令牌，</p><p id="6fc4" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">0表示被屏蔽的令牌。</p><p id="ad0e" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><a class="ae iu" href="https://gist.github.com/Shriram016/60bbc46238692bb331edf977a436f5a0" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">加载罗伯塔分词器:</strong> </a></p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="7327" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">获取输入标识和注意屏蔽:</strong></p><p id="3d35" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">使用tokenizer.encode_plus()函数可以很容易地获得输入id和注意掩码。encoder_plus以下列格式给出输出。</p><p id="87fc" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><s> <strong class="jv hj">问题</strong>&lt;/s&gt;&lt;/s&gt;<strong class="jv hj">上下文</strong> &lt; /s &gt;</s></p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="9b84" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">获取开始令牌和结束令牌:</strong></p><p id="09b9" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">输入文本被转换成记号，并且对应于selected_text的记号的开始和结束位置被用于形成输出向量。</p><p id="9392" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">示例:</p><p id="557c" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">正文:</strong>可能以前有。此外，没有空调太热了，睡不着</p><p id="06b5" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">精选_正文:</strong>太热了</p><p id="e9c9" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">文本到令牌:</strong> [' &lt; s &gt;'，'也许'，'用过'，'到'，'有'，'.'，'除了'，'没有'，' ac '，'它'，'`'，' s '，'太'，'热'，'到'，'睡'，'&lt; /s &gt; ']</p><p id="5793" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">Roberta tokenizer将给定文本转换为标记。如上所述，<s>和</s> <strong class="jv hj"> </strong>用于表示给定句子的开始和结束。</p><p id="7e76" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">开始令牌:</strong>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0……]</p><p id="3bc5" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">结束令牌:</strong>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0……]</p><p id="eeba" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">开始和结束标记的长度将等于MAX_LENGTH值。</p><p id="4746" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">如果看到令牌列表，第12个和第13个索引处的值对应于selected_text <strong class="jv hj">“太热”。</strong>这就是为什么开始令牌和结束令牌中的第12和第13个索引被标记为1，否则为0。</p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="bf0b" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">为问答模式加载预训练的RoBERTa:</p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="bbf6" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">构建模型:</p><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es no"><img src="../Images/d78e8d5a4a6946fd68da9b8143d5b248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKswBMaughzLyW7HOwb_Kg.png"/></div></div></figure><p id="2138" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">以上是模型体系结构，其中模型的输入是input_ids和attention_mask，模型处理输入并给出开始索引和结束索引，从中可以从给定文本中提取所选文本</p><p id="efe2" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">绩效指标</strong></p><p id="d0e9" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">该TFRoBERTA for Question Answering模型提供了<strong class="jv hj">最佳Jaccard分数0.7，这是所有模型中的最佳分数。</strong></p><p id="97fb" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">可以做的进一步改进</strong></p><p id="c6c6" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated">可以尝试各种其他版本的BERT模型，以查看是否有任何模型比当前模型提供更好的性能(更高的Jaccard分数)。此外，我们可以尝试集成多个模型来提高模型性能。</p><h1 id="4e1c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">模型的部署</h1><p id="a487" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated">将你的模型部署到产品中有多种方法，这里我使用了一种最简单的方法，使用<strong class="jv hj"> Flask来部署模型。</strong></p><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es np"><img src="../Images/082253ce785703d25759e116c7e26c99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iZu9LdKVFhhwcks5viLvlg.png"/></div></div></figure><figure class="ma mb mc md fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nq"><img src="../Images/05c64407d94cc637c756597cbefa528c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUDJohJZZzJJ27n535TUvQ.png"/></div></div></figure><figure class="ma mb mc md fd ij"><div class="bz dy l di"><div class="mg mh l"/></div></figure><h2 id="54ae" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">代码参考:</h2><p id="77f3" class="pw-post-body-paragraph le lf hi jv b jw jx lg lh jy jz li lj ka lk ll lm kc ln lo lp ke lq lr ls kg hb bi translated"><a class="ae iu" href="https://github.com/Shriram016/Tweet-Sentiment-Extraction" rel="noopener ugc nofollow" target="_blank">https://github.com/Shriram016/Tweet-Sentiment-Extraction</a></p><p id="c9c2" class="pw-post-body-paragraph le lf hi jv b jw lt lg lh jy lu li lj ka lv ll lm kc lw lo lp ke lx lr ls kg hb bi translated"><strong class="jv hj">联系人<br/> </strong>邮箱Id:shri16ram@gmail.com<br/>领英:<a class="ae iu" href="https://www.linkedin.com/in/shriram016/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/shriram016/</a><br/>手机:+91–7200681570</p><h2 id="fd70" class="kq iw hi bd ix kr ks kt jb ku kv kw jf ka kx ky jj kc kz la jn ke lb lc jr ld bi translated">参考资料:</h2><ol class=""><li id="6797" class="jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><a class="ae iu" href="https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50" rel="noopener" target="_blank">https://towards data science . com/overview-of-text-similarity-metrics-3397 c 4601 f 50</a></li><li id="e353" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated"><a class="ae iu" href="https://www.kaggle.com/c/tweet-sentiment-extraction/overview/evaluation" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/tweet-情操-提取/概述/评价</a></li><li id="3552" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated"><a class="ae iu" href="https://monkeylearn.com/sentiment-analysis/#:~:text=Sentiment%20analysis%20(or%20opinion%20mining,brand%20reputation%2C%20and%20understand%20customers." rel="noopener ugc nofollow" target="_blank">https://monkey learn . com/情操-分析/#:~:text =情操%20分析% 20(或% 20意见% 20挖掘，品牌% 20信誉% 2C % 20了解% 20客户。</a></li><li id="29ba" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated"><a class="ae iu" href="https://towardsdatascience.com/rnn-simplified-a-beginners-guide-cf3ae1a8895b" rel="noopener" target="_blank">https://towards data science . com/rnn-simplified-a-初学者指南-cf3ae1a8895b </a></li><li id="fdb9" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated"><a class="ae iu" href="https://valiancesolutions.com/bi-lstm-explained/#:~:text=Using%20bidirectional%20will%20run%20your,any%20point%20in%20time%20to" rel="noopener ugc nofollow" target="_blank">https://valiance solutions . com/bi-lstm-explained/#:~:text =使用% 20 bidirectional % 20 will % 20 run % 20 your，any % 20 point % 20 in % 20 time % 20 to</a></li><li id="2046" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated"><a class="ae iu" href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2019/06/understanding-transformers-NLP-state-the-art-models/</a></li><li id="b819" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated">【http://jalammar.github.io/illustrated-transformer/】</li><li id="5563" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated"><a class="ae iu" href="https://yashuseth.blog/2019/06/12/bert-explained-faqs-understand-bert-working/#:~:text=What%20is%20BERT%3F,task%2Dspecific%20fine%2Dtuning" rel="noopener ugc nofollow" target="_blank">https://yashuseth . blog/2019/06/12/Bert-explained-FAQs-understand-Bert-working/#:~:text = What % 20 is % 20 Bert % 3F，task % 2d specific % 20 fine % 2d tuning</a>。</li><li id="336f" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated"><a class="ae iu" href="https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8" rel="noopener" target="_blank">https://towards data science . com/Bert-Roberta-distil Bert-xlnet-one-to-use-3d 5 ab 82 ba 5 f 8</a></li><li id="1dae" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated"><a class="ae iu" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li></ol></div></div>    
</body>
</html>