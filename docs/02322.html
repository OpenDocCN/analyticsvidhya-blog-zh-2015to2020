<html>
<head>
<title>Illustrative Example of Principal Component Analysis(PCA) vs Linear Discriminant Analysis(LDA): Is PCA good guy or bad guy ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析(PCA)与线性判别分析(LDA)的说明性例子:PCA是好人还是坏人？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/illustrative-example-of-principal-component-analysis-pca-vs-linear-discriminant-analysis-lda-is-105c431e8907?source=collection_archive---------7-----------------------#2019-12-11">https://medium.com/analytics-vidhya/illustrative-example-of-principal-component-analysis-pca-vs-linear-discriminant-analysis-lda-is-105c431e8907?source=collection_archive---------7-----------------------#2019-12-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2853dfde864a614d7a9d6e3b42306030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXQ5sgMF0XmiY4Jc6gJVwA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://sebastianraschka.com/Articles/2014_python_lda.html" rel="noopener ugc nofollow" target="_blank">塞巴斯蒂安</a></figcaption></figure><p id="2dd6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">嗨，在这篇文章中，我将解释降维技术是如何影响预测模型的。这里我们使用虹膜数据集和K-NN分类器。我们将在Iris数据集上比较PCA和LDA。</p><p id="40b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在开始实验之前，最好先复习一下PCA和LDA的概念。所以我会试着用简短的笔记来解释它们。让我们从PCA开始。</p><h1 id="b0f4" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">主成分分析</strong></h1><p id="a18f" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">PCA是一种常用于降维的统计工具。它有助于在应用任何ML模型之前将高维数据转换为低维数据。这是一种无监督的学习算法。让我先用一个PCA的例子来解释一下。在下图中，我们从俯视图、仰视图和侧视图中看到一个物体。请注意，一个对象可以有360个视图。如果这个杯子(物体)是数据，那么PCA帮助我们找到杯子的最大部分被看到的视图(方向)。例如，如果只有侧视图和底视图，PCA会给我们一个侧视图，因为可以看到大面积的茶叶。这里，侧视图被认为是第一主成分。得到第一个主分量后，我们在垂直于第一个主分量的方向<strong class="ix hj">旋转杯子。覆盖最大部分并垂直于第一主分量的方向称为第二主分量。这样我们可以找到第三个，第四个等等。</strong></p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/e19a2feef846d4fb44fe2923fbdca592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLxOv5OdAg5L0dYZ_gQ36Q.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图1:一个物体的不同视图(来源:谷歌搜索)</figcaption></figure><h2 id="e870" class="lb ju hi bd jv lc ld le jz lf lg lh kd jg li lj kh jk lk ll kl jo lm ln kp lo bi translated">执行PCA的步骤:</h2><p id="daf7" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">将数据标准化，并找到协方差矩阵。然后求特征向量和各自的特征值。第一主成分无非是具有最大特征值的特征向量，以此类推。</p><h1 id="b6f7" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">线性判别分析(LDA):</h1><p id="b22e" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">LDA是一种监督降维技术。它根据数据做出假设。它是费歇尔线性判别式的推广。LDA找不到主成分。相反，它增加了类间距离，减少了类内距离。关于LDA的详细解释可以在<a class="ae iu" href="http://www.sci.utah.edu/~shireen/pdfs/tutorials/Elhabian_LDA09.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><h2 id="a8fb" class="lb ju hi bd jv lc ld le jz lf lg lh kd jg li lj kh jk lk ll kl jo lm ln kp lo bi translated">加载必要的库</h2><pre class="kx ky kz la fd lw lx ly lz aw ma bi"><span id="8778" class="lb ju hi lx b fi mb mc l md me">from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/>from tqdm import tqdm<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>import seaborn as sn<br/>from sklearn.metrics.pairwise import euclidean_distances<br/>import warnings<br/>warnings.filterwarnings("ignore")</span></pre><h2 id="db8a" class="lb ju hi bd jv lc ld le jz lf lg lh kd jg li lj kh jk lk ll kl jo lm ln kp lo bi translated"><strong class="ak">加载虹膜数据并执行标准化</strong></h2><pre class="kx ky kz la fd lw lx ly lz aw ma bi"><span id="ee88" class="lb ju hi lx b fi mb mc l md me">dataset = pd.read_csv('iris.csv') #read the data into dataframe<br/>X = dataset.iloc[:, :-1].values   #store the dependent features in X<br/>y = dataset.iloc[:, 4].values   #store the independent variable in y<br/>X = StandardScaler().fit_transform(X)</span></pre><h2 id="c6d4" class="lb ju hi bd jv lc ld le jz lf lg lh kd jg li lj kh jk lk ll kl jo lm ln kp lo bi translated"><strong class="ak">执行PCA并可视化数据</strong></h2><pre class="kx ky kz la fd lw lx ly lz aw ma bi"><span id="2fe9" class="lb ju hi lx b fi mb mc l md me"># initializing the pca<br/>from sklearn import decomposition<br/>pca = decomposition.PCA()</span><span id="1a8a" class="lb ju hi lx b fi mf mc l md me"># configuring the parameteres<br/># the number of components = 2<br/># we have taken only 2 components as it is easy to visualize<br/>pca.n_components = 2<br/># pca_reduced will contain the 2-d projects of simple data<br/>pca_data = pca.fit_transform(X)<br/>print("shape of pca_reduced.shape = ", pca_data.shape)</span><span id="eacd" class="lb ju hi lx b fi mf mc l md me">#&gt;&gt;&gt;   shape of pca_reduced.shape =  (150, 2)</span><span id="db2c" class="lb ju hi lx b fi mf mc l md me"># attaching the label for each 2-d data point<br/>pca_data = np.vstack((pca_data.T, y)).T</span><span id="6334" class="lb ju hi lx b fi mf mc l md me"># creating a new data from which help us in ploting the result data<br/>pca_df = pd.DataFrame(data=pca_data, columns=("1st_principal", "2nd_principal", "label"))</span><span id="7812" class="lb ju hi lx b fi mf mc l md me">sn.FacetGrid(pca_df, hue="label", size=4).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()</span><span id="094a" class="lb ju hi lx b fi mf mc l md me">plt.show()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/869d60efbe5881997aa202c050fe4da2.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*WuAwcwsB_fI9toSu5hkfEA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用两个主成分可视化虹膜数据的PCA</figcaption></figure><h2 id="e761" class="lb ju hi bd jv lc ld le jz lf lg lh kd jg li lj kh jk lk ll kl jo lm ln kp lo bi translated"><strong class="ak">绘制主成分数与解释的累积最大方差的关系</strong></h2><pre class="kx ky kz la fd lw lx ly lz aw ma bi"><span id="088d" class="lb ju hi lx b fi mb mc l md me"># PCA for dimensionality redcution (not-visualization)<br/>pca.n_components = 4<br/>pca_data = pca.fit_transform(X)</span><span id="2b47" class="lb ju hi lx b fi mf mc l md me">percentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_)</span><span id="3bc8" class="lb ju hi lx b fi mf mc l md me">cum_var_explained = np.cumsum(percentage_var_explained)</span><span id="2fb6" class="lb ju hi lx b fi mf mc l md me"># Plot the PCA spectrum<br/>plt.figure(1, figsize=(6, 4))<br/>plt.xticks(np.arange(0, 4, step=1),(1,2,3,4))<br/>plt.plot(cum_var_explained, linewidth=2)<br/>plt.axis('tight')<br/>plt.grid()<br/>plt.xlabel('n_components')<br/>plt.ylabel('Cumulative_explained_variance')<br/>plt.show()</span></pre><p id="b9d5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们取一维，大约。解释了72%的方差，如果我们取二维，大约。解释了95%的差异。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/db5426187448cabd3842734f32b0cb33.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*_Ltue020GvQk-CuI8LkRmA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">描绘由特征解释的差异的图</figcaption></figure><h2 id="4d8c" class="lb ju hi bd jv lc ld le jz lf lg lh kd jg li lj kh jk lk ll kl jo lm ln kp lo bi translated">执行LDA并可视化数据</h2><pre class="kx ky kz la fd lw lx ly lz aw ma bi"><span id="38c7" class="lb ju hi lx b fi mb mc l md me">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/>lda = LinearDiscriminantAnalysis(n_components=2)<br/>lda_data = lda.fit(X, y).transform(X)</span><span id="4edf" class="lb ju hi lx b fi mf mc l md me"># attaching the label for each 2-d data point<br/>lda_data = np.vstack((lda_data.T, y)).T</span><span id="4b3e" class="lb ju hi lx b fi mf mc l md me"># creating a new data fram which help us in ploting the result data<br/>lda_df = pd.DataFrame(data=lda_data, columns=("1st_principal", "2nd_principal", "label"))</span><span id="4568" class="lb ju hi lx b fi mf mc l md me">sn.FacetGrid(lda_df, hue="label", size=4).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()</span><span id="4e89" class="lb ju hi lx b fi mf mc l md me">plt.show()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/e98aa421d8e52f5b1da02a93e7eafe29.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*r5zyUVji_uRPVfOE8zSsjw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用线性判别式的用于可视化虹膜数据的LDA</figcaption></figure><h2 id="3f91" class="lb ju hi bd jv lc ld le jz lf lg lh kd jg li lj kh jk lk ll kl jo lm ln kp lo bi translated"><strong class="ak">对原始虹膜数据应用K-NN</strong></h2><pre class="kx ky kz la fd lw lx ly lz aw ma bi"><span id="c801" class="lb ju hi lx b fi mb mc l md me">def divide_training_dataset_to_k_folds(x_train,y_train,folds):<br/> temp = len(x_train)/folds<br/> x_train = x_train.tolist()<br/> y_train = y_train.tolist()<br/> group = []<br/> label = []<br/> end = 0.0<br/> while end &lt; len(x_train):<br/>  group.append(x_train[int(end):int(end + temp)])<br/>  label.append(y_train[int(end):int(end + temp)])<br/>  end += temp<br/> return group,label</span></pre><p id="df0f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">定义随机交叉验证技术:</p><pre class="kx ky kz la fd lw lx ly lz aw ma bi"><span id="f2f7" class="lb ju hi lx b fi mb mc l md me">from sklearn.metrics import accuracy_score<br/>def RandomSearchCV(x_train,y_train,classifier, param_range, folds):<br/> # x_train: its numpy array of shape, (n,d)<br/> # y_train: its numpy array of shape, (n,) or (n,1)<br/> # classifier: its typically KNeighborsClassifier()<br/> # param_range: its a tuple like (a,b) a &lt; b<br/> # folds: an integer, represents number of folds we need to devide the data and test our model<br/> params = list(range(1,51))<br/> #1.divide numbers ranging from  0 to len(X_train) into groups= folds<br/> # ex: folds=3, and len(x_train)=100, we can devide numbers from 0 to 100 into 3 groups i.e: group 1: 0-33, group 2:34-66, group 3: 67-100<br/> temp = len(x_train)/folds<br/> temp = int(temp)<br/>groups,labels = divide_training_dataset_to_k_folds(x_train,y_train, folds)</span><span id="5079" class="lb ju hi lx b fi mf mc l md me"> #2.for each hyperparameter that we generated in step 1 and using the above groups we have created in step 2 you will do cross-validation as follows:<br/> # first we will keep group 1+group 2 i.e. 0-66 as train data and group 3: 67-100 as test data, and find train and test accuracies<br/> # second we will keep group 1+group 3 i.e. 0-33, 67-100 as train data and group 2: 34-66 as test data, and find train and test accuracies<br/> # third we will keep group 2+group 3 i.e. 34-100 as train data and group 1: 0-33 as test data, and find train and test accuracies<br/> # based on the 'folds' value we will do the same procedure<br/> # find the mean of train accuracies of above 3 steps and store in a list "train_scores"<br/> # find the mean of test accuracies of above 3 steps and store in a list "test_scores"</span><span id="e850" class="lb ju hi lx b fi mf mc l md me"> train_scores = []<br/> test_scores  = []<br/> for k in tqdm(params):<br/>  trainscores_folds = []<br/>  testscores_folds = []  <br/>  for i in range(folds):<br/>   X_train = [groups[iter] for iter in range(folds) if iter != i]<br/>   X_train = [j for sublist in X_train for j in sublist]<br/>   Y_train = [labels[iter] for iter in range(folds) if iter != i]<br/>   Y_train = [j for sublist in Y_train for j in sublist]<br/>   X_test  = groups[i]<br/>   Y_test  = labels[i]<br/>   classifier.n_neighbors = k<br/>   classifier.fit(X_train,Y_train)<br/>   Y_predicted = classifier.predict(X_test)<br/>   testscores_folds.append(accuracy_score(Y_test, Y_predicted))<br/>   Y_predicted = classifier.predict(X_train)<br/>   trainscores_folds.append(accuracy_score(Y_train, Y_predicted))<br/>  train_scores.append(np.mean(np.array(trainscores_folds)))<br/>  test_scores.append(np.mean(np.array(testscores_folds)))<br/>#3. return both "train_scores" and "test_scores"<br/> return train_scores, test_scores,params</span></pre><p id="b0d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">k-NN分类器</p><pre class="kx ky kz la fd lw lx ly lz aw ma bi"><span id="fa63" class="lb ju hi lx b fi mb mc l md me">from sklearn.metrics import accuracy_score<br/>from sklearn.neighbors import KNeighborsClassifier<br/>import matplotlib.pyplot as plt</span><span id="cb09" class="lb ju hi lx b fi mf mc l md me">classifier = KNeighborsClassifier()<br/>param_range = (1,50)<br/>folds = 3</span><span id="9b8a" class="lb ju hi lx b fi mf mc l md me">X = dataset.iloc[:, :-1].values#store the dependent features in X<br/>y = dataset.iloc[:, 4].values  #store the independent variable in y<br/>X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, random_state=42,test_size=0.30)<br/>trainscores,testscores,params=RandomSearchCV(X_train,y_train,classifier, param_range, folds)</span><span id="93a7" class="lb ju hi lx b fi mf mc l md me">#  plot hyper-parameter vs accuracy plot as shown in reference notebook and choose the best hyperparameter</span><span id="8c6d" class="lb ju hi lx b fi mf mc l md me">plt.plot(params,trainscores, label='train curve')<br/>plt.plot(params,testscores, label='test curve')<br/>plt.title('Hyper-parameter VS accuracy plot')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/7e39608cc0638c076b9442407a9d6581.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*EbmfuIVsfhyf35oBkF5_cQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">最大测试精度约为。97%</figcaption></figure><h2 id="5aae" class="lb ju hi bd jv lc ld le jz lf lg lh kd jg li lj kh jk lk ll kl jo lm ln kp lo bi translated">基于PCA的K-NN在改进虹膜数据中的应用</h2><pre class="kx ky kz la fd lw lx ly lz aw ma bi"><span id="5a1d" class="lb ju hi lx b fi mb mc l md me">X = pca_df.iloc[:, :-1].values#store all the dependent features in X<br/>y = pca_df.iloc[:, -1].values   #store the independent variable in y</span><span id="abf9" class="lb ju hi lx b fi mf mc l md me">X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, random_state=42,test_size=0.30) #training data = 70% and test data = 30%</span><span id="79aa" class="lb ju hi lx b fi mf mc l md me">trainscores,testscores,params=RandomSearchCV(X_train,y_train,classifier, param_range, folds)</span><span id="8830" class="lb ju hi lx b fi mf mc l md me">#  plot hyper-parameter vs accuracy plot as shown in reference notebook and choose the best hyperparameter</span><span id="2cdc" class="lb ju hi lx b fi mf mc l md me">plt.plot(params,trainscores, label='train curve')<br/>plt.plot(params,testscores, label='test curve')<br/>plt.title('Hyper-parameter VS accuracy plot')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/06c7f07767c79d00d3ef95cdbaa058b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*x_zjf0noPjn-xywTSDhV8Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">最大测试精度约为。91%</figcaption></figure><h2 id="8779" class="lb ju hi bd jv lc ld le jz lf lg lh kd jg li lj kh jk lk ll kl jo lm ln kp lo bi translated">基于LDA的K-NN在改进虹膜数据上的应用</h2><pre class="kx ky kz la fd lw lx ly lz aw ma bi"><span id="2108" class="lb ju hi lx b fi mb mc l md me">X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y, random_state=42,test_size=0.30)</span><span id="49ca" class="lb ju hi lx b fi mf mc l md me">trainscores,testscores,params=RandomSearchCV(X_train,y_train,classifier, param_range, folds)</span><span id="adbe" class="lb ju hi lx b fi mf mc l md me">#  plot hyper-parameter vs accuracy plot as shown in reference notebook and choose the best hyperparameter</span><span id="8d86" class="lb ju hi lx b fi mf mc l md me">plt.plot(params,trainscores, label='train curve')<br/>plt.plot(params,testscores, label='test curve')<br/>plt.title('Hyper-parameter VS accuracy plot')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/bcafb0840f342051cac6a030230ee4d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*nEeYBdM6L6jgln5--AzEyw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">最大测试精度约为。97%</figcaption></figure><h1 id="ae51" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">结论:</strong></h1><p id="aaee" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">综上所述，我们可以从上面的结果中观察到，PCA在标记数据上表现不佳。另一方面，LDA并没有降低K-NN模型的性能，而且降低了数据集的复杂度。由于PCA是无监督的技术，它不考虑类别标签。因此，我们可以得出结论，对于有标记的数据，LDA是比PCA更好的降维技术。</p><p id="4de6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">代码链接:<a class="ae iu" href="https://github.com/GopiSumanth/MachineLearning/blob/master/PCA_%2B_K_NN_on_IRIS.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a></p><p id="9a95" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注:使用LDA的缩减数据集给出了与原始数据集相同的准确度，即:97%。然而，使用PCA的简化数据集给出了非常低的91%的准确度！！！</p></div></div>    
</body>
</html>