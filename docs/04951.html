<html>
<head>
<title>Logistic Regression Algorithm from Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中从头开始的逻辑回归算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-algorithm-from-scratch-with-python-447b17c7502?source=collection_archive---------19-----------------------#2020-04-06">https://medium.com/analytics-vidhya/logistic-regression-algorithm-from-scratch-with-python-447b17c7502?source=collection_archive---------19-----------------------#2020-04-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/40348005aaef313556909b0957338ce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*uS7Eis_bcJ-NeG29LRpYbQ.jpeg"/></div></figure><p id="e6aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">有许多机器学习包和框架可以帮助你训练你的模型，但它们并没有显示幕后发生了什么，在每一步中你的数据发生了什么，以及所涉及的数学，因此在本文中，我将在没有任何框架的情况下从头实现逻辑回归算法。</p><p id="1b8a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我要用的数据集是鸢尾花数据集，你可以在这里找到它。</p><p id="092b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Iris flower数据集是由英国统计学家和生物学家罗纳德·费雪在其1936年的论文《分类问题中多重测量的使用》中引入的多元数据集。它有时被称为安德森虹膜数据集，因为埃德加·安德森收集的数据量化了三个相关物种的虹膜花的形态变化。该数据集由来自三种鸢尾(刚毛鸢尾、海滨鸢尾和杂色鸢尾)的每一种的50个样本组成。测量每个样品的四个特征:萼片和花瓣的长度和宽度，以厘米为单位。</p></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><h1 id="6dc3" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">预处理我们的数据</h1><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="50a6" class="kz jt hi kv b fi la lb l lc ld">import numpy as np<br/>import scipy.optimize as opt<br/>import pandas as pd</span><span id="1451" class="kz jt hi kv b fi le lb l lc ld"># the get the same random order of row<br/>np.random.seed(4)</span><span id="ae44" class="kz jt hi kv b fi le lb l lc ld">#the location of your IRIS.csv<br/>data = pd.read_csv('data/IRIS.csv')</span><span id="edd8" class="kz jt hi kv b fi le lb l lc ld">#replace flowers name by numbers 1,2,3<br/>species={'Iris-setosa':0,'Iris-versicolor':1,'Iris-virginica':2}</span><span id="2f3c" class="kz jt hi kv b fi le lb l lc ld"># reorder the row of the dataset<br/>data  = data.sample(frac=1)</span><span id="4097" class="kz jt hi kv b fi le lb l lc ld">data = data.replace({'species':species})</span><span id="e2b4" class="kz jt hi kv b fi le lb l lc ld">X = data.iloc[:,:-1].values<br/>y = data.iloc[:,-1].values<br/>y = y[:,np.newaxis]</span><span id="2ed9" class="kz jt hi kv b fi le lb l lc ld"># split our data <br/>train_X ,test_X = X[:100,:],X[100:,:]<br/>train_y ,test_y = y[:100,:],y[100:,:]</span></pre><h1 id="c73d" class="js jt hi bd ju jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp bi translated">sigmoid函数</h1><p id="0bfe" class="pw-post-body-paragraph im in hi io b ip lk ir is it ll iv iw ix lm iz ja jb ln jd je jf lo jh ji jj hb bi translated">既然我们在分类问题中，我们假设的结果<strong class="io hj"> h(x) </strong>函数应该只在1或0之间。所以我们需要使用sigmoid函数<strong class="io hj"> g(z) : </strong></p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/68e6de8bf306d3fe543fb548896ff3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*Tux41zTkPotJZz3FnXl4NA.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">假设函数h(x)</figcaption></figure><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="0c8f" class="kz jt hi kv b fi la lb l lc ld">#sigmoid function code <br/>def sigmoid(z) : <br/>    h = 1 / (1 + np.exp(-z))<br/>    return h</span></pre><h1 id="bc1d" class="js jt hi bd ju jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp bi translated">价值函数</h1><p id="1a1c" class="pw-post-body-paragraph im in hi io b ip lk ir is it ll iv iw ix lm iz ja jb ln jd je jf lo jh ji jj hb bi translated">逻辑回归的成本函数表示如下:</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/b61f379166ade4695af86217f37379d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*_OOJfgx2ajZR0FOUxRR9Wg.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">价值函数</figcaption></figure><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="2579" class="kz jt hi kv b fi la lb l lc ld">#cost function <br/>def costFunction(theta, X, y):<br/>    m = X.shape[0]<br/>    h = sigmoid(X @ theta)<br/>    temp1 = np.multiply(y,np.log(h))<br/>    temp2 =np.multiply( (1 - y), np.log(1 - h))<br/>    cost = -(1/m)* np.sum(temp1 + temp2) <br/>    return cost</span></pre><h1 id="d508" class="js jt hi bd ju jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp bi translated">梯度下降</h1><p id="7a35" class="pw-post-body-paragraph im in hi io b ip lk ir is it ll iv iw ix lm iz ja jb ln jd je jf lo jh ji jj hb bi translated">梯度下降不断改变参数以逐渐降低成本函数。随着每次迭代，我们将更接近全局最小值。每次迭代时，参数必须同时调整！“步长”/迭代的大小由参数α(学习速率)决定。我们需要仔细选择α，如果我们选择小α，成本函数会变慢，如果我们选择大α，我们的成本函数将无法收敛</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/423685107ac1098ca4bfef113e729daf.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*uQQcDUkZK0ZAek05Wa6NqQ.png"/></div></figure><p id="0907" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">计算成本函数的导数后，我们得到:</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es lw"><img src="../Images/f6939ba53b6727eb78d7c55cf7c7e039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ByNy7FONioFv69t3faoCg.png"/></div></div></figure><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="eccc" class="kz jt hi kv b fi la lb l lc ld">#gradient descent code<br/>def gradient(theta,X,y):<br/>    m = X.shape[0]<br/>    temp = sigmoid(np.dot(X, theta)) - y<br/>    grad = np.dot(temp.T, X).T / m<br/>    <br/>    return grad</span></pre><h1 id="2838" class="js jt hi bd ju jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp bi translated">成本函数优化</h1><p id="6b98" class="pw-post-body-paragraph im in hi io b ip lk ir is it ll iv iw ix lm iz ja jb ln jd je jf lo jh ji jj hb bi translated">在运行优化函数之前，我们需要初始化参数θ</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="f417" class="kz jt hi kv b fi la lb l lc ld">m = train_X.shape[0] # number of train set row <br/>m_test = test_X.shape[0] # number of test set row</span><span id="bc43" class="kz jt hi kv b fi le lb l lc ld">#add a coulmn of ones the data set (the bias)<br/>train_X = np.hstack(( np.ones((m,1)) ,train_X)) <br/>test_X = np.hstack(( np.ones((m_test,1)) ,test_X))</span><span id="ade8" class="kz jt hi kv b fi le lb l lc ld"># number of classes<br/>k = 3</span><span id="9b3d" class="kz jt hi kv b fi le lb l lc ld">n =train_X.shape[1]</span><span id="f949" class="kz jt hi kv b fi le lb l lc ld"># initialize theta<br/>theta = np.zeros((n,k))</span></pre><p id="01af" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们可以运行我们的优化函数，我将使用scipy <em class="mb"> fmin_cg </em>函数</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="847b" class="kz jt hi kv b fi la lb l lc ld">for i in range(k) :<br/>    theta[:,i] = opt.fmin_cg(<br/>        f=costFunction,<br/>        x0=theta[:,i],<br/>        fprime=gradient,<br/>        args=(train_X,(train_y == i).flatten()),<br/>        maxiter=50<br/>    )</span></pre><p id="8362" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">参数θ的优化值应该是这样的</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="b98a" class="kz jt hi kv b fi la lb l lc ld">print(theta)</span></pre><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/60d44a1da09411aa42a39d536885eb74.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*9xdgY4qbLnJFVFttp3POZg.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">希腊字母的第八字</figcaption></figure><p id="2ea2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在让我们检查模型的准确性，为此我们应该使用测试集数据:</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="90c4" class="kz jt hi kv b fi la lb l lc ld">prediction = np.argmax(test_X @ theta,axis=1) <br/>accuracy = np.mean(prediction == test_y.flatten()) * 100<br/>accuracy</span></pre><p id="8929" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你应该得到100%的准确性，所以我们的模型做得很好</p><p id="d30b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以从零开始实现机器学习的目的是为了获得对机器学习算法中使用的数学的强烈直觉。</p><p id="584d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在下一次，我将使用sklearn解决同样的问题，这样我们可以比较这两种方法，并了解sklearn方法在幕后做什么。</p></div></div>    
</body>
</html>