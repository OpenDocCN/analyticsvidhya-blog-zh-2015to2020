<html>
<head>
<title>Feature Selection: Embedded Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择:嵌入式方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-embedded-methods-a7940036973f?source=collection_archive---------2-----------------------#2020-12-13">https://medium.com/analytics-vidhya/feature-selection-embedded-methods-a7940036973f?source=collection_archive---------2-----------------------#2020-12-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="358a" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">权威指南</h2><div class=""/><div class=""><h2 id="2dcd" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">3 基于嵌入式方法选择相关特性</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/be5a08507f48e6f07665cda4c357cc8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oYqHNNds5crc9_jk"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">照片由<a class="ae jw" href="https://unsplash.com/@edgr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">edu·格兰德</a>在<a class="ae jw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><h2 id="91e1" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">目录</h2><ul class=""><li id="3565" class="ku kv hi kw b kx ky kz la ki lb km lc kq ld le lf lg lh li bi translated"><a class="ae jw" href="#e412" rel="noopener ugc nofollow">嵌入方法</a></li><li id="88e3" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated"><a class="ae jw" href="#3998" rel="noopener ugc nofollow">套索</a></li><li id="f518" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated"><a class="ae jw" href="#a430" rel="noopener ugc nofollow">功能重要性</a></li><li id="ec2c" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated"><a class="ae jw" href="#bc99" rel="noopener ugc nofollow">结论</a></li></ul><p id="c9f4" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated"><em class="md">这篇文章是关于特性选择的博客系列的第三部分，也是最后一部分。看看滤镜(</em><a class="ae jw" href="https://tzinie.medium.com/feature-selection-73bc12a9b39e" rel="noopener"><em class="md">part 1</em></a><em class="md">)和包装器(</em><a class="ae jw" rel="noopener" href="/analytics-vidhya/feature-selection-85539d6a2a88"><em class="md">part 2</em></a><em class="md">)的方法。</em></p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="e412" class="ml jy hi bd jz mm mn mo kd mp mq mr kh ix ms iy kl ja mt jb kp jd mu je kt mv bi translated">嵌入式方法</h1><p id="e097" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated">嵌入式方法结合了过滤器和包装器方法的优点。如果您仔细研究这三种不同的方法，您可能会想知道包装方法和嵌入方法之间的核心区别是什么。</p><p id="639c" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">乍一看，两者都是基于机器学习模型的学习过程来选择特征。然而，包装方法基于评估度量反复考虑不重要的特征，而<strong class="kw hs">嵌入式方法并行执行特征选择和算法训练</strong>。换句话说，特征选择过程是分类/回归模型的组成部分。</p><p id="0bf5" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">包装器和过滤器方法是离散的过程，从某种意义上说，特征要么被保留，要么被丢弃。然而，这通常会导致较高的方差。另一方面，嵌入式方法更加连续，因此不会受到高可变性的影响。</p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="3998" class="ml jy hi bd jz mm mn mo kd mp mq mr kh ix ms iy kl ja mt jb kp jd mu je kt mv bi translated">套索</h1><p id="e9a9" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated">最小绝对收缩和选择算子(LASSO)是一种同时执行变量选择和正则化的<em class="md">收缩</em>方法。我知道，这听起来很奇怪，但很快你就会意识到这只是 L1 正则化的线性回归。但是让我们一次做一件事，从 L1 正则化开始。</p><p id="23c7" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">正则化是<em class="md">将系数(权重)向零收缩</em>的过程。这到底是什么意思？这意味着你在惩罚更复杂的模型以避免过度拟合。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mz"><img src="../Images/a509007d016bf2b89701f01e43748e90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7OPgojau8hkiPUiHoGK_w.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">如果你没注意到，我们想要中间的那个。</figcaption></figure><p id="d2d4" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">但是这如何转化为特征选择呢？您可能听说过其他正则化技术，如岭回归或弹性网，但 LASSO 允许将系数设置为 0。如果系数为零，则不考虑该特征，因此，它在某种程度上被丢弃。</p><blockquote class="na nb nc"><p id="bc83" class="lo lp md kw b kx lq is lr kz ls iv lt nd lu lv lw ne lx ly lz nf ma mb mc le hb bi translated">注:弹性网是套索和岭回归的结合。这意味着它还可以执行特征选择。岭回归不能做到这一点，因为它只允许系数非常接近零，但实际上永远不会为零。</p></blockquote><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ng"><img src="../Images/853a3b6b5c80dde1aff50d88b275ddd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*l2RtKPDGeQP-QkQDpeK1Ig.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">套索目标:第一任期；RSS |第二学期；L1 常模</figcaption></figure><p id="c32c" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">复杂度参数λ(λ)是非负的，并且控制收缩量。其值越大，收缩量越大，您构建的模型就越简单。当然，它的值是一个您应该调整的超参数。</p><blockquote class="na nb nc"><p id="3fb0" class="lo lp md kw b kx lq is lr kz ls iv lt nd lu lv lw ne lx ly lz nf ma mb mc le hb bi translated">惩罚的思想被用在许多算法中，包括神经网络；我们称之为<em class="hi">重量衰减</em>。</p></blockquote><p id="0465" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">让我们看一个例子……我从<a class="ae jw" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" rel="noopener ugc nofollow" target="_blank"> sklearn </a>(二元分类任务)中选择了乳腺癌数据集。数据集中只有 569 个样本，因此，我们将对每个步骤进行分层交叉验证。</p><pre class="jh ji jj jk fd nh ni nj nk aw nl bi"><span id="113a" class="jx jy hi ni b fi nm nn l no np">from sklearn.linear_model import LassoCV<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import StratifiedKFold</span><span id="6675" class="jx jy hi ni b fi nq nn l no np">cancer = load_breast_cancer()</span><span id="2231" class="jx jy hi ni b fi nq nn l no np">X = cancer.data<br/>y = cancer.target</span><span id="0eb3" class="jx jy hi ni b fi nq nn l no np">skf = StratifiedKFold(n_splits=10<br/>lasso = LassoCV(cv=skf, random_state=42).fit(X, y)</span><span id="edc2" class="jx jy hi ni b fi nq nn l no np">print('Selected Features:', list(cancer.feature_names[np.where(lasso.coef_!=0)[0]]))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nr"><img src="../Images/8e66d86cb7de936f391c65f4843d1572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iMNbGY6pT0V9giZFiSdWrQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">套索:选定的特征</figcaption></figure><pre class="jh ji jj jk fd nh ni nj nk aw nl bi"><span id="03cc" class="jx jy hi ni b fi nm nn l no np">lr = LogisticRegression(C=10, class_weight='balanced', max_iter=10000, random_state=42)<br/>preds = cross_val_predict(lr, X[:, np.where(lasso.coef_!=0)[0]], y, cv=skf)<br/>print(classification_report(y, preds))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ns"><img src="../Images/0092f8e0210452a18a59e07ae52eea86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*igibBv5eja1CGCcflW4cbA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">分类报告；套索</figcaption></figure></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="a430" class="ml jy hi bd jz mm mn mo kd mp mq mr kh ix ms iy kl ja mt jb kp jd mu je kt mv bi translated">特征重要性</h1><p id="eea1" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated"><em class="md">的特色重要性</em>有印象吗？</p><p id="1a42" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">通过几行代码，我们真的可以得到特征空间的要点——提取有用的信息——从而得到一个更好、更易解释的模型。我们有两条路可以走；基于树的方法和排列重要性。</p><h2 id="4ed3" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">基于树的方法</h2><p id="b5c6" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated">决策树、RandomForest、ExtraTree、XGBoost 是可以用来获得特征重要性的一些基于树的方法。我最喜欢的是 RandomForest 和 Boosted Trees (XGBoost，LightGBM，CatBoost ),因为它们通过引入随机性来改善简单决策树的方差。</p><p id="4176" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated"><strong class="kw hs">但是怎么做呢？</strong></p><p id="35f0" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">为了回答这个问题，我继续选择随机森林作为我的算法。随机森林使用<strong class="kw hs">均值减少杂质</strong> <em class="md">(基尼指数)</em> <strong class="kw hs"> </strong>来估计一个特征的重要性。值越低，特性越重要。基尼指数的定义是:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nt"><img src="../Images/dca9f13c29d3248bab363fdc0873b728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RnrL7akbekezdWQwpFKdlg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">基尼指数公式</figcaption></figure><p id="dddc" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">其中第二项是样本<code class="du nu nv nw ni b">i</code>的每类概率的平方和。为使用特征<code class="du nu nv nw ni b">j</code>的树的每个节点测量特征<code class="du nu nv nw ni b">j</code>的基尼指数，并对集合中的所有树进行平均。如果到达该节点的所有样本都与单个类相链接，那么该节点可以被称为<strong class="kw hs">纯</strong>。点击阅读更多相关信息<a class="ae jw" href="https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb" rel="noopener" target="_blank">。</a></p><pre class="jh ji jj jk fd nh ni nj nk aw nl bi"><span id="0273" class="jx jy hi ni b fi nm nn l no np">from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.feature_selection import SelectFromModel<br/>import matplotlib.pyplot as plt</span><span id="f56d" class="jx jy hi ni b fi nq nn l no np">cancer = load_breast_cancer()</span><span id="8311" class="jx jy hi ni b fi nq nn l no np">X = cancer.data<br/>y = cancer.target</span><span id="de7e" class="jx jy hi ni b fi nq nn l no np">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<strong class="ni hs">0.2</strong>, random_state=42, stratify=y)</span><span id="4f44" class="jx jy hi ni b fi nq nn l no np">rf = RandomForestClassifier(n_estimators = 100, class_weight='balanced', random_state=42)<br/>rf.fit(X_train, y_train)</span><span id="d7b8" class="jx jy hi ni b fi nq nn l no np">importances = rf.feature_importances_<br/>indices = np.argsort(importances)[::-1]</span><span id="9ae6" class="jx jy hi ni b fi nq nn l no np">plt.figure()<br/>plt.title("Feature importances")<br/>plt.bar(range(X_train.shape[1]), importances[indices],<br/>        color="lightsalmon", align="center")<br/>plt.xticks(range(X_train.shape[1]), cancer.feature_names[indices], rotation=90)<br/>plt.xlim([-1, X_train.shape[1]])<br/>plt.show()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nx"><img src="../Images/17d503ddb944901dae3a489a20f8cb36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F_ExGYodooYALtY_T4YuWg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">特征重要性；随机森林</figcaption></figure><p id="6f70" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">当根据要素的重要性选择要素时，这可以很好地估计您想要设置的阈值。我选择 0.06，这导致 7 个特征。</p><pre class="jh ji jj jk fd nh ni nj nk aw nl bi"><span id="847b" class="jx jy hi ni b fi nm nn l no np">sfm = SelectFromModel(rf, threshold=0.06)</span><span id="e680" class="jx jy hi ni b fi nq nn l no np">sfm.fit(X_train, y_train)</span><span id="e80b" class="jx jy hi ni b fi nq nn l no np">X_important_train = sfm.transform(X_train)<br/>X_important_test = sfm.transform(X_test)</span><span id="8924" class="jx jy hi ni b fi nq nn l no np">rf = RandomForestClassifier(n_estimators = 100, class_weight='balanced', random_state=42)</span><span id="d930" class="jx jy hi ni b fi nq nn l no np">rf.fit(X_important_train, y_train)<br/>y_pred = rf.predict(X_important_test)</span><span id="b701" class="jx jy hi ni b fi nq nn l no np">print(classification_report(y_test, y_pred))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ny"><img src="../Images/a2e4791d908e40892a0935b1ba53d0ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_F965YHShHkgtSE4m_1B3g.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">分类报告；随机森林</figcaption></figure><blockquote class="na nb nc"><p id="372e" class="lo lp md kw b kx lq is lr kz ls iv lt nd lu lv lw ne lx ly lz nf ma mb mc le hb bi translated">你可以简单地用<code class="du nu nv nw ni b">XGBClassifier()</code>替换<code class="du nu nv nw ni b">RandomForestClassifier()</code>，对任何提升树算法重复同样的操作</p></blockquote><h2 id="390c" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">排列重要性</h2><p id="ef00" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated">一个不太出名但仍然值得探索的想法。该方法改变一个特征的值，然后测量模型误差的增加。当且仅当洗牌导致增加时，该特征才是重要的。这些步骤如下:</p><ol class=""><li id="593f" class="ku kv hi kw b kx lq kz ls ki nz km oa kq ob le oc lg lh li bi translated">照常训练模型。估算误差:<code class="du nu nv nw ni b">L(y, f(X))</code>；<code class="du nu nv nw ni b">L</code>:丢失，<code class="du nu nv nw ni b">y</code>:真实目标向量，<code class="du nu nv nw ni b">f(X)</code>；估计目标向量。</li><li id="0544" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le oc lg lh li bi translated">对于输入特征向量中的每个特征<code class="du nu nv nw ni b">j</code>，生成一个新的输入矩阵<code class="du nu nv nw ni b">X'</code>，其中特征<code class="du nu nv nw ni b">j</code>被置换——以断开特征<code class="du nu nv nw ni b">j</code>与目标变量<code class="du nu nv nw ni b">y</code>的关联。用新的输入矩阵再次估算误差:<code class="du nu nv nw ni b">L(y, f(X'))</code>。通过简单的操作计算重要性:<code class="du nu nv nw ni b">L(y, f(X'))-L(y, f(X))</code>。</li><li id="fb10" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le oc lg lh li bi translated">根据要素的重要性对其进行排序。</li></ol><pre class="jh ji jj jk fd nh ni nj nk aw nl bi"><span id="47de" class="jx jy hi ni b fi nm nn l no np">from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="b678" class="jx jy hi ni b fi nq nn l no np">import eli5<br/>from eli5.sklearn import PermutationImportance</span><span id="653e" class="jx jy hi ni b fi nq nn l no np">cancer = load_breast_cancer()</span><span id="da76" class="jx jy hi ni b fi nq nn l no np">X = cancer.data<br/>y = cancer.target</span><span id="d05a" class="jx jy hi ni b fi nq nn l no np">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<strong class="ni hs">0.2</strong>, random_state=42, stratify=y)</span><span id="9540" class="jx jy hi ni b fi nq nn l no np">rf = RandomForestClassifier(n_estimators = 100, class_weight='balanced', random_state=42)<br/>rf.fit(X_train, y_train)</span><span id="8caf" class="jx jy hi ni b fi nq nn l no np">perm = PermutationImportance(rf, random_state=42).fit(X_test, y_test)</span><span id="319d" class="jx jy hi ni b fi nq nn l no np">eli5.show_weights(perm, features_names=cancer.feature_names)</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es od"><img src="../Images/aa62195188669dfbe852c3d12a19fd54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B38frwmye7Y0FgBTvr3eHw.png"/></div></div></figure><p id="a68d" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">我选择重要性高于 0.08 的特征。</p><pre class="jh ji jj jk fd nh ni nj nk aw nl bi"><span id="c82d" class="jx jy hi ni b fi nm nn l no np">rf = RandomForestClassifier(n_estimators = 100, class_weight='balanced', random_state=42)<br/>preds= cross_val_predict(rf, X[:, np.where(perm.feature_importances_&gt;=0.008)[0]], y, cv=skf)<br/>print(classification_report(y, preds))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es oe"><img src="../Images/1128c8aa2cb73f62b1a6e5939c1300f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GzCT8WY30XzsnLylCQN2tA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">分类报告；排列重要性</figcaption></figure><p id="3750" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">这种方法很少用于选择特征，而是出于可解释性的原因。</p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><p id="167b" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">如果我们不执行特征选择会发生什么？</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es of"><img src="../Images/bc3167255811a135f5c61e9addb443fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4EP-aTKYNmyJWrsGLXO8vA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">分类报告；无特征选择</figcaption></figure><p id="bd97" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">嗯🤔…好吧，性能方面没有明显优势。但是，请始终记住，每种方法都会导致更小的特征空间，从而导致更快的训练和更多可解释的模型。因此，虽然我们在性能上没有改进，但我们通过使用 4 个功能而不是 30 个功能获得了相同的结果！</p><h1 id="bc99" class="ml jy hi bd jz mm og mo kd mp oh mr kh ix oi iy kl ja oj jb kp jd ok je kt mv bi translated">结论</h1><p id="70ba" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated">到本系列的结尾，我们可以得出结论，特性选择就是通过我们在不同文章中描述的过程，自动保持相关和有用的特性。我们从不同的角度研究特征选择任务；从统计学到机器学习方法。总的来说，它可以产生更好的模型，实现更高的性能和更好的可解释性。最后但并非最不重要的是，拥有一个特征子集使机器学习算法能够更快地训练。</p></div></div>    
</body>
</html>