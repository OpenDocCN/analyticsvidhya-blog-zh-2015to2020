<html>
<head>
<title>Understanding BERT architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解伯特建筑</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-bert-architecture-3f35a264b187?source=collection_archive---------1-----------------------#2019-11-10">https://medium.com/analytics-vidhya/understanding-bert-architecture-3f35a264b187?source=collection_archive---------1-----------------------#2019-11-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/eca4196e1fdf8ae9501a722d1bea18bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*y1R4X-8WNUmpRvx0"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">安妮·斯普拉特在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="b7ba" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT </a>可能是近年来NLP中最令人兴奋的发展之一。就在上个月，甚至谷歌也宣布在其搜索中使用BERT，这应该是过去五年中它在理解搜索方面的“最大飞跃”。这是来自谷歌的一个巨大证明。关于搜索！这就是伯特的重要性。</p><p id="bca2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在有一些惊人的资源可以详细了解BERT、变形金刚和注意力网络(注意力和变形金刚是BERT的积木)。我在脚注中把它们联系起来。<strong class="ix hj">这篇文章是关于更好地理解架构和参数</strong>，<em class="jt">一旦你已经相当好地理解了BERT</em>。</p><p id="aa38" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">幸运的是，使用Keras(和keras_bert ),这个模型很容易加载到Python中。下面的代码加载模型，并打印所有层的摘要。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="b42f" class="kd ke hi jz b fi kf kg l kh ki">import keras<br/>from keras_bert import get_base_dict, get_model, compile_model, gen_batch_inputs</span><span id="8417" class="kd ke hi jz b fi kj kg l kh ki"># Build &amp; train the model<br/>model = get_model(<br/>    token_num=30000,<br/>    head_num=12,<br/>    transformer_num=12,<br/>    embed_dim=768,<br/>    feed_forward_dim=3072,<br/>    seq_len=512,<br/>    pos_num=512,<br/>    dropout_rate=0.05,<br/>)<br/>compile_model(model)<br/>model.summary()</span></pre><p id="5e43" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我用过的所有参数，包括令牌数，这里都是来自BERT base模型(BERT有两个变体，一个小变体叫base，另一个叫large)。这些参数是(记住符号，因为我们稍后将使用它们):</p><ol class=""><li id="ac17" class="kk kl hi ix b iy iz jc jd jg km jk kn jo ko js kp kq kr ks bi translated">令牌数(T) = 30k。这是从单词块标记化派生的不同标记的编号。这将单个单词分解成组成单词，以提高覆盖率。例:播放转换为(播放，##ing)。因此，只要模型知道“睡眠”这个词，它就可以推断出“睡眠”的含义，即使它是第一次看到这个词</li><li id="6ac3" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">head_num (A) = 12。每个变压器层总共12个注意头</li><li id="a0f0" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">变压器数量(L) = 12</li><li id="827d" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">embed_dim (H) =嵌入长度=768</li><li id="27cd" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">前馈Dim (FFD) = H*4 =3072</li><li id="161e" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">seq_len (S)=输入句子中可以包含的最大标记数= 512</li><li id="168f" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">pos_num (P) =要编码的位置= S = 512</li></ol><p id="7480" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这应该会给你一个很长的关于BERT中所有层的总结，看起来像这样:</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="125e" class="kd ke hi jz b fi kf kg l kh ki">Layer (type)                    Output Shape         Param #     Connected to                     <br/>==================================================================================================<br/>Input-Token (InputLayer)        (None, 512)          0                                            <br/>__________________________________________________________________________________________________<br/>Input-Segment (InputLayer)      (None, 512)          0                                            <br/>__________________________________________________________________________________________________<br/>Embedding-Token (TokenEmbedding [(None, 512, 768), ( 23040000    Input-Token[0][0]                <br/>__________________________________________________________________________________________________<br/>Embedding-Segment (Embedding)   (None, 512, 768)     1536        Input-Segment[0][0]              <br/>__________________________________________________________________________________________________<br/>Embedding-Token-Segment (Add)   (None, 512, 768)     0           Embedding-Token[0][0]            <br/>                                                                 Embedding-Segment[0][0]          <br/>__________________________________________________________________________________________________<br/>Embedding-Position (PositionEmb (None, 512, 768)     393216      Embedding-Token-Segment[0][0]    <br/>__________________________________________________________________________________________________<br/>Embedding-Dropout (Dropout)     (None, 512, 768)     0           Embedding-Position[0][0]         <br/>__________________________________________________________________________________________________<br/>Embedding-Norm (LayerNormalizat (None, 512, 768)     1536        Embedding-Dropout[0][0]          <br/>__________________________________________________________________________________________________<br/>Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     2362368     Embedding-Norm[0][0]             <br/>__________________________________________________________________________________________________<br/>Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention[<br/>__________________________________________________________________________________________________<br/>Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     0           Embedding-Norm[0][0]             <br/>                                                                 Encoder-1-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-1-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-1-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-1-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-1-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-1-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-1-FeedForward[0][0]      <br/>__________________________________________________________________________________________________<br/>Encoder-1-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-1-MultiHeadSelfAttention-<br/>                                                                 Encoder-1-FeedForward-Dropout[0][<br/>__________________________________________________________________________________________________<br/>Encoder-1-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-1-FeedForward-Add[0][0]  <br/>__________________________________________________________________________________________________<br/>Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention[<br/>__________________________________________________________________________________________________<br/>Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-1-FeedForward-Norm[0][0] <br/>                                                                 Encoder-2-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-2-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-2-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-2-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-2-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-2-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-2-FeedForward[0][0]      <br/>__________________________________________________________________________________________________<br/>Encoder-2-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-2-MultiHeadSelfAttention-<br/>                                                                 Encoder-2-FeedForward-Dropout[0][<br/>__________________________________________________________________________________________________<br/>Encoder-2-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-2-FeedForward-Add[0][0]  <br/>__________________________________________________________________________________________________<br/>Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention[<br/>__________________________________________________________________________________________________<br/>Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-2-FeedForward-Norm[0][0] <br/>                                                                 Encoder-3-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-3-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-3-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-3-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-3-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-3-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-3-FeedForward[0][0]      <br/>__________________________________________________________________________________________________<br/>Encoder-3-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-3-MultiHeadSelfAttention-<br/>                                                                 Encoder-3-FeedForward-Dropout[0][<br/>__________________________________________________________________________________________________<br/>Encoder-3-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-3-FeedForward-Add[0][0]  <br/>__________________________________________________________________________________________________<br/>Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention[<br/>__________________________________________________________________________________________________<br/>Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-3-FeedForward-Norm[0][0] <br/>                                                                 Encoder-4-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-4-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-4-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-4-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-4-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-4-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-4-FeedForward[0][0]      <br/>__________________________________________________________________________________________________<br/>Encoder-4-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-4-MultiHeadSelfAttention-<br/>                                                                 Encoder-4-FeedForward-Dropout[0][<br/>__________________________________________________________________________________________________<br/>Encoder-4-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-4-FeedForward-Add[0][0]  <br/>__________________________________________________________________________________________________<br/>Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention[<br/>__________________________________________________________________________________________________<br/>Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-4-FeedForward-Norm[0][0] <br/>                                                                 Encoder-5-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-5-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-5-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-5-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-5-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-5-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-5-FeedForward[0][0]      <br/>__________________________________________________________________________________________________<br/>Encoder-5-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-5-MultiHeadSelfAttention-<br/>                                                                 Encoder-5-FeedForward-Dropout[0][<br/>__________________________________________________________________________________________________<br/>Encoder-5-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-5-FeedForward-Add[0][0]  <br/>__________________________________________________________________________________________________<br/>Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention[<br/>__________________________________________________________________________________________________<br/>Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-5-FeedForward-Norm[0][0] <br/>                                                                 Encoder-6-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-6-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-6-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-6-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-6-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-6-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-6-FeedForward[0][0]      <br/>__________________________________________________________________________________________________<br/>Encoder-6-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-6-MultiHeadSelfAttention-<br/>                                                                 Encoder-6-FeedForward-Dropout[0][<br/>__________________________________________________________________________________________________<br/>Encoder-6-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-6-FeedForward-Add[0][0]  <br/>__________________________________________________________________________________________________<br/>Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention[<br/>__________________________________________________________________________________________________<br/>Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-6-FeedForward-Norm[0][0] <br/>                                                                 Encoder-7-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-7-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-7-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-7-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-7-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-7-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-7-FeedForward[0][0]      <br/>__________________________________________________________________________________________________<br/>Encoder-7-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-7-MultiHeadSelfAttention-<br/>                                                                 Encoder-7-FeedForward-Dropout[0][<br/>__________________________________________________________________________________________________<br/>Encoder-7-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-7-FeedForward-Add[0][0]  <br/>__________________________________________________________________________________________________<br/>Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention[<br/>__________________________________________________________________________________________________<br/>Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-7-FeedForward-Norm[0][0] <br/>                                                                 Encoder-8-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-8-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-8-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-8-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-8-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-8-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-8-FeedForward[0][0]      <br/>__________________________________________________________________________________________________<br/>Encoder-8-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-8-MultiHeadSelfAttention-<br/>                                                                 Encoder-8-FeedForward-Dropout[0][<br/>__________________________________________________________________________________________________<br/>Encoder-8-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-8-FeedForward-Add[0][0]  <br/>__________________________________________________________________________________________________<br/>Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention[<br/>__________________________________________________________________________________________________<br/>Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     0           Encoder-8-FeedForward-Norm[0][0] <br/>                                                                 Encoder-9-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-9-MultiHeadSelfAttentio (None, 512, 768)     1536        Encoder-9-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-9-FeedForward (FeedForw (None, 512, 768)     4722432     Encoder-9-MultiHeadSelfAttention-<br/>__________________________________________________________________________________________________<br/>Encoder-9-FeedForward-Dropout ( (None, 512, 768)     0           Encoder-9-FeedForward[0][0]      <br/>__________________________________________________________________________________________________<br/>Encoder-9-FeedForward-Add (Add) (None, 512, 768)     0           Encoder-9-MultiHeadSelfAttention-<br/>                                                                 Encoder-9-FeedForward-Dropout[0][<br/>__________________________________________________________________________________________________<br/>Encoder-9-FeedForward-Norm (Lay (None, 512, 768)     1536        Encoder-9-FeedForward-Add[0][0]  <br/>__________________________________________________________________________________________________<br/>Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-9-FeedForward-Norm[0][0] <br/>                                                                 Encoder-10-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-10-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-10-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-10-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-10-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-10-FeedForward-Dropout  (None, 512, 768)     0           Encoder-10-FeedForward[0][0]     <br/>__________________________________________________________________________________________________<br/>Encoder-10-FeedForward-Add (Add (None, 512, 768)     0           Encoder-10-MultiHeadSelfAttention<br/>                                                                 Encoder-10-FeedForward-Dropout[0]<br/>__________________________________________________________________________________________________<br/>Encoder-10-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-10-FeedForward-Add[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]<br/>__________________________________________________________________________________________________<br/>Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-10-FeedForward-Norm[0][0]<br/>                                                                 Encoder-11-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-11-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-11-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-11-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-11-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-11-FeedForward-Dropout  (None, 512, 768)     0           Encoder-11-FeedForward[0][0]     <br/>__________________________________________________________________________________________________<br/>Encoder-11-FeedForward-Add (Add (None, 512, 768)     0           Encoder-11-MultiHeadSelfAttention<br/>                                                                 Encoder-11-FeedForward-Dropout[0]<br/>__________________________________________________________________________________________________<br/>Encoder-11-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-11-FeedForward-Add[0][0] <br/>__________________________________________________________________________________________________<br/>Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]<br/>__________________________________________________________________________________________________<br/>Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     0           Encoder-11-FeedForward-Norm[0][0]<br/>                                                                 Encoder-12-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-12-MultiHeadSelfAttenti (None, 512, 768)     1536        Encoder-12-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-12-FeedForward (FeedFor (None, 512, 768)     4722432     Encoder-12-MultiHeadSelfAttention<br/>__________________________________________________________________________________________________<br/>Encoder-12-FeedForward-Dropout  (None, 512, 768)     0           Encoder-12-FeedForward[0][0]     <br/>__________________________________________________________________________________________________<br/>Encoder-12-FeedForward-Add (Add (None, 512, 768)     0           Encoder-12-MultiHeadSelfAttention<br/>                                                                 Encoder-12-FeedForward-Dropout[0]<br/>__________________________________________________________________________________________________<br/>Encoder-12-FeedForward-Norm (La (None, 512, 768)     1536        Encoder-12-FeedForward-Add[0][0] <br/>__________________________________________________________________________________________________<br/>MLM-Dense (Dense)               (None, 512, 768)     590592      Encoder-12-FeedForward-Norm[0][0]<br/>__________________________________________________________________________________________________<br/>MLM-Norm (LayerNormalization)   (None, 512, 768)     1536        MLM-Dense[0][0]                  <br/>__________________________________________________________________________________________________<br/>Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]<br/>__________________________________________________________________________________________________<br/>MLM-Sim (EmbeddingSimilarity)   (None, 512, 30000)   30000       MLM-Norm[0][0]                   <br/>                                                                 Embedding-Token[0][1]            <br/>__________________________________________________________________________________________________<br/>Input-Masked (InputLayer)       (None, 512)          0                                            <br/>__________________________________________________________________________________________________<br/>NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    <br/>__________________________________________________________________________________________________<br/>MLM (Masked)                    (None, 512, 30000)   0           MLM-Sim[0][0]                    <br/>                                                                 Input-Masked[0][0]               <br/>__________________________________________________________________________________________________<br/>NSP (Dense)                     (None, 2)            1538        NSP-Dense[0][0]                  <br/>==================================================================================================<br/>Total params: 109,705,010<br/>Trainable params: 109,705,010<br/>Non-trainable params: 0</span></pre><p id="801d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是一份令人生畏的清单。正如BERT论文中提到的，可训练参数的总数约为110万。这让人放心，我们加载的模型是正确的。</p><p id="1bc9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样的情况也可以在图像中可视化，这有助于我们更好地理解计算图:</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="6503" class="kd ke hi jz b fi kf kg l kh ki">from keras.utils import plot_model<br/>plot_model(model, to_file='bert.png')</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/5e16d890b397d1c9580594058142a49c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*86FzrMy0Hncxr0c7OFBDnw.png"/></div></div></figure><p id="ecdc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是模型中各个步骤的简要介绍:</p><ol class=""><li id="a397" class="kk kl hi ix b iy iz jc jd jg km jk kn jo ko js kp kq kr ks bi translated">两个输入:一个来自单词标记，一个来自段层</li><li id="9670" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">这些相加，加起来就是第三次嵌入:位置嵌入，接着是丢失和层标准化</li><li id="0406" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">然后开始多头自我关注层-每组有9个步骤(在上面的图像中，所有单元都以编码器-1开始)，共有12个这样的层。这里面的108行就是为了捕捉这些。如果我们更好地理解这些，我们几乎完全理解了架构</li><li id="8912" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">在这12层之后，有两个输出——一个用于NSP(下一句预测),一个用于MLM(屏蔽语言建模)</li></ol><p id="1a19" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">分层核算:</strong></p><p id="956d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上到下浏览各层，我们可以看到以下内容:</p><ol class=""><li id="033d" class="kk kl hi ix b iy iz jc jd jg km jk kn jo ko js kp kq kr ks bi translated">输入-令牌和段不像预期的那样具有任何可训练参数。</li><li id="b715" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">令牌嵌入参数= 23040000 (H * T) —因为30k (T)个令牌中的每一个都需要维度768 (H)中的表示</li><li id="e96b" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">段嵌入参数= 1536 (2*H ),因为我们需要两个长度为(H)的向量。向量分别代表线段A和线段B</li><li id="7723" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">位置嵌入增加了标记嵌入和段嵌入。参数= 393216 (H*P)。这是因为它需要为从1到512 (P)开始的记号生成P个向量，每个向量的长度为H。伯特的位置嵌入是经过训练的，不像<em class="jt">那样是固定的，你所需要的只是注意力；</em>应用了一个丢弃，然后完成了图层标准化</li><li id="c469" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">图层规范化参数= 1536 (2*H)。归一化需要学习两个参数——每个嵌入位置的平均值和标准偏差，因此是2*H</li><li id="04e8" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">编码器:multi head self attention:multi head attention = 2362368</li></ol><p id="395c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这需要一点解释。这是在这个步骤中发生的事情[ <a class="ae iu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> ref </a> ]:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/c9fc924f3b7001a77eb1d65cc206d117.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fds9z3K5x9r2m9sJgBrKbQ.png"/></div></div></figure><p id="0ef5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">共有12个头，输入尺寸为768。因此每个头生成长度为768/12 = 64的嵌入。生成了三个嵌入— Q、K、v。总共:每个磁头768*64*3个参数，或者所有磁头12*768*64*3个参数。加上Q，K，V的偏差，还有768*3。合计= 12 * 768 * 64 * 3+768 * 3；这是在连接所有头部之后。然后应用一个额外的权重(上图中向右的W0)。那就是全连通的密集层，输出维度=输入维度。因此，参数(带偏差)= 768*768 + 768。所以这一步的总参数= A * D *(D/A)* 3+D * 3+D * D+D = 12 * 768 * 64 * 3+768 * 3+768 * 768+768 = 2362368</p><p id="d461" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">7.另一层标准化，遵循与#5相同的逻辑</p><p id="b78e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">8.前馈:前馈。这实际上是一个前馈<em class="jt">网络</em>，它有<em class="jt">两个</em>完全连接的前馈层。它将输入维度(H)转换为FFD，然后再转换回H，其间有ReLu激活。所以有偏差的总参数=(H * FFD+FFD)+(FFD * H+H)=(768 * 3072+3072)+(3072 * 768+768)= 4722432；这跟随着另一个脱落层</p><p id="c0c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">9.另一层标准化，遵循与#5相同的逻辑</p><p id="e36d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">步骤6-9涵盖了一个单一的变压器层，相同的设置重复12(L)次</p><p id="eeac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这遵循两个产出目标。一个用于MLM(屏蔽语言建模)，一个用于NSP(下一句预测)。让我们观察它们的参数:</p><p id="de5f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">10.MLM密集:它接受一个嵌入作为输入，并试图预测屏蔽词的嵌入。所以参数(有偏差)= H * H + H = 768*768 + 768 = 590592</p><p id="8c02" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">11.MLM-诺姆:标准化层，参数计数遵循与#5相同的逻辑</p><p id="9b72" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">12.MLM-Sim:嵌入相似性:这是计算MLM范数的输出和输入屏蔽令牌的嵌入之间的相似性。但是这一层也学习令牌级别偏差。这就是这一层中的T (=30k)个参数(直觉上，我认为这类似于令牌级先验，但如果我错了，请纠正我)。</p><p id="a5a7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">13.NSP密集:密集:这将输入的D长度嵌入转换为另一个D长度嵌入。参数= D *D + D = 590592</p><p id="76f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">14.NSP:密集:前一层的输出D长度嵌入然后被转换成两个向量，每个向量分别表示IsNext和NotNext。因此，参数= 2*D + 2= 1538</p><p id="1bb5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">整个网络的概述到此结束。通过这些，我得到了以下问题的答案:</p><ol class=""><li id="9138" class="kk kl hi ix b iy iz jc jd jg km jk kn jo ko js kp kq kr ks bi translated">序列和位置嵌入来自哪里，以及两者都是可训练的事实</li><li id="ba96" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">详细了解变压器单元内部发生的情况</li><li id="c51a" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">图层规范化的作用</li><li id="b958" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">同时传播MLM和NSP的任务</li></ol><p id="7b2b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">脚注:</p><p id="b1f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank"> PyTorch预排实施注意事项</a></p><p id="0587" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" rel="noopener" href="/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a">伯特中的三种嵌入类型</a></p><p id="eebd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://colab.research.google.com/drive/1Nlhh2vwlQdKleNMqpmLDBsAwrv_7NnrB#forceEdit=true&amp;sandboxMode=true&amp;scrollTo=abSHyknGrI7b" rel="noopener ugc nofollow" target="_blank"> Colab笔记本了解BERT </a>中的注意力——这也有一个很酷的交互可视化，解释了Q，K，V嵌入如何相互作用以产生注意力分布</p><p id="ba85" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">解释变形金刚、自我注意和交叉注意</a></p></div></div>    
</body>
</html>