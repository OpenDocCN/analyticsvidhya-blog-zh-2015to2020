<html>
<head>
<title>Hadoop Single Node Cluster Setup | Pseudo-Distributed Mode</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Hadoop单节点集群设置|伪分布式模式</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hadoop-single-node-cluster-setup-b11b957681f2?source=collection_archive---------1-----------------------#2020-04-17">https://medium.com/analytics-vidhya/hadoop-single-node-cluster-setup-b11b957681f2?source=collection_archive---------1-----------------------#2020-04-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/fe22dff9e293eced11d1b3b433a2e608.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*Tz-2COs4oRh58HJpcZqEsg.jpeg"/></div></figure><blockquote class="im in io"><p id="5db3" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Hadoop可以以3种不同的模式安装:独立模式、伪分布式模式和完全分布式模式。</p><p id="c0b0" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">独立模式</strong>是Hadoop运行的默认模式。独立模式主要用于在你不真正使用<a class="ae jo" href="http://hdfstutorial.com/" rel="noopener ugc nofollow" target="_blank"> HDFS </a>的地方进行调试。<br/> <strong class="is hj">伪分布式模式</strong>也被称为<strong class="is hj">单节点集群</strong>，其中NameNode和DataNode将驻留在同一台机器上。<br/> <strong class="is hj">全分布式模式</strong>是Hadoop 的<strong class="is hj">生产模式，其中将运行多个节点。</strong></p></blockquote></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><blockquote class="im in io"><p id="3e99" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个草稿可以帮助你创建自己的<strong class="is hj">定制hadoop伪模式集群。<br/> </strong> <em class="hi">本设置使用的环境为</em> <strong class="is hj"> <em class="hi"> ubuntu 18.04，hadoop版本为3.1.2 </em> </strong> <em class="hi">。</em></p></blockquote></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="aaaf" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">先决条件</h1><h2 id="ce1e" class="ku jx hi bd jy kv kw kx kc ky kz la kg lb lc ld kk le lf lg ko lh li lj ks lk bi translated">创建新用户[可选]</h2><blockquote class="im in io"><p id="c255" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>如果您想为新用户安装hadoop，请遵循此步骤，否则跳到下一部分。</p></blockquote><p id="4f45" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">打开一个新的终端<em class="ir"> (Ctrl+Alt+T) </em> &amp;键入以下命令。</p><p id="3cb9" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">首先，创建一个组<em class="ir"> hadoop </em></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="c3ab" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo addgroup hadoop</em></span></pre><p id="25d8" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">并在同一个<em class="ir"> hadoop </em>组内添加一个新用户<em class="ir"> hdfsuser </em>。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="3088" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo adduser --ingroup hadoop hdfsuser</em></span></pre><blockquote class="im in io"><p id="0bfd" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>如果您想将现有用户添加到组中，则使用以下命令</p></blockquote><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="2181" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">usermod -a -G hadoop username</em></span></pre><p id="96aa" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">现在给<em class="ir"> hdfsuser </em>必要的根权限来安装文件。根用户权限可以通过更新<em class="ir"> sudoers </em>文件来提供。</p><p id="dfa7" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">在终端中运行以下命令，打开<em class="ir"> sudoers </em>文件</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="c080" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo visudo</em></span></pre><p id="6221" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">添加或编辑以下行</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="847d" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">hdfsuser ALL=(ALL:ALL) ALL</em></span></pre><p id="4110" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">现在，保存更改<em class="ir"> (Ctrl+O &amp;按enter) </em>并关闭编辑器<em class="ir"> (Ctrl+X)。</em></p><p id="b0d0" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">因此，现在让我们切换到我们新创建的用户进行进一步的安装。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="57c1" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">su - hdfsuser</em></span></pre><h2 id="9757" class="ku jx hi bd jy kv kw kx kc ky kz la kg lb lc ld kk le lf lg ko lh li lj ks lk bi translated">Java安装</h2><p id="1482" class="pw-post-body-paragraph ip iq hi is b it ly iv iw ix lz iz ja lb ma jd je le mb jh ji lh mc jl jm jn hb bi translated">Hadoop是使用Java构建的，运行MapReduce代码需要Java。对于最新的hadoop安装，Java版本应为Java 8或更高版本，即Java 1.8+。如果您的系统中已经运行了java，那么通过在终端中运行以下命令来检查您是否拥有所需的版本</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="303a" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">java -version</em></span></pre><p id="8312" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">如果您有所需的版本，请跳到下一步。</p><blockquote class="im in io"><p id="c025" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>如果你也计划安装hive，那么最好选择java 8，因为新版本不再有URLClassLoader。</p></blockquote><p id="864b" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">您可以从您的OS软件包管理器或oracle官方网站<em class="ir">(</em><a class="ae jo" href="https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html" rel="noopener ugc nofollow" target="_blank">https://www . Oracle . com/java/technologies/javase/javase-JDK 8-downloads . html</a><em class="ir">)安装Java。</em></p><p id="b288" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated"><strong class="is hj">安装使用资质<em class="ir"> (java 8) </em> </strong></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="09dc" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo apt-get update<br/></em><em class="ir">sudo apt install openjdk-8-jre-headless </em><em class="ir">openjdk-8-jdk</em></span></pre><p id="289d" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">要验证您的安装，请在终端中运行以下命令</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="fbc7" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">java -version</em></span></pre><h2 id="570d" class="ku jx hi bd jy kv kw kx kc ky kz la kg lb lc ld kk le lf lg ko lh li lj ks lk bi translated">设置SSH密钥</h2><p id="ddf5" class="pw-post-body-paragraph ip iq hi is b it ly iv iw ix lz iz ja lb ma jd je le mb jh ji lh mc jl jm jn hb bi translated">Hadoop核心使用Shell (SSH)在从属节点上启动服务器进程。它要求主机和所有从机以及从机之间的无密码SSH连接。</p><p id="1c1f" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">我们需要一个无密码的SSH，因为当集群运行时，通信过于频繁。作业跟踪器应该能够快速发送任务到任务跟踪器。</p><blockquote class="im in io"><p id="8853" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>不要跳过这一步，除非你已经有了一个无密码的SSH设置。这一步对于启动hadoop服务是必不可少的，比如资源管理器&amp;节点管理器。</p></blockquote><p id="387f" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated"><strong class="is hj">安装所需的软件包</strong></p><p id="d0d0" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">在终端中运行以下命令</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="9cbf" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo apt-get install ssh<br/>sudo apt-get install sshd</em></span></pre><p id="9075" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated"><strong class="is hj">生成密钥……</strong></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="46bd" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">ssh-keygen -t rsa<br/>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br/>chmod og-wx ~/.ssh/authorized_keys</em></span></pre><p id="234b" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">现在，我们已经成功地生成了ssh密钥，并将密钥值复制到authorized_keys。</p><p id="01ce" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">通过在终端中运行以下命令来验证安全连接。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="3364" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">ssh localhost</em></span></pre><blockquote class="im in io"><p id="ec15" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>如果它不要求输入密码并让您登录，则配置成功，否则移除生成的密钥并再次执行步骤。</p><p id="3b88" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">不要忘记从本地主机退出(在终端中键入exit并按enter键)</p></blockquote><p id="a8ef" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">现在我们的hadoop安装先决条件已经成功完成。</em>T15】</strong></p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="d4ea" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">Hadoop 3.x安装</h1><p id="ce78" class="pw-post-body-paragraph ip iq hi is b it ly iv iw ix lz iz ja lb ma jd je le mb jh ji lh mc jl jm jn hb bi translated">现在。让我们从<em class="ir">(</em><a class="ae jo" href="https://hadoop.apache.org/releases.html" rel="noopener ugc nofollow" target="_blank"><em class="ir">【https://hadoop.apache.org/releases.html</em></a><em class="ir">)，</em>下载最新的稳定版本，开始hadoop的安装过程，对于旧版本<em class="ir"> </em>请访问<em class="ir">(</em><a class="ae jo" href="https://archive.apache.org/dist/hadoop/common/" rel="noopener ugc nofollow" target="_blank"><em class="ir">【https://archive.apache.org/dist/hadoop/common/</em></a><em class="ir">)。</em></p><p id="5981" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">要下载您选择的版本，请使用以下命令。<em class="ir">(根据个人喜好更改目录和下载链接)。</em></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="aaa8" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">cd /usr/local<br/>sudo wget </em><a class="ae jo" href="http://archive.apache.org/dist/hadoop/common/hadoop-3.1.2/hadoop-3.1.2.tar.gz" rel="noopener ugc nofollow" target="_blank"><em class="ir">http://archive.apache.org/dist/hadoop/common/hadoop-3.1.2/hadoop-3.1.2.tar.gz</em></a></span></pre><p id="9dca" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">在相同的位置提取hadoop文件。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="b32a" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo tar xvzf hadoop-3.1.2.tar.gz</em></span></pre><p id="fc5d" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">重命名提取的文件夹</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="7cbd" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo mv hadoop-3.1.2 hadoop</em></span></pre><h2 id="6271" class="ku jx hi bd jy kv kw kx kc ky kz la kg lb lc ld kk le lf lg ko lh li lj ks lk bi translated">在伪分布式模式下设置Hadoop</h2><p id="ee55" class="pw-post-body-paragraph ip iq hi is b it ly iv iw ix lz iz ja lb ma jd je le mb jh ji lh mc jl jm jn hb bi translated">现在，让我们将hadoop的所有权提供给我们的<em class="ir"> hdfsuser【如果不想更改所有权，请跳过】</em></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="96f5" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo chown -R hdfsuser:hadoop /usr/local/hadoop</em></span></pre><p id="ee46" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">将<em class="ir"> hadoop </em>文件夹的模式改为读取、写入&amp;执行的工作模式。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="09a3" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo chmod -R 777 /usr/local/hadoop</em></span></pre><h2 id="a1df" class="ku jx hi bd jy kv kw kx kc ky kz la kg lb lc ld kk le lf lg ko lh li lj ks lk bi translated"><strong class="ak">禁用IPv6 </strong></h2><p id="bf66" class="pw-post-body-paragraph ip iq hi is b it ly iv iw ix lz iz ja lb ma jd je le mb jh ji lh mc jl jm jn hb bi translated">IPv6网络目前不支持Apache Hadoop。它只在IPv4堆栈上测试和开发过。Hadoop需要IPv4才能工作，只有IPv4客户端才能与集群对话。</p><p id="edf5" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">看一看<a class="ae jo" href="https://issues.apache.org/jira/browse/HADOOP-3437" rel="noopener ugc nofollow" target="_blank"> HADOOP-3437 </a>和<a class="ae jo" href="https://issues.apache.org/jira/browse/HADOOP-6056" rel="noopener ugc nofollow" target="_blank"> HADOOP-6056 </a>来理解为什么必须禁用IPv6才能让HADOOP工作。</p><p id="f2d9" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">您可以通过在终端中运行以下命令来检查IPv6配置的状态</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="2cf8" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">cat /proc/sys/net/ipv6/conf/all/disable_ipv6</em></span></pre><p id="c9dd" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">如果结果不是1，则按照以下步骤禁用IPv6</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="8631" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo nano /etc/sysctl.conf</em></span></pre><p id="a605" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">现在将以下几行添加到文件的末尾</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="39ed" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir"># Disable ipv6<br/>net.ipv6.conf.all.disable_ipv6=1<br/>net.ipv6.conf.default_ipv6=1<br/>net.ipv6.conf.lo.disable_ipv6=1</em></span></pre><p id="6e70" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">保存文件并退出</p><blockquote class="im in io"><p id="afe0" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果ipv6仍然没有被禁用，那么问题将是/etc/sysctl.conf没有被激活。要解决这个问题，可以运行以下命令来激活conf</p></blockquote><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="371d" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo sysctl -p</em></span></pre><h2 id="a516" class="ku jx hi bd jy kv kw kx kc ky kz la kg lb lc ld kk le lf lg ko lh li lj ks lk bi translated">添加Hadoop环境变量</h2><p id="d62c" class="pw-post-body-paragraph ip iq hi is b it ly iv iw ix lz iz ja lb ma jd je le mb jh ji lh mc jl jm jn hb bi translated">将hadoop路径添加到环境中是必要的，否则您将不得不移动到hadoop目录来运行命令。</p><p id="50fe" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">运行以下命令打开<em class="ir"> bashrc </em>文件</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="5930" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo nano ~/.bashrc</em></span></pre><p id="5d27" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">将以下几行添加到<em class="ir"> bashrc </em>文件的末尾</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="073b" class="ku jx hi lq b fi lu lv l lw lx"># HADOOP ENVIRONMENT<strong class="lq hj"><br/></strong>export HADOOP_HOME=/usr/local/hadoop<br/>export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop<br/>export HADOOP_MAPRED_HOME=/usr/local/hadoop<br/>export HADOOP_COMMON_HOME=/usr/local/hadoop<br/>export HADOOP_HDFS_HOME=/usr/local/hadoop<br/>export YARN_HOME=/usr/local/hadoop<br/>export PATH=$PATH:/usr/local/hadoop/bin<br/>export PATH=$PATH:/usr/local/hadoop/sbin</span><span id="b8b5" class="ku jx hi lq b fi md lv l lw lx"><strong class="lq hj"># </strong>HADOOP NATIVE PATH<br/>export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native<br/>export HADOOP_OPTS=-Djava.library.path=$HADOOP_PREFIX/lib</span></pre><p id="a5f9" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">现在，通过运行以下命令加载hadoop环境变量</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="3317" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">source ~/.bashrc</em></span></pre><h2 id="687c" class="ku jx hi bd jy kv kw kx kc ky kz la kg lb lc ld kk le lf lg ko lh li lj ks lk bi translated">正在配置Hadoop …</h2><p id="ed69" class="pw-post-body-paragraph ip iq hi is b it ly iv iw ix lz iz ja lb ma jd je le mb jh ji lh mc jl jm jn hb bi translated">将工作目录更改为hadoop配置位置</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="08ed" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">cd /usr/local/hadoop/etc/hadoop/</em></span></pre><ul class=""><li id="7031" class="me mf hi is b it iu ix iy lb mg le mh lh mi jn mj mk ml mm bi translated"><strong class="is hj"> hadoop-env.sh <br/> </strong>通过运行以下命令打开hadoop-env文件</li></ul><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="56eb" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo nano hadoop-env.sh</em></span></pre><p id="0778" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">将以下配置添加到文件<em class="ir">的末尾(根据您的设置更改java路径和用户名)</em></p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="a221" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true<br/>export JAVA_HOME=/usr<br/>export HADOOP_HOME_WARN_SUPPRESS="TRUE"<br/>export HADOOP_ROOT_LOGGER="WARN,DRFA"<br/></em>export HDFS_NAMENODE_USER="hdfsuser"<br/>export HDFS_DATANODE_USER="hdfsuser"<br/>export HDFS_SECONDARYNAMENODE_USER="hdfsuser"<br/>export YARN_RESOURCEMANAGER_USER="hdfsuser"<br/>export YARN_NODEMANAGER_USER="hdfsuser"</span></pre><ul class=""><li id="f81a" class="me mf hi is b it iu ix iy lb mg le mh lh mi jn mj mk ml mm bi translated"><strong class="is hj"> yarn-site.xml <br/> </strong>通过运行以下命令打开yarn-site文件</li></ul><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="39b3" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo nano yarn-site.xml</em></span></pre><p id="be19" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">在配置标记(<configuration> </configuration>)之间添加以下配置</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="de90" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">&lt;property&gt;<br/>&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br/>&lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;<br/>&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;<br/>&lt;/property&gt;</em></span></pre><ul class=""><li id="bcdb" class="me mf hi is b it iu ix iy lb mg le mh lh mi jn mj mk ml mm bi translated"><strong class="is hj"> hdfs-site.xml <br/> </strong>通过运行以下命令打开hdfs-site文件</li></ul><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="6f12" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo nano hdfs-site.xml</em></span></pre><p id="c682" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">在配置标记(<configuration> </configuration>)之间添加以下配置</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="9c87" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">&lt;property&gt;<br/>&lt;name&gt;dfs.replication&lt;/name&gt;<br/>&lt;value&gt;1&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br/>&lt;value&gt;/usr/local/hadoop/yarn_data/hdfs/namenode&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br/>&lt;value&gt;/usr/local/hadoop/yarn_data/hdfs/datanode&lt;/value&gt;<br/>&lt;/property&gt;<br/></em>&lt;property&gt;<br/>&lt;name&gt;dfs.namenode.http-address&lt;/name&gt;<br/>&lt;value&gt;localhost:50070&lt;/value&gt;<br/>&lt;/property&gt;</span></pre><ul class=""><li id="224b" class="me mf hi is b it iu ix iy lb mg le mh lh mi jn mj mk ml mm bi translated"><strong class="is hj"> core-site.xml <br/></strong></li></ul><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="54ae" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo nano core-site.xml</em></span></pre><p id="3c9b" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">在配置标记(<configuration> </configuration>)之间添加以下配置</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="6ed1" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">&lt;property&gt;<br/>&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br/>&lt;value&gt;/bigdata/hadoop/tmp&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>&lt;name&gt;fs.default.name&lt;/name&gt;<br/>&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;<br/>&lt;/property&gt;</em></span></pre><ul class=""><li id="bfdf" class="me mf hi is b it iu ix iy lb mg le mh lh mi jn mj mk ml mm bi translated"><strong class="is hj"> mapred-site.xml <br/> </strong>运行以下命令打开核心站点文件</li></ul><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="00dc" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo nano mapred-site.xml</em></span></pre><p id="b67b" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">在配置标记(<configuration> </configuration>)之间添加以下配置</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="627a" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">&lt;property&gt;<br/>&lt;name&gt;mapred.framework.name&lt;/name&gt;<br/>&lt;value&gt;yarn&lt;/value&gt;<br/>&lt;/property&gt;<br/>&lt;property&gt;<br/>&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;<br/>&lt;value&gt;localhost:10020&lt;/value&gt;<br/>&lt;/property&gt;</em></span></pre><p id="fb3b" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ir">全部完成…现在让我们为hadoop创建一些目录来保存数据</em> </strong></p><h2 id="e300" class="ku jx hi bd jy kv kw kx kc ky kz la kg lb lc ld kk le lf lg ko lh li lj ks lk bi translated">正在创建目录…</h2><p id="258e" class="pw-post-body-paragraph ip iq hi is b it ly iv iw ix lz iz ja lb ma jd je le mb jh ji lh mc jl jm jn hb bi translated">让我们为dfs创建一个临时目录，如core-site.xml文件中提到的那样</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="01c5" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo mkdir -p /bigdata/hadoop/tmp<br/>sudo chown -R hdfsuser:hadoop /bigdata/hadoop/tmp<br/>sudo chmod -R 777 /bigdata/hadoop/tmp</em></span></pre><blockquote class="im in io"><p id="fa9c" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>如果不想改变所有权，跳过chown命令。在接下来的步骤中也要记住这一点。</p></blockquote><p id="b363" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">正如我们在yarn-site.xml文件中提到的，现在创建保存数据的目录</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="8bf8" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">sudo mkdir -p /usr/local/hadoop/yarn_data/hdfs/namenode<br/>sudo mkdir -p /usr/local/hadoop/yarn_data/hdfs/datanode<br/>sudo chmod -R 777 /usr/local/hadoop/yarn_data/hdfs/namenode<br/>sudo chmod -R 777 /usr/local/hadoop/yarn_data/hdfs/datanode<br/>sudo chown -R hdfsuser:hadoop /usr/local/hadoop/yarn_data/hdfs/namenode<br/>sudo chown -R hdfsuser:hadoop /usr/local/hadoop/yarn_data/hdfs/datanode</em></span></pre><p id="295d" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">好的，目前为止还不错。我们已经完成了所有必要的配置，现在让我们启动资源管理器和节点管理器。</p><h2 id="de2f" class="ku jx hi bd jy kv kw kx kc ky kz la kg lb lc ld kk le lf lg ko lh li lj ks lk bi translated">正在完成…</h2><p id="5c07" class="pw-post-body-paragraph ip iq hi is b it ly iv iw ix lz iz ja lb ma jd je le mb jh ji lh mc jl jm jn hb bi translated">在启动hadoop核心服务之前，我们需要通过格式化namenode来清理集群。每当您更改namenode或datanode配置时，不要忘记这样做。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="3c1e" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">hdfs namenode -format</em></span></pre><p id="a694" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">现在，我们可以通过运行以下命令来启动所有hadoop服务</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="6b15" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">start-dfs.sh<br/>start-yarn.sh</em></span></pre><blockquote class="im in io"><p id="7d68" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>你也可以使用<strong class="is hj"> start-all.sh </strong>来启动所有的服务</p></blockquote></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><p id="46f7" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated"><strong class="is hj">临时演员……</strong></p><blockquote class="im in io"><p id="a3e3" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以通过导航到以下url来检查您的namenode是否已启动并正在运行。</p></blockquote><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="b323" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">http://localhost:50070</em></span></pre><blockquote class="im in io"><p id="0dd9" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要访问ResourceManager，请导航到ResourceManager web UI，网址为</p></blockquote><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="efb9" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">http://localhost:8088</em></span></pre><blockquote class="im in io"><p id="445a" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要检查HDFS是否正在运行，您可以使用Java进程状态工具。</p></blockquote><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="c0e7" class="ku jx hi lq b fi lu lv l lw lx">jps</span></pre><p id="d55e" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">这给出了java中正在运行的进程的列表。</p><p id="c81c" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">如果安装成功，您应该会看到列出了这些服务，</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="7b2d" class="ku jx hi lq b fi lu lv l lw lx">ResourceManager<br/>DataNode<br/>SecondaryNameNode<br/>NodeManager<br/>NameNode</span></pre><p id="a783" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>如果您发现服务中没有列出namenode，那么请确保您已经格式化了namenode。<br/>如果datanode丢失，则可能是由于用户对datanode目录没有足够的权限，因此请将目录更改为用户具有读&amp;写权限的位置，或者使用前面描述的chown方法。</p><blockquote class="im in io"><p id="fb39" class="ip iq ir is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要停止所有hadoop核心服务，请尝试以下任一方法</p></blockquote><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="a755" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">stop-dfs.sh<br/>stop-yarn.sh</em></span></pre><p id="c594" class="pw-post-body-paragraph ip iq hi is b it iu iv iw ix iy iz ja lb jc jd je le jg jh ji lh jk jl jm jn hb bi translated">运筹学</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="1903" class="ku jx hi lq b fi lu lv l lw lx"><em class="ir">stop-all.sh</em></span></pre></div></div>    
</body>
</html>