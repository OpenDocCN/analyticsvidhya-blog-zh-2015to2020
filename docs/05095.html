<html>
<head>
<title>k-Means Clustering: Comparison of Initialization strategies.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-均值聚类:初始化策略的比较。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/comparison-of-initialization-strategies-for-k-means-d5ddd8b0350e?source=collection_archive---------2-----------------------#2020-04-11">https://medium.com/analytics-vidhya/comparison-of-initialization-strategies-for-k-means-d5ddd8b0350e?source=collection_archive---------2-----------------------#2020-04-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="64fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">k-Means是一种数据划分算法，是聚类算法中最直接的选择之一。k-Means流行的一些原因是:</p><ol class=""><li id="9fd3" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">执行速度快。</li><li id="e8ec" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">在线和小批量实施也可用，因此需要更少的内存。</li><li id="f6ad" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">容易解读。聚类的质心通常给出了聚类中存在的数据的大致概念。对于能够检测质心甚至可能不在聚类内的非凸聚类的一些其他聚类算法来说，这是不可能的。</li><li id="0294" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">k-Means的结果可以用作其他算法的起点。通常的做法是使用k均值的质心作为高斯混合模型的起始点。</li></ol><p id="9e35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，k-Means的<strong class="ih hj">结果严重依赖于初始化</strong>。在下面的文章中，我们将比较通过以下初始化策略获得的结果:</p><ol class=""><li id="3c6d" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">Forgy初始化</li><li id="92f2" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">随机分区初始化</li><li id="f941" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">kmeans++初始化</li></ol><p id="6980" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">标准做法是从不同的起始点开始k均值，并记录每次初始化的WSS(平方和内)值。然后，我们接受对应于最小WSS的聚类解决方案。</p><p id="1988" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为了比较这些方法，我们将选择一个具有3个聚类和2个变量的人工数据。然后，我们将使用每种方法重复初始化过程10次，并可视化初始化策略选择的初始点。</strong></p><p id="2dd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jr">请注意，我们只是比较各种初始化方法的结果。执行k-Means后的最终聚类在本文中没有详细讨论。</em></p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><p id="258f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用于试验的数据</strong></p><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es jz"><img src="../Images/41b25cf49d79ac8bd6efcff78b159b2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*II8YYrwHJDfws7_FufaXbA.png"/></div></figure><h1 id="281b" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak"> Forgy初始化</strong></h1><p id="36f3" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">该方法是k-Means的快速初始化方法之一。如果我们选择k个聚类，Forgy方法从数据中随机选择任意k个点作为初始点。</p><p id="192b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种方法是有意义的，因为通过k-Means检测的聚类更有可能接近数据中存在的模式。通过从数据中随机选择点，我们更有可能得到接近模式的点。</p><p id="a591" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是Forgy在Python中的初始化实现:</p><pre class="ka kb kc kd fd lk ll lm ln aw lo bi"><span id="c5e1" class="lp ki hi ll b fi lq lr l ls lt">def forgy_initialize(X, k):<br/>    '''Return Randomly sampled points from the data'''</span><span id="15f2" class="lp ki hi ll b fi lu lr l ls lt">    return X[np.random.choice(range(X.shape[0]), replace = False,     size = k), :]</span></pre><p id="2c4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用Forgy的方法来看看10种不同的初始化:</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/94fa664e629b7c062fddc768d574c175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWcn7KOZiMgMlNA6tDJjhA.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">用Forgy方法选择的10种初始构型</figcaption></figure><p id="9ede" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以观察到以下情况:</p><ol class=""><li id="6894" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">在迭代1、7、8中:Forgy的方法在每个聚类中初始化一个中心。这是运行k-Means的<strong class="ih hj">良好起点</strong>的指示，因为起点已经在各自的群集中，因此接近真实的质心。k-Means最有可能在几次迭代中收敛到全局最优。</li><li id="bff1" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">在第2、3、4、5、6、9、10次迭代中:Forgy的方法将2个点初始化为位于同一聚类内。这不是一个理想的状态。这可能会导致算法达到局部最优解，但结果不佳。举例来说，下面是我从这个配置运行k-Means时发生的情况:</li></ol><figure class="ka kb kc kd fd ke er es paragraph-image"><div class="er es me"><img src="../Images/179aea4a66efbcaeb6199c8affa87121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*L0PU3N-fpVeB_PW0qWbMMw.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">从不正确的初始点运行k均值的结果(条件2)</figcaption></figure><p id="61f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们记得从不同的初始配置运行k-Means(然后我们可能得到一些好的配置，如在迭代1、7、8中),可以避免上述情况，这将产生全局最优。</p><h1 id="290f" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">随机分割法</strong></h1><p id="82fe" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">在这种方法中，我们将数据中的每个点随机分配给一个随机的聚类ID。然后，我们根据聚类ID对这些点进行分组，并取平均值(每个聚类ID)来产生初始点。众所周知，随机分割法可以产生接近数据平均值的初始点。</p><p id="2206" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是使用随机分区方法生成初始点的代码:</p><pre class="ka kb kc kd fd lk ll lm ln aw lo bi"><span id="6502" class="lp ki hi ll b fi lq lr l ls lt">def random_partition(X, k):<br/>    '''Assign each point randomly to a cluster. Then calculate the             Average data in each cluster to get the centers'''<br/>    indices = np.random.choice(range(0, k), replace = True, size = X.shape[0])<br/>    mean = []<br/>    for count in range(k):<br/>        mean.append(X[indices == count].mean(axis=0))<br/>        <br/>    return np.concatenate([val[ None, :] for val in mean], axis = 0)</span></pre><p id="bb93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看看使用随机分区方法的10种不同的初始化:</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/29d532cc896723bae2cf29dcaac4383d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IfDyfaEnspJjQY3hc6ofTw.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">随机分割法选取初始点</figcaption></figure><p id="d309" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所见，选择的初始点非常接近数据的全局平均值。这不是初始化k均值的推荐方法。请参考https://dl.acm.org/doi/10.1145/584792.584890的<a class="ae mf" href="https://dl.acm.org/doi/10.1145/584792.584890" rel="noopener ugc nofollow" target="_blank">的结论部分，其中总结了随机划分相对有效的聚类算法。</a></p><p id="3a0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果使用这种方法初始化，k-Means更容易陷入局部最优。</p><h1 id="2e43" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak"> kmeans++ </strong></h1><p id="13e6" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">这是一种标准方法，通常比Forgy的方法和初始化k-Means的随机划分方法效果更好。</p><p id="ac36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该方法在<a class="ae mf" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf" rel="noopener ugc nofollow" target="_blank">http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf</a>中有详细描述</p><p id="2c14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，想法是选择初始点，它们彼此尽可能远离。</p><ol class=""><li id="bc59" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">我们从从数据中选择一个随机点开始。</li><li id="8425" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">然后，我们选择下一个点，使得它更有可能位于离第一个点很远的地方。我们通过从概率分布中采样一个点来做到这一点，该概率分布与一个点到第一个中心的距离的平方成比例。</li><li id="6350" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">剩余的点由概率分布生成，该概率分布与每个点到其最近中心的距离的平方成比例。因此，离其最近中心距离较远的点更有可能被采样。</li></ol><p id="e795" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是使用kmeans++生成初始点的代码</p><pre class="ka kb kc kd fd lk ll lm ln aw lo bi"><span id="c1a7" class="lp ki hi ll b fi lq lr l ls lt">def dist(data, centers):<br/>    distance = np.sum((np.array(centers) - data[:, None, :])**2, axis = 2)<br/>    return distance</span><span id="5825" class="lp ki hi ll b fi lu lr l ls lt">def kmeans_plus_plus(X, k, pdf_method = True):<br/>    '''Initialize one point at random.<br/>    loop for k - 1 iterations:<br/>        Next, calculate for each point the distance of the point from its nearest center. Sample a point with a <br/>        probability proportional to the square of the distance of the point from its nearest center.'''<br/>    centers = []<br/>    X = np.array(X)<br/>    <br/>    # Sample the first point<br/>    initial_index = np.random.choice(range(X.shape[0]), )<br/>    centers.append(X[initial_index, :].tolist())<br/>    <br/>    print('max: ', np.max(np.sum((X - np.array(centers))**2)))<br/>    <br/>    # Loop and select the remaining points<br/>    for i in range(k - 1):<br/>        print(i)<br/>        distance = dist(X, np.array(centers))<br/>        <br/>        if i == 0:<br/>            pdf = distance/np.sum(distance)<br/>            centroid_new = X[np.random.choice(range(X.shape[0]), replace = False, p = pdf.flatten())]<br/>        else:<br/>            # Calculate the distance of each point from its nearest centroid<br/>            dist_min = np.min(distance, axis = 1)</span><span id="76f9" class="lp ki hi ll b fi lu lr l ls lt">if pdf_method == True:<br/>                pdf = dist_min/np.sum(dist_min)</span><span id="b2cd" class="lp ki hi ll b fi lu lr l ls lt"># Sample one point from the given distribution<br/>                centroid_new = X[np.random.choice(range(X.shape[0]), replace = False, p = pdf)]<br/>            else:<br/>                index_max = np.argmax(dist_min, axis = 0)<br/>                centroid_new = X[index_max, :]<br/>        centers.append(centroid_new.tolist())<br/>        <br/>    return np.array(centers)</span></pre><p id="2f1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看使用kmeans++方法的10种不同的初始化:</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/340e912d0fad9d8c1ec20a8ca2cc6ad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dwG65wYPsYHI5ujX81CnKw.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">用kmeans++方法选择初始点</figcaption></figure><p id="6a37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们可以看到的，在所有10个案例中，所有的初始点都在各自的集群内。这种生成初始点的方法有助于k-Means在几次迭代中收敛到全局最小值。<strong class="ih hj">这是k-Means算法生成初始点的推荐方法。</strong></p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="fa40" class="kh ki hi bd kj kk mg km kn ko mh kq kr ks mi ku kv kw mj ky kz la mk lc ld le bi translated"><strong class="ak">结论</strong></h1><p id="b612" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">总的来说，我们看到初始化k-Means的方法给出了最好的初始点是kmeans++。即使我们通过运行kmeans++获得了很好的初始点，仍然建议从不同的起点运行k-Means。</p><p id="0b52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管k-Means是一种相对简单的算法，它做出了许多假设，但由于其速度、可扩展性和易于解释，它在各种设置中仍然非常有用。</p></div></div>    
</body>
</html>