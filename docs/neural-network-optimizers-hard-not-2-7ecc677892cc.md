# 深度学习优化器——难？不是。[2]

> 原文：<https://medium.com/analytics-vidhya/neural-network-optimizers-hard-not-2-7ecc677892cc?source=collection_archive---------41----------------------->

![](img/1397b7a86d860f2f9ac535c94cb1f58e.png)

在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上由 [Franck V.](https://unsplash.com/@franckinjapan?utm_source=medium&utm_medium=referral) 拍摄的照片

在[之前的文章](/@hmrishavbandyopadhyay/deep-learning-optimizers-hard-not-3f5c4f7b4e96)中，我谈到了随机梯度下降和一些最优化的基础知识。虽然 SGD 非常受欢迎，但它的学习速度是固定的或衰减的，所以它通常会变得很慢。为了提高优化的速度，我们使用动量。“真的，气势？跟物理里一样？”——嗯，与*不同，*但它提出了一个类似于我们在物理学中看到的动量的类比。

# 动力

动量法被设计成在出现高曲率时加速学习。动量累积了过去梯度的指数衰减移动平均值，并向评估的方向移动。

非常有趣的是，动量沿着“速度”v 建立。这个速度表示参数在学习空间中移动的“速度”。现在，我们将与物理世界进行类比。要使粒子在空间中移动，力是必要的。这里，负梯度就是力的类比。它通过累积加速学习来推动参数通过参数空间。

在梯度下降和 SGD 中，步长被定义为梯度的范数乘以学习速率。然而，为了动力，我们需要加速学习。动量的步长取决于梯度序列的大小。如果更多的梯度指向相同的方向，则步长更大，因为最终的“速度”取决于梯度的指数衰减平均值。

然而，如果唯一的力是梯度，我们就有问题了。想象一个球被推下一个抛物线结构。如果球上唯一的力是推力，那么球将达到抛物线的最小值，并继续上升。这是这里的一个问题。为了解决这个问题，我们在模型中添加了另一个“力”——它将阻止参数超出成本函数最小值的进一步运动。我们可以假设这个“力”等于粘性阻力，它与-v 成正比。因此，当运动不合理时，需要这个力来防止运动。

重要的是要注意，这样的动量不能作为优化算法。它最多可以被认为是一般随机梯度下降算法的扩展。用 PyTorch 实现的 SGD 可以在 PyTorch 文档中查看以供参考。这是一个有趣的实现，结合了动量和普通的 SGD。

诸如 SGD 的算法具有固定的学习率或线性衰减的学习率。这些正慢慢被它们的适应性对手所掩盖。具有自适应学习率的优化算法是必要的，因为学习率极难设置，并且对模型输出和学习有显著影响。一些关注自适应学习率的流行优化算法是 AdaGrad、RMSProp 和 Adam(自适应矩),其中 RMSProp 是 AdaGrad 算法的修改，Adam 通过 RMSProp 引入矩。

AdaGrad 算法通过与所有模型参数的历史平方值之和的平方根成反比来调整所有模型参数的学习率。

神经网络迭代地优化，并且从迭代开始的地方进行一些初始化是必要的。初始点在确定模型是否训练和收敛或不收敛方面起着巨大的作用。

我希望这篇文章有助于打破优化算法的概念及其在深度学习领域的必要性。如果有任何问题，请在评论区告诉我:)