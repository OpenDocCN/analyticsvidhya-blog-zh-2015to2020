# 支持向量机简介

> 原文：<https://medium.com/analytics-vidhya/support-vector-machine-86bbcb650e81?source=collection_archive---------20----------------------->

支持向量机是分类问题中最常用的监督学习算法。这篇文章讨论了线性 SVM 算法背后的数学/直觉，简单介绍了用于优化部分的拉格朗日乘子。

# 线性 SVM:

在 2-D 空间中，有 2 组点由它们的坐标(x1，x2)表示，它们的标签由值为 1 或-1 的 y 表示。

![](img/07134f68f37aeeb0131f68ef34a3c910.png)![](img/5d44eaf9baf5934623538ba9d5e80e57.png)

这个问题的目标是找到一个超平面 s.t，它完美地将两组点分类，如下所示。在这种情况下，超平面是分割两组点的直线。

![](img/c7c8d7e55bf660862d54ed2c3a89b052.png)

这意味着，位于该超平面任一侧的任何点都满足条件

![](img/5ca5b8448e868523f849a7596d962289.png)

显然，可以有几个这样的超平面来区分这两类。但是我们如何选择正确的超平面呢？

![](img/1845ecf9ee59efa5ad0a42e619a0c448.png)

首先让我们理解几何余量的概念:

![](img/e460acd93283d6b30142724774782a90.png)![](img/aa07a4c37aed6ada21a02ffb49cc6031.png)

因此，

![](img/056dc4e3568b7a6b6ea92800d8d9a589.png)

为了找到正确的超平面，我们必须最大化几何余量，如下图所示

![](img/4ebfca1d32d1a7c13f3c7fd204ebf53e.png)

虚线表示边缘超平面，距离边缘超平面最近的点是+1/-1。这些点是支持向量。例如，在下图中，支持向量(最接近边缘超平面的点)用绿色和鲜红色着色。

![](img/f8cfa84ccb754b1b1efe9c34fc532ce9.png)

如果这些点的位置被改变，那么优化的决策边界的位置将受到影响。因此，最靠近决策边界的点决定了决策边界的位置。

# 优化线性 SVM:

在寻找超平面的最优解时，我们先前建立了几何余量必须最大化。

![](img/2b15483ed1182354ea34448dce6da1d3.png)

相当于

![](img/a6c15866d60e7e9824f77ed03104de9f.png)

服从于:

![](img/06b4d0461b84ed4f9a6bcd03cfb2ef3f.png)

我们可以使用拉格朗日乘数法来解决这个优化问题。

![](img/e7ffdbc2eb4c7b255e6154a5f6e7efa3.png)![](img/aa40a5f2b6f2a56c369dbd51c376efdf.png)![](img/2a895a26a9036975c0404ab7393a87a6.png)![](img/03b6b4ce77f3788770bfe88b50c032a9.png)

如我们所见，w 代表的权重是训练向量 x1…xm iff 的线性组合

![](img/bf2776fd16f89416d2f8f10847de45ad.png)

此外，如果满足上述条件，则

![](img/c1b9db31f51956bbcf74f49ea97da08b.png)

因此，所有的支持向量都位于边缘超平面上。

在 python 上解决这个问题时，我们首先试图找到接近支持向量(满足上述等式的 x1…xms)的超平面，然后通过递增地改变 w 的值来优化最大余量，以找到最佳超平面。结果看起来会像这样。虚线表示次优超平面。

![](img/ec6b2dd437fa606de5c38ea9c7162a02.png)