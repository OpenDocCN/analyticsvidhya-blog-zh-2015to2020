<html>
<head>
<title>Decision Trees — simple and interpret-able algorithm.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树——简单易懂的算法。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-trees-simple-and-interpret-able-algorithm-c1c154b6a347?source=collection_archive---------17-----------------------#2020-08-26">https://medium.com/analytics-vidhya/decision-trees-simple-and-interpret-able-algorithm-c1c154b6a347?source=collection_archive---------17-----------------------#2020-08-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/bae0cb0d16e9e7f7a49ef0fd06ac1df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cJLZECsL9SP3O6gw"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">杰里米·毕晓普在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="e671" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">我决定用NumPy实现一些传统的机器学习算法来理解它们。这里可以找到<a class="ae hv" href="https://github.com/nachiket273/ML_Algo_Implemented" rel="noopener ugc nofollow" target="_blank">的储存库</a>。这将是一系列关于这些算法的文章，从决策树开始，主要是我自己的理解。</p></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="36ee" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">决策树是用于分类和回归任务的最简单和流行的方法之一。决策树是一种监督学习方法。其想法是创建一个模型(树),根据从训练数据集中学习到的简单决策规则来预测目标。决策树是非参数方法。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es ka"><img src="../Images/2692e4d0e44b60e106440830b6ac961c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*tnQFYfPvg-b8JzGNgxOy5w.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">参考:<a class="ae hv" href="https://scikit-learn.org/stable/auto_examples/tree/plot_iris_dtc.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/tree/plot _ iris _ DTC . html</a></figcaption></figure><h1 id="eaf7" class="kf kg hy bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">决策树中使用的重要术语</strong></h1><p id="477f" class="pw-post-body-paragraph jc jd hy je b jf ld jh ji jj le jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz hb bi translated"><strong class="je hz">根节点:</strong>树的最顶端节点。<br/> <strong class="je hz">拆分:</strong>根据某种拆分标准将节点划分为一个或多个子节点的过程。<br/> <strong class="je hz">父节点:</strong>被拆分成一个或多个节点的节点。<br/> <strong class="je hz">子节点:</strong>拆分生成的子节点。<br/> <strong class="je hz">叶/端节点:</strong>不进一步拆分的节点。决策树具有与作为类值/回归值的叶节点相关联的样本。<br/> <strong class="je hz">决策节点:</strong>根据某种决策拆分成一个或多个子节点的节点。</p><h1 id="0352" class="kf kg hy bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">创建决策树时的假设</h1><ul class=""><li id="9a46" class="li lj hy je b jf ld jj le jn lk jr ll jv lm jz ln lo lp lq bi translated">在开始，整个数据集被认为是在根，递归分裂，以创建树。</li><li id="f4e4" class="li lj hy je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">要素最好是分类的，连续值要素在创建模型之前被离散化。</li><li id="4413" class="li lj hy je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">通过使用一些统计方法来完成将属性放置为树的根或内部节点的顺序。</li></ul><h1 id="7852" class="kf kg hy bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">递归二进制分裂</h1><p id="34ed" class="pw-post-body-paragraph jc jd hy je b jf ld jh ji jj le jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz hb bi translated">为了形成决策树，所有的特征被考虑用于分裂，并且不同的分裂点被尝试以决定最佳分裂。允许最大信息增益(最小化杂质)的特征和值被用于分割节点。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es lw"><img src="../Images/e5cf938d4f247f1138a2be4193d3f772.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/1*199BkDOil4ssogLBUJvjSA.gif"/></div></figure><p id="9eb2" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">可以使用不同的标准来计算信息增益。让我们来看一个最常见的标准——熵。</p><h2 id="a2aa" class="lx kg hy bd kh ly lz ma kl mb mc md kp jn me mf kt jr mg mh kx jv mi mj lb mk bi translated">熵:</h2><p id="778a" class="pw-post-body-paragraph jc jd hy je b jf ld jh ji jj le jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz hb bi translated">熵代表信息系统中的随机性。熵越高，随机性越大，对系统做出任何决定就越困难。因此，对于分割，可以选择最大程度地降低熵的特征和值。</p><p id="7348" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">n态系统的香农熵定义为:</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es ml"><img src="../Images/08169f2d9bd85463f74704f85bff7f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/1*efUbjDYCxl9K_2VI970ZzA.gif"/></div></figure><p id="26e2" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">让我们考虑一个简单的例子，8个球，5个实心，3个空心。我们需要根据球的位置找到一个分割点。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mm"><img src="../Images/aa472586a7b7f0789747e5e5ebad4768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEXCEDwBOk01vRxlYEXa8w.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">当在位置4分裂时，以熵作为标准的信息增益计算。</figcaption></figure><p id="00b7" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">从上图可以看出，在4号位拆分时，信息增益为0.0488，大大降低了熵，稳定了系统，所以这是一个很好的拆分。</p><p id="52df" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">重复该过程，直到达到停止标准。</p><p id="2c29" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">熵标准可用于分类。可用于分类的另一个标准是基尼指数。</p><h1 id="cb44" class="kf kg hy bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">基尼指数。</h1><p id="0e05" class="pw-post-body-paragraph jc jd hy je b jf ld jh ji jj le jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz hb bi translated">基尼指数是从1中减去每一类概率的平方和。它喜欢更大的分区。对于二进制分割，公式如下:</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es mn"><img src="../Images/847beeacb2dffc9865953cf9624558c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/1*_nGPv4XbSRclIyx1QHA3TQ.gif"/></div></figure><h1 id="72f4" class="kf kg hy bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">回归标准</h1><p id="56b5" class="pw-post-body-paragraph jc jd hy je b jf ld jh ji jj le jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz hb bi translated">类似于分类，如果目标是连续值，我们需要不同的标准来最小化距离。最常见的标准是均方误差(MSE)和平均绝对误差(MAE ),前者使用叶节点的平均值来最小化L2误差，后者最小化L1损耗。</p><h2 id="3683" class="lx kg hy bd kh ly lz ma kl mb mc md kp jn me mf kt jr mg mh kx jv mi mj lb mk bi translated">MSE:</h2><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/7e92f4f6a33be56be623203f8ed2b301.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/1*OV8beT40T3UrQybFUnUkLg.gif"/></div></figure><h2 id="7816" class="lx kg hy bd kh ly lz ma kl mb mc md kp jn me mf kt jr mg mh kx jv mi mj lb mk bi translated">梅:</h2><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/387dddf789981c7dba24a5e8c3195a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/1*tJZ0Rcbq_Lz8FWT_VCMg4w.gif"/></div></figure><h1 id="c607" class="kf kg hy bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">什么时候停止分裂？</h1><p id="f142" class="pw-post-body-paragraph jc jd hy je b jf ld jh ji jj le jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz hb bi translated">对于包含大量特征的数据集，分割可能会持续很长时间，这将导致复杂的树，并且树可能会过度适应数据。所以需要有停止标准来避免这些问题。可以使用不同的停止标准，例如</p><ul class=""><li id="7ee1" class="li lj hy je b jf jg jj jk jn mq jr mr jv ms jz ln lo lp lq bi translated">叶节点中的最小样本数。</li><li id="b88c" class="li lj hy je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">树的最大深度。</li></ul><h1 id="22ef" class="kf kg hy bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">修剪</h1><p id="9669" class="pw-post-body-paragraph jc jd hy je b jf ld jh ji jj le jl jm jn lf jp jq jr lg jt ju jv lh jx jy jz hb bi translated">修剪包括除去利用不太重要特征的树枝。这降低了树的复杂性，并且可以避免过度拟合。修剪可以从根或叶开始。最简单的修剪方法是从叶子开始。<strong class="je hz">减少错误修剪</strong>包括移除该节点的子树，使其成为叶子，并在该节点分配最常见的类。如果结果树在验证集上的表现不比原始树差，则移除节点。该过程迭代地继续，修剪节点的移除给出了最大的优势。当进一步修剪会降低性能时，修剪会停止。</p><h1 id="2cde" class="kf kg hy bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">决策树的优势</strong></h1><ul class=""><li id="2822" class="li lj hy je b jf ld jj le jn lk jr ll jv lm jz ln lo lp lq bi translated">易于理解、解释和想象。</li><li id="3e5b" class="li lj hy je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">需要最少的数据预处理，如删除空白、一键编码。</li><li id="f73e" class="li lj hy je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">能够处理数字和分类数据</li><li id="4523" class="li lj hy je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">即使其假设与生成数据的真实模型有些冲突，也能表现良好。</li></ul><h1 id="7f31" class="kf kg hy bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">决策树的缺点</h1><ul class=""><li id="f328" class="li lj hy je b jf ld jj le jn lk jr ll jv lm jz ln lo lp lq bi translated">过度拟合-会创建复杂的树，不能很好地概括。</li><li id="57e3" class="li lj hy je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">对数据变化不稳定。像随机森林这样的集合方法有助于避免这个问题。</li><li id="eaa0" class="li lj hy je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">阶级不平衡会造成有偏见的树。</li></ul><p id="97d7" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">这只是决策树的一个基本介绍。这里的库<a class="ae hv" href="https://github.com/nachiket273/ML_Algo_Implemented/tree/master/DecisionTree" rel="noopener ugc nofollow" target="_blank">包含了带有NumPy的决策树的最小实现。<strong class="je hz">注意，这是一个非常简单的实现，不需要太关注性能。(对于大型数据集，拟合函数可能非常慢)</strong></a></p><p id="798b" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><strong class="je hz">参考文献:</strong></p><ul class=""><li id="e298" class="li lj hy je b jf jg jj jk jn mq jr mr jv ms jz ln lo lp lq bi translated"><a class="ae hv" href="https://scikit-learn.org/stable/modules/tree.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/tree.html</a></li></ul></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="5b4e" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">如果你喜欢这篇文章，请一定给我一个掌声，并关注我以获取我未来文章的更新。</p><p id="c308" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><em class="mt">还有，随时在</em><a class="ae hv" href="https://www.linkedin.com/in/nachikettanksale/" rel="noopener ugc nofollow" target="_blank"><em class="mt">LinkedIn</em></a><em class="mt">上联系我或者在</em><a class="ae hv" href="https://twitter.com/nachiket273" rel="noopener ugc nofollow" target="_blank"><em class="mt">Twitter</em></a><em class="mt">上关注我。</em></p><p id="79f0" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">如果你喜欢我的作品，请考虑赞助我，它会帮助我推出更多这样的作品。</p></div></div>    
</body>
</html>