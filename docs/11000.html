<html>
<head>
<title>Batch Normalization: A different perspective from Quantized Inference Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批量规范化:与量化推理模型不同的视角</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/batch-normalization-a-different-perspective-from-quantized-inference-model-f362cac2bd85?source=collection_archive---------19-----------------------#2020-11-12">https://medium.com/analytics-vidhya/batch-normalization-a-different-perspective-from-quantized-inference-model-f362cac2bd85?source=collection_archive---------19-----------------------#2020-11-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5f32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">众所周知，训练中批量标准化的好处是减少内部协变量偏移，从而优化训练以更快收敛。本文试图引入一个不同的视角，在批量归一化层的帮助下恢复量化损失，从而保持模型的准确性。该文章还给出批量标准化的简化实现，以减少边缘设备上的负载，该负载通常会限制神经网络模型的计算。</p><p id="4b2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">批量标准化理论</strong></p><p id="ceed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练神经网络的过程中，我们必须确保网络学习得更快。加快速度的方法之一是使网络输入正常化，同时使网络的间歇层正常化。这种中间层规范化就是所谓的批量规范化。批次范数的优势还在于它有助于最小化内部协变量移位，如本文所述。</p><p id="e300" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像TensorFlow、Keras和Caffe这样的框架都有相同的表示，只是附加了不同的符号。一般而言，批量标准化可由以下数学公式描述:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/8bbca860411e0c3b6fe7f4333fe09423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*4YuW-o9QbJ3daKlq.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><em class="jp">批量归一化方程</em></figcaption></figure><p id="3f23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，等式(1.1)是Keras/TensorFlow的表示。而等式(1.2)是Caffe框架使用的表示。在这篇文章中，方程(1.1)风格被用于上下文的延续。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jq"><img src="../Images/5b691338f0b9151754363bdb78e8c54d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/0*WLGftMsCQT0dan0u.png"/></div></figure><p id="906c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们将等式(1.1)修改如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jr"><img src="../Images/9a51d505252946c5ccd1b79aff6d9bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/0*ZbG2JalTkJsDUBXh.png"/></div></figure><p id="6b97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，通过观察(1.4)的等式，在减少乘法和加法的数量方面，存在优化的选择。可以为每个通道离线计算偏置梳(读为组合偏置)因子。此外,“gamma/sqrt(方差)”的比率可以离线计算，并且可以在实现批量范数方程时使用。该方程可用于量化推理模型，以降低复杂性。</p><p id="cb42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">量化推理模型</strong></p><p id="c3bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">部署在边缘设备中的推理模型通常是整数运算友好的CPU，如ARM Cortex-M/A系列处理器或FPGA设备。现在，为了使推理模型对边缘设备的架构友好，将在Python中创建一个仿真。然后将推理模型的输入、权重和输出链转换成定点格式。在定点格式中，选择8位的Q用整数.小数格式表示。这个模拟模型将帮助您在设备上更快地开发推理模型，还将帮助您评估模型的准确性。</p><p id="5568" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:Q2.6表示6位小数，2位整数。</p><p id="eec5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，表示每个层的Q格式的方式如下:</p><ol class=""><li id="27f1" class="js jt hi ih b ii ij im in iq ju iu jv iy jw jc jx jy jz ka bi translated">取输入、输出和各层/权重的最大值和最小值。</li><li id="7168" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated">使用Python函数获取表示动态范围所需的小数位数(通过使用最大值/最小值)如下:</li></ol><blockquote class="kg kh ki"><p id="b812" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">def get _ fract _ bits(tensor _ float):<br/>#假设8位中，有一位作为符号<br/>fract _ dout = 7—NP . ceil(NP . log2(ABS(tensor _ float)。max()))</p><p id="a011" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">frac _ dout = frac _ dout . astype(' int 8 ')</p><p id="e4fe" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">return fract_dout</p></blockquote><p id="c093" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">举例:<br/> </strong>我们把Resnet-50当做一个模型来量子化。让我们使用用Imagenet训练的Keras内置Resnet-50。</p><blockquote class="kg kh ki"><p id="86a8" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">#创建模型<br/> def model_create():</p><p id="4923" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">model = TF . compat . v1 . keras . applications . resnet 50 . resnet 50(include _ top = True，weights='imagenet '，input_tensor=None，input_shape=None，</p><p id="7a53" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">池=无，类=1000)</p><p id="fc2a" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">回报模型</p></blockquote><p id="9cab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们为resnet-50准备输入。下图取自ImageNet数据集。</p><blockquote class="kg kh ki"><p id="bbc0" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">def准备_输入():</p><p id="31f2" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">img = image . load _ img(" D:\ \ Elephant _ water . jpg "，target_size=(224，224))</p><p id="7ca9" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">x_test = image.img_to_array</p><p id="3aa9" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">x_test = np.expand_dims(x_test，axis=0)</p><p id="9b75" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">x =预处理_输入(x _测试)# from tensor flow . compat . v1 . keras . applications . resnet 50导入预处理_输入，解码_预测</p><p id="6a45" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">返回x</p></blockquote><p id="f9e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们调用上面的两个函数，找出输入的Q格式。</p><blockquote class="kg kh ki"><p id="5cc7" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">模型=模型_创建()</p><p id="bd99" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">x =准备输入()</p></blockquote><p id="7aa1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果观察输入‘x’，它的动态范围在-123.68到131.32之间。这使得很难适合8位，因为考虑到一个符号位，我们只有7位来表示这些数字。因此，此输入的Q格式将变为Q8.0，其中7位是输入数字，1位是符号位。因此，它将数据剪切在-128到+127之间(- ⁷到⁷ -1)。因此，在这种输入量化转换中，我们会丢失一些数据(最明显的是131.32被削波至127)，其损失可以通过信号与量化噪声比看出，这将在下面描述。</p><p id="dd04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你对每个层的权重和输出使用相同的方法，我们将会有一些Q格式，我们可以固定它来模拟量化。</p><blockquote class="kg kh ki"><p id="fbdd" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">#让我们获取第一层属性</p><p id="e147" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">(填充，_) = model.layers[1]。填料</p><p id="4536" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">#让我们获取第二层属性</p><p id="c95c" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">wts = model.layers[2]。获取权重()</p><p id="74a8" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">跨度= model.layers[2]。大步</p><p id="7c3c" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">W=wts[0]</p><p id="6054" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">b=wts[1]</p><p id="54d3" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">hparameters =dict(</p><p id="f2be" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">pad=padding[0]，</p><p id="486d" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">步幅=步幅[0]</p><p id="e1c4" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi">)</p><p id="5248" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">#让我们量化权重。</p><p id="0dc9" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">quant_bits = 8 #这将是我们的数据路径。</p><p id="b2c6" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">wts_qn，wts_bits_fract = Quantize(W，quant_bits) #权重和偏差都将使用wts_bits_fract进行量化。</p><p id="884e" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">#让量化偏差也在wts_bits_fract</p><p id="6fb0" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">b_qn = (np.round(b *(2 &lt;<wts_bits_fract/></p><p id="1234" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">names_model,names_pair = getnames_layers(model)</p><p id="8097" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">layers_op = get_each_layers(model,x,names_model)</p><p id="35d2" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">quant_bits = 8</p><p id="8b0f" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">print(“Running conv2D”)</p><p id="668c" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated"># Lets extract the first layer output from convolution block.</p><p id="02f5" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">Z_fl = layers_op[2] # This Number is first convolution.</p><p id="acd1" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated"># Find out the maximum bits required for final convolved value.</p><p id="d568" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">fract_dout = get_fract_bits(Z_fl)</p><p id="949d" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">fractional_bits = [0,wts_bits_fract,fract_dout]</p><p id="9070" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated"># Quantized convolution here.<br/> Z，cache _ conv = conv _ forward(x . as type(' int 8 ')，wts_qn，b_qn[np.newaxis，np.newaxis，np.newaxis，…]，hparameters，fractional_bits)</p></blockquote><p id="9119" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，如果您观察上面的代码片段，卷积运算将获取输入、权重和输出，并定义其分数位。<br/>即:fractional_bits=[0，7，-3] <br/>其中，第1个元素表示输入的分数表示的0位(Q8.0) <br/>第2个元素表示权重的分数表示的7位(Q1.7)。<br/>第3个元素代表输出的分数表示的-3位(Q8.0，但需要额外的3位用于整数表示，因为范围超出了8位表示)。</p><p id="117d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将不得不对每一层重复以获得Q格式。</p><p id="18c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在已经熟悉了量化，我们可以开始讨论量化对SQNR和精度的影响。</p><p id="478b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">信号与量化噪声比</strong></p><p id="6417" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们已经通过使用Q格式将动态范围从浮点表示减少到定点表示，因此我们已经将值离散化为最接近的整数表示。这引入了量化噪声，量化噪声可以通过信号与量化噪声比进行数学量化。(参考:<a class="ae kn" href="https://en.wikipedia.org/wiki/Signal-to-quantization-noise_ratio" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Signal-to-quantization-noise _ ratio</a>)</p><p id="8078" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上式所示，我们将测量信号功率与噪声功率之比。应用于对数标度的这种表示转换为dB (10log10SQNR)。这里，信号是浮点输入，我们将其量化为最接近的整数，噪声是量化噪声。<br/> <em class="kj">示例</em>:输入的大象示例的最大值为131.32，但是我们将它表示为最接近的整数，即127。因此，量化噪声= 131.32–127 = 4.32。<br/>因此，SQNR = 131.3 /4.3 = 924.04，即29.66 db，这表明与48dB(6 *无比特数)的可能性相比，我们仅获得了接近30dB的可能性。</p><p id="2fca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">取决于结构，可以为每个单独的网络建立SQNR对准确性的这种反映。但是间接的，我们可以说SQNR越好，精确度越高。</p><p id="8f5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">量化环境中的卷积:</strong></p><p id="3ee7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CNN中的卷积运算是众所周知的，我们用输入乘以核，累加得到结果。在此过程中，我们必须记住，我们是以8位作为输入进行操作的，因此乘法的结果至少需要16位，然后将其累积在32位累加器中，这将有助于保持结果的精度。则结果被舍入或截断为8位，以携带8位宽的数据。</p><blockquote class="kg kh ki"><p id="5c69" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">def conv _单步_量化(a_slice_prev，W，b，ip _ fract，wt _ fract，fract _ dout):</p><p id="0392" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi">“””</p><p id="84a6" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">在输出激活的单个切片(a_slice_prev)上应用由参数W定义的一个过滤器</p><p id="f6cc" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">前一层的。</p><p id="1e3e" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">参数:</p><p id="2a1c" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">a _ slice _ prev-shape(f，f，n_C_prev)的输入数据切片</p><p id="8ade" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">W —包含在窗口中的权重参数—形状矩阵(f，f，n_C_prev)</p><p id="f77c" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">b —包含在形状(1，1，1)的窗口矩阵中的偏差参数</p><p id="12d8" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">退货:</p><p id="7272" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">z-标量值，对输入数据的切片x上的滑动窗口(W，b)进行卷积的结果</p><p id="5cad" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi">“””</p><p id="5f33" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">a _ slice和w之间的元素式乘积。暂时不添加偏差。</p><p id="4fcf" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">s = NP . multiply(a _ slice _ prev . astype(' int 16 ')，W) #让结果保存在16位中</p><p id="80f6" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">#对卷s的所有条目求和</p><p id="86c9" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">z = NP . sum(s . as type(' int32 ')#最终结果存储在int 32中。</p><p id="8cbf" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated"># 32位的结果将被指令化为8位以恢复数据路径。</p><p id="c103" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">#将偏差b添加到Z。将b强制转换为float()，以便Z产生标量值。</p><p id="0960" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">#将偏置设为32位，以添加到z。</p><p id="182e" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">Z = Z+(b&lt;&lt; ip_fract).astype(‘int32’)</p><p id="7f03" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated"># Lets find out how many integer bits are taken during addition.</p><p id="6c52" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated"># You can do this by taking leading no of bits in C/Assembly/FPGA programming</p><p id="2ba2" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated"># Here lets simulate</p><p id="4c3f" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">Z = Z &gt;&gt;(IP _ frac+wt _ frac—frac _ dout)</p><p id="62b4" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">如果(Z &gt; 127):</p><p id="44ca" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">Z = 127</p><p id="dc0c" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">elif(Z &lt; -128):</p><p id="71e9" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">Z = -128</p><p id="494a" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">else:</p><p id="e6b8" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">Z = Z.astype(‘int8’)</p><p id="f8ea" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">return Z</p></blockquote><p id="a2f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">The above code is inspired from AndrewNg’s deep learning specialization course, where convolution from scratch is taught. Then modified the same to fit for Quantization.</p><p id="f776" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">量子化环境中的批量范数</strong></p><p id="490b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如等式1.4所示，我们修改了表示法，以降低复杂性并执行批量标准化。下面的代码展示了相同的实现。</p><blockquote class="kg kh ki"><p id="c2f3" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">def calculate_bn(x，bn_param，Bn_fract_dout):</p><p id="342a" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">x_ip = x[0] <br/> x_fract_bits = x[1]</p><p id="dc27" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">bn _ param _ gamma _ s = bn _ param[0][0]</p><p id="53bd" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">bn _ param _ frac _ bits = bn _ param[0][1]</p><p id="7ac9" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">op = x _ IP * bn _ param _ gamma _ s . as type(NP . int 16)# x * gamma _ s</p><p id="1aeb" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">#此输出将有x _ frac _ bits+bn _ param _ frac _ bits</p><p id="f10f" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">分形位=x分形位+ bn参数分形位</p><p id="1fcf" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">bn_param_bias = bn_param[1][0]</p><p id="b7ca" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">bn _ param _ frac _ bits = bn _ param[1][1]</p><p id="5caa" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">bias = bn _ param _ bias . astype(NP . int 16)</p><p id="3467" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">#让我们将偏差调整到小数位数</p><p id="4391" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">bias = bias&lt;&lt; (fract_bits — bn_param_fract_bits)</p><p id="d1dd" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">op = op + bias # + bias</p><p id="0186" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated"># Convert this op back to 8 bits, with Bn_fract_dout as fractional bits</p><p id="12db" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">op = op &gt;&gt;(frac _ bits—Bn _ frac _ dout)</p><p id="cea2" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">BN_op = op.astype(np.int8)</p><p id="155e" class="if ig kj ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">返回BN_op</p></blockquote><p id="be16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在有了量化推理模型的这些部分，我们可以看到批量范数对量化的影响。</p><p id="2da3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结果</strong></p><p id="3ef5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用ImageNet训练的Resnet-50用于python仿真，量化推理模型。从上面的部分，我们结合在一起，只分析第一个卷积其次是批量范数层。</p><p id="ebec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就复杂性和维持模型的准确性而言，卷积运算是网络中最繁重的。我们来看看量化为8位后的卷积数据。下图左侧表示64个通道(或应用的滤波器)输出的卷积输出，其平均值用于比较。蓝色表示浮点参考，绿色表示量化实现。差值图(左侧)给出了浮点值和量化值之间存在多少差异的指示。差值图中画线是平均值，其值约为4。这意味着浮点值和量化值之间的平均差值接近4。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ko"><img src="../Images/3001e5990a29421ea556a6590778c2fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/0*67mLexApZ8cT4i7C.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><em class="jp">卷积和批量范数输出</em></figcaption></figure><p id="3058" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们来看右边的图，这是批量标准化部分。如你所见，绿色和蓝色曲线如此接近，它们的差异范围缩小到小于0.5范围。平均线在0.135左右，以前卷积的情况下在4左右。这表明我们正在将浮点和量化实现之间的差异从均值4减少到0.135(几乎接近0)。</p><p id="0914" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们来看看SQNR图，以了解批量定额的影响。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kp"><img src="../Images/c751670a884de289f554197abcc51437.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/0*t_2vM-Y2COLyLerW.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><em class="jp">层序列的信号与量化噪声比</em></figcaption></figure><p id="63da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以防数值不可见，我们有以下SQNR数<br/>输入SQNR : 25.58 dB(进入模型的输入)<br/>卷积SQNR : -4.4dB(第一次卷积的输出)<br/>批量标准SQNR : 20.98 dB(批量标准化输出)</p><p id="a4ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以看到，输入SQNR约为25.58dB，降至-4.4 dB，这表明损耗巨大，因为8位以上的表示受到限制。但是希望并没有落空，因为批处理规范化有助于将SQNR恢复到20.98 dB，使其接近输入SQNR。</p><p id="3c75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结论</strong></p><ul class=""><li id="1f56" class="js jt hi ih b ii ij im in iq ju iu jv iy jw jc kq jy jz ka bi translated">批量标准化有助于校正平均值，从而调整通道间的量化变化。</li><li id="e3c9" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc kq jy jz ka bi translated">批量标准化恢复SQNR。从上面的演示可以看出，与卷积层相比，我们看到了SQNR的恢复。</li><li id="88bd" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc kq jy jz ka bi translated">如果edge上的量化推理模型是理想的，则考虑包括批量归一化，因为它作为量化损失的恢复，也有助于保持准确性，以及更快收敛的训练好处。</li><li id="4f87" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc kq jy jz ka bi translated">使用(1.4)可以降低批量标准化的复杂性，从而可以离线计算许多参数，以减少边缘设备上的负载。</li></ul><p id="8ba8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献</strong></p><ul class=""><li id="3087" class="js jt hi ih b ii ij im in iq ju iu jv iy jw jc kq jy jz ka bi translated"><a class="ae kn" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">https://medium.com/r/?URL = https % 3A % 2F % 2 farxiv . org % 2 fpdf % 2f 1502.03167 . pdf</a></li><li id="d3e9" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc kq jy jz ka bi translated"><a class="ae kn" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/specializations/deep-learning</a></li><li id="6daa" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc kq jy jz ka bi translated"><a class="ae kn" href="https://en.wikipedia.org/wiki/Signal-to-quantization-noise_ratio" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Signal-to-quantization-noise _ ratio</a></li></ul></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><p id="7153" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kj">最初发表于</em><a class="ae kn" href="https://ignitarium.com/blogs/batch-normalization-a-different-perspective-from-quantized-inference-model/" rel="noopener ugc nofollow" target="_blank">T5【https://ignitarium.com】</a><em class="kj">。</em></p></div></div>    
</body>
</html>