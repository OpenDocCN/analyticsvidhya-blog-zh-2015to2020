<html>
<head>
<title>Logistic Regression — The journey from Odds to log(odds) to MLE to WOE to … let’s see where it ends!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归——从赔率到对数(赔率)到最大似然估计再到悲哀的旅程……让我们看看它的终点在哪里！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979?source=collection_archive---------4-----------------------#2020-02-09">https://medium.com/analytics-vidhya/logistic-regression-the-journey-from-odds-to-log-odds-to-mle-to-woe-to-lets-see-where-it-2982d1584979?source=collection_archive---------4-----------------------#2020-02-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a9d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好的，在我开始之前，我想向你们简要介绍一下背后的原因。现在许多图书馆都提供了直接使用ML算法的方法，而不需要知道背后的细节。从长远来看，接受黑箱结果对任何人都没有帮助，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/5c12e1e66357dd33ee9574edafec693d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/1*DauKPQhTwUJBntX6NtLbjQ.gif"/></div></figure><p id="89fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">应该对事情如何发展有一定程度的了解，因为知识永远是一生的资产，有助于更好地掌握任何实现。</p><p id="c1ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们走吧！！！</p><p id="f020" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归是一种在银行(信贷和风险)行业广泛使用的技术，用于检查违约问题的概率。这是一个广义线性模型(GLM)——我们将在本博客中进一步讨论我们到底指的是什么。</p><h1 id="d1a1" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><strong class="ak">最大似然估计:</strong></h1><p id="99a2" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">逻辑回归的工作原理是最大似然法，这是一种在给定观测值的情况下，通过找到最大化观测值可能性的参数值来估计模型参数的方法。这意味着找到使事件1的概率<strong class="ih hj"> <em class="ko"> p </em> </strong> <em class="ko">和非事件0的概率</em><strong class="ih hj"><em class="ko">(1-p)</em></strong><em class="ko">最大化的参数，如你所知:</em></p><blockquote class="kp"><p id="db52" class="kq kr hi bd ks kt ku kv kw kx ky jc dx translated"><strong class="ak"> <em class="kz">概率(事件+非事件)= 1 </em> </strong></p></blockquote><p id="077a" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">如果你有任何不明白的地方，不要担心，因为我们会分解每一个术语和每一个步骤。</p><p id="0c89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将讨论逻辑回归中的一些重要术语</p><p id="6fae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在将因变量转换为与自变量相关的logit变量(因变量发生或不发生的概率的自然对数)后，逻辑回归应用最大似然估计。这样，逻辑回归估计了某一事件发生的概率。在下面的等式中，概率对数作为解释变量的函数线性变化:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lf"><img src="../Images/e950cccd000e367565431be5b7187b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E_JI8HC6F6v48y6XfU2AnA.jpeg"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图1</figcaption></figure><p id="f380" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以现在，人们可以简单地问，为什么是odds，log(odds)而不是probability？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/888b61f2b167650f88fdac9ea545d0f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*cV7r3bfv534fAPauZBhGYQ.gif"/></div></figure><p id="44ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原因如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lp"><img src="../Images/179127c760f0c6f623fc112fffb9e899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*775mxlFYT5XShVj6o1T2qg.jpeg"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图2</figcaption></figure><p id="de0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过将概率转换为log(odds)，我们将范围从[0，1]扩大到[- ∞，+∞ ]。通过拟合概率模型，我们将会遇到范围受限的问题，并且通过应用对数变换，我们掩盖了所涉及的非线性，并且我们可以仅拟合变量的线性组合。</p><h2 id="0ed7" class="lq jm hi bd jn lr ls lt jr lu lv lw jv iq lx ly jz iu lz ma kd iy mb mc kh md bi translated">寻找逻辑回归的最佳正弦曲线以对观察值进行分类的步骤:</h2><ol class=""><li id="6056" class="me mf hi ih b ii kj im kk iq mg iu mh iy mi jc mj mk ml mm bi translated">现在，在将概率转换为对数(赔率)后，我们在y轴上得到范围从-∞到+∞的值。参考下图。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mn"><img src="../Images/cd885b25d939fc31b2afc7aff6e6c7a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OCJM33Bkv_qq_o2nfzRjEw.jpeg"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图3</figcaption></figure><p id="d154" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.像线性回归一样画一条候选线，并将趋向+-无穷大的数据点投影到这条线上。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mo"><img src="../Images/e6f739f529eb58bb1a83bc27de043792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4rClFLRVOq53clkEeovgTQ.jpeg"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图4</figcaption></figure><p id="2cd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这样，您将获得每个观察的logit值。</p><p id="6c40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">logit值= log(p/1–p)</p><p id="c0dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">- ∞ &lt; logit值</p><p id="3e40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.从这些logit值中，您可以获得候选线获得的每个观察的预测概率值。</p><p id="db0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过对这些值应用Sigmoid或逻辑函数，您可以获得范围在0到1之间的值(即我们训练数据中每个观察的预测概率)。下面我将推导这个方程(你可以跳过，只看最终的sigmoid公式)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mp"><img src="../Images/85db0ea49b6385d83638333e2bcfa7e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SJWhBvWloOJDV4BSDikS3w.jpeg"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图5</figcaption></figure><p id="d23a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.现在我们有了训练数据观测值的预测概率，我们将绘制它们，其中y轴的范围从0到1(目标变量)，x轴表示独立变量(预测值)，以获得对数据进行分类的正弦曲线。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mq"><img src="../Images/afe608ef1ccaa8f372ff813653175757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EfiagoWd2nDKPQIHKHIwbA.jpeg"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图6</figcaption></figure><p id="9412" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以保持阈值概率值(例如0.5)其中1类的任何东西&lt; 0.5 will be of class 0 and &gt; 0.5。</p><p id="a7e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.现在是时候检查我们的正弦(S形)曲线在训练数据上的表现了。(它的分类有多正确)。为此，我们将使用“最大似然估计”。</p><p id="64bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要得到最大似然，将所有预测的概率相乘，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es mr"><img src="../Images/2be05a9b20d978822b5663ddd4799b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOzDkrv6mcu-PNWkCrf-lw.jpeg"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图7</figcaption></figure><p id="0866" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.多次执行上述步骤，并通过旋转获得最佳拟合的候选线(参考步骤2 ),就像线性回归一样。通过选择最大似然值最高的曲线，获得对数据进行最佳分类的正弦曲线。</p><h2 id="1d04" class="lq jm hi bd jn lr ls lt jr lu lv lw jv iq lx ly jz iu lz ma kd iy mb mc kh md bi translated">为什么不能用普通最小二乘法[OLS]得到最佳候选线:</h2><ol class=""><li id="70e4" class="me mf hi ih b ii kj im kk iq mg iu mh iy mi jc mj mk ml mm bi translated">对类标签0/1值的转换将原始数据推到+ve和-ve无穷大。(参见图1)。</li><li id="f5a8" class="me mf hi ih b ii ms im mt iq mu iu mv iy mw jc mj mk ml mm bi translated">因此，残差，即数据点(∞)和候选线之间的距离也是∞。</li><li id="5fdc" class="me mf hi ih b ii ms im mt iq mu iu mv iy mw jc mj mk ml mm bi translated">不可能找到最小二乘。</li><li id="15e6" class="me mf hi ih b ii ms im mt iq mu iu mv iy mw jc mj mk ml mm bi translated">因此，MLE是首选。</li></ol><h2 id="612f" class="lq jm hi bd jn lr ls lt jr lu lv lw jv iq lx ly jz iu lz ma kd iy mb mc kh md bi translated">另一个问题是，如果有人对0-1问题进行线性回归，而不是逻辑回归，会发生什么？别担心，每个问题都有答案！</h2><ol class=""><li id="c6fd" class="me mf hi ih b ii kj im kk iq mg iu mh iy mi jc mj mk ml mm bi translated">误差项往往在X(独立变量)的中间值处较大，在极值处较小，这违反了线性回归假设，即误差应具有零均值且应呈正态分布</li><li id="cd4e" class="me mf hi ih b ii ms im mt iq mu iu mv iy mw jc mj mk ml mm bi translated">在X的结束值处生成大于1小于0的无意义预测</li><li id="b921" class="me mf hi ih b ii ms im mt iq mu iu mv iy mw jc mj mk ml mm bi translated">普通的最小二乘(OLS)估计是低效的，标准误差是有偏差的</li><li id="563a" class="me mf hi ih b ii ms im mt iq mu iu mv iy mw jc mj mk ml mm bi translated">X中间值的高误差方差和末端的低方差</li></ol><p id="f183" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有这些问题都可以通过逻辑回归来解决。参考下图。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mx"><img src="../Images/fe1db29270d1da0b9b2a36dbf3931190.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*xAmbHd7rWeZKGs8EaLKPJw.png"/></div></figure><p id="8211" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><h1 id="161d" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">逻辑回归中涉及的术语:</h1><p id="670e" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated"><strong class="ih hj">信息价值(四):</strong></p><p id="be14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在将变量包含在模型中之前，这对于变量的初步过滤非常有用。工业上主要使用IV来在拟合模型之前的第一步中消除主要变量，因为最终模型中存在的变量数量约为10个。因此，需要进行初步处理来减少400+左右的变量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es my"><img src="../Images/5b2a1d57bd36d0e4c78158746482dc9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjnVus8vQFbUq5NeAeyDow.jpeg"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图8</figcaption></figure><p id="c8e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">&lt; 0.02 : useless for prediction<br/> 0.0.2到0.1:弱预测器<br/> 0.1到0.3:中预测器<br/> 0.3到0.5:强预测器<br/> &gt; 0.5:可疑预测器</p><p id="4190" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">赤池信息标准(AIC): </strong></p><p id="38ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这衡量了给定数据集的统计模型的相对质量。这是偏差和方差之间的权衡。在两个模型之间的比较中，具有较小AIC的模型优于具有较高值的模型。<br/> — —如果我们仔细观察下面的等式，k参数(模型中包含的变量的数量)是对模型的过拟合现象的惩罚。这意味着我们可以通过在模型中加入更多不那么重要的变量来人为地证明模型的训练准确性；通过这样做，我们可以在训练数据上获得更好的准确性，但是在测试数据上，准确性会降低。这种现象可能是逻辑回归中的某种正则化:</p><p id="f359" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">AIC = -2*ln(L) + 2*k <br/> L =最大似然值(为数学方便应用对数变换)<br/> k =模型中变量的数量</p><p id="4593" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">受试者工作特性(ROC)曲线:</strong></p><p id="f249" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个图示，说明了二元分类器在其判别阈值变化时的性能。该曲线是通过在各种阈值下绘制真阳性率(TPR)对假阳性率(FPR)来创建的。<br/>理解ROC曲线效用的一个简单方法是，如果我们把阈值(threshold是一个介于0和1之间的实值，用来把预测的产出概率转换成类，就像logistic回归预测概率一样)保持得很低，我们会把大部分预测的观测值放在正的类别下，即使其中一些应该放在负的类别下。另一方面，将门槛保持在非常高的水平会惩罚积极类别，但消极类别会有所改善。理想情况下，阈值的设置方式应该是在两个类别之间权衡价值，并产生更高的总体准确性:</p><p id="2cfc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ko">最佳阈值=可能达到最大值(灵敏度+特异性)的阈值</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mz"><img src="../Images/d17f8f6f0fa85975f04b351dd3174af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*t5nqr29QzhXWMAFF9WJn9Q.png"/></div></figure><p id="62c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你不知道术语TPR和FPR，请点击以下链接查看混淆矩阵是什么<a class="ae na" href="https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62" rel="noopener" target="_blank"/></p><p id="96b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望我已经尽力把这个分类算法说的简单明了了。希望你喜欢的内容和评论，如果你有任何疑问或建议。谢谢大家！</p></div></div>    
</body>
</html>