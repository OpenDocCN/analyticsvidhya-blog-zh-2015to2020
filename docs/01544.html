<html>
<head>
<title>Understanding and implementation of Residual Networks(ResNets)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">剩余网络的理解和实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-and-implementation-of-residual-networks-resnets-b80f9a507b9c?source=collection_archive---------0-----------------------#2019-10-30">https://medium.com/analytics-vidhya/understanding-and-implementation-of-residual-networks-resnets-b80f9a507b9c?source=collection_archive---------0-----------------------#2019-10-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0efa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">剩余学习框架简化了网络的训练，比以前使用的网络要深得多。</p><p id="30ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文主要基于微软研究院发表的研究论文“<strong class="ih hj">图像识别的深度残差学习</strong>”。[ <a class="ae je" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">链接到吴恩达的研究论文</a>和卷积神经网络课程。</p><h1 id="53ee" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak">目录:</strong></h1><ul class=""><li id="6415" class="kd ke hi ih b ii kf im kg iq kh iu ki iy kj jc kk kl km kn bi translated">简介—深度神经网络的问题</li><li id="bbde" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">为什么会发生这种情况，我们如何解决？</li><li id="773e" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">解决方案:剩余学习框架</li><li id="ec11" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">ResNet中使用的块类型</li><li id="6391" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">从剩余块中提取</li><li id="8257" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">研究论文的结果</li><li id="6769" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">ResNet成功了吗？</li><li id="47e5" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">构建您的第一个ResNet模型(50层)</li><li id="bb6e" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">摘要</li></ul><h1 id="82f6" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak">简介—深度神经网络问题:</strong></h1><p id="611e" class="pw-post-body-paragraph if ig hi ih b ii kf ik il im kg io ip iq kt is it iu ku iw ix iy kv ja jb jc hb bi translated">近年来，神经网络变得更加深入，最先进的网络从几层(如AlexNet)发展到超过一百层。</p><ul class=""><li id="867c" class="kd ke hi ih b ii ij im in iq kw iu kx iy ky jc kk kl km kn bi translated">非常深的网络的主要好处之一是它可以表示非常复杂的功能。</li><li id="ebbf" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">然而，训练它们的一个巨大障碍是梯度消失:非常深的网络通常有一个梯度信号，它很快变为零，因此梯度下降非常慢。</li><li id="a038" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">更具体地说，在梯度下降过程中，当我们从最后一层返回到第一层时，我们在每一步上都乘以权重矩阵。如果梯度很小，由于大量的乘法运算，梯度会以指数形式快速下降到零(或者，在极少数情况下，以指数形式快速增长并“爆炸”到非常大的值)。</li></ul><p id="219d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像VGG-16这样的常规网络被称为“普通”网络。</p><p id="1cbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在平面网络中，随着层数从20层增加到56层(如下所示)，即使经过数千次迭代，56层网络的训练误差也比20层网络差。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es kz"><img src="../Images/9c471e1b8d045cb936d2b0c3f456ccbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4fv-3pCDh2ibQK33oDVFdA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">具有20层和56层“普通”网络的CIFAR-10上的训练误差(左)和测试误差(右)。更深的网络具有更高的训练误差，从而具有更高的测试误差。</figcaption></figure><p id="42a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理论上，我们期望有一个更深的网络只会有所帮助，但实际上，更深的网络具有更高的训练误差，从而具有更高的测试误差。</p><h1 id="688e" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak">为什么会出现这种情况，我们如何解决？</strong></h1><p id="b8eb" class="pw-post-body-paragraph if ig hi ih b ii kf ik il im kg io ip iq kt is it iu ku iw ix iy kv ja jb jc hb bi translated">当更深的网络能够开始收敛时，一个<em class="jd">退化</em>问题就暴露出来了:随着网络深度的增加，精度达到饱和(这可能不足为奇)，然后迅速退化。</p><p id="354c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用更深的网络会降低模型的性能。微软研究论文试图用<strong class="ih hj">深度剩余学习框架解决这个问题。</strong></p><p id="01b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解决方案:</strong>残差块/身份块</p><p id="a217" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个想法是让网络去适应剩余映射，而不是让层去学习底层映射。因此，不是说H(x)，初始映射<em class="jd">，</em>让网络拟合，F(x) := H(x)-x，这给出H(x) := F(x) + x</p><p id="45fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">方法是在<em class="jd">中添加一个</em> <strong class="ih hj"> <em class="jd">快捷方式</em> </strong> <em class="jd">或一个</em> <strong class="ih hj"> <em class="jd">跳过连接</em> </strong>，这样可以让信息更容易地从一层流向下一层，也就是说，你可以绕过正常的CNN，从一层流向下一层。</p><p id="7c90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">残余块:</p><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es lp"><img src="../Images/0a1de11debaf30cc94ed269b4f585e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*N5FPYoOt6OhC-xTGO5chFQ.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">剩余学习:一个积木</figcaption></figure><p id="193a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">从剩余块中提取两项:</strong></p><ol class=""><li id="400e" class="kd ke hi ih b ii ij im in iq kw iu kx iy ky jc lq kl km kn bi translated">添加额外的/新的层不会损害模型的性能，因为如果这些层没有用，正则化会跳过它们。</li><li id="3427" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc lq kl km kn bi translated">如果额外的/新的层是有用的，即使存在正则化，层的权重或核也将是非零的，并且模型性能可能略有提高。</li></ol><p id="2217" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，通过添加新的层，由于“跳过连接”/“剩余连接”，保证了模型的性能不会降低，但是可以稍微提高。</p><p id="031d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过将这些ResNet块相互堆叠，可以形成一个非常深的网络。拥有带有快捷方式的ResNet块也使得其中一个块学习标识函数变得非常容易。这意味着您可以在额外的ResNet块上进行堆栈，而几乎没有损害定型集性能的风险。</p><p id="8451" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ResNet中使用两种主要类型的块，主要取决于输入/输出维度是相同还是不同。</p><p id="3dc0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1 <strong class="ih hj">。身份块</strong>——和我们上面看到的一样。身份模块是ResNets中使用的标准模块，对应于输入激活与输出激活尺寸相同的情况<em class="jd">。</em></p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lr"><img src="../Images/8304e6959402f95b04d16be63818e32e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BCbJZXwGDtEdytj9ag_YWw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated"><strong class="bd jh">身份块。</strong>跳过连接“跳过”2层</figcaption></figure><figure class="la lb lc ld fd le"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="b903" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">卷积块</strong> —当输入和输出维度不匹配时，我们可以使用这种类型的块<em class="jd">。与identity块的不同之处在于快捷路径中有一个CONV2D层。</em></p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lu"><img src="../Images/28e9f744fb6806f96175952830e83ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sb_4xKI_bRoX6jmZcNTRWw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">卷积块</figcaption></figure><figure class="la lb lc ld fd le"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h1 id="e188" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak">论文结果:</strong></h1><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lv"><img src="../Images/e36a408d9ce17a013859759011f11311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9sHmRVTn0O2VSpL9qZkWQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">ImageNet的架构。构建块显示在括号中，其中堆叠了块的数量。</figcaption></figure><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lw"><img src="../Images/050bf72cc1643d36eedcc82653fda765.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oeORk3YzCxc3p9xvhB8r-w.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">ImageNet培训。细曲线表示训练误差，粗曲线表示中心作物的验证误差。左图:18层和34层的平面网络。右图:18层和34层的结果。</figcaption></figure><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lx"><img src="../Images/2f2c65275a9fe19683787dc92b3e32a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NijIT5KTI8AHnTJb1Y2sIA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">ImageNet验证的头号错误(%，10次裁剪测试)</figcaption></figure><p id="2d99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">34层ResNet的性能优于18层ResNet和普通计数器部件。因此，无论是普通网络还是ResNet网络，在深度ResNet网络上，退化问题都比浅网络得到了更好的解决。</p><p id="bea2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于更深层次的网络(50及以上),作者引入了瓶颈架构以获得经济收益。</p><h1 id="1047" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">ResNet成功了吗？——是的。</h1><ul class=""><li id="d6a6" class="kd ke hi ih b ii kf im kg iq kh iu ki iy kj jc kk kl km kn bi translated">以3.57%的前5名错误率(集合模型)在ILSVRC 2015分类竞赛中获得第一名</li><li id="7ace" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">在ILSVRC和Coco 2015比赛中分别获得ImageNet检测、ImageNet定位、COCO检测和COCO分割的第一名。</li><li id="7301" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">用ResNet-101替换更快R-CNN中的VGG-16层。他们观察到28%的相对改进</li><li id="6e85" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">高效训练的网络也有100层和1000层。</li></ul><h1 id="6d0b" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">构建您的第一个ResNet模型(50层)</h1><p id="67c0" class="pw-post-body-paragraph if ig hi ih b ii kf ik il im kg io ip iq kt is it iu ku iw ix iy kv ja jb jc hb bi translated">现在你已经有了构建一个非常深的ResNet所必需的模块。下图详细描述了该神经网络的体系结构。图中的“ID BLOCK”代表“身份块”，而“ID BLOCK x3”意味着您应该将3个身份块堆叠在一起。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es ly"><img src="../Images/f7bf23030f2c0cec18b9066b4b37227a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_XlZtWXLShYsprXVtTyjpQ.png"/></div></div></figure><p id="5d06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述ResNet-50型号的详细信息如下:</p><ul class=""><li id="8505" class="kd ke hi ih b ii ij im in iq kw iu kx iy ky jc kk kl km kn bi translated">零填充:用(3，3)填充输入</li><li id="ec65" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">阶段1:2D卷积具有64个形状为(7，7)的滤波器，并使用(2，2)的步长。它的名字叫“conv1”。BatchNorm应用于输入的通道轴。最大池使用(3，3)窗口和(2，2)步距。</li><li id="f94d" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">阶段2:卷积块使用三组大小为64×64×256的滤波器，f=3，s=1，并且块是“a”。这两个单位块使用三组大小为64×64×256的滤波器，f=3，块是“b”和“c”。</li><li id="146a" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">阶段3:卷积块使用三组大小为128×128×512的滤波器，f=3，s=2，块是“a”。3个单位块使用三组大小为128×128×512的滤波器，f=3，块为“b”、“c”和“d”。</li><li id="c481" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">阶段4:卷积块使用三组大小为256×256×1024的滤波器，f=3，s=2，块是“a”。这5个单位块使用三组大小为256×256×1024的滤波器，f=3，块是“b”、“c”、“d”、“e”和“f”。</li><li id="f2a3" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">阶段5:卷积块使用三组大小为512×512×2048的滤波器，f=3，s=2，块是“a”。这两个单位块使用三组大小为256×256×2048的滤波器，f=3，块是“b”和“c”。</li><li id="3c06" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">2D平均池使用形状为(2，2)的窗口，其名称为“avg_pool”。</li><li id="4c55" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">展平没有任何超参数或名称。</li><li id="7cff" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">全连接(密集)层使用softmax激活将其输入减少到类的数量。它的名字应该是' fc' + str(类)。</li></ul><figure class="la lb lc ld fd le"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h1 id="a7aa" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak">概要:</strong></h1><ul class=""><li id="761a" class="kd ke hi ih b ii kf im kg iq kh iu ki iy kj jc kk kl km kn bi translated">非常深的神经网络(普通网络)实现起来不实际，因为它们由于消失梯度而难以训练。</li><li id="ed17" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">跳跃连接有助于解决消失梯度问题。它们还使得ResNet块很容易学习标识函数。</li><li id="6152" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">有两种主要类型的ResNets块:身份块和卷积块。</li><li id="898a" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">通过将这些块堆叠在一起，构建非常深的剩余网络。</li></ul><h1 id="84aa" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">参考资料:</h1><ul class=""><li id="cb65" class="kd ke hi ih b ii kf im kg iq kh iu ki iy kj jc kk kl km kn bi translated">微软研究院的“图像识别深度残差学习”研究论文— [ <a class="ae je" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">链接至论文</a></li><li id="a99b" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated"><a class="ae je" href="https://www.coursera.org/learn/convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/卷积神经网络</a></li><li id="ef17" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">【https://www.appliedaicourse.com T4】</li><li id="2997" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated"><a class="ae je" rel="noopener" href="/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624">https://medium . com/@ 14 Prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-art-image-cf 51669 e 1624</a></li><li id="9c56" class="kd ke hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated"><a class="ae je" href="https://engmrk.com/residual-networks-resnets/" rel="noopener ugc nofollow" target="_blank">https://engmrk.com/residual-networks-resnets/</a></li></ul></div></div>    
</body>
</html>