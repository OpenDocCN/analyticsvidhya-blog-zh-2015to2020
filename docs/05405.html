<html>
<head>
<title>Deep Learning Optimizers — Hard? Not. [2]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习优化器——难？不是。[2]</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-network-optimizers-hard-not-2-7ecc677892cc?source=collection_archive---------41-----------------------#2020-04-19">https://medium.com/analytics-vidhya/neural-network-optimizers-hard-not-2-7ecc677892cc?source=collection_archive---------41-----------------------#2020-04-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/1397b7a86d860f2f9ac535c94cb1f58e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zRB-I-4Z7KZTg8nw"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae iu" href="https://unsplash.com/@franckinjapan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Franck V. </a>拍摄的照片</figcaption></figure><p id="f361" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在<a class="ae iu" rel="noopener" href="/@hmrishavbandyopadhyay/deep-learning-optimizers-hard-not-3f5c4f7b4e96">之前的文章</a>中，我谈到了随机梯度下降和一些最优化的基础知识。虽然SGD非常受欢迎，但它的学习速度是固定的或衰减的，所以它通常会变得很慢。为了提高优化的速度，我们使用动量。“真的，气势？跟物理里一样？”——嗯，与<em class="jt">不同，</em>但它提出了一个类似于我们在物理学中看到的动量的类比。</p><h1 id="cfb1" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">动力</h1><p id="c6b0" class="pw-post-body-paragraph iv iw hi ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">动量法被设计成在出现高曲率时加速学习。动量累积了过去梯度的指数衰减移动平均值，并向评估的方向移动。</p><p id="83c8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">非常有趣的是，动量沿着“速度”v建立。这个速度表示参数在学习空间中移动的“速度”。现在，我们将与物理世界进行类比。要使粒子在空间中移动，力是必要的。这里，负梯度就是力的类比。它通过累积加速学习来推动参数通过参数空间。</p><p id="10b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在梯度下降和SGD中，步长被定义为梯度的范数乘以学习速率。然而，为了动力，我们需要加速学习。动量的步长取决于梯度序列的大小。如果更多的梯度指向相同的方向，则步长更大，因为最终的“速度”取决于梯度的指数衰减平均值。</p><p id="90b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，如果唯一的力是梯度，我们就有问题了。想象一个球被推下一个抛物线结构。如果球上唯一的力是推力，那么球将达到抛物线的最小值，并继续上升。这是这里的一个问题。为了解决这个问题，我们在模型中添加了另一个“力”——它将阻止参数超出成本函数最小值的进一步运动。我们可以假设这个“力”等于粘性阻力，它与-v成正比。因此，当运动不合理时，需要这个力来防止运动。</p><p id="35ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">重要的是要注意，这样的动量不能作为优化算法。它最多可以被认为是一般随机梯度下降算法的扩展。用PyTorch实现的SGD可以在PyTorch文档中查看以供参考。这是一个有趣的实现，结合了动量和普通的SGD。</p><p id="27a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">诸如SGD的算法具有固定的学习率或线性衰减的学习率。这些正慢慢被它们的适应性对手所掩盖。具有自适应学习率的优化算法是必要的，因为学习率极难设置，并且对模型输出和学习有显著影响。一些关注自适应学习率的流行优化算法是AdaGrad、RMSProp和Adam(自适应矩),其中RMSProp是AdaGrad算法的修改，Adam通过RMSProp引入矩。</p><p id="4845" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">AdaGrad算法通过与所有模型参数的历史平方值之和的平方根成反比来调整所有模型参数的学习率。</p><p id="480b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">神经网络迭代地优化，并且从迭代开始的地方进行一些初始化是必要的。初始点在确定模型是否训练和收敛或不收敛方面起着巨大的作用。</p><p id="3584" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我希望这篇文章有助于打破优化算法的概念及其在深度学习领域的必要性。如果有任何问题，请在评论区告诉我:)</p></div></div>    
</body>
</html>