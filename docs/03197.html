<html>
<head>
<title>Solving Differential Equations with Transformers: Deep Learning for Symbolic Mathematics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用变压器解微分方程:符号数学的深度学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/solving-differential-equations-with-transformers-21648d3a1695?source=collection_archive---------2-----------------------#2020-01-20">https://medium.com/analytics-vidhya/solving-differential-equations-with-transformers-21648d3a1695?source=collection_archive---------2-----------------------#2020-01-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/78eadfa824c2bf871f12169b022d136b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iW1K9rDaejqeuwEoBoNL9Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://upload.wikimedia.org/wikipedia/commons/f/f4/Lorenz_attractor.svg" rel="noopener ugc nofollow" target="_blank">https://upload . wikimedia . org/Wikipedia/commons/f/F4/Lorenz _ attractor . SVG</a></figcaption></figure><p id="a4d2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我将介绍一种新的神经网络方法来解决一阶和二阶常微分方程，这种方法在Guillaume Lample和Franç ois Charton(脸书人工智能研究)的ICLR 2020聚焦论文“符号数学的深度学习”中介绍。本文使用seq2seq转换器处理积分和求解一阶和二阶常微分方程的符号计算任务，我们今天将重点讨论后者。</p><p id="7adc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了给本文提供背景，尽管神经网络方法在结构清晰的统计模式识别任务中取得了巨大成功，例如对象检测(计算机视觉)、语音识别、语义分析(自然语言处理)，但符号推理不是它的强项之一。</p><p id="dfca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">符号计算不仅需要人工智能推断复杂的数学规则，还需要对抽象数学符号之间的关系有灵活的上下文理解。在创作的时候，计算机代数系统(CAS)(如Matlab、Mathematica)在符号数学任务上拥有最先进的性能，由复杂算法的后端驱动，如100页长的Risch无限积分算法。</p><p id="88fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，这些半算法远非完美:在特定情况下会失败，有时会无限期超时。在他们的论文中，Lample和Charton开发了一种seq2seq方法，试图超越计算机代数系统。他们的贡献可归纳如下:</p><ol class=""><li id="6a8b" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated"><strong class="ix hj">展示seq2seq变形金刚在3个符号数学任务中的潜力，并延伸到符号推理中</strong></li><li id="659c" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj">在这些任务中实现最先进的性能(使用非平凡数据集&amp;推理时间约束)</strong></li><li id="a6c7" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj">介绍一种新的方法来生成任意大的表达式数据集&amp;相应的解决方案，其中每个表达式都是从指定的问题空间中均匀采样的。</strong></li></ol><p id="ba59" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在执行推理之前，在任务构造和定义以及数据集生成方面进行了大量的预处理。Lample和Charton做了两个有见地的观察——第一，函数整合的具体情况可以通过模式识别显著简化；其次，形式数学可以使用自然语言前缀语法来描述(也在以前的研究中)。作者首先将数学表达式解析为树结构，随后将树表示为序列，然后研究其问题空间的大小，最后提出数据集生成的方法。尽管作者调查问题空间和数据生成的方法非常有趣，但出于本文的目的，我将集中讨论“表达式作为树”、“树作为序列”和“实验”部分。很快我会写一篇全论文的后续文章！</p><p id="cbe0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">像树一样的表情</strong></p><p id="ef8f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">树数据结构本质上是分层的，可以反映数学表达式的重要特征。通过将运算符和函数视为树的内部节点；操作数作为节点；常量和变量作为叶子，作者实现了3件事:编码操作信息的顺序，描述操作符的结合性，通过消除对括号的需要来简化。为了说明这个方法，让我们解析表达式<em class="kh"> a </em> +2 <em class="kh"> ab+b </em>:</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es ki"><img src="../Images/fb6cb021814010ca8fc6ff5029e8af22.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*077J-i-SyOxF7Jv2kT9MUg.png"/></div></figure><p id="397e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">按照文中的约定，该树与右侧相关联。为了保证表达式和树之间的一对一映射(特别是不同的表达式对应于不同的树，即使它们在评估中是相同的)，施加了约束(例如，每个父代至多2个子代，不能使用一元运算符)。</p><p id="f93c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">作为序列的树</strong></p><p id="bfa7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者选择使用seq2seq模型，而不是树到树网络，以避免运行时开销，并避免必须在不同的语法语法之间进行选择(例如，Penn Treebank，Head Driven短语结构语法)。为了将树转换成序列，作者使用前缀符号将表达式建模为自然语言。例如，2(a+b)将变成[∫2幂+ a b 2]。</p><p id="5e77" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">型号</strong></p><p id="e300" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本文方法的一个显著特点是使用了一个非常简单的标准seq2seq转换模型。该模型由2017⁴的Vaswani等人提出，有8个注意头，6层，产生512维的输出。该模型利用了编码器-解码器结构，并添加了堆叠自关注和逐点全连接层。</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/de9b172676d01012ba41dda20407b5c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*kUjYowtCMEI4YedVE0M9jg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">基于“你只需要关注”的模式</figcaption></figure><p id="5728" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如在原始论文中所描述的，注意力mechanisms⁵在两个方面改进了传统的序列模型:它更好地编码了语言数据中的长距离依赖性；避免了必须将句子输入编码成固定长度的特征向量，允许解码器选择性地注意相关的输入单词。这种转换模型通过用自我注意机制完全取代循环模型和卷积，扩展了原始的注意模型。通过多头注意力机制，这款2017型号通过并行性促进了加速。比例点积注意力，定义为:</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/d884c889cb0a23fcf491b5df20da97d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*XtBCjLKUftnnwYX656tLGw.png"/></div></figure><p id="3ce8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中Q是输入查询的矩阵，K和V分别是输入键和值的矩阵。σ对由<em class="kh"> 1 </em> / <em class="kh"> sqrt(d_k) </em>缩放的查询和键的点积应用softmax函数，以获得关注的权重。通过使用点积运算，并且并行使用8个注意力头，变压器模型变得比其递归模型对应物在计算上更高效。这种优势在本文的微分方程求解实验中得到了证明，通过使用Transformers，Lample和Charton能够在30秒内计算出比CAS更高精度的解，优于Mathematica，后者在20%的测试案例中会无限超时。</p><p id="079d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">实验&amp;评估</strong></p><p id="dd12" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用于求解一阶和二阶常微分方程的seq2seq模型在每个具有40M表达式的训练集上进行训练。如论文中所述，作者考虑满足以下条件的表达式:</p><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/a8ba74a5cb59d37b1ee4de3b5b3c4d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O8_vKyhVhK88s4hXjEKmGA.png"/></div></div></figure><p id="e3d5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">问题空间的大小可以通过应用</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/9c5170ccdf11b76f5044d62822532421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*j5C2sgM7FXuc4FT1Y3ztKw.png"/></div></figure><p id="d933" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其他实现细节包括使用学习率为1e-04的Adam优化器，移除超过512个标记的表达式，以及用256个表达式的批量大小进行训练。推理结果由波束搜索产生，对数似然损失最小。SymPy用于比较推断结果是否等同于参考答案。</p><p id="2ac7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">评估&amp;结果</strong></p><p id="8294" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对5000个方程的测试集进行评估，结果如下:</p><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/15cb12566b343e3ff9bdff0d79774582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pdEww_zkGNW_HcfhbVGzzQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">光束尺寸会显著影响测试精度</figcaption></figure><p id="c524" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者着重进行了两个比较:与Mathematica(计算时间限于30s)、Matlab和Maple的结果比较，使用不同光束尺寸的结果比较。由于Mathematica的超时和延迟(作者提到这使得在更大的集合上测试不可行)，前者的结果是在500个方程的测试集上计算的，如下所示:</p><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/e6236021cda86f39f27754926b9b674e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TyFr9ifczYM8p61pTM4S6w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者指出，转换器通常需要1秒钟来计算结果</figcaption></figure><p id="779a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作者还得出结论，更宽的光束可以显著提高精确度。值得注意的是，变压器的结果是针对<em class="kh">回忆@k=1 </em>、<em class="kh">回忆@k=10 </em>和<em class="kh">回忆@k=50 </em>(对应于3个射束尺寸)报告的，这意味着如果射束搜索找到的前<em class="kh"> k </em>个结果中至少有一个与参考答案相等，则推断结果被判定为正确。</p><p id="7b28" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">结论</strong></p><p id="40bd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本文将seq2seq变压器模型作为传统方法(计算机代数系统和树到树模型)的替代方案和竞争对手。Lample和Charton将数学建模为自然语言，引入了将一元二叉树解析为序列的方法，引入了研究它们的问题空间的综合方法，以及数据生成的方法。尽管符号数学计算长期以来一直由CAS主导，但Lample和Charton证明了神经结构在函数积分和求解带约束的一阶和二阶微分方程任务中的优越性，证明了序列模型在符号推理任务中的潜力。</p><p id="febf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">实施注意事项</strong></p><p id="4e3e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">截至2020年1月21日，本论文还没有提供代码和数据集。为了用一般的神经网络实现微分方程解算器，除了使用Python库，我们还可以考虑朱莉娅编程Language⁹中的Rewrite.jl⁶、ModelingToolki.jl⁷和Transformers.jl⁸软件包——所有这些软件包都在积极维护和改进中。凭借“看起来像Python，感觉像Lisp，运行起来像C/Fortran”的口号，Julia是科学和数值计算的绝佳选择。</p><p id="cf21" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">参考文献</strong></p><p id="ce0d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[1] <a class="ae iu" href="https://arxiv.org/pdf/1912.01412.pdf" rel="noopener ugc nofollow" target="_blank"> G .兰普尔和f .查顿。符号数学的深度学习，ICLR 2020。</a><br/>【2】<a class="ae iu" href="https://arxiv.org/pdf/1912.05752v2.pdf" rel="noopener ugc nofollow" target="_blank">e . Davis .深度学习在符号整合中的应用综述(Lample and Charton，2019) ArXiv e-prints，abs/1912.05752v2，2019 .</a><br/>【3】<a class="ae iu" href="http://aitp-conference.org/2019/aitp19-proceedings.pdf" rel="noopener ugc nofollow" target="_blank">b .皮奥特罗斯基，c .布朗，j .厄本，c .卡利西克。神经网络可以学习符号重写吗？，AITP 2019。</a><br/>【4】<a class="ae iu" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">a .瓦斯瓦尼、n .沙泽尔、n .帕尔马、j .乌兹科雷特、l .琼斯、A. N .戈麦斯、l .凯泽、I .波洛苏欣。你需要的只是关注。ArXiv电子印花，abs/1706.03762。, 2017.</a><br/>【5】<a class="ae iu" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">d .巴丹瑙，k .秋，y .本吉奥。通过联合学习对齐和翻译的神经机器翻译。ArXiv电子印花，abs/1409.0473。,2014.</a><br/>【6】<a class="ae iu" href="https://github.com/HarrisonGrodin/Rewrite.jl" rel="noopener ugc nofollow" target="_blank">rewrite . JL</a><br/>【7】<a class="ae iu" href="https://github.com/JuliaDiffEq/ModelingToolkit.jl" rel="noopener ugc nofollow" target="_blank">modelingtookit . JL</a><br/>【8】<a class="ae iu" href="https://github.com/chengchingwen/Transformers.jl" rel="noopener ugc nofollow" target="_blank">transformers . JL</a><br/>【9】<a class="ae iu" href="https://julialang.org/" rel="noopener ugc nofollow" target="_blank">Julia编程语言</a></p></div></div>    
</body>
</html>