<html>
<head>
<title>How I Built An Algorithm to Takedown Atari games!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何建立一个算法来打倒雅达利游戏！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-i-built-an-algorithm-to-takedown-atari-games-a13d3b3def69?source=collection_archive---------11-----------------------#2020-01-17">https://medium.com/analytics-vidhya/how-i-built-an-algorithm-to-takedown-atari-games-a13d3b3def69?source=collection_archive---------11-----------------------#2020-01-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d5b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">电子游戏。我花了很多时间在这上面。我几乎每天都玩。但是有时候超越下一个层次是不可能的。作为一个对人工智能感兴趣的孩子和一个游戏玩家，我决定创建一个机器学习模型来击败所有我不会的游戏。👍</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/6a5886d087e32ddb2101d1293e5bd5b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Yk81KUNmbBREeDZNJGerA.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">雅达利突破</figcaption></figure><blockquote class="jt ju jv"><p id="5851" class="if ig jw ih b ii ij ik il im in io ip jx ir is it jy iv iw ix jz iz ja jb jc hb bi translated">在我们开始之前，试试这个很酷的技巧:在谷歌中输入“雅达利突破”,然后进入图片！</p></blockquote><p id="27c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了创建一个在专家水平上玩Atari游戏的算法，我从Deep Q learning获得了一些帮助。但是在我开始之前，我们需要了解一些基础知识来理解发生了什么。如果你是机器学习的新手，先看看这篇<a class="ae ka" rel="noopener" href="/@sumeetpathania2003/introduction-to-the-world-of-ai-b07c4c2d3307"> <strong class="ih hj">文章</strong> </a>！</p><h2 id="3ee7" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">快速浏览强化学习</h2><p id="ffaf" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">强化学习是一种人工智能，它使用奖励和惩罚系统，目标是最大化获得的奖励。这种类型的学习有四个主要组成部分:代理人、环境、行动和回报。</p><p id="6be6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们先试着更好地理解这一点。</p><p id="bc35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设你是一个10岁的孩子，需要为即将到来的数学考试学习。作为一个10岁的孩子，你不想学习，但如果你想，你妈妈会给你糖果，即使你完成了作业。</p><p id="84bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个例子中，你，孩子是<strong class="ih hj">代理</strong>，即将到来的数学考试代表<strong class="ih hj">环境</strong>，每个问题/工作表代表<strong class="ih hj">状态，</strong>决定工作与否是<strong class="ih hj">动作，</strong>和<strong class="ih hj">奖励</strong>是每次你完成一个工作表就得到糖果。</p><h2 id="1ba2" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">q学习</h2><p id="7b3c" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">q学习是代理决定采取什么行动的一种方式。起初，代理不知道该做什么。由于这不是监督学习，代理没有过去的经验，它不知道采取什么行动来最大化奖励。(回想一下孩子的例子，如果他不知道自己拿巧克力是为了什么，他就拿不到巧克力)。这就是Q表的用武之地！</p><h2 id="c4cc" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">q表</h2><p id="047b" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">因为在q-learning中没有<strong class="ih hj">策略</strong>，所以Q-tables的目标是在每个状态下找到最佳策略或行动过程。Q表中的Q代表“质量”(动作的质量)。本质上，Q表的目标是规划出采取什么行动来获得最高的回报。</p><p id="3095" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">想象一只老鼠被困在迷宫里。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lb"><img src="../Images/1c82e1ae3d864c11154c0c1051c934be.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/0*Pqrht0rfLFyZFa6j.jpg"/></div></figure><p id="7b91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它可以采取4种行动:1步左，右，上，下。这就是Q表派上用场的地方。Q表告诉我们在每种状态下采取哪一步来最有效地获得奶酪。新游戏开始时，牌桌看起来会像这样:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lc"><img src="../Images/1fee0078b9142475134937670a50ad61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j7n38dhuGGnW4z2BbY7ltA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">列=操作，行=状态</figcaption></figure><h2 id="c086" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">计算每个动作的值</h2><p id="af22" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">Q表最终需要告诉我们采取什么行动来获得尽可能高的回报。这是通过使用<strong class="ih hj">贝尔曼</strong>方程的Q学习算法来完成的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ld"><img src="../Images/29319138f616cd7fa006ec5e39a51581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/0*k0OwtJu3f5fPrsZq"/></div></figure><p id="4611" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是…如果每个Q值都等于零，我们在开始时可以采取什么行动呢？如果有一条老鼠还没有探索过的更好的路线呢？</p><p id="5fcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是<strong class="ih hj">勘探/开发</strong>权衡派上用场的地方。</p><p id="bf65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">顾名思义，这有助于Q-tables决定是探索和尝试新路线，还是坚持一条路线并利用其好处/回报。</p><p id="ff2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在两者之间取得平衡至关重要。首先，需要充分探索变体空间，以便识别最强的变体。通过首先识别，然后继续利用最佳行动，你可以从环境中获得最大的总回报。然而，你还想继续探索其他可行的变体，以防它们在未来提供更好的回报。</p><p id="b870" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我们使用<strong class="ih hj">ε贪婪策略</strong>的地方，通常写为<strong class="ih hj"> ε </strong>，ε的希腊语符号。</p><p id="607b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">epsilon贪婪策略中的“贪婪”代表你可能认为它做的事情。让我们回到鼠标的例子。在初始化一组试验后，不同路线的20次左右的试验，老鼠可以开始逐渐变得贪婪，并开始只使用它在大多数尝试中找到的20条路线中的最佳路线。</p><p id="1b3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说:</p><p id="bd4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们设置<em class="jw"> e </em> =0.05，该算法将在95%的时间里利用最佳变量，在5%的时间里探索随机备选方案。这在实践中非常有效。</p><p id="4d8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，这就是你理解深度Q网络所需要知道的全部！</p><h2 id="fb02" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">深入探究深度Q学习</h2><p id="3a28" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">这就是事情变得有趣的地方。😎</p><p id="1831" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我如何创建一个算法来玩专家级别的突破！</p><p id="1a1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Q学习</strong> + <strong class="ih hj">强化学习+神经网络</strong> = <strong class="ih hj">深度Q网络</strong></p><p id="d765" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">快速注意:</strong>对于Atari游戏，由于它们比简单的鼠标迷宫大得多，所以很难对每个动作和像素使用Q表。它效率低下，不可扩展。对于更复杂的游戏，我们使用神经网络来使学习变得更容易和更有效，并在游戏中变得更好。有了这些神经网络，我们大大减少了可能性的数量，从几十亿减少到只有几百万。给定一个状态，这是通过神经网络逼近每个动作的Q值来完成的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/66b44bd5a2d66347851d5170dffa9953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DOj1p-Bue6rK1WlT.png"/></div></div></figure><h2 id="9bbf" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">预处理</h2><p id="6d69" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">在这一步中，我们简单地降低帧和堆栈帧的复杂度。首先，让我们降低复杂性。</p><p id="3d60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看下图。你认为我们能移除什么？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/6a5886d087e32ddb2101d1293e5bd5b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Yk81KUNmbBREeDZNJGerA.jpeg"/></div></div></figure><p id="a3b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以做的几件事是将图像灰度化，缩小尺寸，裁剪框架以移除不必要的东西(比如顶部的数字)。完成后，我们可以堆叠4帧。为什么我们要叠4帧？想象一下，两辆车在路上面对面。从单个图像中，你无法判断汽车是停着还是将要撞上，但是如果你从视频中获得几帧图像，你可以很容易地判断汽车是否在移动。同样，我们的DQN也更容易做出假设。</p><p id="83ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，卷积网络处理接收到的4个帧。这些卷积层允许您利用这些帧的一些空间属性。</p><h2 id="c03b" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">体验接力</h2><p id="961a" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">我们网络的下一部分是体验接力。它帮助我们做两件重要的事情:</p><ul class=""><li id="551c" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated"><strong class="ih hj">减少相关数据</strong></li><li id="dd00" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><strong class="ih hj">更好地利用以前的经验</strong></li></ul><p id="9361" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们按顺序训练网络，我们的代理人可能会受到相关性的影响。因为帧与帧之间不会发生太多影响代理行为的网络权重和偏差变化。</p><p id="10ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过从重放缓冲区随机取样，我们可以打破这种相关性。这可以防止动作值发生剧烈的波动或偏离。</p><p id="4951" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本质上，样本转换被存储，然后从“转换池”中随机选择以更新知识。</p><h2 id="a605" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">用损失函数改善网络</h2><p id="7d85" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">还记得我在上面说过用贝尔曼方程更新我们的Q表吗？这一次，我们想更新我们的神经网络权重，以减少误差。</p><p id="cf32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过取我们的Q_target(来自下一状态的最大可能值)和Q_value(我们对Q值的当前预测)之间的差来计算误差(或TD误差)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/809ac508c8169ca9bc3620d338f78168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JTf2RX4gtpVX3cOK.png"/></div></div></figure><p id="483a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">仅此而已。我们有自己的DQN，可以玩从毁灭战士到乒乓的雅达利游戏！😎</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/b1c2d21cb887686fcdf623a14030052f.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*l-9VVHewBpnQUv9f.gif"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/50869beed6d06de4fca1c622a918b827.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/0*h_83RKsh02Rmv3zy.gif"/></div></figure><h2 id="b36c" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">dqn的局限性</h2><p id="d36d" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">深度强化学习被堆积如山的炒作所包围。而且理由很充分！强化学习是一个令人难以置信的通用范式，原则上，一个健壮的、高性能的RL系统应该在所有方面都很棒。将这种范式与深度学习的力量相结合是一种完美的融合。Deep RL是看起来最像AGI的东西之一，这是一种推动数十亿美元资金的梦想。</p><p id="8e61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不幸的是，它还不太好用。</p><p id="936a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，100%有可能克服这些限制。如果我不相信强化学习，我就不会从事这项工作。但是在前进的道路上有很多问题。以下是其中的几个例子:</p><p id="e3ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">强化学习通常需要一个奖励函数</strong></p><ul class=""><li id="8623" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated">RL有一种令人讨厌的倾向，会过度满足你的奖励，导致你意想不到的事情。这就是为什么雅达利是RL的一个很好的基准。然而，将RL用于其他任务可能会被证明是低效的。</li></ul><p id="815b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">深度强化学习可能样本低效</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lv"><img src="../Images/d4c7c021aee97c8af104db1a924a1906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yXaMt1YeYTshj1Mo.png"/></div></div></figure><ul class=""><li id="13a5" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated">DeepMind的一篇论文对原始DQN架构的几项增量改进进行了研究，证明了所有改进的组合可以提供最佳性能。在雅达利尝试的57款游戏中，超过40款游戏的性能超过了人类水平。</li><li id="cc65" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">通过查看图表，您可以知道训练atari游戏需要多长时间。RainbowDQN在大约<em class="jw">1800万</em>帧时超过100%阈值。这相当于大约83小时的游戏体验。</li></ul></div><div class="ab cl lw lx gp ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hb hc hd he hf"><h2 id="3690" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">关键要点</h2><ul class=""><li id="a9ee" class="lf lg hi ih b ii kw im kx iq md iu me iy mf jc lk ll lm ln bi translated">dqn是一个强大的工具，可以比人类更快地学习和掌握策略</li><li id="796e" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">深度q网络是本质强化学习+神经网络</li><li id="45f0" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">预处理是限制暴露在网络中的不必要信息的重要步骤。</li><li id="34c3" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">经验接力更有效地利用观察到的经验</li><li id="a7ed" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">RL非常有用，但是在完善DQNs/ RL之前，我们仍然有一些障碍</li></ul></div><div class="ab cl lw lx gp ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="hb hc hd he hf"><p id="1ae8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如果你喜欢读这篇文章，请关注我，关注我未来的文章。此外，请随意与他人分享这篇文章！</strong></p><p id="e662" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Medium和<a class="ae ka" href="https://www.linkedin.com/in/sumeet-pathania-93b052194/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上关注我，了解我在人工智能方面的最新进展。</p><p id="c5db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想知道我目前在做什么，以及我在人工智能和类似项目上的经历，请免费订阅我的时事通讯！<a class="ae ka" href="http://eepurl.com/gFbCFX" rel="noopener ugc nofollow" target="_blank">http://eepurl.com/gFbCFX</a></p></div></div>    
</body>
</html>