<html>
<head>
<title>Syncnet Model with the VidTIMIT Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有VidTIMIT数据集的Syncnet模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/syncnet-model-with-vidtimit-dataset-dd9de2cb2fb5?source=collection_archive---------5-----------------------#2020-10-18">https://medium.com/analytics-vidhya/syncnet-model-with-vidtimit-dataset-dd9de2cb2fb5?source=collection_archive---------5-----------------------#2020-10-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7e9d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">预测视频是真是假</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/6ab309fa45b9c365b8d6fb0e89ccd6af.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/0*HYxFO278--jjKDze"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">找到项目的<a class="ae jj" href="https://github.com/Neha13022020/Syncnet_model_VIDTIMIT_dataset" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a>。LinkedIn <a class="ae jj" href="https://www.linkedin.com/in/neha-sikerwar-785514113/" rel="noopener ugc nofollow" target="_blank">简介</a>。</figcaption></figure><h1 id="21a5" class="jk jl hi bd jm jn jo jp jq jr js jt ju io jv ip jw ir jx is jy iu jz iv ka kb bi translated">介绍</h1><p id="5131" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">视频处理技术有了很大的进步。而且制作篡改视频要容易很多，可以糊弄人眼。这样的内容导致假新闻或误传。会影响到人民和国家。</p><p id="8ced" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">所以在这个项目中，我试图检测视频是否被篡改，或者你可以说，是真的还是假的？我在整个项目中参考了<a class="ae jj" href="https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/chung16a.pdf" rel="noopener ugc nofollow" target="_blank">这篇</a>研究论文。他们专注于确定视频中嘴部动作和语音之间的音频-视频同步。他们将音频-视频同步用于电视广播。但是这里我使用了VidTIMIT数据集。这真的是一篇很好的研究论文，他们开发了一个独立于语言和独立于说话人的解决方案来解决唇形同步问题，没有标记数据。</p><p id="5434" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">有更多关于同样问题的最新研究论文。像，<a class="ae jj" href="https://arxiv.org/pdf/1706.05739.pdf" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae jj" href="https://www.eurasip.org/Proceedings/Eusipco/Eusipco2018/papers/1570439304.pdf" rel="noopener ugc nofollow" target="_blank">这个</a>。而且他们的成绩也不错。但我选择应用syncnet模型，因为它非常容易，模型结构简单，并且我们已经为该模型提供了预训练的权重。因此，让我们更多地了解syncnet模型及其架构。</p><h1 id="6af9" class="jk jl hi bd jm jn jo jp jq jr js jt ju io jv ip jw ir jx is jy iu jz iv ka kb bi translated">关于Syncnet及其架构</h1><p id="7e45" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">网络接收音频和视频输入的剪辑。双流ConvNet架构，支持从无标记数据中学习声音和嘴部图像之间的联合嵌入。</p><h2 id="fa19" class="ld jl hi bd jm le lf lg jq lh li lj ju kl lk ll jw kp lm ln jy kt lo lp ka lq bi translated">音频数据</h2><p id="2b45" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">输入音频数据是MFCC值。你可以在这里阅读更多关于MFCC的信息。梅尔频率倒谱系数是自动语音和说话人识别中广泛使用的特征。它识别音频信号中有利于识别语言内容并丢弃所有其他内容的成分。每个时间步长使用13个Mel频带。层架构基于VGGM，但修改了滤波器大小，以接收非常规维度的输入。</p><h2 id="b212" class="ld jl hi bd jm le lf lg jq lh li lj ju kl lk ll jw kp lm ln jy kt lo lp ka lq bi translated">视频数据</h2><p id="74b0" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">视觉网络的输入是一系列嘴部区域，作为5帧的111×111×5(宽×高×高)尺寸的灰度图像。下面是研究论文中架构的截图。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lr"><img src="../Images/e8e32a3f1e0cbb29bcaed35812479288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/0*dy0qeF49kYKyEvrr"/></div></figure><p id="9b7e" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">关键思想是音频和视频网络的输出对于未被篡改或真实的视频是相似的，而对于被篡改或伪造的视频是不同的。因此，我们可以计算网络输出之间的欧几里德距离。距离越大，相似度越小，意味着视频是假的。</p><h1 id="9ff6" class="jk jl hi bd jm jn jo jp jq jr js jt ju io jv ip jw ir jx is jy iu jz iv ka kb bi translated">关于VidTIMIT数据集</h1><p id="3621" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated"><a class="ae jj" href="http://conradsanderson.id.au/vidtimit/" rel="noopener ugc nofollow" target="_blank"> VidTIMIT </a>数据集由43个人的视频和相应的录音组成，背诵短句。每人10句话。每个人的视频被存储为JPEG图像的编号序列。相应的音频存储为WAV文件。要解压缩文件夹:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="4557" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">因为我们有43个用户，每个用户有10个音频，所以我从他们各自的图像和音频中创建了430个未被篡改的视频。然后，为了创建篡改的视频，我用来自同一个VidTIMIT数据集的3个错误音频替换了每个正确的音频。基本上，我创建了3个对应每个真实视频的假视频。因此，在我们最终的数据集中，篡改和非篡改视频的比例为1:3。我用cv2.VideoWriter创建了视频。我正在展示创建非篡改视频的代码。同样，你也可以创建被篡改的视频。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h1 id="1968" class="jk jl hi bd jm jn jo jp jq jr js jt ju io jv ip jw ir jx is jy iu jz iv ka kb bi translated">音频和视频处理</h1><p id="79c7" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">在本节中，我们将为音频和视频文件创建功能。我从<a class="ae jj" href="https://github.com/voletiv/syncnet-in-keras/blob/master/syncnet_params.py" rel="noopener ugc nofollow" target="_blank">这里</a>得到的所有加工参数和从<a class="ae jj" href="https://github.com/voletiv/syncnet-in-keras/blob/master/syncnet_functions.py" rel="noopener ugc nofollow" target="_blank">这里</a>得到的功能。他还实现了相同的syncnet模型。而且所有的功能都很清晰。我们一个一个来看。视频文件是. mp4格式，音频文件是。wav格式。</p><h2 id="b71b" class="ld jl hi bd jm le lf lg jq lh li lj ju kl lk ll jw kp lm ln jy kt lo lp ka lq bi translated">视频处理:</h2><p id="b711" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">在视频处理中，我们首先检测帧和嘴。将嘴部图像转换成灰度并调整大小。然后取嘴部的矩形坐标，并以此准备视频特征。然后将5帧的特征堆叠在一起。所以我用了一个以视频为输入的函数。如果有人不想处理视频，而是有帧/图像要处理，他们可以使用其他功能进行特征化。所有功能都存在于我的GItHub repo中。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h2 id="25da" class="ld jl hi bd jm le lf lg jq lh li lj ju kl lk ll jw kp lm ln jy kt lo lp ka lq bi translated">音频处理:</h2><p id="1760" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">对于音频处理，首先，我们使用scipy (scipy.io.wavfile)读取wav音频文件。然后，speechpy.feature.mfcc为每0.2秒的剪辑创建mfcc特征。我们考虑了MFCC的12个特征。然后将特征整形为(N//20，12，20，1)，其中N为len(mfcc_features)。请找到下面的代码。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h1 id="81b9" class="jk jl hi bd jm jn jo jp jq jr js jt ju io jv ip jw ir jx is jy iu jz iv ka kb bi translated">建模</h1><p id="03ec" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">在建模部分，我们将创建两个流的结构，如架构部分所示。一个流处理嘴部帧，另一个流处理音频特征。在Keras中实现，2个顺序模型。然后我们有函数来加载我从Vikram Voleti和他的GitHub <a class="ae jj" href="https://github.com/voletiv/syncnet-in-keras" rel="noopener ugc nofollow" target="_blank">库</a>中获得的预训练权重。我只从他的回购中获得了建模的所有功能。任何人都可以参考他的演示文件，清楚地了解完整的管道。我们有不同的模式，像“唇”，“音频”或“都有”。如果我们选择“lip”模式，那么只有lip序列模型将被加载。如果选择了“音频”模式，则只会加载音频序列模式。如果选择了“both”模式，那么lip和音频模型都将被加载到一个列表中，如下面的代码所示。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h2 id="8bd0" class="ld jl hi bd jm le lf lg jq lh li lj ju kl lk ll jw kp lm ln jy kt lo lp ka lq bi translated">Syncnet lip模型:</h2><p id="6e23" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">在嘴唇模型层中，我们将形状输入为(嘴部框架的高度、嘴部框架的宽度、视频通道的数量)。这里我们有7块层。第一、第二和第五块由卷积和最大池层组成。第三和第四区块为conv层。第6和第7块是致密层。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h2 id="6ff6" class="ld jl hi bd jm le lf lg jq lh li lj ju kl lk ll jw kp lm ln jy kt lo lp ka lq bi translated">Syncnet音频模型:</h2><p id="84f1" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">在音频模型层中，我们的输入形状为(音频MFCC通道= 12，音频时间步长= 20，1)。这里我们也有7块层。第2块和第5块由卷积层和最大池层组成。第一、第三和第四区块为conv层。第6和第7块是致密层。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h1 id="7cfa" class="jk jl hi bd jm jn jo jp jq jr js jt ju io jv ip jw ir jx is jy iu jz iv ka kb bi translated">评估和结果</h1><p id="f7ea" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">根据这篇研究论文，“为了找到音频和视频之间的时间偏移，我们采用了滑动窗口方法。对于每个样本，计算一个5帧视频特征和1秒范围内的所有音频特征之间的距离。”所以我们在这里也实现了同样的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lu"><img src="../Images/f04f4898865ae088e8af612af7060858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qEslnvFl3HIRmP6y"/></div></div></figure><p id="40aa" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">同样在论文中，他们实现了置信度得分来寻找主动说话者。但在我们的例子中，所有视频中只有一个扬声器。他们还举了配音视频的例子，其中嘴唇的动作与音频不匹配。所以我用置信度找到了音频和视频之间的关联。高置信度意味着音频和视频之间的相关性更高。所以那段视频更有可能是真的。而如果置信度得分较低，则意味着音频和视频之间的相关性较低，因此视频被伪造或篡改的可能性较大。</p><p id="12ef" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">这里我考虑标签1为非篡改或真实的视频。对于篡改或伪造的视频，标记为0。在建模部分之后，我们可以从模型中预测音频和视频阵列。然后用那些数组，我要计算它们之间的欧氏距离(函数参考GitHub repo)。从距离函数中，我们将得到一个31值的距离数组。然后，利用这些值，我们可以计算每个音频-视频对的置信度得分。置信度可以通过距离数组的中值和最小值之间的差来计算。最后根据置信度得分，我们可以预测视频的真假。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lz"><img src="../Images/2f3d60fa0ee7592da0908e39c5268b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBCQKJ-H0NxGaWMCAtttAQ.png"/></div></div></figure><p id="0b5c" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">我找到了所有视频的置信度得分，并绘制成图，如图所示。橙色代表假视频的置信度值，蓝色代表真视频的置信度值。正如我们所看到的，较低的置信度得分值用于篡改/伪造的视频，而较高的置信度得分值用于真实/未篡改的视频。所以我们可以在置信度值的基础上明确地将它们分为真的或假的。</p><p id="34c2" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">因此，我们必须从置信度得分中选择一个阈值来分类视频是真的还是假的。从图中，我们可以看到3.5是一个很好的分类阈值。或者，我们可以从2到4个置信度值运行循环，并选择一个度量(precision_score，因为我们想要更低的假阳性)。然后，我们可以检查在什么置信度分数作为阈值时，我们会获得最高的精度。在我的例子中，以3.5为门槛，我得到了相当好的结果。我的roc-auc值为0.8461231275184763。在精确度和召回率之间总是有一个权衡。可以根据自己的要求选择。</p><h1 id="de0a" class="jk jl hi bd jm jn jo jp jq jr js jt ju io jv ip jw ir jx is jy iu jz iv ka kb bi translated">来自实时视频的片段</h1><p id="eadc" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">如果您使用的是视频直播，您可以创建4秒的视频块。然后可以像我们之前所做的那样处理这些块，然后检查视频/块是否被篡改。</p><p id="3f24" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">为了创建4秒的块，FFmpeg <a class="ae jj" rel="noopener" href="/@taylorjdawson/splitting-a-video-with-ffmpeg-the-great-mystical-magical-video-tool-️-1b31385221bd">命令</a>如下。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h1 id="12a4" class="jk jl hi bd jm jn jo jp jq jr js jt ju io jv ip jw ir jx is jy iu jz iv ka kb bi translated">部署</h1><p id="28a1" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">现在我们可以在云中部署模型，因为我们已经保存了权重和模型。我参考了这些(<a class="ae jj" rel="noopener" href="/analytics-vidhya/how-to-deploy-your-machine-learning-model-in-the-cloud-87d3451a466d">这个</a>和<a class="ae jj" href="https://pythonise.com/series/learning-flask/flask-uploading-files#related" rel="noopener ugc nofollow" target="_blank">这个</a>)关于部署的好文章。为了进行部署，我们需要在像AWS或heroku这样的云平台中设置环境。但是首先我们需要使用Flask创建一个简单的web API。因此，我创建了一个app.py文件，我们必须将它放在与所有其他文件相同的文件夹中。也是一个HTML表单，可以接受2个输入文件，一个音频文件。wav格式)和一个视频(. mp4格式)和提交按钮。当提交表单时，它将向'/predict' route发出post请求，我们应该能够获得表单页面的数据。这个HTML文件我们必须放在模板文件夹中。所以文件夹的结构应该是这样的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es ma"><img src="../Images/7ec8f07f9ffbf0e30b02a497367090ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iJwxJjfcHn5palZU"/></div></div></figure><p id="2a71" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">所有这些之后，我们将检查它是否在我们的本地系统中工作。唯一的挑战是为dlib设置虚拟环境。我用了anaconda提示。按照此处<a class="ae jj" href="https://www.learnopencv.com/install-opencv-3-and-dlib-on-windows-python-only/" rel="noopener ugc nofollow" target="_blank">或此处</a><a class="ae jj" href="https://www.codesofinterest.com/2019/12/build-and-install-dlib-anaconda-windows.html" rel="noopener ugc nofollow" target="_blank">的步骤</a>设置dlib。然后，我们只需在anaconda提示符下运行3个命令:</p><ol class=""><li id="add9" class="mb mc hi ke b kf ky ki kz kl md kp me kt mf kx mg mh mi mj bi translated">cd C:\Users\Neha\Desktop\deploy</li></ol><p id="dbed" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">(将目录更改为您的路径)</p><p id="88c8" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">2.conda激活环境_dlib</p><p id="51bb" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">(“env_dlib”是我的虚拟环境)</p><p id="2ea5" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">3.python app.py</p><p id="647a" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">然后我们用“<a class="ae jj" href="http://localhost:8080/index" rel="noopener ugc nofollow" target="_blank">http://localhost:8080/index</a>”在浏览器中打开localhost(根据你的改变端口号)。然后，上传文件并获得您的视频预测。太好了！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mk"><img src="../Images/bfbe18473d06254ea527b2b1278abe24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rqkfbdHolhJLmY_4"/></div></div></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml lt l"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">点击这里查看演示</figcaption></figure><p id="a7af" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">对于云中的部署，我们需要一个深度学习AMI实例(所以我们不需要安装所有的深度学习库)，它不符合AWS中免费层的条件。所以我没有做。如果您打算尝试在普通实例中部署，那么dlib安装将是一个真正的挑战。因此，如果您有适当的资源，那么您可以继续在云中部署该模型。祝你好运！</p><h1 id="246b" class="jk jl hi bd jm jn jo jp jq jr js jt ju io jv ip jw ir jx is jy iu jz iv ka kb bi translated">未来的改进</h1><p id="f9a8" class="pw-post-body-paragraph kc kd hi ke b kf kg ij kh ki kj im kk kl km kn ko kp kq kr ks kt ku kv kw kx hb bi translated">在本节中，我尝试用syncnet模型进行迁移学习。为了创建数据集，我尝试将tf.data.Dataset直接用于音频和视频功能，但由于深度并不是对所有视频都是固定的，所以它会给出ValueError。所以我用了tf-records。经过处理或特征化的音频和视频，我把它们写入tfrecords文件。代码如下所示。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="8cc8" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">然后，我们可以轻松地创建数据集。要读取tf-records文件以及如何从中创建数据集，请参考我的GitHub <a class="ae jj" href="https://github.com/Neha13022020/Syncnet_model_VIDTIMIT_dataset" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="a93c" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">这里我就不计算置信度得分了。相反，我将使用对比损失函数，它将y_true和y_pred作为输入，并计算它们之间的损失。所以y_pred将是音频和视频之间的距离数组，y_true将是0(代表假)或1(代表真)的数组。以下是研究论文中的损耗和距离公式。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mm"><img src="../Images/80678148dc5b627b1d9df09a9c703c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6AaWp1WnmE_A6GMM"/></div></div></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="2cb6" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">现在我们有了数据集和函数。我们需要训练模型。因此，在培训部分，我将介绍两种方法。第一种方法是，我们可以用以前模型的所有层创建一个新模型，并添加一个定制层来计算距离，并将使用对比损失作为模型的损失函数。第二种方法是冻结前一个模型的顶层，仅用您的数据训练最后的密集层，然后调用距离函数和损失函数。我们可以使用TensorFlow的GradientTape()函数进行训练。第一种方法的模型如下:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="8c7d" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">所以现在结构看起来像这样:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mn"><img src="../Images/80cf9b132c1af4ba1039ac82f6d2d3bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*a9mGiS-w7C4XSLqqyZklfQ.png"/></div></div></figure><p id="fc89" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">对于第二种方法，我想从两个顺序模型中只训练完全连接的第6层和第7层。如果有人想训练更多的层，可以用下面的代码来完成，只要让这些层可训练。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="3ad6" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">然后，我列出了两个模型的可训练变量，并展示了如何使用GradientTape()函数进行训练。比如:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="734d" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">这里我使用了定制的距离函数和损失函数。因此，由于梯度问题，我不能训练模型。因为不是所有的函数都是可微的。但是如果有人使用适当的TensorFlow实现，它完全是可微分的和可训练的。所以我把它留给以后的工作。这是任何人在未来参考这种笔记本可以做的改进。</p><p id="e16a" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated"><strong class="ke hj">注意:</strong>这个案例研究的所有代码都可以在项目的<a class="ae jj" href="https://github.com/Neha13022020/Syncnet_model_VIDTIMIT_dataset" rel="noopener ugc nofollow" target="_blank"> Github repo </a>中获得。</p><h1 id="9d06" class="jk jl hi bd jm jn jo jp jq jr js jt ju io jv ip jw ir jx is jy iu jz iv ka kb bi translated">参考</h1><ul class=""><li id="1c29" class="mb mc hi ke b kf kg ki kj kl mo kp mp kt mq kx mr mh mi mj bi translated"><a class="ae jj" href="https://www.robots.ox.ac.uk/~vgg/publications/2016/Chung16a/chung16a.pdf" rel="noopener ugc nofollow" target="_blank">https://www . robots . ox . AC . uk/~ vgg/publications/2016/chung 16a/chung 16a . pdf</a></li><li id="6a69" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="http://conradsanderson.id.au/vidtimit/" rel="noopener ugc nofollow" target="_blank">http://conradsanderson.id.au/vidtimit/</a></li><li id="9b6e" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-fre" rel="noopener ugc nofollow" target="_blank">http://www . practical cryptography . com/miscellaneous/machine-learning/guide-Mel-fr</a>频率-倒谱-系数-mfccs/</li><li id="16d8" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://gitlab.idiap.ch/bob/bob.paper.eusipco2018" rel="noopener ugc nofollow" target="_blank">https://gitlab.idiap.ch/bob/bob.paper.eusipco2018</a></li><li id="95ea" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated">【https://github.com/voletiv/syncnet-in-keras T2】号</li><li id="5301" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated">【https://github.com/joonson/syncnet_python T4】</li><li id="1e6e" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a></li><li id="a7d2" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/keras/Optimizer/Optimizer</a></li><li id="ca9f" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://stackoverflow.com/questions/59731667/why-does-training-using-tf-gradienttape-in-tensorflow-2-have-different-behavior" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/59731667/why-training-using-TF-gradient tape-in-tensor flow-2-have-different-behavior</a></li><li id="5f99" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/GradientTape</a></li><li id="9519" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://keras.io/examples/keras_recipes/tfrecord/" rel="noopener ugc nofollow" target="_blank">https://keras.io/examples/keras_recipes/tfrecord/</a></li><li id="284e" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://stackoverflow.com/questions/47861084/how-to-store-numpy-arrays-as-tfrecord?rq=1" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/47861084/how-to-store-numpy-arrays-as-TF record？rq=1 </a></li><li id="4a87" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://stackoverflow.com/questions/30008859/how-to-create-a-video-with-raw-images-rgb-format-and-add-audio-to-it-in-pytho" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/30008859/how-to-create-a-video-with-raw-images-RGB-format-and-add-audio-to-it-in-pytho</a></li><li id="d610" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://stackoverflow.com/questions/7833807/get-wav-file-length-or-duration" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/7833807/get-wav-file-length-or-duration</a></li><li id="1085" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://theailearner.com/2018/10/15/creating-video-from-images-using-opencv-python/" rel="noopener ugc nofollow" target="_blank">https://theailearner . com/2018/10/15/creating-video-from-images-using-opencv-python/</a></li><li id="54d8" class="mb mc hi ke b kf ms ki mt kl mu kp mv kt mw kx mr mh mi mj bi translated"><a class="ae jj" href="https://zulko.github.io/moviepy/getting_started/getting_started.html" rel="noopener ugc nofollow" target="_blank">https://zulko . github . io/movie py/getting _ started/getting _ started . html</a></li></ul></div><div class="ab cl mx my gp mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="hb hc hd he hf"><p id="0dac" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">谢谢你读到这里。</p><p id="1f90" class="pw-post-body-paragraph kc kd hi ke b kf ky ij kh ki kz im kk kl la kn ko kp lb kr ks kt lc kv kw kx hb bi translated">有意见、问题或补充吗？下面评论！您也可以通过<a class="ae jj" href="mailto:nehasinghsikerwar@gmail.com" rel="noopener ugc nofollow" target="_blank">电子邮件</a>直接与我联系，或者通过<a class="ae jj" href="https://www.linkedin.com/in/neha-sikerwar-785514113/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。</p></div></div>    
</body>
</html>