<html>
<head>
<title>The step-by-step approach using Decision Tree Modeling using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python的决策树建模的逐步方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-step-by-step-approach-using-decision-tree-modeling-using-python-4ef1b4a3fe3?source=collection_archive---------20-----------------------#2020-09-10">https://medium.com/analytics-vidhya/the-step-by-step-approach-using-decision-tree-modeling-using-python-4ef1b4a3fe3?source=collection_archive---------20-----------------------#2020-09-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7b0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大家好，</p><p id="82fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望你们都很好，很安全！！</p><p id="3d36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我试图展示我在决策树建模方面的知识和经验。相信这篇文章能对任何想从头了解决策树模型的人有所帮助。</p><p id="ca2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">声明人:所使用的数据、Python代码、输出和图形与任何项目无关。这篇文章的读者必须有统计学和python编程的背景</p><p id="9a1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">开始吧！！！</p><p id="aafe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们开始之前，一些与决策树相关的关键词。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/5a4ba7dd0a599bcd22d232c389f96369.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/0*lbFwWyPa1Az37oKK"/></div></figure><p id="1caf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了预测案例，决策树使用涉及输入变量值的规则。规则以树状结构分层排列，节点由线条连接。节点代表决策规则，线条对规则进行排序。树顶部的第一个规则称为<em class="jl">根节点。</em>后续规则被命名为<em class="jl">内部或内部节点。</em>只有连接的节点称为<em class="jl">叶节点。</em>要对新案例进行评分，请检查输入值，并应用决策树模型定义的规则。</p><p id="98d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树模型可以通过以下三个步骤实现:</p><p id="f405" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.预测新情况:在我们建立模型之前，我们必须清楚我们的模型将要预测什么。可以有如下三种类型的预测:</p><p id="e6bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jl">决策</em> —您的模型使用输入为每个案例做出最佳决策(例如:将案例分类为潜在和非潜在客户)</p><p id="627b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jl">排名</em> —您的模型使用输入对每个案例进行最佳排名。(例如，模型试图将高价值案例排在低价值案例的前面。)</p><p id="b402" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jl">估计</em> —您的模型使用输入来最佳预测目标值。(例如，如果您的变量是数值型的，则为具有输入观测值的所有案例的目标平均值；如果您的变量是分类型的，则为事件的概率)</p><p id="34b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.输入选择:这一步需要搜索最能预测(决策/排序/估计)响应变量值的输入。这可以通过做来实现</p><p id="e37b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入减少—维度空间中的冗余，以及</p><p id="4c4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入减少——在你的维度空间中无关紧要</p><p id="0328" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">冗余输入不会给出任何其他输入没有解释过的新信息。</p><p id="eb7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不相关的输入不提供关于目标的任何信息。对于决策树模型，建模算法会自动忽略不相关的输入。</p><p id="77cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.优化复杂性:拟合一个模型需要搜索所有可能的模型。</p><p id="e022" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它涉及偏差和方差之间的权衡。不够复杂的模型可能不够灵活，这可能导致<em class="jl">欠拟合</em>，从而系统性地丢失信号(高偏置)。</p><p id="0343" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人们认为最复杂的模型应该总是比其他模型表现得更好。但是复杂模型可能过于灵活，这可能导致过度拟合，即适应特定样本中的细微误差(高方差)。</p><p id="db67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们必须选择适当的复杂度来给出最好的概括。</p><p id="9ec2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将通过使用以上三个步骤来实现决策树建模。</p><ol class=""><li id="2150" class="jm jn hi ih b ii ij im in iq jo iu jp iy jq jc jr js jt ju bi translated">预测新情况:让我们考虑值为“是”或“否”的二元响应变量。因此，我们的模型输出将是一个<em class="jl">决策(或者“是”或者“否”)</em>。</li><li id="f2dd" class="jm jn hi ih b ii jv im jw iq jx iu jy iy jz jc jr js jt ju bi translated">输入选择:决策树建模通过使用三种技术来处理<em class="jl">维数灾难</em>问题。</li></ol><p id="f9e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">卡方自动交互检测(CHAID):当我们处理区间标度输入时，输入变量的所有值都作为潜在的分裂点。让我用两个输入x1和x2来表示，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es ka"><img src="../Images/18583ff7183feafe215537bf69f4fc09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Yw5t5j-Yj3k36Rqi"/></div></div></figure><p id="60aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于特定输入(比如x1)的选定数据点，生成两个组。输入值小于该数据点的情况称为<em class="jl">分支向左。</em>输入值大于该数据点的情况称为<em class="jl">向右分支。</em>见下图:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es ka"><img src="../Images/37d730c5b7411e889f51d6c80e1620c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*F91fh2vAp8370i_X"/></div></div></figure><p id="093f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将简单地通过计算目标结果的比例来填充上面的2*2列联表。见下图(下图仅为示例，并非实际数据):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es ka"><img src="../Images/70a53b350db54f7c8ed41e3976b90eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gfgvQ1-2lReWRDVB"/></div></div></figure><p id="f2a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们有了这个表，我们将计算皮尔逊卡方统计量，用于量化表中各列计数的独立性。较大的卡方值表示左分支中“是”的比例不同于右分支中“是”的比例。结果比例的较大差异表明分割良好。</p><p id="6779" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将对所有其他数据点进行同样的处理。由于我们将基于所有其他数据点进行多重比较，因此<em class="jl">类型1错误</em>的几率会增加。因此，我们将使用<em class="jl"> logworth = </em> -log(卡方值<em class="jl"> p值</em>)来操作<em class="jl"> p值</em>。</p><p id="6f7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为每个分割点对应一个统计测试，所以<em class="jl"> p值</em>需要调整一个等于正在进行的测试数量的因子。这个膨胀的<em class="jl"> p值</em>被称为<em class="jl"> Bonferroni校正。</em></p><p id="a4a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jl">要进行拆分，必须至少有一个logworth超过阈值(决策树模型中的一个参数)。</em></p><p id="ce87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你知道皮尔逊卡方统计的细节，这很简单。</p><p id="b43d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">熵和信息增益:熵是每个标签的概率乘以该标签的对数概率的总和。在上面X1 = 0.23的2*2表中，熵的计算方法是P(事件)=事件的比例* log(事件的比例)</p><p id="a786" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，对于要发生的分割，我们将进行如下计算:</p><p id="89ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分裂发生前的熵:如你所见，我们在原始数据中有6个是和6个否。P(是)= 0.50 * log(0.50)而P(否)= 0.50 * log(0.50)而熵_before = P(是)+P(否)</p><p id="057c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">左分支的熵(即X1 &lt; 0.23): P(Yes) = 0.55 * log(0.55) then P(No) = 0.45* log(0.45) and entropy_left = P(Yes)+P(No)</p><p id="953a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Entropy for Branch Right(that is X1 &gt; 0.23): P(是)= 0.40* log(0.40)然后P(否)= 0.60* log(0.60)并且熵_右= P(是)+P(否)</p><p id="7d80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分割后的熵:我们现在将合并熵_左和熵_右，即熵_后=熵_左+熵_右。</p><p id="35c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，通过比较熵_before和熵_after，我们将得到信息增益，即信息增益=熵_ before-熵_after。</p><p id="62ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">信息增益衡量我们通过使用该特定特征进行分割获得了多少信息(X1 = 0.23)。在树的每个节点，对每个特征进行这种计算，并选择具有最大信息增益的特征进行分割。</p><p id="1a7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼系数:基尼系数，也称为基尼系数，计算随机选择时某一特定特征被错误分类的概率。如果所有的元素都与一个类联系在一起，那么它就可以被称为纯类。</p><p id="2cc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来感知基尼指数的标准，就像熵的性质一样，<em class="jl">基尼指数在值0和1之间变化，其中0表示分类的纯度，即所有元素都属于一个指定的类别或者只存在一个类别。1表示元素在各种类中的随机分布。基尼指数值为0.5，表明一些阶层的要素分布均等。</em></p><p id="c5db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼指数是通过从1中减去每一类概率的平方和来确定的，在数学上，基尼指数可以表示为:</p><p id="f979" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼指数= 1 —总和(P)**2。</p><p id="4905" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在设计决策树时，基尼系数最小的特征将被优先考虑。</p><p id="3ad9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.优化复杂性:决策树建模是基于输入空间的不同规则的集合。我们将通过评估测试或验证数据的规则来找到合适的复杂度。可以使用不同的统计数据来衡量决策树模型的性能。同样，它基于目标变量的预测类型和测量水平。</p><p id="9473" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们有一个<em class="jl">二元</em>目标变量，其预测类型等于<em class="jl">决策。</em>适当的统计可以是多个真阳性和多个真阴性。</p><p id="ff66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">真阳性:使用您的模型将主要决策(是)与<em class="jl">实际的</em>主要结果进行匹配会产生真阳性。</p><p id="ab49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">真正的否定:使用您的模型将次要决策(否)与实际的<em class="jl">次要结果相匹配会产生真正的否定。</em></p><p id="4355" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">精度= tp + tn。我们将使用不同数量的规则(决策树模型)来分析准确性，以设置适当的复杂度。</p><p id="d4a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们用Python来实现它。我尝试将决策树模型的整个过程自动化，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es kf"><img src="../Images/fa25b1cab83226613693ef8e159d862c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*27jHUjOoJYwUCC2j"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es ka"><img src="../Images/7ba4bde5b35ccaa854c0de245b07b3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gA0PVYyeeVy8Mja-"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es ka"><img src="../Images/7855a1c300b07224f9ba15f134f1d5b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v9fqAwWj81wNMgKc"/></div></div></figure><p id="b784" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以清楚地看到，精确度在18个叶节点树附近最大。因此，我们将采用18叶决策树。脚本中的一些参数需要澄清。</p><p id="1119" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">方法=用于构建决策树模型的标准。它主要用于决策树模型开发过程中的输入/特征选择。Python sklearn只支持熵和基尼。你也可以用这个片段来表达基尼。</p><p id="a938" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">max_leaf_nodes =减少叶节点的数量。这会影响树的大小。</p><p id="9f8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">max_depth =降低树的深度以构建一般化的树。根据对测试数据的验证，将树的深度设置为3、5、10</p><p id="eba3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望有帮助。</p><p id="e7fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谢谢大家！！！</p><p id="c3f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">乌尔维什·沙阿</p></div></div>    
</body>
</html>