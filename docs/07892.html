<html>
<head>
<title>The Mathematics Behind Principal Component Analysis (PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析背后的数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mathematics-behind-principal-component-analysis-pca-1cdff0a808a9?source=collection_archive---------2-----------------------#2020-07-11">https://medium.com/analytics-vidhya/mathematics-behind-principal-component-analysis-pca-1cdff0a808a9?source=collection_archive---------2-----------------------#2020-07-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="2a1c" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">在本文中，我将从数学上推导PCA是如何工作的，并从头开始用python和使用scikit learn实现它。</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jh"><img src="../Images/b3b60aedfd34eadd06006bfa977c12b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VFsrjlSuoW_By8Q2"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">沃洛季米尔·赫里先科在<a class="ae jx" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="1976" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">降维指的是减少数据中特征/变量数量的技术。我们对</strong>进行降维处理</p><ul class=""><li id="2983" class="kb kc hi il b im in iq ir jy kd jz ke ka kf jg kg kh ki kj bi translated">在2D图上可视化高维数据。</li><li id="c28e" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kg kh ki kj bi translated">去掉相关的和多余的特征。</li><li id="d215" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kg kh ki kj bi translated">防止过度拟合数据。</li><li id="513e" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kg kh ki kj bi translated">解决维数灾难。</li></ul><p id="1e82" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">主成分分析是最流行的降维技术之一。PCA背后的关键思想是<strong class="il hj">它创建新特征</strong>并将原始数据投射到这些特征上，这些特征是原始特征的线性组合，目的是最大化数据的总变化，即保留尽可能多的信息。</p><h1 id="eb09" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">主成分分析的几何解释</h1><p id="01c0" class="pw-post-body-paragraph ii ij hi il b im ln io ip iq lo is it jy lp iw ix jz lq ja jb ka lr je jf jg hb bi translated">让我们试着在二维空间中理解PCA。我们将尝试将二维数据简化为一维数据。同样的想法也可以扩展到更高的维度。我们的目标是保持具有最大传播/方差的方向。先说一个简单的案例。</p><p id="3643" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">注意:在应用PCA之前，请始终标准化您的数据。</strong></p><p id="b62b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">假设我们有以下标准化数据:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es ls"><img src="../Images/81d3ba7446c0ab7076c0feca568bcc61.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*xMCGtu_ZLkl9WdcnXXWHEw.png"/></div></figure><p id="80f2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">假设您必须从X1和X2中选择一个特征，您会选择哪个特征来表示数据？根据您的看法，哪一项功能似乎更重要？解释数据最大变化的那个。这就是特征X1。这正是PCA所做的。它找到具有最大扩散的特征并丢弃其他特征，目的是<strong class="il hj">最小化信息损失</strong>。</p><p id="54a5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">让我们举一个稍微复杂的例子，我们不能简单地删除一个特性。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lt"><img src="../Images/016741d18bd10cb00e8cbdb919411e31.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*M2vjbSothG8wOrUbRbEfzw.png"/></div></div></figure><p id="f819" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">这里，特征X1和X2具有相等的分布。所以我们分不清哪个特征更重要。但是如果我们试图找到一个解释数据变化的方向(或轴),我们可以找到一条与数据非常吻合的线。所以如果我们把轴稍微旋转θ，我们得到f1和f2(垂直于f1)。然后我们可以去掉f2，说f1是最重要的特性。</p><p id="bf86" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">这正是PCA所做的。它获取数据并试图找到方向f1，使得投射到f1上的点的方差最大。</p><blockquote class="if ig ih"><p id="4e0d" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">请注意，我们正在通过创建新要素来转换数据。</p></blockquote><p id="8dd3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">好吧，这听起来很酷，但是PCA是如何找到方向的呢？PCA利用<em class="ik">线性代数</em>和<em class="ik">最优化理论</em>的概念，寻找传播最大的方向。让我们调查一下。</p><h1 id="27ba" class="kp kq hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated"><strong class="ak">PCA背后的数学</strong></h1><p id="dc17" class="pw-post-body-paragraph ii ij hi il b im ln io ip iq lo is it jy lp iw ix jz lq ja jb ka lr je jf jg hb bi translated">我们的目标是找到具有最大扩散的方向，并将数据点投影到该方向上。</p><p id="051e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">让我们试着找到一条线，使投影点到原点的距离最大化，即投影距离的方差最大化。</strong></p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lu"><img src="../Images/00752a32c6214db9cc86aea8164bc531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zRmHNiLcvE1bcqbejNLSjg.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lv"><img src="../Images/e53727bfb8ecef8f62f8e0f4170cf64a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZssrXxDXy92H4lm1JVQEmg.png"/></div></div></figure><p id="40ba" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">这种方法被称为<em class="ik">方差最大化方法</em>。还有另一种方法可以构造PCA的优化函数。</p><p id="0d95" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">考虑PCA的另一种方式是，它拟合穿过我们数据的最佳直线，目的是最小化每个点的投影误差‘d’。</strong>这种方法被称为<em class="ik">距离最小化方法</em>。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lw"><img src="../Images/6dfb6e7815ede0f584e98b35a139518c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cx5CexCv6SEboLDUo3pXPQ.png"/></div></div></figure><p id="d3b6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">请注意，这两个优化问题虽然看起来不同，但却是相同的。因为(x^T *x)项独立于u，所以为了最小化函数，我们必须最大化(u^t *u)，这与我们的第一个优化问题相同。</p><p id="8cfc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">在使用拉格朗日乘数法解决这个优化问题之前，让我们先了解一些术语。</p><blockquote class="if ig ih"><p id="4036" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">假设X是我们的数据矩阵，有n个观测值和d个特征。</p></blockquote><ol class=""><li id="64ed" class="kb kc hi il b im in iq ir jy kd jz ke ka kf jg lx kh ki kj bi translated"><strong class="il hj">协方差矩阵</strong></li></ol><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es ly"><img src="../Images/b572cf9a69bcdb172ea7090ea4ad7445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*mSPg5pSSZxYRVgOWyDEw9w.png"/></div></figure><p id="5c8f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">协方差矩阵(或方差协方差矩阵)是形状=要素数量的方阵，其对角元素是每个要素的方差，非对角元素是要素之间的协方差。</p><p id="a03f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj"> 2。特征值和特征向量</strong></p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lz"><img src="../Images/cbf45323bf22c4baecf0a1693e302f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJLCZk6ICDI8eEFjNj4qKg.png"/></div></div></figure><p id="b801" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">对应于每个特征值，存在一个特征向量。所有的特征向量都是正交的。</p></div><div class="ab cl ma mb gp mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="hb hc hd he hf"><h1 id="b2af" class="kp kq hi bd kr ks mh ku kv kw mi ky kz la mj lc ld le mk lg lh li ml lk ll lm bi translated"><strong class="ak">解决优化问题</strong></h1><p id="1e8c" class="pw-post-body-paragraph ii ij hi il b im ln io ip iq lo is it jy lp iw ix jz lq ja jb ka lr je jf jg hb bi translated">让我们再来看看我们的优化问题。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mm"><img src="../Images/43b0baafe91a9f9ae98d53d04eced312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fwAidyYmoS_i7wcHeLwY1g.png"/></div></div></figure><p id="6d4b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">现在让我们尝试使用拉格朗日乘数法来解决我们修改后的优化问题。</strong></p><p id="01bb" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">设λ是我们的拉格朗日乘数。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mn"><img src="../Images/20f6b66f033fae6ad6ef96227b678a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eyyTQk8tBlCDYQlURIAxUQ.png"/></div></div></figure><p id="3818" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">因此，u是对应于最大特征值λ的协方差矩阵S的特征向量。</p></div><div class="ab cl ma mb gp mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="hb hc hd he hf"><p id="966a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">让我们把所有的步骤再写一遍:</p><p id="2b67" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">假设我们的数据矩阵X有d个特征，我们想把它们减少到k个特征。</p><ol class=""><li id="d508" class="kb kc hi il b im in iq ir jy kd jz ke ka kf jg lx kh ki kj bi translated">列标准化你的数据。</li><li id="935c" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg lx kh ki kj bi translated">求协方差矩阵。</li><li id="5dfc" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg lx kh ki kj bi translated">求协方差矩阵的所有特征值和特征向量。</li><li id="dbf4" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg lx kh ki kj bi translated">则最大特征值λ1对应的v1是方差最大的方向，λ2对应的v2是方差第二大的方向，以此类推。</li><li id="4461" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg lx kh ki kj bi translated">为了获得k个特征，将原始数据矩阵与对应于前k个最大特征值的特征向量矩阵相乘。</li></ol><p id="c237" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">结果矩阵是具有减少的特征的矩阵。</p><p id="4b68" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">我们来理解一下特征值的解释。</strong></p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mo"><img src="../Images/fc660a54414752124e38f232296b48fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fWLY0KihmpvMyGJglMpDvw.png"/></div></div></figure></div><div class="ab cl ma mb gp mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="hb hc hd he hf"><h1 id="80b5" class="kp kq hi bd kr ks mh ku kv kw mi ky kz la mj lc ld le mk lg lh li ml lk ll lm bi translated">Python中的PCA</h1><p id="0acb" class="pw-post-body-paragraph ii ij hi il b im ln io ip iq lo is it jy lp iw ix jz lq ja jb ka lr je jf jg hb bi translated">我将在<a class="ae jx" href="https://www.kaggle.com/uciml/iris" rel="noopener ugc nofollow" target="_blank">虹膜数据集</a>上实现PCA。链接:</p><p id="3ac4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">从无到有</strong></p><pre class="ji jj jk jl fd mp mq mr ms aw mt bi"><span id="f685" class="mu kq hi mq b fi mv mw l mx my">#Importing the data<br/>data= pd.read_csv("Iris.csv")<br/>data.head()</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es mz"><img src="../Images/816504fc677d07d0c3d1a65f4a5d6728.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*RTQ7olzdbZIczyMo89XSbQ.png"/></div></figure><pre class="ji jj jk jl fd mp mq mr ms aw mt bi"><span id="48c6" class="mu kq hi mq b fi mv mw l mx my">X = data.iloc[:, 0:4]<br/>y = data.species</span><span id="5c75" class="mu kq hi mq b fi na mw l mx my">#<strong class="mq hj">Step 1: Let's scale the data.</strong></span><span id="1c22" class="mu kq hi mq b fi na mw l mx my">from sklearn.preprocessing import StandardScaler<br/>X_scaled = StandardScaler().fit_transform(X)</span><span id="dfd7" class="mu kq hi mq b fi na mw l mx my">#<strong class="mq hj">Step 2: Find the covariance matrix</strong>.</span><span id="a09c" class="mu kq hi mq b fi na mw l mx my">covar_matrix = (1 / X_scaled.shape[0]) * np.matmul(X_scaled.T,X_scaled)<br/>print (covar_matrix.shape)</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es nb"><img src="../Images/52e5d96f8629c5fbf25562b43e440874.png" data-original-src="https://miro.medium.com/v2/resize:fit:168/format:webp/1*1SqIwB7yU6_ok_SYk5ruxQ.png"/></div></figure><pre class="ji jj jk jl fd mp mq mr ms aw mt bi"><span id="6078" class="mu kq hi mq b fi mv mw l mx my">#<strong class="mq hj">Step 3: Find the eigenvalues and eigenvectors.</strong></span><span id="844d" class="mu kq hi mq b fi na mw l mx my">from scipy.linalg import eigh</span><span id="d24a" class="mu kq hi mq b fi na mw l mx my">#eigh function returns the eigenvalues in ascending order<br/>#We specify the top 2 eigenvalues (out of 0, 1, 2, 3)</span><span id="39ad" class="mu kq hi mq b fi na mw l mx my">values, vectors = eigh(covar_matrix, eigvals=(2, 3))<br/>print (vectors.shape)<br/>print (vectors)</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es nc"><img src="../Images/86ea319688ff52eab14907980e88c1e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*t5tKHyMwe1Ox4g8o39uJGw.png"/></div></figure><pre class="ji jj jk jl fd mp mq mr ms aw mt bi"><span id="5fd9" class="mu kq hi mq b fi mv mw l mx my">#<strong class="mq hj">Step 4: Project the original data on eigenvectors</strong>.</span><span id="0734" class="mu kq hi mq b fi na mw l mx my">pca_components = np.matmul(X_scaled, vectors)<br/>pca_data = pd.DataFrame(np.hstack((pca_components, y.to_numpy().reshape(-1, 1))), columns=["Component 1", "Component 2", "Species"])<br/>pca_data.head()</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es nd"><img src="../Images/b4cde19c8d021573a19c882229abb7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*D4kZ3z6EJSWn8ZnvYBlung.png"/></div></figure><pre class="ji jj jk jl fd mp mq mr ms aw mt bi"><span id="49fc" class="mu kq hi mq b fi mv mw l mx my">#<strong class="mq hj">Let's calculate the percentage of variation explained.<br/></strong>print (values.sum() / eigh(covar_matrix)[0].sum() * 100)</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es ne"><img src="../Images/c4a3c432d3568551a6131de21e821773.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*skYndlm5MD-9JZFadn1L7g.png"/></div></figure><pre class="ji jj jk jl fd mp mq mr ms aw mt bi"><span id="03df" class="mu kq hi mq b fi mv mw l mx my">#<strong class="mq hj">Now let’s plot the principal components.</strong><br/>sns.scatterplot(x= "Component 1", y= "Component 2", hue= "Species", data= pca_data)</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es nf"><img src="../Images/7605a31fa49c83d9f6e8341692cce1bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*NXwdo5FyXPiJJ-u5eeVJhg.png"/></div></figure><p id="59a9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">使用scikit学习</strong></p><pre class="ji jj jk jl fd mp mq mr ms aw mt bi"><span id="f78e" class="mu kq hi mq b fi mv mw l mx my">from sklearn.decomposition import PCA</span><span id="715e" class="mu kq hi mq b fi na mw l mx my">#Using the <strong class="mq hj">n_components</strong> argument<br/>pca = PCA(n_components= 2)<br/>pca_components = pca.fit_transform(X_scaled)</span><span id="5cd3" class="mu kq hi mq b fi na mw l mx my">pca_data = pd.DataFrame(np.hstack((pca_components, y.to_numpy().reshape(-1, 1))), columns=["Component 1", "Component 2", "Species"])<br/>pca_data.head()</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es ng"><img src="../Images/2b567ec88987529257f7b0494b669ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*IdDHmRvZLUaz4ER_8Nk8CA.png"/></div></figure><p id="a99f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">请注意，在sklearn实现中，组件被交换了。</p><pre class="ji jj jk jl fd mp mq mr ms aw mt bi"><span id="bcde" class="mu kq hi mq b fi mv mw l mx my">print (pca.explained_variance_ratio_.sum() *100)</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es nh"><img src="../Images/91ae0b73355dfa45b344823f7f4326f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*61OYVFCNPithNiBUpVPwKw.png"/></div></figure><pre class="ji jj jk jl fd mp mq mr ms aw mt bi"><span id="ce36" class="mu kq hi mq b fi mv mw l mx my">sns.scatterplot(x= “Component 1”, y= “Component 2”, hue= “Species”, data= pca_data)</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es nf"><img src="../Images/f82370d5a8cd90a88b517f3e8983d9a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*qRfomt6YupEtdAx7kARhyg.png"/></div></figure></div><div class="ab cl ma mb gp mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="hb hc hd he hf"><p id="c1f0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">感谢您的阅读。</p><p id="c86d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">你可以在LinkedIn上联系我:<a class="ae jx" href="http://www.linkedin.com/in/madhav-samariya-7a636b17a" rel="noopener ugc nofollow" target="_blank">www.linkedin.com/in/madhav-samariya-7a636b17a</a></p></div></div>    
</body>
</html>