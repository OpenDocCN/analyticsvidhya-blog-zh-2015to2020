<html>
<head>
<title>Build a Data ETL Tool with Kaggle API on my Raspberry Pi Hadoop Cluster</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在我的Raspberry Pi Hadoop集群上用Kaggle API构建数据ETL工具</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/build-a-data-etl-tool-with-kaggle-api-on-my-raspberry-pi-hadoop-cluster-db4ed1e9883d?source=collection_archive---------19-----------------------#2019-12-27">https://medium.com/analytics-vidhya/build-a-data-etl-tool-with-kaggle-api-on-my-raspberry-pi-hadoop-cluster-db4ed1e9883d?source=collection_archive---------19-----------------------#2019-12-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="4fd5" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="246f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">有了我和我的同学林振之前构建的树莓派Hadoop集群，我们决定做一些有趣的事情。在我们想到的所有选项中，最终的决定是用Kaggle API实现制作一个数据ETL(提取、转换和加载)服务工具。构建这个的最大动机是，我们希望从数据工程师领域学习更多，并且拥有一个有用的工具，可以轻松地从Kaggle提取、转换和加载数据。考虑到我们的下一步是通过构建辅助项目进入数据科学领域，我们希望这个工具真的非常有用。</p><h1 id="b1c4" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">我们的Kaggle数据ETL工具的结构:</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/7eb7346f96f26d733e99b28253ec2955.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8c4NApp-D55B56uI5LLzwA.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">我们的Kaggle数据ETL工具结构图</figcaption></figure><h1 id="2b37" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">1.蜂巢装置</h1><p id="2e74" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">为了Hive SQL的易用性和特性，我们决定在我们的Hadoop集群上安装Hive。关于蜂巢的安装，我们简单的按照<a class="ae kb" href="http://nicholaspropes.com/files/documents/2016-09-05-Raspberry-Pi-Hadoop-Setup-v1-%281%29.pdf?fbclid=IwAR0bNr39ccY3ib-yJ-38MVvY7IGMHmdDAg-VkkLUjnFEBAZ3_OO-A6135fw" rel="noopener ugc nofollow" target="_blank">这篇文章</a>的说明。然而，在安装过程中，有一件事我们想提出来。</p><p id="790e" class="pw-post-body-paragraph jd je hi jf b jg ks ji jj jk kt jm jn jo ku jq jr js kv ju jv jw kw jy jz ka hb bi translated"><strong class="jf hj"> <em class="kx">从德比切换到马里亚布</em> </strong></p><p id="dbf9" class="pw-post-body-paragraph jd je hi jf b jg ks ji jj jk kt jm jn jo ku jq jr js kv ju jv jw kw jy jz ka hb bi translated">切换DB引擎有几个原因。在阅读了<a class="ae kb" href="https://db-engines.com/en/system/Derby%3BMariaDB" rel="noopener ugc nofollow" target="_blank">这篇关于Derby和MariaDB的比较的文章</a>之后，为了工具的未来发展，我们决定改用MariaDB。我们只需按照本文中<a class="ae kb" href="https://howtoraspberrypi.com/mariadb-raspbian-raspberry-pi/" rel="noopener ugc nofollow" target="_blank">的说明来完成翻译。</a></p><h1 id="6fad" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">2.数据流图</h1><p id="144d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这个数据流程图展示了Kaggle数据集如何流经我们的ETL工具。我将首先简要地向您介绍这个过程，进一步的细节将在下面的段落中提供。我们创建了三个python文件来负责处理数据集。</p><p id="ed42" class="pw-post-body-paragraph jd je hi jf b jg ks ji jj jk kt jm jn jo ku jq jr js kv ju jv jw kw jy jz ka hb bi translated">首先，我们将使用实现Kaggle API的extractor.py从Kaggle中提取数据集，并将其存储到未处理的数据文件夹中。第二，未处理的数据将由transformer.py处理，并转换成我们想要的格式。在这个阶段之后，处理过的数据将被存储在数据文件夹中。最后，我们将使用loader.py将处理后的数据作为表加载到hadoop集群中。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ky"><img src="../Images/37030e19cc38c859ffd5243185c6b8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oV99hF6SUK_FjGm1hXMmmw.png"/></div></div></figure><h1 id="305c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">3.带Kaggle API的提取器</h1><p id="564a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj"> <em class="kx"> Kaggle API </em> </strong></p><p id="6f38" class="pw-post-body-paragraph jd je hi jf b jg ks ji jj jk kt jm jn jo ku jq jr js kv ju jv jw kw jy jz ka hb bi translated">在构建提取器之前，我们首先按照<a class="ae kb" href="https://github.com/Kaggle/kaggle-api" rel="noopener ugc nofollow" target="_blank"> Kaggle官方Githu </a> b的指示实现Kaggle API。</p><p id="29bf" class="pw-post-body-paragraph jd je hi jf b jg ks ji jj jk kt jm jn jo ku jq jr js kv ju jv jw kw jy jz ka hb bi translated"><strong class="jf hj"> <em class="kx">提取器</em> </strong></p><p id="eeb4" class="pw-post-body-paragraph jd je hi jf b jg ks ji jj jk kt jm jn jo ku jq jr js kv ju jv jw kw jy jz ka hb bi translated">对于提取器，我们实现了Kaggle API并构建了这个命令行界面，人们可以简单地输入Kaggle数据集名称并获得所需的数据集。</p><pre class="kd ke kf kg fd kz la lb lc aw ld bi"><span id="12ca" class="le ig hi la b fi lf lg l lh li">import click<br/>import kaggle<br/>from models.Kaggle_driver import Kaggle_Api</span><span id="aa44" class="le ig hi la b fi lj lg l lh li">@click.command()<br/>@click.option("--name", prompt="Kaggle Dataset Name", help="Kaggle dataset name.")<br/>@click.option("--dir", default="../data/unprocessed", help="The specify the storage directory.")<br/>def fetch_dataset(name, dir):<br/>    <em class="kx">"""Fetch kaggle data to specific directory"""<br/>    </em>try:<br/>        api = Kaggle_Api()<br/>        api.download_dataset(name, dir)<br/>    except:<br/>        print("An Error occurred! Please check dataset name and exist of the directory that you want to store")<br/>   </span><span id="4364" class="le ig hi la b fi lj lg l lh li">if __name__ == '__main__':<br/>    fetch_dataset()</span></pre><h1 id="564b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">4.变压器</h1><p id="a7c8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对于转换器，我们有数百种方法来设计模型，将脏数据集转换为干净的数据集，可以轻松地放入Hadoop集群，并在未来轻松进行分析。我们仍然决定将它构建成一个命令行界面。对于启动程序，我们只是构建了一个函数，可以检查每一列的值，并用“-”替换“，”号。我将在稍后的加载器部分解释为什么我们决定从转换器的这个特定函数开始。</p><pre class="kd ke kf kg fd kz la lb lc aw ld bi"><span id="40fd" class="le ig hi la b fi lf lg l lh li">import pandas as pd<br/>import click<br/></span><span id="317b" class="le ig hi la b fi lj lg l lh li">@click.command()<br/>@click.option("--name", prompt="CSV Name", help="Type Your CSV Name.")<br/>@click.option("--dir", default="../data", help="The specify the storage directory.")<br/>def CommaTransform (name, dir):<br/>    df = pd.read_csv(f"{dir}/unprocessed/{name}")<br/>    Col_list = df.columns<br/>    for col in Col_list:<br/>        df[col] = df[col].astype(str)<br/>        df[col] = df[col].replace({',': '-'}, regex=True)<br/>        <br/>    df.to_csv(f"{dir}/{name}", index=None)<br/></span><span id="8e51" class="le ig hi la b fi lj lg l lh li">if __name__ == '__main__':<br/>    CommaTransform()</span></pre><h1 id="fa51" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">5.装货设备</h1><p id="b814" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">最后，我们构建了这个加载器命令行接口，让用户可以轻松地将处理后的数据集作为Hive表上传到Hadoop集群中。用户只需要输入表名(“Hive表名”)、CSV文件名(“数据集名”)和CSV目录(“CSV文件路径”)的值，加载器就可以完成这项工作。在整个过程中，我们遇到了一些困难，我们想在这里提出来，并解释我们实施的解决方案。</p><p id="45e5" class="pw-post-body-paragraph jd je hi jf b jg ks ji jj jk kt jm jn jo ku jq jr js kv ju jv jw kw jy jz ka hb bi translated"><strong class="jf hj">熊猫和蜂巢的数据类型的不同命名系统</strong></p><p id="5b65" class="pw-post-body-paragraph jd je hi jf b jg ks ji jj jk kt jm jn jo ku jq jr js kv ju jv jw kw jy jz ka hb bi translated">当我们试图将数据集作为Hive表上传到Hadoop时，我们意识到了这个命名问题。我们解决这个问题的方法是创建一个json文件，用字典数据结构将pandas和Hive之间的对应名称配对，并使用下面显示的<strong class="jf hj"><em class="kx">pandas _ to _ Hive _ dtype _ converter</em></strong>函数来完成任务。</p><pre class="kd ke kf kg fd kz la lb lc aw ld bi"><span id="8291" class="le ig hi la b fi lf lg l lh li">{<br/>    "object" : "STRING",<br/>    "int64" : "BIGINT",<br/>    "float64" : "DOUBLE",<br/>    "bool": "BOOLEAN",<br/>    "datetime64" : "DATE",<br/>    "timedelta[ns]" : "STRING",<br/>    "category" : "STRING"<br/>}</span></pre><p id="6b27" class="pw-post-body-paragraph jd je hi jf b jg ks ji jj jk kt jm jn jo ku jq jr js kv ju jv jw kw jy jz ka hb bi translated"><strong class="jf hj">创建Hive表时用“，”分隔列的混乱</strong></p><p id="405f" class="pw-post-body-paragraph jd je hi jf b jg ks ji jj jk kt jm jn jo ku jq jr js kv ju jv jw kw jy jz ka hb bi translated">因为我们使用的Hive sql命令行，当我们试图创建一个表时，Hive系统会生成列，并在CSV文件中用“，”分隔每一列。但是，如果在一个单独的列中有包含" "的值，这个简单的Hive sql命令行可能会导致严重的问题。hive系统会错误地将这些值视为单独的列，而它们应该放在Hive表的同一列中。我们在变形金刚部分解决了这个问题，我相信你已经看过了。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lk"><img src="../Images/715dc2f5daeaa267368509f56bbfe82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*-ERiEHD5TAzHnTauZ7LREQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图片显示了创建配置单元表时可能会引起混淆的列类型</figcaption></figure><pre class="kd ke kf kg fd kz la lb lc aw ld bi"><span id="f279" class="le ig hi la b fi lf lg l lh li">import json<br/>import click<br/>import os<br/>import pandas as pd</span><span id="0eb0" class="le ig hi la b fi lj lg l lh li">from preprocess.get_csv_summary import Csv_summerizer<br/>from pyhive import hive</span><span id="d57f" class="le ig hi la b fi lj lg l lh li">def get_config():<br/>    if os.environ['HOME'] == '/home/pi':<br/>        # read config json file<br/>        with open('conf/config_prod.json', 'r') as myfile:<br/>            data = myfile.read()<br/>    else:<br/>        # read config json file<br/>        with open('conf/config_dev.json', 'r') as myfile:<br/>            data = myfile.read()<br/>    <br/>    return json.loads(data)</span><span id="4c41" class="le ig hi la b fi lj lg l lh li">def pandas_to_hive_dtype_converter(pandas_dtype):<br/>    <em class="kx">""" Convert pandas datatype to hive sql datatype """<br/>    </em>with open('conf/pd_to_hive_dtype_map.json', 'r') as myfile:<br/>        dtype_map = myfile.read()</span><span id="6b43" class="le ig hi la b fi lj lg l lh li">    converter = json.loads(dtype_map)</span><span id="0f4b" class="le ig hi la b fi lj lg l lh li">    return converter[pandas_dtype]</span><span id="2de9" class="le ig hi la b fi lj lg l lh li">@click.command()<br/>@click.option("--table_name", prompt="Hive table name", help="Specify the destination Hive table that you want to import csv")<br/>@click.option("--csv_name", prompt="Csv name", help="The specify the storage directory.")<br/>@click.option("--csv_dir", default="Hive_ETL/data/", help="The CSV file source directory.")<br/>def load_csv_to_hive(table_name, csv_name, csv_dir):<br/>    config = get_config()</span><span id="b05c" class="le ig hi la b fi lj lg l lh li">    '''' <br/>        convert csv information to hive sql syntax<br/>    '''<br/>    # get csv summary to form sql command<br/>    csv_info = Csv_summerizer(f"{config['data_folder']}{csv_name}")<br/>    name_dtype_dic = csv_info.get_columns_datatype()</span><span id="045b" class="le ig hi la b fi lj lg l lh li">    hive_table_column_type_query = ''<br/>    for column_name, dtype in name_dtype_dic.items():<br/>        hive_table_column_type_query = hive_table_column_type_query + '`' + column_name + '`' + ' ' + pandas_to_hive_dtype_converter(dtype) + ', '</span><span id="2874" class="le ig hi la b fi lj lg l lh li">    hive_table_column_type_query = hive_table_column_type_query[:-2]</span><span id="509a" class="le ig hi la b fi lj lg l lh li">    #<em class="kx">TODO: change way to fix first row (column name) import to hive problem<br/>    </em>csv_df = pd.read_csv(f"../data/{csv_name}")<br/>    csv_df.to_csv(f"../data/{csv_name}", header=False, index=False)</span><span id="b7e1" class="le ig hi la b fi lj lg l lh li">    ''' <br/>        connect to hive with pyhive<br/>    '''<br/>    conn = hive.Connection(host=config['host_name'], <br/>                            port=config['port'], username=config['user'], <br/>                            password=config['password'], <br/>                            database=config['database'],<br/>                            auth='CUSTOM')<br/>    cur = conn.cursor()<br/>    cur.execute(f"CREATE TABLE IF NOT EXISTS {table_name} ({hive_table_column_type_query}) row format delimited fields terminated by ','")<br/>    cur.execute(f"LOAD DATA LOCAL INPATH '{csv_dir}{csv_name}' OVERWRITE INTO TABLE {table_name}")<br/>    print('data loaded')<br/>    cur.execute(f"select * from {table_name}")<br/>    result = cur.fetchall()<br/>    print(result)<br/>    </span><span id="5a82" class="le ig hi la b fi lj lg l lh li">if __name__ == '__main__':<br/>    load_csv_to_hive()</span></pre><h1 id="2d15" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">6.演示视频</h1><p id="f2cf" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们使用SQuirrel SQL来管理我们的Hive数据库。下面是从提取数据到加载数据为Hive表的整个过程的视频演示。</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="ll lm l"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">Kaggle ETL工具视频演示</figcaption></figure><h1 id="d6f6" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结论</h1><p id="eb54" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">最终，我们成功地构建了Kaggle数据ETL工具的第一个版本。这个过程非常具有挑战性；然而，我们确实从学习这个新东西中获得了很多乐趣，并使它成为一个有用的工具。然而，我们对这个工具有更多的期待，并将在未来继续改进它。希望你们喜欢阅读这篇文章，并获得一些见解！</p></div></div>    
</body>
</html>