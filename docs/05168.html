<html>
<head>
<title>Super-Resolution Using Autoencoders and TF2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自动编码器和TF2.0的超分辨率</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/super-resolution-using-autoencoders-and-tf2-0-505215c1674?source=collection_archive---------7-----------------------#2020-04-13">https://medium.com/analytics-vidhya/super-resolution-using-autoencoders-and-tf2-0-505215c1674?source=collection_archive---------7-----------------------#2020-04-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/568e81f77dad7cbb8da9643c0859fcb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VYH3i2-2CZ6Fyd7Bv9UHFw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:<a class="ae iu" href="https://www.researchgate.net/figure/The-structure-of-proposed-Convolutional-AutoEncoders-CAE-for-MNIST-In-the-middle-there_fig1_320658590" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/figure/The-structure-of-proposed-convolutionary-auto encoders-CAE-for-The-middle-The-there _ fig 1 _ 320658590</a></figcaption></figure><p id="9005" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过这篇文章，我将展示如何制作一个自动编码器网络，可以将图像超分辨率提高到4倍，也可以将图像超分辨率提高到10倍，甚至更高。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="6fd7" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">为什么要使用自动编码器？</h1><p id="502b" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">你可能有很多原因想要使用这种类型的网络，其中一个例子是在聊天应用程序中传输图像。像WhatsApp和Instagram这样的聊天应用程序使用这种类型的压缩，尽管更复杂，训练时间更长，数据量更多，但他们使用这种压缩是因为由于图像的压缩，上传的数据量下降，使得上传速度上升，并且存储在服务器中的数据量下降，而图像质量没有任何损失。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="7ab8" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">链接到笔记本</h1><div class="ld le ez fb lf lg"><a href="https://colab.research.google.com/drive/1YXy_9B4h-NC7KU_40aP5U2SgVCVw4H_D" rel="noopener  ugc nofollow" target="_blank"><div class="lh ab dw"><div class="li ab lj cl cj lk"><h2 class="bd hj fi z dy ll ea eb lm ed ef hh bi translated">谷歌联合实验室</h2><div class="ln l"><h3 class="bd b fi z dy ll ea eb lm ed ef dx translated">编辑描述</h3></div><div class="lo l"><p class="bd b fp z dy ll ea eb lm ed ef dx translated">colab.research.google.com</p></div></div><div class="lp l"><div class="lq l lr ls lt lp lu io lg"/></div></div></a></div><p id="cdb5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我正在链接一个Colab笔记本，因为这个项目是<a class="ae iu" href="https://colab.research.google.com/drive/1YXy_9B4h-NC7KU_40aP5U2SgVCVw4H_D" rel="noopener ugc nofollow" target="_blank">在这里</a>。我将使用谷歌Colab，因为他们为培训提供免费的云GPU，使得对这些任务的高功率机器的需求变得无用。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="f044" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">概观</h1><p id="9d2d" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">首先，我将带您逐个构建网络的编码器部分，然后制作编码器和解码器，连接它并用自定义数据对其进行训练，在整个过程中，我还将解释自动编码器神经网络及其改进方法。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="4627" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">要导入的库</h1><p id="22f2" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">我们将使用<strong class="ix hj"> Tensorflow 2.0 </strong>构建网络，使用U <strong class="ix hj"> rllib </strong>模块下载图像，使用<strong class="ix hj"> Numpy </strong>处理图像数组，使用<strong class="ix hj"> Open-Cv2 </strong>调整图像大小，使用<strong class="ix hj"> Os </strong>模块加载图像，使用<strong class="ix hj"> Matplotlib </strong>显示图像。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="af22" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">什么是自动编码器？</h1><p id="ff28" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">自动编码器只是一个无监督的神经网络，它通过设计学习如何减少数据并尽可能少地丢失数据来重建数据。</p><p id="1d49" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可能已经猜到了，它由一个编码器和一个解码器组成。编码器的作用是将数据压缩到一个较低的维度，以便只有最有影响的特征像PCA一样保留下来，而解码器的作用是从较低维度的表示中重建数据，使其尽可能接近原始数据。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="0508" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">构建编码器</h1><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="5faa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们有2个Conv2D模块，因为我们的图像只有256像素的高度和宽度。然而，当使用1920 X 1080像素的图像时，我建议使用4个以上的Conv2D模块，滤波器增加2倍。这将确保有足够的参数可以改变，并且有足够的过滤器最终可以用于将图像超分辨率提高到10倍。</p><p id="8e9d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是模型总结。</p><pre class="lv lw lx ly fd mb mc md me aw mf bi"><span id="a063" class="mg kb hi mc b fi mh mi l mj mk">Model: "model" _________________________________________________________________ Layer (type)                 Output Shape              Param #    ================================================================= input_2 (InputLayer)         [(None, 256, 256, 3)]     0          _________________________________________________________________ conv2d_5 (Conv2D)            (None, 256, 256, 64)      1792       _________________________________________________________________ conv2d_6 (Conv2D)            (None, 256, 256, 64)      36928      _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 128, 128, 64)      0          _________________________________________________________________ conv2d_7 (Conv2D)            (None, 128, 128, 128)     73856      _________________________________________________________________ conv2d_8 (Conv2D)            (None, 128, 128, 128)     147584     _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 64, 64, 128)       0          _________________________________________________________________ conv2d_9 (Conv2D)            (None, 64, 64, 256)       295168     ================================================================= Total params: 555,328<br/>Trainable params: 555,328 <br/>Non-trainable params: 0 _________________________________________________________________</span></pre><p id="add7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如你所看到的，我们有256个过滤器，我们可以停在128或64，但我们没有太多的图像来训练，而且图像的实际尺寸只有256X 256，因此，我们必须通过使用大量的过滤器来补偿它。虽然在现实生活中，图像将是1920 X 1080，因此压缩到192 X 108将更加明显。</p><p id="79a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">图像以更好地理解编码器的流程</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/12edcd0619a9154375323a18cb741a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*XEV1KsKANbjJ5Z_X3XwOCA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">编码器网络</figcaption></figure></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="27d3" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">整个模型</h1><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="a26d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如您所见，我们使用的是上采样而不是Conv2D转置，这是因为Conv2D转置具有可训练的参数，同时有利于我们的任务，它增加了训练时间和我们拥有的少量数据，它不是必需的，但肯定对于生产级模型，Conv2D转置层而不是上采样2D层是必须的。</p><p id="2590" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是总结，</p><pre class="lv lw lx ly fd mb mc md me aw mf bi"><span id="a0d7" class="mg kb hi mc b fi mh mi l mj mk">Model: "model_15" _______________________________________________________________<br/>Layer (type)                    Output Shape         Param #<br/>===============================================================<br/>input_21 (InputLayer)           [(None, 256, 256, 3)   0<br/>________________________________________________________________ conv2d_141 (Conv2D)             (None, 256, 256, 64)  1792          ________________________________________________________________ conv2d_142 (Conv2D)             (None, 256, 256, 64)  36928                 ________________________________________________________________ max_pooling2d_36 (MaxPooling2D) (None, 128, 128, 64)   0           ________________________________________________________________ conv2d_143 (Conv2D)             (None, 128, 128, 128  73856           ________________________________________________________________ conv2d_144 (Conv2D)             (None, 128, 128, 128  147584                   ________________________________________________________________ max_pooling2d_37 (MaxPooling2D) (None, 64, 64, 128)    0                       ________________________________________________________________ conv2d_145 (Conv2D)             (None, 64, 64, 256)   295168              ________________________________________________________________ up_sampling2d_21 (UpSampling2D) (None, 128, 128, 256   0                        ________________________________________________________________ conv2d_146 (Conv2D)             (None, 128, 128, 128  295040               ________________________________________________________________ conv2d_147 (Conv2D)             (None, 128, 128, 128  147584                   ________________________________________________________________ add_20 (Add)                    (None, 128, 128, 128   0           ________________________________________________________________ up_sampling2d_22 (UpSampling2D) (None, 256, 256, 128   0                 ________________________________________________________________ conv2d_148 (Conv2D)             (None, 256, 256, 64)  73792               ________________________________________________________________ conv2d_149 (Conv2D)             (None, 256, 256, 64)  36928                      ________________________________________________________________ add_21 (Add)                    (None, 256, 256, 64)   0                      ________________________________________________________________ conv2d_150 (Conv2D)             (None, 256, 256, 3)   1731                   ================================================================ Total params: 1,110,403 <br/>Trainable params: 1,110,403 <br/>Non-trainable params: 0 ________________________________________________________________</span></pre><p id="ce72" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可能已经注意到，我们使用编码器和解码器之间的残差连接，这是为了减少最终重建图像的有损结果，根据您的训练数据量，这可能需要也可能不需要，使其成为可选的。</p><p id="40be" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">剩余连接在此图中更加明显:</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/52b3b1f41f58d4b1ad495fc7c08f95a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*PI_S9eFtjmjFcAUrDYHV6w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">自动编码器网络</figcaption></figure><p id="270c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可能已经注意到，我们使用均方误差作为损失函数，这可能是最差的，但也是最快的，这就是我在这里使用它的原因。</p><p id="c04f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不过我建议不要用于生产，而是使用<a class="ae iu" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">永久损失</a>，它比MSE好得多。解释损失的研究论文可以在<a class="ae iu" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="6412" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">获取数据和训练方法</h1><p id="3abc" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">我已经使用Urllib从image net中检索了汽车图像，然后使用OpenCV将图像大小调整为其大小的1/4。使用丢失大量数据的缩放图像作为输入，使用超高分辨率原始图像作为损失比较图像。</p><p id="1425" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过这种方式，模型将慢慢了解如何通过优化重建图像和原始图像之间的损失，将图像超分辨率为其原始分辨率的4倍。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="fcfa" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">决赛成绩</h1><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/10d73ef4ec65e246946f08e6b764b906.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*N3OrTRbENme50kMFj8oGyg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">输入图像</figcaption></figure><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/baa6954c9cd2b1308e57981a74001996.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*F9GTK-F5oAz3JIrzHvlj1w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">重现像</figcaption></figure><p id="5ffd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">如你所见，我们的自动编码器做得非常出色！</strong></p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="1499" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">需要改进的地方</h1><p id="aab5" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">以下是你可以用来将超分辨率提高到10倍甚至更多的方法！</p><ol class=""><li id="2bc7" class="mo mp hi ix b iy iz jc jd jg mq jk mr jo ms js mt mu mv mw bi translated">最重要的一点将是使用全高清图像来训练模型，也使用超过5000张图像。</li><li id="1304" class="mo mp hi ix b iy mx jc my jg mz jk na jo nb js mt mu mv mw bi translated">使用Conv2D转置图层，而不是放大图层。</li><li id="c195" class="mo mp hi ix b iy mx jc my jg mz jk na jo nb js mt mu mv mw bi translated">用<a class="ae iu" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">感知损失</a>代替MSE。</li><li id="0b9b" class="mo mp hi ix b iy mx jc my jg mz jk na jo nb js mt mu mv mw bi translated">使用Adam optimizer而不是Adadelta。</li><li id="b93d" class="mo mp hi ix b iy mx jc my jg mz jk na jo nb js mt mu mv mw bi translated">使用具有动态正则化的<a class="ae iu" href="https://arxiv.org/abs/1909.11862" rel="noopener ugc nofollow" target="_blank">卷积层</a>。</li></ol></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="c54a" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">注意</h1><p id="55cb" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">这是我第一次写文章和解释一些东西，所以请让我知道我可以改进的方法，如果你有不明白的地方也告诉我。我非常乐意帮忙。此外，对代码的深入解释可以在这里找到-<a class="ae iu" href="https://www.coursera.org/learn/image-super-resolution-autoencoders-keras/" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/image-super-resolution-auto encoders-keras/</a></p></div></div>    
</body>
</html>