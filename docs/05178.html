<html>
<head>
<title>Effect of Batch Size on Training Process and results by Gradient Accumulation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度累积下批量大小对训练过程和结果的影响</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/effect-of-batch-size-on-training-process-and-results-by-gradient-accumulation-e7252ee2cb3f?source=collection_archive---------17-----------------------#2020-04-13">https://medium.com/analytics-vidhya/effect-of-batch-size-on-training-process-and-results-by-gradient-accumulation-e7252ee2cb3f?source=collection_archive---------17-----------------------#2020-04-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="276e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本实验中，我们研究了批量大小和梯度累积对训练和测试准确性的影响。</p><p id="6395" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们研究了图像分类背景下的批量大小，采用MNIST数据集进行实验。众所周知，在机器学习社区中，很难对超参数的影响做出一般性的陈述，因为行为通常会因数据集和模型而异。因此，我们得出的结论只能作为一个路标，而不是关于批量的一般陈述。</p><p id="ab07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批量大小是现代深度学习系统中需要调整的重要超参数之一。从业者通常希望使用更大的批量来训练他们的模型，因为它允许GPU的并行性加速计算。然而，众所周知，批量太大会导致泛化能力差。另一方面，经验表明，使用较小的批量可以更快地收敛到好的解决方案，因为它允许模型在看到所有数据之前就开始学习。但是不利的一面是，这个模型不能保证收敛到全局最优。</p><p id="d0d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人们普遍认为，在一个和整个训练数据集之间存在一个最佳批量，这将提供最佳的泛化能力，从而实现更高的精度，它通常取决于数据集和所讨论的模型。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="7aff" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">实验:</h1><p id="7d53" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">我们的实验使用卷积神经网络(CNN)将MNIST数据集中的图像(包含手写数字0到9的图像)分类到相应的数字标签“0”到“9”。下图显示了手写数字的图像。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/c2489aba1b1837093c5124fab496827c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRLzfkgjkwHDhX_L9ASwsA.png"/></div></div></figure><p id="7192" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">互联网上最著名的MNIST分类器达到了99.8%的准确率！！太神奇了。最好的Kaggle核MNIST分类器达到99.75%。这个实验演示了确定用于训练模型的任何分类器的最佳批量大小的研究。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="611d" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">建筑亮点:</h1><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es kz"><img src="../Images/9bed208c582cb03f48566acdb69b3fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*t8gVBuO4gjDQ9YfEMBLJPw.png"/></div></figure><p id="b51c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们训练了6个不同的模型，每个模型具有32、64、128、256、512和1024个样本的不同批量，保持所有模型中所有其他超参数相同。然后，我们分析了每个模型架构的验证准确性。</p><p id="5166" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该笔记本在<a class="ae la" href="https://github.com/nainci/MNIST-BatchSize-Experiment/blob/master/mnist-digit-base-batchsize-test.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上有售。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="b8ba" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结果:</h1><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es lb"><img src="../Images/e387df63b4bdd4e36b348d28d55cfa44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*2lLJ3WBQ8YKMuDRh9o1fbQ.png"/></div></figure><p id="7d32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果解释了根据图图例以不同颜色显示的不同批量的曲线。x轴上是时期的数量，在本实验中取为“20”，y轴表示训练精度图。</p><p id="0d2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从实验中可以明显看出，准确度随着批量的增加而提高，然而，实验中第二大批量512的批量与最大批量的准确度相差很小。因此，它可以作为最佳样本大小，以便考虑计算资源的最佳利用和较低的复杂性。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="de2b" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">梯度累积:</h1><p id="2551" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">可以克服使用较小批量来训练模型的低存储器GPU约束的策略是梯度累积。</p><p id="2dae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度累积是一种将用于训练神经网络的一批样本分成几个小批样本的机制，这些小批样本将按顺序运行。</p><p id="19a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度累积的工作方式是，在网络反向传播过程中，参数不会在小批量的每一步中更新，而是累积梯度结果。在完成小批量的所有步骤时，小批量的所有先前步骤的累积梯度更新模型参数。这个过程与使用较高的批量大小来训练网络一样好，因为梯度被更新相同的次数。</p><p id="0317" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在给定的代码中，optimizer在累积了8批批量大小为128的梯度后步进，这与使用128*8 = 1024的批量大小产生了相同的净效果。</p><p id="dbb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要记住的一件事是BatchNorm层的性质，它仍将在每批中起作用。您需要用GroupNorm图层替换它们，以便在执行渐变累积时有效。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="963e" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">实验代码:</h1><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lc"><img src="../Images/b229bd2419a0310ec0fd61250e3b1232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VODWhpgifQE1DDePG9mv6w.png"/></div></div></figure></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="6673" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">结果:</h1><p id="33e3" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">在提交给Kaggle的DigitRecognizer竞赛时，我们获得了<a class="ae la" href="https://www.kaggle.com/ncyjain/mnist-digit-base" rel="noopener ugc nofollow" target="_blank"> 99.257% </a>的分数，批量为128。采用步长为8的梯度累积后，精度提高到<a class="ae la" href="https://www.kaggle.com/ncyjain/mnist-digit-base-accumulate-gradient?scriptVersionId=31423509" rel="noopener ugc nofollow" target="_blank"> 99.442% </a>。您可以点击链接查看各自的笔记本。</p><p id="f8b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">干杯！</p></div></div>    
</body>
</html>