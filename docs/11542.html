<html>
<head>
<title>Reinforcement Agent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">增强剂</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-agent-3bb7d1611a1b?source=collection_archive---------23-----------------------#2020-12-07">https://medium.com/analytics-vidhya/reinforcement-agent-3bb7d1611a1b?source=collection_archive---------23-----------------------#2020-12-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="659e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我以前的文章中，我已经给出了关于强化学习及其一些算法的简要细节。</p><p id="517a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">今天，我要告诉你一些简单的解释，关于强化代理如何在一个从未互动过的环境中工作。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/33b9c702fc22f00412e12e6021d67aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Do-wfpoMR64LvMqi7_qUg.jpeg"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae jp" href="https://unsplash.com/@danielkcheung?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">张家瑜</a>在<a class="ae jp" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="8661" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强化智能体当在一个<strong class="ih hj"><em class="jq"/></strong><strong class="ih hj"><em class="jq">环境</em> </strong>内行动时，它学习到<strong class="ih hj"><em class="jq"/></strong>的最优行为来实现价值函数以获得奖励。代理的行为将由策略控制。首先，一个带有输入值的代理链接。然后它改变数据集的表示。这将有助于代理识别硬输入表示，并且这将有助于比初始输入值更进一步。</p></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><p id="11d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个代理都有<strong class="ih hj"> <em class="jq">工作记忆</em> </strong>。其中一些主要可以分为两个部分。这些记忆与记忆细胞一致。第一个木桶是<strong class="ih hj"> <em class="jq">知觉缓冲</em> </strong>。它获得原始向量输入。<strong class="ih hj"> <em class="jq">参加缓冲器</em> </strong>是第二个，开始时为空。最后，第三个缓冲器保存初始输入的副本，称为<strong class="ih hj"> <em class="jq">忽略缓冲器</em> </strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jy"><img src="../Images/b5acf5ff2b895df088357641be10b80a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6WKaooYNbNq3RcxKwTmDtA.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae jp" href="https://unsplash.com/@brett_jordan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">布雷特·乔丹</a>在<a class="ae jp" href="https://unsplash.com/s/photos/robot?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="a8f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理以两种方式处理这些购物篮。为了获得输入的三个副本，代理复制了bucket 2中的初始输入。否则，桶被代理删除，因为它将能够得到非常清楚的两个桶。我上一篇文章讨论的<strong class="ih hj"> <em class="jq">奖励函数</em> </strong>，不是类标签独立的。要学习价值函数，可以使用奖励函数。当考虑分类新的(看不见的)数据集时，在开始时，代理能够与它交互。然后它决定奖励的摄入是积极的还是消极的。通过与数据集交互，代理将了解模式的负值或正值。对于分类新的看不见的模式，可以使用这个值。</p></div></div>    
</body>
</html>