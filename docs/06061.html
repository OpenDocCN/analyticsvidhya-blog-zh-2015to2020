<html>
<head>
<title>K-Nearest Neighbors algorithm (KNN) With Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Python的k近邻算法(KNN)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbors-algorithm-knn-with-python-e570f6bb8aed?source=collection_archive---------18-----------------------#2020-05-10">https://medium.com/analytics-vidhya/k-nearest-neighbors-algorithm-knn-with-python-e570f6bb8aed?source=collection_archive---------18-----------------------#2020-05-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/ad1da4b25a7af018f5e935bd4451d441.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EWe5iOQaFBGdH1qRk8jVUA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">k近邻</figcaption></figure><div class=""/><p id="4ce9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">本文包括六个部分:</p><ol class=""><li id="430a" class="js jt hx iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">什么是K近邻(K-NN)？</li><li id="1e45" class="js jt hx iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">要在K个最近邻中找到“K”？</li><li id="bb70" class="js jt hx iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">距离函数</li><li id="31e7" class="js jt hx iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">pros K-最近邻</li><li id="fa14" class="js jt hx iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">K-最近邻的缺点</li><li id="9bca" class="js jt hx iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">k-最近邻用<strong class="iw hy"> Python </strong> ( <strong class="iw hy"> <em class="kg">用代码</em> </strong>)</li></ol><p id="43c3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi kh translated"><span class="l ki kj kk bm kl km kn ko kp di">在</span>这篇文章中，我们将探索并看到著名的<strong class="iw hy">监督机器学习算法的<em class="kg">代码</em>，</strong>“K-最近邻”。这可用于解决<strong class="iw hy">分类</strong>和<strong class="iw hy">回归</strong>问题。</p><h1 id="710a" class="kq kr hx bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">什么是K-最近邻？</h1><p id="c854" class="pw-post-body-paragraph iu iv hx iw b ix lo iz ja jb lp jd je jf lq jh ji jj lr jl jm jn ls jp jq jr hb bi translated">K-最近邻是a</p><p id="7628" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">k-最近邻(kNN)是一种受监督的机器学习算法，可用于解决分类和回归任务。该算法通过计算输入样本和每个训练实例之间的相似性来进行预测。在K-NN算法中，输出是一个类成员。直观上，K总是正整数。K-最近邻算法本质上归结为在K个与给定的“看不见的”观察最相似的实例之间形成多数投票。相似性根据两个数据点之间的距离度量来定义。</p><blockquote class="lt lu lv"><p id="7b72" class="iu iv kg iw b ix iy iz ja jb jc jd je lw jg jh ji lx jk jl jm ly jo jp jq jr hb bi translated">以下两个属性可以很好地定义KNN</p></blockquote><p id="8a69" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">懒惰学习算法:- </strong> KNN是一种懒惰学习算法，因为它没有专门的训练阶段，并且在分类时使用所有的数据进行训练。</p><p id="c588" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">非参数学习算法:- </strong> KNN也是一种非参数学习算法，因为它不对底层数据做任何假设</p><h1 id="1fad" class="kq kr hx bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">如何找到最佳k值</h1><p id="c83e" class="pw-post-body-paragraph iu iv hx iw b ix lo iz ja jb lp jd je jf lq jh ji jj lr jl jm jn ls jp jq jr hb bi translated">“K”的选择对我们从KNN得到的结果有很大的影响。“K”是用于识别新数据点的相似邻居的数字。KNN算法中的“k”基于特征相似性</p><p id="3f3f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">选择正确的“K”值是一个称为参数调整的过程，对于提高精度非常重要。求“k”的值并不容易。</p><h1 id="4417" class="kq kr hx bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">一些常用的距离函数有</h1><figure class="ma mb mc md fd hk er es paragraph-image"><div class="er es lz"><img src="../Images/23b7ce95cc8b6b669b30a917213adb87.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*PAkBztaaau_QMEQ9o-4ZDg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">曼哈顿和闵可夫斯基(距离测量)</figcaption></figure><figure class="ma mb mc md fd hk er es paragraph-image"><div class="er es me"><img src="../Images/331e978269b907b050a8ac8ed6864d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*q88c7dFfrDuxFnCpy5iL7Q.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">欧几里得(距离度量)</figcaption></figure><h1 id="6d4c" class="kq kr hx bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">KNN的支持者</h1><ol class=""><li id="2122" class="js jt hx iw b ix lo jb lp jf mf jj mg jn mh jr jx jy jz ka bi translated">KNN非常容易实现。实现KNN只需要两个参数</li><li id="6535" class="js jt hx iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">由于KNN算法在进行预测之前不需要训练，<strong class="iw hy">新数据可以无缝添加</strong>，这不会影响算法的准确性。</li><li id="1acd" class="js jt hx iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated"><strong class="iw hy">无训练期:</strong> KNN被称为<strong class="iw hy">懒惰学习者(基于实例的学习)</strong>。它在训练期间什么也学不到</li></ol><h1 id="b510" class="kq kr hx bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">KNN<strong class="ak">的缺点</strong></h1><p id="cafc" class="pw-post-body-paragraph iu iv hx iw b ix lo iz ja jb lp jd je jf lq jh ji jj lr jl jm jn ls jp jq jr hb bi translated"><strong class="iw hy"> 1。它不适用于大型数据集</strong>。</p><p id="b864" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 2。</strong>KNN算法不适用于高维数据，因为在大量维的情况下，算法很难计算每个维的距离。</p><p id="3c06" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 3。需要特征缩放:</strong>在将KNN算法应用于任何数据集之前，我们需要进行特征缩放(标准化和规范化)。如果我们不这样做，KNN可能会产生错误的预测。</p><p id="f931" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 4。</strong> KNN对数据集中的噪声很敏感。我们需要手动估算缺失值并移除异常值。</p><h1 id="855b" class="kq kr hx bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">蟒蛇皮KNN</h1><div class="hh hi ez fb hj mi"><a href="https://github.com/Abhijeetap/K-Nearest_Neighbor_algorithm_with_python/blob/master/k-NN%20%28k-Nearest%20Neighbors%29.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hy fi z dy mn ea eb mo ed ef hw bi translated">abhijeetap/K-最近邻算法与python</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">github.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw hp mi"/></div></div></a></div><figure class="ma mb mc md fd hk er es paragraph-image"><div class="er es mx"><img src="../Images/f00bae264c8c0b16177b8e34a3acfa6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*PHZQ7-Vq5YKtwZAFT2yaCg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">虹膜数据集</figcaption></figure><p id="e391" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">虹膜数据目标名称</p><p id="0282" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">X=iris.values[:，0:4] <br/> Y=iris.values[:，4] <br/> x_train，x_test，y_train，y_test=train_test_split(X，Y，test_size=0.2，random_state=47)</p><figure class="ma mb mc md fd hk er es paragraph-image"><div class="er es my"><img src="../Images/282ed701fd421e98ad82e18086820de0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*Y16pxA6htsp-yXgeOQOhRw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">k-神经网络</figcaption></figure><figure class="ma mb mc md fd hk er es paragraph-image"><div class="er es mz"><img src="../Images/562178c3b37d3f1b5b5dce4d4b087e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*3vDAzJUvs59RyFgMiavdPA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">输出</figcaption></figure></div><div class="ab cl na nb gp nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="hb hc hd he hf"><p id="e4ec" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="1fcf" class="kq kr hx bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">我关于机器学习算法的其他帖子</h1><div class="hh hi ez fb hj mi"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/random-forest-algorithm-with-python-7ccfbe9bcb47"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hy fi z dy mn ea eb mo ed ef hw bi translated">用Python实现随机森林算法</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">在本文中，我们将探索著名的监督机器学习算法“随机…</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">medium.com</p></div></div><div class="mr l"><div class="nh l mt mu mv mr mw hp mi"/></div></div></a></div><div class="hh hi ez fb hj mi"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/what-is-the-support-vector-machine-svm-dc89207c011"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hy fi z dy mn ea eb mo ed ef hw bi translated">什么是支持向量机(SVM)</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">本文包括三个部分:</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">medium.com</p></div></div><div class="mr l"><div class="ni l mt mu mv mr mw hp mi"/></div></div></a></div><div class="hh hi ez fb hj mi"><a rel="noopener follow" target="_blank" href="/@abhi.pujara97/naïve-bayes-algorithm-with-python-7b3aef57fb59"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hy fi z dy mn ea eb mo ed ef hw bi translated">用Python实现朴素贝叶斯算法</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">本文包括五个部分:</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">medium.com</p></div></div><div class="mr l"><div class="nj l mt mu mv mr mw hp mi"/></div></div></a></div></div><div class="ab cl na nb gp nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="hb hc hd he hf"><p id="8ad1" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">快乐学习！！！</strong></p><p id="b2ea" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">快乐编码:)</strong></p><p id="bcfd" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">别忘了拍手拍手拍手…</p><figure class="ma mb mc md fd hk er es paragraph-image"><div class="er es nk"><img src="../Images/9dafdaa65202a7f5008c6e1960120daf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*XBWI6eNVLS_70s1qS46E1w.gif"/></div></figure></div></div>    
</body>
</html>