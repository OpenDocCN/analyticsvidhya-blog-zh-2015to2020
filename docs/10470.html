<html>
<head>
<title>Creating Your own Intent Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">åˆ›å»ºæ‚¨è‡ªå·±çš„æ„å›¾åˆ†ç±»å™¨</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/analytics-vidhya/creating-your-own-intent-classifier-b86e000a4926?source=collection_archive---------0-----------------------#2020-10-20">https://medium.com/analytics-vidhya/creating-your-own-intent-classifier-b86e000a4926?source=collection_archive---------0-----------------------#2020-10-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/613957715cd5e20a15fee88d0061e37e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*tSPpZJ5P1KeeynGNTxW5YQ.jpeg"/></div></figure><p id="42cf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">ä½œä¸ºNLPçš„ç²‰ä¸ï¼Œæˆ‘æ€»æ˜¯æƒ³çŸ¥é“å½“æˆ‘è¦æ±‚è°·æ­ŒåŠ©æ‰‹æˆ–Alexaåšä¸€äº›äº‹æƒ…æ—¶ï¼Œå®ƒæ˜¯å¦‚ä½•ç†è§£çš„ã€‚é—®é¢˜æ¥ç€æ˜¯ï¼Œæˆ‘æ˜¯å¦èƒ½è®©æˆ‘çš„æœºå™¨ä¹Ÿç†è§£æˆ‘ï¼Ÿè§£å†³æ–¹æ³•æ˜¯-æ„å›¾åˆ†ç±»ã€‚</p><blockquote class="jk jl jm"><p id="b159" class="im in jn io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated">æ„å›¾åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€ç†è§£çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ç®—æ³•å­¦ä¹ åœ¨å®ƒæ¥å—è®­ç»ƒçš„çŸ­è¯­çš„åŸºç¡€ä¸Šå¯¹ç»™å®šçŸ­è¯­è¿›è¡Œåˆ†ç±»ã€‚</p></blockquote><p id="923a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">è®©æˆ‘ä»¬ä¸¾ä¸€ä¸ªæœ‰è¶£çš„ä¾‹å­ï¼›æˆ‘åœ¨åšä¸€ä¸ªåƒAlexaä¸€æ ·çš„åŠ©æ‰‹ã€‚</p><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es jr"><img src="../Images/85e950876c98c0c32457935b682ab9da.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*-DqJRDWGgroeST2ORdHzzw.jpeg"/></div></figure><p id="0b7d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†æ‰§è¡Œ3é¡¹ä»»åŠ¡ï¼Œå³å¼€ç¯ã€å…³ç¯å’Œå‘Šè¯‰æˆ‘ä»¬å¤©æ°”å¦‚ä½•ã€‚è®©æˆ‘ä»¬ç»™è¿™ä¸‰ä¸ªä»»åŠ¡èµ·ä¸ªåå­—:å¼€ç¯ã€å…³ç¯å’Œå¤©æ°”ã€‚è¿™äº›æ‰€æœ‰çš„ä»»åŠ¡åœ¨NLUè¢«ç§°ä¸º<em class="jn">â€˜æ„å›¾â€™</em>ã€‚æ¢å¥è¯è¯´ï¼Œ<em class="jn">æ„å›¾æ˜¯ä¸€ç»„å±äºå…±åŒåç§°çš„ç›¸ä¼¼çŸ­è¯­ï¼Œè¿™æ ·æ·±åº¦å­¦ä¹ ç®—æ³•å°±å¾ˆå®¹æ˜“ç†è§£ç”¨æˆ·è¦è¯´çš„è¯</em>ã€‚æ¯ä¸ªæ„å›¾è¢«ç»™äºˆä¸€å®šæ•°é‡çš„è®­ç»ƒçŸ­è¯­ï¼Œä»¥ä¾¿å®ƒèƒ½å¤Ÿå­¦ä¹ å¯¹å®æ—¶çŸ­è¯­è¿›è¡Œåˆ†ç±»ã€‚</p><p id="fbe1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬çŸ¥é“ä»€ä¹ˆæ˜¯æ„å›¾åˆ†ç±»ï¼Œè®©æˆ‘ä»¬å¼€å§‹é…·çš„ä¸œè¥¿ï¼æˆ‘å†™äº†ä¸€ä¸ª<a class="ae jw" href="https://github.com/horizons-ml/intent-classifier/blob/main/intent_classification.ipynb" rel="noopener ugc nofollow" target="_blank">ç¬”è®°æœ¬</a>å¦‚æœä½ æƒ³è·Ÿç€æˆ‘ï¼Œä½ å¯ä»¥åœ¨æˆ‘çš„<a class="ae jw" href="https://github.com/horizons-ml/intent-classifier" rel="noopener ugc nofollow" target="_blank"> Github repo </a>è¿™é‡Œæ‰¾åˆ°ã€‚</p><p id="67e0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">ä¸ºæ–¹ä¾¿èµ·è§ï¼Œè®©æˆ‘ä»¬éµå¾ªä»¥ä¸‹ç›®å½•ç»“æ„:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="1405" class="kc kd hi jy b fi ke kf l kg kh">Your directory<br/>â”œâ”€â”€â”€models <br/>â”œâ”€â”€â”€utils<br/>â””â”€â”€â”€intent_classification.ipynb</span></pre><h2 id="6b9a" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">å®‰è£…ä¾èµ–é¡¹</h2><p id="43e7" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="4824" class="kc kd hi jy b fi ke kf l kg kh">pip install wget tensorflow==1.5 pandas numpy keras</span></pre><h2 id="b49a" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">èµ„æ–™ç»„</h2><p id="b18f" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">æˆ‘ä»¬å°†ä½¿ç”¨å…¬å¼€å¯ç”¨çš„<a class="ae jw" href="https://github.com/clinc/oos-eval" rel="noopener ugc nofollow" target="_blank"> CLINC150æ•°æ®é›†</a>ã€‚å®ƒæ”¶é›†äº†10ä¸ªé¢†åŸŸä¸­150ç§ä¸åŒæ„å›¾çš„çŸ­è¯­ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šå…³äºæ•°æ®é›†çš„ä¿¡æ¯ã€‚</p><p id="d094" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ä¸‹è½½æ•°æ®é›†:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="11f5" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">wget</strong><br/>url = 'https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_full.json'<br/>wget.download(url)</span></pre><h2 id="f1c0" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">å‡†å¤‡æ•°æ®é›†</h2><p id="7941" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">æ•°æ®é›†å·²ç»åˆ†ä¸ºâ€œè®­ç»ƒâ€ã€â€œæµ‹è¯•â€å’Œâ€œéªŒè¯â€é›†ï¼Œä½†æˆ‘ä»¬å°†åˆ›å»ºè‡ªå·±çš„è®­ç»ƒå’ŒéªŒè¯é›†ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦æµ‹è¯•é›†ã€‚æˆ‘ä»¬å°†é€šè¿‡åˆå¹¶æ‰€æœ‰é›†åˆï¼Œç„¶åä½¿ç”¨scikit-learnå°†å®ƒä»¬åˆ†æˆâ€œè®­ç»ƒâ€å’Œâ€œéªŒè¯â€é›†åˆæ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™ä¹Ÿå°†åˆ›å»ºæ›´å¤šçš„è®­ç»ƒæ•°æ®ã€‚</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="476e" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">numpy</strong> <strong class="jy hj">as</strong> <strong class="jy hj">np</strong><br/><strong class="jy hj">import</strong> <strong class="jy hj">json<br/></strong><em class="jn"># Loading json data</em><br/><strong class="jy hj">with</strong> open('data_full.json') <strong class="jy hj">as</strong> file:<br/>  data = json.loads(file.read())<br/><br/><em class="jn"># Loading out-of-scope intent data</em><br/>val_oos = np.array(data['oos_val'])<br/>train_oos = np.array(data['oos_train'])<br/>test_oos = np.array(data['oos_test'])<br/><br/><em class="jn"># Loading other intents data</em><br/>val_others = np.array(data['val'])<br/>train_others = np.array(data['train'])<br/>test_others = np.array(data['test'])<br/><br/><em class="jn"># Merging out-of-scope and other intent data</em><br/>val = np.concatenate([val_oos,val_others])<br/>train = np.concatenate([train_oos,train_others])<br/>test = np.concatenate([test_oos,test_others])</span><span id="baa8" class="kc kd hi jy b fi lg kf l kg kh">data = np.concatenate([train,test,val])<br/>data = data.T<br/><br/>text = data[0]<br/>labels = data[1]</span></pre><p id="c1fa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å†…å®¹åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ‹†åˆ†:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="c2fc" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">from</strong> <strong class="jy hj">sklearn.model_selection</strong> <strong class="jy hj">import</strong> train_test_split</span><span id="0127" class="kc kd hi jy b fi lg kf l kg kh">train_txt,test_txt,train_label,test_labels = train_test_split(text,labels,test_size = 0.3)</span></pre><h2 id="f7e0" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">æ•°æ®é›†é¢„å¤„ç†</h2><p id="bd35" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">ç”±äºæ·±åº¦å­¦ä¹ æ˜¯ä¸€ä¸ªæ•°å­—æ¸¸æˆï¼Œå®ƒå¸Œæœ›æˆ‘ä»¬çš„æ•°æ®æ˜¯æ•°å­—å½¢å¼çš„ã€‚æˆ‘ä»¬å°†æ ‡è®°æˆ‘ä»¬çš„æ•°æ®é›†ï¼›æ„æ€æ˜¯å°†å¥å­åˆ†è§£æˆä¸ªä½“ï¼Œå¹¶å°†è¿™äº›ä¸ªä½“è½¬æ¢æˆæ•°å­—è¡¨ç¤ºã€‚æˆ‘ä»¬å°†ä½¿ç”¨K <a class="ae jw" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer" rel="noopener ugc nofollow" target="_blank">æ—¶ä»£æ ‡è®°å™¨</a>æ¥æ ‡è®°æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç çš„çŸ­è¯­:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="d78e" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">from</strong> <strong class="jy hj">tensorflow.python.keras.preprocessing.text</strong> <strong class="jy hj">import</strong> Tokenizer<br/><strong class="jy hj">from</strong> <strong class="jy hj">tensorflow.python.keras.preprocessing.sequence</strong> <strong class="jy hj">import</strong> pad_sequences</span><span id="37f3" class="kc kd hi jy b fi lg kf l kg kh">max_num_words = 40000<br/>classes = np.unique(labels)<br/><br/>tokenizer = Tokenizer(num_words=max_num_words)<br/>tokenizer.fit_on_texts(train_txt)<br/>word_index = tokenizer.word_index</span></pre><p id="f925" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">ä¸ºäº†å°†æˆ‘ä»¬çš„æ•°æ®è¾“å…¥æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬æ‰€æœ‰çš„çŸ­è¯­å¿…é¡»é•¿åº¦ç›¸åŒã€‚æˆ‘ä»¬å°†ç”¨<em class="jn"> 0 </em>å¡«å……æ‰€æœ‰çš„è®­ç»ƒçŸ­è¯­ï¼Œè¿™æ ·å®ƒä»¬çš„é•¿åº¦å°±ç›¸åŒäº†ã€‚</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="da3a" class="kc kd hi jy b fi ke kf l kg kh">ls=[]<br/><strong class="jy hj">for</strong> c <strong class="jy hj">in</strong> train_txt:<br/>    ls.append(len(c.split()))<br/>maxLen=int(np.percentile(ls, 98))</span><span id="ea4a" class="kc kd hi jy b fi lg kf l kg kh">train_sequences = tokenizer.texts_to_sequences(train_txt)<br/>train_sequences = pad_sequences(train_sequences, maxlen=maxLen,              padding='post')</span><span id="6a98" class="kc kd hi jy b fi lg kf l kg kh">test_sequences = tokenizer.texts_to_sequences(test_txt)<br/>test_sequences = pad_sequences(test_sequences, maxlen=maxLen, padding='post')</span></pre><p id="cce0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†æ ‡ç­¾è½¬æ¢æˆç‹¬çƒ­ç¼–ç å½¢å¼ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šå…³äºä¸€é”®ç¼–ç <a class="ae jw" href="https://victorzhou.com/blog/one-hot/" rel="noopener ugc nofollow" target="_blank">çš„ä¿¡æ¯ã€‚</a></p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="9249" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">from</strong> <strong class="jy hj">sklearn.preprocessing</strong> <strong class="jy hj">import</strong> OneHotEncoder,LabelEncoder<br/><br/>label_encoder = LabelEncoder()<br/>integer_encoded = label_encoder.fit_transform(classes)<br/><br/>onehot_encoder = OneHotEncoder(sparse=<strong class="jy hj">False</strong>)<br/>integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)<br/>onehot_encoder.fit(integer_encoded)</span><span id="180b" class="kc kd hi jy b fi lg kf l kg kh">train_label_encoded = label_encoder.transform(train_label)<br/>train_label_encoded = train_label_encoded.reshape(len(train_label_encoded), 1)<br/>train_label = onehot_encoder.transform(train_label_encoded)</span><span id="cf3d" class="kc kd hi jy b fi lg kf l kg kh">test_labels_encoded = label_encoder.transform(test_labels)<br/>test_labels_encoded = test_labels_encoded.reshape(len(test_labels_encoded), 1)<br/>test_labels = onehot_encoder.transform(test_labels_encoded)</span></pre><h2 id="1ad4" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">åœ¨æˆ‘ä»¬åˆ›å»ºæ¨¡å‹ä¹‹å‰..</h2><p id="4f59" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">åœ¨æˆ‘ä»¬å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<a class="ae jw" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">å…¨å±€å‘é‡</a>ã€‚æ‰‹å¥—æ˜¯ç”±æ–¯å¦ç¦å¤§å­¦åœ¨å¤§å‹è¯­æ–™åº“ä¸Šè®­ç»ƒçš„å•è¯çš„Nç»´å‘é‡è¡¨ç¤ºã€‚ç”±äºå®ƒæ˜¯åœ¨å¤§å‹è¯­æ–™åº“ä¸Šè®­ç»ƒçš„ï¼Œå®ƒå°†å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ çŸ­è¯­ã€‚</p><p id="54e6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ä¸‹è½½GloVe:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="a23d" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">wget</strong><br/>url ='https://www.dropbox.com/s/a247ju2qsczh0be/glove.6B.100d.txt?dl=1'<br/>wget.download(url)</span></pre><p id="2d40" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">ä¸‹è½½å®Œæˆåï¼Œæˆ‘ä»¬ä¼šå°†å…¶å­˜å‚¨åœ¨Pythonå­—å…¸ä¸­:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="ad96" class="kc kd hi jy b fi ke kf l kg kh">embeddings_index={}<br/><strong class="jy hj">with</strong> open('glove.6B.100d.txt', encoding='utf8') <strong class="jy hj">as</strong> f:<br/>    <strong class="jy hj">for</strong> line <strong class="jy hj">in</strong> f:<br/>        values = line.split()<br/>        word = values[0]<br/>        coefs = np.asarray(values[1:], dtype='float32')<br/>        embeddings_index[word] = coefs</span></pre><p id="c970" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">å› ä¸ºGloVeåŒ…å«æ¥è‡ªå¤§å‹è¯­æ–™åº“çš„æ‰€æœ‰å•è¯çš„å‘é‡è¡¨ç¤ºï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦é‚£äº›å‡ºç°åœ¨è¯­æ–™åº“ä¸­çš„å•è¯å‘é‡ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåµŒå…¥çŸ©é˜µï¼Œå®ƒåªåŒ…å«æ•°æ®é›†ä¸­å‡ºç°çš„å•è¯çš„å‘é‡è¡¨ç¤ºã€‚å› ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†å·²ç»è¢«æ ‡è®°åŒ–äº†ï¼Œæ‰€ä»¥Kerasæ ‡è®°åŒ–å™¨ä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ‡è®°åˆ†é…äº†ä¸€ä¸ªå”¯ä¸€çš„ç¼–å·ã€‚è¿™ä¸ªå”¯ä¸€çš„æ•°å­—å¯ä»¥è¢«è®¤ä¸ºæ˜¯åµŒå…¥çŸ©é˜µä¸­æ¯ä¸ªå•è¯çš„å‘é‡çš„ç´¢å¼•ï¼›è¿™æ„å‘³ç€æ¥è‡ªè®°å·èµ‹äºˆå™¨çš„æ¯ä¸ªç¬¬<em class="jn">n</em>ä¸ªå­—ç”±åµŒå…¥çŸ©é˜µä¸­ç¬¬<em class="jn">n</em>ä¸ªä½ç½®å¤„çš„å‘é‡è¡¨ç¤ºã€‚</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="fdee" class="kc kd hi jy b fi ke kf l kg kh">all_embs = np.stack(embeddings_index.values())<br/>emb_mean,emb_std = all_embs.mean(), all_embs.std()</span><span id="3ad4" class="kc kd hi jy b fi lg kf l kg kh">num_words = min(max_num_words, len(word_index))+1</span><span id="cab6" class="kc kd hi jy b fi lg kf l kg kh">embedding_dim=len(embeddings_index['the'])</span><span id="b37c" class="kc kd hi jy b fi lg kf l kg kh">embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))<br/><strong class="jy hj">for</strong> word, i <strong class="jy hj">in</strong> word_index.items():<br/>    <strong class="jy hj">if</strong> i &gt;= max_num_words:<br/>        <strong class="jy hj">break</strong><br/>    embedding_vector = embeddings_index.get(word)<br/>    <strong class="jy hj">if</strong> embedding_vector <strong class="jy hj">is</strong> <strong class="jy hj">not</strong> <strong class="jy hj">None</strong>:<br/>        embedding_matrix[i] = embedding_vector</span></pre><h2 id="c0f7" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">æ¨¡å‹å‡†å¤‡</h2><p id="edd9" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">è®©æˆ‘ä»¬æŠŠæˆ‘ä»¬çš„æ¨¡å‹çš„æ¶æ„æ”¾åœ¨ä¸€èµ·ï¼Œçœ‹çœ‹æ¨¡å‹çš„è¿è¡Œæƒ…å†µã€‚</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="5219" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">from</strong> <strong class="jy hj">tensorflow.python.keras.models</strong> <strong class="jy hj">import</strong> Sequential<br/><strong class="jy hj">from</strong> <strong class="jy hj">tensorflow.python.keras.layers</strong> <strong class="jy hj">import</strong> Dense, Input, Dropout, LSTM, Activation, Bidirectional,Embedding</span><span id="8626" class="kc kd hi jy b fi lg kf l kg kh">model = Sequential()<br/><br/>model.add(Embedding(num_words, 100, trainable=<strong class="jy hj">False</strong>,input_length=train_sequences.shape[1], weights=[embedding_matrix]))<br/>model.add(Bidirectional(LSTM(256, return_sequences=<strong class="jy hj">True</strong>, recurrent_dropout=0.1, dropout=0.1), 'concat'))<br/>model.add(Dropout(0.3))<br/>model.add(LSTM(256, return_sequences=<strong class="jy hj">False</strong>, recurrent_dropout=0.1, dropout=0.1))<br/>model.add(Dropout(0.3))<br/>model.add(Dense(50, activation='relu'))<br/>model.add(Dropout(0.3))<br/>model.add(Dense(classes.shape[0], activation='softmax'))</span><span id="9671" class="kc kd hi jy b fi lg kf l kg kh">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])</span></pre><p id="ad59" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">æˆ‘ä»¬å°†åœ¨åµŒå…¥å±‚ä¸­ä¼ é€’åµŒå…¥çŸ©é˜µä½œä¸º<em class="jn">æƒé‡ã€‚</em></p><h2 id="f023" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">æ¨¡ç‰¹åŸ¹è®­</h2><p id="9683" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">æœ€åæ˜¯è®­ç»ƒæ¨¡å‹çš„æ—¶é—´ã€‚</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="bd66" class="kc kd hi jy b fi ke kf l kg kh">history = model.fit(train_sequences, train_label, epochs = 20,<br/>          batch_size = 64, shuffle=<strong class="jy hj">True</strong>,<br/>          validation_data=[test_sequences, test_labels])</span></pre><p id="65ee" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">è¿™å¤§çº¦éœ€è¦ä¸€ä¸ªå°æ—¶å·¦å³ï¼Œå–å†³äºæ‚¨çš„æœºå™¨ã€‚åŸ¹è®­å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æŒ‡æ ‡æƒ³è±¡ä¸º:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="ceb9" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">matplotlib.pyplot</strong> <strong class="jy hj">as</strong> <strong class="jy hj">plt</strong><br/>%matplotlib inline</span><span id="ebd3" class="kc kd hi jy b fi lg kf l kg kh">plt.plot(history.history['acc'])<br/>plt.plot(history.history['val_acc'])<br/>plt.title('Model Accuracy')<br/>plt.ylabel('Accuracy')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Train', 'Validation'], loc='upper left')<br/>plt.show()</span></pre><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/1a44b0bdebdb86af58725b5f9539754d.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*GqMmGQ7euG2w1jAC08oxMg.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">æ¨¡å‹ç²¾åº¦æ›²çº¿</figcaption></figure><p id="3e9e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Wohooï¼ï¼æˆ‘ä»¬å¾—åˆ°äº†92.45%çš„è®­ç»ƒå‡†ç¡®ç‡å’Œ88.86%çš„éªŒè¯å‡†ç¡®ç‡ï¼Œè¿™æ˜¯ç›¸å½“ä¸é”™çš„ã€‚</p><p id="d663" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">è¿™æ˜¯æŸå¤±æ›²çº¿:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="f546" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">matplotlib.pyplot</strong> <strong class="jy hj">as</strong> <strong class="jy hj">plt</strong><br/>%matplotlib inline</span><span id="8fca" class="kc kd hi jy b fi lg kf l kg kh">plt.plot(history.history['loss'])<br/>plt.plot(history.history['val_loss'])<br/>plt.title('Model Loss')<br/>plt.ylabel('Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Train', 'Validation'], loc='upper left')<br/>plt.show()</span></pre><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/95166bff41235dcdbfbc92d8babf1335.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*2Cx2fbsH3hlyWfSTmr93PA.png"/></div></figure><p id="b663" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">åŸ¹è®­æŸå¤±çº¦ä¸º0.2ï¼ŒéªŒè¯æŸå¤±çº¦ä¸º0.5ã€‚æ‚¨å¯ä»¥å°è¯•æ¨¡å‹æ¶æ„ï¼Œçœ‹çœ‹æŸå¤±æ˜¯å¦ä¼šä¸‹é™<a class="ae jw" href="https://emojipedia.org/winking-face/" rel="noopener ugc nofollow" target="_blank">ğŸ˜‰</a></p><h2 id="20b5" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">ä¿å­˜æ¨¡å‹ã€æ ‡è®°å™¨ã€æ ‡ç­¾ç¼–ç å™¨å’Œæ ‡ç­¾</h2><p id="0982" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">è®©æˆ‘ä»¬ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ã€æ ‡è®°å™¨ã€æ ‡ç­¾ç¼–ç å™¨å’Œæ ‡ç­¾ï¼Œä»¥ä¾¿åœ¨å°†æ¥çš„æ¡ˆä¾‹ä¸­ä½¿ç”¨å®ƒä»¬ã€‚</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="7496" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">pickle</strong><br/><strong class="jy hj">import</strong> <strong class="jy hj">json</strong></span><span id="91ed" class="kc kd hi jy b fi lg kf l kg kh">model.save('models/intents.h5')<br/><br/><strong class="jy hj">with</strong> open('utils/classes.pkl','wb') <strong class="jy hj">as</strong> file:<br/>   pickle.dump(classes,file)<br/><br/><strong class="jy hj">with</strong> open('utils/tokenizer.pkl','wb') <strong class="jy hj">as</strong> file:<br/>   pickle.dump(tokenizer,file)<br/><br/><strong class="jy hj">with</strong> open('utils/label_encoder.pkl','wb') <strong class="jy hj">as</strong> file:<br/>   pickle.dump(label_encoder,file)</span></pre><h2 id="1b70" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">æ˜¯æ—¶å€™çœ‹çœ‹æ‰€æœ‰çš„æ´»åŠ¨äº†</h2><p id="cf1b" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">æˆ‘ä»¬ç»å†äº†æ¼«é•¿çš„æ—…ç¨‹..è®©æˆ‘ä»¬çœ‹çœ‹æœ€ç»ˆçš„ç›®çš„åœ°æ˜¯ä»€ä¹ˆæ ·çš„ã€‚</p><p id="879a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">æˆ‘åˆ›å»ºäº†ä¸‹é¢çš„ç±»æ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="ce97" class="kc kd hi jy b fi ke kf l kg kh">import numpy as np<br/>from tensorflow.python.keras.preprocessing.sequence import pad_sequences</span><span id="ce34" class="kc kd hi jy b fi lg kf l kg kh"><strong class="jy hj">class</strong> <strong class="jy hj">IntentClassifier</strong>:<br/>    <strong class="jy hj">def</strong> __init__(self,classes,model,tokenizer,label_encoder):<br/>        self.classes = classes<br/>        self.classifier = model<br/>        self.tokenizer = tokenizer<br/>        self.label_encoder = label_encoder<br/><br/>    <strong class="jy hj">def</strong> get_intent(self,text):<br/>        self.text = [text]<br/>        self.test_keras = self.tokenizer.texts_to_sequences(self.text)<br/>        self.test_keras_sequence = pad_sequences(self.test_keras, maxlen=16, padding='post')<br/>        self.pred = self.classifier.predict(self.test_keras_sequence)<br/>        <strong class="jy hj">return</strong> self.label_encoder.inverse_transform(np.argmax(self.pred,1))[0]</span></pre><p id="b167" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">è¦ä½¿ç”¨è¯¥ç±»ï¼Œæˆ‘ä»¬åº”è¯¥é¦–å…ˆåŠ è½½æˆ‘ä»¬ä¿å­˜çš„æ–‡ä»¶:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="6fb3" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">pickle</strong><br/><br/><strong class="jy hj">from</strong> <strong class="jy hj">tensorflow.python.keras.models</strong> <strong class="jy hj">import</strong> load_model</span><span id="7c06" class="kc kd hi jy b fi lg kf l kg kh">model = load_model('models/intents.h5')<br/><br/><strong class="jy hj">with</strong> open('utils/classes.pkl','rb') <strong class="jy hj">as</strong> file:<br/>  classes = pickle.load(file)<br/><br/><strong class="jy hj">with</strong> open('utils/tokenizer.pkl','rb') <strong class="jy hj">as</strong> file:<br/>  tokenizer = pickle.load(file)<br/><br/><strong class="jy hj">with</strong> open('utils/label_encoder.pkl','rb') <strong class="jy hj">as</strong> file:<br/>  label_encoder = pickle.load(file)</span></pre><p id="63a5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">è€ƒè¯•æ—¶é—´åˆ°äº†ï¼ğŸ˜‹</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="3ff3" class="kc kd hi jy b fi ke kf l kg kh">nlu = IntentClassifier(classes,model,tokenizer,label_encoder)<br/>print(nlu.get_intent("is it cold in India right now"))<br/><em class="jn"># </em>Prints 'weather'</span></pre><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/8fbd245584752b4a1fd3fcbd62d4953e.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*ALbH32xuEYB7VsCJReddPw.jpeg"/></div></figure><p id="0ea2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">å°±æ˜¯è¿™æ ·ï¼Œä¼™è®¡ä»¬ï¼æ„Ÿè°¢æ‚¨çš„é˜…è¯»ğŸ˜ƒã€‚å¿«ä¹å­¦ä¹ ï¼</p></div></div>    
</body>
</html>