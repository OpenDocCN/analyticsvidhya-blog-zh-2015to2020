<html>
<head>
<title>Removing Duplicate Files Using Hashing and Parallel Processing in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python中的散列和并行处理删除重复文件</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/removing-duplicate-docs-using-parallel-processing-in-python-53ade653090f?source=collection_archive---------13-----------------------#2020-09-28">https://medium.com/analytics-vidhya/removing-duplicate-docs-using-parallel-processing-in-python-53ade653090f?source=collection_archive---------13-----------------------#2020-09-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3a2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">特别是在处理原始源文件时，我们发现数据的一个重要问题是重复，我们也在寻找快速有效的方法来消除重复文件。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/2d2d6384d8e4a1ccc7a2a2a5253a13f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*4Gz4qZpML368unuMDQrkpA.png"/></div></figure><p id="e059" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有一个这样的方法，如何使用哈希和并行处理快速有效地消除重复的文档。本文详细介绍了:在顺序和并行实现中使用散列法删除重复文档</p><h2 id="acc1" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated"><strong class="ak">方法是:</strong></h2><p id="9e77" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">1.计算所有文件的哈希值2。使用哈希值识别唯一文件。3.删除重复的文件。</p><p id="d06f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请在下面找到重要功能的详细信息</p><h2 id="8ebc" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">1.计算哈希值</h2><p id="5709" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">这个函数将文件路径作为输入。它返回每个文件的哈希值作为输出。我目前在这里使用md5散列算法。您可以使用自己选择的任何其他哈希算法。<br/>你也可以发送块大小作为参数。例如，对于大文件，您只想计算数据的前几个字节的哈希值，而不是整个文件。在这种情况下，您可以使用块大小。将值设置为blocksize时，请确保在下面的函数中用file.read(block_size)替换file.read()。</p><pre class="je jf jg jh fd kl km kn ko aw kp bi"><span id="5102" class="jl jm hi km b fi kq kr l ks kt">def calculate_hash_val(path, block_size=''):<br/>    file = open(path, 'rb')<br/>    hasher = hashlib.md5()<br/>    data = file.read()<br/>    while len(data) &gt; 0:<br/>        hasher.update(data)<br/>        data = file.read()<br/>    file.close()<br/>    return hasher.hexdigest()</span></pre><p id="2ed8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。向字典添加独特的文件</strong></p><p id="e4f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个函数将一个空字典和一个包含所有输入文件的字典作为键，并将它们各自的哈希值(我们使用上面的calculate_hash_val函数计算出来的)作为它们的值。该函数返回没有任何重复的dic_unique</p><pre class="je jf jg jh fd kl km kn ko aw kp bi"><span id="87b8" class="jl jm hi km b fi kq kr l ks kt">def find_unique_files(dic_unique, dict1):<br/>    for key in dict1.keys():<br/>        if key not in dic_unique:<br/>            dic_unique[key] = dict1[key]</span></pre><p id="34b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。从源文件中删除重复文件</strong></p><p id="b6d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">识别出唯一文件后，最后一步是删除剩余的重复文件。下面的函数用于从输入文件夹中删除重复项。它有两个输入all_inps和unique_inps，分别包含文件路径和哈希值。</p><pre class="je jf jg jh fd kl km kn ko aw kp bi"><span id="b3f1" class="jl jm hi km b fi kq kr l ks kt">def remove_duplicate_files(all_inps ,unique_inps):<br/>    for file_name in all_inps.keys():<br/>      if all_inps[file_name] in unique_inps and file_name!=unique_inps[all_inps[file_name]]:<br/>            os.remove(file_name)<br/>        elif all_inps[file_name] not in unique_inps:<br/>            os.remove(file_name)</span></pre><p id="0d8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">所需进口和申报</strong></p><p id="a3a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请将完整的输入文件夹路径分配给“输入文件路径”变量</p><pre class="je jf jg jh fd kl km kn ko aw kp bi"><span id="a324" class="jl jm hi km b fi kq kr l ks kt">import datetime, os, sys, logging, hashlib<br/>from pathlib import Path<br/>from os import listdir<br/>from os.path import isfile, join<br/><br/>input_files_path = r'H:\files\input'<br/>input_files = [f for f in listdir(input_files_path) if isfile(join(input_files_path, f))]<br/>input_files = [os.path.join(input_files_path, x) for x in input_files]<br/>inp_dups = {}<br/>unique_inps = {}</span></pre><p id="1056" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将把上述函数同样用于顺序和并行实现。请分别在下面找到这两种方法的代码。</p><h2 id="1e66" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated"><strong class="ak">方法1(顺序实施)</strong></h2><pre class="je jf jg jh fd kl km kn ko aw kp bi"><span id="c659" class="jl jm hi km b fi kq kr l ks kt">def rmv_dup_process(input_files):<br/>    all_inps={}<br/>    for file_path in input_files:<br/>        if Path(file_path).exists():<br/>           files_hash = calculate_hash_val(file_path)<br/>           inp_dups[files_hash]=file_path<br/>           all_inps[file_path] = files_hash<br/>        else:<br/>            print('%s is not a valid path, please verify' % file_path)<br/>            sys.exit()<br/><br/>    find_unique_files(unique_inps, inp_dups)<br/>    print(inp_dups)<br/>    remove_duplicate_files(all_inps, unique_inps)<br/>if __name__ == '__main__':<br/>    datetime1 = datetime.datetime.now()<br/>    rmv_dup_process(input_files)<br/>    datetime2 = datetime.datetime.now()<br/><br/>    print( "processed in",str(datetime2 - datetime1))</span></pre><h2 id="925e" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">方法2(并行实施)</h2><p id="3f3f" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">这也可以使用并行处理来实现，以使过程运行得更快，如果您想使用并行处理，只需用下面的代码替换上面的顺序实现逻辑:</p><pre class="je jf jg jh fd kl km kn ko aw kp bi"><span id="579a" class="jl jm hi km b fi kq kr l ks kt">#If you are using multiprocessing, you can also update the number of #processes in the file. The default value I used is 4<br/>if __name__ == '__main__':<br/>    pool = multiprocessing.Pool()<br/>    pool = multiprocessing.Pool(processes=4)<br/>    keys_dict = pool.map(calculate_hash_val, input_files)<br/>    pool.close()<br/>    <br/>    inp_dups = dict(zip(keys_dict, input_files))<br/>    all_inps = dict(zip(input_files, keys_dict))<br/><br/>    find_unique_files(unique_inps, inp_dups)<br/>    remove_duplicate_files(all_inps, unique_inps)</span></pre><p id="be5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就是这样。源代码请在下面找到github链接。在接下来的几天里，我还将在github repo中添加更多并行处理的实现。</p><p id="a1cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ku" href="https://github.com/KiranKumarChilla/Removing-Duplicate-Docs-Using-Hashing-in-Python" rel="noopener ugc nofollow" target="_blank">https://github . com/KiranKumarChilla/Removing-Duplicate-Docs-Using-Hashing-in-Python</a></p></div></div>    
</body>
</html>