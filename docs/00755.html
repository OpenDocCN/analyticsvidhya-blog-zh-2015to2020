<html>
<head>
<title>Fundamentals of Bag Of Words and TF-IDF</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词袋和TF-IDF的基础</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fundamentals-of-bag-of-words-and-tf-idf-9846d301ff22?source=collection_archive---------0-----------------------#2019-09-04">https://medium.com/analytics-vidhya/fundamentals-of-bag-of-words-and-tf-idf-9846d301ff22?source=collection_archive---------0-----------------------#2019-09-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d7c2fa95463237cd232d50e5fcec22de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iBqHP8rkCY6S4j0CzSt2SQ.jpeg"/></div></div></figure><p id="09fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这篇文章将带你了解机器如何检查两个句子是否有相似意思的基本概念。涉及到NLP的一些基本概念。</p><p id="389c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">自然语言处理 ( <strong class="is hj"> NLP </strong>)是计算机科学和人工智能的一个领域，涉及计算机和人类(自然)语言之间的交互，特别是如何对计算机进行编程，以有效地处理大量自然语言数据。</p><p id="404f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">简单来说，自然语言处理(NLP)是计算机理解人类说话的能力。NLP有助于以一种聪明而有用的方式分析、理解和从人类语言中获取意义。</p><h1 id="ddf8" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">为什么我们需要这样的模型来检查句子之间的相似性？</strong></h1><p id="490b" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">对文本建模的一个问题是它很混乱，像机器学习算法这样的技术更喜欢定义良好的固定长度输入和输出。</p><p id="ee4f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">机器学习算法不能直接处理原始文本；文本必须转换成数字。具体来说，数字的向量。</p><p id="7e39" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">概念:相似的文本必须产生更接近的向量。</strong></p><blockquote class="kr"><p id="adc9" class="ks kt hi bd ku kv kw kx ky kz la jn dx translated">在语言处理中，向量x来自文本数据，以反映文本的各种语言特性</p></blockquote><p id="062d" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">当前使用利用文本数据进行特征提取的流行且简单的方法是:</p><ol class=""><li id="55b6" class="lg lh hi is b it iu ix iy jb li jf lj jj lk jn ll lm ln lo bi translated">词汇袋</li><li id="f547" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">TF-IDF</li><li id="ab26" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">Word2Vec</li></ol><h1 id="9ae6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">包话(鞠躬):</h1><p id="eb00" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated"><strong class="is hj">词袋模型</strong>是在<a class="ae lu" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理</a>和<a class="ae lu" href="https://en.wikipedia.org/wiki/Information_retrieval" rel="noopener ugc nofollow" target="_blank">信息检索</a> (IR)中使用的简化表示。在这个模型中，一个文本(比如一个句子或者一个文档)被表示为它的单词的<a class="ae lu" href="https://en.wikipedia.org/wiki/Multiset" rel="noopener ugc nofollow" target="_blank">包(multiset) </a>，不考虑语法甚至词序，但是保持<a class="ae lu" href="https://en.wikipedia.org/wiki/Multiplicity_(mathematics)" rel="noopener ugc nofollow" target="_blank">多样性</a>。</p><p id="de36" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">单词袋模型通常用于<a class="ae lu" href="https://en.wikipedia.org/wiki/Document_classification" rel="noopener ugc nofollow" target="_blank">文档分类</a>的方法中，其中每个单词的(出现频率)被用作用于训练<a class="ae lu" href="https://en.wikipedia.org/wiki/Statistical_classification" rel="noopener ugc nofollow" target="_blank">分类器</a>的<a class="ae lu" href="https://en.wikipedia.org/wiki/Feature_(machine_learning)" rel="noopener ugc nofollow" target="_blank">特征</a>。</p><p id="bf1d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">单词袋模型易于理解和实现，并在诸如语言建模和文档分类等问题上取得了巨大成功。</p><p id="6ad9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它涉及两件事:</p><ol class=""><li id="322e" class="lg lh hi is b it iu ix iy jb li jf lj jj lk jn ll lm ln lo bi translated"><strong class="is hj">已知单词的词汇表:</strong>这一步围绕着构建一个文档语料库，该语料库由所提供的数据中出现的整个文本中的所有唯一单词组成。这有点像一本字典，每个索引对应一个单词，每个单词是一个不同的维度。</li></ol><blockquote class="lv lw lx"><p id="9a3c" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated">例如:如果我们对一道意大利面食有4条评价。</p><p id="6b37" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated">点评1:这种面食很好吃，价格也实惠。</p><p id="99a5" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated">点评2:这个面食不好吃，价格实惠。</p><p id="5fe5" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated">点评3:这种面食好吃又便宜。</p><p id="cc43" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated">点评4:面食好吃，面食好吃。</p></blockquote><p id="1a36" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，如果我们计算所有四篇评论中的独特词的数量，我们将总共得到12个独特词。以下是12个独特的单词:</p><ol class=""><li id="896f" class="lg lh hi is b it iu ix iy jb li jf lj jj lk jn ll lm ln lo bi translated">“这”</li><li id="ad66" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">意大利面</li><li id="abaa" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">'是'</li><li id="d231" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">“非常”</li><li id="01cd" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">“美味”</li><li id="da21" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">和'</li><li id="1eca" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">“负担得起”</li><li id="8b50" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">“不是”</li><li id="b56e" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">“美味”</li><li id="f923" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">“便宜”</li><li id="c07a" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">“口味”</li><li id="1d97" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">很好</li></ol><p id="f762" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 2。已知单词存在的度量:</strong>现在，如果我们对下表中的每个单词进行第一次检查和绘图计数，我们将得到，其中第1行对应于唯一单词的索引，第2行对应于单词在检查中出现的次数。(此处回顾1)</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/835043f533132ac905c398c9c4e387b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*xt00ncNuHRjveh-xeLr1eQ.png"/></div></figure><p id="52d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将生成一个d-unique单词的稀疏向量，对于每个文档(review ),我们将填充相应单词在文档中出现的次数。</p><p id="621e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">点评4:面食好吃，面食味道好。</strong></p><p id="813c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，在评论4中,“意大利面”具有计数2，而在评论1中它是2。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/36b7bb5406a347209c1428e6efb2dc06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*XgoX_DiHVG-QBEFc8-Aoow.png"/></div></figure><p id="8802" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在将评论转换成这样的向量后，我们可以比较不同的句子并计算它们之间的欧几里德距离，从而检查两个句子是否相似。如果没有共同的词汇，距离就会大得多，反之亦然。</p><p id="db25" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们使用的术语有小的变化时，BOW不能很好地工作，因为这里我们有意思相似但单词不同的句子。</p><p id="c456" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这会产生一个有很多零分数的向量，称为稀疏向量或稀疏表示。</p><p id="b639" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">稀疏向量在建模时需要更多的内存和计算资源，并且大量的位置或维度会使建模过程对于传统算法来说非常具有挑战性。</p><p id="e9ed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，在使用单词袋模型时，有减少词汇量的压力。</p><p id="1f5c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一步可以使用简单的<strong class="is hj">文本清理技术</strong>，例如:</p><blockquote class="lv lw lx"><p id="93cc" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated"><strong class="is hj"> </strong>忽略大小写</p><p id="b4ab" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated"><strong class="is hj"> </strong>忽略标点符号</p><p id="f60e" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated"><strong class="is hj"> </strong>忽略不包含太多信息的常用词，称为停用词，如“a”、“of”等。</p><p id="7467" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated"><strong class="is hj"> </strong>修复拼错的单词。</p><p id="d6c7" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated"><strong class="is hj"> </strong>使用词干算法将单词缩减为其词干(例如，从“播放”到“播放”)。</p></blockquote><h1 id="822d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak"> N元模型:</strong></h1><p id="3ba5" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">一个更复杂的方法是创建一个分组单词的词汇表。这既改变了词汇表的范围，又允许单词包从文档中获取更多的含义。</p><p id="601d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种方法中，每个单词或标记被称为一个“<strong class="is hj">克</strong>”。创造一个由两个单词组成的词汇表，又被称为<strong class="is hj">二元模型</strong>。同样，只有出现在语料库中的二元模型被建模，而不是所有可能的二元模型。</p><blockquote class="kr"><p id="2e1e" class="ks kt hi bd ku kv kw kx ky kz la jn dx translated"><em class="mi">一个N-gram是一个N-token单词序列:一个2-gram(通常称为bigram)是一个两个单词的单词序列，如“请转”、“转你的”或“你的作业”，一个3-gram(通常称为三元模型)是一个三个单词的单词序列，如“请转你的”或“转你的作业”。</em></p></blockquote><p id="5679" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated"><strong class="is hj">弓的代码:</strong></p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/42e823e384d9cba2ef7de10cb1597e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*KdtPDhRaA72aB2NDtwuqUA.jpeg"/></div></figure></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><h1 id="c53e" class="jo jp hi bd jq jr mr jt ju jv ms jx jy jz mt kb kc kd mu kf kg kh mv kj kk kl bi translated">TF-IDF:</h1><p id="17a7" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated"><strong class="is hj">TF–IDF</strong>或<strong class="is hj"> TFIDF </strong>，是<strong class="is hj">词频-逆文档频率</strong>的简称，是一个数字统计量，意在反映一个词对于集合中的<a class="ae lu" href="https://en.wikipedia.org/wiki/Document" rel="noopener ugc nofollow" target="_blank">文档</a>或<a class="ae lu" href="https://en.wikipedia.org/wiki/Text_corpus" rel="noopener ugc nofollow" target="_blank">语料库</a>的重要性。TF–IDF值与单词在文档中出现的次数成比例地增加<a class="ae lu" href="https://en.wikipedia.org/wiki/Proportionality_(mathematics)" rel="noopener ugc nofollow" target="_blank">,并被语料库中包含该单词的文档数量所抵消，这有助于调整某些单词通常更频繁出现的事实。TF–IDF是当今最流行的术语加权方案之一；数字图书馆中83%的基于文本的推荐系统使用TF–IDF。</a></p><p id="33b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这一概念包括:</p><blockquote class="lv lw lx"><p id="7271" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated"><strong class="is hj">数数</strong>。计算每个单词在文档中出现的次数。</p><p id="a2ba" class="iq ir ly is b it iu iv iw ix iy iz ja lz jc jd je ma jg jh ji mb jk jl jm jn hb bi translated"><strong class="is hj">频率</strong>。计算文档中所有单词中每个单词出现的频率。</p></blockquote><h1 id="f069" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">词频:</strong></h1><p id="069d" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">术语频率(TF)与信息检索结合使用，并显示表达式(术语、单词)在文档中出现的频率。术语频率表示特定术语在整个文档中的重要性。它是单词wi相对于评论rj中的总单词数在评论rj中出现的次数。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/e291f1cf2bb3a7cd0add1a1f8ab88feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*dl_epsnVXAblq7rzcVklhg.jpeg"/></div></figure><p id="d5dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TF可以说是在文档中找到一个单词的概率是多少(review)。</p><h1 id="d8d5" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">反向文档频率:</h1><p id="66fb" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">逆文档频率是对单词提供多少信息的度量，即它在所有文档中是常见的还是罕见的。它用于计算语料库中所有文档中稀有词的权重。在语料库中很少出现的单词具有高IDF分数。它是包含该词的文档的对数比例倒数(通过将文档总数除以包含该词的文档数，然后取该商的对数获得):</p><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/9e7ab9037033d5f08e01020d9118de7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JwV9bRi_r-jUdzCdyuYPsA.jpeg"/></div></div></figure><h1 id="5e22" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">词频-逆文档频率:</strong></h1><p id="a487" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">TF–IDF计算如下</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es my"><img src="../Images/60db52e3b9197f261518e0d4fe9f9248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*5N3UB1mG4wWpMa0ck3SamA.jpeg"/></div></figure><p id="6140" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TF-IDF中的高权重是通过高术语<a class="ae lu" href="https://en.wikipedia.org/wiki/Frequency_(statistics)" rel="noopener ugc nofollow" target="_blank">频率</a>(在给定文档中)和该术语在整个文档集合中的低文档频率来实现的；因此，权重倾向于过滤掉常见术语。由于IDF的对数函数中的比率总是大于或等于1，因此IDF(和TF–IDF)的值大于或等于0。随着一个术语出现在更多的文档中，对数内部的比率接近1，使IDF和TF–IDF更接近0。</p><p id="5fbd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TF-IDF为文档语料库中不太常用的词提供了较大的值。当IDF和TF值都很高时，TF-IDF值也很高，即该词在整个文档中很少见，但在文档中很常见。</p><p id="7931" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TF-IDF也不接受单词的语义。</p><p id="eb2e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">让我们举个例子来更清楚地了解一下。</strong></p><p id="389d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一句话:汽车在路上行驶。</p><p id="5d4f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">句子2:卡车在高速公路上行驶。</p><p id="e37d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个例子中，每个句子都是一个单独的文档。</p><p id="1959" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们现在将计算代表我们的语料库的上述两个文档的TF-IDF。</p><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/0f121356d9a8f27f1d49023923a92f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*laEhq0Xj9zP01eKdKTVsjQ.png"/></div></div></figure><p id="a2a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从上表可以看出，常用词的TF-IDF为零，说明它们不显著。另一方面，“汽车”、“卡车”、“公路”和“高速公路”的TF-IDF不为零。这些话有更多的意义。</p><p id="f1b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">复习</strong>——tfi df是本学期TF和IDF分数的乘积。</p><p id="e108" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> TF </strong> =术语在文档中出现的次数/文档中的总字数</p><p id="8871" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> IDF </strong> = ln(文档数/该术语出现的文档数)</p><p id="1f2c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TFIDF得分越高，该术语越稀有，反之亦然。</p><p id="95cb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TFIDF被像Google这样的搜索引擎成功地用作内容的排名因素。</p><p id="da13" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">整个想法是降低频繁出现的术语的权重，同时增加不常用的术语。</p><p id="5a28" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">TF-IDF的代码:</strong></p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es na"><img src="../Images/c21d6c4e9ba42be135d40151e3118e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*eXE3qg69oQk--UM6UmCJRw.jpeg"/></div></figure></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><p id="ae65" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TFIDF被像Google这样的搜索引擎成功地用作内容的排名因素。</p><p id="d89d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">整个想法是降低频繁出现的术语的权重，同时增加不常用的术语。</p><h1 id="b9e2" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Word2Vec:</h1><p id="da0a" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">Word2Vec模型用于学习单词的向量表示，称为“单词嵌入”。这通常是作为预处理步骤完成的，在此之后，学习到的向量被输入到判别模型(通常是RNN)中，以生成预测并执行各种有趣的事情。它需要单词的语义。我将在我的下一篇博客中详尽地介绍这个Word2Vec。—<a class="ae lu" rel="noopener" href="/analytics-vidhya/deep-dive-into-word2vec-7fcefa765c17">https://medium . com/analytics-vid hya/deep-dive-into-word 2 vec-7 fcefa 765 c 17</a></p><p id="5fc8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ly">参考文献:</em></p><div class="nb nc ez fb nd ne"><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">tf-idf</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">在信息检索中，tf-idf或TFIDF是词频-逆文档频率的缩写，是一种数值型词频。</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">en.wikipedia.org</p></div></div></div></a></div><div class="nb nc ez fb nd ne"><a href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">单词袋模型简介</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">词袋模型是在用机器学习算法对文本建模时表示文本数据的一种方式。的…</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">machinelearningmastery.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns io ne"/></div></div></a></div></div></div>    
</body>
</html>