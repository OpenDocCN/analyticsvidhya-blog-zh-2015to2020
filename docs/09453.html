<html>
<head>
<title>A Newbie’s Chronicles on Distributed Data Processing — S01E02: Optimizations Galore!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分布式数据处理新手编年史—第一季第二集:大量优化！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-newbies-chronicles-on-distributed-data-processing-s01e02-optimizations-galore-a91eff7cdb02?source=collection_archive---------29-----------------------#2020-09-06">https://medium.com/analytics-vidhya/a-newbies-chronicles-on-distributed-data-processing-s01e02-optimizations-galore-a91eff7cdb02?source=collection_archive---------29-----------------------#2020-09-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="48d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">声明:如果你还没有阅读这一季的第一集(S01E01) ，请随意先阅读它，以获得更多的上下文和更好的阅读流程。</p><p id="bd2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本系列的前面，我们已经介绍了我们需要的功能，以及在使用Spark时要记住的一些基本原则。好的，让我们从上一集停止的地方继续。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h1 id="9319" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">从概念验证开始</h1><p id="1a3d" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">现在，我们已经了解了分布式data domain所需的功能和需要牢记的基本事项，我们已经准备好从一个简单的概念验证开始，它可以正确地进行合并。</p><p id="4ab3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，确切的工作包括几个额外的步骤，比如从我们的实际数据源中<em class="ko">序列化/反序列化，以及<em class="ko">预处理</em>——然而，由于它们有点太具体，我将跳过它们，直接切入正题。为了说明问题的关键，让我们以我们非常熟悉的复仇者联盟为例:</em></p><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="ku kv l"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">实现所有增量事件ft的基本POC。复仇者联盟</figcaption></figure><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es la"><img src="../Images/282fe419ef852b6c84a386d415fcb3f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_maKIJZFC51ykIytvXd0g.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">同一POC的流程图:对于跳过代码的人，这是第二次机会</figcaption></figure><p id="6c72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作品？太好了！🎉</p><p id="96c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我能够让这个简单的概念验证在一个2节点集群中处理100 GB的增量数据，大约需要55分钟才能完成。耶！⭐️</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h1 id="c25e" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">负载测试</h1><p id="84d2" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">当然，我们没有利用Spark &amp; Scala，只是在一个2节点集群上运行它，并且只处理100 GB的增量数据。事实上，我们不得不越来越多地扩展，尝试不同的配置，看看哪种配置能够在资源利用率、完成工作所需的时间以及最终成本之间取得最佳平衡。</p><p id="b35f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当进行负载测试时，我们必须在每次运行中跟踪不同的参数，以确保我们能够得到一组最佳的参数值。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lh"><img src="../Images/16111faa8da6a67e0e28fd0e521e9158.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VmeGJtzwjbX5tWUhR66BpQ.jpeg"/></div></div></figure><ol class=""><li id="8354" class="li lj hi ih b ii ij im in iq lk iu ll iy lm jc ln lo lp lq bi translated">集群配置<br/>集群大小<br/>驱动程序内存<br/>驱动程序内核<br/>执行器内存<br/>执行器内核<br/>执行器数量<br/>实例类型</li><li id="ff34" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">与数据相关(偏斜？)<br/><br/>独特事件的数量<br/>按事件类型分布的事件(插入/更新/删除)<br/>处理的增量数据总量</li><li id="3c47" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">输出指标<br/>完成工作所需的时间<br/>成本(取决于集群配置)</li></ol><p id="6ce2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们开始用2个群集节点处理100 GB的增量数据，然后慢慢地开始按比例增加到200 GB、500 GB、1 TB。</p><p id="90fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">负载测试进展顺利，直到1 TB的增量数据，使用60节点集群。但是，我们无法进入下一个级别— 2 TB。在多次尝试调整参数后，很明显，可伸缩性的问题一定在于代码而不是配置。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h1 id="0951" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">优化、优化和优化</h1><p id="ea4e" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">通常很难找出Spark式代码中的优化范围，尤其是如果您是新手的话——然而，诀窍是要注意一些重要的方面，如内存优化和最小化数据混乱。</p><p id="3efa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们请我们团队中的一位专家提出改进的范围。回车，<a class="ae jd" href="https://www.linkedin.com/in/sanketsahu9394/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">桑吉萨胡</strong> </a>。在一个小时的时间里，他设置了背景并向他介绍了POC代码，他有几个适用于Spark jobs的建议:</p><ul class=""><li id="5210" class="li lj hi ih b ii ij im in iq lk iu ll iy lm jc lw lo lp lq bi translated"><strong class="ih hj">使用更好的内存配置:</strong></li></ul><p id="a9a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">内存配置通常遵循钟形曲线。如果您提供更少的执行器和每个执行器更多的内存，数据洗牌是最小的；但是垃圾收集变成了开销，并行性降低了。另一方面，如果您提供更多的执行器和更少的内存，尽管您利用了并行性，但是由于每个执行器的内存更少，所以会有大量的数据重排。</p><p id="482f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是关于在这两种情况之间找到一个平衡/权衡！</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lx"><img src="../Images/c2860f2746cc221a6057d6ce404f5787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aeaR8lSe3sG-RiOhbCCPnQ.jpeg"/></div></div></figure><ul class=""><li id="6194" class="li lj hi ih b ii ij im in iq lk iu ll iy lm jc lw lo lp lq bi translated"><strong class="ih hj">最小化数据洗牌:</strong></li></ul><p id="cf32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">几乎所有的繁重工作都发生在执行者身上，很少的辅助步骤发生在驱动者身上。由于这种分布式特性，有很大的空间进行<strong class="ih hj">数据洗牌</strong>:即在集群内来回发送大量数据。</p><blockquote class="ly lz ma"><p id="81ac" class="if ig ko ih b ii ij ik il im in io ip mb ir is it mc iv iw ix md iz ja jb jc hb bi translated">等等，等一下。我迷路了。驱动者、执行者&amp;洗牌？</p></blockquote><p id="27b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好的。抱歉，我想得太多了。简而言之:</p><p id="25d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">驱动</strong>:主要处理读取我们的代码、将代码分割成阶段/任务、分割并委托给执行者等的过程。<em class="ko">有点类似于一个专注的Scrum大师。</em></p><p id="c3c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">执行者</strong>:顾名思义，执行者是在数据/工作者节点中运行的流程——他们处理驱动程序委托给他们的任务。这可能类似于人们在短跑中捡票。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es me"><img src="../Images/286fe305959082cea9a30c3964e67c36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*s_GVqsm2PBEsDUGU9UeOXA.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">驱动程序-执行器架构。集群管理器</figcaption></figure><p id="40d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据混洗</strong>:现在我们知道数据帧块存储在集群的分布式文件系统中。所以，任何时候我们使用像<em class="ko"> df1.join(df2) </em>这样的动作，首先必须连接<em class="ko"> df1 </em>和<em class="ko"> df2 </em>的块，然后所有这些结果块必须逻辑合并以给出最终结果。为此，数据需要在集群中流动。</p><blockquote class="ly lz ma"><p id="de94" class="if ig ko ih b ii ij ik il im in io ip mb ir is it mc iv iw ix md iz ja jb jc hb bi translated">为什么数据洗牌又不好了？</p></blockquote><p id="a365" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，对于我们必须用Spark作业处理的通常规模的数据，不必要的数据洗牌会导致集群内不必要的网络调用来产生数据流。因为这可能确实会影响性能，所以通常需要最少的数据混洗。</p><p id="601b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，让我们考虑我们正在执行的连接操作:</p><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="ku kv l"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">我们的概念证明中的加入操作</figcaption></figure><p id="d4f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要从现有数据中删除现有的DELETE &amp; UPSERT条目，不需要连接整个数据帧。如果我们只连接被认为是主键的列就足够了，然后我们可以从现有数据中保留那些不匹配的主键。</p><p id="cde7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在类似的行中，优化后的更改如下所示:</p><figure class="kp kq kr ks fd kt"><div class="bz dy l di"><div class="ku kv l"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">只发送“左反”连接的主键</figcaption></figure><p id="c7cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，即使实施了所有这些建议，我们的工作仍然在2 TB的增量数据上失败。我们遇到了一个障碍，我们不确定为什么这些优化还不够。我们必须深入挖掘，看看任务失败的地方/原因。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="063f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要了解我们是如何调试这个未知的路障并解决它的，我们来进行<a class="ae jd" rel="noopener" href="/@athityakumar/a-newbies-chronicles-on-distributed-data-processing-s01e03-grand-finale-99b7cc8601cb">本季大结局:第一季第三集</a>！</p></div></div>    
</body>
</html>