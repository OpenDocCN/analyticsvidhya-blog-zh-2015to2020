<html>
<head>
<title>Python: Web-scraping to CSVs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python:网络抓取到CSV</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/django-web-scraping-to-csvs-a1082d41ca64?source=collection_archive---------34-----------------------#2020-06-29">https://medium.com/analytics-vidhya/django-web-scraping-to-csvs-a1082d41ca64?source=collection_archive---------34-----------------------#2020-06-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="59b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您不需要将信息存储在数据库中，网络搜集是从来源(如网站)检索信息的一种很好的方式。假设你有自己的新闻网站，你想展示其他新闻网站的最新头条，这将是一个转向网络抓取的好理由。</p><p id="7495" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">幸运的是，有很多方法可以搜索所有语言，包括Python。Python有几个不同的库，包括<a class="ae jd" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>和<a class="ae jd" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>。Python还有一个用于导入/导出CSV的内置特性。我最近有一个项目需要网络抓取，所以很明显我转向了Python，这非常简单。</p><p id="975a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Web抓取需要对HTML有一个基本的了解，因为一旦你从实际的页面中获得数据，你将需要搜索HTML元素来获得你所需要的特定信息。但是首先，你需要<code class="du je jf jg jh b">pip install beautifulsoup4</code>和<code class="du je jf jg jh b">pip install requests</code>，这将允许你发送HTTP请求到网站。然后像这样导入这些库:</p><pre class="ji jj jk jl fd jm jh jn jo aw jp bi"><span id="9994" class="jq jr hi jh b fi js jt l ju jv">from bs4 import BeautifulSoup<br/>import requests</span></pre><p id="791a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后你就可以开始编码了！你所需要的只是你抓取的网站的URL。请注意，一些网站有更高的安全性，并防止任何形式的网络抓取。</p><pre class="ji jj jk jl fd jm jh jn jo aw jp bi"><span id="0d34" class="jq jr hi jh b fi js jt l ju jv">page = requests.get(your_url_here)<br/>soup = BeautifulSoup(page.content, 'html-parser')</span></pre><p id="7c8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您将需要指定您需要的解析类型，因为BeautifulSoup可以适应各种类型的解析。因为我们正在处理HTML元素，所以一个<em class="jw"> html解析器</em>是合适的。</p><p id="0640" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是乐趣所在(乐趣是相对的)。这时你需要检查你的控制台，并找出你需要的元素。一旦找到元素，用BeautifulSoup找到它们就非常容易了——可以使用元素标签和/或类名。例如:</p><pre class="ji jj jk jl fd jm jh jn jo aw jp bi"><span id="c2a8" class="jq jr hi jh b fi js jt l ju jv">div = soup.find('div', class_='class_name')</span></pre><p id="6062" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后你甚至可以添加一个<code class="du je jf jg jh b">.text</code>来获得一个元素的内部文本:</p><pre class="ji jj jk jl fd jm jh jn jo aw jp bi"><span id="ebf4" class="jq jr hi jh b fi js jt l ju jv">text = soup.find('div', class_='class_name').text</span></pre><p id="70d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我参与的项目也需要将收集到的数据导出到CSV文件中。这就是Python内置CSV特性的由来。假设您从那个<code class="du je jf jg jh b">.find</code>方法得到的结果产生了多个div，您需要遍历这些div，然后将每个div中的文本导出到一个CSV文件中。首先，在页面顶部，您需要导入该库:</p><pre class="ji jj jk jl fd jm jh jn jo aw jp bi"><span id="5cc4" class="jq jr hi jh b fi js jt l ju jv">import csv</span></pre><p id="45f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后你就可以开始用它编码了！因此，您可以遍历多个div，并打开一个包含这些结果的新CSV:</p><pre class="ji jj jk jl fd jm jh jn jo aw jp bi"><span id="d5a9" class="jq jr hi jh b fi js jt l ju jv">all_text = []<br/>for div in divs:<br/>  get_text = div.text<br/>  text_dict = {'Text': get_text}<br/>  all_text.append(text_dict)</span><span id="169d" class="jq jr hi jh b fi jx jt l ju jv">open('text.csv', 'w') as out_file:<br/>  headers: [<br/>    'Text'<br/>  ]<br/>  writer = csv.DictWriter(out_file, fieldnames=headers)<br/>  writer.writeheader()<br/>  for text in all_text:<br/>    writer.writerow(text)</span></pre><p id="d6a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这段代码将自动在您的目录中打开一个新的<code class="du je jf jg jh b">.csv</code>文件，其中包含您所请求的数据。<code class="du je jf jg jh b">'text.csv'</code>是你的CSV文件的名字，想怎么叫都行。您可以随意调用头，借助代码的力量，如果头与原始字典的键匹配，您不需要将整个字典传递给<code class="du je jf jg jh b">writerow()</code>。</p><p id="210c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TL；DR，web抓取数据并通过Python导出到CSV非常简单。医生也很棒。</p></div></div>    
</body>
</html>