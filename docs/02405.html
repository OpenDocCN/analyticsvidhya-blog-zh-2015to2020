<html>
<head>
<title>Credit Default Analysis using Machine Learning from scratch (Part 2 of the series)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始使用机器学习进行信用违约分析(本系列的第2部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/credit-default-analysis-using-machine-learning-from-scratch-part-of-the-series-2-194d2d78a1a7?source=collection_archive---------4-----------------------#2019-12-15">https://medium.com/analytics-vidhya/credit-default-analysis-using-machine-learning-from-scratch-part-of-the-series-2-194d2d78a1a7?source=collection_archive---------4-----------------------#2019-12-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="dee3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">利用机器学习预测2年内可以违约的客户。</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/deab13e4f01f8ddf8fd4b56d15c77f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p6ahiOqtwW6mxgk-WCDvlg.jpeg"/></div></div></figure><p id="5619" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您阅读信用违约分析系列的第2部分。即使你没有看过第一部分，也没关系。在第一部分中，我们重点关注预处理元素，如缺失值分析和异常值分析，并为我们将在此展示的建模准备好数据。让我们首先可视化所有预测器的数据类型，并将因变量的数据类型改为“类别”。如果你想在阅读本文之前阅读第一部分，这里有第一部分的链接。<a class="ae jq" rel="noopener" href="/@saketgarodia/credit-default-analysis-using-machine-learning-from-scratch-part-1-8dbaad1fae14?source=friends_link&amp;sk=c2559676ba1b34b01ad9c6beab69180f">https://medium . com/@ saketgarodia/credit-default-analysis-using-machine-learning-from scratch-part-1-8d bad 1 FAE 14？source = friends _ link&amp;sk = c 2559676 ba 1b 34 b 01 ad 9 c 6 beab 69180 f</a></p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="b950" class="jw jx hi js b fi jy jz l ka kb">df = pd.read_csv(‘cleaned_all.csv’)<br/>print(df.dtypes)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kc"><img src="../Images/0e0ccfd5d5e57371aef7feabcae00056.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*DF7y_ruMcRFl1bPhZKIQ2w.png"/></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">SeriousDlqin2yrs年是因变量</figcaption></figure><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="f441" class="jw jx hi js b fi jy jz l ka kb">#Importing all the necessary packages</span><span id="65b6" class="jw jx hi js b fi kh jz l ka kb">df.SeriousDlqin2yrs = (df.SeriousDlqin2yrs).astype(‘category’)</span><span id="0a6b" class="jw jx hi js b fi kh jz l ka kb">import numpy as np</span><span id="70f7" class="jw jx hi js b fi kh jz l ka kb">import pandas as pd</span><span id="168d" class="jw jx hi js b fi kh jz l ka kb">import seaborn as sns</span><span id="8044" class="jw jx hi js b fi kh jz l ka kb">import matplotlib.pyplot as plt</span><span id="8ef4" class="jw jx hi js b fi kh jz l ka kb">def warn(*args, **kwargs): pass</span><span id="771b" class="jw jx hi js b fi kh jz l ka kb">import warnings</span><span id="88f3" class="jw jx hi js b fi kh jz l ka kb">warnings.warn = warn</span><span id="3b7a" class="jw jx hi js b fi kh jz l ka kb">from sklearn.model_selection import <strong class="js hj">train_test_split</strong></span><span id="e960" class="jw jx hi js b fi kh jz l ka kb">from sklearn.model_selection import GridSearchCV</span><span id="cb80" class="jw jx hi js b fi kh jz l ka kb">from sklearn.model_selection import cross_val_score</span><span id="1bc5" class="jw jx hi js b fi kh jz l ka kb">from sklearn.model_selection import KFold</span><span id="50a7" class="jw jx hi js b fi kh jz l ka kb">from sklearn.metrics import roc_curve</span><span id="2321" class="jw jx hi js b fi kh jz l ka kb">from sklearn.metrics import precision_recall_curve</span><span id="0d50" class="jw jx hi js b fi kh jz l ka kb">from sklearn.metrics import roc_auc_score</span><span id="64e3" class="jw jx hi js b fi kh jz l ka kb">from sklearn.linear_model import LogisticRegression</span><span id="577d" class="jw jx hi js b fi kh jz l ka kb">from sklearn.metrics import classification_report</span><span id="3d62" class="jw jx hi js b fi kh jz l ka kb">from sklearn.metrics import confusion_matrix</span><span id="efeb" class="jw jx hi js b fi kh jz l ka kb">from sklearn.ensemble import RandomForestClassifier</span><span id="ab60" class="jw jx hi js b fi kh jz l ka kb">from sklearn.ensemble import GradientBoostingClassifier</span><span id="a6fc" class="jw jx hi js b fi kh jz l ka kb">import pandas as pd # for data analytics</span><span id="2377" class="jw jx hi js b fi kh jz l ka kb">import numpy as np # for numerical computation</span><span id="c7b5" class="jw jx hi js b fi kh jz l ka kb">from matplotlib import pyplot as plt, style # for ploting</span><span id="ccb7" class="jw jx hi js b fi kh jz l ka kb">import seaborn as sns # for ploting</span><span id="fa55" class="jw jx hi js b fi kh jz l ka kb">from sklearn.metrics import fbeta_score, make_scorer, precision_score, recall_score, confusion_matrix # for evaluation</span><span id="5dfc" class="jw jx hi js b fi kh jz l ka kb">import itertools</span><span id="9f41" class="jw jx hi js b fi kh jz l ka kb">style.use(‘ggplot’)</span><span id="8218" class="jw jx hi js b fi kh jz l ka kb">np.random.seed(42)</span></pre><p id="3285" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们看看是否有机会使用PCA来减少特征的数量。为此，让我们检查由其他预测因素解释的严重滞后2年的差异量。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="b931" class="jw jx hi js b fi jy jz l ka kb">from sklearn.preprocessing import StandardScaler</span><span id="1e74" class="jw jx hi js b fi kh jz l ka kb">scaler = StandardScaler()</span><span id="3453" class="jw jx hi js b fi kh jz l ka kb">df_std = pd.DataFrame(scaler.fit_transform(df.drop([‘Unnamed: 0’, ‘SeriousDlqin2yrs’],axis=1)), columns = df.drop([‘Unnamed: 0’, ‘SeriousDlqin2yrs’],axis=1).columns)</span><span id="3095" class="jw jx hi js b fi kh jz l ka kb">from sklearn.decomposition import PCA</span><span id="8b4c" class="jw jx hi js b fi kh jz l ka kb">pca = PCA()</span><span id="562d" class="jw jx hi js b fi kh jz l ka kb">pca.fit(df_std)</span><span id="77ff" class="jw jx hi js b fi kh jz l ka kb">print(pca.explained_variance_ratio_.cumsum())</span><span id="1a4b" class="jw jx hi js b fi kh jz l ka kb">Output</span><span id="0fdf" class="jw jx hi js b fi kh jz l ka kb">[0.24612617 0.41598383 0.55069126 0.65415629 0.74505298 0.81772095  0.87925848 0.93071701 0.96391217 0.98814172 1.        ]</span></pre><p id="357e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面代码片段的输出中我们可以看到，很难将特征的数量压缩到2或3，因为每个预测器都包含因变量的大量方差。此外，我们的数据集非常庞大，10个特征对我们来说并不算多，所以我们将继续不使用主成分，而是使用所有的特征。</p><p id="8245" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们使用Python的seaborn库来可视化所有预测值的分布。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="11e8" class="jw jx hi js b fi jy jz l ka kb">plt.figure(figsize=(9, 8))</span><span id="e5bc" class="jw jx hi js b fi kh jz l ka kb">df[[‘RevolvingUtilizationOfUnsecuredLines’, ‘age’, ‘NumberOfTime30–59DaysPastDueNotWorse’, ‘DebtRatio’, ‘MonthlyIncome’, ‘NumberOfOpenCreditLinesAndLoans’, ‘NumberOfTimes90DaysLate’, ‘NumberRealEstateLoansOrLines’, ‘NumberOfTime60–89DaysPastDueNotWorse’, ‘NumberOfDependents’]].hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ki"><img src="../Images/ecb552da652b480128bf46563e3ab064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qtKedIv0E-zK6G9jgqJpSg.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ki"><img src="../Images/00861d097845f89ed6b355671f9c15a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uBjLPZxQS8OS3kyUA9GQGg.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kj"><img src="../Images/faab4dde88fbc39e251a2d6e689156ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iv952mcAk0hQdnialgjniQ.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kk"><img src="../Images/9685ff8206e8dc5a5e202bb8e945b4eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*nDQbA7wtjFSSIU91iq8COg.png"/></div></figure><p id="3627" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的分布我们可以看出，有些预测值是偏态的。因此，我们将进行转换并消除偏斜，因为我们要尝试的一些模型需要正态分布的预测值，尽管这对于基于树的模型来说是不必要的。让我们进行变换，并再次可视化预测值的分布。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="a85a" class="jw jx hi js b fi jy jz l ka kb">dataset = df[[‘RevolvingUtilizationOfUnsecuredLines’, ‘age’, ‘NumberOfTime30–59DaysPastDueNotWorse’, ‘DebtRatio’, ‘MonthlyIncome’, ‘NumberOfOpenCreditLinesAndLoans’, ‘NumberOfTimes90DaysLate’, ‘NumberRealEstateLoansOrLines’, ‘NumberOfTime60–89DaysPastDueNotWorse’, ‘NumberOfDependents’]]</span><span id="207f" class="jw jx hi js b fi kh jz l ka kb">print(dataset.shape)</span><span id="de29" class="jw jx hi js b fi kh jz l ka kb">dataset[‘TimRevolvingUtilizationOfUnsecuredLinese’] = np.log(dataset[‘RevolvingUtilizationOfUnsecuredLines’] + 1)</span><span id="b979" class="jw jx hi js b fi kh jz l ka kb">dataset[‘NumberOfTimes90DaysLate’] = np.log(dataset[‘NumberOfTimes90DaysLate’] + 1)</span><span id="be18" class="jw jx hi js b fi kh jz l ka kb">dataset[‘NumberOfTime60–89DaysPastDueNotWorse’] = np.log(dataset[‘NumberOfTime60–89DaysPastDueNotWorse’] + 1)</span><span id="fe10" class="jw jx hi js b fi kh jz l ka kb">dataset[‘NumberOfTime30–59DaysPastDueNotWorse’] = np.log(dataset[‘NumberOfTime30–59DaysPastDueNotWorse’] + 1)</span><span id="f2b3" class="jw jx hi js b fi kh jz l ka kb">dataset[‘NumberRealEstateLoansOrLines’] = np.log(dataset[‘NumberRealEstateLoansOrLines’] + 1)</span><span id="892e" class="jw jx hi js b fi kh jz l ka kb">dataset[‘MonthlyIncome’] = np.log(dataset[‘MonthlyIncome’] + 1)</span><span id="b30d" class="jw jx hi js b fi kh jz l ka kb">fig, ax = plt.subplots(((dataset.shape[1]-1) // 3 + 1), 3, figsize=(18,24))</span><span id="9f57" class="jw jx hi js b fi kh jz l ka kb">j=0</span><span id="a8ba" class="jw jx hi js b fi kh jz l ka kb">for i in range(dataset.shape[1]-1):</span><span id="e6e3" class="jw jx hi js b fi kh jz l ka kb">var_val = dataset.iloc[:,i].values</span><span id="457e" class="jw jx hi js b fi kh jz l ka kb">var_name = dataset.columns[i]</span><span id="1154" class="jw jx hi js b fi kh jz l ka kb">sns.distplot(var_val, ax=ax[i//3,j], color='g')</span><span id="bb9c" class="jw jx hi js b fi kh jz l ka kb">ax[i//3,j].set_title('Distribution of var : ' + var_name, fontsize=14)</span><span id="e7ea" class="jw jx hi js b fi kh jz l ka kb">ax[i//3,j].set_xlim([min(var_val), max(var_val)])</span><span id="0ab9" class="jw jx hi js b fi kh jz l ka kb">j+=1</span><span id="2d94" class="jw jx hi js b fi kh jz l ka kb">if j == 3:</span><span id="91c6" class="jw jx hi js b fi kh jz l ka kb">j=0</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kl"><img src="../Images/e2f58e43c16e409c4e8fe9d58fe26b35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mExgu-b7C697Iw0hJw-nLw.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es km"><img src="../Images/a5966c56a1242e28abb83570acea8841.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_VbG9J3ZwcAXBWTr7M4mCg.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kn"><img src="../Images/4d3908a785925c30a7d1dd378867e35c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IuQOEGdP9jpkTE7_ayM4qg.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ko"><img src="../Images/c2b4e489944ee5adca3f855e475c5cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*8r00QTH5FfhjqrxDvS9CLw.png"/></div></figure><p id="ac6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，一旦我们完成了预处理和所需的转换，我们就可以开始建模了。让我们首先将数据集分为训练集和测试集，这样我们就可以调整模型参数，并使用测试集找到最佳模型。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="7118" class="jw jx hi js b fi jy jz l ka kb">from sklearn.model_selection import train_test_split</span><span id="783a" class="jw jx hi js b fi kh jz l ka kb">X_train, X_test, y_train, y_test = train_test_split(dataset,df[‘SeriousDlqin2yrs’],test_size = 0.3,random_state=100,shuffle = True)</span><span id="142c" class="jw jx hi js b fi kh jz l ka kb">print(y_train.value_counts()[1]/X_train.shape[0])</span><span id="63d0" class="jw jx hi js b fi kh jz l ka kb">print(y_test.value_counts()[1]/X_test.shape[0])</span><span id="c54f" class="jw jx hi js b fi kh jz l ka kb">#Output<br/>0.06688571428571428 <br/>0.06673333333333334</span></pre><p id="8a88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，只有大约7%的数据的“SeriousDlqin2yrs”为1，这意味着数据集高度不平衡，这在进行任何类型的异常检测时都是正常的。</p><p id="5ddc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用T-sne可视化一下我们的模型。T-sne提供了数据的二维视图，帮助我们了解数据是如何混合在一起的。有时我们从SNE霸王龙那里得到的直觉可能是有用的。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="316f" class="jw jx hi js b fi jy jz l ka kb">from sklearn.model_selection import train_test_split</span><span id="7bab" class="jw jx hi js b fi kh jz l ka kb">X_TSNE, _, y_TSNE, _ = train_test_split(X_train,y_train,test_size = 0.9,random_state=100,shuffle = True)</span><span id="cba2" class="jw jx hi js b fi kh jz l ka kb">print(y_TSNE.value_counts()[1]/X_TSNE.shape[0])</span><span id="68f2" class="jw jx hi js b fi kh jz l ka kb">print(y_TSNE.value_counts())</span><span id="1ea7" class="jw jx hi js b fi kh jz l ka kb">print(X_TSNE.shape[0])</span><span id="8e0e" class="jw jx hi js b fi kh jz l ka kb">model = TSNE(learning_rate=50)</span><span id="7bb5" class="jw jx hi js b fi kh jz l ka kb">tsne_features = model.fit_transform(X_TSNE)</span><span id="0033" class="jw jx hi js b fi kh jz l ka kb">#print(tsne_features.head())<br/>import seaborn as sns</span><span id="bbca" class="jw jx hi js b fi kh jz l ka kb">sns.scatterplot(x = tsne_features[:,0], y = tsne_features[:,1], hue = y_TSNE)</span><span id="a1df" class="jw jx hi js b fi kh jz l ka kb">plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kp"><img src="../Images/6c04ad2670c7cf11d797e3bb9c748085.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*92jayhHzfGPMuTuOXFON2g.png"/></div></figure><p id="0a8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，数据集分布非常紧密，因此我们不会对这种不平衡的数据集使用欠采样和过采样技术，因为这会带来很多偏差。你可以阅读欠采样和过采样技术，有很多关于它们的博客。</p><p id="3130" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在尝试各种建模技术之前，让我们首先缩放我们的模型。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="96ce" class="jw jx hi js b fi jy jz l ka kb">from sklearn.preprocessing import StandardScaler</span><span id="0322" class="jw jx hi js b fi kh jz l ka kb">scaler = StandardScaler()</span><span id="e268" class="jw jx hi js b fi kh jz l ka kb">X_train = scaler.fit_transform(X_train)</span><span id="5bae" class="jw jx hi js b fi kh jz l ka kb">X_test = scaler.transform(X_test)</span></pre><p id="6503" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们导入我们将要使用的所有库。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="b17b" class="jw jx hi js b fi jy jz l ka kb">from sklearn.metrics import accuracy_score, log_loss</span><span id="c839" class="jw jx hi js b fi kh jz l ka kb">from sklearn.neighbors import KNeighborsClassifier</span><span id="d278" class="jw jx hi js b fi kh jz l ka kb">from sklearn.svm import SVC, LinearSVC, NuSVC</span><span id="092b" class="jw jx hi js b fi kh jz l ka kb">from sklearn.tree import DecisionTreeClassifier</span><span id="34f6" class="jw jx hi js b fi kh jz l ka kb">from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier</span><span id="0cd9" class="jw jx hi js b fi kh jz l ka kb">from sklearn.naive_bayes import GaussianNB</span><span id="78fb" class="jw jx hi js b fi kh jz l ka kb">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis</span><span id="dc73" class="jw jx hi js b fi kh jz l ka kb">from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis</span><span id="82cc" class="jw jx hi js b fi kh jz l ka kb">from sklearn.model_selection import cross_val_score</span></pre><p id="c564" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将尝试一些只保留默认参数的分类算法，并使用AUC(<a class="ae jq" href="https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5" rel="noopener" target="_blank">https://towardsdatascience . com/understanding-AUC-roc-curve-68b 2303 cc9 C5</a>)度量检查它们在交叉验证集上的性能。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="556c" class="jw jx hi js b fi jy jz l ka kb">classifiers = [</span><span id="1e47" class="jw jx hi js b fi kh jz l ka kb">LogisticRegression(),</span><span id="ef9d" class="jw jx hi js b fi kh jz l ka kb">KNeighborsClassifier(3),</span><span id="640c" class="jw jx hi js b fi kh jz l ka kb">DecisionTreeClassifier(),</span><span id="75d3" class="jw jx hi js b fi kh jz l ka kb">LinearDiscriminantAnalysis(),</span><span id="fc42" class="jw jx hi js b fi kh jz l ka kb">QuadraticDiscriminantAnalysis(),</span><span id="bc1a" class="jw jx hi js b fi kh jz l ka kb">SVC(kernel=”sigmoid”, C=0.025, probability=True)</span><span id="7713" class="jw jx hi js b fi kh jz l ka kb">]</span><span id="c487" class="jw jx hi js b fi kh jz l ka kb"># Logging for Visual Comparison</span><span id="53d6" class="jw jx hi js b fi kh jz l ka kb">log_cols=[“Classifier”, “cross_val_score”]</span><span id="d7d2" class="jw jx hi js b fi kh jz l ka kb">log = pd.DataFrame(columns=log_cols)</span><span id="3ebe" class="jw jx hi js b fi kh jz l ka kb">for clf in classifiers:</span><span id="42e7" class="jw jx hi js b fi kh jz l ka kb">auc_scores = cross_val_score(clf,X_train,y_train,cv=5,scoring=make_scorer(roc_auc_score))</span><span id="4809" class="jw jx hi js b fi kh jz l ka kb">name = clf.__class__.__name__</span><span id="93f1" class="jw jx hi js b fi kh jz l ka kb">print(“=”*30)</span><span id="8909" class="jw jx hi js b fi kh jz l ka kb">print(name)</span><span id="fdde" class="jw jx hi js b fi kh jz l ka kb">print(‘****Results****’)</span><span id="1bfb" class="jw jx hi js b fi kh jz l ka kb">print(“Accuracy: {:.2%}”.format(auc_scores.mean()))</span><span id="8195" class="jw jx hi js b fi kh jz l ka kb">log_entry = pd.DataFrame([[name, auc_scores.mean()]], columns=log_cols)</span><span id="f459" class="jw jx hi js b fi kh jz l ka kb">log = log.append(log_entry)</span><span id="0bd3" class="jw jx hi js b fi kh jz l ka kb">print(“=”*30)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kq"><img src="../Images/6b0bb04fc99dfd26393d7635403e9b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*hgZQbRWGKgOw8mn3F3ssnQ.png"/></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">不同建模技术的最佳交叉验证AUC</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kr"><img src="../Images/83a2221665de5c6d56ba7c70c889fd14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*7UT0yCFCC595LV5qzllduA.png"/></div></figure><p id="8775" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，在超参数的默认值下，二次行列式分析具有最佳性能。在我们建立了最佳模型之后，我们将更深入地理解度量值。</p><p id="c46e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们尝试机器学习社区使用的一些最强大的模型，它们是集成技术。合奏使用许多模型的力量来获得最佳性能。我们将使用随机森林分类器和梯度推进分类器。随机森林分类器通过首先使用引导数据构建几棵树，然后使用所有树基于多数投票进行预测。使用随机森林的另一个好处是，每个节点上的特征选择是随机发生的，这可以防止过度拟合，从而使模型对新数据更加健壮。我们将使用RandomizedSearchCV，通过5重交叉验证来调整我们的模型。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="82d4" class="jw jx hi js b fi jy jz l ka kb">from sklearn.model_selection import RandomizedSearchCV</span><span id="bf50" class="jw jx hi js b fi kh jz l ka kb">clf = RandomForestClassifier()</span><span id="d646" class="jw jx hi js b fi kh jz l ka kb"># use a full grid over all parameters</span><span id="e8c3" class="jw jx hi js b fi kh jz l ka kb">param_grid = {‘n_estimators’ : [50,100,150], “max_depth”: [3, None],</span><span id="9acf" class="jw jx hi js b fi kh jz l ka kb">“max_features”: [1, 3, 10],</span><span id="4e64" class="jw jx hi js b fi kh jz l ka kb">“min_samples_split”: [2, 3, 10],</span><span id="d906" class="jw jx hi js b fi kh jz l ka kb">}</span><span id="78eb" class="jw jx hi js b fi kh jz l ka kb"># run grid search</span><span id="7b61" class="jw jx hi js b fi kh jz l ka kb">grid = RandomizedSearchCV(clf, param_grid, cv=5,scoring=’roc_auc’,n_iter = 10, iid=False)</span><span id="1f11" class="jw jx hi js b fi kh jz l ka kb">grid.fit(X_train, y_train)</span><span id="2515" class="jw jx hi js b fi kh jz l ka kb">print(“Grid-Search with roc_auc”)</span><span id="e650" class="jw jx hi js b fi kh jz l ka kb">print(“Best parameters:”, grid.best_params_)</span><span id="6df4" class="jw jx hi js b fi kh jz l ka kb">print(“Best cross-validation score (auc_roc)): {:.3f}”.format(grid.best_score_))</span><span id="97f4" class="jw jx hi js b fi kh jz l ka kb">#print(“Test set AUC: {:.3f}”.format(</span><span id="02cb" class="jw jx hi js b fi kh jz l ka kb">#roc_auc_score(y_test, grid.decision_function(X_test))))</span><span id="1375" class="jw jx hi js b fi kh jz l ka kb">#print(“Test set accuracy: {:.3f}”.format(grid.score(X_test, y_test)))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ks"><img src="../Images/64812c98ea21370bc70677b9cf7a192a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-U_8oBWmO8FEWyEXlZG_yw.png"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx translated"><strong class="bd kt">输出</strong></figcaption></figure><p id="fa6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到了0.853的AUC分数，这是惊人的。现在让我们尝试梯度推进技术。梯度推进通过改善先前模型的误差来工作。它建立一个模型，然后根据前一个模型的残差依次建立另一个模型，依此类推，从而利用几个模型的能力。这是机器学习社区中最广泛使用的模型之一，用于测试集上的性能。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="57cf" class="jw jx hi js b fi jy jz l ka kb">clf = GradientBoostingClassifier(random_state=0)</span><span id="45d0" class="jw jx hi js b fi kh jz l ka kb"># use a full grid over all parameters</span><span id="9694" class="jw jx hi js b fi kh jz l ka kb">param_grid = {‘n_estimators’ : [20,30,40], “max_depth”: [2,5,7,9],</span><span id="d664" class="jw jx hi js b fi kh jz l ka kb">“max_features”: [2,3,5] ,’learning_rate’ : [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]</span><span id="2771" class="jw jx hi js b fi kh jz l ka kb">}</span><span id="d094" class="jw jx hi js b fi kh jz l ka kb"># run grid search</span><span id="ac80" class="jw jx hi js b fi kh jz l ka kb">grid = RandomizedSearchCV(clf, param_grid, cv=5,scoring=’roc_auc’,n_iter = 10, iid=False)</span><span id="fdfe" class="jw jx hi js b fi kh jz l ka kb">grid.fit(X_train, y_train)</span><span id="e4f8" class="jw jx hi js b fi kh jz l ka kb">print(“Grid-Search with roc_auc”)</span><span id="a4a4" class="jw jx hi js b fi kh jz l ka kb">print(“Best parameters:”, grid.best_params_)</span><span id="d65d" class="jw jx hi js b fi kh jz l ka kb">print(“Best cross-validation score (auc_roc)): {:.3f}”.format(grid.best_score_))</span><span id="aabf" class="jw jx hi js b fi kh jz l ka kb">#print(“Test set AUC: {:.3f}”.format(</span><span id="b93d" class="jw jx hi js b fi kh jz l ka kb">#roc_auc_score(y_test, grid.decision_function(X_test))))</span><span id="d2f7" class="jw jx hi js b fi kh jz l ka kb">#print(“Test set accuracy: {:.3f}”.format(grid.score(X_test, y_test)))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ku"><img src="../Images/b7caaa57ed2a14331ba60f393cea0e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RFsDH9KvaRIv1AU-SXxciQ.png"/></div></div></figure><p id="a81b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">哇，我们使用梯度增强技术得到了0.861的惊人AUC。现在我们已经找到了最佳的超参数集，让我们使用整个训练集来训练我们的模型，这样所有的数据都可以用作最后一步。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="3abd" class="jw jx hi js b fi jy jz l ka kb">gbc = GradientBoostingClassifier(n_estimators= 40, max_features= 3, max_depth= 5, learning_rate= 0.25)</span><span id="7984" class="jw jx hi js b fi kh jz l ka kb">gbc.fit(X_train, y_train)</span><span id="6ebe" class="jw jx hi js b fi kh jz l ka kb">y_pr=gbc.predict(X_test)</span><span id="5976" class="jw jx hi js b fi kh jz l ka kb">print(“Test set AUC: {:.3f}”.format(roc_auc_score(y_test, gbc.predict_proba(X_test)[:, 1])))</span><span id="af30" class="jw jx hi js b fi kh jz l ka kb">print(classification_report(y_test, y_pr,target_names=[“0”,”1"]))</span><span id="e373" class="jw jx hi js b fi kh jz l ka kb">confusion_matrix(y_test, y_pr)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kv"><img src="../Images/9800dce22ebb11aecaf12b731421b7a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*ByMMbZ4AXlwLGts8RysusA.png"/></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">最终模型分析</figcaption></figure><p id="91dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们绘制精度-召回F分数曲线，以形象化我们在不同阈值下如何区分“0”和“1”类。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="c27d" class="jw jx hi js b fi jy jz l ka kb">tresholds = np.linspace(0, 1, 100)</span><span id="9d5d" class="jw jx hi js b fi kh jz l ka kb">y_scores = gbc.predict_proba(X_test)[:, 1]</span><span id="2f03" class="jw jx hi js b fi kh jz l ka kb">scores = []</span><span id="9eb5" class="jw jx hi js b fi kh jz l ka kb">for treshold in tresholds:</span><span id="5da8" class="jw jx hi js b fi kh jz l ka kb">y_hat = (y_scores &gt; treshold).astype(int)</span><span id="e480" class="jw jx hi js b fi kh jz l ka kb">scores.append([recall_score(y_pred=y_hat, y_true=y_test.values),</span><span id="918f" class="jw jx hi js b fi kh jz l ka kb">precision_score(y_pred=y_hat, y_true=y_test.values),</span><span id="d425" class="jw jx hi js b fi kh jz l ka kb">fbeta_score(y_pred=y_hat, y_true=y_test.values, beta=2)])</span><span id="f9f6" class="jw jx hi js b fi kh jz l ka kb">scores = np.array(scores)</span><span id="89b6" class="jw jx hi js b fi kh jz l ka kb">print(scores[:, 2].max(), scores[:, 2].argmax())</span><span id="e863" class="jw jx hi js b fi kh jz l ka kb">plt.plot(tresholds, scores[:, 0], label=’$Recall$’)</span><span id="8958" class="jw jx hi js b fi kh jz l ka kb">plt.plot(tresholds, scores[:, 1], label=’$Precision$’)</span><span id="2acf" class="jw jx hi js b fi kh jz l ka kb">plt.plot(tresholds, scores[:, 2], label=’$F_2$’)</span><span id="419c" class="jw jx hi js b fi kh jz l ka kb">plt.ylabel(‘Score’)</span><span id="6b2c" class="jw jx hi js b fi kh jz l ka kb">plt.xlabel(‘Threshold’)</span><span id="1e83" class="jw jx hi js b fi kh jz l ka kb">plt.legend(loc=’best’)</span><span id="02c8" class="jw jx hi js b fi kh jz l ka kb">plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kw"><img src="../Images/0f76e37d6555bdf7d2d00788fff38ba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*C1GB29XHluFgBNzfpCKAaA.png"/></div></figure><p id="a178" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在算法使用的阈值为0.5 ( predict_proba = 0.5)时，精度值为0.56，召回值为0.20。精度告诉我们，在全部阳性分类数据点中，被正确分类为阳性的数据点的百分比。回忆告诉我们在所有积极的数据点中，真正被归类为积极的数据点的百分比。(<a class="ae jq" href="https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c" rel="noopener" target="_blank">https://towards data science . com/beyond-accuracy-precision-and-recall-3da 06 bea 9 f6c</a>)</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kx"><img src="../Images/ce46d3b6abe057f4c98885e21d5928bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*IYohJ8rrTJ6MF3D7Xdtdaw.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ky"><img src="../Images/6a5489055385c9f8288cfd1f7d37a84f.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*JbkRH247ZOFF2kr8rlUElw.png"/></div></figure><p id="786e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可能会认为我们的模型表现不好，因为召回值只有0.20。但是，在现实世界中，大多数建立在不平衡数据集上的模型都具有这样的精度和召回值。不同的企业和行业可以以不同的方式使用该模型。总是有一个精确召回的权衡。我给你举个例子。假设我们为银行建立了这个模型。可能会有不同风险偏好的银行。一些风险偏好较低的银行无法向违约概率低于0.5的人提供贷款。在这种情况下，这些低偏好银行将通过降低概率的阈值来使用该模型，从而增加召回值并降低精确度。以类似的方式，高风险偏好银行可以提高门槛，从而向违约概率为0.6的人提供贷款。</p><p id="882b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们绘制ROC曲线。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="e85a" class="jw jx hi js b fi jy jz l ka kb">from sklearn.metrics import roc_curve</span><span id="1d15" class="jw jx hi js b fi kh jz l ka kb">fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, gbc.predict_proba(X_test)[:, 1])</span><span id="a696" class="jw jx hi js b fi kh jz l ka kb">plt.plot(fpr_rf, tpr_rf, label=”ROC Curve RF”)</span><span id="3e93" class="jw jx hi js b fi kh jz l ka kb">plt.xlabel(“FPR”)</span><span id="3ef7" class="jw jx hi js b fi kh jz l ka kb">plt.ylabel(“TPR (recall)”)</span><span id="60aa" class="jw jx hi js b fi kh jz l ka kb">F_2 = list(scores[:, 2])</span><span id="8074" class="jw jx hi js b fi kh jz l ka kb">close_default_rf = tresholds[F_2.index(max(F_2))]</span><span id="99b2" class="jw jx hi js b fi kh jz l ka kb">close_default_rf = np.argmin(np.abs(thresholds_rf — close_default_rf))</span><span id="ed65" class="jw jx hi js b fi kh jz l ka kb">plt.plot(fpr_rf[close_default_rf], tpr_rf[close_default_rf], ‘^’, markersize=10, label=”threshold RF”, fillstyle=”none”, c=’k’, mew=2)</span><span id="eae7" class="jw jx hi js b fi kh jz l ka kb">plt.legend(loc=4)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kz"><img src="../Images/9784da9fdde6f6d9e49f919327585691.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*mWDhHL7Hmh1jY8emqaSvww.png"/></div></figure><p id="634c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ROC曲线下的面积告诉我们，在不同的阈值下，模型能够在多大程度上区分“1”和“0”。AUC越高，我们的模型预测得越好。</p><p id="0c90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读。我只是一个数据科学博客的初学者，因此希望听到您的反馈。</p><p id="6d65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢你阅读:D</p></div></div>    
</body>
</html>