<html>
<head>
<title>A Gentle Introduction to implementing BERT using Hugging Face!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个温和的介绍实现伯特使用拥抱脸！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-gentle-introduction-to-implementing-bert-using-hugging-face-35eb480cff3?source=collection_archive---------8-----------------------#2020-05-31">https://medium.com/analytics-vidhya/a-gentle-introduction-to-implementing-bert-using-hugging-face-35eb480cff3?source=collection_archive---------8-----------------------#2020-05-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d885" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将分享我使用<a class="ae jd" href="https://huggingface.co/transformers/model_doc/bert.html" rel="noopener ugc nofollow" target="_blank">拥抱人脸库</a>实现来自变压器(BERT) 的<a class="ae jd" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">双向编码器表示的心得。BERT是Google为不同的自然语言处理(NLP)任务开发的最新模型。在这篇文章中，我们将使用</a><a class="ae jd" href="https://nlp.stanford.edu/sentiment/treebank.html" rel="noopener ugc nofollow" target="_blank">斯坦福树库数据集</a>构建一个情感分析分类器，该数据集包含电影评论的肯定句和否定句。为此，我们将使用拥抱人脸库中的<code class="du je jf jg jh b"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/bert.html?highlight=bert#bertforsequenceclassification" rel="noopener ugc nofollow" target="_blank">BertForSequenceClassification</a></code>模块。该代码可在<a class="ae jd" href="https://github.com/rajatbhatnagar94/bert_getting_started" rel="noopener ugc nofollow" target="_blank">https://github.com/rajatbhatnagar94/bert_getting_started</a>获得。</p><p id="c6f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">快速笔记</strong></p><p id="adf7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了完全理解本文，我强烈建议您对Pytorch概念有一个基本的了解。我发现<a class="ae jd" href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html" rel="noopener ugc nofollow" target="_blank">这个</a>对理解它的基础很有帮助。我发现<a class="ae jd" href="https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1" rel="noopener" target="_blank">这篇</a>文章对于理解BERT模型内部工作的基础很有用。</p><p id="12b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">下载并保存数据集</strong></p><p id="67b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了简单起见，我在这里上传了Github资源库<a class="ae jd" href="https://github.com/rajatbhatnagar94/bert_getting_started/tree/master/data" rel="noopener ugc nofollow" target="_blank">中的数据。您可以从这里下载培训、开发和测试集，并保存在任何目录中(在我的例子中是示例<code class="du je jf jg jh b">data/&lt;dataset_type&gt;.csv</code>),或者直接克隆存储库。数据集非常小，所以我们不需要花很长时间来训练或克隆存储库。</a></p><figure class="jj jk jl jm fd jn er es paragraph-image"><div class="er es ji"><img src="../Images/bd4d3d6737e67f3cfc037b9fdd3ffd1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*lMlwRSQfzaYw1ckgS-jgZw.jpeg"/></div></figure></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><h1 id="baf9" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">我们开始吧！</h1><p id="3b08" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">让我们导入将在整个教程中使用的所有库。安装库的说明在<a class="ae jd" href="https://github.com/rajatbhatnagar94/bert_getting_started/blob/master/README.md" rel="noopener ugc nofollow" target="_blank"> README.md </a>中给出。</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="b8d2" class="le jy hi jh b fi lf lg l lh li">import os<br/>import pandas as pd<br/>import torch<br/>import transformers<br/>import sklearn<br/>from transformers import BertTokenizer,BertForSequenceClassification<br/>from IPython.core.display import display, HTML</span></pre><h1 id="6f11" class="jx jy hi bd jz ka lj kc kd ke lk kg kh ki ll kk kl km lm ko kp kq ln ks kt ku bi translated"><strong class="ak">读取所有数据集</strong></h1><p id="37d9" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">培训、测试和开发数据集可以按如下方式导入:</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="24da" class="le jy hi jh b fi lf lg l lh li">dataset = {<br/>            "name": "Stanford treebank",<br/>            "train_path": "data/train.csv",<br/>            "dev_path": "data/dev.csv",<br/>            "test_path": "data/test.csv",<br/>            'classes': ['neg', 'pos']<br/>          }</span><span id="0a2e" class="le jy hi jh b fi lo lg l lh li">def read_data():<br/>    train = pd.read_csv(dataset['train_path'], sep='\t')<br/>    dev = pd.read_csv(dataset['dev_path'], sep='\t')<br/>    test = pd.read_csv(dataset['test_path'], '\t')<br/>    return train, dev, test<br/>train, dev, test = read_data()</span></pre><h1 id="97a9" class="jx jy hi bd jz ka lj kc kd ke lk kg kh ki ll kk kl km lm ko kp kq ln ks kt ku bi translated">制作批次和使用数据加载器</h1><p id="b4c5" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">一旦我们有了原始数据，我们需要在训练数据之前做两件事:</p><ol class=""><li id="6ecd" class="lp lq hi ih b ii ij im in iq lr iu ls iy lt jc lu lv lw lx bi translated">将文本句子标记化，并将其转换为矢量化形式</li><li id="c2ae" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated">使用<a class="ae jd" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank">数据加载器</a>为训练、开发和测试集创建批量矢量化记号</li></ol><p id="fa78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">对文本句子进行分词并转换成矢量化形式</strong></p><p id="06f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将数据转换成我们将传递给BERT模型的格式。为此我们将使用拥抱脸提供的<code class="du je jf jg jh b"><a class="ae jd" href="https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus" rel="noopener ugc nofollow" target="_blank">tokenizer.encode_plus</a></code>功能。首先我们定义记号赋予器。为此，我们将使用<code class="du je jf jg jh b"><a class="ae jd" href="https://huggingface.co/transformers/model_doc/bert.html#berttokenizer" rel="noopener ugc nofollow" target="_blank">BertTokenizer</a></code>。</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="49a0" class="le jy hi jh b fi lf lg l lh li">tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')</span></pre><p id="870b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">稍后我们将传递两个变量给伯特的正向函数，即<code class="du je jf jg jh b"><a class="ae jd" href="https://huggingface.co/transformers/glossary.html#input-ids" rel="noopener ugc nofollow" target="_blank">input_ids</a></code>和<code class="du je jf jg jh b"><a class="ae jd" href="https://huggingface.co/transformers/glossary.html#attention-mask" rel="noopener ugc nofollow" target="_blank">attention_mask</a>.</code><code class="du je jf jg jh b"><a class="ae jd" href="https://huggingface.co/transformers/glossary.html#input-ids" rel="noopener ugc nofollow" target="_blank">input_ids</a></code>仅仅是记号的数字表示。当我们向输入标记添加填充时,<code class="du je jf jg jh b"><a class="ae jd" href="https://huggingface.co/transformers/glossary.html#attention-mask" rel="noopener ugc nofollow" target="_blank">Attention_mask</a></code>是有用的。注意掩码告诉我们哪些input _ ids对应于填充。添加填充是因为我们希望所有输入的句子长度相同(至少对于一个批处理来说)，这样我们就能够正确地形成张量对象。我们将使用<code class="du je jf jg jh b"><a class="ae jd" href="https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus" rel="noopener ugc nofollow" target="_blank">tokenizer.encode_plus</a></code>函数来获取<code class="du je jf jg jh b">input_ids, attention_mask.</code></p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="7e06" class="le jy hi jh b fi lf lg l lh li">def encode(data, tokenizer):<br/>    input_ids = []</span><span id="9d00" class="le jy hi jh b fi lo lg l lh li">    attention_mask = []</span><span id="81eb" class="le jy hi jh b fi lo lg l lh li">    for text in data:</span><span id="5ca5" class="le jy hi jh b fi lo lg l lh li">        tokenized_text = tokenizer.encode_plus(text,<br/>                                            max_length=128,<br/>                                            add_special_tokens =          True,<br/>                                            pad_to_max_length=True,<br/>                                            padding_side='right',<br/>                                            return_attention_mask=True)</span><span id="2293" class="le jy hi jh b fi lo lg l lh li">        input_ids.append(tokenized_text['input_ids'])</span><span id="41c6" class="le jy hi jh b fi lo lg l lh li">        attention_mask.append(tokenized_text['attention_mask'])<br/>    <br/>    return torch.tensor(input_ids, dtype=torch.long), torch.tensor(attention_mask, dtype=torch.long)</span></pre><p id="0ea6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的<code class="du je jf jg jh b">encode</code>函数将迭代所有的句子，对于每个句子——标记文本，截断或添加填充以使其长度为128，添加特殊标记([CLS]、[SEP]、[PAD])并返回<code class="du je jf jg jh b">attention_mask.</code>,我们需要将所有这些传递给BERT分类器的forward函数。</p><p id="152b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">使用</strong> <a class="ae jd" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">数据加载器</strong> </a> <strong class="ih hj">创建批量矢量化记号，用于训练、开发和测试集</strong></p><p id="fb7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在将输入传递给train函数之前，我们需要做的第二件事是批量生成数据集。我们创建一个函数<code class="du je jf jg jh b">get_batches</code>，它调用上面的<code class="du je jf jg jh b">encode</code>函数来创建批处理。我们使用Pytorch的<code class="du je jf jg jh b"><a class="ae jd" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset" rel="noopener ugc nofollow" target="_blank">TensorDataset</a></code>和<code class="du je jf jg jh b"><a class="ae jd" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank">DataLoader</a></code>函数来完成这个任务。</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="c813" class="le jy hi jh b fi lf lg l lh li">def get_batches(df, tokenizer, batch_size=2):</span><span id="eef1" class="le jy hi jh b fi lo lg l lh li">    x = list(df['text'].values)<br/>    <br/>    y_indices = df['classification'].apply(lambda each_y: dataset['classes'].index(each_y))<br/>    <br/>    y = torch.tensor(list(y_indices), dtype=torch.long)</span><span id="aa1d" class="le jy hi jh b fi lo lg l lh li">    input_ids, attention_mask = encode(x, tokenizer)</span><span id="8253" class="le jy hi jh b fi lo lg l lh li">    tensor_dataset = torch.utils.data.TensorDataset(input_ids, attention_mask, y)</span><span id="41fb" class="le jy hi jh b fi lo lg l lh li">    tensor_randomsampler = torch.utils.data.RandomSampler(tensor_dataset)</span><span id="5536" class="le jy hi jh b fi lo lg l lh li">    tensor_dataloader = torch.utils.data.DataLoader(tensor_dataset, sampler=tensor_randomsampler, batch_size=batch_size)</span><span id="3477" class="le jy hi jh b fi lo lg l lh li">    return tensor_dataloader</span></pre><p id="618b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为培训、测试和开发设备制作批次:</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="2640" class="le jy hi jh b fi lf lg l lh li">batch_train = get_batches(train, tokenizer, batch_size=2)<br/>batch_dev = get_batches(dev, tokenizer, batch_size=2)<br/>batch_test = get_batches(test, tokenizer, batch_size=2)</span></pre><p id="d75f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了批次<code class="du je jf jg jh b">batch_train, batch_dev, batch_test</code>。批处理的每个元素都是一个元组，其中包含训练我们的模型所需的<code class="du je jf jg jh b">input_ids (batch_size x max_sequence_length), attention_mask (batch_size x max_sequence_length) and labels (batch_size x number_of_labels</code>！</p><h1 id="8aee" class="jx jy hi bd jz ka lj kc kd ke lk kg kh ki ll kk kl km lm ko kp kq ln ks kt ku bi translated">编写列车功能</h1><p id="ec45" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">现在我们都准备好训练我们的模型了。这个训练函数就像我们如何处理一个普通Pytorch模型一样。我们首先将模式设置为training，然后遍历每一批，并将其传输到GPU。然后我们将<code class="du je jf jg jh b">input_ids, attention_mask</code>和<code class="du je jf jg jh b">input_ids</code>传递给模型。它给我们输出，由<code class="du je jf jg jh b">loss, logits, hidden_states_output</code>和<code class="du je jf jg jh b">attention_mask_output</code>组成。<code class="du je jf jg jh b">loss</code>包含分类损失值。我们调用<code class="du je jf jg jh b">loss</code>的后向函数来计算BERT模型参数的梯度。然后我们调用<code class="du je jf jg jh b">clip_grad_norm_</code>来防止梯度变得太高或太低。然后我们调用<code class="du je jf jg jh b">optimizer.step()</code>来更新由<code class="du je jf jg jh b">loss.backward(). scheduler.step()</code>计算的梯度，用于根据调度器更新学习率。</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="aedf" class="le jy hi jh b fi lf lg l lh li">def train_model(batch, model, optimizer, scheduler, epochs, device):</span><span id="3efe" class="le jy hi jh b fi lo lg l lh li">    model.train()  # Set the mode to training</span><span id="c4a1" class="le jy hi jh b fi lo lg l lh li">    for e in range(epochs):</span><span id="35c8" class="le jy hi jh b fi lo lg l lh li">        for i, batch_tuple in enumerate(batch):</span><span id="73cc" class="le jy hi jh b fi lo lg l lh li">            batch_tuple = (t.to(device) for t in batch_tuple)</span><span id="70dd" class="le jy hi jh b fi lo lg l lh li">            input_ids, attention_mask, labels = batch_tuple</span><span id="9b70" class="le jy hi jh b fi lo lg l lh li">            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)</span><span id="036c" class="le jy hi jh b fi lo lg l lh li">            loss, logits, hidden_states_output, attention_mask_output = outputs</span><span id="410e" class="le jy hi jh b fi lo lg l lh li">            if i % 100 == 0:<br/>                print("loss - {0}, iteration - {1}/{2}".format(loss, e + 1, i))</span><span id="6f17" class="le jy hi jh b fi lo lg l lh li">            model.zero_grad()</span><span id="dc21" class="le jy hi jh b fi lo lg l lh li">            optimizer.zero_grad()</span><span id="c7fc" class="le jy hi jh b fi lo lg l lh li">            loss.backward()</span><span id="7ca7" class="le jy hi jh b fi lo lg l lh li">            torch.nn.utils.clip_grad_norm_(model.parameters(), <br/>parameters['max_grad_norm'])</span><span id="5baa" class="le jy hi jh b fi lo lg l lh li">            optimizer.step()</span><span id="1b0c" class="le jy hi jh b fi lo lg l lh li">            scheduler.step()</span></pre><h1 id="e6a6" class="jx jy hi bd jz ka lj kc kd ke lk kg kh ki ll kk kl km lm ko kp kq ln ks kt ku bi translated">估价</h1><p id="b060" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">评估函数类似于我们之前编写的<code class="du je jf jg jh b">train_model</code>函数。我们将模型的模式设置为<code class="du je jf jg jh b">eval</code>。然后我们迭代每一批，并在<code class="du je jf jg jh b">torch.no_grad()</code>下执行模型的正向函数。<code class="du je jf jg jh b">torch.no_grad()</code>确保这次我们不计算梯度。我们获得了与训练步骤类似的输出。我们将利用<code class="du je jf jg jh b">logits</code>变量来得到预测。<code class="du je jf jg jh b">logits</code>变量包含每个类的预测，不包括Softmax。我们将简单地找出<code class="du je jf jg jh b">logits</code>的<code class="du je jf jg jh b">argmax</code>来得到预测的标签。一个更有趣的输出字段是<code class="du je jf jg jh b">attention_mask_output</code>。<code class="du je jf jg jh b">attention_mask_output</code>包含每一层的“注意力”。它是一个大小为12的元组，表示BERT模型的12层。每个元组由形状的注意张量组成(<code class="du je jf jg jh b">batch_size (2), number_of_heads (12), max_sequence_length (128), max_sequence_length (128)</code>)。我们选择<code class="du je jf jg jh b">attention_mask_output</code>的最后一层和最后一个头，并返回对应于<code class="du je jf jg jh b">[CLS]</code>标记的值。对应于<code class="du je jf jg jh b">[CLS]</code>记号的值给了我们给予句子中每个记号的重要性值的总体印象，这导致了分类器的预测。这是一个形状张量<code class="du je jf jg jh b">(batch_size(2), max_sequence_length(128))</code>。每个批次的值相加为1，填充标记的值为0。我们将用它来显示最后的注意力层。</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="d205" class="le jy hi jh b fi lf lg l lh li">def evaluate(batch, model, device):</span><span id="b4e4" class="le jy hi jh b fi lo lg l lh li">    input_ids, predictions, true_labels, attentions = [], [], [], []</span><span id="3ee4" class="le jy hi jh b fi lo lg l lh li">    model.eval()</span><span id="65f3" class="le jy hi jh b fi lo lg l lh li">    for i, batch_cpu in enumerate(batch):</span><span id="5a4a" class="le jy hi jh b fi lo lg l lh li">        batch_gpu = (t.to(device) for t in batch_cpu)</span><span id="8077" class="le jy hi jh b fi lo lg l lh li">        input_ids_gpu, attention_mask, labels = batch_gpu</span><span id="89bd" class="le jy hi jh b fi lo lg l lh li">        with torch.no_grad():</span><span id="433e" class="le jy hi jh b fi lo lg l lh li">            loss, logits, hidden_states_output, attention_mask_output = model(input_ids=input_ids_gpu, attention_mask=attention_mask, labels=labels)</span><span id="38cb" class="le jy hi jh b fi lo lg l lh li">            logits =  logits.cpu()</span><span id="5ac8" class="le jy hi jh b fi lo lg l lh li">            prediction = torch.argmax(logits, dim=1).tolist()</span><span id="e444" class="le jy hi jh b fi lo lg l lh li">            true_label = labels.cpu().tolist()</span><span id="cb15" class="le jy hi jh b fi lo lg l lh li">            input_ids_cpu = input_ids_gpu.cpu().tolist()</span><span id="d178" class="le jy hi jh b fi lo lg l lh li">            attention_last_layer = attention_mask_output[-1].cpu() # selection the last attention layer</span><span id="9b60" class="le jy hi jh b fi lo lg l lh li">            attention_softmax = attention_last_layer[:,-1, 0].tolist()  # selection the last head attention of CLS token</span><span id="fc39" class="le jy hi jh b fi lo lg l lh li">            input_ids += input_ids_cpu</span><span id="6681" class="le jy hi jh b fi lo lg l lh li">            predictions += prediction</span><span id="a8b3" class="le jy hi jh b fi lo lg l lh li">            true_labels += true_label</span><span id="b40c" class="le jy hi jh b fi lo lg l lh li">            attentions += attention_softmax</span><span id="6509" class="le jy hi jh b fi lo lg l lh li">    return input_ids, predictions, true_labels, attentions</span></pre><h1 id="a99e" class="jx jy hi bd jz ka lj kc kd ke lk kg kh ki ll kk kl km lm ko kp kq ln ks kt ku bi translated">最后，运行代码</h1><p id="a75d" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">现在我们有了执行代码的所有函数。我们定义了一些超参数，如周期数、学习率、热身步数、训练步数和max_grad_norm。我们还从<code class="du je jf jg jh b">BertForSequenceClassification</code>启动我们的模型，并将其移动到我们在开始时定义的设备。我们还用上面选择的超参数定义了优化器和调度器。</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="e624" class="le jy hi jh b fi lf lg l lh li">epochs=2</span><span id="e946" class="le jy hi jh b fi lo lg l lh li">parameters = {<br/>    'learning_rate': 2e-5,<br/>    'num_warmup_steps': 1000,<br/>    'num_training_steps': len(batch_train) * epochs,<br/>    'max_grad_norm': 1<br/>}</span><span id="c492" class="le jy hi jh b fi lo lg l lh li">device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")</span><span id="f983" class="le jy hi jh b fi lo lg l lh li">model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, output_hidden_states=True, output_attentions=True)</span><span id="8de1" class="le jy hi jh b fi lo lg l lh li">model.to(device)</span><span id="0781" class="le jy hi jh b fi lo lg l lh li">optimizer = transformers.AdamW(model.parameters(), <br/>lr=parameters['learning_rate'], correct_bias=False)</span><span id="6040" class="le jy hi jh b fi lo lg l lh li">scheduler = transformers.get_linear_schedule_with_warmup(optimizer,<br/>                                                         num_warmup_steps=parameters['num_warmup_steps'],<br/>                                                         num_training_steps=parameters['num_training_steps'])</span></pre><p id="ffda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练模型。这应该不会花很多时间，因为训练集中的数据量较少。我用1个GPU大概花了8分钟。我们在每100次迭代时打印损失。您还可以通过调用evaluate函数来计算开发集的临时精度，但是我在这个基本实现中避免了这一点。</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="1b88" class="le jy hi jh b fi lf lg l lh li">train_model(batch_train, model, optimizer, scheduler, epochs, device)</span></pre><p id="d237" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练模型之后，我们可以评估开发和测试集。在对开发集进行评估后，我们使用<code class="du je jf jg jh b">sklearn.metrics</code>显示分类报告。</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="c2a7" class="le jy hi jh b fi lf lg l lh li">input_ids, predictions, true_labels, attentions = evaluate(batch_dev, model, device)</span><span id="d9a0" class="le jy hi jh b fi lo lg l lh li">print(sklearn.metrics.classification_report(true_labels, predictions))</span></pre><figure class="jj jk jl jm fd jn er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es md"><img src="../Images/bbe73d7296e367fc906ee44cd2468d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YBAQG2HPb62doZrhfAO8bg.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">使用BERT分类器的斯坦福树库数据集的结果</figcaption></figure><p id="f7ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过很少的超参数调整，我们得到了92 %的F1分数。可以通过使用不同的超参数、优化器和调度器来提高分数。我们不会在本文中讨论这些。</p><p id="d2a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以这是使用BERT建立情感分类器的一个非常基本的方法。您可以遵循类似的方法来探索可以使用BERT解决的不同任务。</p><p id="4c4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来的部分展示了<code class="du je jf jg jh b">[CLS]</code>标记赋予句子中每个标记的重要性的可视化。这给了我们一些信息——为什么模型做出了一个特定的预测，以及句子中最重要的标记是什么。</p><h1 id="e6c9" class="jx jy hi bd jz ka lj kc kd ke lk kg kh ki ll kk kl km lm ko kp kq ln ks kt ku bi translated">注意层可视化</h1><p id="5884" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">当我最初制作这个分类器时，我无法找出为什么要做一个预测。为了可视化预测，您可以使用各种方法。LIME是一个非常著名的库，但是它非常慢。我已经使用了之前获得的注意力层来进行可视化。我已经用它们的注意力得分的强度突出显示了记号。下图显示了几个例子。</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="bc45" class="le jy hi jh b fi lf lg l lh li">def get_length_without_special_tokens(sentence):<br/>    length = 0<br/>    for i in sentence:<br/>        if i == 0:<br/>            break<br/>        else:<br/>            length += 1<br/>    return length</span><span id="2cf7" class="le jy hi jh b fi lo lg l lh li">def print_attention(input_ids_all, attentions_all, tokenizer):<br/>    for input_ids, attention in zip(input_ids_all, attentions_all):<br/>        html = []<br/>        len_input_ids = get_length_without_special_tokens(input_ids)<br/>        input_ids = input_ids[:len_input_ids]<br/>        attention = attention[:len_input_ids]<br/>        for input_id, attention_value in zip(input_ids, attention):<br/>            token = tokenizer.convert_ids_to_tokens(input_id)<br/>            attention_value = attention_value<br/>            html.append('&lt;span style="background-color: rgb(255,255,0,{0})"&gt;{1}&lt;/span&gt;'.format(10 * attention_value, token))<br/>        html_string = " ".join(html)<br/>        display(HTML(html_string))</span><span id="cf5b" class="le jy hi jh b fi lo lg l lh li">print_attention(input_ids, attentions, tokenizer)</span></pre><figure class="jj jk jl jm fd jn er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es mm"><img src="../Images/10333d319e75425d6b2d61321eb1ee47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r-jYjVhT_sMbbDhVGvR6iA.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">突出由注意力层给出的重要标记</figcaption></figure><h1 id="5d5c" class="jx jy hi bd jz ka lj kc kd ke lk kg kh ki ll kk kl km lm ko kp kq ln ks kt ku bi translated">保存和加载模型</h1><p id="9735" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">保存模型很重要，这样你就不用再训练它了。可以调用<code class="du je jf jg jh b">model.save_pretrained</code>和<code class="du je jf jg jh b">tokenizer.save_pretrained</code>函数分别保存模型和分词器。在初始化模型时，你可以通过在<code class="du je jf jg jh b">BertForSequenceClassification.from_pretrained('/path')</code>中给出相同的路径来重新加载模型。</p><pre class="jj jk jl jm fd la jh lb lc aw ld bi"><span id="af57" class="le jy hi jh b fi lf lg l lh li">def save(model, tokenizer):<br/>    output_dir = './output'<br/>    if not os.path.exists(output_dir):<br/>        os.makedirs(output_dir)<br/>    print("Saving model to {}".format(output_dir))<br/>    model.save_pretrained(output_dir)<br/>    tokenizer.save_pretrained(output_dir)</span><span id="ff88" class="le jy hi jh b fi lo lg l lh li">save(model, tokenizer)</span></pre><h1 id="64b2" class="jx jy hi bd jz ka lj kc kd ke lk kg kh ki ll kk kl km lm ko kp kq ln ks kt ku bi translated">结论</h1><p id="d3f7" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">就是这样！您已经成功实现了一个简单的BERT分类器，用于将电影评论分为正面或负面。这是一个非常基本的实现，只是让你开始。希望你喜欢。</p></div></div>    
</body>
</html>