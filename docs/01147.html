<html>
<head>
<title>Deep Anomaly Detection for large scale enterprise data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大规模企业数据的深度异常检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-anomaly-detection-9f19896c8b2?source=collection_archive---------0-----------------------#2019-10-04">https://medium.com/analytics-vidhya/deep-anomaly-detection-9f19896c8b2?source=collection_archive---------0-----------------------#2019-10-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/3c07f7879ea32e9013569b222eb0683e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dbUjPJKmlT5x1jYN.jpg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图片来源:lynda.com</figcaption></figure><div class=""/><div class=""><h2 id="010a" class="pw-subtitle-paragraph iu hw hx bd b iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl dx translated">基于深度学习的自动编码神经网络异常检测</h2></div><p id="1af5" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">一般来说，<a class="ae ki" href="https://www.sciencedirect.com/topics/computer-science/anomaly-detection" rel="noopener ugc nofollow" target="_blank">异常检测</a>旨在帮助区分非常罕见和/或偏离正常的事件。这对于金融行业非常重要，就像在个人银行业务中一样，异常可能是至关重要的事情——就像<a class="ae ki" href="https://www.sciencedirect.com/science/article/pii/S1877050915007103" rel="noopener ugc nofollow" target="_blank">信用卡欺诈</a>。在其他情况下，异常可能是公司寻求利用的东西。其他一些应用包括通信网络入侵、假新闻和误传、医疗保健分析、工业损害检测、制造、安全和监控等。</p><p id="0472" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">本文中展示的用例来自<a class="ae ki" href="https://www.sap.com/index.html" rel="noopener ugc nofollow" target="_blank"> SAP </a>领域，尤其是金融领域。业务目标是发现金融交易中的异常行为。</p><p id="5bea" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在会计信息系统中，一个典型的金融交易会是这样的。</p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kj"><img src="../Images/884cfd00d1e96c89248606f17852c64d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yl55yXjcGqAX8EUfurvkhw.png"/></div></div></figure><p id="b163" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">大多数此类条目属于正常交易，但也有相当一部分显示出恶意行为，结果证明是异常行为。每个金融领域中最广泛使用的用例是检测欺诈，异常检测方法可以在需要大量手动工作的情况下大大有助于检测欺诈。</p><p id="9952" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在本文中，我将讲述一种使用<a class="ae ki" href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/" rel="noopener ugc nofollow" target="_blank"> Autoencoder神经网络</a> (AENN)的前沿异常检测方法。这是一种基于深度学习的异常检测方法。</p><h1 id="8783" class="ko kp hx bd kq kr ks kt ku kv kw kx ky jd kz je la jg lb jh lc jj ld jk le lf bi translated">嗯，关于数据集</h1><p id="33fc" class="pw-post-body-paragraph jm jn hx jo b jp lg iy jr js lh jb ju jv li jx jy jz lj kb kc kd lk kf kg kh hb bi translated">用于这个用例的数据集可以在提供的GitHub链接中找到。这是一个由财务数据组成的合成数据集，经过修改后看起来更类似于人们通常在SAP-ERP系统(尤其是财务和成本控制模块)中观察到的真实数据集。</p><p id="0d65" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">数据集包含FICO BKPF表(包含已过帐日记帐分录标题)和BSEG表(包含已过帐日记帐分录段)中可用的7个分类属性和2个数值属性。</p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ll"><img src="../Images/4645734a46133d6e649332773fc41622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FyjbqEBCgwi3I2aotHw_rA.png"/></div></div></figure><blockquote class="lm ln lo"><p id="5406" class="jm jn lp jo b jp jq iy jr js jt jb ju lq jw jx jy lr ka kb kc ls ke kf kg kh hb bi translated">在数据中还可以找到另一个属性“标签”,它解释了事务的真实性质是正常的还是异常的。这是为了验证模型而提供的，不会在培训部分使用。</p></blockquote><p id="7328" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><strong class="jo hy">异常分类:</strong></p><p id="6fbb" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">通常，在行业中，异常根据用例以多种方式分类。当对通常记录在大规模AIS或ERP系统中的真实日志条目进行详细检查时，可以观察到两个普遍的特征:</p><ul class=""><li id="256a" class="lt lu hx jo b jp jq js jt jv lv jz lw kd lx kh ly lz ma mb bi translated">特定交易属性展示<strong class="jo hy">各种不同的属性值</strong>，例如客户信息、过账子分类账、金额信息以及</li><li id="5103" class="lt lu hx jo b jp mc js md jv me jz mf kd mg kh ly lz ma mb bi translated">交易显示出特定属性值之间的<strong class="jo hy">强依赖性，例如客户信息和支付类型、过账类型和总账之间的依赖性。</strong></li></ul><p id="0002" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">从这一观察中，可以区分两类异常日志条目，即<strong class="jo hy">【全局】</strong>和<strong class="jo hy">【局部】异常。</strong></p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/82aac54f870094a5f028cb290bf13bec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TL--RbCVf4nq16rLFih9Wg.png"/></div></div></figure><p id="0b64" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><strong class="jo hy">全局会计异常</strong>是显示不寻常或罕见的单个属性值的日志条目。此类异常通常与扭曲的属性有关，例如很少使用的分类账或不寻常的过账时间。传统上，审计师在年度审计期间进行的“危险信号”测试旨在捕捉这种异常情况。但是，此类测试通常会导致大量误报警报，因为诸如反向过账、准备金和年终调整等事件通常与低欺诈风险相关。此外，在咨询审计师和法务会计师时，“全球性”异常通常是指“错误”而不是“欺诈”。</p><p id="1e22" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><strong class="jo hy">本地会计异常</strong>是表现出异常或罕见的属性值组合的日志条目，而它们的属性值经常出现，例如异常会计记录、总账账户的不规则组合、几个会计部门使用的用户账户。这种类型的异常明显更难检测，因为犯罪者试图通过模仿常规的活动模式来掩饰他们的活动。因此，此类异常通常会带来很高的欺诈风险，因为它们对应的流程和活动可能不符合组织标准。</p><blockquote class="lm ln lo"><p id="b4b8" class="jm jn lp jo b jp jq iy jr js jt jb ju lq jw jx jy lr ka kb kc ls ke kf kg kh hb bi translated">先决条件:观众应该熟悉神经元和神经网络在深度学习中如何工作的基本知识。<a class="ae ki" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">这里</a>是一个很好的教程，给你一个对神经网络的精确理解。</p></blockquote><h1 id="d4c7" class="ko kp hx bd kq kr ks kt ku kv kw kx ky jd kz je la jg lb jh lc jj ld jk le lf bi translated">使用自动编码器神经网络的异常检测——理论</h1><p id="f6ab" class="pw-post-body-paragraph jm jn hx jo b jp lg iy jr js lh jb ju jv li jx jy jz lj kb kc kd lk kf kg kh hb bi translated">自动编码器已经广泛应用于计算机视觉和语音处理。但鲜为人知的事实是，它们也可以用于异常检测。在本节中，我们将介绍自动编码器神经网络的主要元素。</p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/40ae8876ab1cab8abcc123ad3e8e778e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8wSz9FM9LovCAYR-uyaig.png"/></div></div></figure><p id="8a3f" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">一个典型的自动编码器由两个非线性映射函数组成，称为<strong class="jo hy">编码器</strong> -f(x)和<strong class="jo hy">解码器</strong> -g(x)神经网络。编码器通常遵循神经元减少的漏斗状范例，解码器通常是编码器的对称镜像。存在被称为较低维度的<strong class="jo hy">潜在层</strong>的隐藏中心层，其将是输入数据的压缩丰富表示，足以重构它，将最小化<strong class="jo hy">重构误差</strong>。</p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mj"><img src="../Images/1fc36adf2c83b1e8ffc1213a57458d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YPBJsVDEzLCmrzxP.png"/></div></div></figure><p id="b7cd" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">使用这种算法范式进行异常检测的思想包括两个主要步骤:学习系统的正常行为(基于过去的数据)和实时检测异常行为(通过处理实时数据)。</p><p id="e699" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">由于异常数据集的本质是高度偏向于规则的，网络学习如何重建规则的事务，而对于异常却不能这样做。基于如此高的重构误差，我们可以识别交易是正常的还是异常的。这里我们的损失函数是重建误差本身。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="5c98" class="mp kp hx ml b fi mq mr l ms mt">Loss function(reconstruction error) = arg <strong class="ml hy">min</strong> || x — g(f(x)) ||</span></pre><p id="0131" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在这个用例中，我们使用了由给出的二元交叉熵损失。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="be05" class="mp kp hx ml b fi mq mr l ms mt">−(xlog(x’)+(1−x)log(1−x’))</span></pre><p id="df74" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">x是输入数据，x '是g(f(x))。这是测量给定的两个分布有多相似。损失越低，输入及其重建就越相似。</p><h1 id="16e9" class="ko kp hx bd kq kr ks kt ku kv kw kx ky jd kz je la jg lb jh lc jj ld jk le lf bi translated">履行</h1><blockquote class="lm ln lo"><p id="ebf4" class="jm jn lp jo b jp jq iy jr js jt jb ju lq jw jx jy lr ka kb kc ls ke kf kg kh hb bi translated">注意:这里有点技术性，所以我建议所有非技术人员跳过这一部分。你可以经历它，但不要被它吓倒:)</p></blockquote><p id="b537" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">导入必要的库并设置一些参数。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="28ea" class="mp kp hx ml b fi mq mr l ms mt"># importing utilities<br/>import os<br/>import sys<br/>from datetime import datetime</span><span id="e35c" class="mp kp hx ml b fi mu mr l ms mt"># importing data science libraries<br/>import pandas as pd<br/>import random as rd<br/>import numpy as np</span><span id="7634" class="mp kp hx ml b fi mu mr l ms mt"># importing pytorch libraries<br/>import torch<br/>from torch import nn<br/>from torch import autograd<br/>from torch.utils.data import DataLoader</span><span id="7099" class="mp kp hx ml b fi mu mr l ms mt"># import visualization libraries<br/>import matplotlib.pyplot as plt<br/>from mpl_toolkits.mplot3d import Axes3D<br/>import seaborn as sns<br/>from IPython.display import Image, display<br/>sns.set_style('darkgrid')</span><span id="2bad" class="mp kp hx ml b fi mu mr l ms mt"># ignore potential warnings<br/>import warnings<br/>warnings.filterwarnings("ignore")</span></pre><p id="d69a" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">设置随机种子，如果可以的话使用GPU。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="be52" class="mp kp hx ml b fi mq mr l ms mt">rseed = 1234 <br/>rd.seed(rseed)<br/>np.random.seed(rseed)<br/>torch.manual_seed(rseed) <br/>if (torch.backends.cudnn.version() != None and USE_CUDA == True):<br/>    torch.cuda.manual_seed(rseed)</span><span id="580d" class="mp kp hx ml b fi mu mr l ms mt">USE_CUDA = True</span></pre><p id="053e" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">将数据导入熊猫数据框。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="fc0f" class="mp kp hx ml b fi mq mr l ms mt">ad_dataset = pd.read_csv('./data/fraud_dataset_v2.csv')<br/>ad_dataset.head()</span></pre><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mv"><img src="../Images/0df30ef8cce21bc3bf22a946da6b5dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qHhdnAJxmCzYzBw5Mo3hcA.png"/></div></div></figure><p id="3f29" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">看形状，标注value_counts。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="1a2d" class="mp kp hx ml b fi mq mr l ms mt">ad_dataset.shape</span><span id="8c75" class="mp kp hx ml b fi mu mr l ms mt">Out[#]: (533009, 10)</span><span id="947d" class="mp kp hx ml b fi mu mr l ms mt">ad_dataset.label.value_counts()<br/>Out[#]: regular    532909<br/>         global         70<br/>         local          30<br/>         Name: label, dtype: int64</span></pre><p id="e9c2" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">如您所见，这是一个高度偏向的数据集，对于大多数真实世界的数据来说都是如此。异常占总数据的0.018%。在这种情况下，任何典型的机器学习算法都不会表现得很好。但是本文中展示的方法是一个巧妙的技巧，可以利用自动编码器来发现异常。</p><p id="c1ee" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">因为autoencoder是一种无人监管的技术，所以让我们移除标签以进行进一步处理。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="d7b2" class="mp kp hx ml b fi mq mr l ms mt">label = ad_dataset.pop('label')</span></pre><p id="da69" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">现在让我们把分类属性和数字属性分开。将一键编码添加到分类属性中以对其进行矢量化。对数值变量应用对数比例和最小-最大比例。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="9659" class="mp kp hx ml b fi mq mr l ms mt">categorical_attr = ['KTOSL', 'PRCTR', 'BSCHL', 'HKONT', 'WAERS', 'BUKRS']<br/>ad_dataset_categ_transformed = pd.get_dummies(ad_dataset[categorical_attr])</span><span id="6b1f" class="mp kp hx ml b fi mu mr l ms mt">numeric_attr_names = ['DMBTR', 'WRBTR']</span><span id="a81e" class="mp kp hx ml b fi mu mr l ms mt"># add a small epsilon to eliminate zero values from data for log scaling<br/>numeric_attr = ad_dataset[numeric_attr] + 1e-7<br/>numeric_attr = numeric_attr.apply(np.log)</span><span id="2f4f" class="mp kp hx ml b fi mu mr l ms mt">ad_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())</span></pre><p id="f623" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">连接数字和类别属性。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="3196" class="mp kp hx ml b fi mq mr l ms mt">ad_subset_transformed = pd.concat([ad_dataset_categ_transformed, ad_dataset_numeric_attr], axis = 1)</span><span id="a1e1" class="mp kp hx ml b fi mu mr l ms mt">ad_subset_transformed.shape<br/>Out[#]: (533009, 618)</span></pre><p id="bb8b" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">现在我们来实现编码器网络(618–512–256–128–64–32–16–8–4–3)。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="96ea" class="mp kp hx ml b fi mq mr l ms mt"># implementation of the encoder network<br/>class encoder(nn.Module):</span><span id="7057" class="mp kp hx ml b fi mu mr l ms mt">def __init__(self):</span><span id="066f" class="mp kp hx ml b fi mu mr l ms mt">super(encoder, self).__init__()</span><span id="8aab" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 1 - in 618, out 512<br/>        self.encoder_L1 = nn.Linear(in_features=ori_subset_transformed.shape[1], out_features=512, bias=True) # add linearity <br/>        nn.init.xavier_uniform_(self.encoder_L1.weight) # init weights according to [9]<br/>        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]</span><span id="a91a" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 2 - in 512, out 256<br/>        self.encoder_L2 = nn.Linear(512, 256, bias=True)<br/>        nn.init.xavier_uniform_(self.encoder_L2.weight)<br/>        self.encoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="518c" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 3 - in 256, out 128<br/>        self.encoder_L3 = nn.Linear(256, 128, bias=True)<br/>        nn.init.xavier_uniform_(self.encoder_L3.weight)<br/>        self.encoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="3b64" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 4 - in 128, out 64<br/>        self.encoder_L4 = nn.Linear(128, 64, bias=True)<br/>        nn.init.xavier_uniform_(self.encoder_L4.weight)<br/>        self.encoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="b06e" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 5 - in 64, out 32<br/>        self.encoder_L5 = nn.Linear(64, 32, bias=True)<br/>        nn.init.xavier_uniform_(self.encoder_L5.weight)<br/>        self.encoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="dff9" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 6 - in 32, out 16<br/>        self.encoder_L6 = nn.Linear(32, 16, bias=True)<br/>        nn.init.xavier_uniform_(self.encoder_L6.weight)<br/>        self.encoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="25e0" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 7 - in 16, out 8<br/>        self.encoder_L7 = nn.Linear(16, 8, bias=True)<br/>        nn.init.xavier_uniform_(self.encoder_L7.weight)<br/>        self.encoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="c05c" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 8 - in 8, out 4<br/>        self.encoder_L8 = nn.Linear(8, 4, bias=True)<br/>        nn.init.xavier_uniform_(self.encoder_L8.weight)<br/>        self.encoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="a908" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 9 - in 4, out 3<br/>        self.encoder_L9 = nn.Linear(4, 3, bias=True)<br/>        nn.init.xavier_uniform_(self.encoder_L9.weight)<br/>        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="62b4" class="mp kp hx ml b fi mu mr l ms mt"># init dropout layer with probability p<br/>        self.dropout = nn.Dropout(p=0.0, inplace=True)<br/>        <br/>    def forward(self, x):</span><span id="a812" class="mp kp hx ml b fi mu mr l ms mt"># define forward pass through the network<br/>        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))<br/>        x = self.encoder_R2(self.dropout(self.encoder_L2(x)))<br/>        x = self.encoder_R3(self.dropout(self.encoder_L3(x)))<br/>        x = self.encoder_R4(self.dropout(self.encoder_L4(x)))<br/>        x = self.encoder_R5(self.dropout(self.encoder_L5(x)))<br/>        x = self.encoder_R6(self.dropout(self.encoder_L6(x)))<br/>        x = self.encoder_R7(self.dropout(self.encoder_L7(x)))<br/>        x = self.encoder_R8(self.dropout(self.encoder_L8(x)))<br/>        x = self.encoder_R9(self.encoder_L9(x))</span><span id="bb6d" class="mp kp hx ml b fi mu mr l ms mt">return x</span></pre><p id="274b" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">实例化编码器并放入on</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="ad5d" class="mp kp hx ml b fi mq mr l ms mt"># init training network classes / architectures<br/>encoder_train = encoder()</span><span id="7ee0" class="mp kp hx ml b fi mu mr l ms mt"># push to cuda if cudnn is available<br/>if (torch.backends.cudnn.version() != None and USE_CUDA == True):<br/>    encoder_train = encoder().cuda()</span></pre><p id="87b6" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">现在，解码器网络实现是编码器的对称镜像。(3–4–8–16–32–64–128–256–512–618)</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="0707" class="mp kp hx ml b fi mq mr l ms mt"># implementation of the decoder network<br/>class decoder(nn.Module):</span><span id="3d35" class="mp kp hx ml b fi mu mr l ms mt">def __init__(self):</span><span id="8f57" class="mp kp hx ml b fi mu mr l ms mt">super(decoder, self).__init__()</span><span id="d7bd" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 1 - in 3, out 4<br/>        self.decoder_L1 = nn.Linear(in_features=3, out_features=4, bias=True) # add linearity <br/>        nn.init.xavier_uniform_(self.decoder_L1.weight)  # init weights according to [9]<br/>        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]</span><span id="4e20" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 2 - in 4, out 8<br/>        self.decoder_L2 = nn.Linear(4, 8, bias=True)<br/>        nn.init.xavier_uniform_(self.decoder_L2.weight)<br/>        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="19df" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 3 - in 8, out 16<br/>        self.decoder_L3 = nn.Linear(8, 16, bias=True)<br/>        nn.init.xavier_uniform_(self.decoder_L3.weight)<br/>        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="36fa" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 4 - in 16, out 32<br/>        self.decoder_L4 = nn.Linear(16, 32, bias=True)<br/>        nn.init.xavier_uniform_(self.decoder_L4.weight)<br/>        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="da4c" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 5 - in 32, out 64<br/>        self.decoder_L5 = nn.Linear(32, 64, bias=True)<br/>        nn.init.xavier_uniform_(self.decoder_L5.weight)<br/>        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="cc73" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 6 - in 64, out 128<br/>        self.decoder_L6 = nn.Linear(64, 128, bias=True)<br/>        nn.init.xavier_uniform_(self.decoder_L6.weight)<br/>        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)<br/>        <br/>        # specify layer 7 - in 128, out 256<br/>        self.decoder_L7 = nn.Linear(128, 256, bias=True)<br/>        nn.init.xavier_uniform_(self.decoder_L7.weight)<br/>        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="2b5a" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 8 - in 256, out 512<br/>        self.decoder_L8 = nn.Linear(256, 512, bias=True)<br/>        nn.init.xavier_uniform_(self.decoder_L8.weight)<br/>        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="c742" class="mp kp hx ml b fi mu mr l ms mt"># specify layer 9 - in 512, out 618<br/>        self.decoder_L9 = nn.Linear(in_features=512, out_features=ori_subset_transformed.shape[1], bias=True)<br/>        nn.init.xavier_uniform_(self.decoder_L9.weight)<br/>        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)</span><span id="1537" class="mp kp hx ml b fi mu mr l ms mt"># init dropout layer with probability p<br/>        self.dropout = nn.Dropout(p=0.0, inplace=True)</span><span id="c4bf" class="mp kp hx ml b fi mu mr l ms mt">def forward(self, x):</span><span id="6a96" class="mp kp hx ml b fi mu mr l ms mt"># define forward pass through the network<br/>        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))<br/>        x = self.decoder_R2(self.dropout(self.decoder_L2(x)))<br/>        x = self.decoder_R3(self.dropout(self.decoder_L3(x)))<br/>        x = self.decoder_R4(self.dropout(self.decoder_L4(x)))<br/>        x = self.decoder_R5(self.dropout(self.decoder_L5(x)))<br/>        x = self.decoder_R6(self.dropout(self.decoder_L6(x)))<br/>        x = self.decoder_R7(self.dropout(self.decoder_L7(x)))<br/>        x = self.decoder_R8(self.dropout(self.decoder_L8(x)))<br/>        x = self.decoder_R9(self.decoder_L9(x))<br/>        <br/>        return x</span></pre><p id="6bbf" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">实例化解码器，放到GPU上。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="3e81" class="mp kp hx ml b fi mq mr l ms mt"># init training network classes / architectures<br/>decoder_train = decoder()</span><span id="12dd" class="mp kp hx ml b fi mu mr l ms mt"># push to cuda if cudnn is available<br/>if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):<br/>    decoder_train = decoder().cuda()</span></pre><p id="1de4" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">现在设置损失函数和一些超参数。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="bd1a" class="mp kp hx ml b fi mq mr l ms mt"># define the optimization criterion / loss function<br/>loss_function = nn.BCEWithLogitsLoss(reduction='mean')</span><span id="37cd" class="mp kp hx ml b fi mu mr l ms mt"># define learning rate and optimization strategy<br/>learning_rate = 1e-3<br/>encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)<br/>decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)</span><span id="7def" class="mp kp hx ml b fi mu mr l ms mt"># specify training parameters<br/>num_epochs = 8<br/>mini_batch_size = 128</span></pre><p id="7cb6" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">将数据加载到张量和GPU上。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="cc9c" class="mp kp hx ml b fi mq mr l ms mt"># convert pre-processed data to pytorch tensor<br/>torch_dataset = torch.from_numpy(ad_subset_transformed.values).float()</span><span id="71c7" class="mp kp hx ml b fi mu mr l ms mt"># convert to pytorch tensor - none cuda enabled<br/>dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)<br/># note: we set num_workers to zero to retrieve deterministic results</span><span id="744f" class="mp kp hx ml b fi mu mr l ms mt"># determine if CUDA is available at compute node<br/>if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):<br/>    dataloader = DataLoader(torch_dataset.cuda(), batch_size=mini_batch_size, shuffle=True)</span></pre><p id="3897" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">现在开始我们的训练。(注意:我建议不要复制粘贴下面的代码，因为格式可能会出错。请从下面提到的<a class="ae ki" href="https://github.com/gotorehanahmad/anomaly_detection" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a>获取代码。)</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="bc2c" class="mp kp hx ml b fi mq mr l ms mt"># init collection of mini-batch losses<br/>losses = []</span><span id="f115" class="mp kp hx ml b fi mu mr l ms mt"># convert encoded transactional data to torch Variable<br/>data = autograd.Variable(torch_dataset)</span><span id="7054" class="mp kp hx ml b fi mu mr l ms mt"># train autoencoder model<br/>for epoch in range(num_epochs):</span><span id="32bf" class="mp kp hx ml b fi mu mr l ms mt"># init mini batch counter<br/>    mini_batch_count = 0<br/>    <br/>    # determine if CUDA is available at compute node<br/>    if(torch.backends.cudnn.version() != None) and (USE_CUDA == True):<br/>        <br/>        # set networks / models in GPU mode<br/>        encoder_train.cuda()<br/>        decoder_train.cuda()</span><span id="53a9" class="mp kp hx ml b fi mu mr l ms mt"># set networks in training mode (apply dropout when needed)<br/>    encoder_train.train()<br/>    decoder_train.train()</span><span id="b007" class="mp kp hx ml b fi mu mr l ms mt"># start timer<br/>    start_time = datetime.now()<br/>        <br/>    # iterate over all mini-batches<br/>    for mini_batch_data in dataloader:</span><span id="d354" class="mp kp hx ml b fi mu mr l ms mt"># increase mini batch counter<br/>        mini_batch_count += 1</span><span id="9643" class="mp kp hx ml b fi mu mr l ms mt"># convert mini batch to torch variable<br/>        mini_batch_torch = autograd.Variable(mini_batch_data)</span><span id="ac18" class="mp kp hx ml b fi mu mr l ms mt"># =================== (1) forward pass ============================</span><span id="f966" class="mp kp hx ml b fi mu mr l ms mt"># run forward pass<br/>        z_representation = encoder_train(mini_batch_torch) # encode mini-batch data<br/>        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data<br/>        <br/>        # =================== (2) compute reconstruction loss ======</span><span id="313b" class="mp kp hx ml b fi mu mr l ms mt"># determine reconstruction loss<br/>        reconstruction_loss = loss_function(mini_batch_reconstruction, mini_batch_torch)<br/>        <br/>        # =================== (3) backward pass ====================</span><span id="fd88" class="mp kp hx ml b fi mu mr l ms mt"># reset graph gradients<br/>        decoder_optimizer.zero_grad()<br/>        encoder_optimizer.zero_grad()</span><span id="f698" class="mp kp hx ml b fi mu mr l ms mt"># run backward pass<br/>        reconstruction_loss.backward()<br/>        <br/>        # =================== (4) update model parameters =========</span><span id="f6e7" class="mp kp hx ml b fi mu mr l ms mt"># update network parameters<br/>        decoder_optimizer.step()<br/>        encoder_optimizer.step()</span><span id="dec6" class="mp kp hx ml b fi mu mr l ms mt"># =================== monitor training progress ===================</span><span id="d875" class="mp kp hx ml b fi mu mr l ms mt"># print training progress each 1'000 mini-batches<br/>        if mini_batch_count % 1000 == 0:<br/>            <br/>            # print the training mode: either on GPU or CPU<br/>            mode = 'GPU' if (torch.backends.cudnn.version() != None) and (USE_CUDA == True) else 'CPU'<br/>            <br/>            # print mini batch reconstuction results<br/>            now = datetime.utcnow().strftime("%Y%m%d-%H:%M:%S")<br/>            end_time = datetime.now() - start_time<br/>            print('[LOG {}] training status, epoch: [{:04}/{:04}], batch: {:04}, loss: {}, mode: {}, time required: {}'.format(now, (epoch+1), num_epochs, mini_batch_count, np.round(reconstruction_loss.item(), 4), mode, end_time))</span><span id="09ad" class="mp kp hx ml b fi mu mr l ms mt"># reset timer<br/>            start_time = datetime.now()</span><span id="bb33" class="mp kp hx ml b fi mu mr l ms mt"># =================== evaluate model performance ================<br/>    <br/>    # set networks in evaluation mode (don't apply dropout)<br/>    encoder_train.cpu().eval()<br/>    decoder_train.cpu().eval()</span><span id="b71f" class="mp kp hx ml b fi mu mr l ms mt"># reconstruct encoded transactional data<br/>    reconstruction = decoder_train(encoder_train(data))<br/>    <br/>    # determine reconstruction loss - all transactions<br/>    reconstruction_loss_all = loss_function(reconstruction, data)<br/>            <br/>    # collect reconstruction loss<br/>    losses.extend([reconstruction_loss_all.item()])<br/>    <br/>    # print reconstuction loss results<br/>    now = datetime.utcnow().strftime("%Y%m%d-%H:%M:%S")<br/>    print('[LOG {}] training status, epoch: [{:04}/{:04}], loss: {:.10f}'.format(now, (epoch+1), num_epochs, reconstruction_loss_all.item()))</span><span id="9a88" class="mp kp hx ml b fi mu mr l ms mt"># =================== save model snapshot to disk ================<br/>    <br/>    # save trained encoder model file to disk<br/>    encoder_model_name = "ep_{}_encoder_model.pth".format((epoch+1))<br/>    torch.save(encoder_train.state_dict(), os.path.join("./models", encoder_model_name))</span><span id="65d4" class="mp kp hx ml b fi mu mr l ms mt"># save trained decoder model file to disk<br/>    decoder_model_name = "ep_{}_decoder_model.pth".format((epoch+1))<br/>    torch.save(decoder_train.state_dict(), os.path.join("./models", decoder_model_name))</span></pre><p id="5add" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">绘制损失图。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="5ddb" class="mp kp hx ml b fi mq mr l ms mt"># plot the training progress<br/>plt.plot(range(0, len(losses)), losses)<br/>plt.xlabel('[training epoch]')<br/>plt.xlim([0, len(losses)])<br/>plt.ylabel('[reconstruction-error]')<br/>#plt.ylim([0.0, 1.0])<br/>plt.title('AENN training performance')</span></pre><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mw"><img src="../Images/6b435f1a41e1b98118d67df9335e73f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zIJS5gWUIlu9eEgca12cIQ.png"/></div></div></figure><p id="d1ff" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我们的训练到此结束。现在让我们看看如何利用我们的模型来获得预测。</p><p id="3d5c" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">加载预先训练的模型。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="1c87" class="mp kp hx ml b fi mq mr l ms mt"># restore pretrained model checkpoint<br/>encoder_model_name = "ep_8_encoder_model.pth"<br/>decoder_model_name = "ep_8_decoder_model.pth"</span><span id="26f2" class="mp kp hx ml b fi mu mr l ms mt"># init training network classes / architectures<br/>encoder_eval = encoder()<br/>decoder_eval = decoder()</span><span id="e731" class="mp kp hx ml b fi mu mr l ms mt"># load trained models<br/>encoder_eval.load_state_dict(torch.load(os.path.join("models", encoder_model_name)))<br/>decoder_eval.load_state_dict(torch.load(os.path.join("models", decoder_model_name)))</span></pre><p id="36f3" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">对整个数据进行重建。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="0222" class="mp kp hx ml b fi mq mr l ms mt"># convert encoded transactional data to torch Variable<br/>data = autograd.Variable(torch_dataset)</span><span id="474e" class="mp kp hx ml b fi mu mr l ms mt"># set networks in evaluation mode (don't apply dropout)<br/>encoder_eval.eval()<br/>decoder_eval.eval()</span><span id="9dab" class="mp kp hx ml b fi mu mr l ms mt"># reconstruct encoded transactional data<br/>reconstruction = decoder_eval(encoder_eval(data))</span></pre><p id="be26" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">获得整个数据的重建损失。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="0185" class="mp kp hx ml b fi mq mr l ms mt"># determine reconstruction loss - all transactions<br/>reconstruction_loss_all = loss_function(reconstruction, data)<br/>print(reconstruction_loss_all)</span><span id="b384" class="mp kp hx ml b fi mu mr l ms mt">reconstruction loss: 0.0034663924</span></pre><p id="5a1d" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">确定个别交易的重建损失。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="9e4a" class="mp kp hx ml b fi mq mr l ms mt"># init binary cross entropy errors<br/>reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])</span><span id="40d2" class="mp kp hx ml b fi mu mr l ms mt"># iterate over all detailed reconstructions<br/>for i in range(0, reconstruction.size()[0]):</span><span id="d987" class="mp kp hx ml b fi mu mr l ms mt"># determine reconstruction loss - individual transactions<br/>    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).item()</span></pre><p id="e49d" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">根据贴有标签的重建损失绘制数据点。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="3030" class="mp kp hx ml b fi mq mr l ms mt"># prepare plot<br/>fig = plt.figure()<br/>ax = fig.add_subplot(111)</span><span id="c8f2" class="mp kp hx ml b fi mu mr l ms mt"># assign unique id to transactions<br/>plot_data = np.column_stack((np.arange(len(reconstruction_loss_transaction)), reconstruction_loss_transaction))</span><span id="b6c5" class="mp kp hx ml b fi mu mr l ms mt"># obtain regular transactions as well as global and local anomalies<br/>regular_data = plot_data[label == 'regular']<br/>global_outliers = plot_data[label == 'global']<br/>local_outliers = plot_data[label == 'local']</span><span id="f0a5" class="mp kp hx ml b fi mu mr l ms mt"># plot reconstruction error scatter plot<br/>ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', alpha=0.4, marker="o", label='regular') # plot regular transactions<br/>ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker="^", label='global') # plot global outliers<br/>ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C2', marker="^", label='local') # plot local outliers</span><span id="c10c" class="mp kp hx ml b fi mu mr l ms mt"># add plot legend of transaction classes<br/>ax.legend(loc='best')</span></pre><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mx"><img src="../Images/ef116fd4170a5301bab752195d71d2d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZuI8W2V3y9ljjVrPkjaLCg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">重建错误与交易</figcaption></figure><p id="0958" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">该图显示了所选择的方法如何从高度有偏差的数据集中优雅地发现异常。让我们看看有多少异常被发现。</p><pre class="kk kl km kn fd mk ml mm mn aw mo bi"><span id="0d5b" class="mp kp hx ml b fi mq mr l ms mt">ad_dataset['label'] = label<br/>ad_dataset[reconstruction_loss_transaction &gt;= 0.1].label.value_counts()</span><span id="cab5" class="mp kp hx ml b fi mu mr l ms mt">Out[#]: global    59<br/>        local      2<br/>        Name: label, dtype: int64</span><span id="68b0" class="mp kp hx ml b fi mu mr l ms mt">ad_dataset[(reconstruction_loss_transaction &gt;= 0.018) &amp; (reconstruction_loss_transaction &lt; 0.05)].label.value_counts()</span><span id="4005" class="mp kp hx ml b fi mu mr l ms mt">Out[#]: local   23<br/>        Name: label, dtype: int64</span></pre><p id="0af6" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">如您所见，在70个全球项目中，检测到59个，占84%，在30个本地项目中，检测到23个，占76.6%。考虑到离群值仅占整个数据的0.018%，这比任何其他旧技术都要好得多。</p><p id="cc40" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">这里是用于代码实现的<a class="ae ki" href="https://github.com/gotorehanahmad/anomaly_detection" rel="noopener ugc nofollow" target="_blank"> Github链接</a>以及数据集。</p><p id="2714" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我希望这能让您清楚地理解这种方法以及如何实现它。</p><h1 id="e749" class="ko kp hx bd kq kr ks kt ku kv kw kx ky jd kz je la jg lb jh lc jj ld jk le lf bi translated">结论</h1><p id="08a8" class="pw-post-body-paragraph jm jn hx jo b jp lg iy jr js lh jb ju jv li jx jy jz lj kb kc kd lk kf kg kh hb bi translated">这得出结论，如果设计得好，将深度学习算法应用于经典的结构化数据机器学习问题将会给出有希望的结果。识别正确的算法、适当的损失函数和理想的数据集可以帮助数据科学家利用深度学习，并利用其功能来提高古老方法的性能。本文中提到的用例是关于金融交易的，但深度异常检测的概念可以扩展到其他领域，如制造和营销。</p><h1 id="c828" class="ko kp hx bd kq kr ks kt ku kv kw kx ky jd kz je la jg lb jh lc jj ld jk le lf bi translated">参考</h1><p id="6165" class="pw-post-body-paragraph jm jn hx jo b jp lg iy jr js lh jb ju jv li jx jy jz lj kb kc kd lk kf kg kh hb bi translated"><a class="ae ki" href="https://arxiv.org/abs/1709.05254" rel="noopener ugc nofollow" target="_blank">使用深度自动编码器网络检测大规模会计数据中的异常</a></p><p id="3c50" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><a class="ae ki" href="https://www.researchgate.net/publication/270968895_A_survey_of_anomaly_detection_techniques_in_financial_domain" rel="noopener ugc nofollow" target="_blank">金融领域异常检测技术综述</a></p><p id="ff8e" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><a class="ae ki" href="https://arxiv.org/abs/1901.03407" rel="noopener ugc nofollow" target="_blank">用于异常检测的深度学习:综述</a></p><blockquote class="lm ln lo"><p id="b78f" class="jm jn lp jo b jp jq iy jr js jt jb ju lq jw jx jy lr ka kb kc ls ke kf kg kh hb bi translated"><em class="hx">关于我</em></p></blockquote><p id="9703" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我是<a class="ae ki" href="https://wavelabs.ai/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Wavelabs.ai </a>的资深AI专家。我们Wavelabs帮助您利用人工智能(AI)来彻底改变用户体验并降低成本。我们使用人工智能独特地增强您的产品，以达到您的全部市场潜力。我们试图将尖端研究引入您的应用中。</p><p id="d7f2" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">欢迎访问<a class="ae ki" href="https://wavelabs.ai/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Wavelabs.ai </a>了解更多信息。</p></div><div class="ab cl my mz gp na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="hb hc hd he hf"><p id="601c" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">嗯，这都是在这个职位。感谢阅读:)</p><p id="0123" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">保持好奇！</p><p id="d066" class="pw-post-body-paragraph jm jn hx jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">你可以在<a class="ae ki" href="https://www.linkedin.com/in/rehan-a-18675296?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p></div></div>    
</body>
</html>