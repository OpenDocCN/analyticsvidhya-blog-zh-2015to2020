<html>
<head>
<title>Dialog Augmentation and Intent Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对话增强和意图检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/dialog-augmentation-and-intent-detection-5d76bdf0fcb8?source=collection_archive---------10-----------------------#2020-12-19">https://medium.com/analytics-vidhya/dialog-augmentation-and-intent-detection-5d76bdf0fcb8?source=collection_archive---------10-----------------------#2020-12-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c520e29b3c60a5384b153c0d12e42641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RMALA1RXElL7La5lhjHroQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">米盖尔·Á的照片。来自Pexels的Padri和Aseem Srivastava</figcaption></figure><p id="05ad" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对话系统中的意图检测是将对话系统中的对话推向正确方向的决定性步骤。通常，在构建对话系统时，对语言理解部分的关注较少，从而导致不明确的响应生成。我们将尝试重新构建众所周知的少数民族意图的意图检测问题陈述，并增加具有类似意图的对话，以便更好地理解少数民族意图。所提出的体系结构和机制在dialog上显示了良好的结果，并且这种特殊的增强机制也可以扩展到其他与dialog相关的分类问题。在这项研究中，它的工作将使用最先进的机器学习模型和最先进的深度学习技术，在面向目标的对话系统上使用更少的意图分类器来产生更好的理解。在深度学习时代，对话系统中的意图检测不再是一个经典的机器学习问题。</p><p id="2f2c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="js">下面附上Google Colab笔记本、GitHub代码和数据集链接，供大家参考。</em></p><h1 id="b646" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">直觉</h1><p id="e750" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">在用机器学习革新对话系统时，对话对话中的意图检测(ID)是非常关键的阶段。利用自然语言理解(LU)，意图检测有助于在对话管理中即兴语言生成和进步。以前，意图检测的任务已经用不同的数据集做过多次。这里，我们将尝试精确地检测训练样本数量非常少的缩进，并遇到聊天机器人无法理解用户意图的阶段。</p><p id="4536" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于对话系统，尤其是面向目标的对话系统，聊天机器人应该足够健壮，能够检测用户的意图，而不需要考虑任何特殊的假设。这就是我们处理的地方，即使在数据集中分类的意图数量较少，也能在测试阶段正确识别缩进。为此，我们将使用由<em class="js"> Sonos于2017年6月</em>发布的ATIS数据集。我们将在数据集部分看到更多关于数据的内容。</p><blockquote class="kw kx ky"><p id="1992" class="iu iv js iw b ix iy iz ja jb jc jd je kz jg jh ji la jk jl jm lb jo jp jq jr hb bi translated">复述生成技术利用具有少数人意图的对话并扩充相似的对话导致对对话系统中的对话有更好的理解。</p></blockquote><h1 id="1ad9" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">为什么这很重要？</h1><p id="32de" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">对话系统是为了完成他们所接受的任务或领域而构建的。一般来说，研究者很少关注带有意图部分的“路”来诊断话语中很少出现的意图。在各种机器学习技术的帮助下，ID将帮助我们更好地理解人类的意图和对话流。</p><p id="d24b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">同时，我们的输出也会对对话管理系统产生积极的影响，从而重现更好的语言生成模型。</p><p id="4416" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">根据提取的信息，系统可以决定要采取的适当行动，以帮助用户实现他们的需求。逻辑单元应用在我们的日常生活中变得越来越重要。许多设备，如智能手机，都有使用LU技术构建的个人助理。<br/>帮助中心等多个产品支持系统使用该ID，以减少对大量员工复制粘贴无聊的常见问题解答的需求。聊天机器人，自动电子邮件应答器，回答来自知识库的问题和答案的推荐者。</p><h1 id="9e31" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">ATIS数据集</h1><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/21b3bb00e67f1d75dd096cc5fc4ea0cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*tw3GnqpU0u3H5vvkP1qxpA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">表1:上表显示了从用户到机器人的示例对话框以及ATIS数据集中相应的注释意图。</figcaption></figure><p id="86ac" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">用于ID任务的数据集是<strong class="iw hj">ATIS-航空公司旅行信息系统，这是最初由微软发布的基准数据集。</strong>在数据集中，我们正在处理与航空旅行和航班调度协议相关的8个不同意图。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/9059527f6c61f7ab54c12c5e4236a013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*mC-6SC004Y6OWrqz_cusEA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图1:从项目中的ATIS数据集使用的意图及其对应的频率图。</figcaption></figure><p id="c283" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">此外，ATIS数据集包含4，978个训练样本以及943个词汇。基准数据集通常也用于槽填充问题语句，提供129个槽计数。我们感兴趣的部分是意图计数，总计26。对于我们的实验，我们要处理5种不同的意图。</p><h1 id="2c72" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">系统模型化</h1><p id="c12f" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">我们的方法是两种不同成分的混合，首先通过增加对话话语的释义来利用具有少数意图的对话。因为这些话语属于与父对话相同的意图，所以ATIS数据集可以被扩充三次。在我们的工作中，我们增加了来自4个次要意图类的对话数据。为了生成释义，我们使用了一个预先训练好的T5模型。其次，伴随着这些对话，在BERT嵌入的基础上从头开始训练神经网络。</p><p id="8c41" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在我们的方法中，我们结合前馈神经网络的实现，使用了变压器架构中基于BERT的编码器部分。整个架构如图2所示。</p><p id="600f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">作为输入，我们使用了在无监督语料库上进行预处理的BERT嵌入。这些输入被馈送到前馈神经网络(图2)。该模块由一个液滴层和两个隐藏尺寸为512的前馈层组成。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/e577c88ad739329ebd2a1d46fa45cf35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Zknj1_Mj7Ht0YXGFwkwMA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图2:提出的模型的体系结构，进一步分为两个部分:(I)释义生成部分。(二)分类器前馈神经网络。接收到的输出是意图类的软最大概率分布</figcaption></figure><h2 id="912a" class="lj ju hi bd jv lk ll lm jz ln lo lp kd jf lq lr kh jj ls lt kl jn lu lv kp lw bi translated">1.释义生成</h2><p id="1b81" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">为了处理有限数据问题，我们首次引入了新的数据增强集成意图检测问题。我们的复述生成模型使用T5模型，该模型对于许多NLP任务是鲁棒的。T5是来自Google的transformer模型，它以端到端的方式进行训练，以文本作为输入，以修改后的文本作为输出。T5模型使用在巨大文本语料库上训练的相同文本到文本转换器，在许多NLP问题语句上实现了最先进的结果，如摘要、句子提取、机器翻译等。</p><p id="d286" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这个项目中，我们以原始句子作为输入，以转述(ATIS数据集的重复句子)句子作为输出来训练T5。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/818bb1a6767e8ce380b5ff5a1b2561b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SADnVbuMuBwwWtqql7KfFg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图3:使用Google T5模型的对话增强通过生成对话的释义保持多数类不变，利用了少数类的对话。</figcaption></figure><h2 id="77bd" class="lj ju hi bd jv lk ll lm jz ln lo lp kd jf lq lr kh jj ls lt kl jn lu lv kp lw bi translated">2.意图分类器神经网络</h2><p id="1153" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">我们使用BERT嵌入从3层前馈神经网络传递，给出5个输出，最后通过softmax层。</p><p id="d316" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的前馈神经网络在开始时包含一个丢弃层，丢弃值为0.5，在其上应用ReLU激活函数。来自该激活函数的输出通过第一线性层，将768维作为输入，并产生512维输出到下一隐藏层。现在，第二线性层为意图分类产生5个输出，其进一步通过softmax函数以获得概率分布。<br/>在i5处理器的常规8GB RAM上对模型进行30个时期和32个批量的训练花费了2个小时用于数据扩充，并且花费了大约12个小时在分类架构上训练模型。</p><h1 id="cee8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结果</h1><p id="8c24" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">我们使用一个常规的准确性度量来比较基线和其他模型的结果。我们还评估了我们的模型的F1分数，但是由于没有使用其他模型，所以比较变得无关紧要。然而，精确度、召回率、f1分数都是在代码文件中计算和显示的。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/86a7c96747d721c7bd84fc3bf6ded025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*1oXCBbRayQO8fPsDuS7BdA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">表2:我们消融研究的结果和最终架构的结果。</figcaption></figure><p id="ea98" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">上述项目的代码/数据可以在我的GitHub链接中找到(请给一些星星！):<a class="ae lz" href="https://github.com/as3eem/Dialog-Intent-Augmentation-and-Detection" rel="noopener ugc nofollow" target="_blank">https://github . com/as 3 EEM/Dialog-Intent-Augmentation-and-Detection</a></p></div></div>    
</body>
</html>