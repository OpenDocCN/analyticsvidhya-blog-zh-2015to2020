<html>
<head>
<title>Anatomy of a Convolutional Neural Network Layer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络层的剖析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/anatomy-of-a-convolutional-neural-network-layer-d5a5e46f9b58?source=collection_archive---------15-----------------------#2019-10-11">https://medium.com/analytics-vidhya/anatomy-of-a-convolutional-neural-network-layer-d5a5e46f9b58?source=collection_archive---------15-----------------------#2019-10-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0bea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我写这些是因为我对CNN的解释感到有点困惑。我在2D看到了一些卷积如何工作的很好的演示——例如，<a class="ae jd" href="http://setosa.io/ev/image-kernels/" rel="noopener ugc nofollow" target="_blank">http://setosa.io/ev/image-kernels/</a>,但没有看到卷积层中所有权重如何聚集在一起计算结果的简明描述。所以，这里有一个简短的总结:</p><p id="5578" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">卷积层的总体目的是将一个秩为3的张量(三维矩阵)映射到另一个张量。这三个维度是通道的数量、高度和宽度。首先，我们通常有一个3通道(RGB)的输入图像。当我们浏览CNN时，频道的数量通常会上升，而高度/宽度会下降(尽管这取决于NN架构师)。因此，一个卷积层完成的转换的总体情况是:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/831aeca951b525e2e3ae958e6125fabb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jtbmMN6PpaPDF-7nvtNTNg.png"/></div></div></figure><p id="4903" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> in_nchannels </strong>由前一层决定，而<strong class="ih hj"> out_nchannels </strong>可以由NN设计者自由选择。</p><p id="a505" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种转变是怎么做到的？对于每个输出通道，我们有一个权重核，它是一个大小为3的秩张量<br/><strong class="ih hj">in _ nchannels</strong>*<strong class="ih hj">kernel _ size</strong>*<strong class="ih hj">kernel _ size<br/></strong>核的大小也由NN设计者选择，通常第一层使用5或7，其他层使用3。在输入张量上扫描核以给出秩2输出；对于扫描中的每个位置，我们取内核权重与相应输入片段中的激活的点积:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jq"><img src="../Images/29d17936390ef564acd0ffd77dd9f6fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZQw6b0l4uvFCl2jrt2l7pg.png"/></div></div></figure><p id="2868" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们堆叠所有输出通道，以给出秩为3的结果；这就是我们如何得到上面第一幅图中的输出。由于我们每个输出通道都需要一个内核，因此该层中所有内核的权重总数为:<br/><strong class="ih hj">out _ n channels</strong>*<strong class="ih hj">in _ n channels</strong>*<strong class="ih hj">kernel _ size</strong>*<strong class="ih hj">kernel _ size<br/></strong>示例:Resnet34的第一层有3个输入通道，64个输出通道，内核大小为7。层中的总权重数为:<br/>64 * 3 * 7 * 7 = 9408</p><p id="c9c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步幅:扫描从左上方开始，然后以小增量重复改变位置。这个增量称为步幅。跨距的典型值为1(保持图像高度/宽度大致不变)或2(将图像高度/宽度大致减半)。</p><p id="88c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">填充:如果没有填充，卷积层将通过一个或几个像素的边界来减小图像大小。例如，步长为1的卷积会将高度/宽度减少kernel_size-1。可以使用填充来避免这种情况。我将把细节作为一个练习，但是作为两个特殊的例子，如果步幅是1，填充(kernel_size-1)/2将保持相同的高度/宽度，而如果步幅是2，它将高度/宽度减半。例如:<br/> kernel_size 3，stride 1，padding 1 —保持相同的高度/宽度<br/> kernel_size 3，stride 2，padding 1 —将高度/宽度减半<br/> kernel_size 7，stride 2，padding 3 —将高度/宽度减半</p><p id="6bea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:上面的描述有些简化，因为我们实际上是一次处理一小批图像。这给输入和输出增加了一个额外的维度(小批量)。您可以将卷积层视为一个循环，它对小批量中的每个图像进行上述处理，然后沿着额外的维度堆叠输出。</p></div></div>    
</body>
</html>