<html>
<head>
<title>End-to-End Memory Network : Highlights</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">端到端内存网络:亮点</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/end-to-end-memory-network-highlights-29dba9a6d59b?source=collection_archive---------5-----------------------#2020-03-01">https://medium.com/analytics-vidhya/end-to-end-memory-network-highlights-29dba9a6d59b?source=collection_archive---------5-----------------------#2020-03-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4a2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，由于Siri、Alexa、Cortana和谷歌助手，神经网络越来越受欢迎。越来越多的应用程序将神经网络与自然语言处理结合使用，以创建将定义21世纪的产品。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/95adf2fdcd6329a032aeb5dd426f02d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rEuhxUxTmhcC9_ic"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">托马斯·科尔诺斯基在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="a3f3" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">背景</h1><p id="6ad7" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">神经网络是模拟动物大脑功能的松散算法。它由一组相互连接的神经元、连接、权重和传播函数组成。这些组件一起帮助学习和训练模型。神经网络有两大类，即前馈网络和递归网络。前馈网络是神经元的有向无环图，而递归网络允许神经元在相同或先前层之间具有连接。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kx"><img src="../Images/c71f3b5161907c78a20cdc393597ab25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*oD7Dz2jDDV7in2CVcPxPKA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="https://www.researchgate.net/profile/Dana_Hughes3/publication/305881131/figure/fig5/AS:391681317851147@1470395511494/Feed-forward-and-recurrent-neural-networks.png" rel="noopener ugc nofollow" target="_blank">前馈vs递归网络</a></figcaption></figure><h1 id="1bb4" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">递归神经网络</h1><p id="d9db" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">RNN的最大优势之一是，它可以将信息存储为记忆，因此有助于利用上下文信息。长短期记忆(LSTM)是最受欢迎的RNN架构之一。每个LSTM单元由一个单元和三个门组成，即输入门、输出门和遗忘门。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ky"><img src="../Images/e4f76c75837e4ea2458e0aa32cf33e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*KJGCL1eH0McG7BTOJtrSQQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="https://acheremskoy.files.wordpress.com/2017/01/lstm.png" rel="noopener ugc nofollow" target="_blank"> LSTM网</a></figcaption></figure><p id="8277" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，顾名思义，LSTM使用的记忆概念是短期的。有许多存储器网络通过提供相对长期的存储器来解决这个问题。端到端存储网络就是这样一个例子。</p><h1 id="b866" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">端到端存储网络</h1><p id="cba8" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">该网络的模型架构由<em class="kz"> Sainbayar Sukhbaatar、Arthur Szlam、Jason Weston和Rob Fergus在<a class="ae jt" href="https://arxiv.org/abs/1503.08895" rel="noopener ugc nofollow" target="_blank">论文</a>中发表。</em>为了便于理解，让我们从一个示例开始，该示例中的解决方案是将内存作为架构的一部分:</p><p id="f668" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">故事:</p><ol class=""><li id="8ae5" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lf lg lh li bi translated">玛丽买了牛奶。</li><li id="9553" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">约翰搬到了卧室。</li><li id="6962" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">桑德拉回到了厨房。</li><li id="d742" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">玛丽走到走廊上。</li></ol><p id="e025" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查询:牛奶在哪里？</p><p id="9a2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">答案:走廊</p><p id="cef0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">按照这个例子，在回答查询之前，我们需要考虑整个故事的上下文。在这种使用情况下，端到端内存网络的使用变得非常重要。该存储器网络的架构如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lo"><img src="../Images/2da043e6d28d7c75cbb2eb83c75fe0b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gyuJ699Pyp8ScM-W2C4lHg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="https://arxiv.org/abs/1503.08895" rel="noopener ugc nofollow" target="_blank">图片来源:端到端存储网络</a></figcaption></figure><p id="cd28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更简单的理解，让我们把它分成两部分。</p><p id="bb3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">架构的第一部分有助于为查询找到相关的句子。它从将查询转换成大小为k的单词嵌入开始。作为该过程的一部分，查询q首先被转换成大小为V的向量(V是所使用的词汇的大小)。例如，以下是查询“牛奶在哪里”(解析后)的向量:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lp"><img src="../Images/ebfa97a8a2bd57a00a62a030411bf1ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyUwv36QbnpLjAVR4L_Y3A.png"/></div></div></figure><p id="996b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后用一个包字模型或者嵌入B(k × V)，把向量转换成一个大小为k的字嵌入，设向量最后叫做u(大小为k)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/ee35a4ca46c17ea6a6fbd421b93002ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*GSXEvfwUyA5gpXEoFBeKJw.png"/></div></figure><p id="99fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步，故事需要转化为记忆。与上面的方法类似，每个句子都被解析，然后使用嵌入A(k × V)编码成大小为k的向量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/a8f5271831b0d7574eecdd8a591bbba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*a7lGklagxw3I41HtBkXH6A.png"/></div></figure><p id="ca83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将故事和查询作为向量嵌入。我们取查询和每个内存向量的内积，然后进行softmax运算，以找到最佳匹配。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/cff853691b0c5f22d69ce311cea7dbae.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*XwnEur4Xky71WnlpNYDYng.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lt"><img src="../Images/d0bbc078b0e1cd2fb552891098fca45f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-GGBS9lUlelBh2Muy_87Q.png"/></div></div></figure><p id="73d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个阶段，我们已经找到了与查询最相关的句子。例如，如果查询是“牛奶在哪里”，基于计算的概率向量，找到的相关句子是“玛丽得到了牛奶”。</p><p id="e6a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第二部分，将计算查询的最终答案。我们将通过使用嵌入C(k × V)将故事的句子编码为向量来重新开始。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/bb7d1e070ab29407c879d78ca47385a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*n8qoOH1oOqz8KxPOljqJGA.png"/></div></figure><p id="a12f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用上一部分的概率向量，我们将如下计算输出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/bf4fb61f1cdcc2c7419dafc1ac002a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/1*JCPRDc0zRlI1tL7ikKWTLA.png"/></div></figure><p id="3e8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更容易解释，我们将之前找到的查询和相关句子(即“牛奶在哪里”和“玛丽得到了牛奶”的组合)作为新的查询进行评估。使用这个新的查询，并再次将每个句子作为记忆向量，我们试图从故事的上下文中推断出答案。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/9a72ed6dfcd38caa68d746f8b337bb03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XLwjbBKi4MalZOxQDjunuw.png"/></div></div></figure><p id="40cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，借助于矩阵W (V × k ),通过下式获得输出预测。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/84c6b44b8bdd9609f2108fe834d810cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*mlKkuFJgXVnP2PYPBL_CGQ.png"/></div></figure><h1 id="a55d" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结论</h1><p id="d8da" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">我们可以用大量的故事来训练这个网络，以及相应的查询和回答。像LSTM一样，可以增加架构的复杂性，以包括多个层，从而提高结果的准确性。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ly"><img src="../Images/4327e0beecd4c95bc33c8103501a6319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l9kmm19SDJ3NT-kNwgtXEA.png"/></div></div></figure></div></div>    
</body>
</html>