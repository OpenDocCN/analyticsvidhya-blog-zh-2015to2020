<html>
<head>
<title>Reinforcement learning with A3C</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用A3C进行强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-with-a3c-20837aafe0ca?source=collection_archive---------7-----------------------#2020-05-22">https://medium.com/analytics-vidhya/reinforcement-learning-with-a3c-20837aafe0ca?source=collection_archive---------7-----------------------#2020-05-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a3965c523a85272af6761efcc2058d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vhHURLxFoRlPkIe9"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">乔治·特罗瓦托在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="e4eb" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">这篇文章的动机来自我最近参加的一个比赛，我们必须在Atari SpaceInvaders游戏上训练一个模型，并最大化代理在100次运行中获得的分数。</p><p id="b07c" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">鉴于这是我第一次体验强化学习，我从深度Q网络和它周围的变体开始。虽然测试分数对初学者来说很令人满意，但它相当不稳定，需要大量的训练才能获得好成绩。</p><p id="5e17" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">接下来是A3C——这是一种由谷歌Deep Mind开发的强化学习算法，它可以在短时间内达到的分数完全击败了像Deep Q Networks (DQN)这样的大多数算法。</p><p id="69d1" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">A3C代表异步优势行动者-批评家，其中</p><h2 id="02bb" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku bi translated">异步的</h2><p id="23b5" class="pw-post-body-paragraph jc jd hi je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hb bi translated">意味着多重处理。在这里，多个代理一起解决同一个问题，并相互分享他们所学到的信息。随着许多人试图解决问题，解决方案以更快的方式达成。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/b2c18cbdfcfd4b93ebc3098bef7d6197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ATMB-IiLRFdScUo0uMHKg.png"/></div></div></figure><p id="a4a7" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">这些代理中的每一个都同时与它自己的环境副本进行交互。这确实比只有一个代理更有效，因为每个代理的经历对其他代理来说都是独立和独特的。这样我们就有了不同的经验。</p><h2 id="a2c0" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku bi translated">演员兼评论家</h2><p id="628f" class="pw-post-body-paragraph jc jd hi je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hb bi translated">演员-评论家模型基本上是深度卷积Q-学习模型，其中代理实现Q-学习。这里的输入是图像(当前状态的快照)，它们被输入到一个深度卷积神经网络中。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/0897bf0f66e021236d6681e88e169c28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5m-G7YY7X5Vqw0SWj9rG1Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">鸣谢:安尼施·法德尼斯</figcaption></figure><p id="a818" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">在一个基本的深度卷积Q-学习模型中，输出将是代理对给定状态可能采取的动作的Q值。然而在A3C中，有两个输出，一个是不同动作的q值，另一个是计算代理实际所处状态的值。</p><h2 id="0071" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku bi translated">优势</h2><p id="7e37" class="pw-post-body-paragraph jc jd hi je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hb bi translated">优势是这样一个值，它告诉我们某个动作与基于该状态的预期平均值相比是否有改进。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/26e4b31d6efe15fc5f01c3177cee29ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DSw3u_aZQ_HIXu2sL97y3g.png"/></div></div></figure><p id="ddf1" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">Q(s，a)指的是Q值或在某一状态下采取行动的预期未来回报。V(s)指的是处于某种状态的价值。该模型的目标是最大化优势价值</p></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="244e" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">现在我们已经建立了基本的理解，让我们一起来理解这个模型的完整工作。将它们结合在一起的一个主要组件是共享内存。</p><h2 id="db23" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku bi translated">记忆</h2><p id="15fa" class="pw-post-body-paragraph jc jd hi je b jf kv jh ji jj kw jl jm jn kx jp jq jr ky jt ju jv kz jx jy jz hb bi translated">我们利用长短期记忆(LSTM)细胞来实现这一点。从深度卷积Q网络得到的输出现在被传递到LSTM层，该层将把值传递到全连接层。LSTM层为模型提供了记忆，以记住过去的经验并据此做出决策。</p><p id="c4c6" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">全连接层的最终输出，从该层为参与者神经网络选择动作。该值还被传递到critic神经网络，在那里该值被更新。通过计算批评家的价值损失和执行者的策略损失来更新神经网络的权重，然后通过网络反向传播误差。</p><p id="f9aa" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">这种算法是目前强化学习领域的最新技术。它已被证明是成功的各种游戏环境的分数，这是很难实现的人类玩家在短时间内单独。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/1b7bc6096d6e3944671367d86caf6ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/1*_SLKsfQPyf1BYj5KUBmWLQ.gif"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">A3C训练有素的模型玩太空入侵者</figcaption></figure><p id="a3da" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">其中一个主要的进步是AlphaGo，它是一个人工智能，在古代围棋比赛中击败了世界上最好的棋手。</p><p id="3145" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">如果你对游戏着迷，希望看到人工智能打败游戏，一定要多看看强化学习。这是一个非常有趣的领域，随着人们提出不同的策略和想法来解决问题，这个领域正在不断发展。很快强化学习将解决现实世界的情况，直到那时，继续学习，继续探索！</p></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><h2 id="5422" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jn kl km kn jr ko kp kq jv kr ks kt ku bi translated">参考</h2><ul class=""><li id="08db" class="li lj hi je b jf kv jj kw jn lk jr ll jv lm jz ln lo lp lq bi translated"><a class="ae iu" href="https://arxiv.org/abs/1602.01783" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1602.01783</a></li><li id="0842" class="li lj hi je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated"><a class="ae iu" rel="noopener" href="/@samsam23122001/the-advantage-of-the-asynchronous-actor-critic-algorithm-d15f5afbf52a">https://medium . com/emergent-future/simple-reinforcement-learning-with-tensor flow-part-8-asynchronous-actor-critic-agents-a3c-c 88 f 72 a5 e9 f 2</a></li><li id="fff2" class="li lj hi je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">【https://pathmind.com/wiki/deep-reinforcement-learning T4】</li><li id="04ef" class="li lj hi je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated">大卫·西尔弗的RL课程</li><li id="f483" class="li lj hi je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated"><a class="ae iu" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf</a></li><li id="716b" class="li lj hi je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated"><a class="ae iu" href="https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb" rel="noopener" target="_blank">https://towards data science . com/dqn-part-1-vanilla-deep-q-networks-6 EB 4a 00 feb FB</a></li><li id="3d29" class="li lj hi je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated"><a class="ae iu" href="https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b" rel="noopener" target="_blank">https://towards data science . com/welcome-to-deep-reinforcement-learning-part-1-dqn-C3 cab 4d 41 b 6 b</a></li></ul><h1 id="784b" class="lw kb hi bd kc lx ly lz kg ma mb mc kk md me mf kn mg mh mi kq mj mk ml kt mm bi translated">与我联系</h1><ul class=""><li id="c978" class="li lj hi je b jf kv jj kw jn lk jr ll jv lm jz ln lo lp lq bi translated"><a class="ae iu" href="https://www.linkedin.com/in/nihal-das/" rel="noopener ugc nofollow" target="_blank"> <strong class="je hj">领英</strong>T3】</a></li><li id="4845" class="li lj hi je b jf lr jj ls jn lt jr lu jv lv jz ln lo lp lq bi translated"><a class="ae iu" href="https://github.com/Nihal2409/" rel="noopener ugc nofollow" target="_blank">T5】GitHubT7】</a></li></ul><p id="b832" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><strong class="je hj"> ***感谢大家阅读本文。非常感谢您的建议！*** </strong></p></div></div>    
</body>
</html>