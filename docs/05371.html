<html>
<head>
<title>Using machine learning to identify accents in spectrograms of speech</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习识别语音频谱图中的重音</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/using-machine-learning-to-identify-accents-in-spectrograms-of-speech-5db91c191b6b?source=collection_archive---------7-----------------------#2020-04-19">https://medium.com/analytics-vidhya/using-machine-learning-to-identify-accents-in-spectrograms-of-speech-5db91c191b6b?source=collection_archive---------7-----------------------#2020-04-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/598ef3a9d24cc3965e4b1e321ca8f03d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*D-_Mnvd5g4cC8q0d1K1sUw.png"/></div></figure><div class=""/><div class=""><h2 id="87d7" class="pw-subtitle-paragraph im ho hp bd b in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dx translated"><span class="l je jf jg bm jh ji jj jk jl di">ML竞赛中的一个</span> TF Keras卷积模型。第一部分。</h2></div><h2 id="9e23" class="jm jn hp bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">描述</h2><p id="ad6d" class="pw-post-body-paragraph kk kl hp km b kn ko iq kp kq kr it ks jx kt ku kv kb kw kx ky kf kz la lb lc hb bi translated">语音识别软件使我们的设备能够响应我们的语音。我们在手机、汽车和家用电器中都可以看到它。但是对于有口音的人来说——甚至是同一国家不同地区的方言和慢吞吞的声音——人工智能说话者可能看起来非常不同:注意力不集中，反应迟钝，甚至孤立。研究人员发现，与母语为英语的人相比，智能说话者在解析非母语人士的语音时会多犯大约30%的错误。其他研究表明，语音识别软件通常对男性来说比女性更适用于T2。</p><p id="405b" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">算法偏差通常源于它们被训练的数据集。改善非母语人士使用语音识别软件的体验的方法之一是在一组不同的语音样本上训练算法。现有语音样本的口音检测有助于生成这些训练数据集，这是缩小“口音差距”和消除语音识别软件偏见的重要一步。</p><h2 id="276a" class="jm jn hp bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">关于数据</h2><p id="a3a8" class="pw-post-body-paragraph kk kl hp km b kn ko iq kp kq kr it ks jx kt ku kv kb kw kx ky kf kz la lb lc hb bi translated"><a class="ae ld" href="https://en.wikipedia.org/wiki/Spectrogram" rel="noopener ugc nofollow" target="_blank">声谱图</a>是声音随时间变化的各种频率的直观表示。x轴代表时间(以秒为单位)，y轴代表频率(以赫兹为单位)。颜色表示特定时间特定频率的振幅(即声音有多大)。</p><figure class="lk ll lm ln fd hk er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es lj"><img src="../Images/e36f7c31a88da9d3a4d9471ad79d6e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GoeOm2EqmtGKJ9bZnnl82A.png"/></div></div></figure><p id="6b22" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">这些频谱图是从<a class="ae ld" href="https://voice.mozilla.org/en/datasets" rel="noopener ugc nofollow" target="_blank"> Mozilla Common Voice数据集</a>中的音频样本生成的。每个语音剪辑以22，050 Hz采样，并包含来自以下三个国家之一的口音:加拿大、印度和英国。有关光谱图的更多信息，请参见<a class="ae ld" href="https://datasciencecapstone.org/competitions/16/identifying-accents-speech/page/49/" rel="noopener ugc nofollow" target="_blank">主页</a>。</p><h2 id="3766" class="jm jn hp bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">用于图像分类的卷积神经网络</h2><p id="9fb6" class="pw-post-body-paragraph kk kl hp km b kn ko iq kp kq kr it ks jx kt ku kv kb kw kx ky kf kz la lb lc hb bi translated">卷积神经网络(<strong class="km hq"> ConvNets </strong>或<strong class="km hq">CNN</strong>)是一种神经网络，一种深度学习模型，已经在许多分类图像能力和领域(如图像识别、对象检测、图像分割)中证明了很好的结果。他们正在为机器人和自动驾驶汽车的视觉提供动力。</p><blockquote class="ls lt lu"><p id="8526" class="kk kl lv km b kn le iq kp kq lf it ks lw lg ku kv lx lh kx ky ly li la lb lc hb bi translated">CNN与普通的NN(多层感知器)非常相似，但它们“明确假设输入是图像，这允许我们将某些属性编码到架构中。这使得转发功能更有效地实现，并极大地减少了网络中的参数数量”。[2]</p></blockquote><p id="e0ae" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">常规神经网络不能很好地与图像缩放，接收32×32×3图像的神经元将具有3072个权重。具有更大尺寸的图像的多层网络可以产生数百万或数十亿个参数，因此它在计算上非常昂贵，并且会导致过拟合。<em class="lv">“在一个ConvNet中，一个层中的神经元只会连接到它之前的层的一小部分区域，而不是以完全连接的方式连接所有的神经元。在ConvNet架构结束时，我们将把整个图像简化为一个单一的类得分向量。”</em>【2】视觉识别的卷积神经网络CS231 Standford。</p><figure class="lk ll lm ln fd hk er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es hg"><img src="../Images/69b2eff93e9b259715143be1edc4ac5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*TsZ_gq-phMdM3O1igLkD6w.jpeg"/></div></div></figure><p id="e73f" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">一个简单的ConvNet是一系列层，ConvNet的每一层都通过一个可微函数将一个激活量转换为另一个激活量。我们使用三种主要类型的层来构建ConvNet架构:<strong class="km hq">卷积层</strong>、<strong class="km hq">池层</strong>和<strong class="km hq">全连接层</strong>(与常规神经网络中看到的完全一样)。我们将堆叠这些层，形成一个完整的ConvNet <strong class="km hq">架构。详细解释见参考文献[2]。</strong></p><p id="dac0" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">在下一张图中，我们可以观察到CNN的三个基本层，摘自<a class="ae ld" href="https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/" rel="noopener ugc nofollow" target="_blank">一篇解释清楚的博文</a>:</p><figure class="lk ll lm ln fd hk er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es lz"><img src="../Images/b3d81229465d8559638b8798982bf4db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YonJ5F70SRmZ0EpOUVkhFw.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">CNN图层图。来源:<a class="ae ld" href="https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/" rel="noopener ugc nofollow" target="_blank">https://adeshpande 3 . github . io/A-初学者% 27s-理解指南-卷积神经网络/ </a></figcaption></figure><h2 id="1c1c" class="jm jn hp bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">ConvNet中的层</h2><p id="e711" class="pw-post-body-paragraph kk kl hp km b kn ko iq kp kq kr it ks jx kt ku kv kb kw kx ky kf kz la lb lc hb bi translated">标准ConvNet中有四种主要操作:</p><ol class=""><li id="dd29" class="me mf hp km b kn le kq lf jx mg kb mh kf mi lc mj mk ml mm bi translated">盘旋</li><li id="12c4" class="me mf hp km b kn mn kq mo jx mp kb mq kf mr lc mj mk ml mm bi translated">非线性(ReLU)</li><li id="bd69" class="me mf hp km b kn mn kq mo jx mp kb mq kf mr lc mj mk ml mm bi translated">混合或二次抽样</li><li id="8822" class="me mf hp km b kn mn kq mo jx mp kb mq kf mr lc mj mk ml mm bi translated">分类(全连接层)</li></ol><blockquote class="ls lt lu"><p id="e796" class="kk kl lv km b kn le iq kp kq lf it ks lw lg ku kv lx lh kx ky ly li la lb lc hb bi translated">“这种架构基于最早的卷积神经网络之一LeNet，它有助于推动深度学习领域。<a class="ae ld" href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" rel="noopener ugc nofollow" target="_blank">Yann le Cun的这项开创性工作自1988年以来经过多次成功迭代后被命名为LeNet5 </a>。[5]卷积神经网络的直观解释</p></blockquote><p id="e97a" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated"><strong class="km hq">卷积层</strong>:一个输入图像是像素的矩阵，HxWxC，H代表高度，W代表宽度，C代表通道。灰度图像只有一个通道，而RGB或彩色图像有三个通道:红色、绿色和蓝色。在Conv图层中，仅考虑输入图像中的一小部分区域，计算权重的点积并生成单个值。这种称为卷积的操作通过在输入数据的那个小方块上学习图像特征来保持图像中像素之间的空间关系。</p><p id="d321" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">权重或CONV层的参数由一组可学习的过滤器或内核或<em class="lv">特征检测器</em>组成。<strong class="km hq">内核</strong>从左上角到右下角滑过整个图像，计算过滤器的权重和内核覆盖的图像像素之间的点积(称为<em class="lv">感受域</em>)。下一步是将滤波器向右移动1个单位，然后再向右移动1个单位，当stride等于1时，依此类推。对于一个32×32×3的图像和12个5x5x3的内核以及1的步距，输出将是一个28×28×12的向量，称为“特征图”或“激活图”。对于HxWxC图像，HkxWkxC和stride s的K个内核，输出将是H-Hk+s x W-Wk+s x K，我们正在考虑<em class="lv">零填充</em>。</p><figure class="lk ll lm ln fd hk er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ms"><img src="../Images/c130a576969b3027716c271e476171dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*rRT3tkMOKX-8c7nqW_iHLQ.gif"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">注意，3×3矩阵在每一步中只“看到”输入图像的一部分。</figcaption></figure><p id="82d8" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">一个过滤器(红色轮廓)滑过输入图像(卷积运算)产生一个特征图。另一个过滤器(带有绿色轮廓)在同一图像上的卷积给出了不同的特征图，如图所示。重要的是要注意，卷积运算捕获原始图像中的局部依赖性。还要注意这两个不同的过滤器是如何从相同的原始图像生成不同的特征图的。</p><p id="901e" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">但是，过滤器的值是什么呢？它们是参数或权重，在训练阶段自行学习。</p><ul class=""><li id="38bb" class="me mf hp km b kn le kq lf jx mg kb mh kf mi lc mt mk ml mm bi translated">激活层的RELU层:在每一个卷积操作之后，使用了一个叫做<em class="lv">激活</em>的附加操作。ReLU代表整流线性单元，是一种非线性操作。其输出由下式给出:</li></ul><figure class="lk ll lm ln fd hk er es paragraph-image"><div class="er es mu"><img src="../Images/61feb87faaae38518f47d08b8658e26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*njuH4XVXf-l9pR_RorUOrA.png"/></div></figure><p id="e3f7" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">它将应用逐元素激活函数，例如在零处的max(0，x)阈值，因此用零替换特征图中的所有负像素值。它为我们的模型提供了强制性的非线性，因为我们希望我们的ConvNet学习的真实世界数据通常是非线性的。此操作不会改变输出的大小。</p><figure class="lk ll lm ln fd hk er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mv"><img src="../Images/4fbdd2ebfebd8511eb81218bf644ee38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Szrb_n0wR6m8UnodQdoomA.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">应用RELU函数。来源[8]</figcaption></figure><blockquote class="ls lt lu"><p id="1254" class="kk kl lv km b kn le iq kp kq lf it ks lw lg ku kv lx lh kx ky ly li la lb lc hb bi translated"><strong class="km hq">池层</strong>:“该层周期性地包含在Conv层之间，以减少参数的数量、模型的空间大小和计算成本。这就是它被称为子采样或下采样的原因。最重要的是，它将有助于防止或控制过度拟合。池层对输入的每个深度切片进行独立操作，并在空间上调整其大小，通常使用最大值或AVG运算。它从定义的窗口内的矫正特征图中获取最大元素(MAX)或平均值(AVG)。最常见的窗口形式是跨度为2的2x2池层。在我们例子中，28×28×12的输入产生14×14×12的输出”。<em class="hp">关于这个环节的完整解释【2】用于视觉识别的卷积神经网络:</em><a class="ae ld" href="http://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank"><em class="hp">http://cs231n.github.io/convolutional-networks/</em></a></p></blockquote><p id="b842" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">在下一张图中，我们可以观察到2x2池图层在要素地图中的应用。</p><figure class="lk ll lm ln fd hk er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mw"><img src="../Images/d715facd34442b79e0a7c31657b17006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zf_t_OKZbPSb11quAP9sSw.png"/></div></div></figure><p id="cdee" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated"><em class="lv">“除了减少特征维度和参数数量之外，合并层使得网络对于输入图像中的小变换、旋转、缩放、扭曲和平移不变。这有助于检测图像中的物体，无论它们位于何处。来源【ujjwalkarn对卷积神经网络的直观解释。</em></p><p id="359d" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">每个过滤器将低级特征的局部补丁组成高级表示。这就是为什么CNN在计算机视觉方面如此强大。</p><blockquote class="ls lt lu"><p id="f87b" class="kk kl lv km b kn le iq kp kq lf it ks lw lg ku kv lx lh kx ky ly li la lb lc hb bi translated"><strong class="km hq">全连接层</strong>:“在分类图像问题中，这一层基本上接受一个输入，无论它之前的conv或ReLU或pool层的输出是什么，并输出一个N维向量，其中N是程序必须从中选择的类的数量。卷积层和池层的输出代表输入图像的高级特征。全连接图层的目的是使用这些特征根据训练数据集将输入图像分类到不同的类中。”。<em class="hp">参见[3]理解卷积神经网络的初学者指南</em> <a class="ae ld" href="https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/" rel="noopener ugc nofollow" target="_blank"> <em class="hp">的扩展解释https://adeshpande 3 . github . io/A-初学者% 27s-理解卷积神经网络指南/ </em> </a></p></blockquote><p id="bcff" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">完全连接的图层通过学习先前图层中特征地图的非线性组合来提高Conv图层的预测能力。</p><h2 id="5d9e" class="jm jn hp bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">CNN背后的直觉</h2><p id="5a25" class="pw-post-body-paragraph kk kl hp km b kn ko iq kp kq kr it ks jx kt ku kv kb kw kx ky kf kz la lb lc hb bi translated">一般来说，理解CNN如何工作的一种方式是，每个卷积层都比前一层检测到更复杂的特征。在第一层中，ConvNet可以学习从原始像素中检测不同位置的边缘，然后在第二层中使用这些边缘来检测简单的形状，如拐角，然后使用这些形状来确定更高级别的特征，如更高层中的面部形状。</p><figure class="lk ll lm ln fd hk er es paragraph-image"><div class="er es mx"><img src="../Images/e358f913fbb866d885ad995f90908c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*M8tQYbmcDg8a_xDp5aXZxA.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">从卷积深度信念网络中学习特征。来源[9]</figcaption></figure><p id="87c3" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">例如，我们可以通过在两个相邻像素上取值1和1来检测边缘，而在其他地方取值0。也就是说，我们减去两个相邻的像素。当并排的像素相似时，我们得到的值大约为零。然而，在边缘上，相邻像素在垂直于边缘的方向上非常不同。然后，每一层应用不同的过滤器，通常是成百上千个，就像上面显示的那样，并组合它们的结果</p><figure class="lk ll lm ln fd hk er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es my"><img src="../Images/9857394478a9a321a99028d856788a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MK-Oc57X2AtN5-3hxHGNFg.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated"><a class="ae ld" href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" rel="noopener ugc nofollow" target="_blank">http://colah . github . io/posts/2014-07-Understanding-Convolutions/</a></figcaption></figure><p id="d719" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">CNN的一个重要理由是他们速度快。非常快。卷积是计算机图形的核心部分，在GPU的硬件层面上实现。</p><h2 id="9652" class="jm jn hp bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">关于挑战</h2><p id="16ae" class="pw-post-body-paragraph kk kl hp km b kn ko iq kp kq kr it ks jx kt ku kv kb kw kx ky kf kz la lb lc hb bi translated">我们的目标是从语音样本的频谱图中预测说话者的口音。有三种口音的身份，来自加拿大，印度和英国的人。我们将使用CNN模型来预测给定光谱图的重音。</p><p id="c205" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated"><strong class="km hq"> <em class="lv">我们将在第二部分继续。描述和开发一个用Keras框架构建的CNN，然后它将在Azure ML服务中接受训练。</em> </strong></p><h2 id="634d" class="jm jn hp bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">参考资料:</h2><p id="54f6" class="pw-post-body-paragraph kk kl hp km b kn ko iq kp kq kr it ks jx kt ku kv kb kw kx ky kf kz la lb lc hb bi translated">[1]问题及比赛描述:<a class="ae ld" href="https://datasciencecapstone.org/competitions/16/identifying-accents-speech/page/49/" rel="noopener ugc nofollow" target="_blank">https://datasciencecapstone . org/competitions/16/identifying-accents-speech/page/49/</a></p><p id="d9fc" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">[2]用于视觉识别的卷积神经网络:<a class="ae ld" href="http://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank">http://cs231n.github.io/convolutional-networks/</a></p><p id="e991" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">[3]理解卷积神经网络的初学者指南<a class="ae ld" href="https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/" rel="noopener ugc nofollow" target="_blank">https://adeshpande 3 . github . io/A-初学者% 27s-理解卷积神经网络指南/ </a></p><p id="911d" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">[4]了解用于NLP的卷积神经网络<a class="ae ld" href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" rel="noopener ugc nofollow" target="_blank">http://www . wild ml . com/2015/11/Understanding-convolutionary-Neural-Networks-for-NLP/</a></p><p id="dace" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">[5]ujwalkarn对卷积神经网络的直观解释，<a class="ae ld" href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/" rel="noopener ugc nofollow" target="_blank">https://ujwalkarn . me/2016/08/11/Intuitive-explain-conv nets/</a></p><p id="ef63" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">[6]基于梯度的学习应用于文档识别。Yann Lecun、León Bottou、Yoshua Bengio和Patrick Haffner。【http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf T4】</p><p id="801d" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">[7]https://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12/视觉CVPR深度学习方法2012教程<a class="ae ld" href="https://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12/" rel="noopener ugc nofollow" target="_blank"/></p><p id="67a7" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">[8]<a class="ae ld" href="http://mlss.tuebingen.mpg.de/2015/slides/fergus/Fergus_1.pdf" rel="noopener ugc nofollow" target="_blank">http://mlss . tuebingen . mpg . de/2015/slides/Fergus/Fergus _ 1 . pdf</a></p><p id="ec7b" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">[9]用于分层表示的可扩展无监督学习的卷积深度信念网络<a class="ae ld" href="http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf" rel="noopener ugc nofollow" target="_blank">http://web . eecs . umich . edu/~ hong lak/icml 09-convolialdeepbeliefnetworks . pdf</a></p><p id="c615" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">[10]了解卷积<a class="ae ld" href="http://colah.github.io/posts/2014-07-Understanding-Convolutions/" rel="noopener ugc nofollow" target="_blank">http://colah . github . io/posts/2014-07-了解卷积/ </a></p><p id="12c2" class="pw-post-body-paragraph kk kl hp km b kn le iq kp kq lf it ks jx lg ku kv kb lh kx ky kf li la lb lc hb bi translated">[6]<a class="ae ld" href="https://www.pyimagesearch.com/2018/11/26/instance-segmentation-with-opencv/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2018/11/26/instance-segmentation-with-opencv/</a></p></div></div>    
</body>
</html>