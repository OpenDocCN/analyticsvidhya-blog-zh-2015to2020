<html>
<head>
<title>Understanding the GPT-2 Source Code Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解新GPT协议源代码第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-gpt-2-source-code-part-2-4a980c36c68b?source=collection_archive---------0-----------------------#2019-05-19">https://medium.com/analytics-vidhya/understanding-the-gpt-2-source-code-part-2-4a980c36c68b?source=collection_archive---------0-----------------------#2019-05-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e65a421434f0bd70afc8c5a9e1754d1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ceUS920Pza9WHJaWd-iebA.jpeg"/></div></div></figure><p id="afa3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嗨！这是试图理解GPT-2的源代码的系列文章的下一篇，希望能学到一些东西。第1部分可以在这里找到<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-1-4481328ee10b">。如果有任何问题，不清楚的地方或反馈，请不要犹豫，在评论中提出来！</a></p><p id="56d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这一部分，我将浏览encoder.py和encode.py。</p><h1 id="90d9" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">什么是编码？</h1><p id="539f" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">需要理解的最重要的事情之一是，当您向模型中输入文本时，它不能仅仅使用该文本。在训练之前，机器对什么是“苹果”或“梨”以及它们之间的关系毫无概念。事实上，对于机器来说，出现“苹果”或“梨”这两个词会让它彻底糊涂。它宁愿看到像1和2这样的数字来代表它们。这就是编码的作用！它能把单词转换成数字！</p><h1 id="aab9" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">OpenAI是怎么做到的？</h1><p id="163f" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">首先我们来看看encode.py，内容给出如下。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="a745" class="lb jq hi kx b fi lc ld l le lf">#!/usr/bin/env python3<br/># Usage:<br/>#  PYTHONPATH=src ./encode.py &lt;file|directory|glob&gt; /path/to/output.npz<br/>#  PYTHONPATH=src ./train --dataset /path/to/output.npz</span><span id="f99a" class="lb jq hi kx b fi lg ld l le lf">import argparse<br/>import numpy as np</span><span id="e20a" class="lb jq hi kx b fi lg ld l le lf">import encoder<br/>from load_dataset import load_dataset</span><span id="65ac" class="lb jq hi kx b fi lg ld l le lf">parser = argparse.ArgumentParser(<br/>    description='Pre-encode text files into tokenized training set.',<br/>    formatter_class=argparse.ArgumentDefaultsHelpFormatter)<br/>parser.add_argument('--model_name', metavar='MODEL', type=str, default='117M', help='Pretrained model name')<br/>parser.add_argument('--combine', metavar='CHARS', type=int, default=50000, help='Concatenate files with &lt;|endoftext|&gt; separator into chunks of this minimum size')<br/>parser.add_argument('in_text', metavar='PATH', type=str, help='Input file, directory, or glob pattern (utf-8 text).')<br/>parser.add_argument('out_npz', metavar='OUT.npz', type=str, help='Output file path')</span><span id="6b1b" class="lb jq hi kx b fi lg ld l le lf">def main():<br/>    args = parser.parse_args()<br/>    enc = encoder.get_encoder(args.model_name)<br/>    print('Reading files')<br/>    chunks = load_dataset(enc, args.in_text, args.combine)<br/>    print('Writing', args.out_npz)<br/>    np.savez_compressed(args.out_npz, *chunks)</span><span id="1aaa" class="lb jq hi kx b fi lg ld l le lf">if __name__ == '__main__':<br/>    main()</span></pre><p id="d78d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如解析器所示，encode.py接受4个参数。我不知道他们为什么不使用这里的消防图书馆，所以如果有人知道，请告诉我！</p><p id="c096" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这四个参数是</p><ul class=""><li id="3ba8" class="lh li hi is b it iu ix iy jb lj jf lk jj ll jn lm ln lo lp bi translated">model_name —据我所知，目前只有117M和345M。</li><li id="121f" class="lh li hi is b it lq ix lr jb ls jf lt jj lu jn lm ln lo lp bi translated">组合——写着“用分隔符将文件连接成这个最小大小的块”。我现在不明白。因此，我计划进一步深入源代码，以便找出这个参数具体做什么。</li><li id="2cce" class="lh li hi is b it lq ix lr jb ls jf lt jj lu jn lm ln lo lp bi translated">in _ text输入。txt文件</li><li id="5da3" class="lh li hi is b it lq ix lr jb ls jf lt jj lu jn lm ln lo lp bi translated">out _ npz-npz格式的输出文件。</li></ul><p id="675d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们接下来看到的第一条有趣的线是</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="55e8" class="lb jq hi kx b fi lc ld l le lf">enc = encoder.get_encoder(args.model_name)</span></pre><p id="b6c0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，让我们研究一下encoder.py，看看get_encoder函数做了什么。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="e7eb" class="lb jq hi kx b fi lc ld l le lf">def get_encoder(model_name):<br/>    with open(os.path.join('models', model_name, 'encoder.json'), 'r') as f:<br/>        encoder = json.load(f)<br/>    with open(os.path.join('models', model_name, 'vocab.bpe'), 'r', encoding="utf-8") as f:<br/>        bpe_data = f.read()<br/>    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\n')[1:-1]]<br/>    return Encoder(<br/>        encoder=encoder,<br/>        bpe_merges=bpe_merges,<br/>    )</span></pre><p id="f130" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该函数做3件事。</p><ol class=""><li id="bc95" class="lh li hi is b it iu ix iy jb lj jf lk jj ll jn lv ln lo lp bi translated">获取encoder.json</li><li id="02ce" class="lh li hi is b it lq ix lr jb ls jf lt jj lu jn lv ln lo lp bi translated">获取vocab.bpe并在新行处拆分，忽略第一个和最后一个字符</li><li id="d900" class="lh li hi is b it lq ix lr jb ls jf lt jj lu jn lv ln lo lp bi translated">Initialize返回用encoder.json和vocab.bpe初始化的编码器类</li></ol><p id="7dd0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">117M型号和345M型号的encoder.json和vocab.bpe相同，因此型号名称没有那么重要。当您打开encoder.json并查看其内容时，您会看到</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="cda5" class="lb jq hi kx b fi lc ld l le lf">{“!”: 0, “\””: 1, “#”: 2, “$”: 3, “%”: 4, “&amp;”: 5, “‘“: 6, “(“: 7, “)”: 8, “*”: 9, “+”: 10, “,”: 11, “-”: 12,</span></pre><p id="69d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">依此类推，直到</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="eae7" class="lb jq hi kx b fi lc ld l le lf">“\u0120Collider”: 50253, “\u0120informants”: 50254, “\u0120gazed”: 50255, “&lt;|endoftext|&gt;”: 50256}</span></pre><p id="0c3d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，很明显，这个encoder.json表示每个单词或符号映射到的数字。</p><p id="7cea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是，对于vocab.bpe，打开后看到的是</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="6e07" class="lb jq hi kx b fi lc ld l le lf">#version: 0.2<br/>Ġ t<br/>Ġ a<br/>h e<br/>i n<br/>r e<br/>o n<br/>Ġt he<br/></span></pre><p id="eb7c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我不知道这是关于什么的。</p><h1 id="3d4e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">vocab.bpe是做什么的？</h1><p id="690e" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">显然，这是一种叫做字节对编码的东西。根据维基百科的说法，这是一种压缩技术，用那里的例子来说，给定一个字符串</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="92e7" class="lb jq hi kx b fi lc ld l le lf">aaabdaaabac</span></pre><p id="52ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于aa重复不止一次，我们可以用一个未使用的字节来替换它，Z它可以压缩为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="68e4" class="lb jq hi kx b fi lc ld l le lf">ZabdZabac<br/>Z=aa</span></pre><p id="870d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于ab重复，所以可以用Y替换为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="c91c" class="lb jq hi kx b fi lc ld l le lf">ZYdZYac<br/>Y=ab<br/>Z=aa</span></pre><p id="bc8d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">依此类推，直到没有重复的字节对。然而，从文件来看，似乎至少有一个轻微的修改作为字符，例如“h”，我怀疑是未使用的，用于表示单个字母“e”，而不是算法似乎建议的一对字符。</p><p id="0a0d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于这方面的源代码不太可用，我决定在网上搜索！我首先发现的是这张<a class="ae jo" href="https://www.aclweb.org/anthology/P16-1162" rel="noopener ugc nofollow" target="_blank">纸</a>。TLDR，这基本上是关于如何字节对编码可以用来找到新词的意义。</p><p id="45e2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">举一个例子，在论文中给出了新的单词</p><p id="b95c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">德语中的“Abwasser | behandlungs | anlange ”,如果我们使用字节对编码，它可以被分割成3个子词“污水处理厂”,而如果我们只是从一开始就将其编码成一个向量，在遇到它时，没有办法说出它是关于什么的。</p><p id="14a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，我仍然对这样的序列感到困惑</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="e2e4" class="lb jq hi kx b fi lc ld l le lf">Ġ t<br/>Ġ a<br/>h e<br/>i n<br/>r e<br/>o n<br/>Ġt h</span></pre><p id="3ea6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">似是而非。因此，我决定在网上实施，这里可以找到<a class="ae jo" href="https://github.com/rsennrich/subword-nmt/blob/master/subword_nmt/learn_bpe.py" rel="noopener ugc nofollow" target="_blank"/>。谢谢，瑞科·森里奇！我查看了代码，我的理解是，并不是ġ对应于t，而是ġt是字节对！</p><p id="2bfa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">冒着有点无聊的风险，我想我会解释一下。如果不感兴趣，请到下一节！</p><p id="5921" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以，我基本上做的是搜索输出文件中提到的任何内容。我发现在learn_bpe函数中提到了它。有两个例子。第一个是</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="62a5" class="lb jq hi kx b fi lc ld l le lf">outfile.write(‘#version: 0.2\n’)</span></pre><p id="317b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第二个是</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="71b2" class="lb jq hi kx b fi lc ld l le lf">outfile.write(‘{0} {1}\n’.format(*most_frequent))</span></pre><p id="ab5c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个版本非常明显地表明OpenAI使用了这个python文件！最频繁基本上是轮流获取最频繁的字节对，并将它们追加写入outfile。</p><h1 id="1aa4" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">编码数据集</h1><p id="28bf" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">encode.py中的下一行(不是注释)是</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="afea" class="lb jq hi kx b fi lc ld l le lf">chunks = load_dataset(enc, args.in_text, args.combine)</span></pre><p id="8fa2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里的enc是先前返回的编码器实例。现在，让我们看看load_dataset函数。第一部分是</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="6566" class="lb jq hi kx b fi lc ld l le lf">def load_dataset(enc, path, combine):<br/>    paths = []<br/>    if os.path.isfile(path):<br/>        # Simple file<br/>        paths.append(path)<br/>    elif os.path.isdir(path):<br/>        # Directory<br/>        for (dirpath, _, fnames) in os.walk(path):<br/>            for fname in fnames:<br/>                paths.append(os.path.join(dirpath, fname))<br/>    else:<br/>        # Assume glob<br/>        paths = glob.glob(path)</span></pre><p id="debb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这基本上是将目录中一个或多个文本文件的路径附加到一个名为paths的列表中。os.walk是一个奇特的函数，可以遍历目录中的文件。Globs基本上是带有通配符的文件。比如，*。txt可以是任何形式的文本文件，如a.txt、adfdj.txt等，因为*是一个特殊的通配符。因此，它是一个球体。如果我错了，请在评论中告诉我！</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="5eda" class="lb jq hi kx b fi lc ld l le lf">token_chunks = []<br/>    raw_text = ''<br/>    for path in tqdm.tqdm(paths):<br/>        if path.endswith('.npz'):<br/>            # Pre-encoded<br/>            with np.load(path) as npz:<br/>                for item in npz.files:<br/>                    token_chunks.append(npz[item])<br/>        else:<br/>            # Plain text<br/>            with open(path, 'r') as fp:<br/>                raw_text += fp.read()<br/>            if len(raw_text) &gt;= combine:<br/>                tokens = np.stack(enc.encode(raw_text))<br/>                token_chunks.append(tokens)<br/>                raw_text = ''<br/>            else:<br/>                raw_text += '&lt;|endoftext|&gt;'<br/>    if raw_text:<br/>        tokens = np.stack(enc.encode(raw_text))<br/>        token_chunks.append(tokens)<br/>    return token_chunks</span></pre><p id="4ae7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一部分是。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="2fed" class="lb jq hi kx b fi lc ld l le lf">if path.endswith('.npz'):<br/>            # Pre-encoded<br/>            with np.load(path) as npz:<br/>                for item in npz.files:<br/>                    token_chunks.append(npz[item])</span></pre><p id="f881" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">用于已经编码的文件。基本上，它所做的就是用这个新编码的文件覆盖输出文件。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="b1f1" class="lb jq hi kx b fi lc ld l le lf">else:<br/>            # Plain text<br/>            with open(path, 'r') as fp:<br/>                raw_text += fp.read()<br/>            if len(raw_text) &gt;= combine:<br/>                tokens = np.stack(enc.encode(raw_text))<br/>                token_chunks.append(tokens)<br/>                raw_text = ''<br/>            else:<br/>                raw_text += '&lt;|endoftext|&gt;'</span></pre><p id="47c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，我终于明白了组合参数的含义。如果文本少于combine的字符数，文本文件将被忽略。现在，让我们通过查看编码器编码的方法来看看enc.encode(raw_text)是做什么的。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="a712" class="lb jq hi kx b fi lc ld l le lf">def encode(self, text):<br/>        bpe_tokens = []<br/>        for token in re.findall(self.pat, text):<br/>            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))<br/>            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))<br/>        return bpe_tokens</span></pre><p id="c15a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TLDR；</p><p id="3071" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里发生的最基本的事情是</p><ol class=""><li id="e7d4" class="lh li hi is b it iu ix iy jb lj jf lk jj ll jn lv ln lo lp bi translated">对于文本中的每个模式，将该模式作为令牌返回</li><li id="d33a" class="lh li hi is b it lq ix lr jb ls jf lt jj lu jn lv ln lo lp bi translated">将令牌编码成utf-8格式，并连接成一个名为token的字符串</li><li id="935b" class="lh li hi is b it lq ix lr jb ls jf lt jj lu jn lv ln lo lp bi translated">扩展bpe令牌数组，在令牌中包含字节对，即字符对。</li></ol><p id="3b7b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于那些好奇的人来说，</p><p id="a146" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了理解这一点，让我们从头来过函数。下面是第一行有趣的内容。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="cbbe" class="lb jq hi kx b fi lc ld l le lf">for token in re.findall(self.pat, text):</span></pre><p id="7050" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">self.pat在哪里，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="dcab" class="lb jq hi kx b fi lc ld l le lf">self.pat = re.compile(r”””’s|’t|’re|’ve|’m|’ll|’d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+”””)</span></pre><h1 id="4ff7" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">对此正则表达式的解释</h1><p id="d01b" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">re.compile只是用来预编译字符串的。现在让我们来看看这根弦。这个字符串叫做<a class="ae jo" href="https://docs.python.org/3/library/re.html" rel="noopener ugc nofollow" target="_blank">正则表达式</a>。基本上，它所做的是表示模式文本。例如，如果模式由单个单词“a”组成，并且如果我们试图在文本“我吃了很多馅饼”中找到所有的模式，那么该模式将只出现在两个实例中:在“had”和“a”中。</p><p id="af1b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">关于这个字符串，首先要注意的是其中有很多“|”。如果你有使用大多数编程语言的经验，除了python，我敢肯定你会知道它的意思是或者。</p><p id="9368" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">问号对应于前一个字符的0或1次重复。那么，对于“？”它基本上接受任意数量的空格。</p><p id="7cdf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">按<a class="ae jo" href="https://stackoverflow.com/questions/14891129/regular-expression-pl-and-pn" rel="noopener ugc nofollow" target="_blank">这里的</a>，</p><blockquote class="lw lx ly"><p id="4c12" class="iq ir lz is b it iu iv iw ix iy iz ja ma jc jd je mb jg jh ji mc jk jl jm jn hb bi translated"><code class="du md me mf kx b">\p{L}</code>匹配类别“字母”中的单个代码点。<br/> <code class="du md me mf kx b">\p{N}</code>匹配任何脚本中的任何种类的数字字符。</p></blockquote><p id="6d4c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是，这里需要注意的是，这不在python的re库中，只在regex库中可用。所以，OpenAI团队从写作开始</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="922c" class="lb jq hi kx b fi lc ld l le lf">import regex as re</span></pre><p id="feab" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为“+”表示一个或多个，\p{L}+可以匹配任何单词，\p{N}+可以匹配任何数字</p><p id="deac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">\s匹配任何Unicode空白字符，包括\n等。While \S匹配任何非空白字符。^是新的路线和？！意味着如果它前面的图案不出现，它将放大它前面的图案。</p><p id="496f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以，基本上，它所做的就是把像“他们”这样的词分成“他们”和“他们”等等。</p><h1 id="7c9d" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">字节编码器的解释</h1><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="4e7e" class="lb jq hi kx b fi lc ld l le lf">token = ‘’.join(self.byte_encoder[b] for b in token.encode(‘utf-8’))</span></pre><p id="ebb7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，令牌模式被编码成utf-8格式。然后在for b in循环中，改成函数ord给的数。</p><p id="1548" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，order(" a "。encode('utf-8 '))给出97而</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="a69d" class="lb jq hi kx b fi lc ld l le lf">[b for b in “a”.encode(‘utf-8’)] </span></pre><p id="163d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">也给出了97。据我所知，byte_encoder在某些情况下会返回稍加修改的编码unicode。通过稍微修改，我的意思是2⁸.的数字跳跃self.byte_encoder用bytes_to_unicode函数初始化。以下是哪一个</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="7e46" class="lb jq hi kx b fi lc ld l le lf"><a class="ae jo" href="http://twitter.com/lru_cache" rel="noopener ugc nofollow" target="_blank">@lru_cache</a>()<br/>def bytes_to_unicode():<br/>    """<br/>    Returns list of utf-8 byte and a corresponding list of unicode strings.<br/>    The reversible bpe codes work on unicode strings.<br/>    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.<br/>    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.<br/>    This is a signficant percentage of your normal, say, 32K bpe vocab.<br/>    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.<br/>    And avoids mapping to whitespace/control characters the bpe code barfs on.<br/>    """<br/>    bs = list(range(ord("!"), ord("~")+1))+list(range(ord("¡"), ord("¬")+1))+list(range(ord("®"), ord("ÿ")+1))<br/>    cs = bs[:]<br/>    n = 0<br/>    for b in range(2**8):<br/>        if b not in bs:<br/>            bs.append(b)<br/>            cs.append(2**8+n)<br/>            n += 1<br/>    cs = [chr(n) for n in cs]<br/>    return dict(zip(bs, cs))</span></pre><p id="689c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我理解正确的话，lru_cache是一个来自functools模块的装饰函数，它缓存函数的结果，这样即使你第二次调用一个函数，也不需要进行不必要的处理。但是因为这个函数只被调用一次，所以我认为没有理由使用这个装饰器。如果有人能解释请告诉我！</p><p id="fa26" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为我认为代码是不言自明的，我将跳过它，除了提到chr()与ord()相反，当你给它一个数字时，它返回一个字符。因此，该函数返回一个字典，其中包含从1到2⁸的键以及相应的字符。</p><p id="5990" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我无法理解这些评论，但我认为2⁸转变的部分原因如下。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="3de7" class="lb jq hi kx b fi lc ld l le lf">And avoids mapping to whitespace/control characters the bpe code barfs on.</span></pre><p id="05da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但坦率地说，我并不完全理解它。然而，总体效果是令牌被转换成合适的格式。</p><h1 id="e5ce" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">对字节对编码标记化的解释</h1><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="29c1" class="lb jq hi kx b fi lc ld l le lf">bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))</span></pre><p id="c25e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，让我们看看self.bpe函数。一开始是这样的</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="856f" class="lb jq hi kx b fi lc ld l le lf">if token in self.cache:<br/>            return self.cache[token]<br/>        word = tuple(token)<br/>        pairs = get_pairs(word)</span></pre><p id="aef4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">get_pairs基本上将每个字符对配对并返回。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="6ca6" class="lb jq hi kx b fi lc ld l le lf">while True:<br/>            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))<br/>            if bigram not in self.bpe_ranks:<br/>                break<br/>            first, second = bigram<br/>            new_word = []<br/>            i = 0<br/>            while i &lt; len(word):<br/>                try:<br/>                    j = word.index(first, i)<br/>                    new_word.extend(word[i:j])<br/>                    i = j<br/>                except:<br/>                    new_word.extend(word[i:])<br/>                    break</span><span id="2c09" class="lb jq hi kx b fi lg ld l le lf">                if word[i] == first and i &lt; len(word)-1 and word[i+1] == second:<br/>                    new_word.append(first+second)<br/>                    i += 2<br/>                else:<br/>                    new_word.append(word[i])<br/>                    i += 1</span></pre><p id="61f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，首先要注意的是self.bpe_ranks。这是由定义的</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="d228" class="lb jq hi kx b fi lc ld l le lf">self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))</span></pre><p id="c963" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在init函数中。其中bpe_merges是vocab.bpe给出的数组，因此，最频繁的字节对被赋予最小的数字，最不频繁的被赋予最大的数字。因此，当我们看下面这条线时，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1ba1" class="lb jq hi kx b fi lc ld l le lf">bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))</span></pre><p id="7fb1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">显而易见，二元模型表示数据集中整个词汇表中最常见的字符对。float('inf ')意味着如果在vocab.bpe的bpe_ranks中没有找到该对，则返回infinity。由于无穷大不可能是最小值，所以干脆丢弃。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ff04" class="lb jq hi kx b fi lc ld l le lf">try:<br/>                    j = word.index(first, i)<br/>                    new_word.extend(word[i:j])<br/>                    i = j<br/>                except:<br/>                    new_word.extend(word[i:])<br/>                    break</span></pre><p id="ac35" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了。index函数，第二个参数表示范围的起始索引，而第一个参数表示在元组字中搜索的值。如果我没有超过word中给定的所有值，那么首先在new_word中添加word[i:j]。</p><p id="b020" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我确实超出了限制，那么将引发一个错误，except将捕获它，从I到结尾的所有字符都将添加到new_word中。</p><p id="406a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，更重要的是，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f810" class="lb jq hi kx b fi lc ld l le lf">if word[i] == first and i &lt; len(word)-1 and word[i+1] == second:<br/>                    new_word.append(first+second)<br/>                    i += 2<br/>                else:<br/>                    new_word.append(word[i])<br/>                    i += 1</span></pre><p id="489a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，成对的单词也被添加到new_word中！最后，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="3d57" class="lb jq hi kx b fi lc ld l le lf">new_word = tuple(new_word)<br/>word = new_word<br/>if len(word) == 1:<br/>   break<br/>else:<br/>   pairs = get_pairs(word)</span></pre><p id="a095" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，我们看到循环将继续下去，直到单词的长度变为1，以及word被赋予new_word。现在，在下一个循环中，如果我们回想一下字节对编码是怎么回事，现在就有可能找到2个字母字符和另一个2个字母字符或1个字母字符之间的字节对或字符对。在下一个循环中，甚至可以产生更长的字节对，因为它总是保证选择最常见的字节对，因为下面的行！</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ff88" class="lb jq hi kx b fi lc ld l le lf">bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))</span></pre><p id="17a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，字符的长度没有变成一也是很有可能的。然而，在while true循环中还有一个break语句，那就是</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="eb09" class="lb jq hi kx b fi lc ld l le lf">if bigram not in self.bpe_ranks:<br/> break</span></pre><p id="3570" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，可以有把握地说，如果word中没有更多的有效字节对，而word又不能被简化为更小的标记，那么循环将会终止。</p><p id="c818" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="4770" class="lb jq hi kx b fi lc ld l le lf">word = ‘ ‘.join(word)<br/> self.cache[token] = word<br/> return word</span></pre><p id="3368" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">单词由一个空格连接，然后返回。</p><p id="df39" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">回到编码功能，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="6682" class="lb jq hi kx b fi lc ld l le lf">bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(‘ ‘))</span></pre><p id="90c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">bpe令牌由self.encoder转换成数字，self . encoder是一个以前加载的json文件。</p><p id="4bd2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">抱歉，如果以上解释变得有点复杂。我不确定我是否完全理解了这里的所有代码。特别是try/except块，所以如果有人可以指出我的错误或不清楚，请说出来。</p><h1 id="f0ae" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">保存到输出</h1><p id="56f7" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">回到load_dataset.py，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="a810" class="lb jq hi kx b fi lc ld l le lf">if raw_text:<br/>        tokens = np.stack(enc.encode(raw_text))<br/>        token_chunks.append(tokens)<br/>return token_chunks</span></pre><p id="26b2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出的标记被转换成numpy数组并附加到标记上，最后在encode.py中</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1920" class="lb jq hi kx b fi lc ld l le lf">np.savez_compressed(args.out_npz, *chunks)</span></pre><p id="596d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出按原样保存。</p><h1 id="ebbd" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">关于数据我们需要小心的事情</h1><p id="282b" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">正如我们从编码过程中看到的，end_tokens不会自动添加到训练数据中。因此，最好建议在使用您自己的定制数据集来微调数据时，在文本的末尾提供结束标记，尤其是在文本很短的情况下！</p><h1 id="1fae" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">然后</h1><p id="f6c7" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在下一个故事中，我将尝试进入sample.py和model.py！如果你有兴趣，请在这里阅读<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-3-9796a5a5cc7c">！</a></p></div></div>    
</body>
</html>