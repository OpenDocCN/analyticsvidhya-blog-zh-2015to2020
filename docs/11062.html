<html>
<head>
<title>DCGAN Tutorial — Generate Fake Celebrity image</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DCGAN 教程-生成假名人图像</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/dcgan-tutorial-by-aniket-maurya-f6c137ef0053?source=collection_archive---------8-----------------------#2020-11-16">https://medium.com/analytics-vidhya/dcgan-tutorial-by-aniket-maurya-f6c137ef0053?source=collection_archive---------8-----------------------#2020-11-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="c027" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这是一个初学者友好的教程，使用 PyTorch 在 CelebA 数据集上生成假名人图像。</h2></div><h1 id="b221" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">什么是 DCGAN？</h1><p id="e2f8" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">DCGAN(深度卷积生成对抗网络)由亚历克·拉德福德、卢克·梅茨和索史密斯·钦塔拉在 2016 年创建，用于训练深度生成对抗网络。在<a class="ae kl" href="https://arxiv.org/abs/1511.06434" rel="noopener ugc nofollow" target="_blank"> DCGAN 论文</a>中，作者训练了可以生成虚假名人图像和虚假卧室的网络。</p><p id="1d65" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated">该架构由两个网络组成——生成器和鉴别器。发电机是甘斯的心脏。它从随机噪声中产生看起来真实的假图像。</p><blockquote class="kr"><p id="e6a5" class="ks kt hi bd ku kv kw kx ky kz la kk dx translated"><a class="ae kl" href="https://colab.research.google.com/github/aniketmaurya/blog/blob/master/_notebooks/2020-11-16-DCGAN.ipynb#scrollTo=vNMrpS4lf4It" rel="noopener ugc nofollow" target="_blank">这篇文章可以作为 Jupyter Notebook 打开，在 CelebA 数据集上训练 DCGAN 生成虚假名人图像。</a></p></blockquote><p id="e3aa" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">鉴别者希望真假图像分布尽可能远，而生成者希望缩小真假图像分布之间的距离。简单地说，生成器试图通过产生看起来真实的图像来欺骗鉴别器，而鉴别器试图从真实图像中捕捉假图像。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lg"><img src="../Images/9e09ab11a6dc620ff5fc8672e2c8615e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NZ7j9xIEju8Qfb1d.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated"><em class="lw">视觉概念的矢量运算。来源:DCGAN 论文</em></figcaption></figure><h1 id="4a7d" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">报纸上的培训详情</h1><p id="d5e0" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">预处理:将图像缩放到 tanh 激活范围内，[-1，1]。使用 128 的小批量和学习率为 0.0002 的 Adam optimizer 进行训练。用正态分布μ(0，0.02)初始化的所有权重。</p><p id="9bc7" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated"><strong class="jr hj"> <em class="lx">作者指南</em> </strong>:</p><ul class=""><li id="055c" class="ly lz hi jr b js km jv kn jy ma kc mb kg mc kk md me mf mg bi translated">所有的池层在鉴别器中被替换为步长卷积，在鉴别器中被替换为分数步长卷积。</li><li id="7a3f" class="ly lz hi jr b js mh jv mi jy mj kc mk kg ml kk md me mf mg bi translated">没有使用完全连接的层或池层。</li><li id="0ed7" class="ly lz hi jr b js mh jv mi jy mj kc mk kg ml kk md me mf mg bi translated">发生器和鉴别器中使用的批处理</li><li id="dc6a" class="ly lz hi jr b js mh jv mi jy mj kc mk kg ml kk md me mf mg bi translated">ReLu 激活用于所有层的生成器，除了使用 tanh 的最后一层</li><li id="15f2" class="ly lz hi jr b js mh jv mi jy mj kc mk kg ml kk md me mf mg bi translated">鉴别器对所有层使用 LeakyReLu</li></ul><p id="5c3c" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated"><strong class="jr hj">在本帖中，我将训练一个 GAN 生成名人脸。</strong></p><h1 id="22e3" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">发电机</h1><p id="3e8a" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">生成器由转置卷积、批量归一化和激活函数层组成。</p><ul class=""><li id="c25e" class="ly lz hi jr b js km jv kn jy ma kc mb kg mc kk md me mf mg bi translated">首先，大小为 100 的随机噪声将被整形为 100 x1x 1(py torch 中的通道优先)。</li><li id="9913" class="ly lz hi jr b js mh jv mi jy mj kc mk kg ml kk md me mf mg bi translated">它通过转置的 CNN 层，该层对输入张量进行上采样。</li><li id="5a9b" class="ly lz hi jr b js mh jv mi jy mj kc mk kg ml kk md me mf mg bi translated">应用批量标准化。</li><li id="8869" class="ly lz hi jr b js mh jv mi jy mj kc mk kg ml kk md me mf mg bi translated">如果该层不是最后一层，则应用 ReLu 激活。</li></ul><p id="31e1" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated">第一个通道大小是 1024，然后对于 RGB 图像逐块减小到 3。最后，我们将得到一个 3x64x64 的张量，这将是我们的图像。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es mm"><img src="../Images/64e15b053f20507c1c36990e2614c816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XnrPCo1cwtiRNn-2.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated"><em class="lw">来自 DCGAN 论文的发电机架构</em></figcaption></figure><pre class="lh li lj lk fd mn mo mp mq aw mr bi"><span id="42e1" class="ms iy hi mo b fi mt mu l mv mw"><strong class="mo hj">class</strong> <strong class="mo hj">Generator(</strong>nn<strong class="mo hj">.</strong>Module<strong class="mo hj">):</strong><br/>    <br/>    <strong class="mo hj">def</strong> __init__<strong class="mo hj">(</strong><em class="lx">self</em><strong class="mo hj">,</strong> in_channels<strong class="mo hj">=</strong>3<strong class="mo hj">,</strong> z_dim<strong class="mo hj">=</strong>100<strong class="mo hj">):</strong><br/>        <em class="lx">super</em><strong class="mo hj">(</strong>Generator<strong class="mo hj">,</strong> <em class="lx">self</em><strong class="mo hj">).</strong>__init__<strong class="mo hj">()</strong><br/>        <br/>        <em class="lx">self</em><strong class="mo hj">.</strong>gen <strong class="mo hj">=</strong> nn<strong class="mo hj">.</strong>Sequential<strong class="mo hj">(</strong><br/>            <em class="lx">self</em><strong class="mo hj">.</strong>create_upblock<strong class="mo hj">(</strong>z_dim<strong class="mo hj">,</strong> 1024<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">=</strong>4<strong class="mo hj">,</strong> stride<strong class="mo hj">=</strong>1<strong class="mo hj">,</strong> padding<strong class="mo hj">=</strong>0<strong class="mo hj">),</strong><br/>            <em class="lx">self</em><strong class="mo hj">.</strong>create_upblock<strong class="mo hj">(</strong>1024<strong class="mo hj">,</strong> 512<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">=</strong>4<strong class="mo hj">,</strong> stride<strong class="mo hj">=</strong>2<strong class="mo hj">,</strong> padding<strong class="mo hj">=</strong>1<strong class="mo hj">),</strong><br/>            <em class="lx">self</em><strong class="mo hj">.</strong>create_upblock<strong class="mo hj">(</strong>512<strong class="mo hj">,</strong> 256<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">=</strong>4<strong class="mo hj">,</strong> stride<strong class="mo hj">=</strong>2<strong class="mo hj">,</strong> padding<strong class="mo hj">=</strong>1<strong class="mo hj">),</strong><br/>            <em class="lx">self</em><strong class="mo hj">.</strong>create_upblock<strong class="mo hj">(</strong>256<strong class="mo hj">,</strong> 128<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">=</strong>4<strong class="mo hj">,</strong> stride<strong class="mo hj">=</strong>2<strong class="mo hj">,</strong> padding<strong class="mo hj">=</strong>1<strong class="mo hj">),</strong><br/>            <em class="lx">self</em><strong class="mo hj">.</strong>create_upblock<strong class="mo hj">(</strong>128<strong class="mo hj">,</strong> 3<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">=</strong>4<strong class="mo hj">,</strong> stride<strong class="mo hj">=</strong>2<strong class="mo hj">,</strong> padding<strong class="mo hj">=</strong>1<strong class="mo hj">,</strong> final_layer<strong class="mo hj">=True),</strong><br/>        <strong class="mo hj">)</strong><br/>    <br/>    <strong class="mo hj">def</strong> <strong class="mo hj">create_upblock(</strong><em class="lx">self</em><strong class="mo hj">,</strong> in_channels<strong class="mo hj">,</strong> out_channels<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">=</strong>5<strong class="mo hj">,</strong> stride<strong class="mo hj">=</strong>2<strong class="mo hj">,</strong> padding<strong class="mo hj">=</strong>1<strong class="mo hj">,</strong> final_layer<strong class="mo hj">=False):</strong><br/>        <strong class="mo hj">if</strong> final_layer<strong class="mo hj">:</strong><br/>            <strong class="mo hj">return</strong> nn<strong class="mo hj">.</strong>Sequential<strong class="mo hj">(</strong><br/>                nn<strong class="mo hj">.</strong>ConvTranspose2d<strong class="mo hj">(</strong>in_channels<strong class="mo hj">,</strong> out_channels<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">,</strong> stride<strong class="mo hj">,</strong> padding<strong class="mo hj">,</strong> bias<strong class="mo hj">=False),</strong><br/>                nn<strong class="mo hj">.</strong>BatchNorm2d<strong class="mo hj">(</strong>out_channels<strong class="mo hj">),</strong><br/>                nn<strong class="mo hj">.</strong>Tanh<strong class="mo hj">()</strong><br/>            <strong class="mo hj">)</strong><br/>        <strong class="mo hj">return</strong> nn<strong class="mo hj">.</strong>Sequential<strong class="mo hj">(</strong><br/>            nn<strong class="mo hj">.</strong>ConvTranspose2d<strong class="mo hj">(</strong>in_channels<strong class="mo hj">,</strong> out_channels<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">,</strong> stride<strong class="mo hj">,</strong> padding<strong class="mo hj">,</strong> bias<strong class="mo hj">=False),</strong><br/>            nn<strong class="mo hj">.</strong>BatchNorm2d<strong class="mo hj">(</strong>out_channels<strong class="mo hj">),</strong><br/>            nn<strong class="mo hj">.</strong>ReLU<strong class="mo hj">(True)</strong><br/>        <strong class="mo hj">)</strong><br/>    <br/>    <strong class="mo hj">def</strong> <strong class="mo hj">forward(</strong><em class="lx">self</em><strong class="mo hj">,</strong> noise<strong class="mo hj">):</strong><br/>        """<br/>        noise: random vector of shape=(N, 100, 1, 1)<br/>        """<br/>        <strong class="mo hj">assert</strong> <em class="lx">len</em><strong class="mo hj">(</strong>noise<strong class="mo hj">.</strong>shape<strong class="mo hj">)==</strong>4<strong class="mo hj">,</strong> 'random vector of shape=(N, 100, 1, 1)'<br/>        <br/>        <strong class="mo hj">return</strong> <em class="lx">self</em><strong class="mo hj">.</strong>gen<strong class="mo hj">(</strong>noise<strong class="mo hj">)</strong></span></pre><h1 id="1000" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">鉴别器</h1><p id="79ef" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">鉴别器的架构与普通图像分类模型的架构相同。它包含卷积层、激活层和批处理标准化。在 DCGAN 的论文中，使用跨距而不是池来减小内核的大小。此外，网络中没有完全连接的层。使用泄漏斜率为 0.2 的泄漏 ReLU。</p><p id="653f" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated">鉴别器想要预测假图像是假的，而真实图像是真实的。另一方面，生成器想要欺骗鉴别器，使其将生成器生成的假图像预测为真实图像。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es mx"><img src="../Images/f6d4aaa8a6e390f3247ee9a776951deb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EE2Z55oTyvPUdlBA.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated"><em class="lw">来源:deep learning . ai GANs specialization</em></figcaption></figure><p id="7956" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated"><em class="lx">来源:deep learning . ai GANs specialization</em></p><pre class="lh li lj lk fd mn mo mp mq aw mr bi"><span id="6e61" class="ms iy hi mo b fi mt mu l mv mw"><strong class="mo hj">class</strong> <strong class="mo hj">Discriminator(</strong>nn<strong class="mo hj">.</strong>Module<strong class="mo hj">):</strong></span><span id="77b1" class="ms iy hi mo b fi my mu l mv mw">    <strong class="mo hj">def</strong> __init__<strong class="mo hj">(</strong><em class="lx">self</em><strong class="mo hj">,</strong> im_chan<strong class="mo hj">=</strong>3<strong class="mo hj">,</strong> hidden_dim<strong class="mo hj">=</strong>32<strong class="mo hj">):</strong><br/>        <em class="lx">super</em><strong class="mo hj">(</strong>Discriminator<strong class="mo hj">,</strong> <em class="lx">self</em><strong class="mo hj">).</strong>__init__<strong class="mo hj">()</strong><br/>        <em class="lx">self</em><strong class="mo hj">.</strong>disc <strong class="mo hj">=</strong> nn<strong class="mo hj">.</strong>Sequential<strong class="mo hj">(</strong><br/>            <em class="lx">self</em><strong class="mo hj">.</strong>make_disc_block<strong class="mo hj">(</strong>im_chan<strong class="mo hj">,</strong> hidden_dim<strong class="mo hj">),</strong><br/>            <em class="lx">self</em><strong class="mo hj">.</strong>make_disc_block<strong class="mo hj">(</strong>hidden_dim<strong class="mo hj">,</strong> hidden_dim <strong class="mo hj">*</strong> 2<strong class="mo hj">),</strong><br/>            <em class="lx">self</em><strong class="mo hj">.</strong>make_disc_block<strong class="mo hj">(</strong>hidden_dim<strong class="mo hj">*</strong>2<strong class="mo hj">,</strong> hidden_dim <strong class="mo hj">*</strong> 4<strong class="mo hj">,</strong> stride<strong class="mo hj">=</strong>1<strong class="mo hj">),</strong><br/>            <em class="lx">self</em><strong class="mo hj">.</strong>make_disc_block<strong class="mo hj">(</strong>hidden_dim<strong class="mo hj">*</strong>4<strong class="mo hj">,</strong> hidden_dim <strong class="mo hj">*</strong> 4<strong class="mo hj">,</strong> stride<strong class="mo hj">=</strong>2<strong class="mo hj">),</strong><br/>            <em class="lx">self</em><strong class="mo hj">.</strong>make_disc_block<strong class="mo hj">(</strong>hidden_dim <strong class="mo hj">*</strong> 4<strong class="mo hj">,</strong> 1<strong class="mo hj">,</strong> final_layer<strong class="mo hj">=True),</strong><br/>        <strong class="mo hj">)</strong></span><span id="c922" class="ms iy hi mo b fi my mu l mv mw">    <strong class="mo hj">def</strong> <strong class="mo hj">make_disc_block(</strong><em class="lx">self</em><strong class="mo hj">,</strong> input_channels<strong class="mo hj">,</strong> output_channels<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">=</strong>4<strong class="mo hj">,</strong> stride<strong class="mo hj">=</strong>2<strong class="mo hj">,</strong> final_layer<strong class="mo hj">=False):</strong><br/>        <strong class="mo hj">if</strong> <strong class="mo hj">not</strong> final_layer<strong class="mo hj">:</strong><br/>            <strong class="mo hj">return</strong> nn<strong class="mo hj">.</strong>Sequential<strong class="mo hj">(</strong><br/>                nn<strong class="mo hj">.</strong>Conv2d<strong class="mo hj">(</strong>input_channels<strong class="mo hj">,</strong> output_channels<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">,</strong> stride<strong class="mo hj">),</strong><br/>                nn<strong class="mo hj">.</strong>BatchNorm2d<strong class="mo hj">(</strong>output_channels<strong class="mo hj">),</strong><br/>                nn<strong class="mo hj">.</strong>LeakyReLU<strong class="mo hj">(</strong>0.2<strong class="mo hj">)</strong><br/>            <strong class="mo hj">)</strong><br/>        <strong class="mo hj">else:</strong><br/>            <strong class="mo hj">return</strong> nn<strong class="mo hj">.</strong>Sequential<strong class="mo hj">(</strong><br/>                nn<strong class="mo hj">.</strong>Conv2d<strong class="mo hj">(</strong>input_channels<strong class="mo hj">,</strong> output_channels<strong class="mo hj">,</strong> kernel_size<strong class="mo hj">,</strong> stride<strong class="mo hj">)</strong><br/>            <strong class="mo hj">)</strong></span><span id="97d6" class="ms iy hi mo b fi my mu l mv mw">    <strong class="mo hj">def</strong> <strong class="mo hj">forward(</strong><em class="lx">self</em><strong class="mo hj">,</strong> image<strong class="mo hj">):</strong><br/>        disc_pred <strong class="mo hj">=</strong> <em class="lx">self</em><strong class="mo hj">.</strong>disc<strong class="mo hj">(</strong>image<strong class="mo hj">)</strong><br/>        <strong class="mo hj">return</strong> disc_pred<strong class="mo hj">.</strong>view<strong class="mo hj">(</strong><em class="lx">len</em><strong class="mo hj">(</strong>disc_pred<strong class="mo hj">),</strong> <strong class="mo hj">-</strong>1<strong class="mo hj">)</strong></span></pre><p id="fb45" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated">根据论文定义学习率、z_dim(噪声维数)、批量和其他配置。</p><h1 id="57e1" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">鉴频器损耗</h1><p id="a6e0" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">由于鉴别器想要增加生成的和真实的分布之间的距离，我们将训练它在生成的图像被分类为真实的或者真实的图像被分类为假的时候给出高损失。</p><h1 id="45a0" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">发电机损耗</h1><p id="ab9d" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">当发生器不能欺骗鉴别器时，BCE 损失将会很高。当生成的图像被鉴别器鉴别为伪图像时，会造成很大的损失。<em class="lx">注意，生成器永远不知道真实图像。</em></p><pre class="lh li lj lk fd mn mo mp mq aw mr bi"><span id="b764" class="ms iy hi mo b fi mt mu l mv mw">display_step <strong class="mo hj">=</strong> 500</span><span id="cf4e" class="ms iy hi mo b fi my mu l mv mw">n_epochs <strong class="mo hj">=</strong> 50<br/>cur_step <strong class="mo hj">=</strong> 0<br/>mean_generator_loss <strong class="mo hj">=</strong> 0<br/>mean_discriminator_loss <strong class="mo hj">=</strong> 0<br/><strong class="mo hj">for</strong> epoch <strong class="mo hj">in</strong> <em class="lx">range</em><strong class="mo hj">(</strong>n_epoch<strong class="mo hj">):</strong><br/>    <strong class="mo hj">for</strong> real<strong class="mo hj">,</strong> _ <strong class="mo hj">in</strong> tqdm<strong class="mo hj">(</strong>dataloader<strong class="mo hj">):</strong><br/>        real <strong class="mo hj">=</strong> real<strong class="mo hj">.</strong>to<strong class="mo hj">(</strong>device<strong class="mo hj">)</strong><br/><br/><br/>        <em class="lx"># update the discriminator</em><br/>        <em class="lx"># create fake images from random noise</em><br/>        disc_optimizer<strong class="mo hj">.</strong>zero_grad<strong class="mo hj">()</strong><br/>        noise <strong class="mo hj">=</strong> torch<strong class="mo hj">.</strong>randn<strong class="mo hj">(</strong>cur_batch_size<strong class="mo hj">,</strong> z_dim<strong class="mo hj">,</strong> 1<strong class="mo hj">,</strong> 1<strong class="mo hj">,</strong> device<strong class="mo hj">=</strong>device<strong class="mo hj">)</strong><br/>        fake_images <strong class="mo hj">=</strong> gen<strong class="mo hj">(</strong>noise<strong class="mo hj">)</strong><br/>        logits_fake <strong class="mo hj">=</strong> disc<strong class="mo hj">(</strong>fake_images<strong class="mo hj">.</strong>detach<strong class="mo hj">())</strong><br/>        logits_real <strong class="mo hj">=</strong> disc<strong class="mo hj">(</strong>real<strong class="mo hj">)</strong><br/><br/>        disc_loss_fake <strong class="mo hj">=</strong> criterion<strong class="mo hj">(</strong>fake_logits<strong class="mo hj">,</strong> torch<strong class="mo hj">.</strong>zeros_like<strong class="mo hj">(</strong>loss_fake<strong class="mo hj">))</strong><br/>        disc_loss_real <strong class="mo hj">=</strong> criterion<strong class="mo hj">(</strong>real_logits<strong class="mo hj">,</strong> torch<strong class="mo hj">.</strong>ones_like<strong class="mo hj">(</strong>logits_real<strong class="mo hj">))</strong><br/><br/>        disc_loss <strong class="mo hj">=</strong> <strong class="mo hj">(</strong>disc_loss_fake <strong class="mo hj">+</strong> disc_loss_real<strong class="mo hj">)</strong> <strong class="mo hj">/</strong> 2<br/>        <em class="lx"># Keep track of the average discriminator loss</em><br/>        mean_discriminator_loss <strong class="mo hj">+=</strong> disc_avg_loss<strong class="mo hj">.</strong>item<strong class="mo hj">()</strong> <strong class="mo hj">/</strong> display_step<br/><br/>        disc_loss<strong class="mo hj">.</strong>backward<strong class="mo hj">(</strong>retain_graph<strong class="mo hj">=True)</strong><br/>        disc_optimizer<strong class="mo hj">.</strong>step<strong class="mo hj">()</strong><br/><br/><br/>        <em class="lx"># Update the generator</em><br/>        gen_optimizer<strong class="mo hj">.</strong>zero_grad<strong class="mo hj">()</strong><br/>        noise <strong class="mo hj">=</strong> torch<strong class="mo hj">.</strong>randn<strong class="mo hj">(</strong>cur_batch_size<strong class="mo hj">,</strong> z_dim<strong class="mo hj">,</strong> 1<strong class="mo hj">,</strong> 1<strong class="mo hj">,</strong> device<strong class="mo hj">=</strong>device<strong class="mo hj">)</strong><br/>        fake_images <strong class="mo hj">=</strong> gen<strong class="mo hj">(</strong>noise<strong class="mo hj">)</strong><br/>        logits_fake <strong class="mo hj">=</strong> disc<strong class="mo hj">(</strong>fake_images<strong class="mo hj">)</strong><br/><br/>        gen_loss <strong class="mo hj">=</strong> criterion<strong class="mo hj">(</strong>logits_fake<strong class="mo hj">,</strong> torch<strong class="mo hj">.</strong>ones_like<strong class="mo hj">(</strong>logits_fake<strong class="mo hj">))</strong><br/>        gen_loss<strong class="mo hj">.</strong>backward<strong class="mo hj">()</strong><br/>        gen_optimizer<strong class="mo hj">.</strong>step<strong class="mo hj">()</strong><br/><br/>        <em class="lx"># Keep track of the average generator loss</em><br/>        mean_generator_loss <strong class="mo hj">+=</strong> gen_loss<strong class="mo hj">.</strong>item<strong class="mo hj">()</strong> <strong class="mo hj">/</strong> display_step<br/><br/>        <em class="lx">## Visualization code ##</em><br/>        <strong class="mo hj">if</strong> cur_step <strong class="mo hj">%</strong> display_step <strong class="mo hj">==</strong> 0 <strong class="mo hj">and</strong> cur_step <strong class="mo hj">&gt;</strong> 0<strong class="mo hj">:</strong><br/>            <em class="lx">print</em><strong class="mo hj">(</strong>f"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}"<strong class="mo hj">)</strong><br/>            show_tensor_images<strong class="mo hj">(</strong>fake_images<strong class="mo hj">)</strong><br/>            show_tensor_images<strong class="mo hj">(</strong>real<strong class="mo hj">)</strong><br/>            mean_generator_loss <strong class="mo hj">=</strong> 0<br/>            mean_discriminator_loss <strong class="mo hj">=</strong> 0<br/>        cur_step <strong class="mo hj">+=</strong> 1</span></pre><h1 id="9e4e" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">参考</h1><p id="951b" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">[1.】<a class="ae kl" href="https://arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank">深度卷积生成对抗网络的无监督表示学习</a></p><p id="2722" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated">[2.】<a class="ae kl" href="https://www.coursera.org/specializations/generative-adversarial-networks-gans" rel="noopener ugc nofollow" target="_blank">生成性对抗网络(GANs)专门化</a></p><p id="7533" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated">[3.】<a class="ae kl" href="https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html" rel="noopener ugc nofollow" target="_blank"> DCGAN 教程— PyTorch 官方</a></p><p id="7490" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated">如果你想深入学习 GANs，我强烈推荐 Coursera 上的<a class="ae kl" href="https://www.coursera.org/specializations/generative-adversarial-networks-gans" rel="noopener ugc nofollow" target="_blank"> GANs 专精</a>。</p></div><div class="ab cl mz na gp nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="hb hc hd he hf"><p id="f836" class="pw-post-body-paragraph jp jq hi jr b js km ij ju jv kn im jx jy ko ka kb kc kp ke kf kg kq ki kj kk hb bi translated">如果你得到了一些反馈，请通过<a class="ae kl" href="https://twitter.com/aniketmaurya" rel="noopener ugc nofollow" target="_blank">https://twitter.com/aniketmaurya</a>联系我</p></div></div>    
</body>
</html>