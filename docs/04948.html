<html>
<head>
<title>Breast Cancer Detection — Benchmarking 3 SOTA networks.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">乳腺癌检测——3个SOTA网络的基准。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/breast-cancer-detection-benchmarking-3-sota-networks-767991a77521?source=collection_archive---------16-----------------------#2020-04-06">https://medium.com/analytics-vidhya/breast-cancer-detection-benchmarking-3-sota-networks-767991a77521?source=collection_archive---------16-----------------------#2020-04-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1a4e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">作者:David El Malih | Randa Elmrabet tar mach | Fatma Moalla | Hamza Rami</h2></div><h1 id="8268" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">概观</h1><p id="bf51" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">深度神经网络已经成为医学成像领域的游戏规则改变者。特别是，自动乳腺癌检测对于辅助放射科医生的日常工作非常重要。在这篇文章中，我将介绍3个目标检测算法的应用。目的是<strong class="jr hj">基准测试</strong>这3个算法的结果:<strong class="jr hj">fast-RCNN</strong>、<strong class="jr hj"> RetinaNet </strong>和<strong class="jr hj"> FCOS物体探测器。</strong></p><h1 id="0528" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">MIAS数据库</h1><p id="0c02" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">为了训练每个网络，我们使用了mini-MIAS数据库:MIAS数据库包含322个1024 x 1024的乳房x光照片以及一些细节:</p><ul class=""><li id="fb32" class="kl km hi jr b js kn jv ko jy kp kc kq kg kr kk ks kt ku kv bi translated"><strong class="jr hj">背景组织特征:<br/> </strong> - F:脂肪性<br/> - G:脂肪-腺性<br/> - D:致密-腺性</li><li id="8c35" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated"><strong class="jr hj">存在异常的级别:</strong> <br/> - CALC:钙化<br/> -环:边界清晰/边界清楚的肿块<br/> -毛刺:毛刺状肿块<br/> -杂项:其他，边界不清的肿块<br/> -弓:结构扭曲<br/> -不对称:不对称<br/> -正常</li><li id="c9df" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated"><strong class="jr hj">异常严重程度:<br/> </strong> - B:良性<br/> - M:恶性</li><li id="3f45" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated"><strong class="jr hj"> <em class="lb"> x，y </em>异常中心的图像坐标。</strong></li><li id="a74f" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated"><strong class="jr hj">包围异常的圆的近似半径(以像素为单位)。</strong></li></ul><p id="3e2f" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">该项目的目的是预测异常的<em class="lb"> x，y </em>位置(如果乳房x光片中有异常)、包围异常的圆的半径(以像素为单位)以及异常的类别。这是一个<strong class="jr hj">物体探测</strong>任务。</p><p id="3bdc" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">图像与一起给出。pgm扩展名和注释位于CSV格式的. txt文件中。该项目的第一部分是将数据库转换为更常见的格式(COCO或Pascal VOC)，使用一些已经实现了我们将要使用的算法的存储库(例如facebook的<a class="ae lf" href="https://github.com/facebookresearch/maskrcnn-benchmark" rel="noopener ugc nofollow" target="_blank"> maskrcnn-benchmark </a>)。同时，我们也在进行数据扩充和train / val分割。(查看<a class="ae lf" href="https://github.com/delmalih/MIAS-mammography-obj-detection/blob/master/generate_COCO_annotations.py" rel="noopener ugc nofollow" target="_blank">脚本</a>了解更多详情)。</p><p id="1089" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">迷你MIAS数据库的下载链接:【http://peipa.essex.ac.uk/info/mias.html T42】</p><h1 id="9950" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">算法</h1><ul class=""><li id="4c52" class="kl km hi jr b js jt jv jw jy lg kc lh kg li kk ks kt ku kv bi translated"><strong class="jr hj">更快-RCNN: </strong></li></ul><p id="c85c" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">更快的RCNN是一个<strong class="jr hj">两阶段对象检测器</strong>，使用<strong class="jr hj">锚框</strong>作为<strong class="jr hj">边界框</strong>预测的先验。这是最广泛使用的R-CNN系列的最新版本。在R-CNN系列论文中，版本之间的演变通常是在计算效率(整合不同的训练阶段)、减少测试时间和提高性能(mAP)方面。</p><p id="c9a6" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">该网络包括:</p><ul class=""><li id="b979" class="kl km hi jr b js kn jv ko jy kp kc kq kg kr kk ks kt ku kv bi translated">一个特征提取器(通常是CNN)来获得整个图像的特征图。我们使用在ImageNet上预训练的Resnet101作为主干。</li><li id="89dd" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated">区域提议网络(RPN)生成图像中可能对象的“边界框”或位置。它始于输入图像被送入骨干卷积神经网络。首先调整输入图像的大小，使其最短边为600像素，最长边不超过1000像素。</li><li id="ef46" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated">预测该对象属于哪个类的分类器和获得对象边界框坐标的更精确版本的回归层。</li></ul><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lj"><img src="../Images/6569d56247003eb743bce401ced0c5cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ICs04uIDAgu7dLQXx0Fo1g.png"/></div></div></figure><p id="fdee" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">查看这篇文章，这篇文章对网络进行了更深入的描述。<br/> <a class="ae lf" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">【论文】</a> | <a class="ae lf" href="https://github.com/facebookresearch/detectron2" rel="noopener ugc nofollow" target="_blank">【实现。(pytorch)] </a> | <a class="ae lf" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank">【实现。(tensorflow)] </a> | <a class="ae lf" href="https://towardsdatascience.com/faster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46" rel="noopener" target="_blank">【中篇】</a></p><ul class=""><li id="9662" class="kl km hi jr b js kn jv ko jy kp kc kq kg kr kk ks kt ku kv bi translated"><strong class="jr hj">视网膜网:</strong></li></ul><p id="2ff1" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">RetinaNet由脸书人工智能研究中心(FAIR)开发，是一个一级目标探测器，这意味着定位和分类任务可以同时完成。FAIR给出的主要贡献是网络的损失函数:焦点损失。较低的损失是由“容易的”负样本(没有对象的边界框)造成的，因此损失集中在“硬的”正样本(实际包含框的边界框)。焦点损耗解决了一级检测器的类别不平衡问题:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lv"><img src="../Images/11d37f52e3377ff7a0cc371bfa846481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*TAtN0YPKDndzATPgM1dDDA.png"/></div></figure><p id="b4ad" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">该网络包括:</p><ul class=""><li id="f456" class="kl km hi jr b js kn jv ko jy kp kc kq kg kr kk ks kt ku kv bi translated">一个特征提取器(通常是CNN)来获得整个图像的特征图。我们使用在ImageNet上预训练的Resnet101作为主干。</li><li id="49c6" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated">一种特征金字塔网络(FPN ),用于在每个尺度上获取高级特征，以获取不同大小和形状的对象。它使用跳跃连接来建立与主干层的链接。</li><li id="c05f" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated">两个子网络:一个对每一层的边界框进行分类，另一个得到更精确的坐标。</li></ul><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lj"><img src="../Images/97f5e5f4f4933e9153b027454758d14c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qA1yyFK33YiUGvzKuO2HtQ.png"/></div></div></figure><p id="d025" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">查看这篇对网络有更深入描述的文章。<a class="ae lf" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank"> <br/>【论文】</a>|<a class="ae lf" href="https://github.com/fizyr/keras-retinanet" rel="noopener ugc nofollow" target="_blank">【Implem。(keras)] </a> | <a class="ae lf" href="https://github.com/facebookresearch/detectron2" rel="noopener ugc nofollow" target="_blank">【实现。(pytorch)] </a> | <a class="ae lf" href="https://towardsdatascience.com/review-retinanet-focal-loss-object-detection-38fba6afabe4" rel="noopener" target="_blank">【中条】</a></p><ul class=""><li id="4f35" class="kl km hi jr b js kn jv ko jy kp kc kq kg kr kk ks kt ku kv bi translated"><strong class="jr hj"> FCOS物体探测器:</strong></li></ul><p id="6cb5" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">FCOS物体检测器是最新的算法(2019年8月)，其特殊性在于它是一个<strong class="jr hj">无锚物体检测器。</strong></p><p id="c8f9" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">基于锚的物体检测有一些未解决的问题:<br/> -需要调整许多超参数(锚的数量，每个锚的长宽比等。)<br/> -样本之间的类别不平衡:基于锚的模型通过计算锚盒和地面真相盒之间的IOU来设置正盒(带有对象的盒)。然而，他们一次生产数千个盒子，并且仅将其中的一些盒子标记为阳性盒子，而将大多数盒子标记为阴性(背景)。</p><p id="22d4" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">FCOS结构与RetinaNet相同:使用<a class="ae lf" href="https://arxiv.org/abs/1612.03144" rel="noopener ugc nofollow" target="_blank">特征金字塔网络(FPN) </a>创建特征图，并在每个特征图后添加头部来训练<strong class="jr hj">分类</strong>、<strong class="jr hj">包围盒回归</strong>和一个名为<strong class="jr hj">中心度</strong>的高贵指标。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lw"><img src="../Images/a0fc9ed682f87ba0e31e2a2c11e9ce7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KrlIMcJ_Ftnb5ugv7s8KlA.png"/></div></div></figure><p id="300e" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">FCOS把地面真值箱中的每一点都当作正样本。这导致远离对象中心的位置产生大量低质量的预测边界框。为了防止这种情况，他们添加了有效的索引来抑制这种被称为<strong class="jr hj">中心度</strong>的预测包围盒。它是一个描述点到地面真值盒中心距离的指标，作为一个分支添加在特征图之后。定义如下所示。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lj"><img src="../Images/e012df7f1c798c9677de2b064f10d156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xAR3ctVnZDufrD_7bwWnSA.png"/></div></div></figure><p id="831b" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">查看这篇对网络有更深入描述的文章。<a class="ae lf" href="https://arxiv.org/pdf/1904.01355.pdf" rel="noopener ugc nofollow" target="_blank"> <br/>【论文】</a>|<a class="ae lf" href="https://github.com/DetectionTeamUCAS/FCOS_Tensorflow" rel="noopener ugc nofollow" target="_blank">【Implem。(tensorflow)] </a> | <a class="ae lf" href="https://github.com/tianzhi0549/FCOS" rel="noopener ugc nofollow" target="_blank">【实现。(pytorch)] </a> | <a class="ae lf" rel="noopener" href="/lsc-psd/fcos-one-shot-anchor-free-object-detection-a3102da819bc">【中条】</a></p><h1 id="8edf" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">输出过滤</h1><p id="95c6" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">一旦训练完成，每种算法都会输出数千个包围盒以及它们的预测类别和置信度得分。其中只有少数应该符合地面真相。因此，我们需要过滤原始输出，否则，我们将无法可视化相关的预测框。</p><p id="6e86" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">有两种类型的过滤技术:</p><p id="75cf" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated"><strong class="jr hj"> 1 —对置信分值应用阈值<br/> </strong>由于只有1或2个基础事实框，如果网络训练有素，大多数预测框应该具有非常低的置信分值。因此，我们应该只保留具有高分数的那些(例如，移除那些具有小于0.75的置信分值的那些)。<br/>这将删除所有不相关的预测框。</p><p id="898f" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated"><strong class="jr hj"> 2 — NMS(非最大抑制)<br/> </strong>我们期待模型在地面真值包围盒上学习得很好。然后，我们应该期望许多预测的盒子与一个真实的盒子相匹配。在这种情况下，只有一个盒子被认为是真阳性，其他的被认为是假阴性。我们应该预料到这些盒子彼此非常接近，大小也差不多。(因此，我们可以认为匹配相同基础真值框的每对预测框的IoU(交集/并集)大于0.5)。对于这组预测的盒子，我们保留具有最高置信度得分的那个。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lx"><img src="../Images/ac422e106ef0eeb4c9592d6f406ee148.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*9vgYblWTjP0Np7q_PUmlrw.png"/></div></figure><p id="2f18" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">查看<a class="ae lf" href="https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c" rel="noopener" target="_blank">这篇文章</a>了解更多信息。<br/>下面是一个带有原始预测框并经过过滤的图像示例:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lj"><img src="../Images/faba4d97bad86b4205808a9efe4c82de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Tfx2J2J9Qr5XZovsuRHNg.png"/></div></div></figure><h1 id="4d81" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">结果</h1><p id="d4f8" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">在展示我们的结果之前，我们需要介绍一下我们用来比较这些算法的指标。</p><p id="3009" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">对于每个基本事实框，如果模型预测了一个框，则其位置或类别(或两者)是错误的，或者两者都是正确的。否则，没有预测框链接到这个基础事实框。因此，有3种不同的情况:</p><ul class=""><li id="4226" class="kl km hi jr b js kn jv ko jy kp kc kq kg kr kk ks kt ku kv bi translated"><strong class="jr hj"> FN(假阴性)</strong>:有一个地面真值框，但没有与之链接的预测框。</li><li id="6338" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated"><strong class="jr hj"> FP(误报)</strong>:预测的箱子位置和/或类别错误</li><li id="0ea5" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated"><strong class="jr hj"> TP(真正)</strong>:有一个地面真值，链接到它的预测框(IoU &gt; 0.5)有正确的类。</li></ul><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lw"><img src="../Images/eec6630459de310c50785a2ecca08444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YO4KrLa8S0hnQ5YmlAW7HQ.png"/></div></div></figure><p id="b3e9" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">有了这些数字，我们可以创建两个新的指标:<strong class="jr hj">精度</strong>和<strong class="jr hj">召回</strong>。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lj"><img src="../Images/a091f1dae50afb2bf8d5fb2b8da93336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5jtP62wKUcU1pTityVzwEg.png"/></div></div></figure><p id="58fc" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">以下是每种算法每类的精度和召回率结果:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lw"><img src="../Images/03052c0a89ce87d2aeaaf2140b791481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R5F85_V3dW2Ng1-A8THCmA.png"/></div></div></figure><p id="e994" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">我们开始通过绘制每类/肿瘤类型的精确度和召回率来评估3种SOTA算法的性能。让我们记住，获得高召回意味着低假阳性率，这意味着我们在错误的位置和/或错误的类别错误中有所下降。而获得高精度意味着低的假阴性率，意味着没有预测。</p><ul class=""><li id="66f7" class="kl km hi jr b js kn jv ko jy kp kc kq kg kr kk ks kt ku kv bi translated">上面的直方图显示了这些算法在CALC类的召回率和精确度方面的不佳表现。事实上，RetinaNet给出了该类的低召回率，但是在相同的类上有非常好的精确度。</li><li id="9e80" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated">对于ARCH和CIRC，我们通过使用3 SOTA获得了很好的召回分数，但是精确度分数低于其他类别，尤其是当我们使用FCOS时。</li><li id="14ad" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated">SPIC和ASYM具有可比的结果，因为我们用FCOS和RetinaNet获得了出色的召回率和精确度分数，而用fast-R CNN获得了稍低的性能。</li><li id="666e" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated">对于杂项类，我们使用3种SOTA方法获得了100%的召回率，使用RetinaNet和FCOS获得了100%的准确率。</li></ul><p id="cff9" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">总的来说，我们使用3个SOTAs中的一个在MISC、ASYM和ARCH类上获得了100%的召回率，并且我们可以注意到，考虑到召回率和精确度，更快的R-CNN为CALC类提供了最好的性能。</p><p id="a088" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated"><strong class="jr hj">最终结果</strong></p><p id="dec5" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">为了从这三个网络中选择一个，我们需要一个指标来比较它们，这个指标要结合精确度和召回率，并考虑到假阴性比假阳性要糟糕得多。因此，我们依赖于<strong class="jr hj"> F5得分</strong>，它是精确度和召回率的调和平均值，召回率被认为是精确度的5倍重要:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lj"><img src="../Images/c0da18bcce564aa955052e19d4f3770d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a28EHb6BO-SZnGChTEBelA.png"/></div></div></figure><p id="ae2a" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">使用这一新指标，以下是3种算法的结果:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lw"><img src="../Images/1bed21d0d3d521f01a415128254885ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F013rYTjzHOaZFeZDmWjdQ.png"/></div></div></figure><p id="d8b7" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">实验结果表明，该算法的f5评分为98.45%，优于其他算法。这一突出的结果是由于在CALC分类上的高召回率，也是由于在检测小肿瘤方面更快的RCNN优于其他算法的事实。</p><p id="4707" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">至于RetinaNet，我们获得了100%的准确率，这是一个很好的结果，但仍然不够，94.72%的召回率使其在f5分数方面排名第三，仅次于fast-RCNN和FCOS。</p><h1 id="d619" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">丰富</h1><ul class=""><li id="13f3" class="kl km hi jr b js jt jv jw jy lg kc lh kg li kk ks kt ku kv bi translated">就性能(精度/召回率)而言，这三种算法是互补的。因此，考虑集成学习技术以受益于每个算法的优点是适当的。查看<a class="ae lf" href="https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2" rel="noopener" target="_blank">这篇文章</a>了解更多信息。</li><li id="c5e2" class="kl km hi jr b js kw jv kx jy ky kc kz kg la kk ks kt ku kv bi translated">由于“只有”大约400张可用图像，我们应该尝试更好的数据扩充，使用<a class="ae lf" href="https://arxiv.org/pdf/1805.09501.pdf" rel="noopener ugc nofollow" target="_blank">自动增强</a>或<a class="ae lf" href="https://arxiv.org/pdf/1711.04340.pdf" rel="noopener ugc nofollow" target="_blank"> DAGANs </a>或使用迁移学习，首先在更大的数据库(如<a class="ae lf" href="https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM" rel="noopener ugc nofollow" target="_blank"> DDSM数据库</a>)上训练我们的网络</li></ul><h1 id="4f26" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">参考</h1><p id="ef12" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">我们的github回购:<a class="ae lf" href="https://github.com/delmalih/mias-mammography-obj-detection" rel="noopener ugc nofollow" target="_blank">https://github.com/delmalih/mias-mammography-obj-detection</a></p><p id="8d3a" class="pw-post-body-paragraph jp jq hi jr b js kn ij ju jv ko im jx jy lc ka kb kc ld ke kf kg le ki kj kk hb bi translated">[1] K .何等。艾尔。“更快的R-CNN:走向实时目标检测”——<a class="ae lf" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank"/><br/>【2】t .-y . Lin等。艾尔。“密集物体探测的焦损失”<a class="ae lf" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank"/><br/>【3】沈等。艾尔。“FCOS:全卷积一阶段物体检测”——<a class="ae lf" href="https://arxiv.org/pdf/1904.01355.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1904.01355.pdf</a><br/>【4】faster CNN实现——<a class="ae lf" href="https://github.com/facebookresearch/maskrcnn-benchmark" rel="noopener ugc nofollow" target="_blank">https://github.com/facebookresearch/maskrcnn-benchmark</a><br/>【5】retina net实现——<a class="ae lf" href="https://github.com/fizyr/keras-retinanet" rel="noopener ugc nofollow" target="_blank">https://github.com/fizyr/keras-retinanet</a><br/>【6】FCOS实现——<a class="ae lf" href="https://github.com/tianzhi0549/FCOS" rel="noopener ugc nofollow" target="_blank">https://github.com/tianzhi0549/FCOS</a><br/>【7】MIAS数据库——<a class="ae lf" href="https://tinyurl.com/y6elv5dr" rel="noopener ugc nofollow" target="_blank">https://tinyurl.com/y6elv5dr</a></p></div><div class="ab cl ly lz gp ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="hb hc hd he hf"><h1 id="4b66" class="ix iy hi bd iz ja mf jc jd je mg jg jh io mh ip jj ir mi is jl iu mj iv jn jo bi translated">项目海报</h1><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mk"><img src="../Images/1a4d25da362e949f2ac313b3b5357790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c-dWHiXTJ60MyFnch2qlFA.png"/></div></div></figure></div></div>    
</body>
</html>