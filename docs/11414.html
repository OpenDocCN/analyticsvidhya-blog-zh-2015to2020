<html>
<head>
<title>Real time tweets streaming with Kafka and Logstash</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过Kafka和Logstash实时传输推文</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/real-time-tweets-streaming-with-kafka-and-logstash-2a42aec74a5a?source=collection_archive---------5-----------------------#2020-12-02">https://medium.com/analytics-vidhya/real-time-tweets-streaming-with-kafka-and-logstash-2a42aec74a5a?source=collection_archive---------5-----------------------#2020-12-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3ec0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将首先简要解释这两者，然后展示一个简单的例子，在这个例子中，我使用Kafka通过Logstash将tweets实时传输到ElasticSearch中。</p><h1 id="a0bf" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">卡夫卡</h1><p id="a77b" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我最近开始使用Apache Kafka，我对它爱不释手，当你想要实时传输事件或拥有一个具有不同组件的应用程序，这些组件相互发送和使用数据时，它是一个非常好的工具。让我详细解释一下这两个问题。</p><ol class=""><li id="e422" class="kg kh hi ih b ii ij im in iq ki iu kj iy kk jc kl km kn ko bi translated"><strong class="ih hj">为什么数据流很重要？<br/> </strong>在当今这个每天都有大量数据流动的时代，你需要技术来存储、读取、分析和<strong class="ih hj"> <em class="kp">流</em> </strong>你的数据。数据流涉及对来自多个来源的大量实时数据的处理。一些例子是IOT设备发射数据，在移动或网络应用程序中(当你想分析你的应用程序中的用户交互时，每一次点击都是一个事件)，关于金融市场的信息等，你得到的想法。</li><li id="2a69" class="kg kh hi ih b ii kq im kr iq ks iu kt iy ku jc kl km kn ko bi translated"><strong class="ih hj">一个应用程序如何让不同的组件发出和使用数据？<br/>听起来很简单，一个来源和一个目的地。但这不会一成不变。一旦你的应用开始增长，不同的需求就会出现，比如数据的存储方式和类型。就像你希望有一个不同的用户活动数据源，一个不同的你在应用程序上实际显示的数据，一个不同的保存你的销售数据等等。这个列表可以逐渐增长。当您有许多源和目标系统时，您需要在这些系统之间有许多连接，这是非常难以管理的。</strong></li></ol><p id="4fc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">阿帕奇卡夫卡解决了这两个问题，在卡夫卡中，你可以有任何你能想到的数据流。它可以是你的应用数据、交易、用户互动等。一旦你的数据在Kafka中，你可以在任何你想要的系统中获取它。Kafka充当跨多个应用程序和服务的通用数据管道，我们将来自不同系统的所有数据驻留在一个位置，使Kafka成为真正的数据源。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es kv"><img src="../Images/b7b60193a2c28d9a9ca5e6bb619db541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*-7t-DSUPHaAbaczkBvUuMQ.png"/></div></figure><h2 id="63d7" class="ld je hi bd jf le lf lg jj lh li lj jn iq lk ll jr iu lm ln jv iy lo lp jz lq bi translated">卡夫卡的作品</h2><p id="6728" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">卡夫卡在出版-订阅模式上工作。这意味着它允许应用程序的某个部分在某个称为<em class="kp">主题</em>的类别下发布数据，而其他部分可以通过使用主题名称来获取数据。</p><p id="b3fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你的应用程序中发布数据的部分叫做<strong class="ih hj"> <em class="kp">生产者</em> </strong>，另一个消费数据的部分叫做<strong class="ih hj"> <em class="kp">消费者</em> </strong>。Kafka为这些任务提供了生产者API和消费者API。</p><h1 id="e22a" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">Logstash</h1><p id="73d3" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">Logstash是一个轻量级的服务器端数据处理管道，它可以同时从各种来源接收数据，然后解析、过滤、转换数据，最后将其转发给下游系统。这个下游系统通常是ElasticSearch(这两个和Kibana一起被称为ELK stack)，尽管它并不总是必须是ElasticSearch。</p><p id="9a27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">logstash有很多可用的输入插件，您可以使用它们从各种来源获取数据，kafka就是其中之一。</p><h1 id="22ba" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">动手示例—实时推文流</h1><p id="5327" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">这个小项目使用tweepy包从twitter获取数据，然后将数据上传到Kafka。然后kafka充当Logstash的输入系统，直接将数据放入ElasticSearch，最后使用Kibana可视化数据。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lr"><img src="../Images/e2acd6de554c7c9599cc253c2a62f9da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bFahUEJOChAbZLbrj6Nzmw.png"/></div></div></figure><h2 id="4d7d" class="ld je hi bd jf le lf lg jj lh li lj jn iq lk ll jr iu lm ln jv iy lo lp jz lq bi translated"><strong class="ak">让我们从安装和设置开始:</strong></h2><ol class=""><li id="0017" class="kg kh hi ih b ii kb im kc iq lw iu lx iy ly jc kl km kn ko bi translated"><strong class="ih hj">卡夫卡</strong></li></ol><p id="2a4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">前往此链接，<a class="ae lz" href="https://kafka.apache.org/downloads" rel="noopener ugc nofollow" target="_blank">https://kafka.apache.org/downloads</a>并下载二进制文件。</p><p id="403b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">进入下载的文件夹，然后通过运行以下命令启动<strong class="ih hj"> ZooKeeper服务器</strong>和<strong class="ih hj"> Kafka服务器</strong>:</p><pre class="kw kx ky kz fd ma mb mc md aw me bi"><span id="a752" class="ld je hi mb b fi mf mg l mh mi"><em class="kp"># Start ZooKeeper Server</em><br/>bin/zookeeper-server-start.sh config/zookeeper.properties<br/><br/><em class="kp"># Start Kafka Server</em><br/>bin/kafka-server-start.sh config/server.properties</span></pre><p id="db50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，您必须创建一个主题，您的系统将向该主题发布数据。</p><pre class="kw kx ky kz fd ma mb mc md aw me bi"><span id="01f1" class="ld je hi mb b fi mf mg l mh mi">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic trump</span></pre><p id="506d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我创建了一个名为“trump”的主题，因为我们将跟踪包含关键字trump的推文。</p><p id="d6c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kp">你可以这样查看主题列表:</em></p><pre class="kw kx ky kz fd ma mb mc md aw me bi"><span id="f536" class="ld je hi mb b fi mf mg l mh mi">bin/kafka-topics.sh --list --zookeeper localhost:2181</span></pre><p id="342d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。ElasticSearch，Logstash，Kibana: </strong></p><p id="9703" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从这个链接根据你的系统下载文件<a class="ae lz" href="https://www.elastic.co/downloads/" rel="noopener ugc nofollow" target="_blank">https://www.elastic.co/downloads/</a></p><p id="e15a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下载完成后，运行<code class="du mj mk ml mb b">elasticsearchfolder/bin/elasticsearch</code>(或Windows上的<code class="du mj mk ml mb b">elasticsearchfolder\bin\elasticsearch.bat</code>)和<code class="du mj mk ml mb b">kibanafolder/bin/kibana</code>(或Windows上的<code class="du mj mk ml mb b">kibanafolder\bin\kibana.bat</code>)分别触发elasticsearch和kibana。</p><p id="712f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于logstash，在运行之前需要一个配置文件。在<code class="du mj mk ml mb b">logstashfolder/bin/</code>中，你必须用<em class="kp">创建一个文件。conf </em>扩展名(例如yourconfig.conf)并在其中粘贴以下代码。</p><pre class="kw kx ky kz fd ma mb mc md aw me bi"><span id="59b9" class="ld je hi mb b fi mf mg l mh mi">input {<br/>    kafka {<br/>        topics =&gt; "trump" #your kafka should have this topic at this point.<br/>    }<br/>}</span><span id="ece9" class="ld je hi mb b fi mm mg l mh mi">output {<br/>    elasticsearch { hosts =&gt; ["localhost:9200"] index =&gt;    "practice_index"}<br/>    stdout { codec =&gt; "rubydebug" }<br/>}</span></pre><p id="45ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kp">(记住这个index的值，在这种情况下是practice_index，您以后会用到它)</em></p><p id="c006" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行<code class="du mj mk ml mb b">bin/logstash -f yourconfig.conf.</code></p><p id="cc87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设置已经完成，此时您已经在Kafka中准备好了一个主题，并运行了ELK。你所需要的只是一个python脚本，它获取tweets并把它们放到kafka中。</p><h2 id="352b" class="ld je hi bd jf le lf lg jj lh li lj jn iq lk ll jr iu lm ln jv iy lo lp jz lq bi translated">python脚本</h2><p id="a656" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">为了让这个脚本工作，您需要一些来自twitter的密钥和一个不记名令牌，这样您就能够通过他们的API获取数据。前往<a class="ae lz" href="https://developer.twitter.com/en/" rel="noopener ugc nofollow" target="_blank">https://developer.twitter.com/en/</a>生成这些。</p><p id="7e7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要运行下面的脚本，您必须安装一些软件包。运行以下命令:</p><pre class="kw kx ky kz fd ma mb mc md aw me bi"><span id="1160" class="ld je hi mb b fi mf mg l mh mi">pip install python-dotenv tweety kafka-python</span></pre><p id="d7a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，你将需要一只<strong class="ih hj"> <em class="kp">。env </em> </strong>文件，其中包含您的twitter访问令牌和其他密钥。该文件应该与该文件位于同一个文件夹中，其内容应该如下所示:</p><pre class="kw kx ky kz fd ma mb mc md aw me bi"><span id="87e9" class="ld je hi mb b fi mf mg l mh mi">TWITTER_API_KEY = your_twitter_api_key</span><span id="a29f" class="ld je hi mb b fi mm mg l mh mi">TWITTER_API_SECRET = your_twitter_api_secret</span><span id="3b72" class="ld je hi mb b fi mm mg l mh mi">TWITTER_BEARER_TOKEN = your_twitter_bearer_token</span><span id="caf3" class="ld je hi mb b fi mm mg l mh mi">TWITTER_ACCESS_TOKEN = your_twitter_access_token</span><span id="97f1" class="ld je hi mb b fi mm mg l mh mi">TWITTER_TOKEN_SECRET = your_twitter_token_secret</span></pre><p id="343b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在来看实际的脚本，代码简单易懂，我把脚本分成了几个部分。<strong class="ih hj"> <em class="kp">第1节</em> </strong>讲述从哪里挑选。env file和<strong class="ih hj"> <em class="kp"> Section 2 </em> </strong>从您的. env .<strong class="ih hj"><em class="kp">Section 3</em></strong>执行您的身份验证，而<strong class="ih hj"> <em class="kp"> Section 4 </em> </strong>包含一个简单的类，该类继承了由<em class="kp"> tweepy </em>包提供的StreamListener类(要了解更多信息，请阅读流文档<a class="ae lz" href="http://docs.tweepy.org/en/latest/streaming_how_to.html" rel="noopener ugc nofollow" target="_blank">这里</a>)。最后<strong class="ih hj"> <em class="kp">第5节</em> </strong>把所有东西放在一起:初始化一个卡夫卡制作人，初始化一个我们创建的类的对象，初始化一个流对象，最后启动跟踪关键字“trump”的流。</p><pre class="kw kx ky kz fd ma mb mc md aw me bi"><span id="eedc" class="ld je hi mb b fi mf mg l mh mi">import os<br/>from dotenv import load_dotenv<br/>from pathlib import Path<br/>import tweepy as tw<br/>import json<br/>import kafka</span><span id="a58c" class="ld je hi mb b fi mm mg l mh mi"># Section 1<br/>env_path = Path(“.”) / “.env”<br/>load_dotenv(dotenv_path=env_path)</span><span id="2a0e" class="ld je hi mb b fi mm mg l mh mi"># Section 2<br/>consumer_key = os.getenv(“TWITTER_API_KEY”)<br/>consumer_secret = os.getenv(“TWITTER_API_SECRET”)<br/>access_token = os.getenv(“TWITTER_ACCESS_TOKEN”)<br/>access_token_secret = os.getenv(“TWITTER_TOKEN_SECRET”)</span><span id="ad46" class="ld je hi mb b fi mm mg l mh mi"># Section 3<br/>auth = tw.OAuthHandler(consumer_key, consumer_secret)<br/>auth.set_access_token(access_token, access_token_secret)<br/>api = tw.API(auth, wait_on_rate_limit=True)</span><span id="4576" class="ld je hi mb b fi mm mg l mh mi"># Section 4<br/>class MyStreamListener(tw.StreamListener):<br/>  def on_status(self, data):<br/>    tweet_dict = {<br/>      “text”: data.text,<br/>      “user_name”: data.user.name,<br/>      “screen_name”: data.user.screen_name,<br/>      “id_string”: data.user.id_str,<br/>      “location”: data.user.location,<br/>    }</span><span id="b08c" class="ld je hi mb b fi mm mg l mh mi">    print(data.text)<br/>    #THE FOLLOWING LINE SENDS DATA IN KAFKA (Under topic "trump").<br/>    producer.send(“trump”, json.dumps(tweet_dict).encode(“utf-8”))<br/>    return True<br/>  def on_error(self, status_code):<br/>    if status_code == 420:<br/>    return False</span><span id="e8aa" class="ld je hi mb b fi mm mg l mh mi"># Section 5<br/>producer = kafka.KafkaProducer(bootstrap_servers=”localhost:9092")<br/>myStreamListener = MyStreamListener()<br/>myStream = tw.Stream(auth=api.auth, listener=myStreamListener)<br/>myStream.filter(track=[“trump”], is_async=True)</span></pre><p id="7c2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦您运行了脚本，您应该开始在打印tweepy消息的终端中看到一些日志，这意味着脚本已经成功执行，并且您能够使用tweepy传输数据。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mn"><img src="../Images/4ec465d2e1d8955a82ff9ea012d8de13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQbkyXPd9FscQqjCOuNSgA.png"/></div></div></figure><p id="b03d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它成功运行的事实意味着数据确实进入了kafka，要检查这一点，您应该查看运行logstash的终端。它还应该显示连续的日志，显示logstash正在获取输入，并成功地将其发送到elasticsearch。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mo"><img src="../Images/3c4915ea978866eddc27d29b6a3d25b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djj_8oZW-AtDVeH1S-ae6g.png"/></div></div></figure><p id="1b8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在是时候在Kibana中可视化这个了，前往localhost:5601，你应该会看到一个漂亮的Kibana仪表板。您必须在索引管理下创建一个索引模式，这是您需要在logstash配置文件中给出的索引的地方。一旦创建好了，转到kibana下的discover部分，你应该会看到关于你的数据的统计，比如有多少点击和什么时候。这是我离开一段时间后的样子。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mp"><img src="../Images/6e1eb32689a5c177e14073536e222d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zl81uaMQrTn0s_KY-Wzbsw.png"/></div></div></figure><p id="1784" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它显示，一小时内有177153条关于特朗普的推文。</p></div><div class="ab cl mq mr gp ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="hb hc hd he hf"><p id="6c63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你觉得这篇文章很有见地，请点击“鼓掌”按钮，如果有任何疑问/意见，请随时发表评论。我很想和你聊聊。:)</p></div></div>    
</body>
</html>