<html>
<head>
<title>Supervised Machine learning — Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督机器学习—线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/supervised-machine-learning-linear-regression-b6baa9a66cd9?source=collection_archive---------13-----------------------#2019-12-30">https://medium.com/analytics-vidhya/supervised-machine-learning-linear-regression-b6baa9a66cd9?source=collection_archive---------13-----------------------#2019-12-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e76d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<em class="jd">监督式机器学习中，</em>通过提供贴有正确标签的数据来训练模型。监督机器学习有几种算法，其中一种是线性回归。</p><p id="de53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本帖中，您将看到关于线性回归的简介、线性回归的类型、线性回归的假设、线性回归的工作原理、估计系数和截距的方法以及在python中实现模型的简要说明。</p><h2 id="9fcc" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">线性回归简介</strong></h2><p id="e91c" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在监督机器学习类别下的机器学习中，<em class="jd">线性回归</em>是最简单的参数化方法之一，您可以将其应用于数据集，以建模因变量(或响应变量)与一个或多个自变量(或解释变量)之间的关系。</p><p id="8958" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在线性回归中，因变量的值本质上是连续的。线性回归用于预测、时序数据、确定因果关系等等。</p><h2 id="4c9b" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">线性回归的类型</strong></h2><ol class=""><li id="7f87" class="ke kf hi ih b ii jz im ka iq kg iu kh iy ki jc kj kk kl km bi translated"><strong class="ih hj">简单线性回归:</strong>如果只有一个自变量那么就是简单线性回归。</li><li id="f555" class="ke kf hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated"><strong class="ih hj">多元线性回归:</strong>如果有两个或两个以上的自变量，则称为多元线性回归。</li></ol><h2 id="0593" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">线性回归的假设</strong></h2><ol class=""><li id="47bf" class="ke kf hi ih b ii jz im ka iq kg iu kh iy ki jc kj kk kl km bi translated"><strong class="ih hj">线性:</strong>自变量和因变量之间必须存在线性关系。</li><li id="890e" class="ke kf hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated"><strong class="ih hj">同方差或齐性:</strong>残差(或误差项)必须有一个恒定的方差，与自变量无关。</li><li id="4db3" class="ke kf hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated"><strong class="ih hj">缺乏多重共线性:</strong>自变量之间一定没有相关性。</li><li id="0de1" class="ke kf hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated"><strong class="ih hj">独立于残差(或误差):</strong>残差必须彼此独立，即t处的误差不应预测t+1处的误差。</li></ol><h2 id="4075" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">线性回归的工作原理</strong></h2><p id="eb7a" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">线性回归研究自变量和因变量之间的线性关系，线性关系用线性方程表示如下。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es ks"><img src="../Images/d63a46e807f5223d6537ba77f570b305.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*S4X3QmHMX4BcXfV0G8TRMQ.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">线性回归方程</figcaption></figure><p id="ef0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，<strong class="ih hj"><em class="jd"/></strong>为预测值，<strong class="ih hj"> n </strong>为自变量(或特征)个数，<strong class="ih hj"><em class="jd"/></strong>为第I个特征(或自变量)<strong class="ih hj"> βj </strong>为第j个模型参数。</p><p id="3879" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">— β0称为截距(或偏差)。如果数据的范围包括<em class="jd"> x1 = x2 …… = xn = 0 </em>，那么β0就是<em class="jd"> y </em>在<em class="jd"> x1 = x2 …… = xn </em> = 0时的平均值。</p><p id="6668" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">— β1至βn称为系数(或权重或斜率)。参数βj代表当所有剩余的回归变量<em class="jd">【Xi】</em>(<em class="jd">I</em>≦<em class="jd">j</em>)保持不变时，单位变化<em class="jd"> xj </em>的响应(或因变量)<em class="jd"> y </em>的预期变化。</p><p id="50eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型试图用二维空间中的一条线或三维空间中的一个平面来拟合尽可能多的数据点，等等。在下图中，这条线代表一条近似的线，可以解释“x”轴和“y”轴之间的关系。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/48a27da50b405b84ba8e2638264e5d09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0e3mIv8ZFC6ZOwzTtNaFQ.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">线性回归线</figcaption></figure><p id="6d78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于线性回归的目标是拟合通过最接近大多数点的分布的直线，我们需要训练一个线性回归模型<em class="jd">来找到最小化均方误差(MSE)的参数β的值。</em>均方误差是预测值和真实值之间的平方差之和除以样本数。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lj"><img src="../Images/6ce279dce4a6bf4107cc76dcaae46d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*rJfsZOccHINudt1RYeR70Q.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated"><em class="lk">线性回归模型的MSE成本函数</em></figcaption></figure><p id="707b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里y^(i)是第I个样本的第I个预测值，y(i)是第I个样本的第I个实际值(或真值), m是样本数。</p><h2 id="52f5" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">估计系数和截距的方法</strong></h2><p id="c172" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated"><strong class="ih hj">正规方程</strong></p><p id="2b2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">法线方程是一种分析方法，用于找出最小化成本函数的参数值。正规方程有助于直接或在go中找到最优解。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es ll"><img src="../Images/89ae75b1cef4b8e01a06ee9d99db227f.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*sDneiZKa_d--4hCZoM2-5A.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">正规方程</figcaption></figure><p id="63ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，<strong class="ih hj"> y </strong>是观测值的<em class="jd"> n </em> × 1向量，<strong class="ih hj"> X </strong>是回归变量层次的<em class="jd">n</em>×p矩阵，<strong class="ih hj"> <em class="jd"> β </em> </strong>是回归系数的<em class="jd"> p </em> × 1向量。</p><p id="92eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果(<em class="jd"> XT X) </em>不可逆，则正规方程方法不适用，在这种情况下，我们可以使用梯度下降法。</p><p id="d8df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">梯度下降</strong></p><p id="d3ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降法是一种寻找参数最优值的迭代方法。</p><p id="d0bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设你迷失在半山腰，只能感觉到脚下地面的坡度。到达山脚的最好方法是向最陡的斜坡方向前进。这就是梯度下降所做的，它测量误差函数关于参数的梯度，它沿着梯度下降的方向前进。一旦梯度下降为零，你就达到了最小值！</p><p id="9d8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最初，我们从β的随机值开始(这被称为<em class="jd">随机初始化</em>)。然后我们逐步改进它，一次一步，每一步都试图降低成本函数，直到算法达到最小值。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es lm"><img src="../Images/866b6bd988207ce96fd2cb6565c3537f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*QGk1tMUcW97lci9gQOWiLQ.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">梯度下降</figcaption></figure><p id="f05f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在梯度下降中，步长的大小很重要，称为学习率超参数。如果学习率太小，算法可能需要时间来找到最小值，并且可能需要太多的迭代。如果学习率太大，算法可能会错过最佳值。</p><p id="ebf5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，并不是所有的成本函数看起来都像规则的碗形，如下图所示。如果随机初始化从左边开始，那么它收敛到局部最小值。如果从右边开始，可能需要很长时间才能达到全局最小值。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div class="er es ln"><img src="../Images/1272a9cbf6af2775c1b1f0eaa6b6b9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*SKtJS0D1WND8Q_XGlmMYcg.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">梯度下降法的局部和全局最小值</figcaption></figure><p id="5854" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在线性回归中，MSE成本函数恰好是凸函数，这意味着如果您在曲线上选取任意两点，连接它们的线段永远不会穿过曲线。这意味着没有局部最小值，只有一个全局最小值。</p><h2 id="661c" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">用python实现</strong></h2><p id="b12a" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">下面的代码是使用python中的scikit-learn库实现线性回归，你可以在这里找到来自<a class="ae lo" href="https://github.com/Divyashree-eswar/Linear-Regression/blob/master/Salary_Data.csv" rel="noopener ugc nofollow" target="_blank">的数据集。</a></p><pre class="kt ku kv kw fd lp lq lr ls aw lt bi"><span id="bdf3" class="je jf hi lq b fi lu lv l lw lx">#importing the libraries<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="d429" class="je jf hi lq b fi ly lv l lw lx">#importing the dataset<br/>dataset = pd.read_csv("Salary_Data.csv")<br/>X = dataset.iloc[:,:-1]<br/>y = dataset.iloc[:,1]</span><span id="df33" class="je jf hi lq b fi ly lv l lw lx">#spliting the data into training set and test set<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =1/3, random_state =0)</span><span id="362a" class="je jf hi lq b fi ly lv l lw lx">#building the linear regression model<br/>from sklearn.linear_model import LinearRegression<br/>reg = LinearRegression()<br/>reg.fit(X_train,y_train)</span><span id="5492" class="je jf hi lq b fi ly lv l lw lx">#performing the prediction on the test set <br/>y_pred = reg.predict(X_test)</span><span id="b833" class="je jf hi lq b fi ly lv l lw lx">#plotting the regressison line.<br/>plt.scatter(X_test, y_test, color = 'red')<br/>plt.plot(X_train, reg.predict(X_train), color = 'blue')<br/>plt.title('Salary vs Experience (Test set)')<br/>plt.xlabel('Years of Experience')<br/>plt.ylabel('Salary')<br/>plt.show()</span></pre><h1 id="8f77" class="lz jf hi bd jg ma mb mc jk md me mf jo mg mh mi jr mj mk ml ju mm mn mo jx mp bi translated">结论</h1><p id="c5c3" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">我们已经学习了线性回归的概念、假设、正规方程、梯度下降以及使用scikit-learn库在python中实现。</p></div></div>    
</body>
</html>