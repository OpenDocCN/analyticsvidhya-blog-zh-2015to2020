<html>
<head>
<title>How I trained a self-supervised neural network to beat GnuGo on small (7x7) boards</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何训练一个自我监督的神经网络在小(7x7)棋盘上击败GnuGo</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-i-trained-a-self-supervised-neural-network-to-beat-gnugo-on-small-7x7-boards-6b5b418895b7?source=collection_archive---------7-----------------------#2020-01-25">https://medium.com/analytics-vidhya/how-i-trained-a-self-supervised-neural-network-to-beat-gnugo-on-small-7x7-boards-6b5b418895b7?source=collection_archive---------7-----------------------#2020-01-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="37a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章是我大约一年前写的一篇<a class="ae jd" href="https://towardsdatascience.com/alphazero-implementation-and-tutorial-f4324d65fdfc" rel="noopener" target="_blank">文章</a>的后续，这篇文章讲述了我实现了一个非常类似AlphaGo Zero算法的东西，并在我的普通台式电脑上运行了它。这些代码产生的网络在性能方面我会(也确实)总结为“没那么好”。</p><p id="e1e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在过去的一年里，我尝试了一些不同的方法(比如添加一些<a class="ae jd" href="https://blog.janestreet.com/accelerating-self-play-learning-in-go/" rel="noopener ugc nofollow" target="_blank">辅助损失函数</a>，顺便提一下，我发现了一个问题，如下文更详细描述的，这导致我的网络以次优的方式被评估(但不是被训练)。结果，我训练的网络(即使没有任何辅助损失函数或其他附加功能)比我想象的要好得多。</p><p id="ac88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么网络有多好呢？嗯，当我与他们对战时(我是一个极端的新手，所以你可以这样认为)，我让网络采取第一步行动，我几乎总是在<code class="du je jf jg jh b">7x7</code>棋盘上输(我唯一一次击败它是在我发布的<a class="ae jd" href="https://youtu.be/a5vq1OjZrCU" rel="noopener ugc nofollow" target="_blank">游戏视频</a>中)。我打第一招，网络打第二招的时候，偶尔能打得过。你可以在<a class="ae jd" href="https://youtu.be/a5vq1OjZrCU" rel="noopener ugc nofollow" target="_blank">这个Youtube视频</a>里看我和它打几局。</p><h1 id="afde" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">神经网络vs GNU围棋</strong></h1><p id="506c" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">它对GNU Go的表现如何？以下是我在不使用任何树搜索的情况下运行网络时的一些统计数据(即，我将网络的移动定义为具有最高概率输出的移动位置):</p><ul class=""><li id="38c4" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated"><strong class="ih hj">网络先下</strong> (GNU Go下第二步开局棋):<br/>网络赢<strong class="ih hj">93%</strong>(128局119胜)</li><li id="a0a4" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated"><strong class="ih hj">网络下第二</strong> (GNU Go下第一步开局棋):<br/>网络胜<strong class="ih hj">9.4%</strong>(128局12胜)</li></ul><p id="11f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这两个条件的平均值将是网络赢得51%的时间。注意，上面的统计数据可以用<a class="ae jd" href="https://github.com/cody2007/alpha_go_zero_implementation" rel="noopener ugc nofollow" target="_blank"> GitHub存储库</a>中的<code class="du je jf jg jh b">net_vs_gui.py</code>脚本复制。</p><h1 id="9451" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">培训和模型详情</strong></h1><p id="30d6" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">在大约两个月的时间里，我在两张GPU卡上进行了培训(有关我的设置的更多详细信息，请参见下面的章节),这与我之前描述的<a class="ae jd" href="https://towardsdatascience.com/alphazero-implementation-and-tutorial-f4324d65fdfc" rel="noopener" target="_blank">非常相似</a>。所有的自我游戏训练都来自持续32个“回合”的游戏(其中“回合”意味着每个玩家移动一次——因此每个玩家有机会放置32次石头)。自玩游戏推出800次。我没有为网络实现任何退出机制，所有游戏持续了32回合(如果网络不能移动，游戏无论如何都会进行到下一回合，没有移动)。</p><p id="bc5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每个<code class="du je jf jg jh b">128*5</code>自玩游戏从“生成器模型”中生成后，我通过“当前模型”对自玩批处理池(由每个包含<code class="du je jf jg jh b">32</code>“回合”[如上定义]的<code class="du je jf jg jh b">35*128</code>游戏组成)的<code class="du je jf jg jh b">32*5*5</code>训练步骤进行反向传播。在梯度更新之后，对“当前”和“发电机”模型进行相互评估，以查看“当前”模型是否可以提升到下一个“发电机”模型(见下图二)。</p><p id="071b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经将模型权重和定义提交到了<a class="ae jd" href="https://github.com/cody2007/alpha_go_zero_implementation" rel="noopener ugc nofollow" target="_blank">存储库</a>(参见<code class="du je jf jg jh b">models/</code>)。该模型有5层，每层包含128个卷积滤波器。值和策略标题各有一个带有128个过滤器的独立全连接层，其后是最后一个与相应输出维度匹配的全连接层(分别为<code class="du je jf jg jh b">1</code>和<code class="du je jf jg jh b">7x7</code>)。否则，模型详细信息(批次定额等。)和AlphaGo Zero以及相关论文大体相似。</p><p id="d7ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的第一组图是训练曲线，第二组图是网络被“提升”的次数(当前训练的模型被设置为新的“生成器”模型，用于生成新的自玩训练示例)。垂直线表示促销发生的时间。本文中的所有图形都可以用我包含在存储库中的笔记本来复制(<code class="du je jf jg jh b">notebooks/training_visualizations.ipynb</code>)。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es kz"><img src="../Images/10dfba31baa54186f47a66a2610c081f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WoMT7TKSRN6jlcbx_LlAwg.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">模型的训练。<strong class="bd jk">浅红色</strong>线是对随机移动的对手评估网络时的统计数据。<strong class="bd jk">深红色</strong>线是反对GNU Go模型的。所有的评估都没有使用树搜索(网络的移动被选择为其概率最高的位置)。请注意，所有网络训练数据都来自自我游戏——与GNU Go和随机对手的游戏只是为了可视化训练进度。还要注意，在这些图上对GNU Go的评估带有下面描述的评估错误。</figcaption></figure><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lp"><img src="../Images/ae9e3324841d0854aaf8e55f98821d59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*Ay2oYifD_fhvrtimDnjRyA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">在整个训练过程中，评估“当前”模型(通过back-prop训练的重量的当前版本)与“生成器”模型(用于生成自我游戏训练数据的模型)。垂直线表示发电机模型何时升级为新的“当前”模型。</figcaption></figure><h1 id="75ce" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">示例游戏</strong></h1><p id="34ea" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">你也可以在<a class="ae jd" href="https://youtu.be/a5vq1OjZrCU" rel="noopener ugc nofollow" target="_blank">这个视频</a>里看到一些我玩游戏的例子。下面是网络对战GNU Go(随机对手)和自身(自玩)的例子。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lq"><img src="../Images/c027f4b2135e1d93d1f7b3eaf8fee44d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_AKrMLe0k3TINL_EaZlniA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">神经网络在上面两个游戏中分别扮演黑。请注意，随机对手只是随机移动。</figcaption></figure><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lr"><img src="../Images/4a899cc778edc6b0959bd0b3729a4fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f9xDIlK_tir-UuasSvusCg.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">神经网络在和自己作对。诸如此类的游戏被用来训练神经网络。</figcaption></figure><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lr"><img src="../Images/e953f8cbcd1d8019fd84e2041eb1dcd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AndvNBS2fi-z3CBDhI0xpg.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">神经网络在和自己作对。诸如此类的游戏被用来训练神经网络。</figcaption></figure><h1 id="be40" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">bug</strong></h1><p id="2ca2" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">据我所知，我之前所做工作的主要和唯一的主要问题是，我没有在Tensorflow批处理范数函数中设置<code class="du je jf jg jh b">training</code>标志(它默认总是处于<code class="du je jf jg jh b">training</code>模式，在该模式下，平均值和方差统计随着每个网络评估而更新)。结果，我相信当网络在测试时被评估时，网络总是以条件差的状态结束。当我与网络对战时，我也察觉到了这一点——他们似乎总是有不错的开局，然后随着游戏的进行，一切都变成了犯很多粗心的错误。</p><h2 id="1c12" class="ls jj hi bd jk lt lu lv jo lw lx ly js iq lz ma jw iu mb mc ka iy md me ke mf bi translated">一个未解之谜</h2><p id="df0c" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">虽然我正在训练的网络表现良好，如以上部分所述，但是在发现这个bug的过程中仍然存在一个异常，我还没有弄清楚。让我先总结一下我如何进行训练的一些细节。代码在内存中保存了三个模型:</p><p id="9fcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="mg">(之所以使用混合精度浮点数，是因为它</em> <a class="ae jd" href="https://arxiv.org/abs/1710.03740" rel="noopener ugc nofollow" target="_blank"> <em class="mg">加快了(计算)训练时间</em> </a> <em class="mg">。)</em></p><ul class=""><li id="f788" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated"><strong class="ih hj">【main】</strong>:该模型用于生成新的自弹训练批次。“主”存储为<code class="du je jf jg jh b">float16</code> s。</li><li id="ea51" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated"><strong class="ih hj">“eval 32”</strong>:该模型使用“main”创建的self-play训练批次进行训练。“eval32”存储为<code class="du je jf jg jh b">float32</code> s。</li><li id="854b" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated"><strong class="ih hj">【eval】</strong>:该模型的目的是在围棋对弈中对“主”模型进行评估(这是将“eval32”模型转换成<code class="du je jf jg jh b">float16</code>的副本)。一旦它能够以足够高的概率战胜“主”模型，它就被提升(复制/覆盖)到“主”模型。</li></ul><p id="cc80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">反向传播从不直接发生在“主”和“评估”模型上，它们位于“评估32”模型的下游。出于这个原因，看起来您永远不会想要在<code class="du je jf jg jh b">training</code>模式下运行这些模型——统计数据应该由“eval32”模型在训练时设置并保持固定。因此，我认为下面的<code class="du je jf jg jh b">training</code>旗帜配置最有意义:</p><ul class=""><li id="8aa4" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated"><strong class="ih hj">【主】</strong> : <code class="du je jf jg jh b">training</code> = <code class="du je jf jg jh b">False</code></li><li id="1efa" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated"><strong class="ih hj"> "eval32" </strong> : <code class="du je jf jg jh b">training</code> = <code class="du je jf jg jh b">True</code></li><li id="85a0" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated"><strong class="ih hj">【eval】</strong>:<code class="du je jf jg jh b">training</code>=<code class="du je jf jg jh b">False</code></li></ul><p id="ba24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，我发现用上述配置进行训练会导致模型表现不佳。只有当我在训练期间将所有标志设置为<code class="du je jf jg jh b">True</code>时，我才能得到像样的表演模型(如果在<code class="du je jf jg jh b">training</code>标志设置为<code class="du je jf jg jh b">False</code>的情况下玩；如果我在<code class="du je jf jg jh b">training</code>标志设置为<code class="du je jf jg jh b">True</code>的情况下播放它们，性能仍然很差)。如果有人知道为什么会这样，请告诉我！我认为这种配置的训练会导致模型更差，而不是更好。</p><h1 id="1bd4" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">代码</strong></h1><p id="5b05" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">我修复了bug的代码是GitHub上的<a class="ae jd" href="https://github.com/cody2007/alpha_go_zero_implementation" rel="noopener ugc nofollow" target="_blank">可用的</a>——我再次在公共领域发布了它。除了bugfix之外，我还增加了在两个GPU上同时训练的能力，这是我在训练本文中谈到的当前模型时所做的。</p><h1 id="016f" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">我的设置</strong></h1><p id="f197" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">所有代码都已经在Centos 8上使用Python 2.7.16、Tensorflow v1.15.0进行了测试和编写，并使用NVCC vv 10 . 2 . 89(Nvidia Cuda编译器)进行了编译。我在双GPU设置(使用Nvidia 2080 Ti和Nvidia Titan X卡)、四核Intel i5–6600k CPU @ 3.50 GHz和48 Gb RAM(代码本身通常在使用35 Gb左右时达到峰值)上运行并测试了所有代码。我还没有在其他配置上测试过它(虽然有一段时间我运行的是Ubuntu 18.04而不是Centos 8，而且一切都在那里工作)。如果您要使用更少的RAM来运行这个设置，我建议只运行一个GPU(并将RAM需求减半)，而不是减少树搜索深度(这将是减少RAM使用的替代方法)。</p><h1 id="11ae" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">更进一步</strong></h1><p id="0eaa" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">一个明显的下一步就是增加电路板的尺寸，看看我能在我的设置中做到什么程度。然而，计算机Go邮件列表上的其他人建议说，他们期望我应该能够通过这种类型的训练设置获得更高性能的模型。他们的想法包括设置komi(我目前使用的值为0，这可能会剥夺网络的学习信号，因为黑色[谁先玩]赢得了大多数的自玩游戏)，使用技术来确保足够多的游戏品种，并减少自玩游戏的初始缓冲区大小，以便更快地开始训练。你可以在<a class="ae jd" href="http://computer-go.org/pipermail/computer-go/2020-January/011062.html" rel="noopener ugc nofollow" target="_blank">邮件列表线程</a>上了解更多信息。</p><h2 id="9aac" class="ls jj hi bd jk lt lu lv jo lw lx ly js iq lz ma jw iu mb mc ka iy md me ke mf bi translated">框架投诉</h2><p id="c1d9" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">虽然我想继续上面的一些想法，但不幸的是，我最近更新了我的系统(用“yum update”更新了通用发行版包——没有对Tensorflow进行更新)，现在当我的代码在第二个GPU上启动时，我的代码的多GPU部分崩溃了——尽管同样的代码已经完美地工作了大约一年——这个问题似乎是在我尝试运行模型时Tensorflow 1.15崩溃。</p><p id="c86e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我还不确定我是否会尝试修补一艘正在下沉的船或者完全改变框架。Tensorflow在许多方面都非常好用，我非常感谢所有开发人员所做的出色工作。然而，这些年来，当函数的名称和语义似乎不必要地改变和贬值，或者事情没有明显的原因就停止工作时，保持任何代码在其上一致地运行就越来越像是一件苦差事，就像我现在所经历的。</p><p id="2f80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在2015年Tensorflow首次发布之前和发布期间(在我知道Tensorflow之前)，我一直在开发自己的神经网络框架，目的类似于Tensorflow(尽管我的用例在范围上比TF更窄)，除了我编写的一些其他自定义CUDA内核之外，我还直接调用和使用了<a class="ae jd" href="https://developer.nvidia.com/cudnn" rel="noopener ugc nofollow" target="_blank"> cuDNN </a>。无论如何，提到这一点的目的是，这段代码今天仍然可以编译和运行，尽管自从我转到TF后，我已经几年没有碰过它了。</p><p id="9fab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，对我来说，下一步可能是将我的代码转移到我的旧框架中。从长远来看，也许我最终会清理它并发布框架。我知道我不是唯一一个厌倦了软件框架无缘无故地改变的人。幸运的是，对于深度学习来说，它似乎并没有完全渗透进来cuDNN似乎一直保持着相当稳定的状态(正如我在2015年编译和运行的代码所证明的那样)，因此没有理由在它的基础上构建的任何东西都不会接近相同水平的稳定性的极限。</p><h2 id="c9f8" class="ls jj hi bd jk lt lu lv jo lw lx ly js iq lz ma jw iu mb mc ka iy md me ke mf bi translated">超越围棋</h2><p id="932d" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">从长远来看，我希望能有一个网络让<a class="ae jd" href="http://arcanefortune.com" rel="noopener ugc nofollow" target="_blank">玩一款游戏</a>，或者我正在开发的游戏的一部分。可能那里的方法将需要采取人类监督和自我监督学习的更混合的方法(也许类似于<a class="ae jd" href="https://deepmind.com/research/publications/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning" rel="noopener ugc nofollow" target="_blank"> AlphaStar </a>)。很可能我永远不会像DeepMind那样在规模实验室中运行网络训练，但我所看到的Go使用最少硬件的进步无疑是对我的鼓励。这些网络可能不在李·塞多尔的水平上，但它们仍然足够好，足以成为我们其他人的好对手:)</p></div></div>    
</body>
</html>