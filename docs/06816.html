<html>
<head>
<title>The Linear Algebraic Love for ML: Or How Change of Basis, EigenDecomposition &amp; PCA all come together beautifully!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性代数对ML的热爱:或者说基的改变、特征分解和PCA是如何完美地结合在一起的！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-linear-algebraic-love-for-ml-or-how-change-of-basis-eigendecomposition-pca-all-come-f3d37715978c?source=collection_archive---------10-----------------------#2020-06-03">https://medium.com/analytics-vidhya/the-linear-algebraic-love-for-ml-or-how-change-of-basis-eigendecomposition-pca-all-come-f3d37715978c?source=collection_archive---------10-----------------------#2020-06-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2eda2da1ccdd805d1f9227d969f9223e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lifBC7GN5ugoZFc_6G7pDA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:<a class="ae iu" href="https://mathconceptions.wordpress.com/2013/06/12/word-clouds-indicate-retention/" rel="noopener ugc nofollow" target="_blank"> Mathconceptions </a></figcaption></figure><p id="a401" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们处理向量空间时，我们经常互换使用向量和向量坐标的概念。但这是两个截然不同的概念。矢量的坐标总是相对于基底而言的。</p><p id="3be3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">而如果我们省略了基，自然会假设它是标准基{(0，1)，(1，0)}。为了本文的目的，我们称标准基坐标系为CS-1。CS-1中的一个向量，通常写为:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/cd9afcbf19fa43d04fc57bf76959945f.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*Pz4U-w2QMKbqSgEbj2cmrg.png"/></div></figure><p id="c20f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它也可以写成矩阵形式:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jy"><img src="../Images/7b649f8644502870ba2a5c4e8006b12d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*GyTQm0yPPjR9UnrEao4KHw.png"/></div></figure><p id="d244" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以上面的坐标实际上代表了矢量<strong class="ix hj"> a </strong>，并且可以写成:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jz"><img src="../Images/c51c99ca869653bc868f49c19a4d3082.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*w0nYllRTuL0Q16Kovsinew.png"/></div></figure><h1 id="9f51" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">基础的改变:</h1><p id="117e" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">现在，让我们考虑第二个坐标系CS-2，其基向量如下所示，该坐标系:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/df58da6509fb78cbac7be78bad6af989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-dEBKWODE4x2Cs-6DoAx1A.png"/></div></div></figure><p id="bb20" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一个坐标系统可以被认为是一个由它自己的基本向量构成的向量空间。如果你有一个由标准坐标系CS-1表示的默认向量空间作为你的锚点，你可以把这个第二向量空间看作是应用一个或多个变换(平移、旋转等)后的结果。)在缺省向量空间上。</p><p id="ee3c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">并考虑CS-2中的一个向量，其坐标w.r.t. CS-2已给定。这写为其基本向量的线性组合，看起来像:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/e1794c21ef9cae14e9313883dfcbabf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gjKVk1R0PbTBYvV_P2oKZA.png"/></div></div></figure><p id="7d4a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但最后一个等式是向量在CS-2中是如何表示的。我们更了解CS-1，对吗？所以为了在CS-1中转换这个向量，我们首先要在CS-1中表示CS-2的基向量。从变换的角度来看，这就像说，标准基向量，<strong class="ix hj"> i </strong>和<strong class="ix hj"> j </strong>如何被变换为<strong class="ix hj"> b1 </strong>和<strong class="ix hj"> b2 </strong>。让我们说:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/5c7485dd623ce7283d885fa40cf84edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*nMOev2ciIHjRwr-9k-qiGQ.png"/></div></figure><p id="2973" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面最后一个等式中的矩阵被称为基矩阵的<strong class="ix hj">变化。我们得到了用CS-1表示的向量。</strong></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/238ec08358c70c9a80f3b315fca604e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*RMOV5Pvly6str8H5I86L5g.png"/></div></figure><p id="81ee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在一般形式下，我们可以说:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/ae3a643e998caea2371a070b0a4dcfb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1MepxHbI3HWjH14qKKZ8w.png"/></div></div></figure><p id="9c51" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">简单地说，我们可以说，变换矩阵(基矩阵的变化)给出了新坐标系(CS-2)的基向量——用原始坐标CS-1表示。</p><h1 id="de1e" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">分解转换:</h1><p id="0e32" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">在讨论本征分解之前，我们需要理解，一个变换可以分解为多个依次应用的变换，这将产生与原始变换相同的效果。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es li"><img src="../Images/c7af697e45293f646ccba4c6e60404f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*ISCQkmBnS9RlDpdmiJCGpw.png"/></div></figure><p id="a2ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">也就是说，不是应用原始的复杂变换<strong class="ix hj"> A </strong>，而是通过将它带到不同的向量空间(由另一组基(<strong class="ix hj"> M </strong>)来尝试应用等效的更简单的变换(<strong class="ix hj"> B </strong>)，然后将变换后的向量带回我们的原始向量空间(<strong class="ix hj"> M^-1 </strong>)。</p><p id="f232" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中:</p><p id="5b6a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> M </strong>:基矩阵的改变</p><p id="1f00" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> B </strong>:备用上的简单转换</p><p id="8614" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> M^-1 </strong>:基矩阵的逆变换</p><h1 id="fbf2" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">特征分解:</h1><p id="c55e" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">这里不得不引用一个实对称变换矩阵的有趣性质:一个<em class="lj"> n×n实</em>对称矩阵有<em class="lj"> n个</em>线性无关且正交的特征向量(即特征向量相互垂直)，它有<em class="lj"> n个</em>实特征值对应于那些特征向量。所以特征向量也跨越了整个n维空间。换句话说，这些特征向量可以用作同一个<em class="lj"> n </em> -dim空间的基向量。</p><p id="1340" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，实对称矩阵可以正交对角化。这就是所谓的<strong class="ix hj"> <em class="lj">【谱定理】</em> </strong>。有趣的事实:这个名字是因为，矩阵的特征值被称为它的“谱”——解释为什么会这样，需要触及Banach空间，泛函分析，量子力学中的本征态等。—这些都不在本文作者的讨论范围之内！</p><p id="eb5a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">回到我们之前的位置，通过正交对角化，矩阵<strong class="ix hj"> A </strong>可以分解为:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/114c0815097e7512c2510ca103a95928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*LRERexgEAZehgt62OMwqrg.png"/></div></figure><p id="5ea8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<strong class="ix hj"> D </strong>为对角矩阵，其对角元素无非是<strong class="ix hj"> A </strong>的特征值。而这个方程叫做a的<strong class="ix hj"> <em class="lj">【特征分解】</em> </strong>。</p><p id="d95f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">直接引用[Ref。] <em class="lj">“也就是说，A的行为就像一个对角矩阵，当我们改变坐标时:更准确地说，标准基中的映射x - &gt; Ax，当写成M的坐标时，与[xm] - &gt; D[xm]相同。”</em></p><p id="b575" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，结合上述思想，我们可以说，当变换矩阵是实对称的时，相应的变换可以分解成3个更简单的变换:旋转、缩放、逆旋转。也就是说，它通过以下方式变换一个向量:将其带到本征基坐标，沿着其本征向量拉伸或收缩该向量(拉伸或收缩的量与对应的本征值成正比)，然后将其带回我们的标准坐标。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/fadd7b941acfda6d7360f28080ea40ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qLWiHoYZUT00rK3mz47sRA.png"/></div></div></figure><h1 id="5212" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">主成分分析(PCA):</h1><p id="e959" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">PCA可以从多个参照系来解释——最大化方差或最小化重构误差。这里我们将看到PCA被解释为协方差矩阵的特征分解。让数据矩阵和转换矩阵定义如下:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/37569a1b818f364be4b39f791bacb35f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0OsEeVbshjGrVxNN2VovDg.png"/></div></div></figure><p id="4392" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该变换给出为:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ln"><img src="../Images/833efe65fa303aa749ebb7d34c83ded2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hJ5HdzJOQOxPVnXAy1srLQ.png"/></div></div></figure><p id="a43f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">求这个变换矩阵的协方差矩阵:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/1b7b3d91951ba2837795ebab29ea503b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DT5Hz48wcREE5y454Lc6LQ.png"/></div></div></figure><p id="39a8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> V </strong>是原始数据矩阵变换前的协方差矩阵。</p><p id="bf82" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最佳变换<strong class="ix hj"> W </strong>将把协方差(变换后)减少到一个对角矩阵(意味着，在变换后，变量之间没有相关性)</p><p id="7044" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了使<strong class="ix hj"> WVW^T </strong>成为对角矩阵，</p><p id="a0cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> W </strong>:特征基:作为基的<strong class="ix hj"> V </strong>的特征向量。</p><p id="760d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，数据点从Rd变换到Rk，其中Rk是原始协方差矩阵<strong class="ix hj"> V </strong>的k个特征向量所跨越的子空间。</p><p id="6b15" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的思路流中，你可以看到几个看似不相关的概念如何融合在一起，以帮助理解特定的机器学习构造。</p><h1 id="eabe" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">参考资料:</h1><p id="078d" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated"><a class="ae iu" href="https://www.math.wustl.edu/~freiwald/309orthogdiag.pdf" rel="noopener ugc nofollow" target="_blank">https://www.math.wustl.edu/~freiwald/309orthogdiag.pdf</a></p></div></div>    
</body>
</html>