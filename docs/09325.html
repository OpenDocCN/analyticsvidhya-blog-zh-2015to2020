<html>
<head>
<title>Logistic Regression in Brief</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简而言之，逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-in-brief-515819a0da72?source=collection_archive---------28-----------------------#2020-09-01">https://medium.com/analytics-vidhya/logistic-regression-in-brief-515819a0da72?source=collection_archive---------28-----------------------#2020-09-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6dcb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归是一种分类算法，用于将观察值分配给一组离散的类。与输出连续数值的线性回归不同，逻辑回归使用逻辑sigmoid函数转换其输出，以返回一个概率值，该概率值可映射到两个或多个离散类。</p><p id="dfe4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/3bcd04983c395d1cbb20b0242c416261.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*cKBCYGfqjsqXKR_3JMBYqA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">逻辑回归|作者图片</figcaption></figure><p id="0ddc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给出学习时间和考试分数的数据。逻辑回归可以帮助预测学生是通过还是失败。1表示通过，0表示失败。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jp"><img src="../Images/8c602912c57c8896f20b94644e876aeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*mqjsd1o0TmrmM9k29mcxwg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图形表示|作者图片</figcaption></figure><h1 id="7ce4" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">假设函数</h1><p id="e55d" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">统计<strong class="ih hj">假设</strong>是对数据总体之间关系的解释，它是从概率角度解释的。一个<strong class="ih hj">机器学习假设</strong>是一个逼近目标<strong class="ih hj">函数</strong>的候选模型，用于将输入映射到输出。逻辑回归的假设函数是</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kt"><img src="../Images/47abe8b40610c7cf9db7c6f7e74b477f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bJ0MBDeMmtdEx0B72ggwQ.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">假设函数|作者图片</figcaption></figure><p id="8847" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> <em class="ky"> θ </em> </strong>为模型的参数，<strong class="ih hj"> <em class="ky"> X </em> </strong>为输入向量，<strong class="ih hj"> <em class="ky"> g </em> </strong>为<strong class="ih hj"> <em class="ky"> Sigmoid函数。</em> </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kz"><img src="../Images/0bbd536745b5a66bc0c20ebce4955795.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*tMMyC0-2m0knO3X8LWakTw.png"/></div></figure><p id="30e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Sigmoid函数:</strong>为了将预测值映射到概率，我们使用了<a class="ae la" href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#activation-sigmoid" rel="noopener ugc nofollow" target="_blank"> sigmoid </a>函数。该函数将任何实数值映射到0和1之间的另一个值。在机器学习中，我们使用sigmoid将预测映射到概率。</p><p id="d05d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归假设函数的完整表达式为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lb"><img src="../Images/d3d77c02cda96481ffea2aff6156331b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o_Wg7zr7tHColUbj9wMf6A.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">逻辑回归的假设函数|作者图片</figcaption></figure><h1 id="7968" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">判别边界</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lc"><img src="../Images/a18d7280c579b764d324b39100e0cafe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-1G2aO76qfuuSP5FmqdSaA.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">决策边界|作者图片</figcaption></figure><p id="8723" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们当前的预测函数返回0到1之间的概率分数。为了将其映射到一个离散的类别(真/假，猫/狗)，我们选择一个阈值或临界点，高于该阈值或临界点时，我们将值分类为类别1，低于该阈值或临界点时，我们将值分类为类别2。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ld"><img src="../Images/dd2f2d29bcb7ae8a27483bcf8fc48910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FgZURkKn8pmTHzA3HsGp0A.jpeg"/></div></div></figure><p id="d202" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，如果我们的阈值是0.5，而我们的预测函数返回0.7，我们会将此观察结果归类为阳性。如果我们的预测是0.2，我们会把观察结果归类为负面的。对于具有多个类别的逻辑回归，我们可以选择具有最高预测概率的类别。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es le"><img src="../Images/a863d4ad959d3f42f96b693ad5a78813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*WzC4Z3VsylTWC-vj_PfVEg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图形表示|作者图片</figcaption></figure><h1 id="28a7" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">做预测</h1><p id="5adb" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">利用我们对sigmoid函数和决策边界的了解，我们现在可以编写一个预测函数。逻辑回归中的预测函数返回我们的观察值为正、真或“是”的概率。我们称之为class 1，它的符号是P(class=1)。随着概率越来越接近1，我们的模型越来越确信观察值在类1中。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lf"><img src="../Images/c57c1280c597113686a60c8189d0d429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g_5mtZCC10zdBRBTU9BqTw.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">假设函数第1部分|作者图片</figcaption></figure><p id="3aad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们使用假设函数。其中<strong class="ih hj"> <em class="ky"> X1 </em> </strong>是<strong class="ih hj"> <em class="ky">学过</em> </strong>和<strong class="ih hj"> <em class="ky"> X2 </em> </strong>是<strong class="ih hj"> <em class="ky">睡过。</em>T15】</strong></p><p id="f3a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不过，这一次我们将使用sigmoid函数来转换输出，以返回介于0和1之间的概率值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lg"><img src="../Images/d6f0c60b39b139e6d9bca0e030ecfe0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gBeUmFbUXFJmb3PG3kR3Pw.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">假设函数第2部分|作者图片</figcaption></figure><p id="e5ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果模型返回0.4，它认为只有40%的机会通过。如果我们的决策界限是0.5，我们会将这个观察归类为“失败”</p><h1 id="9783" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">价值函数</h1><p id="b56e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">不幸的是，我们不能(或者至少不应该)使用和线性回归一样的成本函数MSE (L2)。为什么？有一个很好的数学解释，但现在我只想说这是因为我们的预测函数是非线性的(由于sigmoid变换)。像我们在MSE中所做的那样对该预测求平方会导致具有许多局部最小值的非凸函数。如果我们的成本函数有许多局部极小值，梯度下降可能找不到最优的全局极小值。</p><p id="f273" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们不使用均方误差，而是使用一种称为交叉熵的成本函数，也称为对数损失。交叉熵损失可以分为两个独立的代价函数:一个用于<strong class="ih hj"> <em class="ky"> y=1 </em> </strong>，一个用于<strong class="ih hj"> <em class="ky"> y=0 </em> </strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lh"><img src="../Images/99f3030b02f5103eb31e3787750d93be.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*oWJaqh-Fv3RvT_J2WgxQZw.png"/></div></figure><p id="18d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当你看一下<strong class="ih hj"> <em class="ky"> y=1 </em> </strong>和<strong class="ih hj"> <em class="ky"> y=0 </em> </strong>的成本函数图时，取对数的好处就显现出来了。这些平滑单调的函数(总是增加或总是减少)使得计算梯度和最小化成本变得容易。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/171aa6f9a0a74e31330461a980d5f213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*Y_t5KT4tEVWWf64eY1WJjw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图片来自吴恩达关于逻辑回归的幻灯片。</figcaption></figure><p id="b296" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要注意的关键是，成本函数惩罚自信和错误的预测多于奖励自信和正确的预测！必然结果是，由于成本函数的逻辑性质，增加预测精度(接近0或1)对降低成本的回报递减。</p><p id="f906" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">以上功能压缩成一个</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lj"><img src="../Images/a997ab1c04b1fbf0d65bd15b7e8d180a.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*lftvF96lwYCntZ9IkWBMDQ.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">逻辑回归的成本函数</figcaption></figure><p id="52e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将上面等式中的<strong class="ih hj"> <em class="ky"> y </em> </strong>和<strong class="ih hj"><em class="ky">(1y)</em></strong>相乘是一个巧妙的技巧，让我们用同一个等式来求解<strong class="ih hj"> <em class="ky"> y=1 </em> </strong>和<strong class="ih hj"> <em class="ky"> y=0 </em> </strong>两种情况。如果<strong class="ih hj"> <em class="ky"> y=0 </em> </strong>，第一边抵消。如果<strong class="ih hj"> <em class="ky"> y=1 </em> </strong>，则第二边抵消。在这两种情况下，我们只执行我们需要执行的操作。</p><p id="752f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">矢量化成本函数</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lk"><img src="../Images/57e645987dd58fa4d2f54a2a18c291ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*z5HLpH-h3-XuugPTMMXt3A.png"/></div></figure><h1 id="9a87" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">梯度下降</h1><p id="0d52" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">为了最小化我们的成本，我们使用<strong class="ih hj"> <em class="ky">梯度下降</em> </strong>，就像之前线性回归一样。还有其他更复杂的优化算法，如共轭梯度法，如<strong class="ih hj"><em class="ky">【BFGS】</em></strong>，但你不必担心这些。像Scikit-learn这样的机器学习库隐藏了它们的实现，这样你就可以专注于更有趣的事情了！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ll"><img src="../Images/fb2138d76dec355c455fa2f3acd0d9ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QKJ550uGTft7m3527XvrQQ.jpeg"/></div></div></figure><p id="6264" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> <em class="ky"> J(θ) </em> </strong>为<strong class="ih hj"> <em class="ky">代价函数。</em>T15】</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lm"><img src="../Images/ed1fb357250f842b52ec69ebeb35c8bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cr4HpRcxHH_JANAmhgBKNw.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">成本函数的部分推导|作者图片</figcaption></figure><p id="aade" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在把它们放在一起..</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ll"><img src="../Images/dc542714a923199ccdcb7927128e9c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQ5uQzA53OGXHwMr2pnhpQ.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">渐变下降|作者图片</figcaption></figure><p id="6b71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重复此步骤，直到收敛。</p><p id="2aca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是逻辑回归背后的全部数学原理。在我的下一篇博客中，我们将实现这些方程并预测一些离散值。</p></div></div>    
</body>
</html>