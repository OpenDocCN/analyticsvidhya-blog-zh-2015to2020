<html>
<head>
<title>Recurrent Neural Networks — Complete and In-depth</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络——完整而深入</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-is-rnn-a157d903a88?source=collection_archive---------6-----------------------#2020-12-02">https://medium.com/analytics-vidhya/what-is-rnn-a157d903a88?source=collection_archive---------6-----------------------#2020-12-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="c9c4" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">什么是 RNN？</h1><p id="2378" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">递归神经网络是一种深度学习神经网络，它记住输入序列，将其存储在记忆状态/细胞状态中，并预测未来的单词/句子。</p><h1 id="891d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"> <em class="kb">为什么是 RNN？</em>T3】</strong></h1><p id="3102" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">RNNs 可以很好地处理序列形式的输入。举个例子，考虑一下，<strong class="jf hj"><em class="kc">我喜欢吃冰淇淋。我最喜欢的是巧克力 _ _ _ _</em></strong>。</p><p id="d80f" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">对于人类来说，用<strong class="jf hj"> <em class="kc">冰淇淋</em> </strong> <em class="kc">，</em>这个词来填空是显而易见的，但是机器要理解上下文，记住句子中前面的词，才能预测后面的词。这就是 rnn 有用的地方。</p><p id="d12b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj"> <em class="kc">应用</em> : </strong> —语音识别(谷歌语音搜索)、机器翻译(谷歌翻译)、时间序列预测、销售预测等。</p><h1 id="62f7" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"><em class="kb">RNN 的建筑和工作</em> </strong></h1><p id="802d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们考虑 x11、x12、x13 作为输入，O1、O2、O3 分别作为隐藏层 1、2 和 3 的输出。输入在不同的时间间隔被发送到网络，所以假设 x11 在时间 t1、x12 @ t2 和 x13 @ t3 被发送到隐藏层 1。</p><p id="649c" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">此外，让我们假设权重在前向传播中是相同的。</p><p id="20c4" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">输出 O3 取决于 O2，O2 又取决于 O1，如下所示。</p><p id="95e9" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">O1 = f(x11*w) →其中 w 为重量，f 为<a class="ae ki" href="https://tejasta.medium.com/activation-functions-in-neural-networks-69197497bd1d" rel="noopener">激活函数</a>。</p><p id="b4c4" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">O2 = f(O1+x12*w)</p><p id="75e0" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">O3 = f (O2 + x13*w)</p><p id="d69f" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">最后，O3 的输出是 ŷ指示的实际输出</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kj"><img src="../Images/c92033d01835607109f234efa42d4f53.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*vSueL1DtV4F20j1aI40wUw.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">简单 RNN 的建筑和工作</figcaption></figure><p id="7fae" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">现在，损失函数将被计算为(y-ŷ)^2.)目标是将<a class="ae ki" href="https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23" rel="noopener" target="_blank"> <strong class="jf hj">损失函数</strong> </a>减少到我们得到 y = ŷ的点，以便达到全局最小值，该全局最小值建立了必须添加到网络中的适当权重。这是通过使用<strong class="jf hj">优化器</strong>调整权重在反向传播中实现的。</p><p id="a945" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj"> <em class="kc">链式微分法则</em> </strong> <em class="kc"> </em>在反向传播中的应用举例:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kv"><img src="../Images/bdd586c8e9f5c400b2da57a2f190c151.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*x4fIuSUMmC6O5t6oINbRiw.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">微分链式法则</figcaption></figure><h1 id="1bc8" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"> <em class="kb">双向 RNN </em> </strong></h1><p id="9e20" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">例子:<strong class="jf hj"><em class="kc"><em class="kc">我肚子 ____ 饿了，今天午饭我一口气能吃 3 个大披萨</em></em></strong>。所以，忘记机器吧，人类不读完整个句子就无法预测合适的单词。在这种情况下，我们利用双向递归神经网络，它不仅提供来自过去的信息，还保存来自未来的信息。</p><p id="82c0" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">双向 RNN 的概念是耦合<em class="kc">两个具有相同输入的隐藏层并产生输出</em>。发明在于，我们为感兴趣的特定隐藏层获得的输出将具有来自过去和未来的信息。参见下面的架构，</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/04fa71459ced12adecad8f64b69c10b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*FslFSmcloogNhe4sKhdC9w.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">双向 RNN 架构</figcaption></figure><p id="172c" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">为了清楚起见，为了预测 ŷ13 的产量，我们有 O1，O2(从正向)，还有 O|3(从反向)。双 RNN 的缺点是速度慢。</p><h1 id="9bce" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">RNN 的弊端</strong>—</h1><p id="fa92" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">1.<strong class="jf hj">消失渐变问题</strong>——这发生在我们使用某些激活函数的时候。因此，在反向传播期间，从一层到另一层的权重更新将非常小，并且在某一点上，必须添加的新权重将变得等于旧权重，因此没有变化，并且训练网络是困难的。</p><p id="7520" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">2.<strong class="jf hj">爆炸梯度问题</strong> —在这种情况下，权重更新是如此之大，以至于网络无法从训练数据中学习，因此永远无法达到全局最小值。</p><p id="354b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">因此，<strong class="jf hj"> LSTM(长短期记忆)</strong>和<strong class="jf hj"> GRU(门控循环单位)</strong>效果更好。</p><h1 id="7b3c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><em class="kb">LSTM——长短期记忆</em></h1><p id="9a6d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">LSTMs 解决了消失梯度问题。</p><p id="43c8" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">LSTMs 具有两种状态，即隐藏状态和小区状态，如同只具有隐藏状态的 rnn 一样。</p><p id="9db1" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">当上下文变化时，LSTMs 会忘记一些不重要的信息，因此即使对于长句也非常有效，但 RNN 不是这样的。</p><h1 id="516f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><em class="kb">LSTM 的建筑和工作</em></h1><p id="e839" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">LSTM 的主要组成部分是-</p><p id="1c06" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">1.存储单元</p><p id="021a" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">2.输入门</p><p id="03bc" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">3.忘记大门</p><p id="5bc3" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">4.输出门</p><p id="dabf" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">下面是 LSTM 的结构。让我们来了解一下操作</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kx"><img src="../Images/a33e2a9b8c7fe66356c4d347eed6a5e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*IRQRZ3maagx1SXPQyGntpw.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">LSTM 建筑</figcaption></figure><ol class=""><li id="27a6" class="ky kz hi jf b jg kd jk ke jo la js lb jw lc ka ld le lf lg bi translated"><strong class="jf hj">忘记大门</strong></li></ol><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/7801f996f737591dc46637541b17bf98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*moo3XMWDqraYuiYREliscQ.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">忘记大门</figcaption></figure><p id="1bbc" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">这里，输入 ht-1 和 xt 被传递到 sigmoid 激活函数，该函数输出 0 和 1 之间的值。0 表示<strong class="jf hj">完全忘记</strong>，1 表示<strong class="jf hj">完全保留信息</strong>。我们使用 sigmoid 函数作为门。</p><p id="96fc" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">注:bf 是偏差，Wf 是两个输入的综合权重。</p><p id="28d8" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">2.<strong class="jf hj">输入门</strong></p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/1a11718ad33568829ac7a14b1060ddc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*VHSZhzZjGFnzS3HIH-NZDA.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">输入门</figcaption></figure><p id="755b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">这一阶段的动机是识别新信息并添加到单元状态中。这分两步完成。</p><p id="d98e" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">步骤 1 </strong>:根据输入 ht-1 和 xt，s 形层输出一个介于 0 和 1 之间的值。如上图所示。同时，这些输入将被传递到 tanh 层，该层输出-1 和 1 之间的值，并为输入创建向量。</p><p id="876e" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">步骤 2: </strong>将 s 形层和 tanh 层的输出相乘</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/3f29fc46be9c423f212c9528fc547f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*b0Pb8m6afwKXkNWUEiKRpw.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">更新单元状态</figcaption></figure><p id="1ed4" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">现在，单元状态从 Ct-1(先前的 LSTM 单元输出)更新为 Ct(当前的 LSTM 单元输出)，如上所述。</p><p id="be10" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">3 <strong class="jf hj">。输出门</strong></p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/ff24b3aae793970a16395efd87c07429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*jfMhmd04N7OXKP_YMf1c0Q.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">输出门</figcaption></figure><p id="1f75" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">首先，单元状态通过 tanh 函数传递，同时我们将输入 ht-1 和 xt 发送到 sigmoid 函数层。然后发生乘法，并且<strong class="jf hj"> ht </strong>是该存储单元的输出，并且被传递到下一个单元。</p><h1 id="3f26" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><em class="kb">门控循环单元</em></h1><p id="950b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">为了更快的计算和更少的内存消耗，使用了 gru。当精度是关键时，LSTMs 表现更好。gru 没有细胞状态，只有隐藏状态。</p><h1 id="3ebc" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"><em class="kb">GRU 的建筑和做工</em> </strong></h1><p id="65ae" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">GRU 的主要组成部分是-</p><p id="c9f9" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">1.更新门(z)</p><p id="fb42" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">2.复位门(rt)</p><p id="cb6a" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">下图代表 GRU</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/cf7acf7d5f9a05feacad88079629495f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*OY5JxRGFJ3CvLpjdR4G6FA.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">门控循环单元框图</figcaption></figure><ol class=""><li id="4236" class="ky kz hi jf b jg kd jk ke jo la js lb jw lc ka ld le lf lg bi translated"><strong class="jf hj">更新门</strong> —必须向前传递的信息量</li></ol><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lh"><img src="../Images/bc878edd7db84b6c39a457dc9d3a0342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*RZPETBvIZDTDpYHQ-r82cw.png"/></div></figure><p id="954f" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">其中，W(z)是与 xt 相关的权重，U(z)是与来自前一状态(ht-1)的输入相关的权重，σ是 sigmoid 激活函数。</p><p id="ec08" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">输出 zt 将在 0 和 1 之间，基于此信息将被传递。</p><p id="78e7" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj"> 2。重置门</strong> —确定要忘记的信息量</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es li"><img src="../Images/f43cf5c938f680f01661cc6188cc02b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*KRkokx2hxXBbtQibUIgPwA.png"/></div></figure><p id="5055" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">其中，W(r)是与 xt 相关的权重，U(r)是与来自前一状态(ht-1)的输入相关的权重，σ是 sigmoid 激活函数。</p><p id="05bf" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">基于将被遗忘的信息，输出 rt 将在 0 和 1 之间。</p><p id="88ed" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">现在，重要的一步是在网络中添加一个叫做复位门的存储元件。该重置门提取重要信息或关键点，并赋值= 1，其余所有句子将赋值= 0</p><p id="0203" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">数学上我们计算如下:</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lj"><img src="../Images/e1ff80e51d697d621d3e46502b005fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*kQuC2MAqPATFF29nP8h1tg.png"/></div></figure><p id="e57f" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">最后，我们使用下面的公式</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lk"><img src="../Images/c620bdae947f759a5c4ca44b76db7463.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*bTEnCTzJkgFVhEQpczgj1w.png"/></div></figure><p id="5dea" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">使用这个公式，我们计算将被传递到后续单元的 ht 的当前状态。</p><h1 id="676c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><em class="kb">序列到序列学习</em></h1><p id="199f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">序列到序列学习背后的思想是，以一种语言接收的输入数据被转换成另一种语言。例如:英语→索马里语。</p><p id="e4f2" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">序列到序列学习的类型</p><p id="469b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">1.序列到序列—输出等于输入的数量。</p><p id="bf97" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">2.序列到向量——对于“n”个输入，给出一个输出</p><p id="dc86" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">3.向量到序列—1 个输入接收“n”个输出</p><p id="06df" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">4.向量到向量-针对单个输入接收单个输出</p><p id="9413" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">下图总结了上述 4 种学习方法的架构</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/a9013869333586e7ede8122bf44302a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*TrsIt5MjmQdO70yhqyrDfQ.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">序列-序列学习的 4 种方法</figcaption></figure><h1 id="6253" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><em class="kb">编码器—解码器/ Sutskever 神经机器翻译模型</em></h1><p id="112d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">输入序列和输出序列的长度并不总是相同的。例子—</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/7f976b11cf4fe58816c6e989176e856d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*B3-cj9OcNevlF5WUm-UlmQ.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">编解码器的应用实例</figcaption></figure><p id="b470" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">在上面的翻译中，我们看到，在英语中我们有 3 个字符，但在索马里，它是 2 个字符。在这种情况下，使用编码器和解码器。</p><h1 id="9989" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><em class="kb">编码器-解码器的架构和工作方式</em></h1><p id="a8ff" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">编码器是由 LSTM 或 GRU 单元组成的输入网络，解码器是由 LSTM 或 GRU 单元组成的输出网络。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/b7e82dad9a37c1895bad939067e277b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*hFUEambkq38pTOXDQIjvHQ.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">编码器-解码器架构</figcaption></figure><p id="9d4a" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">编码器</strong> —我们将 A、B、C 字输入到编码器网络，我们得到一个概括了输入信息的上下文向量‘w’。</p><p id="1204" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">注意:当网络点击<eos>时，它停止进程。</eos></p><p id="5e36" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">解码器</strong> —上下文向量“w”被发送到解码器网络，如上图所示。对于解码器网络的每个输入，我们得到输出(X，Y，Z)。</p><p id="7580" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">解码器网络的最终输出与输入序列进行比较，并计算损失函数。在反向传播中使用优化器将该损失函数减少到点<em class="kc">实际结果=预测结果</em>。</p><p id="512d" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">编码器和解码器的缺点— </strong>上下文向量概括了整个输入序列，但不是输入序列中的所有单词都有价值包括在摘要中。这可以通过使用基于注意力的模型来克服。</p><h1 id="5232" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><em class="kb">注意力模式</em></h1><p id="c363" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">概念——想象你正在听一场演讲，在演讲结束时，你不会记住演讲者所说的每一句话，但你会记住演讲的要点或摘要。这就是注意力模型的概念。</p><h1 id="21ac" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><em class="kb">注意力模型的架构和工作方式</em></h1><p id="b1d3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们在编码器和解码器之间有一个神经网络。神经网络的输出将是解码器的输入。在这一点上，我们还必须理解，神经网络的输出将是在它接收的输入中具有最大注意力或焦点的输出，或者是对预测重要的词。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es kw"><img src="../Images/5f08214c895f95764dfc8fb692a85b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*xhGSt6wFb0eSkSxuhQ3PYQ.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">基于注意力模型的体系结构</figcaption></figure><p id="dfe6" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">要学习高级概念，请参考下面链接的精彩文章-</p><p id="3962" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">变形金刚</strong>——<a class="ae ki" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-transformer/</a>v</p><p id="2556" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">伯特</strong>——<a class="ae ki" href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" rel="noopener ugc nofollow" target="_blank">http://jalammar . github . io/a-visual-guide-to-use-BERT-first-time/</a></p><p id="e306" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">GTP 3</strong>—<a class="ae ki" href="http://jalammar.github.io/how-gpt3-works-visualizations-animations/" rel="noopener ugc nofollow" target="_blank">http://jalammar . github . io/how-gp T3-works-visualizations-animations/</a></p><p id="80ff" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">致谢</strong></p><ol class=""><li id="f4a3" class="ky kz hi jf b jg kd jk ke jo la js lb jw lc ka ld le lf lg bi translated">https://www.youtube.com/user/krishnaik06/featured 的克里斯·纳伊克<a class="ae ki" href="https://www.youtube.com/user/krishnaik06/featured" rel="noopener ugc nofollow" target="_blank"/></li><li id="8295" class="ky kz hi jf b jg ll jk lm jo ln js lo jw lp ka ld le lf lg bi translated"><a class="ae ki" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li><li id="6a96" class="ky kz hi jf b jg ll jk lm jo ln js lo jw lp ka ld le lf lg bi translated">施拉姆·瓦苏德万—<a class="ae ki" href="https://www.youtube.com/channel/UCma2b1uVLajAq9nHSEJh9HQ" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/channel/UCma2b1uVLajAq9nHSEJh9HQ</a></li><li id="a479" class="ky kz hi jf b jg ll jk lm jo ln js lo jw lp ka ld le lf lg bi translated">序列对序列学习-<a class="ae ki" href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/2014/file/a 14 AC 55 a4 f 27472 C5 d 894 EC 1c 3c 743d 2-paper . pdf</a></li></ol><p id="b90c" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">通过以下方式联系我—</p><p id="0d41" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">电子邮件—tejasta@gmail.com</p><p id="3268" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">领英—【https://www.linkedin.com/in/tejasta/ T4】</p><p id="6e16" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">感谢阅读！</p></div></div>    
</body>
</html>