<html>
<head>
<title>A Data Engineer with Data Science Tools</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">拥有数据科学工具的数据工程师</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-data-engineer-with-data-science-tools-59aab02988c9?source=collection_archive---------2-----------------------#2018-08-24">https://medium.com/analytics-vidhya/a-data-engineer-with-data-science-tools-59aab02988c9?source=collection_archive---------2-----------------------#2018-08-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6ce9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在过去的4年里，作为一名大数据开发人员，我喜欢我的工作，并为客户和市场解决了一些很酷但很重要的问题。我从Apache Hadoop v1.0开始，转到Hadoop v2.7。然后强大的阿帕奇火花显示了他的存在和数据处理能力。所以有了Apache Hadoop2.7，我开始在Apache Spark 2.3中开发。是的，我是数据工程师！！</p><p id="4a9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">工作的感觉棒极了。我可以而且仍然可以连续工作几个小时而不休息。从各种来源获取数据，例如RDBMS、SAP、社交网络(Faceboook、Google、Twitter)、第三方API(bold chat、CXSocial)，然后对其进行清理，然后使用Spark engine进行处理，这为优化我以及客户的时间和资源提供了机会。然而，这些数据的最终归宿不仅仅是HDFS( Hadoop分布式文件系统)或任何数据库(SQl，NoSQL)，而是面向业务经理和决策者。因此，数据在数据科学团队中开始了新的旅程。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/a9079d2e7395d56919fc7a360f810516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zZRJQRILT0MeEQiH7sz1YA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">带有Spark引擎的Hadoop</figcaption></figure><p id="f5ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据数据科学团队的定义，他们是使用统计学、数学、概率和计算机算法来创建所谓的“机器学习”算法的人。现在，另一种称为“深度学习”的技术正在各行各业使用，它模仿人类神经元细胞，解决复杂的问题。经过处理和清理的数据到达团队，然后他们将这些数据输入到Python，R程序，实现他们设计的ML/DL算法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jt"><img src="../Images/706262607578d71ea75269f864ecbf02.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*jxNsqHLYD5aCbFsVAikbYA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">数据科学成分</figcaption></figure><p id="59e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，大多数开发人员和数据科学家都非常了解上述过程。现在，我在这里试图关注的是这两个团队的整合。我发现很多时候数据工程师看不到数据科学家的世界，反之亦然。并非总是如此，但有时这会造成交付和沟通的滞后。此外，直到数据工程师团队完成他们的行动，数据科学团队仍然坐着。当然，与此同时，他们可以完善他们早期的模型，利用来自数据工程师团队的样本数据，他们可以开始构建模型，但他们仍然需要实际数据来训练和测试他们的模型。那么，有什么办法呢？</p><p id="744a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单的答案是同时使用两个场。我知道，他们都是(数据工程师- Hadoop/Spark，数据科学)两个巨大的知识海洋，但我已经尝试过了，可以说这将有助于团队快速和全面地使用庞大的数据集。这并不是说数据工程师团队必须知道所有算法及其内部机制，但是如果他们知道哪些算法适用于哪种数据分析，例如预测、分类、聚类，这将节省大量项目时间。数据科学家也是如此；如果他们了解Spark编程和Hadoop基础知识(存储、接收、流)，他们就不必等待数据工程师团队的完成。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ju"><img src="../Images/bf2a66a216027a7d32d101e534d3307d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HWjxMbW1VaLWHWLrNtrunQ.png"/></div></div></figure><p id="329a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在的问题是我们如何实现这一目标。Apache Hadoop和Spark已经建立了对机器学习的支持。Spark有一个名为“Spark MLlib”的内置库，它可以帮助我们用Scala编写程序并创建ML模型。使用Spark的优势在于它非常快，快到Apache Spark项目的标语上写着“<em class="jv">快如闪电的统一分析引擎”。</em>它的内存处理速度比传统处理引擎快100倍，数据科学家还需要什么来训练他/她的Ml/DL模型，该模型对万亿字节的数据进行疯狂迭代。对于熟悉Python(最流行的ML/DL语言)或R的人来说，还有一个选择，他们可以使用PySpark(熟悉Python的人)或SparkR(会用R编程的人)。有了这些版本的Spark，他们可以用自己熟悉的语言编写，只需使用核心Spark抽象，他们就可以运行程序，在引擎上训练模型，并很快得到结果。</p><p id="e7c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于批量数据，使用Spark可能是一种选择，但对于流数据或实时分析数据，Spark或此类快速处理引擎是必须的。甚至可以使用Kafka messaging、Spark Streaming和Pyspark to tarin模型获得类似Twitter的流数据。另一个最好的部分是存储。该模型的输出甚至测试数据都可以存储在HDFS中，具有很高的容量、分布性和容错性。甚至Hadoop和Spark也支持像AWS S3、谷歌云和Azure这样的云存储。</p><p id="7659" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们看看Hadoop、Spark和Python的组件，它们可用于提取、转换、加载和分析数据。之后，我们将看到一个用例，以了解如何做到这一点。</p><h2 id="43f1" class="jw jx hi bd jy jz ka kb kc kd ke kf kg iq kh ki kj iu kk kl km iy kn ko kp kq bi translated"><strong class="ak"> Apache Hadoop工具:</strong></h2><p id="e57e" class="pw-post-body-paragraph if ig hi ih b ii kr ik il im ks io ip iq kt is it iu ku iw ix iy kv ja jb jc hb bi translated"><strong class="ih hj">用于数据摄取</strong> : Sqoop(用于Oracle、My SQL、MSSQL等RDBMS系统)</p><p id="6e37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">:Flume(用于Twitter、系统日志等非结构化数据)</p><p id="a865" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">:Kafka(用于流数据)</p><p id="e590" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用于存储</strong> : HDFS</p><p id="04c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">:Aws S3</p><p id="5ab3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">:谷歌云存储</p><p id="74dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">:Azure存储</p><p id="62f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用于处理引擎</strong>:地图缩小</p><p id="0bfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">:火花(PySpark)</p><p id="330b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">对于集群管理器</strong>:纱线</p><p id="5f66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">:Mesos</p><p id="c3cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">:独立(如果我们想在我们的台式机/笔记本电脑上运行，那么它将使用不同的并行内核)</p><p id="d373" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用于调度</strong> : Oozie</p><p id="d133" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">:Control M(不是开源的，但是有一些额外的特性)</p><p id="ef9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在这里有一个用例，我们从Twitter中提取数据，进行情感分析，并根据响应预测certein事件的发生。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kw"><img src="../Images/912aee4c2125f4ec8d7ea0fdae8fcfb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Me-vzIx7vVfNXcOPLhNkiw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">使用PySpark的Twitter数据分析</figcaption></figure><p id="af7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们使用Twitter开发者API从Twitter获取数据。为此，我们将使用Kafka消息服务，并为消费者和经纪人编码。Kafka的输出将被输入到Spark streaming。Spark streaming让我们可以选择输入流的持续时间来选择和创建窗口，以便将连续流转换为小批量数据。这些来自流的小批量将被馈送到PySprk引擎。Pyspark将以非常高的速度处理数据，并将输出存储在Hive Table/RDBMS/HDFS或云存储(S3)中。</p><p id="e5a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">包含ML/DL建模程序的Python代码将在PySpark引擎中运行，并将提取和加载存储组件。当最终输出可用时，可视化工具将创建所需的图表/图形来显示数据的洞察力。</p><p id="3619" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总而言之，机器学习和深度学习的蓬勃发展完全是因为过去5年中计算能力的提高。所以，如果我们不使用这些技术，那么我们就不能充分发挥ML或DL的潜力。</p></div></div>    
</body>
</html>