<html>
<head>
<title>Quantile Regression and Prediction Intervals</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分位数回归和预测区间</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/quantile-regression-and-prediction-intervals-e4a6a33634b4?source=collection_archive---------3-----------------------#2019-11-03">https://medium.com/analytics-vidhya/quantile-regression-and-prediction-intervals-e4a6a33634b4?source=collection_archive---------3-----------------------#2019-11-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="1c37" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">预测区间</strong></h1><p id="87e7" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">通常对于回归，我们最小化RMSE误差来得到一些相互依赖的变量的点估计。然而，我们很难衡量价值的确定性。让我们看一个例子。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/d2493cae0149ccd14595820c94d54af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*KwdiMSym6PGPmfD7o3dIdg.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">图1:多种可能的预测(<a class="ae kn" href="https://otexts.com/fpp2/perspective.html#perspective" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="b7b6" class="pw-post-body-paragraph jd je hi jf b jg ko ji jj jk kp jm jn jo kq jq jr js kr ju jv jw ks jy jz ka hb bi translated">正如我们从上图中看到的，模型可以通过不同的方式学习参数，从而给出不同的未来值，也就是说，预测中会有一些变化。在大多数预测方案中，变化将随着预测期的延长而增加。这意味着我们预测的未来越远，我们就越不确定。</p><p id="740d" class="pw-post-body-paragraph jd je hi jf b jg ko ji jj jk kp jm jn jo kq jq jr js kr ju jv jw ks jy jz ka hb bi translated">为了量化预测的不确定性，我们使用<strong class="jf hj">预测区间</strong>。因此，不仅仅是一个单一的值，我们现在也有一个如图2所示的区间，我们可以在一定程度上确信原始值位于其中。预测间隔通常是我们拥有多少数据、这些数据中有多少变化、我们预测多远以及使用哪种预测方法的函数。从下图我们可以看出，如果我们想要更自信，那么我们就会有更大的区间。这是在价值和范围之间的权衡。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/0cf7e5921da7576541d397a2026a3d76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*TaZu8Ga5a-PxDZ9r9-1R0Q.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">图二。10年预测的80%(深色)和95%(浅色)预测区间</figcaption></figure><p id="2e2d" class="pw-post-body-paragraph jd je hi jf b jg ko ji jj jk kp jm jn jo kq jq jr js kr ju jv jw ks jy jz ka hb bi translated">对于估计了标准差σₕ的<em class="kt"> h </em>步预测和输出变量<em class="kt"> y </em>，预测区间可以计算为:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es ku"><img src="../Images/5504dfe48157b78920344bd5f658d997.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*_Q43RZLM2DdIQwZU52ooCg.png"/></div></figure><p id="77d4" class="pw-post-body-paragraph jd je hi jf b jg ko ji jj jk kp jm jn jo kq jq jr js kr ju jv jw ks jy jz ka hb bi translated">常数<em class="kt"> c </em>取决于覆盖概率。这些值在列表<a class="ae kn" href="https://otexts.com/fpp2/prediction-intervals.html" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="ea3b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">分位数回归</strong></h1><p id="ae29" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">生成预测区间的另一种方法是通过分位数回归。与线性回归相反，线性回归在给定预测变量的某些值的情况下估计响应变量的条件均值，分位数回归旨在估计响应变量的条件分位数(通常是中值)。</p><p id="2933" class="pw-post-body-paragraph jd je hi jf b jg ko ji jj jk kp jm jn jo kq jq jr js kr ju jv jw ks jy jz ka hb bi translated">最小二乘回归基于残差(y-y’)在独立变量值之间具有恒定方差的假设。我们不能相信违背这一假设的线性回归模型及其预测区间。由于基于分位数损失的回归提供了合理的预测区间，即使残差具有非常数方差或非正态分布，分位数回归也能派上用场。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kv"><img src="../Images/66ffbf02e35b3ace83bb451c69fa229e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6ebeUABFr6w8M8Xl.png"/></div></div><figcaption class="kj kk et er es kl km bd b be z dx translated">左:线性关系b/w X1和y。残差方差恒定。右图:X2和Y呈线性关系，但Y的方差随着X2的增加而增加。(异方差)[3]</figcaption></figure><p id="7b94" class="pw-post-body-paragraph jd je hi jf b jg ko ji jj jk kp jm jn jo kq jq jr js kr ju jv jw ks jy jz ka hb bi translated">正如回归最小化平方误差损失函数来预测单点估计值一样，分位数回归在预测某个分位数时最小化<em class="kt">分位数损失</em>。分位数损失定义为:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es la"><img src="../Images/77a64b48bc431e364c21d33564cdc885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uwFTFjEBFNF_30Vi.png"/></div></div></figure><p id="43a5" class="pw-post-body-paragraph jd je hi jf b jg ko ji jj jk kp jm jn jo kq jq jr js kr ju jv jw ks jy jz ka hb bi translated">其中，γ是所需的分位数，取值范围为(0，1)。</p><p id="ff07" class="pw-post-body-paragraph jd je hi jf b jg ko ji jj jk kp jm jn jo kq jq jr js kr ju jv jw ks jy jz ka hb bi translated">现在，如果我们为中位数(即第50个分位数)训练模型，分位数损失就是<em class="kt">绝对</em>误差的总和。为了创建预测区间，我们现在可以使用其他分位数值。例如，在下图中，我们有0.977和0.023个百分点。这给出了在其边界内具有0.95概率的真值的预测区间。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lb"><img src="../Images/7d0c3b6a0d1923f0122cf36b6b47a19f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*a-z7XbsVI5Vfhu4k9yW4iA.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">分位数0.023、0.5和0.977的预测值和实际值(测试实例)。[5]</figcaption></figure><h1 id="7d12" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">结论</strong></h1><p id="1abc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">回归模型的输出值受到不确定性的影响，这种不确定性可以通过预测区间来建模。此外，线性回归模型不能用于残差具有非恒定方差或非正态分布的数据集。分位数回归解决了这个问题。</p><p id="b80f" class="pw-post-body-paragraph jd je hi jf b jg ko ji jj jk kp jm jn jo kq jq jr js kr ju jv jw ks jy jz ka hb bi translated"><strong class="jf hj">参考文献:</strong></p><ol class=""><li id="0926" class="lc ld hi jf b jg ko jk kp jo le js lf jw lg ka lh li lj lk bi translated"><a class="ae kn" href="https://otexts.com/fpp2/perspective.html#perspective" rel="noopener ugc nofollow" target="_blank">预测:原理与实践</a></li><li id="79a9" class="lc ld hi jf b jg ll jk lm jo ln js lo jw lp ka lh li lj lk bi translated">优步天气预报简介</li><li id="cc9d" class="lc ld hi jf b jg ll jk lm jo ln js lo jw lp ka lh li lj lk bi translated"><a class="ae kn" href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" rel="noopener ugc nofollow" target="_blank">https://heart beat . fritz . ai/5-regression-loss-functions-all-machine-learners-should-know-4fb 140 e 9 D4 b 0</a></li><li id="3807" class="lc ld hi jf b jg ll jk lm jo ln js lo jw lp ka lh li lj lk bi translated"><a class="ae kn" href="https://www.wikiwand.com/en/Quantile_regression#/Conditional_quantile_and_quantile_regression" rel="noopener ugc nofollow" target="_blank">https://www . wiki wand . com/en/Quantile _ regression #/Conditional _ Quantile _ and _ Quantile _ regression</a></li><li id="ac40" class="lc ld hi jf b jg ll jk lm jo ln js lo jw lp ka lh li lj lk bi translated"><a class="ae kn" href="https://www.evergreeninnovations.co/blog-quantile-loss-function-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://www . evergreenwinnovations . co/blog-quantile-loss-function-for-machine-learning/</a></li></ol></div></div>    
</body>
</html>