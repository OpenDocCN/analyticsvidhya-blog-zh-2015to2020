<html>
<head>
<title>StreamSets Transformer: Natural Language Processing in PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">StreamSets Transformer:PySpark中的自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/streamsets-transformer-natural-language-processing-in-pyspark-9ad955f5d7c0?source=collection_archive---------15-----------------------#2019-12-12">https://medium.com/analytics-vidhya/streamsets-transformer-natural-language-processing-in-pyspark-9ad955f5d7c0?source=collection_archive---------15-----------------------#2019-12-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8b69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我之前的两篇博客中，我展示了使用Scala扩展StreamSets Transformer是多么容易:1)训练Spark ML RandomForestRegressor模型，2)序列化训练好的模型并保存到亚马逊S3 。</p><p id="aef7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客中，您将学习如何在StreamSets Transformer中使用PySpark为自然语言处理(NLP)训练Spark ML逻辑回归模型。该模型将被训练成将给定的推文分类为积极的<em class="je">情绪</em>或消极的<em class="je">情绪</em>。</p><p id="0f30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">先决条件</p><ul class=""><li id="1728" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated"><a class="ae jd" href="https://streamsets.com/products/transformer/" rel="noopener ugc nofollow" target="_blank">流集转换器</a>版本3.12.0+</li><li id="9ec8" class="jf jg hi ih b ii jo im jp iq jq iu jr iy js jc jk jl jm jn bi translated"><a class="ae jd" href="https://streamsets.com/documentation/transformer/latest/help/transformer/Installation/StagePrerequisites.html#task_gn4_b1x_dhb" rel="noopener ugc nofollow" target="_blank"> PySpark处理器先决条件</a></li><li id="cdbc" class="jf jg hi ih b ii jo im jp iq jq iu jr iy js jc jk jl jm jn bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/NumPy" rel="noopener ugc nofollow" target="_blank"> NumPy </a>库安装在同一台机器上</li></ul><p id="f66a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们深入细节之前，这里是管道概述。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/bdbe4fdf719775915f3a8d206b78dca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oDtezDOsQ8Z0rPeU.png"/></div></div></figure><p id="afc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输入</strong></p><p id="6ffb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">两个<a class="ae jd" href="https://streamsets.com/documentation/transformer/latest/help/transformer/Origins/File.html#concept_jcx_f2d_qgb" rel="noopener ugc nofollow" target="_blank">文件</a>原点被配置为加载包含将用于训练模型的<em class="je">正面</em>和<em class="je">负面</em>推文的数据集。</p><p id="f7d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">转换</strong></p><ul class=""><li id="d9d7" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated"><a class="ae jd" href="https://streamsets.com/documentation/transformer/latest/help/transformer/Processors/FieldRemover.html#concept_svw_dxf_fhb" rel="noopener ugc nofollow" target="_blank">字段移除器</a>处理器被配置为仅保留tweet Id和tweet文本字段，因为在本例中，tweet的其他字段/值不用于训练模型。</li><li id="5ffc" class="jf jg hi ih b ii jo im jp iq jq iu jr iy js jc jk jl jm jn bi translated"><a class="ae jd" href="https://streamsets.com/documentation/transformer/latest/help/transformer/Processors/SparkSQLExp.html#concept_akj_gsz_mhb" rel="noopener ugc nofollow" target="_blank"> Spark SQL表达式</a>处理器使我们能够将值为<strong class="ih hj"> 1 </strong>和<strong class="ih hj"> 0 </strong>的true(情感)列添加到两个数据帧中。该列将用于训练模型。</li><li id="f530" class="jf jg hi ih b ii jo im jp iq jq iu jr iy js jc jk jl jm jn bi translated">联合处理器被配置为将两个数据帧组合成一个数据帧，用于训练模型。</li><li id="9046" class="jf jg hi ih b ii jo im jp iq jq iu jr iy js jc jk jl jm jn bi translated">PySpark处理器是我们用来训练和评估模型的代码所在。(详见下文。)</li></ul><p id="a585" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> PySpark处理器</strong></p><p id="4a1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是PySpark代码插入到<strong class="ih hj"> PySpark </strong> <strong class="ih hj">处理器</strong>&gt;&gt;<strong class="ih hj">PySpark</strong><strong class="ih hj">标签</strong> &gt; &gt; <strong class="ih hj"> PySpark代码</strong>部分。它基本上采用输入数据(帧)并基于它训练Spark ML逻辑回归模型-代码细节包括训练-测试分裂，标记文本，删除停用词，设置超参数调整网格，跨折叠交叉验证，在“训练”分裂数据集上训练，并在“测试”分裂数据集上评估训练的模型。(请参阅行内注释进行演练。)</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/74dac3bc7cfc517c303e378a58515913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0bs0Gg_bWEee6tRp.png"/></div></div></figure><pre class="ju jv jw jx fd kf kg kh ki aw kj bi"><span id="4418" class="kk kl hi kg b fi km kn l ko kp"># Import required libraries<br/>from pyspark.ml.feature import VectorAssembler, StopWordsRemover, Tokenizer, CountVectorizer, IDF<br/>from pyspark.ml.linalg import Vectors<br/>from pyspark.ml import Pipeline, PipelineModel<br/>from pyspark.sql.functions import *<br/>from pyspark.ml.tuning import CrossValidator, ParamGridBuilder<br/>from pyspark.ml.evaluation import MulticlassClassificationEvaluator<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.sql.types import FloatType<br/><br/># Setup variables for convenience and readability <br/>trainSplit = 0.8<br/>testSplit = 0.2<br/>maxIter = 10<br/>regParam = 0.3<br/>elasticNetParam = 0.8<br/>numberOfCVFolds = 3<br/><br/># The input dataframe is accessible via inputs[0]<br/>df = inputs[0]<br/><br/># Split dataset into "train" and "test" sets<br/>(train, test) = df.randomSplit([trainSplit, testSplit], 42) <br/><br/>tokenizer = Tokenizer(inputCol="text",outputCol="tokenized")<br/>stopWordsRemover = StopWordsRemover(inputCol=tokenizer.getOutputCol(),outputCol="stopWordsRemoved")<br/>countVectorizer = CountVectorizer(inputCol=stopWordsRemover.getOutputCol(),outputCol="countVectorized")<br/>idf = IDF(inputCol=countVectorizer.getOutputCol(),outputCol="inverted")<br/><br/># MUST for Spark features<br/>assembler = VectorAssembler(inputCols=[idf.getOutputCol()], outputCol="features")<br/><br/># LogisticRegression Model<br/>lr = LogisticRegression(maxIter=maxIter, regParam=regParam, elasticNetParam=elasticNetParam)<br/><br/># Setup pipeline -- pay attention to the order -- it matters!<br/>pipeline = Pipeline(stages=[tokenizer, stopWordsRemover, countVectorizer, idf, assembler, lr])<br/><br/># Setup evaluator -- default is F1 score<br/>classEvaluator = MulticlassClassificationEvaluator(metricName="accuracy")<br/><br/># Setup hyperparams grid<br/>paramGrid = ParamGridBuilder().addGrid(lr.elasticNetParam,[0.0]).addGrid(countVectorizer.vocabSize,[5000]).build()<br/><br/># Setup cross validator<br/>cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=classEvaluator, numFolds=numberOfCVFolds) <br/><br/># Fit model on "train" set<br/>cvModel = cv.fit(train)<br/><br/># Get the best model based on CrossValidator<br/>model = cvModel.bestModel<br/><br/># Run inference on "test" set<br/>predictions = model.transform(test)<br/><br/># Return accuracy as output dataframe<br/>accuracy = classEvaluator.evaluate(predictions)<br/>output = spark.createDataFrame([accuracy], FloatType()).withColumnRenamed("value","Accuracy")</span></pre><p id="a941" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设一切顺利,“输出”数据帧将包含“准确性”,它将被写到文件中的一个位置，该位置是在上述管道中标有“捕获准确性”的文件目标中配置的。</p><p id="0169" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重申我在其他ML相关博客中提到的内容，不言而喻，模型的准确性将取决于训练和测试数据集的大小和质量，以及特征工程和超参数调整——这并不是这篇博客的重点，而是展示如何将StreamSets Transformer扩展到其他用例。</p><p id="20e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">GitHub上的管道和数据集</strong></p><p id="630b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您想亲自尝试一下，请从GitHub下载管道。</p><p id="4382" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">总结</strong></p><p id="cbcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客中，您了解了如何轻松地扩展StreamSets Transformer的功能。</p><p id="3e16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管该平台易于扩展，但需要注意的是，定制代码仍然利用StreamSets Transformer的底层内置功能和强大功能。仅举几个例子:</p><ul class=""><li id="efcf" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated">在任何Spark集群、本地Hadoop或云托管Spark服务(例如Databricks)上执行。</li><li id="b2ba" class="jf jg hi ih b ii jo im jp iq jq iu jr iy js jc jk jl jm jn bi translated">渐进式错误处理可以准确地了解错误发生的位置和原因，而无需解密复杂的日志文件。</li><li id="7a72" class="jf jg hi ih b ii jo im jp iq jq iu jr iy js jc jk jl jm jn bi translated">高度仪表化的管道揭示了每个操作以及应用程序作为一个整体是如何执行的。</li></ul></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><p id="86fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je">原载于2019年12月12日</em><a class="ae jd" href="https://streamsets.com/blog/streamsets-transformer-natural-language-processing-in-pyspark/" rel="noopener ugc nofollow" target="_blank"><em class="je">https://streamsets.com</em></a><em class="je">。</em></p></div></div>    
</body>
</html>