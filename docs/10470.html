<html>
<head>
<title>Creating Your own Intent Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">创建您自己的意图分类器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/creating-your-own-intent-classifier-b86e000a4926?source=collection_archive---------0-----------------------#2020-10-20">https://medium.com/analytics-vidhya/creating-your-own-intent-classifier-b86e000a4926?source=collection_archive---------0-----------------------#2020-10-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/613957715cd5e20a15fee88d0061e37e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*tSPpZJ5P1KeeynGNTxW5YQ.jpeg"/></div></figure><p id="42cf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">作为NLP的粉丝，我总是想知道当我要求谷歌助手或Alexa做一些事情时，它是如何理解的。问题接着是，我是否能让我的机器也理解我？解决方法是-意图分类。</p><blockquote class="jk jl jm"><p id="b159" class="im in jn io b ip iq ir is it iu iv iw jo iy iz ja jp jc jd je jq jg jh ji jj hb bi translated">意图分类是自然语言理解的一部分，其中机器学习/深度学习算法学习在它接受训练的短语的基础上对给定短语进行分类。</p></blockquote><p id="923a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们举一个有趣的例子；我在做一个像Alexa一样的助手。</p><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es jr"><img src="../Images/85e950876c98c0c32457935b682ab9da.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*-DqJRDWGgroeST2ORdHzzw.jpeg"/></div></figure><p id="0b7d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为简单起见，我们将执行3项任务，即开灯、关灯和告诉我们天气如何。让我们给这三个任务起个名字:开灯、关灯和天气。这些所有的任务在NLU被称为<em class="jn">‘意图’</em>。换句话说，<em class="jn">意图是一组属于共同名称的相似短语，这样深度学习算法就很容易理解用户要说的话</em>。每个意图被给予一定数量的训练短语，以便它能够学习对实时短语进行分类。</p><p id="fbe1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们知道什么是意图分类，让我们开始酷的东西！我写了一个<a class="ae jw" href="https://github.com/horizons-ml/intent-classifier/blob/main/intent_classification.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>如果你想跟着我，你可以在我的<a class="ae jw" href="https://github.com/horizons-ml/intent-classifier" rel="noopener ugc nofollow" target="_blank"> Github repo </a>这里找到。</p><p id="67e0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为方便起见，让我们遵循以下目录结构:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="1405" class="kc kd hi jy b fi ke kf l kg kh">Your directory<br/>├───models <br/>├───utils<br/>└───intent_classification.ipynb</span></pre><h2 id="6b9a" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">安装依赖项</h2><p id="43e7" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">使用以下命令安装所需的依赖项:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="4824" class="kc kd hi jy b fi ke kf l kg kh">pip install wget tensorflow==1.5 pandas numpy keras</span></pre><h2 id="b49a" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">资料组</h2><p id="b18f" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">我们将使用公开可用的<a class="ae jw" href="https://github.com/clinc/oos-eval" rel="noopener ugc nofollow" target="_blank"> CLINC150数据集</a>。它收集了10个领域中150种不同意图的短语。你可以在这里阅读更多关于数据集的信息。</p><p id="d094" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将使用以下方式下载数据集:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="11f5" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">wget</strong><br/>url = 'https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_full.json'<br/>wget.download(url)</span></pre><h2 id="f1c0" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">准备数据集</h2><p id="7941" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">数据集已经分为“训练”、“测试”和“验证”集，但我们将创建自己的训练和验证集，因为我们不需要测试集。我们将通过合并所有集合，然后使用scikit-learn将它们分成“训练”和“验证”集合来实现这一点。这也将创建更多的训练数据。</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="476e" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">numpy</strong> <strong class="jy hj">as</strong> <strong class="jy hj">np</strong><br/><strong class="jy hj">import</strong> <strong class="jy hj">json<br/></strong><em class="jn"># Loading json data</em><br/><strong class="jy hj">with</strong> open('data_full.json') <strong class="jy hj">as</strong> file:<br/>  data = json.loads(file.read())<br/><br/><em class="jn"># Loading out-of-scope intent data</em><br/>val_oos = np.array(data['oos_val'])<br/>train_oos = np.array(data['oos_train'])<br/>test_oos = np.array(data['oos_test'])<br/><br/><em class="jn"># Loading other intents data</em><br/>val_others = np.array(data['val'])<br/>train_others = np.array(data['train'])<br/>test_others = np.array(data['test'])<br/><br/><em class="jn"># Merging out-of-scope and other intent data</em><br/>val = np.concatenate([val_oos,val_others])<br/>train = np.concatenate([train_oos,train_others])<br/>test = np.concatenate([test_oos,test_others])</span><span id="baa8" class="kc kd hi jy b fi lg kf l kg kh">data = np.concatenate([train,test,val])<br/>data = data.T<br/><br/>text = data[0]<br/>labels = data[1]</span></pre><p id="c1fa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，我们将使用以下内容创建训练和验证拆分:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="c2fc" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">from</strong> <strong class="jy hj">sklearn.model_selection</strong> <strong class="jy hj">import</strong> train_test_split</span><span id="0127" class="kc kd hi jy b fi lg kf l kg kh">train_txt,test_txt,train_label,test_labels = train_test_split(text,labels,test_size = 0.3)</span></pre><h2 id="f7e0" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">数据集预处理</h2><p id="bd35" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">由于深度学习是一个数字游戏，它希望我们的数据是数字形式的。我们将标记我们的数据集；意思是将句子分解成个体，并将这些个体转换成数字表示。我们将使用K <a class="ae jw" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer" rel="noopener ugc nofollow" target="_blank">时代标记器</a>来标记我们使用以下代码的短语:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="d78e" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">from</strong> <strong class="jy hj">tensorflow.python.keras.preprocessing.text</strong> <strong class="jy hj">import</strong> Tokenizer<br/><strong class="jy hj">from</strong> <strong class="jy hj">tensorflow.python.keras.preprocessing.sequence</strong> <strong class="jy hj">import</strong> pad_sequences</span><span id="37f3" class="kc kd hi jy b fi lg kf l kg kh">max_num_words = 40000<br/>classes = np.unique(labels)<br/><br/>tokenizer = Tokenizer(num_words=max_num_words)<br/>tokenizer.fit_on_texts(train_txt)<br/>word_index = tokenizer.word_index</span></pre><p id="f925" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了将我们的数据输入深度学习模型，我们所有的短语必须长度相同。我们将用<em class="jn"> 0 </em>填充所有的训练短语，这样它们的长度就相同了。</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="da3a" class="kc kd hi jy b fi ke kf l kg kh">ls=[]<br/><strong class="jy hj">for</strong> c <strong class="jy hj">in</strong> train_txt:<br/>    ls.append(len(c.split()))<br/>maxLen=int(np.percentile(ls, 98))</span><span id="ea4a" class="kc kd hi jy b fi lg kf l kg kh">train_sequences = tokenizer.texts_to_sequences(train_txt)<br/>train_sequences = pad_sequences(train_sequences, maxlen=maxLen,              padding='post')</span><span id="6a98" class="kc kd hi jy b fi lg kf l kg kh">test_sequences = tokenizer.texts_to_sequences(test_txt)<br/>test_sequences = pad_sequences(test_sequences, maxlen=maxLen, padding='post')</span></pre><p id="cce0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，我们需要将标签转换成独热编码形式。你可以在这里阅读更多关于一键编码<a class="ae jw" href="https://victorzhou.com/blog/one-hot/" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="9249" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">from</strong> <strong class="jy hj">sklearn.preprocessing</strong> <strong class="jy hj">import</strong> OneHotEncoder,LabelEncoder<br/><br/>label_encoder = LabelEncoder()<br/>integer_encoded = label_encoder.fit_transform(classes)<br/><br/>onehot_encoder = OneHotEncoder(sparse=<strong class="jy hj">False</strong>)<br/>integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)<br/>onehot_encoder.fit(integer_encoded)</span><span id="180b" class="kc kd hi jy b fi lg kf l kg kh">train_label_encoded = label_encoder.transform(train_label)<br/>train_label_encoded = train_label_encoded.reshape(len(train_label_encoded), 1)<br/>train_label = onehot_encoder.transform(train_label_encoded)</span><span id="cf3d" class="kc kd hi jy b fi lg kf l kg kh">test_labels_encoded = label_encoder.transform(test_labels)<br/>test_labels_encoded = test_labels_encoded.reshape(len(test_labels_encoded), 1)<br/>test_labels = onehot_encoder.transform(test_labels_encoded)</span></pre><h2 id="1ad4" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">在我们创建模型之前..</h2><p id="4f59" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">在我们开始训练我们的模型之前，我们将使用<a class="ae jw" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">全局向量</a>。手套是由斯坦福大学在大型语料库上训练的单词的N维向量表示。由于它是在大型语料库上训练的，它将帮助模型更好地学习短语。</p><p id="54e6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将使用以下方式下载GloVe:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="a23d" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">wget</strong><br/>url ='https://www.dropbox.com/s/a247ju2qsczh0be/glove.6B.100d.txt?dl=1'<br/>wget.download(url)</span></pre><p id="2d40" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下载完成后，我们会将其存储在Python字典中:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="ad96" class="kc kd hi jy b fi ke kf l kg kh">embeddings_index={}<br/><strong class="jy hj">with</strong> open('glove.6B.100d.txt', encoding='utf8') <strong class="jy hj">as</strong> f:<br/>    <strong class="jy hj">for</strong> line <strong class="jy hj">in</strong> f:<br/>        values = line.split()<br/>        word = values[0]<br/>        coefs = np.asarray(values[1:], dtype='float32')<br/>        embeddings_index[word] = coefs</span></pre><p id="c970" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因为GloVe包含来自大型语料库的所有单词的向量表示，所以我们只需要那些出现在语料库中的单词向量。我们将创建一个嵌入矩阵，它只包含数据集中出现的单词的向量表示。因为我们的数据集已经被标记化了，所以Keras标记化器为数据集中的每个标记分配了一个唯一的编号。这个唯一的数字可以被认为是嵌入矩阵中每个单词的向量的索引；这意味着来自记号赋予器的每个第<em class="jn">n</em>个字由嵌入矩阵中第<em class="jn">n</em>个位置处的向量表示。</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="fdee" class="kc kd hi jy b fi ke kf l kg kh">all_embs = np.stack(embeddings_index.values())<br/>emb_mean,emb_std = all_embs.mean(), all_embs.std()</span><span id="3ad4" class="kc kd hi jy b fi lg kf l kg kh">num_words = min(max_num_words, len(word_index))+1</span><span id="cab6" class="kc kd hi jy b fi lg kf l kg kh">embedding_dim=len(embeddings_index['the'])</span><span id="b37c" class="kc kd hi jy b fi lg kf l kg kh">embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))<br/><strong class="jy hj">for</strong> word, i <strong class="jy hj">in</strong> word_index.items():<br/>    <strong class="jy hj">if</strong> i &gt;= max_num_words:<br/>        <strong class="jy hj">break</strong><br/>    embedding_vector = embeddings_index.get(word)<br/>    <strong class="jy hj">if</strong> embedding_vector <strong class="jy hj">is</strong> <strong class="jy hj">not</strong> <strong class="jy hj">None</strong>:<br/>        embedding_matrix[i] = embedding_vector</span></pre><h2 id="c0f7" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">模型准备</h2><p id="edd9" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">让我们把我们的模型的架构放在一起，看看模型的运行情况。</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="5219" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">from</strong> <strong class="jy hj">tensorflow.python.keras.models</strong> <strong class="jy hj">import</strong> Sequential<br/><strong class="jy hj">from</strong> <strong class="jy hj">tensorflow.python.keras.layers</strong> <strong class="jy hj">import</strong> Dense, Input, Dropout, LSTM, Activation, Bidirectional,Embedding</span><span id="8626" class="kc kd hi jy b fi lg kf l kg kh">model = Sequential()<br/><br/>model.add(Embedding(num_words, 100, trainable=<strong class="jy hj">False</strong>,input_length=train_sequences.shape[1], weights=[embedding_matrix]))<br/>model.add(Bidirectional(LSTM(256, return_sequences=<strong class="jy hj">True</strong>, recurrent_dropout=0.1, dropout=0.1), 'concat'))<br/>model.add(Dropout(0.3))<br/>model.add(LSTM(256, return_sequences=<strong class="jy hj">False</strong>, recurrent_dropout=0.1, dropout=0.1))<br/>model.add(Dropout(0.3))<br/>model.add(Dense(50, activation='relu'))<br/>model.add(Dropout(0.3))<br/>model.add(Dense(classes.shape[0], activation='softmax'))</span><span id="9671" class="kc kd hi jy b fi lg kf l kg kh">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])</span></pre><p id="ad59" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将在嵌入层中传递嵌入矩阵作为<em class="jn">权重。</em></p><h2 id="f023" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">模特培训</h2><p id="9683" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">最后是训练模型的时间。</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="bd66" class="kc kd hi jy b fi ke kf l kg kh">history = model.fit(train_sequences, train_label, epochs = 20,<br/>          batch_size = 64, shuffle=<strong class="jy hj">True</strong>,<br/>          validation_data=[test_sequences, test_labels])</span></pre><p id="65ee" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这大约需要一个小时左右，取决于您的机器。培训完成后，我们可以将指标想象为:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="ceb9" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">matplotlib.pyplot</strong> <strong class="jy hj">as</strong> <strong class="jy hj">plt</strong><br/>%matplotlib inline</span><span id="ebd3" class="kc kd hi jy b fi lg kf l kg kh">plt.plot(history.history['acc'])<br/>plt.plot(history.history['val_acc'])<br/>plt.title('Model Accuracy')<br/>plt.ylabel('Accuracy')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Train', 'Validation'], loc='upper left')<br/>plt.show()</span></pre><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/1a44b0bdebdb86af58725b5f9539754d.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*GqMmGQ7euG2w1jAC08oxMg.png"/></div><figcaption class="li lj et er es lk ll bd b be z dx translated">模型精度曲线</figcaption></figure><p id="3e9e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Wohoo！！我们得到了92.45%的训练准确率和88.86%的验证准确率，这是相当不错的。</p><p id="d663" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是损失曲线:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="f546" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">matplotlib.pyplot</strong> <strong class="jy hj">as</strong> <strong class="jy hj">plt</strong><br/>%matplotlib inline</span><span id="8fca" class="kc kd hi jy b fi lg kf l kg kh">plt.plot(history.history['loss'])<br/>plt.plot(history.history['val_loss'])<br/>plt.title('Model Loss')<br/>plt.ylabel('Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Train', 'Validation'], loc='upper left')<br/>plt.show()</span></pre><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/95166bff41235dcdbfbc92d8babf1335.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*2Cx2fbsH3hlyWfSTmr93PA.png"/></div></figure><p id="b663" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">培训损失约为0.2，验证损失约为0.5。您可以尝试模型架构，看看损失是否会下降<a class="ae jw" href="https://emojipedia.org/winking-face/" rel="noopener ugc nofollow" target="_blank">😉</a></p><h2 id="20b5" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">保存模型、标记器、标签编码器和标签</h2><p id="0982" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">让我们保存训练好的模型、标记器、标签编码器和标签，以便在将来的案例中使用它们。</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="7496" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">pickle</strong><br/><strong class="jy hj">import</strong> <strong class="jy hj">json</strong></span><span id="91ed" class="kc kd hi jy b fi lg kf l kg kh">model.save('models/intents.h5')<br/><br/><strong class="jy hj">with</strong> open('utils/classes.pkl','wb') <strong class="jy hj">as</strong> file:<br/>   pickle.dump(classes,file)<br/><br/><strong class="jy hj">with</strong> open('utils/tokenizer.pkl','wb') <strong class="jy hj">as</strong> file:<br/>   pickle.dump(tokenizer,file)<br/><br/><strong class="jy hj">with</strong> open('utils/label_encoder.pkl','wb') <strong class="jy hj">as</strong> file:<br/>   pickle.dump(label_encoder,file)</span></pre><h2 id="1b70" class="kc kd hi bd ki kj kk kl km kn ko kp kq ix kr ks kt jb ku kv kw jf kx ky kz la bi translated">是时候看看所有的活动了</h2><p id="cf1b" class="pw-post-body-paragraph im in hi io b ip lb ir is it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj hb bi translated">我们经历了漫长的旅程..让我们看看最终的目的地是什么样的。</p><p id="879a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我创建了下面的类来使用我们的模型:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="ce97" class="kc kd hi jy b fi ke kf l kg kh">import numpy as np<br/>from tensorflow.python.keras.preprocessing.sequence import pad_sequences</span><span id="ce34" class="kc kd hi jy b fi lg kf l kg kh"><strong class="jy hj">class</strong> <strong class="jy hj">IntentClassifier</strong>:<br/>    <strong class="jy hj">def</strong> __init__(self,classes,model,tokenizer,label_encoder):<br/>        self.classes = classes<br/>        self.classifier = model<br/>        self.tokenizer = tokenizer<br/>        self.label_encoder = label_encoder<br/><br/>    <strong class="jy hj">def</strong> get_intent(self,text):<br/>        self.text = [text]<br/>        self.test_keras = self.tokenizer.texts_to_sequences(self.text)<br/>        self.test_keras_sequence = pad_sequences(self.test_keras, maxlen=16, padding='post')<br/>        self.pred = self.classifier.predict(self.test_keras_sequence)<br/>        <strong class="jy hj">return</strong> self.label_encoder.inverse_transform(np.argmax(self.pred,1))[0]</span></pre><p id="b167" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要使用该类，我们应该首先加载我们保存的文件:</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="6fb3" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import</strong> <strong class="jy hj">pickle</strong><br/><br/><strong class="jy hj">from</strong> <strong class="jy hj">tensorflow.python.keras.models</strong> <strong class="jy hj">import</strong> load_model</span><span id="7c06" class="kc kd hi jy b fi lg kf l kg kh">model = load_model('models/intents.h5')<br/><br/><strong class="jy hj">with</strong> open('utils/classes.pkl','rb') <strong class="jy hj">as</strong> file:<br/>  classes = pickle.load(file)<br/><br/><strong class="jy hj">with</strong> open('utils/tokenizer.pkl','rb') <strong class="jy hj">as</strong> file:<br/>  tokenizer = pickle.load(file)<br/><br/><strong class="jy hj">with</strong> open('utils/label_encoder.pkl','rb') <strong class="jy hj">as</strong> file:<br/>  label_encoder = pickle.load(file)</span></pre><p id="63a5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">考试时间到了！😋</p><pre class="js jt ju jv fd jx jy jz ka aw kb bi"><span id="3ff3" class="kc kd hi jy b fi ke kf l kg kh">nlu = IntentClassifier(classes,model,tokenizer,label_encoder)<br/>print(nlu.get_intent("is it cold in India right now"))<br/><em class="jn"># </em>Prints 'weather'</span></pre><figure class="js jt ju jv fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/8fbd245584752b4a1fd3fcbd62d4953e.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*ALbH32xuEYB7VsCJReddPw.jpeg"/></div></figure><p id="0ea2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">就是这样，伙计们！感谢您的阅读😃。快乐学习！</p></div></div>    
</body>
</html>