<html>
<head>
<title>Different Types of Machine Learning Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不同类型的机器学习算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/different-types-of-machine-learning-algorithm-b4f76b5730fd?source=collection_archive---------21-----------------------#2020-05-27">https://medium.com/analytics-vidhya/different-types-of-machine-learning-algorithm-b4f76b5730fd?source=collection_archive---------21-----------------------#2020-05-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/3eac0554c45aebf8d935368ac2f5c753.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*caTD6J4nJ-VG2-4Xql1Xtw.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">(<a class="ae iq" href="https://data-flair.training/blogs/wp-content/uploads/sites/2/2019/07/machine-learning-types.jpg" rel="noopener ugc nofollow" target="_blank">https://data-flair . training/blogs/WP-content/uploads/sites/2/2019/07/machine-learning-types . jpg</a>)</figcaption></figure><ol class=""><li id="916a" class="ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated"><strong class="it hj">监督学习</strong> : <em class="jj">监督学习所需</em> <strong class="it hj"> <em class="jj">训练标记数据。</em></strong></li></ol><p id="9e93" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated">在监督学习中，我们在带标签的数据集上训练你的模型，这意味着我们既有原始输入数据，也有它的结果。我们将数据分为训练数据集和测试数据集，其中训练数据集用于训练我们的模型，而测试数据集用作预测结果或查看模型准确性的新数据。</p><p id="65bc" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated">2.<strong class="it hj">无监督学习</strong> : <em class="jj">无监督学习不需要明确地标记或分类数据。</em></p><p id="9002" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated">在无监督学习中，用于训练的信息在数据集中既没有分类也没有标记。无监督学习研究系统如何从未标记的数据中推断出描述隐藏结构的函数。无监督学习的主要任务是<strong class="it hj">在数据中寻找模式</strong>。</p><p id="1548" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated">3.<strong class="it hj">强化</strong> : <em class="jj">强化是一种动态规划，它使用奖励和惩罚系统来训练算法。代理通过与其环境交互来学习。</em></p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es jz"><img src="../Images/c0e374236cbd4458f46fb5f9b1b41667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o7-FKhwQlgaSX0oTSw_3Gw.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">(<a class="ae iq" href="https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190522174744/MachineLearning.png" rel="noopener ugc nofollow" target="_blank">https://media . geeks forgeeks . org/WP-content/cdn-uploads/20190522174744/machine learning . png</a>)</figcaption></figure><p id="2297" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated"><strong class="it hj">监督学习分为两类算法</strong>:</p><ul class=""><li id="d222" class="ir is hi it b iu iv iw ix iy iz ja jb jc jd je ki jg jh ji bi translated"><strong class="it hj">分类</strong> : <em class="jj">分类产生</em> <strong class="it hj"> <em class="jj">离散值</em> </strong> <em class="jj">并对数据集进行严格的分类。当我们希望我们的结果反映数据集中数据点的归属时，我们使用分类。</em></li></ul><p id="ce36" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated">一个分类问题是当输出变量是一个类别时，比如“苹果”或“香蕉”或“红色”和“蓝色”。</p><ul class=""><li id="1647" class="ir is hi it b iu iv iw ix iy iz ja jb jc jd je ki jg jh ji bi translated"><strong class="it hj">回归</strong> : <em class="jj">回归给我们</em> <strong class="it hj"> <em class="jj">连续的</em> </strong> <em class="jj">结果，让你更好的区分个别点。</em></li></ul><p id="7c31" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated">一个回归问题是当输出变量是一个实值，如“美元”或“重量”。</p><p id="0719" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated"><strong class="it hj">无监督学习分为两类算法</strong>:</p><ul class=""><li id="ff49" class="ir is hi it b iu iv iw ix iy iz ja jb jc jd je ki jg jh ji bi translated"><strong class="it hj">聚类</strong>:聚类问题是你想要发现数据中的内在分组，比如按照购买行为对客户进行分组。</li><li id="b859" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated"><strong class="it hj">关联</strong>:关联规则学习问题是您想要发现描述大部分数据的规则。</li></ul><h1 id="e3c6" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">使用最广泛的监督学习算法有<strong class="ak"> : </strong></h1><ul class=""><li id="1494" class="ir is hi it b iu lm iw ln iy lo ja lp jc lq je ki jg jh ji bi translated">逻辑回归。</li><li id="fb19" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">线性<strong class="it hj">回归</strong>。</li><li id="4ff1" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">朴素贝叶斯。</li><li id="48bf" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">支持向量机。</li><li id="8fb3" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">决策树。</li><li id="9ac3" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">k最近邻算法。</li><li id="8955" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">随机森林</li></ul><h1 id="8f23" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">逻辑回归</strong></h1><ul class=""><li id="31b3" class="ir is hi it b iu lm iw ln iy lo ja lp jc lq je ki jg jh ji bi translated"><em class="jj">逻辑回归根据一组给定的独立变量估计离散值。</em></li><li id="2b65" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">它预测值在0-1之间的概率logit函数。这是一种预测二元类的统计方法。结果或目标在本质上将是二元的。</li><li id="1450" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">这是一种分类算法，而不是回归算法。</li><li id="b9e7" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated"><em class="jj">逻辑模型用于模拟某一类别或事件存在的概率，如通过或失败、赢或输、活着或死了、健康或生病。</em></li></ul><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es lr"><img src="../Images/ef5e665ed9f0adb9ded60d73db4dd6dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YxVImAMf317pbRqthEPYEw.jpeg"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">(<a class="ae iq" href="https://i.stack.imgur.com/rhVmk.jpg" rel="noopener ugc nofollow" target="_blank">https://i.stack.imgur.com/rhVmk.jpg</a>)</figcaption></figure><h1 id="a8c4" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">线性回归</h1><ul class=""><li id="25d6" class="ir is hi it b iu lm iw ln iy lo ja lp jc lq je ki jg jh ji bi translated"><em class="jj">线性回归是监督学习技术下最简单的机器学习算法之一，用于解决回归问题。</em></li><li id="acc7" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">它用于在自变量的帮助下预测连续的因变量。线性回归的目标是找到能够准确预测连续因变量输出的最佳拟合线。</li><li id="517a" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">通过寻找最佳拟合线，算法建立了因变量和自变量之间的关系。</li><li id="9962" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">线性回归的输出应该只是连续值，如价格、年龄、工资等。</li></ul><h1 id="6c0e" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">朴素贝叶斯</h1><ul class=""><li id="bb1e" class="ir is hi it b iu lm iw ln iy lo ja lp jc lq je ki jg jh ji bi translated">朴素贝叶斯分类器假设一个类中特定特征的存在与任何其他特征的存在无关。</li><li id="8042" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated"><em class="jj"> Navive Bayes分类器考虑数据的所有属性或特征，以独立地对某个类或事件的概率做出贡献。</em></li></ul><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es ls"><img src="../Images/27fe7b157d8494d2b7b9d25f9a61e25b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lzdDTCuhRShkDFhCU3jcPg.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">(<a class="ae iq" href="https://miro.medium.com/max/6190/1*39U1Ln3tSdFqsfQy6ndxOA.png" rel="noopener">https://miro . medium . com/max/6190/1 * 39 u1ln 3 tsdfqsfqy 6 ndxoa . png</a>)</figcaption></figure><ul class=""><li id="8e12" class="ir is hi it b iu iv iw ix iy iz ja jb jc jd je ki jg jh ji bi translated"><em class="jj">这种算法需要一个</em> <strong class="it hj"> <em class="jj">少量的训练数据</em> </strong> <em class="jj">来估计必要的参数。</em></li><li id="9bde" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">与更复杂的方法相比，朴素贝叶斯分类器非常快。</li></ul><h1 id="e027" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">支持向量机</h1><p id="0f4b" class="pw-post-body-paragraph jk jl hi it b iu lm jm jn iw ln jo jp iy lt jr js ja lu ju jv jc lv jx jy je hb bi translated"><em class="jj">SVM的目标是训练一个模型，把新的看不见的物体分配到一个特定的类别。</em></p><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es lw"><img src="../Images/69e2927dbaa204417b4740c84ffe5cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHhx0N3mDWPH4iuI6GjR7Q.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">(<a class="ae iq" href="https://www.researchgate.net/profile/Hamid_Baghaee/publication/330557084/figure/fig5/AS:770135056977924@1560625914689/General-classification-hyperplane-representation-of-SVM-algorithm.png" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/profile/Hamid _ Baghaee/publication/330557084/figure/fig 5/AS:770135056977924 @ 1560625914689/General-class ification-hyperplane-re presentation-of-SVM算法. png </a>)</figcaption></figure><p id="6115" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated"><em class="jj">在SVM，我们将特征空间线性划分为两类。基于新的看不见的物体的特征，它将物体放置在分离平面之上或之下，导致分类。</em></p><p id="23d3" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated"><em class="jj">这使得它成为非概率线性分类器的一个例子。</em></p><p id="d72e" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi">.</p><p id="c58e" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi">.</p><h1 id="0554" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">决策树</h1><ul class=""><li id="4b36" class="ir is hi it b iu lm iw ln iy lo ja lp jc lq je ki jg jh ji bi translated"><em class="jj">决策树以树结构的形式建立分类或回归模型。</em></li><li id="113f" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated"><em class="jj">它将一个数据集分解成越来越小的子集，同时一个相关的决策树被增量开发。</em></li></ul><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es lx"><img src="../Images/f5547c71608d041bd3d6eb081e16ae7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mC4l0X7jgjJx0i6wHUksqg.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">(<a class="ae iq" href="https://miro.medium.com/max/1430/1*rSQIIAboJftqAv_BReNipg.png" rel="noopener">https://miro . medium . com/max/1430/1 * rSQIIAboJftqAv _ bren IPG . png</a>)</figcaption></figure><ul class=""><li id="455c" class="ir is hi it b iu iv iw ix iy iz ja jb jc jd je ki jg jh ji bi translated"><em class="jj">最后的结果是一个有决策节点和叶节点的树，叶节点代表一个决策或分类。</em></li><li id="942e" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated"><em class="jj">决策树可以处理分类数据和数值数据。</em></li></ul><p id="5621" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi">.</p><p id="cf7a" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi">.</p><h1 id="9bf8" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">k-最近邻(KNN)</h1><p id="1edc" class="pw-post-body-paragraph jk jl hi it b iu lm jm jn iw ln jo jp iy lt jr js ja lu ju jv jc lv jx jy je hb bi translated"><em class="jj"> K最近邻是一种简单的算法，它存储所有可用的案例，并根据相似性度量对新案例进行分类。</em></p><p id="6f52" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated"><em class="jj">分类是通过对其邻居的多数投票来完成的。</em></p><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/2f619780c163f0d45ed9dadc2e633adc.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*0Pqqx6wGDfFm_7GLebg2Hw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">(<a class="ae iq" href="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531424125/KNN_final_a1mrv9.png" rel="noopener ugc nofollow" target="_blank">https://RES . cloud inary . com/dyd 911 kmh/image/upload/f _ auto，q _ auto:best/v 1531424125/KNN _ final _ a1mrv 9 . png</a>)</figcaption></figure><p id="534d" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated"><em class="jj">数据被分配给具有最近邻居的类。</em></p><p id="f6ed" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated"><em class="jj">随着最近邻数量的增加，k值的准确度可能会增加。</em></p><p id="9fa1" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi">.</p><p id="b532" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi">.</p><p id="43ae" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi">.</p><h1 id="de8d" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">最广泛使用的无监督学习算法有:</h1><ul class=""><li id="3e37" class="ir is hi it b iu lm iw ln iy lo ja lp jc lq je ki jg jh ji bi translated">k均值聚类。</li><li id="5f59" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">主成分分析。</li><li id="f77b" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">关联规则。</li></ul><h1 id="983e" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">k均值聚类</h1><ul class=""><li id="9230" class="ir is hi it b iu lm iw ln iy lo ja lp jc lq je ki jg jh ji bi translated">K-Means是最流行的“聚类”算法之一。</li><li id="d9ef" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated"><em class="jj"> k-means聚类是一种无监督聚类算法，只需要一组未标记的点。</em></li></ul><figure class="ka kb kc kd fd ij er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es lz"><img src="../Images/4b94f3ca9da05e2c93e0fee2b9ad46b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BmfPGhfKXD5onepOW8UTVA.jpeg"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">(<a class="ae iq" href="https://dendroid.sk/wp-content/uploads/2013/01/kmeansimg-scaled1000.jpg" rel="noopener ugc nofollow" target="_blank">https://trend oid . sk/WP-content/uploads/2013/01/kmean simg-scaled 1000 . jpg</a>)</figcaption></figure><ul class=""><li id="1c6a" class="ir is hi it b iu iv iw ix iy iz ja jb jc jd je ki jg jh ji bi translated"><em class="jj">该算法将采用未标记的点，并通过计算不同离散点之间距离的平均值，逐步学习如何将它们聚类成组。</em></li><li id="4fd1" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">K-mean使用质心之间的垂直平分线将聚类分成不同的类别，然后重新分配质心，直到我们将所有数据点分类。</li></ul><p id="67b8" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi">.</p><p id="db62" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi">.</p><h1 id="fecb" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">主成分分析</h1><ul class=""><li id="1794" class="ir is hi it b iu lm iw ln iy lo ja lp jc lq je ki jg jh ji bi translated"><em class="jj">PCA的目标是识别数据中的模式并检测变量之间的相关性。</em></li><li id="e022" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated"><em class="jj">主成分分析是一种用于强调数据集中的变化和突出强模式的技术。</em></li><li id="92e7" class="ir is hi it b iu kj iw kk iy kl ja km jc kn je ki jg jh ji bi translated">它通常用于使数据易于探索和可视化。</li></ul><p id="84d2" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated">下面是虹膜数据集的PCA ~</p><figure class="ka kb kc kd fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/3384161c534b3342854e006564b3a7df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*9mxQcfm2FhJ8e3Fs3P-auw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">(<a class="ae iq" href="https://lh3.googleusercontent.com/proxy/dYK1A9KskdaCSmMbCidqCfnT7GXaXrN0eupqWPyGg5_ki0HRXZrKoNux_ShSrsgSZ4wBbiYUMHtc8z75vBtnFvV_Xygu9IQEqfP3sfI2364n-TRN" rel="noopener ugc nofollow" target="_blank">https://lh3 . Google user content . com/proxy/dyk 1 a 9 kskdacsmmbcidqcfnt 7 gxaxrn 0 eupqwpygg 5 _ ki 0 hrxzrkonux _ shsrsgsz 4 bbiyumhtc 8 z 75 vbtnfvv _ xy gu 9 IQ eqfp 3 SFI 2364n-TRN</a>)</figcaption></figure><p id="4d0c" class="pw-post-body-paragraph jk jl hi it b iu iv jm jn iw ix jo jp iy jq jr js ja jt ju jv jc jw jx jy je hb bi translated"><strong class="it hj"> <em class="jj"> —纳夫乔特·辛格</em> </strong></p><blockquote class="mb mc md"><p id="4c38" class="jk jl jj it b iu iv jm jn iw ix jo jp me jq jr js mf jt ju jv mg jw jx jy je hb bi translated">谢谢你抽出宝贵的时间</p></blockquote></div></div>    
</body>
</html>