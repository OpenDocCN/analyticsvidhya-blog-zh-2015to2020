<html>
<head>
<title>What I learned from building a Deep Neural Network from scratch, and why you should do it too!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我从零开始建立深度神经网络中学到了什么，以及为什么你也应该这样做！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-i-learned-from-building-a-deep-neural-network-from-scratch-and-why-you-should-do-it-too-a2e6f422d3db?source=collection_archive---------9-----------------------#2020-07-27">https://medium.com/analytics-vidhya/what-i-learned-from-building-a-deep-neural-network-from-scratch-and-why-you-should-do-it-too-a2e6f422d3db?source=collection_archive---------9-----------------------#2020-07-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="886e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">通过深度神经网络的实际实现和Python中真实数据集的正则化，我对神经网络的理解之旅</h2></div><blockquote class="ix iy iz"><p id="5d64" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">本文使用数学等式，将<a class="ae jx" href="https://golopot.github.io/tex-to-unicode/" rel="noopener ugc nofollow" target="_blank"> this </a> Tex添加到Unicode扩展中，以获得更好的阅读体验。</p></blockquote><p id="7dfc" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">我和神经网络斗争了很长时间。由于某种原因，在经历了无数的教程和指南后，我还是不能很好地掌握数学和神经网络的内部工作，老实说，这很令人沮丧，因为我认为自己在数学方面相当不错。因此，我不顾一切地试图理解正向传播和反向传播背后的数学原理，决定从头开始构建自己的神经网络，并希望能边学边学。根据我的经验，神经网络背后的数学并不难理解，相反，事实证明，有效实现它的过程才是具有挑战性的。现在，我同意一个人不需要真正理解神经网络的来龙去脉来有效地使用它，这要感谢Tensorflow和Pytorch，它们使得实现你想要的任意多层的神经网络变得非常方便。但这不是这个活动的重点。这里的要点是理解和欣赏深度神经网络的幕后实现，以便我们可以学习使用Tensorflow、Pytorch或Theano等库来充分发挥它们的潜力。</p><p id="fd22" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">让我带你看看我用Python从头开始构建深度神经网络的过程，并希望说服你也自己构建一个…</p><h1 id="39fc" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">神经网络快速概述</h1><p id="0b59" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">深度前馈网络，也称为前馈神经网络，或多层感知器(MLPs)，代表了最典型的深度学习模型。MLP的目标是逼近某个函数f* <em class="jc">。根据Goodfellow、Bengio和库维尔在他们的书“</em> <a class="ae jx" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank"> <em class="jc">深度学习</em> </a> <em class="jc">”中的说法，对于一个分类器，y= f* </em> (x)将一个输入x映射到一个类别y .一个前馈网络定义了一个映射y = f(x；θ)并学习产生最佳函数近似的参数θ。</p><figure class="kz la lb lc fd ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ky"><img src="../Images/22b0f85a3906fd7d11dee338db2de038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*17Bi2En5WTFsZRpf"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来自吴恩达关于神经网络和深度学习的课程:<a class="ae jx" href="https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/Neural-Networks-Deep-Learning？专业化=深度学习</a></figcaption></figure><p id="2833" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">神经网络的组件</p><ul class=""><li id="176c" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw lt lu lv lw bi translated">输入层X</li><li id="ae3b" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw lt lu lv lw bi translated">任意数量的隐藏层</li><li id="a4c5" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw lt lu lv lw bi translated">参数W(权重)和b(偏差)</li><li id="b1f4" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw lt lu lv lw bi translated">每层的激活功能</li><li id="d8bd" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw lt lu lv lw bi translated">输出图层y^ (yhat)</li></ul><p id="0a32" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">在这里阅读更多关于神经网络<a class="ae jx" href="https://towardsdatascience.com/deep-learning-feedforward-neural-network-26a6705dbdc7" rel="noopener" target="_blank"/></p></div><div class="ab cl mc md gp me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="hb hc hd he hf"><h1 id="484a" class="kb kc hi bd kd ke mj kg kh ki mk kk kl io ml ip kn ir mm is kp iu mn iv kr ks bi translated">概观</h1><p id="ad69" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">我们将在Python中实现的是——</p><ol class=""><li id="344d" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw mo lu lv lw bi translated">挑选真实的数据集</li><li id="4531" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw mo lu lv lw bi translated">为我们的神经网络准备好数据集，即预处理</li><li id="4c20" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw mo lu lv lw bi translated">构建神经网络</li></ol><ul class=""><li id="5501" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw lt lu lv lw bi translated">初始化参数，he初始化和Xavier初始化</li><li id="73b7" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw lt lu lv lw bi translated">正则化、L2正则化和脱落正则化</li></ul><p id="424f" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">4.带脱落正则化的前向传播</p><ul class=""><li id="84da" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw lt lu lv lw bi translated">ReLU和Sigmoid激活</li></ul><p id="2202" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">5.用L2正则化计算成本</p><ul class=""><li id="0939" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw lt lu lv lw bi translated">对数损失成本函数</li></ul><p id="4314" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">6.具有L2和脱落正则化的反向传播</p><ul class=""><li id="b4ff" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw lt lu lv lw bi translated">ReLU和Sigmoid函数的导数</li></ul><p id="a1b0" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">7.训练模型并预测结果</p><p id="5524" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">注意:</p><ul class=""><li id="1ee9" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw lt lu lv lw bi translated">本文假定您理解Python</li><li id="ac6d" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw lt lu lv lw bi translated">本文并没有详细介绍一些概念和理论，但是，我提供了一些研究论文和文章的链接，这些论文和文章深入探讨了这些概念。我试图通过实际的实现来理解这些概念是如何运作的。</li></ul><h1 id="ecf0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">数据集</h1><p id="71ed" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">本活动的目的是通过实际实施加深对神经网络的理解。所以我选择了一个二元分类问题。这里的任务是训练一个深层的神经网络来区分电动汽车和电动公交车(<a class="ae jx" href="https://www.kaggle.com/positivepc/object-detection?)" rel="noopener ugc nofollow" target="_blank">链接到数据集</a>)。这个数据集包含了900多张电动汽车和电动公交车的图片，这些图片已经被分成了一个测试集文件夹和一个训练集文件夹，这样就方便了。</p><p id="94a9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">首先，让我们导入我们将用于这个项目的所有库-</p><pre class="kz la lb lc fd mp mq mr ms aw mt bi"><span id="eb3d" class="mu kc hi mq b fi mv mw l mx my">import numpy as np <br/>import matplotlib.pyplot as plt <br/>import cv2 <br/>import os <br/>import random <br/>import seaborn as sns <br/>from tqdm import tqdm</span></pre><p id="427d" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">接下来，让我们写一个函数，它将允许我们从测试和训练集图像中创建我们的X和y数组</p><figure class="kz la lb lc fd ld"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="1d10" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">这里，我们需要加载图像，并将其重新缩放到64 x 64，因为它们的大小不同。函数get_data(文件)采用文件夹名(“Training_set”或“test”)，该文件夹名又包含两个不同的文件，分别是“电动汽车”和“电动公交车”。由于该功能会逐一检查每个文件，因此对数据集进行洗牌很重要，这样就不会将所有的电动汽车一起成批处理，也不会将所有的电动公交车一起成批处理。</p><p id="23cb" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">数据集被预分割成训练集(743个图像)和测试集(191个图像)，所以我们只需要调用这些文件夹上的get_data()函数来获得我们的X_train、y_train和X_test、y_test数组。</p><figure class="kz la lb lc fd ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es nb"><img src="../Images/06c39efd5820ea134cfd1559c326646c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PkHvW98MT9FX9gtg"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">作者的形象</figcaption></figure><figure class="kz la lb lc fd ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es nb"><img src="../Images/d60566126daa16f11ca0173465df552e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0RgdUyoHqlZ0sRJb"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">作者的形象</figcaption></figure><p id="cf72" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">这是一些来自我们数据集的图像例子以及它们的标签-</p><figure class="kz la lb lc fd ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es nc"><img src="../Images/44c27262e720c4b6fb89b853107cb0b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l3dzdUPsI09fbHwK"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">作者的形象</figcaption></figure><h1 id="2980" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">图像的预处理</h1><figure class="kz la lb lc fd ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es nd"><img src="../Images/f4693fd30bf45a45adca5d8117bba44b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FW8uuYFgNSjY3gsu"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">均衡前后图像的直方图。(<a class="ae jx" href="https://en.wikipedia.org/wiki/Histogram_equalization" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Histogram_equalization</a>)</figcaption></figure><p id="0123" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">我们需要处理图像，通过抑制不必要的失真来改善图像数据，或者增强对我们的模型的更好性能很重要的一些图像特征。我们需要做的第一件事是使图像符合统一的纵横比，这是我们在get_data(file)函数中将图像调整为64 x 64时所做的。接下来，我们将通过直方图均衡化来标准化光照。这种方法通过增加图像的整体对比度来帮助更好地分布发光强度。(更多关于直方图均衡化的信息<a class="ae jx" href="https://en.wikipedia.org/wiki/Histogram_equalization" rel="noopener ugc nofollow" target="_blank">请点击这里</a>)</p><p id="8b1b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">图像中的像素在范围[0，255]内，通过将每个像素除以255，我们能够将它们缩放到范围[0，1]。这使得在训练网络的同时成本收敛更快(关于图像预处理的大文章<a class="ae jx" href="https://becominghuman.ai/image-data-pre-processing-for-neural-networks-498289068258#:~:text=This%20is%20done%20to%20expose,characteristics%20in%20the%20data%2Dset.)" rel="noopener ugc nofollow" target="_blank">这里</a>)。最后，通过将图像转换为灰度，我们能够将RGB通道压缩为一个通道，并降低数据集的维度，从而加快训练速度。</p><blockquote class="ix iy iz"><p id="c006" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">关于我们如何处理图像以增强神经网络性能的补充阅读<a class="ae jx" href="https://arxiv.org/ftp/arxiv/papers/1710/1710.06805.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></blockquote><figure class="kz la lb lc fd ld"><div class="bz dy l di"><div class="mz na l"/></div></figure><figure class="kz la lb lc fd ld er es paragraph-image"><div class="er es ne"><img src="../Images/259c5738073b0a4c0e59bda90e58a965.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/0*mW0AwUgSz-Km0eo2"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图片来自作者</figcaption></figure><p id="3495" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">这是预处理后随机图像的外观</p><p id="14a6" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">需要做的最后一件事是拉平数据，这基本上意味着减少输入数据集XXX的维度。如上所述，输入数据的形状是(m，64，64，3 ),现在当它被处理并变成灰度级时，它的形状将是(m，64，64，1)。数组X_train和X_test现在需要以形状(64 <em class="jc"> 64 </em> 1，m)展开，以便减少维数，并获得可以输入到我们的神经网络中的形式。</p><pre class="kz la lb lc fd mp mq mr ms aw mt bi"><span id="f418" class="mu kc hi mq b fi mv mw l mx my">#flattening X  </span><span id="1811" class="mu kc hi mq b fi nf mw l mx my">X_train= X_train.reshape(X_train.shape[0],-1).T X_test= X_test.reshape(X_test.shape[0],-1).T</span></pre><figure class="kz la lb lc fd ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ng"><img src="../Images/ae4047dd7bf31ce86caf8c832652577e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O5QEA2RbEP469M03"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图片来自作者</figcaption></figure><blockquote class="nh"><p id="97a7" class="ni nj hi bd nk nl nm nn no np nq jw dx translated">现在数据已经准备好了，我们可以开始构建我们的神经网络了…</p></blockquote><h1 id="2b90" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io nr ip kn ir ns is kp iu nt iv kr ks bi translated">初始化</h1><p id="a55b" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">首先，我们需要通过给网络中每一层的每个神经元分配随机权重来初始化神经网络。这些权重不能用零初始化，因为这将导致所有神经元在训练期间学习相同的特征。用相等的权重(在这种情况下为零)初始化将意味着所有隐藏单元对成本具有相同的影响，因此将具有几乎相同的梯度。这将阻止不同的神经元学习不同的东西。为了克服这个问题，我们需要打破这种对称性，用随机值初始化权重。</p><p id="c1f3" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">然而，还有另一个问题，如果用太小的值初始化权重，将导致学习缓慢，并引入消失梯度的问题。这发生在相对于早期层中的权重的梯度变得非常小时，这将意味着对我们的参数的更新非常小，并且成本函数最终停留在相同的值。另一方面，如果初始化的权重太大，可能会导致发散(更多信息<a class="ae jx" href="https://www.deeplearning.ai/ai-notes/initialization/" rel="noopener ugc nofollow" target="_blank">此处</a>)。这可以通过使用用于ReLU激活的<em class="jc"> He初始化(</em><a class="ae jx" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank"><em class="jc">He et al 2015</em></a><em class="jc">)</em>或用于tanh激活的<em class="jc"> Xavier初始化(</em><a class="ae jx" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank"><em class="jc">Glorot et al 2010</em></a><em class="jc">)</em>来解决。</p><ul class=""><li id="35ac" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw lt lu lv lw bi translated">关于激活功能的伟大文章<a class="ae jx" href="https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" rel="noopener" target="_blank">在这里</a></li><li id="0743" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw lt lu lv lw bi translated">解释消失/爆炸渐变问题的视频<a class="ae jx" href="https://www.youtube.com/watch?v=qO_NLVjD6zE" rel="noopener ugc nofollow" target="_blank">这里</a></li></ul><p id="532e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">使用大小为64 x 64和3个颜色通道(RGB)的输入图像来决定初始化神经网络的矩阵形状的快速指南</p><figure class="kz la lb lc fd ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es nu"><img src="../Images/034077d3380c152337c5f2247a3fd6ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NSVA-EMg5ht2vwmc"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来自吴恩达关于神经网络和深度学习的课程:<a class="ae jx" href="https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/Neural-Networks-Deep-Learning？专业化=深度学习</a></figcaption></figure><p id="2525" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">我们将对所有隐藏层使用ReLU激活函数，对输出层使用Sigmoid激活函数。</p><figure class="kz la lb lc fd ld"><div class="bz dy l di"><div class="mz na l"/></div></figure><h1 id="6fc3" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">关于正则化的快速注释</h1><p id="2433" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">在我们继续向前传播之前，理解过拟合的概念是很重要的。就像消失/爆炸渐变一样，过度拟合也会导致我们的模型表现不佳。一般来说，过度拟合发生在数据训练得太好的时候。这意味着该模型非常适合训练数据，但不能推广到其他数据。</p><figure class="kz la lb lc fd ld er es paragraph-image"><div class="er es nv"><img src="../Images/a51d00c39c753eef6147b8e576198936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/0*PbF6bvkD9cOGhfA7"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated"><a class="ae jx" href="https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2015/02/avoid-over-fitting-regularity/</a></figcaption></figure><p id="6a28" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">这种情况的一个常见原因是在小数据集上训练模型，算法将试图确保它完全满足所有数据点，并最终过度拟合。对付它的一个方法是使用正规化。正则化本质上迫使模型更简单，从而减少过度拟合的机会(关于过度拟合的好文章<a class="ae jx" href="https://elitedatascience.com/overfitting-in-machine-learning" rel="noopener ugc nofollow" target="_blank">在这里</a></p><p id="f975" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">谈到正则化的实际应用，有两种最常见的正则化方法用于神经网络，L-2正则化和丢弃正则化(后者不太常见)。这两种方法都非常有效，但Srivastava、Hinton、Krizhevsky、Sutskever和Slakhutdinov在2014年发表的这篇<a class="ae jx" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>证明，如果同时使用L-2正则化和Dropout正则化，可以提高模型的准确性。现在就来吧！！当Geoffrey Hinton让你对你的模型做些什么的时候，你不要问问题，你只要做就行了！</p><ul class=""><li id="d680" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw lt lu lv lw bi translated">L-2正则化-这里我们将权重W的平方值添加到成本函数中，这一术语也称为权重衰减，有助于使权重更接近于零。(进一步详情<a class="ae jx" href="http://www.deeplearningbook.org/contents/regularization.html" rel="noopener ugc nofollow" target="_blank">此处</a>)</li><li id="8810" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw lt lu lv lw bi translated">退出正则化-这种技术在每次迭代中关闭随机神经元，这意味着在每次迭代中，您训练一个不同的模型，该模型只使用一些神经元。这使得神经元对其他神经元不太敏感，从而减少过度拟合。</li></ul><p id="5cd1" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">我们的模型可能会也可能不会过度拟合数据，但这里的目标是学习深度神经网络的概念和实现，因此我们将实现上面提到的两种正则化技术。</p><blockquote class="ix iy iz"><p id="35fe" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><a class="ae jx" href="https://towardsdatascience.com/regularization-techniques-for-neural-networks-e55f295f2866" rel="noopener" target="_blank">这里</a>是一篇进一步详细讨论正规化的文章。</p></blockquote><h1 id="0f92" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">正向传播</h1><p id="a154" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">在神经网络的这个阶段，输入被输入到神经网络中。这导致使用当前权重集跨隐藏层的正向计算流。这些通过层的正向计算的结果是基于当前权重的预测结果<em class="jc"> y^ </em>，使用损失函数将该预测结果与实际预期输出(<em class="jc"> y </em>)进行比较。这个损失函数的导数需要根据我们网络的所有层中的权重来计算，当我们实现反向传播时更是如此。</p><p id="87fc" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">简而言之，我们在前向传播期间计算并存储神经网络的中间变量。</p><p id="7778" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">让我们快速回顾一下计算步骤-</p><p id="3e18" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">注意:大写字母是指包含来自给定层<em class="jc"> l </em>中所有神经元或节点的值的矩阵和向量。例如，<em class="jc"> a^l </em>是层<em class="jc"> l </em>中单个节点的激活，而<em class="jc"> A^l </em>是包含层<em class="jc"> l </em>中每个节点的激活值的向量。</p><figure class="kz la lb lc fd ld"><div class="bz dy l di"><div class="mz na l"/></div></figure><h1 id="df06" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">价值函数</h1><p id="8381" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">成本函数J是神经网络在给定训练数据和预期输出的情况下表现如何的度量。对数损失或交叉熵成本衡量分类模型的性能，其中预测输入是介于0和1之间的概率值，目标是最小化该值。随着预测值(<em class="jc"> y^ </em>)偏离实际标签(<em class="jc"> y </em>)，测井曲线损失增加。(更多信息<a class="ae jx" href="http://wiki.fast.ai/index.php/Log_Loss#:~:text=Logarithmic%20loss%20(related%20to%20cross,is%20to%20minimize%20this%20value.&amp;text=Log%20loss%20increases%20as%20the%20predicted%20probability%20diverges%20from%20the%20actual%20label.)" rel="noopener ugc nofollow" target="_blank">此处</a>)</p><figure class="kz la lb lc fd ld"><div class="bz dy l di"><div class="mz na l"/></div></figure><h1 id="f343" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">反向传播</h1><p id="d9ff" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">这里的主要目标是通过使用微分学的链式法则来学习损失函数相对于不同权重的梯度。然后使用这些梯度更新权重。这些梯度是从输出节点开始反向学习的，这一过程称为反向传播。</p><p id="788e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">简单地说，计算出的梯度显示了我们的参数需要改变多少才能使成本最小化。</p><p id="280d" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">让我们回顾一下计算和代码:</p><figure class="kz la lb lc fd ld"><div class="bz dy l di"><div class="mz na l"/></div></figure><blockquote class="ix iy iz"><p id="37d9" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">关于反向传播的精彩文章<a class="ae jx" href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener ugc nofollow" target="_blank">这里</a></p></blockquote><h1 id="f86b" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">更新参数</h1><p id="3c7e" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">在这个最后的步骤中，我们使用反向传播期间计算的梯度来更新我们的参数</p><figure class="kz la lb lc fd ld"><div class="bz dy l di"><div class="mz na l"/></div></figure><h1 id="cf3f" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">把所有东西放在一起</h1><p id="a7b1" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">现在让我们写一个函数把所有的东西放在一起。这里，我们需要多次循环梯度下降，以使成本收敛。超参数学习率、迭代次数、lambda和keep prob可以进一步微调，以提高我们的神经网络的性能。</p><figure class="kz la lb lc fd ld"><div class="bz dy l di"><div class="mz na l"/></div></figure><h1 id="9cdb" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">微调模型以获得更好的结果</h1><p id="56a6" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">最佳实践是将测试数据集进一步划分为交叉验证集和测试集。交叉验证集用于通过更改和调整不同的超参数来比较和评估模型的性能，一旦找到这些超参数的最佳值，它们将用于在训练集上训练模型，然后最终在测试集上进行测试。</p><p id="b4ba" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">注意:我没有在这里这样做，因为运行多个模型在计算上是昂贵的，如上所述，这个项目的重点不是找到最好的模型，而是通过实际的实现来学习。</p><h2 id="e7b7" class="mu kc hi bd kd nw nx ny kh nz oa ob kl jy oc od kn jz oe of kp ka og oh kr oi bi translated">偏差与方差</h2><p id="36f4" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">偏差-高偏差表示模型与训练数据不太吻合，训练误差较大，而低偏差表示模型与训练数据非常吻合，训练误差较小。</p><p id="8e29" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">方差-高方差意味着模型无法对验证/测试集做出准确预测，验证误差较大，而低方差意味着模型非常适合验证/测试集，验证误差较小。</p><figure class="kz la lb lc fd ld er es paragraph-image"><div class="er es oj"><img src="../Images/2a684dde1ac4fc00847cf597f0131c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/0*iZLfxQzdKpaC22s1"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated"><a class="ae jx" href="https://missinglink.ai/guides/neural-network-concepts/neural-network-bias-bias-neuron-overfitting-underfitting/" rel="noopener ugc nofollow" target="_blank">https://missing link . ai/guides/neural-network-concepts/neural-network-bias-bias-neuron-over fitting-under fitting/</a></figcaption></figure><p id="e37a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">当模型经历低偏差和高方差时会出现过度拟合，处理此问题的一些方法是</p><ul class=""><li id="940f" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw lt lu lv lw bi translated">使用更大的网络，即更多层</li><li id="43f7" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw lt lu lv lw bi translated">训练时间更长，即迭代次数或周期数更多</li></ul><p id="e3e0" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">当模型经历高偏差和高方差时，会出现欠拟合，处理这个问题的一些方法是</p><ul class=""><li id="0c51" class="lo lp hi jd b je jf jh ji jy lq jz lr ka ls jw lt lu lv lw bi translated">获取更多数据或在图像分类的情况下使用<a class="ae jx" href="https://towardsdatascience.com/image-augmentation-for-deep-learning-histogram-equalization-a71387f609b2" rel="noopener" target="_blank">图像增强</a></li><li id="c84f" class="lo lp hi jd b je lx jh ly jy lz jz ma ka mb jw lt lu lv lw bi translated">使用正则化</li></ul><h1 id="b055" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">训练模型并预测结果</h1><p id="a27b" class="pw-post-body-paragraph ja jb hi jd b je kt ij jg jh ku im jj jy kv jm jn jz kw jq jr ka kx ju jv jw hb bi translated">正如我上面提到的，我没有创建交叉验证数据集，也没有通过尝试超参数学习率、lambda和保持概率的不同值来调整模型。我确信，通过改变这些超参数，尝试不同的架构，甚至使用图像增强来获得更多数据，可以对该模型进行微调，以实现更好的性能。然而，要记住的一点是，运行多个模型在计算上是昂贵的，尤其是因为我们还使用了L2和下降正则化。</p><blockquote class="ix iy iz"><p id="4540" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">你可以从我的Github <a class="ae jx" href="https://github.com/harsheev/DeepNeuralNetwork_from_scratch" rel="noopener ugc nofollow" target="_blank">这里</a>下载完整的笔记本并试一试！</p></blockquote><figure class="kz la lb lc fd ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ok"><img src="../Images/5e3099a3fc9fda9a22e3e9a776152da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jR-AWNTi9-DLCJoB"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图片来自作者</figcaption></figure><h1 id="7059" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">预测结果</h1><figure class="kz la lb lc fd ld er es paragraph-image"><div class="er es ol"><img src="../Images/5cf169ce037d3927470efce08f2d0943.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/0*buVig4BgNcpqDQf5"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">s形曲线(<a class="ae jx" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Sigmoid_function</a>)</figcaption></figure><p id="9532" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">为了获得对新数据集的预测，我们需要使用我们新学习的参数W和b对输入数据X实施正向传递。这种正向传递的输出将是概率，因为我们在最后/输出层上使用sigmoid函数。观察sigmoid曲线，我们可以有把握地假设，如果激活大于0.5，则y^=为1，否则y^=为0。</p><p id="9853" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">让我们写一个函数，根据概率来预测输出-</p><figure class="kz la lb lc fd ld"><div class="bz dy l di"><div class="mz na l"/></div></figure><figure class="kz la lb lc fd ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es om"><img src="../Images/8c92939e3648652ec0efdf39823b8e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TvnVSnv71M09Db0-"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图片来自作者</figcaption></figure><p id="d379" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">在没有任何调整的情况下，我们的训练精度为<em class="jc"> 99.4% </em>，测试/交叉验证精度为<em class="jc"> 80.6% </em>，这是一个良好的开端，我可以很高兴地说，我们的小项目取得了巨大的成功，结果可以通过微调超参数来改善。</p><p id="8c1c" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">我非常鼓励所有有抱负的数据科学家在学习神经网络的旅程中至少做一次这项活动，因为它不仅会让你对神经网络更加熟悉，还会帮助你对许多使用的概念形成有用的直觉。现在，当你使用TensorFlow和PyTorch时，你可以肯定你确切地知道幕后发生了什么。这个活动给了我很大的帮助，我希望你也会觉得有用。</p><p id="ed7d" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">最后，我想与你分享一些我用来学习和理解本文中使用的一些概念的资源</p><h2 id="8fc2" class="mu kc hi bd kd nw nx ny kh nz oa ob kl jy oc od kn jz oe of kp ka og oh kr oi bi translated">我推荐的关于深度学习的书籍</h2><blockquote class="ix iy iz"><p id="baa2" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jd hj">神经网络和深度学习:一本教科书</strong>(Charu c . Aggarwal):<a class="ae jx" href="https://www.pdfdrive.com/neural-networks-and-deep-learning-a-textbook-e184020999.html" rel="noopener ugc nofollow" target="_blank">https://www . pdf drive . com/Neural-Networks-and-Deep-Learning-A-Textbook-e 184020999 . html</a></p><p id="08eb" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jd hj">深度学习</strong>(伊恩·古德菲勒和约舒阿·本吉奥和亚伦·库维尔):<a class="ae jx" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">http://www.deeplearningbook.org/</a></p></blockquote><h2 id="e19e" class="mu kc hi bd kd nw nx ny kh nz oa ob kl jy oc od kn jz oe of kp ka og oh kr oi bi translated">在线课程</h2><blockquote class="ix iy iz"><p id="2bcd" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jd hj">神经网络与深度学习:</strong>(<a class="ae jx" href="https://www.coursera.org/learn/neural-networks-deep-learning?)" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/Neural-Networks-Deep-Learning？)</a></p><p id="02b2" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jd hj">改进深度神经网络:超参数调整、正则化和优化:</strong>(<a class="ae jx" href="https://www.coursera.org/learn/deep-neural-network?)" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/learn/deep-neural-network?)</a></p></blockquote></div></div>    
</body>
</html>