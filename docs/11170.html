<html>
<head>
<title>Comparison of Hyperparameter Tuning algorithms: Grid search, Random search, Bayesian optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调整算法的比较:网格搜索、随机搜索、贝叶斯优化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms-grid-search-random-search-bayesian-optimization-5326aaef1bd1?source=collection_archive---------1-----------------------#2020-11-21">https://medium.com/analytics-vidhya/comparison-of-hyperparameter-tuning-algorithms-grid-search-random-search-bayesian-optimization-5326aaef1bd1?source=collection_archive---------1-----------------------#2020-11-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="407f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在模型训练阶段，模型学习其参数。但也有一些秘密旋钮，称为<em class="jd">超参数</em>，模型无法自行学习——这些留给我们<em class="jd">来调整</em>。调整超参数可以显著提高模型性能。不幸的是，没有明确的程序来计算这些超参数值。这就是为什么超参数调整通常被认为是一门艺术而不是科学。</p><p id="e573" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我讨论了3种最流行的超参数调优算法——网格搜索、随机搜索和贝叶斯优化。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/3c0008c567ac148369668795a72c5d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*E24h6bZWQ5QQY42p"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">调整你的模型的秘密旋钮称为<strong class="bd ju"> <em class="jv">超参数</em> </strong></figcaption></figure><h1 id="3c44" class="jw jx hi bd ju jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">什么是超参数调谐？</h1><p id="aeb2" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">模型训练是模型学习其参数的过程。除此之外，每个模型也有一些超参数，它不能<em class="jd">学习，</em>但可以<em class="jd">调整</em>。与在训练期间学习的模型参数相反，模型超参数由数据科学家在训练之前设置。这个调整各种超参数值的过程称为超参数调整。(注意术语超参数<em class="jd">调谐</em>的用法，而不是超参数<em class="jd">训练</em>)。</p><blockquote class="ky kz la"><p id="2115" class="if ig jd ih b ii ij ik il im in io ip lb ir is it lc iv iw ix ld iz ja jb jc hb bi translated">模型<strong class="ih hj">参数</strong>在训练过程中自动从数据中学习。<br/>模型<strong class="ih hj">超参数</strong>是手动设置和调整的，在训练过程中使用，以帮助学习模型参数。</p></blockquote></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><h1 id="a9bc" class="jw jx hi bd ju jy ll ka kb kc lm ke kf kg ln ki kj kk lo km kn ko lp kq kr ks bi translated"><strong class="ak">超参数调整算法</strong></h1><h2 id="4f0c" class="lq jx hi bd ju lr ls lt kb lu lv lw kf iq lx ly kj iu lz ma kn iy mb mc kr md bi translated">1.网格搜索</h2><p id="34ad" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">这是最基本的超参数调优方法。你定义一个超参数值的<em class="jd">网格</em>。调整算法<strong class="ih hj">以连续的方式彻底地</strong>搜索这个空间，并为每个 可能的超参数值组合<strong class="ih hj"> <em class="jd">训练一个模型。</em></strong></p><p id="0385" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，为了训练SVM模型，我们可以如下定义我们的超参数网格空间(<em class="jd"> C，gamma，kernel </em>)。网格搜索算法训练多个模型(每个组合一个)，最终保留超参数值的<em class="jd">最佳</em>组合。</p><pre class="jf jg jh ji fd me mf mg mh aw mi bi"><span id="a172" class="lq jx hi mf b fi mj mk l ml mm">{ <br/>  'C': [0.1, 1, 10, 100, 1000], <br/>  'gamma': [0.1, 0.01 ,0.001, 0.0001], <br/>  'kernel': ['rbf', 'linear']<br/>}</span></pre><blockquote class="ky kz la"><p id="5f44" class="if ig jd ih b ii ij ik il im in io ip lb ir is it lc iv iw ix ld iz ja jb jc hb bi translated">网格搜索<strong class="ih hj">在实践中并不</strong>经常使用，因为随着要训练的超参数数量的增加，要训练的模型数量呈指数增长。这在计算能力和时间上都非常低效。</p></blockquote><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/c799920f87fdfe34b2d4365af7bab106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/0*1e43OU4nGe09muyi.gif"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">来源— <a class="ae mo" href="https://sigopt.com/blog/common-problems-in-hyperparameter-optimization" rel="noopener ugc nofollow" target="_blank"> SigOpt </a></figcaption></figure><h2 id="91e8" class="lq jx hi bd ju lr ls lt kb lu lv lw kf iq lx ly kj iu lz ma kn iy mb mc kr md bi translated">2.随机搜索</h2><p id="3342" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">随机搜索不同于网格搜索，因为我们<strong class="ih hj">不再为每个超参数提供一组明确的可能值</strong>；相反，我们<strong class="ih hj">为每个超参数</strong>提供一个统计分布，从其中<strong class="ih hj">采样</strong>值。本质上，我们为每个超参数定义一个采样分布，以执行随机搜索。</p><p id="f051" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用随机搜索，我们还可以<strong class="ih hj">控制或限制使用的超参数组合</strong>的数量。与网格搜索不同，网格搜索评估每一个可能的组合；在随机搜索中，我们可以指定只训练固定数量的模型，并在此之后终止优化算法。搜索迭代的次数也可以基于时间或资源来设置。</p><p id="e55c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，为了训练SVM模型，我们可以将超参数(<em class="jd"> C，gamma </em>)定义为对数均匀分布。(其他一些常见的分布有<code class="du mp mq mr mf b">exponential</code>、<code class="du mp mq mr mf b">normal</code>、<code class="du mp mq mr mf b">lognormal</code>、<code class="du mp mq mr mf b">uniform</code>和<code class="du mp mq mr mf b">random</code>)。随机搜索算法从它们各自的分布<em class="jd"> </em>中为<em class="jd"> C </em>和<em class="jd">γ</em>采样一个值，并使用它来训练一个模型。这个过程重复几次，训练多个模型。最终保留超参数值的<em class="jd">最佳</em>组合。</p><pre class="jf jg jh ji fd me mf mg mh aw mi bi"><span id="411b" class="lq jx hi mf b fi mj mk l ml mm">{<br/>  'C': loguniform(1e-1, 1e3),<br/>  'gamma': loguniform(1e-4, 1e-1),<br/>  'kernel': ['rbf', 'linear']<br/>}</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ms"><img src="../Images/36a5c4e908ef4a08ec3315277986eb5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/0*Khu3ysAZzevTjc64.gif"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">来源— <a class="ae mo" href="https://sigopt.com/blog/common-problems-in-hyperparameter-optimization" rel="noopener ugc nofollow" target="_blank"> SigOpt </a></figcaption></figure><h2 id="deab" class="lq jx hi bd ju lr ls lt kb lu lv lw kf iq lx ly kj iu lz ma kn iy mb mc kr md bi translated">3.贝叶斯优化</h2><p id="90a3" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">在前两种方法中，我们通过建立具有各种超参数值的多个模型来执行单独的实验。所有这些实验都是相互独立的。因为每个实验都是独立进行的，所以我们不能使用一个实验的信息来改进下一个实验。</p><blockquote class="ky kz la"><p id="562b" class="if ig jd ih b ii ij ik il im in io ip lb ir is it lc iv iw ix ld iz ja jb jc hb bi translated">贝叶斯优化是一种基于<strong class="ih hj"> <em class="hi">序列模型的优化</em> (SMBO) </strong>算法，该算法使用来自 <strong class="ih hj">先前迭代的<strong class="ih hj">结果来决定下一个超参数值候选</strong>。</strong></p><p id="a2c2" class="if ig jd ih b ii ij ik il im in io ip lb ir is it lc iv iw ix ld iz ja jb jc hb bi translated">因此，这种方法不是盲目地搜索超参数空间(如网格搜索和随机搜索)，而是提倡使用智能来挑选下一组超参数，这将提高模型性能。我们反复重复这个过程，直到我们收敛到一个最优值。</p></blockquote><p id="1c52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个有趣的类比是将这比作装袋和增压。仔细想想，想法很像！<br/>在装袋中，我们平行且相互独立地建造许多树。另一方面，Boosting是一个连续的过程，在这个过程中，每增加一棵树，我们都要学习纠正它的前身树中的错误。</p><p id="3152" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">贝叶斯优化创建了概率模型，将超参数映射到目标函数得分的概率。更多数学细节，请参考<a class="ae mo" href="https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f" rel="noopener" target="_blank">这个</a>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mt"><img src="../Images/87336a5b6e2c8b886b8418a6d0a636ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/0*YY5AQ5suLWHbyW5i.gif"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">来源— <a class="ae mo" href="https://sigopt.com/blog/common-problems-in-hyperparameter-optimization" rel="noopener ugc nofollow" target="_blank"> SigOpt </a></figcaption></figure><h1 id="7a20" class="jw jx hi bd ju jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">摘要</h1><p id="a80a" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">贝叶斯优化方法是高效的，因为它们以<em class="jd">知情的方式</em> <strong class="ih hj">选择超参数。</strong>通过对从过去的结果看起来更有希望的超参数进行优先排序，贝叶斯方法可以在比网格搜索和随机搜索更少的时间(更少的迭代)内找到最佳超参数。</p></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><h1 id="7b4d" class="jw jx hi bd ju jy ll ka kb kc lm ke kf kg ln ki kj kk lo km kn ko lp kq kr ks bi translated">选择最佳超参数</h1><p id="792a" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">我们已经讨论了很多关于<em class="jd">最佳</em>超参数的内容。但是我们如何选择最好的呢？哪个度量用于区分好的和坏的超参数？</p><p id="5429" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，最简单的方法如下:</p><ol class=""><li id="e459" class="mu mv hi ih b ii ij im in iq mw iu mx iy my jc mz na nb nc bi translated">将数据集分为训练集(70%)和验证集(30%)。</li><li id="69df" class="mu mv hi ih b ii nd im ne iq nf iu ng iy nh jc mz na nb nc bi translated">选择任何超参数调整算法—网格搜索、随机搜索或贝叶斯优化。</li><li id="1354" class="mu mv hi ih b ii nd im ne iq nf iu ng iy nh jc mz na nb nc bi translated">决定并创建您希望优化的超参数列表。</li><li id="06c0" class="mu mv hi ih b ii nd im ne iq nf iu ng iy nh jc mz na nb nc bi translated">训练多个模型-为每个超参数值组合训练一个模型。</li><li id="ece6" class="mu mv hi ih b ii nd im ne iq nf iu ng iy nh jc mz na nb nc bi translated">计算每个模型的验证集准确性。</li><li id="b3e5" class="mu mv hi ih b ii nd im ne iq nf iu ng iy nh jc mz na nb nc bi translated">选择在验证集上达到最高准确度的模型。</li></ol><p id="b59f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是您的<em class="jd">最佳模型</em>，用于训练该模型的超参数是您的<em class="jd">最佳超参数</em>。</p><h2 id="8670" class="lq jx hi bd ju lr ls lt kb lu lv lw kf iq lx ly kj iu lz ma kn iy mb mc kr md bi translated">交叉验证</h2><p id="5907" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">虽然上述方法是合理的，但是将数据集直接拆分为训练集和验证集是不可取的，因为您可能会丢失进入验证集且从未用于训练的数据点中的许多重要信息。为了避免这种情况，我们通常使用<strong class="ih hj">交叉验证(CV) </strong>来调整我们的超参数。</p><blockquote class="ky kz la"><p id="7919" class="if ig jd ih b ii ij ik il im in io ip lb ir is it lc iv iw ix ld iz ja jb jc hb bi translated">Scikit-learn是一个流行的库，它提供了GridSearchCV、RandomizedSearchCV和BayesSearchCV的现成实现。</p></blockquote><p id="327f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管我们很想通过使用交叉验证的超参数调整来提高我们的模型性能，但重要的是要记住，它会显著地<strong class="ih hj">增加总的</strong> <strong class="ih hj">模型训练时间</strong>。</p></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><p id="07df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我列出了一些免费和开源的超参数优化库。一定要去看看！</p><ul class=""><li id="1be7" class="mu mv hi ih b ii ij im in iq mw iu mx iy my jc ni na nb nc bi translated"><a class="ae mo" href="https://optuna.org/" rel="noopener ugc nofollow" target="_blank"> Optuna </a></li><li id="1c19" class="mu mv hi ih b ii nd im ne iq nf iu ng iy nh jc ni na nb nc bi translated"><a class="ae mo" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank">远视</a></li><li id="874a" class="mu mv hi ih b ii nd im ne iq nf iu ng iy nh jc ni na nb nc bi translated"><a class="ae mo" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a></li><li id="ec51" class="mu mv hi ih b ii nd im ne iq nf iu ng iy nh jc ni na nb nc bi translated"><a class="ae mo" href="https://github.com/HIPS/Spearmint" rel="noopener ugc nofollow" target="_blank">留兰香</a></li><li id="32e7" class="mu mv hi ih b ii nd im ne iq nf iu ng iy nh jc ni na nb nc bi translated"><a class="ae mo" href="https://scikit-optimize.github.io/#skopt.Optimizer" rel="noopener ugc nofollow" target="_blank">sci kit-优化</a></li></ul></div></div>    
</body>
</html>