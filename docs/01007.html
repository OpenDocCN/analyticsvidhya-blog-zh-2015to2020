<html>
<head>
<title>The Learning Problem: Comparison between Brain and Machine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习问题:大脑和机器的比较</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-learning-problem-comparison-between-brain-and-machine-339c8de6635a?source=collection_archive---------19-----------------------#2019-09-24">https://medium.com/analytics-vidhya/the-learning-problem-comparison-between-brain-and-machine-339c8de6635a?source=collection_archive---------19-----------------------#2019-09-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="bc3a" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">第一章:神经元和感知器</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/456d15655679b8a99e449346f7a4f266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7rjzz-2nBK7vhOwF8qSkCw.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">左边来源:<a class="ae jn" href="https://www.extremetech.com/wp-content/uploads/2016/01/connectome.jpg" rel="noopener ugc nofollow" target="_blank">https://www . extreme tech . com/WP-content/uploads/2016/01/connectome . jpg</a>；右边:<a class="ae jn" href="https://clusterdata.nl/wp-content/uploads/2018/01/maxresdefault-1.jpg" rel="noopener ugc nofollow" target="_blank">https://cluster data . nl/WP-content/uploads/2018/01/maxresdefault-1 . jpg</a>—本人修改</figcaption></figure><p id="f158" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">这是致力于从根本上分析和分解学习问题的系列文章的第一章。在这样做的时候，我想强调生物领域和人工领域之间的比较。特别是，作为一名物理学家，我对理解涌现和自组织现象感兴趣，这些主题通常与复杂系统相关(例如大脑或深度神经网络)。因此，中心思想将是:展示如何以自下而上的方式应用充分理解的解决方案(例如，单个神经元),以理解复杂的情况。为了建立一个坚实的结构来解决这个问题，我们需要两个工具:数学建模和模拟(又名编码)。我将从头开始讲述这一切，让我们开始吧！</strong></p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><blockquote class="kr ks kt"><p id="f5f2" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated">如果你不能解决一个问题，那么有一个更简单的问题你可以解决:找到它。(<strong class="jq hj"> George Pólya，关于理解、学习和教学问题解决的数学发现)</strong></p></blockquote><p id="c976" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">物理学传统上忠实于这一理念:复杂的情况优先在简单的解决方案之上进行研究。甚至在更抽象的层面上，在讨论与认知过程的约束的一般性质有关的问题时，物理学家可能会觉得，例如，自然语言对于一个起点来说是太复杂的主题。他很可能试图构建一个越来越复杂的结构，这个结构由具有认知味道的简单过程的明确实现组成。<em class="ku">选择这些阶段的一个主要标准是它们的可分析性。它们的性质可以用非抽象的方式来研究，避免了复杂性的模糊性所带来的神秘结论。</em></p><p id="feaa" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这样，为了理解学习问题，我们应该处理它的基本单元。特别是，我们将比较<em class="ku">神经元</em>和<em class="ku">感知器</em>。让我们继续第一个。</p><h2 id="b852" class="ky kz hi bd la lb lc ld le lf lg lh li jx lj lk ll kb lm ln lo kf lp lq lr ls bi translated">神经元:理解复杂性的生物候选者[1]</h2><p id="4eec" class="pw-post-body-paragraph jo jp hi jq b jr lt ij jt ju lu im jw jx lv jz ka kb lw kd ke kf lx kh ki kj hb bi translated">对生物学背景的简短描述是必要的，尽管在未来很长一段时间内，它不可能超越<a class="ae jn" href="https://neurology.mhmedical.com/book.aspx?bookid=1049" rel="noopener ugc nofollow" target="_blank">艾瑞克·坎德尔的</a>对神经科学的描述(因此，你可以查看这篇杰出的文章以获得该领域的总体概述)。这里将只总结那些对于模型的构造是必不可少的特征。</p><p id="173b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">基本元素自然是<em class="ku">神经元</em>和<em class="ku">突触</em>。人类神经系统中有相当多种类的神经元——在大小、结构和功能上都有差异。作为上下文的选择，为了当前的目的，我们将考虑“典型”类型的神经元。如果基本原理取决于单个神经元的结构，物理学就不太可能对它们的阐明有多大贡献。超过一定水平，复杂的功能必然是大量简单元素相互作用的结果。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ly"><img src="../Images/6976e392a991b3633a9d9263e8ad0258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OAupKc8QH6G36LDzKE8-9A.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图1A来源:<a class="ae jn" href="https://www.forbes.com/sites/andreamorris/2018/08/27/scientists-discover-a-new-type-of-brain-cell-in-humans/" rel="noopener ugc nofollow" target="_blank">https://www . Forbes . com/sites/Andrea Morris/2018/08/27/scientists-discover-a-new-type-of-brain-cell-in-humans/</a>—本人修改</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/7d8c84612eb818bfe46510a1206780c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MzlrxSlV-ryQDAJtOVoBLQ.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图1B来源:<a class="ae jn" href="https://ib.bioninja.com.au/standard-level/topic-6-human-physiology/65-neurons-and-synapses/neurons.html" rel="noopener ugc nofollow" target="_blank">https://IB . bio ninja . com . au/standard-level/topic-6-human-physiology/65-neurons-and-synapses/neurons . html</a></figcaption></figure><p id="03ba" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">图1描绘了一个神经元及其示意图。神经元通过突触进行通信，突触是沿着<em class="ku">突触前</em>神经元的<em class="ku">轴突</em>的点，在这些点上，神经元可以将已经在其<em class="ku">胞体</em>中执行的计算结果传递给<em class="ku">树突</em>或者甚至直接传递给<em class="ku">突触后</em>神经元的胞体。输出部分是轴突。通常只有一个轴突离开胞体，然后在下游重复分支，与许多突触后神经元沟通。</p><p id="f9f0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，从我们的角度来看，神经元和突触的<em class="ku">动力学</em>，非常类似于电信号在电缆中的传播，基于以下序列:</p><ul class=""><li id="6ef0" class="ma mb hi jq b jr js ju jv jx mc kb md kf me kj mf mg mh mi bi translated">神经轴突处于全有或全无状态。在第一种状态下，它基于在胞体中对来自树突的信号进行求和的结果，传播信号<em class="ku">尖峰</em>，或<em class="ku">动作电位</em>。传播信号的形状和振幅非常稳定，并在轴突的分支点复制。此外，轴突中行进冲动的存在阻断了第二次冲动传递的可能性。</li><li id="222d" class="ma mb hi jq b jr mj ju mk jx ml kb mm kf mn kj mf mg mh mi bi translated">当行进信号到达轴突的末端时，它导致<em class="ku">神经递质— </em>我们的<em class="ku"> </em>先兆<em class="ku"> — </em>分泌到突触末端。</li><li id="7672" class="ma mb hi jq b jr mj ju mk jx ml kb mm kf mn kj mf mg mh mi bi translated">神经递质穿过突触到达突触后神经元膜，并与<em class="ku">受体</em>结合，从而导致后者打开，允许离子电流渗透。</li><li id="4887" class="ma mb hi jq b jr mj ju mk jx ml kb mm kf mn kj mf mg mh mi bi translated">突触后电位(<strong class="jq hj"> PSP </strong>)以分级的方式向细胞体扩散，在细胞体中，来自所有连接到突触后神经元的突触前神经元的输入被相加。如果在短时间内到达的PSP的总和超过某个<em class="ku">阈值</em>，尖峰发射的概率变得很大。</li></ul><p id="7e5d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在发放尖峰信号之后，神经元需要时间来恢复。我们将这段时间命名为<em class="ku">绝对不应期</em>，在此期间，神经元不能发出第二个棘波(大自然以这种方式设定了最大棘波频率，<em class="ku">事实上</em>限制了神经元在固定时间内可以处理的信息量)。</p><p id="7d2f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">前面的描述表明，神经元将计算结果传递给其他神经元的唯一方式是通过神经递质的释放。为了数学建模，我们将做一个强有力的假设，即:<strong class="jq hj">阈下电位不会导致神经递质</strong>的释放。换句话说，神经递质仅由尖峰释放。</p><p id="0eee" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">既然我们已经对神经元的功能有了基本的介绍，我们可以看看一个数学模型，即<em class="ku">漏积分点火</em> ( <strong class="jq hj"> LIF </strong>)神经元。对于那些对这个模型背后的“重”数学感兴趣的人，我将简要地介绍它。</p><h2 id="ed47" class="ky kz hi bd la lb lc ld le lf lg lh li jx lj lk ll kb lm ln lo kf lp lq lr ls bi translated">从数学角度看漏积分发射模型[2]</h2><p id="da01" class="pw-post-body-paragraph jo jp hi jq b jr lt ij jt ju lu im jw jx lv jz ka kb lw kd ke kf lx kh ki kj hb bi translated">处理神经元模型的标准方法是<em class="ku">电路模拟</em>。这是相当技术性的，但我会尽量保持低水平。对于你们中已经学过<em class="ku">电子学</em> <em class="ku"> 101 </em>的人来说，这只是一个简单的𝑅𝐶电路的快速回顾。</p><p id="c8a9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们之前一直在谈论发生在体细胞中的求和(有时称为<em class="ku">整合</em>)过程，它与触发超过某个临界阈值的动作电位的机制相结合，是神经元动力学的核心。</p><p id="c376" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们深入数学，以便建立一个神经元动力学的现象学模型。我们把临界电压描述为正式的<em class="ku">阈值</em> 𝜽.如果<em class="ku">电压</em> 𝑢(𝑡(所有输入的总和)从下面到达𝜽，我们说神经元触发了一个尖峰。在这个模型中，我们有两个不同的组成部分，它们都是定义动力学所必需的:首先，一个描述潜在𝑢(𝑡演化的方程；第二，产生尖峰信号的机制。</p><p id="e47a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下面是积分点火模型类中最简单的模型，由两部分组成:描述𝑢(𝑡演化的线性微分方程和尖峰脉冲点火的阈值。</p><p id="f483" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">变量𝑢(𝑡)描述了我们的神经元电位的即时值。在没有任何输入的情况下，电位处于静止状态𝑣。如果神经元接收到一个输入(一个<em class="ku">电流</em> ) 𝐼(𝑡)，电位𝑢(𝑡将偏离其静止值。</p><p id="d03d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了得出将瞬时电压𝑢(𝑡-𝑣与输入电流𝐼(𝑡联系起来的等式，我们使用电学理论中的基本定律。如果将电流脉冲(𝐼(𝑡)注入神经元，额外的电荷将使细胞膜带电。细胞膜将因此充当<em class="ku">电容</em> 𝐶.的<em class="ku">电容</em>电荷会慢慢地从细胞膜中漏出，因为这个梯子不是一个完美的绝缘体。我们可以通过在我们的模型中增加一个有限泄漏电阻𝑅来考虑这一点。</p><p id="3ac9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">代表泄漏积分点火模型的基本电路由电容器𝐶 <em class="ku">与由电流𝐼(𝑡驱动的电阻器𝑅</em>并联组成)；参见图2</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mo"><img src="../Images/b7a35436e1ecf25fa7b9b80a517161cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JvFPiUaBjkaNgPbIbuoMFw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图2 —来源:<a class="ae jn" href="https://neuronaldynamics.epfl.ch/online/Ch1.S3.html" rel="noopener ugc nofollow" target="_blank">https://neuronaldynamics.epfl.ch/online/Ch1.S3.html</a>。左边:被细胞膜(大圆圈)包围的神经元接收(正)输入电流𝐼(𝑡，这增加了细胞内的电荷。相应的电路显示在底部。右边:细胞膜对阶跃电流的反应(上)和平滑的电压信号(下)</figcaption></figure><p id="411e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了分析电路，我们使用<em class="ku">电流守恒定律</em>并将电流分成两部分:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mp"><img src="../Images/8c87637334db38fee9e453ad33ea835d.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*sgPELoaX6or6EhbDexCCVw@2x.png"/></div></figure><p id="5479" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">第一个分量是通过线性电阻𝑅的电流，它可以通过<em class="ku">欧姆定律</em>计算得出。第二个元件对电容器𝐶.充电因此，通过对电容器使用<em class="ku">欧姆定律</em>和<em class="ku">电流-电压关系</em>，我们得到:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mq"><img src="../Images/3759066c5ffe05e19a3deb79a47e3d76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/1*7x6bwRLR4ML5bqQzJBCNUg@2x.png"/></div></div></figure><p id="a1e5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">幸运的是，在𝑢(𝑡，这是一个线性微分方程，很容易求解，特别是如果我们考虑一个恒定的输入电流𝐼(𝑡=𝑖，从𝑡 = 0开始，到时间𝑡 = 𝚫.结束为了简单起见，我们假设在时间𝑡 = 0时膜电位处于其静止值𝑢(0) = 𝑣.</p><p id="b42f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">0 </p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mr"><img src="../Images/1a2c3b234db8514c464aa86cbf25faa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*Enr-764r8vXOso3i8iIaQg@2x.png"/></div></figure><p id="155c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">If the input current never stopped, the potential would approach for 𝑡→ ∞ to the asymptotic value 𝑢(∞)=𝑣 +𝑅𝑖. We can understand this result by taking a look at Figure2 right-side bottom. Once a plateau is reached, the charge on the capacitor no longer changes. All input current must then flow through the resistor. Additionally, for notation purposes we usually denote <a class="ae jn" href="https://en.wikipedia.org/wiki/RC_time_constant" rel="noopener ugc nofollow" target="_blank"> <em class="ku"> RC </em>的解为𝜏 </a>，实际上是我们电路的时间常数。现在我们已经介绍了我们的成分，是时候开始烘烤一些代码了。</p><h2 id="88d9" class="ky kz hi bd la lb lc ld le lf lg lh li jx lj lk ll kb lm ln lo kf lp lq lr ls bi translated">让我们编码一个泄漏的整合和发射神经元</h2><p id="93d4" class="pw-post-body-paragraph jo jp hi jq b jr lt ij jt ju lu im jw jx lv jz ka kb lw kd ke kf lx kh ki kj hb bi translated">我们将使用<a class="ae jn" href="https://brian2.readthedocs.io/en/stable/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="ku"> Brian2 </em> </a>，这是一个非常高效的<em class="ku"> Python </em>库，用于模拟尖峰神经网络(即生物神经网络)，您可以按照官方<a class="ae jn" href="https://brian2.readthedocs.io/en/stable/introduction/install.html" rel="noopener ugc nofollow" target="_blank">说明</a>轻松安装。</p><p id="5593" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我经常使用<a class="ae jn" href="https://jupyter.org" rel="noopener ugc nofollow" target="_blank"> <em class="ku"> Jupyter笔记本</em> </a> <em class="ku"> </em>来运行<em class="ku"> Python </em>脚本:这是一个交互式环境，让你编码并集成<em class="ku"> Markdown </em>和各种专门为科学计算和机器学习设计的有用插件。</p><p id="8c4e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一旦你准备好安装，你可以开始导入Brian2。</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="5488" class="ky kz hi mt b fi mx my l mz na">import brian2 as br2</span></pre><p id="9730" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">之后，我们将定义我们模型的一些全局参数:<em class="ku"> N </em>是神经元的数量；<em class="ku">τ</em>是先前定义的电路时间常数(即𝜏=<em class="ku">RC)；v_r </em>为静息膜电位(即𝑣)；<em class="ku"> I_c </em>为恒定输入电流(即𝑖)；<em class="ku"> v_th </em>为尖峰临界电压(即阈值𝜗)</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="f387" class="ky kz hi mt b fi mx my l mz na">N = 1</span><span id="20a6" class="ky kz hi mt b fi nb my l mz na">tau  = 10 *br2.ms<br/>v_r = 0 *br2.mV<br/>I_c = 18 * br2.mV</span><span id="2a26" class="ky kz hi mt b fi nb my l mz na">#v_th is a string, it's going to be clear in a while<br/>v_th = "v &gt; 15*mV"</span></pre><p id="c57b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">你应该注意到，在定义变量时，我们也设置了物理单位。</p><p id="2b8e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，我们可以勾画出动力学:<em class="ku"> Brian2 </em>让你写描述你的模型的方程，你也必须指定单位(我知道电流在这里显然是用<em class="ku">伏特</em>来测量的——这是一个挑战，但一切都是为了一致性)</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="6d95" class="ky kz hi mt b fi mx my l mz na">eqs = '''<br/>dv/dt = -(v-I)/tau : volt<br/>I : volt<br/>'''</span></pre><p id="5b36" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">通过调用<a class="ae jn" href="https://brian2.readthedocs.io/en/stable/reference/brian2.groups.neurongroup.NeuronGroup.html?highlight=NeuronGroup" rel="noopener ugc nofollow" target="_blank"> <em class="ku">神经元组</em> </a>，我们正在创建一组神经元(这里我们有<em class="ku"> 1 </em>神经元)，它们具有之前定义的动力学、<em class="ku"> v_r </em>和<em class="ku"> I_c </em>作为我们神经元的特征。为了记录它的活动，我们调用<a class="ae jn" href="https://brian2.readthedocs.io/en/stable/reference/brian2.monitors.statemonitor.StateMonitor.html?highlight=StateMonitor" rel="noopener ugc nofollow" target="_blank"> <em class="ku"> StateMonitor </em> </a>，<em class="ku"> v_trace </em>可以用来提取时间和电压值，我们很快就会看到。</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="8946" class="ky kz hi mt b fi mx my l mz na">lif = br2.NeuronGroup(N, model = eqs, threshold =v_th, reset = 'v = 0*mV')<br/>lif.v = v_r   <br/>lif.I = I_c</span><span id="2c8a" class="ky kz hi mt b fi nb my l mz na">v_trace = br2.StateMonitor(lif, 'v', record = True)</span></pre><p id="4dbd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后一步是运行模拟并绘制结果。如前所述，我们从<em class="ku"> v_trace </em>中提取时间作为我们的x轴和电压(<em class="ku"> v[0] </em>指的是第一个——也是唯一一个——神经元)。你必须小心物理单位——再一次——并且记住为了绘图我们需要一个维度数据。</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="c8d9" class="ky kz hi mt b fi mx my l mz na">br2.run(0.1*br2.second)</span><span id="69e4" class="ky kz hi mt b fi nb my l mz na">br2.figure(1)<br/>br2.plot(v_trace.t[:]/br2.ms, v_trace.v[0]/br2.mV)<br/>br2.xlabel('Time (ms)', fontsize = 24)<br/>br2.ylabel('v (mV)', fontsize = 24)<br/>br2.yticks([0,4,8,12,16])<br/> <br/>br2.show()</span></pre><p id="311a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">结果就是一连串的尖峰信号。你应该使用参数来观察不同种类的行为。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/2c2c22f939357eafdaeb245a3836dd75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Cnny-_CcfQElvCaTlWmgLQ.png"/></div></figure><p id="ef9e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们已经描述了一个相当生物学的尖峰神经元模型，但是从这个领域到人工领域的过渡是什么呢？再来说说<em class="ku">感知器。</em></p><h2 id="931e" class="ky kz hi bd la lb lc ld le lf lg lh li jx lj lk ll kb lm ln lo kf lp lq lr ls bi translated">感知机:理解复杂性的人工候选者[3] [4]</h2><p id="7afd" class="pw-post-body-paragraph jo jp hi jq b jr lt ij jt ju lu im jw jx lv jz ka kb lw kd ke kf lx kh ki kj hb bi translated">通过继续使用复杂系统的范式，从明斯基和Papert的书:</p><blockquote class="kr ks kt"><p id="005f" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated">尽管我们没有同样详尽的“学习”理论，但我们至少可以证明，在“学习”、“适应”或“自组织”确实发生的情况下，它的发生可以被彻底阐明，并且不带有复杂系统的神秘的鲜为人知的原则的暗示。是否有这样的原则我们无法知道。但是感知机没有提供证据；我们分析它的成功为这个论点增加了另一个旁证，即起作用的控制论过程是可以理解的，而那些不能理解的过程是可疑的。</p></blockquote><p id="5201" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">正如上面的引文所暗示的和图灵所大力提倡的，心理现象只不过是一个非常复杂的结构在相对简单的过程上运作的表达。这是大脑正式化的第一个想法。</p><p id="d9a2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这样看来，感知机是人工智能的第一块砖。我们将形式化生物神经元，<em class="ku">事实上</em>得到感知器，然后引入<em class="ku">罗森布拉特的感知器学习算法。</em></p><p id="6c48" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们现在关注单个神经元的<em class="ku">逻辑</em>结构。前面几节的描述提出了以下方案:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nd"><img src="../Images/375951b858cec5f3c8fd45b3eb6f3a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*ZFlzROot9R3M89WGCRI0vg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图3</figcaption></figure><ul class=""><li id="0345" class="ma mb hi jq b jr js ju jv jx mc kb md kf me kj mf mg mh mi bi translated">有一个<em class="ku">处理单元</em>，大圆圈，代表的是躯体。</li><li id="ac16" class="ma mb hi jq b jr mj ju mk jx ml kb mm kf mn kj mf mg mh mi bi translated">许多输入线在逻辑上连接到soma，如图3中的输入箭头所示。它们代表树突和突触。</li><li id="a47c" class="ma mb hi jq b jr mj ju mk jx ml kb mm kf mn kj mf mg mh mi bi translated">输入通道由它们从与之相连的输入变量(<strong class="jq hj"> <em class="ku"> x </em> </strong>)接收的信号激活。这些变量是我们的突触前轴突，它们具有内在的<em class="ku">逻辑</em>性质，因为它们可以激活通道(携带尖峰信号)或不激活它(突触前神经元中的亚阈值活动)。</li></ul><p id="60f6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于每一个输入行，我们关联一个参数<em class="ku">w</em>——下标指的是不同的输入通道。每个<em class="ku"> w </em>的数值实际上是突触后电位(<strong class="jq hj"> PSP </strong>)的数量，如果通道被激活，这些电位将被添加到细胞体。</p><p id="ed96" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">此外，还有一条单独的<em class="ku">逻辑</em>输出线(图3中的输出箭头)。它表达了一个逻辑事实，即我们的神经元产生一个相关的输出——一个尖峰。</p><p id="50fe" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们可以按以下方式安排该单元的操作:</p><ul class=""><li id="cc8d" class="ma mb hi jq b jr js ju jv jx mc kb md kf me kj mf mg mh mi bi translated">在给定的时刻，一些逻辑输入被激活。</li><li id="7cd2" class="ma mb hi jq b jr mj ju mk jx ml kb mm kf mn kj mf mg mh mi bi translated">soma接收一个输入，该输入是已激活通道的<strong class="jq hj"> PSP </strong>值的线性和——变量<em class="ku"> x </em>指示通道是激活(<em class="ku"> x = 1 </em>)还是非激活(<em class="ku"> x = -1 </em>)</li><li id="6ba9" class="ma mb hi jq b jr mj ju mk jx ml kb mm kf mn kj mf mg mh mi bi translated">将<strong class="jq hj"> PSP </strong>的总和与神经元的阈值进行比较，如果超过阈值，则输出通道被激活。</li></ul><p id="8779" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">形式上，我们会得到:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ne"><img src="../Images/7e42f06d60e33969b7317d1221b6914c.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*e8iI2MGv8PU2yBLBbAGOOg@2x.png"/></div></figure><p id="57ad" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">用<em class="ku"> h </em>作为我们神经元的PSP，用<em class="ku"> n </em>作为突触前神经元的数量。数学上<em class="ku"> h </em>不过是<a class="ae jn" href="https://en.wikipedia.org/wiki/Dot_product" rel="noopener ugc nofollow" target="_blank"> <em class="ku">点积</em> </a>。</p><p id="046c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">无论如何，由我们的小“机器”实现的操作可以表示为:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/16512b7c383ec3475bf3973447d5641b.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*CXKtLDLRx30UOf5wLBBuYg@2x.png"/></div></figure><p id="54e4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">用<em class="ku">符号</em>作为<a class="ae jn" href="https://en.wikipedia.org/wiki/Sign_function" rel="noopener ugc nofollow" target="_blank">符号函数</a>，𝜽作为我们神经元的阈值。<em class="ku"> H </em>基本上是<em class="ku"> 1 </em>如果它的自变量(<em class="ku"> h+ </em> 𝜽)为正，为负则为-1——更准确地说，是<em class="ku"> 0 </em>如果<em class="ku"> h + </em> 𝜽 = 0。在我们的生物学类比中，变量<em class="ku"> y </em>表示在输出轴突中是否会出现尖峰。</p><p id="a50b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">既然我们的感知机已经成型，我们就可以讨论这个数学结构如何参与学习过程。下面介绍一下<em class="ku">罗森布拉特的感知器学习算法。</em></p><h2 id="4ceb" class="ky kz hi bd la lb lc ld le lf lg lh li jx lj lk ll kb lm ln lo kf lp lq lr ls bi translated">感知器的学习过程[4]</h2><p id="5465" class="pw-post-body-paragraph jo jp hi jq b jr lt ij jt ju lu im jw jx lv jz ka kb lw kd ke kf lx kh ki kj hb bi translated">神经元和感知器之间有一个明显的相似之处，但是我们如何使用这个阶梯模型来学习呢？我们将展示感知器可以用来解决<em class="ku">分类</em>问题，即它可以告诉你，<em class="ku">如果我们有两组点</em>，一个点属于一组还是另一组。我们可以说，这个问题可以被认为是一个二元(是/否)决策，其中上面定义的变量<em class="ku"> y </em>(输出值)是这两个二元值中的一个(<em class="ku"> yes = 1 </em>，<em class="ku"> no = -1 </em>)。</p><p id="86d8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">根据前面将<em class="ku"> h </em>定义为点积，更清楚的是<strong class="jq hj"> <em class="ku"> x </em> </strong>形式上是一个向量(输入向量<em class="ku"/>)。<strong class="jq hj"> <em class="ku"> x </em> </strong>的每一个部件，用机器学习的术语来讲，叫做<em class="ku">特征、</em>。同理<strong class="jq hj"> <em class="ku"> w </em> </strong>是矢量<em class="ku">重量</em>矢量<em class="ku">。阈值项𝜽通常被称为<em class="ku">偏置项</em>。在简化符号的方向上，我们将按照以下方式对<em class="ku">和</em>进行扩充，包括<strong class="jq hj"> <em class="ku"> x </em> </strong>和<strong class="jq hj"> <em class="ku"> w </em> </strong>:</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/a825a582432aeb4de309be0f3f81f52a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*CXeNkMeP5mLJ2eHx60Lukg@2x.png"/></div></figure><p id="1f57" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">基本上就是通过将求和从<em class="ku"> i = 0 </em>扩展到<em class="ku"> i = n </em>。</p><p id="fe25" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们将要构建的学习算法将考虑所有这些术语，特别是术语<em class="ku">学习</em>特别指的是找到<strong class="jq hj"> <em class="ku"> w </em> </strong>分量的最佳值，以便实现可能的最佳分类。看看下面图4中的<em class="ku">二维</em>(此处<strong class="jq hj"> <em class="ku"> x </em> </strong>只有两个分量)问题就都清楚了。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nh"><img src="../Images/50f3411018f63ccacd57cb4a44438c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yF4wXHHEb1zrZLy8KXUryQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图4——左边:错误分类的数据。右边:完全保密的数据。</figcaption></figure><p id="1c63" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，在实践中，分类意味着找到一种方法来区分我们拥有的两种不同“类型”的数据。每种数据由不同的<em class="ku">标签</em>(上图中的加号或减号)指定。可以说，这是一个<em class="ku">两个标签</em>的分类问题。</p><p id="f548" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">是时候介绍一下<em class="ku">感知器学习算法</em> ( <strong class="jq hj"> PLA </strong>)，它会根据数据确定<strong class="jq hj"><em class="ku"/></strong>应该是什么样的。这种学习算法包括简单的<em class="ku">迭代</em>方法。通过涉及术语“迭代”，我们将在我们的模型中引入一个新参数，即<em class="ku">时间</em>。在迭代<em class="ku"> t </em>时，其中<em class="ku"> t </em> <em class="ku"> = 0，1，2 </em> …，有一个<em class="ku">权向量</em>的当前值，称之为<strong class="jq hj"> <em class="ku"> w </em> </strong> <em class="ku"> (t)。</em>算法挑选一个点，与其标签相关联，当前被错误分类，调用它(<strong class="jq hj"> x </strong> <em class="ku"> (t) </em>，<em class="ku"> y(t) </em>)，并使用它来更新<strong class="jq hj"> <em class="ku"> w </em> </strong> <em class="ku"> (t)。</em>由于例子分类错误，我们有<em class="ku">y(t)≠sign(</em><strong class="jq hj"><em class="ku">w</em></strong><em class="ku">(t)</em><strong class="jq hj"><em class="ku">x</em></strong><em class="ku">(t))，</em>其中点代表点积。</p><p id="b018" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="ku">权重更新规则</em>如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ni"><img src="../Images/4e1214ced39d22196b764865ccac06e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*P7wg-Gqpf00Rni3rIPoTjA@2x.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nh"><img src="../Images/dd8a222e5c76d8cdb96cb52deedd5e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JpApwtU4mYTOQNdIwiSC-g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图5 —最后一次更新</figcaption></figure><p id="1f62" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">该规则将边界向正确分类<strong class="jq hj"><em class="ku">x</em></strong><em class="ku">(t)</em>的方向移动，如上图所示。该算法继续进一步迭代，直到数据集中不再有错误分类的点。有了这个关于<strong class="jq hj"> PLA </strong>的直观视图，现在是时候编写代码了。</p><h2 id="5f5c" class="ky kz hi bd la lb lc ld le lf lg lh li jx lj lk ll kb lm ln lo kf lp lq lr ls bi translated">让我们编写感知器学习算法</h2><p id="08c3" class="pw-post-body-paragraph jo jp hi jq b jr lt ij jt ju lu im jw jx lv jz ka kb lw kd ke kf lx kh ki kj hb bi translated">我们将通过编写一个简单的<em class="ku"> Python </em>脚本从头开始实现<strong class="jq hj"> PLA </strong>。让我们用<em class="ku"> Jupyter </em>打开一个新的笔记本，开始导入<em class="ku"> matplotlib.pyplot </em>和<em class="ku"> numpy </em></p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="50df" class="ky kz hi mt b fi mx my l mz na">import matplotlib.pyplot as plt<br/>import numpy as np </span></pre><p id="b5c5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们将使用<a class="ae jn" href="https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.rand.html" rel="noopener ugc nofollow" target="_blank"><em class="ku">numpy . rand(</em></a><em class="ku">)创建一个由<em class="ku"> 5个</em>点组成的二维随机数据集(有两个可能的标签，<em class="ku"> -1 </em>和<em class="ku"> +1 </em>)。</em>为了重现结果，我们将固定随机种子——你实际上可以选择每一个整数，<a class="ae jn" href="https://en.wikipedia.org/wiki/137_(number)#In_physics" rel="noopener ugc nofollow" target="_blank"> <em class="ku"> 137 </em>对于物理学家来说有着特殊的意义</a>。</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="4eb3" class="ky kz hi mt b fi mx my l mz na"># Setting the random seed <br/>np.random.seed(seed = 137)</span><span id="fa98" class="ky kz hi mt b fi nb my l mz na"># Generate x1 and x2, coordinates of our points<br/>number_of_points = 5<br/>x1 = np.random.rand(number_of_points)<br/>x2 = np.random.rand(number_of_points)</span><span id="e884" class="ky kz hi mt b fi nb my l mz na"># We have two labels, namely -1 and 1<br/>possible_ys = np.array([-1,1])</span><span id="9f1b" class="ky kz hi mt b fi nb my l mz na"># We randomly build the label y to point (x1,x2) association <br/>y = np.random.choice(possible_ys, number_of_points)</span></pre><p id="5e83" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">数据由三元组值表示。</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="c6d5" class="ky kz hi mt b fi mx my l mz na"># We create data as triplets of values<br/>data = []<br/>for i in range(number_of_points):<br/>    data.append((x1[i],x2[i],y[i]))</span></pre><p id="30c4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">您可以通过执行下面一行来查看您的数据，您将得到:</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="4cb3" class="ky kz hi mt b fi mx my l mz na"># Taking a look at data<br/>data</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nj"><img src="../Images/b300c509128fe46e58749053a3b1792b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W4RkEoavMx58zuSOYV0yWA.png"/></div></div></figure><p id="7128" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下一步是绘制它们，用“-”表示对应于“-1”的标签，用“+”表示对应于“1”的标签</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="976a" class="ky kz hi mt b fi mx my l mz na"># Plotting our data <br/>plt.plot([x1 for (x1,x2,y) in data if y==-1], [x2 for (x1,x2,y) in data if y==-1], '_', mec='r', mfc='none')<br/>plt.plot([x1 for (x1,x2,y) in data if y==1], [x2 for (x1,x2,y) in data if y==1], '+', mec='b', mfc='none')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nk"><img src="../Images/db6a11ea53f7fd6324ab5da0f8e0270d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8_R1z2kQdioF7OQjHVBxFg.png"/></div></div></figure><p id="9cb3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">之后，我们可以从学习模型开始。首先要做的是初始化权重，通常一个“小”的随机值是更快收敛的最佳解决方案。</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="5218" class="ky kz hi mt b fi mx my l mz na"># Initializing the weight vector<br/>w = np.random.rand(3)*10e-03</span></pre><p id="6220" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，我们定义函数"<em class="ku"> predict" </em>，它只不过是<em class="ku">符号(</em><strong class="jq hj"><em class="ku">w</em></strong><em class="ku">(t)</em><strong class="jq hj"><em class="ku">x</em></strong><em class="ku">(t))</em></p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="aa2d" class="ky kz hi mt b fi mx my l mz na">def predict(x1,x2):<br/>    # w[0] is the threshold value, x0 = 1 <br/>    h= w[0] + w[1]*x1 + w[2]*x2<br/>    <br/>    if h&lt;0:<br/>        return -1<br/>    else:<br/>        return 1</span></pre><p id="8f64" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">学习模型的最后一部分是对应于<em class="ku">权重更新规则</em>的函数“fit”。本质是:对于每个点，我们将预测标签与实际标签进行比较，如果它们不相等，则更新<em class="ku">权重向量</em>的每个分量。</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="89da" class="ky kz hi mt b fi mx my l mz na">def fit(data):<br/>    stop = False<br/>    <br/>    while stop == False:<br/>        stop = True<br/>        <br/>        for x1,x2,y in data:        <br/>            ypredict= predict(x1,x2) <br/>            if y != ypredict: <br/>                              <br/>                stop = False<br/>            <br/>                w[1]= w[1] + x1*y<br/>                w[2]= w[2] + x2*y<br/>                w[0]= w[0] + y </span></pre><p id="e0ad" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">通过对我们的数据应用函数“<em class="ku">fit</em>”,<strong class="jq hj">PLA</strong>将收敛到一个解。如果是这种情况，我们将得到一个打印的“成功！”作为下一个单元格的输出。</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="6c89" class="ky kz hi mt b fi mx my l mz na">fit(data) </span><span id="6372" class="ky kz hi mt b fi nb my l mz na"># Check if the model is predicting correct labels<br/>for (x1, x2, y) in data:<br/>    if predict(x1,x2) != y:<br/>        print('FAIL')<br/>        break<br/>else:<br/>    print('SUCCESS!')</span></pre><p id="a81f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后但同样重要的是，我们可以绘制我们的结果。"<em class="ku"> f(x) </em>"是由<strong class="jq hj"> PLA </strong>给出的线的函数形式(你可以通过应用简单的代数计算得到——通过解释<em class="ku"> x2 </em>的表达式得到<em class="ku">事实上</em>)。</p><pre class="iy iz ja jb fd ms mt mu mv aw mw bi"><span id="04fa" class="ky kz hi mt b fi mx my l mz na">def f(x):<br/>    return -(w[0] + w[1]*x)/w[2]</span><span id="0670" class="ky kz hi mt b fi nb my l mz na">d = range(0,2)<br/>plt.plot(d, [f(x) for x in d])</span><span id="4ffe" class="ky kz hi mt b fi nb my l mz na">plt.plot([x1 for (x1,x2,y) in data if y==-1], [x2 for (x1,x2,y) in data if y==-1], '_', mec='r', mfc='none')<br/>plt.plot([x1 for (x1,x2,y) in data if y==1], [x2 for (x1,x2,y) in data if y==1], '+', mec='b', mfc='none')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nk"><img src="../Images/c67fb80b84dcbfa5e808aa2f71ce6549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3WU7pfjyymHq-V3dozBlw.png"/></div></div></figure><p id="e663" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这样，我们解决了一个简单的分类问题，也就是说，我们一直在学习！</p><h2 id="812d" class="ky kz hi bd la lb lc ld le lf lg lh li jx lj lk ll kb lm ln lo kf lp lq lr ls bi translated">结论</h2><p id="d14f" class="pw-post-body-paragraph jo jp hi jq b jr lt ij jt ju lu im jw jx lv jz ka kb lw kd ke kf lx kh ki kj hb bi translated">我们已经到达了冒险的终点:我们已经走过了神经元的数学模型及其实现。考虑到这一点，我们一直从它那里获得灵感，以便使神经元形式化并构建感知机。不过要小心，感知器不是一个生物学上合理的模型。神经元和质子之间的关系更类似于鸟类和飞机之间的关系。在这里,“飞行”的问题实际上是学习的问题，但是，对于一架完全可靠的飞机来说，道路要比这长得多。</p><p id="74dd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">特别是感知器有一个<em class="ku">固有的问题，</em>如果没有重大的改变，这个问题是不可克服的。Minsky和Papert非常清楚地指出了这个问题，以至于人工智能的研究已经中断了很长时间。因此，我想请你指出这个严重的感知机的局限性。我认为，通过查看上一节中的代码并调整<em class="ku"> np.random.seed() </em>和<em class="ku"> number_of_points，您可以抓住问题的直觉。</em>试试看，然后告诉我！</p><h2 id="3edf" class="ky kz hi bd la lb lc ld le lf lg lh li jx lj lk ll kb lm ln lo kf lp lq lr ls bi translated">参考</h2><p id="2b35" class="pw-post-body-paragraph jo jp hi jq b jr lt ij jt ju lu im jw jx lv jz ka kb lw kd ke kf lx kh ki kj hb bi translated">[1]阿米特博士(1989年)。<em class="ku">大脑功能建模:吸引子神经网络的世界</em>。剑桥:剑桥大学出版社。</p><p id="5380" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[2]伍尔夫拉姆·郭士纳、沃纳·m·基斯特勒、理查德·诺德和利亚姆·帕宁斯基。2014.神经元动力学:从单个神经元到认知的网络和模型。美国纽约剑桥大学出版社。<em class="ku">网络版:</em><a class="ae jn" href="https://neuronaldynamics.epfl.ch/online/index.html" rel="noopener ugc nofollow" target="_blank">https://neuronaldynamics.epfl.ch/online/index.html</a></p><p id="5c28" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[3]马文·明斯基和西摩·帕佩特。1988.感知器:扩展版。麻省理工学院出版社，美国马萨诸塞州剑桥。</p><p id="4d83" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[4]弗兰克·罗森布拉特。1962.神经动力学原理；感知器和大脑机制理论。华盛顿，斯巴达书籍</p><p id="3d62" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[5]亚塞尔·阿布·穆斯塔法、马利克·马格东·伊斯梅尔和林轩天。2012.从数据中学习。AMLBook。</p></div></div>    
</body>
</html>