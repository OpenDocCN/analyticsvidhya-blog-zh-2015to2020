<html>
<head>
<title>Multi-layer Perceptron using Keras on MNIST dataset for Digit Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于MNIST数据集的多层感知器用于数字分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-layer-perceptron-using-keras-on-mnist-dataset-for-digit-classification-problem-relu-a276cbf05e97?source=collection_archive---------2-----------------------#2019-09-13">https://medium.com/analytics-vidhya/multi-layer-perceptron-using-keras-on-mnist-dataset-for-digit-classification-problem-relu-a276cbf05e97?source=collection_archive---------2-----------------------#2019-09-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/64654d81aa80e6a8332dce8f291d87d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HB2hrDZDqs8l5KFA.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="a27c" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">ReLu激活+退出+批处理规范化+ AdamOptimizer</h2></div><h2 id="8b84" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">加载MNIST数据集</strong></h2><p id="716c" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hb bi translated">每个MNIST数据点都有两个部分:一个手写数字的图像和一个相应的标签。我们称图像为“x”，标签为“y”。训练集和测试集都包含图像及其对应的标签；例如，训练图像是mnist.train.images，训练标签是mnist.train.labels。</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="0e6b" class="ji jj ht le b fi li lj l lk ll"># the data, shuffled and split between a train and test sets <br/>(X_train, y_train), (X_test, y_test) = mnist.load_data()</span></pre><p id="bde2" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">如果Keras没有使用TensorFlow作为后端设置“KERAS_BACKEND=tensorflow ”,请使用以下命令</p><h2 id="7a53" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">导入库</strong></h2><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es lr"><img src="../Images/281d267eb1f5e9e67799832a62c8d041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*gKboTOxRzlTPnSICg6_Efw.png"/></div></figure><h2 id="048e" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">剧情功能</strong></h2><p id="fe2b" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hb bi translated">#<a class="ae ls" href="https://gist.github.com/greydanus/f6eee59eaf1d90fcb3b534a25362cea4" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/greydanus/f 6 eee 59 ea f1 d 90 fcb 3b 534 a 25362 CEA 4</a><br/>#<a class="ae ls" href="https://stackoverflow.com/a/14434334" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/a/14434334</a><br/>#该函数用于更新每个历元和误差的图形</p><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es lt"><img src="../Images/99e88319386fc42850b4e315ee5212d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*CDuVp7WuA67BwRIRt6ymog.png"/></div></figure><h2 id="5132" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">整形输入尺寸:</strong></h2><p id="3bc5" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hb bi translated">如果你观察输入形状，它是一个二维向量。对于每张图片，我们有一个(28*28)的向量。我们将把(28*28)向量转换成1 * 784的一维向量。</p><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es lu"><img src="../Images/88c63bb9a330fad694b4e124b088b846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*fPO4jf7EnEGFmTAxqpa1Cw.png"/></div></figure><p id="bc54" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">在将输入图像从3d矢量转换成2d矢量之后。</p><figure class="kz la lb lc fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lv"><img src="../Images/3ee6456f11a2887fcfa8d25c7406b16b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hxICT1iekqoXkUq9hEZDhg.png"/></div></div></figure><p id="bf65" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">接近255的数据点像素值的例子是黑色，接近0的是白色。中间是灰色的。print(X_train[0])</p><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es lw"><img src="../Images/4e0e3ef5ac6c3f2997d8151f6ddea09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*G3-IqzEoQedcigjsbiKOcQ.png"/></div></figure><h2 id="07c9" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">正常化:</strong></h2><p id="11ca" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hb bi translated">如果我们观察上面的矩阵，每个单元格的值都在0–255之间。在我们应用机器学习算法之前，让我们试着将数据标准化。X = &gt;(X-Xmin)/(Xmax-Xmin)= X/255</p><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es lx"><img src="../Images/e3bc34dc9f271cc1956bb26c166fc479.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*x60lTdkH0GMBQyu1sd0ioA.png"/></div></figure><h2 id="66d0" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">标签:</strong></h2><p id="b77d" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hb bi translated">让我们把它转换成一个10维向量。例如:假设一个图像是5，把它转换成5 =&gt; [0，0，0，0，0，1，0，0，0，0]。MLPs需要这种转换。</p><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es ly"><img src="../Images/f781b08c3acc45de4a83bc7763098d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*orNsjn6ecTNrX6PuwR0cBg.png"/></div></figure><h2 id="ff53" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">逐步构建Softmax分类器</h2><p id="0a3a" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hb bi translated">#<a class="ae ls" href="https://keras.io/getting-started/sequential-model-guide/" rel="noopener ugc nofollow" target="_blank">https://keras.io/getting-started/sequential-model-guide/</a></p><p id="f709" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">顺序模型是层的线性堆叠。您可以通过将层实例列表传递给构造函数来创建顺序模型:</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="b72a" class="ji jj ht le b fi li lj l lk ll"><strong class="le hu">  model = Sequential([Dense(32, input_shape=(784,)),<br/>  Activation(‘relu’),Dense(10),Activation(‘softmax’)])</strong></span><span id="2421" class="ji jj ht le b fi lz lj l lk ll"># You can also simply add layers via the .add() method:</span><span id="5642" class="ji jj ht le b fi lz lj l lk ll"><strong class="le hu">  model = Sequential()<br/>  model.add(Dense(32, input_dim=784))<br/>  model.add(Activation(‘relu’))</strong></span></pre><p id="6a7d" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">#<a class="ae ls" href="https://keras.io/layers/core/" rel="noopener ugc nofollow" target="_blank">https://keras.io/layers/core/</a>#参数显示在Keras层</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="ef42" class="ji jj ht le b fi li lj l lk ll"><strong class="le hu">keras.layers.Dense</strong>(units, activation=None, use_bias=True, kernel_initializer=’glorot_uniform’, bias_initializer=’zeros’, kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)</span></pre><p id="b279" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated"><strong class="ki hu"> Dense实现运算:output = activation(dot(input，kernel)+ bias) </strong>其中activation是作为激活参数传递的元素激活函数，kernel是由层创建的权重矩阵，而# bias是由层创建的偏差向量(仅当use_bias为真时适用)</p><p id="02d1" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated"><strong class="ki hu">输出=激活(dot(input，kernel) + bias) = &gt; y =激活(WT。X + b) </strong></p><h2 id="b15f" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">激活:</h2><p id="a966" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hb bi translated">激活可以通过激活层使用，也可以通过所有转发层支持的激活参数使用。</p><div class="hh hi ez fb hj ma"><a href="https://keras.io/activations/" rel="noopener  ugc nofollow" target="_blank"><div class="mb ab dw"><div class="mc ab md cl cj me"><h2 class="bd hu fi z dy mf ea eb mg ed ef hs bi translated">激活- Keras文档</h2><div class="mh l"><h3 class="bd b fi z dy mf ea eb mg ed ef dx translated">激活既可以通过激活层使用，也可以通过所有转发支持的激活参数使用…</h3></div><div class="mi l"><p class="bd b fp z dy mf ea eb mg ed ef dx translated">keras.io</p></div></div></div></a></div><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="01eb" class="ji jj ht le b fi li lj l lk ll"># from keras.layers import Activation, Dense</span><span id="ecb1" class="ji jj ht le b fi lz lj l lk ll"># model.add(Dense(64))<br/># model.add(Activation(‘tanh’))</span><span id="8955" class="ji jj ht le b fi lz lj l lk ll"># This is equivalent to:<br/># model.add(Dense(64, activation=’tanh’))</span></pre><p id="4873" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">有许多激活功能可用<strong class="ki hu">例如:tanh、relu、softmax </strong></p><h2 id="d19e" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">建筑模型:</strong></h2><p id="b7f8" class="pw-post-body-paragraph kg kh ht ki b kj kk iu kl km kn ix ko jt kp kq kr jx ks kt ku kb kv kw kx ky hb bi translated"><strong class="ki hu">第一步:</strong></p><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/2ac7e99266656ce51fd25854611b20e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*_F2B4xJCOCtocL56AQo45Q.png"/></div></figure><p id="474f" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">模型需要知道它应该期望什么样的输入形状。由于这个原因，顺序模型中的第一层(也只有第一层，因为后面的层可以进行自动形状推断)。它需要接收关于其输入形状的信息。可以使用input_shape和input_dim来传递输入的形状。output_dim表示该层需要的节点数量，这里我们有10个节点。</p><p id="8c97" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated"><strong class="ki hu">第二步:</strong></p><p id="aaaa" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">在训练模型之前，您需要配置学习过程，这是通过compile方法完成的。它接收三个参数:<br/> 1 —一个优化器。这可能是现有优化器的字符串标识符，<a class="ae ls" href="https://keras.io/optimizers/" rel="noopener ugc nofollow" target="_blank">https://keras.io/optimizers/</a><br/>2——一个损失函数。这是模型试图最小化的目标。，<a class="ae ls" href="https://keras.io/losses/" rel="noopener ugc nofollow" target="_blank">https://keras.io/losses/</a><br/>3—指标列表。对于任何分类问题，您都希望将其设置为metrics=['accuracy']。<a class="ae ls" href="https://keras.io/metrics/" rel="noopener ugc nofollow" target="_blank">https://keras.io/metrics/</a></p><figure class="kz la lb lc fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/f30e75d85827251eb435fd3aa88f528d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c2Yamwfz7hlGRQyze39qTA.png"/></div></div></figure><p id="5a22" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated"><strong class="ki hu"> <em class="ml">注意</em> </strong> <em class="ml"> : </em>当使用categorical _ crossentropy loss时，您的目标应该是分类格式(例如，如果您有10个类，每个样本的目标应该是一个10维向量，除了。对于对应于样本类别的索引处的1)。这就是为什么我们把标签转换成向量。</p><p id="ae32" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated"><strong class="ki hu">步骤3 </strong> : Keras模型在输入数据和标签的Numpy数组上训练。<br/>为了训练模型，通常会使用拟合函数。</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="866d" class="ji jj ht le b fi li lj l lk ll">fit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)</span></pre><p id="50ce" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">fit()函数为模型定型固定数量的历元(数据集上的迭代)。</p><p id="b426" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">它返回一个历史对象。这是历史。历史属性是连续时期的训练损失值和指标值以及认证损失值和认证指标值(如果适用)的记录。<a class="ae ls" href="https://github.com/openai/baselines/issues/20" rel="noopener ugc nofollow" target="_blank">https://github.com/openai/baselines/issues/20</a></p><figure class="kz la lb lc fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mm"><img src="../Images/3a3b9bf112e294b4d21d08784a1c3dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5LYzUPZOY2wUPpnHKhplg.png"/></div></div></figure><h2 id="d1f7" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">评估:</h2><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es mn"><img src="../Images/a74d408841fd8a438fc1dba5807c4724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*LsCyFhOj1pk1k2QYF-amMQ.png"/></div></figure><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="e038" class="ji jj ht le b fi li lj l lk ll">print(history.history.keys())<br/>dict_keys([‘val_loss’, ‘val_acc’, ‘loss’, ‘acc’])<br/>history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))</span></pre><p id="e41e" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">只有当您通过参数validation_data时，我们才会得到val_loss和val_acc。</p><ol class=""><li id="d897" class="mo mp ht ki b kj lm km ln jt mq jx mr kb ms ky mt mu mv mw bi translated">val_loss:验证损失</li><li id="5613" class="mo mp ht ki b kj mx km my jt mz jx na kb nb ky mt mu mv mw bi translated">val_acc:验证准确性</li><li id="872f" class="mo mp ht ki b kj mx km my jt mz jx na kb nb ky mt mu mv mw bi translated">损失:培训损失</li><li id="fcca" class="mo mp ht ki b kj mx km my jt mz jx na kb nb ky mt mu mv mw bi translated">acc:列车精度</li></ol><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es nc"><img src="../Images/6add444a713ef7db3ddd39ffb8acca41.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*_85VCAqLkhOS66_0URqjWg.png"/></div></figure><h2 id="14ed" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">构建MLP + ReLu激活+退出+批处理规范化+ AdamOptimizer</h2><div class="hh hi ez fb hj ma"><a href="https://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras" rel="noopener  ugc nofollow" target="_blank"><div class="mb ab dw"><div class="mc ab md cl cj me"><h2 class="bd hu fi z dy mf ea eb mg ed ef hs bi translated">Keras中的BatchNormalization函数在哪里调用？</h2><div class="mh l"><h3 class="bd b fi z dy mf ea eb mg ed ef dx translated">如果我想使用Keras中的BatchNormalization函数，那么我需要只在开头调用一次吗？我读了…</h3></div><div class="mi l"><p class="bd b fp z dy mf ea eb mg ed ef dx translated">stackoverflow.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni hp ma"/></div></div></a></div><div class="hh hi ez fb hj ma"><a href="https://stackoverflow.com/questions/34716454/where-do-i-call-the-batchnormalization-function-in-keras" rel="noopener  ugc nofollow" target="_blank"><div class="mb ab dw"><div class="mc ab md cl cj me"><h2 class="bd hu fi z dy mf ea eb mg ed ef hs bi translated">Keras中的BatchNormalization函数在哪里调用？</h2><div class="mh l"><h3 class="bd b fi z dy mf ea eb mg ed ef dx translated">如果我想使用Keras中的BatchNormalization函数，那么我需要只在开头调用一次吗？我读了…</h3></div><div class="mi l"><p class="bd b fp z dy mf ea eb mg ed ef dx translated">stackoverflow.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni hp ma"/></div></div></a></div><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es nj"><img src="../Images/3bc23ffa433edb9ef8c11c4457cc1f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*WP4U4AWuDckwvD0Y4WFt1A.png"/></div></figure><figure class="kz la lb lc fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nk"><img src="../Images/963890601a1fbb2b5793db6297695840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*91OdnwI7XHoKibmGo8T8vQ.png"/></div></div></figure><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es nl"><img src="../Images/4b05489e2c930141914d7a075384855f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*mfXgm-nekDyoR_C0227K0g.png"/></div></figure><figure class="kz la lb lc fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nm"><img src="../Images/5c83cd23e947bf27ea685f44236f198f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d1_k3Wp6lf1S56UkxXLqAQ.png"/></div></div></figure><figure class="kz la lb lc fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nn"><img src="../Images/696ffb6f65b62d058de562f552f91fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hvvqOuu7kSiP5CqrO_0JwQ.png"/></div></div></figure><figure class="kz la lb lc fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es no"><img src="../Images/83fb8fe7f62a2da924f8096cf14498f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fl8EtoWUHkXtlUez73ob1g.png"/></div></div></figure><p id="a246" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">训练集和测试集的交叉熵损失值在前2个历元急剧下降，然后趋于恒定。</p><h2 id="d881" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">标绘权重</strong></h2><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es np"><img src="../Images/9cdfa156a30892e7f23fa03b2530df67.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*X4_KqKPhwxd7tlHTULdBGQ.png"/></div></figure><figure class="kz la lb lc fd hk er es paragraph-image"><div class="er es nq"><img src="../Images/740d1231d55d1c2851c520eb064614f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*bKQNR8Y0lpkda-WiYkdBOA.png"/></div></figure><p id="169b" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">层_1的中值权重为零，节点之间的变化较小，而层_2的变化较大。在输出图层中，权重值具有较小扩散的负值。</p><p id="bdc7" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated">================代码===============</p><p id="266f" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi translated"><a class="ae ls" href="https://github.com/ranasingh-gkp/Applied_AI_O/blob/master/Module%208_NN%2C%20Computer%20vision%2C%20Deep%20learning/Keras_Hyperparameter_Mnist.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Rana Singh-gkp/Applied _ AI _ O/blob/master/Module % 208 _ NN % 2C % 20 computer % 20 vision % 2C % 20 deep % 20 learning/Keras _ Hyperparameter _ mnist . ipynb</a></p><p id="f64e" class="pw-post-body-paragraph kg kh ht ki b kj lm iu kl km ln ix ko jt lo kq kr jx lp kt ku kb lq kw kx ky hb bi">======================================</p><h2 id="1656" class="ji jj ht bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated"><strong class="ak">参考文献:</strong></h2><ol class=""><li id="af55" class="mo mp ht ki b kj kk km kn jt nr jx ns kb nt ky mt mu mv mw bi translated">维基网</li><li id="1aef" class="mo mp ht ki b kj mx km my jt mz jx na kb nb ky mt mu mv mw bi translated">应用人工智能</li><li id="0da9" class="mo mp ht ki b kj mx km my jt mz jx na kb nb ky mt mu mv mw bi translated">keras.io</li></ol></div></div>    
</body>
</html>