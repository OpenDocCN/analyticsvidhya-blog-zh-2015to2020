<html>
<head>
<title>Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-e5eecc7f26f1?source=collection_archive---------6-----------------------#2020-11-07">https://medium.com/analytics-vidhya/linear-regression-e5eecc7f26f1?source=collection_archive---------6-----------------------#2020-11-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b67f4340fc0de8396a5829212707b03c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AV1ewdXfagt9hIDD"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">杰斯温·托马斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="dbff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di"> O </span>我们的目标是找到机器学习中变量之间的关系。我们有许多算法可用于各种用例。线性回归是最流行的一种，也是你将学习的第一种。在这篇文章中，我会给你一个关于线性回归的基本介绍。</p></div><div class="ab cl kc kd gp ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="hb hc hd he hf"><blockquote class="kj kk kl"><p id="79e9" class="iv iw km ix b iy iz ja jb jc jd je jf kn jh ji jj ko jl jm jn kp jp jq jr js hb bi translated">线性回归试图通过将线性方程拟合到观察到的数据来模拟两个变量之间的关系。</p></blockquote><p id="c318" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中一个观测数据是独立值，我们称之为‘x’，另一个观测数据是依赖值，我们称之为‘y’。这意味着 y 的值随着 x 的变化而变化。</p><p id="285a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们使用线性回归算法之前，我们应该首先确定我们的数据是否有变量之间的关系。相关性和散点图可以帮助我们发现变量之间的关系。</p><p id="4653" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果可以用单个变量(x1)进行分析，则称为<strong class="ix hj">简单线性回归</strong>。如果可以用多个变量(x1，x2，x3…)进行分析，则称为<strong class="ix hj">多元线性回归</strong>。</p><p id="6809" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要把它放在最独特的(！)举例，假设你正在买房，对你来说重要的是房子的面积。房子的价格取决于房子的面积。这里，房子的面积是 x，我们的自变量。取决于 x 的是房子的价格，y，x 是我们的第一个特征。如果房子的面积是我们唯一拥有的自变量，那就意味着我们可以用<strong class="ix hj"> <em class="km">简单线性回归</em> </strong>。但是除了房子的面积，位置，房间数量和年龄也影响房子的价格。现在我们有四个影响房价的特征。所以，我们就用<strong class="ix hj"> <em class="km">多元线性回归</em> </strong>。</p><p id="f9fd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们拥有的数据可以有这样一个图表，它的回归线是:</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/08cc435977cfea91787a0af567a26119.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*BWKXQ6OjeojGXhNyjHgitw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来自我的一个多元线性回归项目</figcaption></figure><p id="4a41" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如你所见，我们的线并不穿过每个点。如果会，那就完美了，但是不可能有那种完美的关系。</p><p id="309a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这条线不是魔法造成的。在引擎盖下有一些数学上的魔法在进行着。这个等式位于回归线之下:</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/15b2dd7cfa7327762761396117443e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*tNKotBjOQnKscPUg7GYqfg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">假设函数</figcaption></figure><blockquote class="kj kk kl"><p id="1153" class="iv iw km ix b iy iz ja jb jc jd je jf kn jh ji jj ko jl jm jn kp jp jq jr js hb bi translated"><strong class="ix hj">这个函数叫‘假设函数’。</strong></p></blockquote><ul class=""><li id="66f9" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lb lc ld le bi translated">y '是我们试图预测的价值，房子的价格。</li><li id="3f0c" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">b (w0)是 y 轴的截距，或者我们可以称之为“偏差”。是价值观平衡了我们所做的一切。</li><li id="bbd5" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">x1 是我们的第一依赖值。如果我们继续简单的线性回归，这个方程属于，房子的面积是 x1。</li><li id="a241" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">w1 是未来第一的重量。</li></ul><p id="9f52" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">多元线性回归方程:</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/08fa493906aacfa886b871d52bcdfab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*dUW5g_f6puH7ZZk1tLkGVg.png"/></div></figure><p id="ee83" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">x1 是房屋的面积，x2 是房屋的位置，x3 是房屋的房间数等等… w1、w2 和 w3 是要素的权重。</p><p id="de03" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以看到数学是机器学习的重要组成部分，而这仅仅是开始。</p><p id="f8b4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们的路线错了呢？</p><p id="8c65" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的意思是，我们如何更新我们的 w0 (b)和 w1？我们如何选择哪一个值对可以构成我们数据的最佳直线？</p><blockquote class="ll"><p id="d79e" class="lm ln hi bd lo lp lq lr ls lt lu js dx translated">带成本函数！</p></blockquote><h2 id="70fc" class="lv lw hi bd lx ly lz ma mb mc md me mf jg mg mh mi jk mj mk ml jo mm mn mo mp bi translated">什么是“成本函数”？</h2><p id="1874" class="pw-post-body-paragraph iv iw hi ix b iy mq ja jb jc mr je jf jg ms ji jj jk mt jm jn jo mu jq jr js hb bi translated">我们的目标是最小化估计的 y 值和实际的 y 值之间的差异。所以，我们需要将 w0 和 w1 的值更新为<strong class="ix hj"> <em class="km">，减少</em> </strong>的差异。</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/a10a2d92c1d5d9d995f01ffda0a0c29a.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*kztdNWsACE7fwEifRUWt9w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">成本函数(J)</figcaption></figure><p id="f0cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性回归的成本函数是预测 y 值和实际 y 值之间的均方根误差。</p><h2 id="8801" class="lv lw hi bd lx ly mw ma mb mc mx me mf jg my mh mi jk mz mk ml jo na mn mo mp bi translated">什么是梯度下降？</h2><blockquote class="kj kk kl"><p id="c0a4" class="iv iw km ix b iy iz ja jb jc jd je jf kn jh ji jj ko jl jm jn kp jp jq jr js hb bi translated">在理论层面上，梯度下降是一种最小化函数的算法。给定由一组参数定义的函数，梯度下降从一组初始参数值开始，并迭代地向使函数最小化的一组参数值移动。这种迭代最小化是使用微积分实现的，在函数梯度的负方向上采取步骤。</p></blockquote><p id="f70c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度下降<strong class="ix hj"> <em class="km">下降</em> </strong>成本函数。每一步的大小称为<strong class="ix hj">学习率。我们应该谨慎选择学习速度。</strong></p><ol class=""><li id="b962" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js nb lc ld le bi translated">你不应该选择学习率很大。你可以错过当地的最小值。</li></ol><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/b6a81ca78cdfd1015ad0635b6a43abb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*dILGj3W0krVOjrc1naEoKQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/gradient-descent-in-linear-regression/</a></figcaption></figure><p id="967e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.如果选择的学习率太小，将需要更多的时间才能达到局部最小值。</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/111f1d40e7094610545cf68268ee9703.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*N2_3OdkqZzQLu4r6ScoHTQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/" rel="noopener ugc nofollow" target="_blank">https://www . geeks forgeeks . org/gradient-descent-in-linear-regression/</a></figcaption></figure><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/1dd628edc8feae91d6f9d9c4043373a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*hDqdRd3OlzYy2W_dusqoLg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/</a></figcaption></figure><p id="3fa2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">蓝线是一个学习率的好选择。</p></div><div class="ab cl kc kd gp ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="hb hc hd he hf"><p id="8ace" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们先用 Sklearn 库把现在学到的东西编码一下。</p><p id="ea31" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们需要我们的图书馆来工作。</p><pre class="kr ks kt ku fd nf ng nh ni aw nj bi"><span id="b849" class="lv lw hi ng b fi nk nl l nm nn">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas pd</span></pre><p id="1f07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们需要我们的数据集(你可以在这里找到<a class="ae iu" href="https://bilkav.com/satislar.csv" rel="noopener ugc nofollow" target="_blank">数据</a>):</p><pre class="kr ks kt ku fd nf ng nh ni aw nj bi"><span id="1b9e" class="lv lw hi ng b fi nk nl l nm nn"># We are using read_csv method of Pandas to read our data</span><span id="a0f5" class="lv lw hi ng b fi no nl l nm nn">data = pd.read_csv("satislar.csv")</span></pre><p id="20cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们有两列；“aylar”和“satislar”(月份和销售额)。我们应该把它们分成 x，独立变量' aylar '和 y，因变量' satislar '。</p><pre class="kr ks kt ku fd nf ng nh ni aw nj bi"><span id="042e" class="lv lw hi ng b fi nk nl l nm nn">X = data.iloc[:, 0:1].values<br/># ':' means take all the rows, '0:1' means take only first column, <br/># and turn it into an array with .values</span><span id="1034" class="lv lw hi ng b fi no nl l nm nn">y = data.iloc[:, -1:].values<br/># '-1:' means take only the last column, ’satislar' column</span></pre><p id="c70f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们已经成功地将数据分割为 X 和 y。现在，我们可以再次将它们分割为训练和测试数据。</p><pre class="kr ks kt ku fd nf ng nh ni aw nj bi"><span id="ae58" class="lv lw hi ng b fi nk nl l nm nn"># we'll use Sklearn Library</span><span id="03a8" class="lv lw hi ng b fi no nl l nm nn">from sklearn.model_selection import train_test_split</span></pre><p id="d04d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">导入库之后，我们可以编写一行代码来进行分割:</p><pre class="kr ks kt ku fd nf ng nh ni aw nj bi"><span id="c60d" class="lv lw hi ng b fi nk nl l nm nn"># You always have to follow this order to split the data</span><span id="bdb8" class="lv lw hi ng b fi no nl l nm nn">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.22, random_state=0)</span></pre><ul class=""><li id="09c3" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lb lc ld le bi translated">X 是包含 X_train 和 X_test 值的数据，y 是包含 y_train 和 y_test 值的数据。</li><li id="aaf0" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">test_size=.22 表示包含在测试分割中的数据集的比例。默认情况下，该值为 0.33，但我们有一个小数据集。我给的价值较小。</li><li id="43c6" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">random_state = 0 控制在应用分割之前应用于数据的洗牌。</li></ul><pre class="kr ks kt ku fd nf ng nh ni aw nj bi"><span id="7219" class="lv lw hi ng b fi nk nl l nm nn">from sklearn.linear_model import LinearRegression<br/>lin_reg = LinearRegression()</span></pre><p id="57d2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们创建了一个名为“lin_reg”的对象。我们将拟合我们的模型，然后我们将使我们的模型预测:</p><pre class="kr ks kt ku fd nf ng nh ni aw nj bi"><span id="3f62" class="lv lw hi ng b fi nk nl l nm nn">lin_reg.fit(X_train, y_train)<br/>y_pred = lin_reg.predict(X_test)</span></pre><p id="6787" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">搞定了。你可以查一下 y_pred 和 y_test，看看你做的怎么样。但是有一个更简单的方法。</p><pre class="kr ks kt ku fd nf ng nh ni aw nj bi"><span id="9ce5" class="lv lw hi ng b fi nk nl l nm nn"># you can plot the results</span><span id="50c7" class="lv lw hi ng b fi no nl l nm nn">plt.scatter(X, y, color = 'red')<br/>plt.plot(X_test, y_pred, color = 'yellow')</span></pre><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es np"><img src="../Images/2256005ce9057fee67cb52e74a06aaa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*kcK60AVqVAv1mbosSmn1Jw.png"/></div></figure><p id="8fca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看起来很好！</p><pre class="kr ks kt ku fd nf ng nh ni aw nj bi"><span id="ad43" class="lv lw hi ng b fi nk nl l nm nn"># and you can print r square value</span><span id="304f" class="lv lw hi ng b fi no nl l nm nn">from sklearn.metrics import r2_score<br/>r2_score(y_test, y_pred)<br/>&gt;&gt; 0.9774483391303704</span></pre><p id="c0b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">r 平方得分非常接近 1。这意味着我们的模型工作正常。</p></div><div class="ab cl kc kd gp ke" role="separator"><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh ki"/><span class="kf bw bk kg kh"/></div><div class="hb hc hd he hf"><div class="kr ks kt ku fd nq"><a href="https://developers.google.com/machine-learning/crash-course/descending-into-ml/linear-regression" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">下降到 ML:线性回归|机器学习速成班</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">" type": "thumb-down "，" id ":" missingtheinformationneed "，" label ":"缺少我需要的信息" }，{ "type"…</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">developers.google.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe io nq"/></div></div></a></div><div class="of og ez fb oh nq"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">sk learn . model _ selection . train _ test _ split-sci kit-learn 0 . 23 . 2 文档</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">sci kit-learn:Python 中的机器学习</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">scikit-learn.org</p></div></div><div class="nz l"><div class="oi l ob oc od nz oe io nq"/></div></div></a></div><div class="of og ez fb oh nq"><a href="http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">线性回归</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">在试图用一个线性模型来拟合观察到的数据之前，建模者应该首先确定是否存在一个线性模型。</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">www.stat.yale.edu</p></div></div><div class="nz l"><div class="oj l ob oc od nz oe io nq"/></div></div></a></div><div class="of og ez fb oh nq"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">sklearn.linear_model。线性回归-sci kit-学习 0.23.2 文档</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">普通最小二乘线性回归。线性回归拟合系数为 w = (w1，...，wp)到…</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">scikit-learn.org</p></div></div><div class="nz l"><div class="ok l ob oc od nz oe io nq"/></div></div></a></div><div class="of og ez fb oh nq"><a href="https://www.geeksforgeeks.org/ml-linear-regression/#:~:text=Cost%20function%28J%29%20of%20Linear,the%20model%20uses%20Gradient%20Descent." rel="noopener  ugc nofollow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">ML |线性回归- GeeksforGeeks</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">线性回归是一种基于监督学习的机器学习算法。它执行回归任务…</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">www.geeksforgeeks.org</p></div></div><div class="nz l"><div class="ol l ob oc od nz oe io nq"/></div></div></a></div><div class="of og ez fb oh nq"><a href="https://www.geeksforgeeks.org/gradient-descent-in-linear-regression/" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">线性回归中的梯度下降</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">在线性回归中，该模型的目标是获得最佳拟合的回归线，以根据预测值来预测 y 值</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">www.geeksforgeeks.org</p></div></div><div class="nz l"><div class="om l ob oc od nz oe io nq"/></div></div></a></div><div class="of og ez fb oh nq"><a href="https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab dw"><div class="ns ab nt cl cj nu"><h2 class="bd hj fi z dy nv ea eb nw ed ef hh bi translated">梯度下降和线性回归导论</h2><div class="nx l"><h3 class="bd b fi z dy nv ea eb nw ed ef dx translated">梯度下降算法是那些“最成功”的算法之一，可以为解决问题提供一个新的视角…</h3></div><div class="ny l"><p class="bd b fp z dy nv ea eb nw ed ef dx translated">spin.atomicobject.com</p></div></div><div class="nz l"><div class="on l ob oc od nz oe io nq"/></div></div></a></div></div></div>    
</body>
</html>