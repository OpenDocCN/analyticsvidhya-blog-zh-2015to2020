<html>
<head>
<title>Understanding Positional Encoding in Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解变压器中的位置编码</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-positional-encoding-in-transformers-def92aca1dfe?source=collection_archive---------2-----------------------#2020-11-23">https://medium.com/analytics-vidhya/understanding-positional-encoding-in-transformers-def92aca1dfe?source=collection_archive---------2-----------------------#2020-11-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="adbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理解变压器模型的一个重要关键</p><p id="0957" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我开始学习 transformer 模型时，我发现理解它们如何工作的最大障碍是位置编码和自我关注的概念。所以我写了这篇文章，希望它能帮助别人。我将在这里处理位置编码，在以后的文章中处理自我关注。</p><p id="2d44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章不是关于变形金刚，只是位置编码。如果你在学习变形金刚的时候陷入概念的话，它更像是一种资源。出于学习变形金刚的目的，我建议你先读一读引发这一切的研究论文，<a class="ae jd" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>。你也可以看看杰伊·阿拉玛的神奇贴<a class="ae jd" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">《变形金刚》</a>。</p><p id="27a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">位置编码</strong></p><p id="d7b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与“RNN”和“LSTM”等顺序算法不同，变压器没有内置的机制来捕捉单词在句子中的相对位置。这很重要，因为单词之间的距离提供了重要的上下文信息。这就是位置编码介入的地方。</p><p id="18d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">位置编码不是模型架构的一部分。它实际上是预处理的一部分。位置编码向量被生成为与每个单词的嵌入向量大小相同。计算之后，位置编码矢量被加到嵌入矢量上。“注入”到嵌入向量中的模式允许算法学习该空间信息。</p><p id="fbc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么它是如何工作的呢？</p><p id="1668" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，在下面的公式中，‘<em class="je">PE’</em>是单词在序列中的位置(<em class="je">pos’</em>)和嵌入(I)的函数。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jf"><img src="../Images/5bbb964a009649dd74e1d3c9e9f928c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*5qEu5eRUL_a-BE29Pb47zw.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><strong class="bd jr">位置编码公式</strong></figcaption></figure><p id="1420" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，使用以下公式计算角度:</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es js"><img src="../Images/af2fc24b48d76590afe2b0a30f83e753.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*kQtpubDQ0TpaWZ_ViyIkXw.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><strong class="bd jr">角度计算</strong></figcaption></figure><p id="b865" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，取角度的<em class="je">正弦</em>或<em class="je">余弦</em>。这给出了在位置<em class="je">‘位置’</em>和嵌入索引’<em class="je">I’</em>的单词的值。<em class="je">‘pos’</em>保持单词的常数，因为嵌入索引，<em class="je"> i </em>增加，给出该单词的唯一模式</p><p id="e09f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在转到下一个单词时，‘<em class="je">pos’</em>递增。这将模式稍微向右移动。'<em class="je"> PE' </em>公式对偶数嵌入索引应用正弦函数(<em class="je"> i' </em>)，对奇数嵌入索引应用余弦函数。这就是为什么您会在下图中看到交错模式(棋盘)。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jt"><img src="../Images/a3c88a99873e87f522a36e9a4fe7f623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*fFGraCX20xQnaL1_MNZz1Q.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><strong class="bd jr">图 1:位置编码</strong></figcaption></figure><p id="f8fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么这个公式？论文的作者是这样解释的:</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ju"><img src="../Images/1b5b71547c784a0c0626cf032ab08a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*7pDrvw1ufWxgJX7SgsbjBA.png"/></div></figure><p id="b8e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数学解释来自下面提到的<a class="ae jd" href="https://www.reddit.com/r/MachineLearning/comments/6jdi87/r_question_about_positional_encodings_used_in/) post. &gt; If k is fixed, than sin(k/a) and cos(k/a) are constant and PE_{pos+k} is some matrix which depends on" rel="noopener ugc nofollow" target="_blank"> Reddit </a>。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jv"><img src="../Images/edb9493b2ddf722d4247b36ead51a3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*73lNiY3SHH51ovo_TKKKGQ.png"/></div></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jw"><img src="../Images/be68d9fc1be809d060615b3667d6a370.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*78GctLKWIwapBkLs9g6WPw.png"/></div></figure><p id="3441" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">史蒂文·斯密特在他的<a class="ae jd" href="https://stevensmit.me/taking-a-look-at-transformer-architecture/" rel="noopener ugc nofollow" target="_blank">博客</a>中做了另一个很好的解释:</p><blockquote class="jx jy jz"><p id="c22b" class="if ig je ih b ii ij ik il im in io ip ka ir is it kb iv iw ix kc iz ja jb jc hb bi translated">使用正弦和余弦函数的原因之一是它们是周期性的，因此无论模型是在长度为 5 的序列上学习，还是在长度为 500 的序列上学习，编码将总是具有相同的范围([-1，1])。</p></blockquote><p id="d33c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，变压器内的剩余连接有助于逐层加强位置编码。</p><p id="479b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">位置编码代码:</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kd"><img src="../Images/9f09b6228d06a8110453bad7ffeb7424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m253ywxMF5J-uLTTaV8bpw.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated"><strong class="bd jr">图 2:代码</strong></figcaption></figure><p id="c43c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在你知道了。同样，位置嵌入被添加到嵌入向量，该嵌入向量成为变换器的输入。转换器是一个深度学习模型，在训练时将学习嵌入的位置数据的含义。</p><p id="d9db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一次:自我关注</p></div></div>    
</body>
</html>