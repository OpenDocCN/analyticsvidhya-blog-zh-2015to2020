<html>
<head>
<title>Magic of TF-IDF</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TF-IDF的魔力</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/magic-of-tf-idf-202649d39c2f?source=collection_archive---------10-----------------------#2020-06-19">https://medium.com/analytics-vidhya/magic-of-tf-idf-202649d39c2f?source=collection_archive---------10-----------------------#2020-06-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/6ba5bf192dcbfcab092d503ce3d63f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IfZz-L5zY3qj9ZKC.png"/></div></div></figure><div class=""/><div class=""><h2 id="63d5" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">词频逆文档频(TFIDF)可以创造奇迹！</h2></div><p id="4cf9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">您知道吗，术语频率逆文档频率是Karen sprck Jones在1972年的一篇论文中提出的——“术语特异性的统计解释及其在检索中的应用”？😲</p><figure class="kf kg kh ki fd hk er es paragraph-image"><div class="er es ke"><img src="../Images/5f7dc12fa2dc909f9b0cb613ddf322f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/0*FpS2b_UDA8PCzAkc"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">凯伦·斯皮克·琼斯</figcaption></figure><p id="775b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">好吧！回到目前的场景，在从<strong class="jk hu"> TFIDF </strong>开始之前，让我简单解释一下<strong class="jk hu"> BoW </strong>，以便更容易理解为什么要引入TFIDF。</p><h2 id="7b92" class="kn ko ht bd kp kq kr ks kt ku kv kw kx jr ky kz la jv lb lc ld jz le lf lg lh bi translated">单词袋(蝴蝶结)</h2><p id="2ee3" class="pw-post-body-paragraph ji jj ht jk b jl li iu jn jo lj ix jq jr lk jt ju jv ll jx jy jz lm kb kc kd hb bi translated">单词包(BoW)是一种文本表示，它携带文档中单词出现的信息。它被称为单词包，因为它包含了文档中的所有单词，其中单词在文档中的<strong class="jk hu"> <em class="ln">顺序和结构是未知的</em> </strong>。迷惑？简单地说，这就像我们有一个空袋子，我们有一个文档词汇表的集合。我们把单词一个接一个地放进袋子里，我们得到了什么？装满单词的袋子。😲</p><figure class="kf kg kh ki fd hk er es paragraph-image"><div class="er es lo"><img src="../Images/8a28334d0ae1642332e9b4627f27366e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/0*KwLaTHYlVY6tLASn.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">来源:<a class="ae lp" href="https://dudeperf3ct.github.io/lstm/gru/nlp/2019/01/28/Force-of-LSTM-and-GRU/" rel="noopener ugc nofollow" target="_blank">https://dude perf 3c t . github . io/lstm/gru/NLP/2019/01/28/LSTM和gru部队/ </a></figcaption></figure><p id="ed19" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">制作单词袋模型，[注:以<a class="ae lp" href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/#:~:text=A%20bag%2Dof%2Dwords%20is,the%20presence%20of%20known%20words." rel="noopener ugc nofollow" target="_blank">单词袋</a>的温和介绍为例]</p><ol class=""><li id="b379" class="lq lr ht jk b jl jm jo jp jr ls jv lt jz lu kd lv lw lx ly bi translated"><strong class="jk hu">收集资料<br/> </strong> <em class="ln">【这是最好的时代，<br/>这是最坏的时代，<br/>这是智慧的时代，<br/>这是愚昧的时代】</em></li><li id="5e4a" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated"><strong class="jk hu">做一个数据的词汇<br/> </strong>【“它”、“曾经”、“最好”、“的”、“时代的”、“最坏的”、“年龄的”、“智慧的”、“愚蠢的”】</li><li id="0f09" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated"><strong class="jk hu">创建矢量<br/>T25】“那是最坏的时代”= [1，1，1，0，1，1，1，0，0] <br/>“那是智慧的时代”= [1，1，1，0，1，0，0，1，1，0] <br/>“那是愚昧的时代”= [1，1，1，0，1，0，0，0，1]</strong></li><li id="8ec0" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated">使用计数法或频率法(如TFIDF)对单词评分。我们将在本文中讨论这一点。</li></ol><p id="4272" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">现在让我们开始使用TFIDF吧！！！</strong></p><h1 id="8ae3" class="me ko ht bd kp mf mg mh kt mi mj mk kx iz ml ja la jc mm jd ld jf mn jg lg mo bi translated">术语频率逆文档频率(TFIDF)</h1><p id="1191" class="pw-post-body-paragraph ji jj ht jk b jl li iu jn jo lj ix jq jr lk jt ju jv ll jx jy jz lm kb kc kd hb bi mp translated">引入erm频率逆文档频率(TFIDF)来克服BOW问题。</p><p id="224d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">TFIDF是反映单词对文档的重要性的统计度量。TF-IDF主要用于文档搜索和信息检索，通过评分给出单词在文档中的重要性。TFIDF得分越高，该术语越稀有，反之亦然。</p><p id="7ae9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">文档中一个单词的TF-IDF是通过乘以两个不同的度量来计算的:术语频率和逆文档频率。</p><blockquote class="my"><p id="cf90" class="mz na ht bd nb nc nd ne nf ng nh kd dx translated"><strong class="ak"> TFIDF = TF * IDF </strong></p></blockquote><p id="ce19" class="pw-post-body-paragraph ji jj ht jk b jl ni iu jn jo nj ix jq jr nk jt ju jv nl jx jy jz nm kb kc kd hb bi translated"><em class="ln">哪里</em>，</p><p id="206d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">TF(term) =术语在文档中出现的次数/文档中的术语总数</p><p id="1ebb" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">IDF(术语)= log(文档总数/包含术语的文档数)</p><h2 id="db65" class="kn ko ht bd kp kq kr ks kt ku kv kw kx jr ky kz la jv lb lc ld jz le lf lg lh bi translated">TFIDF的一些应用:</h2><ol class=""><li id="b0fb" class="lq lr ht jk b jl li jo lj jr nn jv no jz np kd lv lw lx ly bi translated">信息检索</li><li id="4ee5" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated">文本挖掘</li><li id="e577" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated">用户建模</li><li id="927a" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated">关键词提取</li><li id="a397" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated">搜索引擎</li></ol><h2 id="8537" class="kn ko ht bd kp kq kr ks kt ku kv kw kx jr ky kz la jv lb lc ld jz le lf lg lh bi translated">在Python中实现TFIDF</h2><p id="1d68" class="pw-post-body-paragraph ji jj ht jk b jl li iu jn jo lj ix jq jr lk jt ju jv ll jx jy jz lm kb kc kd hb bi translated">从一个简单的例子开始，</p><pre class="kf kg kh ki fd nq nr ns nt aw nu bi"><span id="141f" class="kn ko ht nr b fi nv nw l nx ny"><strong class="nr hu">sentence1</strong> = “Go until jurong point, crazy.. Available only in bugis n great world la e buffet… Cine there got amore wat…”</span><span id="3643" class="kn ko ht nr b fi nz nw l nx ny"><strong class="nr hu">sentence2</strong> = “Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C’s apply 08452810075over18's”</span></pre><p id="fdc9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">上面这句话的词汇</strong>形成了:</p><pre class="kf kg kh ki fd nq nr ns nt aw nu bi"><span id="318f" class="kn ko ht nr b fi nv nw l nx ny">{'question', 'great', 'wat', 'rate', 'only', 'T', 'in', 'FA', 'tkts', 'final', 'jurong', 'point', 'Go', 'la', 'crazy..', 'Cine', 'until', 'got', 'to', 'receive', '2', 'comp', 'std', '&amp;', 'wkly', 'amore', 'Available', 'world', 'n', '87121', ')', '08452810075over18', '2005', 'Cup', 'Text', 'entry', 'apply', '.', 'there', 'win', 'buffet', 'e', 'May', "'s", '21st', '(', 'txt', 'Free', '...', ',', 'a', 'C', 'bugis'}</span></pre><p id="3996" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">创建<strong class="jk hu">频率</strong> <strong class="jk hu">字典</strong></p><pre class="kf kg kh ki fd nq nr ns nt aw nu bi"><span id="638e" class="kn ko ht nr b fi nv nw l nx ny"><strong class="nr hu">def</strong> create_word_dict(total, sentence):<br/>    wordDict = dict.fromkeys(total, 0)<br/>    <strong class="nr hu">for</strong> word <strong class="nr hu">in</strong> sentence:<br/>        wordDict[word] += 1<br/>    <strong class="nr hu">return</strong> wordDict</span></pre><h2 id="5698" class="kn ko ht bd kp kq kr ks kt ku kv kw kx jr ky kz la jv lb lc ld jz le lf lg lh bi translated">计算词频</h2><p id="128b" class="pw-post-body-paragraph ji jj ht jk b jl li iu jn jo lj ix jq jr lk jt ju jv ll jx jy jz lm kb kc kd hb bi translated"><strong class="jk hu">词频(TF) </strong>是文档中一个词的计数。有几种方法可以计算这个频率，最简单的是一个单词在文档中出现的实例的原始计数。</p><pre class="kf kg kh ki fd nq nr ns nt aw nu bi"><span id="350a" class="kn ko ht nr b fi nv nw l nx ny"><strong class="nr hu">def</strong> computeTF(wordDict, doc):     <br/>    tfDict = {}<br/>    corpusCount = len(doc)     <br/>    <strong class="nr hu">for</strong> word, count <strong class="nr hu">in</strong> wordDict.items():         <br/>        tfDict[word] = count/float(corpusCount)     <br/>    <strong class="nr hu">return</strong>(tfDict)</span></pre><figure class="kf kg kh ki fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es oa"><img src="../Images/8687dbdf7a8207a68f876beb6552b342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_27MBB2vMPgvvLkJ0bkhLg.png"/></div></div><figcaption class="kj kk et er es kl km bd b be z dx translated">术语频率结果</figcaption></figure><h2 id="b139" class="kn ko ht bd kp kq kr ks kt ku kv kw kx jr ky kz la jv lb lc ld jz le lf lg lh bi translated">计算逆文档频率</h2><p id="60a2" class="pw-post-body-paragraph ji jj ht jk b jl li iu jn jo lj ix jq jr lk jt ju jv ll jx jy jz lm kb kc kd hb bi translated"><strong class="jk hu">逆文档频率(idf) </strong>告诉我们一个单词在整个文档集中的常见或罕见程度。该指标的计算方法是，将文档总数除以包含一个单词的文档数，然后计算对数。如果一个术语频繁地与其他文档一起传播，可以说它不是一个相关的词，如“the”、“is”、“are”等停用词。</p><pre class="kf kg kh ki fd nq nr ns nt aw nu bi"><span id="3a70" class="kn ko ht nr b fi nv nw l nx ny"><strong class="nr hu">def</strong> computeIDF(docList):<br/>    idfDict = {}<br/>    N = len(docList)<br/><br/>    idfDict = dict.fromkeys(docList[0].keys(), 0)<br/>    <strong class="nr hu">for</strong> word, val <strong class="nr hu">in</strong> idfDict.items():<br/>        idfDict[word] = math.log10(N / (float(val) + 1))<br/><br/>    <strong class="nr hu">return</strong> (idfDict)</span></pre><figure class="kf kg kh ki fd hk er es paragraph-image"><div class="er es ob"><img src="../Images/daad80ea77feb3912f6bd0742a47d484.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*d84iTSdZC2HBQVBQU7yhBQ.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">IDF结果</figcaption></figure><p id="f012" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">最后，计算TFIDF </strong></p><pre class="kf kg kh ki fd nq nr ns nt aw nu bi"><span id="ed86" class="kn ko ht nr b fi nv nw l nx ny"><strong class="nr hu">def</strong> computeTFIDF(tfBow, idfs):<br/>    tfidf = {}<br/>    <strong class="nr hu">for</strong> word, val <strong class="nr hu">in</strong> tfBow.items():<br/>        tfidf[word] = val*idfs[word]<br/>    <strong class="nr hu">return</strong>(tfidf)</span></pre><figure class="kf kg kh ki fd hk er es paragraph-image"><div class="er es oc"><img src="../Images/c0252eb183e59681c3f368a7250671be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*MKAVOBuLMo5inDIVCu6k8w.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">TFIDF的结果</figcaption></figure><p id="d095" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">注意</strong>:主要的直觉是，如果一个单词在一个文档中频繁出现，我们认为它是重要的，但是如果一个单词在太多其他文档中出现，它给我们一种直觉，它不是一个唯一的标识符，所以我们通过使用tfidf给它一个低分数来标记那些术语不那么重要。</p><p id="ec90" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi mp translated"><span class="l mq mr ms bm mt mu mv mw mx di"> G </span>上述代码的ithub链接以及其他使用tfidf的代码:<a class="ae lp" href="https://github.com/pemagrg1/Magic-Of-TFIDF" rel="noopener ugc nofollow" target="_blank">https://github.com/pemagrg1/Magic-Of-TFIDF</a></p><h2 id="252a" class="kn ko ht bd kp kq kr ks kt ku kv kw kx jr ky kz la jv lb lc ld jz le lf lg lh bi translated">作业:</h2><p id="550a" class="pw-post-body-paragraph ji jj ht jk b jl li iu jn jo lj ix jq jr lk jt ju jv ll jx jy jz lm kb kc kd hb bi translated">一些你可以自己尝试的东西…</p><ol class=""><li id="64de" class="lq lr ht jk b jl jm jo jp jr ls jv lt jz lu kd lv lw lx ly bi translated">采取任何电影评论列表，获得最相关的词，以及TFIDF热门词。对于预处理<strong class="jk hu">，</strong>使用单词tokenize，lower，lemma，标点移除，num移除，移除单个字符。然后，得到词频和TFIDF顶词。https://www.kdnuggets.com/2018/08/wtf-tf-idf.html</li><li id="9c01" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated">如果不想从头开始编写TFIDF，也可以使用Sklearn的TFIDF矢量器。使用sk-learn Tfidf矢量器，您可以向量化您的语料库并应用聚类算法。</li></ol><h2 id="3961" class="kn ko ht bd kp kq kr ks kt ku kv kw kx jr ky kz la jv lb lc ld jz le lf lg lh bi translated">参考资料:</h2><p id="9445" class="pw-post-body-paragraph ji jj ht jk b jl li iu jn jo lj ix jq jr lk jt ju jv ll jx jy jz lm kb kc kd hb bi translated">[1]<a class="ae lp" href="https://www.kdnuggets.com/2018/08/wtf-tf-idf.html" rel="noopener ugc nofollow" target="_blank">https://www.kdnuggets.com/2018/08/wtf-tf-idf.html</a></p><p id="8569" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">[2]https://en.wikipedia.org/wiki/Tf%E2%80%93idf<a class="ae lp" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"/></p><p id="ac3d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">[3]http://www.tfidf.com/<a class="ae lp" href="http://www.tfidf.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="37e0" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><a class="ae lp" href="https://monkeylearn.com/blog/what-is-tf-idf/" rel="noopener ugc nofollow" target="_blank">https://monkeylearn.com/blog/what-is-tf-idf/</a></p><p id="a511" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">[5]<a class="ae lp" href="https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089" rel="noopener" target="_blank">https://towardsdatascience . com/TF-IDF-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796 d339a 4089</a></p><p id="a84f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">[6]<a class="ae lp" href="https://www.coursera.org/learn/audio-signal-processing/lecture/4QZav/dft" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/audio-signal-processing/lecture/4 qzav/DFT</a></p><p id="5bef" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">[7]<a class="ae lp" href="https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76" rel="noopener" target="_blank">https://towards data science . com/natural-language-processing-feature-engineering-using-TF-IDF-E8 b 9d 00 e 7 e 76</a></p><p id="378d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">[8]<a class="ae lp" href="https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089" rel="noopener" target="_blank">https://towardsdatascience . com/TF-IDF-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d 339 a 4089</a></p><p id="37cd" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">[9]<a class="ae lp" href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/#:~:text=A%20bag%2Dof%2Dwords%20is,the%20presence%20of%20known%20words." rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/gentle-introduction-bag-words-model/#:~:text = A % 20 bag % 2d of % 2d words % 20 is，the % 20 presence % 20 of % 20 know % 20 words。</a></p><h2 id="94f3" class="kn ko ht bd kp kq kr ks kt ku kv kw kx jr ky kz la jv lb lc ld jz le lf lg lh bi translated">用于实施的附加介质资源</h2><ol class=""><li id="8478" class="lq lr ht jk b jl li jo lj jr nn jv no jz np kd lv lw lx ly bi translated"><a class="ae lp" rel="noopener" href="/@armandj.olivares/a-basic-nlp-tutorial-for-news-multiclass-categorization-82afa6d46aa5">新闻多类分类的基本自然语言处理教程</a></li><li id="7225" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated"><a class="ae lp" rel="noopener" href="/hackernoon/finding-the-most-important-sentences-using-nlp-tf-idf-3065028897a3">使用自然语言处理寻找最重要的句子&amp; TF-IDF </a></li><li id="5ba1" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated"><a class="ae lp" rel="noopener" href="/@acrosson/summarize-documents-using-tf-idf-bdee8f60b71">使用Tf-Idf </a>汇总文件</li><li id="eb34" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated"><a class="ae lp" rel="noopener" href="/machine-learning-intuition/document-classification-part-2-text-processing-eaa26d16c719">文档分类</a></li><li id="65a8" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated"><a class="ae lp" rel="noopener" href="/free-code-camp/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3">基于内容推荐器</a></li><li id="9845" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated"><a class="ae lp" href="https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-9-neural-networks-with-tfidf-vectors-using-d0b4af6be6d7" rel="noopener" target="_blank">推特情绪分析</a></li><li id="cae6" class="lq lr ht jk b jl lz jo ma jr mb jv mc jz md kd lv lw lx ly bi translated">Fi <a class="ae lp" href="https://towardsdatascience.com/finding-similar-quora-questions-with-bow-tfidf-and-random-forest-c54ad88d1370" rel="noopener" target="_blank">用BOW，TFIDF和Xgboost </a>寻找相似的Quora问题</li></ol></div></div>    
</body>
</html>