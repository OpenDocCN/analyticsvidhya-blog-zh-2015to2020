<html>
<head>
<title>Maths behind Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归背后的数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-simplified-a038de4a8143?source=collection_archive---------8-----------------------#2019-09-26">https://medium.com/analytics-vidhya/logistic-regression-simplified-a038de4a8143?source=collection_archive---------8-----------------------#2019-09-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f834" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">逻辑回归的几何解释</h2></div><p id="31ec" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇文章中，我将用一些方程以最简单的方式解释逻辑回归。我期望有一些非常基础的机器学习主题知识作为先决条件。</p><h2 id="6817" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">如何训练逻辑回归模型？</h2><p id="5de7" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">有许多超平面P’可以将两个类分开，但是我们想要一个平面，使得数据点离该平面的总距离尽可能大(这将使我们的模型更好地用于数据的一般化)。所以检查下图，我们的目标是找到最好地分离两个类的超平面<strong class="iz hj"> P </strong>。为了找到这个平面<strong class="iz hj"> P </strong>，我们需要找到双参数<strong class="iz hj"> W </strong> &amp; <strong class="iz hj"> b </strong>使得平面(P)与两个类的距离最大化。这里，<strong class="iz hj"> W </strong>是垂直于平面<strong class="iz hj"> P </strong>的向量，而<strong class="iz hj"> b </strong>是偏置，如果平面通过原点，偏置为零。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es kt"><img src="../Images/7c8876999de25ce26921e4bb6bfbed94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*K40K1QkmSkzkC9A5pMkmkQ.png"/></div></figure><p id="f99f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该点到平面P的距离可以用下面的公式表示。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lb"><img src="../Images/273671efb97279f7beeff511c5c5ab1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*6ngtBN647r9FSrHF8-zSGw.png"/></div></figure><p id="ef01" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里||w||总是垂直于平面P，所以它的值总是1。所以距离的公式变成如下。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lc"><img src="../Images/d248e637fef53017873cd885dc845a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*1zfM4gN05nnzVTwDXcSTJA.png"/></div></figure><p id="ce8d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意，当W.T*Xi &gt; 0时，我们的Yi为正(+1)，当W.T*Xi  &lt; 0时，我们的Yi为负(-1)</p><h2 id="9e82" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">分类器怎么知道点分类正确与否？</h2><p id="57cd" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">让我们通过案例来理解上述问题</p><ol class=""><li id="db90" class="ld le hi iz b ja jb jd je jg lf jk lg jo lh js li lj lk ll bi translated">当Yi为正且<strong class="iz hj"> W.T*Xi </strong>为正时，则<strong class="iz hj"> Yi*W.T*Xi </strong> &gt;为0，表示该点分类正确</li><li id="a27a" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated">当Yi为正而<strong class="iz hj"> W.T*Xi </strong>为负时，则<strong class="iz hj"> Yi*W.T*Xi </strong> &lt;为0，这意味着该点分类不正确</li><li id="8ced" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated">当Yi为负且<strong class="iz hj"> W.T*Xi </strong>为正时，则<strong class="iz hj"> Yi*W.T*Xi </strong> &lt;为0，这意味着该点被错误分类</li><li id="81f7" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated">当Yi为负且<strong class="iz hj"> W.T*Xi </strong>为负时，则<strong class="iz hj"> Yi*W.T*Xi </strong> &gt;为0，这意味着该点被正确分类</li></ol><p id="addd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以得出结论，当Yi*W.T*Xi为正时，我们的模型对这些点进行了正确分类，因此我们需要一个模型，使得<strong class="iz hj"> W.T*Xi </strong>对于更多的数据点为正。这个问题在数学上可以写成如下。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lr"><img src="../Images/3d07375575b55dc04c354a38ea82b698.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*p2jnaTsdKSfTheVRi7gDlw.png"/></div></figure><p id="c0dc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里注意，X是训练数据，Y是数据的标签，所以我们只能调整W的值(在平面不通过原点的情况下也是b的值)。</p><p id="c536" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面的等式说我们需要找到一个使<strong class="iz hj"> W* </strong>最大化的平面<strong class="iz hj"> P </strong>(正确分类更多点)。</p><h2 id="5d92" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">为什么叫逻辑回归？Sigmoid函数的需求是什么？</h2><p id="8707" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">很简单，我们的分类器从超平面中找到Yi*W.T*Xi的值，并将所有值相加。现在，假设我们的数据集有一个异常值，如下图所示，它离平面非常远，如下图所示。然后，由于这个孤立点，我们的模型将考虑不同的超参数，并使超平面可能看起来像P ’,这是一个与我们所要求的完全不同的平面。在这里，由于一个孤立点，整个模型都会改变，并且会有不同的表现。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ls"><img src="../Images/75d4a6a059320256bbe2158f656bc805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*s2Os1Vn6kzycS8IkO5EwUw.png"/></div></div></figure><p id="7377" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们之前讨论的，我们的分类器简单地总结了如下所有的距离，所以我们的平面将与实际平面非常不同。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lx"><img src="../Images/d3a40977f4c60fcc8bf2ac2754d157b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*POokTXsNlbV-eUZKM5o5_A.png"/></div></figure><p id="b022" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以通过压缩过大的值来解决这个问题。sigmoid函数的公式如下，其范围从0到1</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es ly"><img src="../Images/698e80fca4ce988c9265dfe00efd58e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*ghxlvvf3Lth0srn6400UqQ.png"/></div></figure><p id="0dd2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">为什么Sigmoid函数有意义？</strong></p><p id="d8b1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">把这个问题想成一个概率的观点，当一个查询点离超平面的距离为零时，该点分类的概率是多少？<strong class="iz hj"> 0.5 </strong>。<strong class="iz hj"> </strong>这是sigmoid函数的精确值，此时值(在本例中为距离)为零，请看下图中的sigmoid函数</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lz"><img src="../Images/c46843100d238cd53813a0b3ef6c4467.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*tCHjDGEq0LhJBtwuQYCUMA.png"/></div></figure><p id="c741" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以我们将用一个sigmoid函数来修改我们原来的问题方程</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es ma"><img src="../Images/bdb1653475390113dd0db8b5f108f41c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*ri8A0g62rNvbLyyWP7q1Yw.png"/></div></figure><p id="6c87" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上述优化问题可以修改如下(跳过所有复杂的数学)</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mb"><img src="../Images/657027c2fe87ac947bd3abe272a4678f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*yQqRdmXzNPpWr5is5MYLgQ.png"/></div><figcaption class="mc md et er es me mf bd b be z dx translated">最优化问题逻辑回归</figcaption></figure><p id="c2af" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的等式中，有<strong class="iz hj"> log(1+something) </strong>，它总是正的，我们想要一个使完整等式最小化的W值(对我们来说，这个值是W*)。为了使这个值最小(接近零)，我们需要W的值为无穷大。现在我们的X和Yi是来自数据的值，所以它们的值不能改变，所以我们需要我们的W是无穷大，使方程的值为零。</p><p id="1b3d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当这个权重W值太高时，我们的模型遭受过拟合的问题</p><p id="07b9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">如何才能解决这个过拟合问题？</strong></p><p id="b7c6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">正规化。</strong>当我们训练我们的模型进行预测时，它通常会过度拟合训练数据，并且无法对未来数据进行归纳。正则化是一种通过添加复杂性项来惩罚模型的技术，该复杂性项会给更复杂的模型带来更大的损失。在下面的等式中，正则化函数形式的惩罚被添加到损失函数中。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mg"><img src="../Images/89a086806b00e9967c0837a718ed184c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*F0vhCZPoTqouELBuo44C8Q.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">L2正则化逻辑回归</figcaption></figure><p id="dbec" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">正则化如何帮助解决过度拟合问题？</strong>正则化防止<strong class="iz hj"> W </strong>变成+无穷大或-无穷大，这是损失项和惩罚项之间的折衷。</p><p id="32d0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">如何帮助</strong>λ<strong class="iz hj">？</strong>当lambda为零时，唯一的损失函数不存在会使模型过拟合的惩罚项。随着λ值的增加，正则化项的影响也增加，并且模型更好地泛化。</p><h2 id="a36a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">L1正则化</h2><p id="c166" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">其中L2正则化是罚函数中权重的平方，而L1正则化是罚函数中的绝对权重值，因此使用L1正则化的损失函数的等式如下。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mh"><img src="../Images/8209ed0cb3514f1bd23ae13cc6bd7611.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_VgRrreU2uMMsinXqVVBog.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">L1正则化的logistic回归方程</figcaption></figure><p id="cab0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意，L1正则化在权重矩阵中产生稀疏性，这意味着所有不太重要的特征变为零。当我们有非常高的数据维度时，这是非常有用的。</p><h2 id="6a97" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">逻辑回归如何预测？</h2><p id="7061" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">在对数据进行模型训练后，我们有了权重矩阵<strong class="iz hj"> W </strong>，我们的查询点是我们想要预测的<strong class="iz hj"> Xq </strong>。我们只需要W和Xq的简单乘法来预测我们的查询点的类别</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mi"><img src="../Images/9bbe1d3c73a66b500e676d58bac74de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*sOjdQ-3mS7kaNC2n4yc_FA.png"/></div></figure><p id="dbb5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意，如果W.TXq的值较大，我们的模型在预测类别时会更有信心(如果该值更接近0，则模型的信心会较低)</p><p id="d382" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归是高度<strong class="iz hj">可解释的</strong>。在训练模型之后，我们有权重W。如果我们找到每个特征的绝对值，并将它们降序排列，那么最重要的特征将位于顶部。</p><h2 id="c935" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">逻辑回归的假设</h2><p id="64d8" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">只有当我们作出以下假设时，上述一切才有可能。</p><ol class=""><li id="c4e7" class="ld le hi iz b ja jb jd je jg lf jk lg jo lh js li lj lk ll bi translated">数据应该是线性可分的</li><li id="80b3" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated">数据不应是多重共线的</li></ol><h2 id="d394" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">逻辑回归的时空复杂性</h2><p id="dbc8" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">在训练时，我们必须遍历每个数据点，因此训练时间复杂度为<strong class="iz hj"> O(nd) </strong></p><p id="1725" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该模型的时空复杂度将是O(d) ,因为我们只需要存储用于预测的权重矩阵</p><p id="fcb4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该模型的运行时复杂度将为<strong class="iz hj"> O(d)，</strong>因此这对于低延迟应用非常有用。</p><p id="997d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">仅此而已！</p><p id="b543" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你想联系我，请在我的LinkedIn、<a class="ae mj" href="https://twitter.com/isapansoni" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上联系我</p><p id="ea04" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢阅读帖子，继续鼓掌</p></div></div>    
</body>
</html>