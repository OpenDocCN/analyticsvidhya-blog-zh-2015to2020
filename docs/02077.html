<html>
<head>
<title>The Unusual impact of K-NN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K-NN的不同寻常的影响</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-unusual-impact-of-k-nn-48617b81ba80?source=collection_archive---------16-----------------------#2019-11-28">https://medium.com/analytics-vidhya/the-unusual-impact-of-k-nn-48617b81ba80?source=collection_archive---------16-----------------------#2019-11-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/4479c03e687182b519b236c0c471525a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wct8Nk_GgKoJXHG0sPO4kA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">信用:<a class="ae hv" href="http://68church.com/wp-content/uploads/2017/06/Love-Your-Neighbor.jpg" rel="noopener ugc nofollow" target="_blank">68教堂</a></figcaption></figure><div class=""/><pre class="iv iw ix iy fd iz ja jb jc aw jd bi"><span id="e50c" class="je jf hy ja b fi jg jh l ji jj">This blog is beginner friendly, will start from scratch and cover upto a medium-advance level of intuitive understanding of K-NN. </span><span id="1e17" class="je jf hy ja b fi jk jh l ji jj">All the techniques and concepts are covered using <a class="ae hv" href="https://jamesclear.com/first-principles" rel="noopener ugc nofollow" target="_blank">first principle technique</a>.</span><span id="1467" class="je jf hy ja b fi jk jh l ji jj">This blog uses various references as well which I have mentioned below.</span><span id="20a7" class="je jf hy ja b fi jk jh l ji jj">Criticism and doubts are welcome.<br/>For more you can go through resources referred in the end.</span><span id="f73f" class="je jf hy ja b fi jk jh l ji jj">Hope you enjoy learning with me :)</span></pre></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><h2 id="d790" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">目录</h2><ol class=""><li id="3b43" class="ko kp hy kq b kr ks kt ku kb kv kf kw kj kx ky kz la lb lc bi translated">对K-最近邻(K-NN)的直观理解</li><li id="123b" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">K-NN的行为</li><li id="0b6e" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">K-NN的优缺点</li><li id="c392" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">使用K-NN而不是分类和回归</li><li id="af2f" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">什么时候不用K-NN？</li><li id="d3f5" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">伪代码实现</li><li id="cbcb" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">修改你的学习</li><li id="f2dd" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">K-NN上的结论和面试问题</li><li id="2b57" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">参考</li></ol></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><h1 id="97c7" class="li jf hy bd js lj lk ll jw lm ln lo ka lp lq lr ke ls lt lu ki lv lw lx km ly bi translated">1.K-NN直觉理解</h1><p id="4f7a" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated"><em class="mo">在最简单的意义上，K-NN是一种机器学习模型，可用于分类和回归问题，根据其“K”个最近邻居的行为预测查询输出。</em></p><p id="a05f" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><a class="ae hv" href="https://datascienceplus.com/k-nearest-neighbors-knn-with-python/" rel="noopener ugc nofollow" target="_blank"><strong class="kq hz">K-NN的伪算法</strong> </a> <strong class="kq hz"> : </strong></p><p id="6b13" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">训练算法:</p><ol class=""><li id="a5ce" class="ko kp hy kq b kr mp kt mq kb mu kf mv kj mw ky kz la lb lc bi translated">存储所有数据</li></ol><p id="32cd" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">预测算法:</p><ol class=""><li id="6599" class="ko kp hy kq b kr mp kt mq kb mu kf mv kj mw ky kz la lb lc bi translated">计算从x到数据中所有点的距离</li><li id="7571" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">通过增加与x的距离对数据中的点进行排序</li><li id="44fb" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">预测“k”个最近点的多数标注</li></ol></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><p id="6f25" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">让我们假设一个分类，例如</p><p id="9363" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">假设我们要使用两个特征'<strong class="kq hz">身高</strong>和'<strong class="kq hz">头发长度</strong>'来对两个阶级'<strong class="kq hz">男性</strong>和'<strong class="kq hz">女性</strong>'进行分类。</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mx"><img src="../Images/60700e2b8824b8323079a0f69cc17bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zNpbwGWt8wEqWmk0hqm6Kw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">数据集可视化</figcaption></figure><p id="9300" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> <em class="mo">对数据集</em> </strong>的观察:<strong class="kq hz">数据集不平衡</strong>，即与男性数据点相比，女性数据点的数量非常多。并且它还<strong class="kq hz">包含很少的离群值</strong>来模拟真实生活数据集。<strong class="kq hz">数据集男女比例为70:30 </strong></p><p id="a54a" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">现在，我们想使用K-NN来预测查询点x <em class="mo"> _q的类别。</em></p><p id="3c15" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><em class="mo">注意:K-NN模型中的“K”是一个超参数，需要对其进行超调以获得K-NN的良好性能</em></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es my"><img src="../Images/7e1d7416b3df5648e023aa6f18b25d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-lYiBKAU_ZML2Kq9Ma7VA.png"/></div></div></figure><p id="8f1d" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">现在让我们取k = 5，并计算在<em class="mo"> x_q. </em>周围的5个最近的邻居，将这些邻居可视化在假想的圆内。</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mz"><img src="../Images/3fb71d661801f2ae34bd832cc0eccc71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-3lwzTOiuNcxjjvDp3ekA.png"/></div></div></figure><p id="0014" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">现在，看看裁剪区域的大图</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es na"><img src="../Images/091380834b60d33175e448bef318cb21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*lXeFRQ7Ib9IRzTGcuQR0aA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">黄色圆圈内的数据点代表X_q的5个最近邻居</figcaption></figure><p id="e5b2" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">由于我们已经计算了<em class="mo"> x_q的5个最近邻，</em>该检查我的<em class="mo"> x_q </em>属于哪个类了。</p><p id="012e" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">取所有5个最近的邻居，并找到多数标签。在这个特定的例子中，5个邻居中有4个属于女性类(<em class="mo">红色方块</em>)。这意味着大多数邻居是女性。</p><p id="13f9" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">我的查询数据点<em class="mo"> x_q </em>有80%的概率属于女性类，因此我们将我们的查询点标记为女性。就这么简单。</p></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><h1 id="50a7" class="li jf hy bd js lj lk ll jw lm ln lo ka lp lq lr ke ls lt lu ki lv lw lx km ly bi translated">2.K-NN的行为</h1><h2 id="4265" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">2.1】K-NN中K的影响</strong></h2><p id="efcb" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">我们将看到改变K-NN中的“K”如何影响模型的性能。</p><p id="7627" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> <em class="mo"> x_q </em>是我们的查询点。使用K-NN中‘K’的不同值预测<em class="mo"> x_q的类别。</em> </strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nb"><img src="../Images/89a0a85b43acd769841d16c7c58a01e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6fzJ5yKYYVRDTQgpBMf0g.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js"> <em class="nc"> x_q </em>是我们的查询点</strong></figcaption></figure></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><h2 id="8b1e" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak"> a. K=1 </strong></h2><ol class=""><li id="db19" class="ko kp hy kq b kr ks kt ku kb kv kf kw kj kx ky kz la lb lc bi translated">实际班级标签:<strong class="kq hz">女</strong></li></ol><p id="f298" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">让我们看看我的模型预测了什么</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nd"><img src="../Images/a14df768a4ae8f2a81d1ff05e13e39ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xarhrDsnrmpRyYUcO1DBwA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">正确预测</strong>:x _ q的实际类标签是女性，我们的模型也预测了女性</figcaption></figure><p id="d140" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">预测类标签:<strong class="kq hz">女</strong></p><p id="4aa8" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">正确预测</strong>:x _ q的实际类标签是女性，我们的模型也预测了女性</p><p id="977a" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">2.实际阶级标签:<strong class="kq hz">女</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ne"><img src="../Images/a4e9a119102b5cc6e2f90e07c2e672f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A0B4xytiKHwQnGW8GxKgXA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">错误预测</strong>:x _ q的实际类别标签是女性，但我们的模型预测的是男性</figcaption></figure><p id="270e" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">预测类标签:<strong class="kq hz">男</strong></p><p id="6c93" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">错误预测</strong>:x _ q的实际类别标签是女性，但我们的模型预测了男性，因为它的1-NN是男性，这可能是异常值或例外。</p><p id="b7f8" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">3.实际班级标签:<strong class="kq hz">男</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nf"><img src="../Images/0df5f522bea642d1225ad2c64cf6a043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*22i0azVYI7ivXsQvZQp0-A.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">正确预测</strong>:x _ q的实际类标签是男性，我们的模型也预测了男性</figcaption></figure><p id="7f8e" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">预测类标签:<strong class="kq hz">男</strong></p><p id="d2b2" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">正确预测</strong>:x _ q的实际类标签是男性，我们的模型也预测了男性</p><p id="a065" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">4.实际班级标签:<strong class="kq hz">男</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ng"><img src="../Images/7d6fb89e760cfe2c64e40f2d3cd529ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XR2L_eSYDHUTz3Nynvbpkg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">错误预测</strong>:x _ q的实际类别标签是男性，但我们的模型预测的是女性</figcaption></figure><p id="6858" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">预测类标签:<strong class="kq hz">女</strong></p><p id="7eef" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">错误预测</strong>:x _ q的实际类别标签是男性，但我们的模型预测为女性，因为它的1-NN是女性。</p><p id="5e63" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">观察(当K=1时):</strong></p><p id="d0ca" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">当选择k=1时，获得错误预测的可能性非常高。这是因为我们的模型过于努力地拟合训练数据，导致模型过度拟合，导致模型中的高方差。</strong></p><p id="69bf" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">现在，让我们看看当k=5时，K-NN性能如何变化</p></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><h2 id="23cd" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak"> b. K= 5 </strong></h2><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nh"><img src="../Images/3e00e3a8d219eb6010096c1278cd6ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UoQXBcsqpzjr24K7foP89Q.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">在所有3个数字中，我们的查询点是正确分类的</figcaption></figure><p id="2ef9" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">观察(当K=5时):</strong></p><p id="dc84" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">当K = 5时，观察到我们的模型能够分类所有的查询点x_q。</p><p id="498b" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">当K-NN中的“K”增加时，模型的高方差的影响减小。因此，看起来这是过度适应和欠适应之间的完美权衡(细节在博客后面)。</p></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><h2 id="7fd4" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak"> c. K= 100 </strong></h2><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ni"><img src="../Images/ef854d6124d8a6eed9e99072c8da2ea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8s0UYweOmcN5zGPnmJM4mA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图1: <strong class="bd js">正确分类</strong> — — —图2: <strong class="bd js">错误分类</strong>——<strong class="bd js">—</strong>图3: <strong class="bd js">错误分类</strong></figcaption></figure><p id="a60f" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">观察(当K=100时):</strong></p><p id="9cd1" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">在K-NN中使用非常高的“K”显然不符合模型。这意味着哪个是多数阶级，它将支配我的少数阶级，并且模型在预测阶级标签时将高度偏向我的多数阶级。</p><h2 id="9b09" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">2.2]在K-NN中选择正确的“K”值</h2><p id="f875" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">在选择正确的“K”值之前，我们首先需要了解我的模型性能是如何通过以图形方式改变“K”值而改变的。我们只需要绘制上述观察结果</p><p id="1c6a" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">过度配合和欠配合之间的权衡</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nj"><img src="../Images/0894581bbefa2554cd2527156f8c6e84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OpCDFFUdPUquhwE1tbrF7w.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">随着“K”的增加，验证准确度分数增加，并且在阈值之后，性能逐渐降低</figcaption></figure><p id="e45b" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">使用肘点或膝点来选择最佳的‘K’值</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nk"><img src="../Images/20979c4fd6a9bd0ac20d525bec5ab768.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dWtOUrMFgSWn3WtlYrnK6A.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">方差和偏差之间的权衡</figcaption></figure><p id="6710" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">为了选择“K”的最佳值，我们需要注意上图中的拐点或拐点，并选择我们获得最大验证或测试精度的点。</p><p id="9663" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">这里，在K=8时可以观察到“K”的最佳值。</p><h2 id="89a3" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">2.3]K-NN的判定边界</h2><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nl"><img src="../Images/4ad526f2f5b5b91fb05b380f80c881e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZMTz31Yk62KBmjtvx-iB9Q.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">上面的例子不是指我们的数据集，而是取自kaggle以便理解(<a class="ae hv" href="https://stats.stackexchange.com/questions/21572/how-to-plot-decision-boundary-of-a-k-nearest-neighbor-classifier-from-elements-o" rel="noopener ugc nofollow" target="_blank">参考</a>)</figcaption></figure><p id="be6a" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">K-NN的决策面是非线性的，因此有时它比其他线性模型(如逻辑回归)更好，特别是当数据在模型性能方面不是线性可分的时候。</p><h2 id="5498" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">2.4]使用哪种距离度量？</h2><p id="80b5" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">在训练K-NN模型时，我们已经看到，为了计算邻居，我们需要计算查询点和所有其他点之间的距离，以找到“K”个最近的邻居。但是我们有许多可用的距离度量，从中选择哪一个呢？</p><p id="b43a" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">选择<strong class="kq hz">距离度量</strong>取决于你的数据集和需求。没有经验法则。但是，要理解数据集的要求并了解哪种距离测量在各种情况下都适用，最好了解一些常见距离测量的属性。</p><p id="8e83" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> a .欧几里德距离</strong></p><p id="a90e" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> b .曼哈顿距离</strong></p><p id="1936" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> c .闵可夫斯基距离</strong></p><p id="f8f9" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> d .余弦距离</strong></p><p id="5091" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> e .海明距离</strong></p></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><p id="68d8" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> a .欧几里德距离</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nm"><img src="../Images/a1e8618150214c664d44b2e3ae35ee82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cwSWcViVZ7iAJ_Xb3Y5aGw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">我们有二维的两个向量点x1和x2，<a class="ae hv" href="https://hlab.stanford.edu/brian/making7.gif" rel="noopener ugc nofollow" target="_blank">欧几里得距离b/w x1和x2</a>如图所示。</figcaption></figure><p id="5c54" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">欧几里得的通式:</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nn"><img src="../Images/de6d3e231bf46fd13899db584448994d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d1oOlFb1KRTCG3ad_OwwZw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">欧氏距离测度的一般公式</figcaption></figure><p id="69d0" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">简单来说，<strong class="kq hz">欧几里德距离</strong>可以解释为两点之间的最短距离或者两点的位移。</p><p id="72ec" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> b .曼哈顿距离</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es no"><img src="../Images/bfa001f26dc9b17208b7d3bdc52a4f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_32yQXTHpYixVQxA9hFq5g.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">A和B是两个二维向量点，d(A，B)是A和B之间的<a class="ae hv" href="https://predictiveprogrammer.com/wp-content/uploads/2019/04/manhattan_distance.jpg" rel="noopener ugc nofollow" target="_blank"> <strong class="bd js">曼哈顿距离</strong></a></figcaption></figure><p id="2176" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">曼哈顿距离的一般公式:</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es np"><img src="../Images/ffd603448117d258b28dfd3c207730e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fLGXZrFWpe90VUlGa-YrIA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">曼哈顿距离测度的一般公式</figcaption></figure><p id="3ca3" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">曼哈顿距离</strong>可以看做两个矢量点的绝对差之和。</p><p id="e3e1" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> c .闵可夫斯基距离</strong></p><p id="2946" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">闵可夫斯基距离的一般公式:</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nq"><img src="../Images/8e84b0f4409ad7f020d437f1fd1eea60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ls9BAV5j1SUMC7J6pnqVTw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">闵可夫斯基距离的一般公式</figcaption></figure><p id="6265" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">如果闵可夫斯基距离中的<strong class="kq hz">‘p’= 1</strong>，则变成<strong class="kq hz">曼哈顿距离</strong>。</p><p id="1157" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">如果<strong class="kq hz">‘p’= 2</strong>在闵可夫斯基那里，就会变成<strong class="kq hz">欧几里德距离</strong>。</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nr"><img src="../Images/76202e8a8ac8cabbe94c931289dfd022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D-qvj4FcVoK2EZHe3lxy2g.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">比较</strong> <a class="ae hv" href="https://theprofessionalspoint.blogspot.com/2019/01/knn-algorithm-in-machine-learning.html" rel="noopener ugc nofollow" target="_blank"> <strong class="bd js">欧几里德距离和曼哈顿距离</strong> </a></figcaption></figure><p id="ffc9" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> d .余弦距离</strong></p><p id="ddc0" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">它可以解释为两个矢量点之间的角度差。</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ns"><img src="../Images/2c042ab63eeaea41bc061197c78228cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9rEfhjxEyueHhqdkXHL2zA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">余弦距离的广义公式</figcaption></figure><p id="028e" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">直观的讲就是和向量点有多相似。x1和x2之间的角度越小，它们越相似，反之亦然。</p><p id="f8af" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> e .海明距离</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es nt"><img src="../Images/724f8cac40e31a5ef08df943b07bc0c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*zUlnpA0NvMYSpSotk3pKgw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><a class="ae hv" href="https://i.pinimg.com/originals/f7/9c/fe/f79cfe41a0c007a2cf4f94856ff88133.jpg" rel="noopener ugc nofollow" target="_blank">海明距离</a></figcaption></figure><p id="a7ff" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">为了<strong class="kq hz">理解汉明距离</strong>，以两个相同长度的字符串为例，然后在相应的索引中统计两个字符串中不同的字数。总差之和直观上就是海明距离。</p><p id="b1b3" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">汉明距离最常见的例子是<a class="ae hv" href="https://www.researchgate.net/figure/Hamming-distance-matrix-visualizing-the-number-of-genes-differing-between-any-2-samples_fig1_11054265" rel="noopener ugc nofollow" target="_blank">基因编码</a>。</p><p id="0899" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">现在我们已经看到了一些常见距离度量的性质。让我们理解在选择距离度量时应该记住几个概念和约束。</p><p id="fc22" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">维数灾难:</strong>直观意义上，维数灾难是指当维数或特征的数量太大时，两点之间的差值为零。要获得更精确的理解，请参考<a class="ae hv" href="https://www.wikiwand.com/en/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">和</a>。</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es nu"><img src="../Images/72a696386f68238d607022b948c97564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*dYsktfszpYL4d4RZvkYsvQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">维度的诅咒</figcaption></figure><p id="6a49" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">选择正确的距离度量:</strong></p><p id="925c" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> <em class="mo">欧几里德距离</em>和<em class="mo">曼哈顿距离</em>对维度诅咒的影响最大。</strong>因此，当数据集的维数过大时，尽量避免使用欧几里德和曼哈顿距离度量，因为它们直接计算两个向量点之间的差异。</p><p id="2033" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">余弦距离也会受到维数灾难的影响，但与上面两个相比，影响较小</strong>，因为它计算角度差。这也可以作为欧几里德距离的替代，以减少维数灾难的影响。</p><p id="8422" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">汉明距离不会因为维数灾难</strong>而受到影响，因为它计算的是1–0差异。但是海明距离的应用却很少。</p><p id="13c7" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">希望如此，现在您可能已经知道在K-NN中计算邻居到查询点的距离时如何选择所需的距离度量</p><h2 id="ed2a" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">2.5</strong><strong class="ak">距离加权K-NN </strong></h2><p id="4e74" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">我们不应该给那些更靠近查询点的邻居更多的权重，而不是给所有的邻居相同的权重吗？</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nv"><img src="../Images/6b7e6ca95d7fda4f623e9a4a16d51bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bchr8gL5wIyRe4CpB6rIrg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">该图像中的预测类标签应该是什么？</figcaption></figure><p id="f7db" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">使用简单的K-NN多数投票，我们的模型会说查询点属于女性类，但我们应该给靠近查询点的两个邻居更多的权重，而给其他三个邻居较少的权重吗？</p><p id="90fb" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">距离加权K-NN正是这样做的。它是简单K-NN的<strong class="kq hz">修改版本，给最近的邻居更大的权重，给更远的邻居更小的权重。</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nw"><img src="../Images/cca3e11797eec500f4da6f0fffddf7cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P1ISzVuhNe3mgyLnORv5bA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">距离加权K-NN </strong>示例</figcaption></figure><p id="f13d" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">在上面的例子中，如果我们使用简单的K-NN，那么我们预测的类别标签将是女性，因为多数投票，但是<strong class="kq hz">使用加权的K-NN，我们能够得到正确的类别标签(男性)。</strong></p><p id="5940" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">加权K-NN在回归问题中也很有用。</strong></p><h2 id="93a8" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">2.6]可解释性</h2><p id="1c7b" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">K-NN给出的<strong class="kq hz">解释</strong>并不十分严谨，但它足以让我们知道为什么模型给出特定的预测。K-NN给出的唯一解释形式是它的‘K’个邻居。我们只能看到其“K”个邻居的行为或性质，并解释基于查询点相对于其邻居的相似性通过模型给出特定预测的推理。</p><p id="2f26" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">例如</strong>，让我们利用医疗数据集来预测一个人是否患有<strong class="kq hz">乳腺癌</strong>？</p><p id="2283" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">在使用K-NN之后，模型已经预测‘是’这个人患有乳腺癌。因此，为了知道为什么模型说如果这个人患有癌症，我们将查看其“k”个邻居，并检查查询点与其邻居之间的相似性。</p><p id="1f89" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">让k=5，因此在检查查询点和它的5个邻居之间的相似性之后，我们观察到5个邻居中的4个具有作为查询点的save症状，并且它们都具有阳性乳腺癌。这就是为什么我的K-NN模型也说查询点有80%的概率患乳腺癌。</p><h2 id="8df3" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">2.7</strong><strong class="ak">特征重要性</strong></h2><blockquote class="nx ny nz"><p id="c1fb" class="lz ma mo kq b kr mp mb mc kt mq md me oa mr mg mh ob ms mj mk oc mt mm mn ky hb bi translated"><a class="ae hv" rel="noopener" href="/bigdatarepublic/feature-importance-whats-in-a-name-79532e59eea3"> <em class="hy">作为数据科学家，我们通常专注于优化模型性能。然而，有时理解模型中的特征如何有助于预测也同样重要。不幸的是，表现更好的模型往往也更不透明。</em> </a></p></blockquote><p id="6820" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">抱歉，但是K-NN没有隐含地给出任何类型的特征重要性。从K-NN中明确获得特征重要性的一种方法是<strong class="kq hz">前向特征选择。</strong>更多详情请参考<a class="ae hv" href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/" rel="noopener ugc nofollow" target="_blank">本。</a></p><h2 id="8c07" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">2.8 —用于多类分类的K-NN</h2><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es od"><img src="../Images/b2a92992f967fd601d4468823cfdd5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vYXCZsd81DTLIwjCZE-SkA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">多类分类示例</figcaption></figure><p id="9859" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">K-NN可以很容易地扩展到多类分类。因为最终K-NN只对类标签做多数表决，所以类是二进制还是多进制并不重要。</p><h2 id="8342" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">2.9</strong><strong class="ak">用于回归的K-NN</strong></h2><p id="6fcf" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">在学习如何解决回归问题之前，首先我们需要了解什么是回归问题？</p><p id="8e54" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">分类问题和回归问题之间的区别在于，在分类的情况下，我们的类别标签是离散值(例如0，1，2，3…等)，但在回归的情况下，我的类别标签或我的预测变量“Y”属于真实值(0.01，0.05，0.1，1.5，11，1.99…等)。</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es oe"><img src="../Images/21ee123ed60134ac09d8f38c62b6edd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AFgIrj_wYRh0rdMKvM_1Sw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">回归和分类的区别</figcaption></figure><p id="0cf5" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">现在我们知道了什么是回归问题，让我们深入研究解决回归问题。</p><p id="ea86" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">让我们举一个例子:</p><p id="21a7" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">我们想要预测一个公司想要雇佣的员工的估计工资应该是多少，基于它的“资格”、“经验”、“技能”、“个性”等等。</p><p id="6f08" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">我们将应用K-NN，并根据相似性找出查询人最近的“K”个邻居，并尝试找出邻居工资的集中趋势。</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es of"><img src="../Images/3ff8d8abfe6505a95d4e541acfa6588a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WnJf3ZoiTG78MdTLt8a52A.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">回归问题示例</figcaption></figure><p id="cfd3" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">假设最佳k=5:</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es og"><img src="../Images/82ad33b2f54d118ec22344186694b195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bNOQcTkClG6BWbv0DfFy9A.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">在找到“k”个邻居后</figcaption></figure><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es oh"><img src="../Images/06ea58f1caa613a31baaceed66c50b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDsYDtji94xsWWXtcVdpeQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">寻找邻居工资的平均值</figcaption></figure><p id="8ba9" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">K-NN预测查询人的薪资为26.4 K(即1000)。</strong></p><h2 id="505b" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">2.10]K-NN的复杂性</h2><p id="0a0b" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">K-NN的复杂性取决于我们在K-NN模型中使用的算法。</p><p id="6980" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">在研究K-NN的伪算法时，我们已经看到，在训练时间，它存储所有的数据点，而所有的工作K-NN在测试时间内寻找邻居。</p><p id="db9d" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">这就是为什么K-NN也被称为<strong class="kq hz">懒学习者、</strong>的原因之一，因为它在训练时间里什么也学不到。</p><p id="23e0" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">根据我们使用什么样的<strong class="kq hz">数据结构</strong>来存储训练数据点，决定训练时间复杂度和运行时间复杂度。</p><p id="6a69" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">如果你想了解更多关于K-NN中可以使用的算法的细节和工作原理，请参考<a class="ae hv" href="https://www.wikiwand.com/en/K-d_tree" rel="noopener ugc nofollow" target="_blank">本</a>和<a class="ae hv" href="https://www.wikiwand.com/en/Locality-sensitive_hashing" rel="noopener ugc nofollow" target="_blank">本</a>。这里我只讨论K-NN中使用的一些算法的运行时间复杂度和一些优缺点。</p><p id="0928" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> A .蛮K-NN </strong></p><p id="774f" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> B. K D树K-NN </strong></p><p id="9fc4" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> C .局部敏感哈希K-NN </strong></p></div><div class="ab cl jl jm gp jn" role="separator"><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq jr"/><span class="jo bw bk jp jq"/></div><div class="hb hc hd he hf"><h2 id="9785" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak"> A .蛮K-NN </strong></h2><p id="f456" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">Brute K-NN在线性结构中存储训练数据，并且在测试它时，粗暴地转到‘nd’的所有数据点并寻找‘K’个邻居。</p><p id="b5ab" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">因此，运行时间复杂度为O (n d k)</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es oi"><img src="../Images/96d4f4658d99c8f660a17427fc9cecc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*tRbCFTnxa-VIHfBCROrqwQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">Brute K-NN</figcaption></figure><p id="7eeb" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">反对:</strong></p><p id="0783" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">对于大型数据集，运行时间复杂度变得太高。</strong></p><h2 id="43bd" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak"> B. K D树K-NN </strong></h2><p id="d51a" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated"><strong class="kq hz">使用树状结构(如二叉查找树)存储训练数据点，大大降低了运行时间复杂度。</strong></p><p id="e61d" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">但是它只对数据点的低维有用(例如d=1到5或6)。对高维数据点使用k-d树K-NN是大忌，因为它会以2^dimension.因子增加运行时间复杂度</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es oj"><img src="../Images/10491a525905e15d1c98164dd1547e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*fjAt3FPiq8iUTgsDS1TE8g.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">K-D树</figcaption></figure><p id="c375" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">反对:</strong></p><p id="6007" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">仅对低维数据有用(如计算机图形、几何数据等)。</strong></p><h2 id="1828" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak"> C .局部敏感度哈希K-NN </strong></h2><p id="cf0f" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">它是一种<strong class="kq hz">随机化算法</strong>，使用<strong class="kq hz">哈希</strong>使用一些随机超平面对相似的训练数据点进行聚类。</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es ok"><img src="../Images/84aa8902015a03ee394a7281cdc7571a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*3eKTUKZRVTmIXju7PCOxQQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">局部敏感哈希K-NN</figcaption></figure><p id="9d11" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">优点:</strong></p><p id="07e0" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">能很好的处理高维数据。</strong></p><p id="0bef" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">缺点:</strong></p><p id="7387" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">如果数据集很大，存储训练数据可能需要相当大的时间复杂度。</strong></p><h2 id="588c" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">2.11]如何处理分类特征</h2><p id="ad99" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">如果输入变量是分类的，我们需要使用某种<strong class="kq hz">特征工程</strong>到<strong class="kq hz"> </strong>将分类输入特征转换成数字特征。但是老实说，如何特征化分类输入特征取决于特征的性质和需求是什么。<strong class="kq hz">特征工程是机器学习的艺术部分</strong>，它是经过大量实践和反复试验而产生的。在这里，我指的是一些你可以用来将分类特征转换成数字特征的技术。</p><p id="8db3" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> a. </strong> <a class="ae hv" href="https://maelfabien.github.io/machinelearning/NLP_2/#1-bag-of-words" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hz">包话(鞠躬)</strong> </a></p><p id="87ed" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">b .</strong>T8<strong class="kq hz">Tf-Idf</strong>T11】</p><p id="ec7a" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">c .</strong><a class="ae hv" rel="noopener" href="/@jayeshbahire/introduction-to-word-vectors-ea1d4e4b84bf"><strong class="kq hz">Avg word 2 vec</strong></a></p><p id="5b2c" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">d .</strong>T20<strong class="kq hz">Tf-Idf word 2 vec</strong>T23】</p><h1 id="6c01" class="li jf hy bd js lj ol ll jw lm om lo ka lp on lr ke ls oo lu ki lv op lx km ly bi translated"><strong class="ak"> 3。K-NN的优缺点</strong></h1><h2 id="06e5" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">a.K-NN的优势</h2><ol class=""><li id="84cb" class="ko kp hy kq b kr ks kt ku kb kv kf kw kj kx ky kz la lb lc bi translated">简单且易于理解和实现。</li><li id="8d14" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">由于K-NN的非线性决策边界，它有时可以优于线性机器学习模型(例如，逻辑回归和线性SVM)，特别是当数据不是线性可分的时候。</li><li id="34d3" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">易于扩展为多类分类问题。</li></ol><h2 id="a4be" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">b . K-NN的缺点</strong></h2><ol class=""><li id="89ef" class="ko kp hy kq b kr ks kt ku kb kv kf kw kj kx ky kz la lb lc bi translated">高运行时间复杂性。</li><li id="2912" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">因此，高等待时间不能用于要求低等待时间互联网应用中。</li><li id="ce72" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">没什么可解释的。</li><li id="e3ab" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">没有给出隐含特征的重要性。</li><li id="7c1d" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">当数据维数较高时，K-NN方法会遭遇<strong class="kq hz">维数灾难。</strong></li><li id="fc88" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">不平衡数据集无法正常工作。</li></ol><h1 id="84d7" class="li jf hy bd js lj ol ll jw lm om lo ka lp on lr ke ls oo lu ki lv op lx km ly bi translated">4.使用K-NN而不是分类和回归</h1><p id="7674" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">在上面的部分中，我们已经看到K-NN的缺点多于优点。所以这个问题可能会出现在你的脑海中，如果K-NN有这么多的缺点，我们是否应该使用K-NN？如果是的话，我们还可以在什么情况下使用K-NN。</p><p id="4575" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">让我们看看几个我们可以使用K-NN作为优势的情况:</p><h2 id="90a5" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">4.1]用于输入缺失值的K-NN</h2><p id="d5e4" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">使用K-NN模型寻找或预测缺失值是输入特征缺失值的最佳技术之一。</p><p id="8745" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">我们可以使用那些缺失值的数据点作为查询点或测试点，其余的数据作为训练点。对缺失特征应用K-NN，其中缺失的特征值将成为预测变量。</p><h2 id="2359" class="je jf hy bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak"> 4.2】寻找两个数据点之间的相似性</strong></h2><p id="6c14" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">让我们举个例子来理解这一点:</p><p id="4fea" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">假设我们想要<strong class="kq hz">在亚马逊上推荐类似的产品，只使用产品的“标题”作为输入特征。</strong></p><p id="fb73" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">我们可以使用K-NN输出的概率分数作为另一个机器学习模型</strong>的输入特征。这可以解释为“两个数据点有多相似？或者两个产品有多相似？”。这种技术也被称为<a class="ae hv" href="https://www.datacamp.com/community/tutorials/ensemble-learning-python" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hz">集成建模</strong> </a>其中我们一起使用多个模型进行预测。</p><h1 id="6ad7" class="li jf hy bd js lj ol ll jw lm om lo ka lp on lr ke ls oo lu ki lv op lx km ly bi translated"><strong class="ak"> 5。哪里不用K-NN </strong></h1><p id="f3d6" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">很多时候，我们了解了一个模型的所有东西，但是忘记了在哪里不使用它。在这里，我们将确切地了解，我们不应该应用K-NN的情况。</p><ol class=""><li id="bed7" class="ko kp hy kq b kr mp kt mq kb mu kf mv kj mw ky kz la lb lc bi translated"><strong class="kq hz">当数据集不平衡时连同</strong> <a class="ae hv" href="https://developers.google.com/machine-learning/crash-course/classification/accuracy" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hz">准确度</strong> </a> <strong class="kq hz">作为度量</strong></li></ol><p id="a83d" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">如果我们以“准确性”作为衡量标准，当我们的数据集不平衡时，我们经常会对模型的性能做出错误的解释。</p><p id="c722" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">假设我的数据集不平衡，比率为90:10，其中90%的数据属于类{1}，10%的数据属于类{0}。</p><p id="5a54" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">现在，即使我们预测所有数据点都属于{1}类，我们的“T16准确性T17”指标仍会显示我的模型90%准确，这无疑是对模型性能的错误解释。我们可以明确地看到，我们的模型偏向于多数阶级，而且模型是不合适的。我们在本博客的开始部分已经看到，K-NN在寻找最近邻居时很容易偏向多数类。</p><p id="1870" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> 2。维度高时:维度的诅咒</strong></p><p id="c4cc" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">使用欧几里德距离或曼哈顿距离度量容易遭受维数灾难。因此，当输入的维数很高时，使用欧氏距离确实不是一个好主意。</p><p id="40f6" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> 3。当查询数据点远离列车数据时</strong></p><p id="3883" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">这意味着很有可能训练数据的分布与测试数据不同，我们的模型将无法更好地执行。</p><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es oq"><img src="../Images/23de0133eb9d7ff333a222d1bd0cfb21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*vgrHnoZ34-3otgTk7Js56g.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">查询点远离列车数据点</figcaption></figure><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es or"><img src="../Images/c8c8ba1d2a6ee315b5fde72aef7a4fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AuBznUTlvFnPsrD4gC4UzQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">训练数据的分布与测试数据的分布非常不同</figcaption></figure><p id="4727" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> 4。当数据随机或有噪声时</strong></p><p id="1327" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">嗯，在这种情况下，很少有任何模型可以做任何明智的事情。这就是为什么在应用机器学习模型之前，预处理数据并去除异常值是必要步骤之一的原因。</p><h1 id="a469" class="li jf hy bd js lj ol ll jw lm om lo ka lp on lr ke ls oo lu ki lv op lx km ly bi translated">6.使用Sklearn库实现伪代码</h1><ol class=""><li id="492e" class="ko kp hy kq b kr ks kt ku kb kv kf kw kj kx ky kz la lb lc bi translated"><strong class="kq hz">导入库</strong></li></ol><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es os"><img src="../Images/4341b0df124607c49f21badad19b5d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*BviHupitbsrSXwitbQTQyw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">导入库</strong></figcaption></figure><p id="9793" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> 2。导入数据集</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ot"><img src="../Images/b5530dfe7cb9ff6a851be27cfdc7c7c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jh2pAntvSnZQgx1bQFSFmg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">导入数据集</strong></figcaption></figure><p id="ee62" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> 3。预处理和分割训练和测试数据集</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es ou"><img src="../Images/8d94783a436f84f150de6cef47b6959f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*tlMUBrEQTRjeGpDs-qLt8g.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">预处理并剥离训练和测试数据集</strong></figcaption></figure><p id="cfcd" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> 4。特征缩放</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es ov"><img src="../Images/71475d1bfd9bd72923825ad48fe62506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*-vNrTGMCVTGZzeK9hKU7OQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">特征缩放</strong></figcaption></figure><p id="ee63" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> 5。应用K-NN、训练、预测和评估</strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div class="er es ow"><img src="../Images/a7860a0032b5c73b90d8194cd5df80fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*lotg1k86QDNQoeA3tI7jvQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">训练、预测和评估</strong></figcaption></figure><p id="f35a" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> 5。错误V/S 'K' </strong></p><figure class="iv iw ix iy fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ox"><img src="../Images/a6b032beba62d1c5b0ca684d34786f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*euqJQU51nrBF2UX2u9m27Q.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd js">错误V/S‘K’</strong></figcaption></figure><p id="8ed1" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz"> 6。K-NN的代码编译</strong></p><pre class="iv iw ix iy fd iz ja jb jc aw jd bi"><span id="ca51" class="je jf hy ja b fi jg jh l ji jj">#Importing Libraries<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="1a6e" class="je jf hy ja b fi jk jh l ji jj">#Importing the Dataset<br/>url = "<a class="ae hv" href="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a>"</span><span id="6795" class="je jf hy ja b fi jk jh l ji jj"># Assign colum names to the dataset<br/>names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']</span><span id="3ebb" class="je jf hy ja b fi jk jh l ji jj"># Read dataset to pandas dataframe<br/>dataset = pd.read_csv(url, names=names)<br/>dataset.head()</span><span id="0a33" class="je jf hy ja b fi jk jh l ji jj">#Preprocessing<br/>X = dataset.iloc[:, :-1].values<br/>y = dataset.iloc[:, 4].values</span><span id="aa87" class="je jf hy ja b fi jk jh l ji jj">#Train Test Split<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)</span><span id="03b5" class="je jf hy ja b fi jk jh l ji jj">#Feature Scaling<br/>from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>scaler.fit(X_train)</span><span id="02af" class="je jf hy ja b fi jk jh l ji jj">X_train = scaler.transform(X_train)<br/>X_test = scaler.transform(X_test)</span><span id="4138" class="je jf hy ja b fi jk jh l ji jj">#Training and Predictions<br/>from sklearn.neighbors import KNeighborsClassifier<br/>classifier = KNeighborsClassifier(n_neighbors=5)<br/>classifier.fit(X_train, y_train)<br/>y_pred = classifier.predict(X_test)</span><span id="703c" class="je jf hy ja b fi jk jh l ji jj">#Evaluating the Algorithm<br/>from sklearn.metrics import classification_report, confusion_matrix<br/>print(confusion_matrix(y_test, y_pred))<br/>print(classification_report(y_test, y_pred))</span><span id="4c67" class="je jf hy ja b fi jk jh l ji jj">#Comparing Error Rate with the K Value<br/>error = []</span><span id="bfd1" class="je jf hy ja b fi jk jh l ji jj"># Calculating error for K values between 1 and 40<br/>for i in range(1, 40):<br/>    knn = KNeighborsClassifier(n_neighbors=i)<br/>    knn.fit(X_train, y_train)<br/>    pred_i = knn.predict(X_test)<br/>    error.append(np.mean(pred_i != y_test))</span><span id="6d3a" class="je jf hy ja b fi jk jh l ji jj">plt.figure(figsize=(12, 6))<br/>plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',<br/>         markerfacecolor='blue', markersize=10)<br/>plt.title('Error Rate K Value')<br/>plt.xlabel('K Value')<br/>plt.ylabel('Mean Error')</span></pre><h1 id="6ece" class="li jf hy bd js lj ol ll jw lm om lo ka lp on lr ke ls oo lu ki lv op lx km ly bi translated">7.修正你的倾向</h1><ol class=""><li id="9e31" class="ko kp hy kq b kr ks kt ku kb kv kf kw kj kx ky kz la lb lc bi translated">直观解释K-NN。</li><li id="8663" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">解释K-NN在各种情况下的行为。</li><li id="08bb" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">K-NN的优缺点是什么？</li><li id="6a31" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">除了分类和回归，K-NN还可以用在哪里？</li><li id="18e9" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">在哪些情况下我们不应该使用K-NN？</li><li id="f6d3" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">编写K-NN的伪算法。</li><li id="0f43" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated">使用sklearn库实现K-NN的伪代码。</li></ol><h1 id="273f" class="li jf hy bd js lj ol ll jw lm om lo ka lp on lr ke ls oo lu ki lv op lx km ly bi translated">8.结论和面试问题</h1><p id="4fcb" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated"><strong class="kq hz">结论</strong>:</p><p id="afeb" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">除了这些学习如果你想学习如何在真实生活数据集上应用K-NN(极性分析关于  <a class="ae hv" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hz"> <em class="mo">亚马逊美食评论</em> </strong> </a> <strong class="kq hz"> <em class="mo">【使用K-NN】)</em></strong>请参考<a class="ae hv" href="https://github.com/adityaadarsh/amazon-fine-food-reviws-polarity-analyses" rel="noopener ugc nofollow" target="_blank">这个</a><a class="ae hv" href="https://github.com/adityaadarsh/amazon-fine-food-reviws-polarity-analyses/blob/master/1.EDA%20on%20amazon%20fine%20reviews.ipynb" rel="noopener ugc nofollow" target="_blank">这个</a>和<a class="ae hv" href="https://github.com/adityaadarsh/amazon-fine-food-reviws-polarity-analyses/blob/master/2.%20Amazon%20Fine%20Food%20Reviews%20Analysis_KNN-Copy1.ipynb" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="9403" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">我试图涵盖K-NN和用例场景的几乎所有方面。如果你还有任何疑问或任何话题，请通过我的<a class="ae hv" href="http://adityaadarsh99@gmai.com" rel="noopener ugc nofollow" target="_blank">邮箱</a>联系我。对于现实生活数据的代码实现，请通过上面的链接。</p><p id="6842" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated"><strong class="kq hz">面试问题:经过</strong> <a class="ae hv" href="https://github.com/adityaadarsh/amazon-fine-food-reviws-polarity-analyses/blob/master/Dive%20Deep%20into%20KNN.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hz">这个环节</strong> </a> <strong class="kq hz">。</strong></p><h1 id="cedd" class="li jf hy bd js lj ol ll jw lm om lo ka lp on lr ke ls oo lu ki lv op lx km ly bi translated"><strong class="ak"> 9。参考文献</strong></h1><ol class=""><li id="bdd4" class="ko kp hy kq b kr ks kt ku kb kv kf kw kj kx ky kz la lb lc bi translated"><a class="ae hv" href="https://www.appliedaicourse.com/course/11/Applied-Machine-learning-course" rel="noopener ugc nofollow" target="_blank">appliedaicourse.com</a></li><li id="0c51" class="ko kp hy kq b kr ld kt le kb lf kf lg kj lh ky kz la lb lc bi translated"><a class="ae hv" href="https://www.wikiwand.com/news" rel="noopener ugc nofollow" target="_blank">wikiwand.com</a></li></ol><p id="ac44" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">3.开源代码库</p><p id="cf24" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">4.stackoverflow</p><p id="5111" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">5.<a class="ae hv" href="https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/" rel="noopener ugc nofollow" target="_blank">解析维迪亚</a></p><p id="4631" class="pw-post-body-paragraph lz ma hy kq b kr mp mb mc kt mq md me kb mr mg mh kf ms mj mk kj mt mm mn ky hb bi translated">6.<a class="ae hv" href="https://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">机器学习掌握度</a></p><h1 id="acc6" class="li jf hy bd js lj ol ll jw lm om lo ka lp on lr ke ls oo lu ki lv op lx km ly bi translated">结束:)</h1><p id="d74b" class="pw-post-body-paragraph lz ma hy kq b kr ks mb mc kt ku md me kb mf mg mh kf mi mj mk kj ml mm mn ky hb bi translated">希望你喜欢。</p></div></div>    
</body>
</html>