<html>
<head>
<title>“Few shot vid2vid” , the GAN that can transfer motion with several images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“少拍vid2vid”，可以用几个图像转移动作的GAN</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/few-shot-vid2vid-the-gan-that-can-transfer-motion-with-several-images-debe32d5c426?source=collection_archive---------9-----------------------#2019-11-11">https://medium.com/analytics-vidhya/few-shot-vid2vid-the-gan-that-can-transfer-motion-with-several-images-debe32d5c426?source=collection_archive---------9-----------------------#2019-11-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9c30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章解释了2018年8月20日在arXiv上发布的视频到视频合成[1]和2019年10月28日发布的少数镜头视频到视频合成[2]。</p><ul class=""><li id="f25b" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">前者是一篇关于一个叫vid2vid的GAN的论文，它可以根据区域划分遮罩等语义图像合成似是而非的视频。</li><li id="5485" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">后者是一篇关于GAN的论文，名为少拍vid2vid，它进一步发展了前者，可以基于少量参考图像和一个语义图像组成一个视频。</li></ul></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><h1 id="ce8d" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">视频到视频合成</strong></h1><h2 id="cd33" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated">摘要</h2><p id="c221" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">本文通过一个名为vid2vid的GAN，基于区域划分遮罩、线条画草图等语义图像，生成逼真的高分辨率视频。本文摘要如下。</p><blockquote class="lp lq lr"><p id="cfb6" class="if ig ls ih b ii ij ik il im in io ip lt ir is it lu iv iw ix lv iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi">他们提出一个叫vid2vid的GAN来合成视频。与之前研究中的pix2pixHD和COVST相比，生成的视频受干扰更小，因为它是利用前一帧的条件概率生成的。他们以“时空渐进”的方式训练模型，这种方式交替执行时间渐进训练和空间渐进训练，时间渐进训练随着学习的进行增加用于合成的帧数，空间渐进训练像PG-GAN一样逐渐增加分辨率。</em>T5】</strong></p></blockquote><p id="9450" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的示例中，两个高分辨率视频是基于左下区域划分遮罩生成的。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es lw"><img src="../Images/3e45baef48adece6410ad560c437c16d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*rRz3FAHoZO-VBr0yF0P7UA.gif"/></div></div></figure><h2 id="94c7" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated"><strong class="ak">问题表述</strong></h2><p id="a729" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">考虑生成一个视频，它由一组生成的图像组成，生成的图像从时间<strong class="ih hj"> <em class="ls"> 1 </em> </strong>到<strong class="ih hj"> <em class="ls"> T </em> </strong>和语义图像从时间<strong class="ih hj"><em class="ls">1</em><strong class="ih hj"><em class="ls">T</em></strong>到时间<strong class="ih hj"> <em class="ls"> s </em> </strong>。</strong></p><p id="4aea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，在每个时间t的图像生成被公式化为从时间<strong class="ih hj"> <em class="ls"> t-L </em> </strong>到<strong class="ih hj"><em class="ls">t-1</em></strong><em class="ls"/>和语义图像<strong class="ih hj"> <em class="ls"> s </em> </strong>从<strong class="ih hj"> <em class="ls"> t-L </em> </strong>到<strong class="ih hj"> <em class="ls"> t </em> </strong>的生成图像的联合概率</p><p id="7824" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">整个图像序列(视频)的公式如下。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mi"><img src="../Images/2ce1e08b433ce2fc5ec25a51ab9f0638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VeJo-expyqoU9UgwHH9WgQ.png"/></div></div></figure><h2 id="8dec" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated">发电机(F)</h2><p id="e94c" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">发电机<strong class="ih hj"> <em class="ls"> F </em> </strong>的结构如下图所示。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mj"><img src="../Images/3724b68b3a0fa1c1a5d12171f384cbf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-PJpGNsVUjIXTct3svNCA.png"/></div></div></figure><p id="8ed9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大致解释一下，它由两幅图像相乘得到的蒙版<strong class="ih hj"> <em class="ls"> m </em> </strong>组成。第一部分是前一次(<strong class="ih hj"> t-1 </strong>)被光流扭曲的图像(蓝色)。第二部分是合成另一部分(红色)的中间图像。屏蔽<strong class="ih hj"> <em class="ls"> m </em> </strong>具有从<strong class="ih hj"> 0 </strong>到<strong class="ih hj"> 1 </strong>的时间连续值。使用掩模<strong class="ih hj"> <em class="ls"> m </em> </strong>为每个位置分配两幅图像。由于视频是时间连续的，所以有一个自然的假设，即图像被分成可以使用光流表达的图像和不能使用光流表达的图像。</p><p id="05ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">后面的中间图像<strong class="ih hj"> <em class="ls"> h </em> </strong>可以进一步分解如下:下标B表示背景，下标F表示前景。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mk"><img src="../Images/d7bf3f1e52ff47f66dd3408577ff4049.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3NqpgFNHYlLmj4QFEiLog.png"/></div></div></figure><p id="b0de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">遮罩<em class="ls"/><strong class="ih hj"><em class="ls">m _ B</em></strong>指示背景在时间<strong class="ih hj"> t </strong>的位置，并决定哪一部分将由前景函数<strong class="ih hj"> <em class="ls"> h_F </em> </strong>或背景函数<strong class="ih hj"> <em class="ls"> h_B </em> </strong>生成。</p><p id="82dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">前台函数<strong class="ih hj"> <em class="ls"> h_F </em> </strong>负责难以用光流表达的激烈运动的结构，后台函数<strong class="ih hj"> <em class="ls"> h_B </em> </strong>负责可以用光流表达的运动不大的部分。其实<strong class="ih hj"> <em class="ls"> F </em> </strong>的第一项就包含了一个利用光流扭曲图像的项，所以<strong class="ih hj"> <em class="ls"> h_B </em> </strong>负责的是时间<strong class="ih hj"><em class="ls"/></strong>处新出现的背景等仅用光流无法表现的部分。</p><p id="9fdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重写F包括这些结果如下。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es ml"><img src="../Images/e05e7e2d0069ec308646ba87cfbcfcfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dWXahSGs1nB2tjUgRfbHtQ.png"/></div></div></figure><h2 id="3a1c" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated">鉴别器</h2><p id="8bd7" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">vid2vid模型引入了两个鉴别器，图像鉴别器和视频鉴别器。</p><ol class=""><li id="d29f" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc mm jj jk jl bi translated">前者是区分(真实图像，对应语义图像)或(生成图像，对应语义图像)的鉴别器，考虑从语义图像生成的图像是否似是而非。</li><li id="aafb" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc mm jj jk jl bi translated">后者区分(真实图像，前一时刻对应的光流)或(生成图像，前一时刻对应的光流)，确定视频的运动是否自然。</li></ol><p id="c891" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，已知应该在多个尺度上引入鉴别器以防止模式崩溃。在vid2vide模型中，在多个尺度上引入了图像鉴别器。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div class="er es mn"><img src="../Images/dd90671bd1ebcd546d71328c37e6c3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*8D0x3gqONd5xf5iLMPlIXg.png"/></div></figure><h2 id="bca0" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated">目标函数</h2><p id="aea6" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">优化发生器(<strong class="ih hj"> F </strong>)和鉴别器的目标函数如下。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mo"><img src="../Images/9fd6f568018b29b472af4725c2a5eac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQFp5oL09ccXu7mC8nsw0Q.png"/></div></div></figure><p id="d0f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">括号中的第一项<strong class="ih hj"> <em class="ls"> L_I </em> </strong>和第二项<strong class="ih hj"> <em class="ls"> L_V </em> </strong>是生成器<strong class="ih hj"> <em class="ls"> F </em> </strong>和鉴别器的极大极小公式化，如在正常的GAN目标函数中一样。下标I代表图像鉴别器，下标V代表视频鉴别器。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mp"><img src="../Images/04fd88c85b5121e081dec95fb38aa716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gLplCLicqK_8G5RDiRGaCA.png"/></div></div></figure><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mq"><img src="../Images/5fa8ec2e067e1bd0bdabd97501897ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zn2TSJydoBm9BgmSOWIXGg.png"/></div></div></figure><p id="b85f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第三项<strong class="ih hj"> <em class="ls"> L_W </em> </strong>与光流有关。第一项是真实光流和预测光流之间的差异，第二项是在时间t由光流扭曲的图像和一个时间之前的图像之间的差异。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div class="er es mr"><img src="../Images/9afcd3088169e18cd9b34b51d37b9758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*XsZn0TtoEdGRzMH0z8jnzQ.png"/></div></figure><h2 id="b713" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated">训练方法</h2><p id="0b56" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">训练是以“时空渐进的方式”进行的。简单来说就是从小帧数和粗分辨率开始学习，逐渐交替增加帧数和分辨率的学习方法。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div class="er es ms"><img src="../Images/c50661b40f2cfa4d08f8485b312c32f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*CUyL44JxicdCaMhVlG-FDg.png"/></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">时空渐进方式</figcaption></figure><h2 id="0acc" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated">结果</h2><p id="238f" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">我介绍一部分结果。首先，使用相同的分割掩模生成两个视频的例子。由于遮罩是相同的，汽车和背景(行道树和建筑物)的位置是不变的，但您可以看到建筑物和行道树以及汽车的类型可以自由转换。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es lw"><img src="../Images/e2512c9f1f532e707313277b086a4760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*NfpmqVrXnjjX1M8MReWCRg.gif"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">由vid2vid生成的视频</figcaption></figure><p id="27f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来是与其他方法的比较。由于图像是用前一次的条件概率生成的，所以与其他方法相比，可以生成自然的视频。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mx"><img src="../Images/72850faa48987a331816c2e77b67ebf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Mbz6Ve9qV1MBZ1Y30ZEvkw.gif"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">(左下)分割蒙版，(左)<em class="my"> pix2pixHD，(中)COVST，(右)vid2vid </em></figcaption></figure></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><h1 id="20fa" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">少镜头视频到视频合成</h1><h2 id="b6d1" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated">摘要</h2><p id="e10b" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">该论文于2019年10月28日提交给arXiv。本研究在vid2vid的基础上提出了“少镜头vid2vid”模型。在vide2vid中，只能在学习过的视频中进行合成，但使用“很少拍摄的vide 2 vid”，即使在培训中没有看到的视频中也可以进行视频合成。</p><p id="2746" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总结如下。</p><blockquote class="lp lq lr"><p id="7415" class="if ig ls ih b ii ij ik il im in io ip lt ir is it lu iv iw ix lv iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi">他们提出了GAN，称为“少镜头vid2vid”，用少镜头合成视频。虽然它是基于vid2vid，但他们使用修改的SPADE[4]将样式插入到要通过少量镜头合成的样本中。结合下面的2个步骤，组成一个新的时间帧图像。</em> </strong></p><p id="a96c" class="if ig ls ih b ii ij ik il im in io ip lt ir is it lu iv iw ix lv iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi"> 1。从合成图像(视频)和诸如关键点和区域划分掩模的语义图像中提取光流W和遮挡图M。</em>T25】</strong></p><p id="21d5" class="if ig ls ih b ii ij ik il im in io ip lt ir is it lu iv iw ix lv iz ja jb jc hb bi translated"><strong class="ih hj">2<em class="hi">2。提取你想要用编码器e合成的图像的特征，并用作可变风格参数，将其与语义图像一起放入修改的SPADE ResBlock。</em>T29】</strong></p></blockquote><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es mz"><img src="../Images/0062f2cc86b62ad4d90e275e1a8f32ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-DdDqbKETWlu8XPilxA9Iw.png"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">视频2视频(左)对少数镜头(视频2视频)</figcaption></figure><h2 id="bc15" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated"><strong class="ak">改进了少量镜头学习的架构</strong></h2><p id="3db3" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">基本上，它遵循vid2vid，但为了实现少镜头学习，他们修改了一个产生中间生成图像的模块<strong class="ih hj"> <em class="ls"> H </em> </strong>。在vid2vid中，模块<strong class="ih hj"> H </strong>的参数是固定值，与输入数据无关。但少拍vid2vid允许模型根据你想要合成的输入样本(<strong class="ih hj"> <em class="ls"> e_K，s_K </em> </strong>)动态改变参数。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es na"><img src="../Images/0fd46e478106d1f06562cc8319c8555c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pOflMEQEfa7dC1xbeusV6w.png"/></div></div></figure><p id="ab87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，我们如何使动态改变参数成为可能呢？有三个主要步骤。</p><ol class=""><li id="5327" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc mm jj jk jl bi translated">使用编码器(<strong class="ih hj"> <em class="ls"> E_P </em> </strong>)从(<strong class="ih hj"> <em class="ls"> e_K，s_K </em> </strong>)中提取每帧的特征，并计算每层编码器的特征<strong class="ih hj"><em class="ls">q ^ l</em></strong>(<strong class="ih hj"><em class="ls">l</em></strong>为层ID)。</li><li id="63fc" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc mm jj jk jl bi translated">计算每帧的注意力权重，并计算注意力加权的<strong class="ih hj"> <em class="ls"> q </em> </strong>。</li><li id="fd23" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc mm jj jk jl bi translated">用SPADE将注意力加权的<strong class="ih hj"> <em class="ls"> q </em> </strong>和语义图像<strong class="ih hj"> <em class="ls"> s </em> </strong>插入生成器<strong class="ih hj"> <em class="ls"> H </em> </strong>中，并修改SPADE以处理动态参数。</li></ol><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es nb"><img src="../Images/74a15036b5cecae690497a66b2938858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eJu3zJA2b3vpW4lQlD6_DA.png"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">很少拍摄vid2vid的生成器架构</figcaption></figure><p id="a5b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，在步骤1和2中，帧图像E和语义图像<strong class="ih hj"> <em class="ls"> s </em> </strong>分别由<strong class="ih hj"> <em class="ls"> E_F </em> </strong>和<strong class="ih hj"> <em class="ls"> E_A </em> </strong>处理。使用编码器<strong class="ih hj"> <em class="ls"> E_F </em> </strong>从帧图像E计算特征<strong class="ih hj"> <em class="ls"> q </em> </strong>，使用编码器<strong class="ih hj"> <em class="ls"> E_A </em> </strong>从语义图像<strong class="ih hj"> <em class="ls"> s </em> </strong>计算注意力权重。注意，针对编码器<strong class="ih hj"> <em class="ls"> E_F </em> </strong>的每一层提取特征q。</p><p id="760f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第三步，他们使用标准化层称为铲[4]，这是用于风格转移。GauGAN使用SPADE可以从简单的线条和颜色绘制的插图中生成逼真的图像。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div class="er es nc"><img src="../Images/e90f9b642ad8d5eed3c6d0bf80535b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/1*Z6GzxRqaR066rkhiS1ZPiw.gif"/></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">高根。<a class="ae nd" href="https://www.nvidia.com/en-us/research/ai-playground/" rel="noopener ugc nofollow" target="_blank">https://www.nvidia.com/en-us/research/ai-playground/</a></figcaption></figure><p id="1248" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原始论文中提出的SPADE的结构如下。每个卷积图都作为系数和偏差输入到每个批次的归一化特征值中。</p><div class="lx ly lz ma fd ab cb"><figure class="ne mb nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><img src="../Images/3de5a3729ee4bc63a3fcd129e12e5720.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*sJD4UNs9ZOPHIy85tGm-AQ.png"/></div></figure><figure class="ne mb nk ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><img src="../Images/0356317dd9af800f344227777d03977a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*91mKIhRBTfXcb7QjiIkqfA.png"/></div><figcaption class="mt mu et er es mv mw bd b be z dx nl di nm nn translated">(左)SPADE ResBlock(右)SPADE架构</figcaption></figure></div><p id="55c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在原始SPADE中，<strong class="ih hj"><em class="ls"/></strong>和<strong class="ih hj"><em class="ls"/></strong>使用固定值滤波器进行卷积。但是由于SPADE in won shot vid 2 vid需要动态改变参数，所以使用编码器<strong class="ih hj"> <em class="ls"> E_P </em> </strong>和<strong class="ih hj"> <em class="ls"> E_A </em> </strong>处理的权重<strong class="ih hj"> <em class="ls"> θ_H </em> </strong>对帧图像进行卷积。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es no"><img src="../Images/261a37ce949b001849ba9cc7e8be1bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J9oGkzxEDEMlSENrpwGzLQ.png"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">改进的黑桃在少数镜头视频2视频</figcaption></figure><p id="7a68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ls"> θ_H </em> </strong>分为<strong class="ih hj"> <em class="ls"> θ_S </em> </strong>、<strong class="ih hj"> <em class="ls"> θ_β </em> </strong>、<strong class="ih hj"> <em class="ls"> θ_γ </em> </strong>，在不同的部位进行卷积处理。用数学公式写成如下:</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div class="er es np"><img src="../Images/58b2f8d532d9f385e64719d3327c8eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*_RjbXbkv12m0XpqxAgNvIw.png"/></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">数学表达式中的黑桃</figcaption></figure><h2 id="faaf" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated">结果</h2><p id="b2ed" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">下面是一个用少拍vid2vid进行少拍学习的例子。首先，这里有一个例子，使用正在跳舞的人的关键点，用几个镜头转移舞蹈。可以看到转移非常成功。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es nq"><img src="../Images/375b9e9f320afd2c13e8aad52e92039a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*fEZyTZEHrfJZC1k8sLlMwA.gif"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">顶部raw是样本，左栏是舞蹈语义图像</figcaption></figure><p id="20c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来是一个道路和城市的示例(城市空间数据集)。同样，使用遮罩图像，可以按照您想要传输的示例图像的样式来生成电影。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es nq"><img src="../Images/8e22fa1c452c93e6f8c2b0cb49bf184c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*lG7sWSIwZgxuc4HnBDv-Cw.gif"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">城市空间数据集中生成的视频</figcaption></figure><p id="4f73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与其他方法相比，这是一个例子。可以看出，与现有方法相比，该方法可以很好地转移姿态。</p><figure class="lx ly lz ma fd mb er es paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="er es nr"><img src="../Images/014b88fb578c123e4c53e055ec7b8ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LJ1xf9aJ-Kl-k4uS1cmLiA.png"/></div></div><figcaption class="mt mu et er es mv mw bd b be z dx translated">与其他方法比较。最右列的“出局”表示很少拍摄vid2vid</figcaption></figure><h2 id="60b9" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated">结论</h2><p id="a812" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">在这篇帖子里，我介绍了可以合成自然视频的vid2vid和可以合成少镜头的少镜头vid2vid。GAN的进化真的很神奇，所以我真的很期待看到新的研究。</p></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><div class="lx ly lz ma fd ns"><a href="https://twitter.com/AkiraTOSEI" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hj fi z dy nx ea eb ny ed ef hh bi translated">阿基拉</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">akira的最新推文(@AkiraTOSEI)。机器学习工程师/数据科学家/物理学硕士/…</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">twitter.com</p></div></div><div class="ob l"><div class="oc l od oe of ob og mg ns"/></div></div></a></div></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><h1 id="9120" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">每周机器学习时事通讯</h1><div class="oh oi ez fb oj ns"><a href="https://www.getrevue.co/profile/akiratosei" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab dw"><div class="nu ab nv cl cj nw"><h2 class="bd hj fi z dy nx ea eb ny ed ef hh bi translated">阿基拉的ML新闻杂志</h2><div class="nz l"><h3 class="bd b fi z dy nx ea eb ny ed ef dx translated">关于我:制造工程师/机器学习工程师/物理学硕士/ ExaWizards Inc. _ _ _ _ _…</h3></div><div class="oa l"><p class="bd b fp z dy nx ea eb ny ed ef dx translated">www.getrevue.co</p></div></div><div class="ob l"><div class="ok l od oe of ob og mg ns"/></div></div></a></div></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><h2 id="d876" class="kw jz hi bd ka kx ky kz ke la lb lc ki iq ld le km iu lf lg kq iy lh li ku lj bi translated">参考</h2><ol class=""><li id="8bce" class="jd je hi ih b ii lk im ll iq ol iu om iy on jc mm jj jk jl bi translated">廷-王春等。视频到视频合成arXiv:1808.06601，2018</li><li id="c37f" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc mm jj jk jl bi translated">丁-王春等。少镜头视频到视频合成，arXiv:1910.12713，2019</li><li id="0754" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc mm jj jk jl bi translated">英伟达研究:视频到视频合成，<a class="ae nd" href="https://www.youtube.com/watch?v=GrP_aOSXt5U&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=GrP_aOSXt5U&amp;feature = youtu . be</a></li><li id="4ba2" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc mm jj jk jl bi translated">使用空间自适应归一化的语义图像合成。arXiv:1903.07291，2019</li></ol></div></div>    
</body>
</html>