<html>
<head>
<title>Matrix Calculus for DeepLearning (Part1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于深度学习的矩阵演算(第一部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/matrix-calculus-for-deeplearning-part1-3e52e7f22f58?source=collection_archive---------27-----------------------#2020-05-29">https://medium.com/analytics-vidhya/matrix-calculus-for-deeplearning-part1-3e52e7f22f58?source=collection_archive---------27-----------------------#2020-05-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c3a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2020年5月29日</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/03a977d5537811da58f79f0e84675762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T5s_uAxQiatiiuuL.jpg"/></div></div></figure><p id="6ea7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学分:基于特伦斯·帕尔和杰瑞米·霍华德的论文<a class="ae jp" href="https://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank">深度学习所需的矩阵演算</a>。感谢这篇论文。</p><p id="03c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇论文对初学者很友好，但是我想写这篇博客来记录一些要点，这样可以更好地理解这篇论文。当我们学习一些稍微困难的主题时，我们发现用我们所学的方法向一个初学者解释是困难的，他可能不知道那个领域的任何事情，所以这个博客是为初学者准备的。</p><p id="3e28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习就是线性代数和微积分。如果你试图阅读任何深度学习论文，矩阵微积分是理解这个概念的必要组成部分。可能是单词<em class="jq">需要</em>可能不是正确的单词，因为Jeremy的课程显示了如何在只有最低水平的微积分的情况下成为世界级的深度学习实践者，请查看<a class="ae jp" href="https://course.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a>了解课程。</p><p id="3f62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我以三篇博客的形式写下了我对纸的理解。这是第一部分，并检查该网站的两个以上的部分。</p><p id="80f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习基本上是使用多层神经元。每个神经元是做什么的？？</p><h1 id="c561" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">介绍</h1><p id="b5bc" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">每个神经元对输入应用一个函数并给出一个输出。神经网络中单个计算单元的激活通常使用边权重向量<strong class="ih hj"> w </strong>与输入向量<strong class="ih hj"> x </strong>加上标量偏差(阈值)的点积来计算:</p><p id="52ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">z(x) = <strong class="ih hj"> w </strong> <strong class="ih hj"> x </strong> + b</p><p id="c77e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">粗体字母是矢量。<strong class="ih hj"> w </strong>是矢量</p><p id="72ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">函数<em class="jq"> z(x) </em>被称为该单元的仿射函数，后面是一个校正的线性单元，它将负值剪裁为零:max(0，z(x))。这种计算发生在神经元中。神经网络由许多这样的单元组成，组织成称为层的多个神经元集合。一层单位的激活成为下一层单位的输入。当输入、权重和函数被视为向量时，数学变得简单，而价值流可以被视为矩阵运算。</p><p id="3bf8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里使用的最重要的数学是<em class="jq">微分</em>，计算变化率和优化损失函数以减少误差是主要目的。训练阶段就是选择权重<strong class="ih hj"> w </strong>和偏置<em class="jq"> b </em>，这样我们就能得到所有N个输入<strong class="ih hj"> x </strong>的期望输出。为此，我们最小化损失函数。为了减少损失，我们使用SGD。测量输出相对于重量变化的变化与计算输出w.r.t重量<strong class="ih hj"> w </strong>的(偏)导数是一样的。所有这些都需要激活(x)相对于模型参数<strong class="ih hj"> w </strong>和<em class="jq"> b </em>的偏导数(梯度)。我们的目标是逐渐调整<strong class="ih hj"> w </strong>和<em class="jq"> b </em>，使所有<strong class="ih hj"> x </strong>输入的总损失函数不断变小。</p><h1 id="532f" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">标量导数规则</h1><p id="8c13" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">解决问题所需的基本规则</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ku"><img src="../Images/8a8026ebab54772305e79aa858ddd373.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/0*F7CHqhHAw0ebDK1_.png"/></div></figure><h1 id="47a3" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">偏导数</h1><p id="faf0" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">神经网络是多个参数的函数，所以我们来讨论一下。\ xy的导数是什么(x和y相乘)？？这取决于我们是相对于x还是y变化，我们一次计算一个变量的导数，给出两个导数，在这种情况下，我们称之为偏导数。用δ符号代替<em class="jq"> d </em>来表示。\关于<em class="jq"> x </em>的偏导数只是通常的标量导数，只是将方程中的任何其他变量视为常数。</p><h1 id="a9bc" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">矩阵计算</h1><p id="6b93" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">我们会看到如何计算f(x，y)的梯度</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kv"><img src="../Images/712356843d17d1d2100ab4a9a7bb12f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/0*V37DQqQystiRabt0.png"/></div></figure><p id="2fa7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f(x，y)的梯度只是它的偏导数的向量。\梯度向量组织特定标量函数的所有偏导数。如果我们有两个函数，我们也可以通过堆叠梯度将它们的梯度组织成一个矩阵。当我们这样做时，我们得到雅可比矩阵，其中梯度是行。</p><p id="7c2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更普遍地定义雅可比矩阵，让我们将多个参数组合成一个向量自变量:f (x，y，z)f(<strong class="ih hj">x</strong>)</p><p id="4ffe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设<strong class="ih hj"> y </strong> = f ( <strong class="ih hj"> x </strong>)是m个标量值函数的向量，每个函数取长度为n = |x|的向量<strong class="ih hj"> x </strong> \其中|x|是x中元素的基数(计数)</p><p id="94c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y₁= f₁(x)= 3倍₁x₂</p><p id="c46a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y₂ = f₂(x) = 2x₁ + x⁸₂</p><p id="b087" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">雅可比矩阵是所有m × n个可能偏导数(m行n列)的集合，是m个梯度相对于<strong class="ih hj"> x. </strong>的叠加</p><p id="9fc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">恒等函数<strong class="ih hj"> f </strong> ( <strong class="ih hj"> x </strong> ) = <strong class="ih hj"> x </strong>，其中fi ( <strong class="ih hj"> x </strong> ) = x i，具有n个函数，并且每个函数具有保存在单个向量<strong class="ih hj"> x </strong>中的n个参数。因此，雅可比矩阵是一个方阵，因为m = n</p><h1 id="8c05" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">向量的逐元素运算</h1><p id="8751" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在深度学习中，了解元素智能操作非常重要。对于元素式二元运算，我们简单地说就是将一个运算符应用于每个向量的第一项以获得输出的第一项，然后应用于输入的第二项以获得输出的第二项，依此类推。我们可以用符号y = f (w) O g(x)来概括基于元素的二元运算，其中m = n = |y| = |w| = |x|</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kw"><img src="../Images/02fa10931427387b5fd00b2bc635fdc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GsBAWcQLlIUo6ZNf.png"/></div></div></figure><h1 id="fd35" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">涉及标量展开的导数</h1><p id="1efa" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">当我们将标量乘或加到向量上时，我们隐式地将标量扩展为向量，然后执行元素级的二元运算。例如</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kx"><img src="../Images/a00fd6ea32cbc492956dcb81865d85d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/0*XbHdNRAmEpzg-VyQ.png"/></div></figure><p id="c21a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(符号-&gt; 1代表一个适当长度的向量。z是任何不依赖于x的标量，这很有用，因为对于任何x，I，∂z/∂x=为0，这将简化我们的偏导数计算</p><h1 id="c8e3" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">向量和归约</h1><p id="07b7" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">对向量的元素求和是深度学习中的重要操作，例如网络损失函数。\设y = sum( <strong class="ih hj"> f </strong> ( <strong class="ih hj"> x </strong>))。请注意，我们在这里很小心地将参数保留为向量<strong class="ih hj"> x </strong>，因为每个函数f i都可以使用向量中的所有值，而不仅仅是x i。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ky"><img src="../Images/39696f0237064e2af347017ac7b7dc43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/0*ZojYI16DjPi-9Ywx.png"/></div></figure><p id="f5d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在简单y = sum( <strong class="ih hj"> x </strong> )= [1，1 …1]的梯度中。因为对于j，∂x i/ ∂x j = 0！= i .转置，因为我们假设默认为垂直向量。保持所有向量和矩阵的形状有序是非常重要的，否则就不可能计算复杂函数的导数。</p><p id="7138" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">#博客9 </strong></p><p id="0a76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对我有帮助的链接:特伦斯·帕尔和杰瑞米·霍华德的论文<a class="ae jp" href="https://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank">深度学习所需的矩阵演算</a>。</p><p id="7f29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Nikhil B的博客</p><p id="58e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是博客的第一部分，在博客2中，我会解释链式法则。</p></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><p id="f251" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jq">原发布于</em><a class="ae jp" href="https://kirankamath.netlify.app/blog/matrix-calculus-for-deeplearning-part1/" rel="noopener ugc nofollow" target="_blank"><em class="jq">https://kirankamath . netlify . app</em></a><em class="jq">。</em></p></div></div>    
</body>
</html>