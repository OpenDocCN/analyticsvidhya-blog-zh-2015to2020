<html>
<head>
<title>Markov Chain Auto Tweets Generator to Resample Imbalanced dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">马尔可夫链自动Tweets生成器对不平衡数据集进行重采样</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/markov-chain-auto-tweets-generator-to-resample-imbalanced-dataset-b83055b8a1bb?source=collection_archive---------6-----------------------#2019-09-05">https://medium.com/analytics-vidhya/markov-chain-auto-tweets-generator-to-resample-imbalanced-dataset-b83055b8a1bb?source=collection_archive---------6-----------------------#2019-09-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a100" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><strong class="ak">利用马尔可夫链对数据集中的少数民族进行重新采样，证明了有希望的结果！</strong></h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/52f116a537afc648c679d90c9e97ac44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*g1EmGKcw-wVFpnCK_TaSig.jpeg"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">不平衡数据集的示例:灰苹果占多数，而红苹果占少数</figcaption></figure><p id="94d2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">不平衡的语料库或数据集对监督机器学习人员来说一直是一个挑战。不平衡意味着并非数据集中的所有类都具有相同的大小，可能会偏向一个或多个类(多数)而不是其他较少出现的类(少数)。</p><p id="f3aa" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在情感分析<a class="ae kf" href="https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/" rel="noopener ugc nofollow" target="_blank">挑战</a>中，训练数据集不平衡。该挑战旨在检测推文中的仇恨和辱骂言论。训练数据集包含人工判断的推文，0表示没有仇恨或辱骂，1表示没有。然后，使用训练数据集来预测测试数据集推文。</p><blockquote class="kg kh ki"><p id="ea7b" class="jj jk kj jl b jm jn ij jo jp jq im jr kk jt ju jv kl jx jy jz km kb kc kd ke hb bi translated">这是一个完整的Python工作示例</p></blockquote><p id="520a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">首先导入必要的库:</p><pre class="iy iz ja jb fd kn ko kp kq aw kr bi"><span id="6873" class="ks kt hi ko b fi ku kv l kw kx">import pandas as pd, textblob, string<br/>pd.set_option('display.max_colwidth', -1) #To show the whole tweet field<br/>import numpy as np<br/>import random<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer<br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn import metrics,svm<br/>import seaborn as pl<br/>import re<br/>from nltk.tokenize import WordPunctTokenizer<br/>from bs4 import BeautifulSoup</span></pre><p id="4923" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们探索数据集</p><pre class="iy iz ja jb fd kn ko kp kq aw kr bi"><span id="08af" class="ks kt hi ko b fi ku kv l kw kx">#Import Training and Testing Data<br/>train = pd.read_csv('train.csv')<br/>print("Training Set:"% train.columns, train.shape, len(train))<br/>test = pd.read_csv('test_tweets.csv')<br/>print("Test Set:"% test.columns, test.shape, len(test))<br/>#Percentage of Positive/Negative<br/>print("Positive: ", train.label.value_counts()[0]/len(train_cl)*100,"%")<br/>print("Negative: ", train.label.value_counts()[1]/len(train_cl)*100,"%")<br/>'''<br/>Training Set: (31962, 3) 31962<br/>Test Set: (17197, 2) 17197<br/>Positive:  83.45735868130633 %<br/>Negative:  6.295807475218332 %<br/>'''<br/>#Show a bar chart to explore the difference in classes<br/>pl.countplot(train["label"])</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ky"><img src="../Images/d1fa8c63c02ef412debf007cc9ffd15c.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*nnKeP_rFx_QfIu2m1izpjw.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">这两个类别之间的巨大差异(积极的推文与消极的推文)</figcaption></figure><h1 id="1b5c" class="kz kt hi bd la lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">特征工程</h1><p id="a85a" class="pw-post-body-paragraph jj jk hi jl b jm lq ij jo jp lr im jr js ls ju jv jw lt jy jz ka lu kc kd ke hb bi translated">首先，我们需要清理数据集，如果我们打印前五条推文，我们可以看到以下内容:</p><pre class="iy iz ja jb fd kn ko kp kq aw kr bi"><span id="8db9" class="ks kt hi ko b fi ku kv l kw kx">train.head()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/3bba373c50b8f0ccd60fd2bd4f6a6862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7qwHhtsb2eEpLxVMJE06CA.png"/></div></div></figure><p id="e1e3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">推文需要清理，我们需要删除用户提及，#标签，特殊字符等。</p><pre class="iy iz ja jb fd kn ko kp kq aw kr bi"><span id="a353" class="ks kt hi ko b fi ku kv l kw kx">tok = WordPunctTokenizer()<br/>pat1 = r'@[A-Za-z0-9]+'<br/>pat2 = r'https?://[A-Za-z0-9/]+'<br/>combined_pat = r'|'.join((pat1, pat2))</span><span id="fb01" class="ks kt hi ko b fi ma kv l kw kx">def tweet_cleaner(text):<br/>    soup = BeautifulSoup(text, 'lxml')<br/>    souped = soup.get_text()<br/>    stripped = re.sub(combined_pat, '', souped)<br/>    try:<br/>        clean = stripped.decode("utf-8-sig").replace(u"\ufffd", "?")<br/>    except:<br/>        clean = stripped<br/>    letters_only = re.sub("[^a-zA-Z]", " ", clean)    <br/>    # During the letters_only process two lines above, it has created unnecessay white spaces,<br/>    # I will tokenize and join together to remove unneccessary white spaces<br/>    words = tok.tokenize(letters_only)    <br/>    sentence=[]<br/>    for word in words:<br/>        sentence.append(word)<br/>        sentence.append(" ")<br/>    words="".join(sentence).strip()    <br/>    return words<br/>nums = [0,len(train)]<br/>clean_tweet_texts = []<br/>for i in range(nums[0],nums[1]):<br/>    clean_tweet_texts.append(tweet_cleaner(train['tweet'][i]))<br/>nums = [0,len(test)]<br/>test_tweet_texts = []<br/>for i in range(nums[0],nums[1]):<br/>    test_tweet_texts.append(tweet_cleaner(test['tweet'][i]))</span><span id="0b6e" class="ks kt hi ko b fi ma kv l kw kx">train_clean = pd.DataFrame(clean_tweet_texts,columns=['tweet'])<br/>train_clean['label'] = train.label<br/>train_clean['id'] = train.id<br/>test_clean = pd.DataFrame(test_tweet_texts,columns=['tweet'])<br/>test_clean['id'] = test.id</span></pre><p id="e2eb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们对比一下清洗后的五条负面推文:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mb"><img src="../Images/4678c13352031ec0b9798010a1532c3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4s_6GuRZuAfHllyy3g4diA.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">清洁前后的推文</figcaption></figure><h1 id="a927" class="kz kt hi bd la lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">马尔可夫链自动推文生成</h1><p id="bbde" class="pw-post-body-paragraph jj jk hi jl b jm lq ij jo jp lr im jr js ls ju jv jw lt jy jz ka lu kc kd ke hb bi translated">为了减少训练数据集中多数类的偏差，我们需要对少数类进行上采样。我在<a class="ae kf" rel="noopener" href="/@muabusalah/re-sampling-imbalanced-training-corpus-for-sentiment-analysis-c9dc97f9eae1">这篇文章</a>中测试过采样和欠采样技术。但后来我决定通过应用马尔可夫链从现有的推文中形成新的负面推文。</p><p id="be98" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><a class="ae kf" href="https://brilliant.org/wiki/markov-chains/" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj">马尔可夫链</strong> </a>是一种数学模型，描述了一系列学术上可能发生的事件，其中每个事件的概率只取决于前一个事件达到的状态。</p><p id="261d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">要构建这个链，需要将文本中的每个单词插入到字典中，其中的关键字是前一个单词，每次都要增加内部字典中该单词的计数器。这将生成一个字典，其中每个键指向所有跟随它的单词，以及实例的数量。出现的次数越多，这个词出现的机会就越大。</p><pre class="iy iz ja jb fd kn ko kp kq aw kr bi"><span id="d7d4" class="ks kt hi ko b fi ku kv l kw kx">#constructing the normalized mapping (Outer Dictionary)<br/>tempMapping = {}<br/>'''tuple of words) -&gt; {dict: word -&gt; *normalized* number of times the word appears following the tuple}<br/>Example: {('cnn', 'calls'): {'michigan': 1.0}<br/>Contains the set of words that can start sentences'''<br/>mapping = {}<br/>starts = []<br/>'''We want to be able to compare words independent of their capitalization. This is not strong case here as all data set consists of tweets which are normalized to small letter.'''</span><span id="719c" class="ks kt hi ko b fi ma kv l kw kx">def fixCaps(word):<br/>    # Ex: "FOO" -&gt; "foo"<br/>    if word.isupper() and word != "I":<br/>        word = word.lower()<br/>        # Ex: "LaTeX" =&gt; "Latex"<br/>    elif word [0].isupper():<br/>        word = word.lower().capitalize()<br/>        # Ex: "wOOt" -&gt; "woot"<br/>    else:<br/>        word = word.lower()<br/>    return word</span><span id="e33c" class="ks kt hi ko b fi ma kv l kw kx">#Create Dictionary keys<br/>def toHashKey(lst):<br/>    return tuple(lst)<br/>'''Returns the tweets, split into a list of words by adding . to the end of each tweet, there are no punctuation.'''<br/>def wordlist(data):<br/>    split_it = '. '.join([text for text in data])<br/>    wordlist = [fixCaps(w) for w in re.findall(r"[\w']+|[.,!?;]", split_it)]    <br/>    return wordlist</span><span id="8b8b" class="ks kt hi ko b fi ma kv l kw kx">'''addItemToTempMapping -- adds "word" to the "tempMapping" dict under "history".<br/>tempMapping (and mapping) both match each word to a list of possible next words.<br/>Given history = ["the", "rain", "in"] and word = "Spain", we add "Spain" to<br/>the entries for ["the", "rain", "in"], ["rain", "in"], and ["in"].'''<br/>def addItemToTempMapping(history, word):<br/>    global tempMapping<br/>    while len(history) &gt; 0:<br/>        first = toHashKey(history)<br/>        if first in tempMapping:<br/>            if word in tempMapping[first]:<br/>                tempMapping[first][word] += 1.0<br/>            else:<br/>                tempMapping[first][word] = 1.0<br/>        else:<br/>            tempMapping[first] = {}<br/>            tempMapping[first][word] = 1.0<br/>        history = history[1:]<br/># Building and normalizing the mapping</span><span id="7505" class="ks kt hi ko b fi ma kv l kw kx">def buildMapping(wordlist, markovLength):<br/>    global tempMapping<br/>    starts.append(wordlist [0])<br/>    for i in range(1, len(wordlist) - 1):<br/>        if i &lt;= markovLength:<br/>            history = wordlist[: i + 1]<br/>        else:<br/>            history = wordlist[i - markovLength + 1 : i + 1]<br/>        follow = wordlist[i + 1]<br/>        '''if the last elt was a period, add the next word to the start list'''<br/>        if history[-1] == "." and follow not in ".,!?;":<br/>            starts.append(follow)<br/>        addItemToTempMapping(history, follow)<br/>    # Normalize the values in tempMapping, put them into mapping<br/>    for first, followset in tempMapping.items():<br/>        total = sum(followset.values())<br/>        # Normalizing here:<br/>        mapping[first] = dict([(k, v / total) for k, v in followset.items()])</span><span id="1f39" class="ks kt hi ko b fi ma kv l kw kx"># Returns the next word in the sentence (chosen randomly),<br/># given the previous ones.<br/>def next(prevList):<br/>    sum = 0.0<br/>    retval = ""<br/>    index = random.random()<br/>    # Shorten prevList until it's in mapping<br/>    while toHashKey(prevList) not in mapping:<br/>        try:<br/>            prevList.pop(0)<br/>        except:<br/>            prevList=random.choice(starts)<br/>    # Get a random word from the mapping, given prevList<br/>    for k, v in mapping[toHashKey(prevList)].items():<br/>        sum += v<br/>        if sum &gt;= index and retval == "":<br/>            retval = k<br/>    return retval</span><span id="0070" class="ks kt hi ko b fi ma kv l kw kx">def genSentence(markovLength):<br/>    # Start with a random "starting word"<br/>    curr = random.choice(starts)<br/>    sent = curr.capitalize()<br/>    prevList = [curr]<br/>    # Keep adding words until we hit a period<br/>    while (curr not in "."):<br/>        curr = next(prevList)<br/>        prevList.append(curr)        <br/>        # if the prevList has gotten too long, trim it<br/>        if len(prevList) &gt; markovLength:<br/>            prevList.pop(0)<br/>        if (curr not in ".,!?;"):<br/>            sent += " " # Add spaces between words (but not punctuation)<br/>        sent += curr<br/>    return sent</span></pre><p id="8e29" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在让我们生成新的负面推文:</p><pre class="iy iz ja jb fd kn ko kp kq aw kr bi"><span id="d0a8" class="ks kt hi ko b fi ku kv l kw kx">#extracting racist/sexist tweets<br/>negativeTweets=train_clean[train_clean['label']==1]<br/>newlyGeneratedTweets=pd.DataFrame()<br/>#Repeat with the New Sentences for 3 times<br/>for i in range(3):<br/>    data = negativeTweets['tweet']<br/>    markovLength = 2<br/>    numberofsentences=data.size<br/>    buildMapping(wordlist(data), markovLength)<br/>    newTweets = []<br/>    for i in range(numberofsentences):<br/>        newTweet=genSentence(markovLength)<br/>        # Sentence should have more than 10 chatachters<br/>        if len(newTweet) &gt; 15:<br/>            newTweets.append(newTweet)<br/>    d={'tweet':newTweets,'label':1}<br/>    df=pd.DataFrame(data=d)<br/>    newlyGeneratedTweets=newlyGeneratedTweets.append(df)<br/>    newlyGeneratedTweets=newlyGeneratedTweets.reset_index()<br/>    newlyGeneratedTweets=newlyGeneratedTweets.drop(['index'], axis=1)<br/>    print(newlyGeneratedTweets.shape)<br/>'''<br/>(2057, 2)<br/>(4101, 2)<br/>(6160, 2)<br/>'''</span></pre><p id="a99f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我选择生成大约6k条推文，并将它们添加到原始训练数据中，原因是不会遇到<a class="ae kf" href="https://elitedatascience.com/overfitting-in-machine-learning" rel="noopener ugc nofollow" target="_blank">过拟合</a>的情况，在这种情况下，对训练数据集的预测是完美的，但当涉及到测试数据时，性能不会那么好。</p><p id="26c9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我通过使用<a class="ae kf" rel="noopener" href="/datadriveninvestor/an-introduction-to-grid-search-ff57adcc0998"><strong class="jl hj">GridSearchCV</strong></a><strong class="jl hj"/>微调参数来增强我的分类方法，但是我没有将它包括在这个例子中，我还尝试了不同的分类器，其中表现最好的是<a class="ae kf" href="https://scikit-learn.org/stable/modules/naive_bayes.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj"> MultinomialNB </strong> </a></p><pre class="iy iz ja jb fd kn ko kp kq aw kr bi"><span id="5dc8" class="ks kt hi ko b fi ku kv l kw kx">train_cl=train_clean.append(newlyGeneratedTweets)<br/>#train_cl=train_cl.append(hateTweets)<br/>train_cl=train_cl.drop(['id'], axis=1)<br/>#Percentage of Positive/Negative<br/>print("Positive: ", train_cl.label.value_counts()[0]/len(train_cl)*100,"%")<br/>print("Negative: ", train_cl.label.value_counts()[1]/len(train_cl)*100,"%")<br/>pl.countplot(train_cl["label"])</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mc"><img src="../Images/bbd21ff3bfa2acb56f85d1145aef42dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*YgDmc6D3B4--u3jCikTmBA.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">现在少数民族增加到22%左右</figcaption></figure><p id="8232" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">你可以删除小的推文，比如少于10或125个字符的。现在让我们分割训练数据并应用分类器来看看我们的方法是否工作良好</p><pre class="iy iz ja jb fd kn ko kp kq aw kr bi"><span id="2de3" class="ks kt hi ko b fi ku kv l kw kx"># Remove small tweets<br/>train_cl=train_cl[train_cl['tweet'].str.len()&gt;15]<br/>x_train, x_test, y_train, y_test = train_test_split(train_cl['tweet'],train_cl['label'], test_size=0.33, random_state=42)<br/>pr=Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),<br/>                     ('tfidf', TfidfTransformer(use_idf=True, norm='l2')),<br/>                     ('clf', MultinomialNB(alpha=0.02))])<br/>pr.fit(x_train, y_train)<br/>predictions = pr.predict(x_test)<br/>f1Score=metrics.f1_score(y_test,predictions)<br/>print(f1Score)</span></pre><p id="ee5f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><a class="ae kf" rel="noopener" href="/@muabusalah/twitter-hate-speech-sentiment-analysis-6060b45b6d2c"> f1得分</a>结果是压倒性的，为0.9703465523401215，但当我们应用测试数据时，情况并非如此:</p><pre class="iy iz ja jb fd kn ko kp kq aw kr bi"><span id="00b1" class="ks kt hi ko b fi ku kv l kw kx">#Now working with Real challenge Data<br/>train_x=train_cl['tweet']<br/>valid_x=test_clean['tweet']<br/>train_y=train_cl['label']</span><span id="1f92" class="ks kt hi ko b fi ma kv l kw kx">pr=Pipeline([('vect', CountVectorizer(ngram_range=(1, 2))),<br/>                     ('tfidf', TfidfTransformer(use_idf=True, norm='l2')),<br/>                     ('clf', MultinomialNB(alpha=0.02))])<br/>pr.fit(x_train, y_train)<br/>predictions = pr.predict(valid_x)<br/>d={'id':test['id'],'label':predictions}<br/>df=pd.DataFrame(data=d)<br/>df.to_csv("test_predictions_MultiNomialNB_Markov.csv", index=False)</span></pre><p id="0ccf" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">将结果提交到竞赛页面后，我能达到的最好结果是:0.752655538694992，这超过了其他方法，我尝试了<a class="ae kf" rel="noopener" href="/@muabusalah/twitter-hate-speech-sentiment-analysis-6060b45b6d2c">完全随机化树</a>和<a class="ae kf" rel="noopener" href="/@muabusalah/re-sampling-imbalanced-training-corpus-for-sentiment-analysis-c9dc97f9eae1"> AllKNN </a>。</p><p id="d415" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">完整代码可在<a class="ae kf" href="https://github.com/mabusalah/TwitterSentimentAnalysis/blob/master/MarkovChainTweetAutoGeneration.ipynb" rel="noopener ugc nofollow" target="_blank"> gihub </a>上获得。</p></div></div>    
</body>
</html>