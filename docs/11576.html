<html>
<head>
<title>Introduction to Loss Change Allocation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">损失变动分配简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-loss-change-allocation-afca065be618?source=collection_archive---------17-----------------------#2020-12-09">https://medium.com/analytics-vidhya/introduction-to-loss-change-allocation-afca065be618?source=collection_archive---------17-----------------------#2020-12-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6b90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将写损失变化分配(LCA)，这是优步人工智能实验室推出的一种新方法，有助于提供对深度神经网络训练过程的可见性。</p><p id="ce38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Samsung Next的一次演讲中，Jason Yosinski(LCA论文的作者)通过绘制计算的兴起和数据访问的增加与我们对机器学习模型的科学理解的对比，说明了对这种技术的需求。约辛斯基认为，我们的能力和理解之间日益扩大的差距是一个新领域的机会，类似于“人工智能神经科学”。在这个领域，Yosinski认为LCA是一个潜在的强大工具，它可以提供每个参数、每个迭代的模型训练视图。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/b4eb95f6b8b299b564114e7310ee8e55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1v7RJP1MsyZX-pSPpYvR3w.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:<a class="ae jt" href="https://www.youtube.com/watch?v=737PKW1Rt_g&amp;list=LL&amp;index=2" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=737PKW1Rt_g&amp;list = LL&amp;index = 2</a></figcaption></figure><p id="97a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章的其余部分，我将尽我所能总结LCA，并使用我自己实现的LCA计算从原始论文中重新创建一个更简单的实验。</p><h1 id="f224" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">损失变化分配</h1><p id="b21f" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">LCA的基本直觉是，对于训练的每次迭代，目标函数的改进(“损失变化”)可以分布在模型中的所有参数上(“分配”)。每个模型参数在改善(或损害)目标函数方面值得多少信任可以被称为该特定迭代的参数LCA。</p><p id="d3a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更正式地说，LCA是从两个训练步骤之间的目标函数的变化开始导出的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kx"><img src="../Images/31f42c5ac06b706001bd9ecafa72d05c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HoRThhS8kbcvIzDVvn9yrw.png"/></div></div></figure><p id="3cde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中L(*)是目标(例如，均方损失或交叉熵损失)，作为θ的函数，即在特定迭代中的模型参数。对于神经网络，模型参数通常被描述为层间权重矩阵的条目。然而，对于这种计算，所有这些参数被展平以形成一个长向量θ。</p><p id="b8e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用目标函数中的这个增量作为起点，然后我们可以使用来自微积分的一阶泰勒近似来近似这个量:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/fae82e5167c5a6ed5b82c7ed4af16123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MUcIVcNeupzHQl2Y41Gfmg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">注意:在最初的LCA论文中，作者还在目标函数中使用了一个更精确的delta估计量，以确保训练的累积近似误差总是&lt; 1%。第二个估计量比一阶泰勒近似要复杂得多，在本文中将被忽略。</figcaption></figure><p id="237e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的等式中，右手边是1)步骤t处目标函数相对于模型参数的梯度和2)训练步骤之间模型参数值的变化之间的点积。注意:这两个对象都是向量，其长度与模型中的参数数量相同，并且这些向量的点积近似于模型在特定训练步骤的改进。</p><p id="12b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，回想一下线性代数，两个向量的点积是作为相应项的乘积之和来计算的，在作者的注释中是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kz"><img src="../Images/9ee9ebdd14f8c556dcef8e2dc0260fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*CpU2c4aNLaMO9w9bXPlh8w.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">在这个等式中，K是模型中的参数，并且总和的每一项是在时间步长t的一个且仅一个模型参数I的函数</figcaption></figure><p id="ed21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从这里，我们可以看到，点积等于分量的总和，其中每个分量是单个模型参数I的函数。更具体地说，该分量是1)时间步长t处参数I的变化和2)时间步长t处其对梯度的贡献的乘积</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es la"><img src="../Images/29de6b9a09d5ae014fe3ce6454adef90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ZF4BDnuZtuj_mnTqZSrSw.png"/></div></div></figure><p id="41b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">综合来看，这就是作者定义的在步骤t(上图中的A_t，I)对特定参数I的损失变化分配。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lb"><img src="../Images/9e2a892b8d9a2b3dca73b5d2bd6b5cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cEu_gbcCRD63jp0tdfwV0A.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">论文中的一个有用的图为LCA提供了直观的视觉效果。由于目标函数仅沿着部分(a)中的θdim-2减小，因此LCA仅沿着部分(c)中的θdim-2而不是沿着部分(b)中的θdim-1累积。</figcaption></figure><p id="0bec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经定义了LCAs，让我们看看它们的一些好的属性，这些属性可以帮助我们深入了解培训过程。</p><p id="62bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个很好的特性是LCAs与目标函数直接相关。例如，单个训练步骤的所有参数的LCAs之和大约等于该步骤的目标函数的变化。</p><p id="6862" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以做的另一件事是对所有训练迭代中的单个参数的LCAs求和，以了解单个参数对整个学习过程的贡献有多大。我们也可以对一个层中的所有参数重复这一过程，以了解整个层的贡献。</p><p id="cbb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，LCAs的标志信息非常丰富。负的LCA是一件好事，表明特定的参数改进了该训练步骤中的模型。另一方面，一个正的LCA是一件不好的事情，并且表明在训练期间该参数在该点上伤害了模型。</p><h1 id="021f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">实验结果</h1><p id="ce99" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在定义损失变化分配并讨论其好处后，作者利用这一技术来探索神经网络在两个典型分类任务MNIST和CIFAR-10上的训练过程。使用LCA，作者对神经网络的训练进行了一些细微的观察，我建议读者参考底部链接的论文。不过，<em class="lc">重点外卖是，</em> <em class="lc">学习很吵</em>。</p><p id="b496" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了证明这一事实，我重新创建了一个更简单的实验，其中一个3层前馈全连接架构被训练来对MNIST上的手写数字进行分类。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ld"><img src="../Images/78edb70be0ca5ea9154a9586f77ed781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQRSBnVtqxIzdfwV7inxjA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd jw">左图:</strong>神经网络架构草图。有784 * 100+100+100 * 50+50+50 * 10+10 = 84060个参数。<strong class="bd jw">右图:</strong>来自MNIST数据集的样本图像。</figcaption></figure><p id="43b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了最后一层之外，网络使用ReLU激活，并且使用学习率为0.002的Adam优化器对4个时期进行训练(这些细节由论文提供)。批次大小未指定，但我推断为227，因为在论文中每个时期有220个梯度更新步骤，训练集的大小为50，000幅图像。</p><p id="6518" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练网络接近收敛，并且在每个梯度更新步骤，使用上述方法计算每个参数的LCA。一个微妙的细节是，尽管梯度更新是基于小批量进行的，<em class="lc">每一步的训练损失和LCAs都是从整个训练集的损失中导出的</em>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/fee4621b0a7fda783a02336337d09d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kOhjvPO_V7EQuVTU6Jem4Q.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">3层模糊神经网络在4个训练周期内几乎收敛</figcaption></figure><p id="bf5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用一阶泰勒近似法计算LCA，从视觉上看，每个训练步骤的LCA之和非常接近训练损失的真实变化。这一步与论文不同，在论文中，作者使用了更细粒度的近似，因此在训练中近似的累积误差小于1%。然而，我发现使用一阶泰勒近似的最终结果与论文相似，所以我选择使用更简单的方法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lf"><img src="../Images/0b5f725d7871e77364fffad1b29028e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ul_PMwIdzJ8P540-Wsz60A.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">每步训练损失的实际增量与近似增量的比较，后者用于计算LCAs。前50步被省略了，因为最初的损失剧烈波动。然而，实际增量和近似增量之间的总体相关性是. 805±0.0283。</figcaption></figure><h2 id="9b1d" class="lg jv hi bd jw lh li lj ka lk ll lm ke iq ln lo ki iu lp lq km iy lr ls kq lt bi translated">主要收获:只有略多于一半的参数有助于学习</h2><p id="70df" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">通过检查LCAs，我能够重现这篇论文的主要结果:训练噪音很大。具体来说，本文通过比较大于0(有害)的非零LCA的百分比和小于0(有益)的非零LCA的百分比对此进行了量化。</p><p id="2177" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">经过3次独立运行，论文中帮助LCAs的百分比为55.82±0.09，我的结果为54.48±0.23。这是令人惊讶的，因为我们可能会期望帮助LCAs的百分比要高得多，考虑到神经网络在分类任务(如MNIST)中的表现有多强。</p><p id="0508" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看这一结果的另一种方式是显示非零LCAs的直方图。在这里，我们还可以看到正值和负值几乎平分秋色:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/ad55fafc77d6f52387bf41a5053b064f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MYrOk2tRfUdMiAiHo_1kkA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来自我的娱乐<strong class="bd jw">(左)</strong>和原始论文<strong class="bd jw">(右)</strong>的LCA值直方图</figcaption></figure><p id="506e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当将每次迭代的LCAs与训练进行比较时，也可以看到相同的趋势:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lv"><img src="../Images/185e81e2d395cecab90e4d8ad518e934.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f8STMl53FUrzaX11XNbRGA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">我的再创作<strong class="bd jw">(左)</strong>和原始论文<strong class="bd jw">(右)</strong>中对每个训练步骤有帮助的参数的百分比。注意:在我的娱乐中有更多的零价值LCA。</figcaption></figure><p id="414a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在所有情况下，LCAs描绘了这样一幅画面，即深度学习的训练经常非常嘈杂，并且很大一部分参数在训练期间没有效果或者积极地伤害了目标。</p><p id="76bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">也就是说，我的结果和论文之间有两个显著的差异。首先，1)在我的实验中，LCA的大小通常较小，2)非零LCA的数量较大，这在比较不同时间的LCA时尤其明显。尽管我的神经网络在训练过程中收敛，并且结果大体相似，但这些问题仍然存在。我目前还不知道这是为什么，我很乐意在评论中进一步讨论任何想法。</p><h1 id="09d9" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">结论:</strong></h1><p id="b8f2" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">损失变化分配是一种潜在的强大工具，用于提供对神经网络训练过程的洞察。通过提供每个参数、每个迭代的度量，LCA提供了比传统度量(如训练损失或验证准确性)更高维度和更丰富的训练过程特征。</p><p id="85a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LCAs提供的一个直接见解是，训练过程似乎非常嘈杂，我能够在一个小玩具例子上独立验证这一观察。</p><p id="a17a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我想进一步探索的两个方向是:</p><ol class=""><li id="9eef" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated">神经结构搜索的应用，以及逐层LCAs是否可以作为某种学习信号用于生成最佳神经网络结构。</li><li id="9f0d" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">可解释的人工智能:例如，在一个多类分类问题中，使用LCAs来识别在正确分类特定类时很重要的某些权重或训练周期。</li></ol></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><p id="8c8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谢谢你看我的帖子！如果您想了解有关LCA的更多信息，以下链接可能会有所帮助:</p><p id="f2af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原LCA文件:【https://arxiv.org/pdf/1909.01440.pdf T2】</p><p id="460b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原回购:【https://github.com/uber-research/loss-change-allocation T4】</p><p id="d04c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于LCA的优步博客文章:<a class="ae jt" href="https://eng.uber.com/loss-change-allocation/" rel="noopener ugc nofollow" target="_blank">https://eng.uber.com/loss-change-allocation/</a></p><p id="149e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的代码回购:<a class="ae jt" href="https://github.com/MattD18/lca-demo" rel="noopener ugc nofollow" target="_blank">https://github.com/MattD18/lca-demo</a></p><p id="6fd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我在研究这个主题时发现了一个关于LCA的不错的中型帖子:<a class="ae jt" rel="noopener" href="/data-in-all-things/loss-change-allocation-a-microscope-into-model-training-da0f142a047d">https://Medium . com/data-in-all-things/loss-change-allocation-a-microscope-into-model-training-da0 f 142 a 047d</a></p></div></div>    
</body>
</html>