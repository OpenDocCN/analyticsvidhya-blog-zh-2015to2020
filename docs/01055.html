<html>
<head>
<title>A walk through Random Forest Decision Tree(RFDT) Algorithm with Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带代码的遍历随机森林决策树(RFDT)算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-walk-through-random-forest-decision-tree-rfdt-algorithm-with-code-932271af4ec7?source=collection_archive---------5-----------------------#2019-09-28">https://medium.com/analytics-vidhya/a-walk-through-random-forest-decision-tree-rfdt-algorithm-with-code-932271af4ec7?source=collection_archive---------5-----------------------#2019-09-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/75f54e2e07cecb40a2fc9fc703e6134a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8oKVWjyKs_bjoz-v3NPUw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@amosg?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">阿莫斯G </a>在<a class="ae iu" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="1a8e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">先决条件:机器学习基础</em></p><p id="a907" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我将带您浏览一下随机森林决策树(RFDT)。出发前，让我们先装备好一些必备的东西，这样我们就不会在森林里迷路了；).在步入森林之前，首先要掌握的概念是决策树本身。我们去看看。</p><h1 id="e773" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">什么是决策树</h1><p id="7afc" class="pw-post-body-paragraph iv iw hi ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">一棵<strong class="ix hj"> <em class="jt">决策树</em> </strong>不过是一个<strong class="ix hj"> <em class="jt"> if-else </em> </strong>分支的序列或者简称为<strong class="ix hj"> <em class="jt">嵌套if-else </em> </strong>。我们大多数了解计算机编程基础的人都知道if-else语句是如何工作的。但是为了完整起见，让我简单解释一下if-else是如何工作的。</p><p id="9a69" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你已经知道if-else分支的工作原理，你可以跳过下一段。</p><p id="fa7d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑下面的伪代码。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="f45c" class="lg jv hi lc b fi lh li l lj lk">if(condition1)<br/> {<br/>  statement1<br/> }<br/>else if(condition2)<br/> {<br/>  statement2<br/> }<br/>else<br/> {<br/>  statement3<br/> }</span></pre><ul class=""><li id="c61e" class="ll lm hi ix b iy iz jc jd jg ln jk lo jo lp js lq lr ls lt bi translated">如果条件1被评估为真，将执行语句1，并跳过所有其他语句。</li><li id="15a5" class="ll lm hi ix b iy lu jc lv jg lw jk lx jo ly js lq lr ls lt bi translated">如果条件1为假，条件2为真，则只执行语句2。</li><li id="bb18" class="ll lm hi ix b iy lu jc lv jg lw jk lx jo ly js lq lr ls lt bi translated">如果条件1和条件2都为假，则只执行语句3。这种结构叫做<strong class="ix hj"> <em class="jt">嵌套if-else </em> </strong>。我们可以根据需要自由地拥有任意多的if-else语句。</li></ul><p id="ddd8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果或<em class="jt">否则</em>，我们根据每个<em class="jt">处的条件进行分支。我们可以用树形图的形式表达嵌套的if-else语句，并由此命名为<strong class="ix hj"> <em class="jt">决策树。</em> </strong></em></p><p id="d1c2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们考虑XOR真值表的例子。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/e38fb58fdc21f02f302c4bf7ee72e222.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*ZZIhDssFdcNKxvEgPGOB3w.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">异或真值表</figcaption></figure><p id="9cb9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面的真值表可以用如下所示的嵌套if-else来表示。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="553f" class="lg jv hi lc b fi lh li l lj lk">if(A==0)<br/> {<br/>  if(B==0)<br/>   {<br/>    output=0<br/>   }<br/>  else<br/>   {<br/>    output=1<br/>   }<br/> }<br/>else<br/> {<br/>  if(B==0)<br/>   {<br/>    output=1<br/>   }<br/>  else<br/>   {<br/>    output=0<br/>   }<br/> }</span></pre><p id="23f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这可以用如下所示的树形图来表示。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/7cb24050929937e99e0620453b70e589.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*KQda5sCxGbCnK0jJsWxOXw.jpeg"/></div></figure><p id="e73e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们评估一个条件并基于它进行分支的每个点称为一个节点/顶点。</p><ul class=""><li id="4bb3" class="ll lm hi ix b iy iz jc jd jg ln jk lo jo lp js lq lr ls lt bi translated">树的起始节点/顶点称为根节点/顶点。</li><li id="1296" class="ll lm hi ix b iy lu jc lv jg lw jk lx jo ly js lq lr ls lt bi translated">终止节点/顶点称为叶节点/顶点。</li><li id="0e42" class="ll lm hi ix b iy lu jc lv jg lw jk lx jo ly js lq lr ls lt bi translated">除了根和叶以外的所有节点/顶点都称为内部节点/顶点。</li></ul><p id="b27e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在叶节点，我们做出最后的决定。在上面的例子中，在叶节点，我们决定输出是0还是1。</p><p id="e6fb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">就机器学习而言，上述真值表中的A列和B列称为输入特征，输出列称为类别/标签。选择哪个特征应该是根节点及其后续节点是基于这些特征的熵或gini杂质，这超出了我们当前讨论的范围。</p><p id="0a0c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以从这个<a class="ae iu" href="https://homepage.cs.uri.edu/faculty/hamel/courses/2018/spring2018/csc581/lecture-notes/31a-decision-trees.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>中读到更多关于决策树构造的数学细节。</p><h1 id="7ea7" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">过拟合问题</strong></h1><p id="3fb3" class="pw-post-body-paragraph iv iw hi ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">树的深度指的是树的层数。所以有很多层的树有很高的深度。例如，参考下图。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/27622709cd23d36b64a2bd41d9c9678d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*G8cTDxs6AXaT1IZCYFDGCQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">高深度树</figcaption></figure><p id="c9cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到，随着深度的增加，最终的决定是在评估了很多条件之后做出的。因此，具有很高深度的机器学习模型将对训练数据给出很高的预测精度，但它可能无法对看不见的测试数据进行一般化预测，即，很高的深度可能导致<strong class="ix hj"> <em class="jt">过拟合</em> </strong>。</p><p id="4dbe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果深度太小，即使在训练数据上，模型的性能也会很差。这在拟合 下称为<strong class="ix hj"> <em class="jt">。</em></strong></p><h1 id="cb4c" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">装袋解决过度装配</h1><p id="6828" class="pw-post-body-paragraph iv iw hi ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">Bagging是一种用于解决过拟合问题并结合许多机器学习模型优点的集成技术。打包也称为Bootstarpped聚合。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/af079d97d0939248952029fb9450a922.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*3yXxDw_0As8mf-tytHSTng.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">装袋技术</figcaption></figure><p id="987d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在bagging中，实际的列车数据集D通过替换进行采样，以形成n个数据集D1、D2、D3…Dn，如上图所示。这些新数据集中每一个都有m个数据点。现在，独立的机器学习模型M1，M2，M3…Mn在这些数据集上接受训练。所有这些模型的结果被汇总以获得最终输出。聚集可以通过平均值、中间值、多数投票分类器等。</p><p id="a42c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">方差是模型性能随着输入数据集的变化而变化的量。如果方差很高，则模型过度拟合。例如，假设输入数据集D有10k个数据点。不同的模型作用于D的子集，比如说5k个点。因此，如果D中的100个点发生变化，它不会显著影响所有模型的组合结果，因为每个模型都使用D的一个小样本集。这就是bagging可以抵抗过度拟合的方式。</p><h1 id="3145" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">随机森林决策树</h1><p id="5f76" class="pw-post-body-paragraph iv iw hi ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">在装袋中，我们对行进行随机采样，即我们从实际数据集D中随机选择数据点，以生成新的数据集D1、D2、D3…Dn。在RFDT中，除了行采样，我们还进行列采样。即，我们从实际数据集D中随机选择行(数据点)和列(特征)以形成新的数据集D1、D2、D3…Dn。</p><p id="831d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，如果实际数据集D具有n个数据点和D个特征，则形成的每个新数据集将仅具有m个数据点和D’个特征，其中m <n and="" d="" thus="" rfdt="" introduces="" randomness="" to="" the="" data="" set="" in="" terms="" of="" both="" points="" features.=""/></p><p id="eed4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">In short,</p><p id="9420" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> RFDT= Bagging +特征采样</strong></p><p id="8675" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在RFDT中，基本模型是M1、M2、M3...Mn都是决策树模型。</p><p id="208c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RFDT模型的性能随着树的深度和使用的基本模型的数量而变化。这是RFDT模型的两个超参数。</p><h1 id="7aa7" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">我们来编码吧！</h1><p id="03f9" class="pw-post-body-paragraph iv iw hi ix b iy ks ja jb jc kt je jf jg ku ji jj jk kv jm jn jo kw jq jr js hb bi translated">作为一个例子，我采用XOR数据集，因为它很容易解释。我已经将数据集作为csv文件放在这个<a class="ae iu" href="https://github.com/jijoga/XOR-dataset" rel="noopener ugc nofollow" target="_blank">链接</a>中。</p><p id="265c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据集有3列；即A、B和XOR。a和B是输入特征，而XOR列是我们的机器学习模型必须预测的输出。</p><p id="cdb2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们使用pandas加载数据集。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="9e97" class="lg jv hi lc b fi lh li l lj lk">import pandas as pd<br/>data=pd.read_csv('XOR dataset.csv')</span></pre><p id="1284" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据集的样本:</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="1179" class="lg jv hi lc b fi lh li l lj lk">data.head()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es md"><img src="../Images/7d9751488cb2c33c7499897668ff62f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*zvGpEucUVVNus6x2HZEgpg.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">数据集样本</figcaption></figure><p id="04f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们使用seaborn库绘制数据点的散点图，以获得数据的清晰图像。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="0ecf" class="lg jv hi lc b fi lh li l lj lk">import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="de16" class="lg jv hi lc b fi me li l lj lk">fig, ax = plt.subplots(figsize=(8, 6))<br/>sns.scatterplot(ax=ax,x="A", y="B",hue='XOR', data=data,s=60)<br/>plt.show()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/35591cc3fb5acdac4d52a0a454f4a0bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*VrM2f0zrH8-qByprgKXUTQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">数据点散点图</figcaption></figure><p id="e4fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们将数据分为输入训练数据和输出类标签</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="b400" class="lg jv hi lc b fi lh li l lj lk">X_train,y_train=data[['A','B']],data['XOR']</span></pre><p id="2025" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在输入训练数据已经准备好了，让我们将RFDT模型放在上面。我在sklearn中使用了GridSearchCV函数来尝试一些超参数(max_depth和n_estimators)的组合，并获得了理想的模型。max_depth是每个决策树模型的最大深度，n_estimators是决策树模型的数量。你可以通过这个<a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">链接</a>了解更多关于GridSearchCV的信息。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="145f" class="lg jv hi lc b fi lh li l lj lk">from sklearn.model_selection import GridSearchCV<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="96e0" class="lg jv hi lc b fi me li l lj lk">tuned_param={'max_depth':range(1,11),'n_estimators':range(1,11)}<br/>model=GridSearchCV(RandomForestClassifier(n_jobs=-1),param_grid=tuned_param,return_train_score=True)<br/>model.fit(X_train, y_train)</span></pre><p id="3355" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们打印出最好的超参数和相应的精度</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="f612" class="lg jv hi lc b fi lh li l lj lk">print("Best hyper paramters:",model.best_params_)<br/>print("Best accuracy value: ",model.best_score_ )</span></pre><p id="2377" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><p id="8f4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最佳超参数:{'max_depth': 7，' n_estimators': 6} <br/>最佳精度值:0.9925</p><p id="7720" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，使用6个基本模型和最大深度7，我们得到99.25%的准确度。让我们绘制各种超参数组合的精度热图，以便了解模型性能如何随着超参数的变化而变化。在热图中，随着颜色亮度的增加，准确度值增加，反之亦然。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="a4fd" class="lg jv hi lc b fi lh li l lj lk">ac_df=pd.DataFrame(model.cv_results_['params'])</span><span id="c135" class="lg jv hi lc b fi me li l lj lk">#Creating a data frame with hyperparameters and accuracy<br/>ac_df["accuracy"]=model.cv_results_['mean_test_score']<br/> <br/>#Pivoting the dataframe for plotting heat map<br/>ac_df=ac_df.pivot(index='max_depth',columns='n_estimators',values='accuracy')</span><span id="17e8" class="lg jv hi lc b fi me li l lj lk">#Plotting the graph<br/>plt.figure(figsize=(15,8))<br/>sns.heatmap(data=auc_df,annot=True)<br/>plt.title("Accuracy plot for Train data")<br/>plt.show()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/8d06bc7c1c404d7b248dd41a22b3928e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQD-41GncJUe1mvvgu2uhA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">超参数的准确度图</figcaption></figure><p id="7769" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们检查模型的决策面如何随超参数而变化。我将n_estimators固定为6，并将max_depth参数从1改为8。下图中的蓝色区域表示落在该区域中的任何点都将被模型预测为蓝点(0)。绿色(1)也是如此。我们可以看到，对于n_estimators=6和max_depth=7，获得了最佳决策表面，这又是最佳超参数。</p><pre class="kx ky kz la fd lb lc ld le aw lf bi"><span id="1d44" class="lg jv hi lc b fi lh li l lj lk">#Reference:<a class="ae iu" href="https://stackoverflow.com/questions/51297423/plot-scikit-learn-sklearn-svm-decision-boundary-surface" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/questions/51297423/plot-scikit-learn-sklearn-svm-decision-boundary-surface</a><br/>#<a class="ae iu" href="https://stackoverflow.com/questions/31726643/how-do-i-get-multiple-subplots-in-matplotlib" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/questions/31726643/how-do-i-get-multiple-subplots-in-matplotlib</a><br/>import numpy as np<br/>def make_meshgrid(x, y, h=.02):<br/>    x_min, x_max = x.min() - 1, x.max() + 1<br/>    y_min, y_max = y.min() - 1, y.max() + 1<br/>    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))<br/>    return xx, yy</span><span id="e46a" class="lg jv hi lc b fi me li l lj lk">def plot_contours(ax, clf, xx, yy, **params):<br/>    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])<br/>    Z = Z.reshape(xx.shape)<br/>    out = ax.contourf(xx, yy, Z, **params)<br/>    return out</span><span id="21ff" class="lg jv hi lc b fi me li l lj lk">fig, ax = plt.subplots(nrows=4, ncols=2,figsize=(15,20))</span><span id="b1cd" class="lg jv hi lc b fi me li l lj lk"># Set-up grid for plotting.<br/>X0, X1 = X_train['A'], X_train['B']<br/>xx, yy = make_meshgrid(X0, X1)</span><span id="bed1" class="lg jv hi lc b fi me li l lj lk">depth=1<br/>for row in ax:<br/>    for col in row:<br/>            model=RandomForestClassifier(max_depth=depth,n_estimators=6,n_jobs=-1)<br/>        clf = model.fit(X_train, y_train)<br/>        plot_contours(col, clf, xx, yy, cmap=plt.cm.winter, alpha=0.8)<br/>        col.scatter(X_train['A'], X_train['B'], c=y_train, cmap=plt.cm.winter, s=40, edgecolors='k')<br/>        col.set_ylabel('B')<br/>        col.set_xlabel('A')<br/>        col.set_xticks(())<br/>        col.set_yticks(())<br/>        col.set_title('n_estimator=6 and max_depth={}'.format(depth))<br/>        depth+=1<br/>plt.show()</span></pre><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/0f27c2bfd854496689f1cccfa9304c23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dtCQLktR6kFTm_sEEl6vxg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">决策面</figcaption></figure></div></div>    
</body>
</html>