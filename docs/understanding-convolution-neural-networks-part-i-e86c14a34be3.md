# 理解卷积神经网络-第一部分

> 原文：<https://medium.com/analytics-vidhya/understanding-convolution-neural-networks-part-i-e86c14a34be3?source=collection_archive---------16----------------------->

在本文中，我将解释用于构建卷积神经网络的主要模块，然后从头开始构建卷积神经网络。CNN 已经被证明在图像分类、分割、目标检测等方面工作得相当好。与具有较少参数的全连接神经网络相比，CNN 往往表现得相当好。让我们定义一些我们将在本文中使用的术语。

f =过滤器尺寸

n_filers =过滤器数量

p =填充

s =步幅

**为什么选择 CNN？**

![](img/750e2fb9df9407eadd5958c1b9483003.png)

图 1.1 10 个 3 x 3 滤波器的卷积运算。

在图 1.1 中，假设我们的过滤器大小为 3，过滤器数量为 10。输出卷的大小为 30 x 30 x 10。该卷积层中的参数总数为[3 x 3 +1] x 10 = 100。现在让我们假设这是一个完全连接的神经网络，参数的数量将是[32 x 32 x 3]x[30 x 30 x 10]≃2700 万。这需要训练很多参数，而且计算量很大。由于**参数共享**和 s **连接的稀疏性**，卷积层的参数数量较少。参数共享意味着想象一个负责检测图像中垂直边缘的滤波器，在卷积运算期间，该滤波器将在整个图像上滑动，寻找垂直边缘。这意味着在图像的一部分有用的特征检测器，例如在这种情况下的垂直边缘检测器，可能在图像的另一部分也有用，因此权重被共享。连接的稀疏性意味着在每一层中，每个输出值只依赖于少量的输入，如图 1.2 所示。这使得 CNN 更容易过度拟合，并且可以用较小的训练集来训练。

![](img/4642c9bf6c390273c76527e83487e66a.png)

图 1.2 图像 I 与滤波器 K 的卷积[Ihab Sami Mohamed，2017]

在图 1.2 中，卷积运算期间，输出图像中的值 **4** 仅取决于输入图像中红色窗口中的 9 个值。这就是所谓的连接稀疏性。

**卷积层**

在卷积层，卷积滤波器遍历图像并计算矩阵(图像中像素与图像上卷积窗口的元素乘积)。卷积运算缩小图像，图像边界的像素在输出中仅使用一次，因为它们在卷积窗口中不重叠。这可能导致图像边缘的信息丢失。为了避免这个问题，使用了填充。卷积层的超级参数是:填充，步幅和过滤器大小。

![](img/cb0744caabcb4a3ad3c378c11181bf73.png)

图 1.3 使用 3 x 3 滤波器的输入图像的卷积运算“x”。

在图 1.3 中，大小为 3 x3 的输入图像部分与大小为 3 x 3 的滤波器进行卷积，在每个位置对像素值进行逐元素乘法运算。**有效卷积**表示无填充，**相同卷积**表示填充，使得输出大小与输入大小相同。按照惯例，在计算机视觉中卷积滤波器通常是奇数。如果填充相同，p = f-1/2，其中 f 是滤波器大小。**步进卷积**是指将卷积窗口在图像上移动 s 行和 s 列。

**填充**

它在图像的边框上添加零。例如，如果一个(n x n)形状的图像被填充了一个数量 p，它将产生如下图 1.4 所示的图像

![](img/bf062b3ae9e88cf7dd7e68555cd30f30.png)

图 1.4 3 x 3 的图像用数量 2 填充。

填充有助于恢复卷积运算过程中可能丢失的边缘信息。它允许您使用卷积层，而不必收缩体积的高度和宽度。这对于建立更深层次的网络很重要，因为否则当你进入更深层次时，高度/宽度会缩小。一个重要的特例是**相同卷积**，其中高度/宽度在一层之后被精确保留。它帮助我们在图像的边缘保留更多的信息。如果没有填充，下一层的值很少会受到图像边缘像素的影响。

**跨步**

跨距意味着卷积滤波器在计算下一个值之前跳跃多长时间。如果图像是(n×n)，步距是 2，滤波器是(f×f)，滤波器将在水平和垂直方向上以步距 2 的重叠对图像进行卷积。在图 1.5 中，过滤器将计算红色窗口像素的元素乘积，然后它将在水平方向移动 2 个像素，并与蓝色窗口和绿色窗口卷积。然后垂直移动 2 个像素，直到图像结束。

![](img/3feb8b9411f0ffdf706084cd55a24056.png)

图 1.5 滤波器为 3 x 3、步长为 2 的卷积运算。

**热卢层**

它使用 ReLU(整流线性激活函数),如果大于零，则直接返回输入提供的值，如果输入为零或小于零，则返回零值。

![](img/1d5af6238b33d56232ad29e2649b776a.png)

其中 Z 是输入。

![](img/a4ad9fc97e13a77373d9a527f639237a.png)

图 1.6 在 3 x 3 输入图像上重新激活。

ReLU 是隐藏层的默认选择。它使学习更快，因为它防止梯度变为零，从而减慢训练。

**最大池层**

Maxpooling 计算 image (n x n)中窗口(f x f)的最大值。过滤器尺寸 *f* 和步幅 *s* 是该层的超参数。

![](img/b1b73083c72af0b2ccb57786a95b6183.png)

图 1.7 过滤器大小为 2 x 2、步幅为 2 的最大池。

池(POOL)层减少了输入的高度和宽度。它有助于减少计算，并有助于使特征检测器对其在输入中的位置更加不变。最大池化背后的想法是，它在窗口中选取最大值，该值可能是任何特定的特性，因此它会保留在最大池输出中。假设在左上角的 2 x 2 窗口中检测到一个垂直边缘，则取最大值来保留该特征。如果没有检测到，那么最大值仍然很小，并且该特征没有首先被检测到。值得注意的一点是，max pool layer 没有用于训练反向传播的参数。

**展平图层**

它对输入向量执行“展平”。它将 n 维特征矩阵转换为一个向量，该向量可以输入到一个完全连接的神经网络分类器中。展平层没有任何要学习的参数。

**参考文献**

伊哈卜·萨米·穆罕默德·穆罕默德。*使用激光测距仪和机器学习技术检测和跟踪托盘*。Diss。硕士论文，欧洲高级机器人学硕士 Plus (EMARO+)，热那亚大学，2017。