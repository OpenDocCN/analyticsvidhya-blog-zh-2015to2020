<html>
<head>
<title>The Wayback Machine Scraper</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回程机器铲运机</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-wayback-machine-scraper-63238f6abb66?source=collection_archive---------1-----------------------#2020-01-18">https://medium.com/analytics-vidhya/the-wayback-machine-scraper-63238f6abb66?source=collection_archive---------1-----------------------#2020-01-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/464e4f80928c82bb57fac99db9b9548d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*tS_BrrYOkZRCXisEAEldcA.jpeg"/></div></figure><div class=""/><p id="11be" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">为什么是Wayback机铲运机？</strong></p><p id="f5a0" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">收集数据的Web抓取是一种常见的做法，我想抓取一些新闻网站来收集某些数据元素，如新闻标题、摘要和每篇文章的url。这个想法是编译一个新闻数据集来训练话题模型，如LDA、NMF和奇异值分解。更多关于主题模型实现的信息将在接下来的文章中发布。</p><p id="55f2" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我可以走api路线，使用一些新闻api来收集相同的数据点，但是对于大多数新闻API，您要么必须支付费用来每天发出更高的请求，要么使用几个不同的API来从多个新闻来源收集数据。<a class="ae jk" href="https://archive.org/help/wayback_api.php" rel="noopener ugc nofollow" target="_blank">折返机</a>铲运机解决了这两个问题。Wayback Machine api可以免费使用，但每天的请求量很高。Wayback Machine存档了大多数新闻网站，所以你可以使用一个api从不同的新闻来源收集相同的信息。</p><p id="66ba" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">根据请求的强度，抓取可能会使服务器过载，并且有被阻塞的风险。Wayback machine api最大限度地降低了这种风险，因为我们不是针对单个新闻网站。我们仍然必须遵循Wayback机器api规则，但我们不会遇到单个新闻网站服务器过载的风险。</p><p id="3b5d" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">铲运机深潜</strong></p><p id="1123" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">scraper是用python构建的，使用一些流行的Python包来进行web抓取。以下是我们案例中使用的一些包。</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="29e7" class="ju jv hp jq b fi jw jx l jy jz">import sys<br/>import requests as rq<br/>from bs4 import BeautifulSoup as bs<br/>from time import sleep<br/>from time import time<br/>from random import randint<br/>from warnings import warn<br/>import json<br/>import pandas as pd</span></pre><p id="d9ec" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du ka kb kc jq b">BeautifulSoup</code>是一个流行的用于html和xml解析的python包。我们使用它遍历不同的html标签，并提取所有必要的数据，在本例中，是新闻标题、摘要和每篇文章的url链接。</p><p id="4878" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">整个刮削过程如下:</p><ol class=""><li id="fe47" class="kd ke hp io b ip iq it iu ix kf jb kg jf kh jj ki kj kk kl bi translated">使用<code class="du ka kb kc jq b">Wayback Server CDX API</code>编译一个URL列表。</li></ol><p id="05fb" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du ka kb kc jq b">Wayback Server CDX API</code>作为Wayback机器网站捕获的http入口，允许对数据进行深入查询。下面是一个示例查询，它为每个MSNBC新闻网站“capture”返回一些索引数据</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="ed11" class="ju jv hp jq b fi jw jx l jy jz">'<a class="ae jk" href="http://web.archive.org/cdx/search/cdx?url=nbcnews.com/politics&amp;collapse=digest&amp;from=20190401&amp;to=20190431&amp;output=json'" rel="noopener ugc nofollow" target="_blank">http://web.archive.org/cdx/search/cdx?url=nbcnews.com/politics&amp;collapse=digest&amp;from=20190401&amp;to=20190431&amp;output=json'</a></span></pre><p id="d537" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在上面的查询中，<code class="du ka kb kc jq b">url=</code>是必需的参数，这里我们将其设置为<code class="du ka kb kc jq b">GET</code> MSNBC政治部分屏幕截图。对于nbcnews.com/politics部分的每个“捕获”,上述查询将为每个索引返回一行。<code class="du ka kb kc jq b">Wayback CDX Server</code>响应上述GET查询，并将结果作为JSON数组输出。输出的列如下所示。</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="f7a9" class="ju jv hp jq b fi jw jx l jy jz">[['urlkey',<br/>  'timestamp',<br/>  'original',<br/>  'mimetype',<br/>  'statuscode',<br/>  'digest',<br/>  'length'],<br/> ['com,nbcnews)/politics',<br/>  '20190401012911',<br/>  'https://www.nbcnews.com/politics',<br/>  'text/html',<br/>  '200',<br/>  'FVZYAKIUIFOQY5NCP7AI4LJB4JNLYQOF',<br/>  '38471'],</span></pre><p id="d903" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一旦我们得到如上所示的CDX列的部分，我们将使用“时间戳”和“原始”列来组合最终的Wayback机器url，然后我们使用它来打开特定的html页面并抓取所需的数据点。</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="fea8" class="ju jv hp jq b fi jw jx l jy jz"># MSNBC Wayback machine archive urls<br/>url = '<a class="ae jk" href="http://web.archive.org/cdx/search/cdx?url=nbcnews.com/politics&amp;collapse=digest&amp;from=20190401&amp;to=20190431&amp;output=json'" rel="noopener ugc nofollow" target="_blank">http://web.archive.org/cdx/search/cdx?url=nbcnews.com/politics&amp;collapse=digest&amp;from=20190401&amp;to=20190431&amp;output=json'</a></span><span id="9c45" class="ju jv hp jq b fi km jx l jy jz">urls = rq.get(url).text<br/>parse_url = json.loads(urls) #parses the JSON from urls.</span><span id="3f93" class="ju jv hp jq b fi km jx l jy jz">## Extracts timestamp and original columns from urls and compiles a url list.</span><span id="01f5" class="ju jv hp jq b fi km jx l jy jz">url_list = []<br/>for i in range(1,len(parse_url)):<br/>    orig_url = parse_url[i][2]<br/>    tstamp = parse_url[i][1]<br/>    waylink = tstamp+'/'+orig_url<br/>    url_list.append(waylink)</span><span id="b1de" class="ju jv hp jq b fi km jx l jy jz">## Compiles final url pattern.</span><span id="a629" class="ju jv hp jq b fi km jx l jy jz">for url in url_list:<br/>    final_url = '<a class="ae jk" href="https://web.archive.org/web/'+url_list[0" rel="noopener ugc nofollow" target="_blank">https://web.archive.org/web/'+url</a></span></pre><p id="7841" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.使用<code class="du ka kb kc jq b">BeautifulSoup</code>解析html页面。</p><p id="b522" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一旦我们编译了最终URL的列表，我们就使用<code class="du ka kb kc jq b">html.parser</code>来解析来自<code class="du ka kb kc jq b">final_url</code>的每个html页面。</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="6215" class="ju jv hp jq b fi jw jx l jy jz"># Open page<br/>req = rq.get(final_url).text</span><span id="d167" class="ju jv hp jq b fi km jx l jy jz"># parse html using beautifulsoup and store in soup<br/>soup = bs(req,'html.parser')<br/>soup</span></pre><p id="e591" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du ka kb kc jq b">soup</code>存储<code class="du ka kb kc jq b">final_url</code>的html输出，我们可以解析<code class="du ka kb kc jq b">soup</code>的html标签来收集所需的数据。为了知道要搜索哪些标签，需要检查Wayback机器捕获的html页面。有许多中型文章展示了如何检查html页面，挑选出特定的标签进行抓取。</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="e035" class="ju jv hp jq b fi jw jx l jy jz"># Get list of article tags that contain news titles<br/>articles= soup.find_all('article')</span><span id="a3ee" class="ju jv hp jq b fi km jx l jy jz">for article in articles:<br/>        try:<br/>            <br/>            if article != None:<br/>                #title and link<br/>                if article.find_all('h2') != None:<br/>                    #get news title<br/>                    title = article.find_all('h2')[1].a.text <br/>                    #get individual news article link<br/>                    link = article.find_all('h2')[1].a['href'] <br/>                else:<br/>                    title = 'N/A'<br/>                    link = 'N/A'</span></pre><p id="d1a1" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">标签给出了所有新闻文章的列表，它们的标题，以及每篇文章的链接。然后，我们使用单个文章<code class="du ka kb kc jq b">link</code>来收集文章摘要。</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="2d36" class="ju jv hp jq b fi jw jx l jy jz">req = rq.get(link).text<br/>soup=bs(req,'html.parser') # Parse each individual news article<br/>article = soup.find('div',attrs={'class':'article container___2EGEI'})<br/>article.div.text # news summary</span></pre><p id="0b83" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.将数据导出为csv格式。</p><p id="bee8" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一旦我们收集了所有的数据点，我们就使用<code class="du ka kb kc jq b">pandas.to_csv</code>函数将数据导出为csv格式。</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="fc23" class="ju jv hp jq b fi jw jx l jy jz">import pandas as pd</span><span id="fa1c" class="ju jv hp jq b fi km jx l jy jz">nbc_df = pd.DataFrame({'title':news_title<br/>                       ,'summary':news_summary<br/>                       ,'source':news_source<br/>                       ,'article_link':news_link})<br/>nbc_df.to_csv('nbc_articles.csv',index=False)</span></pre><p id="5bf3" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里我们只抓取了新闻标题、摘要、来源和文章链接。通过更多的html检查和解析，我们可以做同样的事情来抓取图像、图像标题和文章作者。</p><p id="e970" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下面是将上述各个部分组合在一起的完整代码。</p><figure class="jl jm jn jo fd hk"><div class="bz dy l di"><div class="kn ko l"/></div></figure><p id="0cc6" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">刮的开心！</p></div></div>    
</body>
</html>