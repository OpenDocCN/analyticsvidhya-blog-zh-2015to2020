# 文本预处理—自然语言处理基础

> 原文：<https://medium.com/analytics-vidhya/text-preprocessing-nlp-basics-430d54016048?source=collection_archive---------3----------------------->

![](img/c6da5d40583ab5e5e8a0bf0a690523ec.png)

文本预处理是自然语言处理流水线中的第一步，对最终处理有潜在的影响。文本预处理是将文本转换成特定任务的可预测和可分析形式的过程。任务是方法和领域的结合。例如，用 TF-IDF (approach)从 Tweets (domain)中提取热门关键词就是一个任务的例子。文本预处理的主要目标是将文本分解成机器学习算法可以消化的形式。在本报告中，我们将对大量的毒性评论进行文本预处理，并根据不同类型的毒性对评论进行分类。

我们将使用以下链接中给出的数据集:

[https://www . ka ggle . com/c/jigsaw-toxic-comment-class ification-challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge)

# 文本预处理技术

有不同的方法来预处理你的文本。下面列出了一些帮助预处理输入文本的技术。

## 噪声消除

噪声去除是指去除干扰文本分析过程的数字、字符和文本片段。它是文本预处理中最重要的步骤之一。它高度依赖于域。例如，在 tweets 数据中，噪音可以是除了标签之外的所有特殊字符，因为它表示可以表征 tweet 的概念。噪音的问题是，如果有噪音，即如果将未清理的数据输入到机器学习模型中，它会产生不一致的结果。

有各种方法可以消除噪音。这包括删除标点符号*、*、删除特殊字符*、*删除数字、删除 HTML 格式、删除特定领域关键字*、*(例如，retweet 的“RT”)、删除源代码、删除标题等等。这完全取决于你在哪个领域工作，以及什么被归类为你的任务的噪声。

## 标记化

记号化被定义为将文本分割成更小的单元(即记号)的过程，可能同时丢弃某些字符，例如标点符号。记号可以是单词、数字、符号、n-gram 或字符。N-grams 是 n 个单词或字符的组合。标记化通过定位单词边界来完成这项任务。

输入:朋友，罗马同胞，借我你的耳朵
输出:[' **朋友'，'，'，'罗马人'，'，'，'同胞'，'，'，'借我'，'你的'，'耳朵']**

最广泛使用的标记化过程是空白标记化。在这个过程中，通过基于两个单词之间的空白分割标记，将整个文本分割成标记。

我们对数据执行的第一个任务是将注释分成更小的单元，称为标记，可以是单词、数字或符号。将文本分割成标记后，我们计算每种标记的数量。

从 nltk.tokenize 中，我们可以导入 word_tokenize 来执行标记化任务。

```
from nltk.tokenize import word_tokenize
sentence = "Hello, I am Nupur"
tokens = word_tokenize(sentence)
print(tokens)Output = ['Hello', ',', 'I', 'am', 'Nupur']
```

使用 wordcloud 包的数据集中每个令牌出现的频率:

![](img/f08bc271df54d2c932107a3e0944ef1c.png)

**标记化的限制**

标记化的挑战取决于语言的类型。像英语和法语这样的语言被称为空格分隔的，因为大多数单词都是用空格分隔的。像汉语和泰语这样的语言被认为是不分段的，因为单词没有明确的边界。标记未分割的语言需要额外的词汇和形态信息。记号化也受到书写系统的影响。语言的结构可以分为三类:

**孤立:**词不分成更小的单位。例如:普通话

**粘合:**词分成更小的单位。例如:日语、泰米尔语

**屈折**:词素之间的界限在语法意义上不明确、不明确。例如:拉丁语

## 用小写字体书写

这是最简单的文本预处理技术，它包括输入文本的每个标记的小写。它有助于处理数据集中的稀疏问题。例如，一个文本中出现了混合大小写的单词“canada”，也就是说，在某些地方出现了单词“Canada ”,而在其他地方出现了单词“Canada”。为了消除这种变化，使它不会引起进一步的问题，我们使用小写技术来消除稀疏问题，并减少词汇的大小。

尽管它在减少稀疏性问题和词汇大小方面表现出色，但它有时会因增加模糊性而影响系统的性能。比如‘苹果是智能手机最好的公司’。这里，当我们执行小写时，苹果被转换成苹果，这产生了歧义，因为模型不知道苹果是一家公司还是一种水果，并且它可能将苹果解释为水果的可能性更高。

在给定的数据集中，我们在标记化后执行小写任务，并将所有标记小写。

```
lowercase_words = []
for word in tokens:
   word = word.lower()
   lowercase_words.append(word)
```

## 正常化

规范化是将标记转换成其基本形式(语素)的过程。从单词中去除词尾变化以获得单词的基本形式。它有助于减少数据中唯一令牌和冗余的数量。它降低了数据维度，并从文本中删除了单词的变体。

有两种技术可以执行标准化。它们是词干化和词汇化。

*   **词干:**

词干提取是从单词中移除屈折形式的基本的基于规则的过程。令牌被转换成它的根形式。例如，单词“troubled”在执行词干分析后被转换为“trouble”。

词干提取有不同的算法，但最常见的算法是波特算法，这也是已知的对英语有效的算法。波特的算法包括顺序应用的 5 个单词缩减阶段。

由于词干提取遵循一种粗糙的启发式方法，即砍掉标记的末尾，希望正确地转换成它的根形式，所以它有时可能会生成无意义的术语。例如，它可能会将标记“increase”转换为“increase”，导致标记失去意义。

词干提取有两种错误——词干提取过度和词干提取不足。词干过度是指词干不同的两个单词被词干化为同一个词根的问题。这也称为假阳性。词干不足是指词干相同的两个单词没有连在一起。这也被称为假阴性。轻度词干化倾向于减少过度词干化错误，但是增加不足词干化错误，而重度词干化增加过度词干化错误，但是减少不足词干化错误。

NLTK 包有一个 PorterStemmer 类用于词干提取。

```
from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()
for word in lowercase_words:
    tokens = ps.stem(word)
    print(tokens)
```

*   **词汇化:**

词干化类似于词干化，不同之处在于词干化指的是通过使用词汇和词的形态分析来正确地做事情，旨在从词中删除词形变化，并返回该词的基本形式或词典形式，也称为词干。它对单词进行全面的词法分析，以准确识别每个单词的词条。它可以使用字典(如 Wordnet)进行映射，或者使用一些其他基于规则的方法。

```
from nltk.stem import WordNetLemmatizer
wml = WordNetLemmatizer()
lemma = []for word in lowercase_words:
    tokens = wml.lemmatize(word)
    lemma.append(tokens)
```

例如，如果为引理化给出的标记是“increase”，则它返回“increase”作为其引理，而词干处理返回“increase”。

![](img/98188230b4511ef404c0e3f8dd96829d.png)

尽管词汇化被证明比词干化更好，但这两种形式的规范化都不能提高英语信息检索的总体性能。在某些情况下，它被证明是有用的，而在其他情况下，它会妨碍性能。

## 停用词删除

停用词是语言中常用的词。例如“一个”，“一个”，“这个”，“是”，“什么”等。停用词被从文本中删除，这样我们就可以专注于更重要的词，并防止停用词被分析。如果我们搜索“什么是文本预处理”，我们希望更多地关注“文本预处理”，而不是“是什么”。

对于不同的应用，停用词可以有不同的含义。在某些应用中，将限定词中的所有停用词移至介词是合适的。但是在一些应用中，比如情感分析，去除诸如 not、good 等标记。会让算法偏离轨道。

从评论数据集中，我们将删除所有停用词，记住不要删除停用词，如 not 或 good，因为这些词对于我们语料库的毒性分析至关重要。

```
from nltk.corpus import stopwords
filter_words = []
Stopwords = set(stopwords.words('english'))for word in lemma:
    if word not in Stopwords:
         filter_words.appemd(word)
```

从数据集中删除停用词后:

![](img/e0e41433a07a7fbab0572e919eaaf5eb.png)

## 对象标准化

文本数据通常包含任何词汇词典中都不存在的单词和短语。如果您的应用程序没有从这些单词中受益，并且只是导致稀疏性问题，您可以考虑从数据集中删除这些单词。

一些例子是首字母缩略词、单词的标签和口语俚语。借助正则表达式和手动准备的数据字典，可以修复这种类型的噪声。

## 删除标点符号

下一步是删除标点符号，因为它们对模型没有价值。删除标点符号将有助于减少训练集的大小。

我们将删除注释中的标点符号，如逗号和句号，因为它在处理文本数据时不会添加任何额外的信息。

我们可以使用正则表达式，通过提供一组标点符号来删除评论中的所有标点符号，这样无论何时遇到任何列出的标点符号，都可以将它们从文本中删除。

![](img/e9e1f9bfb16d8f7e8b60a5a33d544ce1.png)

## 删除空白

在删除标点符号后，我们删除文本数据中的所有空白，因为它们是无用的，只会增加训练集的大小。我们将从评论中删除所有空格，只保留那些有助于语料库毒性分析的标记。

在下一篇博客中，我们将讨论更多关于 NLP 的内容。

敬请期待，快乐学习！