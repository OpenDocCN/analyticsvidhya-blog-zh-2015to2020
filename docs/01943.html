<html>
<head>
<title>Part-Of-Speech and Viterbi Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">词性与维特比算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/part-of-speech-and-viterbi-algorithm-11138ef0c63d?source=collection_archive---------2-----------------------#2019-11-23">https://medium.com/analytics-vidhya/part-of-speech-and-viterbi-algorithm-11138ef0c63d?source=collection_archive---------2-----------------------#2019-11-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="60ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">词性标注在自然语言处理中起着至关重要的作用。词性标注是命名实体识别(NER)、问答、信息抽取和词义消歧的基础[1]。已经开发了许多算法来促进计算有效的词性标注，例如Viterbi算法、Brill tagger和Baum-Welch算法[2]。在这篇文章中，我们将重点关注著名的维特比算法，其背后的理论，以及它在python中的一步一步的实现。</p><h1 id="07a1" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">什么是词类？</h1><p id="9de8" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">词性是指一个词在给定句子中的用途。在英语中，一个单词可以属于9个主要词类之一:冠词、名词、形容词、代词、动词、副词、连词、感叹词和介词。这些主要职位可以进一步分为子类。词性标注是指在给定的句子中，标注与最能描述单词用法的词性相对应的单词。Penn Treebank是一个标准的词性标注集，用于词性标注单词。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/e28ab3ff5f6cc857285f99fe618ba121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n29KzWmkyv3TSzyNOVr0Gg.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">来源:<a class="ae kw" href="https://www.researchgate.net/publication/320858849_Comparing_and_Aligning_Process_Representations/figures?lo=1&amp;utm_source=google&amp;utm_medium=organic" rel="noopener ugc nofollow" target="_blank">研究之门</a></figcaption></figure><h1 id="0cad" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">词性标注问题</h1><p id="579b" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">一个词的词性标签可以根据使用它的上下文而变化。例如，考虑下列句子中突出显示的单词</p><ul class=""><li id="9bf0" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated">在我的<em class="lg">背上</em></li><li id="392d" class="kx ky hi ih b ii lh im li iq lj iu lk iy ll jc lc ld le lf bi translated">赢得选民<em class="lg">回来</em></li></ul><p id="5e22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">单词<em class="lg"> back </em>在上述每个句子中有不同的用途，根据其用途，不同的标签分配如下</p><ul class=""><li id="b773" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated">在我的<em class="lg">背上</em> : <em class="lg">背上</em> /NN</li><li id="7f96" class="kx ky hi ih b ii lh im li iq lj iu lk iy ll jc lc ld le lf bi translated">赢得选民<em class="lg">回</em> : <em class="lg">回</em> /RB</li></ul><p id="ea4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在所有的可能性中，我们如何决定分配给哪个POS标签呢？给定一个句子，尝试每一种可能的组合并找到与句子语义最匹配的组合是不可行的。一种方法是利用单词出现的上下文。如上所述，POS标签取决于其使用的上下文。有些词类标签有一套规则，规定在句子中什么词类标签应该在它们之前或之后。例如，出现在限定词和名词之间的单词应该是形容词。隐马尔可夫模型是一种有效建模词性标注问题的方法</p><h1 id="8355" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">隐马尔可夫模型(HMM)</h1><p id="43c0" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">隐马尔可夫模型是一种概率序列模型，它基于先验计算序列的概率，并选择具有最大概率的最佳可能序列。这里，完成词性标注的句子被认为是一组单词序列和标签序列。HMM是马尔可夫链的扩展。</p><p id="4e43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">马尔可夫链通过假设当前状态的概率仅依赖于前一状态来对问题建模。例如，考虑每天有三种可能状态的天气预报问题，即:晴天和雨天。马尔可夫链模型表明，今天天气晴朗的概率取决于昨天是晴天还是雨天。它没有考虑到前天的天气。马尔可夫链由以下组件定义:</p><ul class=""><li id="129f" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated"><em class="lg"> N </em> =状态数。在上例中，<em class="lg"> N </em> =2(晴天，雨天)。</li><li id="bc3c" class="kx ky hi ih b ii lh im li iq lj iu lk iy ll jc lc ld le lf bi translated"><em class="lg"> p(a/b) </em> =假设前一状态为<em class="lg"> b </em>，状态<em class="lg"> a </em>发生的概率。这被称为<strong class="ih hj">跃迁概率</strong>。</li></ul><p id="77ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在HMM中，状态是不可观察的，就像词性标注问题一样。状态是隐藏的标签，只有单词是可见的。因此，HMM具有以下组件以及上述马尔可夫链模型的组件:</p><ul class=""><li id="8a86" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated"><em class="lg"> p(o/b) = </em>状态概率<em class="lg"> b </em>给出<em class="lg"> o </em>作为输出。这被称为<strong class="ih hj">发射概率</strong>。</li></ul><h1 id="84ce" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">将词性标注建模为HMM</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lm"><img src="../Images/e5c39ab7c7d6d33b1cda48e6257319a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fyhgTk0BW-Iv5SwwLlzBZw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">来源:<a class="ae kw" href="https://sites.google.com/a/iitgn.ac.in/nlp-autmn-2019/" rel="noopener ugc nofollow" target="_blank"> Mayank Singh NLP 2019 </a></figcaption></figure><p id="37a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">词性标注问题是通过将标签视为状态，将单词视为观察值来建模的。例如，在上图中，对于观察<em class="lg">后面的</em> <strong class="ih hj"> </strong>有4种可能的状态。因此，跃迁和发射概率也修改如下。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ln"><img src="../Images/0448353f1d499da2315e05263497b498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yrq-_w_U-7ko5XBdPHHmRw.png"/></div></div></figure><p id="d77a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设<em class="lg"> {w_1 w_2 w_3…w_n} </em>表示一个句子，<em class="lg"> {t_1 t_2 t_3…t_n} </em>表示序列标签，这样<em class="lg"> w_i和t_i </em>分别属于集合<em class="lg"> W </em>和<em class="lg"> T </em>对于所有的<em class="lg"> 1≤i≤n </em>，那么，</p><p id="29e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lg"> p(w_1 w_2 w_3…w_n，t_1 t_2 t_3…t_n) </em>是<em class="lg"> w_i </em>被赋予标签<em class="lg"> t_i </em>的概率对于所有<em class="lg"> 1≤i≤n. </em>这可以用帮助HMM来计算。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lo"><img src="../Images/8c5bb5dde4c95679d68628e856b45b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*txq89-IfNnyV3oboaQhkgA.png"/></div></div></figure><p id="bde1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的目标是找到序列<em class="lg"> {t1 t2 t3…tn} </em>，该序列最大化上述等式中定义的概率。一种直接的方法是强力法，即计算所有可能组合的概率。例如，考虑上图中的句子“<em class="lg">承诺支持账单</em>”。有2x1x4x2x2 = 32种可能的组合。计算32种组合的概率听起来可能，但随着句子长度的增加，计算量呈指数增长。这就是维特比算法的用处。</p><h1 id="bb02" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">维特比算法</h1><p id="af37" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">维特比算法背后的直觉是使用动态编程通过存储重复的计算来减少计算的数量。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lp"><img src="../Images/87933def405589481446e90ee22b2a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*khm7djBiMFCaF-B7fdTONw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">来源:<a class="ae kw" href="https://sites.google.com/a/iitgn.ac.in/nlp-autmn-2019/" rel="noopener ugc nofollow" target="_blank"> Mayank Singh NLP 2019 </a></figcaption></figure><h2 id="6193" class="lq je hi bd jf lr ls lt jj lu lv lw jn iq lx ly jr iu lz ma jv iy mb mc jz md bi translated">维特比算法的特殊性是什么？</h2><p id="f692" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">在蛮力方法中，为了找到标签序列<em class="lg"> {VBD，t0，JJ，DT，NN} </em>和<em class="lg"> {VBD，t0，RB，DT，NN} </em>的概率，我们计算两次较小路径<em class="lg">(VBD-&gt;t0)</em>的概率。在维特比算法中，我们存储为路径<em class="lg">(VBD-&gt;~ T9)完成的概率计算，以在序列概率的进一步计算中使用它。</em></p><p id="51e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将路径的概率和信息存储如下:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es me"><img src="../Images/9b4bd7a5028ecc2573bce7a25a380f5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZwBezFz0s9xfEW1YV_fWTQ.png"/></div></div></figure><p id="f833" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的每一步都对应于句子中的每个单词。以下图像将有助于理解维特比算法。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mf"><img src="../Images/418e5ced91178b3ea3c0431b57a5b89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HTFDamlGmUXl14BYYMGbxA.jpeg"/></div></div></figure><p id="4c17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上图我们可以观察到，随着句子的长度(令牌数)，算法的计算时间也在增加。每一步需要计算的参数如上所示。状态指示对应于单词(步骤)的标签。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mg"><img src="../Images/4dc604f2753d42e1e491b92d908410c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Si-zmcQHAVP4W72V3YdxBg.jpeg"/></div></div></figure><p id="6ffc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图说明了如何计算特定状态下每一步的增量值。值<strong class="ih hj"> j </strong>给了我们最佳的先前标签(状态),这使得当前状态最有可能。</p><p id="8e0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面给出的是Viterbi算法在python中的实现。</p><p id="1932" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们用于实现的<strong class="ih hj">数据集</strong>是<strong class="ih hj">棕色语料库</strong>【5】。<strong class="ih hj"> </strong>数据集的几个特征如下:</p><ul class=""><li id="2fe8" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated">由57340个词性标注句子、115343个标记和49817个类型组成。</li><li id="f1a0" class="kx ky hi ih b ii lh im li iq lj iu lk iy ll jc lc ld le lf bi translated">语料库分为15类。</li><li id="7cbf" class="kx ky hi ih b ii lh im li iq lj iu lk iy ll jc lc ld le lf bi translated">总共包含467个bigram标签。</li><li id="3f0d" class="kx ky hi ih b ii lh im li iq lj iu lk iy ll jc lc ld le lf bi translated">语料库包括9580个具有多于1个标签的歧义类型和40237个具有明确标签的类型。然而，与明确类型相比，模糊类型出现的频率更高。</li></ul><p id="6d23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">访问<a class="ae kw" href="http://clu.uni.no/icame/manuals/BROWN/INDEX.HTM" rel="noopener ugc nofollow" target="_blank">此处</a>获取更多关于布朗语料库的详细信息</p><p id="1752" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是通过nltk库从brown语料库中获取数据的几种方法</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mh"><img src="../Images/785a7b6581f8190229cd03848092f569.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*fGQUuAHojD3YcfD9t2z_Jw.png"/></div></figure><p id="5b9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面提供了与维特比算法相关的代码。代码是从零开始实现的，为了更好地理解这个概念，对代码进行了注释。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="bfd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里是上面代码的GitHub要点的<a class="ae kw" href="https://gist.github.com/girish1511/9e3b6bc2a4a2eca0b904aff7e7fe7685" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><h1 id="2f2d" class="jd je hi bd jf jg mr ji jj jk ms jm jn jo mt jq jr js mu ju jv jw mv jy jz ka bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="61aa" class="kx ky hi ih b ii kb im kc iq mw iu mx iy my jc lc ld le lf bi translated">[1]<a class="ae kw" href="https://www.oreilly.com/library/view/hands-on-natural-language/9781789139495/d522f254-5b56-4e3b-88f2-6fcf8f827816.xhtml" rel="noopener ugc nofollow" target="_blank">https://www . oreilly . com/library/view/hands-on-natural-language/9781789139495/d522f 254-5b 56-4e3b-88 F2-6 fcf 8 f 827816 . XHTML</a></li><li id="a2fc" class="kx ky hi ih b ii lh im li iq lj iu lk iy ll jc lc ld le lf bi translated">[2]https://en.wikipedia.org/wiki/Part-of-speech_tagging<a class="ae kw" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="noopener ugc nofollow" target="_blank"/></li><li id="62cf" class="kx ky hi ih b ii lh im li iq lj iu lk iy ll jc lc ld le lf bi translated">[3]<a class="ae kw" href="https://www.freecodecamp.org/news/a-deep-dive-into-part-of-speech-tagging-using-viterbi-algorithm-17c8de32e8bc/" rel="noopener ugc nofollow" target="_blank">https://www . freecodecamp . org/news/a-deep-dive-into-词性标注-使用-viterbi-algorithm-17c 8de 32 E8 BC/</a></li><li id="fda1" class="kx ky hi ih b ii lh im li iq lj iu lk iy ll jc lc ld le lf bi translated"><a class="ae kw" href="https://sites.google.com/a/iitgn.ac.in/nlp-autmn-2019/" rel="noopener ugc nofollow" target="_blank">https://sites.google.com/a/iitgn.ac.in/nlp-autmn-2019/</a></li><li id="a5de" class="kx ky hi ih b ii lh im li iq lj iu lk iy ll jc lc ld le lf bi translated">[5]弗朗西斯、w .纳尔逊和亨利·库塞拉。"棕色文集。"罗德岛普罗维登斯布朗大学语言学系(1964年)。</li></ul><h1 id="d59d" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">协作</strong></h1><p id="3e27" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">与<a class="ae kw" rel="noopener" href="/@chennuri.prateek"> Prateek Chennuri </a>合作完成</p></div></div>    
</body>
</html>