<html>
<head>
<title>Understanding Neural Networks. From neuron to RNN, CNN, and Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经网络。从神经元到RNN、CNN和深度学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90?source=collection_archive---------0-----------------------#2018-09-11">https://medium.com/analytics-vidhya/understanding-neural-networks-from-neuron-to-rnn-cnn-and-deep-learning-cd88e90e0a90?source=collection_archive---------0-----------------------#2018-09-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/eb9d9bf4edaff4ae6247cdce87e2fc24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TtbpAISiZjcS6ep8kW4P3g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><p id="29c3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">神经网络是目前最流行的机器学习算法之一。随着时间的推移，神经网络在准确性和速度方面优于其他算法，这一点已得到决定性的证明。有各种变体，如CNN(卷积神经网络)、RNN(递归神经网络)、自动编码器、深度学习等。对于数据科学家或机器学习从业者来说，神经网络正慢慢成为统计学家的线性回归。因此，有必要对什么是神经网络、它是如何构成的以及它的范围和局限性有一个基本的了解。这篇文章试图解释一个神经网络，从其最基本的构建模块神经元开始，然后深入研究其最流行的变体，如CNN，RNN等。</p><p id="117f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">什么是神经元？</strong></p><p id="ef04" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">顾名思义，神经网络的灵感来自人脑的神经结构，就像人脑一样，基本的构建模块被称为神经元。它的功能类似于人的神经元，即它接受一些输入并激发一个输出。在纯数学术语中，机器学习世界中的神经元是数学函数的占位符，它唯一的工作是通过对提供的输入应用该函数来提供输出。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es js"><img src="../Images/8dfe60f6c54c0850b880c96ea4e1b3c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XqXu-hBHocGoHh_65Rl8lQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><p id="77aa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">神经元中使用的功能通常被称为激活功能。迄今为止，已经尝试了5种主要的激活功能，step、sigmoid、tanh、ReLU和leaky ReLU。下面将详细描述其中的每一项。</p><p id="46ea" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">激活功能</strong></p><p id="1991" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">步进功能</strong></p><p id="6332" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">阶跃函数被定义为</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es jx"><img src="../Images/56eb7c60d849084c98c3127d26b40eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*0iOzeMS3s-3LTU9hYH9ryg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加到学分</figcaption></figure><p id="e1d2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">其中，如果x的值大于等于零，则输出为1，如果x的值小于零，则输出为0。可以看出，阶跃函数在零点是不可微的。目前，神经网络使用反向传播方法和梯度下降来计算不同层的权重。由于阶跃函数在零处是不可微的，因此它不能利用梯度下降方法取得进展，并且在更新权重的任务中失败。</p><p id="f2a2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了克服这个问题，引入了sigmoid函数来代替阶跃函数。</p><p id="9957" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">s形函数</strong></p><p id="ecaf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">sigmoid函数或逻辑函数在数学上定义为</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es jy"><img src="../Images/beed00e0619e143199c02ad0eb747b3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*MIeka59unAhS7MQk5e7FOg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><p id="cc03" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">当z或自变量趋于负无穷大时函数值趋于零，当z趋于无穷大时函数值趋于1。需要记住的是，该函数代表因变量行为的近似值，并且是一种假设。现在问题来了，为什么我们使用sigmoid函数作为近似函数之一。这有一些简单的原因。</p><blockquote class="jz ka kb"><p id="aabe" class="iu iv kc iw b ix iy iz ja jb jc jd je kd jg jh ji ke jk jl jm kf jo jp jq jr hb bi translated">1.它捕捉数据中的非线性。尽管是近似的形式，但是非线性的概念对于精确建模是必不可少的。</p><p id="3dbc" class="iu iv kc iw b ix iy iz ja jb jc jd je kd jg jh ji ke jk jl jm kf jo jp jq jr hb bi translated">2.sigmoid函数始终是可微分的，因此可以与梯度下降和反向传播方法一起用于计算不同层的权重</p><p id="945a" class="iu iv kc iw b ix iy iz ja jb jc jd je kd jg jh ji ke jk jl jm kf jo jp jq jr hb bi translated">3.因变量遵循sigmoid函数的假设本质上假设自变量为高斯分布，这是我们在许多随机发生的事件中看到的一般分布，这是一个很好的一般分布。</p></blockquote><p id="d003" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然而，sigmoid函数也有梯度消失的问题。从图中可以看出，sigmoid函数将其输入压缩到一个非常小的输出范围[0，1]，并且具有非常陡峭的梯度。因此，存在输入空间的大区域，其中即使大的变化也会在输出中产生非常小的变化。这被称为消失梯度问题。这个问题随着层数的增加而增加，从而使神经网络的学习停滞在某一水平。</p><p id="bfff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">双曲正切函数</strong></p><p id="8b4d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">双曲正切(<em class="kc"> z </em>)函数是sigmoid的重新缩放版本，其输出范围为[1，1]而非[0，1]。[1]</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kg"><img src="../Images/598a40a550380db789cf42a4f3bbed50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ouueb-J2gBRvA-M_1rsXZA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><p id="4dde" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在某些地方使用双曲正切函数而不是sigmoid函数的一般原因是因为数据以0为中心，所以导数较高。较高的梯度有助于提高学习速度。下面附上的是两个函数tanh和sigmoid的梯度图。[2]</p><p id="93e1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于双曲正切函数，对于[-1，1]之间的输入，我们有[0.42，1]之间的导数。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kh"><img src="../Images/86ebb74a77ec755c7b8b5c699d8c1dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eIziz9oNjPltM_4Nm3AZnA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><p id="2da3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">另一方面，对于sigmoid函数，对于[0，1]之间的输入，我们有[0.20，0.25]之间的导数</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/0480dac8ed942e5192dbabd99e53f047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3o1PQmzCE4IAJriWjT4bqw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><p id="5723" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上面的图片可以看出，双曲正切函数的导数范围比Sigmoid函数更大，因此学习速度更快。然而，消失梯度的问题仍然存在于双曲正切函数中。</p><p id="d645" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> ReLU功能</strong></p><p id="e3da" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">校正的线性单元是深度学习模型中最常用的激活函数。如果接收到任何负输入，函数返回0，但是对于任何正值x，它返回该值。所以，可以写成f(x)=max (0，x)。</p><p id="a3b7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">图形看起来是这样的[3]</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/61feb87faaae38518f47d08b8658e26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*njuH4XVXf-l9pR_RorUOrA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><p id="5ecb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">泄漏的ReLU是最著名的之一。它与正数的ReLU相同。但它不是所有负值都为0，而是具有恒定的斜率(小于1。).</p><blockquote class="jz ka kb"><p id="0f45" class="iu iv kc iw b ix iy iz ja jb jc jd je kd jg jh ji ke jk jl jm kf jo jp jq jr hb bi translated">该斜率是用户在构建模型时设置的参数，通常称为α。例如，如果用户设置α=0.3，则激活函数为。<code class="du kk kl km kn b">f(x) = max (0.3*x, x)</code>这具有理论上的优势，即通过在所有值上受到<code class="du kk kl km kn b">x</code>的影响，它可以更完整地利用包含在x中的信息</p></blockquote><p id="f649" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">还有其他的选择，但是从业者和研究人员普遍发现使用ReLU之外的任何东西都没有足够的好处。在一般实践中，ReLU也比sigmoid或tanh函数表现得更好。</p><p id="f4b7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">神经网络</strong></p><p id="3bef" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">到目前为止，我们已经讨论了神经元和激活函数，它们共同构成了任何神经网络的基本构件。现在，我们将深入探讨什么是神经网络及其不同类型。我强烈建议人们，如果他们对此有疑问，重新审视神经元和激活功能。</p><p id="8924" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在了解神经网络之前，必须了解神经网络中的层是什么。层只不过是神经元的集合，这些神经元接受输入并提供输出。这些神经元中的每一个的输入都通过分配给神经元的激活功能来处理。例如，这里有一个小的神经网络。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/bef61b992eb85260524bb2aa5fd5d126.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cscVFhnr2uz4puDArSOt6Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><blockquote class="jz ka kb"><p id="95fd" class="iu iv kc iw b ix iy iz ja jb jc jd je kd jg jh ji ke jk jl jm kf jo jp jq jr hb bi translated">网络最左边的层称为<strong class="iw hj">输入层</strong>，最右边的层称为<strong class="iw hj">输出层</strong>(在这个例子中，它只有一个节点)。节点的中间层被称为<strong class="iw hj">隐藏层</strong>，因为在训练集中没有观察到它的值。我们还说我们的示例神经网络有3个<strong class="iw hj">输入单元</strong>(不包括偏差单元)、3个<strong class="iw hj">隐藏单元</strong>和1个<strong class="iw hj">输出单元</strong>【4】</p></blockquote><p id="2378" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">任何神经网络都有一个输入层和一个输出层。例如，隐藏层的数量在不同的网络之间根据要解决的问题的复杂性而不同。</p><p id="9114" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里要注意的另一个要点是，每个隐藏层可以具有不同的激活函数，例如，隐藏层1可以使用sigmoid函数，隐藏层2可以使用ReLU，后面是隐藏层3中的Tanh，它们都在同一神经网络中。再次使用的激活函数的选择取决于所讨论的问题和所使用的数据类型。</p><p id="d858" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，为了让神经网络做出准确的预测，这些神经元中的每一个都在每一层学习一定的权重。他们学习权重的算法被称为反向传播，其细节超出了本文的范围。</p><p id="f8f3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">具有一个以上隐藏层的神经网络通常被称为深度神经网络</strong>。</p><p id="dcb3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">卷积神经网络</strong></p><p id="704f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">卷积神经网络(CNN)是在计算机视觉领域大量使用的神经网络的变体之一。它的名字来源于它所包含的隐藏层的类型。CNN的隐藏层通常由卷积层、汇集层、全连接层和标准化层组成。这里它仅仅意味着不使用上面定义的正常激活函数，而是使用卷积和汇集函数作为激活函数。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/714d9ab81d7af9ef5d26beca950beb85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dvDZzVJ88XWX-nLliWgvWw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><p id="b5d9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了详细理解它，我们需要理解什么是卷积和汇集。这两个概念都是从计算机视觉领域借来的，定义如下。</p><blockquote class="jz ka kb"><p id="4c3f" class="iu iv kc iw b ix iy iz ja jb jc jd je kd jg jh ji ke jk jl jm kf jo jp jq jr hb bi translated"><strong class="iw hj">卷积</strong>:卷积对两个信号(在1D)或两个图像(在2D)进行运算:你可以把其中一个看作“输入”信号(或图像)，另一个(称为内核)作为输入图像上的“滤波器”，产生一个输出图像(所以卷积把两个图像作为输入，产生第三个作为输出)。[5]</p><p id="9447" class="iu iv kc iw b ix iy iz ja jb jc jd je kd jg jh ji ke jk jl jm kf jo jp jq jr hb bi translated">通俗地说，它接收输入信号并对其应用滤波器，本质上是将输入信号与内核相乘以获得修改后的信号。数学上，两个函数f和g的卷积定义为</p></blockquote><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/c86d5fa96c5a23df9dc917cca3e15dfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*3qXQLNXZPZaJBBaj1KQw7w.png"/></div></figure><p id="1d32" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">也就是输入函数和核函数的点积。</p><p id="68af" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在图像处理的情况下，更容易将内核想象为在整个图像上滑动，从而在该过程中改变每个像素的值。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/5255ec29066959eef0ee0af954799302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TwMfGALfE0naUC8pLOK4Vg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><p id="43d4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">图片鸣谢:机器学习大师[6]</p><p id="14fd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">汇集</strong>:汇集是一个<strong class="iw hj">基于样本的离散化过程</strong>。目标是对输入表示(图像、隐藏层输出矩阵等)进行下采样。)，减少其维数，并允许对包含在被装仓的子区域中的特征进行假设。</p><blockquote class="jz ka kb"><p id="44aa" class="iu iv kc iw b ix iy iz ja jb jc jd je kd jg jh ji ke jk jl jm kf jo jp jq jr hb bi translated">有两种主要类型的池，通常称为最大和最小池。顾名思义，最大池基于从所选区域选取最大值，最小池基于从所选区域选取最小值。</p></blockquote><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/7260e233f939fb75c3170ae6011e6d57.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*tpqURdMORniAGj2LnbmwVA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片鸣谢:<a class="ae kt" href="https://computersciencewiki.org/index.php/Max-pooling_/_Pooling" rel="noopener ugc nofollow" target="_blank">https://computersciencewiki . org/index . PHP/Max-Pooling _/_ Pooling</a></figcaption></figure><p id="d186" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，可以看出，卷积神经网络或CNN基本上是一种深度神经网络，除了用于引入非线性的激活功能之外，它还包括具有卷积和汇集功能的隐藏层。</p><p id="69b6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">更详细的解释可以在</p><p id="240d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">http://colah.github.io/posts/2014-07-Conv-Nets-Modular/<a class="ae kt" href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/" rel="noopener ugc nofollow" target="_blank"/></p><p id="3c08" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">递归神经网络(RNN) </strong></p><p id="5d75" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">递归神经网络或简称为RNN，是在自然语言处理中大量使用的神经网络的一个非常重要的变体。在一般的神经网络中，假设两个连续的输入是相互独立的，则通过若干层来处理输入并产生输出。</p><p id="7bf2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然而，这一假设在许多现实生活场景中并不成立。例如，如果一个人想要预测一只股票在给定时间的价格，或者想要预测一个序列中的下一个单词，那么必须考虑对先前观察的依赖性。</p><blockquote class="jz ka kb"><p id="cde6" class="iu iv kc iw b ix iy iz ja jb jc jd je kd jg jh ji ke jk jl jm kf jo jp jq jr hb bi translated">rnn被称为<em class="hi">递归</em>，因为它们对序列的每个元素执行相同的任务，输出取决于之前的计算。考虑rnn的另一种方式是，它们有一个“存储器”，可以捕获到目前为止已经计算过的信息。理论上，rnn可以利用任意长序列中的信息，但实际上，它们仅限于回顾几个步骤。[7]</p></blockquote><p id="9409" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从建筑的角度来看，RNN是这样的。人们可以把它想象成一个多层神经网络，每一层代表某一时刻t的观测值。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ku"><img src="../Images/81d90a951a99be6a5ea66f12548653e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*buMl05BvEPJ5P7sB5ImTCg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:取自谷歌搜索，未提及出处。让我知道，如果这是你的形象，我可以添加信用</figcaption></figure><p id="be68" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">RNN在自然语言处理方面取得了巨大的成功，特别是他们的变体LSTM，能够比RNN追溯更久。如果你有兴趣了解LSTM，我当然会鼓励你去参观</p><p id="3d7b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae kt" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</strong></a></p><p id="d6ca" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这篇文章中，我试图从理论的角度介绍神经网络，从最基本的结构——神经元开始，一直到最流行的神经网络版本。这篇文章的目的是让读者了解一个神经网络是如何从零开始建立的，它被用于哪些领域，它最成功的变体是什么。</p><p id="3f1a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我知道还有许多其他流行的版本，我会在以后的文章中介绍。如果您想提前讨论某个主题，请随时提出建议。</p></div><div class="ab cl kv kw gp kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="hb hc hd he hf"><p id="5796" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">参考</strong></p><p id="079e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">1.http://ufldl.stanford.edu/wiki/index.php/Neural_Networks<a class="ae kt" href="http://ufldl.stanford.edu/wiki/index.php/Neural_Networks" rel="noopener ugc nofollow" target="_blank"/></p><p id="c8ff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.<a class="ae kt" href="https://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function</a></p><p id="f7da" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.<a class="ae kt" href="https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/dans Becker/rectified-linear-units-relu-in-deep-learning</a></p><p id="bb1a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">4.<a class="ae kt" href="http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/" rel="noopener ugc nofollow" target="_blank">http://ufldl . Stanford . edu/tutorial/supervised/MultiLayerNeuralNetworks/</a></p><p id="621a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">5.<a class="ae kt" href="https://www.cs.cornell.edu/courses/cs1114/2013sp/sections/S06_convolution.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . Cornell . edu/courses/cs 1114/2013 sp/sections/S06 _ convolution . pdf</a></p><p id="33fa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">6.<a class="ae kt" href="http://machinelearninguru.com/computer_vision/basics/convolution/image_convolution_1.html" rel="noopener ugc nofollow" target="_blank">http://machine learning uru . com/computer _ vision/basics/convolution/image _ convolution _ 1 . html</a></p><p id="6bf9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">7.<a class="ae kt" href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" rel="noopener ugc nofollow" target="_blank">http://www . wild ml . com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/</a></p></div></div>    
</body>
</html>