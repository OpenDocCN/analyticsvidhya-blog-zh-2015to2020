# 解决 Quora 重复问题挑战正确的方法

> 原文：<https://medium.com/analytics-vidhya/solving-quora-duplicate-questions-challenge-the-right-way-477cbdd44744?source=collection_archive---------13----------------------->

## **QUORA 重复问题分类问题**

**简介**:

对重复问题或语义相似性进行分类的问题是 NLP 领域中的关键研究领域之一，因为任何 NLP 问题通常都遵循精确的管道来获得最适合所讨论问题的最终模型。第一步是文本预处理，试图标准化输入数据，然后进行数据探索，以了解数据分布和结构，然后是最重要的部分，即特征提取和探索，嵌入是最有希望在最先进的论文和竞赛提交中使用的特征，最后是特征选择和模型训练和评估。为了确保在提议的解决方案中遵循相同的途径，并且由于在每一步中我必须选择的选项和组合的数量，我试图尽可能地保持代码模块化，以避免代码重复并简化试错阶段，同时选择具有最强参数支持它们的选项作为最终解决方案。代码在 25GB 磁盘和 12GB 内存的 Google Colab 实例上运行，为了绕过硬件限制并确保结果可靠，在对最终结果进行平均的同时，管道在一小部分数据上运行了几次，尽管由于可用硬件甚至无法在有限的数据大小上处理它们，一些选项被丢弃。

下图(图 1)总结了在笔记本电脑解决方案上实施的流程以及每个步骤可用的选项。

![](img/55b610d8a876bc97c0e2bc023045eea0.png)

图一

**建设 NLP 管道:**

以下是对基于笔记本电脑解决方案的渠道构建的每一步的探索:

**数据集探索:**

在数据探索阶段之后，要保留的主要事实是，数据是不平衡的，有利于非重复问题类(图 2)，这是我在选择模型、准确性度量和预处理方法时考虑的因素。在未来的迭代中，应该以单变量和双变量的方式对提取的特征进行进一步的探索。

![](img/f970c88f8bfa45625ea69acf54040d0c.png)

图二

**数据预处理和采样:**

为了处理数据，我选择了删除停用词，规范化错别字，并对文本进行词条化，以规范化整个数据集的文本问题，在未来的迭代中可以提取更多的错别字。

这一步应该谨慎使用，因为一些错别字可能具有预测能力，因此删除它们或标准化文本可能会使最终模型的准确性更差，这就是为什么我选择保留原始版本和纯文本版本，以在未来的迭代中测试标准化问题是否会提高或降低模型的准确性。

**特征提取:**

可以从输入数据中提取三种类型的要素:

NLP 特征:这些特征是基于单词或句子的比率来计算的，并且被分成两类:正常的 NLP 特征表示从两个句子或其组合中提取的比率，例如问题长度，以及基于模糊文本匹配方法提取的模糊比率，例如模糊和模糊部分比率。

图形特征:这些特征是从将问题表示为节点，将它们的邻居表示为边的图中提取的，然后为每个节点计算度量，例如页面排名，这种方法的问题是大多数问题只有一个邻居，因此图形度量在大多数行中为空或常数。

我选择在提取 NLP 和图形特征的同时处理原始数据，因为这些特征本质上是形态学的，没有考虑语义或含义问题，而语义或含义是数据预处理最终试图归一化的内容。另一方面，由于上面强调的相同原因，我在提取嵌入时使用了干净的版本，但是为了支持这个决定，我将测试两个版本，以检查它们对最终模型性能的影响。

嵌入:有许多方法可以创建嵌入和嵌入提取的特性。

Word2vec 和 Glove:这些嵌入是为每个单词计算的，因此为了创建整个问题句子的嵌入，我有两个选项，要么计算 word2vec 嵌入的总和，要么计算 word 2 vec 嵌入的 TFIDF-Count 加权总和，第一个选项在解决方案笔记本上实现，第二个选项实现了，但由于内存限制而被放弃，直到将来迭代。

Fasttext:fast text 的主要优势是它能够处理 vocab 外的嵌入，因此它可以用于未清理的数据，因为每个单词都可以嵌入，但是由于 fast text 嵌入矩阵的大小和内存限制，这个选项已经实现，但在进一步迭代之前被放弃。

BERT : Bert 是一个记录良好的 transformer 模型，在语言建模方面有很好的记录，输入的嵌入由模型的最后一个隐藏层表示，它可以用作特性集的输入。

对于这个架构，我选择了在 QUORA 重复问题数据集上训练的预训练 BERT 模型。

XLNET : XLNET 是最强的候选语言，因为根据 papers with code 网站[https://papers with code . com/sota/question-answering-on-QUORA-question-pairs，](https://paperswithcode.com/sota/question-answering-on-quora-question-pairs,)的说法，它在 QUORA 重复问题基准测试中击败了其他语言，但不幸的是，训练这样的模型需要 TPU 和更大的内存，因此为了测试 XLNET 对问题进行分类的能力，我选择了一个预训练的版本。

可以将原始嵌入向量或用诸如 SVD 之类的降维算法提取的压缩版本添加到特征集中，但是由于硬件限制，并且由于类似问题在距离方面应该尽可能接近的事实，因此最重要的是那些向量之间的距离而不是它们的实际值，我已经决定为每个嵌入选项计算几个距离度量，而不是实际嵌入值。

对于每个模型，都选择了合适的框架，考虑到它的简单性或对任务的适用性，其他框架(如作为服务嵌入)和其他嵌入方法(如 Google universal sentence encoder)可以在未来的迭代中使用。

为了估计对整个数据运行管道所需的内存和计算时间，我创建了一个初始化方法，该方法初始化和测试管道代码，并返回运行 NLP 管道所需的估计内存和计算时间，同时处理 0.001%的数据，并在采样期间保持重复和非重复问题的比例不变，以避免选择偏差。

**功能选择:**

由于有大量的特征，下一步是提取最重要的特征，通过使用基于模型的提取方法和统计方法(如互信息得分)来获得具有最高方差的特征，以便进行补偿

样本大小，我已经决定运行这个方法几次，同时在每个步骤中积累特征的重要性。

一般来说，在处理大量特性时，减少特性的数量是一个经验法则，但是为了支持这个决定，我决定在两个版本上训练模型。

**训练模特:**

我决定在选定的特性上训练 XGBOOST 和 Light GBM 模型，因为它们在类似的问题上表现很好，尤其是在不平衡数据集的情况下。

在未来的迭代中，我将添加更多的模型并堆叠它们，同时微调每个模型的超参数。

其他模型，如暹罗神经网络，对此类问题显示出优越的结果[https://www.aclweb.org/anthology/R19-1116/](https://www.aclweb.org/anthology/R19-1116/)，但它们涉及另一种预处理方法，因为它们将每个问题的嵌入作为输入，并根据这些向量对其相似性进行分类。

**结果和解释:**

运行特征选择方法，清楚地显示根据所有度量，Bert 嵌入距离是最强的特征，这是很明显的，因为该模型是在 QUORA 重复问题数据集上训练的(图 3)。

![](img/c6adde3e4c31c8c4b349596833d7cd8a.png)

图三

尽管问题 ID 哈希是最具预测性的特征，但我已经决定放弃它们，因为我没有看到任何关于它们高性能的解释，我已经决定让最终的模型在一定程度上可以解释。

在精度和 F1 得分方面，根据所选特征训练两个模型为生产环境提供了可接受的结果(图 4)，由于数据集的不平衡性质，F1 得分是所选的比较精度指标，微调超参数和堆叠模型在理论上应能进一步提高精度。

![](img/ca52ea1a7eb812b765a9e143ffceb6ed.png)

图 4

使用 SMOTE 并在整个特征集上训练模型，降低了模型的性能，这证明所选择的方法是最佳的方法(图 5)。

![](img/1ccfb915eb2f9838d0a5206a72feecd4.png)

图五

**聚类:**

对问题进行聚类涉及另一种方法，每个问题应单独处理，聚类过程中涉及的特征本质上是以一个问题为中心的，不同问题之间的差异不再重要，在这种情况下，重要的是每个问题的嵌入，因此我决定创建一个包含堆叠问题及其 BERT 嵌入的数据框，因为这是分类问题最有效的嵌入模型。 因此，我可以有把握地假设该模型是该数据集的代表性语言模型，然后我决定应用 BIRCH 聚类算法来对数据进行聚类，BIRCH 是一种快速聚类算法，在有大量特征时表现良好，其他聚类算法(如 DBSCAN 或 OPTICS)可以在未来的迭代中使用，

为了探索聚类，我决定为每个聚类创建一个词云，以了解聚类的主题，这种方法对于找到最佳聚类数也很有用，增加聚类数，直到每个聚类的主题连贯和紧凑是我用来找到最佳聚类数的方法。

举例来说，下图(图 6)清楚地显示，群组 1 包含与健康和健康生活方式相关的问题。

![](img/f8590b758f67cdca9be42637c57ccb9c.png)

图 6

可以在未来的迭代中使用主题建模预训练模型。

**数据清洁和模块化:**

通过保持代码符合 Google 风格指南和 OOP 坚实原则(尤其是单一责任原则和开闭原则),个人对代码的干净性和模块化给予了关注，以确保代码模块化并对未来的修改开放。

Colab 笔记本:

[](https://colab.research.google.com/drive/1SGq_EoIAYslOorI9AmKIm-8b1P1OIZ8d?usp=sharing) [## 谷歌联合实验室

### 编辑描述

colab.research.google.com](https://colab.research.google.com/drive/1SGq_EoIAYslOorI9AmKIm-8b1P1OIZ8d?usp=sharing) 

Github:

[https://github . com/anasboussarhan/quora-duplicate-questions](https://github.com/anassboussarhan/quora-duplicate-questions)

参考资料:

https://github.com/Wrosinski/Kaggle-Quora

【https://github.com/aerdem4/kaggle-quora-dup 

[https://github.com/stys/kaggle-quora-question-pairs](https://github.com/stys/kaggle-quora-question-pairs)

[https://github.com/dysdsyd/kaggle-question-pairs-quora](https://github.com/dysdsyd/kaggle-question-pairs-quora)