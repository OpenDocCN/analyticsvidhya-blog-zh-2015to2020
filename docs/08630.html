<html>
<head>
<title>Understanding Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-logistic-regression-in-depth-intuition-99ad14724464?source=collection_archive---------2-----------------------#2020-08-06">https://medium.com/analytics-vidhya/understanding-logistic-regression-in-depth-intuition-99ad14724464?source=collection_archive---------2-----------------------#2020-08-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="f439" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">先决条件</h1><p id="66fc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在直接进入本文之前，请确保您了解<em class="kb">线性回归</em>算法背后的数学原理。如果你没有，没必要担心；跟随<a class="ae kc" rel="noopener" href="/@anujvyas/understanding-linear-regression-in-depth-intuition-6c9f3b1cbb51"> <strong class="jf hj">这条</strong> </a>通过！</p><h1 id="5d5b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="3954" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">本文试图成为您在理解逻辑回归算法的数学和统计原理时所需要的参考。<strong class="jf hj">逻辑回归</strong>是人们用来建立机器学习模型的首批<strong class="jf hj">分类算法</strong>之一，但只有少数人真正理解其背后的数学原理。</p><p id="eb6c" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">为了解释逻辑回归实际上是做什么的，我尽可能使这篇文章简单。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ki"><img src="../Images/9c4aeed05492bba7610bece5f511d770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N2vk1UgnYC1gq-97-o4f_w.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">介绍</figcaption></figure><h1 id="bcf0" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">目录</h1><ul class=""><li id="8622" class="ky kz hi jf b jg jh jk jl jo la js lb jw lc ka ld le lf lg bi translated">什么是逻辑回归？</li><li id="e4e7" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">为什么叫逻辑回归？</li><li id="1ac3" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">逻辑回归工作</li><li id="0e4c" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">什么是最大似然估计？</li><li id="452b" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">逻辑回归的假设</li></ul></div><div class="ab cl lm ln gp lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="hb hc hd he hf"><h1 id="bf35" class="if ig hi bd ih ii lt ik il im lu io ip iq lv is it iu lw iw ix iy lx ja jb jc bi translated">什么是逻辑回归？</h1><p id="3a0c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果你还记得，<strong class="jf hj"> <em class="kb">线性回归</em> </strong>用于确定/预测因变量的连续值，因此它是一个<strong class="jf hj"> <em class="kb">回归算法</em> </strong>。而<strong class="jf hj"><em class="kb"/></strong>的逻辑回归，则是<em class="kb"/>一般用作<strong class="jf hj"><em class="kb"/></strong><em class="kb">的分类算法，例如预测某事是真还是假(二元分类)</em>。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ly"><img src="../Images/3d60463a668100447858eea546e4e097.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GLENyHkoq-TrhDRzYSsK-Q.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated"><em class="lz">注:逻辑回归是一种特殊类型的广义线性模型(GLM) </em></figcaption></figure><h2 id="aa69" class="ma ig hi bd ih mb mc md il me mf mg ip jo mh mi it js mj mk ix jw ml mm jb mn bi translated">什么是分类算法？</h2><p id="3371" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">顾名思义，这些算法仅<strong class="jf hj">根据用于分类的输入预测离散值</strong><em class="kb">例如，将患者分类为是否患有糖尿病、将电子邮件分类为垃圾邮件或垃圾邮件、数字分类等是最常见的分类任务。</em></p><p id="018b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">同样在<em class="kb">线性回归</em>中，因变量是一组<em class="kb">连续值</em>，但在<strong class="jf hj"> <em class="kb">逻辑回归</em> </strong>中，因变量仅由一组<strong class="jf hj"><em class="kb"/></strong><em class="kb">特定值组成，如0/1、真/假、垃圾/火腿等。</em></p><h2 id="d389" class="ma ig hi bd ih mb mc md il me mf mg ip jo mh mi it js mj mk ix jw ml mm jb mn bi translated">逻辑回归的类型</h2><ul class=""><li id="ed69" class="ky kz hi jf b jg jh jk jl jo la js lb jw lc ka ld le lf lg bi translated">二项式逻辑回归<br/> <em class="kb">用于</em> <strong class="jf hj"> <em class="kb">二元分类</em> </strong> <em class="kb">:目标变量只能有</em> <strong class="jf hj"> <em class="kb">两个</em> </strong> <em class="kb">可能的结果，如0或1，分别代表肥胖/不肥胖、死亡/活着、垃圾邮件/非垃圾邮件等……</em></li><li id="fc3d" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">多项逻辑回归<br/> <em class="kb">用于</em> <strong class="jf hj"> <em class="kb">多类分类</em> </strong> <em class="kb">:目标变量可以有</em> <strong class="jf hj"> <em class="kb"> 3个或更多</em> </strong> <em class="kb">个可能的结果，如疾病A/疾病B/疾病C或数字分类等...</em></li></ul><p id="d784" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">在本文中，我们将重点讨论二项逻辑回归，也就是通常所说的逻辑回归。</p></div><div class="ab cl lm ln gp lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="hb hc hd he hf"><h1 id="06c1" class="if ig hi bd ih ii lt ik il im lu io ip iq lv is it iu lw iw ix iy lx ja jb jc bi translated">为什么叫逻辑回归？</h1><p id="5d08" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这是许多学生/初学者问的最常见的问题，这里是我的理解。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es mo"><img src="../Images/9cf352d101f50b810539f0068ac2f94f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fG8dRr2a1c6RivTM"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated"><a class="ae kc" href="https://unsplash.com/@brucemars?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布鲁斯·马尔斯</a>在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="b3d5" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">逻辑</strong>由<a class="ae kc" href="https://en.wikipedia.org/wiki/Logit" rel="noopener ugc nofollow" target="_blank"> <em class="kb">逻辑函数</em> </a>缩写而成，其工作方式与线性 <strong class="jf hj"> <em class="kb">回归</em> </strong>算法十分<em class="kb">相似，因此得名<strong class="jf hj">逻辑回归</strong>。</em></p><p id="aab3" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj"/></p><p id="6601" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><em class="kb">阈值(默认值= 0.5)的设置是逻辑回归的一个非常重要的方面，取决于分类问题本身。</em></p></div><div class="ab cl lm ln gp lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="hb hc hd he hf"><h1 id="8401" class="if ig hi bd ih ii lt ik il im lu io ip iq lv is it iu lw iw ix iy lx ja jb jc bi translated">逻辑回归工作</h1><p id="c9ee" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">考虑将患者分类为肥胖或不肥胖的例子，其中如果输出为1表示患者肥胖，0表示不肥胖。</p><p id="9492" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj"> <em class="kb">注:</em> </strong> <em class="kb">由于线性回归使用普通的最小二乘法(OLE)来选择其最佳拟合线，这在逻辑回归的情况下无法做到，要知道为什么；观看</em> <a class="ae kc" href="https://www.youtube.com/watch?v=BfKanl1aSG0&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=15&amp;t=0s" rel="noopener ugc nofollow" target="_blank"> <em class="kb">本视频</em> </a> <em class="kb">。相反，</em> <strong class="jf hj"> <em class="kb">逻辑回归</em> </strong> <em class="kb">利用</em> <strong class="jf hj"> <em class="kb">最大似然</em> </strong> <em class="kb">选择最佳拟合线。</em></p><p id="4ac5" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj">逻辑回归</strong>模型首先将概率转化为<a class="ae kc" href="https://www.youtube.com/watch?v=ARfXDSkQf1Y&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=10" rel="noopener ugc nofollow" target="_blank">对数(odds)或我们所说的概率对数</a>，如下图所示。</p><div class="kj kk kl km fd ab cb"><figure class="mp kn mq mr ms mt mu paragraph-image"><img src="../Images/2c2107d71dc0631132c2aabcb73f67c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*nbbaYT-Ky59gtz-qIigzhw.png"/></figure><figure class="mp kn mv mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/93a645757f41386e56a62872ff814be1.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*Xgq1pp32MUdYN5maM79Urg.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx mw di mx my translated">概率到对数的转换(odds) || <a class="ae kc" href="https://www.youtube.com/watch?v=BfKanl1aSG0&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=15&amp;t=0s" rel="noopener ugc nofollow" target="_blank">图片来源</a></figcaption></figure></div><p id="daa2" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj"> <em class="kb">注:</em> </strong> <em class="kb">左图像的y轴由范围从0到1的概率组成，默认阈值为0.5，右图像的</em> <strong class="jf hj"> <em class="kb"> y轴由范围从+无穷大到-无穷大的log(odds)值</em> </strong> <em class="kb">组成。使用</em> <strong class="jf hj"> <em class="kb"> logit函数完成从概率到对数(赔率)的转换。</em> </strong></p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es mz"><img src="../Images/74c714f8e9c08b4bbc216ca61fd703ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*AekqPg4dDn-sQAxRI4uxXg.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">Logit函数</figcaption></figure><p id="2671" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj"> <em class="kb">注:</em> </strong> <em class="kb">如果我们使用默认的阈值= 0.5，代入上式，最终会得到</em><strong class="jf hj"><em class="kb">log(odds)= 0</em></strong><em class="kb">这是log(odds)图的中心。同样，代入1将得到答案为</em><strong class="jf hj"><em class="kb">+无穷大</em> </strong> <em class="kb">，而代入0将得到答案为</em><strong class="jf hj"><em class="kb">-无穷大</em> </strong> <em class="kb">。</em></p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es na"><img src="../Images/d569a967da5ca89f29f68d628ebab454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w3o7LQtl_RIp2VXwEPkMbA.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">日志的解释(赔率)</figcaption></figure><p id="9460" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">为了找到每个候选人的log(odds)值，我们将它们投影到如下所示的线上。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es nb"><img src="../Images/b1c47918a577cb6710cce05a6e1d89e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*6LSvotKBzbayooSM5kdFbw.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">每个数据点的Log(odds)值|| <a class="ae kc" href="https://www.youtube.com/watch?v=BfKanl1aSG0&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=15&amp;t=0s" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure><p id="b21b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">一旦我们找到每个候选人的对数(赔率)值，我们将使用下面给出的公式将每个候选人的对数(赔率)转换回概率。<strong class="jf hj"> <em class="kb">(乙状结肠函数)</em> </strong></p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es nc"><img src="../Images/a19f95088e55d4437632bbf8d4034dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wv9gsCBUqe-J1l6O9YlbqQ.png"/></div></div></figure><div class="kj kk kl km fd ab cb"><figure class="mp kn nd mr ms mt mu paragraph-image"><img src="../Images/552bf4c74115b82cec5281a084e4a10a.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*oSV2q3-TzB3LNwCMWRs3Yg.png"/></figure><figure class="mp kn ne mr ms mt mu paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/89e6913ff611daf6e74ed9a6bfb371f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*qgp6FaEJXfMMl2FoqY00dQ.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx nf di ng my translated">判决阈值= 0.5的Sigmoid函数|| <a class="ae kc" href="https://www.youtube.com/watch?v=BfKanl1aSG0&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=15&amp;t=0s" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure></div><h2 id="f338" class="ma ig hi bd ih mb mc md il me mf mg ip jo mh mi it js mj mk ix jw ml mm jb mn bi translated">什么是<a class="ae kc" href="https://www.youtube.com/watch?v=BfKanl1aSG0&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=15&amp;t=0s" rel="noopener ugc nofollow" target="_blank">最大似然估计</a>？</h2><blockquote class="nh ni nj"><p id="6910" class="jd je kb jf b jg kd ji jj jk ke jm jn nk kf jq jr nl kg ju jv nm kh jy jz ka hb bi translated">最大似然估计(MLE)是一种似然最大化方法，而普通最小二乘法(OLS)是一种残差最小化方法。</p><p id="7bb3" class="jd je kb jf b jg kd ji jj jk ke jm jn nk kf jq jr nl kg ju jv nm kh jy jz ka hb bi translated">最大化似然函数确定了最有可能产生观察数据的参数。因此，最大似然法用于选择最佳拟合线。</p></blockquote><p id="c804" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">在这种情况下，可能性是所有数据点肥胖和不肥胖的概率的乘积。</p><p id="1a63" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">找到第一条线的可能性后，我们将对数线旋转一点，再次计算可能性。这是使用<strong class="jf hj">梯度下降</strong>算法完成的。</p><h2 id="2a63" class="ma ig hi bd ih mb mc md il me mf mg ip jo mh mi it js mj mk ix jw ml mm jb mn bi translated">Logistic回归中的梯度下降和成本函数</h2><p id="a85c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果你记得，线性回归中的成本函数看起来像…</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es nn"><img src="../Images/43618961103808b226829c6499e070e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*Kj-uBWNBOUFVp00w898r8w.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">线性回归:成本函数|| <a class="ae kc" href="https://www.internalpointers.com/post/cost-function-logistic-regression" rel="noopener ugc nofollow" target="_blank">图片来源</a></figcaption></figure><p id="c05b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">如果您试图使用线性回归的成本函数来生成逻辑回归问题中的J(θ)，您将最终得到一个<strong class="jf hj">非凸函数</strong>，它有<strong class="jf hj">多个局部最小值</strong>:一个形状怪异的图形，不容易找到最小全局点，如下图所示。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es no"><img src="../Images/9e2c49df367eea5690cf08cd6d304e16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*yKtIN-iRdNX5hBQ-jRbcuQ.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">非凸函数|| <a class="ae kc" href="https://www.internalpointers.com/post/cost-function-logistic-regression" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure><p id="b638" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">对于逻辑回归，成本函数定义如下。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es np"><img src="../Images/26b32839a9fc15ab4dae187fc0256418.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*DAHp7srgv5VMdZdbA5E1ZA.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">Logistic回归:成本函数|| <a class="ae kc" href="https://www.internalpointers.com/post/cost-function-logistic-regression" rel="noopener ugc nofollow" target="_blank">图片来源</a></figcaption></figure><p id="4690" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">我们使用这个成本函数来生成J(θ)，这将给出下面的图形。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es nq"><img src="../Images/cf0f48fb593a2336d4d58bc7b3591c05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AD0FlN1SiWshBjL71zp4jg.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">凸函数|| <a class="ae kc" href="https://www.internalpointers.com/post/cost-function-logistic-regression" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure><p id="7d3a" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">重复该过程，直到找到最佳拟合对数(比值)线。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ki"><img src="../Images/73d6456cc9e2c88d6b448ec1f8914461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*h8mYW76y2YPJojggMH_Zfw.gif"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">Sigmoid曲线(概率)和Log(赔率)线|| <a class="ae kc" href="https://www.youtube.com/watch?v=BfKanl1aSG0&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=15&amp;t=0s" rel="noopener ugc nofollow" target="_blank"> GIF来源</a></figcaption></figure><p id="e98f" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><strong class="jf hj"> <em class="kb">注:</em></strong><em class="kb">logistic回归模型使用classifier.coeff_方法显示的系数是针对log(odds)线而不是sigmoid曲线的。</em></p><p id="4686" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">这就是逻辑回归的工作原理！唷…</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es nr"><img src="../Images/b0144dc57510ccff4e42fdb436d5ff8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*ePlwfJ5MARRKJi2pvc9MJg.jpeg"/></div></figure></div><div class="ab cl lm ln gp lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="hb hc hd he hf"><h1 id="f65f" class="if ig hi bd ih ii lt ik il im lu io ip iq lv is it iu lw iw ix iy lx ja jb jc bi translated">逻辑回归的假设</h1><ul class=""><li id="564c" class="ky kz hi jf b jg jh jk jl jo la js lb jw lc ka ld le lf lg bi translated">因变量必须是<strong class="jf hj">分类的</strong>。</li><li id="1803" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">该模型应具有<strong class="jf hj">很少或没有多重共线性</strong>，即独立变量不应相互关联。</li><li id="fc70" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">独立变量<strong class="jf hj">与对数</strong>线性相关(几率)。</li><li id="146a" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">逻辑回归需要相当大的样本量。</li></ul></div><div class="ab cl lm ln gp lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="hb hc hd he hf"><h1 id="ca9b" class="if ig hi bd ih ii lt ik il im lu io ip iq lv is it iu lw iw ix iy lx ja jb jc bi translated">参考</h1><ul class=""><li id="caea" class="ky kz hi jf b jg jh jk jl jo la js lb jw lc ka ld le lf lg bi translated"><a class="ae kc" href="https://www.youtube.com/watch?v=yIYKR4sgzI8&amp;list=PLblh5JKOoLUICTaGLRoHQDuF_7q2GfuJF&amp;index=12" rel="noopener ugc nofollow" target="_blank">Josh Starmer的stats quest</a></li><li id="6b17" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated"><a class="ae kc" href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">机器学习掌握度</a></li><li id="b536" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated"><a class="ae kc" href="https://www.internalpointers.com/post/cost-function-logistic-regression" rel="noopener ugc nofollow" target="_blank">国际点</a>的成本函数</li><li id="563f" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">Animesh Agarwal的博客</li><li id="1da9" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated"><a class="ae kc" href="https://www.geeksforgeeks.org/understanding-logistic-regression/" rel="noopener ugc nofollow" target="_blank"> GeeksforGeeks </a></li></ul><h1 id="d869" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">帮我接通</h1><ul class=""><li id="b953" class="ky kz hi jf b jg jh jk jl jo la js lb jw lc ka ld le lf lg bi translated"><a class="ae kc" href="https://www.linkedin.com/in/anujkvyas" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="36b8" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated"><a class="ae kc" href="https://github.com/anujvyas" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="e59f" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated"><a class="ae kc" href="https://www.kaggle.com/anujvyas" rel="noopener ugc nofollow" target="_blank">卡格尔</a></li></ul><p id="cc2b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated">如果你从这个博客中学到了什么，一定要给它一个👏🏼<br/>会在其他博客上和你见面，直到那时和平✌🏼</p></div></div>    
</body>
</html>