<html>
<head>
<title>Optimizer &amp; Loss Functions In Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的优化器和损失函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimizer-loss-functions-in-neural-network-2520c244cc22?source=collection_archive---------5-----------------------#2020-10-03">https://medium.com/analytics-vidhya/optimizer-loss-functions-in-neural-network-2520c244cc22?source=collection_archive---------5-----------------------#2020-10-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d6d1408ebbea9ce019b9bf8b6c202be1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uSruGL2Nf3p8pipSFIjN7g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:互联网</figcaption></figure><p id="e83e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这篇博客中，我们将了解在训练神经网络时最常用的各种优化器和损失函数。这样做的先决条件是对梯度下降算法如何工作有基本的了解。因此，我强烈建议大家参考我以前的博客<a class="ae js" rel="noopener" href="/@gauravrajpal1994/introduction-to-neural-networks-1d111bb4649"> <strong class="iw hj"> <em class="jt">神经网络简介</em> </strong> </a>，以更好地了解梯度下降算法，该算法在反向传播期间用于更新我们的权重和最小化成本函数，我们的目标是达到全局最小值。</p><p id="640a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在学习了梯度下降算法的基础知识之后，我们将关注以下内容:</p><ol class=""><li id="1a10" class="ju jv hi iw b ix iy jb jc jf jw jj jx jn jy jr jz ka kb kc bi translated">梯度下降的变体。</li></ol><p id="ee9c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.梯度下降的挑战/问题。</p><p id="b7f6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.不同类型的优化器。</p><p id="6aaa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们回忆一下梯度下降方程，它是:<strong class="iw hj"> w = w - alpha * dE/dw </strong>其中<strong class="iw hj">，dE/dw </strong>表示误差相对于权重的变化率，而<strong class="iw hj"> alpha </strong>表示学习率。这里<strong class="iw hj">‘E’</strong>是成本函数。</p><p id="9acc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有多种方法来计算这个成本函数。基于我们计算这个成本函数的方式，有不同的梯度下降方差。让我们更好地理解它。</p><p id="0237" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi kd translated"><span class="l ke kf kg bm kh ki kj kk kl di">V</span>T16】渐变下降的变体。</p><p id="4372" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">批量梯度下降</strong>:假设在我们的数据集中总共有‘m’个观察值，我们使用所有这些观察值来计算成本函数‘E’。因此，我们采用整个训练集，执行前向传播并计算成本函数。然后我们用这个成本函数的变化率来更新我们称之为权重的参数。此外，我们必须记住，因为我们在这里使用的是整个训练集，每个时期参数只会更新一次。</p><p id="b740" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">随机梯度下降</strong>:如果我们使用单一观测值来计算成本函数，我们称之为随机梯度下降，通常简称为 SGD。这里每次迭代通过神经网络计算误差，然后更新参数。然后我们将进行第二次观察，并执行类似的步骤。这个过程一直持续到我们数据集中的观察数。因此，对于 1 个时期，我们将有“m”次更新，其中“m”代表我们数据集中的观察次数。</p><p id="cdf6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">小批量梯度下降:</strong>在这种类型的梯度下降中，我们从整个训练数据集中提取一个子集，并计算成本函数。这种类型的梯度下降主要用于训练深度学习模型。</p><p id="e6e8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jt">让我们看看成本函数的变化</em>:</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/85cd9870b44018cd85e312934923778e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*arl0S6cRXR6TlSueDg5PcA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="d4ab" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">当我们获取整个数据集时，成本函数平滑地降低。对于 SGD 来说，并不是那么顺利，因为我们是基于单次观察来更新参数的，所以涉及到很多迭代。模型也可能开始学习噪声。与 SGD 相比，在最小批量梯度下降的情况下，成本函数的更新更平滑，因为我们不是在每次迭代之后而是在数据的每个子集之后更新参数。</p><p id="1c2e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jt">让我们看看计算成本和时间:</em></p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/8b40c9a2ccd0251ec547da3825fe82e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F3VNpZsjIEgCm_VgurezOA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="78f5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们可以看到，小批量梯度下降比其他方法给出了更好的结果，也是在建立深度学习模型时最常用的方法。</p><p id="7ddc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这些是我们讨论过的梯度下降的变体，你们会经常遇到。希望现在你已经熟悉这些术语了？？</p><p id="53e1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们跳到我们在使用梯度下降时所面临的挑战。</p><p id="7462" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi kd translated"><span class="l ke kf kg bm kh ki kj kk kl di"> C </span> <strong class="iw hj">渐变下降的挑战</strong>。</p><p id="fa11" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这一部分，我们将重点关注梯度下降的挑战。让我们来看看吧。</p><ol class=""><li id="b293" class="ju jv hi iw b ix iy jb jc jf jw jj jx jn jy jr jz ka kb kc bi translated"><strong class="iw hj">它卡在局部最小值(SGD 动量)</strong></li></ol><p id="3817" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们说下面是我们的成本函数的图表。因为我们希望最小化成本，所以我们希望达到'<strong class="iw hj">全局最小值</strong> -在整个成本函数或全局中具有最小成本值的点被称为全局最小值。其邻居中的最低点称为<strong class="iw hj">局部最小值。</strong>这是梯度下降一般会卡住的点。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/da991c2864a59ea4c7cde4774c19e2df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aJyrtHPKD2dNmxKe--BE2w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="c592" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用梯度下降算法，我们将计算每一个点的斜率，并且在一些点达到局部最小值之后。局部最小值处的斜率为 0，因此 dE/dw 项变为 0，这表示梯度或斜率，因为这些参数将不会得到更新并将被卡住。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kt"><img src="../Images/96d8333d1668a71f6d8cd58b27e5eb71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sADj-NdraYbNZtnmZpwwjg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="da8e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的目标不是达到局部最小值，我们的目标是达到全局最小值。所以，我们需要一些局部最小值的推动，把我们从这个场景中带出来。</p><p id="38b5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们用一个例子来理解这一点:</p><p id="2b2c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">假设我们有一个初速度为 u 的球在某个高度。现在球以一定的速度滚下山坡，比如说 v，我们可以很明显地说 v &gt; u，因为球正在滚下。当球到达局部极小值时，它会获得一些速度，最终会给它一个推力，球会从局部极小值出来。所以，我们看到要把球推出局部最小值，我们需要一些累积速度之类的东西。就神经网络而言，我们可以说累积速度相当于加权梯度。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ku"><img src="../Images/8ffd24128bee220dd56379abd5081ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i3DsPE7DK1FvMUXfeFQSZQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="d2c5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里，如果我们看到局部最小值，斜率或梯度将为 0，这将最终使当前时间“t”处的当前梯度等于 0。我们仍将剩下某个值，即先前在‘t-1’累积的梯度，因此加权梯度‘Vt’将具有某个值，该值将给出所需的推力，并且它将从局部最小值中出来。“β”值告诉我们它应该给当前和先前累积的梯度多少值。通常，β值取为 0.9，这意味着它将为当前梯度赋予 10%的权重，为先前累积的梯度赋予 90%的权重。现在，我们的新梯度“Vt”将用于更新神经网络的参数。<strong class="iw hj"> <em class="jt">这也被称为随机梯度下降与动量。</em>T3】</strong></p><p id="f577" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，使用动量随机梯度下降的概念，我们可以解决陷入局部最小值的问题。</p><p id="90d6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 2。在整个培训过程中保持相同的学习速度。</strong></p><p id="c057" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是我们在训练神经网络时会遇到的第二个问题。一般来说，与其他变量相比，一些变量可能会导致更快的收敛，但如果我们在整个训练阶段应用相同的学习速率，我们会强制它们同步。这可能会导致收敛速度变慢。</p><p id="8ab5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的目标是，随着培训的进展，学习率也应根据成本函数进行更新。让我们看看如何解决这个问题。</p><p id="a574" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">回到我们的梯度下降方程，其中 dE/dw 是在我们的训练过程中更新的梯度，因此我们可以使用这一项来更新学习率。<strong class="iw hj">这里有一些问题，因为一些梯度在某个点可能是正的或负的，所以它们可能会相互抵消，因此，为了消除符号，我们可以取梯度的平方和，并使用该值来更新学习率。</strong></p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kv"><img src="../Images/c936fb175a65cb8b3ba1911fc05306e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmwtAA6PzJMTgfvxOnqUbQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="72d1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这种方法可能存在另一个问题，即<em class="jt">任何数的平方，无论是负数还是正数，都将始终是正数，因此，将所有正数相加将始终增加θ值，反过来，这将降低学习速率，在一些迭代之后，学习速率将趋于零，变得非常小，我们的参数将几乎类似于先前的参数，这将导致收敛变慢。</em></p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/d8d7baa689a2195136e4361c1bc5f07b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WlI-ROvf69_G9lAUeFE5Aw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="a4ac" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了克服这个问题，我们可以使用带有动量方程的 SGD，它为我们当前的梯度和先前累积的梯度分配一些量级，我们将取梯度的平方来抵消负值的影响。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kx"><img src="../Images/549f34778ad6b3531b465c2aea80d4a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wo0D-waf1phfBEMoNOwepA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="baa3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是 RMSProp 的更新方程式。从上面的等式中我们可以看出，我们使用的是α，它除以加权平均值的平方根，再加上一些小误差。分母中使用了误差，因此该值不会变成 0。通常，该误差值非常小，即 0.00000001。</p><p id="e359" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们理解这个等式如何帮助我们在训练过程中更新学习率- <em class="jt">当梯度的平方很高时，加权平均值将很高，这反过来降低了学习率。类似地，当梯度的平方较低时，加权平均值将较低，这反过来增加了学习速率。</em></p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/12260cd8c98cdb3817490f855fd91693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*bkY6_QXHuqGXCFpEiONpKw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="b3bd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi kd translated"><span class="l ke kf kg bm kh ki kj kk kl di"> D </span> <strong class="iw hj">不同类型的优化者。</strong></p><ol class=""><li id="57bf" class="ju jv hi iw b ix iy jb jc jf jw jj jx jn jy jr jz ka kb kc bi translated">带动量的随机梯度下降。</li><li id="1545" class="ju jv hi iw b ix kz jb la jf lb jj lc jn ld jr jz ka kb kc bi translated">RMSProp。</li><li id="5923" class="ju jv hi iw b ix kz jb la jf lb jj lc jn ld jr jz ka kb kc bi translated">亚当。</li></ol><p id="3b7a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">到目前为止，我们已经介绍了上一节中讨论的两个优化器，以克服使用梯度下降时面临的挑战。在<strong class="iw hj">带动量的 SGD</strong>中，我们看到它使用先前累积梯度的加权和解决了陷入局部最小值的问题。在<strong class="iw hj"> RMSProp </strong>中，我们看到它使用平方梯度的和解决了所有参数的相同学习速率的问题。</p><p id="c8a6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，我们将看看最常用和最广泛使用的优化器，即<strong class="iw hj"> ADAM。</strong>它结合了 SGD 和 momentum 来解决局部极小问题，以及 RMSProp，后者使用先前梯度的平方和来解决相同的学习率问题。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/054e1398239c292197bf1b82c3d84510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gqOtPQVk1vYiWKPCo3vKVg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="9a1d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从 ADAM 的更新方程中，我们可以看到，我们在这里使用“Vt”作为梯度，它是当前和先前累积梯度的加权和，以解决局部最小值问题，梯度平方和的平方根也解决了相同的学习速率问题。</p><h2 id="137d" class="lf lg hi bd lh li lj lk ll lm ln lo lp jf lq lr ls jj lt lu lv jn lw lx ly lz bi translated">所以，这就是在构建我们的神经网络中使用的各种类型的优化器背后的详细直觉。我希望你喜欢学习它。</h2><h1 id="d30a" class="ma lg hi bd lh mb mc md ll me mf mg lp mh mi mj ls mk ml mm lv mn mo mp ly mq bi translated">损失函数:</h1><p id="f9b0" class="pw-post-body-paragraph iu iv hi iw b ix mr iz ja jb ms jd je jf mt jh ji jj mu jl jm jn mv jp jq jr hb bi translated">在本节中，我们将了解什么是损失函数，以及它们中的哪些在深度学习中最常用。</p><p id="cdf2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在看损失函数之前，让我们回顾一下神经网络。如果我们看到下图，我们将了解我们的预测是如何计算的，误差函数帮助我们更新神经网络中的权重和偏差，这有助于我们改进模型的预测和性能。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/34a6046615aff7dc24c9e924e11f42a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qUz_Iqy2c02qth0MTkH3dw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><ol class=""><li id="270e" class="ju jv hi iw b ix iy jb jc jf jw jj jx jn jy jr jz ka kb kc bi translated"><strong class="iw hj">均方误差:</strong>到目前为止，我们一直使用 MSE(均方误差)来计算误差/损失函数/成本函数。让我们看看如何计算 MSE。</li></ol><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/1c98f4c2874eb2b4fb157d2631178ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CXl-l9aU0qps30W1hrHJOA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="75ed" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jt"> MSE 一般用在我们有回归类型的问题，目标变量是连续的时候。</em></p><p id="b812" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 2。平均绝对误差:</strong> MAE 是用于计算损失函数的另一个度量。让我们看看如何计算 MAE。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/87c443a3352d388758d0d6cb6eee6527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZNAyobakZk_Kjh7EQr_NXA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="8152" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jt">当我们有回归类型的问题并且目标变量是连续的时，也使用 MAE。</em></p><p id="dc09" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 3。均方根误差:</strong> RMSE 就是 MSE 的平方根。我们知道如何计算 MSE，所以在 RMSE，我们将只计算所得值的平方根。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/bc31d152568b0cc68cd9549fd351a540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*Ds2E4l9xQbzDMwEWlbS1Eg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="9a5d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，到目前为止，我们看到了回归任务的一些损失函数。现在让我们看看用于分类任务的损失函数。</p><p id="f860" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">分类任务可以进一步分为<strong class="iw hj">二元分类</strong>和<strong class="iw hj">多类分类。</strong>在二元分类中，我们的目标变量中只有 2 个目标类，而在多类分类中，我们可以预测目标变量中的 2 个以上的类。让我们理解两者中使用的损失函数:</p><p id="9a6b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 1。二元交叉熵/对数损失。</strong></p><p id="65ab" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jt">“校正预测概率对数的负平均值”</em></p><p id="1d29" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是分类问题中最常见的损失函数类型。它将每个预测的概率与实际的类输出进行比较，后者可以是 0 或 1。然后，它计算分数，根据与实际值的距离来惩罚概率。</p><p id="c5ca" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们用一个例子来理解什么是修正概率。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es na"><img src="../Images/cb2f4ec4b8641e048cb484c0d1e5b87b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*ky6EsvK4qtdmPe0KPxbvgQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="1bea" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里，我们看到每个观察值的校正概率栏。我们已经预测了概率栏，它包含了类别 1 的概率。对于实际值为 0 的情况，预测概率表示类 1 的概率，因此，它属于类 0 的概率将通过从 1 中减去该值来获得。在我们的示例中，ID8、ID2 和 ID5 都会发生这种情况。</p><p id="bfe0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，我们采用属于目标类别 1 的修正概率，对于实际目标为 0 的概率，我们从 1 中减去预测概率。然后，我们对这些校正后的概率取对数，因为取对数将对小的差异给出较少的惩罚(例如，如果实际值为 1，预测概率为 0.9，我们分配较少的惩罚，如果实际值为 1，预测值为 0.6，我们分配稍高的惩罚)。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/12125d7af89606bacf323f67d2474d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghpeOlnkm210O4UmfJGhlw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="c097" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们可以使用下面的公式，而不是计算修正后的概率。假设实际类是 1，(1-y)项将是 0。类似地，如果实际类别为 0，y*log(p)项变为 0</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/5c8042dc652a7c5cbb843ad81f2510bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSrIlvXeHWa58_T4ZASu9w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya 分析</figcaption></figure><p id="3860" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这就是我们计算二元交叉熵的方法，这是一个非常有用的损失函数，用于分类问题。</p><p id="3fd5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi kd translated"><span class="l ke kf kg bm kh ki kj kk kl di"> C </span> <strong class="iw hj">结论</strong></p><p id="bac8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，我们学习了各种优化器，如解决局部最小值问题的随机梯度下降，解决相同学习速率问题的 RMSProp，然后我们学习了 ADAM，它是两者的结合。然后，我们学习各种损失函数，可用于回归和分类问题。</p><p id="42e2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请务必在 LinkedIn 上联系我:<a class="ae js" href="https://www.linkedin.com/in/gaurav-rajpal/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/gaurav-rajpal/</a></p><p id="ce91" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="jt">敬请关注我们在深度学习中使用图像数据集的演示项目的进一步更新。</em> </strong></p><p id="5324" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">问候，</p><p id="f1cd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">高拉夫·拉杰帕尔·(gauravrajpal1994@gmail.com)</p></div></div>    
</body>
</html>