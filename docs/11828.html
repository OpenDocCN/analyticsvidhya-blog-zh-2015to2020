<html>
<head>
<title>The Art of Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维的艺术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-art-of-dimensionality-reduction-80fded9726c5?source=collection_archive---------12-----------------------#2020-12-20">https://medium.com/analytics-vidhya/the-art-of-dimensionality-reduction-80fded9726c5?source=collection_archive---------12-----------------------#2020-12-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="e7a2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">灾难恢复是预测建模问题中最关键的步骤之一。世界正在产生大量的大规模数据。因此，优化数据的维度空间至关重要。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/d8b6549fe1cbaf91ccc2b233065a6a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bvw5-dQ-XHIgDk3n9ixkMw.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">粉色是一种浅红色，通常与爱情和浪漫联系在一起。人们将<strong class="bd jn">色</strong>与通常被认为是女性的品质联系在一起，比如温柔、善良、亲切和同情(照片由<a class="ae jo" href="https://unsplash.com/@isiparente?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Isi Parente </a>在<a class="ae jo" href="https://unsplash.com/s/photos/projection?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄)</figcaption></figure><h1 id="5ef4" class="jp jq hi bd jn jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">什么是降维？</h1><p id="ce4f" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">假设您想要解决一个预测性建模问题，出于同样的原因，您开始收集数据。你永远不知道你想要什么样的特性，需要多少数据。因此，你去寻找上限，并收集所有可能的特征和观察。</p><p id="afed" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">因此，你意识到你已经收集了大量的数据。而且，这些额外的功能加剧了噪音和时间。</p><ol class=""><li id="a188" class="lh li hi ki b kj lc km ld kp lj kt lk kx ll lb lm ln lo lp bi translated"><strong class="ki hj">噪声</strong>:可能有一些特征，模型发现不相关。因此，他们只是给模型增加了噪声。</li><li id="bfb2" class="lh li hi ki b kj lq km lr kp ls kt lt kx lu lb lm ln lo lp bi translated"><strong class="ki hj">时间</strong>:我说的时间是计算时间。对于每个额外的特征，我们需要计算它的梯度并优化它。</li></ol><p id="921f" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">我们做灾难恢复有两个原因。第一，减少噪音，这使得我们的模型更健壮。第二，减少计算时间。</p><p id="b78e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">因此，我们将讨论一些降低数据帧维度的标准方法。</p><p id="625a" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj">我们将讨论</strong> : <br/> 1。相关系数<br/> 2。主成分分析<br/> 3。k选择:特征重要性</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="8dee" class="jp jq hi bd jn jr mc jt ju jv md jx jy io me ip ka ir mf is kc iu mg iv ke kf bi translated">1.相关系数</h1><p id="de4f" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">线性回归的假设之一是特征是独立的。如果它们是相关的，那么结果可能会误导。这使得不可避免地要移除高度相关的特征。</p><p id="69b6" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">一种直觉是，如果两个特征高度相关，它们将向模型提供相似的信息。因此，我们可以删除其中一个特征。这也将减少数据集的维度。</p><p id="8e6b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">同样值得注意的是，最常用于寻找相关性的矩阵是皮尔逊系数。并且它仅测量线性关系的程度。因此，任何非线性相关特征将导致零皮尔逊系数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="ab fe cl mh"><img src="../Images/28911e209963738be3a091740ef03231.png" data-original-src="https://miro.medium.com/v2/format:webp/1*4TLlBVcG4znzhjif0bvr2g.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">皮尔逊相关系数</figcaption></figure></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="1f92" class="jp jq hi bd jn jr mc jt ju jv md jx jy io me ip ka ir mf is kc iu mg iv ke kf bi translated">2.主成分分析</h1><p id="3f20" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">主相关系数是一种将数据投影到低维空间以使方差最大的技术。如果我们考虑线性投影，那么均方差是最小的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mi"><img src="../Images/aa09a73ac7ac5e7fcbf607224366dbda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lxu31L4sCiKcCDqGr4IAow.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">PCA:线性投影</figcaption></figure><p id="0858" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">在图示中，我们试图最大化线性投影的方差。数据点以这样的方式投影，使得<strong class="ki hj">均方距离</strong>最小。</p><p id="039e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">我们可以使用主成分分析来显著降低数据的维数。我们还可以检查新空间中的数据解释了多少差异。一旦我们感知到重要的结果，我们就可以使用这个投影空间。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="b6be" class="jp jq hi bd jn jr mc jt ju jv md jx jy io me ip ka ir mf is kc iu mg iv ke kf bi translated">3.k选择</h1><p id="a439" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">Sci-kit学习库中的<strong class="ki hj"> SelectKBest </strong>选择K个最佳特征，方差低的特征被丢弃。</p><p id="d81b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">选择K个最佳特征的另一种方式是通过使用来自决策树或随机森林的特征重要性图表。</p><p id="5ad1" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">我们可以训练基于树的模型，并且可以直接得到每个特征的重要性。大多数数据科学家也使用这种技术。此外，这是一种简单有效的降低数据维数的方法。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="8967" class="jp jq hi bd jn jr mc jt ju jv md jx jy io me ip ka ir mf is kc iu mg iv ke kf bi translated">摘要</h1><p id="6447" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">博士很重要！它不仅减少了噪声和计算量，而且提供了特征的重要性。</p><p id="06da" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">在本文中，我们讨论了灾难恢复的三个关键技术，即相关性、主成分分析和K选择。</p><p id="6134" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">希望你喜欢这篇文章，我相信你会在现实世界的问题中应用这些技术。</p><p id="9294" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">想了解更多关于机器学习和数据科学的内容，请订阅我的youtube频道。</p><div class="mj mk ez fb ml mm"><a href="https://www.youtube.com/channel/UCqq_T7ktsZO62k7CaibgQvA" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hj fi z dy mr ea eb ms ed ef hh bi translated">阿尤什·奥斯特瓦尔[IITK]</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">合十礼！！！🙏🙏这个频道是给想学数据科学和机器学习的人看的。我已经花了很多…</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">www.youtube.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na jh mm"/></div></div></a></div><p id="88a8" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj">坚持学习！<br/>继续享受！</strong></p></div></div>    
</body>
</html>