<html>
<head>
<title>Fundamentals of Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的基础</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fundamentals-of-reinforcement-learning-81deca1b71c6?source=collection_archive---------23-----------------------#2020-06-14">https://medium.com/analytics-vidhya/fundamentals-of-reinforcement-learning-81deca1b71c6?source=collection_archive---------23-----------------------#2020-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b956" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">学习做出与众不同的决定</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a679721d92ac7efc2216b3d37fec0ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lLIN9QZThcLyPYvo1lfWZg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">使用强化学习的学习代理人玩Atari游戏(太空入侵者和突围)</figcaption></figure><h1 id="5ecd" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍</h1><p id="fec2" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">由于各种原因，如计算能力的提高、实验资源的可用性等，设计能够学会自己完成工作的机器是近来研究最多的课题之一。，这导致发现重大创新，使生活更简单。如果你只有数据，那么算法将提供见解，或者你训练一个模型，它识别你的脸和我们周围看到的许多其他用例，这些用例是使用机器学习和深度学习建立的。强化学习因其在制定<strong class="kh hj">顺序</strong> <strong class="kh hj">决策</strong>过程中已被证明的能力而获得了很多关注，因此正在蓬勃发展。</p><h1 id="948a" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">RL的概念</h1><p id="bf3b" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">强化学习基本上由一个<strong class="kh hj">代理</strong>(决策者)组成，它试图从给定环境中的<strong class="kh hj">状态</strong>中学习，它与被称为<strong class="kh hj">环境</strong>的环境交互，并根据在<strong class="kh hj">情节</strong>期间环境提供的反馈采取的某些<strong class="kh hj">动作</strong>来改变其状态。这种反馈是数字的(正、负或零)，被称为<strong class="kh hj">奖励</strong>。代理的最佳行为是学习，这样它总是得到好的反馈，即通过采取适当的行动来最大化回报。因此，在RL中，我们向代理提供场景，它可以自己找出答案，或者发现如何以最受欢迎的方式做出决定。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lb"><img src="../Images/3b73752d5b79c94cfdb9d7e3bf0ef7d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*B-Bbol2gAeBmI_hSNbM7hg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">智能体在强化学习中的学习周期</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lc"><img src="../Images/1c108fd29491ef836a4b655f301676a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*TtV-NCGFYsPKZYIRocyPcQ.png"/></div></figure><p id="2d56" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">让我们以众所周知的PUBG游戏为例来理解RL中使用的术语:</p><ul class=""><li id="8ed4" class="li lj hi kh b ki ld kl le ko lk ks ll kw lm la ln lo lp lq bi translated">PUBG中的玩家在这里是一个<strong class="kh hj">代理</strong>，战场是他的<strong class="kh hj">环境</strong></li><li id="c4bf" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">一个完整的游戏是一个<strong class="kh hj">插曲</strong>，行走、奔跑等是<strong class="kh hj">状态</strong>——帮助选择动作</li><li id="9b36" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">代理有许多<strong class="kh hj">动作</strong>要做，如向左、向右、向前和向后移动、奔跑、射击、杀戮、弯腰、跳跃、换枪等。,</li><li id="8064" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">代理人在这里得到的奖励是正面的，如果他杀了人，如果他在队友的帮助下生存到最后，奖励为零；如果他被其他玩家射杀，奖励为负面。</li></ul><p id="7709" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">为了赢得比赛，我们必须在每个时间段采取适当的行动，以获得最大的回报。简单地说，我们从一个状态开始，采取一个行动，然后改变到另一个状态，并为该行动获得奖励，并重复该过程以了解更多关于环境设置的信息。</p><p id="d65b" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">RL面临的挑战很少。其中一些是:</p><ul class=""><li id="7314" class="li lj hi kh b ki ld kl le ko lk ks ll kw lm la ln lo lp lq bi translated">权衡:正如我们所理解的，代理人必须优化报酬，还必须不断地与环境互动，即，它需要探索很多。这导致了勘探和开发之间的权衡。它必须选择是否应该继续探索可能导致较低回报的新状态，或者选择已经看到并获得相当多回报的道路。</li><li id="fc33" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">概括:代理能理解或学习在它以前看不到的状态下行为是好是坏吗？</li><li id="94ba" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">延迟后果:我们还需要理解，如果一个代理人在当前状态下给出了很高的奖励，是因为仅仅是这种状态还是它为达到这种状态而采取的一系列决策？</li></ul><p id="33b5" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">适用于RL的关键概念很少。对这些的良好认识将让我们理解智能体决策过程和环境模型的形成。</p><p id="66b7" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated"><strong class="kh hj">马尔可夫性质:</strong>如果一个主体从一种状态变化到另一种状态，这叫做转移，它进行转移的概率叫做转移概率。一般来说，如果我们有一个主体从一种状态到另一种状态的所有概率，那么它就用一个转移矩阵来表示。马尔可夫性质说“<em class="lw">未来独立于过去给定的现在</em>”。下面的等式描述了在时间t内转移到下一个状态St+1的概率仅取决于当前状态St和在处采取的动作，而与历史无关</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lx"><img src="../Images/dad9df67219cb8c61256dedfc6905c05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*t3QknuXBF-U9xdsLdq-zUg.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/91de852bc59e8b70096d5ca8e218efd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*tdpgeIIcC1Bys86vTmYtuA.png"/></div></figure><p id="9bd1" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">这是转移概率矩阵，它包含了所有状态的转移概率。例如，P12描述了从状态1转换到状态2的概率，依此类推。</p><p id="2797" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">当我们遍历环境中遵循马尔可夫性质的一组状态时，就称之为马尔可夫链。它们可能包括马尔可夫链中的随机状态集，也有转移概率，我们可以计算出导致高回报的最优链。</p><ul class=""><li id="ea4f" class="li lj hi kh b ki ld kl le ko lk ks ll kw lm la ln lo lp lq bi translated">在RL中，我们更关心优化代理从环境中获得的总回报，而不是它通过从一个状态转换到另一个状态获得的即时回报。所以我们用一个叫做<strong class="kh hj">回报(Gt) </strong>的函数来衡量最优性，这个函数是代理人从时间t(等式1)收到的回报的总和。</li><li id="33cd" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">在雅达利、阿尔法围棋、国际象棋或PUBG等许多游戏中，我们知道游戏会在特定的时间步骤后终止。如果这是设置，那么它被称为<strong class="kh hj">阶段性</strong>任务。如果我们开始另一个游戏，那么我们就开始了新的一集，所以每一集都是相互独立的。还可能存在这样的问题，即它不会像用于个人辅助的某些机器人那样结束，这些机器人直到来自环境的外部信号将其置于终止状态时才终止。这些被称为<strong class="kh hj">连续</strong>任务。</li></ul><p id="c958" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">在情节性任务中，我们可以计算回报，这是其奖励的总和，直到终止，但在连续任务中，由于没有终止，在计算回报时，奖励总计为<strong class="kh hj">无穷大</strong>。所以我们引入一个折现因子gamma <strong class="kh hj"> (ɤ) </strong>通过折现来计算连续任务的收益。它的值从0到1。它在决定我们是重视眼前的回报还是未来的回报方面起着至关重要的作用。如果<strong class="kh hj"> ɤ=0 </strong>那么注意力就在眼前的奖励上，如果<strong class="kh hj"> ɤ=1 </strong>那么就在未来的奖励上。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lz"><img src="../Images/1175306d0dccd611efaa67f8723c63d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*Fsq7yZtysnioto5fXlvQCw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">等式1 —使用时间步长t的折扣系数返回</figcaption></figure><p id="f350" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">如果我们有一个问题陈述，说你在接下来的k个时间步骤中执行某个动作得到1的奖励，折扣因子为0.8和0.2，那么回报将是</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/6a2fa17cd013f78233b3f3ab621f7068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*Z8szuycvjBj-37xzOWAEKQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">γ分别为0.8和0.2</figcaption></figure><blockquote class="mb mc md"><p id="5c79" class="kf kg lw kh b ki ld ij kk kl le im kn me lf kq kr mf lg ku kv mg lh ky kz la hb bi translated">我们看到，ɤ=0.8的Gt甚至在未来也有不错的回报，但是ɤ=0.2的回报仅在当前的时间步长内是高的，在未来几乎趋于零。因此，基于问题陈述，我们可以设置<strong class="kh hj"> ɤ </strong>，以便于决定<strong class="kh hj">立即</strong>或<strong class="kh hj">未来</strong>奖励的重要性。</p></blockquote><p id="bdd5" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">我们现在知道一个代理改变了它的状态，并因为这种转变而获得了奖励。让我们通过一个例子来详细了解:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mh"><img src="../Images/313a147aa811634c01f8520b1ba43dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*5onmpf1sAWr9r9L9mtWO0w.png"/></div></figure><p id="9b59" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">考虑这样一种情况，其中学生是代理，他有四个状态家庭、学校、班级、电影和折扣因子0.8。从一种状态转换到另一种状态的概率显示在蓝色方框中，奖励显示在棕色方框中。代理可能有许多情节，即一系列的状态遍历。举个例子，</p><p id="ad87" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">1.Home-&gt; School-&gt; Class-&gt; Home-&gt; termin ate—让我们计算state Home的返回。G = 3 + 5*0.8 + 5*0.8*0.8 = 10.2 <br/> 2。Home-&gt;School-&gt;School-&gt;Movie-&gt;Home-&gt;termin ate—本集收益为G = 3+2 * 0.8+(-10)* 0.8 * 0.8+3 * 0.8 * 0.8 =—0.264</p><p id="b4c8" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">很明显，第一集比第二集有更高回报。所以跟着它走是可行的。收益是一个重要的概念，因为它可以决定代理人的最优路径。</p><h1 id="c7a1" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">马尔可夫奖励过程(MRP):</h1><p id="6fec" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">MRP是一个马尔可夫过程设置，它指定了一个奖励函数和一个折扣因子<strong class="kh hj"> ɤ.</strong>使用下面列出的元组(S，P，R，γ)来正式表示它:</p><ul class=""><li id="29a6" class="li lj hi kh b ki ld kl le ko lk ks ll kw lm la ln lo lp lq bi translated">s:有限状态空间。</li><li id="10d6" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">P:指定P(s`|s)的转移概率模型。</li><li id="c333" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">r:将状态映射到奖励的奖励函数(实数)R(s) = E[ri |si = s]，∀ i = 0，1，.。。。(E是期望值，I是每个时间步长)</li><li id="993c" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">γ:折扣因子-介于0和1之间。</li></ul><p id="bbb0" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated"><strong class="kh hj">状态价值函数:</strong>状态价值函数Vt(s)是在时间t从状态s开始的期望收益之和</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mi"><img src="../Images/ba5ab9740176c02fd733a2c8c7d442fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*e5FBsX3_qEIZ5Nb2gguBTg.png"/></div></figure><p id="238b" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">简单地说，价值函数表示一个代理人在那个特定状态下有多好。在情节中导致高回报的状态之间的转换是最佳的MRP。我们有不同的方法来评估V(s)。他们是</p><p id="df86" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">1.蒙特卡洛模拟法:在MRP中，对每集的收益进行计算和平均。所以状态值函数计算为Vt(s)= Sum(Gt)/发作次数。</p><p id="61fd" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">2.解析解:如果时间步长的数量是无限的，那么我们无法计算回报的总和或平均值。在这个过程中，我们定义γ&lt;1 and State value function is equal to sum of Immediate Reward(reward obtained for transitioning from State s to s`) and discounted sum of future rewards. The equation can be represented in Matrix form as V=R + γPV. Rearranging gives V by multiplying inverse matrix of (I − γP) with R.</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mj"><img src="../Images/866c8825588e3be5f9a9ab42453a93a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*XeGzqHpgOcV972ezKvO4Gg.png"/></div></figure><p id="d81d" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">3. Iterative Solution: In this method we calculate Value Function at time step t by iterating through its previous value functions at time t-1,t-2 etc., We will look into this in depth soon.</p><h1 id="28c1" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">Markov Decision Process (MDP):</h1><p id="020a" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">An MDP is simply an MRP but with the specifications of a set of actions that an agent can take from each state. It is represented a tuple (S, A, P, R, γ) which denotes:</p><ul class=""><li id="8111" class="li lj hi kh b ki ld kl le ko lk ks ll kw lm la ln lo lp lq bi translated">S : A finite state space.</li><li id="4fef" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">A : A finite set of actions which are available from each state s.</li><li id="1a04" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">P : A transition probability model that specifies P(s`|s).</li><li id="996f" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">R : A reward function that maps states to rewards (real numbers) R(s) = E[ri |Si = s, Ai=a] , ∀ i = 0, 1, . . . .(state s, action taken a, E here is expected value and i is every time step)</li><li id="0dbc" class="li lj hi kh b ki lr kl ls ko lt ks lu kw lv la ln lo lp lq bi translated">γ : Discount factor — lies between 0 and 1. An episode of a MDP is thus represented as (s0, a0, r0, s1, a1, r1, s2, a2, r2, . . .).</li></ul><p id="3f7d" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">In MRP, we have transition probability of going from one state to the other. In MDP, the notation is slightly changed. We define transition probability with respect to action as well P(Si+1|Si , ai). An example of robot transitioning between different states also depends on the action it takes if its moving forward, left, right or halted. All the other notations of returns(Gt), discount factor(γ) are exactly the same as referred in MRP.</p><h1 id="9b44" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">Policy and Q-Function:</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mk"><img src="../Images/8d3cf2f4487a9b44e847291711ceadf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*msjuq174FOac26Ip2k29UA.png"/></div></figure><p id="0a89" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">Suppose there is a robot which is currently at a state S1, it can take actions left or right with probabilities al and ar respectively for taking left or right which lands in two different states S2 and S3. Value function and rewards of that state are also mentioned and discount factor = 0.8</p><p id="c065" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">Let us calculate the value function of S1:<br/>V1 = al(R+γ* V2)+ar(R+γ* V3)<br/>= al(2+0.8 * 10)+ar(1+0.8 * 15)<br/>= al * 10+ar * 13.5</p><p id="9579" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">如果a1 = 0.2，ar = 0.8，那么<strong class="kh hj"> V1 = 12.8 </strong>，如果a1 = 0.8，ar = 0.2，那么<strong class="kh hj"> V1 = 10.7 </strong>显然给出了采取行动的更大概率<strong class="kh hj"> ar </strong>将给出状态S1值方面的更好结果。</p><blockquote class="mb mc md"><p id="eb5d" class="kf kg lw kh b ki ld ij kk kl le im kn me lf kq kr mf lg ku kv mg lh ky kz la hb bi translated">为了评估过渡到一个状态有多好，我们使用<strong class="kh hj">值函数</strong>，但是为了确定从这个状态采取<strong class="kh hj">动作</strong>‘a’有多好？这就是<strong class="kh hj">政策</strong>概念的由来。</p></blockquote><h2 id="12ac" class="ml jo hi bd jp mm mn mo jt mp mq mr jx ko ms mt jz ks mu mv kb kw mw mx kd my bi translated">政策π:</h2><p id="57ca" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">在MDP，政策是一种将状态映射到行动的决策机制。对于策略π，如果在时间t代理处于状态s，它将以π(a|s)给出的概率选择动作a。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mz"><img src="../Images/81089f451240a35131695ea28c2a4f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*Kyli-i5lFMHBwthAPRsGmA.png"/></div></figure><p id="2c83" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">给定一个政策<strong class="kh hj"> π </strong>我们如何评估它是好是坏？直觉和MRP一样，我们计算预期的回报。我们可以定义如下:</p><p id="a396" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated"><strong class="kh hj">状态价值函数</strong>(过渡到一个状态有多好):价值函数在一个代理的给定状态s，是通过遵循一个策略<strong class="kh hj"> π </strong>并到达下一个状态，直到我们到达一个终止状态所获得的期望收益。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es na"><img src="../Images/18938c9a29532f79277c4cddd30f54dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*P8qFz0hBf6Xi7fYPf8cRjA.png"/></div></figure><h2 id="f6aa" class="ml jo hi bd jp mm mn mo jt mp mq mr jx ko ms mt jz ks mu mv kb kw mw mx kd my bi translated">状态-动作值函数或Q-函数:</h2><p id="e566" class="pw-post-body-paragraph kf kg hi kh b ki kj ij kk kl km im kn ko kp kq kr ks kt ku kv kw kx ky kz la hb bi translated">状态s和行动a的状态-行动值函数被定义为从时间t时的状态St = s开始并在= a时采取行动，然后遵循策略π的预期回报。它在数学上写为</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/ce2c018d24ca1f4fa563fb6f71b9f0b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*9_2pbvwkU8VzCzRZK8Jx2w.png"/></div></figure><p id="01c3" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">这告诉我们，在状态s遵循策略π的情况下，执行动作a的价值。</p><p id="54d3" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">这些只是RL中MDP的积木。还有更多的概念，如贝尔曼备份运营商，寻找最佳价值函数和政策和动态规划等。，让我们在我的下一篇博客里看看吧。</p><p id="4e21" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">希望这篇文章能让你对RL的理解有所提升。</p><p id="7130" class="pw-post-body-paragraph kf kg hi kh b ki ld ij kk kl le im kn ko lf kq kr ks lg ku kv kw lh ky kz la hb bi translated">感谢您的宝贵时间！</p></div></div>    
</body>
</html>