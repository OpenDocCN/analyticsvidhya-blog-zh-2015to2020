<html>
<head>
<title>How did I tackle a real-world problem with GuidedLDA?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我是如何用GuidedLDA解决现实世界中的问题的？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-i-tackled-a-real-world-problem-with-guidedlda-55ee803a6f0d?source=collection_archive---------1-----------------------#2019-10-10">https://medium.com/analytics-vidhya/how-i-tackled-a-real-world-problem-with-guidedlda-55ee803a6f0d?source=collection_archive---------1-----------------------#2019-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/d511dc46e43b5fb693b31452dfb41518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*weJWjasDpk8EIHo1YbwU9g.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">导向LDA识别的主题和每个主题中的关键字的交互式可视化快照(pyLDAvis)</figcaption></figure><div class=""/><p id="7ac1" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">用于交互的在线平台的普遍使用和来自用户输入的大量文本数据使得消化数据越来越耗时。Sown to Grow是一家在线教育公司，旨在通过提供一个平台来设定目标、思考策略并与老师互动，从而增强学生的能力。为了让这家公司能够在美国扩大规模，自动解析反射是必要的。它有助于教师定制反馈，并将有限的资源分配给弱势儿童。</p><h1 id="7890" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">数据</strong></h1><p id="fef6" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">该公司分享了180，000名学生的反映，这些反映基于公司的评分系统被认为是高质量的(有一个/多个策略)。由于隐私原因，我无法展示实际数据，但我的数据框架如下所示:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="1ab2" class="le jt hx la b fi lf lg l lh li">content               index</span><span id="1f79" class="le jt hx la b fi lj lg l lh li">0                  reflection            0 <br/><br/>1                  reflection            1<br/><br/>.<br/><br/>.<br/><br/>.</span><span id="9f10" class="le jt hx la b fi lj lg l lh li">184835             reflection            184835</span></pre><p id="22ea" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在清理数据(包括删除重复的、不相关的内容和非英语内容)之后，我最终得到了104k的反射，用于确定策略。下面是我用来纠正拼错单词的函数。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="8f29" class="le jt hx la b fi lf lg l lh li">from enchant.checker import SpellChecker</span><span id="4fc7" class="le jt hx la b fi lj lg l lh li">def spell_check(text):       <br/>    '''<br/>    spell_check: function for correcting the spelling of the reflections<br/>    Expects:  a string<br/>    Returns: a list<br/>    '''<br/>    Corr_RF = []<br/>    #Grab each individual reflection<br/>    for refl in text.split():<br/>        #Check to see if the words are in the dictionary<br/>        chkr = SpellChecker("en_US", refl)<br/>        for err in chkr:<br/>            #for the identified errors or words not in dictionary get the suggested correction<br/>            #and replace it in the reflection string<br/>            if len(err.suggest()) &gt; 0:<br/>                sug = err.suggest()[0]<br/>                err.replace(sug)<br/>        Corr_RF.append(chkr.get_text())<br/>        #return the dataframe with the new corrected reflection column<br/>    return ' '.join(Corr_RF)</span><span id="7a2c" class="le jt hx la b fi lj lg l lh li">data['Corrected_content'] = data.content.apply(spell_check)</span><span id="d69a" class="le jt hx la b fi lj lg l lh li">document = data #to change the name of the dataframe to documents</span></pre><p id="3aca" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了删除非英语内容，我使用langdetect来标记文本的语言并删除非英语内容。当输入句子时，langdetect相当准确，但当只输入一个单词时，它就不完美了。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="be40" class="le jt hx la b fi lf lg l lh li">from langdetect import detect</span><span id="f23c" class="le jt hx la b fi lj lg l lh li">def lang_detect(text):<br/>    '''<br/>    lang_detect: function for detecting the language of the reflections<br/>    Expects: a string<br/>    Returns: a list of the detected languages<br/>    '''<br/>    lang = []<br/>    for refl in text:<br/>        lang.append(detect(refl))<br/>    return lang</span></pre><h1 id="bc88" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">解决问题的初步策略</strong></h1><h2 id="a566" class="le jt hx bd ju lk ll lm jy ln lo lp kc jf lq lr kg jj ls lt kk jn lu lv ko lw bi translated">常规LDA</h2><p id="7f25" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">然后，我开始通过潜在的狄利克雷分配(LDA)使用Gensim主题建模包对反射中的主题进行建模。为了准备主题建模的数据，我对文档进行了分词(将文档拆分为句子，将句子拆分为单词)，删除了标点符号，并将其小写。长度小于三个字符的单词也将被删除。所有这些都可以使用Gensim简单预处理模块来完成。之后，我定义了一个函数，将第三人称中的单词转换为第一人称，将过去时态和将来时态中的动词转换为现在时态。然后，单词被简化为它们的根形式(词干和词尾)。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="6596" class="le jt hx la b fi lf lg l lh li">import gensim</span><span id="74b5" class="le jt hx la b fi lj lg l lh li">from gensim.utils import simple_preprocess</span><span id="bb61" class="le jt hx la b fi lj lg l lh li">from gensim.parsing.preprocessing import STOPWORDS</span><span id="5685" class="le jt hx la b fi lj lg l lh li">from nltk.stem import WordNetLemmatizer, SnowballStemmer</span><span id="e0b7" class="le jt hx la b fi lj lg l lh li">from nltk.stem.porter import *</span><span id="7071" class="le jt hx la b fi lj lg l lh li">from nltk.corpus import wordnet</span><span id="28ea" class="le jt hx la b fi lj lg l lh li">import numpy as np</span><span id="a43c" class="le jt hx la b fi lj lg l lh li">np.random.seed(42)</span></pre><p id="85be" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">导入必要的包和模块后。现在是进行一些预处理的时候了，如前所述:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="357f" class="le jt hx la b fi lf lg l lh li">def lemmatize_stemming(text):<br/>    stemmer = SnowballStemmer('english')<br/>    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))<br/>def preprocess(text):<br/>    result = []<br/>    for token in gensim.utils.simple_preprocess(text):<br/>        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) &gt; 3:<br/>            result.append(lemmatize_stemming(token))<br/>    return result</span><span id="55b1" class="le jt hx la b fi lj lg l lh li">processed_docs = documents['content'].map(preprocess)</span></pre><p id="3d37" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面的例子显示了预处理的结果(我使用了一个假设的例子):</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="e44f" class="le jt hx la b fi lf lg l lh li">doc_sample = documents[documents['index'] == 34].values[0][0]<br/>print('original document: ')<br/>words = []<br/>for word in doc_sample.split(' '):<br/>    words.append(word)<br/>print(words)<br/>print('\n\n tokenized and lemmatized document: ')<br/>print(preprocess(doc_sample))</span></pre><p id="56a4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">示例:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="650d" class="le jt hx la b fi lf lg l lh li">original document:</span><span id="cd38" class="le jt hx la b fi lj lg l lh li">['Something', 'I', 'think', 'I', 'have', 'done', 'correct', 'is', 'studying', 'in', 'advance.']</span><span id="9a43" class="le jt hx la b fi lj lg l lh li">tokenized and lemmatized document:</span><span id="703e" class="le jt hx la b fi lj lg l lh li">['think', 'correct', 'studi', 'advanc']</span></pre><p id="01b6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">要在数据集上创建一个单词包，可以使用Gensim字典。单词包是一个来自“processed_docs”的字典，包含一个单词在整个文档(语料库)中出现的次数(字数)。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="463c" class="le jt hx la b fi lf lg l lh li">dictionary = gensim.corpora.Dictionary(processed_docs)<br/>count = 0<br/>for k, v in dictionary.iteritems():<br/>    print(k, v)<br/>    count += 1<br/>    if count &gt; 10:<br/>        break</span></pre><p id="e210" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">移除出现在少于15个文档和高于0.5个文档中的标记(整个文档的分数，而不是绝对值)。之后，保留100000个最常用的代币。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="ae34" class="le jt hx la b fi lf lg l lh li">dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)</span></pre><p id="f2b3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我创建了一个字典，显示哪些单词以及这些单词在每个文档中出现的次数，并将它们保存为bow_corpus:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="8aa6" class="le jt hx la b fi lf lg l lh li">bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]</span></pre><p id="653e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，数据已经准备好运行LDA主题模型了。我使用的Gensim LDA能够在多个内核上运行。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="8cc6" class="le jt hx la b fi lf lg l lh li">lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=7, id2word=dictionary, passes=2, workers=2)</span></pre><p id="cf3a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">要检查每个主题的单词及其相对权重:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="d8c3" class="le jt hx la b fi lf lg l lh li">for idx, topic in lda_model.print_topics(-1):<br/>    print('Topic: {} \nWords: {}'.format(idx, topic))</span><span id="a7db" class="le jt hx la b fi lj lg l lh li">Topic: 0</span><span id="2d15" class="le jt hx la b fi lj lg l lh li">Words: 0.046*"time" + 0.044*"read" + 0.041*"week" + 0.030*"work" + 0.024*"studi" + 0.022*"go" + 0.016*"good" + 0.016*"book" + 0.015*"like" + 0.014*"test"</span><span id="6068" class="le jt hx la b fi lj lg l lh li">Topic: 1</span><span id="d5dd" class="le jt hx la b fi lj lg l lh li">Words: 0.055*"read" + 0.036*"question" + 0.034*"answer" + 0.025*"time" + 0.018*"text" + 0.017*"strategi" + 0.017*"work" + 0.016*"think" + 0.014*"go" + 0.014*"look"</span><span id="9022" class="le jt hx la b fi lj lg l lh li">Topic: 2</span><span id="88b6" class="le jt hx la b fi lj lg l lh li">Words: 0.037*"need" + 0.021*"work" + 0.018*"word" + 0.018*"write" + 0.015*"time" + 0.015*"complet" + 0.015*"essay" + 0.014*"goal" + 0.013*"help" + 0.012*"finish"</span><span id="4fb1" class="le jt hx la b fi lj lg l lh li">Topic: 3</span><span id="a8fa" class="le jt hx la b fi lj lg l lh li">Words: 0.042*"note" + 0.041*"help" + 0.032*"studi" + 0.029*"understand" + 0.027*"quiz" + 0.024*"question" + 0.021*"time" + 0.016*"better" + 0.014*"take" + 0.014*"test"</span><span id="356c" class="le jt hx la b fi lj lg l lh li">Topic: 4</span><span id="bcaa" class="le jt hx la b fi lj lg l lh li">Words: 0.031*"write" + 0.031*"work" + 0.027*"time" + 0.025*"think" + 0.024*"sure" + 0.019*"check" + 0.017*"thing" + 0.017*"strategi" + 0.014*"question" + 0.014*"help"</span><span id="7e7f" class="le jt hx la b fi lj lg l lh li">Topic: 5</span><span id="3a18" class="le jt hx la b fi lj lg l lh li">Words: 0.058*"work" + 0.057*"grade" + 0.046*"goal" + 0.033*"class" + 0.027*"week" + 0.022*"math" + 0.017*"scienc" + 0.016*"improv" + 0.016*"want" + 0.016*"finish"</span></pre><p id="08e9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如您从每个主题中的单词所看到的，一些单词在主题之间是共享的，并且没有可以为每组单词标记的不同主题。</p><h2 id="caf5" class="le jt hx bd ju lk ll lm jy ln lo lp kc jf lq lr kg jj ls lt kk jn lu lv ko lw bi translated">词性标注</h2><p id="94bc" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">在LDA之后，我决定标记每个反射的词性，并从中提取动词。因为我假设学生在反思他们所做的事情，所以动词过去式的反思可以给我学习策略主题的线索(例如，我研究了笔记并练习了过去的考试)。我解析了反射，并通过词性标注提取了反射中使用的所有动词。然后，我寻找动词的时态，以确定拥有学习策略的反思和其中使用的动词时态之间的关系。我注意到有些反思明显有学习策略，但不一定是过去时。</p><p id="8a63" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">所以，这也没有帮助我找到学习策略的独特主题。然而，LDA和POS都给了我一个想法，那就是使用GuidedLDA ( <a class="ae lx" href="https://guidedlda.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Github repo </a>)。导向LDA是一种半监督学习算法。其思想是为用户认为代表语料库中潜在主题的主题设置一些种子词，并引导模型围绕这些术语收敛。我使用了J. Jagarlamudi、H. Daume III和R. Udupa的论文“将词法先验合并到主题模型中”中解释的算法的python实现。该论文讨论了如何将先验(在这种情况下，先验是指种子词)设置到模型中，以将它引导到某个方向。</p><p id="c25d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在常规LDA中，首先每个单词被随机分配给一个主题，该主题由Dirichlet先验通过Alpha参数控制(现在你知道LDA的名字是从哪里来的了)。下一步是找出哪个术语属于哪个主题。LDA使用一种非常简单的方法，每次为一个术语寻找主题。</p><p id="6289" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们假设我们想要为单词“学习”找到主题。LDA将首先假设语料库中的每隔一个单词被分配给正确的主题。在最后一步中，每个单词都均匀地分布在所有主题中，并假设这是这些单词的正确主题。然后，LDA计算出“学习”这个词经常出现在哪个单词中。那么，这是这些术语中最常见的话题。我们将对该主题进行“研究”。“研究”可能会接近“教科书”和“笔记”中的任何主题。现在这三个词比这一步之前更接近了。然后，模型移动到下一个单词，并根据需要重复这个过程直到收敛。有了导向LDA，我们明确地希望模型以一种方式收敛，即单词“学习”和“教科书”在一个主题中。为了做到这一点，GuidedLDA为“研究”和“教科书”提供了一些额外的帮助，使其位于特定的主题中。在该算法中，应该给予单词多少额外提升的参数由seed_confidence控制，并且其范围可以在0和1之间。当seed_confidence为0.1时，您可以将种子单词向种子主题倾斜10%以上。</p><p id="aba7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">要使用GuidedLDA的python实现，您可以:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="fa99" class="le jt hx la b fi lf lg l lh li">pip install guidedlda</span></pre><p id="8b17" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">或者</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="8570" class="le jt hx la b fi lf lg l lh li"><a class="ae lx" href="https://github.com/vi3k6i5/GuidedLDA" rel="noopener ugc nofollow" target="_blank">https://github.com/vi3k6i5/GuidedLDA</a><br/>cd GuidedLDA<br/>sh build_dist.sh<br/>python setup.py sdist<br/>pip install -e .</span></pre><p id="8d76" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">与任何NLP工作一样，启动GuidedLDA需要对数据进行预处理。为此，我定义了自己的预处理函数:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="3d46" class="le jt hx la b fi lf lg l lh li">def get_wordnet_pos(word):</span><span id="6062" class="le jt hx la b fi lj lg l lh li">    '''tags parts of speech to tokens<br/>    Expects a string and outputs the string and <br/>    its part of speech'''<br/>    <br/>    tag = nltk.pos_tag([word])[0][1][0].upper()<br/>    tag_dict = {"J": wordnet.ADJ,<br/>                "N": wordnet.NOUN,<br/>                "V": wordnet.VERB,<br/>                "R": wordnet.ADV}<br/>    return tag_dict.get(tag, wordnet.NOUN)</span><span id="ad46" class="le jt hx la b fi lj lg l lh li">def word_lemmatizer(text):</span><span id="9ce2" class="le jt hx la b fi lj lg l lh li">    '''lemamtizes the tokens based on their part of speech'''<br/>    <br/>    lemmatizer = WordNetLemmatizer()<br/>    text = lemmatizer.lemmatize(text, get_wordnet_pos(text))<br/>    return text</span><span id="503e" class="le jt hx la b fi lj lg l lh li">def reflection_tokenizer(text):</span><span id="cfec" class="le jt hx la b fi lj lg l lh li">     '''expects a string an returns a list of lemmatized tokens <br/>        and removes the stop words. Tokens are lower cased and <br/>        non- alphanumeric characters as well as numbers removed. '''</span><span id="6aaa" class="le jt hx la b fi lj lg l lh li">    text=re.sub(r'[\W_]+', ' ', text) #keeps alphanumeric characters<br/>    text=re.sub(r'\d+', '', text) #removes numbers<br/>    text = text.lower()<br/>    tokens = [word for word in word_tokenize(text)]<br/>    tokens = [word for word in tokens if len(word) &gt;= 3]<br/>    #removes smaller than 3 character<br/>    tokens = [word_lemmatizer(w) for w in tokens]<br/>    tokens = [s for s in tokens if s not in stop_words]<br/>    return tokens</span></pre><p id="c3db" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在定义了用于预处理的所有必要函数之后，是时候将它应用到dataframe的目标列(这里是corrected_content ),并将其保存为新列“lemmatized_tokens”。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="aa40" class="le jt hx la b fi lf lg l lh li">df['lemmatize_token'] = df.corrected_content.apply(reflection_tokenizer)</span></pre><p id="ecd7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，是时候生成术语-文档矩阵了。为此，我使用了scikit学习包中的CountVectorizer类:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="7197" class="le jt hx la b fi lf lg l lh li">from sklearn.feature_extraction.text import CountVectorizer</span></pre><p id="8ad4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，我们需要实例化CountVectorizer。有关参数的完整列表，您可以参考scikit learn网站。我将标记器更改为我之前定义的自定义标记器，并将停用词更改为我基于自己的数据集创建的停用词列表。这里，我使用了4个单词的n-gram范围。现在，是时候拟合和转换语料库以生成术语-文档矩阵了:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="5872" class="le jt hx la b fi lf lg l lh li">token_vectorizer = CountVectorizer(tokenizer = reflection_tokenizer, min_df=10, stop_words=stop_words, ngram_range=(1, 4))</span><span id="a7cb" class="le jt hx la b fi lj lg l lh li">X = token_vectorizer.fit_transform(df.corrected_content)</span></pre><p id="38c3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了用GuidedLDA对主题进行建模，在导入包之后，会创建一个术语字典。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="de35" class="le jt hx la b fi lf lg l lh li">import guidedlda</span><span id="622a" class="le jt hx la b fi lj lg l lh li">tf_feature_names = token_vectorizer.get_feature_names()word2id = dict((v, idx) for idx, v in enumerate(tf_feature_names))</span></pre><p id="2136" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，是时候提供一个种子词列表来建模了。为此，我使用了文本的语义，以及从LDA建模和POS动词词典中获得的初始关键字。为此，我创建了一个列表列表，其中每个列表都包含了我希望在特定主题下进行分组的关键字。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="4a84" class="le jt hx la b fi lf lg l lh li">seed_topic_list= [['take', 'note', 'compare', 'classmate', 'highlight', 'underline', 'jot', 'write', 'topic', 'main', 'complete', 'point', 'copy', 'slide'],</span><span id="a712" class="le jt hx la b fi lj lg l lh li">['read', 'study', 'review', 'skim', 'textbook', 'compare', 'note', 'connect', 'sketch', 'summarize', 'relationship', 'map', 'concept', 'diagram', 'chart'],</span><span id="3937" class="le jt hx la b fi lj lg l lh li">['question', 'essay', 'assignment', 'exam', 'test', 'quiz', 'answer', 'practice', 'review', 'repeat', 'strength', 'weak', 'solve', 'problem', 'identify'],</span><span id="3e4b" class="le jt hx la b fi lj lg l lh li">['plan', 'calendar', 'time', 'task', 'list', 'manage', 'procrastinate', 'due', 'stress', 'manage', 'anxiety', 'express', 'break', 'sleep', 'nap', 'eat', 'exercise'],</span><span id="e122" class="le jt hx la b fi lj lg l lh li">['group', 'partner', 'classmate', 'brainstorm', 'ask', 'answer', 'verify', 'peer', 'teach', 'clarify'],</span><span id="f814" class="le jt hx la b fi lj lg l lh li">['ask','aid', 'resource', 'teacher', 'tutor', 'peer', 'verify', 'explain', 'clear', 'talk']]</span></pre><p id="7c9b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如你所看到的，我已经为6个主题提供了带有种子词的模型。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="a569" class="le jt hx la b fi lf lg l lh li">model = guidedlda.GuidedLDA(n_topics=6, n_iter=100, random_state=7, refresh=10)<br/>seed_topics = {}<br/>for t_id, st in enumerate(seed_topic_list):<br/>    for word in st:<br/>        seed_topics[word2id[word]] = t_id<br/>model.fit(X, seed_topics=seed_topics, seed_confidence=0.15)</span></pre><p id="3f9a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">检查每个主题的单词:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="3e00" class="le jt hx la b fi lf lg l lh li">n_top_words = 15<br/>topic_word = model.topic_word_<br/>for i, topic_dist in enumerate(topic_word):<br/>     topic_words = np.array(tf_feature_names)[np.argsort(topic_dist)][:-(n_top_words+1):-1]<br/>     print('Topic {}: {}'.format(i, ' '.join(topic_words)))</span></pre><p id="a3b4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">结果看起来是这样的:</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="f881" class="le jt hx la b fi lf lg l lh li">Topic 0: write time reading book know essay start idea take people read keep focus first complete</span><span id="6b07" class="le jt hx la b fi lj lg l lh li">Topic 1: read study time note take test reading quiz question book look understand day word review</span><span id="0bd2" class="le jt hx la b fi lj lg l lh li">Topic 2: question time study quiz understand check problem note answer knowledge take practice ask mistake learn</span><span id="3d61" class="le jt hx la b fi lj lg l lh li">Topic 3: time finish assignment homework complete study reflection school day test quiz home keep win last</span><span id="75b7" class="le jt hx la b fi lj lg l lh li">Topic 4: question answer time read look text reading evidence write find understand word know back right</span><span id="dcbc" class="le jt hx la b fi lj lg l lh li">Topic 5: ask finish teacher talk time school stay attention pay focus extra test pay attention homework know</span></pre><p id="26ef" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了可视化数据，我使用了pyLDAvis软件包强大的交互式可视化功能，下面是结果。可以看出，6个主题明显分开，每个主题的主题可以分为:</p><ol class=""><li id="b412" class="ly lz hx iw b ix iy jb jc jf ma jj mb jn mc jr md me mf mg bi translated">完成作业/完成任务</li></ol><p id="3a13" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.检查过去的测验和问题/理解答案</p><p id="b8d7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.边说边问老师/注意</p><p id="7c9c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">4.阅读/研究笔记和书籍</p><p id="4cba" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">5.回答问题和学习问题</p><p id="7c55" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">6.写故事、文章和书</p><figure class="kv kw kx ky fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/d5b11a6b06f7451143bbb4248c7503ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*sYOEjGlX-6J3tyO_3SWnZQ.gif"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">每个主题中的关键字分布以红色显示</figcaption></figure><p id="abb8" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">源代码可以在<a class="ae lx" href="https://github.com/ShahrzadH/Insight_Project_SHV" rel="noopener ugc nofollow" target="_blank"> GitRepo </a>上找到。我期待听到任何反馈或问题。</p></div></div>    
</body>
</html>