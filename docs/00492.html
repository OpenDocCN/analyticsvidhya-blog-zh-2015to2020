<html>
<head>
<title>Bi-directional RNN &amp; Basics of LSTM and GRU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">双向RNN &amp; LSTM和GRU的基础</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bi-directional-rnn-basics-of-lstm-and-gru-e114aa4779bb?source=collection_archive---------0-----------------------#2019-07-09">https://medium.com/analytics-vidhya/bi-directional-rnn-basics-of-lstm-and-gru-e114aa4779bb?source=collection_archive---------0-----------------------#2019-07-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1e3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我之前关于<a class="ae jd" rel="noopener" href="/@madhuramiah/deep-learning-recurrent-neural-networks-463dbb9db8be">递归神经网络</a>的博客中，我们讨论了消失和爆炸梯度。在这篇博客中，我们将讨论如何处理这些问题，以及我们将使用什么模型。除此之外，我们还将讨论双向RNN以及它们的使用场合。</p><p id="af68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我继续讨论双向RNN之前，让我们稍微讨论一下在线-离线模式。在文本摘要中，你基本上需要看到整个文本，然后才能对其进行摘要。这就是所谓的'<strong class="ih hj"> <em class="je">离线</em> </strong>模式。同时，当您在撰写电子邮件时考虑自动撰写功能时，它不能访问任何未来的单词，而只能访问以前的单词(序列)。这种类型叫做'<strong class="ih hj"> <em class="je">'在线</em> </strong>'模式。当我们考虑离线序列模型时，虽然我们事先知道整篇文章，但RNN只是从头到尾一个字一个字地学习。但是，如果模型还能知道未来的单词，那就更好了，这样它就能更有效地解决问题。对于这种情况，我们使用<strong class="ih hj"> <em class="je">双向RNN的</em> </strong>。</p><h2 id="e203" class="jf jg hi bd jh ji jj jk jl jm jn jo jp iq jq jr js iu jt ju jv iy jw jx jy jz bi translated">双向递归神经网络；</h2><p id="5555" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kc is it iu kd iw ix iy ke ja jb jc hb bi translated">在双向RNN中，我们考虑两个独立的序列。一个从右到左，另一个以相反的顺序。但是，现在的问题是，你如何将两个RNN结合在一起。看下图就清楚明白了。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es kf"><img src="../Images/adc1b1eedca41b6427c91c577599fc36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*sBZq013xGoFvyH5e0sXtdg.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">用于单词序列的双向RNN</figcaption></figure><p id="b791" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑单词序列“我爱芒果汁”。前向层将这样馈送序列。但是，后一层将按照相反的顺序“果汁芒果爱我”来馈送序列。现在，输出将通过每次连接单词序列并相应地生成权重来生成。这也可以用于词性标注问题。</p><h2 id="2480" class="jf jg hi bd jh ji jj jk jl jm jn jo jp iq jq jr js iu jt ju jv iy jw jx jy jz bi translated">LSTM(长短期记忆)</h2><p id="46d0" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kc is it iu kd iw ix iy ke ja jb jc hb bi translated">当我们有一个小的RNN，我们将能够有效地使用RNN，因为没有消失梯度的问题。但是，当我们考虑使用长RNN的没有太多我们能做的与传统的RNN的，因此它没有被广泛使用。这就是导致发现LSTM氏病的原因，该病基本上使用稍微不同的神经元结构。这是基于一个基本的想法创建的——即使序列非常大，渐变也不应该消失。</p><ol class=""><li id="db89" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated">在LSTM，我们将一个神经元称为<strong class="ih hj"> <em class="je">细胞。</em> </strong>在传统的RNN中，模型能够记住某些东西的唯一方式是通过更新隐藏状态和它们各自的权重。但是，在LSTM，这个问题是通过使用一个用于学习和记忆任务的显性记忆单元来解决的。它储存与学习相关的信息。</li><li id="10b1" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">它还使用称为<strong class="ih hj"> <em class="je">【门控机制】</em> </strong>的东西来管理网络存储的信息——如果它必须将信息传递到下一层或忘记它拥有的信息<strong class="ih hj"> <em class="je">。</em> </strong></li><li id="2842" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated"><strong class="ih hj"> <em class="je">恒定误差转盘</em> </strong>是LSTM的另一个很重要的特点。它允许LSTM在传播时有一个平滑和不间断的梯度流。</li></ol><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lf"><img src="../Images/b31b872bfe1a35bb8d65fe8e1d9de73f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aqUTrEWCmkxD90cv0qQwuw.png"/></div></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">LSTM建筑</figcaption></figure><ol class=""><li id="8b8e" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated">这个大矩形框叫做“<strong class="ih hj"> <em class="je">单元格</em> </strong>”，它取时间t的一个<strong class="ih hj"> <em class="je">输入x(t) </em> </strong>，一个<strong class="ih hj"> <em class="je">前一个隐藏层h(t-1)和一个前一个单元格状态c(t-1) </em> </strong>。细胞状态只不过是外显的记忆单位。</li><li id="adac" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">该单元给出2个输出- 1是隐藏状态h(t)的输出，另一个是在任何给定时间t的单元状态c(t)的输出。</li><li id="0d4a" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">恒定误差转盘负责将梯度从c(t-1)平滑地转移到c(t)</li><li id="9278" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">sigmoid函数将输出介于0和1之间的值，而tanh函数将输出介于-1和1之间的值。这是我们将在LSTM使用的两个主要激活功能。</li><li id="e9bc" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">我们将来自x(t)和h(t-1)的输入组合成一个sigmoid激活函数，并将其与先前的单元状态c(t-1)进行乘法运算。这种乘法运算称为“<strong class="ih hj"> <em class="je">门</em> </strong>”。如果sigmoid函数的值接近于1，那么乘法将得到接近于c(t-1)的值，这意味着只擦除以前的一小部分记忆，但保留大部分记忆。相反，如果sigmoid函数接近于0，那么乘法将产生接近于0的值。这意味着从先前的单元状态(存储器)中擦除几乎所有的内容。这整个部分叫做<strong class="ih hj">‘忘门’</strong></li><li id="fbe3" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">下一个门叫做“<strong class="ih hj">更新门</strong>，它使用一个sigmoid和一个tanh函数，两个门都有一个乘法门，后面跟着一个加法门，输出来自“忘记门”。“tanh”函数控制下一个单元状态的值增加或减少多少。sigmoid函数决定应该向新的单元状态c(t)写入多少信息。</li><li id="8d87" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">下一个也是最后一个门称为“<strong class="ih hj">输出门</strong>”。这将具有一个sigmoid函数，其后是一个具有tanh激活函数的乘法门，从而将值释放到前馈侧和递归侧的隐藏状态。这里，sigmoid函数和tanh函数的值越高，传输到下一个隐藏状态h(t)的值就越高。</li><li id="84a7" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">在LSTM，你可以看到所有的3 sigmoid和1 tanh激活函数，其输入是h(t-1)和x(t)的连接，具有不同的权重，比如w(f)，w(i)，w(c)和w(o)。那么训练LSTM模型所需的总参数是正常RNN的4倍。因此，计算成本非常高。为了解决这个问题，他们发明了一种叫做GRU的东西。</li></ol><h2 id="a2fc" class="jf jg hi bd jh ji jj jk jl jm jn jo jp iq jq jr js iu jt ju jv iy jw jx jy jz bi translated">门控循环单元(GRU)</h2><ol class=""><li id="1c98" class="kr ks hi ih b ii ka im kb iq lk iu ll iy lm jc kw kx ky kz bi translated">这是最近在2014年成立的，他们减少了来自LSTM的参数数量，但为了以防GRU不能很好地工作，我们将不得不回滚到LSTM。</li><li id="a9aa" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">在GRU，有<strong class="ih hj"> <em class="je">没有明确的记忆单位。</em> </strong>记忆单元是随着网络结合在一起的。</li><li id="4928" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">GRU中没有遗忘门和更新门，它们被组合在一起，从而减少了参数的数量。</li><li id="826c" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">当比较GRU和LSTM时，它表现良好，但在准确性上可能略有下降。但是我们仍然具有较少数量的可训练参数，这使得使用起来很有利。</li></ol><h2 id="5cd6" class="jf jg hi bd jh ji jj jk jl jm jn jo jp iq jq jr js iu jt ju jv iy jw jx jy jz bi translated"><strong class="ak">结论:</strong></h2><p id="0cce" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kc is it iu kd iw ix iy ke ja jb jc hb bi translated">我们谈到了双向RNN氏症。但是现在，大多数香草RNN被LSTM和GRU所取代。在这项发明之后，我们在以极其有效的方式处理序列数据方面取得了飞跃。在我的博客中，我将解释如何在Keras中使用RNN的词性标注应用程序。</p><p id="c2d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望你喜欢我的博客。感谢阅读:)请在下面留下您的评论或问题，或者在<a class="ae jd" href="https://www.linkedin.com/in/madhu-ramiah-a66a4b48/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我</p></div></div>    
</body>
</html>