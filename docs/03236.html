<html>
<head>
<title>Visualizing the MNIST data-set(cont.)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可视化MNIST数据集(续。)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/visualizing-the-mnist-data-set-cont-3b6e6030f70b?source=collection_archive---------18-----------------------#2020-01-21">https://medium.com/analytics-vidhya/visualizing-the-mnist-data-set-cont-3b6e6030f70b?source=collection_archive---------18-----------------------#2020-01-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7b886402d5215521c4ec9b5110a14147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RIU6bLVcjTFEl0ypXbOPew.png"/></div></div></figure><p id="986e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上一篇文章中，我们讨论了MNIST数据集以及如何使用PCA来可视化它。正如我们上次看到的，主成分分析没有提供一个非常明显的可视化。所以我们将使用<a class="ae jo" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> t-SNE </strong> </a> <strong class="is hj"> </strong>这是目前最先进的降维技术。但首先让我们讨论一下PCA的局限性。</p><h1 id="3c0a" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">PCA的局限性</h1><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/d3f30f90245d859e0c29f589fcd1f18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*FV1HGzE4R5IypXKRpVhr0A.jpeg"/></div></figure><p id="3876" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们的数据点形成一个圆形，在更高的维度<strong class="is hj">超球体，</strong>或接近它的结构，那么主成分分析将不会很好地工作。考虑右侧所示的数据点。在这里，它们形成了一个完整的圆，我们将它简化为1维，但在我们旋转轴并找到两个本征向量后，我们会注意到它们中任何一个上的投影点的方差将完全相等，并且将是总方差的50%。在这种情况下，我们不能比较这两个向量来确定哪一个将是更好的投影点，以便具有投影点的最大可能方差，并且大量信息总是丢失。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/04244a2e65d1bcd0e5e0e4fe0a7869c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*wjucmLqAy1OR6BkgD59MeQ.jpeg"/></div></figure><p id="4628" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于PCA致力于方差最大化，<strong class="is hj">它试图保持数据的全局结构</strong>。它不关心数据中存在的局部结构或聚类。假设我们有一个像左边这样的分布，其中也有局部集群。如果我们想使用PCA将此数据转换为1d，我们可能会将数据投影到v1上，但可以观察到，当我们将数据点投影到v1上时，在v2方向上出现的两个群集将最终在我们的新轴v1上聚集在一起。我们可能无法在1d中区分哪个数据点属于哪个聚类。</p><p id="121e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PCA的另一个限制是它<strong class="is hj">并不总是保持特征</strong>之间的关系。例如，考虑以下数据点的分布。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/7a7cc08637192dc46e3dc68d9bf0d7ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*TFvLcAvyJrMzWXZdIV_THg.jpeg"/></div></figure><p id="e614" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个数据分布中我们可以清楚地看到它是一个正弦函数。但是在我们应用PCA将其降维后，我们会得到v1作为最大方差的方向。但是，一旦我们将数据点投影到v1上，我们会看到我们的点变得有些等距，并失去了正弦函数的性质。一般来说，每当我们从n维到d维时，我们经常会丢失变量之间的许多这样的关系。</p><h1 id="3f31" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">t分布随机邻居嵌入</h1><p id="3aef" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">t-SNE或t-分布式随机邻居嵌入是目前最好的降维技术之一，也是最先进的技术。它是由范德马滕和辛顿在2008年首次提出的。在这里可以看到SNE霸王龙的实时影像<a class="ae jo" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank"/>。虽然使用起来可能很简单，但背后的数学原理实际上相当复杂。对于想深入数学的人，我强烈推荐这个<a class="ae jo" href="https://www.youtube.com/watch?v=ohQXphVSEQM&amp;t=33s" rel="noopener ugc nofollow" target="_blank">视频</a>。但是在实际应用中，我们可以不去理解数学。我们需要完美地调整它的两个最重要的参数，以获得良好的可视化，即<strong class="is hj">困惑</strong>和迭代次数。</p><p id="f61d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">t-SNE中的t代表学生的t分布。这是一种类似于正态分布的分布，因为它呈钟形，但尾部较重。换句话说，t分布比正态分布有更大的机会出现极值，因此尾部更厚。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/4824ac3fc844f6fe8c1d770a97fb0bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*R7KSUuxyD9_eg3S2Yc0LJA.png"/></div></figure><p id="2ae4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">随机</strong>表示t-SNE的过程或算法是概率性的，而不是确定性的。这意味着我们可能在多次运行中得到相同输入的不同输出。</p><p id="100e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一个点的邻域是靠近它的点的数目。有一个名为“<strong class="is hj">perfusion</strong>的可调参数，它粗略地说明了如何在数据的局部和全局方面平衡注意力。换句话说，我们可以认为，困惑定义了有多少(大约)点可能位于邻域内。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es la"><img src="../Images/e6ea04d14a552961e31174b26e3f0486.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*lLNb3rUAg2p0n68qWgz2Ww.jpeg"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">xi附近</figcaption></figure><p id="7a1b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">粗略地说，嵌入基本上意味着<strong class="is hj">将存在于更高维度(比如d维)中的每个点</strong>放置在更低维度的空间(比如2维)中。点以这样一种方式放置，即对于每个点，它的邻域甚至在较低的维度中也被保留。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/208787eee934b2c0ac2d6dc5e3eea2dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NggrWFcUXoWMXDPbnlSypw.jpeg"/></div></div></figure><p id="87b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们的MNIST数据集上应用t-SNE后，我们可能会得到以下结果。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/40d73e2580623b05244eb571f4b34626.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*zqj0ZnVVFArhaGy2691RoQ.jpeg"/></div></figure><p id="dcc0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一个比PCA更好的可视化，因为现在我们可以清楚地看到每个点，没有太多重叠。(注意，上述内容仅使用数据点样本进行训练)。在更精确地调整参数后，我们可能会得到如下结果</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/2971dfa6ed9a1a3a280d6296bfff669b.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*CYnHCyOP_3rnOloUVmLKYQ.jpeg"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">Christopher Olah的博客中的MNIST形象</figcaption></figure><h1 id="4f79" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">t-SNE之前的PCA</h1><p id="8322" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">由于t-SNE需要大量的计算能力和时间，一个试探<strong class="is hj">可能在应用t-SNE之前使用PCA。PCA将做的是，它将只保留那些解释大部分累积方差(假设90%)的特征，并删除其余的特征。在下面的可视化中，我首先将维度从784减少到350，因为这350个特征能够解释90%的差异，然后我运行t-SNE。正如可以观察到的，这次获得了更好的结果。请点击<a class="ae jo" href="https://github.com/vedanshsharma/Visualizing-MNIST-data-set-" rel="noopener ugc nofollow" target="_blank">链接</a>来看看我是如何实现的。</strong></p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es li"><img src="../Images/06022e3fdf97f19b934cc7126e50d64a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*U6xCDabFCE_0sgxh2gI-1g.jpeg"/></div></figure><h1 id="6ff6" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">履行</h1><p id="d82f" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">关于pca的实现，请点击<a class="ae jo" href="https://github.com/vedanshsharma/Visualizing-MNIST-data-set-" rel="noopener ugc nofollow" target="_blank">链接</a>我已经使用scikit-learn实现了t-SNE，我还实现了pca降维。</p></div></div>    
</body>
</html>