<html>
<head>
<title>Fine Tuning BERT for NER on CoNLL 2003 dataset with TF 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TF 2.0在CoNLL 2003数据集上为NER微调BERT</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fine-tuning-bert-for-ner-on-conll-2003-dataset-with-tf-2-2-0-2f242ca2ce06?source=collection_archive---------0-----------------------#2020-09-20">https://medium.com/analytics-vidhya/fine-tuning-bert-for-ner-on-conll-2003-dataset-with-tf-2-2-0-2f242ca2ce06?source=collection_archive---------0-----------------------#2020-09-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9406" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇博客详细介绍了为句子的命名实体识别(NER)标记微调BERT预训练模型的步骤(<a class="ae jd" href="https://www.clips.uantwerpen.be/conll2003/ner/" rel="noopener ugc nofollow" target="_blank"> CoNLL-2003数据集</a>)。如果你刚到NER，我推荐你先浏览一下这个<a class="ae jd" rel="noopener" href="/analytics-vidhya/ner-tensorflow-2-2-0-9f10dcf5a0a"> NER的Tensorflow 2.2.0 </a>博客。</p><p id="73c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看如何使用BERT预训练模型来完成NER任务。在这篇博客中，我不打算深入探讨BERT的基础知识。要了解更多关于伯特的信息，请阅读这个<a class="ae jd" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">博客。</a></p><p id="10cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">杰瑞米·霍华德和塞巴斯蒂安·鲁德在<a class="ae jd" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">文本分类通用语言模型微调</a>中展示了迁移学习如何应用于NLP任务并显著提高性能。我们使用BERT预训练模型，在其上添加一个神经网络层，并针对我们的NER任务对其进行微调。</p><p id="d476" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> CoNLL数据集:</strong> <a class="ae jd" href="https://www.clips.uantwerpen.be/conll2003/ner/" rel="noopener ugc nofollow" target="_blank"> CoNLL-2003 </a> <a class="ae jd" href="https://www.clips.uantwerpen.be/conll2003/ner/" rel="noopener ugc nofollow" target="_blank">数据集</a>包括1393篇英语和909篇德语新闻文章。我们将会关注英国的数据。CoNLL-2003数据文件包含由一个空格分隔的四列。每个单词都被放在单独的一行，每个句子后面都有一个空行。每行的第一项是单词，第二项是词性(POS)标记，第三项是语法块标记，第四项是命名实体标记。我们将在任务中使用命名实体标记。组块标签和命名实体标签具有I-TYPE格式，这意味着单词在TYPE类型的短语内。只有当两个相同类型的短语紧随其后时，第二个短语的第一个单词才会有标签B-TYPE，以表明它开始了一个新短语。标签为O的单词不是短语的一部分。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/654ffbd82f183875f47ed8ba7eec6a56.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*U5yb0cPojDlRSfwDXr936w.png"/></div></figure><p id="7f39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为BERT预处理数据</strong>:让我们看看将我们的数据转换成BERT模型所期望的格式的步骤。我们再来看train.txt中的前两句，第一句是“欧盟拒绝德国号召抵制英国羊肉。”第二句是“彼得·布莱克本”。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="50f1" class="jr js hi jn b fi jt ju l jv jw">-DOCSTART- -X- -X- O<br/>EU NNP B-NP B-ORG<br/>rejects VBZ B-VP O<br/>German JJ B-NP B-MISC<br/>call NN I-NP O<br/>to TO B-VP O<br/>boycott VB I-VP O<br/>British JJ B-NP B-MISC<br/>lamb NN I-NP O<br/>. . O O</span><span id="7b99" class="jr js hi jn b fi jx ju l jv jw">Peter NNP B-NP B-PER<br/>Blackburn NNP I-NP I-PER</span></pre><p id="4ae2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NERProcessor类中的get_train_examples为BERT做了必要的预处理。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="5d7b" class="jr js hi jn b fi jt ju l jv jw">train_examples = processor.get_train_examples(args.data_dir)</span></pre><p id="1e01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在预处理中完成以下步骤:</p><ol class=""><li id="b93b" class="jy jz hi ih b ii ij im in iq ka iu kb iy kc jc kd ke kf kg bi translated">在句首添加“CLS”标记。</li><li id="ae3c" class="jy jz hi ih b ii kh im ki iq kj iu kk iy kl jc kd ke kf kg bi translated">在句尾添加“SEP”标记。</li><li id="d43e" class="jy jz hi ih b ii kh im ki iq kj iu kk iy kl jc kd ke kf kg bi translated">为max_seq_length填充序列和标签</li><li id="b07f" class="jy jz hi ih b ii kh im ki iq kj iu kk iy kl jc kd ke kf kg bi translated">input_ids:使用BERT记号赋予器将记号转换成id</li><li id="48cf" class="jy jz hi ih b ii kh im ki iq kj iu kk iy kl jc kd ke kf kg bi translated">input_mask:指示序列中哪些元素是标记，哪些是填充元素</li><li id="c2b3" class="jy jz hi ih b ii kh im ki iq kj iu kk iy kl jc kd ke kf kg bi translated">segment_ids : 0表示第一个序列，1表示第二个序列。我们有一个单一的序列，因此句子中的所有标记都有0作为标记标识</li><li id="97d5" class="jy jz hi ih b ii kh im ki iq kj iu kk iy kl jc kd ke kf kg bi translated">label_ids:每个标签的整数值</li><li id="85ce" class="jy jz hi ih b ii kh im ki iq kj iu kk iy kl jc kd ke kf kg bi translated">label_mask : True表示哪些元素是标签，False表示填充元素</li><li id="4353" class="jy jz hi ih b ii kh im ki iq kj iu kk iy kl jc kd ke kf kg bi translated">valid_ids: BERT使用单词块标记化。使用BERT标记化，句子(“欧盟拒绝德国呼吁抵制英国羊肉。”被标记为</li></ol><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="5b43" class="jr js hi jn b fi jt ju l jv jw">['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'la', '##mb', '.']</span></pre><p id="9bef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">单词lamb被分成两个标记——“la”和“##mb”。在这种情况下，我们为第一次分割添加了标签，而第二次分割没有添加任何标签。该信息在valid_ids中捕获。' valid_ids '的标记' la '为1，标记' ##mb '为0。填充的令牌在valid_ids中被赋予1。</p><p id="0dfc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看一个输入句子['欧盟'，'拒绝'，'德国'，'打电话'，'抵制'，'英国'，'羔羊'，'.']使用convert_examples_to_features方法进行转换。</p><p id="5299" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于max_seq_length为16(为简单起见选择16)的上述句子，输入_标识(带有‘CLS’和‘分离’标记)、输入_掩码、段_标识、标签_标识、标签_掩码和有效_标识如下所示:</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="a724" class="jr js hi jn b fi jt ju l jv jw">input_ids - [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102, 0, 0, 0, 0]</span><span id="69d6" class="jr js hi jn b fi jx ju l jv jw">input_mask - [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</span><span id="be77" class="jr js hi jn b fi jx ju l jv jw">segment_ids - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span><span id="d9d1" class="jr js hi jn b fi jx ju l jv jw">label_ids - [10, 6, 1, 2, 1, 1, 1, 2, 1, 1, 11, 0, 0, 0, 0, 0]</span><span id="3bf8" class="jr js hi jn b fi jx ju l jv jw">label_mask - [True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False]</span><span id="dc54" class="jr js hi jn b fi jx ju l jv jw">valid_ids - [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1] # 0 for '##mb'<br/></span></pre><p id="cf35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">将特征转换为Tensorflow数据集</strong> : batched_train_data已混洗大小为:[batch_size，max_seq_length]的训练数据。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="de60" class="jr js hi jn b fi jt ju l jv jw">all_input_ids = tf.data.Dataset.from_tensor_slices(np.asarray<br/>([f.input_ids for f in train_features]))</span><span id="a9e7" class="jr js hi jn b fi jx ju l jv jw">all_input_mask = tf.data.Dataset.from_tensor_slices(np.asarray<br/>([f.input_mask for f in train_features]))</span><span id="e80e" class="jr js hi jn b fi jx ju l jv jw">all_segment_ids = tf.data.Dataset.from_tensor_slices(np.asarray<br/>([f.segment_ids for f in train_features]))</span><span id="0c91" class="jr js hi jn b fi jx ju l jv jw">all_valid_ids = tf.data.Dataset.from_tensor_slices(np.asarray<br/>([f.valid_ids for f in train_features]))</span><span id="2a4b" class="jr js hi jn b fi jx ju l jv jw">all_label_mask = tf.data.Dataset.from_tensor_slices(np.asarray<br/>([f.label_mask for f in train_features]))</span><span id="8f30" class="jr js hi jn b fi jx ju l jv jw">all_label_ids = tf.data.Dataset.from_tensor_slices(np.asarray<br/>([f.label_id for f in train_features]))</span><span id="100a" class="jr js hi jn b fi jx ju l jv jw"># Dataset using tf.data<br/>train_data = tf.data.Dataset.zip((all_input_ids, all_input_mask, all_segment_ids, all_valid_ids, all_label_ids,all_label_mask))</span><span id="81c7" class="jr js hi jn b fi jx ju l jv jw">shuffled_train_data = train_data.shuffle(buffer_size=int(len(train_features) * 0.1),seed = args.seed, reshuffle_each_iteration=True)</span><span id="e94c" class="jr js hi jn b fi jx ju l jv jw">batched_train_data = shuffled_train_data.batch(args.train_batch_size)</span></pre><p id="7e6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型建筑:</strong></p><p id="0ac9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用bert-base-cased模型，它有12层变换编码器和“cased”序列。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es km"><img src="../Images/92601320ef3adfd56db178cd5242c3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*UhPgdBfwrlQKtKONmocsNQ.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">来源:http://jalammar.github.io/illustrated-bert/<a class="ae jd" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="7b2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在BertNer类的init方法中，我们创建了一个BertModel对象，使用tf.train.Checkpoint加载模型权重。因此，我们将从BertModel获取sequence_output。实际上，我们应该选择哪一层来获得最佳表现呢？作者在他们的论文中展示了不同选择的性能。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kr"><img src="../Images/431bff3af0581f9baf1d55d20f185445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JhZ5xoOBW5Fx3OZi0bIQjw.png"/></div></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">来源:<a class="ae jd" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-bert/</a></figcaption></figure><p id="a365" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，对于我们的实验，我们将从最后一层的表现。</p><p id="d6dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些标记然后被馈送到具有num_label单元的密集层(除了‘CLS’和‘SEP’标记之外，数据集中存在的标记的数量),以获得每个标记的预测。BERT使用了单词块标记器，它将一些单词分解成子单词，在这种情况下，我们只需要预测单词的第一个标记。对于子词，valid_ids为0，在拆分词的情况下，我们仅使用valid_ids来获得对词和第一个子词的预测。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="7392" class="jr js hi jn b fi jt ju l jv jw">class BertNer(tf.keras.Model):<br/>  def __init__(self, bert_model,float_type, num_labels,   <br/>  max_seq_length, final_layer_initializer=None):<br/>    super(BertNer, self).__init__()<br/>    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,),   <br/>    dtype=tf.int32, name='input_word_ids')<br/>    input_mask = tf.keras.layers.Input(shape=(max_seq_length,),     <br/>    dtype=tf.int32, name='input_mask')<br/>    input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), <br/>    dtype=tf.int32, name='input_type_ids')<br/>    <br/>    bert_config = BertConfig.from_json_file<br/>      (os.path.join(bert_model, "bert_config.json"))<br/>    bert_layer = BertModel(config=bert_config,<br/>      float_type=float_type)<br/>    _, sequence_output = bert_layer(input_word_ids,  <br/>    input_mask,input_type_ids)</span><span id="8f40" class="jr js hi jn b fi jx ju l jv jw">    self.bert = tf.keras.Model(inputs=[input_word_ids, input_mask,  <br/>    input_type_ids],outputs=[sequence_output])</span><span id="2310" class="jr js hi jn b fi jx ju l jv jw">    if type(bert_model) == str:<br/>      init_checkpoint = os.path.join(bert_model,"bert_model.ckpt")     <br/>      checkpoint = tf.train.Checkpoint(model=self.bert)<br/>      checkpoint.restore(init_checkpoint).assert_<br/>      existing_objects_matched()<br/>   <br/>    self.dropout = tf.keras.layers.Dropout(    <br/>    rate=bert_config.hidden_dropout_prob)</span><span id="f968" class="jr js hi jn b fi jx ju l jv jw">    if final_layer_initializer is not None: <br/>      initializer = final_layer_initializer<br/>    else:<br/>      initializer = tf.keras.initializers.TruncatedNormal(   <br/>      stddev=bert_config.initializer_range)</span><span id="a032" class="jr js hi jn b fi jx ju l jv jw">    self.classifier = tf.keras.layers.Dense(num_labels, <br/>    kernel_initializer=initializer, name='output', dtype=float_type)</span><span id="41bc" class="jr js hi jn b fi jx ju l jv jw">def call(self, input_word_ids,input_mask=None,input_type_ids=None,<br/>valid_ids=None, **kwargs):<br/>    sequence_output = self.bert([input_word_ids, input_mask,  <br/>    input_type_ids],**kwargs)<br/>    valid_output = []<br/>    for i in range(sequence_output.shape[0]): <br/>      r = 0<br/>      temp = []<br/>      for j in range(sequence_output.shape[1]):<br/>        if valid_ids[i][j] == 1:<br/>           temp = temp + [sequence_output[i][j]]<br/>        else:<br/>           r += 1<br/>      temp = temp + r * [tf.zeros_like(sequence_output[i][j])]<br/>      valid_output = valid_output + temp<br/>    valid_output = tf.reshape(tf.stack(valid_output)<br/>    ,sequence_output.shape)<br/>    sequence_output = self.dropout(valid_output,  <br/>    training=kwargs.get('training', False)) <br/>    logits = self.classifier(sequence_output)<br/>    return logits</span></pre><h2 id="c04c" class="jr js hi bd kw kx ky kz la lb lc ld le iq lf lg lh iu li lj lk iy ll lm ln lo bi translated">优化器、学习率调度器和损失函数:</h2><p id="9b8a" class="pw-post-body-paragraph if ig hi ih b ii lp ik il im lq io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">我们使用AdamWeightDecay优化器，具有多项式衰减学习率调度。我们使用稀疏分类交叉熵损失。</p><h2 id="31ed" class="jr js hi bd kw kx ky kz la lb lc ld le iq lf lg lh iu li lj lk iy ll lm ln lo bi translated">自定义训练循环:</h2><p id="fc6c" class="pw-post-body-paragraph if ig hi ih b ii lp ik il im lq io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">输入令牌、掩码和标签被传递给模型。使用优化器计算损耗和梯度并更新参数。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="a832" class="jr js hi jn b fi jt ju l jv jw">def train_step(input_ids, input_mask, segment_ids, valid_ids, label_ids,label_mask):<br/>  with tf.GradientTape() as tape:<br/>    logits = ner(input_ids, input_mask,segment_ids, valid_ids, <br/>    training=True) #batchsize, max_seq_length, num_labels<br/>    label_ids_masked = tf.boolean_mask(label_ids,label_mask)        <br/>    logits_masked = tf.boolean_mask(logits,label_mask)<br/>    loss = loss_fct(label_ids_masked, logits_masked)<br/>  grads = tape.gradient(loss, ner.trainable_variables)<br/>  optimizer.apply_gradients(list(zip(grads,       <br/>  ner.trainable_variables)))<br/>  return loss</span><span id="2ca9" class="jr js hi jn b fi jx ju l jv jw">for epoch in epoch_bar:<br/>    for (input_ids, input_mask, segment_ids, valid_ids, <br/>    label_ids,label_mask) in progress_bar(batched_train_data, <br/>    total=pb_max_len, parent=epoch_bar):<br/>      loss = train_step(input_ids, input_mask, segment_ids,<br/>      valid_ids, label_ids,label_mask)<br/>      loss_metric(loss)<br/>      epoch_bar.child.comment = f'loss : {loss_metric.result()}'<br/>    loss_metric.reset_states()</span></pre><h2 id="fafc" class="jr js hi bd kw kx ky kz la lb lc ld le iq lf lg lh iu li lj lk iy ll lm ln lo bi translated">对有效数据集的评估:</h2><p id="9fff" class="pw-post-body-paragraph if ig hi ih b ii lp ik il im lq io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">加载模型，将数据预处理成所需的格式。对有效数据集进行预测。我们使用精确度、召回率和f1分数来评估模型的性能。我们使用seqeval包。seqeval是一个用于序列标记评估的Python框架。seqeval可以评估命名实体识别、词性标注、语义角色标注等组块任务的性能。classification_report metric构建显示主要分类指标的文本报告。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="4da3" class="jr js hi jn b fi jt ju l jv jw">if args.do_eval:<br/>  # load tokenizer<br/>  tokenizer = FullTokenizer(os.path.join(args.output_dir, <br/>  "vocab.txt"), args.do_lower_case)<br/>  # model build hack : fix<br/>  config = json.load(open(os.path.join(args.output_dir,<br/>  "bert_config.json")))<br/>  ner = BertNer(config, tf.float32, num_labels, <br/>  args.max_seq_length)<br/>  ids = tf.ones((1,128),dtype=tf.int64)<br/>  _ = ner(ids,ids,ids,ids, training=False)<br/>  ner.load_weights(os.path.join(args.output_dir,"model.h5"))</span><span id="2d73" class="jr js hi jn b fi jx ju l jv jw">  # load test or development set based on argsK<br/>  if args.eval_on == "dev":<br/>    eval_examples = processor.get_dev_examples(args.data_dir)<br/>  elif args.eval_on == "test":<br/>    eval_examples = processor.get_test_examples(args.data_dir)</span><span id="a450" class="jr js hi jn b fi jx ju l jv jw">  eval_features = convert_examples_to_features(<br/>  eval_examples, label_list, args.max_seq_length, tokenizer)<br/>  all_input_ids = tf.data.Dataset.from_tensor_slices(<br/>  np.asarray([f.input_ids for f in eval_features]))  <br/>  all_input_mask = tf.data.Dataset.from_tensor_slices(<br/>  np.asarray([f.input_mask for f in eval_features]))<br/>  all_segment_ids = tf.data.Dataset.from_tensor_slices(<br/>  np.asarray([f.segment_ids for f in eval_features]))<br/>  all_valid_ids = tf.data.Dataset.from_tensor_slices(<br/>  np.asarray([f.valid_ids for f in eval_features]))<br/>  all_label_ids = tf.data.Dataset.from_tensor_slices(<br/>  np.asarray([f.label_id for f in eval_features]))</span><span id="b557" class="jr js hi jn b fi jx ju l jv jw">  eval_data = tf.data.Dataset.zip((all_input_ids, all_input_mask,        <br/>  all_segment_ids, all_valid_ids, all_label_ids))<br/>  batched_eval_data = eval_data.batch(args.eval_batch_size)<br/>  <br/>  loss_metric = tf.keras.metrics.Mean()<br/>  epoch_bar = master_bar(range(1))<br/>  pb_max_len = math.ceil(<br/>  float(len(eval_features))/float(args.eval_batch_size))</span><span id="6ce4" class="jr js hi jn b fi jx ju l jv jw">  y_true = []<br/>  y_pred = []<br/>  label_map = {i : label for i, label in enumerate(label_list,1)}<br/>  for epoch in epoch_bar:<br/>    for (input_ids, input_mask, segment_ids, valid_ids, label_ids)<br/>    in progress_bar(batched_eval_data, total=pb_max_len,<br/>    parent=epoch_bar):<br/>      logits = ner(input_ids, input_mask, segment_ids, valid_ids,     <br/>      training=False)<br/>      logits = tf.argmax(logits,axis=2)<br/>      for i, label in enumerate(label_ids):<br/>        temp_1 = []<br/>        temp_2 = []<br/>        for j,m in enumerate(label):<br/>          if j == 0:<br/>            continue<br/>          elif label_ids[i][j].numpy() == len(label_map):     <br/>            y_true.append(temp_1)<br/>            y_pred.append(temp_2)<br/>            break<br/>          else:<br/>            temp_1.append(label_map[label_ids[i][j].numpy()])<br/>            temp_2.append(label_map[logits[i][j].numpy()])</span><span id="eb42" class="jr js hi jn b fi jx ju l jv jw">  report = classification_report(y_true, y_pred,digits=4)<br/>  output_eval_file = os.path.join(args.output_dir,<br/>  "eval_results.txt")<br/>  with open(output_eval_file, "w") as writer:<br/>    logger.info("***** Eval results *****")<br/>    logger.info("\n%s", report)<br/>    writer.write(report)</span></pre><p id="4026" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">https://github.com/bhuvanakundumani/BERT-NER-TF2.git的<a class="ae jd" href="https://github.com/bhuvanakundumani/BERT-NER-TF2.git" rel="noopener ugc nofollow" target="_blank">有这方面的源代码</a></p><p id="f506" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><p id="21a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【https://github.com/google-research/bert T4】—谷歌伯特源代码</p><div class="lu lv ez fb lw lx"><a href="https://github.com/kamalkraj/BERT-NER-TF/" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hj fi z dy mc ea eb md ed ef hh bi translated">卡马尔克拉伊/伯特-NER-TF</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">用谷歌伯特做的CoNLL-2003 NER！</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">github.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml jk lx"/></div></div></a></div><div class="lu lv ez fb lw lx"><a href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hj fi z dy mc ea eb md ed ef hh bi translated">首次使用BERT的可视化指南</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">翻译:俄罗斯在机器学习模型方面的进展一直在迅速加快，这种机器学习模型通过语言处理…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">jalammar.github.io</p></div></div><div class="mg l"><div class="mm l mi mj mk mg ml jk lx"/></div></div></a></div><p id="7697" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://github.com/tensorflow/models/blob/master/official/nlp/bert" rel="noopener ugc nofollow" target="_blank"> BERT </a> : <a class="ae jd" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a>Devlin等人，2018</p><p id="15ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://github.com/tensorflow/models/blob/master/official/nlp/transformer" rel="noopener ugc nofollow" target="_blank">Transformer for translation</a>:<a class="ae jd" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>瓦斯瓦尼等人，2017</p></div></div>    
</body>
</html>