<html>
<head>
<title>Reinforcement Learning — Part 02</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习—第二部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-part-02-299026af170e?source=collection_archive---------12-----------------------#2020-05-08">https://medium.com/analytics-vidhya/reinforcement-learning-part-02-299026af170e?source=collection_archive---------12-----------------------#2020-05-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4c49" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">多武装匪徒</h2></div><p id="7cd8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae jt" rel="noopener" href="/@farbluestar/reinforcement-learning-part-01-e6b78118422d">强化学习—第01部分</a> <br/> <a class="ae jt" rel="noopener" href="/@farbluestar/reinforcement-learning-part-03-355be5c7cae4">强化学习—第03部分</a></p><p id="f4a9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di">在本系列的前一篇文章</span>—<a class="ae jt" rel="noopener" href="/@farbluestar/reinforcement-learning-part-01-e6b78118422d">参见第1部分</a>——我们介绍了RL的基本概念和术语。如果您没有看到这篇文章，或者您不熟悉RL的基础知识，请先阅读它。我们将建立在那里介绍的概念之上。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ca"><img src="../Images/fe12f5f7682e2ea82714c19b6fe5264c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5fDVA7_LabJccih9dOBsuA.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">拉我的任何一只胳膊，你可能会得到一颗珍珠…或者一根刺！</figcaption></figure><p id="c4a4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我不是一个赌博的人，事实上，我从来没有去过赌场——好吧，我去过一次，当我听说他们有一个非常实惠的餐厅，有大量的食物！但如果你以前去过赌场，你可能会熟悉强盗机。这个想法是，你拉一只胳膊，如果你“幸运”，你可能会得到一些钱作为回报。我们这里要讨论的机器有多个手臂，就像图中的章鱼一样。</p><p id="71f0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个多臂强盗机的例子很简单，但是它包含了RL的所有核心思想。你可能会问，有多简单？我们可以将RL方法分为两大类:</p><ol class=""><li id="5843" class="ks kt hi iz b ja jb jd je jg ku jk kv jo kw js kx ky kz la bi translated"><strong class="iz hj">表格法</strong>:状态数量少，可能动作数量少的问题。用更好的术语来说，状态和动作<strong class="iz hj"> <em class="lb">空间</em> </strong>都很小。这些问题的值函数可以表示为表格，因此得名。我们通常可以很容易地找到这些问题的精确最优解。但是，属于这一类的现实生活问题非常有限。</li><li id="d3a3" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js kx ky kz la bi translated"><strong class="iz hj">近似方法</strong>:可以有任意多的状态和动作，通常很难处理的问题。对于属于这一类的系统，我们通常满足于近似的解决方案——而且有很多。</li></ol><p id="f60b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">多武装盗匪不仅属于第一类，而且只有一种状态。换句话说，环境的状态是固定的，我们在这里并不真正需要处理它，我们只需要专注于决定采取哪种行动，而不需要将我们的决定与特定的状态关联起来——这被称为<strong class="iz hj"> <em class="lb">非关联</em> </strong>设置。事不宜迟，让我们详细描述一下实验设置…</p><h1 id="baf6" class="lh li hi bd lj lk ll lm ln lo lp lq lr io ls ip lt ir lu is lv iu lw iv lx ly bi translated">k-武装土匪设置</h1><p id="2c2a" class="pw-post-body-paragraph ix iy hi iz b ja lz ij jc jd ma im jf jg mb ji jj jk mc jm jn jo md jq jr js hb bi translated">在这个场景中，我们有一个多臂的强盗机器——k臂。我们的动作仅仅是拉一下手臂，我们有k种可能的动作可供选择。每次我们采取一个行动(拉一只胳膊)，我们都会得到一个随机的奖励。一个行动的价值被定义为如果我们继续采取这个行动，我们会得到的预期回报。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es me"><img src="../Images/bdbd62ee2d1c68039b40324feb517d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*OpAmqyX40kyG20xJv6riqw@2x.png"/></div></figure><p id="d9d5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了理解这个等式，假设你有许多相似的机器。在时间t，你选择同样的手臂(动作a)并拉它。你会从每台机器上获得一些随机奖励。q*是这些奖励的平均值。从另一个角度来看，如果你多次重复这个动作，它就是每一时间步的回报。</p><p id="12b4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不幸的是，这个量对我们来说是未知的(这就是星号下标的意思，表示它是真实的动作值，而不是我们的估计值，通常用Q(a)表示)。如果行动值是已知的，我们将简单地坚持具有最大值的行动，并继续拉这只手臂——剥削。但事实并非如此。</p><p id="37dc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是探索拯救世界的地方。我们可以先选择一些随机的行为，当我们从这些行为中获得回报时，我们会尽力去估计它们的价值。这个评估过程可以通过简单地记录我们从每个行动中得到的回报来完成。然后我们计算每个时间点的平均回报。我们将很快讨论这个过程的细节。</p><blockquote class="mf mg mh"><p id="d2da" class="ix iy lb iz b ja jb ij jc jd je im jf mi jh ji jj mj jl jm jn mk jp jq jr js hb bi translated">这种估计动作值的方法称为<strong class="iz hj">样本平均法</strong>。</p></blockquote><p id="685d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们找到所有可能的动作时，我们最终会得到所有动作值的一些估计值。这是我们可以通过选择具有最大价值的行动来开始利用这些知识的地方。我们仍然需要继续探索，因为我们探索得越多，我们为每个行动收集的数据就越多，我们对行动价值的估计就越准确。一个现在看来是最佳的行动可能在以后变成次优。</p><blockquote class="mf mg mh"><p id="522a" class="ix iy lb iz b ja jb ij jc jd je im jf mi jh ji jj mj jl jm jn mk jp jq jr js hb bi translated">使用动作价值估算来决定选择哪个动作的方法被称为<strong class="iz hj"> <em class="hi">动作价值方法</em> </strong>。</p></blockquote><p id="d80a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于我们的bandit机器，我们将执行1000次拉动的<strong class="iz hj">运行</strong>。由于我们不能同时探索和利用——所谓的<strong class="iz hj"> <em class="lb">探索和利用</em> </strong>之间的冲突，我们将以一定的概率探索ε并利用剩余的时间。这叫<strong class="iz hj"><em class="lb">ε-贪心</em> </strong>法。我们基于我们的价值估计采取的行动被称为<strong class="iz hj"> <em class="lb">贪婪行动</em> </strong>。</p><p id="a85b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于这是一个随机过程，用不同的bandit机器进行多次运行(每次1000次拉动)将会产生不同的结果。我们需要取这些结果的平均值来捕捉学习算法的平均行为，不要被过程中的随机波动所迷惑。</p><blockquote class="mf mg mh"><p id="8005" class="ix iy lb iz b ja jb ij jc jd je im jf mi jh ji jj mj jl jm jn mk jp jq jr js hb bi translated">总结:⇨执行1000次操作中的1次运行，⇨执行2000次运行，取平均值。</p></blockquote><p id="6edd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">S&amp;B(萨顿和巴尔托的书)建议平均超过2000个不同的土匪机器。正如我们将在后面看到的，我发现从实用的角度来看，200已经足够了。现在，让我们把注意力转向估计行动价值的细节…</p><h1 id="c2ab" class="lh li hi bd lj lk ll lm ln lo lp lq lr io ls ip lt ir lu is lv iu lw iv lx ly bi translated">行动价值估计</h1><p id="540f" class="pw-post-body-paragraph ix iy hi iz b ja lz ij jc jd ma im jf jg mb ji jj jk mc jm jn jo md jq jr js hb bi translated">我在上面提到过，我们可以通过跟踪采取行动时获得的奖励来估计任何行动的价值，然后使用这些数据来计算每个时间步的平均值。这种方法有两个问题:</p><ol class=""><li id="6a58" class="ks kt hi iz b ja jb jd je jg ku jk kv jo kw js kx ky kz la bi translated">我们必须把所有的奖励都保存在内存中，如果我们有大量的时间步长，这就不太好了。</li><li id="e431" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js kx ky kz la bi translated">我们必须计算平均值，因此在每一个时间点上，我们要计算所有奖励的总和。这显然是浪费CPU时间。</li></ol><p id="5f6f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们只能在内存中保存最新的动作值，那么效率会更高，并且在每个时间步，我们只需执行简单的修正来获得新值。这就是所谓的<strong class="iz hj"> <em class="lb">移动平均线</em> </strong>(跑步、移动或滚动都是你会听到的术语！).</p><p id="4c34" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">S&amp;B的第2.4节对这种方法有一个简单而详细的推导，它是直截了当的。我建议你至少自己推导一次。结果如下:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ml"><img src="../Images/76368ba284be7658e656dafec2a6562f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FyDs-kkYKhSpk5iwSuC7zA@2x.png"/></div></div></figure><p id="7b83" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里值得注意的是我们如何解释和推广这个简单的公式。请注意以下几点:</p><ul class=""><li id="1f88" class="ks kt hi iz b ja jb jd je jg ku jk kv jo kw js mm ky kz la bi translated">Rn - Qn是我们收到的实际奖励和我们目前的估计之间的差异。它本质上是一个<strong class="iz hj"> <em class="lb">错误术语</em> </strong>。</li><li id="43ca" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">我们实际上是在每个时间步对Qn做了微小的修正。</li><li id="40f9" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">1/n是<strong class="iz hj"> <em class="lb">步长</em></strong>——它决定了我们使用多少误差来校正我们当前的估计。它有时用α表示。</li><li id="653d" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">步长逐级变化，n越大越小。随着n趋于无穷大，我们知道Qn现在是正确的估计值，步长为零。</li></ul><h2 id="a472" class="mn li hi bd lj mo mp mq ln mr ms mt lr jg mu mv lt jk mw mx lv jo my mz lx na bi translated">加权平均值</h2><p id="5a5d" class="pw-post-body-paragraph ix iy hi iz b ja lz ij jc jd ma im jf jg mb ji jj jk mc jm jn jo md jq jr js hb bi translated">步长为1/n的方法适用于稳态问题。对于非平稳问题(回报概率是时间的函数)，给予最近的回报更大的权重(相对于平稳情况下的较小权重)更有意义。实现这一点的一个流行的方法是使用恒定的步长，这样它的影响将随着我们在时间上的前进而被放大。一般来说，该等式看起来像:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es nb"><img src="../Images/905176e3f0cc498d0a9253eb7bce2b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YoGNzZX9HxEwikwL9kKHRA@2x.png"/></div></div></figure><p id="3021" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们递归地应用这个(见S&amp;B 2.5)，我们得到:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es nc"><img src="../Images/38c274690ce05af54fbba941259ffa97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*otTs3_Mep5jGHEnBgXO9tQ@2x.png"/></div></div></figure><p id="7175" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里我们可以看到:</p><ul class=""><li id="fad3" class="ks kt hi iz b ja jb jd je jg ku jk kv jo kw js mm ky kz la bi translated">随着我们追溯到更远的过去，权重呈指数衰减——因此，它有时被称为<strong class="iz hj"> <em class="lb">指数最近加权平均值</em> </strong>。</li><li id="c256" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">当n趋于无穷大时，步长1/n保证收敛到实际动作值。这不是使用恒定步长的情况。收敛的理论条件见S&amp;B方程2.7。</li></ul><p id="0b03" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们找点乐子，实现这些想法吧…</p><h1 id="29b1" class="lh li hi bd lj lk ll lm ln lo lp lq lr io ls ip lt ir lu is lv iu lw iv lx ly bi translated">k臂强盗机RL算法的实现</h1><p id="5c4e" class="pw-post-body-paragraph ix iy hi iz b ja lz ij jc jd ma im jf jg mb ji jj jk mc jm jn jo md jq jr js hb bi translated">我不得不承认我不是python专家，但这不能阻止我。为了让python爱好者不那么高兴，我必须说我在这个练习中使用python的体验并不是最佳的，但是我离题了…然而Python有一个令人惊叹的社区和一个奇妙的echo系统，其中有许多用于数据科学、绘图、统计等的库。这使得它成为一个不可或缺的工具。</p><p id="6a7a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以用最少的努力在一个python文件中编写bandit机器码。然而，这样做可能会受到限制，并且会阻止我们尝试不同的想法和算法。总的来说，这个实验是RL和AI的关键活动。出于这个原因，我要把事情复杂化一点——而且我通常不需要理由就能把事情复杂化！😀最后我会在GitHub上提供完整代码的链接。</p><h2 id="4a3e" class="mn li hi bd lj mo mp mq ln mr ms mt lr jg mu mv lt jk mw mx lv jo my mz lx na bi translated">基本原则</h2><p id="828e" class="pw-post-body-paragraph ix iy hi iz b ja lz ij jc jd ma im jf jg mb ji jj jk mc jm jn jo md jq jr js hb bi translated">我的目标是抽象出我上一篇文章中描述的RL的各种组件(请参见<a class="ae jt" rel="noopener" href="/@farbluestar/reinforcement-learning-part-01-e6b78118422d">第01部分</a>中的图表进行快速回顾),并实现以下目标:</p><ul class=""><li id="23b0" class="ks kt hi iz b ja jb jd je jg ku jk kv jo kw js mm ky kz la bi translated">创建一个紧密映射到RL对象(环境、代理等)的对象模型。)并且不知道任何特定的问题配置。</li><li id="3705" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">确保组件是松散耦合的，并且可以轻松替换以添加/更改功能。</li><li id="c259" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">确保不同的算法可以在实验过程中插入和拔出。</li><li id="ec1b" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">拥有足够的灵活性来展示监控环境和绘图所需的任何指标。</li><li id="af78" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">允许简单直观的实验代码。</li></ul><h2 id="5151" class="mn li hi bd lj mo mp mq ln mr ms mt lr jg mu mv lt jk mw mx lv jo my mz lx na bi translated">代码结构—高级别</h2><ul class=""><li id="9db8" class="ks kt hi iz b ja lz jd ma jg nd jk ne jo nf js mm ky kz la bi translated"><strong class="iz hj">环境对象:</strong>隐藏环境的所有细节——在本例中是bandit机器。环境公开了以下接口:<em class="lb"> execute_action </em>和<em class="lb"> on_update </em>。你调用第一个来请求环境执行一个动作的任务。对于第二种情况，您通过传递状态更改处理程序来订阅环境状态更改事件。</li><li id="776d" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated"><strong class="iz hj">代理对象:</strong>执行所有的决策。它有如下界面:<em class="lb">步骤</em>，<em class="lb">步骤_结果</em>。调用step_outcome向代理提供采取行动的奖励。</li><li id="db22" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated"><strong class="iz hj">实验对象:</strong>充当指挥者，驱动所有活动，收集有趣的数据进行分析和绘图。</li><li id="c62b" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated"><strong class="iz hj">选择器</strong>:抽象动作选择的过程，供代理使用。</li><li id="3029" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated"><strong class="iz hj">动作值提供者</strong>:负责动作值的存储、更新和检索。</li><li id="29a6" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated"><strong class="iz hj">初始化器</strong>:抽象动作值初始化的过程，供动作值提供者使用。</li><li id="2115" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated"><strong class="iz hj">公用设施</strong>:普通公用设施。</li></ul><blockquote class="mf mg mh"><p id="24c6" class="ix iy lb iz b ja jb ij jc jd je im jf mi jh ji jj mj jl jm jn mk jp jq jr js hb bi translated">这些形成了一个包，我称之为“<strong class="iz hj"> <em class="hi">精神错乱</em></strong>”——就像“精神错乱的定义”一样！这不仅仅是一个代码框架，更是一种思维框架，允许我在编写实验代码时，根据RL概念进行思考。这是我第一次尝试，它可能会改变和发展。目前的结果是:一个几乎微不足道的实验代码。</p></blockquote><p id="07e1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是一个图表，显示了您需要提供的主要类和典型的实验流程:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ng"><img src="../Images/8fcb40b526457f1e3b4b800f6bb040ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1gU38T9wHBXCSKtLx0pAfw.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">RL实验的典型流程</figcaption></figure><p id="4143" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们看一下实验代码:</p><figure class="kd ke kf kg fd kh"><div class="bz dy l di"><div class="nh ni l"/></div></figure><p id="9b8e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们看到实验班执行以下简单任务:</p><ul class=""><li id="3eaa" class="ks kt hi iz b ja jb jd je jg ku jk kv jo kw js mm ky kz la bi translated">创造了一个强盗环境。</li><li id="2f4a" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">倾听来自环境的呼叫，宣布它的最新状态，并向我们提供奖励。</li><li id="2f6e" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">创建一个bandit代理，并传递我们想要在这个实验中使用的初始化器和选择器。</li><li id="6cdd" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js mm ky kz la bi translated">实现run()方法。这就是时间前进的地方。它调用我们的代理，接收动作，将其传递给环境，并收集它需要保留的任何数据。</li></ul><p id="589e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">差不多就是这样。我们收集两条数据:奖励和标志，以告诉我们行动是否是最优的。分析代码获取这些信息并绘制成图表。以下是结果(在我的低功率笔记本电脑上运行):</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es nj"><img src="../Images/f11693c67b25ec719acf8590cbe7cc60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kd1DSvq6CD885cPp4vW8VA.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">S&amp;B建议运行2000次，我看200次就足够了。随意探索。</figcaption></figure><p id="421f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的图中，您会注意到以下情况:</p><ol class=""><li id="f8fd" class="ks kt hi iz b ja jb jd je jg ku jk kv jo kw js kx ky kz la bi translated">贪婪方法表现最差，正如所料。</li><li id="163e" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js kx ky kz la bi translated">探索确实有帮助，因为两个ε贪婪的结果都超过了贪婪的结果。</li><li id="bf55" class="ks kt hi iz b ja lc jd ld jg le jk lf jo lg js kx ky kz la bi translated">尽管ε=0.1似乎会提前达到更高的平均回报，但ε=0.01实际上表现更好，因为它将在长期内达到并超过ε=0.1。右边的图解释了原因。你可以看到时间百分比ε=0.1选择最佳行动平台较早，然而，它继续增加ε=0.01。%最佳行动图是使用额外指标(除奖励外)来评估我们学习算法性能的一个例子。这是一个需要掌握的好技能。</li></ol><p id="9e45" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这本书没有超过1000拉，所以情节并没有真正显示长期的结果。但是现在我们已经实现了这一点，我们可以尝试一下！这是5000步的结果，我希望不会有什么意外:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es nk"><img src="../Images/2bee18a09cebdd3bebb6482be7135e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iTCDiL0p05-WrYtTMxGGMQ.png"/></div></div></figure><p id="ca13" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其余的代码非常简单，除了代理和环境实现之外，还包括一个bandit类和一个用于计算运行平均值的小工具。请随意查看完整的<a class="ae jt" href="https://github.com/bluephoton/rl-insanity" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj">代码</strong> </a>并进行实验，如果您有问题，请告诉我。也可以随意克隆和改进它。这是一个粗略的图表，解释了所有的部分是如何组合在一起的。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es nl"><img src="../Images/827a8584da9d68bf15ae136a7b69a6fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dMvFYcX_7FQSGHojdgV9yQ.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">精神错乱的粗略描述！</figcaption></figure><p id="ef92" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我该去读更多的书了。希望我能学到一些新的东西来分享。在此之前，希望这些信息对你有用…</p></div></div>    
</body>
</html>