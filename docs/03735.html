<html>
<head>
<title>Tweets of Disasters are Real?? Using GLOVE and BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">灾难的推文是真的？？使用手套和伯特</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tweets-of-disasters-are-real-e5e0a2d3f4b1?source=collection_archive---------6-----------------------#2020-02-16">https://medium.com/analytics-vidhya/tweets-of-disasters-are-real-e5e0a2d3f4b1?source=collection_archive---------6-----------------------#2020-02-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="db27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们将使用手套嵌入和伯特模型来分析推文。</p><h2 id="5642" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">问题定义</strong></h2><p id="411f" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">我们得到了一组推特信息。从这个数据集中，我们必须找出哪些推文在谈论真正的灾难，哪些没有。</p><h2 id="94a7" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">数据集</strong></h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kd"><img src="../Images/9b0fec29c9706e91413336c2f1754b21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*HBv-DLQKOJe9oTglIMhSoQ.png"/></div></div></figure><p id="46ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">target=1，表示这条推文谈论的是真正的灾难，target=0，表示这条推文不是关于灾难的。</p><h2 id="d9d7" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">阶级平衡还是不平衡？</strong></h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kp"><img src="../Images/e7e515e070939e4ae2d94e3ad14e1f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*wQIGmGVQBpdvqMhQQyEg9g.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">平衡数据集</figcaption></figure><h2 id="82af" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">数据清理</strong></h2><p id="4d89" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">删除网址</em> </strong></p><pre class="ke kf kg kh fd kv kw kx ky aw kz bi"><span id="3ce1" class="jd je hi kw b fi la lb l lc ld">url="social networking site :<a class="ae le" href="https://www.facebook.com" rel="noopener ugc nofollow" target="_blank">https://www.facebook.com</a>"<br/>def remove_URL(text):<br/>    url = re.compile(r'https?://\S+|www\.\S+')<br/>    return url.sub(r'',text)</span><span id="680a" class="jd je hi kw b fi lf lb l lc ld">remove_URL(url)<br/>'social networking site :'</span></pre><p id="f6a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">删除html标签</em> </strong></p><p id="05e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我不会写完整的代码，只是给出代码的主要部分-</p><pre class="ke kf kg kh fd kv kw kx ky aw kz bi"><span id="2dc3" class="jd je hi kw b fi la lb l lc ld">html=re.compile(r'&lt;.*?&gt;')</span></pre><p id="ce64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">移除表情符号</em> </strong></p><p id="dd0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为此，只需使用数据中的表情符号的unicodes重新编译即可。表情、图片、地图都有不同范围的unicode值。</p><p id="f9f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">去掉标点符号</em> </strong></p><p id="85bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用下面提到的功能删除标点符号</p><pre class="ke kf kg kh fd kv kw kx ky aw kz bi"><span id="0258" class="jd je hi kw b fi la lb l lc ld">def remove_punct(text):<br/>    table=str.maketrans('','',string.punctuation)<br/>    return text.translate(table)</span></pre><p id="8b27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">用于矢量化的手套</em> </strong></p><p id="3c7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GLOVE是一种矢量化类型，其中每个单词都有一个固定的矢量表示，与上下文无关。但是我们也可以说手套不是完全独立于上下文的，而是它们被投射到上下文中使用最多的地方。因此，如果我们以单词“bank”为例，则创建单个向量来表示多数上下文。如果语料库包含80%自然相关的“银行”单词和20%金融相关的“银行”单词。我们将看到“银行”被进一步磁化为河流、小溪等事物，而不是联盟、贷款人等。</p><p id="3657" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GLOVE考虑的是词对和词对之间的关系，而不是词和词之间的关系。</p><p id="3915" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GLOVE为高频率单词对赋予较低的权重，以防止无意义的停用词如“the”、“an”不会支配训练过程。</p><p id="33e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个模型在单词的共现矩阵上进行训练，这需要大量的内存。</p><p id="facf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们正在导入已经预先训练好的100维手套向量。所以在这里，文本中的每个单词都将由100维向量来表示。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es lg"><img src="../Images/4c137584903dca13ad749d0779a6d964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1gDIttmNBPmVCzFF4cnXmg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">单词“the”的100D手套表示</figcaption></figure><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es lg"><img src="../Images/607578dbcc70aa0b8c839d794e219906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*438y5TBh2tOXMK3n5XB-NQ.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">单词‘of’的100D手套表示</figcaption></figure><p id="4197" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，从上面两幅图像中我们可以看到，在手套表示中，每一行包含101个条目。(在上面两张图片中，显示了第1行和第4行)。这里，该行的第一个条目是单词本身，其他100个条目代表该单词。</p><p id="d95d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，现在创建一个嵌入字典，使用每行的第一个条目(单词本身)作为索引，该行的其他100个数字作为该索引的数组条目。</p><p id="18f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将使用我们的tweets文本语料库创建一个字典。我们正在索引语料库中出现的每个独特的单词。越频繁出现的单词索引越低，这意味着“索引1”处的单词在我们的数据集中出现频率最高。为此，我们必须遵循以下步骤-</p><ol class=""><li id="aa67" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">创建Tokenizer类的对象。[ tokenizer_obj = Tokenizer() ]</li><li id="8ec4" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在‘tokenizer _ obj’会调用‘fit _ on _ texts()’函数，在这个函数中传递你的文本语料库。这将创建一个字典，其中每个唯一的单词都有一个索引，更频繁的单词将获得较低的索引值。</li><li id="fd2f" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在‘tokenizer _ obj’将调用‘texts _ to _ sequences()’函数，并再次传递文本语料库。这将把文本的每个单词转换成字典的索引。</li></ol><p id="9feb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这之后，我们使用它在手套嵌入中的表示，给每个索引分配一个100维的向量。</p><p id="5b20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，现在文本的每个单词都嵌入了手套。</p><p id="2993" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ku">训练模式</em> </strong></p><p id="ed53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们必须创建我们的模型。我们神经网络的第一层将是嵌入层。(请记住，我们只能在模型的第一层使用这一层，其他地方都不行)</p><p id="1d65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里嵌入矩阵的维数是(字数x 100)。我们从上面的嵌入中得到这个维度。我们将使用trainable = ' False '，因为我们不想更新我们的嵌入矩阵权重。我们使用dropout=0.2来避免过度拟合(我们可以使用超参数调整来找到这个值)</p><p id="1192" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第二层中，我们使用100个LSTM单位，dropout=0.2(这里也可以使用超参数调谐来获得更好的结果)。</p><p id="9243" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第三层中，我们使用具有1个sigmoid激活单元的致密层。这一层将给出输出0或1。</p><p id="1480" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用学习率为(1e-4)的“Adam optimizer”和“binary_crossentropy”作为损失函数。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es lv"><img src="../Images/ea8402300b4f8bc55f6742938c889ed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GsAu1JgbFrwoCJL2qO5BMA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">模型摘要</figcaption></figure><p id="e537" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型的准确率为79.95%。</p><h1 id="f5fb" class="lw je hi bd jf lx ly lz jj ma mb mc jn md me mf jq mg mh mi jt mj mk ml jw mm bi translated"><strong class="ak">伯特概念</strong></h1><p id="bb26" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">BERT只不过是变压器的唯一编码器部分。BERT BASE和BERT LARGE分别具有12和24个编码器层。与变压器的默认配置相比，这些变压器还具有较大的前馈网络(分别为768和1024个隐藏单元)以及自关注层中更多的关注头(分别为12和16个关注头)。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mn"><img src="../Images/faf9437613ba8245432af1b48c7145fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_wMk1QVgjq5l75mBhyeTbg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">在BERT中，我们只对编码器部分感兴趣</figcaption></figure><p id="e3b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">编码器主要有两层自关注层和前馈网络。在任何句子进入编码器之前，我们对句子中的每个单词进行位置编码。位置编码值我们一般取最大值512。所以每个句子最多可以有512个单词。(这里我们说的是通用变压器的编码器)</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mn"><img src="../Images/075d815178e7c542305fb3552ef8a822.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYhxvcfDdDFbow5NFOHCDw.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">编码器</figcaption></figure><p id="bb3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们可以看到，输入位置上的每个单词都通过一个单独的或者它自己的路径。但是在“自我关注层”中，这些路径之间存在相关性，而在“前馈层”中没有相关性。因此，在“前馈网络”中，每条路径可以并行执行。这些“前馈网络”对于每个单词都是完全相同的，但是它们是分开执行的。</p><p id="182b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">更详细的自我关注层</strong></p><p id="cd59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看一句<strong class="ih hj">“这个人跑得很慢，因为他累了。”在这个句子中,“他”指的是“人”。但是我们很容易发现这一点，而计算机则不然。因此，为了更好地理解特定上下文中的单词，我们通过查看输入序列中的其他位置来对每个单词进行编码。这导致每个单词更好的编码。</strong></p><p id="1b4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">现在如何计算这个自我关注度</strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mo"><img src="../Images/70973c7c7fc861b519df1914e0550644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pSu2-43YCPimGzfNWChqJg.png"/></div></div></figure><p id="b346" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤</strong></p><ol class=""><li id="733c" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">从每个编码器输入向量生成3个向量。(对于第一编码器输入是每个字的嵌入，对于其他编码器输入是较低层编码器的输出)。因此，我们为每个单词创建一个查询向量、关键向量和值向量。这些向量是通过将每个嵌入乘以我们在训练过程中训练的三个矩阵(Wq、Wk、Wv)而生成的。新向量(q，k，v)的维数小于嵌入向量。</li><li id="e7c6" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在，我们必须计算每个单词的分数，这将决定在句子的其他部分放置多少焦点，因为我们在某个位置编码一个单词。为了计算分数，我们只需要做这个单词的查询向量(q)与所有其他单词的关键向量(k)的点积。因此，现在单词“SOUMAYAN”在第一位置的第一得分将是(q1.k1)，第二得分将是(q1.k2)。</li><li id="c210" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在将这些得分向量除以“关键”(k)向量的维数的平方根。</li><li id="fd42" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在使用“softmax操作”来标准化每个“分数”向量。使得每个得分向量总和变为1，且所有向量变为正，并且具有0到1之间的值。</li><li id="8ed2" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在将每个值向量乘以softmax操作的输出。因此，它对一些单词给予更高的重要性，而对其他一些单词给予较低的重要性(例如，如果一个单词的值向量乘以0.0011，那么它给予该单词较低的重要性)。</li><li id="2f24" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在把所有这些加权的价值向量加起来。这将在第一位置产生单词“SOUMAYAN”的自我注意层的输出。</li></ol><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mp"><img src="../Images/b99d9bce38c5b6178389cf1c324e2d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l7PiQWFtKPImpvMiDrqc7Q.png"/></div></div></figure><p id="e315" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实际实现中，使用矩阵来完成计算，即，不是像x1、x2等每个单词都有向量。我们有一个矩阵“X ”,其中矩阵的每一行都嵌入了一个单词。这里，我们将通过将矩阵‘X’分别乘以‘Wq’、‘Wk’、‘Wv’矩阵来获得查询向量矩阵、关键向量矩阵和值向量矩阵。</p><p id="2409" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之后，只需将矩阵“Q”乘以“转置(K)”并除以关键矩阵维数的平方根。然后对输出进行softmax，并将其乘以矩阵“V”，这将生成矩阵“Z”。这个矩阵“Z”将是自我关注层的输出。</p><p id="d35a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多头注意力的概念</strong></p><p id="0242" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种多头注意力将在两个方面提高注意力层的性能-</p><ol class=""><li id="2e9b" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">它扩展了模型关注句子不同位置的能力。如果我们使用一个头，那么它只能捕捉一个模式，但是如果我们有许多头，那么我们可以捕捉许多模式。</li><li id="7ef6" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">它赋予注意力层多个“表征子空间”。有了8个注意力，我们将有8个查询(Q)矩阵、8个关键(K)矩阵和8个值(V)矩阵。</li></ol><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mq"><img src="../Images/bffea3534a086e89a905311cb13b6821.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jptg2Pwa_O3FuGRxss9tIA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">这里我们可以看到两个注意力头，但是有8个这样的注意力头</figcaption></figure><p id="4dc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，对于8个注意力头，它将产生8个Z矩阵(Z0到Z7)。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mq"><img src="../Images/f4a3d28f5a2ab3916c2cc23980901080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t3GXxETBFGtTR2r_97uLyQ.png"/></div></div></figure><p id="17cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是前馈网络将只接受一个矩阵而不是八个矩阵。所以我们要把这8个矩阵浓缩成1个矩阵。</p><p id="5816" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们必须连接所有这8个矩阵(Z0-Z7)，之后我们必须将这个连接的矩阵乘以一个权重矩阵(W0)，然后这将产生一个像Z一样的维数的单个矩阵。现在这个Z将被提供给前馈网络。(W0矩阵与模型联合训练)。</p><p id="5d66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">添加和归一化</strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mn"><img src="../Images/38ace211b8ec8fb54da22b4cacc516e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ta0Kb93hQte9fpcbeR_o6A.png"/></div></div></figure><p id="c3d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每个编码器层中存在两个加法和归一化层，一个在自关注层之后，另一个在前馈网络之后。这里，我们首先有一个残留的连接，然后是层标准化。</p><p id="3bbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这都是关于编码器的，现在让我们再次关注BERT。</p><p id="69ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT模型是根据下面提到的两个无监督任务训练的</p><ol class=""><li id="6936" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated"><strong class="ih hj">屏蔽语言模型— </strong>在将单词序列输入BERT之前，每个序列中15%的单词被替换为一个【屏蔽】标记。然后，该模型尝试根据序列中其他未屏蔽单词提供的上下文来预测屏蔽单词的原始值。</li><li id="74ad" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated"><strong class="ih hj">下一个句子预测— </strong>在BERT训练过程中，模型接收句子对作为输入，并学习预测句子对中的第二个句子是否是原始文档中的后续句子。在训练期间，50%的输入是一对，其中第二个句子是原始文档中的后续句子，而在另外50%中，从语料库中随机选择一个句子作为第二个句子。假设随机句子将与第一个句子断开。该NSP主要用于捕获长期依赖关系。</li></ol><p id="9ad5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了帮助模型在训练中区分这两个句子，输入以如下方式处理</p><ol class=""><li id="f2a3" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">在第一个句子的开头加上[CLS]标记，在每个句子的结尾加上[SEP]标记。</li><li id="a7b2" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">指示句子A或句子B的句子嵌入被添加到每个记号。句子嵌入在概念上类似于词汇为2的标记嵌入。</li><li id="b2ac" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">位置嵌入被添加到每个标记中，以指示它在序列中的位置。</li></ol><h1 id="dacc" class="lw je hi bd jf lx ly lz jj ma mb mc jn md me mf jq mg mh mi jt mj mk ml jw mm bi translated">伯特模型</h1><p id="075f" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">这些是我们必须遵循的步骤(每个句子被逐一处理)</p><ol class=""><li id="3adc" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">首先，我们必须对每个句子进行标记化。(在BERT记号化中，不仅生成单词，还生成子单词。子词是通过在该子词之前提到“##”来生成的。举例来说，让我们对单词“学习”进行标记化，如果我们发现只有学习出现在词汇中，那么我们将把它分为“学习”、“学习”。还有一点我们发现，如果“#”存在，那么一般来说词汇中就没有“ing”这个词。)</li><li id="3bdd" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在在句子前面加上两个特殊的记号[CLS]，在句尾加上[SEP]。</li><li id="81bc" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在用id替换这些令牌。</li><li id="4b0b" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在用“0”填充每个句子，使所有句子长度(或者更具体地说，句子中出现的id或标记的数量)相同。这里我们取maxlen=512，即句子中标记或id的数量最大为512。</li><li id="ea4f" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在创建另一个numpy数组，其中包含的1的数量等于句子中出现的记号的数量，包括这两个特殊记号和所有填充序列的0。这是掩码数组。</li><li id="ba48" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在创建另一个numpy数组，它将包含所有第一句话标记的0和所有第二句话标记的1。(只有当我们给出两个句子作为输入时，才需要这个句子分段数组)</li><li id="f4c1" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在将这三组id、掩码和每个句子的片段输入到BERT模型中。</li><li id="666a" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">BERT模型的输出将是每个输入令牌(id)的向量。每个向量由768个数字(浮点数)组成(意味着存在768个隐藏单元)。</li><li id="84e8" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">因为这是一个句子分类问题，所以我们忽略除了第一个向量(与[CLS]标记相关的向量)之外的所有向量。</li><li id="1dcc" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">我们将这个向量作为逻辑回归模型的输入。</li><li id="8d1a" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在，该模型将预测概率0和1作为输出(即，推文是否与灾难有关)。</li></ol><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mn"><img src="../Images/eb1fe3ff7ae88c440379287175ee86b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jc8D1dvgKI6ILzI2juLphA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">建模将如何进行</figcaption></figure><p id="e5ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解释步骤8 </strong></p><p id="95e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">伯特模型实际上输出一个三维张量(序列输出是三维的，但合并输出是2D)。它的三维空间是-</p><ol class=""><li id="d82e" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">句子数量。(或批量大小，即我们一次发送的句子数量)</li><li id="26c7" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">序列中令牌或id的最大数量。(在我们的例子中是512)</li><li id="85fc" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">BERT模型中隐藏单元的数量(对于BERT是768)。</li></ol><p id="ac91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，为了更好地理解这三个维度，我将举一个例子。</p><p id="1c35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我要给我的伯特模型的句子是-</p><p id="260b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“我的名字是Soumayan Bandhu Majumder”。</p><ol class=""><li id="ecdb" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">我假设最大长度=12。</li><li id="c961" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">在标记了上面的句子后，我得到了[ 'My '，' name '，' is '，' Soumayan '，' Bamdhu '，' Majumder' ]</li><li id="2245" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在在上述句子的第一句和最后一句加上['CLS']和[ 'SEP' ]。</li><li id="b517" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在将上述令牌转换为id，并用“0”填充到maxlen。[101,34,13,7,3456,678,567,102,0,0,0,0].</li><li id="dd59" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在从上面制作蒙版阵列。[1,1,1,1,1,1,1,1,0,0,0,0]</li><li id="04ef" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在分段数组这将用于区分两个句子，因为我们只发送一个句子作为输入，所以分段数组的每个条目都将包含0[0，0，0，0，0，0，0，0，0，0]</li><li id="7726" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在，我们将把一个句子的这三个数组作为我们的BERT模型的输入。</li><li id="fb05" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在我们的BERT模型将产生两个输出，第一个是序列输出，第二个是混合输出。</li><li id="253b" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">序列输出的维数将是[1，12，768]，其中1是我们一次发送的句子数，这里是1。12是句子的最大长度。每个单词或记号由长度为768的嵌入来表示。</li><li id="ce15" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在，为了将这个3D张量转换成2D张量，我们正在使用sequence_output[:，0，:]=我们正在获取单个批次的所有句子，我们仅从句子中获取第一个或[CLS]令牌，我们正在使用all或768的嵌入大小。</li><li id="653f" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">现在另一种产出是混合产出(2D产出)。此输出的尺寸为[1，768]。这里句子的数量是1，每个令牌的嵌入大小是768。(这实际上是[CLS]的嵌入或句子的第一个标记，它通过“tanh”激活层。)</li></ol><p id="8afe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT模型输出在这里意味着最后隐藏状态的输出(默认情况下，但我们也可以改变这一点)。现在我们把三维张量转换成二维张量，只需要-</p><p id="819e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> last_hidden_states[0][:，0，:] </strong></p><p id="193c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对所有序列(句子)的第一个位置的输出进行切片，取last_hidden_state的所有隐藏单元输出。(第一个位置包含[CLS]标记)</p><p id="2652" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，该输出将作为输入提供给逻辑回归模型，该模型具有“sigmoid”激活单元、“binary_crossentropy”损失函数和学习速率为2e-6的adam优化器。</p><p id="8279" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，该模型将输出为“1”或“0”，即谈论灾难与否。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mr"><img src="../Images/4ad07bbee08d655ad437d9c421d3d2a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DOOeI-fwB4avC8IGJHt7XA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">LAST_HIDDEN_STATES[0]</figcaption></figure><p id="cd26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意:-在上面的图片中，为了更好地理解,【CLS】标记在每个句子的开头，但实际上,【CLS】的id应该在那里。类似地，在句子的最后给出了[SEP]，但实际上它将是令牌的id[SEP]。在句子的最后一个字段中，也给出了“0”，因为每个句子都不是最大长度，所以在这种情况下，我们用“0”进行填充。</strong></p><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mr"><img src="../Images/2292521becce4805853cef8fb4fb7504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*drn0uRKwuJdiokJwdyOl3Q.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">LAST_HIDDEN_STATES[0][:，0，]</figcaption></figure><p id="b861" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">一个小事情是，这里LAST_HIDDEN_STATES[0]是指隐藏层中众多隐藏状态中，得分预测高的隐藏状态将是输出。</strong></p><p id="be3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意——隐藏状态或带有隐藏层的隐藏单元之间存在差异</strong></p><p id="6b0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，我们已经定义了什么是LAST_HIDDEN_STATES[0]。现在[:，0，:]的意思是[所有句子，第0个或[CLS]位置的句子，所有隐藏的单位]。</p><p id="ae05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个LAST_HIDDEN_STATES[0][:，0，:]被赋予逻辑回归模型，以将其标记为1或0。</p><p id="a86b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT不仅用于微调，像其他模型一样，BERT也可以用于上下文单词嵌入，然后我们可以将这些嵌入用于我们的模型。沿着该路径的该令牌的每个编码器层的输出可以被用作该令牌的特征表示。</p><p id="166f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有六种选择来选择单词的嵌入-</p><ol class=""><li id="488a" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">最后一个隐藏层。</li><li id="6410" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">第一层。</li><li id="636e" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">所有12层的总和。</li><li id="1a33" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">倒数第二个隐藏层(因为最后一个隐藏层更偏向目标值)。</li><li id="4e4f" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">最后4个隐藏层的总和。</li><li id="7d33" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">串联最后4个隐藏层。</li></ol><h1 id="bc67" class="lw je hi bd jf lx ly lz jj ma mb mc jn md me mf jq mg mh mi jt mj mk ml jw mm bi translated"><strong class="ak">现在是编码的时候了</strong></h1><p id="b9ab" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">我不会写完整的代码，只是给你一个如何使用伪代码的想法。</p><p id="950a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">两个主要功能-</p><ol class=""><li id="1c39" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated"><strong class="ih hj">伯特_编码</strong></li></ol><pre class="ke kf kg kh fd kv kw kx ky aw kz bi"><span id="39c9" class="jd je hi kw b fi la lb l lc ld">FUNCTION ARGUMENTS: texts,tokenizer,max_len //this tokenizer is taken from BERT</span><span id="3c63" class="jd je hi kw b fi lf lb l lc ld">ARRAYS: all_tokens[],all_masks[],all_segments[]</span><span id="3ee4" class="jd je hi kw b fi lf lb l lc ld">for (all sentences in texts):</span><span id="0f64" class="jd je hi kw b fi lf lb l lc ld">{</span><span id="aa82" class="jd je hi kw b fi lf lb l lc ld">text=tokenize all sentence</span><span id="1159" class="jd je hi kw b fi lf lb l lc ld">input_sequence=[“[CLS]”]+text+[“[SEP]”]</span><span id="6a7a" class="jd je hi kw b fi lf lb l lc ld">tokens=convert into ids(input_sequence)</span><span id="4752" class="jd je hi kw b fi lf lb l lc ld">pad_length=max_len — length(input_sequences)</span><span id="9b53" class="jd je hi kw b fi lf lb l lc ld">tokens=tokens + [0]*pad_length</span><span id="0103" class="jd je hi kw b fi lf lb l lc ld">masks=[1]*length(input_sequences) + [0]*length(pad_length)</span><span id="a75e" class="jd je hi kw b fi lf lb l lc ld">segments=[0]*max_len</span><span id="626c" class="jd je hi kw b fi lf lb l lc ld">now fill above mentioned three arrays by these tokens,masks,segments respectively</span><span id="b8ae" class="jd je hi kw b fi lf lb l lc ld">}<br/>return( all_tokens[],all_masks[],all_segments[])</span></pre><p id="149d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。构建_模型</strong></p><pre class="ke kf kg kh fd kv kw kx ky aw kz bi"><span id="23cd" class="jd je hi kw b fi la lb l lc ld">FUNCTION ARGUMENT: bert_layer,max_len</span><span id="131e" class="jd je hi kw b fi lf lb l lc ld">Input:input_word_ids,input_mask,segment_ids</span><span id="0609" class="jd je hi kw b fi lf lb l lc ld">— ,sequence_output=bert_layer([input_word_ids,input_mask,segment_ids])// The Bert layer outputs two things (pooled vector and full sequence), we ignore the first one since we are only interested by the sequence.</span><span id="4335" class="jd je hi kw b fi lf lb l lc ld">clf_output=sequence_output[:,0,:]</span><span id="63c6" class="jd je hi kw b fi lf lb l lc ld">output=Dense(1,activation= ‘sigmoid’)(clf_output)</span><span id="639e" class="jd je hi kw b fi lf lb l lc ld">model=Model(inputs=[input_word_ids,input_mask,segment_ids],outputs=output)</span><span id="1c75" class="jd je hi kw b fi lf lb l lc ld">model.compile(Adam(lr=2e-6),loss=binary_crossentropy,metrics=[‘accuracy’])</span><span id="69a5" class="jd je hi kw b fi lf lb l lc ld">return model</span></pre><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mn"><img src="../Images/eb61525b74abc8a0b96451b01b29e868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Be1qXeU4TfIFS-hkF18xSQ.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">模型如何进行</figcaption></figure><pre class="ke kf kg kh fd kv kw kx ky aw kz bi"><span id="e869" class="jd je hi kw b fi la lb l lc ld">module_url = "https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1"<br/>bert_layer = hub.KerasLayer(module_url, trainable=True)</span><span id="f3a1" class="jd je hi kw b fi lf lb l lc ld">train_input = bert_encode(train.text.values, tokenizer, max_len=512)<br/>test_input = bert_encode(test.text.values, tokenizer, max_len=512)<br/>train_labels = train.target.values<br/>model = build_model(bert_layer, max_len=512)<br/>model.fit(<br/>    train_input, train_labels,<br/>    validation_split=0.2,<br/>    epochs=3,<br/>    batch_size=16<br/>)<br/>test_pred = model.predict(test_input)</span></pre><p id="853e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个模型产生了83.026%的准确率。</p></div></div>    
</body>
</html>