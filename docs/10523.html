<html>
<head>
<title>Building a State-of-the-art Text Classifier for any language you want!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为你想要的任何语言建立一个最先进的文本分类器！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-a-state-of-the-art-text-classifier-for-any-language-you-want-fe3ebbdab5c9?source=collection_archive---------13-----------------------#2020-10-22">https://medium.com/analytics-vidhya/building-a-state-of-the-art-text-classifier-for-any-language-you-want-fe3ebbdab5c9?source=collection_archive---------13-----------------------#2020-10-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="255c" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">文本分类是自然语言处理领域一个众所周知的问题。然而，近年来“NLP 进化”的出现使得处理这样的问题变得容易，而不需要该领域的专业知识。最近的进展帮助我们认识到，如果模型首先理解语言(语言建模)，它们可以充当更好的分类器。我们将使用 fastai 图书馆和 ULMFiT 进行这项工作。</h2></div></div><div class="ab cl ix iy gp iz" role="separator"><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc"/></div><div class="hb hc hd he hf"><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/7b8950e68259ca5b7447690137f5e6be.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*Cx-vt9ZHMJPqwsCYL9MCMA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><a class="ae jq" href="https://towardsdatascience.com/understanding-ulmfit-and-elmo-the-shift-towards-transfer-learning-in-nlp-b5d8e2e3f664" rel="noopener" target="_blank">自然语言处理中的迁移学习</a></figcaption></figure><h1 id="a8a8" class="jr js hi bd jt ju jv jw jx jy jz ka kb io kc ip kd ir ke is kf iu kg iv kh ki bi translated">概观</h1><p id="f16c" class="pw-post-body-paragraph kj kk hi kl b km kn ij ko kp kq im kr ks kt ku kv kw kx ky kz la lb lc ld le hb bi translated">这个想法是首先在维基百科数据集上训练(或使用预先训练的模型，如果可用的话)该语言的语言模型，该模型可以在给定一组单词的情况下准确预测下一个单词(有点像我们手机上的键盘在建议单词时所做的)。然后，我们使用这个模型对评论、推文、文章等进行分类，令人惊讶的是，经过一些调整，我们可以用这种语言建立一个最先进的文本分类模型。出于本文的目的，我将为<a class="ae jq" href="https://en.wikipedia.org/wiki/Hindi" rel="noopener ugc nofollow" target="_blank">印地语</a>建立语言模型，并使用它对评论/文章进行分类。</p><p id="e543" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">建立这个模型所用的方法是<a class="ae jq" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank"> ULMFiT </a>(文本分类通用语言模型微调)。ULMFit 背后的基本概念错综复杂，在另一篇文章中解释它的细节会更好。然而，<a class="ae jq" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fastaiv1 </a>库使得语言建模和文本分类的任务变得非常简单明了(只需要不到 20 行代码！！).</p><p id="b8e1" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">杰瑞米·霍华德在他的 mooc 中也描述了 ULMFiT，可以在<a class="ae jq" href="https://course.fast.ai/videos/?lesson=8" rel="noopener ugc nofollow" target="_blank">这些</a> <a class="ae jq" href="https://course19.fast.ai/videos/?lesson=12" rel="noopener ugc nofollow" target="_blank">链接</a>中找到。下面提到的所有代码都可以在我的<a class="ae jq" href="https://github.com/prats0599/hindi-nlp" rel="noopener ugc nofollow" target="_blank"> github </a>上找到。</p><h1 id="ea6b" class="jr js hi bd jt ju jv jw jx jy jz ka kb io kc ip kd ir ke is kf iu kg iv kh ki bi translated"><strong class="ak">维基百科数据集</strong></h1><p id="4177" class="pw-post-body-paragraph kj kk hi kl b km kn ij ko kp kq im kr ks kt ku kv kw kx ky kz la lb lc ld le hb bi translated">我们从下载和清理用印地语写的维基百科文章开始。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="5f77" class="lp js hi ll b fi lq lr l ls lt">%reload_ext autoreload<br/>%autoreload 2<br/>%matplotlib inline</span><span id="2b41" class="lp js hi ll b fi lu lr l ls lt"># importing the required libraries<br/>from fastai import *<br/>from fastai.text import *</span><span id="8cfd" class="lp js hi ll b fi lu lr l ls lt">torch.cuda.set_device(0)</span><span id="23eb" class="lp js hi ll b fi lu lr l ls lt"># Initializing variables<br/># each lang has its code which is defined here(under the colunmn<br/># 'wiki': <a class="ae jq" href="https://meta.wikimedia.org/wiki/List_of_Wikipedias" rel="noopener ugc nofollow" target="_blank">https://meta.wikimedia.org/wiki/List_of_Wikipedias</a></span><span id="b9dc" class="lp js hi ll b fi lu lr l ls lt">data_path = Config.data_path()<br/>lang = 'hi'<br/>name = f'{lang}wiki'<br/>path = data_path/name<br/>path.mkdir(exist_ok=True, parents=True) # create directory<br/>lm_fns = [f'{lang}_wt', f'{lang}_wt_vocab']</span></pre><p id="577a" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">现在让我们从维基百科下载文章。Wikipedia 包含一个 Wikipedia 列表，其中包含关于以特定语言出现的文章数量、编辑数量和深度的信息。“深度”栏(定义为[编辑/文章] × [非文章/文章]×[1-存根比率])是维基百科质量的粗略指标，显示其文章更新的频率。深度越高，文章的质量就越高。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="1990" class="lp js hi ll b fi lq lr l ls lt">def get_wiki(path,lang):<br/>    name = f'{lang}wiki'<br/>    if (path/name).exists():<br/>        print(f"{path/name} already exists; not downloading")<br/>        return</span><span id="d10f" class="lp js hi ll b fi lu lr l ls lt">xml_fn = f"{lang}wiki-latest-pages-articles.xml"<br/>    zip_fn = f"{xml_fn}.bz2"</span><span id="76a3" class="lp js hi ll b fi lu lr l ls lt">if not (path/xml_fn).exists():<br/>        print("downloading...")<br/>        download_url(f'<a class="ae jq" href="https://dumps.wikimedia.org/{name}/latest/{zip_fn}'" rel="noopener ugc nofollow" target="_blank">https://dumps.wikimedia.org/{name}/latest/{zip_fn}'</a>, path/zip_fn)<br/>        print("unzipping...")<br/>        bunzip(path/zip_fn)</span><span id="70b9" class="lp js hi ll b fi lu lr l ls lt">with working_directory(path):<br/>        print("extracting...")<br/>        os.system("python -m wikiextractor.WikiExtractor --processes 4 --no_templates " +<br/>            f"--min_text_length 1800 --filter_disambig_pages --log_file log -b 100G -q {xml_fn}")<br/>    shutil.move(str(path/'text/AA/wiki_00'), str(path/name))<br/>    shutil.rmtree(path/'text')</span><span id="9abe" class="lp js hi ll b fi lu lr l ls lt">def split_wiki(path,lang):<br/>    dest = path/'docs'<br/>    name = f'{lang}wiki'<br/>    if dest.exists():<br/>        print(f"{dest} already exists; not splitting")<br/>        return dest</span><span id="38eb" class="lp js hi ll b fi lu lr l ls lt">    dest.mkdir(exist_ok=True, parents=True)<br/>    title_re = re.compile(rf'&lt;doc id="\d+" url="<a class="ae jq" rel="noopener ugc nofollow" target="_blank" href="/analytics-vidhya/{lang}.wikipedia.org/wiki/?curid=%5Cd+">https://{lang}.wikipedia.org/wiki\?curid=\d+</a>" title="([^"]+)"&gt;')<br/>    lines = (path/name).open()<br/>    f=None</span><span id="cd27" class="lp js hi ll b fi lu lr l ls lt">    for i,l in enumerate(lines):<br/>        if i%100000 == 0: print(i)<br/>        if l.startswith('&lt;doc id="'):<br/># Since "/" is a directory seperator, we replace those with _ when  # saving the file names. <br/>            title = title_re.findall(l)[0].replace('/','_') <br/>            if len(title)&gt;150: continue # if title_length&gt;150 skip  # article.<br/>            if f: f.close()<br/>            f = (dest/f'{title}.txt').open('w')<br/>        else: f.write(l)<br/>    f.close()<br/>    return dest</span></pre><p id="025d" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated"><code class="du lv lw lx ll b">get_wiki</code>函数下载所有文章的 xml 版本作为 zip 文件。然后我们使用 bunzip 提取内容，并运行<a class="ae jq" href="https://github.com/attardi/wikiextractor" rel="noopener ugc nofollow" target="_blank"> Wikiextractor </a>(记住先运行<code class="du lv lw lx ll b">!pip install wikiextractor</code>来清理我们提取的文件。我们清理它是因为 xml 文件包含一些 xml 和元数据，这对语言模型来说不是很好，因为我们不希望模型预测 xml 文件中的下一个标记，而是预测标准文本文件中的标记。我们只选择最小文本长度为 1800 的文章。<br/>然后我们调用<code class="du lv lw lx ll b">split_wiki </code>,它将单个维基百科文件分割成每篇文章一个单独的文件。每篇文章都用</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="28ad" class="lp js hi ll b fi lq lr l ls lt">&lt;doc id=”14" url=”https://hi.wikipedia.org/wiki?curid=14" title=”दैनिक पूजा”&gt;`</span></pre><p id="fb7c" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">所以我们只需要编译一个正则表达式来搜索它们。运行此命令后，路径应该包含以下文件(-models):</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="c7f5" class="lp js hi ll b fi lq lr l ls lt">[PosixPath('/home/jupyter/.fastai/data/hiwiki/hiwiki'),<br/> PosixPath('/home/jupyter/.fastai/data/hiwiki/log'),<br/> PosixPath('/home/jupyter/.fastai/data/hiwiki/docs'),<br/> PosixPath('/home/jupyter/.fastai/data/hiwiki/models'),<br/> PosixPath('/home/jupyter/.fastai/data/hiwiki/hiwiki-latest-pages-articles.xml'),<br/> PosixPath('/home/jupyter/.fastai/data/hiwiki/hiwiki-latest-pages-articles.xml.bz2')]</span></pre><p id="f2ac" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">我们现在准备加载数据并训练我们的语言模型。</p><h1 id="b482" class="jr js hi bd jt ju jv jw jx jy jz ka kb io kc ip kd ir ke is kf iu kg iv kh ki bi translated"><strong class="ak">创建并训练语言模型</strong></h1><p id="dcfc" class="pw-post-body-paragraph kj kk hi kl b km kn ij ko kp kq im kr ks kt ku kv kw kx ky kz la lb lc ld le hb bi translated">我们将数据加载到 fastai 数据集中，并创建我们的模型。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="2399" class="lp js hi ll b fi lq lr l ls lt">data = (TextList.from_folder(dest)<br/>       .split_by_rand_pct(0.1, seed=42)#10% data used for validation<br/>       .label_for_lm()<br/>       .databunch(bs=bs, num_workers=1))<br/>data.save(f'{lang}_databunch') # save databunch.<br/>len(data.vocab.itos), len(data.train_ds)</span><span id="acd4" class="lp js hi ll b fi lu lr l ls lt">learn = language_model_learner(data, AWD_LSTM, drop_mult=0.5, pretrained=False).to_fp16()</span></pre><p id="b6c7" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">我们使用来自 fastai 的<code class="du lv lw lx ll b">language_model_learner</code>类，并使用一个 AWD-LSTM，所有辍学乘以<code class="du lv lw lx ll b">0.5</code>。我们还使用混合精度<code class="du lv lw lx ll b">to_fp16()</code>来快速训练我们的模型。更多关于那个<a class="ae jq" href="https://docs.fast.ai/callback.fp16" rel="noopener ugc nofollow" target="_blank">的细节在这里</a>。</p><p id="f04c" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">现在，我们设置学习率(通常 0.01 是好的，但它随 batch_size 而变化)，并为 20 个时期训练我们的语言模型。你也可以使用<code class="du lv lw lx ll b">learn.lr_find</code>然后<code class="du lv lw lx ll b">learn.recorder.plot()</code>来找到最佳学习率。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="f504" class="lp js hi ll b fi lq lr l ls lt">lr = 1e-2<br/>lr *= bs/48 # scale LR by batchsize.<br/>learn.unfreeze()<br/>learn.fit_one_cycle(20, lr, moms=(0.8, 0.7))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ly"><img src="../Images/64dbbdca40be978a6e3d91114c96d7d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*m6t_vD5RzCUXt-osNfFTaw.jpeg"/></div></figure><p id="4eb4" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">然后，我们在训练完成后保存我们的模型。对于任何语言模型来说，达到 40%的准确率是相当容易的。由于维基百科上可用的印地语文章数量较少，该模型发现更难概括(就上下文而言，英语文章的数量大约是印地语文章的 10 倍)。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="c2bc" class="lp js hi ll b fi lq lr l ls lt">mdl_path = path/'models'<br/>mdl_path.mkdir(exist_ok=True)<br/>learn.to_fp32().save(mdl_path/lm_fns[0], with_opt=False)<br/>learn.data.vocab.save(mdl_path/(lm_fns[1] + '.pkl'))</span></pre><p id="31ed" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">现在我们到了情感分析部分。我们有 3 个可用的印地语分类数据集来检查我们的立场:BBC 文章，IITP 电影评论和 IITP 产品评论。我将只使用 BBC 的数据集来进行情感分析。你可以看看<a class="ae jq" href="https://github.com/prats0599/hindi-nlp" rel="noopener ugc nofollow" target="_blank">回购</a>，在那里我对上述其他数据集的模型进行了评估。</p><h1 id="5b75" class="jr js hi bd jt ju jv jw jx jy jz ka kb io kc ip kd ir ke is kf iu kg iv kh ki bi translated"><strong class="ak">微调语言模型</strong></h1><p id="9ee6" class="pw-post-body-paragraph kj kk hi kl b km kn ij ko kp kq im kr ks kt ku kv kw kx ky kz la lb lc ld le hb bi translated">我们使用熊猫加载训练集、有效集和测试集。我们现在做的是使用这个新的数据集来进一步微调语言模型。<em class="lz">为什么这是重要的一步？</em>嗯，分类数据集中可能有一些词在评论/文章中出现得很多，但在维基百科文章中出现得不多(例如:大片名称、男演员、女演员等)。为了充分利用这一点，我们使用 train、valid 和<strong class="kl hj"> test </strong>数据集来微调语言模型。这个<strong class="kl hj">并不构成欺骗，因为我们只使用了数据集的评论栏，它们被用于预测下一个单词的模型，而不是一个评论是正面、负面还是中性的。这种训练被称为半监督学习。我们可以使用这里所有未标记的数据。</strong></p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="b90c" class="lp js hi ll b fi lq lr l ls lt">train_df = pd.read_csv('data/bbc-articles-hi/hi-train.csv', header=None)<br/>test_df = pd.read_csv('data/bbc-articles-hi/hi-test.csv', header=None)<br/>df = pd.concat([train_df,test_df], sort=False)</span></pre><p id="948c" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">我们将训练和测试数据帧结合起来，作为一个 fastai 数据束加载到模型中。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="172d" class="lp js hi ll b fi lq lr l ls lt">data_lm = (TextList.from_df(df, path, cols=1) # the article col.<br/>    .split_by_rand_pct(0.1, seed=42)<br/>    .label_for_lm()           <br/>    .databunch(bs=bs, num_workers=1))</span></pre><p id="4a26" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">然后，我们定义语言模型学习器，加载先前保存的学习器，并训练它几个时期。<code class="du lv lw lx ll b">language_model_learner</code>类有一个参数 pretrained_fnames，我们在这里传递模型及其 vocab。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="66a5" class="lp js hi ll b fi lq lr l ls lt">learn_lm = language_model_learner(data_lm, AWD_LSTM, pretrained_fnames=lm_fns, drop_mult=1.0)<br/>lr = 1e-3<br/>lr *= bs/48<br/># finetuning the language model<br/>learn_lm.fit_one_cycle(2, lr*10, moms=(0.8,0.7))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ma"><img src="../Images/623cf1462ced8035133a25a995429a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*K4quPtiPlRcPYIy8LPL59A.jpeg"/></div></figure><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="2603" class="lp js hi ll b fi lq lr l ls lt">learn_lm.unfreeze()<br/>learn_lm.fit_one_cycle(8, lr, moms=(0.8,0.7))<br/># saving the model and the encoder.<br/>learn_lm.save(f'{lang}fine_tuned')<br/>learn_lm.save_encoder(f'{lang}fine_tuned_enc')</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mb"><img src="../Images/0568af54ceaffa0730533624fd301aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*XxQCX-OBEWKqOaXNbSeApQ.jpeg"/></div></figure><p id="1efb" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">我们现在准备将数据输入编码器并对商品进行分类。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="d11e" class="lp js hi ll b fi lq lr l ls lt">NOTE: If your instance runs out of memory, try reducing the batch_size or restarting the kernel. You could also make use of garbage collector in python by assigning variables you longer need to None and then calling gc.collect().</span></pre><h1 id="bdd3" class="jr js hi bd jt ju jv jw jx jy jz ka kb io kc ip kd ir ke is kf iu kg iv kh ki bi translated">训练分类器</h1><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="6ffd" class="lp js hi ll b fi lq lr l ls lt">data_clas = (TextList.from_df(train_df, path, vocab=data_lm.vocab, cols=1)<br/>    .split_by_rand_pct(0.1, seed=42)<br/>    .label_from_df(cols=0)<br/>    .databunch(bs=bs, num_workers=1))</span><span id="eae5" class="lp js hi ll b fi lu lr l ls lt"># saving the databunch so you could just load it up again when re-  # running the notebook.<br/>data_clas.save(f'{lang}_textlist_class')<br/>data_clas = load_data(path, f'{lang}_textlist_class', bs=bs, num_workers=1)</span></pre><p id="e91a" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">我们可以看到我们的数据集有 14 个类，它们是:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mc"><img src="../Images/bf4b4a896a382e2c9aa6bc17d1209425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*NbypYu1-_xJ4AE4WfOeqkQ.jpeg"/></div></figure><p id="8089" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">也可以拨打<code class="du lv lw lx ll b">data_clas.show_batch()</code>查看自己的数据。我在笔记本中使用了 3 个指标，即准确性、f1 分数和 Mathews 相关性(mcc)。F1 得分和 mcc 都有其优点和缺点，所以在阅读了这篇<a class="ae jq" href="https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a" rel="noopener" target="_blank">文章</a>后，我尝试使用两者。默认情况下，f1 分数的平均参数设置为“二进制”，由于这是一个多类分类，我们将其设置为“微”。</p><p id="c307" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">我们现在加载分类器，并将其设置为<code class="du lv lw lx ll b">fp16()</code>，因为它有助于快速训练(混合精度)。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="a9a5" class="lp js hi ll b fi lq lr l ls lt">from sklearn.metrics import f1_score</span><span id="48b9" class="lp js hi ll b fi lu lr l ls lt"><a class="ae jq" href="http://twitter.com/np_func" rel="noopener ugc nofollow" target="_blank">@np_func</a><br/>def f1(inp,targ): return f1_score(targ, np.argmax(inp, axis=-1), average='micro')</span><span id="0cf9" class="lp js hi ll b fi lu lr l ls lt">mcc = MatthewsCorreff()</span><span id="ea1e" class="lp js hi ll b fi lu lr l ls lt">learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy,f1, mcc]).to_fp16()<br/>learn_c.load_encoder(f'{lang}fine_tuned_enc')<br/>learn_c.freeze()<br/>lr=2e-2<br/>lr *= bs/48<br/>learn_c.fit_one_cycle(2, lr, moms=(0.8,0.7))<br/>learn_c.fit_one_cycle(2, lr, moms=(0.8,0.7))</span></pre><div class="jf jg jh ji fd ab cb"><figure class="md jj me mf mg mh mi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><img src="../Images/da3a0161c09517751420d6087b3ec197.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*EGgzrjimQsuVPOSUFHJNYQ.jpeg"/></div></figure><figure class="md jj mn mf mg mh mi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><img src="../Images/36c8ae419b31543d71b32680a051fa6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*C-PisfbYtP3gdlEyngpsMQ.jpeg"/></div></figure></div><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="390c" class="lp js hi ll b fi lq lr l ls lt">Note: Where does the magic number 2.6^4 comes from?<br/>When setting discriminative learning rates, 2.6 is the number that you need to divide the learning rate by, when going from one layer to the other.(for NLP RNNs). More about that <a class="ae jq" href="https://forums.fast.ai/t/deep-learning-lesson-4-notes/30983" rel="noopener ugc nofollow" target="_blank">here</a>.</span></pre><p id="a8a3" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">经过一段时间的训练后，我们最终在训练集和测试集上的马修斯相关系数分别为 78.4%和 72.6%。训练集和测试集的准确率分别为 83.8%和 79.8%。这真的很好。我们现在有最后一个调整来进一步改进我们的模型，只增加一个参数。这个想法是，给定一组单词，我们预测前面的单词，而不是语言模型预测下一个单词。要做到这一点，只需复制您的笔记本，并在定义数据分组时将<code class="du lv lw lx ll b">backwards=True</code>传递给<code class="du lv lw lx ll b">.databunch()</code>。对所有的数据分组(甚至是为分类器定义的数据分组)都这样做，训练完成后，我们只需将两个模型集成，瞧！性能得到了提高。</p><pre class="jf jg jh ji fd lk ll lm ln aw lo bi"><span id="af35" class="lp js hi ll b fi lq lr l ls lt">NOTE: IF you're loading the databunches after saving them, don't forget to pass backwards=True there too.</span></pre><p id="3bda" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">在建立了第二语言模型并将其与第一语言模型集成后，我们得到了 84.39%的准确率和 79.13%的马修斯相关系数！如果您需要为另一种语言构建一个，只需用相应的代码替换<code class="du lv lw lx ll b">lang='hi'</code>。</p><p id="eefc" class="pw-post-body-paragraph kj kk hi kl b km lf ij ko kp lg im kr ks lh ku kv kw li ky kz la lj lc ld le hb bi translated">总之，我们已经为印地语的语言建模和文本分类建立了一个最先进的模型。</p></div></div>    
</body>
</html>