<html>
<head>
<title>Amazon Fine Food Reviews Featurization with Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">亚马逊美食评论通过自然语言处理实现特色化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/amazon-fine-food-reviews-featurization-with-natural-language-processing-a386b0317f56?source=collection_archive---------1-----------------------#2020-07-25">https://medium.com/analytics-vidhya/amazon-fine-food-reviews-featurization-with-natural-language-processing-a386b0317f56?source=collection_archive---------1-----------------------#2020-07-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/07c50fb98f564f96d7428ea53ea34188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*3DevKJYTHdHq9n3YeJaurg.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">亚马逊美食评论分析</figcaption></figure><p id="2384" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">首先我们想知道什么是亚马逊美食点评分析？</em>T3】</strong></p><p id="c741" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">这个数据集由亚马逊的美食评论组成。这些数据跨越了10多年的时间，包括截至2012年10月的所有约500，000篇评论。评论包括产品和用户信息、评级和明文评论。我们也有来自所有其他亚马逊类别的评论。</em></p><p id="aa82" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">亚马逊评论通常是消费者产品中最受公众关注的评论。作为一个经常使用亚马逊的用户，我对检查亚马逊评论的大型数据库的结构和可视化这些信息很感兴趣，以便成为一个更聪明的消费者和评论者。</em></p><h1 id="e351" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> <em class="kn">简介</em> </strong></h1><p id="c442" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated"><em class="jo">亚马逊美食点评数据集由亚马逊美食点评组成。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es kt"><img src="../Images/2a73693b62e7dc841fd5bbe035fc5c3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*YvxPMW7R70tDfDcXXEI5VA.png"/></div></div></figure><ol class=""><li id="dfd5" class="lc ld hi is b it iu ix iy jb le jf lf jj lg jn lh li lj lk bi translated"><em class="jo">评论数:568454</em></li><li id="b403" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">用户数量:256059人</em></li><li id="1baf" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">产品数量:74258</em></li><li id="7cba" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated">时间跨度:1999年10月至2012年10月</li><li id="467b" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">数据中的属性/列数:10 </em></li></ol><p id="b654" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">属性信息:</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/a8cd8a3284e6d92eca54e3b6f6d1954c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*JuPxaHKJIZ1hatyYa6EWAw.png"/></div></figure><ol class=""><li id="3498" class="lc ld hi is b it iu ix iy jb le jf lf jj lg jn lh li lj lk bi translated"><em class="jo"> Id </em></li><li id="0de1" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">产品id——产品的唯一标识符</em></li><li id="816c" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo"> UserId —用户的唯一标识符</em></li><li id="7da2" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">轮廓名称</em></li><li id="e5cd" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">有用性分子——认为评论有用的用户数量</em></li><li id="34af" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">helpfullnesdenominator——表示他们认为评论是否有用的用户数量</em></li><li id="7d09" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">分数——1到5分之间的评级</em></li><li id="f4a3" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">时间—审核的时间戳</em></li><li id="1cc0" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">总结——审查的简要总结</em></li><li id="2ed9" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">文本—评论的文本</em></li></ol><h1 id="8e29" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><em class="kn">目标</em></h1><p id="895f" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated"><em class="jo">给定一个评价，确定该评价是正面的(评分为4或5)还是负面的(评分为1或2)。</em></p><p id="aa7e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">【问】如何确定一篇评论是正面还是负面？</em></p><p id="8869" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">【答案】我们可以使用分数/评级。4或5级可以被认为是积极的评价。1或2的评论可能被认为是负面的。3的评论是中性的，被忽略了。这是一种确定评论极性(积极/消极)的近似和代理方式。</em></p><h1 id="c35c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><em class="kn">加载数据</em></h1><p id="be28" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated"><em class="jo">数据集有两种形式</em></p><ol class=""><li id="f094" class="lc ld hi is b it iu ix iy jb le jf lf jj lg jn lh li lj lk bi translated"><em class="jo">。csv文件</em></li><li id="aafa" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo"> SQLite数据库</em></li></ol><p id="a654" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">为了加载数据，我们使用了SQLITE数据集，因为它更易于高效地查询数据和可视化数据。</em></p><p id="34ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">在这里，由于我们只想获得推荐的整体情绪(正面或负面)，我们将有意忽略所有等于3的分数。如果得分高于3，则推荐将被设置为“正面”。否则，它将被设置为“负”。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lr"><img src="../Images/388f17b95419afacef0690b2f9b24d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DL24nnPHdC7w8sF09RHDRg.png"/></div></div></figure><p id="f90a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我使用了一个数据库，里面有超过500，000条关于亚马逊美食的评论，可以通过Kaggle找到，在这里可以找到<a class="ae ls" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank"><em class="jo"/></a><em class="jo">。</em></p><p id="a687" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">当我使用pandas加载数据集时，我在Jupiter笔记本中得到这个输出。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lt"><img src="../Images/6851214a6dda6a306ea16e3e2f028107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mYEPGWwd_zZkob6bOcEiXg.png"/></div></div></figure><p id="c6bd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">但是在输出窗口中你可能想知道什么是有益分子和有益分母？</em>T13】</strong></p><blockquote class="lu lv lw"><p id="442a" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi">有用性分子:</em> </strong> <em class="hi">认为评论对自己有用的人数。</em></p><p id="54d5" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi">有用性分母:</em> </strong> <em class="hi">表示认为评论有用与否的人数。</em></p></blockquote><p id="2392" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在机器学习中，数据清洗非常重要，我们希望使用Pandas对数据进行预处理。</p><p id="d475" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">观察到(如下图所示)评论数据有许多重复条目。因此，有必要删除重复，以获得数据分析的公正结果。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ma"><img src="../Images/df0be0c22bee2a0a658c2e31e7a2db97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRdYyixGsOzJf5fEFd5zUQ.png"/></div></div></figure><p id="db3b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">从上面可以看出，同一用户对具有相同的有用性分子、有用性分母、分数、时间、摘要和文本值的进行了多次评论，在进行分析时发现</em></p><p id="c135" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> ProductId=B000HDOPZG是Loacker Quadratini香草威化饼干，8.82盎司包装(一包8个)</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mb"><img src="../Images/7723fd78a66c8c4f1b1eb640fe58862c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EyU641UXrBiTrdf10OlORg.png"/></div></div></figure><p id="5442" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> ProductId=B000HDL1RQ是Loacker Quadratini柠檬威化饼干、8.82盎司包装(一包8个)等等</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mc"><img src="../Images/9e24b1c32e2cca6025b31971cd1acd66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qO48JZ3MamjxmusG3Qot9w.png"/></div></div></figure><p id="e435" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">我们可以看到，上面两个图像中，两种产品的品牌相同，但味道不同。</em> </strong></p><p id="290f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">经过分析推断，除了ProductId之外具有相同参数的评论属于相同的产品，只是具有不同的味道或数量。因此，为了减少冗余，决定消除具有相同参数的行。</em></p><p id="6f56" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">同样的方法是，我们首先根据产品Id对数据进行排序，然后只保留第一个类似的产品评论，删除其他的。在上面的例子中，只剩下对ProductId=B000HDL1RQ的审查。这种方法确保每个产品只有一个代表，并且不进行排序的重复数据删除可能会导致同一产品仍然存在不同的代表。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es md"><img src="../Images/be7b3a7bda6abd791410061722b140b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bD_Gr-WCaoaks2UkYHT1g.png"/></div></div></figure><p id="f3d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">删除这些重复数据后，我们可以检查原始数据中还剩下多少数据。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es me"><img src="../Images/047e9b8074dd91f3fd51996777f7d8fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QWxi5Pxx_PqmR1Q58igG4Q.png"/></div></div></figure><p id="a0a6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到，在删除重复数据后，只剩下69.25 %的数据，我们观察到原始数据中有30.75%的数据是重复的。</p><p id="0b32" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们还可以看到，在下图中，我们可以观察到在下面给出的两行中，Helpfulness分子的值大于Helpfulness分母，这实际上是不可能的，因此这两行也被从计算中删除。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mf"><img src="../Images/5cadba6dd61e071ced24d68b6b9b54a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4XU6Q5w-cylWb5RJzEp_Bg.png"/></div></div></figure><p id="9e8a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">所以我们要删除包含Helpfulness分子大于Helpfulness分母的行的数据。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mg"><img src="../Images/df9940553f56669df18e8919b2bbb329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wPuY9OjsRM76l6b9YWPTew.png"/></div></div></figure><p id="2f1f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">什么是最终的[‘分数’]。value_counts()它在我们给定的数据集中有307061个点是正的，57110个点是负的。</em></p><blockquote class="lu lv lw"><p id="65a1" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated">但是这里有一个问题，我们可以看到在我们的数据集中有文本和摘要列。这些是文本特征，但在机器学习中，我们只想使用数字特征来建立模型。现在，问题是如何将文本特征转换成数字向量？T9】</p></blockquote><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/aaf744c56a754b653141fcd691a6de09.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*PSe-ClEynOHA5VOPgAjlJQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><strong class="bd jr">文本特征</strong></figcaption></figure><p id="ca18" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">但是你可能会想为什么我们要把文本转换成数字向量呢？</em></p><p id="7290" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">考虑如果我们的文本特征被转换成d维数字向量，那么我们可以画出平面的法线来分离正面评论和负面评论，考虑下面的图像蓝色十字是正面评论，红色十字是负面评论，由平面w分开。</em></p><p id="d0d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">如果假设有人对分类给出了新的评价，那么我们将其转换为数字向量并乘以权重，如果结果为&gt; 0，我们认为是正面评价，如果结果为&lt; 0，我们认为是负面评价。</em></p><p id="9209" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">因为从线性代数来看，所有的点都在平面法线的相同方向上被认为是正的，而平面法线的相反方向被认为是负的。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mi"><img src="../Images/c7063edfa5601b3273a3989a010efc96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FB8mgskBImchCN-hlda_w.png"/></div></div></figure><p id="699d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">所以我们想把文本特征转换成数字向量。但问题是如何转换成数值向量呢？</em></p><blockquote class="lu lv lw"><p id="e447" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi">答案是自然语言处理:</em> </strong> <em class="hi">我们使用自然语言处理的一些技术将文本转换成数值向量。</em></p><p id="bd4f" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi">字</em> </strong> <em class="hi">嵌入或</em> <strong class="is hj"> <em class="hi">字</em> </strong> <em class="hi">矢量化是一种方法论，在</em> <strong class="is hj"> <em class="hi"> NLP到</em> </strong> <em class="hi">图中从词汇</em><strong class="is hj"><em class="hi"/></strong><em class="hi">到</em><strong class="is hj"/><em class="hi">的单词或词组对应一个向量，其中用到的实数</em> <strong class="is hj"> <em class="hi">将文字</em> <strong class="is hj"> <em class="hi">转换成数字</em> </strong> <em class="hi">的</em> <strong class="is hj"> <em class="hi">过程称为矢量化。… </em></strong></strong></p></blockquote><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/ea93c54c4c18c78e97be8e1b14e6267f.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*egmUJz0Pxi2OOrALCVAIuw.png"/></div></figure><p id="1050" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">文本和数值向量有什么关系？</em> </strong></p><ol class=""><li id="6929" class="lc ld hi is b it iu ix iy jb le jf lf jj lg jn lh li lj lk bi translated"><em class="jo">考虑如果两个评论非常相似，那么这两个向量之间的距离很小，即相似的点必须更近。</em></li><li id="a890" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">如果两条评论的向量距离大于评论相异度。</em></li><li id="0192" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">如果两个评论r1、r2更相似，则评论v1和v2的向量表示必定接近。</em></li><li id="26b7" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">相似的文字必须在几何上接近。</em></li><li id="f7d4" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated">我们想找到一种方法，它把文本作为输入，把数字向量作为输出，这样，相似的文本在几何上必须是接近的。</li></ol><blockquote class="lu lv lw"><p id="b5cc" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi">我们使用其中的一些技术来寻找给定文本的矢量表示:</em> </strong></p><p id="8c06" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><em class="hi">包话</em></p><p id="b3b5" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><em class="hi">TF–IDF</em></p><p id="6cbd" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><em class="hi"> Word2vec </em></p><p id="c53a" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><em class="hi">平均Word2vec </em></p><p id="b56a" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><em class="hi">平均TF–IDF word 2 vec</em></p></blockquote><p id="356a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">在自然语言处理中，文本被称为文档，文档的集合被称为语料库。</em></p><p id="4644" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们深入研究自然语言处理技术之前，我们首先对给定的文本数据做一些预处理。我们使用文本预处理是因为要减少给定文本数据的大小，这些都是增加文本数据的长度。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/2b9effbd34950fae9a4bac59e56ed0a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*P9agr-brgGGbTCjA7PEBvQ.png"/></div></figure><p id="da43" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">文本预处理:</em> </strong></p><ol class=""><li id="86f4" class="lc ld hi is b it iu ix iy jb le jf lf jj lg jn lh li lj lk bi translated">从移除html标签开始。</li><li id="7453" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated">删除标点符号或有限的特殊字符，如，或。或#等。</li><li id="593d" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated">检查单词是否由英文字母组成，并且不是字母数字。</li><li id="dfd3" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">检查单词的长度是否大于2(据研究，2个字母中没有形容词)。</em></li><li id="bb04" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">将单词转换成小写。</em></li><li id="0af0" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">删除停用词(停用词在文本中被视为干扰。文本可能包含停用词，如is、am、are、this、a、an、the等)。</em></li><li id="4135" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">最后，像滚雪球一样给单词做词干(词干是将相关单词转化为基本形式的过程。词干有助于将单词简化为一种叫做词干的紧凑形式)。这些词的基本形式是tast，词干之后。</em></li></ol><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ml"><img src="../Images/1319b268c6312778da6c75a0fea80a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sfs9OxEdzIg-LaX5QM2jfA.png"/></div></div></figure><p id="d29d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"><em class="jo">python中文本预处理的所有步骤如下:</em> </strong></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/ec9a8063bc924d04109445ab5d5f079e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*dTqYdMyQdvJ8V8mjWqajEQ.png"/></div></figure><p id="c8a7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">对文本进行预处理后，如下图所示，并将其存储以备将来使用，同样，我们也可以对摘要进行此操作。</em>T11】</strong></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mf"><img src="../Images/c8ccdd6c03c182e81d10f3896216946c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DLJkMjDDKifDxCjLiPg8sA.png"/></div></div></figure><p id="e6e6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">文本特征化技巧:</em> </strong></p><blockquote class="lu lv lw"><p id="525f" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi">鞠躬(袋字):</em> </strong></p></blockquote><p id="2dce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">词袋(BOW)是一种从文本文档中提取特征的方法。这些特征可以用于训练机器学习算法。它创建了在训练集中的所有文档中出现的所有唯一单词的词汇表。</em></p><p id="c6e4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">简单来说，就是一个单词的集合，用字数来表示一个句子，大多不考虑它们出现的顺序。</em>T25】</strong></p><p id="faee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> BOW是一种广泛使用的方法:</em></p><ol class=""><li id="9879" class="lc ld hi is b it iu ix iy jb le jf lf jj lg jn lh li lj lk bi translated">自然语言处理。</li><li id="198e" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">从文档中检索信息。</em></li><li id="d6c8" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><em class="jo">文档分类。</em></li></ol><p id="5e68" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在高层次上，它包括以下步骤。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mn"><img src="../Images/673c5aa14e01d7ad5cdc26b01f57f121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lHax1ehB_kWd2mfqRt0jTw.png"/></div></div></figure><p id="e2e9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">什么是词向量？</strong></p><p id="4eed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在一个层面上，它只是一个权重向量。在简单的1-of-N(或“one-hot”)编码中，向量中的每个元素都与词汇表中的一个单词相关联。给定单词的编码只是一个向量，其中相应的元素被设置为1，所有其他元素都是0。</p><p id="55ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设我们的词汇只有五个词:国王、王后、男人、女人和孩子。我们可以将单词“Queen”编码为:</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/5dedd5d81fc26ec4ba325daacc50e88f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*FSPYz_UNjq5ib4RKnLoFXw.png"/></div></figure><p id="0fad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">标记化:</em> </strong></p><p id="5247" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">标记化是文本分析的第一步。将一个文本段落分解成更小的块，如单词或句子的过程称为标记化。标记是一个单独的实体，它是句子或段落的组成部分。</em></p><h2 id="b9a2" class="mp jq hi bd jr mq mr ms jv mt mu mv jz jb mw mx kd jf my mz kh jj na nb kl nc bi translated">句子标记化</h2><p id="2779" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated"><em class="jo">句子标记器将文本段落分割成句子。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es nd"><img src="../Images/6adc0d5b234a081ee10a615c2ca34a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8QvvAErlYqLL3eEZpP_z1Q.png"/></div></div></figure><h2 id="fec1" class="mp jq hi bd jr mq mr ms jv mt mu mv jz jb mw mx kd jf my mz kh jj na nb kl nc bi translated"><em class="kn">单词标记化</em></h2><p id="a869" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated"><em class="jo">分词器将文本段落拆分成单词。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mc"><img src="../Images/b4ef76fbd7993074cd6e7a2a8633f98c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_RYR1VCC-RwBjThOeAhQvA.png"/></div></div></figure><h2 id="e10b" class="mp jq hi bd jr mq mr ms jv mt mu mv jz jb mw mx kd jf my mz kh jj na nb kl nc bi translated">使用单词包的特征生成</h2><p id="c1df" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated"><em class="jo">在文本分类问题中，我们有一组文本和它们各自的标签。但是我们不能直接在模型中使用文本。你需要把这些文本转换成一些数字或者数字的向量。</em></p><p id="f0f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">单词袋模型(BoW)是从文本中提取特征的最简单的方法。BoW将文本转换成文档中单词出现的矩阵。这个模型关注给定的单词是否出现在文档中。</p><p id="ece4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">举例:假设有三个文档:</em></p><p id="1997" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">医生1:我喜欢狗。</em></p><p id="6f26" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">医生2:我讨厌狗和编织。</p><p id="085b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">文件3:编织是我的爱好和激情。</p><p id="1d17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">现在，您可以通过统计单词在给定文档中的出现次数来创建文档和单词的矩阵。创建包含三个文档中所有单词的文档词汇表，并从句子中提取单词。</em></p><p id="0e92" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">计数并放入该词将在向量中存在多少次，否则放0。如果我们的词汇量的维数很大，那么我们的向量v1是稀疏向量，这意味着向量中的大多数元素都是零。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ne"><img src="../Images/aea9b943b5fcd27a0ac5bb53d5d86c58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*luqh_f_VR3ktTUN4vM1DAw.png"/></div></div></figure><p id="04d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">可以看到，</em> <strong class="is hj"> <em class="jo">每个句子都与我们生成的单词列表进行了比较。基于该比较，向量元素值可以递增</em> </strong> <em class="jo">。这些向量可以在ML算法中用于文档分类和预测。</em></p><p id="1e82" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">示例:Doc1-Doc2 = | | Doc1-Doc2 | | = sqrt(0+1+0+1+1+0+0+0+0)</p><p id="c401" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Doc1-Doc2=sqrt(4)=2</p><p id="74cd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">二进制包词(BoW): </em> </strong> <em class="jo">在二进制包词中我们考虑的不是词出现的次数，我们只考虑词是否退出。如果该字存在，则放1，否则将0赋给向量。</em></p><p id="6fd1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">这个矩阵使用的是单个单词。它可以是两个或更多单词的组合，这被称为二元或三元模型，一般的方法被称为n元模型。</em></p><h1 id="1f50" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><em class="kn">洞察包话:</em></h1><p id="ea74" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated"><em class="jo">BOW模型只考虑一个已知单词是否出现在文档中。它不关心它们出现的意义、上下文和顺序。</em></p><p id="23f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这给出了相似文档将具有彼此相似的字数的洞察。换句话说，两个文档中的单词越相似，文档就越相似。</p><h1 id="f096" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><em class="kn">弓的局限性</em></h1><ol class=""><li id="9d1b" class="lc ld hi is b it ko ix kp jb nf jf ng jj nh jn lh li lj lk bi translated"><strong class="is hj"> <em class="jo">语义</em> </strong> <em class="jo">:基本的BOW方法不考虑文档中单词的含义。它完全忽略了使用它的上下文。根据上下文或邻近单词，同一个单词可以用在多个地方。</em></li><li id="5422" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated"><strong class="is hj"> <em class="jo">矢量大小</em> </strong> <em class="jo">:对于一个大文档，矢量大小可能会很大，导致大量的计算和时间。您可能需要根据与您的用例的相关性来忽略单词。</em></li></ol><h1 id="226a" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"><em class="kn">sci-kit中亚马逊美食评论数据集上Bow的实现</em> </strong></h1><p id="f4b4" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">你不必在需要的时候编写BOW代码。它已经是许多可用框架的一部分，如sci-kit learn中的CountVectorizer。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ni"><img src="../Images/7d4c3cc57b46750fbad1291ce49098b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uWf2VaJkKASsZYnoIqGlqg.png"/></div></div></figure><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es nj"><img src="../Images/ceeb3db0cd9dccca5728022a9346b537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6L13WvB6bDsVHQyEUH9ujg.png"/></div></div></figure><blockquote class="lu lv lw"><p id="26d0" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><strong class="is hj"> TF-IDF(词频-逆文档频):</strong></p></blockquote><p id="1e60" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> TF-IDF代表</em> <strong class="is hj"> <em class="jo">“词频—逆文档频”</em> </strong> <em class="jo">。这是一种量化文档中单词的技术，我们通常计算每个单词的权重，这表示该单词在文档和语料库中的重要性。该方法是信息检索和文本挖掘中广泛使用的技术。</em></p><p id="e5ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">【词频(tf) </em> </strong> <em class="jo">:词频是该词在语料库中各个文档中出现的频率。它是单词在文档中出现的次数与该文档中单词总数的比率。它随着该单词在文档中出现的次数的增加而增加。每个文档都有自己的tf。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es nk"><img src="../Images/b593934f3344964f78e0c25735a4cf89.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/0*Eya6jO4PRUVL_2kp.png"/></div></div></figure><p id="edcb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">术语频率给出了单词‘w’在文档d中出现的频率。如果更频繁，单词‘w’TF将增加。</em></p><p id="9302" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">逆数据频率(idf): </em> </strong> <em class="jo">这个</em> <strong class="is hj"> <em class="jo"> </em> </strong> <em class="jo">衡量文档在整套语料库中的重要性，这个和tf很相似。唯一的区别是tf是文档d中术语t的频率计数器，其中as DF是术语t在文档集合n中出现</em> <strong class="is hj"> <em class="jo">次</em> </strong> <em class="jo">的计数</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/f2f7f63524ca9904e7152c3b091d4aa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/0*SeusojPxCo1RZ3Hq.png"/></div></figure><p id="6641" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> idf为我们提供了语料库中所有文档中稀有词的权重。在语料库中很少出现的单词具有高idf分数。</em></p><p id="3496" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">如果我们已经计算了tf值，并且如果这产生了文档的矢量化形式，为什么不仅仅使用tf来寻找文档之间的相关性呢？我们为什么需要英特尔信息技术峰会？</em></p><p id="2680" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">让我解释一下，虽然我们计算了tf值，但仍然有一些问题，例如，像“是，是”这样最常见的词将具有非常高的值，赋予这些词非常高的重要性。但是用这些词来计算相关性会产生不好的结果。这种常见的词被称为停用词，尽管我们将在后面的预处理步骤中删除停用词，找出该词在所有文档中的重要性，并使用该值进行规范化，这样可以更好地代表文档。</em></p><p id="b307" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">最后，通过取tf和idf的乘积值，我们得到tf-idf分数，tf-idf有许多不同的变体，但现在让我们集中于这个基本版本。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es nm"><img src="../Images/fac84a6a20cbe4ca7a6fbbf8a3037811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/0*9U0wL8qKshUcvzRw.png"/></div></figure><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es nn"><img src="../Images/c03e5ea4b50a627e89858c4d52bb3c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/0*yGUtjE9K7ed7RcTd.png"/></div></figure><p id="168a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">让我们举个例子来更清楚地了解一下。</em></p><p id="f5ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">第一句:汽车在路上行驶。</em></p><p id="3b3a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第二句:卡车在高速公路上行驶。</p><p id="6997" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个例子中，每个句子都是一个单独的文档。</p><p id="edbf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">我们现在将计算代表我们语料库的上述两个文档的tf-idf。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es no"><img src="../Images/2bf25e5d6dd3fd1a96fca092413c2edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*z0K0lu_tlW4nHKMC.png"/></div></div></figure><p id="f2b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">从上表可以看出，常用词的tf-idf为零，说明不显著。另一方面，“汽车”、“卡车”、“公路”和“高速公路”的tf-idf不为零。这些话有更多的意义。</em></p><p id="8db1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在tf-idf中，我们更加重视:</p><ol class=""><li id="687a" class="lc ld hi is b it iu ix iy jb le jf lf jj lg jn lh li lj lk bi translated">生僻字出现在我们的语料库中。</li><li id="fef4" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn lh li lj lk bi translated">我们的文档中出现了更频繁的单词。</li></ol><h1 id="4ce9" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">tf-idf的局限性</h1><ol class=""><li id="7fa0" class="lc ld hi is b it ko ix kp jb nf jf ng jj nh jn lh li lj lk bi translated"><strong class="is hj"> <em class="jo">语义</em> </strong> <em class="jo">:这也不考虑词的语义。它完全忽略了使用它的上下文。根据上下文或邻近单词，同一个单词可以用在多个地方。</em></li></ol><h1 id="9496" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> <em class="kn">在sci-kit learn中的Amazon food reviews数据集上实现TF-IDF</em></strong></h1><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es np"><img src="../Images/f04a4b93a548b85cb66b314ca359e36e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YD9COorfnINBF7Hl4sfoiw.png"/></div></div></figure><p id="b114" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">基于分数的前10个特征由</em>给出</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es nq"><img src="../Images/a43c7aa53d99b73176af1d248b0995ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*RsOvQaKxvMh5dPQz97sH-w.png"/></div></figure><blockquote class="lu lv lw"><p id="943a" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi"> word2vec: </em> </strong></p></blockquote><p id="f793" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">这种技术是最先进的算法，它考虑单词的语义。</em></p><p id="db5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">如果我们给这个词它就转换成矢量。它还自动从文本中学习关系。</em></p><p id="a66b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">word 2 vec模型的输出是</em> <strong class="is hj"> <em class="jo">密集</em> </strong> <em class="jo">矢量。</em> Word2vec模型需要大文本语料库。</p><p id="7423" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">在word2vec中，使用了单词的分布式表示。取一个几百维的向量(比如1000)。每个单词都由这些元素的权重分布来表示。因此，向量中的元素和单词之间不是一对一的映射，而是单词的表示分布在向量中的所有元素上，向量中的每个元素都有助于许多单词的定义。</em></p><p id="7e5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">如果我在一个假设的词向量中标注维度(当然，在算法中没有这种预先指定的标注)，它可能看起来有点像这样:</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es nr"><img src="../Images/0f81fe5669330768c5c4d62023c6ffd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*FuY-LcngjRN523xgtZonIw.png"/></div></figure><p id="562c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这样的向量以某种抽象的方式来代表一个单词的“意思”。正如我们接下来将看到的，简单地通过检查一个大型语料库，就有可能学习到能够以一种令人惊讶的表达方式捕捉单词之间关系的单词向量。我们也可以使用向量作为神经网络的输入。</p><p id="c9bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">矢为国王、男人、王后、&amp;女人:</em> </strong></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es ns"><img src="../Images/f26794512bea07fede6244a783c70616.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*b202zp1SAGfksJCmQy_83A.png"/></div></figure><p id="a619" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">向量构图王的结果——男人+女人=？</em>T47】</strong></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/bfaf2ee3256778f49ac18ccc03bf3ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*dUlcQL-aKz-8N45tCac3PQ.png"/></div></figure><p id="5952" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae ls" href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> <em class="jo">要了解word2vec模型的完整工作原理，请访问此处。</em>T52</strong></a></p><h1 id="fa46" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"><em class="kn">sci-kit learn</em></strong>中亚马逊美食评论数据集上word2vec的实现</h1><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es nu"><img src="../Images/f452c380140badd4e8515d3955dc26b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mIm3KX5QMkyOEyhA6FAwrw.png"/></div></div></figure><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es mg"><img src="../Images/72ac8858997e2ec796e8d436585d9a43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DbjFzt0aA9QLEom3JvfeKQ.png"/></div></div></figure><p id="7c54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">训练后的模型相似度输出如下图:</em> </strong></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es nv"><img src="../Images/f79195613a93cb17ff0d076887ac49d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k9qCrQ2Y8qDD7RftCzCrdQ.png"/></div></div></figure><p id="7c11" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">词语的简单相似度也由:</em> </strong>给出</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es nw"><img src="../Images/92b0baa307399ba00293a00b5ac66d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*79Bnre75Vz9arp13w2sh1w.png"/></div></figure><blockquote class="lu lv lw"><p id="ff8b" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi">平均字2vec: </em> </strong></p></blockquote><p id="0730" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">我们可以使用Word2Vec训练我们的模型，然后使用</em> <strong class="is hj"> <em class="jo">句子</em> </strong> <em class="jo">向量。</em><strong class="is hj"><em class="jo"/></strong><em class="jo"/><strong class="is hj"><em class="jo">word 2 vec</em></strong><em class="jo">矢量。我们可以只取一个</em> <strong class="is hj"> <em class="jo">句子</em> </strong> <em class="jo">中所有词向量的</em><strong class="is hj"><em class="jo"/></strong><em class="jo">。这个</em> <strong class="is hj"> <em class="jo">平均</em> </strong> <em class="jo">矢量将代表你的</em> <strong class="is hj"> <em class="jo">句子</em> </strong> <em class="jo">矢量。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es nx"><img src="../Images/83a648468cbcebb4cd1e251c781fd6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*oW9Q1_fRpyK-AZlvq3KBuQ.png"/></div></figure><p id="6d18" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们需要给出大型文本语料库，其中每个单词都会创建一个向量。它试图从原始文本中自动学习向量之间的关系。向量的维数越大，它的信息量就越大。</p><h1 id="2ccc" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">sci-kit learn中Amazon food reviews数据集上<strong class="ak"><em class="kn">Average word 2 vec</em></strong>的实现</h1><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ny"><img src="../Images/4c06634563b5959eda2af4a3a634c5d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rH7ITCXwJDP7DYOwTuEFUw.png"/></div></div></figure><blockquote class="lu lv lw"><p id="40d6" class="iq ir jo is b it iu iv iw ix iy iz ja lx jc jd je ly jg jh ji lz jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi"> tf-idf加权Word2vec: </em> </strong></p></blockquote><p id="76c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种方法中，我们将首先计算每个单词的tf-idf值，然后乘以每个单词的word2vec值并取其平均值。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es nz"><img src="../Images/582dc81ff9aedcbcd3d9cfbcd825da8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W38PGs3wbgETDvwbq7Wn6Q.png"/></div></div></figure><h1 id="498e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">在sci-kit learn中的Amazon food reviews数据集上实现tf-idf加权word2vec</h1><p id="4395" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated"><em class="jo">首先我们会计算每个词的tf-idf值。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es oa"><img src="../Images/1e0dbfd1774527fcb42f78070a5b6d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJZQYEid9Ry6o1Cww8O3TA.png"/></div></div></figure><p id="1b5d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">然后我们计算每个单词的word2vec，乘以单词值的tf-idf，取其平均值。</em></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ob"><img src="../Images/91177cb9e1fa1cfcd053f2814d025308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DB_u6guDhbL1YqK2MPQnnA.png"/></div></div></figure><p id="a882" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">这是通过使用来自Kaggle的真实世界数据集Amazon Food Reviews的自然语言处理的文本特征化的小介绍。</em></p><p id="6e14" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae ls" href="https://github.com/Sachin-D-N/Amazon_Food_Reviews/blob/main/01.Amazon_Fine_Food_Review_Analysis_Featurization/Amazon%20Fine%20Food%20Reviews%20Analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> <em class="jo">完整代码请访问我的GitHub资源库点击这里</em> </strong> </a></p><h1 id="d573" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">参考</h1><ul class=""><li id="9a23" class="lc ld hi is b it ko ix kp jb nf jf ng jj nh jn oc li lj lk bi translated">应用人工智能</li><li id="11ca" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn oc li lj lk bi translated">Coursera</li><li id="3359" class="lc ld hi is b it ll ix lm jb ln jf lo jj lp jn oc li lj lk bi translated">数据营</li></ul><p id="a98e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">感谢您的阅读和耐心。如果帖子中有错误，请告诉我。让我们在评论中讨论，如果你发现帖子中有什么错误，或者你有什么要补充的..</p></div></div>    
</body>
</html>