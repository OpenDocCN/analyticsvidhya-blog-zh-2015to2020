<html>
<head>
<title>Understanding Gradients in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解机器学习中的梯度</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-gradients-in-machine-learning-60fff04c6400?source=collection_archive---------8-----------------------#2020-10-23">https://medium.com/analytics-vidhya/understanding-gradients-in-machine-learning-60fff04c6400?source=collection_archive---------8-----------------------#2020-10-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6b70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我第一次开始研究神经网络时，我立即遇到了反向传播梯度的公式，从网络末端计算的损失函数开始，一层一层地返回。这些公式看起来已经很复杂了，尽管做了几个基本假设——比如一个完全连接的网络和每个神经元后的 sigmoid 激活函数。我想知道像 TensorFlow 和 Pytorch 这样流行的软件包如何执行相同的操作，但对于任意的数学函数。</p><p id="29f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现代机器学习包使用“自动微分”(或“亲笔签名”)来处理这一点，有了这个名字，听起来好像一切都只是发生；就好像你写了运算，计算机就能算出所有的导数。但不可能这么简单。在诸如卷积之类的运算中，使用位于输入图像上不同位置的某一选定大小的权重滤波器来计算乘法和，并且结果是新图像，其中每个值对应于一个这样的和。“亲笔签名”是怎么处理这个的？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/4bfd472b3bf0dca0a3cdf09cbf08a76a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dsMMakYntIu9b9DFMgoV0Q.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">一个 2x2 卷积:如何对<strong class="bd jt"> <em class="ju">的那个</em> </strong>求导？(图由作者提供)</figcaption></figure><p id="bbf0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">在这里我们将:</strong></p><ol class=""><li id="7480" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><em class="ke">展示反向传播是如何从分化的“链式法则”中出现的。</em></li><li id="a199" class="jv jw hi ih b ii kf im kg iq kh iu ki iy kj jc ka kb kc kd bi translated"><a class="ae kk" href="https://colab.research.google.com/github/joshue031/gradients/blob/main/nn_gradients.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="ke">通过一些具体的例子来计算导数。</em> </a> <em class="ke">我们将使用 TensorFlow，旨在准确理解在张量上调用</em> <code class="du kl km kn ko b"><em class="ke">gradient</em></code> <em class="ke">函数时计算的内容。</em></li></ol><h1 id="dd93" class="kp kq hi bd jt kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">链式法则</h1><p id="a9c7" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">在训练神经网络时，我们的目标是通过调整网络的可训练参数来最小化一些损失函数。这意味着，在每个训练步骤，计算损失函数相对于每个参数<em class="ke"> W </em>的导数，并以减少损失<em class="ke"> L </em>的方式调整这些参数——通过在导数的相反(-)方向推动它们一小步，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/71c8be6ddeed6e176281ce9ddcadacfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*Yg4uPKM299iKI11P_mJ1_g.png"/></div></figure><p id="73dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来的任务是为每个参数找到∂·l·/∂w。现代机器学习软件包使用计算图来解决这个问题，我们将看到这如何允许我们将问题分解成可管理的部分。损失函数是图的参数的多元函数，因此为了找到所有的导数，我们可以应用链式法则。</p><p id="4c64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">链式法则例如规定，对于两个变量<em class="ke"> x1 </em>和<em class="ke"> x2 </em>的函数<em class="ke"> f </em>，它们都是第三个变量<em class="ke"> t </em>的函数，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/c0da42a1202d2985c3af11a2ecc9404e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*YwYFx9Wqdg1Orp1wIlBVSg.png"/></div></figure><p id="be4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑下图:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lt"><img src="../Images/fc717a0315f10d1fa0846f3bfd0cb811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9WoHUeXqILVf8v-FON5voQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(图由作者提供)</figcaption></figure><p id="5688" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们假设<em class="ke"> W </em>、<em class="ke"> V </em>和<em class="ke"> U </em>每个都代表几个参数——它们可以是向量、矩阵或一些高阶张量——但是现在我们只使用单个下标来表示它们。我们还将假设所有输入和输出(<em class="ke"> x1 </em>、<em class="ke"> x2 </em>、<em class="ke"> y1 </em>、<em class="ke"> y2 </em>和<em class="ke"> z </em>)都是向量，并且为了简单起见，将使用单个下标<em class="ke"> x </em>来遍历<em class="ke"> x1 </em>和<em class="ke"> x2 </em>的所有元素，对于<em class="ke"> y </em>也是如此。现在使用链式法则来寻找损耗<em class="ke"> L </em>相对于 W 参数之一的导数，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/060c95ebd3d903e3fc44d16a884a90fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FhPx1ix9OIuG_noD2AEafw.png"/></div></div></figure><p id="6463" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">开始看起来，对于每个参数，我们需要解一个像这样的长方程，但是现在让我们再多做一点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/70419a828ccfbf38468c0c7f6aa58ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*tFLPjXGi2x37s7l86QTsnA.png"/></div></figure><p id="2439" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以定义数量</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/3b01469b417572d1c1736ce633502c77.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*SfNw7xC5Bba8noXpXQVhrQ.png"/></div></figure><p id="daad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">z 方向上的小位移，然后沿着图表“向后发送”。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lx"><img src="../Images/c602b522c3c0de1676ee80c285d1ed94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*VO48Ox_2mAuQHi2yDEdktg.png"/></div></div></figure><p id="f69f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/5b70543a033efe3385d5f8abb248aba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*vmTsm9HA4W9Eh3SnEgH3tw.png"/></div></figure><p id="7e54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此<em class="ke">和</em>的小位移遵循相同的模式，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/477ff1c59b9e9cb6fd51afc0b1617da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*qRW8s_UtpNMGfazjEng8eA.png"/></div></figure><p id="8b4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要注意两件重要的事情:</p><ul class=""><li id="bc76" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ma kb kc kd bi translated">值只能沿着有一些相关性的路径反向传播。例如，图中的值<em class="ke"> dy2 </em>都不会影响任何<em class="ke"> dWi </em>，因为<em class="ke"> y2 </em>的分量相对于 Wi 的所有偏导数都为零。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mb"><img src="../Images/6680ae310a307d84f4ef4dda804cea15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AlzhtZxt-rRChWmK7ebAjQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">为了满足链式法则，梯度沿着与信息最初向前发送的路径相同的路径向后发送。(图由作者提供)</figcaption></figure><ul class=""><li id="25d1" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ma kb kc kd bi translated">当执行反向传播时，不需要立刻担心整个网络。对于每个节点，我们只需要考虑通过输出通道发送的梯度，使用它们来计算该节点处参数的导数，然后通过输入通道发送回要在先前节点中使用的正确梯度。如果我们对每个节点都这样做，整个网络的梯度就可以解决。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mc"><img src="../Images/7af2627ff549bbbd667c2094b83d2743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xOfuFzDmh99ewuw9EetVQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">对于网络的每个节点，我们的任务是:使用输入梯度 dy1 和 dy2 来计算参数 dW 中的位移，并计算输出梯度 dx1 和 dx2。(图由作者提供)</figcaption></figure><p id="9220" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Autograd 处理这方面的系统性，但它并不神奇——鉴于每个节点上应用的函数的导数(必须明确定义),它可以使用这一程序在整个网络中反向传播梯度。</p><p id="e216" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们通过 TensorFlow 中的一些具体例子来了解如何利用张量输入和输出进行求导的细节。</p><p id="b7f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="ke"/></strong><a class="ae kk" href="https://colab.research.google.com/github/joshue031/gradients/blob/main/nn_gradients.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="ke">你可以使用这个 Jupyter 笔记本</em> </strong> </a> <strong class="ih hj"> <em class="ke">在 Colab 中交互地完成这些例子。</em>T49】</strong></p><h1 id="72a0" class="kp kq hi bd jt kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">示例 sigmoid 函数</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es md"><img src="../Images/825e4838e75c644859f5e6c691cbcad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7LwDB8VxFAp-tdfwIJE4Yg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(图由作者提供)</figcaption></figure><p id="1cea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">sigmoid 函数为每个元素<em class="ke"> xi </em>产生一个输出<em class="ke">子</em>，由下式给出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es me"><img src="../Images/49beb49d4645b2ccc071d5d0f685f7b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*v0V3IgIkO04EywSrbdBx1g.png"/></div></figure><p id="0963" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">鉴于:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/4852b0cc9a0f73c015a60a2e7ead4fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*UjZ1umfu5DFfYicPZuX9TQ.png"/></div></figure><p id="71e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算梯度<em class="ke"> dx </em>。记住，如上所述，这意味着用分量计算矢量</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/4e4671dd700847595f49b8bfdfaea003.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*d1N4BTV41sE7SVCLqubCfA.png"/></div></figure><h2 id="c33a" class="mh kq hi bd jt mi mj mk ku ml mm mn ky iq mo mp lc iu mq mr lg iy ms mt lk mu bi translated">张量流代码</h2><p id="61e6" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">下面是问题设置:</p><pre class="je jf jg jh fd mv ko mw mx aw my bi"><span id="0094" class="mh kq hi ko b fi mz na l nb nc">import tensorflow as tf</span><span id="6d9b" class="mh kq hi ko b fi nd na l nb nc"># Define inputs and output gradients.<br/>x = tf.constant([3.0, 4.0, 5.0])<br/>dz = tf.constant([1.0, 2.0, 3.0])</span><span id="a3fb" class="mh kq hi ko b fi nd na l nb nc"># Define the gradient.<br/>def grad_sigmoid(x, dz):<br/>    # (Add implementation here)<br/>dx = grad_sigmoid(x,dz)</span><span id="896f" class="mh kq hi ko b fi nd na l nb nc"># Compute the gradient with Tensorflow.<br/>with tf.GradientTape() as g:<br/>    g.watch(x)<br/>    z = tf.sigmoid(x)<br/>dx_tf = g.gradient(z, x, output_gradients=dz)</span><span id="a358" class="mh kq hi ko b fi nd na l nb nc"># Check the answer.<br/>print(dx == dx_tf)</span></pre><h2 id="d325" class="mh kq hi bd jt mi mj mk ku ml mm mn ky iq mo mp lc iu mq mr lg iy ms mt lk mu bi translated">解决办法</h2><p id="59b4" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">注意，我们将利用 Kronecker delta，如果它的两个索引相同，则等于 1，否则等于 0，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ne"><img src="../Images/e54a92ca539a4672ebedf74f99183d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*0_-3Qvzx2mKpVw4nJIpG9g.png"/></div></figure><p id="5d7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">求导，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nf"><img src="../Images/4d0a4b24a86eaaba33a186e4b5e05680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*B3L4eLR-NrzDdbBan9b0_Q.png"/></div></figure><p id="834c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并计算梯度，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ng"><img src="../Images/5f13748a274f92cd9f4454513a03557c.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*8RnclRGMKwDhbgVLgxsBOA.png"/></div></div></figure><p id="df4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以用矢量点积符号来表示(将数字结果四舍五入到 3 位有效数字)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nh"><img src="../Images/f472632972f684d6a732467d940b8380.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*wHZEQBOJ9QQlCAQqwO5ygQ.png"/></div></figure><p id="0e18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ni"><img src="../Images/197892e154136474d65d48ac1c65d0bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*4peviqt0prSR3DWSYTv58A.png"/></div></figure><p id="bc22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面在 Tensorflow 中实现的内容如下:</p><pre class="je jf jg jh fd mv ko mw mx aw my bi"><span id="0e3e" class="mh kq hi ko b fi mz na l nb nc">def grad_sigmoid(x, dz):<br/>    return dz*tf.sigmoid(x)*(1-tf.sigmoid(x))</span></pre><h1 id="2d0d" class="kp kq hi bd jt kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">示例 softmax 函数</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es md"><img src="../Images/d8e4dac052621f18924343e60de53ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*25sZnQXa0g-Mt4E-DwEd0g.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(图由作者提供)</figcaption></figure><p id="5973" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">sigmoid 函数为输入向量<em class="ke"> x </em>生成一个包含元素的向量<em class="ke"> z </em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nj"><img src="../Images/8f78b6e4dbdba0fe970a18ef8785359c.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*5t1jnbIq0hQb8gBwAth-VQ.png"/></div></figure><p id="cd63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">鉴于:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/eb24590e02bdf9432822cea21417f78c.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*rQbWVxjKHYMQBoyS12TWUQ.png"/></div></figure><p id="ae13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算梯度<em class="ke"> dx </em>。</p><h2 id="3065" class="mh kq hi bd jt mi mj mk ku ml mm mn ky iq mo mp lc iu mq mr lg iy ms mt lk mu bi translated">张量流代码</h2><p id="8ed3" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">下面是问题设置:</p><pre class="je jf jg jh fd mv ko mw mx aw my bi"><span id="7915" class="mh kq hi ko b fi mz na l nb nc">import tensorflow as tf</span><span id="5334" class="mh kq hi ko b fi nd na l nb nc"># Define inputs<br/>x = tf.constant([3.0, 4.0, 5.0])<br/>dz = tf.constant([1.0, 2.0, 3.0])</span><span id="5d40" class="mh kq hi ko b fi nd na l nb nc"># Define the gradient.<br/>def grad_softmax(x, dz):<br/>    # (Add implementation here)<br/>dx = grad_softmax(x, dz)</span><span id="cc6d" class="mh kq hi ko b fi nd na l nb nc"># Compute the gradient with Tensorflow.<br/>with tf.GradientTape() as g:<br/>    g.watch(x)<br/>    z = tf.nn.softmax(x)<br/>dx_tf = g.gradient(z, x, output_gradients=dz)</span><span id="f59d" class="mh kq hi ko b fi nd na l nb nc"># Check the answer.<br/>print(dx == dx_tf)</span></pre><h2 id="b8f4" class="mh kq hi bd jt mi mj mk ku ml mm mn ky iq mo mp lc iu mq mr lg iy ms mt lk mu bi translated">解决办法</h2><p id="75dd" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">求导，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nk"><img src="../Images/88f4817412ef0e83541e5456335f99b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*eS66Iso3Ky_wvpbVy_I7kQ.png"/></div></figure><p id="965b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并计算梯度，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nl"><img src="../Images/1622d002bec26a9521e6c41050d80fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*bLxZVD_IsGMILFFL7yvpcA.png"/></div></figure><p id="3570" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们再次四舍五入到 3 位有效数字，并使用点积符号，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nm"><img src="../Images/d3edc5a52ab08fd8290baf0e3439448c.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*S0S1-HzD4uCOslAJJVXCgA.png"/></div></figure><p id="a5cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面在 TensorFlow 中实现的内容如下:</p><pre class="je jf jg jh fd mv ko mw mx aw my bi"><span id="096c" class="mh kq hi ko b fi mz na l nb nc">def grad_softmax(x,dz):<br/>    return tf.nn.softmax(x) * (dz - tf.tensordot(tf.nn.softmax(x), dz, 1))</span></pre><h1 id="a0e5" class="kp kq hi bd jt kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">示例 3:矩阵乘法</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nn"><img src="../Images/57a66e289b87ff1a9d6450a4846f3b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PDsfd8KBdmzDPc-w8lx_0Q.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(图由作者提供)</figcaption></figure><p id="4153" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们考虑一种情况，其中输入不是向量(秩 1 张量)，而是矩阵(秩 2 张量)。计算将是相似的，但是元素将由两个索引而不是一个来标识。</p><p id="1878" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将两个矩阵<em class="ke"> x </em>和<em class="ke"> y </em>相乘以产生一个包含元素的矩阵<em class="ke"> z </em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es no"><img src="../Images/b9217bfd187f676d799d3e89ecc1ecec.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*b1StUeYXYLPyEMUE1q5Kiw.png"/></div></figure><p id="b0ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es np"><img src="../Images/b29465248bbc1a44046babc20da643ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*o8YTu3A9McLeS8b4egmN-Q.png"/></div></figure><p id="e24c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算梯度<em class="ke"> dx </em>。注意，在计算梯度 dx 的元素时，总和中必须包括<em class="ke"> dz </em>的所有元素。因此，我们现在必须将两个指数相加为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nq"><img src="../Images/15bad4af8009d4b382be0da58ae7179b.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*TmwLA10p4Qq18esdbL-OLw.png"/></div></figure><h2 id="bde6" class="mh kq hi bd jt mi mj mk ku ml mm mn ky iq mo mp lc iu mq mr lg iy ms mt lk mu bi translated">张量流代码</h2><p id="5a28" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">下面是问题设置:</p><pre class="je jf jg jh fd mv ko mw mx aw my bi"><span id="4376" class="mh kq hi ko b fi mz na l nb nc">import tensorflow as tf</span><span id="6448" class="mh kq hi ko b fi nd na l nb nc"># Define inputs<br/>x = tf.constant([[3.0, 4.0], [5.0, 6.0]])<br/>y = tf.constant([[4.0, 5.0], [6.0, 7.0]])<br/>dz = tf.constant([[1.0, 2.0], [3.0, 4.0]])</span><span id="f024" class="mh kq hi ko b fi nd na l nb nc"># Define the gradient.<br/>def grad_matmul(x, y, dz):<br/>    # (Add implementation here)<br/>dx = grad_matmul(x, y, dz)</span><span id="5d7c" class="mh kq hi ko b fi nd na l nb nc"># Compute the gradient with Tensorflow.<br/>with tf.GradientTape() as g:<br/>    g.watch(x)<br/>    z = tf.matmul(x, y)<br/>dx_tf = g.gradient(z, x, output_gradients=dz)</span><span id="f18b" class="mh kq hi ko b fi nd na l nb nc"># Check the answer.<br/>print(dx == dx_tf)</span></pre><h2 id="4649" class="mh kq hi bd jt mi mj mk ku ml mm mn ky iq mo mp lc iu mq mr lg iy ms mt lk mu bi translated">解决办法</h2><p id="1d4b" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">因为我们现在正在处理矩阵，对某个矩阵元素的偏导数会产生两个克罗内克增量，如下所示</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nr"><img src="../Images/d5db6bd830a7a50807821d4e5b1b34a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*dElcqctsyjFm1b1cw0w1-w.png"/></div></figure><p id="d96f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用这个来求导，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ns"><img src="../Images/553ba52b6e4ec7c015774b8de59e53dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*H1VAfPYjOKzOGyAyCu4KDA.png"/></div></figure><p id="7918" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后计算梯度，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nq"><img src="../Images/fcaef015f5a7f3ad4cb8caec6f44b30c.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*bYbpOIy3P0tDBG6c5c0F2w.png"/></div></figure><p id="af04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这实际上只是<em class="ke"> dz </em>与转置的<em class="ke"> y </em>矩阵的另一个矩阵乘法，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nt"><img src="../Images/3dec22ca12282c0cb72e2183866c0e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*e-leGEdGfMH34cr_xqu4pw.png"/></div></figure><p id="e46d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以在 TensorFlow 中实现，如下所示:</p><pre class="je jf jg jh fd mv ko mw mx aw my bi"><span id="7687" class="mh kq hi ko b fi mz na l nb nc">def grad_matmul(x, y, dz):<br/>    return tf.matmul(dz,tf.transpose(y))</span></pre><h1 id="9847" class="kp kq hi bd jt kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">示例 4:卷积</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nu"><img src="../Images/e93d1957bcd06dc8f5deeef4a8287713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G6v9GFtj8yCpvEGQEe_iiA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(图由作者提供)</figcaption></figure><p id="7417" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们用矩阵输入和输出来尝试一个更复杂的例子。我们将矩阵<em class="ke"> x </em>与滤波器<em class="ke"> w </em>的卷积<em class="ke"> z </em>写成</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nv"><img src="../Images/9d3e484030ae88703cd94e1db5db21db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*Kc1F6YUSViQeVmSd_9rjeQ.png"/></div></figure><p id="e364" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定 4x4 输入<em class="ke"> x </em>，2x2 滤波器<em class="ke"> w </em>，因此 3x3 输出<em class="ke"> z </em>，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nw"><img src="../Images/31d3cc1d895fb565c15d0f42ca9efb50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ecczunoEdIMu-Wx2DmOrg.png"/></div></div></figure><p id="50d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">找到坡度<em class="ke"> dw </em>。(注:我们这次找的是<em class="ke"> dw </em>，不是<em class="ke"> dx </em>！)</p><h2 id="3463" class="mh kq hi bd jt mi mj mk ku ml mm mn ky iq mo mp lc iu mq mr lg iy ms mt lk mu bi translated">张量流代码</h2><p id="539f" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">下面是问题设置:</p><pre class="je jf jg jh fd mv ko mw mx aw my bi"><span id="aa9b" class="mh kq hi ko b fi mz na l nb nc">import tensorflow as tf</span><span id="2ac1" class="mh kq hi ko b fi nd na l nb nc"># Use this method to perform the convolution.<br/>def conv2d(x, w):<br/>    return tf.nn.conv2d(tf.reshape(x,[1,x.shape[0],x.shape[1],1]),<br/>                        tf.reshape(w,[w.shape[0],w.shape[1],1,1]), <br/>                        strides=[1,1],<br/>                        padding="VALID")</span><span id="c31b" class="mh kq hi ko b fi nd na l nb nc"># Define inputs<br/>x = tf.constant([[3.0, 4.0, 5.0, 6.0], <br/>                 [4.0, 5.0, 6.0, 7.0],<br/>                 [5.0, 6.0, 7.0, 8.0],<br/>                 [6.0, 7.0, 8.0, 9.0]])<br/>w = tf.constant([[1.0, 2.0], <br/>                 [3.0, 4.0]])<br/>dz = tf.constant([[1.0, 1.0, 1.0], <br/>                  [2.0, 2.0, 2.0],<br/>                  [3.0, 3.0, 3.0]])</span><span id="dae5" class="mh kq hi ko b fi nd na l nb nc"># Define the gradient.<br/>def grad_conv2d(x, w, dz):<br/>    # (Add implementation here)<br/>dx = grad_conv2d(x, w, dz)<br/># Reshape to remove channel and batch number dimensions.<br/>dx = tf.reshape(dx, [w.shape[0],w.shape[1]])</span><span id="3c3a" class="mh kq hi ko b fi nd na l nb nc"># Compute the gradient with Tensorflow.<br/>with tf.GradientTape() as g:<br/>    g.watch(w)<br/>    z = conv2d(x,w)<br/>dx_tf = g.gradient(z, w, <br/>                   output_gradients = tf.reshape(dz,<br/>                                     [1,dz.shape[0],dz.shape[1],1]))</span><span id="3ab3" class="mh kq hi ko b fi nd na l nb nc"># Check the answer.<br/>print(dx == dx_tf)</span></pre><h2 id="55d7" class="mh kq hi bd jt mi mj mk ku ml mm mn ky iq mo mp lc iu mq mr lg iy ms mt lk mu bi translated">解决办法</h2><p id="6d6b" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">计算导数，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ni"><img src="../Images/44ce87ed7a27bb7252f956e3471e30bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*JcSCOSU9FRqh6d8Lrk-Naw.png"/></div></figure><p id="623e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们有了梯度</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nx"><img src="../Images/318c7eaf748d983b0a2a3401e8b6d359.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*pAqXs4LwGDByAtCqq4PEOQ.png"/></div></figure><p id="e23c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们注意到这只是卷积</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ny"><img src="../Images/1977c34a4627d5eeec809f98ff6599e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*t4ZyQCgD-sWiMFKALZSZNg.png"/></div></figure><p id="e0e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以使用上面的函数<code class="du kl km kn ko b">conv2d</code>在 TensorFlow 中实现，如下所示:</p><pre class="je jf jg jh fd mv ko mw mx aw my bi"><span id="56c3" class="mh kq hi ko b fi mz na l nb nc">def grad_conv2d(x, w, dz):<br/>    return conv2d(x, dz)</span></pre><h1 id="8550" class="kp kq hi bd jt kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">结论</h1><p id="7e3f" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">并不是所有的梯度都像上面的例子一样是一个简洁的单行表达式，但是希望这有助于在计算图形中执行“向后传递”时澄清一些核心概念。</p></div></div>    
</body>
</html>