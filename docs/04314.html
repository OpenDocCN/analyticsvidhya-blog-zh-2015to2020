<html>
<head>
<title>Fine tuning BERT for Amazon Food Reviews</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为亚马逊食品评论微调BERT</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fine-tuning-bert-for-amazon-food-reviews-32e474de0e51?source=collection_archive---------2-----------------------#2020-03-14">https://medium.com/analytics-vidhya/fine-tuning-bert-for-amazon-food-reviews-32e474de0e51?source=collection_archive---------2-----------------------#2020-03-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c8318dfc1926b463036263c7fb27cc23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tp69Br5gjXv0wPZYDIRpKA.jpeg"/></div></div></figure><p id="a305" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果在写这篇文章的时候你还没有听说过伯特，欢迎从长达一年的冬眠中醒来。不要费事出去补充你的冰箱了，超市的货架已经被清空了(紧张地看着新冠肺炎)。囤积者可能有足够的食物来等待一群在CPU上跋涉的伯特时代。</p><p id="7694" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">严肃地说，如果你真的想知道厄尼是否会觉得伯特受到了所有的关注而被冷落，那么就跳到由<a class="ae jo" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">杰伊</a>做的这个快速总结吧。如果没有他的精彩文章，深度学习的世界肯定会是一个黑暗而可怕的地方。</p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="5cca" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">目标</h1><p id="002c" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">简单地对大约50万条亚马逊美食评论的BERT基本模型进行微调，并使用它在1-5的范围内预测评级。本教程的代码可以在<a class="ae jo" href="https://github.com/naveenjafer/BERT_Amazon_Reviews" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><h1 id="3c95" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">学习目标</h1><p id="be98" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">我选择这个简单任务和数据集的决定是为了消除所有令人分心的数据解释和转换，这些往往会冲淡大多数教程中的学习目标。你将学会如何做以下事情。</p><ol class=""><li id="37d3" class="le lf hi is b it iu ix iy jb lg jf lh jj li jn lj lk ll lm bi translated">处理和准备您的数据，以便输入到预先训练好的BERT模型中</li><li id="03bb" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated">为您可能拥有的领域特定数据微调BERT模型</li><li id="94cf" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated">保存BERT模型，以便能够加载和重新用于其他</li><li id="c8d5" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated">使用Google Colab在GPU上运行您的模型</li></ol><h1 id="60d3" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">设置您的环境</h1><p id="1927" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">克隆本教程附带的代码</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="a8b1" class="mb jx hi lx b fi mc md l me mf">git clone <a class="ae jo" href="https://github.com/naveenjafer/BERT_Amazon_Reviews" rel="noopener ugc nofollow" target="_blank">https://github.com/naveenjafer/BERT_Amazon_Reviews</a></span></pre><p id="19b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在克隆的repo中创建一个python virtualenv并激活它。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="7b30" class="mb jx hi lx b fi mc md l me mf">python3 -m venv env<br/>source env/bin/activate</span></pre><p id="d644" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下载安装先决条件</p><ol class=""><li id="9d5c" class="le lf hi is b it iu ix iy jb lg jf lh jj li jn lj lk ll lm bi translated"><code class="du mg mh mi lx b">pip install torch</code></li><li id="e306" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><code class="du mg mh mi lx b">pip install transformers</code></li><li id="ba4a" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><code class="du mg mh mi lx b">pip install pandas</code></li></ol><h1 id="f950" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">数据集调查</h1><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/2f1fc9806af830f16989a75256731292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T3Bp0szJAOB3Gpbm77DPQQ.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated">Reviews.csv的7列。我们将只关注2个字段，评论的<strong class="bd jy">文本</strong>和<strong class="bd jy">分数。</strong></figcaption></figure><p id="aa6c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以从<a class="ae jo" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank"> kaggle </a>下载数据。我们将关注“Reviews.csv”文件。在克隆的repo中创建一个文件夹“AMAZON-DATASET ”,并将“Reviews.csv”放入其中。数据集是一个简单的CSV文件，完全独立。</p><h1 id="6238" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">数据集准备</h1><p id="85ba" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">我们将使用pandas处理我们的数据，并将其输入数据加载器。</p><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="b578" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">将数据拆分为定型集和验证集</p><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="b484" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们希望预测的目标列是“分数”。让我们看看它的分布情况。</p><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="a1bf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最大长度。与任何NLP处理管道一样，我们必须为输入句子设置一个固定的最大长度。这个数据集中最长的句子超过了500个单词，为了更快地训练，我将这个MAX_LENGTH设置为100。</p><h1 id="7e17" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">定义数据加载器</h1><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="mo mp l"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated">dataLoader.py</figcaption></figure><p id="ece3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个类继承了torch提供的Dataset类。在这个练习中，我们使用了基本的预训练BERT模型(bert-base-uncased)。对这个模型的架构做一点总结。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="32ce" class="mb jx hi lx b fi mc md l me mf">12-layer, 768-hidden, 12-heads, 110M parameters.<br/>Trained on lower-cased English text.</span></pre><p id="8dd1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用来自预训练的BERT模型的相同的记号赋予器是非常重要的，因为字符串到记号的转换需要与现有的模型一致。</p><blockquote class="mq mr ms"><p id="f93d" class="iq ir mt is b it iu iv iw ix iy iz ja mu jc jd je mv jg jh ji mw jk jl jm jn hb bi translated">您确实不希望单词“Hello”在您的模型中同时由token 45和75表示。</p></blockquote><p id="d01f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">关注__getitem__函数。该函数的目标是简单地为数据加载器提供输入令牌、掩码(我们将会谈到)和输出标签。</p><p id="76c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一个复习句子要经历以下几个转换。我们还将使用maxlen为10的“我喜欢比萨饼”的示例句子，并查看它是如何被处理的。</p><ol class=""><li id="86eb" class="le lf hi is b it iu ix iy jb lg jf lh jj li jn lj lk ll lm bi translated"><strong class="is hj"> Tokenize </strong> <code class="du mg mh mi lx b">tokens = self.tokenizer.tokenize(review)</code>将评论转换成单词列表。<strong class="is hj">【《我》、《爱过的》、《那个》、《披萨》】</strong></li><li id="09ca" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><strong class="is hj">追加CLS和SEP令牌</strong> <code class="du mg mh mi lx b">tokens = ['[CLS]'] + tokens + ['[SEP]']</code>。Bert要求这个特定类“BertModel”的输入具有这种格式的输入。不同的BERT类对此要求略有不同，所以如果您打算实现Bert提供的任何其他类，请参考它们。<strong class="is hj">[“[CLS]”、“我”、“爱过的”、“那个”、“比萨饼”、“[九月]”]</strong></li><li id="8356" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><strong class="is hj">添加填充和截断</strong>对于长度小于maxlen的句子，添加一个填充标记使其长度一致。对于长度超过maxlen的句子，我们截断句子，在末尾加上SEP token。<strong class="is hj"> ["[CLS]"，"我"，"爱过的"，"那个"，"披萨"，"[SEP]"，"[PAD]"，"[PAD]"，"[PAD]"，"[PAD]"] </strong></li><li id="24f1" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><strong class="is hj">将标记转换为标记id</strong>字符串列表将被转换为标记器字典内部的数字。词汇之外的单词也由记号赋予器处理。</li><li id="d08a" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><strong class="is hj">屏蔽</strong>一个列表，在填充令牌的相应位置为0(如果有的话),在所有其他情况下为1。[1,1,1,1,1,1,0,0,0,0]</li></ol><p id="5b18" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">标签减去1，因为在这个多类分类问题中，类从0开始。</p><h1 id="d846" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">初始化数据加载器</h1><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="mo mp l"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated">如果你有4个双核处理器，我建议将线程数设置为6。</figcaption></figure><p id="767e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们创建了两个加载器，一个用于训练集，另一个用于验证。</p><h1 id="48c4" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">定义分类器</h1><p id="984a" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">分类器从torch.nn.module继承而来，需要实现一个forward函数和init。</p><p id="1653" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们进入分类器之前，让我们试着理解我们将完成什么。预训练的BERT模型为我们提供了下图中黄色的整个网络。对于分类任务，在第一个输入标记[‘CLS’]的表示中编码的信息足以进行预测。我们将使用一个带有对数softmax的前馈密集线性图层来进行分类。图像是一个二进制分类器，我们将实现多类的情况。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/e4c492ffe3117d4d17eed47d2e72db8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ulnf7Jw52X2D5ynY.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated">来自http://jalammar.github.io/illustrated-bert/<a class="ae jo" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">的</a></figcaption></figure><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="0b0e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> init函数</strong> -我们定义了线性层，它将长度为768的向量作为输入，并将其映射到长度为5的向量。为什么是768？这是我们使用的BERT模型中隐藏单元的数量，如果您使用不同的模型，请更新它。我们还定义了在这个过程中是否需要训练预训练的BERT层。由于我们选择用Amazon数据集来微调这个模型，我们将解冻BERT层，以便能够下载这个微调后的模型用于未来的任务。</p><p id="f6af" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">正向函数</strong>我们将输入前馈到bert层，以获得上下文化的表示。从这些表示中，我们选择第一个(“CLS ”)表示来填充线性层。</p><h1 id="8aca" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">初始化分类器</h1><p id="7ec1" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">我们在main.py中初始化分类器</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="b270" class="mb jx hi lx b fi mc md l me mf">net = classifier.SentimentClassifier(num_classes, config[“device”], freeze_bert=False)</span></pre><h1 id="724f" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">损失函数和优化器</h1><p id="c8c3" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">我们将使用的损失函数是<a class="ae jo" href="https://pytorch.org/docs/stable/nn.html#nllloss" rel="noopener ugc nofollow" target="_blank"> NLLLoss </a>。我们还将在这个损失函数中使用一个称为权重的可选输入。我们数据集中的类是高度不平衡的。等级5(等级4)占所有示例的60%以上，其他等级徘徊在10%左右。需要这种权衡来减轻损失计算中不平衡训练集的影响。</p><p id="c10a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du mg mh mi lx b">loss_func = nn.NLLLoss(weight=weights)</code></p><p id="5385" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">优化器是一个Adam优化器。<code class="du mg mh mi lx b">optim.Adam(net.parameters(), lr = 2e-5)</code>如果您有资源，可以随意调整学习率。</p><h1 id="3aa4" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">训练循环</h1><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="78c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们在历元上循环一次，在每个历元中，数据被分成大小为64的批。我们使用之前定义的损失函数来计算logits <code class="du mg mh mi lx b">logits = net(seq, attn_masks) </code>和计算损失<code class="du mg mh mi lx b">loss = loss_func(m(logits), labels) </code>。损失被反向传播。</p><p id="2e31" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们每100批计算一次训练精度，保存网络模型(下一个检查点只是更新上一个)。在一个时期结束时，我们计算验证损失和准确性。如果来自这个时期的模型比前一个表现得更好，则它被替换。</p><p id="6210" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑到这种训练需要多长时间，明智的做法是定期保存模型，这样，在发生崩溃时，可以从最后保存的检查点重新加载模型并继续。你不会想每次都重启GTA 5吧。</p><p id="50b4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">保存微调后的模型还可以让您重用该模型来完成与不同领域的食品评论相关的任务。利用这个微调过的模型来分析Zomato领域的食品评论可以构成迁移学习。</p><blockquote class="mq mr ms"><p id="decd" class="iq ir mt is b it iu iv iw ix iy iz ja mu jc jd je mv jg jh ji mw jk jl jm jn hb bi translated">注意:验证集非常庞大，评估验证集需要很长时间。为了适应我的用例并减少循环的运行时间，我在config中引入了一个“validationFraction”参数。这是“evaluate”函数实际使用的验证集的一部分。要获得准确的结果，请增加该分数。</p></blockquote><p id="80a9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">完整的main.py文件</p><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="mo mp l"/></div></figure><h1 id="c9b2" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">在GPU上进行培训(如果您没有物理GPU，请跳过)</h1><p id="b62f" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">在浏览代码时，您会注意到出现了“.至(设备)”。我们确定是否支持CUDA设备，并将其用于GPU计算。BERT，Transformers的发展基于高效的并行化和对GPU的高效利用。在GPU上运行时，不需要修改代码。</p><blockquote class="mq mr ms"><p id="fcaf" class="iq ir mt is b it iu iv iw ix iy iz ja mu jc jd je mv jg jh ji mw jk jl jm jn hb bi translated">注意:如果在GPU上运行出现错误，请执行以下操作。</p><p id="23ac" class="iq ir mt is b it iu iv iw ix iy iz ja mu jc jd je mv jg jh ji mw jk jl jm jn hb bi translated">1)将配置中的<code class="du mg mh mi lx b">forceCPU</code>设置为True并运行模型，如果它运行时没有任何错误，则在此处留下注释，我们将不得不找出GPU版本的问题。</p><p id="d384" class="iq ir mt is b it iu iv iw ix iy iz ja mu jc jd je mv jg jh ji mw jk jl jm jn hb bi translated">2)您的GPU空间可能不足。将批量大小从64减少到合适的2的倍数。</p></blockquote><h1 id="0d17" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">关于Google Colab的培训</h1><p id="3ea8" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">在CPU上为如此大的数据集训练BERT模型可能不是最好的主意(除非你能负担得起更换一个烧坏的处理器)。按照目前的配置，一个6核8线程的i7处理器可能需要72小时才能完成训练。如果您无法访问GPU，Google colab会免费提供一个GPU(不间断持续12小时)。</p><ol class=""><li id="4c8b" class="le lf hi is b it iu ix iy jb lg jf lh jj li jn lj lk ll lm bi translated">前往https://colab.research.google.com/<a class="ae jo" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">的头</a></li><li id="be52" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated">创建新笔记本。</li><li id="7376" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated">点击文件-&gt;上传笔记本。上传回购附带的<a class="ae jo" href="https://github.com/naveenjafer/BERT_Amazon_Reviews/blob/master/BERT_Amazon_Reviews.ipynb" rel="noopener ugc nofollow" target="_blank">BERT _ Amazon _ reviews . ipynb</a>文件。</li><li id="8dc0" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated">打开你的google drive，在最顶层创建一个名为<code class="du mg mh mi lx b">Bert</code>的文件夹。将AMAZON-DATASET文件夹上传到其中。</li><li id="fa3b" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated">回到google colab，运行第二个单元主机google drive到colab。您将被要求验证您的帐户，完成它。使用google drive的优势在于，由检查点保存的输出模型即使在12小时的会话结束时也会保持完整(如果您将文件直接上传到colab，它会被清除)</li><li id="fab3" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated">运行其余的细胞。</li><li id="cc51" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated">最后一个细胞开始训练过程。</li></ol><blockquote class="mq mr ms"><p id="e1d3" class="iq ir mt is b it iu iv iw ix iy iz ja mu jc jd je mv jg jh ji mw jk jl jm jn hb bi translated">注意:如果google colab抱怨空间不足，那么将config中的批处理大小从64减少到2的倍数。分配的12 GB GPU内存不是专用的，并且在初始分配期间表现得相当不稳定。</p></blockquote><h1 id="10aa" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">加载经过训练的模型并在验证集上进行测试</h1><p id="3f82" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">简单运行<code class="du mg mh mi lx b"> python3 main.py eval</code>而不是<code class="du mg mh mi lx b">python3 main.py train</code>它从放置的检查点中选取最后一个模型。如果您想从每个时期结束时完成的验证中挑选最佳模型，只需将名为<code class="du mg mh mi lx b">AmazonReviewClassifier.dat_valTested_tensor(0.6484)</code>的模型替换为<code class="du mg mh mi lx b">AmazonReviewClassifier.dat</code>并运行程序。记住，我们评估的是验证集的子集，因为它太大了，可以随意改变<code class="du mg mh mi lx b">validationFraction</code>来增加或减少验证数据集的抽样部分(大约120000个数据项)。</p><h1 id="90a5" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">结果</h1><p id="aedc" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">我获得了大约92%的训练准确率。该模型还没有完成它的第一个纪元，一旦完成，将更新验证的准确性，但初步运行看起来很有希望。</p><h1 id="926f" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">前进</h1><p id="d884" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">您可以尝试不同的BERT类。BERT有用于任务的类，例如句子预测、分类、屏蔽词预测(这将是一个很好的例子，可以简单地对来自某个领域的未标记语料库的预训练模型进行微调，此时没有固定的目标)。</p><p id="6470" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你在评论中有任何问题，请告诉我。</p><h1 id="4c17" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">要讨论的要点</h1><ol class=""><li id="d038" class="le lf hi is b it ku ix kv jb my jf mz jj na jn lj lk ll lm bi translated">伯特的灾难性遗忘呢？是怎么处理的？或者是？</li></ol><h1 id="a033" class="jw jx hi bd jy jz kz kb kc kd la kf kg kh lb kj kk kl lc kn ko kp ld kr ks kt bi translated">参考</h1><ol class=""><li id="f253" class="le lf hi is b it ku ix kv jb my jf mz jj na jn lj lk ll lm bi translated"><a class="ae jo" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-bert/</a></li><li id="9540" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><a class="ae jo" rel="noopener" href="/swlh/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa">https://medium . com/swlh/无痛-微调-Bert-in-py torch-b91c 14912 CAA</a></li><li id="054a" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><a class="ae jo" href="https://arxiv.org/abs/1905.05583" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.05583</a></li><li id="9b94" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><a class="ae jo" href="https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html" rel="noopener ugc nofollow" target="_blank">https://gluon-NLP . mxnet . io/examples/sentence _ embedding/Bert . html</a></li></ol></div></div>    
</body>
</html>