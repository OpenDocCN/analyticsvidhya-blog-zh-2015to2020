<html>
<head>
<title>Linear Regression in action.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归在起作用。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/working-of-linear-regression-3c0e294a6d22?source=collection_archive---------28-----------------------#2020-05-16">https://medium.com/analytics-vidhya/working-of-linear-regression-3c0e294a6d22?source=collection_archive---------28-----------------------#2020-05-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3402" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">所以，你以前用过线性回归，但不知道算法背后的数学。很多人不知道背后的真实数据。所以，我今天要告诉你。</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/c3efa502304c73c68cfcde18f7c76010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AZzu43KoxDamVpWMVW0zfw.png"/></div></div></figure><p id="5695" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归是<em class="jd">监督学习的一部分，</em>我们有标签数据(“正确答案”)，我们必须根据以前的“正确答案”预测新值。在线性回归中，我们处理的是连续值而不是离散值(一些固定值。)</p><p id="8afa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">让我们以俄勒冈州波特兰市(美国)的房价为例</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jq"><img src="../Images/b5045e76dde710d487362a2c3d6bdc53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSy0ZI3zdxsG0409yAhJcA.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">尺寸与价格曲线图(不按比例)</figcaption></figure><p id="77cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过绘制价格与大小的关系图，我们可以说大小与价格成正比("<em class="jd">随着大小的增加，价格也会增加</em>")。在本文中，我们只处理一个独立变量或一元线性回归。自变量也被称为解释值，因变量通常被称为目标值。</p><blockquote class="jv jw jx"><p id="708e" class="if ig jd ih b ii ij ik il im in io ip jy ir is it jz iv iw ix ka iz ja jb jc hb bi translated">那么我们如何预测这些值呢？</p></blockquote><p id="af4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们用给定的数据点画出假设函数。我们可以以某种方式预测新的输出，但会有一些误差。所以，我们需要一个假设函数，<em class="jd">函数代表问题陈述。</em>假设函数通常用h(x)来表示。</p><p id="adc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个问题我们的假设函数是h(x) = θ + θ1*x <em class="jd">或者y= mx+c </em></p><p id="2a34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">(如果你熟悉这个方程ie。你必须知道直线方程</em> <strong class="ih hj"> <em class="jd"> m或θ1是斜率</em></strong><em class="jd"/><strong class="ih hj"><em class="jd">c或θ是直线的截距</em></strong><em class="jd">)</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jq"><img src="../Images/3d891b65830ec0d2356db2cf91b9a03b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sd0AXJpHWZji-2RaufFA3A.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">算法</figcaption></figure><blockquote class="jv jw jx"><p id="bf58" class="if ig jd ih b ii ij ik il im in io ip jy ir is it jz iv iw ix ka iz ja jb jc hb bi translated">成本函数或目标函数</p></blockquote><p id="9172" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">成本函数不同于假设函数，也可以说是预测函数，也表示为J(θ)。线性回归算法使用的代价函数是<br/> <em class="jd">梯度下降算法。</em></p><p id="e53c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> h(x) = 1/2m * ∑(从i=0到m)【h(Xi)-(易)】</strong></p><p id="7c9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要一条与数据点误差最小的线，这样我们可以更精确地预测，我们如何找到这条线？我们需要最小化∑[(h(x)-y)]</p><p id="417d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以现在我们的代价函数简化为J(θ) = 1/2m * ∑(从I到m) [h(xi)-(易)]。对于y=mx，我们画出J(θ)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/c3efa502304c73c68cfcde18f7c76010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AZzu43KoxDamVpWMVW0zfw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">成本函数J(θ)的曲线图</figcaption></figure><p id="2c93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降看起来像这样</p><p id="0b72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重复直到收敛{</p><p id="faf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd"> θj = θj -α * ( ∂/∂(θj ) ) * J(θ，θ1)，对于j=0和j=1 </em> </strong> <strong class="ih hj">同时更新θ和θ1的值</strong></p><p id="a494" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">}</p><p id="f13f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里α是学习率或找到最小值的步长。</p><blockquote class="jv jw jx"><p id="c6db" class="if ig jd ih b ii ij ik il im in io ip jy ir is it jz iv iw ix ka iz ja jb jc hb bi translated">当我们接近局部最小值时，学习率应该降低(更小的步长),否则我们可能会导致过度拟合。</p></blockquote><p id="21fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在绘制了成本函数J(θ)和给定的数据点之后，我们能够以94%的准确度或0.94的概率进行预测</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jq"><img src="../Images/ac4537abe223fe557dcbf17258dc5e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fODnhJ40fjr32rfNXO7YTA.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">准确率:94%</figcaption></figure><p id="160d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有许多评估方法可以发现错误，例如:-</p><p id="b2b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">平均绝对误差 (MAE)</p><p id="36b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">均方误差</em> (MSE)</p><p id="5eca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">均方根误差</em> (RMSE)</p><p id="a13f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">相对平方误差。</em>(称为r2分数误差)</p><blockquote class="jv jw jx"><p id="f3cb" class="if ig jd ih b ii ij ik il im in io ip jy ir is it jz iv iw ix ka iz ja jb jc hb bi translated"><strong class="ih hj">恭喜你！现在你知道线性回归是如何工作的，你可以使用:sklearn.linear_model中的</strong>导入线性回归<strong class="ih hj">并开始预测。</strong></p></blockquote></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><blockquote class="jv jw jx"><p id="c50d" class="if ig jd ih b ii ij ik il im in io ip jy ir is it jz iv iw ix ka iz ja jb jc hb bi translated">如果你已经到达这里，感谢你阅读这篇文章！。。。喜欢就留个掌声:)</p></blockquote><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ki"><img src="../Images/dcc88128016c33e818d3dbdaa6569bc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UuGyPPk80SK9y9f-"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">由<a class="ae kj" href="https://unsplash.com/@hannynaibaho?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">刘汉宁·内巴霍</a>在<a class="ae kj" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><blockquote class="kk"><p id="0cda" class="kl km hi bd kn ko kp kq kr ks kt jc dx translated">人工智能是新的电力</p></blockquote></div></div>    
</body>
</html>