<html>
<head>
<title>Baby steps in Neural Machine Translation Part 2 (Decoder) — Human is solving the challenge given by the God</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经机器翻译的第二部分(解码器)——人类正在解决上帝赋予的挑战</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/baby-steps-in-neural-machine-translation-part-2-decoder-human-is-solving-the-challenge-given-by-4d097c961e67?source=collection_archive---------25-----------------------#2020-08-16">https://medium.com/analytics-vidhya/baby-steps-in-neural-machine-translation-part-2-decoder-human-is-solving-the-challenge-given-by-4d097c961e67?source=collection_archive---------25-----------------------#2020-08-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><ul class=""><li id="d542" class="if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw bi translated">带你了解机器翻译系统的解码器</li><li id="5368" class="if ig hi ih b ii ix ik iy im iz io ja iq jb is it iu iv iw bi translated">一步一步，引导你通过代码和简短的解释</li><li id="e268" class="if ig hi ih b ii ix ik iy im iz io ja iq jb is it iu iv iw bi translated">为你训练定制的机器翻译系统提供代码和数据。</li></ul><p id="a125" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">这是从<a class="ae jr" rel="noopener" href="/machine-learning-evaluation-metrics-precision/baby-steps-in-neural-machine-translation-part-1-encoder-human-is-solving-the-challenge-given-d3f36b977828">第1部分—机器翻译系统的编码器</a>的后续。第1部分简要解释了代码和通过编码器的张量流。如果你还没有阅读第1部分，我强烈建议你阅读<a class="ae jr" rel="noopener" href="/machine-learning-evaluation-metrics-precision/baby-steps-in-neural-machine-translation-part-1-encoder-human-is-solving-the-challenge-given-d3f36b977828">这篇文章</a>，因为你会对编码器有全面的了解。如果你已经看完了这篇文章，让我们继续解码器部分。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/85fe61029052e7aa245435d9bcabdeb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*R-stADZ5vmFsJBhpcC3m4Q.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">机器翻译系统的解码器部分</figcaption></figure><p id="dfe0" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">解码器部分非常有趣，解码器的要点是前瞻掩模。由于这个掩码，我们在预测当前单词时过滤掉了未来单词的使用。这是什么意思？</p><p id="6afc" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">在训练期间，我们通过将<start>标签附加到目标句子来将整个目标句子传递到解码器的输入，同时我们使用没有<start>标签的目标句子作为解码器的输出来训练解码器。这也意味着我们用当前单词来预测下一个单词。</start></start></p><p id="79e8" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi">But there is something fishy here… We pass the entire target sentence with label &lt;start&gt; to the decoder during training. The decoder will use the future word information to predict my current word. For example: during training, the decoder input is (&lt;start&gt;, 我, 去, 学校) and the decoder output is (我, 去, 学校, &lt;end&gt;). Since we pass the entire target sentence to train the decoder, the decoder will be trained to use the future words information such as “去”, “学校” to predict “我”. This does not make sense!!! Because during prediction state, we do not know the future words. We could only predict “我” using “&lt;start&gt;” information. This is solved by using look-ahead mask. This mask will remove the future words information during training and this is done elegantly.</p><p id="c548" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">所以现在，我们知道了解码器的全貌。从这里开始，我们将更深入地研究张量的流动和变换。为了使流程更清晰，我们在这里假设目标句子被标记化、索引并附加有长度为26的标签(<start>、<end>)，并且我们并行处理64个目标句子。如果目标句子没有26个索引单词，该句子将被附加0，直到26个单词。</end></start></p><pre class="jt ju jv jw fd ke kf kg kh aw ki bi"><span id="1021" class="kj kk hi kf b fi kl km l kn ko">sentence 1 : "我 去 学校" =&gt; [1, 2] =&gt; [1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0...]</span></pre><p id="db58" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">所以维数应该是(64，26)。然后，该张量(64，26)将流过嵌入层，并添加类似于编码器部分的位置信息，如在<a class="ae jr" rel="noopener" href="/machine-learning-evaluation-metrics-precision/baby-steps-in-neural-machine-translation-part-1-encoder-human-is-solving-the-challenge-given-d3f36b977828">部分1 </a>中所解释的。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kp"><img src="../Images/88cca7f45745b7073f052b8c04971394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*R2jwHli8sNav4wl-gZeShw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">定位嵌入公式</figcaption></figure><p id="4c3a" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">具有位置信息的解码器输入张量现在将具有(64，26，512)的维数。到目前为止，一切都类似于编码器。真正神奇的事情发生在张量流向被掩盖的多头注意力的时候。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kq"><img src="../Images/4066b2bad08acdf5f543ab36d371e35f.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*THCMrMVwJ6ouEoB3B46ZKw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">掩蔽的多头-注意</figcaption></figure><p id="db4f" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">前面我们知道，多头注意力只是把(64，26，512)的512个特征拆分成8组26个单词，每组64个特征(64，8，26，64)，并行处理。</p><ul class=""><li id="e604" class="if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw bi translated">soft max(Q * Transpose(K))= soft max((64，8，26，64) * (64，8，64，26) ) = softmax((64，8，26，26)) = (64，8，26，<strong class="ih hj"> 26 </strong></li><li id="fc00" class="if ig hi ih b ii ix ik iy im iz io ja iq jb is it iu iv iw bi translated">softmax(Q*Transpose(K)) * v = (64，8，26，26) * (64，8，26，64) = (64，8，26，26)</li></ul><p id="cc45" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">但是这个过程被修改为包括具有张量维数(64，1，26，26)的前瞻掩码。</p><ul class=""><li id="1bc7" class="if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw bi translated">soft max(Q * Transpose(K))= soft max((64，8，26，64) * (64，8，64，26) ) = softmax((64，8，26，26)) = (64，8，26，<strong class="ih hj"> 26 </strong></li><li id="311a" class="if ig hi ih b ii ix ik iy im iz io ja iq jb is it iu iv iw bi translated"><strong class="ih hj">mask</strong>(soft max(Q * Transpose(K)))=(64，8，26，<strong class="ih hj"> 26 </strong> )-(64，1，26，<strong class="ih hj"> 26 </strong> ) = (64，8，26，<strong class="ih hj"> 26 </strong>)</li><li id="0b29" class="if ig hi ih b ii ix ik iy im iz io ja iq jb is it iu iv iw bi translated">softmax(Q*Transpose(K)) * v = (64，8，26，26) * (64，8，26，64) = (64，8，62，26)</li></ul><p id="2bf8" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">让我们来看看前瞻遮罩的细节。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kr"><img src="../Images/6964d7686cd1d9864ff44314b6cebb20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*M7BWCyluFW3XfcIlMV01pA.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">填充掩码和前瞻掩码</figcaption></figure><p id="4f01" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">前瞻遮罩与填充遮罩一起创建。为简单起见，此处显示的示例填充为长度10，而不是26。但想法是一样的。让我们看看填充面具。这个掩码非常简单，当我们给句子添加新单词时，它只分配“1”。这样做的目的是优化损失计算。当我们训练一个模型时，我们不想考虑来自填充单词的损失。而对于look-head mask来说，它是一个句子长度的方阵。(10, 10).如果我们的句子长度是26，那么前瞻掩码将是(26，26)。当我们想要从句子中删除信息时，前瞻分配“1”。让我们看看行“w1”，我们只想保留第一个单词的信息，并删除其他单词的信息。现在，我们注意到我们阻止了解码器查看未来单词信息。此外，我们还想阻止填充词信息。因此，我们比较这两个掩码，并找出这两个掩码之间的最大值。请记住“1”的意思是:删除特定信息。我们稍后会知道如何删除特定的信息。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ks"><img src="../Images/26a677c718d8590e2ffec8b56011b418.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*k_wAK2apCykz7rOSlT7drg.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">找出填充遮罩和前瞻遮罩之间的最大值</figcaption></figure><p id="92ef" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">因为我们知道前瞻遮罩。让我们回顾一下掩蔽的多头注意力步骤</p><ul class=""><li id="9261" class="if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw bi translated">soft max(Q * Transpose(K))= soft max((64，8，26，64) * (64，8，64，26) ) = softmax((64，8，26，26)) = (64，8，26，<strong class="ih hj"> 26 </strong>)</li><li id="88a6" class="if ig hi ih b ii ix ik iy im iz io ja iq jb is it iu iv iw bi translated"><strong class="ih hj">mask</strong>(soft max(Q * Transpose(K)))=(64，8，26，<strong class="ih hj"> 26 </strong> )-(64，1，26，<strong class="ih hj"> 26 </strong> ) = (64，8，26，<strong class="ih hj"> 26 </strong></li><li id="41a3" class="if ig hi ih b ii ix ik iy im iz io ja iq jb is it iu iv iw bi translated">softmax(Q*Transpose(K)) * v = (64，8，26，26) * (64，8，26，64) = (64，8，62，26)</li></ul><p id="b89a" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">正如我们在编码器部分所讨论的:softmax(Q*Transpose(K))意味着我们试图从其他单词的角度来表示这个单词。在解码器部分，我们试图从其他目标词的角度来表示目标词。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kt"><img src="../Images/d6d1f7b9bc54d0e1cda367f51c36e102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m1-f-oTLox-2hIaPq_vwPg.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">重述:来自其他单词的单词信息表示</figcaption></figure><p id="8cfe" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi">But, we don’t want to represent the target word from future words. For example: we don’t want to represent “我” with information from “去”， “学校”. So, we minus the word representation from look-ahead mask. <strong class="ih hj">mask</strong>(softmax(Q*Transpose(K))) minus the target word representation(64, 8, 26, <strong class="ih hj">26</strong>) with “1”s generated in look-ahead mask(64, 1, 26, <strong class="ih hj">26</strong>). Now, we know that the target word representation does not contain future words information. This is superb!!!!!</p><p id="0165" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">然后，将目标单词表示乘以语言信息张量V (64，8，26，64)以重新获得语言信息，之后，我们连接回8个组以形成512个特征(64，26，512)。这些过程与编码器部分相同。</p><p id="7a20" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">现在，我们有来自掩蔽的多头注意力的输出，它是没有未来单词信息的目标单词表示(64，26，512)。张量将通过归一化过程。这是将使训练和预测更快的过程。先前的<a class="ae jr" rel="noopener" href="/machine-learning-evaluation-metrics-precision/baby-steps-in-neural-machine-translation-part-1-encoder-human-is-solving-the-challenge-given-d3f36b977828">编码器文章</a>中给出了该过程的解释。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ky"><img src="../Images/493c6740a14462e76257a9f61dab5cac.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*jvLJMNQCWZyrhcY7TSkifA.png"/></div></figure><p id="537c" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">到目前为止，我们有来自编码器(64，62，512)的输出和来自掩蔽多头注意力(64，26，512)的输出。我们如何加入这两个输出的张量？</p><p id="0f78" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">答案显示在红圈里。使用另一个多头注意力模块。现在，Q = (64，26，512)是解码器输出，K = V= (64，62，512)是编码器输出。于是，下面的过程又重复了一遍。</p><ul class=""><li id="e7f1" class="if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw bi translated">soft max(Q * Transpose(K))= soft max((64，8，26，64) * (64，8，64，62) ) = softmax((64，8，26，62)) = (64，8，26，<strong class="ih hj"> 62 </strong></li><li id="a55d" class="if ig hi ih b ii ix ik iy im iz io ja iq jb is it iu iv iw bi translated">softmax(Q*Transpose(K)) * v = (64，8，26，<strong class="ih hj"> 62 </strong> ) * (64，8，<strong class="ih hj"> 62 【T7，64) = (64，8，26，<strong class="ih hj"> 64 </strong>)</strong></li></ul><p id="4098" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi">As we have gone through so many time, we know that this process softmax(Q*Transpose(K)) is actually represent the word information from other words perspective. This time, we are representing the target word from the perspective of source words. eg. We are representing the “我” from the perspective of “I”, “go”, “to”, “school”. sofmax means that we want to find the most appropriate source word representation to represent “我”. In this example, the best source word representation would be “I”. After finding the best word representation, we need to get the language information for the source words which is performed in softmax(Q*Transpose(K)) * <strong class="ih hj">v </strong>process. Finally, we will get a tensor (64, 8, 26, <strong class="ih hj">64</strong>) of target word with the source words representation and language information. Another superb move!!!</p><p id="fa56" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">最后，用于训练整个机器翻译的损失函数只是一个交叉熵函数。我们已经到达了解码器的尽头，这也标志着机器翻译系统的终结。如果你一直跟随我到这里，我希望你能从阅读我的博客中获益良多。这是<a class="ae jr" href="https://www.tensorflow.org/tutorials/text/transformer" rel="noopener ugc nofollow" target="_blank"> tensorflow </a>提供的<a class="ae jr" href="https://www.tensorflow.org/tutorials/text/transformer" rel="noopener ugc nofollow" target="_blank">代码</a> s，我发现这个很有用。当您运行代码并发现理解代码有困难时，请随时重温本文。你也可以参考“<a class="ae jr" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”一文。我觉得这些人是天才。如果你想训练你自己的模型，你可以使用来自<a class="ae jr" href="http://opus.nlpl.eu/" rel="noopener ugc nofollow" target="_blank">开放字幕(OPUS) </a>的数据。我已经训练了一个马来语到英语的翻译模型，我想在这里展示一些结果。虽然我只使用20 epoch和80k数据集进行训练，但我觉得翻译相当不错。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kz"><img src="../Images/24a5ffd0da851884d575c6b81730f06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ZpPFuIv7lw0TAuaxwlCUQ.png"/></div></div></figure><pre class="jt ju jv jw fd ke kf kg kh aw ki bi"><span id="e10f" class="kj kk hi kf b fi kl km l kn ko">Input : kenapa kau tidak ikut kami sahaja ?<br/>Predicted translation : why don't you come with us?<br/>Real translation: why don't you just follow us ?</span></pre><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es la"><img src="../Images/17af0499e19182b66c7da5e627e28cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dd96CXAVTBLfFXZywl4geg.png"/></div></div></figure><pre class="jt ju jv jw fd ke kf kg kh aw ki bi"><span id="4bee" class="kj kk hi kf b fi kl km l kn ko">Input : jika aku menjadi orang kaya.<br/>Predicted translation : if i was a kid.<br/>Real translation: if i was a rich guy .</span></pre><p id="a336" class="pw-post-body-paragraph jc jd hi ih b ii ij je jf ik il jg jh im ji jj jk io jl jm jn iq jo jp jq is hb bi translated">希望你能喜欢你的造型。下一篇文章再见。</p></div></div>    
</body>
</html>