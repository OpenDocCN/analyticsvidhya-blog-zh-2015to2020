<html>
<head>
<title>Exploring Cancer Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索癌症数据</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/exploring-cancer-data-d57f2d72975c?source=collection_archive---------23-----------------------#2020-04-05">https://medium.com/analytics-vidhya/exploring-cancer-data-d57f2d72975c?source=collection_archive---------23-----------------------#2020-04-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5686e9b38e81e45b88810ed5fc3f159d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jfAkNIfGRX5-t5MQ"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://unsplash.com/@nci?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">国家癌症研究所</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="be8b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">乳腺癌是一种非常严重的癌症，死亡率仅次于肺癌。</p><p id="8846" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作为这篇文章的一部分，我想使用机器学习算法来探索统计模型如何从数据中学习并做出预测。这是一个<a class="ae iu" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">监督学习</a>的例子，其中我们已经确定了响应变量。</p><p id="c3aa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个数据集取自<a class="ae iu" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29" rel="noopener ugc nofollow" target="_blank">加州大学欧文分校机器学习图书馆</a>。</p><p id="67ab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然医学研究人员肯定会从医学领域的角度来看这个数据集和问题，但我的意图是看数据科学的应用，并演示机器如何通过查看过去的历史数据来学习和预测。本练习缺少一些活动，如<a class="ae iu" href="https://en.wikipedia.org/wiki/Feature_engineering" rel="noopener ugc nofollow" target="_blank">特征工程</a>，这些活动可以丰富模型。</p><p id="6da9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我使用Python作为语言，并使用<a class="ae iu" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> Scikit Learn </a>、<a class="ae iu" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>和<a class="ae iu" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>库来构建统计模型。同样的情况也可以在R和SAS和SPSS以及其他很多机器学习工具中实现。</p><h1 id="b9a3" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">导入所需的库</h1><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="def2" class="la ju hi kw b fi lb lc l ld le"># Import Basic libraries for Data-frames and manipulation<br/>import pandas as pd<br/>import numpy as np</span><span id="c106" class="la ju hi kw b fi lf lc l ld le"># Import libraries for visualization<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>%matplotlib inline</span><span id="0d13" class="la ju hi kw b fi lf lc l ld le"># Import libraries for transformation<br/>from sklearn.decomposition import PCA<br/>from sklearn.impute import SimpleImputer</span><span id="7264" class="la ju hi kw b fi lf lc l ld le"># Import Statistical Modelling libraries<br/>from sklearn.linear_model import LogisticRegressionCV<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import RandomForestClassifier, <br/>from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.svm import SVC<br/>import xgboost as xgb</span><span id="4b3b" class="la ju hi kw b fi lf lc l ld le"># Import Deep Learning libraries<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.wrappers.scikit_learn import KerasClassifier</span><span id="9a1f" class="la ju hi kw b fi lf lc l ld le"># Import library to compute performance metrics<br/>from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix</span><span id="e603" class="la ju hi kw b fi lf lc l ld le">import warnings<br/>warnings.filterwarnings("ignore")</span></pre><p id="2102" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Pandas和Numpy是标准的数据框架和数值程序库。Matplotlib和Seaborn是用于可视化的。Scikit学习库中的各种类已被导入(线性模型、决策树、随机森林等)。此外，还引入了极端梯度增强(XGBOOST)。</p><h1 id="755a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">导入数据集</h1><p id="a2ab" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">数据集由威斯康星大学欧文分校图书馆提供。Pandas用于将数据导入到数据框中。数据帧是加载到内存中的简单(行、列)数据结构。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="f8d2" class="la ju hi kw b fi lb lc l ld le"># Define the column names<br/>column_names = ['ID','Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']<br/># Import the data<br/>data = pd.read_csv("breast-cancer-wisconsin.data", header=None, names=column_names)<br/># Display the top 5 rows<br/>data.head()</span></pre><p id="60e7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于数据文件没有列标题，我们可以在导入数据集时定义列名。默认情况下，data.head()给出导入数据的前5行。</p><p id="ba57" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据导入的输出如下所示。</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/45c7442ad22c06e92f48684e61cb2cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GO4DStiC7L2rGYh8"/></div></div></figure><p id="e8b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里的因变量或响应变量是具有值的<strong class="ix hj">类</strong></p><p id="5fcf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2 —良性(无害)[66%的数据]</p><p id="6cfb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4 —恶性[数据的34%]</p><p id="32e3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">除了ID之外，其余所有变量都被假定为连续变量，取值范围为1-10。</p><h1 id="6fce" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">数据转换</h1><p id="9563" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">数据转换是数据科学过程中非常重要的一步，因为如果没有这一步，几乎不可能构建出<strong class="ix hj">有意义的</strong>统计模型。在我们进入模型构建过程之前，我们需要查看丢失的值，或者我们是否需要转换任何列。在观察数据时，我们发现“裸核”列有一个缺失值(用“？”标识)).我们可以考虑用均值或中值策略来代替它。我已经决定把中位数作为我的首选策略。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="b2b2" class="la ju hi kw b fi lb lc l ld le"># ID column is converted to String<br/>data['ID'] = data['ID'].astype(str)</span><span id="326c" class="la ju hi kw b fi lf lc l ld le"># Replace the ? with NaN value<br/>data.replace(to_replace=['?'],value=pd.np.NaN, inplace=True)</span><span id="f0fb" class="la ju hi kw b fi lf lc l ld le"># Use the SimpleImputer to replace NaN with median<br/>imputer = SimpleImputer(strategy='median', missing_values=pd.np.NaN)<br/>data['Bare Nuclei'] = imputer.fit_transform(data['Bare Nuclei'].values.reshape(-1,1))</span><span id="2cf2" class="la ju hi kw b fi lf lc l ld le"># Create a new column - Malignant <br/># 0- cancer is benign<br/># 1- cancer is malignant<br/>data['Malignant'] = data['Class'].map({2:0,4:1})</span><span id="55fe" class="la ju hi kw b fi lf lc l ld le"># Drop the column 'Class' as this is no longer required.<br/>data.drop(['Class'], axis=1, inplace=True)</span></pre><h1 id="afd1" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">数据可视化</h1><p id="6257" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">为了查看数据集中的数据元素以及它们是如何关联的，最好的图是<a class="ae iu" href="https://en.wikipedia.org/wiki/Heat_map" rel="noopener ugc nofollow" target="_blank">热图</a>。</p><p id="ab17" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们使用Seaborn库来绘制热图。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="b5c7" class="la ju hi kw b fi lb lc l ld le">plt.figure(figsize=(10,5))<br/>sns.heatmap(data.corr(),annot=True, cmap = "YlGnBu")</span></pre><p id="9e49" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">热图如下所示</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/044e18632a6fc7b6ab8d423ab9ba3737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tI8tywl-U_GwQ_P_"/></div></div></figure><p id="c6d3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从热图中，我们可以看到，除了有丝分裂，其他所有属性都与恶性肿瘤呈高度正相关<a class="ae iu" href="https://en.wikipedia.org/wiki/Correlation_and_dependence" rel="noopener ugc nofollow" target="_blank"/>。此外，从热图中可以清楚地看出，各个属性之间也具有非常高的相关性(例如，细胞大小的不均匀性和细胞形状的均匀性)。</p><p id="2ee9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这表明存在多重共线性，我们需要执行<a class="ae iu" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a>来消除这种情况。</p><h1 id="38ab" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">主成分分析</a></h1><p id="16fb" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">主成分分析是去除多重共线性和去除维度的标准方法(每当模型中有大量维度时)。这里，我们使用PCA创建了5个主成分。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="cdd9" class="la ju hi kw b fi lb lc l ld le">X = data.drop(['Malignant','ID'],axis=1)<br/>y = data[['Malignant']]<br/>pca = PCA(n_components = 5)<br/>pca.fit(X)<br/>X = pca.transform(X)</span></pre><p id="6ea3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面的代码从9个变量中创建了5个主成分。</p><p id="cffc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">主成分分析后，5个主成分的热图如下所示</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/e8ab73b8f51ea67eda09260b51d7a318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/0*ZbVY6FhXXVcB9Luu"/></div></figure><p id="b7ac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">很明显，各种主成分之间没有相关性，多重共线性已从我们的数据集中删除。</p><h1 id="cd31" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">模型开发</h1><p id="b7b7" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">现在我们进入开发统计模型的有趣部分。</p><p id="2c3f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，首先我们必须将数据集分为训练和测试两部分。当在训练数据集上训练模型时，使用测试数据集来测试统计模型的性能。</p><p id="3be2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们使用scikit learn library提供的train_test_split函数将数据拆分为训练和测试。75%的数据用于训练，25%用于测试。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="e923" class="la ju hi kw b fi lb lc l ld le">np.random.seed(100)<br/>X_train, X_test, y_train, y_test = train_test_split(X,<br/>                                                    y,<br/>                                                    test_size=0.25, <br/>                                                    random_state=100)</span></pre><p id="6ce2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将种子设置为100以确保结果是可重复的。“test_size”参数决定了测试数据集的大小(在我们的例子中是25%)。</p><p id="6c23" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">统计模型的性能由<a class="ae iu" href="https://developers.google.com/machine-learning/crash-course/classification/accuracy" rel="noopener ugc nofollow" target="_blank">精度分数</a>、<a class="ae iu" href="https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall" rel="noopener ugc nofollow" target="_blank">精度和召回</a>和<a class="ae iu" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank"> F1分数</a>决定。混淆矩阵决定了模型如何进行实际分类和预测分类。</p><p id="c879" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，为了衡量各种模型的结果，我们定义了一个用户定义的函数来打印指标得分。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="551f" class="la ju hi kw b fi lb lc l ld le"># Function to print the accuracy, precision, recall and confusion matrix<br/># Inputs - predicted value (y_pred), actual value (y_true)<br/># Output - prints the various metrics in a % form<br/>def printmetrics(y_pred, y_true):<br/>    acc = np.round(accuracy_score(y_pred=y_pred, y_true=y_true)*100,0)<br/>    prec = np.round(precision_score(y_pred=y_pred, y_true=y_true)*100,0)<br/>    recall = np.round(recall_score(y_pred=y_pred,y_true=y_true)*100,0)<br/>    f1score = np.round(f1_score(y_pred=y_pred, y_true=y_true)*100,0)<br/>    print("Accuracy Score "+ str(acc)+"%")<br/>    print("Precision Score "+ str(prec)+"%")<br/>    print("Recall Score "+ str(recall)+"%")<br/>    print("F1 Score "+ str(f1score)+"%")<br/>    print("*** Confusion Matrix ***")<br/>    print(confusion_matrix(y_pred=y_pred, y_true=y_true))</span></pre><h1 id="895a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">交叉验证的逻辑回归模型</a></h1><p id="71d3" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">逻辑回归是我们将尝试拟合该数据的第一个模型(在PCA之后)。逻辑回归使用Sigmoid函数来拟合线性模型。sigmoid函数由下式给出</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/53bb042b76778e0a42bbcac0c5872300.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/0*z23izabXBb2dfP5s"/></div></figure><p id="1f65" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中z = <em class="lp"> b + w1x1 + w2x2 + … wNxN </em></p><p id="a891" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">sigmoid函数的曲线如下所示</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/fbb09859113dac1245e140483d37fb57.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/0*H3Io-v8unFuFlMgT"/></div></figure><p id="f868" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">来源:<a class="ae iu" href="https://developers.google.com/machine-learning/crash-course/logistic-regression/calculating-a-probability" rel="noopener ugc nofollow" target="_blank">关于机器学习的谷歌课堂</a></p><p id="5825" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="noopener ugc nofollow" target="_blank">交叉验证</a>是一种对数据进行采样以创建k倍的技术。模型在k-1折叠上训练，并在1折叠上测试。这对提高模型的准确性特别有用。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="f26f" class="la ju hi kw b fi lb lc l ld le">logmodel = LogisticRegressionCV(cv=5)<br/>logmodel.fit(X_train, y_train)<br/>pred = logmodel.predict(X_train)<br/>print("#### Training Set Prediction ####")<br/>printmetrics(pred,y_train)<br/>print("#### Testing Set Prediction ####")<br/>printmetrics(logmodel.predict(X_test),y_test)</span></pre><p id="cb3a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型的输出如下所示</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="7f86" class="la ju hi kw b fi lb lc l ld le">Training Set Prediction ####<br/>Accuracy Score 98.0%<br/>Precision Score 97.0%<br/>Recall Score 97.0%<br/>F1 Score 97.0%<br/>*** Confusion Matrix ***<br/>[[347   6]<br/> [  5 166]]<br/>#### Testing Set Prediction ####<br/>Accuracy Score 94.0%<br/>Precision Score 92.0%<br/>Recall Score 94.0%<br/>F1 Score 93.0%<br/>*** Confusion Matrix ***<br/>[[99  6]<br/> [ 4 66]]</span></pre><p id="a4d1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面给出的输出来看，准确率相当高，甚至精度和召回率也很高。该模型仅错误分类了训练集中524个数据点中的11个数据点和175个数据点中的10个数据点。</p><h1 id="11e5" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Decision_tree" rel="noopener ugc nofollow" target="_blank">决策树</a></h1><p id="0826" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">决策树可用于分类和回归，并使用树状结构来做出决策。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="188d" class="la ju hi kw b fi lb lc l ld le">dtreemodel = DecisionTreeClassifier()<br/>dtreemodel.fit(X_train, y_train)<br/>pred = dtreemodel.predict(X_train)<br/>print("#### Training Set Prediction ####")<br/>printmetrics(pred,y_train)<br/>print("#### Testing Set Prediction")<br/>printmetrics(dtreemodel.predict(X_test),y_test</span></pre><p id="6963" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上述代码的输出结果如下</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="8081" class="la ju hi kw b fi lb lc l ld le">#### Training Set Prediction ####<br/>Accuracy Score 100.0%<br/>Precision Score 100.0%<br/>Recall Score 100.0%<br/>F1 Score 100.0%<br/>*** Confusion Matrix ***<br/>[[353   0]<br/> [  0 171]]<br/>#### Testing Set Prediction<br/>Accuracy Score 94.0%<br/>Precision Score 92.0%<br/>Recall Score 93.0%<br/>F1 Score 92.0%<br/>*** Confusion Matrix ***<br/>[[99  6]<br/> [ 5 65]]</span></pre><p id="e010" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在训练数据集上，基于决策树的分类器能够以100%的准确度进行分类，这是非常显著的。然而，在测试集上，它错误地分类了11个数据点。</p><p id="8cc0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">无论如何，这仍然是精彩的表演。</p><h1 id="32df" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">随机森林</a></h1><p id="ec4d" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">随机森林是决策树的集合，使用<a class="ae iu" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank">bagging(Bootstrap Aggregating)</a>算法进行最终预测。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="6e6f" class="la ju hi kw b fi lb lc l ld le">rfmodel = RandomForestClassifier()<br/>rfmodel.fit(X_train,y_train)<br/>print("#### Training Set Prediction ####")<br/>printmetrics(rfmodel.predict(X_train),y_train)<br/>print("#### Testing Set Prediction")<br/>printmetrics(rfmodel.predict(X_test),y_test)</span></pre><p id="e961" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面给出了随机森林模型的输出。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="27a0" class="la ju hi kw b fi lb lc l ld le">#### Training Set Prediction ####<br/>Accuracy Score 100.0%<br/>Precision Score 100.0%<br/>Recall Score 100.0%<br/>#### Confusion Matrix ####<br/>[[353   0]<br/> [  0 171]]<br/>#### Testing Set Prediction<br/>Accuracy Score 94.0%<br/>Precision Score 92.0%<br/>Recall Score 94.0%<br/>#### Confusion Matrix ####<br/>[[99  6]<br/> [ 4 66]]</span></pre><p id="9c02" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样，该模型能够以100%的准确度拟合训练数据(零错误分类的数据点)。然而，在测试集上，它错误地分类了10个数据点。它与决策树非常相似。</p><h1 id="8d92" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank">支持向量机</a></h1><p id="2088" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">支持向量机是另一类可用于分类和回归的机器学习算法。我将SVM应用于数据集，结果非常相似。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="28e9" class="la ju hi kw b fi lb lc l ld le">#### Training Set Prediction ####<br/>Accuracy Score 100.0%<br/>Precision Score 99.0%<br/>Recall Score 100.0%<br/>#### Confusion Matrix ####<br/>[[351   2]<br/> [  0 171]]<br/>#### Testing Set Prediction<br/>Accuracy Score 94.0%<br/>Precision Score 89.0%<br/>Recall Score 97.0%<br/>#### Confusion Matrix ####<br/>[[97  8]<br/> [ 2 68]]</span></pre><p id="5f68" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">测试集和训练集的准确率都很高。这里，精确度很低，但是召回率很高。总的来说，这个算法仅仅错误分类了10个点。</p><p id="b5a7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">除了上面提到的，我还运行了<a class="ae iu" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>算法，它给出了与决策树和随机森林相似的性能。</p><h1 id="9b78" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a></h1><p id="f94b" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">神经网络或人工神经网络是另一类试图模仿人脑中神经元的深度学习算法。</p><p id="4a92" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">典型神经网络具有输入层(数据)、隐藏层和输出层。神经网络的架构如下所示。</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/648361e2e2367d3df5c77b1ffb3f79c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3gjHsud_ZaoMzrjQ"/></div></div></figure><p id="a520" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在乳腺癌数据集上，我运行了一个具有6个隐藏层的神经网络，每层至少有30个节点。使用的激活函数是隐藏层的RELU。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="7f77" class="la ju hi kw b fi lb lc l ld le">Training Set Prediction ####<br/>Accuracy Score 98.0%<br/>Precision Score 96.0%<br/>Recall Score 99.0%<br/>#### Confusion Matrix ####<br/>[[346   7]<br/> [  1 170]]<br/>#### Testing Set Prediction<br/>Accuracy Score 95.0%<br/>Precision Score 92.0%<br/>Recall Score 96.0%<br/>#### Confusion Matrix ####<br/>[[99  6]<br/> [ 3 67]]</span></pre><p id="5518" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">准确性和其他性能测量与逻辑回归或决策树/随机森林算法没有太大不同。这表明当有多个数据变量和大量行时，神经网络工作得最好。对于更小的数据集(就像在这种情况下)，精度和性能与常规算法没有什么不同。</p><h1 id="1d98" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="8f4f" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">从上面的应用来看，很明显，机器学习肯定会在生物学和复杂的癌症检测以及可能的治疗领域找到自己的路。利用有限的数据集，机器学习算法能够从训练数据集学习，并在测试数据集上以很高的准确度进行预测。</p><p id="3559" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">机器学习在许多领域都有应用，随着计算机的改进(更便宜的存储、内存、处理能力)，机器学习对于检测乳腺癌等复杂疾病非常有用。</p></div></div>    
</body>
</html>