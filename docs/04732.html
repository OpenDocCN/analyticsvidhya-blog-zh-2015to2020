<html>
<head>
<title>SHAP Part 1: An Introduction to SHAP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SHAP第一部分:SHAP简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/shap-part-1-an-introduction-to-shap-58aa087a460c?source=collection_archive---------8-----------------------#2020-03-30">https://medium.com/analytics-vidhya/shap-part-1-an-introduction-to-shap-58aa087a460c?source=collection_archive---------8-----------------------#2020-03-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c3538feb56c2beb88ecb8817be08ffc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jCfIMIDWHX86REAFpyr0RQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片鸣谢:<a class="ae iu" href="https://towardsdatascience.com/should-ai-explain-itself-or-should-we-design-explainable-ai-so-that-it-doesnt-have-to-90e75bb6089e" rel="noopener" target="_blank">AI是否应该自我解释</a></figcaption></figure><h2 id="76f7" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">为什么我们需要模型的可解释性？</strong></h2><p id="da94" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">在我们进入问题的“为什么”部分之前，让我们理解一下什么是可解释性。虽然可解释性没有数学定义，但在机器学习文献中可以找到类似这里的启发式定义:“<em class="ko">可解释性是人类可以理解决策原因</em>的程度”，或者这里:“<em class="ko">可解释性是人类可以持续预测模型result⁴ </em>的程度。机器学习模型的可解释性越高，人们就越容易理解为什么该模型会做出某种预测。</p><p id="514c" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">现在，我们已经定义了模型的可解释性，让我们来看看为什么可解释性对机器学习模型很重要。</p><ul class=""><li id="f449" class="ku kv hi jv b jw kp ka kq jg kw jk kx jo ky kn kz la lb lc bi translated"><em class="ko">建立信任</em>:许多组织依靠机器学习模型来做出重要决策。例如，银行使用模型批准或拒绝向申请人提供贷款，医院评估患者的医疗状况风险，或者信用卡公司预测交易是否欺诈。如果机器学习模型也能解释它为什么做出预测，那么机器学习模型的用户会更信任这些预测。</li><li id="fbdd" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn kz la lb lc bi translated"><em class="ko">人类的好奇心&amp;学习</em>:人类对他们的环境有一个心智模型，这个模型通过为他们周围发生的事件寻找解释而不断更新。黑盒机器学习模型可能会让用户感到困惑，因为它对所做的预测提供的解释很少。另一方面，可解释的机器学习模型可以促进学习，并帮助其用户对预测问题形成更好的理解和直觉。</li><li id="363c" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn kz la lb lc bi translated"><em class="ko">检测偏差和边缘案例</em>:考虑一个机器学习模型的例子，该模型用于在银行批准或拒绝贷款。模型可能会在训练数据中发现偏差，并可能歧视某些少数群体。可解释性有助于在模型开发时发现偏差。可解释性还有助于识别模型可能失败的边缘情况。例如，如果发现自动驾驶汽车中的ML模型使用自行车的两个轮子来检测骑自行车的人，它可能会提示我们思考如果模型看到一辆带有侧袋的自行车，轮子可能看不清楚，它会如何表现。</li><li id="b2a6" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn kz la lb lc bi translated"><em class="ko">监管要求</em>:欧洲的GDPR或美国的平等信贷机会法案等法规为个人提供了对机器学习模型做出的对他们有重大影响的决策进行解释的权利，特别是在法律或财务方面。</li></ul><h2 id="b5cf" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">什么是SHAP？</h2><p id="b9a0" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">SHAP(SHapley Additive exPlanation)是一种解释任何机器学习模型输出的博弈论方法。<strong class="jv hj">shap的目标是将任何xᵢ实例的预测解释为其单个特征值贡献的总和。</strong>假设单个特征值在合作游戏中，其支出是预测。在此设置中，Shapley值提供了一种在特征值之间公平分配支出的方法。请注意，这里的“特征值”是指xᵢ.实例的某个特征的数值或分类值下面我们用一个简单的例子来解释这个概念。</p><h2 id="16e6" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">什么是沙普利价值观？</strong></h2><p id="9b97" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">让我们假设A、B、C、D是联盟博弈中具有不同技能组合的四个玩家(即A、B、C &amp; D在同一个队中),并且有一些支付。玩家之间分配奖金的最公平方式是什么？回答这个问题的一个方法是，认为玩家是按顺序加入群体的(例如:A&gt;B&gt;C&gt;D)，然后，我们可以计算每个玩家的边际贡献，作为玩家加入群体时支出的变化。但是，可能存在影响支付计算的交互效应。例如，如果A和B有不同的技能组合，那么只有A和B的组的总支出将是以下3部分的总和:</p><ul class=""><li id="055e" class="ku kv hi jv b jw kp ka kq jg kw jk kx jo ky kn kz la lb lc bi translated">单独用A支付，</li><li id="5797" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn kz la lb lc bi translated">单独用B支付，</li><li id="9def" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn kz la lb lc bi translated">拥有A和b的额外支出。</li></ul><p id="84e7" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">然而，如果我们假设B在A之后加入，那么上述算法将同时拥有A &amp; B 的“<em class="ko">额外支出完全归属于玩家B，这是不正确的。我们发现边际贡献将取决于我们假设玩家加入群体的顺序。Shapley values通过计算所有可能序列中每个玩家的平均边际贡献来克服这一缺点。因此，如果有n个玩家，则考虑n阶乘的可能序列。这里，我们假设可以计算任何玩家子集的支出。</em></p><p id="8e75" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">在机器学习模型的上下文中，实例xᵢ的个体特征值是玩家，并且“<em class="ko">预测yᵢ减去整个训练数据的平均预测</em>”是支付。可以从理论上证明Shapley值是唯一满足以下性质的归属方法:</p><ol class=""><li id="6b3c" class="ku kv hi jv b jw kp ka kq jg kw jk kx jo ky kn li la lb lc bi translated">效率:特征贡献必须加起来是xᵢ预测和平均预测的差。</li><li id="dd6c" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn li la lb lc bi translated">对称性:如果两个特征值j和k对所有可能的联合的贡献相等，则它们的贡献应该相同。</li><li id="d6d0" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn li la lb lc bi translated">哑元:不改变预测值的要素j(无论它被添加到哪个要素值联盟)的Shapley值应该为0。</li><li id="ac9d" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn li la lb lc bi translated">可加性:对于一个有P₁和P₂联合支付的游戏，他们各自的沙普利值应该是ϕ₁ᵢ + ϕ₂ᵢ.</li></ol><p id="27c8" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">对于具有预测函数f(x)和M特征的模型，我们可以如下获得Shapley值:</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/0abd1a40335a6484c4ecfd436c99f3ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0gMGleWFLgVAD-5OMDllYg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">符号:|M|是特征的总数。S表示不包括第I个特征的任何特征子集，而|S|是该子集的大小。fₓ()代表模型的预测函数。</figcaption></figure><p id="716b" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">上面的公式是除了第I个特征值之外的所有可能的特征值子集的总和。这里，|S|！表示出现在第I个特征值之前的特征值的排列数。同理，(|M|-|S|-1)！表示出现在第I个特征值之后的特征值的排列数。上述等式中的差项是将第I个特征值加到s上的边际贡献。另请注意，上述等式要求我们计算任何特征子集的模型预测，这对于ML模型可能是不可行的。</p><h2 id="9c83" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">沙普利价值公司的SHAP</h2><p id="08a2" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">SHAP值是上述方程在假设条件下的解:<em class="ko"> f(xₛ) = E[f(x|xₛ)].即，对于特征值的任何子集s的预测是给定子集xₛ.的f(x)的预测的期望值</em></p><p id="157c" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">SHAP值的精确计算在计算上具有挑战性。SHAP论文描述了两种与模型无关的近似方法，一种是已知的(Shapley采样值)，另一种是新颖的&amp;基于石灰(核SHAP)。SHAP论文还描述了几种特定模型类型的近似方法，如线性SHAP、树形SHAP、深度SHAP等。这些方法假设特征独立和模型线性，以简化SHAP值的计算。我们将在后续文章中详细探讨其中的一些方法。</p><p id="209c" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated"><strong class="jv hj">SHAP论文的主要贡献有</strong>:</p><ul class=""><li id="199d" class="ku kv hi jv b jw kp ka kq jg kw jk kx jo ky kn kz la lb lc bi translated">确定一类新的附加特征重要性度量，它统一了六种现有的方法。</li><li id="6aa8" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn kz la lb lc bi translated">理论结果表明，这类方法存在一个唯一的解，并具有理想的性质。</li><li id="5b8b" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn kz la lb lc bi translated">用于计算特征重要性值的新方法，具有改进的计算性能和与人类直觉更好的一致性。</li></ul><p id="5c8f" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated"><strong class="jv hj">链接到本系列的其他文章:</strong></p><p id="d1e7" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated"><a class="ae iu" rel="noopener" href="/@rakesh.melezhath/shap-part-2-kernel-shap-3c11e7a971b1"> SHAP第二部分:内核SHAP </a></p><p id="7a9f" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated"><a class="ae iu" rel="noopener" href="/@rakesh.melezhath/shap-part-3-tree-shap-3af9bcd7cd9b"> SHAP第三部:树SHAP </a></p><p id="a7ee" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated"><strong class="jv hj">参考文献:</strong></p><ol class=""><li id="8436" class="ku kv hi jv b jw kp ka kq jg kw jk kx jo ky kn li la lb lc bi translated"><a class="ae iu" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">可解释的机器学习——让黑盒模型变得可解释的指南。</a></li><li id="40a1" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn li la lb lc bi translated">SHAP:解释模式预测的统一方法。arXiv:1705.07874</li><li id="df67" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn li la lb lc bi translated">米勒蒂姆。"人工智能中的解释:来自社会科学的见解."arXiv预印本arXiv:1706.07269。(2017)</li><li id="6304" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn li la lb lc bi translated">Kim，Been，Rajiv Khanna，和Oluwasanmi O. Koyejo。“例子还不够，要学会批判！对可解释性的批评。”神经信息处理系统进展(2016)。</li><li id="5950" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn li la lb lc bi translated"><a class="ae iu" href="https://www.youtube.com/watch?v=B-c8tIgchu0" rel="noopener ugc nofollow" target="_blank">可解释的人工智能科学&amp;医学——微软研究院</a></li><li id="9266" class="ku kv hi jv b jw ld ka le jg lf jk lg jo lh kn li la lb lc bi translated"><a class="ae iu" href="https://towardsdatascience.com/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d" rel="noopener" target="_blank">https://towards data science . com/one-feature-attribute-method-to-possible-rule-them-all-Shapley-values-f3e 04534983d</a></li></ol></div></div>    
</body>
</html>