<html>
<head>
<title>Reinforcement Learning in Portfolio Management</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">投资组合管理中的强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-in-portfolio-management-4e4eed75961d?source=collection_archive---------15-----------------------#2020-08-27">https://medium.com/analytics-vidhya/reinforcement-learning-in-portfolio-management-4e4eed75961d?source=collection_archive---------15-----------------------#2020-08-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/5044be9f9795218aaf5c5dc7f10dea25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*vK7tScIfX6rGy1Lig16_bg.png"/></div></figure><p id="08a3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">强化学习(RL)是继监督学习和非监督学习之后的一种机器学习技术。深度强化学习侧重于使用神经网络的学习。在这篇文章中，我将尝试用一种非常简单的方式来解释RL的数学概念。</p><p id="58ab" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">什么是RL，它与有监督和无监督的机器学习问题有什么不同？</p><p id="8f43" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">监督学习——我们有一个带有标记目标变量的<strong class="io hj">结构化数据，我们<strong class="io hj">为一个类似的未标记结构化数据集预测目标</strong>。分类和回归就是例子。</strong></p><p id="ea6c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">无监督学习——我们有一个<strong class="io hj">未标记和非结构化的数据集</strong>,我们<strong class="io hj">学习数据的潜在模式</strong>。聚类和降维就是例子。</p><p id="ead9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">强化学习不同于监督学习和非监督学习。它用于解决基于奖励的问题。让我们看看这个例子就明白了。</p><p id="9bf2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你有没有见过，一只宠物狗是怎么被训练成坐着握手的，每次它做对了，我们就奖励它。在下面(真的很可怕)的图中，我们可以看到我给狗一个指令，它采取一个行动，这给他一个奖励和下一个指令，直到它学会一切。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es jk"><img src="../Images/364a93f08d04ae940cdfaec49cd0bcdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*h7KpJGqWnY8XVzAghV9biw.png"/></div></figure><p id="aeb1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">RL是一种类似的基于奖励的学习方式。在RL中，我们有一个类似的设置如下，(但一个更好更漂亮的图片)。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es jp"><img src="../Images/676a98d1e8e6214c2b7e0905b22082ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*QUO5uCtZB3AL7LfgqVaiqw.png"/></div></figure><p id="06c8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里，环境是我，代理是我的狗，状态是指令，下一个状态是下一个指令，动作是动作。环境给出一个状态，该状态触发代理接收奖励的动作，这一直持续到环境停止给出下一个状态。RL的著名应用有</p><p id="01a3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">1.网络游戏围棋</p><p id="7c25" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.雅达利游戏</p><p id="db9d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.让人形机器人行走</p><p id="3576" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">4.管理投资组合</p><p id="ca7c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">RL可以通过称为马尔可夫决策过程(MDP)的随机过程在数学上形式化。MDP是安德烈·马尔科夫著名的马尔科夫链的延伸。</p><p id="5f65" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">什么是马尔可夫链和MDP？</p><p id="37f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">马尔可夫链根据代理的当前状态和转移概率预测代理的下一个状态。并且这种预测可以持续到收敛，因此被称为链。例如:假设市场上有3家著名的零售店，分别是X、Y和Z，那么我们可以根据今天光顾该店的顾客数量以及该顾客可能光顾同一家店的概率来预测第二天会有多少顾客光顾同一家店。</p><p id="7248" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">MDP是马尔可夫链的一种扩展，在连续的状态中有更少的元素，如动作和奖励，就像在训练狗的例子中一样。</p><p id="9133" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数学上MDP被定义为使总收益最大化的元组(s，a，p，ɣ，r)，元组被定义为:</p><p id="a2eb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">1.s是状态的有限集合</p><p id="bdc8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.a是一组有限的动作</p><p id="4f65" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.p是状态转移概率分布</p><p id="b5eb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">4.ɣ是贴现因子，属于[0，1]</p><p id="b035" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">5.r是奖励函数</p><p id="b80f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">总收益定义为</p><p id="7086" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">R(s0)+ɣ R(s1)+ɣ*ɣ*R(s2)+….</p><p id="c78a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个等式只不过是以ɣ为贴现因子的未来回报的现值。ɣ倾向于权衡眼前的未来回报，而不是延迟的未来回报。</p><p id="6e17" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">RL的一个很好的应用是在项目组合管理中，如论文“<em class="jq">项目组合管理中的对抗性深度强化学习</em>”中所述。</p><p id="adb2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">投资组合管理问题可以定义为-在给定的时间段内，股票经纪人应该如何重新分配投资组合，以使利润最大化。在MDP，我们有连续的市场环境，收盘价等于第二天的开盘价。这里的元组(s，a，p，ɣ，r)应该是:</p><p id="3ced" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一种状态包括固定窗口中的先前的开盘价、收盘价、最高价、最低价、成交量或一些其他财务指标。</p><p id="0f7d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">a-动作是分配给每个资产的权重，使得权重之和等于1。</p><p id="cf9d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">r——回报是每次行动后财富的波动减去交易成本</p><p id="cc53" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">P —转移概率分布</p><p id="84f4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">ɣ —折扣系数</p><p id="afed" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在投资组合管理领域，由于T时刻积累的财富会在t+1时刻重新分配，这表明T时刻的财富是连续的乘积形式，而不是总和。因此总收益是</p><p id="576a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">R(s0)*ɣ R(s1)*ɣ*ɣ*R(s2)*….运筹学</p><p id="d5aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">log(r(s0))+log(ɣr(s1))+log(ɣ*ɣ*r(s2))+….</p><p id="e1ab" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">谢谢你</p><p id="ef13" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你也可以看看neptune.ai的这个博客，详细的解释和可视化。</p><p id="ca9a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">https://Neptune . ai/blog/Markov-决策-过程-强化-学习</p></div></div>    
</body>
</html>