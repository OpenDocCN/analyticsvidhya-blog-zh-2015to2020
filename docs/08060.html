<html>
<head>
<title>Image Classification using CNN in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中基于CNN的图像分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/image-classification-using-cnn-in-pytorch-65b1968d9e1f?source=collection_archive---------6-----------------------#2020-07-16">https://medium.com/analytics-vidhya/image-classification-using-cnn-in-pytorch-65b1968d9e1f?source=collection_archive---------6-----------------------#2020-07-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="da33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将在PyTorch中讨论使用CNN的多类图像分类，这里我们将使用Inception v3深度学习架构。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/842d74d310776f57a2158ae371065b84.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*yFY9-IaII-_oQeP-EPCHwA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">初始架构</figcaption></figure><h1 id="300a" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">从这篇文章中吸取教训</h1><ul class=""><li id="20c4" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc ku kv kw kx bi translated">你将学习更深入的卷积神经网络进行图像分类</li><li id="e1f8" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">为什么PyTorch比其他框架更灵活</li><li id="7c47" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">将Inception v3与cifar10数据集一起使用。</li></ul><p id="4050" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">什么是卷积神经网络？</p><p id="a35e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在深度学习中，卷积神经网络是一类深度神经网络，最常用于分析视觉图像。基于它们的共享权重架构和平移不变性特征，它们也被称为移位不变或空间不变人工神经网络。</p><h1 id="7ab2" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">使用的数据集:-CIFS ar 10数据集</h1><h1 id="e1b6" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">关于使用的数据集:-</h1><p id="7aaf" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated"><a class="ae lg" href="http://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-10 </a>是一个已建立的用于物体识别的计算机视觉数据集。它是<a class="ae lg" href="http://groups.csail.mit.edu/vision/TinyImages/" rel="noopener ugc nofollow" target="_blank">8000万微小图像数据集</a>的子集，由60，000张32x32彩色图像组成，包含10个对象类中的一个，每个类6000张图像。它由亚历克斯·克里热夫斯基、维诺德·奈尔和杰弗里·辛顿收藏。</p><p id="f180" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Kaggle正在为机器学习社区主办一个CIFAR-10排行榜，用于娱乐和练习。你可以在罗德里戈·贝纳森的<a class="ae lg" href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html" rel="noopener ugc nofollow" target="_blank">分类结果页面</a>上看到你的方法与最新研究方法的对比。</p><h1 id="dbab" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">用于代码实现的Git hub链接:-https://git hub . com/vats manish/Inception-v3-with-py torch</h1><h1 id="dbe0" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">《盗梦空间》第三版:-</h1><div class="lh li ez fb lj lk"><a href="https://arxiv.org/abs/1512.00567" rel="noopener  ugc nofollow" target="_blank"><div class="ll ab dw"><div class="lm ab ln cl cj lo"><h2 class="bd hj fi z dy lp ea eb lq ed ef hh bi translated">重新思考计算机视觉的初始架构</h2><div class="lr l"><h3 class="bd b fi z dy lp ea eb lq ed ef dx translated">卷积网络是大多数最先进的计算机视觉解决方案的核心，适用于各种各样的任务…</h3></div><div class="ls l"><p class="bd b fp z dy lp ea eb lq ed ef dx translated">arxiv.org</p></div></div></div></a></div><p id="d875" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一种简单但强大的方法来创建更好的深度学习模型。你可以做一个更大的模型，无论是深度，也就是层数，还是每层中神经元的数量。但是正如你所想象的，这经常会造成复杂的情况:</p><ul class=""><li id="003f" class="kn ko hi ih b ii ij im in iq lt iu lu iy lv jc ku kv kw kx bi translated"><strong class="ih hj">模型越大，越容易过度拟合。</strong>当训练数据很小时，这一点尤其明显</li><li id="2455" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">增加参数数量意味着您需要增加现有的计算资源</strong></li></ul><p id="09b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如该论文所建议的，解决这一问题的方法是转向稀疏连接的网络架构，它将取代全连接的网络架构，尤其是在卷积层内部。这个想法可以在下面的图片中概念化:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/d07ac0d5a6abb9eaa8520e931fda1468.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/0*P2ZHGg9tGhZl0oQ1.png"/></div></figure><p id="a7dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">密集连接的架构</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/8ff8aacb796f513f31dc0edc1c0fef74.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*3cagmTPFOaQsyQLs.png"/></div></figure><p id="e856" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">稀疏连接的架构</p><p id="d6ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文提出了创造深层建筑的新思路。这种方法可以让您保持“计算预算”，同时增加网络的深度和宽度。听起来好得难以置信！概念化的想法看起来是这样的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="er es ly"><img src="../Images/606097301e1937ee2cd0847b2b65a151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JmtsQBl3sulfKR66.png"/></div></div></figure><p id="3fd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们更详细地看一下提议的架构。</p><h1 id="1664" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">提议的建筑细节</h1><p id="d800" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">本文提出了一种新型架构——Google net或Inception v1。基本上就是一个27层深的卷积神经网络(CNN)。以下是模型总结:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/a783d6e2b296b40ecb59b6d0183974e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:148/format:webp/0*yL-PZ5nJx8LUpH2I.png"/></div></figure><p id="0a1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，在上面的图像中，有一个层称为初始层。这实际上是论文方法背后的主要思想。初始层是稀疏连接架构的核心概念。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/86aad3bf96dfb9b2d1256f6160cc1973.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*vb8owp_SfP69HC48.png"/></div></figure><p id="3a1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个初始模块的想法</p><p id="030a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我更详细地解释一下什么是初始层。从报纸上摘录:</p><blockquote class="me mf mg"><p id="16d3" class="if ig mh ih b ii ij ik il im in io ip mi ir is it mj iv iw ix mk iz ja jb jc hb bi translated">“(初始层)是所有这些层(即1×1卷积层、3×3卷积层、5×5卷积层)的组合，它们的输出滤波器组连接成一个输出向量，形成下一级的输入。”</p></blockquote><p id="58b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了上面提到的层，在最初的初始层中还有两个主要的附加组件:</p><ul class=""><li id="bd33" class="kn ko hi ih b ii ij im in iq lt iu lu iy lv jc ku kv kw kx bi translated">1×1卷积层，然后应用另一层，主要用于降维</li><li id="3af9" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">并行最大池层，为初始层提供了另一种选择</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/f09430eb582ead4eeb163d95b2d4c29e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/0*qZuGuHanEJMgM_fJ.png"/></div></figure><p id="b064" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">初始层</p><p id="2a7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了理解起始层结构的重要性，作者引用了人类学习的赫布边原理。上面写着“一起放电的神经元，连接在一起”。作者建议<strong class="ih hj">在深度学习模型中创建后续层时，应该注意前一层的学习。</strong></p><p id="fd18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，假设我们深度学习模型中的一个层已经学会专注于面部的单个部分。网络的下一层可能会集中在图像中的整个面部，以识别那里存在的不同物体。现在要真正做到这一点，该层应该有适当的过滤器大小来检测不同的对象。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/b3bb306a073c278499858f30fa8c53cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*0Hv6eeCfCWc-V-rs.png"/></div></figure><p id="e4c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是先启层脱颖而出的地方。它允许内部层挑选与学习所需信息相关的过滤器尺寸。因此，即使图像中人脸的大小不同(如下图所示)，图层也会相应地识别人脸。对于第一幅图像，它可能需要较大的过滤器尺寸，而对于第二幅图像，它将需要较小的过滤器尺寸。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/9ead3629ad0b40677cc1eb596395605f.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*D-uR19vqr7aeA8-7.png"/></div></figure><p id="08ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">包含所有规格的整体架构如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/8a42cc208010cf18320292decd7a30a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*KD7-8nb9cwSEjvIB.png"/></div></figure><h2 id="f6a3" class="mm jq hi bd jr mn mo mp jv mq mr ms jz iq mt mu kd iu mv mw kh iy mx my kl mz bi translated">使用Pytorch在cifar10数据集上实施Inception v3分步代码解释</h2><p id="f4c3" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">我曾经用google colab(gpu)来训练模型，用google colab(cpu)来测试。</p><p id="e6a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1 —导入有用的库并安装google drive。</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="b553" class="mm jq hi nb b fi nf ng l nh ni">from collections import namedtuple<br/><br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F</span><span id="d1a2" class="mm jq hi nb b fi nj ng l nh ni"><br/>__all__ = ['Inception3', 'inception_v3']<br/><br/>_InceptionOutputs = namedtuple('InceptionOutputs', ['logits', 'aux_logits'])</span><span id="1e99" class="mm jq hi nb b fi nj ng l nh ni">Mount Drive<br/>from google.colab import drive<br/>drive.mount('/content/drive')</span><span id="c8b2" class="mm jq hi nb b fi nj ng l nh ni">Change the path<br/>import os<br/>if not os.path.exists('/content/drive/My Drive/Inception_CIFAR10/'):<br/>  os.makedirs('/content/drive/My Drive/Inception_CIFAR10/')<br/>os.chdir('/content/drive/My Drive/Inception_CIFAR10/')</span></pre><p id="7fb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2 —从头开始第三版</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="4def" class="mm jq hi nb b fi nf ng l nh ni">def inception_v3(pretrained=False, **kwargs):<br/>  if pretrained:<br/>    if 'transform_input' not in kwargs:<br/>      kwargs['transform_input'] = True<br/>    if 'aux_logits' in kwargs:<br/>      original_aux_logits = kwargs['aux_logits']<br/>      kwargs['aux_logits'] = True<br/>    else:<br/>      original_aux_logits = True<br/>    model = Inception3(**kwargs)<br/>    if not original_aux_logits:<br/>      model.aux_logits = False<br/>    return model<br/><br/>  return Inception3(**kwargs)</span></pre><p id="a57f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3-我们将从头开始制作模型，因此将模型返回到参数，因此将关键字返回到我们的模型。**kwargs允许你传递<strong class="ih hj">关键字</strong>可变长度的参数给一个函数。如果你想在一个函数中处理<strong class="ih hj">命名参数</strong>，你应该使用**kwargs。这里有一个例子可以帮助你理解:</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="8647" class="mm jq hi nb b fi nf ng l nh ni">class Inception3(nn.Module):<br/><br/>  def __init__(self, num_classes=10, aux_logits=True, transform_input=True):<br/>    super(Inception3, self).__init__()<br/>    self.aux_logits = aux_logits<br/>    self.transform_input = transform_input<br/>    self.Conv2d_4a_3x3 = BasicConv2d(3, 32, kernel_size=3,padding=1)<br/>    self.Mixed_5b = InceptionA(32, pool_features=8)<br/>    self.Mixed_5c = InceptionA(64, pool_features=72)<br/>    self.Mixed_6a = InceptionB(128)<br/>    self.Mixed_6b = InceptionC(256, channels_7x7=64)<br/>    if aux_logits:<br/>      self.AuxLogits = InceptionAux(512, num_classes)<br/>    self.Mixed_7a = InceptionD(512)<br/>    self.fc = nn.Linear(768, num_classes)<br/><br/>    for m in self.modules():<br/>      if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):<br/>        import scipy.stats as stats<br/>        stddev = m.stddev if hasattr(m, 'stddev') else 0.1<br/>        X = stats.truncnorm(-2, 2, scale=stddev)<br/>        values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)<br/>        values = values.view(m.weight.size())<br/>        with torch.no_grad():<br/>          m.weight.copy_(values)<br/>      elif isinstance(m, nn.BatchNorm2d):<br/>        nn.init.constant_(m.weight, 1)<br/>        nn.init.constant_(m.bias, 0)<br/><br/>  def forward(self, x):<br/>    global aux<br/>    print(x.shape) <br/>    x = self.Conv2d_4a_3x3(x)<br/>    x = self.Mixed_5b(x)<br/>    x = self.Mixed_5c(x)<br/>    x = self.Mixed_6a(x)<br/>    x = self.Mixed_6b(x)<br/>    if self.training and self.aux_logits:<br/>      aux = self.AuxLogits(x)<br/>    x = self.Mixed_7a(x)<br/>    x = F.adaptive_avg_pool2d(x, (1, 1))<br/>    x = F.dropout(x, training=self.training)<br/>    x = torch.flatten(x, 1)<br/>    x = self.fc(x)<br/>    print(x.shape)<br/>    if self.training and self.aux_logits:<br/>      return _InceptionOutputs(x, aux)<br/>    return x</span></pre><p id="c030" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4 —这里我们定义了一个类，并传递我们拥有的类的数量10。<code class="du nk nl nm nb b">aux_logits</code>只会在<code class="du nk nl nm nb b">train()</code>模式下返回，所以确保在下一个纪元和transform_input说改变图像形状之前激活它。</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="60e3" class="mm jq hi nb b fi nf ng l nh ni">class InceptionA(nn.Module):<br/><br/>  def __init__(self, in_channels, pool_features):<br/>    super(InceptionA, self).__init__()<br/>    self.branch1x1 = BasicConv2d(in_channels, 8, kernel_size=1)<br/><br/>    self.branch5x5_1 = BasicConv2d(in_channels, 8, kernel_size=1)<br/>    self.branch5x5_2 = BasicConv2d(8, 16, kernel_size=5, padding=2)<br/><br/>    self.branch3x3dbl_1 = BasicConv2d(in_channels, 8, kernel_size=1)<br/>    self.branch3x3dbl_2 = BasicConv2d(8, 16, kernel_size=3, padding=1)<br/>    self.branch3x3dbl_3 = BasicConv2d(16, 32, kernel_size=3, padding=1)<br/><br/>    self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)<br/><br/>  def forward(self, x):<br/>    branch1x1 = self.branch1x1(x)<br/><br/>    branch5x5 = self.branch5x5_1(x)<br/>    branch5x5 = self.branch5x5_2(branch5x5)<br/><br/>    branch3x3dbl = self.branch3x3dbl_1(x)<br/>    branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)<br/>    branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)<br/><br/>    branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)<br/>    branch_pool = self.branch_pool(branch_pool)<br/><br/>    outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]<br/>    return torch.cat(outputs, 1)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nn"><img src="../Images/80e9d922117cf03a5b038a9bd365f7ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*PFauP9irmYb44e56NNPLuA.png"/></div></figure><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="413c" class="mm jq hi nb b fi nf ng l nh ni">class InceptionB(nn.Module):<br/><br/>  def __init__(self, in_channels):<br/>    super(InceptionB, self).__init__()<br/>    self.branch3x3 = BasicConv2d(in_channels, 32, kernel_size=3, stride=2)<br/><br/>    self.branch3x3dbl_1 = BasicConv2d(in_channels, 32, kernel_size=1)<br/>    self.branch3x3dbl_2 = BasicConv2d(32, 64, kernel_size=3, padding=1)<br/>    self.branch3x3dbl_3 = BasicConv2d(64, 96, kernel_size=3, stride=2)<br/><br/>  def forward(self, x):<br/>    branch3x3 = self.branch3x3(x)<br/><br/>    branch3x3dbl = self.branch3x3dbl_1(x)<br/>    branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)<br/>    branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)<br/><br/>    branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)<br/><br/>    outputs = [branch3x3, branch3x3dbl, branch_pool]<br/>    return torch.cat(outputs, 1)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es no"><img src="../Images/a9f48a37b474f5d5061aab670abca4ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*w7IqFc9prq_CTqPy7AK5ew.png"/></div></figure><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="55d2" class="mm jq hi nb b fi nf ng l nh ni">class InceptionC(nn.Module):<br/><br/>  def __init__(self, in_channels, channels_7x7):<br/>    super(InceptionC, self).__init__()<br/>    self.branch1x1 = BasicConv2d(in_channels, 128, kernel_size=1)<br/><br/>    c7 = channels_7x7<br/>    self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)<br/>    self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))<br/>    self.branch7x7_3 = BasicConv2d(c7, 128, kernel_size=(7, 1), padding=(3, 0))<br/><br/>    self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)<br/>    self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))<br/>    self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))<br/>    self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))<br/>    self.branch7x7dbl_5 = BasicConv2d(c7, 128, kernel_size=(1, 7), padding=(0, 3))<br/><br/>    self.branch_pool = BasicConv2d(in_channels, 128, kernel_size=1)<br/><br/>  def forward(self, x):<br/>    branch1x1 = self.branch1x1(x)<br/><br/>    branch7x7 = self.branch7x7_1(x)<br/>    branch7x7 = self.branch7x7_2(branch7x7)<br/>    branch7x7 = self.branch7x7_3(branch7x7)<br/><br/>    branch7x7dbl = self.branch7x7dbl_1(x)<br/>    branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)<br/>    branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)<br/>    branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)<br/>    branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)<br/><br/>    branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)<br/>    branch_pool = self.branch_pool(branch_pool)<br/><br/>    outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]<br/>    return torch.cat(outputs, 1)</span><span id="a306" class="mm jq hi nb b fi nj ng l nh ni">class InceptionD(nn.Module):<br/><br/>  def __init__(self, in_channels):<br/>    super(InceptionD, self).__init__()<br/>    self.branch3x3_1 = BasicConv2d(in_channels, 32, kernel_size=1)<br/>    self.branch3x3_2 = BasicConv2d(32,64, kernel_size=3, stride=2)<br/><br/>    self.branch7x7x3_1 = BasicConv2d(in_channels, 32, kernel_size=1)<br/>    self.branch7x7x3_2 = BasicConv2d(32,64, kernel_size=(1, 7), padding=(0, 3))<br/>    self.branch7x7x3_3 = BasicConv2d(64, 128, kernel_size=(7, 1), padding=(3, 0))<br/>    self.branch7x7x3_4 = BasicConv2d(128,192, kernel_size=3, stride=2)<br/><br/>  def forward(self, x):<br/>    branch3x3 = self.branch3x3_1(x)<br/>    branch3x3 = self.branch3x3_2(branch3x3)<br/><br/>    branch7x7x3 = self.branch7x7x3_1(x)<br/>    branch7x7x3 = self.branch7x7x3_2(branch7x7x3)<br/>    branch7x7x3 = self.branch7x7x3_3(branch7x7x3)<br/>    branch7x7x3 = self.branch7x7x3_4(branch7x7x3)<br/><br/>    branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)<br/>    outputs = [branch3x3, branch7x7x3, branch_pool]<br/>    return torch.cat(outputs, 1)</span><span id="2472" class="mm jq hi nb b fi nj ng l nh ni">class BasicConv2d(nn.Module):<br/><br/>  def __init__(self, in_channels, out_channels, **kwargs):<br/>    super(BasicConv2d, self).__init__()<br/>    self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)<br/>    self.bn = nn.BatchNorm2d(out_channels, eps=0.001)<br/><br/>  def forward(self, x):<br/>    x = self.conv(x)<br/>    x = self.bn(x)<br/>    return F.relu(x, inplace=True)</span></pre><p id="9a66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5-训练模型</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="8944" class="mm jq hi nb b fi nf ng l nh ni"># incremental training comments out that line of code.<br/><br/># Device configuration<br/>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/><br/>WORK_DIR = './data'<br/>NUM_EPOCHS = 100<br/>BATCH_SIZE = 32<br/>#LEARNING_RATE = 0.01<br/><br/>MODEL_PATH = './model'<br/>MODEL_NAME = 'Inception_v3.pth'<br/><br/># Create model<br/>if not os.path.exists(MODEL_PATH):<br/>  os.makedirs(MODEL_PATH)<br/><br/>#AUGMENTATIONS<br/>transform = transforms.Compose([<br/>  transforms.RandomCrop(32, padding=4),<br/>  #torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.4, saturation=0.5, hue=0.1),<br/>  transforms.RandomHorizontalFlip(),<br/>  torchvision.transforms.RandomVerticalFlip(),<br/>  # torchvision.transforms.RandomAffine(degrees=0, translate=(0.2,0.2), scale=None,shear=50, resample=False, fillcolor=0),<br/>  torchvision.transforms.RandomRotation((20), resample=False,expand=False, center=None),<br/>  transforms.ToTensor(),<br/>  transforms.Normalize([0.4913997551666284, 0.48215855929893703, 0.4465309133731618], [0.24703225141799082, 0.24348516474564, 0.26158783926049628])<br/>])<br/><br/># Load data<br/>dataset = torchvision.datasets.CIFAR10(root=WORK_DIR,<br/>                                        download=True,<br/>                                        train=True,<br/>                                        transform=transform)<br/><br/>dataset_loader = torch.utils.data.DataLoader(dataset=dataset,<br/>                                             batch_size=BATCH_SIZE,<br/>                                             shuffle=True)</span></pre><p id="5917" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6-现在计算模型中参数的数量</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="4cdf" class="mm jq hi nb b fi nf ng l nh ni"># Total parameters<br/>model = inception_v3().to(device)<br/>pytorch_total_params = sum(p.numel() for p in model.parameters())<br/>pytorch_total_params</span><span id="95e9" class="mm jq hi nb b fi nj ng l nh ni">2534260</span><span id="e806" class="mm jq hi nb b fi nj ng l nh ni">def main():<br/>  print(f"Train numbers:{len(dataset)}")<br/>  LEARNING_RATE = 0.001<br/>  MOMENTUM=0.9<br/>  # first train run this line<br/>  #model = inception_v3().to(device)<br/>  print(model)<br/>  #model_save_name = 'Inception_v3e1.pth'<br/>  model.load_state_dict(torch.load(MODEL_NAME))<br/>  # Load model<br/>  #if device == 'cuda':<br/><br/>    #model = torch.load(MODEL_PATH + MODEL_NAME).to(device)<br/>  #else:<br/>    #model = torch.load(MODEL_PATH + MODEL_NAME, map_location='cpu')<br/>  # cast<br/>  cast = torch.nn.CrossEntropyLoss().to(device)<br/>  # Optimization<br/>  optimizer = torch.optim.SGD(<br/>    model.parameters(),<br/>    lr=LEARNING_RATE,<br/>    momentum=MOMENTUM)<br/>  step = 1<br/>  loss_values=[]<br/>  for epoch in range(1, NUM_EPOCHS + 1):<br/>    print(loss_values)<br/>    model.train()<br/>    running_loss = 0.0<br/>    <br/>    # cal one epoch time<br/>    start = time.time()<br/>    correct = 0<br/>    total = 0<br/>    for images, labels in dataset_loader:<br/>      images = images.to(device)<br/>      print(images.shape)<br/>      labels = labels.to(device)<br/>      <br/>      outputs, aux_outputs = model(images)<br/>      loss1 = cast(outputs, labels)<br/>      loss2 = cast(aux_outputs, labels)<br/>      loss = loss1 + 0.4*loss2<br/>      running_loss =+ loss.item() * images.size(0)<br/><br/>      optimizer.zero_grad()<br/>      loss.backward()<br/>      optimizer.step()<br/>      print("epoch: ", epoch)<br/>      print(f"Step [{step * BATCH_SIZE}/{NUM_EPOCHS * len(dataset)}], "<br/>            f"Loss: {loss.item():.8f}.")<br/>      print("Running Loss=",running_loss)<br/>      step += 1<br/>      # equal prediction and acc<br/>      _, predicted = torch.max(outputs.data, 1)<br/>      # val_loader total<br/>      total += labels.size(0)<br/>      # add correct<br/>      correct += (predicted == labels).sum().item()<br/><br/>      print(f"Acc: {correct / total:.4f}.")<br/>        # cal train one epoch time<br/>    end = time.time()<br/>    loss_values.append(running_loss / len(dataset_loader))<br/>    <br/>    print(f"Epoch [{epoch}/{NUM_EPOCHS}], "<br/>          f"time: {end - start} sec!")<br/><br/>    # Save the model checkpoint<br/>    if epoch%20==0:<br/>    #   LEARNING_RATE=LEARNING_RATE/10<br/>    #   torch.save(model, MODEL_PATH + '/' + MODEL_NAME)<br/><br/>      model_save_name = 'Inception_v3_CIFAR10_32BATCH_lr0.001_crop_bflip_rot'+str(epoch)+'.pth'   #WE keep changing this and saving states ,can be found in excel sheet attached<br/>      torch.save(model.state_dict(), model_save_name)<br/>    print("epoch completed and model copy completed")<br/>    <br/>  torch.save(model,MODEL_NAME)<br/>  print(f"Model save to {MODEL_PATH + '/' + MODEL_NAME}.")</span><span id="8f3f" class="mm jq hi nb b fi nj ng l nh ni">if __name__ == '__main__':<br/>  main()</span></pre><p id="a0ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7 —培训准确性</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="f5d8" class="mm jq hi nb b fi nf ng l nh ni">Streaming output truncated to the last 5000 lines.<br/>torch.Size([64, 3, 32, 32])<br/>torch.Size([64, 3, 32, 32])<br/>torch.Size([64, 10])<br/>epoch:  96<br/>Step [959296/1000000], Loss: 0.01983919.<br/>Running Loss= 1.2697083950042725<br/>Acc: 0.9987.<br/>torch.Size([64, 3, 32, 32])<br/>torch.Size([64, 3, 32, 32])<br/>torch.Size([64, 10])<br/>epoch:  96<br/>Step [959360/1000000], Loss: 0.01000556.<br/>Running Loss= 0.6403558254241943<br/>Acc: 0.9988.<br/>torch.Size([64, 3, 32, 32])<br/>torch.Size([64, 3, 32, 32])<br/>torch.Size([64, 10])<br/>epoch:  96<br/>Step [959424/1000000], Loss: 0.01426543.<br/>Running Loss= 0.9129873514175415<br/>Acc: 0.9988.<br/>torch.Size([64, 3, 32, 32])<br/>torch.Size([64, 3, 32, 32])<br/>torch.Size([64, 10])<br/>epoch:  96<br/>Step [959488/1000000], Loss: 0.01021658.<br/>Running Loss= 0.6538611650466919<br/>Acc: 0.9988.<br/>torch.Size([64, 3, 32, 32])<br/>torch.Size([64, 3, 32, 32])<br/>torch.Size([64, 10])<br/>epoch:  96<br/>Step [959552/1000000], Loss: 0.01176894.<br/>Running Loss= 0.7532118558883667<br/>Acc: 0.9988.....................................</span></pre><p id="545a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的模型对参数的每步(历元)计算量很大。如果您想查看每一步的损失，那么您可以使用我的git hub库。</p><p id="b3cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">8-测试模型</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="68d6" class="mm jq hi nb b fi nf ng l nh ni">evice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/><br/>WORK_DIR = './data'<br/>BATCH_SIZE = 32<br/><br/>#MODEL_PATH = './model'<br/>#MODEL_NAME = 'Inception_v3.pth'<br/>#MODEL_NAME = "Inception_v3_CIFAR10_32SIZE_512BATCH_102_lr_low_para10.pth"<br/>MODEL_NAME="Inception_v3_CIFAR10_32BATCH_lr0.001_crop_bflip_rot100.pth"<br/><br/><br/>transform = transforms.Compose([<br/>  transforms.ToTensor(),<br/>  transforms.Normalize([0.4913997551666284, 0.48215855929893703, 0.4465309133731618], [0.24703225141799082, 0.24348516474564, 0.26158783926049628])<br/>])</span></pre><p id="cdce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">9-加载验证数据集</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="ad7b" class="mm jq hi nb b fi nf ng l nh ni">dataset = torchvision.datasets.CIFAR10(root=WORK_DIR,<br/>                                        download=True,<br/>                                        train=False,<br/>                                        transform=transform)<br/><br/>dataset_loader = torch.utils.data.DataLoader(dataset=dataset,<br/>                                             batch_size=BATCH_SIZE,<br/>                                             shuffle=True)</span></pre><p id="bf14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">进行验证并获得验证准确性的代码</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="2f2a" class="mm jq hi nb b fi nf ng l nh ni">def main():<br/>  print(f"Val numbers:{len(dataset)}")<br/>  #model = inception_v3().to(device)<br/>  print(model)<br/>  # Load model<br/>  if device == 'cuda':<br/>    #model = torch.load(MODEL_PATH+"/"+MODEL_NAME).to(device)<br/>    model.load_state_dict(torch.load(MODEL_NAME))<br/>  else:<br/>    #model = torch.load(MODEL_PATH+"/"+MODEL_NAME, map_location='cpu')<br/>    model.load_state_dict(torch.load(MODEL_NAME))<br/>  model.eval()<br/><br/>  correct = 0.<br/>  total = 0<br/>  for images, labels in dataset_loader:<br/>    # to GPU<br/>    images = images.to(device)<br/>    labels = labels.to(device)<br/>    # print prediction<br/>    outputs = model(images)<br/>    # equal prediction and acc<br/>    _, predicted = torch.max(outputs.data, 1)<br/>    # val_loader total<br/>    total += labels.size(0)<br/>    # add correct<br/>    correct += (predicted == labels).sum().item()<br/><br/>  print(f"Acc: {correct / total:.4f}.")</span><span id="2293" class="mm jq hi nb b fi nj ng l nh ni">if __name__ == '__main__':<br/>  <br/>  main()</span></pre><p id="80dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">10-模型摘要</p><pre class="je jf jg jh fd na nb nc nd aw ne bi"><span id="1f03" class="mm jq hi nb b fi nf ng l nh ni">Val numbers:10000<br/>Inception3(<br/>  (Conv2d_4a_3x3): BasicConv2d(<br/>    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>  )<br/>  (Mixed_5b): InceptionA(<br/>    (branch1x1): BasicConv2d(<br/>      (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch5x5_1): BasicConv2d(<br/>      (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch5x5_2): BasicConv2d(<br/>      (conv): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)<br/>      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch3x3dbl_1): BasicConv2d(<br/>      (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch3x3dbl_2): BasicConv2d(<br/>      (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch3x3dbl_3): BasicConv2d(<br/>      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch_pool): BasicConv2d(<br/>      (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (Mixed_5c): InceptionA(<br/>    (branch1x1): BasicConv2d(<br/>      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch5x5_1): BasicConv2d(<br/>      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch5x5_2): BasicConv2d(<br/>      (conv): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)<br/>      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch3x3dbl_1): BasicConv2d(<br/>      (conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(8, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch3x3dbl_2): BasicConv2d(<br/>      (conv): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch3x3dbl_3): BasicConv2d(<br/>      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch_pool): BasicConv2d(<br/>      (conv): Conv2d(64, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(72, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (Mixed_6a): InceptionB(<br/>    (branch3x3): BasicConv2d(<br/>      (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)<br/>      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch3x3dbl_1): BasicConv2d(<br/>      (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch3x3dbl_2): BasicConv2d(<br/>      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch3x3dbl_3): BasicConv2d(<br/>      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)<br/>      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (Mixed_6b): InceptionC(<br/>    (branch1x1): BasicConv2d(<br/>      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7_1): BasicConv2d(<br/>      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7_2): BasicConv2d(<br/>      (conv): Conv2d(64, 64, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)<br/>      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7_3): BasicConv2d(<br/>      (conv): Conv2d(64, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)<br/>      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7dbl_1): BasicConv2d(<br/>      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7dbl_2): BasicConv2d(<br/>      (conv): Conv2d(64, 64, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)<br/>      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7dbl_3): BasicConv2d(<br/>      (conv): Conv2d(64, 64, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)<br/>      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7dbl_4): BasicConv2d(<br/>      (conv): Conv2d(64, 64, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)<br/>      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7dbl_5): BasicConv2d(<br/>      (conv): Conv2d(64, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)<br/>      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch_pool): BasicConv2d(<br/>      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (AuxLogits): InceptionAux(<br/>    (conv0): BasicConv2d(<br/>      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (conv1): BasicConv2d(<br/>      (conv): Conv2d(128, 512, kernel_size=(5, 5), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (fc): Linear(in_features=512, out_features=10, bias=True)<br/>  )<br/>  (Mixed_7a): InceptionD(<br/>    (branch3x3_1): BasicConv2d(<br/>      (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch3x3_2): BasicConv2d(<br/>      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), bias=False)<br/>      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7x3_1): BasicConv2d(<br/>      (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7x3_2): BasicConv2d(<br/>      (conv): Conv2d(32, 64, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)<br/>      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7x3_3): BasicConv2d(<br/>      (conv): Conv2d(64, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)<br/>      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>    (branch7x7x3_4): BasicConv2d(<br/>      (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)<br/>      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)<br/>    )<br/>  )<br/>  (fc): Linear(in_features=768, out_features=10, bias=True)<br/>)</span><span id="4d10" class="mm jq hi nb b fi nj ng l nh ni">torch.Size([32, 10])<br/>torch.Size([32, 3, 32, 32])<br/>torch.Size([32, 10])<br/>torch.Size([32, 3, 32, 32])<br/>torch.Size([32, 10])<br/>torch.Size([32, 3, 32, 32])<br/>torch.Size([32, 10])<br/>torch.Size([32, 3, 32, 32])<br/>torch.Size([32, 10])<br/>torch.Size([32, 3, 32, 32])<br/>torch.Size([32, 10])<br/>torch.Size([32, 3, 32, 32])<br/>torch.Size([32, 10])<br/>torch.Size([32, 3, 32, 32])<br/>torch.Size([32, 10])<br/>torch.Size([32, 3, 32, 32])<br/>torch.Size([32, 10])<br/>torch.Size([16, 3, 32, 32])<br/>torch.Size([16, 10])<br/>Model accuracy on 10000 test images: 99.95%</span></pre><p id="782d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们获得了99.95%的测试准确性，一些工作也像数据扩充一样执行，这里不解释，所以通过我的github repo，您也可以派生模型并部署它</p><p id="22d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型部署部分:-</p><p id="0fd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lg" href="https://github.com/vatsmanish/Deploy_Inception_v3" rel="noopener ugc nofollow" target="_blank">https://github.com/vatsmanish/Deploy_Inception_v3</a></p><p id="7258" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章越来越长，所以我将在下一部分描述模型的部署。</p><p id="7e10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:-<a class="ae lg" href="https://arxiv.org/pdf/1512.00567.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1512.00567.pdf</a></p><div class="lh li ez fb lj lk"><a href="https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/" rel="noopener  ugc nofollow" target="_blank"><div class="ll ab dw"><div class="lm ab ln cl cj lo"><h2 class="bd hj fi z dy lp ea eb lq ed ef hh bi translated">战壕中的深度学习:从零开始理解盗梦空间网络</h2><div class="lr l"><h3 class="bd b fi z dy lp ea eb lq ed ef dx translated">随着越来越多的研究论文从世界各地涌现出来，深度学习正在迅速获得动力。这些文件…</h3></div><div class="ls l"><p class="bd b fp z dy lp ea eb lq ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu jj lk"/></div></div></a></div><div class="lh li ez fb lj lk"><a href="https://github.com/vatsmanish/Inception-v3-with-pytorch/blob/master/InceptionV3FromScratch.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ll ab dw"><div class="lm ab ln cl cj lo"><h2 class="bd hj fi z dy lp ea eb lq ed ef hh bi translated">vatsmanish/Inception-v3-with-py torch</h2><div class="lr l"><h3 class="bd b fi z dy lp ea eb lq ed ef dx translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="ls l"><p class="bd b fp z dy lp ea eb lq ed ef dx translated">github.com</p></div></div><div class="np l"><div class="nv l nr ns nt np nu jj lk"/></div></div></a></div><p id="a84e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我的另一篇关于介质的文章:-</strong><a class="ae lg" rel="noopener" href="/@manish.kumar_61520/introduction-to-vector-autoregression-6ec386db387e">https://medium . com/@ manish . Kumar _ 61520/introduction-to-vector-auto regression-6ec 386 db 387 e</a></p><div class="lh li ez fb lj lk"><a rel="noopener follow" target="_blank" href="/@manish.kumar_61520/the-default-of-credit-card-clients-dataset-81908562a67e"><div class="ll ab dw"><div class="lm ab ln cl cj lo"><h2 class="bd hj fi z dy lp ea eb lq ed ef hh bi translated">信用卡客户数据集的默认值</h2><div class="lr l"><h3 class="bd b fi z dy lp ea eb lq ed ef dx translated">数据集信息</h3></div><div class="ls l"><p class="bd b fp z dy lp ea eb lq ed ef dx translated">medium.com</p></div></div><div class="np l"><div class="nw l nr ns nt np nu jj lk"/></div></div></a></div></div></div>    
</body>
</html>