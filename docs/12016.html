<html>
<head>
<title>Deep Learning Specialization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习专业化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-learning-specialization-d517a9d8db56?source=collection_archive---------15-----------------------#2020-12-28">https://medium.com/analytics-vidhya/deep-learning-specialization-d517a9d8db56?source=collection_archive---------15-----------------------#2020-12-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="a58d" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">神经网络和深度学习(第 3 周和第 4 周笔记)</h2></div><p id="226a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">本周，我们将学习神经网络。如果您没有浏览前一周的笔记，请参考<a class="ae js" href="https://madhurijain27.medium.com/deep-learning-specialization-353c997af655" rel="noopener">第 1 周</a>和<a class="ae js" href="https://madhurijain27.medium.com/deep-learning-specialization-df3938c3234c" rel="noopener">第 2 周</a>的笔记。</p><h2 id="4211" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">神经网络表示法</strong></h2><p id="ff68" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">我们将从具有单个隐藏层的神经网络开始。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es kt"><img src="../Images/626d811cc188016ae6ae9ab9fd9f4ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*rVVegJib7U2OqntsFTw_fQ.png"/></div></figure><p id="8bd4" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">X1、X2 和 X3 是垂直堆叠的输入要素，称为输入图层。带有 4 个圆圈的中间层称为隐藏层。之所以命名为隐藏，是因为在这一层执行的操作没有被观察到。最后一个圆圈称为输出层。每一层都用一个字母和一个上标数字来标识。</p><p id="b3f8" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">输入层= a^[0] = X</p><p id="58fc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">隐藏层= a^[1]</p><p id="40bb" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">输出图层= a^[2] = Y 形帽</p><p id="4570" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">计算特定网络的层数时，不考虑输入图层。完整的网络和注释可以在下面看到。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lb"><img src="../Images/c679e6f9d6a51442a42c823820e1c074.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*MyBHgWtD68PJLhNeCgFa_Q.png"/></div></figure><h2 id="f169" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">神经网络如何计算输出？</strong></h2><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lc"><img src="../Images/ceb51fd502e231cc2b686a26bbe4af4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*l2Cbl0SpORW890trZVB80w.png"/></div></figure><p id="f37d" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如上图所示，类似于逻辑回归 z 和 sigmoid z 是针对隐藏层和输出层中的每个圆。在隐藏层中，w^[1 的维数为(4，3)，b^[1 的维数为(4，1)。在输出层，w^[2]的维数是(1，4)，b^[2]是一个实数。上图是一个训练示例。我们可以使用矢量化类似地计算多个训练示例的输出值。</p><h2 id="e397" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">激活功能的选择</strong></h2><p id="bc46" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">到目前为止，我们已经使用了 sigmoid 函数，但还有其他选项可以被证明是更好的选择。让我们从 tan-h 函数开始。它可以按如下方式绘制，其值介于-1 和+1 之间。tan-h 函数的公式可以写成:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es ld"><img src="../Images/cc329d6c4e2f426ea0f50061306a4d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*cjAC4F1Z1cY2tGQN0mYZdg.png"/></div></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es le"><img src="../Images/0ce97660d11af4560991697e516d9261.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*6_FVj4O8-81uHxSnrCYuKA.png"/></div></figure><p id="ce21" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">除了在二元分类的情况下，tan-h 函数优于 sigmoid 函数。对于不同的层，激活函数可以不同。因此，在上述两层神经网络中，tan-h 函数可用于隐藏层，sigmoid 函数可用于输出层。</p><p id="ad7b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这两个函数的缺点是，当 z 非常小或非常大时，这些函数的斜率/导数变得最小，接近于零，这减缓了梯度下降。因此，另一种流行的选择是整流线性函数(relu)。它的公式是 a = max(0，z)。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lf"><img src="../Images/51b338d86f5c73c1e16fc88d84d7671d.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*Kc5jYKl-m7qHtTyFt3p8Xg.png"/></div></figure><p id="1043" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">relu 函数的一个缺点是，当 z 为负时，它的导数为零。实际上，这种激活功能运行良好，但是存在另一种版本的 relu，称为 leaky relu。它的公式可以写成 a = max(0.01z，z)</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lg"><img src="../Images/aa72749bad02eea0aaabfddf3e41d069.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*xXylLFTW7kMgbB5KpQNfyw.png"/></div></figure><p id="bdcc" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">漏 relu 在实际中用的不多。relu 或 leaky relu 的优点是，对于 z 的大量空间，函数的斜率与 0 非常不同，因此神经网络将比 sigmoid 或 tan-h 函数学习得更快。</p><h2 id="8cbb" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">激活函数的导数</strong></h2><p id="d15d" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">当我们实现神经网络的反向传播时，我们需要计算激活函数的斜率或导数。</p><p id="4b3b" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果 sigmoid 函数是 g(z ),那么函数的斜率，</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lh"><img src="../Images/b4c293b8501a75e0a508f4e2d210a7c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*JfI3h9GmLcGj-d7v4Rs6UA.png"/></div></figure><p id="af3e" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果 g(z) = a，则斜率可以简化为 a *(1–a)。</p><p id="1a9c" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">对于 tan-h 函数，斜率为 1-a:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es li"><img src="../Images/9bbf57c3939685495ad82e6ec6491570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*O22YLpqWHOeXSA4FOxUorQ.png"/></div></figure><p id="9245" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">Relu 和泄漏 relu:</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/359ab900b785a1f1f6eb59c7f193d6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZk9lNRL1CVme1YPR34bzQ.png"/></div></div></figure><h2 id="6156" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">随机初始化</strong></h2><p id="84c8" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">在训练神经网络时，随机初始化权重是很重要的。对于逻辑回归，将 w 和 b 初始化为零是可以的，但是对于神经网络，将权重初始化为零是不行的。可以通过归纳构造一个证明，当权重被初始化为零时，所有的隐藏单元将计算相同的函数。因此，保持更多的隐藏单元将不会在神经网络中保持任何值，并且建议随机初始化权重并将它们乘以小的数，例如 0.01。为乘法保留一个小数字的目的是权重的值最终会更小，导致 z 的最小值。</p><h2 id="701b" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">深层 L 层神经网络</strong></h2><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lo"><img src="../Images/2d365fda77425b8a998f834afff92a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*3b-TAuKKDiYMXbNo6nQRvQ.png"/></div></figure><p id="92e9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">L =网络的层数，在上图中，层数为 4。计算图层时不考虑输入图层。</p><p id="3c87" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">n^[l]= l 层中的单元数量。因此，不同层中的单元数量如下。n^[1] = 5，n^[2] = 5，n^[3] = 3，n^[4] = 1。</p><p id="ed95" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">a^[l] =层 1 中的激活</p><p id="ca64" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">w^[l] =层 l 中的重量</p><p id="9a95" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">b^[l] =层 l 中的偏置参数</p><h2 id="925e" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated"><strong class="ak">深度网络中的前向传播</strong></h2><p id="6b52" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated"><strong class="iy hi">单个训练示例的通用方程</strong></p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lp"><img src="../Images/73e020c7b8d294aec90ccdeff58cec50.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*uJo7VXBYRwrWRXcq_DNvGg.png"/></div></figure><p id="52d9" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">使用矢量化的 m 个训练示例的通用方程。</strong></p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lq"><img src="../Images/a3025df47c2d8c1a9ddcfa5c520c52db.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*i8BrqbS-Q5hntJJPOEmTbQ.png"/></div></figure><h2 id="0d4c" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">深度神经网络为什么会起作用？</h2><p id="62d8" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">我们都不断听说深度神经网络对几组问题非常有效，但其背后的原因是什么？首先，我们来了解一下什么是深度计算？如果我们正在构建一个人脸检测算法，那么第一层可能会进行特征或边缘检测。下一层可以检测一起形成面部部分的边缘组。最后，通过将面部的各个部分放在一起，它可以尝试识别不同类型的面部。这种从简单到复杂的分层表示或组合表示适用于除图像和人脸识别之外的其他类型的数据。</p><h2 id="f990" class="jt ju hh bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">参数和超参数</h2><p id="8124" class="pw-post-body-paragraph iw ix hh iy b iz ko ii jb jc kp il je jf kq jh ji jj kr jl jm jn ks jp jq jr ha bi translated">在神经网络中，W 和 B 被认为是参数，但是我们需要告诉我们的学习算法其他东西，例如学习速率α、迭代次数、隐藏单元数、隐藏层数、激活函数类型等。这些参数最终控制 W 和 b 的值，因此它们被称为超参数。在深度学习中，决定超参数的值是一个经验过程，我们尝试几个值并观察损失函数。</p><p id="3fb6" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">有了这篇文章，我已经完成了深度学习专业化的第一门课程。敬请关注下一节课的笔记！！</p></div></div>    
</body>
</html>