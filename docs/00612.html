<html>
<head>
<title>Hyperparameters Optimization for LightGBM, CatBoost and XGBoost Regressors using Bayesian Optimization.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用贝叶斯优化对LightGBM、CatBoost和XGBoost回归器进行超参数优化。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9?source=collection_archive---------0-----------------------#2019-08-16">https://medium.com/analytics-vidhya/hyperparameters-optimization-for-lightgbm-catboost-and-xgboost-regressors-using-bayesian-6e7c495947a9?source=collection_archive---------0-----------------------#2019-08-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/8cfe9f07f799c8b74988f9d66ddfd7f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ReEApCc0X8HQP8vyHoJTyA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">日本东京一条繁忙的街道。在这里找到说英语的人，就像找到助推算法的最佳超参数一样困难。</strong>😉</figcaption></figure><div class=""/><div class=""><h2 id="fbbf" class="pw-subtitle-paragraph iv hx hy bd b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm dx translated">如何用贝叶斯优化来优化boosting机器学习算法的超参数？</h2></div><p id="790c" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi kj translated"><span class="l kk kl km bm kn ko kp kq kr di"> B </span> oosting机器学习算法被高度使用，因为它们比简单的算法提供了更好的准确性。这些算法的性能取决于超参数。一组最佳的参数有助于获得更高的精度。手动寻找超参数是乏味且计算昂贵的。因此，超参数调整的自动化非常重要。RandomSearch、GridSearchCV和贝叶斯优化通常用于优化超参数。<a class="ae ks" href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a>与其他方法相比，能给出更好更快的结果。</p><p id="5769" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">贝叶斯优化是如何工作的？</strong></p><ol class=""><li id="3df4" class="kt ku hy jp b jq jr jt ju jw kv ka kw ke kx ki ky kz la lb bi translated">建立目标函数的代理概率模型</li><li id="d3fd" class="kt ku hy jp b jq lc jt ld jw le ka lf ke lg ki ky kz la lb bi translated">找到在代理上表现最好的超参数</li><li id="e375" class="kt ku hy jp b jq lc jt ld jw le ka lf ke lg ki ky kz la lb bi translated">将这些超参数应用于真正的目标函数</li><li id="a9e9" class="kt ku hy jp b jq lc jt ld jw le ka lf ke lg ki ky kz la lb bi translated">更新包含新结果的代理模型</li><li id="d289" class="kt ku hy jp b jq lc jt ld jw le ka lf ke lg ki ky kz la lb bi translated">重复步骤2-4，直到达到最大迭代次数或时间</li></ol><p id="89e1" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">贝叶斯优化器建立给定目标函数的概率模型，并使用它来选择最有希望的超参数，以在真实目标函数中进行评估。如果你想深入研究，那么就在这里阅读<a class="ae ks" href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jp hz"/></a>和<a class="ae ks" href="https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f" rel="noopener" target="_blank"> <strong class="jp hz">这里阅读</strong> </a>。</p><p id="3544" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">如何用贝叶斯优化优化超参数？</strong></p><p id="fdf2" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我将使用<a class="ae ks" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a> python包来演示基于贝叶斯模型的优化的应用。通过pip安装贝叶斯优化python包。</p><p id="a07f" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><code class="du lh li lj lk b">pip install bayesian-optimization</code></p><p id="55cc" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">超参数优化过程可以分为三个部分。</p><p id="8a61" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">第1部分—定义目标函数</strong></p><p id="8c32" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">定义一个目标函数，将超参数作为输入，给出一个最大化或最小化的分数作为输出。</p><p id="b210" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">第2部分—定义超参数的搜索空间</strong></p><p id="b71c" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">定义要优化的超参数范围。保持较窄的参数范围以获得更好的结果。</p><p id="f2ca" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">第3部分——定义目标函数的代理模型并调用它。</strong></p><p id="5a30" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">做一个贝叶斯优化函数，调用它使目标产出最大化。贝叶斯优化函数有三个输入:<code class="du lh li lj lk b">Objective Function</code>、<code class="du lh li lj lk b">Search Space</code>和<code class="du lh li lj lk b">random_state</code>。</p><p id="d292" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">让我们实现贝叶斯优化，以提高机器学习算法的回归目的。</p><p id="517f" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">导入库并加载数据。</strong></p><p id="a460" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我将在本教程中使用<a class="ae ks" href="https://www.kaggle.com/c/boston-housing/data" rel="noopener ugc nofollow" target="_blank">波士顿住房</a>数据。</p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><figure class="ll lm ln lo fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/a1e4e2d13f39e45937bbc87a01182dfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oJkq9eJOicpWhtek_pC1SQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">波士顿住房数据的前5行。</figcaption></figure><p id="95c2" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们的数据有13个预测变量(自变量)，价格作为<strong class="jp hz"> </strong>准则变量(因变量)。</p><h2 id="3e5f" class="ls lt hy bd hv lu lv lw lx ly lz ma mb jw mc md me ka mf mg mh ke mi mj mk ml bi translated"><strong class="ak"> 1。LightGBM回归器</strong></h2><p id="df7c" class="pw-post-body-paragraph jn jo hy jp b jq mm iz js jt mn jc jv jw mo jy jz ka mp kc kd ke mq kg kh ki hb bi translated"><strong class="jp hz"> a .目标函数</strong></p><p id="8f27" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">目标函数将返回负的<code class="du lh li lj lk b">l1</code>(绝对损失，别名= <code class="du lh li lj lk b">mean_absolute_error</code>，<code class="du lh li lj lk b">mae</code>)。目标是优化目标函数的输出。也可以用<code class="du lh li lj lk b">l2</code>、<code class="du lh li lj lk b">l2_root</code>、<code class="du lh li lj lk b">poisson </code>代替<code class="du lh li lj lk b">l1</code>。</p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="d6c2" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">注:</strong></p><blockquote class="mr"><p id="ff57" class="ms mt hy bd mu mv mw mx my mz na ki dx translated">LightGBM和XGBoost没有R平方度量。如果你想用R2指标代替其他评估指标，那么就写你自己的R2指标。</p></blockquote><p id="b033" class="pw-post-body-paragraph jn jo hy jp b jq nb iz js jt nc jc jv jw nd jy jz ka ne kc kd ke nf kg kh ki hb bi translated"><strong class="jp hz">参见R2指标的目标函数示例。</strong></p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="fa6f" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz"> b .搜索空间</strong></p><p id="7a7f" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">定义目标函数输入参数的范围。您可以根据想要优化的超参数的数量来定义输入参数的数量。这个例子有6个超参数。</p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="a242" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz"> c .代理模型和优化</strong></p><p id="6b74" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">定义一个贝叶斯优化函数，最大化目标函数的输出。<code class="du lh li lj lk b">init_points</code>和<code class="du lh li lj lk b">n_iter</code>之和等于优化轮次总数。</p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="d890" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz"> <em class="ng">把所有的放在一个函数里。</em> </strong></p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="5c66" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">上述代码的输出将是表，该表将目标函数的输出作为目标，并将输入参数的值作为目标函数。使用<code class="du lh li lj lk b">obtimizer.max['params']</code>获得最佳参数。</p><figure class="ll lm ln lo fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nh"><img src="../Images/0042a69719ff2c1f89d6118d93475a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*71gBxXRp43JfUdrD56oVYw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">LightGBM回归器的超参数优化结果表</figcaption></figure><h2 id="9cc3" class="ls lt hy bd hv lu lv lw lx ly lz ma mb jw mc md me ka mf mg mh ke mi mj mk ml bi translated"><strong class="ak"> 2。Catboost回归器</strong></h2><p id="2fd6" class="pw-post-body-paragraph jn jo hy jp b jq mm iz js jt mn jc jv jw mo jy jz ka mp kc kd ke mq kg kh ki hb bi translated"><strong class="jp hz"> a .目标函数</strong></p><p id="be9f" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">目标函数有两个输入:<code class="du lh li lj lk b">depth</code>和<code class="du lh li lj lk b">bagging_temperature</code>。目标函数将返回测试的最大平均R平方值。</p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="6030" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz"> b .搜索空间</strong></p><p id="9bc9" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">目标函数只有两个输入参数，因此搜索空间也只有两个参数。</p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="efcd" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">c .</strong>T23】代理模型与优化</p><p id="dfc9" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">贝叶斯优化器将优化<code class="du lh li lj lk b">depth</code>和<code class="du lh li lj lk b">bagging_temperature</code>，以优化<code class="du lh li lj lk b">R2</code>值。</p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><figure class="ll lm ln lo fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ni"><img src="../Images/a17d7bbfcf328542f492664e8913ddc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AP40EJGMMifDHjr3HzeGzw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">CatBoost回归器的超参数优化结果表</figcaption></figure><h2 id="a9ff" class="ls lt hy bd hv lu lv lw lx ly lz ma mb jw mc md me ka mf mg mh ke mi mj mk ml bi translated"><strong class="ak"> 3。XGBoost回归器</strong></h2><p id="54f3" class="pw-post-body-paragraph jn jo hy jp b jq mm iz js jt mn jc jv jw mo jy jz ka mp kc kd ke mq kg kh ki hb bi translated"><strong class="jp hz"> a .目标函数</strong></p><p id="268e" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">目标函数给出输入参数<code class="du lh li lj lk b">r2</code>的最大值。</p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="3025" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz">注:</strong></p><ol class=""><li id="28a4" class="kt ku hy jp b jq jr jt ju jw kv ka kw ke kx ki ky kz la lb bi translated">如果<code class="du lh li lj lk b">eval_metric</code>包含在参数中，则使用<code class="du lh li lj lk b">early_stopping_rounds</code>较小的数字(10或更小)。为什么？因为如果给定的早期停止轮次的评估指标值没有提高，那么训练将会停止。</li><li id="882b" class="kt ku hy jp b jq lc jt ld jw le ka lf ke lg ki ky kz la lb bi translated">如果参数中没有定义<code class="du lh li lj lk b">eval_metric</code>，则使用比<code class="du lh li lj lk b">num_boost_rounds</code>大的数字<code class="du lh li lj lk b">early_stopping_rounds</code>。为什么？因为训练将在给定的提前停止回合停止。</li></ol><p id="ab01" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz"> b .搜索空间</strong></p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><p id="2c5d" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hz"> c .代理模型和优化</strong></p><figure class="ll lm ln lo fd hk"><div class="bz dy l di"><div class="lp lq l"/></div></figure><figure class="ll lm ln lo fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nj"><img src="../Images/f9b4dcfd13b076fc5a24401c27aa8c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WQKcJv65czbpInajldjGHw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">XGBoost回归器的超参数优化结果表</figcaption></figure><p id="445d" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我希望，你已经学习了贝叶斯优化的超参数优化的整体概念。超参数调整现在看起来很容易。对吗？</p><h2 id="c4d5" class="ls lt hy bd hv lu lv lw lx ly lz ma mb jw mc md me ka mf mg mh ke mi mj mk ml bi translated"><strong class="ak">结论</strong></h2><p id="e75c" class="pw-post-body-paragraph jn jo hy jp b jq mm iz js jt mn jc jv jw mo jy jz ka mp kc kd ke mq kg kh ki hb bi translated">使用<code class="du lh li lj lk b">Bayesian Optimization</code>很容易优化超参数。LightGBM和XGBoost没有<code class="du lh li lj lk b">r2</code>度量，因此我们应该定义自己的<code class="du lh li lj lk b">r2 metric</code>。<strong class="jp hz">light GBM和XGBoost的</strong> <code class="du lh li lj lk b"><strong class="jp hz">r2</strong></code> <strong class="jp hz">度量差别不大。LightGBM R2度量应该返回3个输出，而XGBoost R2度量应该返回2个输出。</strong></p><p id="78e8" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们可以根据模型需求使用不同的评估标准。保持搜索空间参数范围较窄，以获得更好的结果。<code class="du lh li lj lk b">bayesian-optimization</code>最大化目标函数的输出，因此<code class="du lh li lj lk b">l1</code> &amp; <code class="du lh li lj lk b">l2</code>输出必须为负，<code class="du lh li lj lk b">r2</code>输出必须为正。</p><p id="ceff" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在Github  或<a class="ae ks" href="https://colab.research.google.com/drive/1gWwUa3ASZLJTijy3JoJl_nEqHlwHVIeB" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hz"> Colab笔记本</strong> </a>上查看<a class="ae ks" href="https://github.com/dc-aichara/DS-ML-Public/blob/master/Medium_Files/hyp_tune.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hz">笔记本，查看用例。如有任何疑问，请通过<a class="ae ks" href="https://www.linkedin.com/in/dcaichara/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hz"> LinkedIn </strong> </a>联系我。参数调的开心！感谢您的阅读..！☺️</strong></a></p><h2 id="3ff9" class="ls lt hy bd hv lu lv lw lx ly lz ma mb jw mc md me ka mf mg mh ke mi mj mk ml bi translated"><strong class="ak">参考文献:</strong></h2><ol class=""><li id="0226" class="kt ku hy jp b jq mm jt mn jw nk ka nl ke nm ki ky kz la lb bi translated"><a class="ae ks" href="https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f" rel="noopener" target="_blank">https://towards data science . com/a-基于贝叶斯模型的概念解释-机器学习的超参数优化-b8172278050f </a></li><li id="9aec" class="kt ku hy jp b jq lc jt ld jw le ka lf ke lg ki ky kz la lb bi translated"><a class="ae ks" href="https://towardsdatascience.com/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0" rel="noopener" target="_blank">https://towards data science . com/an-introductive-example-of-Bayesian-optimization-in-python-with-hyperpt-aae 40 fff 4 ff</a>o</li><li id="8e09" class="kt ku hy jp b jq lc jt ld jw le ka lf ke lg ki ky kz la lb bi translated"><a class="ae ks" rel="noopener" href="/spikelab/hyperparameter-optimization-using-bayesian-optimization-f1f393dcd36d">https://medium . com/spike lab/hyperparameter-optimization-using-Bayesian-optimization-f1 f 393 CD 36d</a></li><li id="dc10" class="kt ku hy jp b jq lc jt ld jw le ka lf ke lg ki ky kz la lb bi translated"><a class="ae ks" href="https://www.kaggle.com/omarito/xgboost-bayesianoptimization" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/omarito/xgboost-bayesianoptimization</a></li><li id="f3bb" class="kt ku hy jp b jq lc jt ld jw le ka lf ke lg ki ky kz la lb bi translated"><a class="ae ks" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">https://github.com/fmfn/BayesianOptimization</a></li></ol></div></div>    
</body>
</html>