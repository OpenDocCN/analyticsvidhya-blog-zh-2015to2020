<html>
<head>
<title>Object detection: YOLO</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">物体探测:YOLO</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/object-detection-yolo-fc6647ddd11f?source=collection_archive---------7-----------------------#2020-06-06">https://medium.com/analytics-vidhya/object-detection-yolo-fc6647ddd11f?source=collection_archive---------7-----------------------#2020-06-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f208" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对象检测是一项计算机视觉任务，涉及识别给定图像中一个或多个对象的存在、位置和类型。有许多方法可用于物体检测。其中之一是R-CNN家族，即基于区域的卷积神经网络。这包含了用于物体检测和定位的技术<a class="ae jd" href="https://arxiv.org/abs/1311.2524" rel="noopener ugc nofollow" target="_blank"> R-CNN </a>、<a class="ae jd" href="https://arxiv.org/abs/1504.08083" rel="noopener ugc nofollow" target="_blank">快速R-CNN </a>、<a class="ae jd" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">更快R-CNN </a>和<a class="ae jd" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">屏蔽R-CNN </a>。所有这些模型都使用图像的区域来检测和定位对象。他们不看完整的图像，而是看图像中包含物体的概率很高的部分。同样任务的另一个模型是，YOLO(你只看一次)。</p><p id="8c8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">YOLO是一个单一的卷积网络，它查看完整的图像并预测边界框和这些框的类别概率。这将输入分割成单元网格，每个单元直接预测边界框和对象分类。这款车型有三个版本，<a class="ae jd" href="https://pjreddie.com/media/files/papers/yolo_1.pdf" rel="noopener ugc nofollow" target="_blank"> YOLO v1 </a>，<a class="ae jd" href="https://pjreddie.com/media/files/papers/YOLO9000.pdf" rel="noopener ugc nofollow" target="_blank"> YOLO v2 </a>，<a class="ae jd" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" rel="noopener ugc nofollow" target="_blank"> YOLO v3 </a>。在这里，我们将研究YOLO v3网络及其工作原理。</p><p id="1b1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">YOLO v3是暗网的一个变种，最初它由53个卷积层组成，每个卷积层之后是批量标准化层和泄漏ReLu激活。对于对象检测的任务，在其上堆叠了另外53层，为我们提供了YOLO v3的106层完全卷积架构(<strong class="ih hj">图1 </strong>)。为了更好地理解完整的网络架构，请参考<a class="ae jd" href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b" rel="noopener" target="_blank">本</a>。网络通过网络的步长因子对图像进行下采样。通常，网络中一个层的跨距等于该层的输出小于网络的输入图像的因子。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/2b2657dc55759fa959ffd8c5830590e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*KLxQd4kGufbS72MKZhkVAw.jpeg"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">YOLO v3架构。图片取自这篇<a class="ae jd" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。为了更好地理解建筑，请跟随。</figcaption></figure><p id="e7c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">网络的输入是一批图像，卷积层学习的特征被传递到分类器/回归器，该分类器/回归器进行检测预测。输出是一个包含已识别类的边界框列表。每个包围盒由6个数字<strong class="ih hj"> ( <em class="jq"> Pc，bx，by，bh，bw，c </em> ) </strong>表示。<strong class="ih hj"> <em class="jq"> Pc </em> </strong>是属于特定类的概率，<strong class="ih hj"> <em class="jq"> bx </em> </strong>和<strong class="ih hj"> <em class="jq"> by </em> </strong>是包围盒的中心，<strong class="ih hj"> <em class="jq"> bh </em> </strong>和<strong class="ih hj"> <em class="jq"> bw </em> </strong>分别是包围盒的高度和宽度，<strong class="ih hj"> <em class="jq"> c </em> </strong>是可用的总数</p><p id="6226" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解释这些特征图的预测并找出边界框属于哪个单元，我们将输入图像划分成与最终特征图的维数相等的网格。例如，如果输入图像是416×416个单元，跨距是32，那么特征图将是13×13个单元。然后，我们将输入图像分成13×13个单元。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jr"><img src="../Images/e4a660ae6dda74c6e897ddbb07f04d1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*nz0ttVUi2NAv0Ur8pvPvZg.jpeg"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">图片取自<a class="ae jd" href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b" rel="noopener" target="_blank">此处</a></figcaption></figure><p id="8b8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">包含对象的基本事实框的中心的单元被选择为负责预测对象的单元(标记为红色)。现在，特征图上对应的细胞被认为是负责检测狗的细胞。由于YOLO v3具有三个锚，该单元现在产生三个边界框(锚是边界框先验，其使用k均值聚类来计算。更多信息请遵循<a class="ae jd" rel="noopener" href="/@medhijoydeep/anchor-boxes-in-faster-rcnn-6bd566ec4935">和</a>。为了更好地理解这一点，参见图2 。</p><blockquote class="js jt ju"><p id="e725" class="if ig jq ih b ii ij ik il im in io ip jv ir is it jw iv iw ix jx iz ja jb jc hb bi translated">注意:我们将输入图像划分为网格，只是为了确定预测特征图的哪个单元负责预测。</p></blockquote><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jy"><img src="../Images/7364de202f68cedbf7543a47b5042856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*cgUmQqoEhFer5mVzh1uzeg.jpeg"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">图二。图片取自<a class="ae jd" href="https://machinelearningspace.com/yolov3-tensorflow-2-part-1/" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="42e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">边界框的尺寸是通过对输出应用对数空间变换，然后乘以锚点来计算的，如下所示。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jz"><img src="../Images/cd5a7d1205ffa76b3fc8b208cf486faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*AFDHen7nBmu0TvFwUz8neA.jpeg"/></div></figure><p id="a761" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，<em class="jq"> bx，by，bw，bh </em>是我们预测的<em class="jq"> x，y </em>中心坐标，宽度和高度。<em class="jq"> tx，ty，tw，th </em>为网络输出。<em class="jq"> cx，cy </em>是网格的左上角坐标。<em class="jq"> pw，ph </em>是盒子的锚尺寸。如需详细了解，请查看下图。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ka"><img src="../Images/f5215f6a42c74376002cb485f442b1bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*_LJHrC0sm1cCdj8tP8oeDg.jpeg"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">图片取自<a class="ae jd" href="https://pylessons.com/YOLOv3-introduction/" rel="noopener ugc nofollow" target="_blank">此处</a>。</figcaption></figure><p id="a31b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，预测值<em class="jq"> bw </em>和<em class="jq"> bh </em>通过图像的高度和宽度进行归一化。</p><p id="ad92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，为了检查这三个边界框中的哪一个被分配给地面真实图像中的对象，我们为这些框中的每一个计算元素的乘积，并提取该类包含特定类的概率，如下所示。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kb"><img src="../Images/8db1b74493b59a09b0933b3a88705d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*4v2gy2hO1ky6mUDeCNSThg.jpeg"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">该盒子检测到c=3(汽车),概率得分为0.44。图片取自<a class="ae jd" href="https://pylessons.com/YOLOv3-introduction/" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="425a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">YOLO v3在具有步长31、16和8的不同标度上进行预测。在每个比例下，每个单元使用3个锚点预测3个边界框，使得使用的锚点总数为9。由于这个原因，我们仍然有许多被高概率选择的边界框。(例如:如果图像是416 x 415，那么YOLO预测，((52x 52)+(26x 26)+(13x 13))x 3 = 10647个边界框)。为了将输出减少到更少数量的检测对象，我们使用下面的方法。</p><p id="4cfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们过滤分数低于某个阈值(0.5)的盒子。接下来，使用非最大抑制(NMS ),去除图像中同一物体的多次检测。例如，一个单元的所有三个框可能检测到一个对象，并且相邻的单元也可能检测到相同的对象，在这种情况下，我们使用NMS来移除这些多个边界框。这个NMS使用<a class="ae jd" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">交集超过</strong> </a>。移除多个边界框的步骤是:</p><ul class=""><li id="3626" class="kc kd hi ih b ii ij im in iq ke iu kf iy kg jc kh ki kj kk bi translated">选择得分最高的方框。</li><li id="b99c" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated">计算其与其他框的重叠，并移除与其重叠超过联合阈值的框。</li><li id="557f" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated">返回到步骤1，重复直到没有比当前选择的盒子分数更低的盒子。</li></ul><p id="4f70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在此之后，任何与所选框重叠的框都将被移除，只保留所需的框。</p><h1 id="acc1" class="kq kr hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">实施:</h1><p id="706a" class="pw-post-body-paragraph if ig hi ih b ii lo ik il im lp io ip iq lq is it iu lr iw ix iy ls ja jb jc hb bi translated">现在，我将向您展示YOLO v3在keras中的实现。我使用了预先训练的YOLO模型来预测和解释预测的边界框。<a class="ae jd" href="https://raw.githubusercontent.com/experiencor/keras-yolo3/master/yolo3_one_file_to_detect_them_all.py" rel="noopener ugc nofollow" target="_blank">该</a>代码使用预先训练的权重来准备模型，并使用该模型来执行对象检测。我们不直接使用它，而是使用这个程序的元素，准备我们自己的脚本并保存keras模型。</p><p id="247e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://pjreddie.com/media/files/yolov3.weights" rel="noopener ugc nofollow" target="_blank">在这里</a>你可以找到这个预训练模型的权重。然后，我将这些权重加载到keras模型并保存它(代码大量借用了<a class="ae jd" href="https://github.com/jbrownlee/keras-yolo3" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">暗网代码</strong> </a>)</p><pre class="jf jg jh ji fd lt lu lv lw aw lx bi"><span id="9a21" class="ly kr hi lu b fi lz ma l mb mc">model = make_yolov3_model()<br/>weight_reader = WeightReader('yolov3.weights')<br/>weight_reader.load_weights(model)<br/>model.save('model.h5')</span></pre><p id="773f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型预测被编码的边界框。这些预测可以通过下面的函数解码。</p><pre class="jf jg jh ji fd lt lu lv lw aw lx bi"><span id="b7bf" class="ly kr hi lu b fi lz ma l mb mc">def decode_netout(netout, anchors, obj_thresh, net_h, net_w):<br/>  grid_h, grid_w = netout.shape[:2]<br/>  nb_box = 3<br/>  netout = netout.reshape((grid_h, grid_w, nb_box, -1))<br/>  nb_class = netout.shape[-1] - 5<br/>  boxes = []<br/>  netout[..., :2]  = _sigmoid(netout[..., :2])<br/>  netout[..., 4:]  = _sigmoid(netout[..., 4:])<br/>  netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]<br/>  netout[..., 5:] *= netout[..., 5:] &gt; obj_thresh<br/>  <br/>  for i in range(grid_h*grid_w):<br/>    row = i / grid_w<br/>    col = i % grid_w<br/>    for b in range(nb_box):<br/>      # 4th element is objectness score<br/>      objectness = netout[int(row)][int(col)][b][4]<br/>      if(objectness.all() &lt;= obj_thresh): continue<br/>      # first 4 elements are x, y, w, and h<br/>      x, y, w, h = netout[int(row)][int(col)][b][:4]<br/>      x = (col + x) / grid_w # center position, unit: image width<br/>      y = (row + y) / grid_h # center position, unit: image height<br/>      w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width<br/>      h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height<br/>      # last elements are class probabilities<br/>      classes = netout[int(row)][col][b][5:]<br/>      box = BoundBox(x-w/2, y-h/2, x+w/2, y+h/2, objectness, classes)<br/>      boxes.append(box)<br/>  return boxes</span></pre><p id="fdfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用非最大抑制移除额外的边界框，如下所示。</p><pre class="jf jg jh ji fd lt lu lv lw aw lx bi"><span id="e34d" class="ly kr hi lu b fi lz ma l mb mc">def _interval_overlap(interval_a, interval_b):<br/>  x1, x2 = interval_a<br/>  x3, x4 = interval_b<br/>  if x3 &lt; x1:<br/>    if x4 &lt; x1:<br/>      return 0<br/>    else:<br/>      return min(x2,x4) - x1<br/>  else:<br/>  if x2 &lt; x3:<br/>    return 0<br/>  else:<br/>    return min(x2,x4) - x3</span><span id="4e88" class="ly kr hi lu b fi md ma l mb mc">def bbox_iou(box1, box2):<br/>  intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])<br/>  intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])<br/>  intersect = intersect_w * intersect_h</span><span id="a2bd" class="ly kr hi lu b fi md ma l mb mc">  w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin<br/>  w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin<br/>  union = w1*h1 + w2*h2 - intersect<br/>  return float(intersect) / union</span><span id="96cd" class="ly kr hi lu b fi md ma l mb mc">def do_nms(boxes, nms_thresh):<br/>  if len(boxes) &gt; 0:<br/>    nb_class = len(boxes[0].classes)<br/>  else:<br/>    return<br/>  for c in range(nb_class):<br/>    sorted_indices = np.argsort([-box.classes[c] for box in boxes])<br/>    for i in range(len(sorted_indices)):<br/>    index_i = sorted_indices[i]<br/>    if boxes[index_i].classes[c] == 0: continue<br/>    for j in range(i+1, len(sorted_indices)):<br/>      index_j = sorted_indices[j]<br/>      if bbox_iou(boxes[index_i], boxes[index_j]) &gt;= nms_thresh:<br/>        boxes[index_j].classes[c] = 0</span></pre><p id="7529" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对单个图像运行此命令会产生以下输出。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es me"><img src="../Images/51e2f58900c8e074703ce3f97ffc9089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sj0-L6rVuhpJ-M2ZdEcUdg.jpeg"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mj"><img src="../Images/3f5502622fe0c7a19f89318ab5dc1ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*O1eUCMK3BfROnEemD8daqg.jpeg"/></div></figure><p id="b4ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型可用于您的自定义数据集。详细描述请参考<a class="ae jd" href="https://machinelearningmastery.com/how-to-train-an-object-detection-model-with-keras/" rel="noopener ugc nofollow" target="_blank">本</a>。</p><p id="63d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://github.com/kulkarnikeerti/YOLO-v3-for-object-detection" rel="noopener ugc nofollow" target="_blank">这里的</a>，是完整源代码的链接。</p><h1 id="11bf" class="kq kr hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">参考资料:</h1><ul class=""><li id="9be9" class="kc kd hi ih b ii lo im lp iq mk iu ml iy mm jc kh ki kj kk bi translated"><a class="ae jd" href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b" rel="noopener" target="_blank">https://towards data science . com/yolo-v3-object-detection-53 FB 7d 3 bfe 6 b</a></li><li id="884f" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated"><a class="ae jd" href="https://pylessons.com/YOLOv3-introduction/" rel="noopener ugc nofollow" target="_blank">https://pylessons.com/YOLOv3-introduction/</a></li><li id="d61b" class="kc kd hi ih b ii kl im km iq kn iu ko iy kp jc kh ki kj kk bi translated"><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2018/12/practical-guide-object-detection-yolo-framewor-python/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/12/practical-guide-object-detection-yolo-frame wor-python/</a></li></ul></div></div>    
</body>
</html>