<html>
<head>
<title>Every thing you need to know about LSTMs in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于喀拉斯的LSTMs，你需要知道的一切</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/every-thing-you-need-to-know-about-lstms-in-keras-6f1e95a4d114?source=collection_archive---------20-----------------------#2020-09-18">https://medium.com/analytics-vidhya/every-thing-you-need-to-know-about-lstms-in-keras-6f1e95a4d114?source=collection_archive---------20-----------------------#2020-09-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="0280" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="95a7" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">LSTMs存在了很长时间，但由于其在NLP中的工作而变得流行，即Seq2Seq模型导致了神经机器翻译(例如:语言翻译器和聊天机器人)。我在理解LSTMs时的挣扎(感谢Keras对文档中一些术语的糟糕解释)促使我为那些正在经历类似阶段的人写这篇博客@_@。我向你保证这不会很难！</p><p id="622e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">此外，这个博客还与LSTMs的工作方式有关。</p><h1 id="d8ea" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结构</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/cace770db820b756504598221c2c97d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5W8FrASMi93Z81NlAui4w.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">鸣谢:科拉在LSTM的博客。</figcaption></figure><p id="bf06" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">首先要注意的是，与其他神经网络不同，LSTM以时间步长接收输入，每个时间步长都有一个输出，即Ht-1，Ht，Ht+1，这就是所谓的隐藏状态，我们通常将最后一个隐藏状态称为<strong class="jf hj">输出。</strong></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/6f47c41e0be84929ec01914bce8fbf7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w8Bgd_Gk5DasRC4GI6woTg.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">隐藏状态</figcaption></figure><p id="effa" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">还有一点需要注意的是单元状态，在每个时间步都有一个单元状态<strong class="jf hj">。</strong>看下图就知道什么是细胞状态了。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/8ab9c6353657e5863e51fa6b9d2d1c3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gw8I2P_wxF_2NQyMRztQug.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">细胞状态Ct。</figcaption></figure><p id="27b7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">为了了解数学和更多关于为什么使用这些单元状态和隐藏状态的信息，我建议你去浏览我在最后引用的链接。</p><h1 id="e1ad" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">在Keras实施</h1><h2 id="f37a" class="kx ig hi bd ih ky kz la il lb lc ld ip jo le lf it js lg lh ix jw li lj jb lk bi translated">输入应始终为形状(批量大小、时间步长、要素)</h2><pre class="kh ki kj kk fd ll lm ln lo aw lp bi"><span id="8b80" class="kx ig hi lm b fi lq lr l ls lt">input = Input(shape=(time_steps,features))<br/>lstm = LSTM(units=10)<br/>output = lstm(input)</span></pre><p id="ed2c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">关于“单位”的论点有一个普遍的误解(甚至我一开始也有)，大多数人一开始认为单位是指我们LSTM中细胞的数量，但事实并非如此。</p><p id="c769" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">参数“units”实际上决定了LSTM的输出形状，这意味着所有隐藏状态和单元格状态的形状都是(units，)。</p><p id="0bf8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">lstm单元的数量由我们在输入层给出的输入形状自动决定。</p><p id="0784" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">数据流:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lu"><img src="../Images/4aaef6293c797afeb3ab183a7fc425b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K5qci0yJDz466mswq9iHlQ.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">不同位置的数据形状</figcaption></figure><p id="6ffe" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="lv"> Ht-1 </em>的形状为<strong class="jf hj">(单位，)</strong>，我们将它作为参数<strong class="jf hj">‘单位’</strong>输入。</p><p id="ca16" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="lv"> Xt </em>的形状为<strong class="jf hj">(数量特征，)</strong>，<em class="lv"> Ht-1和Xt </em>串联，新形状变为<strong class="jf hj">(单位+数量特征，)</strong>。</p><p id="1201" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">所有权重W1、W2、W3、W4的形状将为<strong class="jf hj">(单位，单位+数量_特征)</strong>。</p><p id="3339" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">W5的形状= <strong class="jf hj">(单位，单位)</strong></p><p id="c26e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">并且偏置B1、B2、B3、B4、B5将是形状<strong class="jf hj">(单位，)。</strong></p><p id="e3c7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">矩阵乘法后:W *[特征数量，单位]</p><p id="715e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">(单位，单位+数量_特征)*(单位+数量_特征)，</strong></p><p id="cb1f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">形状=(单位，)</p><blockquote class="lw lx ly"><p id="5e56" class="jd je lv jf b jg kb ji jj jk kc jm jn lz kd jq jr ma ke ju jv mb kf jy jz ka hb bi translated"><strong class="jf hj">在最后一步</strong>我们只会得到最终的输出，即隐藏状态</p></blockquote><h2 id="1231" class="kx ig hi bd ih ky kz la il lb lc ld ip jo le lf it js lg lh ix jw li lj jb lk bi translated">2.返回序列</h2><p id="dccc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当参数<strong class="jf hj"> return_sequences </strong>设置为真时，lstm将输出每个时间步的隐藏状态列表，而不是最后一个时间步的隐藏状态。默认设置为<strong class="jf hj">假</strong>。</p><pre class="kh ki kj kk fd ll lm ln lo aw lp bi"><span id="a64c" class="kx ig hi lm b fi lq lr l ls lt">input = Input(shape=(time_steps,features))<br/>lstm = LSTM(units=10,return_sequences=True)<br/>output = lstm(input)</span></pre><p id="5c6c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">输出形状将为(时间步长，单位)</p><h2 id="05bc" class="kx ig hi bd ih ky kz la il lb lc ld ip jo le lf it js lg lh ix jw li lj jb lk bi translated">3.返回状态</h2><p id="8286" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当return_state在参数中设置为True时，lstm将输出最后一个时间步长的hidden_state和cell_state。不幸的是，我不知道有什么方法可以让我们在每个时间步长得到cell_state。</p><p id="9698" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我打赌你知道两者的形状。-).</p><pre class="kh ki kj kk fd ll lm ln lo aw lp bi"><span id="f463" class="kx ig hi lm b fi lq lr l ls lt">input = Input(shape=(time_steps,features))<br/>lstm = LSTM(units=10,return_state=True)<br/>hidden_state,cell_state = lstm(input)</span></pre><h2 id="0715" class="kx ig hi bd ih ky kz la il lb lc ld ip jo le lf it js lg lh ix jw li lj jb lk bi translated">4.返回状态，返回序列</h2><pre class="kh ki kj kk fd ll lm ln lo aw lp bi"><span id="cd81" class="kx ig hi lm b fi lq lr l ls lt">input = Input(shape=(time_steps,features))<br/>lstm = LSTM(units=10,return_state=True)<br/>all_hidden_states,hidden_state,cell_state = lstm(input)</span></pre></div><div class="ab cl mc md gp me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="hb hc hd he hf"><p id="bb69" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">参考资料:</p><p id="a453" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><a class="ae mj" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p></div></div>    
</body>
</html>