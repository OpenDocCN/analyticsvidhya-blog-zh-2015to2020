<html>
<head>
<title>Word Embedding : Methods to generate them from scratch in Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入:在Pytorch中从头开始生成单词的方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/word-embedding-methods-to-generate-them-usage-in-financial-markets-and-experiments-on-twitter-63fae8a5ddd2?source=collection_archive---------6-----------------------#2019-09-27">https://medium.com/analytics-vidhya/word-embedding-methods-to-generate-them-usage-in-financial-markets-and-experiments-on-twitter-63fae8a5ddd2?source=collection_archive---------6-----------------------#2019-09-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a0d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在大型文本语料库中，相关的单词(同义词等)很有可能出现在一组相似的单词周围，例如，1“国王”规则和“女王”规则。这里国王和王后虽然不能直接替换，但似乎有关系，即都指统治者。这个关于人类生成文本的假设被称为<em class="jd">分布假设</em>。</p><p id="0f29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在这就引出了一个问题，你如何用数字表示这种关系和语义相似性？回车，单词嵌入。单词嵌入是单词的<em class="jd">密集</em>向量表示，使得在相似上下文中出现的单词具有更接近的向量表示。整个向量空间是潜在语义的表示。这些单词嵌入进一步有助于带出文档和句子的含义，这为各种下游任务创造了空间，如情绪分析等，然后可用于证实市场趋势。</p><p id="5b1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有多种方式可以生成单词嵌入。传统的方法被称为<em class="jd">基于频率的方法</em>。它们包括像TF-IDF，单词袋等方法，新的方法使用深度学习。基于预测的深度学习模型在提供信息方面要强大得多，因此让我们详细探索它们。通过深度学习的两个流行和普遍存在的模型是跳字模型和连续单词袋模型。让我们使用<strong class="ih hj"> Pytorch </strong>从头开始实现这两者。但在此之前，先简要介绍一下这两种预测模型。</p><h2 id="3f42" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">跳过克模型</strong></h2><p id="c246" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">顾名思义，skip gram模型针对给定的单词(被跳过的)使用函数逼近器(像神经网络一样)来预测其<em class="jd">上下文。</em>给定单词的<em class="jd">上下文</em>是文本语料库中该单词前后的一组单词。多少单词在前面和后面是模型中必须优化的超参数。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es ke"><img src="../Images/994d8cc0e31f94bae5dbf6f0dbce36c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*3xy5IOpScN0aQwwfFbCmGQ.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">图1:用于预测上下文的输入单词</figcaption></figure><p id="9af3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在如图1所示，输入的数字表示(通常是一个<a class="ae kq" href="https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">热编码</a>)被放入模型，并根据<em class="jd">上下文向量</em>进行训练，上下文向量通常是一个<a class="ae kq" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> <em class="jd">单词包</em> </a>向量。</p><p id="64fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了使网络训练更健壮，基于对训练、测试和验证结果的经验观察，在架构上提出了一些改变。代替一个输入单词预测多个上下文单词，进行简化，使得我们将每个<strong class="ih hj"> <em class="jd">(目标，上下文单词)训练对</em> </strong>分解成多个<strong class="ih hj"> <em class="jd">(目标，上下文)训练对</em> </strong>，使得每个上下文由旧的一个单词组成。所以如果我们有一个句子，比如说“快速棕色狐狸跳过懒狗”我们的一些训练样本会是——<strong class="ih hj">(棕色，快速)，(狐狸，棕色)，(跳跃，狐狸)</strong>等等。</p><p id="3a89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，由于这种新的架构，另一个自然的变化是，我们可以根据数字标签来训练网络，以确定给定的<strong class="ih hj"> <em class="jd">(目标，上下文)训练对</em> </strong>是否是有效的上下文。这现在变成了一个上下文有效性分类问题。我们将有效和无效的<strong class="ih hj"> <em class="jd">(目标，上下文)训练对</em> </strong>传入我们的网络，以帮助它更好地学习细微差别。传入无效的、随机生成的上下文对被称为<strong class="ih hj">负采样</strong>。</p><p id="e559" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们训练样本的格式是<strong class="ih hj"> <em class="jd"> (X，Y) </em> </strong>其中<strong class="ih hj"> <em class="jd"> X </em> </strong>是我们的<strong class="ih hj"> <em class="jd">输入</em></strong><strong class="ih hj"><em class="jd">Y</em></strong>是我们的<strong class="ih hj"> <em class="jd">标签。</em> </strong>我们通过使用<strong class="ih hj"> <em class="jd"> [(目标，上下文)，1] </em> </strong>对作为<strong class="ih hj"> <em class="jd">实际输入样本</em> </strong>来实现这一点，其中<strong class="ih hj"> <em class="jd">目标</em> </strong>是我们感兴趣的单词，而<strong class="ih hj"> <em class="jd">上下文</em> </strong>是出现在目标单词附近的上下文单词，并且<strong class="ih hj"> <em class="jd">实际标签1 </em> </strong>指示这是上下文相关的对。我们还输入<strong class="ih hj"> <em class="jd"> [(目标，随机)，0] </em> </strong>对作为<strong class="ih hj"> <em class="jd">负输入样本</em> </strong>其中<strong class="ih hj"> <em class="jd">目标</em> </strong>再次是我们感兴趣的单词，但是<strong class="ih hj"> <em class="jd">随机</em> </strong>只是从我们的词汇表中随机选择的单词，与我们的目标单词没有上下文或关联。</p><p id="d20e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此<strong class="ih hj"> <em class="jd">否定标记0 </em> </strong>表示这是一个上下文不相关的对。我们这样做是为了让模型能够学习哪些词对是上下文相关的，哪些是不相关的，并为语义相似的词生成相似的嵌入。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es kr"><img src="../Images/233646c3e759b1584caf4ab97e4ecf74.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*7PoBAElNcTCCu46FDfCr8Q.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">图2:改进的Skip gram架构优于原来的架构。</figcaption></figure><h2 id="2738" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">连续单词袋模型</strong></h2><p id="b5e1" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">这个模型的工作方式是，当你给出上下文单词作为输入时，它预测一个单词的概率。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es ks"><img src="../Images/a66b917efd8a5d44e51b5cf4c1db9392.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*bkrBASpteKfCaxZDEEeN6g.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">图3:用于预测输入单词的上下文</figcaption></figure><p id="8ae1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CBOW模型架构试图基于源上下文单词(周围单词)来预测当前目标单词(中心单词)。考虑一个简单的句子，<strong class="ih hj"> <em class="jd">“敏捷的棕色狐狸跳过懒惰的狗”</em> </strong>，这可以是成对的<strong class="ih hj"> <em class="jd"> (context_window，target_word) </em> </strong>如果我们考虑大小为2的上下文窗口，我们有这样的例子:<strong class="ih hj"> <em class="jd">(【敏捷，狐狸】、棕色)、(【the，棕色)、敏捷)、(【the，狗】</em> </strong>等等。因此，该模型试图基于上下文窗口词来预测目标词。</p><p id="a6e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以将这种CBOW架构建模为深度学习分类模型，使得我们接受<strong class="ih hj"> <em class="jd">上下文单词作为我们的输入，X </em> </strong>并尝试预测<strong class="ih hj"> <em class="jd">目标单词，Y </em> </strong>。事实上，构建这种架构比skip-gram模型更简单，在skip-gram模型中，我们试图从源目标单词预测一整串上下文单词。</p><h2 id="3cb0" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">实施</strong></h2><p id="a472" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">让我们使用Pytorch、nltk等实现Skip-Gram和CBOW模型。我们将使用比未经处理的社交媒体数据更简单的文本语料库。这里我选择了nltk中的“爱丽丝梦游仙境”语料库作为古腾堡计划的标准语料库。</p><p id="b76c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">任何一种ML建模的第一步都是数据预处理，所以我将继续这两种模型共有的处理步骤。</p><figure class="kf kg kh ki fd kj"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">代码1:矢量化函数，用于清理传递给它的文档中的每个句子</figcaption></figure><p id="bcce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如代码1所示，我们有一个用空字符串替换常规字母表之外的任何内容的函数。然后去掉它周围的空格，取小写字母并对字符串进行标记。此后，它删除nltk给出的所有标准停用词，并将句子连接回去并返回。然后，我们使用python标准函数映射，将语料库中的每个句子映射到它的干净版本，并删除所有少于2个单词的句子。这显示在代码2中。</p><figure class="kf kg kh ki fd kj"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">代码2:删除标点和数字</figcaption></figure><p id="7411" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之后，我们使用Code3的代码将每个独特的单词编码成一个数字。此时，我们现在有了一个编码的数据语料库，我们必须为CBOW和SkipGram修改不同的语料库。</p><figure class="kf kg kh ki fd kj"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">代码3:将单词符号化，并用数字等价物替换它们</figcaption></figure><h2 id="05d5" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak"> CBOW实施</strong></h2><p id="7047" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">现在这些单词被编码成数字，以便传递到神经网络中。让我们从编码的语料库中创建训练样本。如代码4所示。对于语料库的每个句子中的每个单词，我们将它周围的上下文切掉，并将其与该单词配对。</p><figure class="kf kg kh ki fd kj"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">代码4:生成上下文和标签</figcaption></figure><p id="0075" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此后，我们使用Pytorch定义网络的架构。如代码5所示，我们有一个嵌入层，它学习词汇表中每个单词的密集向量表示。该层的输出被传递到一个重新激活的线性层。其被进一步传递到具有<strong class="ih hj"> softmax </strong>激活的另一个线性层。</p><p id="2f7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">具有softmax激活的层总是返回概率分布。在我们的例子中，概率分布将是我们之前构建的词汇的分布。输出告诉给定一组上下文单词，哪个单词最有可能被放在中心。</p><figure class="kf kg kh ki fd kj"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">代码CBOW架构定义</figcaption></figure><p id="f541" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在定义了网络之后，我们根据词汇表的一个热编码向量来训练它，每个热编码向量代表一个目标单词。如代码6所示。</p><figure class="kf kg kh ki fd kj"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">代码6:训练循环</figcaption></figure><p id="a4a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">生成的嵌入可以在图3中看到。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kv"><img src="../Images/646564f3a0a84088938abd986af007d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LNBPpwLtkIQm6W3bZULxeg.jpeg"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">图4:数据帧中的学习单词嵌入</figcaption></figure><h2 id="cee9" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">skip program实现</strong></h2><p id="f01b" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">就像在CBoW实现中一样，我们将数字编码的句子作为输入，然后将它们转换为<strong class="ih hj">(上下文单词，目标单词)</strong>我们对skip-gram模型做了同样的事情。不同之处在于，这种情况下的训练样本格式是(<strong class="ih hj"> [(目标，上下文)，<em class="jd">相关性</em> ])。</strong>这可以通过keras序列预处理模块轻松实现，该模块具有创建训练样本的功能。这显示在代码7中。该函数将编码后的句子、生成样本的窗口大小和词汇大小作为输入。</p><figure class="kf kg kh ki fd kj"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">代码7:skip program训练样本生成</figcaption></figure><p id="d981" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在创建了样本之后，我们将它传递给skipgram模型，其架构在代码8中给出。它由嵌入层组成，一层用于单词，一层用于上下文。这些的输出被逐元素相乘，然后被传递到具有<strong class="ih hj"> sigmod </strong>激活的线性层。这个预测的概率值根据标签进行训练，标签告诉我们训练样本的相关性，即它是正样本还是负样本。</p><figure class="kf kg kh ki fd kj"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">代码8:skip program模型架构定义</figcaption></figure><p id="ac26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">skipgram网络的训练循环如下。这里，我们对多个正样本和负样本运行一个循环，收集损耗，然后使用这个损耗通过反向传播来改变网络权重。</p><figure class="kf kg kh ki fd kj"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="aeae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这包括了两种无处不在的神经方法，从我们选择的任何文本中创建单词嵌入。在金融领域，这可以用于商业文章和社交媒体数据，以帮助识别市场alphas的下游任务。模特们的聚集地在这里<a class="ae kq" href="https://github.com/nautiism/embeddings/blob/master/CBOW.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="jd"/></a>和<a class="ae kq" href="https://github.com/nautiism/embeddings/blob/master/skipGram.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="jd">这里</em> </a>。</p><h2 id="a5ba" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">参考文献:</strong></h2><p id="f47b" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">[1]<a class="ae kq" href="https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram . html</a></p><p id="c132" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]<a class="ae kq" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1301.3781</a>(米科洛夫论文)</p><p id="16f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kq" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/pubs/glove.pdf</a></p></div></div>    
</body>
</html>