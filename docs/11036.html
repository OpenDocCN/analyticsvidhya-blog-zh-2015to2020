<html>
<head>
<title>Optimization in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的优化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimization-in-neural-networks-d8d08ecf6659?source=collection_archive---------7-----------------------#2020-11-15">https://medium.com/analytics-vidhya/optimization-in-neural-networks-d8d08ecf6659?source=collection_archive---------7-----------------------#2020-11-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/2913e3cddfdd5502ee91aeadf13cc956.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*2yNVVD_mjphjdlkfnCHgwg.gif"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://www.google.com/search?q=optimizer+in+neural+network++gif&amp;tbm=isch&amp;ved=2ahUKEwjut8e43vLsAhUzSnwKHZKRAm0Q2-cCegQIABAA&amp;oq=optimizer+in+neural+network++gif&amp;gs_lcp=CgNpbWcQAzoECAAQHlCPgwRYrosEYL-MBGgBcAB4AIABoAGIAd0EkgEDMC40mAEAoAEBqgELZ3dzLXdpei1pbWfAAQE&amp;sclient=img&amp;ei=O8inX66uLrOU8QOSo4roBg&amp;bih=657&amp;biw=1366&amp;hl=en#imgrc=TFiEvOMB1z792M" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><blockquote class="ir is it"><p id="b66c" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">什么是优化？</p></blockquote><p id="0a7d" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">在反向传播期间，它更新神经网络的属性(权重和偏差)。</p><p id="3fbf" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">它还确保反向传播需要多少数据，并且只向网络提供该数量的数据。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es jw"><img src="../Images/c140295f9f02cdaafeabde4cd7af0447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*QjQygCf1xKbyHM1BDONpYA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图1</figcaption></figure><p id="bce6" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">让我们开始吧……</p><blockquote class="ir is it"><p id="1068" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">非基于动量的优化</strong></p></blockquote><p id="9bd7" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">在基于非动量的优化中，新的权重<strong class="ix hj">对以前的权重没有任何依赖性</strong>每次我们输入新的一组输入时，我们都会获得与以前的权重没有任何关系的新的权重。坚持住，当你在这篇文章中更进一步时，你会明白所有这些事情。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/72e319e49ae2c5c0d5fc75e461b38bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*Ll3TIW9d5bFgMiwdRhvPwg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图2</figcaption></figure><h2 id="3d90" class="kc kd hi bd ke kf kg kh ki kj kk kl km jt kn ko kp ju kq kr ks jv kt ku kv kw bi translated">1.批量梯度下降</h2><p id="57dd" class="pw-post-body-paragraph iu iv hi ix b iy kx ja jb jc ky je jf jt kz ji jj ju la jm jn jv lb jq jr js hb bi translated">假设数据集有<strong class="ix hj"> n个训练集输入。</strong>当我们将所有的训练集输入数据发送到计算属性时被称为批量梯度下降。</p><p id="532c" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj"> α=∑(y^ — y) </strong></p><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lc"><img src="../Images/f8fb628207fb066839e687d5c01bb8ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cusJtbAULKulJJLGMPyE_g.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">图3</figcaption></figure><p id="077f" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">优势:</strong></p><p id="0250" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">为了计算新的权重，神经网络必须接受成本(所有被传递的输入的平均总和),这为网络提供了一种<strong class="ix hj">可学习的</strong>能力。</p><p id="cde0" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">缺点:</strong></p><ul class=""><li id="6949" class="lh li hi ix b iy iz jc jd jt lj ju lk jv ll js lm ln lo lp bi translated">由于用户必须发送所有输入数据来计算损失，因此这将是一个非常耗时的过程。</li><li id="991d" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">它会有很高的内存消耗。</li><li id="26c0" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">为了处理大数据集，我们需要高计算系统，导致高成本和复杂性。</li></ul><h2 id="bb96" class="kc kd hi bd ke kf kg kh ki kj kk kl km jt kn ko kp ju kq kr ks jv kt ku kv kw bi translated"><strong class="ak"> 2。小批量梯度下降</strong></h2><p id="9fa2" class="pw-post-body-paragraph iu iv hi ix b iy kx ja jb jc ky je jf jt kz ji jj ju la jm jn jv lb jq jr js hb bi translated">假设我们在一个数据集中有<strong class="ix hj"> 1000 </strong>个数据点。在<strong class="ix hj"> MGD </strong>中，与<strong class="ix hj"> BGD </strong>不同的是，我们不会计算整个数据集的损失，但我们会在<strong class="ix hj">50–256</strong>范围内随机选择小批量数据点，然后计算属性。</p><p id="fbdf" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">优势:</strong></p><ul class=""><li id="b1c2" class="lh li hi ix b iy iz jc jd jt lj ju lk jv ll js lm ln lo lp bi translated">由于批量较小，系统将具有较少的内存消耗，将克服内存丢失的问题(这是以前在BGD)。</li><li id="7d26" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">与<strong class="ix hj"> BGD </strong>相比，计算复杂度更低。</li></ul><p id="4b2e" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">缺点:</strong></p><p id="e662" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">在小学习率的情况下，收敛率(寻找绝对最小值的过程)将会太低。</p><h2 id="b83a" class="kc kd hi bd ke kf kg kh ki kj kk kl km jt kn ko kp ju kq kr ks jv kt ku kv kw bi translated">3.随机梯度下降</h2><p id="18cb" class="pw-post-body-paragraph iu iv hi ix b iy kx ja jb jc ky je jf jt kz ji jj ju la jm jn jv lb jq jr js hb bi translated"><strong class="ix hj"> SGD </strong>从<strong class="ix hj"> n个测试数据集</strong>中随机选择一个数据集来计算新的权重和偏差。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/a81419be6418c5e6937fce0ac866cb5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*1yDCkApJ8iHmRYLKOQO26w.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://www.google.com/search?q=stochastic+gradient+descent&amp;sxsrf=ALeKk03J4LJOWqTKz1YUaWDduREGsX6Umw:1605432755864&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwiGtP6In4TtAhVi7XMBHS6eA-sQ_AUoAXoECBUQAw&amp;biw=1366&amp;bih=657#imgrc=UTjx1JwJ5WLYhM" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><ul class=""><li id="c2f3" class="lh li hi ix b iy iz jc jd jt lj ju lk jv ll js lm ln lo lp bi translated">学习率的高低都不会影响属性的变化。</li><li id="8e21" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">与<strong class="ix hj"> BGD </strong>和<strong class="ix hj"> MGD相比，它的计算复杂度更低。</strong></li><li id="0988" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">它与基于动量的梯度下降配合得非常好。</li></ul><blockquote class="ir is it"><p id="50b7" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">基于动量的优化</strong></p></blockquote><p id="0101" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">在计算新权重的技术中，我们必须考虑以前的权重。</p><p id="567b" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">对于新重量的计算，存在使机器可学习的当前损耗和先前损耗的贡献。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/6a4275c097be09041e4c5c0683454b17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*QBY1jeAWGhsiMkEbkDXmBw.gif"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://www.google.com/search?q=momentum+based+gradient+descent+gifi&amp;tbm=isch&amp;ved=2ahUKEwjehtm7oITtAhVVAysKHUHNCzMQ2-cCegQIABAA&amp;oq=momentum+based+gradient+descent+gifi&amp;gs_lcp=CgNpbWcQAzoECAAQHjoECAAQGFCx3wFY8-kBYIfuAWgAcAB4AIABlgGIAcIFkgEDMC41mAEAoAEBqgELZ3dzLXdpei1pbWfAAQE&amp;sclient=img&amp;ei=KvewX97JJtWGrAHBmq-YAw&amp;bih=657&amp;biw=1366#imgrc=9ZhP9qvD6rr6FM" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="0a4d" class="kc kd hi bd ke kf kg kh ki kj kk kl km jt kn ko kp ju kq kr ks jv kt ku kv kw bi translated">1.阿达格拉德</h2><ul class=""><li id="fb79" class="lh li hi ix b iy kx jc ky jt lx ju ly jv lz js lm ln lo lp bi translated">基于参数调整学习速率。即:-如果参数频繁出现，学习速率将会很慢，否则学习速率将会很快。</li><li id="cc36" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">最适合稀疏数据集(非频繁数据集，有偏数据集)。</li><li id="7251" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">它适应手动调谐。</li></ul><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/54191c260cf773a34976f1b4d4e7b285.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/0*E5iixeGhdPlby-IR.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://www.google.com/search?q=adagrad+formula&amp;sxsrf=ALeKk03xRacFhRCK0lI_1xKZGJN154rpJA:1605433702487&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwj17a_MooTtAhVDgOYKHfDMBc4Q_AUoAXoECBoQAw&amp;biw=1366&amp;bih=600#imgrc=A8qJAoaQVrlb4M" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="5a4e" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj"> Gi，t </strong>是每个更新的权重的平方和。<strong class="ix hj"> gi，t </strong>渐变。Epsilon提供平滑度。</p><p id="53c7" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">缺点:</strong></p><p id="fc5c" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">由于分母上更新权重的梯度平方和的存在，单调地降低了学习速率。</p><h2 id="6d18" class="kc kd hi bd ke kf kg kh ki kj kk kl km jt kn ko kp ju kq kr ks jv kt ku kv kw bi translated"><strong class="ak"> 2。阿达德尔塔</strong></h2><p id="b4dc" class="pw-post-body-paragraph iu iv hi ix b iy kx ja jb jc ky je jf jt kz ji jj ju la jm jn jv lb jq jr js hb bi translated">它是阿达格拉德的延伸。克服了<strong class="ix hj"> Adagrad </strong>学习率单调递减的问题。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/45d07f6e6f16a81e88e256ec2cc0c724.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*ag4WPtdZtvUqFpiLN4TiKg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图4</figcaption></figure><p id="b521" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">优势:</strong></p><p id="a54f" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">不再需要手动调谐。</p><p id="d4cf" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">缺点:</strong></p><p id="5a9c" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">导致训练后期</p><h2 id="d40f" class="kc kd hi bd ke kf kg kh ki kj kk kl km jt kn ko kp ju kq kr ks jv kt ku kv kw bi translated">3.RmsProp</h2><p id="c025" class="pw-post-body-paragraph iu iv hi ix b iy kx ja jb jc ky je jf jt kz ji jj ju la jm jn jv lb jq jr js hb bi translated">也是作为<strong class="ix hj">阿达格拉德更好的替代品。</strong>我们采用<strong class="ix hj">运行梯度</strong>，而不是升级重量下降的平方和。</p><p id="13a4" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj"> <em class="iw">跑步渐变？</em>T35】</strong></p><p id="f016" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">梯度的当前值和先前值之间的差异称为运行梯度。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/dfc5eb09fb7895ff7fae3704195ece08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*0djb8dEKEMDXgH03_Mbe_Q.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图5</figcaption></figure><p id="33df" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj"> Eg </strong>是跑步梯度。</p><p id="b832" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">优势:</strong></p><ul class=""><li id="f525" class="lh li hi ix b iy iz jc jd jt lj ju lk jv ll js lm ln lo lp bi translated">它自动调整学习速度，不依赖手动调谐。</li><li id="182a" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">比<strong class="ix hj"> Adagrad </strong>和<strong class="ix hj"> Adadelta性能效率更高。</strong></li></ul><h2 id="7d78" class="kc kd hi bd ke kf kg kh ki kj kk kl km jt kn ko kp ju kq kr ks jv kt ku kv kw bi translated">4.圣经》和《古兰经》传统中）亚当（人类第一人的名字</h2><ul class=""><li id="1f59" class="lh li hi ix b iy kx jc ky jt lx ju ly jv lz js lm ln lo lp bi translated">Adam被称为自适应动量估计。神经网络中最常用的算法之一。</li><li id="f164" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">实现自适应学习速率。</li><li id="997d" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">它适用于不频繁的数据。</li><li id="5944" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">是<strong class="ix hj"> RmsProp </strong>和<strong class="ix hj"> Adagrad的组合。</strong></li><li id="cbd9" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">在<strong class="ix hj">在线设置</strong>和<strong class="ix hj">离线设置</strong>中工作良好。</li><li id="ee53" class="lh li hi ix b iy lq jc lr jt ls ju lt jv lu js lm ln lo lp bi translated">它有非常少的内存需求。</li></ul><p id="4ecc" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">让我们来了解亚当发明<strong class="ix hj">背后的原因；</strong></p><p id="1de2" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">以前，每当用户应用优化时，他们都使用梯度作为基础来制定它们。但是在神经网络领域取得一定的发展后，一群知识分子决定不使用梯度作为基础来制作优化函数，因此他们有了一个替代选项，即:-使用均值和方差来代替梯度来制作优化函数，这里<strong class="ix hj"> Adam </strong>出现了。</p><p id="1baa" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">Adam使用规则的平均值和方差来表示。由于正则化，用户可以完全控制它，无论是设置学习率，取数据集的平均值和方差等。</p><p id="070d" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">在<strong class="ix hj"> Adagrad </strong>、<strong class="ix hj"> Adadelta </strong>和<strong class="ix hj"> RmsProp </strong>的情况下，用户控制<strong class="ix hj">学习速率</strong>，但在<strong class="ix hj"> Adam </strong>的情况下，用户控制<strong class="ix hj">坡度</strong>。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es md"><img src="../Images/f345e0a766f007a121e5987b1904450d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*4hTailvDqVfk-yCR"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://www.google.com/search?q=adam+optimization+neural+network&amp;sxsrf=ALeKk00tmSF3R6lG9d-ULdAuR63CZDstKQ:1605461823488&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwj9v8Gti4XtAhXFX3wKHaIPDNgQ_AUoAXoECBYQAw&amp;biw=1366&amp;bih=657#imgrc=xRO3XwStS_Q8FM" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="6a2e" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">结论:</strong></p><p id="f3f2" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">这都是我的观点，更多有趣的博客请关注……..</p></div></div>    
</body>
</html>