<html>
<head>
<title>Support Vector Machine — with Math — part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机—数学—第1部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/support-vector-machine-with-math-part-1-df5a4bf69913?source=collection_archive---------8-----------------------#2019-11-17">https://medium.com/analytics-vidhya/support-vector-machine-with-math-part-1-df5a4bf69913?source=collection_archive---------8-----------------------#2019-11-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/646570796f21fa73a72e4c3515239147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*SRLhDoAORm9biKsQgnv15w.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">来源:谷歌</figcaption></figure><p id="dd84" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">SVM是一种流行的监督机器学习算法。它用于回归和分类任务。这是一种有区别的分类算法。<br/>朴素贝叶斯是一个生成分类器——学习一个模型来描述每个基础类的分布，生成模型用于概率性地为新点生成标签。但是，正如所称的那样，SVM是一个区别性的分类器，我们不是为每个类别建模，而是找到一条线或曲线(在二维中)或流形(在更高维中)来将类别彼此分开。<br/>如果你能联系到该图，SVM的正式定义是“SVM的主要目标是找到最佳线性分离超平面，它最大限度地增加了利润。”</p><p id="e8d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">SVM背后的直觉:</strong></p><p id="995b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑分类任务的简单情况，其中两类点被很好地分开</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jo"><img src="../Images/6afaca01dc57caccd74d10a18aef8603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*I7YaYkGKKxS4SIP_Z4jRGA.png"/></div></figure><p id="b46e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性判别分类器会试图画一条直线来分隔两组数据，从而创建一个分类模型。对于像这里显示的2 D数据，这是一个我们可以手工完成的任务。但是我们立刻看到一个问题，有不止一条可能的分界线，可以完美地区分这两个阶级。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jt"><img src="../Images/229ac304121ee14cae06d4c4dc925026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FE-WVP0lNHL1qr3ASQSkDg.png"/></div></div></figure><p id="883b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">显然，我们简单的直觉“在类之间划一条线”是不够的，我们需要想得更深一点。</p><p id="c49a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">支持向量机提供了一种改进的方法。直觉是这样的:不是简单地在类之间画一条零宽度的线，我们可以在每条线周围画一个一定宽度的空白，直到最近的点。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jy"><img src="../Images/85a3340fb0e4e8c495c2c6180c2038fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V1rN0xWnwenseovADgytdA.png"/></div></div></figure><p id="82e5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在支持向量机中，最大化这一界限的线是我们将选择作为最佳模型的线。支持向量机就是这种最大间隔估计器的一个例子。</p><p id="b78a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们现在可以使用sklearn的SVC(支持向量分类器)来进行分类。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jz"><img src="../Images/23e172469a11798a196a7d3d8f72fafe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OUe71-3qf2LmJ69TVdwy7g.png"/></div></div></figure><p id="c7ce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上一行的图表中，我们看到了一条分割线，这条分割线最大化了点集之间的边距。很少有训练点只是触及边缘..这些点是这种拟合的关键元素，被称为支持向量，这就是该算法的名称。在Scikit-Learn中，这些点的身份存储在分类器的support_vectors_attribute中。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/9c131c9f63a40f18c3060fac0abbea72.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*-XsOP1Y9lZfmG-plTmIWKQ.png"/></div></figure><p id="f37f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个分类器成功的一个关键是，对于拟合来说，只有支持向量的位置才是重要的<br/>任何远离边缘、位于正确一侧的点都不会修改拟合。<br/>从技术上讲，这是因为这些点对用于拟合模型的损失函数没有贡献，因此只要它们不越过边界，它们的位置和数量就无关紧要。<br/>因此，当我们选择最优超平面时，我们将在一组超平面中选择一个离最近的数据点(支持向量)距离最高的超平面。如果最佳超平面非常接近数据点，那么余量将非常小，并且它将很好地概括训练数据，但是当看不见的数据到来时，它将不能很好地概括，如上所述。因此，我们的目标是最大限度地提高利润率，使我们的分类器可以很好地推广看不见的实例。</p><h1 id="7770" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">SVM培训工作概述</strong></h1><p id="e033" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn hb bi translated">因此，在训练阶段，我们固定训练点的子集，从中计算任何测试点的相似性。这些选定的训练点被称为支持向量，因为只有这些点将支持我们选择测试点类别的决策。我们希望我们的训练阶段找到尽可能少的支持向量，以便我们必须计算更少数量的相似性。<br/>现在，一旦我们选择了支持向量，我们就为每个支持向量分配一个权重，这基本上说明了我们在做出决策时希望给予该支持向量多大的重要性。我们不仅仅重视单个训练点，而是给予每个支持向量单独的重要性。<br/>为了做出决定，我们只需计算从测试点到每个支持向量的相似性的加权和，并基于该加权和计算类别。</p><h1 id="5058" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">深潜SVM前要知道的概念。</strong></h1><p id="d6cc" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn hb bi translated"><strong class="is hj">向量的长度:<br/> </strong>向量x的长度称为它的范数，记为||x||。计算向量x = (x1，x2，…，xn)的范数的欧几里德范数公式为:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es le"><img src="../Images/176b5426c7b94bb54d39fca2320a7cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*fPK9o6mToEbGSPie4gfL-A.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">向量范数</figcaption></figure><p id="abfb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">矢量的方向:</strong></p><p id="8284" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">向量x = (x1，x2x1，x2)的方向记为w，定义为:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/aeb2cf515ceca65678fd083eb71f9e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*9FE63gTPRFlTRArKZxPJPQ.png"/></div></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es lg"><img src="../Images/2eb03b63e61646bfa601902289238a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCLQ4fpegX8_KOfLzlAshQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">向量的方向示例</figcaption></figure><p id="942d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们看这个图，我们可以看到。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/ccff0cca020ab097e706db92d6db5e02.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*j0xkkUgU4a9JDDSFNa2CkA.png"/></div></figure><p id="b4aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，方向向量w也可以写成:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es li"><img src="../Images/96d5ad157b9246c27b4321cc30479064.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*7FgCVxi7e0hvRZ2GZZbyww.png"/></div></figure><p id="6a5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">方向向量的范数总是1。因此w也称为单位向量。</p><h1 id="0fe0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">两个向量的点积:</strong></h1><p id="8fa9" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn hb bi translated">两个向量的点积返回一个标量。它让我们对这两个向量的关系有了一些了解。<br/>下图显示了两个矢量x和y以及它们之间的夹角θθ。点积的几何公式定义为:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/ad42975e6359f6ceae93d0f28e701ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*_gaK09_qzk-V0r0E534asw.png"/></div></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es lk"><img src="../Images/5a524b03691dec6164a8d99db40bce29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hz5NYgUpK7QXK_2naP6row.png"/></div></div></figure><p id="210e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是从上图我们可以看到</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/15172b7ac8c2c0ee3070076af3543f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*Y9W0DlDWtIgti50tabaiLA.png"/></div></figure><p id="1350" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此我们得到:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/97dcaa37013c1e36c9e66761593bada0.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*4jDiH_kW_VGI7Tv3pYhwAg.png"/></div></figure><p id="3620" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">将cos(θ)代入公式，我们得到:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/3ce786625c72b8af8136db5ee3f7e0c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*xYOJOrRtmqkFj1i8f1e_rg.png"/></div></figure><p id="c0ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是点积的代数公式。一般来说，对于两个n维向量，点积可以计算如下:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/115ef6f37e4c2eeb0c39388cf6da1aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*RedHMJBznw7NNT8lx4zY9A.png"/></div></figure><h1 id="0502" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">超平面</strong></h1><p id="6248" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj ld jl jm jn hb bi translated">我们前面已经看到过术语超平面。但是到底是什么呢？它用于分离更高维度的数据(3D)。<br/>我们先来看二维的情况。二维可线性分离的数据可以用一条线分开。这条线的等式是y=ax+b。我们用x1重命名x，用x2重命名y，我们得到:</p><p id="e9a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ax1 x2+b = 0</p><blockquote class="lp lq lr"><p id="43ce" class="iq ir ls is b it iu iv iw ix iy iz ja lt jc jd je lu jg jh ji lv jk jl jm jn hb bi translated">如果我们定义x = (x1，x2)，w = (a，1)，我们得到:<br/> w⋅x+b=0 <br/>这个方程是由二维向量导出的。但事实上，它也适用于任何数量的维度。这是超平面的方程。</p></blockquote><p id="abd3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦我们有了超平面，我们就可以使用这个超平面来进行预测。我们将假设函数h定义为</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/f3b343e2acf74c208678395ddfeaeb19.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*6zHeRTvtEiUNF725ATtIBA.png"/></div></figure><p id="b880" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">超平面之上或之上的点将被分类为类+1，超平面之下的点将被分类为类-1。<br/>因此，基本上，重申SVM学习算法的目标是找到一个能够准确分离数据的超平面。可能有许多这样的超平面。我们需要找到最好的一个，它通常被称为最优超平面。</p><p id="bd70" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">比较超平面的度量:</strong></p><p id="38d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们首先考虑超平面w⋅x+b =0的方程。我们知道，如果点(x，y)在超平面上，w⋅x+b=0.如果点(x，y)不在超平面上，w⋅x+b的值可以是正的或负的。对于所有的训练样本点，我们想知道最接近超平面的点。我们可以计算β=|w⋅x+b|(垂直距离)。正式定义问题:<br/>给定一个数据集，我们为每个训练样本计算β，B是我们得到的最小β。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/a70128263e1b4b053f971eea776bb963.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*t8-DHCXh5Rk8TwMaTxKx7Q.png"/></div></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es li"><img src="../Images/df815306951931e4ac00cd5a4f5db00c.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*omLleLqJX7g1czwkBufFmg.png"/></div></figure><p id="a7d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们有s个超平面，每个超平面都有一个Bi值，我们将选择具有最大Bi值的超平面。</p><p id="2db8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">前一个度量的问题是，它可能无法区分好的超平面和坏的超平面。因为我们取w⋅x+b的绝对值，我们可以得到一个正确的和一个不正确的超平面的相同值。我们需要调整这个指标。我们可以使用标签y的信息。让我们定义f=y(w⋅x+b)，并且如果点被正确分类，f的符号将总是正的，如果不正确分类，将是负的。<br/>为了使其形式化，给定一个数据集D，我们为每个训练样本计算F，F是我们得到的最小F。在文献中，F被称为数据集的功能裕度。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/bf52b6b14d28cc440aecb97d2e9ffe21.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*nK9YHjHqQGvZJ_fl6V4CbQ.png"/></div></figure><p id="f99d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当比较超平面时，具有最大F的超平面将被优先选择。看来我们找到了正确的衡量标准。然而，这种度量存在一个问题，叫做尺度变异。例如，我们有两个向量w1=(1，4)和w2=(10，20))。因为它们具有相同的单位向量u=(0.6，0.8)，所以两个向量w1和w2表示相同的超平面。然而，当我们计算F时，带有w2的函数将返回一个比带有w1的函数更大的数。我们需要找到一个尺度不变的度量。</p><p id="d08c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们看看最终的版本，它也是比例不变的:我们将f除以向量w的长度，我们定义:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/a85083268e20c9c65964ddaa6b403af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*zrEOzRRKZZTvQ7ysTlVt0w.png"/></div></figure><p id="9373" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了使它形式化，给定一个数据集D，我们为每个训练样本计算γ，M是我们得到的最小γ。在文献中，M被称为数据集的几何边缘</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/d6aaac631eff9b508d487d49df54d98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*TF_4WxrtO776o-5Hnoa11g.png"/></div></figure><p id="80da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当比较超平面时，具有最大M的超平面将被优先选择。我们现在有了一个比较不同超平面的完美标准。我们的目标是找到一个最优超平面，这意味着我们需要找到最优超平面的w和b的值。<br/>求w和b的值的问题叫做优化问题。</p><p id="77c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在下一部分，我将提出如何推导svm优化问题(提示对偶)。</p><p id="1d04" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">参考文献:</strong></p><div class="mb mc ez fb md me"><a href="https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hj fi z dy mj ea eb mk ed ef hh bi translated">理解支持向量机背后的数学原理</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">支持向量机(SVM)是最强大的开箱即用的监督机器学习算法之一。不像…</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">shuzhanfan.github.io</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ik me"/></div></div></a></div><div class="mb mc ez fb md me"><a href="https://jakevdp.github.io/PythonDataScienceHandbook/" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hj fi z dy mj ea eb mk ed ef hh bi translated">Python数据科学手册</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">该网站包含Jake VanderPlas的Python数据科学手册的全文；该内容可在…上获得</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">jakevdp.github.io</p></div></div></div></a></div></div></div>    
</body>
</html>