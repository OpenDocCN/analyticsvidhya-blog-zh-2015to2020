<html>
<head>
<title>Wading through Graph Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">涉水通过图形神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/wading-through-graph-neural-networks-968f2ef138af?source=collection_archive---------21-----------------------#2020-05-10">https://medium.com/analytics-vidhya/wading-through-graph-neural-networks-968f2ef138af?source=collection_archive---------21-----------------------#2020-05-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/d3ec167ba801d83ab4c08a34010f4e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RJCxH5q5COPeErKWg-Nw8Q.jpeg"/></div></div></figure><div class=""/><p id="e573" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个博客中，我将讨论图形神经网络及其变体。让我们从什么是图形神经网络以及它可以应用的领域开始。我们继续进行的顺序如下:</p><h1 id="ab23" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">主题</h1><ol class=""><li id="5da7" class="km kn ht is b it ko ix kp jb kq jf kr jj ks jn kt ku kv kw bi translated"><em class="kx">图及其动机</em></li><li id="decf" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated"><em class="kx">图形卷积</em></li><li id="a6c7" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated"><em class="kx">图形注意力网络</em></li><li id="abce" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated"><em class="kx">门控图神经网络</em></li><li id="501b" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated"><em class="kx">图形自动编码器</em></li><li id="93e8" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated"><em class="kx">图贤者</em></li><li id="7200" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated"><em class="kx">GCN背后的历史</em></li></ol><h1 id="f8bc" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">图形及其动机</h1><p id="b83e" class="pw-post-body-paragraph iq ir ht is b it ko iv iw ix kp iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">随着深度学习时代的发展，我们有许多针对文本、图像、视频等数据集的先进解决方案。这些算法大多由MLPs、RNNs、CNN、Transformers等组成。，它在前面提到的数据集上表现出色。但是，我们也可能会遇到一些非结构化数据集，如下所示:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lg"><img src="../Images/bf9dbd5edf2dde77eb849d715741a457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9PHHNRyatzlkn60c6k85nw.png"/></div></div></figure><p id="744c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你所需要的，就是表示这些系统的图，这就是我们将要进一步讨论的。</p><p id="203b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们简单了解一下图表是什么样子的:</p><p id="d53d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图<strong class="is hu"> G </strong>由两个关键元素<strong class="is hu"> {V，E} </strong>表示，其中<strong class="is hu"> V </strong>是节点的集合，而<strong class="is hu"> E </strong>是在其节点之间定义的边的集合。图形神经网络具有<strong class="is hu">节点特征</strong>和<strong class="is hu">边特征</strong>，其中节点特征表示系统的单个元素的属性，边特征表示系统元素之间的<strong class="is hu">关系、交互或连接</strong>。</p><p id="53b7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">节点特性:</strong></p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ll"><img src="../Images/f7e094f4fc219260df82b7191ec6fbc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QVV78CUhfi_5qdmuc5-8Zw.png"/></div></div></figure><p id="0390" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">边缘特征:</strong></p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lm"><img src="../Images/2c774b13a2c3f7e51cd42a3094f84af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jqwqF-Utszn-VcqIVWhgxA.png"/></div></div></figure><p id="0fc9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图形神经网络不仅可以产生系统元素的特征，还可以了解它们之间的相互作用，从而学习系统中存在的归纳偏差。从系统中学习归纳偏差的能力使其适用于像少点学习、自我监督或零点学习这样的问题。</p><p id="617b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以使用GNNs进行节点预测、链接预测、寻找节点、子图或图形的嵌入等等。现在让我们来谈谈图的卷积。</p><h1 id="03ee" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">图形卷积网络</h1><p id="e3c2" class="pw-post-body-paragraph iq ir ht is b it ko iv iw ix kp iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">所以在一个图问题中，我们得到一个图<strong class="is hu"> G(V，E) </strong>，目标是学习节点和边的表示。对于每个顶点<strong class="is hu"> <em class="kx"> i </em>，</strong>我们需要有一个特征向量<em class="kx"/><strong class="is hu"><em class="kx">【vᵢ</em></strong>并且节点之间的关系可以用一个邻接矩阵<strong class="is hu"> <em class="kx"> A. </em> </strong>来表示</p><p id="54e9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">可以使用简单的神经网络来表示GCN，其数学表示如下:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ln"><img src="../Images/a6bfbc3a80677e2de320081f28070bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yvheJh6eyD24Qs6RLyw3tg.png"/></div></div></figure><p id="c3e5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<strong class="is hu"><em class="kx">【hᵢˡ】</em></strong>是<strong class="is hu"><em class="kx">I</em><em class="kx">lᵗʰ</em></strong>层中的特征向量，<strong class="is hu"><em class="kx">【wˡ】</em></strong>是用于<strong class="is hu"><em class="kx"/></strong>层的权重矩阵，<strong class="is hu"> <em class="kx"> N(i) </em> </strong>是节点<strong class="is hu"> <em class="kx"> i的邻域中的节点集合</em></strong></p><p id="1f7c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，正如你所看到的，给定层中的所有节点的权重是共享的，类似于传统的卷积滤波器，节点的特征向量是在对其邻居节点(也类似于CNN)执行一些<strong class="is hu">数学运算(这里使用边缘参数进行加权求和)后计算的，</strong>我们提出了术语图卷积网络，<strong class="is hu"><em class="kx">【wˡ】</em></strong>可以被称为层<strong class="is hu"><em class="kx"/></strong>中的滤波器</p><p id="7686" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">很明显，我们在这里看到两个问题，第一个是当计算节点的特征向量时，我们不考虑它自己的特征向量，除非存在自循环。其次，这里使用的邻接矩阵不是归一化的，因此它会由于边缘参数的大值而导致缩放问题或梯度爆炸。</p><p id="1a8b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了解决第一个问题，实施自循环，为了解决第二个问题，使用邻接矩阵的规范化形式，如下所示:</p><ol class=""><li id="3de3" class="km kn ht is b it iu ix iy jb lo jf lp jj lq jn kt ku kv kw bi translated">自循环:<strong class="is hu"> Â =A+I </strong></li><li id="92a3" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated">正常化:<strong class="is hu">â=(d^(-1/2))â(d^(-1/2))</strong></li></ol><p id="aff3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<strong class="is hu"> I </strong>是形状为<strong class="is hu"> A </strong>的单位矩阵，而<strong class="is hu"> D </strong>是对角矩阵，其每个对角值对应于矩阵<strong class="is hu"> Â.中相应节点的度数</strong>在每一层中，信息从它的邻居传递到一个节点，因此这个过程被称为<strong class="is hu">消息传递。</strong></p><p id="0fb3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们来讨论一下我们讨论过的简单图形卷积的一些变体。</p><h1 id="953a" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">图形注意网络</h1><p id="1e81" class="pw-post-body-paragraph iq ir ht is b it ko iv iw ix kp iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">在科技界，关注并不是什么新鲜事。它已经广泛应用于LSTMs、GRUs、CNN等架构。现在，它甚至能够成为像伯特和GPT那样的基于变形金刚的模型的唯一和最重要的组成部分。因此，注意机制的主要目的是通过以概率的形式学习输入的权重来关注输入的一些关键区域。让我们简单地看看它在图形神经网络中是如何工作的。</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/3a5a404e7c43bf967187278a75024a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RBRzpyx46gXEwc-x53BqPg.png"/></div></div></figure><p id="6a31" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">左边给出的是一个图形卷积的修正方程。区别在于术语:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ls"><img src="../Images/88b41ab5723962f926068950332eb6c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*XYcIUigJqwE-wbyhI0pOgQ.png"/></div></div></figure><p id="235b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些不过是注意力的砝码。对于<strong class="is hu"><em class="kx"/></strong>层中节点<strong class="is hu">I</strong>的每个邻居节点<strong class="is hu"> <em class="kx"> j </em> </strong>，在更新节点<strong class="is hu"> <em class="kx"> i. </em> </strong>的特征向量时，学习表示其重要性的权重，因此，我们从关注权重越大的节点获取越多的信息，而从权重越小的节点获取越少的信息，而不是找到邻居的简单加权和。</p><p id="6080" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，这些重量可以通过多种方式学习，如softmax或一些挤压功能，如下所示:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lt"><img src="../Images/8327c144a38d9e4dd5cd250054a6c8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8iI1BzN2xGfD7QArvocPmA.png"/></div></div></figure><p id="c806" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了让事情变得更有趣，我们还可以使用多头注意力方法，该方法将使用<strong class="is hu"> K个</strong>头而不是一个头，最后取从<strong class="is hu"> K个</strong>头学习到的所有特征向量的平均值(<strong class="is hu"> <em class="kx">或连接</em> </strong>)。下图对此进行了说明:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lu"><img src="../Images/0091875fbf81fbc90b97755dc1e38c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hpm7Y15vd8fewEPD9zq2ZA.png"/></div></div></figure><h1 id="1e45" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">门控图神经网络</strong></h1><p id="1a32" class="pw-post-body-paragraph iq ir ht is b it ko iv iw ix kp iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">现在，假设我们想要为一个像文本这样的序列数据集建立一个模型，但是序列中的每个元素不能仅仅通过像手套一样的嵌入来表示，而是采取一种类似图形的结构的形式。朋友们，现在我们需要讨论门控图神经网络。这些网络可用于处理序列图。所以，基本上每个节点都使用以前的节点状态和当前的消息状态来更新，就像GRU一样。</p><p id="072c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">节点状态和消息状态类似于RNN单元中的隐藏状态和单元状态。下面的公式清楚地解释了这个理论:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lv"><img src="../Images/d32d11ce51dbe63e0b3122a794b33eb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VbHxvtpglnuyTEpm7fgNqg.png"/></div></div></figure><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lw"><img src="../Images/586f374eb6796c03cd901b0d46b39e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dtj9E_6xENeREq8Cv6thMQ.png"/></div></div></figure><p id="7799" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在给出的方程式中，<strong class="is hu"><em class="kx"/></strong>是节点<strong class="is hu"> <em class="kx"> i i </em> </strong> n层<strong class="is hu"> <em class="kx"> l </em> </strong>和<strong class="is hu"> <em class="kx"> mˡᵢ </em> </strong>是层<strong class="is hu"> <em class="kx"> </em> </strong>节点<strong class="is hu"><em class="kx">I</em></strong>的消息状态<strong class="is hu"> <em class="kx"> l. U </em></strong></p><h1 id="1e1a" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">图形自动编码器</h1><p id="b334" class="pw-post-body-paragraph iq ir ht is b it ko iv iw ix kp iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">类似于其他自动编码器，我们也可以有一个图形自动编码器。图形自动编码器可用于节点压缩或图形数据结构的自我监督训练。在这种情况下，我们首先为图中的节点找到一个潜在的特征向量表示，然后学习从潜在的表示中重建原始的邻接矩阵。同样，为了使事情复杂化，我们可以使用变分图自动编码器。图形自动编码器如下图所示:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lx"><img src="../Images/f356c579f7a57865d986283fa42cc702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I_fzxM4e7U-ZkLiG50AzHA.png"/></div></div></figure><p id="09ba" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">给定一个图<strong class="is hu"> <em class="kx"> G(X，A) </em> </strong>，我们通过学习分布<strong class="is hu"> <em class="kx"> q(z|X，A) </em> </strong>来学习节点的潜在表示。然后我们通过学习分布<strong class="is hu"><em class="kx">【p(a | z)】</em>来尝试重构邻接矩阵<strong class="is hu"><em class="kx"/></strong>。</strong></p><h1 id="c2fd" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">图表圣人</h1><p id="06d8" class="pw-post-body-paragraph iq ir ht is b it ko iv iw ix kp iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">有没有想过这样一个事实:在一个典型的GCN中，我们通过对一个节点的邻居节点的特征进行加权平均来聚集该节点的邻居信息。问题是我们能不能尝试更多或更好的东西。这就是我们讨论图形SAGE的地方。在这种情况下，当节点的特征向量被更新时，我们尝试学习聚集节点的邻域信息的函数近似器，而不是取相邻节点的加权平均值。下图会让它明白得很清楚:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ly"><img src="../Images/259f8c2ba98787f1f26e23bdb9ea9999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfcramCqNRJGGkxf1fgXOQ.png"/></div></div></figure><p id="e7c8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图中的黑盒是一个可微分函数，它将一些向量作为输入，将它们的信息聚合成一个单一的特征向量。相同的等式如下:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lz"><img src="../Images/7d6f07c480b2582777bc299df90fabc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YDC5C0tAUUHxcWmuVjmxAg.png"/></div></div></figure><p id="123d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<strong class="is hu"><em class="kx"/></strong>是<strong class="is hu"> <em class="kx"> k层中节点<strong class="is hu"> <em class="kx"> v </em> </strong>的特征向量，A </em> </strong>和<strong class="is hu"> <em class="kx"> B </em> </strong>是可训练的权重。通过执行一些加权来连接聚集的邻域信息和节点特征向量本身，然后通过非线性函数σ来馈送，以获得下一层的节点特征向量。<strong class="is hu"> AGG </strong>是聚合器函数，可以有多种形式，如均值、池或LSTM聚合器，如下所示:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ma"><img src="../Images/5468c2f75f30dc95a05dd2828bb02089.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BezmMCJz9Y5A8t8wrNXRPg.png"/></div></div></figure><h1 id="746d" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">GCNs背后的一些历史</h1><p id="0db5" class="pw-post-body-paragraph iq ir ht is b it ko iv iw ix kp iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">我们上面讨论的方法都使用空间滤波器，但是我们在图形神经网络中也有另一种类型的滤波器，<strong class="is hu">光谱滤波器</strong>。</p><p id="5fc5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用频谱滤波器要求我们进入频谱域(傅立叶域)，这进一步要求我们计算拉普拉斯L图的特征向量分解，拉普拉斯L图是图G的邻接矩阵的特殊归一化形式<strong class="is hu">。</strong></p><h1 id="ce5b" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">L = I — <strong class="ak"> Â </strong></h1><p id="8222" class="pw-post-body-paragraph iq ir ht is b it ko iv iw ix kp iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">其中<strong class="is hu"> Â </strong>是博客开始时讨论的归一化邻接矩阵，而<strong class="is hu"> I </strong>是单位矩阵<strong class="is hu">。</strong></p><p id="9911" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> L </strong>分解为形式，<strong class="is hu">T3】l=<em class="kx">vλvᵀ</em></strong><em class="kx">其中</em> <strong class="is hu"> <em class="kx"> V </em> </strong> <em class="kx">为</em> <strong class="is hu"> <em class="kx"> L </em> </strong> <em class="kx">的特征向量的矩阵，λ为对角矩阵，其对角值对应于</em> <strong class="is hu"> <em class="kx"> L </em> </strong> <em class="kx">的特征值。</em></p><p id="d41c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正向传递的公式如下所示:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mb"><img src="../Images/ee603ec6ce535adca03f4b25daa4f4ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Um4LmM2N13c9m76uAAI4tQ.png"/></div></div></figure><p id="048f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中W是权重矩阵，点是逐元素乘法。对于大的邻接矩阵，计算分解变得不可行，从而使得该过程在计算上非常昂贵。</p><p id="2880" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae mc" href="https://arxiv.org/abs/1606.09375" rel="noopener ugc nofollow" target="_blank"> Defferrard等人，NeurIPS，2016 </a>引入的切比雪夫图卷积逼近，通过使用切比雪夫逼近并引入一个参数<strong class="is hu"> K. </strong>对于<strong class="is hu"> K=1，</strong>我们通过<strong class="is hu"> </strong>节点特征<em class="kx"> X⁽ˡ⁾ </em>在我们的GNN模型中；对于<em class="kx"> K=2，</em>我们经过<em class="kx"> X⁽ˡ⁾，ax⁽ˡ⁾；对于K=3，</em>我们通过<em class="kx"> X⁽ˡ⁾，AX⁽ˡ⁾，a</em>t44】x⁽ˡ⁾同样对于<strong class="is hu"> K=n </strong>，我们通过<em class="kx"> X⁽ˡ⁾，AX⁽ˡ⁾，a</em>T50】x⁽ˡ⁾……..GNN模型中的一个 ⁿ <em class="kx"> X⁽ˡ⁾ </em>。一旦来自第<strong class="is hu"> n </strong>跳的所有特征被创建，我们连接所有的特征向量，然后再次应用GNN来获得节点的最终特征向量。从下面给出的图像中可以清楚地看到这一过程:</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es md"><img src="../Images/4b15ccf295540f5bc0b05ae4d116e762.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S-BlqFpAFARkJNMw03DLKw.png"/></div></div></figure><p id="6862" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">切比雪夫近似确实消除了频谱域，但是由于在单个层中的迭代<strong class="is hu"> n(由我们决定)</strong>跳，其计算量仍然很大。</p><h2 id="61d4" class="me jp ht bd jq mf mg mh ju mi mj mk jy jb ml mm kc jf mn mo kg jj mp mq kk mr bi translated">进一步近似和重正化技巧</h2><p id="e3fd" class="pw-post-body-paragraph iq ir ht is b it ko iv iw ix kp iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">由于增加<strong class="is hu"> K </strong>的值会导致模型参数数量的增加，我们可以将该值限制为K=2。</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ms"><img src="../Images/cc36d6038b4970ffce4c7c62ba8702fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pTXCfNi9W6jwkZROskUGYA.png"/></div></div></figure><p id="f865" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图中使用的A是没有自循环实施的原始邻接矩阵。通过对θ₀<em class="kx"/>和θ₁<strong class="is hu"><em class="kx"/></strong>使用赋予相同<strong class="is hu"> θ的技巧或近似，我们得到了在GCN部分已经讨论过的重正化技巧(增加自循环)。因此，这种近似给出了适用于1跳和2跳的GCN层，如果我们想要2跳以上，我们可以继续添加这样的GCN层。</strong></p><h1 id="506a" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="a6aa" class="pw-post-body-paragraph iq ir ht is b it ko iv iw ix kp iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">到目前为止，我们讨论了使用图形神经网络背后的动机。在下一篇博客中，我打算讨论GCNs的一些很酷的应用，比如文本分类、链接预测、基于零镜头草图的图像检索等等。</p><h1 id="79b9" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">参考</h1><ol class=""><li id="c1fd" class="km kn ht is b it ko ix kp jb kq jf kr jj ks jn kt ku kv kw bi translated"><a class="ae mc" href="https://arxiv.org/abs/1606.09375" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1606.09375</a></li><li id="e942" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated">https://arxiv.org/abs/1609.02907<a class="ae mc" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank"/></li><li id="b679" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated"><a class="ae mc" href="https://towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49" rel="noopener" target="_blank">https://towards data science . com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be 6d 71d 70 f 49</a></li><li id="9f39" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated"><a class="ae mc" rel="noopener" href="/blogging-guide/medium-subscript-text-and-medium-superscript-format-c169a8717ecf">https://medium . com/blogging-guide/medium-subscript-text-and-medium-superscript-format-c169a 8717 ECF</a></li><li id="ef9c" class="km kn ht is b it ky ix kz jb la jf lb jj lc jn kt ku kv kw bi translated"><a class="ae mc" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">https://tkipf.github.io/graph-convolutional-networks/</a></li></ol></div></div>    
</body>
</html>