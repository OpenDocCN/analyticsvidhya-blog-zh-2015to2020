<html>
<head>
<title>Using GPU to boost XGBoost Training Time</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用GPU提高XGBoost训练时间</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/using-gpu-to-boost-xgboost-training-time-533a114164d7?source=collection_archive---------0-----------------------#2019-10-15">https://medium.com/analytics-vidhya/using-gpu-to-boost-xgboost-training-time-533a114164d7?source=collection_archive---------0-----------------------#2019-10-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2ef8f4010475c5a2cfc3c601847c5a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9NAm_ZXLFg6wZBg8"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">比尔·杰伦在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="b94f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自过去十年以来，产生的数据量增长速度超过了我们处理数据的能力，即使并行处理也可能无法在令人满意的时间内处理这些信息量。为此，越来越多的机器学习爱好者正在使用GPU(图形处理单元)来训练预测模型，因为它可能会将处理时间减少到使用并行编程时的三分之一，正如我们将在本文中看到的那样。这是因为GPU由数百个执行数学运算的简单内核组成，而在多核处理器中，只有少量复杂的处理单元，因此在某些情况下处理时间会更长。</p><p id="7be5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">已经解释过，在本文中，我们将在Google Colab的环境中使用GPU运行时类型内核来创建sklearn的管道，该管道处理数据，基于chi2度量选择最佳功能，并训练一个<em class="jt"> XGBClassifier </em>来预测1994年美国人口普查中的收入，并且，在建立了该管道之后，将训练执行时间与sklearn的<em class="jt"> GridSearchCV </em>方法进行比较，以用于CPU和GPU处理。最后，我们将比较每次训练运行的模型的训练时间和性能。</p><h2 id="7669" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">建设管道</h2><p id="4b23" class="pw-post-body-paragraph iv iw hi ix b iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">在构建管道之前，了解普查数据集的性质对于定义预处理步骤至关重要。在下图中，我们有关于导入的数据框的信息。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/4d13cdd5a8450a9f678bbb11b90a7cfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*nGF7r2Si06zc7ZEEKKeP9Q.png"/></div></figure><p id="6417" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们所看到的，表上总共有32561个条目，其中9个是类别特征的强候选项，6个是数值特征的强候选项。在目前的工作中，我们将对分类特征进行标签编码，以保持列数较少，还因为除了“native_country”之外，没有任何具有高基数的特征，该特征具有高基数，并且将被设计为二元变量:美国或外国。此外，在数字特性中，我们将应用sklearn的<em class="jt"> MinMaxScaler </em>来减少数据集那些列的方差。</p><p id="d117" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">预处理数据的下一步是选择在模型中使用的最佳特征。如上所述，为了实现这一点，我们将使用<em class="jt"> SelectKBest </em>函数，它在我们的管道中接收两个参数:称为chi2的度量，它是两个变量之间的关联值，以及要选择的特性的数量，即被选择的“K”个最佳特性。这就结束了我们管道的预处理步骤。</p><p id="7565" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们管道的最后一部分是定义将要使用的预测器，在本例中是<em class="jt"> XGBClassifier </em>。在目前的工作中，我们将使用<em class="jt"> XGBClassifier </em>创建3个管道，该管道具有一个名为“tree_method”的参数，该参数定义了将在模型训练中使用的资源的数量和类型。因此，我们将在此参数上使用默认的“自动”、“gpu_hist”和“gpu_exact”值，比较计时结果以及测试数据的平均AUC分数。</p><p id="9467" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">管道如下:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/17757c68da6c22466b73459f81f89966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PR0tTmivqFpgPAxDD5xF0Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">用于时序分析的基本管道。</figcaption></figure><p id="8cd2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第一步“full_pipeline”是分类预处理和数值预处理之间管道联合的结果(使用<em class="jt"> FeatureUnion </em>方法)，如下图所示:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/d7d1269e9deb7819e74795d38da0fc07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fMRmaFvtusgfZxefWDp2JQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">预处理整个管道。</figcaption></figure><p id="23ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个管道中，有两个类是为了方便这一步而编写的:一个是<em class="jt"> FeatureSelector </em>，它只选择数据集的指定特性，无论是分类的还是数字的；另一个是<em class="jt"> CategoricalTransformer </em>类，它通过标签编码和特性工程“workclass”和“native_country”变量来转换分类列。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/85a6a5d36f27d0e397a33d8cc07bd007.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*5Rp8dHKL-SykWFS9CZysZA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">预处理管道上使用的FeatureSelector类。</figcaption></figure><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/8fcfaf2475325ebf17d58f31ee77eb0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*YdrHUcL4N3kofmkOyziPtQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">预处理管道上使用的CategoricalTransformer。</figcaption></figure><p id="1221" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最终，将有3个管道，一个用于CPU训练，另外两个用于GPU训练时间评估。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/dc78486dfbb6a60c6fd7b4dd5b4c179d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1k7bVHPO9cBVdZV8qlZ2CQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">用于CPU时间评估的管道。</figcaption></figure><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/1c166f3627afc1d0b5983e246c069a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LwbzMBBaYBl4WO3VM6h4mw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用tree_method="gpu_exact "进行GPU时间评估的流水线。</figcaption></figure><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/f2790476dbf017c402f4d9b252405647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wMAfPE_7LrUf5-WoJUQeVg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用tree_method="gpu_hist "进行GPU时间评估的流水线。</figcaption></figure><h2 id="6c9d" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">训练预测模型</h2><p id="a4c1" class="pw-post-body-paragraph iv iw hi ix b iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">构建了基本管道后，下一部分是构建一个训练方法，以找到更好地预测数据的模型。由于使用的是XGBClassifier<em class="jt">XGBClassifier</em>，这是sklearn对XGBoost的改编，我们将在交叉验证中使用具有5个折叠的<em class="jt"> GridSearchCV </em>方法。最后，三个测试中使用的搜索网格是相同的，可以在下图中看到。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/63e713a57205c35f92c2214e297e1247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*R1US-IxK-oHUJ2ReHZOc7A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">模型调整中使用的搜索空间。</figcaption></figure><p id="5969" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此步骤中使用的<em class="jt"> GridSearchCV </em>取决于我们使用CPU还是GPU的测试，通过将参数“n_jobs”定义为-1(使用CPU时)和1(使用GPU时)来比较性能。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/aa8fa524267e08aaa70c1a45f484f679.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*jJiX3bekio-A1CYdhum4ig.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">用于CPU计时分析的GridSearchCV。</figcaption></figure><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es li"><img src="../Images/f14070c251400d39755010d88d862c49.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*RWzr-n4eQTRSmM3tJPdxdA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">用于GPU时序分析的GridSearchCV。</figcaption></figure><h2 id="c236" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">比较管道</h2><p id="2063" class="pw-post-body-paragraph iv iw hi ix b iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">在这一节中，我们将把执行时间和模型性能的结果与测试数据的AUC指标进行比较，并检查最佳模型的性能是否有差异。</p><p id="f376" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们要检查的第一个管道是“tree_method”参数设置为默认值并在Google Colab的CPU中并行化的管道，它是一个2.3GHz的英特尔至强双核处理器。训练时间可以在下图中看到。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/2173558e6547b2313e09c419f0642bf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*AvJHzgJgtppNVAiKRORpMg.png"/></div></figure><p id="74d5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如我们所见，训练时间为943.9秒，在测试数据上，最佳性能模型的平均AUC分数为0.925390。</p><p id="b2d9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在第二个管道中，我们将使用“gpu_hist”作为“tree_method”参数的值，以使用XGBoost的一种算法来训练模型，在这种情况下，它消耗的Google Colab的GPU Tesla K80的资源较少。由此产生的时间可以在下图中看到。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/187d03fcee145d732063aaa6f308efeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*foz4Yp4cBBBFm1Lu4LuKxw.png"/></div></figure><p id="66cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">训练时间为814.6秒，测试数据的平均AUC得分为0.924636。</p><p id="1332" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我们将在“tree_method”上使用“gpu_exact ”,尽管它已经停止使用，并检查对于这个特定的问题和数据集，它的性能是优于还是劣于“gpu_hist”方法。时间可以在下一张图中看到。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/2234bbf67fc50e705ab09ad1ecc42447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*6n8xslRe4ntctYiMS6adBg.png"/></div></figure><p id="5fd9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用“gpu_exact”方法，我们获得了255.6秒的训练时间和0.925151的平均测试AUC分数，尽管我们使用了相同的参数和折叠数，但与以前的分数不同。这可能是由于XGBoost如何使用GPU方法对其树执行分割。</p><h2 id="b6cc" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated">结论</h2><p id="b308" class="pw-post-body-paragraph iv iw hi ix b iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo kt jq jr js hb bi translated">正如我们所看到的，在两个gpu训练管道中，与CPU训练管道相比，训练时间显著减少，特别是在使用“gpu_exact”参数时，它比使用CPU的训练快4倍，而“gpu_hist”也比CPU训练执行得更好，尽管不如第一个GPU方法。</p><p id="334d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们讨论了所有这些之后，我们可以得出结论，在我们进行测试的这个问题上，使用GPU和XGBoost的分类器可以显著提高训练时间，特别是使用“gpu_exact”。虽然平均测试AUC在每个管道中得分不同，但最终结果彼此之间并没有明显的差异，因此我们可以假设，在训练XGBoost模型时，建议使用Google Colab的GPU，因为它的得分与使用CPU一样高，但训练时间大大减少。</p></div></div>    
</body>
</html>