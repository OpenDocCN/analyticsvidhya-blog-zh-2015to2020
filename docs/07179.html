<html>
<head>
<title>Transformer-xl VS Universal Sentence Encoder</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Transformer-xl VS通用句子编码器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/transformers-vs-universal-sentence-encoder-980c165bc96d?source=collection_archive---------9-----------------------#2020-06-16">https://medium.com/analytics-vidhya/transformers-vs-universal-sentence-encoder-980c165bc96d?source=collection_archive---------9-----------------------#2020-06-16</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><div class=""/><div class=""><h2 id="ffba" class="pw-subtitle-paragraph il hn ho bd b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc dx translated">声明:这篇文章纯粹是实验性的，你可能找不到实验的可靠理论。</h2></div><p id="055d" class="pw-post-body-paragraph jd je ho jf b jg jh ip ji jj jk is jl jm jn jo jp jq jr js jt ju jv jw jx jy hh bi translated">在2020年，我们有很多选择，从Word2Vec这样的简单跳格模型到transformers这样的复杂编码器-解码器架构。正如你所知，LSTM和RNN的编码器-解码器架构，或者变形金刚中实现的奇特的注意力机制，都不是以前没有的。我们也可以像RNN那样实施注意力，但是…</p></div></div>    
</body>
</html>