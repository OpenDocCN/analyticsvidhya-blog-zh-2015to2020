<html>
<head>
<title>Regularisation Techniques in Machine Learning and Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和深度学习中的正则化技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/regularisation-techniques-in-machine-learning-and-deep-learning-8102312e1ef3?source=collection_archive---------4-----------------------#2019-10-08">https://medium.com/analytics-vidhya/regularisation-techniques-in-machine-learning-and-deep-learning-8102312e1ef3?source=collection_archive---------4-----------------------#2019-10-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e09c2d496c91c1da45cc08283d7873c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EAxwcMGV8UgiThzX"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">克里斯·利维拉尼在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="39ee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">机器学习和深度学习从业者在构建ML模型时面临的一个最常见的问题就是“<strong class="ix hj">过拟合<em class="jt"> g </em> </strong>”。</p><h2 id="791c" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated"><strong class="ak"> <em class="kp">什么是过度拟合？</em>T11】</strong></h2><p id="6275" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo ku jq jr js hb bi translated">当机器学习模型在训练数据集上表现良好时，它被称为“过拟合”，但在测试/不可见数据集上的性能相对较差。</p><p id="909c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们通过一个有趣而简单的例子来了解过度拟合背后的直觉。</p><p id="9336" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑两个学生通过学习同一本书来准备考试。学生1试图记住书中的问题和答案，而不是试图理解书中不同主题的潜在概念。学生2试图抓住每个主题背后的概念，而不是像学生1那样死记硬背。</p><p id="0548" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">试卷包含一半数量的问题，与书本中的问题相同，而其余的问题类似，但很难测试学生的理解。</p><p id="6d3d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的场景中，学生1不能在考试中表现得很好，因为他只能回答书中提出的简单问题，而对于其余有点棘手的问题，他不能回答得很好。相比之下，学生2在测试中的每个问题上都表现得很好，因为他对从那本书中学到的概念有了更好的理解。</p><p id="09b9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">我们总是希望我们的机器学习(ML)模型更像学生2，而不是学生1。</em></p><h2 id="83d4" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated"><strong class="ak"> <em class="kp">什么是欠拟合？</em> </strong></h2><p id="a7f5" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo ku jq jr js hb bi translated">这是过度拟合的对应，也是每当我们谈论过度拟合时出现的一个重要概念。</p><p id="33ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果ML模型在训练和测试数据集上都表现不好，则称其为欠拟合。</p><p id="72f8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，一个学生既没有记住书中的任何问题，也没有试图理解书中的任何概念。因此，他在考试中既不能很好地回答简单的问题，也不能很好地回答复杂的问题。</p><p id="298c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> </strong></p><p id="66e6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是过度拟合、欠拟合和最佳/合适拟合的图示。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/be750731435457ec57eb71371ae6efb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*31ADiBCi7kuRJNoIAM4GWA.png"/></div></figure><p id="61b6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">图片来源:谷歌</em></p><p id="8ec9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ML和DL模型在训练阶段很容易过度拟合。出现这种情况的一些原因可能是数据集的数据点数量较少，但要素数量较多。该模型可能最终记忆关于每个特征的输入和输出之间的关系，因为随着数据点的数量减少，该模型在训练期间可能在数据集中找不到这样的重要模式。</p><p id="e57e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">过度拟合的另一个原因可能是非线性机器学习算法，如决策树，因为这些算法在基于可用数据集构建模型时具有更大的自由度，其中其分支和节点可能最终构建非常深的决策树，与测试数据相比，该决策树最终在训练数据上表现非常好。</p><p id="bc80" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ML和DL中有许多正则化技术可以帮助我们防止模型的过度拟合。不同类型的正则化技术将在下面详细讨论。</p><h1 id="6e48" class="la jv hi bd jw lb lc ld ka le lf lg ke lh li lj kh lk ll lm kk ln lo lp kn lq bi translated"><strong class="ak"> (I) L1和L2正规化:- </strong></h1><p id="efc8" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo ku jq jr js hb bi translated">在L1和L2正则化中，模型因过度拟合训练数据而受到惩罚，即每当模型试图正确预测训练数据点上的一切时，就模型的系数而言，损失函数会增加一些惩罚。</p><p id="4acb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在许多机器学习技术中，如逻辑回归、支持向量机等。以及在深度学习技术中，我们将正则化项(惩罚)添加到“损失函数”中，以便对于训练数据，损失项不会变为零或接近零。</p><p id="56d8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是“逻辑损失”函数，它是“逻辑回归”情况下的损失函数:</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/e257b903116b4a911a55c07feb1fddf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*5SFr-mlvZ3878gNKwkH81g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:-stackoverflow.com</figcaption></figure><p id="108b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上图中，损失函数没有正则化项。</p><p id="8011" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ML模型将试图将对数损失减少到接近于零的非常小的值。</p><p id="a90c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果损失函数没有正则化项，那么ML模型将把权重参数“<em class="jt"> x </em>”增加到一个非常高的值(理想的是无穷大)，以使总损失接近于零。但这将导致ML模型的过度拟合，因为它在训练集上表现非常好，这是我们想要避免的。</p><h2 id="cbc0" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated"><strong class="ak"><em class="kp">【L2】正规化(山脊):- </em> </strong></h2><p id="60b3" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo ku jq jr js hb bi translated">为了避免过度拟合，我们添加了一个正则项，如下所示:</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/cfa1a5ee7357f5c08d2608c8c6cb69c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*N6-qdEb8kieXQWJhIteUxA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:-stackoverflow.com</figcaption></figure><p id="e7e1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">损失函数中的第二项是“L2”正则化项。这里将<strong class="ix hj"> <em class="jt">【权重参数的平方值】</em> </strong>与<strong class="ix hj"><em class="jt">λ</em></strong>(建立模型时要调整的超参数)一起添加到逻辑损失函数中。</p><p id="565c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">L2正则化是最广泛使用和被验证的正则化技术之一，由ML从业者使用，帮助我们建立健壮的ML模型，能够很好地概括。</p><p id="05a8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果使权重系数“<em class="jt"> x </em>”变高，以将损失函数中的第一项减小到接近零，则第二项将增加，从而避免总损失函数值变为零。这样，正则化项会对试图对训练数据集点进行非常精确预测的模型造成不利影响。</p><p id="e1b9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">L2正规化的特点:- </em></p><ul class=""><li id="469d" class="lt lu hi ix b iy iz jc jd jg lv jk lw jo lx js ly lz ma mb bi translated">L2正则化，也称为“岭回归”，在大多数情况下比L1正则化表现更好。</li><li id="c8ba" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">不太重要的特征被缩小到低值，但不为零。</li></ul><h2 id="a754" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated"><strong class="ak"><em class="kp">【L1规则化】(套索):- </em> </strong></h2><p id="a504" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo ku jq jr js hb bi translated">下面是添加了L1正则化项的损失函数:</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/a3ceb56257155be57039b0d1af8f21bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*_6YoPGEQ-OTu1zyQqWSO2g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:-stackoverflow.com</figcaption></figure><p id="ee0e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，将<strong class="ix hj"> <em class="jt">【权重参数绝对值】</em> </strong>与<strong class="ix hj"><em class="jt">λ</em></strong>(超参数)一起添加到损失函数中。</p><p id="7954" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">类似于L2正则化，如果使权重系数“<em class="jt"> x </em>”变高，以将损失函数中第一项的值减小到接近零，则第二项，即L1正则化项将增加，从而避免总损失函数变为零。</p><p id="98c2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与L2正则化相比，L1正则化对模型的惩罚更少，因为它在损失函数中使用绝对值而不是权重参数的平方值。</p><p id="c697" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">L1正规化的特点:- </em></p><ul class=""><li id="1d8f" class="lt lu hi ix b iy iz jc jd jg lv jk lw jo lx js ly lz ma mb bi translated">L1正则化，也称为套索回归，与L2正则化不同，将不太重要的特征归零。</li><li id="d7e7" class="lt lu hi ix b iy mc jc md jg me jk mf jo mg js ly lz ma mb bi translated">因此，L1执行内部特征选择。正因为如此，在我们对可以使用的功能数量有某种硬性限制的应用程序中，它是首选。</li></ul><h2 id="8ef7" class="ju jv hi bd jw jx jy jz ka kb kc kd ke jg kf kg kh jk ki kj kk jo kl km kn ko bi translated"><strong class="ak"> <em class="kp">弹力网正规化:- </em> </strong></h2><p id="4559" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo ku jq jr js hb bi translated">弹性网正则化是L1正则化和L2正则化的结合。它可以表示如下:</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/999ed4699cb9921abee29d1131af8a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*WYCpTzQHHYY74Qsk9IR85A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">资料来源:stats.stactexchange.com</figcaption></figure><p id="5fa9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="jt">α</em></strong>上式中的术语<strong class="ix hj"><em class="jt">λ</em></strong>与L1和L2正则化公式中使用的术语相同。</p><p id="7b17" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与L1和L2正则化相比，应用于ML模型以惩罚过度拟合的总体惩罚更多地是在弹性网正则化中。</p><p id="a7d0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">弹性网规化的特点:- </em></p><ul class=""><li id="4604" class="lt lu hi ix b iy iz jc jd jg lv jk lw jo lx js ly lz ma mb bi translated">弹性网是L1和L2正则化之间的一种妥协，试图同时收缩和进行稀疏选择。</li></ul><p id="8dbe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">SKLearn对不同ML算法的实现有一个术语叫做<strong class="ix hj"><em class="jt">【penalty】</em></strong>，其中我们可以指定上面提到的3种正则化技术中的一种，我们希望在训练模型时使用。下面是SKLearn的SGD分类器文档中的一些图像，这些图像代表了“惩罚”项的默认值以及可用选项。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/6d3244c10fcf932a6eab4d2893dac2b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V8pGucJ_yVI4kZXGoaGz-Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:-<a class="ae iu" href="https://scikit-learn.org/stable/modules" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules</a></figcaption></figure><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/22f09a87190bf6762465e1911672def8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQo0eYa8u5fKlN9caE9yzg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:-【https://scikit-learn.org/stable/modules T2】</figcaption></figure><p id="50ed" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Sklearn的<em class="jt">线性模型</em>模块还有Lasso(L1)、Ridge(L2)和Elastic-Net回归和分类子模块，可以直接在数据集上使用，如下所示。</p><p id="9c7d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">示例:- </strong>以下是对非常著名的波士顿房屋预测数据集应用的上述3种正则化技术的示例，以及从其中每一个获得的系数:</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/2acbb0eedc813b2582f9400153fad3e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dbXYil2tk1qJSyoxLzx-5A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw"> 1。在Pandas数据框中加载数据集</strong></figcaption></figure><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/e3aded093bdd3fa5935c2b051e55b6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8J-1S0N56sgdlI0oejUVwA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw"> 2。列车测试分割</strong></figcaption></figure><p id="dc83" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">㈠岭回归(L2):</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/86050b5a17eade41a0191eab9e4f4a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bE6NqPQLk3BEGRa8w5ovw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw"> 3。应用岭回归</strong></figcaption></figure><p id="557e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们所观察到的，在岭回归的情况下，不太重要的特征的权重减小了，但是没有变为零。</p><p id="f0fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">㈡拉索回归(L1):</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/01efdd3afd3570436a18fb127092112c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IXTg8DOTikBGJJgDvyEWNw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw"> 4。应用套索回归</strong></figcaption></figure><p id="41d0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们可以观察到的，L1正则化或套索，使不太重要的特征为零，从而执行内部特征选择。</p><p id="8d61" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">㈢弹性网正规化:</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/f6aedd88080c402be9d295e91eaaa325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5EewoyfNj7LX7-PuUmYUZA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw"> 5。应用弹性网回归</strong></figcaption></figure><h1 id="af18" class="la jv hi bd jw lb lc ld ka le lf lg ke lh li lj kh lk ll lm kk ln lo lp kn lq bi translated"><strong class="ak">(二)数据扩充:- </strong></h1><p id="ff30" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo ku jq jr js hb bi translated">尽管在正则化的情况下，与其他技术相比，数据扩充没有被广泛讨论，但它可以帮助我们减少过拟合。</p><p id="383f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据点数量较少但要素数量较多的数据集更容易过度拟合。数据扩充指的是在训练集中添加更多的相关数据，使得用于训练模型的数据点的总数增加，并且足以让模型理解数据中的潜在模式，以便它可以很好地概括。</p><p id="5f30" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，收集数据的过程既费钱又费时。此外，找到与我们正在解决的问题相关的数据并不总是容易获得的。</p><p id="b078" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">深度学习库<strong class="ix hj"> Keras </strong>有一个模块叫做<strong class="ix hj"><em class="jt">imagedata generator</em></strong>用于数据扩充。DL模型包括非常多的权重参数，因此如果训练数据点的总数较少，则容易过拟合。因此，我们使用数据扩充技术，通过对现有数据点执行各种变换，例如像缩放、翻转、旋转图像等操作，从现有数据点集生成新的数据点。由<strong class="ix hj"> <em class="jt">图像数据生成器</em> </strong>模块对现有的训练数据图像执行，以开发用于训练DL模型的新数据图像。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/6bc6becb719d23b48c8fc1d167126c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J2FpR29aDM_WQ2CTpvE1mg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">第2(a)条。正在加载CIFAR数据集</figcaption></figure><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/e747397fa9bef656240973ecea20196c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2D6zftiFpci3jR9OYHTgPQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">第2条(b)款。使用Keras 中的<strong class="bd jw"> <em class="kp">图像数据生成器</em> </strong> <em class="kp">模块对CIFAR列车数据进行数据扩充</em></figcaption></figure><h1 id="2e4e" class="la jv hi bd jw lb lc ld ka le lf lg ke lh li lj kh lk ll lm kk ln lo lp kn lq bi translated"><strong class="ak">(三)辍学:- </strong></h1><p id="4773" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo ku jq jr js hb bi translated">辍学是深度学习中使用最广泛的正则化技术之一。</p><p id="7ef7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Dropout，顾名思义，是基于一个神经网络中<strong class="ix hj"><em class="jt">【随机丢弃节点】</em> </strong>的过程。我们为dropout指定了一个概率值，它表示在每次迭代中一个节点被丢弃的概率。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/393b1504909eec9c984e82bc23d7de93.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*f6OSYoy4UcH2DQ54e0PCsg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:维基百科</figcaption></figure><p id="f327" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设，我们将一个节点被丢弃的概率指定为0.5(即抛硬币)。在每次迭代中，来自输入层和隐藏层的一些节点被丢弃，从而产生一个更简单的神经网络，该网络仅基于可用节点做出决策。由于在应用丢弃后，每次迭代中可用的节点数量减少，因此每次迭代的计算时间减少。</p><p id="3a51" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，在每次迭代中使用具有不同节点集的神经网络可以帮助我们捕捉数据中更多的随机性，并且通常比使用单个完全连接的神经网络执行得更好。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/d787f96cdd7b22922d102f3508430f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*gsccBOlNJEYwrL902lZBaA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:researchgate.net</figcaption></figure><p id="863e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这类似于ML中的<em class="jt">集合模型</em>(像随机森林和GBDT)使用多个学习器来预测输出。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/845b1755aad874d475aa3b6fa9829f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*JrslpGdiRKnGxWb1J6JciA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw">参考</strong>:<a class="ae iu" href="https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/dropout-regulation-deep-learning-models-keras/</a></figcaption></figure><p id="048a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">I上图，Dropout中的值“0.2”代表每个节点被丢弃的<strong class="ix hj"> <em class="jt">概率值</em> </strong>。概率值是一个超参数，必须进行调整，以便我们可以获得最佳的退出概率值，这将有助于我们获得最佳结果。</p><p id="15b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是一个在MNIST数据集上应用<em class="jt"> CNN和</em>CNN的例子。</p><p id="3dbe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt"> (1) CNN同辍:- </em> </strong></p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mu"><img src="../Images/fc53483b28805002dec51191baa97102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*87OeYfdcOzU4En7Lotg7IA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw">加载数据</strong></figcaption></figure><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/e17cc30043f35873c75e5d544a8cf125.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rO5cgR3FfLzTxla1WO4oZQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw">压差=0.5的CNN</strong></figcaption></figure><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/5e3719d53204283befdc462266c6fd26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mfn5epTLtn2qkuGLJckvUQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw">根据训练数据拟合模型，并指定优化器和指标</strong></figcaption></figure><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/70c7cee1ac5242c27dc41673485738df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2efLHCarVvYF0GIuRlhdVQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw">退出CNN的结果</strong></figcaption></figure><p id="780a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过使用具有Dropout的CNN，在测试数据上实现了接近99.5%的准确度。</p><p id="890f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt"> (2) CNN无辍:- </em> </strong></p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/365cb6144713953dc06d014acec79eda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZRXGvOW-_lXy7wnVY6Jcw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw"> <em class="kp"> CNN无辍</em> </strong></figcaption></figure><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/788ff585e035eb2ab52693bdae07b22b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QqiuOQGh2HmcSE0lvdjAHg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw">CNN没有退出的结果</strong></figcaption></figure><p id="9bcd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过使用CNN，测试数据的准确率接近99%。</p><p id="c448" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">尽管两种情况下的准确性几乎相同，但我们可以看到，使用dropout会产生一个能够很好地概括的模型。</p><h1 id="3d01" class="la jv hi bd jw lb lc ld ka le lf lg ke lh li lj kh lk ll lm kk ln lo lp kn lq bi translated"><strong class="ak"> (IV)提前停止:- </strong></h1><p id="f5e6" class="pw-post-body-paragraph iv iw hi ix b iy kq ja jb jc kr je jf jg ks ji jj jk kt jm jn jo ku jq jr js hb bi translated">早期停止是另一种非常广泛使用的正则化技术，以避免在建立ML和深度学习模型时过度拟合。顾名思义，在模型开始过度适应训练数据集之前，我们在训练阶段<em class="jt">【提前停止】</em>。</p><p id="1f20" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，我们将验证集与训练集一起使用，并且在决定模型何时停止进一步训练之前，我们监视验证错误/丢失。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/5f2f650ee2a922b033f42ed007bce51a.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*JpcNOGNLuquIOL_5y7vXZA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">资料来源:-四年. eu</figcaption></figure><p id="3eda" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上图中，模型将在<strong class="ix hj">【蓝线】</strong>处停止训练，因为在蓝线之后，CV误差开始增加，而训练误差继续减小，导致过度拟合。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es na"><img src="../Images/ba8d4fa82d2e1fd4f943192d0ffeb641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*sXSPzGFYTVuo338B0lJ2yg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:-<a class="ae iu" href="https://machinelearningmastery.com/" rel="noopener ugc nofollow" target="_blank">https://machinelearningmastery.com</a></figcaption></figure><p id="83db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上图中，“<strong class="ix hj"><em class="jt">【monitor】</em></strong>值表示在训练阶段将被监控的指标，以决定模型何时停止进一步训练。这里，我们在训练阶段监控<strong class="ix hj"> <em class="jt">【验证损失】</em> </strong>。<strong class="ix hj"> <em class="jt">【耐心】</em> </strong>的值表示经过多少次迭代后，模型将停止训练，因为它在<em class="jt">“验证错误”中没有发现进一步的改进。</em></p><p id="b51a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一些最大似然算法包括一个提前停止参数，我们必须指定提前停止值。</p><p id="7f54" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是一个XGBoost模型的示例，该模型应用于early_stopping=20的施主选择数据集。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/7b059de5df8d0a13f5b54fc558be2c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHSxHcNXy-N1gunDFtuIUQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw"> 1。加载和读取数据集</strong></figcaption></figure><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/fdcf82a01471a67b6d929c7dfb1338ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1S7ZL_Ke-Dsc7HFEK9sB3A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw"> 2。列车测试分割</strong></figcaption></figure><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nd"><img src="../Images/6477e54b2c5e243b7fd9cccb30aed0a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IeUpc0G6T7j7F-1H25lq9w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw"> 3。应用模型。</strong></figcaption></figure><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/7dc650451a0faccd15046538e06977c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GVtWMWuRjHYjr085nRJcw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw"> 4。提前停止的XG boost = 20</strong></figcaption></figure><p id="e550" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据图4，提前停车轮数= 20。<em class="jt">“验证错误”</em>是被监控的性能指标。训练模型，直到20轮验证误差没有改善。这也有助于我们节省总的训练时间，因为如果指定的时期数是100，早期停止轮次= 20，但是在训练时，在第50个时期之后，如果模型在接下来的20个时期中没有显示任何改进，那么它将进一步停止训练，从而避免过度拟合。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nf"><img src="../Images/6a69284521534842ffc3fa6251feed0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZszyVIphZasf1iClaGkbTw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd jw">本帖中讨论的规范化技术列表。</strong></figcaption></figure><p id="390b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇关于ML和DL中的正则化技术的博客到此结束，其中包括一些ML和DL中最广泛使用的正则化技术，这些技术帮助ML从业者建立了能够很好地概括的健壮ML模型。</p><p id="7346" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的下一篇博客将是关于<strong class="ix hj"><em class="jt">“ML和DL中的性能指标”</em> </strong>我们将深入探讨一些最常用的性能指标的细节，并讨论它们各自的优缺点。</p><p id="0252" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请分享您的必要反馈和问题。</p></div></div>    
</body>
</html>