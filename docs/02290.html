<html>
<head>
<title>Understanding the learning of sigmoid activations in a neural network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经网络中sigmoid激活的学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-learning-of-sigmoid-activations-in-a-neural-network-2e1b2fc27db7?source=collection_archive---------19-----------------------#2019-12-09">https://medium.com/analytics-vidhya/understanding-the-learning-of-sigmoid-activations-in-a-neural-network-2e1b2fc27db7?source=collection_archive---------19-----------------------#2019-12-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="20f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我之前的博客中，我解释了神经网络中的单个神经元试图学习什么。我们知道，神经元试图通过根据损失函数优化权重和偏差来学习分类边界。现在让我们进入实际问题。神经元学习完美分类边界(存在的情况)后会发生什么？为了说明的目的，让我们用一个简单的例子，来自我的<a class="ae jd" rel="noopener" href="/analytics-vidhya/neural-networks-explained-for-machine-learning-beginners-b2acc4d24a95">以前的博客</a>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/021e765e61951ab17b7e2722e24eed6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*DVTpq8wVsEdTHGKjPFg-jg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">资料组</figcaption></figure><p id="bff2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图中，我们有数据集和试图对数据进行分类的神经网络。神经元要做的所有学习就是优化与其相关的权重和偏差，以模拟红色分类线。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jq"><img src="../Images/67b4dd3dee54b43c57918391523f2235.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*BOrS8GitxcSj6vtbJ62U6A.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">网络</figcaption></figure><p id="18d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在假设我们需要25个历元来学习准确的分类边界。如果我们尝试运行模型超过25个时期，比如说100个时期，会怎么样？我们在剩余的75个时代中使用的计算能力是没有价值的吗？这些知识都去了哪里？有人可能会认为，一旦准确率达到100%(在理想情况下)，网络中就再也没有任何事情可做了。这也是一个可信的论点，因为我们已经达到了损失函数曲线的全局最小值。但这是不正确的。学习仍然发生在网络中，但是目的不同于优化损失函数。要理解这一点，我们必须先了解一些概念，然后再开始解释。</p><h1 id="bdf4" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">神经网络中的神经元试图学习什么？</h1><p id="6f1e" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">正如我在以前的博客中解释的那样，每个神经元都有自己的目标，根据它试图学习的特征对其输入进行分类。它试图通过其权重和偏差来学习分类边界，以将正值和负值分配给线相对侧的点(详细解释，我推荐阅读之前的博客)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/021e765e61951ab17b7e2722e24eed6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*DVTpq8wVsEdTHGKjPFg-jg.png"/></div></figure><p id="db76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图中所示的示例中，神经元应该学习线w0*X+w1*Y = 3(其中X和Y是特征，权重w0、w1是1，偏差等于-3)。这一节要注意的第一个问题是，线2*w0*X + 2*w1*Y = 6，3*w0*X+3*w1*Y = 9，..？如果所有方程都表示同一条线，为什么网络选择w0*X+w1*Y = 3(或者实际上是这样的)？</p><h1 id="4c63" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">为什么首先使用乙状结肠？</h1><p id="e09f" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">我们为什么要用sigmoid激活函数？如果目标是将二进制值分配给线的相对两侧，那么我们可以使用阶跃函数(用于感知器)。这个问题的答案来自于反向传播的算法。阶跃函数是不可导的，因此是近似的sigmoid函数，它在所有点上都是光滑和可微的。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ku"><img src="../Images/0c697812455c793679eebcce2262b28b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m1t8FEhf0tkmJRY1qf6PZg.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">Sigmoid激活函数</figcaption></figure><p id="65b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本节要注意的一点是:在所示的图中，所有曲线对应于不同的sigmoid函数。一个神经元选择什么曲线？</p><h1 id="4042" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">将这些点连接起来</h1><p id="bc2f" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">图中不同的sigmoid曲线是由于sigmoid函数中关联的不同权重。我们已经看到，通过将权重和偏差乘以一个常数项，我们得到了对应于相同分类边界的不同方程。并且这些方程产生了图中所示的不同的s形曲线。在这些曲线中，神经元试图优化权重，使得sigmoid函数近似于阶跃函数。随着权重和偏差成比例地增加，激活函数接近阶跃函数。让我们用一个简单的实验来分析一下。对于之前显示的网络和数据集，表中显示了不同时期后学习到的权重。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es kz"><img src="../Images/6c956e8f21918767b6f75bcd99343aa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQB-muOsezY8zSGM7et_4Q.png"/></div></div></figure><blockquote class="la lb lc"><p id="41ea" class="if ig ld ih b ii ij ik il im in io ip le ir is it lf iv iw ix lg iz ja jb jc hb bi translated">结论:即使网络处于其全局最小值，权重和偏差保持更新，这是结果饱和后学习发生的地方。</p></blockquote><p id="601f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">欢迎提出改进文章的建议。希望本文有助于深入理解神经网络的学习。如果你喜欢阅读我的文章，请访问<a class="ae jd" href="https://bhanu77prakash.github.io" rel="noopener ugc nofollow" target="_blank">我的网站</a>获取更多关于人工智能、机器学习、自然语言处理等方面的博客..</p></div></div>    
</body>
</html>