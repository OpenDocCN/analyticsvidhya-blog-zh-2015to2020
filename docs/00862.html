<html>
<head>
<title>Topic Modeling with Amazon Reviews</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">亚马逊评论的主题建模</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/topic-modeling-with-amazon-reviews-8dcb40ffc97d?source=collection_archive---------5-----------------------#2019-09-14">https://medium.com/analytics-vidhya/topic-modeling-with-amazon-reviews-8dcb40ffc97d?source=collection_archive---------5-----------------------#2019-09-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/1a3f0a030410589b5f465c2ffe73b7db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpytbqadO3FtdIyOjx2_yg.png"/></div></div></figure><div class=""/><p id="e813" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">什么是主题建模？</p><p id="4f38" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">主题建模可以描述为一种从文档集合中找到最能代表这些文档中信息的主题的方法。使用这种方法，您可以发现隐藏的模式，注释您的文档并总结一堆文档。</p><h2 id="3253" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated"><strong class="ak">数据</strong></h2><p id="be11" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">我们将使用的数据集是亚马逊关于电子产品的评论。该数据集包含从1996年5月到2014年6月的产品评论。该数据集包括评论文本、评级、有用性投票。对于下面的方法，我们将只使用评论。</p><p id="364a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">数据预处理</strong>是处理文本数据时的一项重要任务。以下是处理数据的步骤，然后创建一个单词包来适应一个<strong class="is hu">潜在的狄利克雷分配</strong> ( <strong class="is hu"> LDA </strong>)</p><p id="f3d4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">第一步:提取数据</strong>——这是最简单的一步，我用的是网页上<a class="ae ko" href="https://cseweb.ucsd.edu/~jmcauley/" rel="noopener ugc nofollow" target="_blank"><strong class="is hu">Julian McAuley</strong></a>、UCSD提供的代码:<a class="ae ko" href="http://jmcauley.ucsd.edu/data/amazon/" rel="noopener ugc nofollow" target="_blank">亚马逊产品数据</a></p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="fbd1" class="jo jp ht ku b fi ky kz l la lb">def parse(path):<br/>    g = gzip.open(path, 'rb')<br/>    for l in g:<br/>        yield eval(l)</span><span id="d1ec" class="jo jp ht ku b fi lc kz l la lb">def getDF(path):<br/>    i = 0<br/>    df = {}<br/>    for d in parse(path):<br/>        df[i] = d<br/>        i += 1<br/>    return pd.DataFrame.from_dict(df, orient='index')</span><span id="a2ca" class="jo jp ht ku b fi lc kz l la lb">df = getDF(r'NLP\reviews_Electronics_5.json.gz')<br/>df.head(10)<br/>df.columns</span></pre><p id="5d6c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">步骤2: Tokenizer，删除停用词&amp;小写doc</strong>——我们将使用Regexptokenizer将所有句子拆分成单词，然后在删除标点符号的同时将其小写。<strong class="is hu">我们选择一个文档来预览结果</strong></p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="6967" class="jo jp ht ku b fi ky kz l la lb">#Regular expression tokenizer<br/>tokenizer = RegexpTokenizer(r'\w+')<br/>doc_1 = df.reviewText[0]</span><span id="a490" class="jo jp ht ku b fi lc kz l la lb"># Using one review<br/>tokens = tokenizer.tokenize(doc_1.lower())</span><span id="d22c" class="jo jp ht ku b fi lc kz l la lb">print('{} characters in string vs {} words in a list'.format(len(doc_1),                                                             len(tokens)))<br/>print(tokens[:10])</span><span id="e1e3" class="jo jp ht ku b fi lc kz l la lb">nltk_stpwd = stopwords.words('english')</span><span id="02a1" class="jo jp ht ku b fi lc kz l la lb">print(len(set(nltk_stpwd)))<br/>print(nltk_stpwd[:10])</span><span id="2ecf" class="jo jp ht ku b fi lc kz l la lb">stopped_tokens = [token for token in tokens if not token in nltk_stpwd]<br/>print(stopped_tokens[:10])</span></pre><p id="5a06" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第三步:用雪球词干分析器做词干——注意雪球词干分析器应用后的单词，要了解雪球词干分析器的更多信息，请访问<a class="ae ko" href="https://snowballstem.org/" rel="noopener ugc nofollow" target="_blank">网站</a>。</p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="dfef" class="jo jp ht ku b fi ky kz l la lb">sb_stemmer = SnowballStemmer('english')<br/>stemmed_tokens = [sb_stemmer.stem(token) for token in stopped_tokens]<br/>print(stemmed_tokens)<br/>"""<br/>This is how results looked:</span><span id="abd4" class="jo jp ht ku b fi lc kz l la lb">'normal', 'receiv', 'review', '<strong class="ku hu">sampl</strong>', 'thorough', 'evalu', 'write', 'review', 'within', 'two', 'day', 'one', 'took', 'longer', 'reason', 'took', 'hear', 'differ', 'model', 'versus', 'thebrainwavz', 'ear', '<strong class="ku hu">headphon</strong>', 'also', 'impress', 'also', 'go', 'pile', 'album', 'tri', 'understand', 'product', '<strong class="ku hu">descript</strong>', 'meant', 'smoother', 'bass', 'final', 'found', 'album', 'allow', 'hear', 'portrait', 'jazz', 'scott', 'lafaro', 'bass', 'came', 'autumn', '<strong class="ku hu">leav</strong>', 'start', 'switch', 'back', 'forth','also', 'haul', 'studio'</span><span id="c89f" class="jo jp ht ku b fi lc kz l la lb">"""</span></pre><p id="88dd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">步骤3A:对所有数据执行上述所有步骤——为了创建LDA模型，我们需要将上述预处理步骤放在一起，以创建一个文档列表(列表列表),然后生成一个文档术语矩阵(唯一术语作为行，评论作为列)。这个矩阵告诉我们每个术语在每个文档中出现的频率。</p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="33e9" class="jo jp ht ku b fi ky kz l la lb">num_reviews = df.shape[0]</span><span id="1761" class="jo jp ht ku b fi lc kz l la lb">doc_set = [df.reviewText[i] for i in range(num_reviews)]</span><span id="d34b" class="jo jp ht ku b fi lc kz l la lb">texts = []</span><span id="ff17" class="jo jp ht ku b fi lc kz l la lb">for doc in doc_set:<br/>    tokens = tokenizer.tokenize(doc.lower())<br/>    stopped_tokens = [token for token in tokens if not token in nltk_stpwd]<br/>    stemmed_tokens = [sb_stemmer.stem(token) for token in stopped_tokens]<br/>    texts.append(stemmed_tokens)# Adds tokens to new list "texts"<br/>    <br/>print(texts[1])</span></pre><p id="30b2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">步骤4:使用语料库创建词典</p><p id="b507" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Gensim的Dictionary方法封装了规范化单词和它们的整数id之间的映射。<a class="ae ko" href="https://radimrehurek.com/gensim/corpora/dictionary.html" rel="noopener ugc nofollow" target="_blank">更多链接</a>！另外，请注意下面代码中的步骤，我们可以评估单词和它们的id之间的映射，为此我们使用token2id方法。</p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="089d" class="jo jp ht ku b fi ky kz l la lb">texts_dict = corpora.Dictionary(texts)<br/>texts_dict.save('elec_review.dict') <br/>print(texts_dict)</span><span id="6a1f" class="jo jp ht ku b fi lc kz l la lb"><strong class="ku hu">#Assess the mapping between words and their ids we use the token2id #method:</strong><br/>print("IDs 1 through 10: {}".format(sorted(texts_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:10]))</span><span id="1db3" class="jo jp ht ku b fi lc kz l la lb">#Here we assess how many reviews have word complaint in it<br/>complaints = df.reviewText.str.contains("complaint").value_counts()<br/>ax = complaints.plot.bar(rot=0)</span><span id="3200" class="jo jp ht ku b fi lc kz l la lb">"""<br/>Attempting to see what happens if we ignore tokens that appear in less <br/>than 30 documents or more than 20% documents.<br/>"""</span><span id="4ed5" class="jo jp ht ku b fi lc kz l la lb">texts_dict.filter_extremes(no_below=20, no_above=0.10) <br/>print(sorted(texts_dict.token2id.items(), key=operator.itemgetter(1), reverse = False)[:10])</span></pre><p id="676d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第五步:将字典转换成称为语料库的单词包</p><p id="f0c7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">单词包格式是(token_id，token_count)元组列表)。语料库的长度为1689188</p><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="698d" class="jo jp ht ku b fi ky kz l la lb"># Step 5: Converting the dictionary to bag of words calling it corpus here<br/>corpus = [texts_dict.doc2bow(text) for text in texts]<br/>len(corpus)</span><span id="31a3" class="jo jp ht ku b fi lc kz l la lb">#Save a corpus to disk in the sparse coordinate Matrix Market format in a serialized format instead of random<br/>gensim.corpora.MmCorpus.serialize('amzn_elec_review.mm', corpus)</span></pre><p id="5e3d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">主题的数量是随机的，可以根据亚马逊通常放置其产品的类别来确定:<br/> 1。电脑—配件<br/> 2。电视&amp;视频<br/> 3。手机&amp;配件<br/> 4。摄影&amp;摄像<br/> 5。家用音响<br/> 6。亚马逊设备<br/> 7。耳机<br/> 8。办公电子<br/> 9。办公用品<br/> 10。智能家居<br/> 11。乐器<br/> 12。视频游戏</p><p id="b666" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">步骤6:拟合LDA模型以评估主题</p><p id="432f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们知道LDA是一种无监督的机器学习方法，但它是什么呢？</p><blockquote class="ld le lf"><p id="e775" class="iq ir lg is b it iu iv iw ix iy iz ja lh jc jd je li jg jh ji lj jk jl jm jn hb bi translated"><a class="ae ko" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" rel="noopener ugc nofollow" target="_blank">潜在狄利克雷分配(LDA)，一种用于文本语料库等离散数据集合的生成概率模型。LDA是一个三级分层贝叶斯模型，其中集合中的每个项目都被建模为一组底层主题的有限混合。反过来，每个主题都被建模为一组潜在主题概率的无限混合物。在文本建模的背景下，主题概率提供了文档的显式表示</a></p></blockquote><pre class="kp kq kr ks fd kt ku kv kw aw kx bi"><span id="eb09" class="jo jp ht ku b fi ky kz l la lb">#Step 6: Fit LDA model<br/>lda_model = gensim.models.LdaModel(corpus,alpha='auto', num_topics=5,id2word=texts_dict, passes=20)</span><span id="7a8c" class="jo jp ht ku b fi lc kz l la lb">#Choosing the number of topics based on various categories of electronics on Amazon<br/>lda_model.show_topics(num_topics=5,num_words=5)</span><span id="0219" class="jo jp ht ku b fi lc kz l la lb">raw_query = 'portable speaker'</span><span id="2abf" class="jo jp ht ku b fi lc kz l la lb">query_words = raw_query.split()<br/>query = []<br/>for word in query_words:<br/>    # ad-hoc reuse steps from above<br/>    q_tokens = tokenizer.tokenize(word.lower())<br/>    q_stopped_tokens = [word for word in q_tokens if not word in nltk_stpwd]<br/>    q_stemmed_tokens = [sb_stemmer.stem(word) for word in q_stopped_tokens]<br/>    query.append(q_stemmed_tokens[0])<br/>    <br/>print(query)</span><span id="668e" class="jo jp ht ku b fi lc kz l la lb"># Words in query will be converted to ids and frequencies  <br/>id2word = gensim.corpora.Dictionary()<br/>_ = id2word.merge_with(texts_dict) # garbage</span><span id="00d6" class="jo jp ht ku b fi lc kz l la lb"># Convert this document into (word, frequency) pairs<br/>query = id2word.doc2bow(query)<br/>print(query)</span><span id="2e87" class="jo jp ht ku b fi lc kz l la lb">#Create a sorted list<br/>sorted_list = list(sorted(lda_model[query], key=lambda x: x[1]))<br/>sorted_list</span><span id="19f6" class="jo jp ht ku b fi lc kz l la lb">#Assessing least related topics<br/>lda_model.print_topic(a[0][0]) #least related</span><span id="4066" class="jo jp ht ku b fi lc kz l la lb">#Assessing most related topics<br/>lda_model.print_topic(a[-1][0]) #most related</span><span id="87c8" class="jo jp ht ku b fi lc kz l la lb">"""</span><span id="9a48" class="jo jp ht ku b fi lc kz l la lb">'0.025*"speaker" + 0.015*"headphon" + 0.013*"music" + 0.013*"bluetooth" + 0.009*"phone" + 0.009*"ear" + 0.009*"8217" + 0.009*"volum" + 0.009*"audio" + 0.008*"pair"'</span><span id="0fc5" class="jo jp ht ku b fi lc kz l la lb">"""</span></pre><p id="0289" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后一步:以上是与一个主题相关的前5个单词。每个单词旁边的浮动是权重，显示给定单词对特定主题的影响程度。我们可以解释为，这里的主题可能接近亚马逊的耳机类别，该类别有各种子类别:“入耳式耳机、耳挂式耳机、入耳式耳机、蓝牙耳机、运动和健身耳机、降噪耳机”</p><p id="d657" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">源代码可以在<a class="ae ko" href="https://github.com/Anjalisk/Topic-Modeling-/blob/master/TopicModeling.py" rel="noopener ugc nofollow" target="_blank"> <strong class="is hu"> Github </strong> </a>上找到。</p></div></div>    
</body>
</html>