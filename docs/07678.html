<html>
<head>
<title>Understanding Transfer Learning as a methodology for efficiency in training CNN’s models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解迁移学习作为CNN模型训练效率的方法论</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-transfer-learning-as-an-methodology-for-efficiency-in-training-cnns-models-1322d316173a?source=collection_archive---------21-----------------------#2020-07-03">https://medium.com/analytics-vidhya/understanding-transfer-learning-as-an-methodology-for-efficiency-in-training-cnns-models-1322d316173a?source=collection_archive---------21-----------------------#2020-07-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/f226473beece83eae35511c2495ebe7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*QmFlFcufVrSZ7gZwsjJ_tg.png"/></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图像分类。图片鸣谢:Gluon-cv.mxnet.io</figcaption></figure><div class=""/></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="c623" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">摘要</h1><p id="32e3" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">使用大型数据集训练ConvNet模型可能需要几个小时，其性能可能会受到应用训练的方法的影响。对于这种情况，一种称为迁移学习的技术开始在ConvNets架构中实施，以提高预训练模型到新模型的实施的时间性能和准确性。迁移学习是一种应用于任何神经网络架构的范式，但在计算机视觉领域，这种技术已经证明了有趣的结果。接下来的论文将通过使用CFAR-10数据集探索迁移学习实现的性能，在DenseNet-121架构中训练评估。在迁移学习过程中实施的技术是微调，冻结整个DenseNet-121架构，以及使用<em class="kt">批量标准化</em>、<em class="kt">亚当</em>算法<em class="kt">优化</em>、<em class="kt">辍学</em>和<em class="kt">学习率</em>对该架构的修改。</p><blockquote class="ku kv kw"><p id="66dc" class="jv jw kt jx b jy kx ka kb kc ky ke kf kz la ki kj lb lc km kn ld le kq kr ks hb bi translated">关键词:迁移学习，DenseNet-121，批量标准化，微调，准确性，损失</p></blockquote></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="07a4" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">介绍</h1><p id="701b" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">卷积神经网络是用于机器学习或人工智能的神经网络领域中的一个类别。ConvNets是用于图像识别和分类的高效架构。因为计算机科学中的神经网络是人脑的功能模式的模拟，并且ConvNets架构类似于人眼的图像识别和分类的视觉皮层，所以在机器学习领域中，为了复制这些ConvNets架构，使用大数据集(图像)来获得该算法的良好响应。大型数据集的训练可能需要数小时，并且性能可能会受到应用训练的方法的影响。</p><p id="300d" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">这些机器学习架构通常需要静态环境下的大量数据。但是，在现实环境中，模式是用户与偏好(权重、参数、偏好)进行交互，使架构成为一个动态模型(Widmer <em class="kt">等人</em>)。1996).针对这种情况，一种称为迁移学习的技术开始实施，并且实际上经常用于ConvNets架构中，以提高预训练模型到新模型的实施的时间性能和准确性(Gulli <em class="kt">等人</em>)。2017) .</p><p id="b2a3" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">迁移学习是一种范式，通过获取预先训练的模型的知识，一个相关任务的解决方案将与新模型兼容。迁移学习最初被讨论并分为三种不同的设置:归纳迁移学习、直推迁移学习和无监督迁移学习(Pan，<em class="kt"> et al. </em> 2009)。然而，随着时间的推移，迁移学习技术因直观性和增强性而得到了改进</p><p id="fab6" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">在本文中，我们将利用CIFAR-10数据集，探讨迁移学习技术在DenseNet-121架构实现中的行为。这种架构保持了一种简单的连接模式，通过将所有层(具有匹配的要素地图大小)直接相互连接来寻求网络中各层之间的最大信息流。为了保持前馈性质，每一层从所有前面的层获得额外的输入，并将它自己的特征映射传递给所有后面的层。(黄<em class="kt">等</em>。2016)</p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lf"><img src="../Images/47119b9ccd9e099c59a498e0d045aa19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CTbsjdm6f1ESaDYrpWpZmw.png"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图一。DenseNet表示。图片来源:黄、等。2016</figcaption></figure><p id="0c2b" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">目前有不同的方法来实现神经网络中的迁移学习，如数据扩充、微调、冻结、多任务、一次性学习、域适应、域混淆和零次学习。没有准确或理想的技术，但在每一种实施的迁移学习技术中都有行为上的改变。本文将介绍一个在预训练的densenet-121架构中使用微调和冻结技术的实施方案，旨在了解CIFAR-10数据集中10000个样本的有效数据集的准确性和丢失方面的时间响应和性能。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="2d09" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">技术</h1><h2 id="803b" class="lp iy ht bd iz lq lr ls jd lt lu lv jh kg lw lx jl kk ly lz jp ko ma mb jt mc bi translated">迁移学习—微调</h2><p id="fba6" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">迁移学习满足了终身机器学习方法的需要，这种方法可以保留和重用以前学到的知识(Pan，<em class="kt"> et al. </em> 2009)。图2比较了传统的学习过程和应用迁移学习的过程。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es md"><img src="../Images/dcb66debdec98380c86920316c43ee95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vz1d1ddPa0VGwVwiznpjXw.png"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图二。迁移学习表征。左(传统机器学习)，右(迁移学习)。图片来源:(潘，<em class="lo">等</em> 2009)</figcaption></figure><p id="92b7" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">基于这一思想，微调补充了迁移学习方法，因为机器学习模型经过一个过程，在该过程中，模型的所有层或一些层被冻结，这是必要的，因此可以在顶层用随机初始化的新分类器初始化训练，但是冻结的层仍然包含参数，并且将支持特征的提取。</p><p id="7fd2" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">实现微调的迁移学习技术调整被重用的模型的更抽象的表示，因此在对新的训练参数的预测中有可重用性和效率。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="d50e" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">实验</h1><p id="d4d5" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">该实验包括将ConvNet置于三个独立的训练过程中。第一种是对整个模型进行常规训练，第二种是添加优化和正则化技术，第三种是应用迁移学习，冻结模型中的所有层，并对最后几层进行微调，以对数据集的验证数据进行训练。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es me"><img src="../Images/a5bbb53d7372ab35395c920642aa51a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r_Vunmr4QYSc4S7l6O0r_A.jpeg"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图3。Densenet 121蓝色(冻结层)，绿色(微调)。图片来源:作者来源</figcaption></figure><h2 id="d875" class="lp iy ht bd iz lq lr ls jd lt lu lv jh kg lw lx jl kk ly lz jp ko ma mb jt mc bi translated">资料组</h2><p id="5adb" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">为了观察迁移学习的行为，选择了CIFAR-10数据集，其包含10个类中的60000个32×32彩色图像，每个类6000个图像。有50000个训练图像和10000个测试图像。这个复杂的数据集被设置为在DenseNet-121架构上进行训练(黄<em class="kt">等人</em>)。2016).</p><h2 id="e688" class="lp iy ht bd iz lq lr ls jd lt lu lv jh kg lw lx jl kk ly lz jp ko ma mb jt mc bi translated">培养</h2><p id="1410" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">所有的模型都预先进行了预处理，它们的输入以(155，155)的大小进行了整形。用于训练的优化器是使用128个批次和32个时期的ADAM。学习率介于0和1之间，早期停止为1e-05。两种架构在分类层中的softmax层之前还具有两个批量标准化层，并且这两个层分别具有256和128的输出空间维度。在最后一批标准化后，最后一个激活层中的最终丢弃值为0.2。ConvNet的基本模型共有6，964，106个可训练参数；在分类器层中具有优化技术的基础模型具有7，252，746个可训练参数，而具有微调实现的ConvNet模型具有4，262，026个可训练参数。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mf"><img src="../Images/9cf30fbde019b98e2d6466075ef98675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R6v0-pGpZXbJNUS6d8pnNg.png"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图4。DenseNet 121基本型号</figcaption></figure><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mf"><img src="../Images/97a2ccbf2da93f555d99d7fb659ca582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cNjqdjK5A11lLIokyFBNRw.png"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图5。DenseNet 121基本模型在分类器层中增加了优化删除和批量标准化</figcaption></figure><p id="4295" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">对于迁移学习训练，实现的工作流是将模型带到分类层，冻结它们，以避免在未来的训练回合中破坏它们包含的任何信息，然后添加未冻结的和可训练的分类层。这些最后的图层将学习将旧要素转换为新数据集的预测。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/11c80d33041c80b47314da98ad5b4e1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*9K-Dhmz3EII7lJlh4ojrqw.png"/></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图6。实施DenseNet 121微调</figcaption></figure></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="0e43" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">结果</h1><p id="62ff" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">两个模型表现相似。差异很小，这是由于数据集的大小。而这个数据集包含大量信息。数据集的可训练样本越多，差异就越大。</p><p id="f010" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated"><strong class="jx hu">准确无误。</strong>应用了优化的基础模型具有95.35%的准确率，比迁移学习模型表现得好得多。该模型的准确率为92.72%，有趣的是，如图4所示，该模型在学习过程中具有稳定的趋势。基本模型具有不同的行为，其中学习过程在时期内增加了准确率，直到时期19达到最高结果。最后，没有优化参数的基础模型执行了86.59%，可能是因为没有应用优化技术。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mh"><img src="../Images/bf3ba2d3eb80add3c17b440aba0266ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a67Js7wyzO7uqhf6JZE6oQ.png"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图7。DenseNet 121基本型号有效精度</figcaption></figure><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mi"><img src="../Images/8b1ac3d68961eb99a0a5b664bdd9028c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yMz14zWCubkQt_A0GEAd8g.png"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图8。DenseNet 121基本模型传输学习有效精度</figcaption></figure><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/61974e11a840ae5af23ffeac9f7958b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*Hv0CEl2YxiiCRcajowoP_Q.png"/></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图9。DenseNet 121基本型号(未应用优化)有效精度</figcaption></figure><p id="b097" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated"><strong class="jx hu">时间。</strong>三个模型中的每一个的损失都是要跟踪的重要特征。微调模型花费了44.10分钟，带有优化器的基本模型花费了139.5分钟。标准模型需要13.8分钟才能完全训练好。微调技术比没有标准化的基本模型快68.38%，比没有优化技术的基本训练模型慢68.70%。然而，当权衡准确性与时间时，迁移学习模型的表现要好得多。</p><p id="ab35" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated"><strong class="jx hu">损耗和过度拟合。</strong>在所有三个实验中，DenseNet模型表现有效，不容易过度拟合。至于损失，与基本模型的验证精度相关的样品的损失验证保持在65%的比率。与具有微调实现的模型相比，损失好48.2%，验证样本中的损失为33.67%。然而，没有迁移学习但用批量标准化优化的DenseNet模型作为训练的最佳模型损失了16.29%</p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mk"><img src="../Images/2dba3aa72bee770efc8567b6d3d8a017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8nwI6hJtE2ScSNo1OpFYA.png"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图10。DenseNet 121基本模型(应用优化)有效损耗</figcaption></figure><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es ml"><img src="../Images/de2da5b01c40aade82861e992866897e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1c2GMcoxzEfBO-D6AVijWw.png"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图11。DenseNet 121基础模型(应用迁移学习)有效损失</figcaption></figure><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mm"><img src="../Images/c15c1e4fa3f50bd87ff5da8c0909f596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Elso676kaqKLdKIyKZ2qfw.png"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">图12。DenseNet 121基本型号(未应用优化)有效损耗</figcaption></figure><h1 id="7688" class="ix iy ht bd iz ja mn jc jd je mo jg jh ji mp jk jl jm mq jo jp jq mr js jt ju bi translated">讨论</h1><p id="126e" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">正如我们在本文开头提到的，我们希望了解DenseNet 121 ConvNet架构的传统学习在传统学习、优化学习和迁移学习环境中的性能。计算机视觉领域的ConvNets架构需要出色的性能来解决需要评估图像中捕获的信息的适用模拟。这导致了性能和时间的问题，迁移学习可以在不牺牲准确性或模型损失的情况下提高性能和时间。正如我们所见，训练一个基本的ConvNet模型需要一系列步骤来避免过拟合、欠拟合或性能精度方面的问题。批量标准化被证明是一个很好的选择，可以提高ConvNet在验证数据中呈现95%的模型训练准确性。当应用迁移学习时，性能会有轻微的降低，但是时间执行确实为采用迁移学习方法创造了价值。</p><h1 id="52c0" class="ix iy ht bd iz ja mn jc jd je mo jg jh ji mp jk jl jm mq jo jp jq mr js jt ju bi translated">结论</h1><p id="4af0" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我们知道，通过添加微调进行迁移学习是在ConvNets中实施的一个很好的选择，ConvNets习惯于在目标任务中使用不同标签的方法。由于实验使用的是CIFAR-10，因此也可以得出结论，迁移学习模型需要模式数据来提高其性能。此外，在基本模型和增强模型之间，显而易见的是，DenseNet倾向于通过大量的可训练参数来提高其精度性能，但为了避免数据过度拟合的风险，批量标准化是在该模型中实施的一个极好的工具。ConvNets目前广泛用于仿真，需要训练的过量数据不仅需要良好的精度，还需要时间性能。因此，通过面对对象源与要执行的模拟的概率不同的模拟问题，在复杂性训练中产生了挑战，dine-tuning方法能够通过冻结ConvNet模型的一些层(如果不是所有层的话)来处理该挑战，该conv net模型的一些层稍后将被解冻，并将该可训练参数带入新的模型训练中，从而同步两个相似的参数并获得准确的模拟。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="d830" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">参考</h1><p id="d90d" class="pw-post-body-paragraph jv jw ht jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">弗朗索瓦·乔莱。用Python进行深度学习。2017</p><p id="5a26" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">格哈德·威德默和米罗斯拉夫·库巴特。在概念漂移和隐藏环境中学习。机器学习，69–101，1996</p><p id="2119" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">安东尼奥·古利和苏吉特·帕尔。用Keras进行深度学习。在Theano和TensorFlow上使用Keras实现神经网络，204，2017</p><p id="7af6" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">黄高，刘庄，劳伦斯·范·德·马滕，基利安·q·温伯格。<a class="ae ms" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener ugc nofollow" target="_blank">密集连接的卷积网络</a>，2016</p><p id="deb2" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">辛诺佳林潘和杨强。<a class="ae ms" href="https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf" rel="noopener ugc nofollow" target="_blank">迁移学习调查</a>，2009</p><p id="b799" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">常青科技。<a class="ae ms" rel="noopener" href="/@evergreenllc2020/dog-breed-image-classification-using-transfer-learning-6d19699d4351">利用迁移学习进行犬种图像分类</a>，2020</p><p id="0d94" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">塞巴斯蒂安·鲁德。转移学习— <a class="ae ms" href="https://ruder.io/transfer-learning/" rel="noopener ugc nofollow" target="_blank">机器学习的下一个前沿，</a> 2017</p><p id="5979" class="pw-post-body-paragraph jv jw ht jx b jy kx ka kb kc ky ke kf kg la ki kj kk lc km kn ko le kq kr ks hb bi translated">迪帕詹·萨卡尔。<a class="ae ms" href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a" rel="noopener" target="_blank">2018年，深度学习</a>中利用真实世界应用转移学习的综合实践指南</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="67ad" class="ix iy ht bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">附录</h1><h2 id="9e5a" class="lp iy ht bd iz lq lr ls jd lt lu lv jh kg lw lx jl kk ly lz jp ko ma mb jt mc bi translated">基本DenseNet模型(无增强，无迁移学习)</h2><div class="hh hi ez fb hj mt"><a href="https://github.com/edward0rtiz/holbertonschool-machine_learning/blob/master/supervised_learning/0x09-transfer_learning/0-transfer_basic.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hu fi z dy my ea eb mz ed ef hs bi translated">Edward 0 rtiz/holbertonschool-机器学习</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">github.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh hl mt"/></div></div></a></div><h2 id="f8df" class="lp iy ht bd iz lq lr ls jd lt lu lv jh kg lw lx jl kk ly lz jp ko ma mb jt mc bi translated">最佳DenseNet模型(在分类层中添加优化的基础模型)</h2><div class="hh hi ez fb hj mt"><a href="https://github.com/edward0rtiz/holbertonschool-machine_learning/blob/master/supervised_learning/0x09-transfer_learning/0-transfer.py" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hu fi z dy my ea eb mz ed ef hs bi translated">Edward 0 rtiz/holbertonschool-机器学习</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">在GitHub上创建一个帐户，为Edward 0 rtiz/holbertonschool-machine _ learning的开发做出贡献。</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">github.com</p></div></div><div class="nc l"><div class="ni l ne nf ng nc nh hl mt"/></div></div></a></div><h2 id="a4ba" class="lp iy ht bd iz lq lr ls jd lt lu lv jh kg lw lx jl kk ly lz jp ko ma mb jt mc bi translated">迁移学习DenseNet模型(在分类层中添加优化和微调的基本模型)</h2><div class="hh hi ez fb hj mt"><a href="https://github.com/edward0rtiz/holbertonschool-machine_learning/blob/master/supervised_learning/0x09-transfer_learning/0-transfer_fine_tuning.py" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hu fi z dy my ea eb mz ed ef hs bi translated">Edward 0 rtiz/holbertonschool-机器学习</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">在GitHub上创建一个帐户，为Edward 0 rtiz/holbertonschool-machine _ learning的开发做出贡献。</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">github.com</p></div></div><div class="nc l"><div class="nj l ne nf ng nc nh hl mt"/></div></div></a></div></div></div>    
</body>
</html>