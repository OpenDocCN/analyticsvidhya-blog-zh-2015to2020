<html>
<head>
<title>Deep Learning Recommendation Machines — DLRM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习推荐机器——DLRM</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-learning-recommendation-machines-dlrm-4fec2a5e7ef8?source=collection_archive---------0-----------------------#2019-07-25">https://medium.com/analytics-vidhya/deep-learning-recommendation-machines-dlrm-4fec2a5e7ef8?source=collection_archive---------0-----------------------#2019-07-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3fb8" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">深入了解脸书最先进的推荐网络</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/3261c6ea3f537d45d3e0cecf77a56af8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lNIfJN_MhW70o89Sj3tRyg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图片来源:<a class="ae jn" href="https://realpython.com/" rel="noopener ugc nofollow" target="_blank">https://realpython.com/</a></figcaption></figure><h1 id="6377" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">介绍</h1><p id="ec26" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">2019年3月31日，一群来自脸书的人发布了一篇<a class="ae jn" href="https://arxiv.org/abs/1906.00091" rel="noopener ugc nofollow" target="_blank">论文</a>，谈论一个最先进的<a class="ae jn" href="https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/" rel="noopener ugc nofollow" target="_blank">深度学习推荐和个性化模型(DLRM) </a>。他们在论文中提到，针对此类任务的深度学习与其他深度学习网络明显不同，因为它们需要处理分类特征，并且它们没有得到很好的研究或理解。</p><p id="f9bd" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这篇文章深入探讨了这个DLRM是如何工作的，它背后的机构，架构，它的优势以及DLRM的一个单独的<a class="ae jn" href="https://github.com/gotorehanahmad/Recommendation-Systems/blob/master/dlrm/dlrm_main.ipynb" rel="noopener ugc nofollow" target="_blank"> jupyter笔记本</a>实现。</p><p id="e195" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">在深入了解其背后的直觉之前，让我们先来谈谈目前部署个性化和推荐的几个地方。脸书使用<a class="ae jn" href="https://code.fb.com/core-data/recommending-items-to-more-than-a-billion-people/" rel="noopener ugc nofollow" target="_blank">推荐引擎</a>来推荐你可能喜欢的网页或者你应该加入的团体。网飞用一个推荐引擎向你建议，接下来看什么。亚马逊用它来建议还应该往购物车里添加什么。</p><p id="93c4" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">有很多互联网公司使用个性化和推荐系统来预测点击率。它基本上意味着预测用户是否会点击广告。因为点击广告意味着创收，所以对公司来说，选择最有可能和最相关的广告向用户展示以最大化收入是至关重要的。</p><p id="3b01" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">在论文中，他们选择了这个CTR预测用例来测试他们的结果。他们挑选的数据集来自一个名为Criteo的展示广告挑战的Kaggle挑战。本次挑战的目标是为CTR估算测试最精确的ML算法。</p><p id="f62e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">Criteo AI Labs Ad Kaggle和Terabyte数据集是开源数据集，由用于广告点击率预测的点击日志组成。每个数据集包含13个连续特征和26个分类特征。Criteo Ad Kaggle数据集包含7天内约4500万个样本。</p><p id="156d" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">在实验中，通常第7天分为验证集和测试集，而前6天用作训练集。在24天内对Criteo Ad数据集进行采样，其中第24天分为验证和测试集，前23天用作训练集。</p><h1 id="d0f6" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">直觉</h1><p id="f2f7" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">个性化和推荐系统在行业中并不新鲜。他们有很长的历史。但是直到最近他们才开始拥抱神经网络。该论文的作者认为，有两个主要的观点促成了这种体系结构的变化:</p><ol class=""><li id="0ea2" class="lh li hi ki b kj lc km ld kp lj kt lk kx ll lb lm ln lo lp bi translated">第一个来自推荐系统的观点。这些系统最初采用的内容过滤是一组专家将产品分类，而用户选择他们喜欢的类别，并根据他们的偏好进行匹配。该领域随后发展到使用协同过滤，其中推荐基于过去的用户行为，例如之前对产品的评级。通过将用户和产品分组来提供推荐的邻域方法和通过矩阵分解技术用某些隐含因素来表征用户和产品的潜在因素方法后来被成功部署。</li><li id="11b3" class="lh li hi ki b kj lq km lr kp ls kt lt kx lu lb lm ln lo lp bi translated">第二种观点来自预测分析，它依赖于统计模型，根据给定的数据对事件的概率进行分类或预测。预测模型从使用简单模型(如线性和逻辑回归)转变为包含深层网络的模型。为了处理分类数据，这些模型采用了嵌入的使用，这将一个和多个热点向量转换成抽象空间中的密集表示。这个抽象空间可以解释为推荐系统发现的潜在因素的空间。</li></ol><p id="b02f" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">在论文中，作者声称他们在DLRM模型中成功地统一了上述两种观点。该模型使用嵌入来处理代表分类数据的稀疏特征，使用多层感知器(MLP)来处理密集特征，然后使用中提出的统计技术显式地交互这些特征。最后，它通过后处理与另一个MLP的交互来发现事件概率。</p><p id="dc55" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">如果你是一个新手，你可能想知道为什么所有这些处理分类和数字特征的失败。答案是双重的:(1)嵌入是比典型的热点向量和多热点向量更丰富的数据表示，可以产生更好的结果;( 2)简单地通过采用一组数字和分类特征来拟合曲线不会给你带来准确性。出现这种情况的主要原因是功能之间缺少交互。</p><p id="f50d" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">交互是功能与其他功能的交互方式。一个例子是喜欢喜剧和恐怖电影的用户喜欢恐怖喜剧电影的可能性有多大。这种交互在推荐引擎的工作中起着重要的作用。因此，简单的线性或逻辑回归不能用于推荐系统。多项式回归将有这种相互作用的成分，但它不能预测看不见的相互作用。因式分解机(FM)通过定义以下形式的模型，将二阶相互作用合并到具有分类数据的线性模型中</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/105a4a7ebab93f80f83f6b3aedd5261e.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*vfi3FTbb_7vMvHqwaVHyMw.png"/></div></figure><p id="6441" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">FMs明显不同于具有多项式核的支持向量机(SVM ),因为它们像矩阵分解一样将二阶交互矩阵分解为其潜在因子(或嵌入向量),这更有效地处理稀疏数据。这通过仅捕获不同嵌入向量对之间的相互作用显著降低了二阶相互作用的复杂性，产生了线性计算复杂性。这些相互作用然后被输入到另一个多层感知器，以获得点击概率。</p><h1 id="28e1" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">DLRM网络</h1><p id="0375" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">这部分解释了DLRM建筑的高层次观点。为了更容易理解，让我们来看看DLRM涉及的四种主要技术，不需要太多的数学知识:</p><ol class=""><li id="a804" class="lh li hi ki b kj lc km ld kp lj kt lk kx ll lb lm ln lo lp bi translated"><strong class="ki hj">嵌入:</strong>嵌入是离散-分类-变量到连续数字向量的映射。<a class="ae jn" href="https://www.tensorflow.org/guide/embedding?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">在神经网络的背景下，嵌入</a>是离散变量的低维、学习的连续向量表示。神经网络嵌入是有用的，因为它们可以减少分类变量的维度，并在转换的空间中有意义地表示类别。</li><li id="c2e4" class="lh li hi ki b kj lq km lr kp ls kt lt kx lu lb lm ln lo lp bi translated"><strong class="ki hj">矩阵分解</strong>:矩阵分解是<a class="ae jn" href="https://en.wikipedia.org/wiki/Recommender_systems" rel="noopener ugc nofollow" target="_blank">推荐系统</a>中使用的一类<a class="ae jn" href="https://en.wikipedia.org/wiki/Collaborative_filtering" rel="noopener ugc nofollow" target="_blank">协同过滤</a>算法。矩阵分解算法通过将用户-项目交互矩阵分解成两个低维度矩形矩阵的乘积来工作。</li><li id="f09b" class="lh li hi ki b kj lq km lr kp ls kt lt kx lu lb lm ln lo lp bi translated"><strong class="ki hj">因式分解机</strong>:因式分解机是一种通用的监督学习算法，可以用于分类和回归任务。它是线性模型的扩展，旨在经济地捕捉高维稀疏数据集中要素之间的交互。例如，在点击预测系统中，因子分解机器模型可以捕捉当来自某个广告类别的广告被放置在来自某个页面类别的页面上时观察到的点击率模式。因子分解机器对于处理高维稀疏数据集的任务是一个很好的选择，比如点击预测和商品推荐。</li><li id="98cc" class="lh li hi ki b kj lq km lr kp ls kt lt kx lu lb lm ln lo lp bi translated"><strong class="ki hj">多层感知器:</strong>多层感知器(MLP)是一类前馈人工神经网络。MLP至少由三层节点组成:输入层、隐藏层和输出层。除了输入节点之外，每个节点都是使用非线性激活函数的神经元。MLP利用一种称为反向传播的监督学习技术进行训练。它的多层和非线性激活将MLP与线性感知器区分开来。它可以区分不可线性分离的数据。</li></ol><p id="bbf0" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">现在结合上述技术，我们可以很容易地理解DLRM建筑。</p><p id="0b4e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">让用户和产品被许多连续的和分类的特征所描述。为了处理分类特征，每个分类特征将由相同维数的嵌入向量表示，推广了矩阵分解中使用的潜在因子的概念。为了处理连续特征，连续特征将通过MLP(我们称之为底部或密集MLP)进行变换，这将产生与嵌入向量相同长度的密集表示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/3774e54cb0c41533e57a61ac58f45231.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*omX24odCuNODKS8ZpoRhWw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">深度学习推荐机</figcaption></figure><p id="5a25" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">不同特征的二阶相互作用的计算是显式完成的，遵循FMs中提供的处理稀疏数据的直觉，可选地通过MLPs传递它们。这是通过取所有嵌入向量对和处理过的密集特征之间的点积来完成的。这些点积与原始处理的密集要素连接在一起，并使用另一个MLP(顶部或输出MLP)进行后处理，然后输入到sigmoid函数中以给出概率。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/5cb3ab1a6eaec119a67860d4569c973d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BW8avliDkPrWQNK9zpT90w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">DLRM摘要</figcaption></figure><h1 id="906e" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">基准</h1><p id="4517" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">作者在Criteo Ad Kaggle数据集上评估了该模型的准确性，并将DLRM的性能与未经广泛调整的深度交叉网络(DCN)进行了比较。请注意，在这种情况下，模型的大小可以容纳数据集中的要素数量。</p><ul class=""><li id="e053" class="lh li hi ki b kj lc km ld kp lj kt lk kx ll lb ly ln lo lp bi translated">特别地，DLRM包括用于处理密集特征的底部MLP和顶部MLP，底部由分别具有512、256和64个节点的三个隐藏层组成，顶部由具有512和256个节点的两个隐藏层组成。</li><li id="f786" class="lh li hi ki b kj lq km lr kp ls kt lt kx lu lb ly ln lo lp bi translated">另一方面，DCN由六个交叉层和一个具有512和256个节点的深度网络组成。使用16的嵌入维数。请注意，这会产生一个DLRM和DCN，两者都有大约540米的参数。</li></ul><p id="041e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">对于使用SGD和Adagrad优化器的两个模型，训练(实线)和验证(虚线)精度都是在完整的单个训练时期内绘制的。没有使用正则化。在这个实验中，DLRM获得了略高的训练和验证精度。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/85e1f6068f20eada239f7881478d9abb.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*y80yDRtjz9diVlWb5KLNHw.png"/></div></figure><h1 id="871d" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">先决条件和代码指南</h1><p id="0e97" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated"><strong class="ki hj">软件需求</strong>:</p><p id="a332" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><em class="lz">pytorch-nightly(2019年6月10日)</em></p><p id="3b10" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><em class="lz"> onnx(可选)</em></p><p id="f8be" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><em class="lz"> torchviz(可选)</em></p><p id="2691" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj"> Dataset </strong> : Dataset可以在底部提供的GitHub repo中找到。也可以选择从<a class="ae jn" href="https://labs.criteo.com/2014/09/kaggle-contest-dataset-now-available-academic-use/" rel="noopener ugc nofollow" target="_blank">https://labs . criteo . com/2014/09/ka ggle-contest-dataset-now-available-academic-use/</a>获取</p><p id="927f" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj">硬件要求</strong> : DLRM是内存密集型和计算密集型。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/d33828ba2bfd44cc318d9d7a726c81a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QMnNohmHs_C9EqW1u43Gkw.png"/></div></div></figure><p id="5d97" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">本<a class="ae jn" href="https://www.youtube.com/watch?v=DFrCEvPgEcQ" rel="noopener ugc nofollow" target="_blank">开放计算项目峰会主题演讲</a>中详细讨论了该模型的示意图</p><p id="944a" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">Criteo数据集的首选硬件是:</p><blockquote class="mb mc md"><p id="fdb2" class="kg kh lz ki b kj lc ij kl km ld im ko me le kr ks mf lf kv kw mg lg kz la lb hb bi translated">CPU —超过8个内核</p><p id="3d62" class="kg kh lz ki b kj lc ij kl km ld im ko me le kr ks mf lf kv kw mg lg kz la lb hb bi translated">GPU —推荐</p><p id="ef9c" class="kg kh lz ki b kj lc ij kl km ld im ko me le kr ks mf lf kv kw mg lg kz la lb hb bi translated">Ram — 64GB(小于64GB对criteo数据集无效)</p></blockquote><p id="62f2" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">如果您有硬件限制，作者可选地创建了两种运行训练的其他方法，一种使用随机生成的数据，另一种使用具有隐含趋势的合成数据。您可以选择类型作为代码中的参数。使用<strong class="ki hj">data _ generation:" random "/" synthetic "</strong>生成更小的数据集，并运行训练循环进行实验。</p><p id="4246" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">在Criteo数据集上运行需要大量的时间，我建议清理任何正在运行的任务，让它通宵训练。如果您是第一次运行，请将train.txt文件路径指定给"<strong class="ki hj"> raw_data_file </strong>"变量，它将运行一系列预处理任务并生成表示第1天到第7天的文件，最后一步，它将所有单个预处理文件整理成一个大型最终预处理文件。从下一次开始，您可以在<strong class="ki hj">“processed _ data _ file”</strong>变量中给出这个大型文件的路径，以跳过预处理步骤进行任何进一步的实验。</p><h1 id="355e" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">笔记</h1><p id="ec87" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated"><strong class="ki hj">注1 </strong>:我建议你写一些效用函数，把分类变量转换成带有索引和偏移量表示的稀疏特征。这对于您自己的数据集来说很方便，因为DLRM只接受稀疏表示作为分类变量的输入。</p><p id="0699" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj">注2 </strong>:这是一个复杂的实现，并且不是很直观。因此，我建议您在动手之前仔细阅读几次代码。按照算法，通过一个数据点的玩具例子。相信我，这很有帮助。</p><p id="13b4" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj">注3 </strong>:由于DLRM是计算密集型的，本文还讨论了并行处理模型训练的并行性。但是这超出了本文的范围，也没有讨论。</p><p id="5738" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj">Github Repo</strong>:<a class="ae jn" href="https://github.com/gotorehanahmad/Recommendation-Systems/tree/master/dlrm" rel="noopener ugc nofollow" target="_blank">https://Github . com/gotorehanahmad/Recommendation-Systems/tree/master/dlrm</a></p><h1 id="61dc" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">参考</h1><p id="3e04" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">1.<a class="ae jn" href="https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/" rel="noopener ugc nofollow" target="_blank">https://ai . Facebook . com/blog/dlrm-an-advanced-开源-深度学习-推荐-模型/ </a></p><p id="b6d1" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">2.<a class="ae jn" rel="noopener" href="/humansforai/recommendation-engines-e431b6b6b446">https://medium . com/humansforai/recommendation-engines-e 431 b 6 b 6 b 446</a></p><p id="00ba" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">3.<a class="ae jn" href="https://code.fb.com/core-data/recommending-items-to-more-than-a-billion-people/" rel="noopener ugc nofollow" target="_blank">https://code . FB . com/core-data/向超过10亿人推荐商品/ </a></p><p id="e52b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">4.<a class="ae jn" href="https://www.analyticsvidhya.com/blog/2018/01/factorization-machines/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/01/factorization-machines/</a></p><p id="891f" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">5.<a class="ae jn" href="https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf" rel="noopener ugc nofollow" target="_blank">https://CSE web . ucsd . edu/classes/fa17/CSE 291-b/reading/rendle 2010fm . pdf</a></p><blockquote class="mb mc md"><p id="1232" class="kg kh lz ki b kj lc ij kl km ld im ko me le kr ks mf lf kv kw mg lg kz la lb hb bi translated"><em class="hi">关于我</em></p></blockquote><p id="04e5" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">我是<a class="ae jn" href="https://wavelabs.ai/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Wavelabs.ai </a>的资深机器学习专家。我们Wavelabs帮助您利用人工智能(AI)来彻底改变用户体验并降低成本。我们使用人工智能独特地增强您的产品，以达到您的全部市场潜力。我们试图将尖端研究引入您的应用中。看看我们。</p></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><p id="96aa" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">嗯，这些都在这个博客里。感谢阅读:)</p><p id="c54c" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">保持好奇！</p><p id="4b2c" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">你可以通过<a class="ae jn" href="https://www.linkedin.com/in/rehan-a-18675296?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。</p></div></div>    
</body>
</html>