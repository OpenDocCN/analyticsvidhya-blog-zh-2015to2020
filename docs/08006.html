<html>
<head>
<title>Techniques of Feature Selection in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的特征选择技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-introduction-to-feature-selection-in-machine-learning-9d6f2d5e47?source=collection_archive---------14-----------------------#2020-07-14">https://medium.com/analytics-vidhya/an-introduction-to-feature-selection-in-machine-learning-9d6f2d5e47?source=collection_archive---------14-----------------------#2020-07-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6b6f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">面临过拟合和低精度的问题？特征选择来拯救</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/e90ee68e6b25c38f26082b657d7b38f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4u0tDZP8pAvzWVuq"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">亨特·哈里特在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="be54" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">降维</h1><ul class=""><li id="2e88" class="kg kh hi ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">降维是减少数据集中可用特征集的过程。</li><li id="5b9d" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">该模型不能直接应用于整个要素集，这可能会导致虚假预测和概化问题，进而使模型变得不可靠。</li><li id="fe56" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">为了防止这些问题，应用了降维。</li></ul><h1 id="8dd7" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">需要降维</h1><p id="0751" class="pw-post-body-paragraph ld le hi ki b kj kk ij lf kl km im lg kn lh li lj kp lk ll lm kr ln lo lp kt hb bi translated">降维可以防止过度拟合。</p><ul class=""><li id="d130" class="kg kh hi ki b kj lq kl lr kn ls kp lt kr lu kt ku kv kw kx bi translated">过度拟合是指模型记忆了数据而无法进行归纳。灵活的模型(如决策树)和高维数据也可能导致过度拟合。</li><li id="8fde" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">由于泛化的问题，过度拟合的模型不能应用于现实世界的问题。</li></ul><h1 id="dc0e" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">降维的类型</h1><ul class=""><li id="07ab" class="kg kh hi ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ki hj">特征选择</strong>:特征选择方法试图通过丢弃最不重要的特征来减少特征。</li><li id="58d7" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated"><strong class="ki hj">特征提取</strong>:特征提取方法试图通过组合特征并将其转换为指定数量的特征来减少特征。</li></ul><h1 id="8805" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">特征选择</h1><ol class=""><li id="ad3d" class="kg kh hi ki b kj kk kl km kn ko kp kq kr ks kt lv kv kw kx bi translated">过滤方法</li><li id="bb33" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt lv kv kw kx bi translated">包装方法</li><li id="c41b" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt lv kv kw kx bi translated">嵌入式方法</li><li id="4c19" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt lv kv kw kx bi translated">特征重要性</li></ol><h1 id="e15a" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">导入所需的库</h1><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lw lx l"/></div></figure><h1 id="2d3b" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">加载预处理的数据</h1><p id="4d4b" class="pw-post-body-paragraph ld le hi ki b kj kk ij lf kl km im lg kn lh li lj kp lk ll lm kr ln lo lp kt hb bi translated">训练数据已经过预处理。所涉及的预处理步骤是，</p><ol class=""><li id="952d" class="kg kh hi ki b kj lq kl lr kn ls kp lt kr lu kt lv kv kw kx bi translated">老鼠归罪</li><li id="4cff" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt lv kv kw kx bi translated">对数变换</li><li id="ff50" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt lv kv kw kx bi translated">平方根变换</li><li id="1254" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt lv kv kw kx bi translated">顺序编码</li><li id="53e4" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt lv kv kw kx bi translated">目标编码</li><li id="4654" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt lv kv kw kx bi translated">z分数标准化</li></ol><p id="4fd2" class="pw-post-body-paragraph ld le hi ki b kj lq ij lf kl lr im lg kn ly li lj kp lz ll lm kr ma lo lp kt hb bi translated">关于上述步骤的详细实现，请参考我的Kaggle数据预处理笔记本:</p><p id="2d1e" class="pw-post-body-paragraph ld le hi ki b kj lq ij lf kl lr im lg kn ly li lj kp lz ll lm kr ma lo lp kt hb bi translated"><a class="ae jn" href="https://www.kaggle.com/srivignesh/data-preprocessing-for-house-price-prediction" rel="noopener ugc nofollow" target="_blank">笔记本链接</a></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lw lx l"/></div></figure><h1 id="006a" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">特征选择</h1><h1 id="2f62" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">1.过滤方法</h1><p id="37be" class="pw-post-body-paragraph ld le hi ki b kj kk ij lf kl km im lg kn lh li lj kp lk ll lm kr ln lo lp kt hb bi translated">过滤方法选择独立于所用模型的特征。它可以使用以下方法来选择一组有用的特征，</p><ul class=""><li id="0d9e" class="kg kh hi ki b kj lq kl lr kn ls kp lt kr lu kt ku kv kw kx bi translated">数字列的相关性</li><li id="c70c" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">类别列的Chi2关联</li></ul><h1 id="9469" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">在sci-kit learn中选择K最佳</h1><p id="ce1b" class="pw-post-body-paragraph ld le hi ki b kj kk ij lf kl km im lg kn lh li lj kp lk ll lm kr ln lo lp kt hb bi translated"><strong class="ki hj">F _回归:</strong></p><p id="9252" class="pw-post-body-paragraph ld le hi ki b kj lq ij lf kl lr im lg kn ly li lj kp lz ll lm kr ma lo lp kt hb bi translated">F_Regression用于数值变量。f _回归由两个步骤组成:</p><ul class=""><li id="b106" class="kg kh hi ki b kj lq kl lr kn ls kp lt kr lu kt ku kv kw kx bi translated">使用每个特征与目标计算相关性。</li><li id="f8e2" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">然后，这种相关性被转换成F值，再转换成p值。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/2ff870d60226b03a843ad7324505c6d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TyJFq-lLIcSSWeR06labmA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">相关公式</figcaption></figure><p id="02a2" class="pw-post-body-paragraph ld le hi ki b kj lq ij lf kl lr im lg kn ly li lj kp lz ll lm kr ma lo lp kt hb bi translated"><strong class="ki hj"> Chi2: </strong></p><ul class=""><li id="3718" class="kg kh hi ki b kj lq kl lr kn ls kp lt kr lu kt ku kv kw kx bi translated">Chi2用于测试分类变量之间的关联。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mc"><img src="../Images/fe9abd2e61cad8ef4ce291679c56647d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AgFUo5da5JgUmlRemjO9kg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Chi2公式</figcaption></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lw lx l"/></div></figure><h1 id="701c" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">2.包装方法</h1><p id="89c9" class="pw-post-body-paragraph ld le hi ki b kj kk ij lf kl km im lg kn lh li lj kp lk ll lm kr ln lo lp kt hb bi translated">包装器方法利用估计器来选择一组有用的特性。可用的技术有，</p><ul class=""><li id="3163" class="kg kh hi ki b kj lq kl lr kn ls kp lt kr lu kt ku kv kw kx bi translated">递归特征消除</li><li id="85da" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">递归特征消除交叉验证</li></ul><h1 id="e72f" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">递归特征消除(RFE)</h1><ul class=""><li id="5bb8" class="kg kh hi ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">提供给RFE的估计器将权重分配给特征(例如，系数)，RFE递归地消除被分配了低权重的特征子集。</li><li id="27ca" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">用初始特征集训练估计器。估计器可能具有coef_或feature _ importances _等属性。利用估计量属性，我们可以找到每个特征的权重。</li><li id="d8fd" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">从当前要素集中移除权重最小的要素。在移除的集合上重复该过程，直到最终达到要选择的指定数量的特征。</li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lw lx l"/></div></figure><h1 id="ad51" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">递归特征消除交叉验证</h1><ul class=""><li id="1ee2" class="kg kh hi ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">RFECV与RFE非常相似，但它在每个训练阶段都使用交叉验证，并最终输出要选择的最佳列数。</li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lw lx l"/></div></figure><h1 id="d369" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">3.嵌入式方法</h1><p id="9911" class="pw-post-body-paragraph ld le hi ki b kj kk ij lf kl km im lg kn lh li lj kp lk ll lm kr ln lo lp kt hb bi translated">嵌入式方法在训练过程本身中选择特征。</p><ul class=""><li id="822a" class="kg kh hi ki b kj lq kl lr kn ls kp lt kr lu kt ku kv kw kx bi translated">当特征的重要性低时，特征的系数变为零，因此该特征不用于进行预测。</li></ul><h1 id="af49" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">套索回归</h1><p id="cdda" class="pw-post-body-paragraph ld le hi ki b kj kk ij lf kl km im lg kn lh li lj kp lk ll lm kr ln lo lp kt hb bi translated">套索代表最小绝对收缩和选择操作符</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es md"><img src="../Images/81706864215ebe1726a8135936b967da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*NVjc67ZRhVyz0qElUTFtsA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">套索成本函数</figcaption></figure><p id="f40a" class="pw-post-body-paragraph ld le hi ki b kj lq ij lf kl lr im lg kn ly li lj kp lz ll lm kr ma lo lp kt hb bi translated">λ =惩罚(调谐参数)</p><p id="0dda" class="pw-post-body-paragraph ld le hi ki b kj lq ij lf kl lr im lg kn ly li lj kp lz ll lm kr ma lo lp kt hb bi translated">当λ = 0时，没有参数被消除，当λ = 1时，它等于线性回归。</p><ul class=""><li id="a72d" class="kg kh hi ki b kj lq kl lr kn ls kp lt kr lu kt ku kv kw kx bi translated">通过最小化该成本函数来找到参数估计。</li><li id="524f" class="kg kh hi ki b kj ky kl kz kn la kp lb kr lc kt ku kv kw kx bi translated">当系数估计值小于λ/2时，系数变为零。</li></ul><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lw lx l"/></div></figure><h1 id="984b" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">4.特征重要性</h1><p id="0d92" class="pw-post-body-paragraph ld le hi ki b kj kk ij lf kl km im lg kn lh li lj kp lk ll lm kr ln lo lp kt hb bi translated">在将模型拟合到为每个特征分配权重的整个特征集之后，计算特征重要性。</p><ul class=""><li id="0d6d" class="kg kh hi ki b kj lq kl lr kn ls kp lt kr lu kt ku kv kw kx bi translated">该模型可能具有诸如coef_或feature_importances_之类的属性，这些属性有助于选择特征子集。使用这种方法，最不重要的特征被删除。</li></ul><h1 id="6541" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">从sci-kit学习中的模型中选择</h1><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lw lx l"/></div></figure></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><p id="7d9d" class="pw-post-body-paragraph ld le hi ki b kj lq ij lf kl lr im lg kn ly li lj kp lz ll lm kr ma lo lp kt hb bi translated"><strong class="ki hj">在我的Kaggle笔记本里找到这个帖子:</strong><a class="ae jn" href="https://www.kaggle.com/srivignesh/feature-selection-techniques" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/srivignesh/feature-selection-techniques</a></p><p id="f952" class="pw-post-body-paragraph ld le hi ki b kj lq ij lf kl lr im lg kn ly li lj kp lz ll lm kr ma lo lp kt hb bi translated"><strong class="ki hj">参考文献:</strong></p><p id="5318" class="pw-post-body-paragraph ld le hi ki b kj lq ij lf kl lr im lg kn ly li lj kp lz ll lm kr ma lo lp kt hb bi translated">[1] M. Ramaswami和R. Bhaskaran，<a class="ae jn" href="https://arxiv.org/pdf/0912.3924.pdf" rel="noopener ugc nofollow" target="_blank">教育数据挖掘中的特征选择技术研究</a> (2009)</p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><p id="5b95" class="pw-post-body-paragraph ld le hi ki b kj lq ij lf kl lr im lg kn ly li lj kp lz ll lm kr ma lo lp kt hb bi translated"><em class="ml">在</em><a class="ae jn" href="https://www.linkedin.com/in/srivignesh-rajan-123569151/" rel="noopener ugc nofollow" target="_blank"><em class="ml">LinkedIn</em></a><em class="ml">，</em><a class="ae jn" href="https://twitter.com/RajanSrivignesh" rel="noopener ugc nofollow" target="_blank"><em class="ml">Twitter</em></a><em class="ml">上联系我！</em></p><h2 id="3fad" class="mm jp hi bd jq mn mo mp ju mq mr ms jy kn mt mu ka kp mv mw kc kr mx my ke mz bi translated">谢谢你！</h2></div></div>    
</body>
</html>