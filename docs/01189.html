<html>
<head>
<title>Creating a Machine Learning Model to Predict Malignant Breast Cancer Tumors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">创建预测恶性乳腺癌肿瘤的机器学习模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/creating-a-machine-learning-model-to-predict-malignant-breast-cancer-tumors-87abc4065432?source=collection_archive---------8-----------------------#2019-10-07">https://medium.com/analytics-vidhya/creating-a-machine-learning-model-to-predict-malignant-breast-cancer-tumors-87abc4065432?source=collection_archive---------8-----------------------#2019-10-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="51ea" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">背景</h1><p id="7548" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">下面的探索使用了来自威斯康星州<a class="ae kb" href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data" rel="noopener ugc nofollow" target="_blank">乳腺癌(诊断)数据集</a>的数据。这种解释的要点是建立一个相对精确的模型来确定乳腺肿瘤组织样本是恶性还是良性。</p><p id="2b7b" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">你可以点击这里查看Kaggle笔记本<a class="ae kb" href="https://www.kaggle.com/zjarnagin/breast-cancer-wisconsin-diagnostic-data-set?scriptVersionId=21652917" rel="noopener ugc nofollow" target="_blank">的链接！</a></p><h1 id="48be" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">设置</h1><p id="b40b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">首先，我们必须设置我们的环境。我们导入各种库(您可以在下面的代码单元中查看)并根据我们在提供的文件中遍历文件名得到的结果设置我们的目录。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="4461" class="kq ig hi km b fi kr ks l kt ku">import pandas as pd <em class="kv"># data processing, CSV file I/O (e.g. pd.read_csv)</em><br/>from sklearn.model_selection import train_test_split <em class="kv"># splitting our data into training and testing data</em><br/>import seaborn as sns <em class="kv"># for creating a correlation heatmap</em><br/>import matplotlib.pyplot as plt <em class="kv"># for displaying our heatmap for analysis</em><br/>from xgboost import XGBClassifier <em class="kv"># eventually, we will use an XGBClassifier for our model</em><br/>from sklearn.metrics import accuracy_score <em class="kv"># to score our model</em><br/><br/><em class="kv"># Input data files are available in the "../input/" directory.</em><br/><br/>import os<br/>for dirname, _, filenames <strong class="km hj">in</strong> os.walk('/kaggle/input'):<br/>    for filename <strong class="km hj">in</strong> filenames:<br/>        print(os.path.join(dirname, filename))</span></pre><h1 id="c47f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">读取我们的数据</h1><p id="31df" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这里，我们从提供的中读取数据。csv文件使用熊猫并将其表示为熊猫数据帧。</p><p id="6676" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">我们对基于细胞特征的诊断预测感兴趣，因此我们为诊断列分配一个“y”变量。</p><p id="c49e" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">值得注意的是，当使用“id”特性作为索引列时，我们会得到一个充满NaN条目的列。我们删除了这个栏目，因为它对我们没有用处。我们也改变了一些熊猫选项。这是为了无论何时我们调用“head ”,我们都可以看到所有的特性和列名，而不会被截断。</p><p id="c87a" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">然后，我们将恶性肿瘤的诊断替换为1(最初表示为“M”)，良性肿瘤的诊断替换为0(最初表示为“B”)。这在我们拟合模型时很有用。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="6c7c" class="kq ig hi km b fi kr ks l kt ku"><em class="kv"># Read the dataset</em><br/>X_full = pd.read_csv('/kaggle/input/breast-cancer-wisconsin-data/data.csv', index_col='id')<br/><br/><em class="kv"># Assign y to the diagnosis column</em><br/>y = X_full.diagnosis<br/><br/><em class="kv"># Assigning our index_col to be the column 'id' shifted our data over, leaving a column with all NaN entries.</em><br/><em class="kv"># We drop that here</em><br/>X = X_full.drop(columns=['Unnamed: 32'])<br/><br/><em class="kv"># Show all values whenever we call head.</em><br/>pd.set_option('display.max_columns', None)<br/>pd.set_option('display.max_rows', None)<br/><em class="kv"># If we run .dtypes on our data frame, we notice that all columns, aside from the diagnosis being a string, our integers.</em><br/><br/><em class="kv"># We replace a malignant diagnosis with 1, and benign with 0</em><br/>X['diagnosis'].replace('M', 1, inplace=True)<br/>X['diagnosis'].replace('B', 0, inplace=True)<br/>y.replace('M', 1, inplace=True)<br/>y.replace('B', 0, inplace=True)</span></pre><h1 id="6898" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">数据分析</h1><p id="f26f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">为了避免过度拟合，我们找到似乎对诊断影响不大的特征。我们通过使用热图关联图来做到这一点。</p><p id="0712" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">我们得到的图将显示一个属性与另一个属性的相关性。我们感兴趣的是哪些属性影响(或不影响)诊断列。我们分析图上的结果，忽略绝对值小于0.5的特征。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="623e" class="kq ig hi km b fi kr ks l kt ku"><em class="kv"># Here, we use the seaborn correlation heatmap to visualize the correlatons of features in our dataset on one another.</em><br/><em class="kv"># Using the filter method, we will drop features which have an absolute value of less than 0.5 on the feature 'diagnosis'</em><br/><br/><em class="kv"># Setting up and displaying our heatmap correlation</em><br/>plt.figure(figsize=(20,20))<br/>cor = X.corr()<br/>sns.heatmap(cor, annot=True, cmap=plt.cm.Reds, fmt='.2f')<br/>plt.show()</span></pre><figure class="kh ki kj kk fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es kw"><img src="../Images/0d633603af2b1048d1d4fa134c1b104a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RdVLP1OLtDQeV8zw.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">显示特征相关性的热图</figcaption></figure><h1 id="082c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">运用我们的分析</h1><p id="3b5a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，我们运行一些代码来做我们上面说过要做的事情:忽略对诊断列影响较小的特性。</p><p id="03c7" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">我们还将数据分为训练和测试数据，以训练和拟合我们的模型。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="cd87" class="kq ig hi km b fi kr ks l kt ku"><em class="kv"># Keep features which have a med-high correlation on the diagnosis</em><br/>features = ['radius_mean', 'perimeter_mean', 'area_mean', 'compactness_mean', 'concavity_mean', <br/>            'concave points_mean', 'radius_se', 'perimeter_se', 'area_se', 'radius_worst', 'perimeter_worst',<br/>           'area_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst']<br/>X = X[features]<br/><br/><em class="kv"># Break off validation set from training data</em><br/>X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,<br/>                                                      random_state=0)</span></pre><h1 id="bf53" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">创建和测试模型</h1><p id="1cb8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在我们已经准备好创建、训练和测试一个模型了。我们必须使用分类模型，因为预测值要么是0(良性)，要么是1(恶性)。我们使用Sklearn的“accuracy_score”函数来评估该模型的准确性。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="db12" class="kq ig hi km b fi kr ks l kt ku"><em class="kv"># We will use an XGBoostClassifier, and score the model using SKLearn Accuracy Score</em><br/><br/>model = XGBClassifier()<br/>model.fit(X_train, y_train)<br/>preds = model.predict(X_valid)<br/>accuracy_score(y_valid, preds)</span></pre><h1 id="d816" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">准确(性)</h1><p id="568c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们的准确度得分大约为0.956。这意味着当我们被提供了用细针抽吸(FNA)从乳腺肿块中获取的细胞核的特征时，我们的模型应该准确地将肿瘤分类为恶性或良性。</p></div></div>    
</body>
</html>