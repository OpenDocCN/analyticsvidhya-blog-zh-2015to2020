<html>
<head>
<title>OpenAI GPT-3: Language Models are Few-Shot Learners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI GPT-3:语言模型是一次性学习者</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/openai-gpt-3-language-models-are-few-shot-learners-82531b3d3122?source=collection_archive---------0-----------------------#2020-06-19">https://medium.com/analytics-vidhya/openai-gpt-3-language-models-are-few-shot-learners-82531b3d3122?source=collection_archive---------0-----------------------#2020-06-19</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><div class=""/><p id="05ec" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hh bi jj translated"><span class="l jk jl jm bm jn jo jp jq jr di"> O </span> <a class="ae js" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank"> penAI </a>最近发表了一篇论文，描述了自然语言处理的深度学习模型<a class="ae js" href="https://github.com/openai/gpt-3" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>，拥有<strong class="in hp">1750亿</strong>个参数(！！！)，<strong class="in hp">比上一版多100x </strong>，GPT-2。该模型对近5000亿个单词进行了预训练，并且在几个NLP基准上实现了SOTA性能，而无需微调。</p><h2 id="8bf4" class="jt ju ho bd jv jw jx jy jz ka kb kc kd iw ke kf kg ja kh ki kj je kk kl km kn bi translated">你注意到了吗？“未经微调”</h2><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ko"><img src="../Images/eb7428c12071501eb91f88732ed35af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hvKEldws7EZkZ_sIhsKJ0Q.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">比较:模型之间的参数大小。</figcaption></figure></div></div>    
</body>
</html>