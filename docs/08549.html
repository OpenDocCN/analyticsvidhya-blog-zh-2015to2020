<html>
<head>
<title>Reconstruct corrupted data using Denoising Autoencoder(Python code)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用去噪自动编码器重建损坏的数据(Python代码)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reconstruct-corrupted-data-using-denoising-autoencoder-python-code-aeaff4b0958e?source=collection_archive---------10-----------------------#2020-08-03">https://medium.com/analytics-vidhya/reconstruct-corrupted-data-using-denoising-autoencoder-python-code-aeaff4b0958e?source=collection_archive---------10-----------------------#2020-08-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="db29" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这篇文章将在几分钟内帮助你揭开使用autoencoder去噪的神秘面纱！！</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/550de188c7bf922195897f33ec7c5b1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qKiQ1noZdw8k05-YRIl6hw.jpeg"/></div></div></figure><p id="d657" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">自动编码器在实践中并不太有用，但它们可以用来非常成功地对图像去噪，只需通过在有噪声的图像上训练网络即可。我们可以通过向训练图像添加高斯噪声，然后将值剪切到0和1之间来生成有噪声的图像。</p><blockquote class="kf"><p id="cfb3" class="kg kh hi bd ki kj kk kl km kn ko ke dx translated">去噪自动编码器迫使隐藏层提取更鲁棒的特征，并限制它仅学习身份。Autoencoder从输入的损坏版本重建输入。</p></blockquote><p id="3965" class="pw-post-body-paragraph jj jk hi jl b jm kp ij jo jp kq im jr js kr ju jv jw ks jy jz ka kt kc kd ke hb bi translated">去噪自动编码器做两件事:</p><ul class=""><li id="e2f0" class="ku kv hi jl b jm jn jp jq js kw jw kx ka ky ke kz la lb lc bi translated">对输入进行编码(保留关于数据的信息)</li><li id="8913" class="ku kv hi jl b jm ld jp le js lf jw lg ka lh ke kz la lb lc bi translated">撤消随机应用于自动编码器输入的损坏过程的影响。</li></ul><p id="e37f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为了描述自动编码器的去噪能力，我们将使用噪声图像作为输入，原始的干净图像作为目标。</p><p id="8063" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="li">举例:</em>上面的图像是输入，下面的图像是目标。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lj"><img src="../Images/3a11149ce02f0832123191810c7f3e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*oDwl7IMqds_E3-9gd5-cVA.png"/></div></figure><h1 id="7e26" class="lk ll hi bd lm ln lo lp lq lr ls lt lu io lv ip lw ir lx is ly iu lz iv ma mb bi translated">问题陈述:</h1><p id="43ab" class="pw-post-body-paragraph jj jk hi jl b jm mc ij jo jp md im jr js me ju jv jw mf jy jz ka mg kc kd ke hb bi translated">为去噪自动编码器建立模型。向网络中添加更深和更多的层。使用MNIST数据集，将噪声添加到数据中，并尝试定义和训练自动编码器来对图像进行降噪。</p><h1 id="524e" class="lk ll hi bd lm ln lo lp lq lr ls lt lu io lv ip lw ir lx is ly iu lz iv ma mb bi translated">解决方案:</h1><p id="6a43" class="pw-post-body-paragraph jj jk hi jl b jm mc ij jo jp md im jr js me ju jv jw mf jy jz ka mg kc kd ke hb bi translated"><strong class="jl hj">导入库和加载数据集:</strong>下面给出了导入库和加载MNIST数据集的标准程序。</p><pre class="iy iz ja jb fd mh mi mj mk aw ml bi"><span id="aae8" class="mm ll hi mi b fi mn mo l mp mq">import torch<br/>import numpy as np<br/>from torchvision import datasets<br/>import torchvision.transforms as transforms</span><span id="09ed" class="mm ll hi mi b fi mr mo l mp mq"># convert data to torch.FloatTensor<br/>transform = transforms.ToTensor()<br/># load the training and test datasets<br/>train_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)<br/>test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)<br/># Create training and test dataloaders<br/>num_workers = 0<br/># how many samples per batch to load<br/>batch_size = 20<br/># prepare data loaders<br/>train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)<br/>test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)</span></pre><p id="dd82" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">可视化数据:</strong>您可以使用标准的matplotlib库来查看您是否正确加载了数据集。</p><pre class="iy iz ja jb fd mh mi mj mk aw ml bi"><span id="c61e" class="mm ll hi mi b fi mn mo l mp mq">import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>    <br/># obtain one batch of training images<br/>dataiter = iter(train_loader)<br/>images, labels = dataiter.next()<br/>images = images.numpy()</span><span id="c940" class="mm ll hi mi b fi mr mo l mp mq"># get one image from the batch<br/>img = np.squeeze(images[0])</span><span id="27e5" class="mm ll hi mi b fi mr mo l mp mq">fig = plt.figure(figsize = (5,5)) <br/>ax = fig.add_subplot(111)<br/>ax.imshow(img, cmap='gray')</span></pre><p id="c564" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">输出应该是这样的:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ms"><img src="../Images/d90f582abe7c9e048ca9d184693babe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*pdHAaisHtj0IGWderuUwbA.png"/></div></figure><p id="5ddd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">网络架构:</strong>最关键的部分是网络生成。这是因为去噪是网络的一个难题；因此，我们需要使用<em class="li">更深的</em>卷积层。对于编码器中的卷积层，建议从32的深度开始，同样的深度向后通过解码器。</p><pre class="iy iz ja jb fd mh mi mj mk aw ml bi"><span id="a680" class="mm ll hi mi b fi mn mo l mp mq">import torch.nn as nn<br/>import torch.nn.functional as F</span><span id="6246" class="mm ll hi mi b fi mr mo l mp mq"># define the NN architecture<br/>class ConvDenoiser(nn.Module):<br/>    def __init__(self):<br/>        super(ConvDenoiser, self).__init__()<br/>        ## encoder layers ##<br/>        # conv layer (depth from 1 --&gt; 32), 3x3 kernels<br/>        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  <br/>        # conv layer (depth from 32 --&gt; 16), 3x3 kernels<br/>        self.conv2 = nn.Conv2d(32, 16, 3, padding=1)<br/>        # conv layer (depth from 16 --&gt; 8), 3x3 kernels<br/>        self.conv3 = nn.Conv2d(16, 8, 3, padding=1)<br/>        # pooling layer to reduce x-y dims by two; kernel and stride of 2<br/>        self.pool = nn.MaxPool2d(2, 2)<br/>        <br/>        ## decoder layers ##<br/>        # transpose layer, a kernel of 2 and a stride of 2 will increase the spatial dims by 2<br/>        self.t_conv1 = nn.ConvTranspose2d(8, 8, 3, stride=2)  # kernel_size=3 to get to a 7x7 image output<br/>        # two more transpose layers with a kernel of 2<br/>        self.t_conv2 = nn.ConvTranspose2d(8, 16, 2, stride=2)<br/>        self.t_conv3 = nn.ConvTranspose2d(16, 32, 2, stride=2)<br/>        # one, final, normal conv layer to decrease the depth<br/>        self.conv_out = nn.Conv2d(32, 1, 3, padding=1)</span><span id="1bfb" class="mm ll hi mi b fi mr mo l mp mq">def forward(self, x):<br/>        ## encode ##<br/>        # add hidden layers with relu activation function<br/>        # and maxpooling after<br/>        x = F.relu(self.conv1(x))<br/>        x = self.pool(x)<br/>        # add second hidden layer<br/>        x = F.relu(self.conv2(x))<br/>        x = self.pool(x)<br/>        # add third hidden layer<br/>        x = F.relu(self.conv3(x))<br/>        x = self.pool(x)  # compressed representation<br/>        <br/>        ## decode ##<br/>        # add transpose conv layers, with relu activation function<br/>        x = F.relu(self.t_conv1(x))<br/>        x = F.relu(self.t_conv2(x))<br/>        x = F.relu(self.t_conv3(x))<br/>        # transpose again, output should have a sigmoid applied<br/>        x = F.sigmoid(self.conv_out(x))<br/>                <br/>        return x</span><span id="f996" class="mm ll hi mi b fi mr mo l mp mq"># initialize the NN<br/>model = ConvDenoiser()<br/>print(model)</span></pre><p id="5828" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">训练:</strong>网络的训练用GPU花费的时间明显更少；因此，我会推荐使用一个。虽然这里我们只关心训练图像，我们可以从<code class="du mt mu mv mi b">train_loader</code>中得到。</p><pre class="iy iz ja jb fd mh mi mj mk aw ml bi"><span id="408e" class="mm ll hi mi b fi mn mo l mp mq"># specify loss function<br/>criterion = nn.MSELoss()</span><span id="80ab" class="mm ll hi mi b fi mr mo l mp mq"># specify loss function<br/>optimizer = torch.optim.Adam(model.parameters(), lr=0.001)<br/># number of epochs to train the model<br/>n_epochs = 20</span><span id="b504" class="mm ll hi mi b fi mr mo l mp mq"># for adding noise to images<br/>noise_factor=0.5</span><span id="dba2" class="mm ll hi mi b fi mr mo l mp mq">for epoch in range(1, n_epochs+1):<br/>    # monitor training loss<br/>    train_loss = 0.0<br/>    <br/>    ###################<br/>    # train the model #<br/>    ###################<br/>    for data in train_loader:<br/>        # _ stands in for labels, here<br/>        # no need to flatten images<br/>        images, _ = data<br/>        <br/>        ## add random noise to the input images<br/>        noisy_imgs = images + noise_factor * torch.randn(*images.shape)<br/>        # Clip the images to be between 0 and 1<br/>        noisy_imgs = np.clip(noisy_imgs, 0., 1.)<br/>                <br/>        # clear the gradients of all optimized variables<br/>        optimizer.zero_grad()<br/>        ## forward pass: compute predicted outputs by passing *noisy* images to the model<br/>        outputs = model(noisy_imgs)<br/>        # calculate the loss<br/>        # the "target" is still the original, not-noisy images<br/>        loss = criterion(outputs, images)<br/>        # backward pass: compute gradient of the loss with respect to model parameters<br/>        loss.backward()<br/>        # perform a single optimization step (parameter update)<br/>        optimizer.step()<br/>        # update running training loss<br/>        train_loss += loss.item()*images.size(0)<br/>            <br/>    # print avg training statistics <br/>    train_loss = train_loss/len(train_loader)<br/>    print('Epoch: {} \tTraining Loss: {:.6f}'.format(<br/>        epoch, <br/>        train_loss<br/>        ))</span></pre><blockquote class="mw mx my"><p id="fec5" class="jj jk li jl b jm jn ij jo jp jq im jr mz jt ju jv na jx jy jz nb kb kc kd ke hb bi translated"><em class="hi">在这种情况下，我们实际上是</em> <strong class="jl hj"> <em class="hi">添加一些噪声</em> </strong> <em class="hi">到这些图像中，我们会将这些</em> <code class="du mt mu mv mi b"><em class="hi">noisy_imgs</em></code> <em class="hi">馈送到我们的模型中。该模型将基于噪声输入产生重建图像。但是，我们希望它产生</em>正常的<em class="hi">无噪声图像，因此，当我们计算损耗时，我们仍然会将重建输出与原始图像进行比较！</em></p></blockquote><p id="6e95" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">因为我们要比较输入和输出图像中的像素值，所以最好使用回归任务的损失。回归就是比较数量，而不是概率值。所以，在这种情况下，我就用<code class="du mt mu mv mi b">MSELoss</code>。</p><p id="d6a8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">结果:</strong>这里让我们给测试图像添加噪声，并让它们通过自动编码器。</p><pre class="iy iz ja jb fd mh mi mj mk aw ml bi"><span id="a50e" class="mm ll hi mi b fi mn mo l mp mq"># obtain one batch of test images<br/>dataiter = iter(test_loader)<br/>images, labels = dataiter.next()</span><span id="5a9e" class="mm ll hi mi b fi mr mo l mp mq"># add noise to the test images<br/>noisy_imgs = images + noise_factor * torch.randn(*images.shape)<br/>noisy_imgs = np.clip(noisy_imgs, 0., 1.)</span><span id="7de1" class="mm ll hi mi b fi mr mo l mp mq"># get sample outputs<br/>output = model(noisy_imgs)<br/># prep images for display<br/>noisy_imgs = noisy_imgs.numpy()</span><span id="4453" class="mm ll hi mi b fi mr mo l mp mq"># output is resized into a batch of iages<br/>output = output.view(batch_size, 1, 28, 28)<br/># use detach when it's an output that requires_grad<br/>output = output.detach().numpy()</span><span id="9483" class="mm ll hi mi b fi mr mo l mp mq"># plot the first ten input images and then reconstructed images<br/>fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(25,4))</span><span id="496e" class="mm ll hi mi b fi mr mo l mp mq"># input images on top row, reconstructions on bottom<br/>for noisy_imgs, row in zip([noisy_imgs, output], axes):<br/>    for img, ax in zip(noisy_imgs, row):<br/>        ax.imshow(np.squeeze(img), cmap='gray')<br/>        ax.get_xaxis().set_visible(False)<br/>        ax.get_yaxis().set_visible(False)</span></pre><p id="6bc4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">它在消除噪音方面做得非常出色，尽管有时很难分辨原始数字是多少。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nc"><img src="../Images/ddfde65b5a982699074d1954dc81eb8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzNzKzPZs7RDa4xU4qqOgQ.png"/></div></div></figure><p id="5c38" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">代码:</strong>你可以在我的Github上找到这个代码:<a class="ae nd" href="https://github.com/Garima13a/Denoising-Autoencoder" rel="noopener ugc nofollow" target="_blank">去噪自动编码器</a></p><p id="f2ed" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">结论:</strong>在本文中，我们学习了如何用python正确编写去噪自动编码器。我们还了解到，去噪对于网络来说是一个难题，因此使用更深的卷积层可以提供非常精确的结果。</p><p id="778b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">参考:我从“Udacity的安全和私人AI奖学金挑战纳米学位项目”中了解到这个主题</p></div></div>    
</body>
</html>