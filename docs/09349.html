<html>
<head>
<title>Understanding Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-and-beyond-ef5cbcc4d83e?source=collection_archive---------3-----------------------#2020-09-03">https://medium.com/analytics-vidhya/gradient-descent-and-beyond-ef5cbcc4d83e?source=collection_archive---------3-----------------------#2020-09-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="b10f" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="5e73" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">让我们达到全局最小值</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/1d00a9ffbd2f1c818e2cc96211516a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*DrJAzS7VSKdMCZj4XFKn1Q.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">梯度下降。图片来自<a class="ae js" href="https://upload.wikimedia.org/wikipedia/commons/5/5b/Gradient_descent_method.png" rel="noopener ugc nofollow" target="_blank">维基共享。</a></figcaption></figure><p id="b9d2" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><em class="kp">优化算法是设计用来收敛到一个解的算法。这里的解决方案可以是通过最小化成本函数比如‘L’的局部最小值或全局最小值。</em></p></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="021f" class="kx ky hi bd kz la lb lc ld le lf lg lh ix li iy lj ja lk jb ll jd lm je ln lo bi translated">这个成本函数是什么？</h1><p id="356b" class="pw-post-body-paragraph jt ju hi jv b jw lp is jy jz lq iv kb kc lr ke kf kg ls ki kj kk lt km kn ko hb bi translated"><em class="kp">成本函数是对我们的模型预测能力的一种衡量。成本函数的形状定义了我们的优化目标。</em></p><p id="d6ba" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">如果成本函数的形状是凸函数，我们的目标是找到唯一的最小值。这相对简单，因为没有局部最小值，我们只需要收敛到全局最小值。</p><p id="de33" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">如果成本函数的形状不是凸函数，我们的目标是找到邻域中可能的最低值。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lu"><img src="../Images/b674875529677ca0b91dc2c71e930433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*2Pqk33Kv7-Y_BJnfSNcrGg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">凸函数。来源<a class="ae js" href="https://en.wikipedia.org/wiki/Convex_function#/media/File:Grafico_3d_x2+xy+y2.png" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</figcaption></figure></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="bb73" class="kx ky hi bd kz la lb lc ld le lf lg lh ix li iy lj ja lk jb ll jd lm je ln lo bi translated">梯度下降</h1><p id="512f" class="pw-post-body-paragraph jt ju hi jv b jw lp is jy jz lq iv kb kc lr ke kf kg ls ki kj kk lt km kn ko hb bi translated"><em class="kp">梯度下降是一种一阶迭代优化算法，用于最小化一个函数L，常用于机器学习</em>和<em class="kp">深度学习。</em></p><p id="6769" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">这是一个<strong class="jv hs">一阶优化算法</strong>,因为在每次迭代中，该算法采用一阶导数来更新参数。参数是指回归问题中的系数或神经网络的权重。这些参数由给出最陡上升方向的梯度更新。在每次迭代中，这是通过在为成本函数L计算的梯度的相反方向上更新参数来执行的。更新的大小由称为<strong class="jv hs">学习速率α的步长决定。</strong></p><p id="eb2f" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">为了理解这个算法，让我们考虑一个人试图尽快到达谷底的情况。</p><p id="35f7" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">很明显，我们沿着最陡的斜坡方向快速向下移动。现在的问题是，我们如何找到这个方向？梯度下降通过测量误差函数的局部梯度找到相同的结果，并且沿着梯度的相反方向前进，直到我们达到全局最小值。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lv"><img src="../Images/468c6b35752c4804e4487a713032bb8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*bJTZHAUxeS1v8zaAP7qSwQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">全局最小值。图片来源:<a class="ae js" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank"> ML词汇表</a>。</figcaption></figure><p id="3ee8" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">如前所述，该算法计算成本函数w.r.t每个参数θ的梯度，这告诉我们我们的成本函数在我们当前位置(当前参数值)的斜率以及我们应该移动以更新我们的参数的方向。我们更新的规模由学习率控制。</p><p id="d28f" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">最常用的学习率有:0.3，0.1，0.03，0.01，0.003，0.001。</p><h2 id="f0f2" class="lw ky hi bd kz lx ly lz ld ma mb mc lh kc md me lj kg mf mg ll kk mh mi ln ho bi translated">学习率α</h2><p id="54cb" class="pw-post-body-paragraph jt ju hi jv b jw lp is jy jz lq iv kb kc lr ke kf kg ls ki kj kk lt km kn ko hb bi translated"><strong class="jv hs">高学习率</strong>导致大步长。虽然有机会快速到达最底部，但我们有超过全局最小值的风险，因为山坡的坡度在不断变化。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mj"><img src="../Images/f7ffa851a02a7abae8d2b760afac0b01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*Kn9spjRyHDp9VKe2OLFkDA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">学习率高。图片由作者提供。</figcaption></figure><p id="04c5" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><strong class="jv hs">学习率低</strong>导致步长小。因此，我们精确地向梯度的相反方向移动。这里的缺点是计算梯度所需的时间。所以我们要花很长时间才能收敛(到达最底部的点)。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mk"><img src="../Images/363ea8b91851c510a1bd1027ae0997d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*mUGEIJRU2uxYt9rQWykQLA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">学习率低。图片由作者提供。</figcaption></figure><p id="8141" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">如前所述，我们的目标是达到全局最小值。但是，当我们的成本函数具有不规则曲线时(主要是在深度学习神经网络的情况下)，随着随机初始化的进行，人们可能会达到一个<strong class="jv hs">局部最小值</strong>，这不如全局最小值好。克服这个问题的一个方法是使用动量的概念。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ml"><img src="../Images/06701b7ea6f42d57a37f41ccbe4d6bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*eetskzKtJEDvzYnINgXpmQ.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">局部最小值问题。图片由作者提供。</figcaption></figure><p id="4567" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><em class="kp">学习率会影响我们的模型收敛的速度。因此，正确的值意味着我们训练模型的时间更少。这是至关重要的，因为更少的训练时间意味着更少的GPU运行时间。</em></p><h2 id="be45" class="lw ky hi bd kz lx ly lz ld ma mb mc lh kc md me lj kg mf mg ll kk mh mi ln ho bi translated">正常化</h2><p id="28a6" class="pw-post-body-paragraph jt ju hi jv b jw lp is jy jz lq iv kb kc lr ke kf kg ls ki kj kk lt km kn ko hb bi translated">在执行梯度下降之前，缩放我们所有的特征变量是重要的。特征缩放在所定义的成本函数的形状中起着巨大的作用。当执行归一化时，梯度下降算法快速收敛，或者成本函数的轮廓将变得更窄和更高，这意味着它将花费更长的时间来收敛。</p><p id="60cc" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">归一化数据意味着对数据进行缩放，以达到(均值)<strong class="jv hs"> μ=0 </strong>，且(标准差)<strong class="jv hs"> σ=1 </strong>。</p><p id="5de4" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">考虑<strong class="jv hs"> <em class="kp"> n </em> </strong>特征变量。一个实例<strong class="jv hs"> <em class="kp"> xᵢ </em> </strong> <em class="kp"> </em>可以按如下比例缩放:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mm"><img src="../Images/c316d18349d93ff0e66aacd93910b9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/format:webp/1*gw8-Z0m-8gs0jqimxSwGCw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">正常化。图片由作者提供。</figcaption></figure><p id="23ae" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">可以使用Scikit-Learn的StandardScaler类来执行特征缩放。</p></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="8540" class="kx ky hi bd kz la lb lc ld le lf lg lh ix li iy lj ja lk jb ll jd lm je ln lo bi translated">优化程序</h1><p id="2afd" class="pw-post-body-paragraph jt ju hi jv b jw lp is jy jz lq iv kb kc lr ke kf kg ls ki kj kk lt km kn ko hb bi translated">在深入讨论细节之前，让我们先定义偏导数。</p><p id="39f8" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><em class="kp">在梯度下降中，我们针对每个参数计算成本函数的梯度。这意味着我们计算当任一参数改变时，成本函数改变多少。这叫做</em> <strong class="jv hs"> <em class="kp">偏导数。</em>T29】</strong></p><p id="8236" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">考虑具有参数θ的成本函数L。它可以表示为L(θ)。我们的目标是通过找到最佳参数θ值来最小化该成本函数。</p><ol class=""><li id="14cf" class="mn mo hi jv b jw jx jz ka kc mp kg mq kk mr ko ms mt mu mv bi translated">我们用随机值初始化参数θ。</li><li id="ff8c" class="mn mo hi jv b jw mw jz mx kc my kg mz kk na ko ms mt mu mv bi translated">我们选择一个学习率α并执行特征缩放(归一化)。</li><li id="6f87" class="mn mo hi jv b jw mw jz mx kc my kg mz kk na ko ms mt mu mv bi translated">在算法的每次迭代中，我们计算成本函数相对于每个参数的梯度，并如下更新它们:</li></ol><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nb"><img src="../Images/4b13ac45f87754e2f727acde6222fe06.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/format:webp/1*tFqRUeQ7KDJXYnXXUu1xVg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">梯度(斜率)。图片由作者提供。</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nc"><img src="../Images/83f2d6301dbb091dccbe3c5eb99e76bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*bHdyA12IKLuspwb7SJk_Gg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">优化步骤。图片由作者提供。</figcaption></figure><p id="5040" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><em class="kp">优化步骤中的负号表示我们在为成本函数L计算的梯度的相反方向上更新我们的参数，w.r.t .参数θ。</em></p><p id="4be4" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><strong class="jv hs">如果梯度小于0 </strong>，我们通过梯度乘以学习率α的值来增加参数。</p><p id="7611" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><strong class="jv hs">如果梯度大于0 </strong>，我们通过梯度乘以学习率α的值来减少参数。</p><p id="6485" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">重复上述步骤，直到成本函数收敛。现在，我们所说的收敛是指，成本函数的梯度等于0。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nd"><img src="../Images/3a33d26bff09693a264f1109357bb14e.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*8Ifusj6TzsR6rryh68LDXA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">趋同。图片由作者提供。</figcaption></figure></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="e6db" class="kx ky hi bd kz la lb lc ld le lf lg lh ix li iy lj ja lk jb ll jd lm je ln lo bi translated">梯度下降的类型</h1><blockquote class="ne nf ng"><p id="9315" class="jt ju kp jv b jw jx is jy jz ka iv kb nh kd ke kf ni kh ki kj nj kl km kn ko hb bi translated"><strong class="jv hs"> <em class="hi">批量渐变下降</em> </strong></p></blockquote><p id="9d1c" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">批量梯度下降在每个训练步骤使用整批训练数据。因此，对于较大的数据集来说，速度非常慢。</p><p id="cdb3" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">学习率是固定的。理论上，如果代价函数是凸函数，则保证达到全局最小值，否则在损失函数不是凸的情况下，达到局部最小值。</p><blockquote class="ne nf ng"><p id="b2f4" class="jt ju kp jv b jw jx is jy jz ka iv kb nh kd ke kf ni kh ki kj nj kl km kn ko hb bi translated"><strong class="jv hs"> <em class="hi">【随机梯度下降】</em> </strong></p></blockquote><p id="a0de" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">批量梯度下降在每次迭代训练中使用整批训练集。这在计算上是昂贵的。</p><p id="dbe3" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">随机梯度下降(此处随机意味着随机)，仅随机取单个实例(方差变大，因为我们对于训练的每次迭代仅使用1个实例)，并且在训练的每次迭代中使用相同的实例。这真的很快，但由于随机性，观察到不规则的模式。然而，如果成本函数是不规则的，这种随机性有时是有帮助的，因为它可以跳出局部最小值，比批量梯度下降有更好的机会找到全局最小值。</p><p id="6b0d" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">我们可以使用一个学习率调度器，设置一个初始的高值，然后随着时间的推移逐渐降低我们的学习率，这样我们就不会从最小值反弹回来。如果学习率降低得太快，你可能会陷入局部最小值，甚至会半途而废。如果学习率降低得太慢，你可能会在最小值附近跳很长时间，如果你过早停止训练，最终会得到一个次优的解决方案。</p><p id="3cab" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">可以使用scikit learning SGD regressor类来实现这一点。</p><p id="940d" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">需要注意的一件重要事情是，我们必须确保我们的训练数据在每个时期的开始都是混洗的，这样，平均起来，参数就趋向于全局最优。如果数据没有被打乱，SGD将不会接近全局最小值，而是逐个标签地优化。</p><blockquote class="ne nf ng"><p id="803d" class="jt ju kp jv b jw jx is jy jz ka iv kb nh kd ke kf ni kh ki kj nj kl km kn ko hb bi translated"><strong class="jv hs"> <em class="hi">小批量梯度下降</em> </strong></p></blockquote><p id="9374" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">小批量梯度下降在称为小批量的小随机实例集上计算梯度。与随机梯度下降相比，我们有更好的机会更接近最小值，但它可能更难摆脱局部最小值。</p><p id="b42a" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">批量大小可以是2的幂，如32、64等。</p><p id="3def" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">建议像在SGD的情况下那样混洗数据，以避免预先存在的顺序。</p><p id="a974" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">小批量梯度下降比批量梯度下降更快，因为每个训练步骤使用的训练样本数量更少。它也能更好地概括。</p><p id="b219" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">缺点是，由于噪声的存在，很难收敛，因为人们可能会在最小区域附近跳跃。这些振荡是我们需要学习率衰减的原因，以随着接近最小值而降低学习率。</p></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="73de" class="kx ky hi bd kz la lb lc ld le lf lg lh ix li iy lj ja lk jb ll jd lm je ln lo bi translated">超越一阶优化</h1><p id="754d" class="pw-post-body-paragraph jt ju hi jv b jw lp is jy jz lq iv kb kc lr ke kf kg ls ki kj kk lt km kn ko hb bi translated">如前所述，梯度下降是一种一阶优化算法，这意味着它只测量成本函数曲线的斜率，而不是曲率。(曲率是指曲线或曲面分别偏离直线或平面的程度)。</p><p id="dd02" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">那么函数的性质或曲率是如何测量的呢？它由二阶导数决定。曲率影响我们的训练。</p><p id="571d" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">如果二阶导数等于0，则称曲率是线性的。</p><p id="af65" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">如果二阶导数大于0，则曲率向上移动。</p><p id="ee09" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">如果二阶导数小于0，则曲率向下移动。</p></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="86e0" class="kx ky hi bd kz la lb lc ld le lf lg lh ix li iy lj ja lk jb ll jd lm je ln lo bi translated">结论</h1><p id="e586" class="pw-post-body-paragraph jt ju hi jv b jw lp is jy jz lq iv kb kc lr ke kf kg ls ki kj kk lt km kn ko hb bi translated">很好的理解梯度下降真的很重要。如上所述，选择正确的学习速度会更容易，但找到正确的学习速度很难。同时，找出衰变率是另一个大任务。在非凸成本函数的情况下，试图摆脱局部极小值，使用动量也是一项困难的任务，我将在另一篇文章中讨论。下一集见。</p><h2 id="cc9b" class="lw ky hi bd kz lx ly lz ld ma mb mc lh kc md me lj kg mf mg ll kk mh mi ln ho bi translated">最初发表于<a class="ae js" href="https://nvsyashwanth.github.io/machinelearningmaster/understanding-gradient-descent/" rel="noopener ugc nofollow" target="_blank"> machinelearningmaster </a>。</h2></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><blockquote class="ne nf ng"><p id="26c7" class="jt ju kp jv b jw jx is jy jz ka iv kb nh kd ke kf ni kh ki kj nj kl km kn ko hb bi translated">嘿，如果你喜欢这篇文章，请点击拍手按钮，分享这篇文章，以示你的支持。关注我，获取更多关于机器学习、深度学习和数据科学的文章。下一场见！T9】</p></blockquote></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="066a" class="kx ky hi bd kz la lb lc ld le lf lg lh ix li iy lj ja lk jb ll jd lm je ln lo bi translated">在网络上找到我</h1><p id="fb5f" class="pw-post-body-paragraph jt ju hi jv b jw lp is jy jz lq iv kb kc lr ke kf kg ls ki kj kk lt km kn ko hb bi translated"><a class="ae js" href="https://github.com/NvsYashwanth" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hs"> GitHub简介:</strong>这是我分叉的地方</a></p><p id="7a6c" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><a class="ae js" href="https://www.linkedin.com/in/nvsyashwanth/" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hs"> LinkedIn简介:</strong>联系分享职业动态</a></p><p id="3222" class="pw-post-body-paragraph jt ju hi jv b jw jx is jy jz ka iv kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><a class="ae js" href="https://twitter.com/YashwanthNvs" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hs">推特:</strong>分享科技推特</a></p><h1 id="c841" class="kx ky hi bd kz la nk lc ld le nl lg lh ix nm iy lj ja nn jb ll jd no je ln lo bi translated">谢谢你</h1></div></div>    
</body>
</html>