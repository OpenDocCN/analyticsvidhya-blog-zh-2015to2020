<html>
<head>
<title>Feature Selection for Dimensionality Reduction(Filter Method).</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维的特征选择(过滤方法)。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-for-dimensionality-reduction-filter-method-201cc9eaa3b5?source=collection_archive---------7-----------------------#2020-06-21">https://medium.com/analytics-vidhya/feature-selection-for-dimensionality-reduction-filter-method-201cc9eaa3b5?source=collection_archive---------7-----------------------#2020-06-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0624faa19ea9bf229dafb71d4a557dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hAv4CqTKUwVWRTlhdSIk-A.jpeg"/></div></div></figure><p id="3966" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi jo translated"><span class="l jp jq jr bm js jt ju jv jw di">在</span>机器学习中选择数据中的重要特征是整个周期的重要部分。</p><p id="2990" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">传递带有不相关要素的数据可能会影响模型的性能，因为模型会学习传递给它的不相关要素。</p><h2 id="f503" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">特征选择的需要:</h2><ul class=""><li id="05ec" class="ks kt hi is b it ku ix kv jb kw jf kx jj ky jn kz la lb lc bi translated">它有助于简化模型，使它们更容易和更快地训练。</li><li id="83ad" class="ks kt hi is b it ld ix le jb lf jf lg jj lh jn kz la lb lc bi translated">减少培训次数。</li><li id="294c" class="ks kt hi is b it ld ix le jb lf jf lg jj lh jn kz la lb lc bi translated">帮助避免维度的<a class="ae li" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">诅咒</a>，</li><li id="6519" class="ks kt hi is b it ld ix le jb lf jf lg jj lh jn kz la lb lc bi translated">通过减少<a class="ae li" rel="noopener" href="/analytics-vidhya/over-fitted-and-under-fitted-models-f5c96e9ac581">过拟合</a>(形式上，减少<a class="ae li" rel="noopener" href="/analytics-vidhya/bias-variance-tradeoff-2b19a4926e7d">方差</a>)来增强通用性</li></ul><h1 id="3053" class="lj jy hi bd jz lk ll lm kd ln lo lp kh lq lr ls kk lt lu lv kn lw lx ly kq lz bi translated">特征选择方法</h1><h2 id="414d" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">有三种一般的特征选择方法:</h2><ol class=""><li id="a697" class="ks kt hi is b it ku ix kv jb kw jf kx jj ky jn ma la lb lc bi translated">过滤方法</li><li id="cdd3" class="ks kt hi is b it ld ix le jb lf jf lg jj lh jn ma la lb lc bi translated"><a class="ae li" rel="noopener" href="/analytics-vidhya/feature-selection-for-dimensionality-reduction-wrapper-method-9979fffd0166">包装方法</a></li><li id="3647" class="ks kt hi is b it ld ix le jb lf jf lg jj lh jn ma la lb lc bi translated"><a class="ae li" rel="noopener" href="/@abhigyan.singh282/feature-selection-for-dimensionality-reduction-embedded-method-e05c74014aa">嵌入方法</a></li></ol><figure class="mc md me mf fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/6bc4c4863f0667c5e0f36723c0747071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*59UDrMQRt4zPtWIsBUROyQ.jpeg"/></div></div></figure><h1 id="3f53" class="lj jy hi bd jz lk ll lm kd ln lo lp kh lq lr ls kk lt lu lv kn lw lx ly kq lz bi translated">过滤方法</h1><ol class=""><li id="9f87" class="ks kt hi is b it ku ix kv jb kw jf kx jj ky jn ma la lb lc bi translated">这种方法通常是在传递数据以构建模型之前的预处理步骤之一。</li><li id="0dc0" class="ks kt hi is b it ld ix le jb lf jf lg jj lh jn ma la lb lc bi translated">执行各种统计测试，并根据得分选择特征。</li><li id="0863" class="ks kt hi is b it ld ix le jb lf jf lg jj lh jn ma la lb lc bi translated">过滤方法不太精确，但计算速度更快。</li><li id="ce99" class="ks kt hi is b it ld ix le jb lf jf lg jj lh jn ma la lb lc bi translated">对于较大的数据集，最好使用过滤方法，因为计算速度更快。</li><li id="f167" class="ks kt hi is b it ld ix le jb lf jf lg jj lh jn ma la lb lc bi translated">过滤方法有利于理论框架和理解数据的结构。</li></ol><h2 id="e383" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">不同的方式有:</h2><ul class=""><li id="d954" class="ks kt hi is b it ku ix kv jb kw jf kx jj ky jn kz la lb lc bi translated"><strong class="is hj">相关法:</strong> <br/> →用于衡量两个连续变量X和y之间的线性相关性<br/> →范围在-1到1之间，其中值越接近1表示高度相关，值越接近-1表示负相关。<br/> →相关性方法有助于识别哪些变量与另一个变量非常相似。<br/> →不同的相关方法包括:<br/> *皮尔逊相关系数。<br/> *斯皮尔曼相关系数。</li></ul><blockquote class="mg mh mi"><p id="f49e" class="iq ir mj is b it iu iv iw ix iy iz ja mk jc jd je ml jg jh ji mm jk jl jm jn hb bi translated">皮尔逊和斯皮尔曼都是完全不同的测试。皮尔逊相关有助于发现变量之间的“线性关系”,而斯皮尔曼相关有助于发现变量之间的“单调关系”。<br/>大多数情况下，皮尔逊相关是首选，然而，我喜欢测试皮尔逊和斯皮尔曼。如果Spearman检验结果大于Pearson检验结果，则表明变量之间表现出更多的单调关系而不是线性关系。</p></blockquote><ul class=""><li id="b41c" class="ks kt hi is b it iu ix iy jb mn jf mo jj mp jn kz la lb lc bi translated"><strong class="is hj">卡方检验:<br/> → </strong> <em class="mj">卡方检验</em>用于数据集中的分类特征。<br/> →我们计算每个特征和目标之间的卡方，并选择所需数量的具有最佳卡方得分的特征。<br/> →确定样本中两个分类变量之间的关联是否反映了它们在总体中的真实关联。</li></ul><figure class="mc md me mf fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/b464be2fe8107be7e918262160f7eafe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*64UZqTuzTJfIAN84AHUaGA.png"/></div></div></figure><ul class=""><li id="67a9" class="ks kt hi is b it iu ix iy jb mn jf mo jj mp jn kz la lb lc bi translated"><strong class="is hj"> Anova: <br/> →方差分析</strong>是一种统计方法，用于检验两个或两个以上组之间存在显著差异的均值。假设假设为<br/> <em class="mj"> * Null:所有组的均值相等。<br/> *备选:至少有一个组的平均值不同。<br/> </em> → <em class="mj"> </em>通过比较不同样本的均值来检查一个或多个因素的影响。<br/> → Anova和T-test在只对两个样本进行测试时基本上执行相同的操作，但是，如果比较两个以上的样本，则使用Anova，因为使用T-test会对错误率产生影响。<br/> →对两个以上的样本进行T检验将产生大约15%的误差率，而使用方差分析将使误差率在95%置信区间内保持在5%。</li></ul><blockquote class="mr"><p id="0e0a" class="ms mt hi bd mu mv mw mx my mz na jn dx translated">置信区间是样本统计值所在的范围。</p></blockquote><figure class="nc nd ne nf ng ij er es paragraph-image"><div class="er es nb"><img src="../Images/c432284b40cb20c2289ae24855e78702.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*8mubIPM1lhbbJnCrH2O9nw.jpeg"/></div></figure><figure class="mc md me mf fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/dc963bd4c294a448539bfb9f25f54123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S18nM2DplTB9Ny1mF8tiJg.jpeg"/></div></figure><ul class=""><li id="699e" class="ks kt hi is b it iu ix iy jb mn jf mo jj mp jn kz la lb lc bi translated"><strong class="is hj">方差膨胀因子(VIF): <br/> → </strong>方差膨胀因子(VIF)提供了多元回归模型中独立变量之间多重共线性的度量。<br/> →检测多重共线性很重要，因为虽然它不会降低模型的解释能力，但会降低独立变量的统计显著性。<br/> →独立变量上的大VIF表示与其他变量之间的高度共线关系，在模型结构和独立变量选择中应予以考虑或调整。<br/> → VIF可以通过我们得到的值来解释:<br/> * 1 —显示非共线性<br/> * 1到5 —显示存在一定程度的共线性<br/> * &gt; 5 —存在高度共线性<br/> →借助于确定系数(R平方)值来确定。<br/>R平方值<a class="ae li" rel="noopener" href="/analytics-vidhya/r-squared-and-adjusted-r-squared-408aaca84fd5">越高</a>越接近1。</li></ul><figure class="mc md me mf fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ni"><img src="../Images/4e7e3c239b6c6832e256407ec3581ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qawQygWlvxYl8ewFaWXSuw.png"/></div></div></figure><blockquote class="mr"><p id="ceb9" class="ms mt hi bd mu mv mw mx my mz na jn dx translated">在这里，1-R2也被称为宽容。</p></blockquote><p id="42cb" class="pw-post-body-paragraph iq ir hi is b it nj iv iw ix nk iz ja jb nl jd je jf nm jh ji jj nn jl jm jn hb bi translated">有许多<strong class="is hj">过滤方法</strong>决定选择哪个特征。理解何时使用来自实践的东西。但是，我建议尝试不同的方法，看看哪种方法最有助于选择特性，而不会对模型的准确性产生太大的影响。上面给出的是一些基本方法，在学习其他方法之前，应该彻底理解它们。</p><p id="7574" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下周将介绍特征选择的包装方法。</p><h2 id="9f5d" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">快乐学习！！！！</h2></div><div class="ab cl no np gp nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="hb hc hd he hf"><p id="bdfd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">喜欢我的文章？请为我鼓掌并分享它，因为这将增强我的信心。此外，我每周日都会发布新文章，所以请保持联系，以了解数据科学和机器学习基础系列的未来文章。</p><p id="8f10" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另外，如果你想的话，可以在linkedIn上联系我。</p><figure class="mc md me mf fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nv"><img src="../Images/37f75f857ddc6f4409c7f93ad11be9e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HMGR72zROG4KI67V"/></div></div><figcaption class="nw nx et er es ny nz bd b be z dx translated"><a class="ae li" href="https://unsplash.com/@alx_andru?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Alex </a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure></div></div>    
</body>
</html>