<html>
<head>
<title>Deep Dive into Back Propagation (Part II)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入研究反向传播(第二部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-dive-into-back-propagation-part-ii-c392f804a784?source=collection_archive---------14-----------------------#2019-12-18">https://medium.com/analytics-vidhya/deep-dive-into-back-propagation-part-ii-c392f804a784?source=collection_archive---------14-----------------------#2019-12-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/d10794f20f9e2a25e2da0ade4d15c63d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*PcJXz1jkffahatSAWp1kag.png"/></div></figure><p id="cda9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是“深入研究反向传播”的第二部分。要继续阅读，我建议如果你还没看过，先看看第一部分。第一部分的链接可以在这里找到<a class="ae jk" rel="noopener" href="/analytics-vidhya/deep-dive-into-back-propagation-part-i-e1a92b6dbdb9">。</a></p><p id="cb7f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们现在将继续关注反向传播过程的示例，并考虑具有两个输入[x1，x2]，单个隐藏层中的三个神经元[h1，h2，h3]和单个输出y的网络。</p><p id="94c9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要更新的权重矩阵是从输入到隐藏层的W1和从隐藏层到输出的W2。注意，在我们的例子中，W2是一个向量，而不是一个矩阵，因为我们只有一个输出。</p><p id="046e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将从网络上输入的前馈传递开始，然后计算输出，基于该误差，使用反向传播来计算偏导数。</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es jl"><img src="../Images/6580840e80454350fb4feaed958308a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*okrmx7t0FnLRmrliAwU9sA.png"/></div></figure><p id="ffa3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">计算三个隐藏神经元的激活值很简单。我们有输入与矩阵W_1的相应权重元素的线性组合。接下来是激活函数。</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es jl"><img src="../Images/0a0980056c020d24c2749f4c0184a3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*hi0fbwtWMHAX209735cxXw.png"/></div></figure><p id="a6d0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">输出是权重为W2的前一层向量H的激活的点积。</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es jq"><img src="../Images/449c7ee1f3c57f3a6c5ee9b19cab1b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*elSERpeY58sz7kWO1LLHhA.png"/></div></figure><p id="84d7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">计算输出后，我们终于可以找到网络错误。</p><p id="3c5c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">提醒一下，最常用的两个误差函数是<a class="ae jk" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方误差(MSE) </a>(常用于回归问题)和<a class="ae jk" href="https://www.ics.uci.edu/~pjsadows/notes.pdf" rel="noopener ugc nofollow" target="_blank">交叉熵</a>(常用于分类问题)。</p><p id="83c7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在本例中，我们使用MSE的一个变体:</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es jq"><img src="../Images/8b9169586a6230a7970821b69b568069.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*2AQA2th4sQSj2sMe6kr3Qg.png"/></div></figure><p id="5be0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">其中d是期望输出，y是计算输出。注意，在这种情况下，<strong class="io hj"> y </strong>和<strong class="io hj"> d </strong>不是向量，因为我们只有一个输出。</p><p id="257b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">误差是它们的平方差，也称为网络的<strong class="io hj">损失函数</strong>。我们将误差项除以2，以简化符号，这一点很快就会清楚。</p><p id="ffb0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">反向传播过程的目的是使误差最小化，在我们的情况下是损失函数。为了做到这一点，我们需要计算它对所有重量的偏导数。</p><p id="dedd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于我们刚刚找到输出y，现在可以通过找到更新值Wkij来最小化误差。上标k表示我们需要更新每一层k。</p><p id="a1d2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如前所述，权重更新值Wkij通过使用梯度以如下方式计算:</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es jr"><img src="../Images/3c302c97067ecd0cfde99a4a78d9389f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*d1McFwsZHT8t_z_WIIDTNw.png"/></div></figure><p id="d3ba" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">(注意d是一个常数值，所以它的偏导数就是一个零)</p><p id="8c5b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">输出相对于每个重量的偏导数定义了梯度，通常用希腊字母δ表示。</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es jq"><img src="../Images/d290b6c4d54f4b312558ce3a65c4082e.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Zp1RvBq5K2LsJFRCRSSjeg.png"/></div></figure><p id="fd36" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将使用链式法则找到梯度的所有元素。</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es jl"><img src="../Images/3b295bda0c619f86b36429b57eebd80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*lJOlnCexLO7SfqHjrIVFIA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">回顾链式法则</figcaption></figure><p id="29e2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">第三部分的链接可以在这里找到<a class="ae jk" rel="noopener" href="/@aungkyawmyint_26195/deep-dive-into-back-propagation-iii-e96234a4f7fd">。</a></p><p id="50cc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">内容鸣谢:Udacity深度学习计划</p></div></div>    
</body>
</html>