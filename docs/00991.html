<html>
<head>
<title>Combining Word Embeddings to form Document Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">组合单词嵌入以形成文档嵌入</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/combining-word-embeddings-to-form-document-embeddings-9135a66ae0f?source=collection_archive---------3-----------------------#2019-09-24">https://medium.com/analytics-vidhya/combining-word-embeddings-to-form-document-embeddings-9135a66ae0f?source=collection_archive---------3-----------------------#2019-09-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2c61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章是关于单词嵌入的介绍性文章的后续，可以在这里找到<a class="ae jd" rel="noopener" href="/@ytn.vj2/word-embeddings-an-introduction-to-the-landscape-dcf20cf391a1"/>。</p><p id="a162" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们已经探索了单词嵌入的领域，并且对它非常熟悉，那么让我们进入如何使用单词嵌入来生成可以传递给传统机器学习算法(如random forests、xgboost等)的特征。</p><h2 id="2377" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">我们为什么要研究这样一种表示文档的方法？</strong></h2><p id="df7e" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">我们经常用于文本的广泛使用的特征提取技术是TF-IDF。这种技术已经被证明可以很好地处理文本数据和传统算法，而且也很容易解释。TF-IDF生成大小为N x V的特征，其中N是观察的数量，V是词汇的大小。这种方法有助于减小特征的大小，以选择嵌入的大小。</p><p id="e8ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文是<a class="ae jd" href="https://towardsdatascience.com/supercharging-word-vectors-be80ee5513d" rel="noopener" target="_blank">“用TF-IDF增强词向量”</a>文章的扩展，该文章将FastText嵌入与TF-IDF结合起来。本文探讨了不同的单词嵌入生成算法，如Word2Vec (Skip-Gram和CBOW)、FastText等。以及组合这些单词嵌入的不同方式。</p><h2 id="1a6b" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">结合单词嵌入的不同方法有:</h2><ul class=""><li id="f87b" class="ke kf hi ih b ii jz im ka iq kg iu kh iy ki jc kj kk kl km bi translated"><strong class="ih hj"> tf-idf加权单词嵌入:</strong>通过将单词嵌入与单词在句子中的tf-idf分数相乘，将这些嵌入与每个句子中每个单词的TF-IDF分数相结合，对句子中的所有单词都这样做，并且将单词TF-IDF与单词嵌入相乘的结果进行累加，并除以句子中单词的累加TF-IDF分数。获得的向量被用作句子嵌入。</li><li id="54d8" class="ke kf hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated"><strong class="ih hj">doc 2 vec</strong>:doc 2 vec算法使用word2vec模型将文档嵌入向量空间，同时添加另一个特征(段落ID)，该特征是文档唯一的。每个段落被映射到由矩阵D中的列表示的唯一向量，并且每个单词也被映射到由矩阵w中的列表示的唯一向量。段落向量和单词向量被平均或连接以预测上下文中的下一个单词。上面的模型被称为段落向量的分布式存储版本(PV-DM)。PV-DM更胜一筹，但训练速度较慢。doc2vec的另一种方法是段落向量的分布式单词包版本(PV-DBOW)。这种算法速度更快，占用内存更少，因为不需要保存字向量。</li></ul><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ks"><img src="../Images/0833a14159d85430df2c687df26f64bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5lJEITiypUEN7QKRXeDubA.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">PV-DM模型和PV-DBOW模型</figcaption></figure><ul class=""><li id="a7dd" class="ke kf hi ih b ii ij im in iq li iu lj iy lk jc kj kk kl km bi translated"><strong class="ih hj">平均单词嵌入:</strong>嵌入也作为模型的一部分被学习。这些嵌入用于通过平均文档中所有单词的嵌入来生成文档嵌入。</li></ul><p id="c4f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用的数据是亚马逊评论，用评论的情绪标记为正面或负面。嵌入大小选择为20，上下文大小选择为5。前1000个观察值用于进行分析。</p><p id="3866" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实验结果如下:</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ll"><img src="../Images/4487ea3d5cb60f31426564360b1e1ee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KzVwlaim2qREyHkEauKG9A.png"/></div></div></figure><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ll"><img src="../Images/f02da225dc89ab0230f6e973b6e76cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KQi2cRYEYkX3cwamIy3IDQ.png"/></div></div></figure><p id="42fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结论</strong> <br/>当单词的嵌入被平均时，作为训练基于情感标签的网络的一部分的共同学习的嵌入看起来更准确。因此，增加TF-IDF信息对模型精度没有太大影响。</p><p id="bdcb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">经过预训练的ELMo和BERT表示接近特定于任务的嵌入，从而证明了它们对不同任务的现成适用性。</p><p id="1f48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果表证明，平均单词嵌入以形成文档嵌入优于实验中尝试的其他备选方案。</p><p id="8173" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望你喜欢这篇文章，如果你觉得这篇文章有用，请留下评论。</p><p id="4a79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个实验的笔记本可以在<a class="ae jd" href="https://github.com/ytnvj2/DocumentEmbedding/blob/master/TFIDFwithEmbeddings.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="8ee3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献</strong><br/><a class="ae jd" href="https://towardsdatascience.com/supercharging-word-vectors-be80ee5513d" rel="noopener" target="_blank">https://towards data science . com/suppression-word-vectors-be 80 ee 5513d</a><br/><a class="ae jd" rel="noopener" href="/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e">https://medium . com/scale about/a-gentle-introduction-to-doc 2 vec-db 3 e 8 c 0 CCE 5 e</a></p></div></div>    
</body>
</html>