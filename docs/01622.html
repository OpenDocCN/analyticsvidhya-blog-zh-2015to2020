<html>
<head>
<title>Mathematics Behind The Artificial Neural Networks: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工神经网络背后的数学:第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mathematics-behind-artificial-neural-networks-part-1-2214dab225c2?source=collection_archive---------10-----------------------#2019-11-04">https://medium.com/analytics-vidhya/mathematics-behind-artificial-neural-networks-part-1-2214dab225c2?source=collection_archive---------10-----------------------#2019-11-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5cdc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在任何机器学习模型中，目标是找到该算法的成本函数，然后最小化该成本函数。简单来说，模型成本越高，算法越差，反之亦然。成本函数由一些参数组成。我们必须调整这些参数，以获得最小值。</p><p id="2b48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated"><span class="l je jf jg bm jh ji jj jk jl di"> D </span>导数:导数用于最小化成本函数值。如果你理解导数，那么你将很容易理解梯度下降算法，它将用于机器学习中的成本优化。</p><p id="b1c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，让我们考虑一个成本函数y = x +3。在下图中，y是成本函数，x是成本函数的参数。我们必须调整参数x，使得y的值减小。这个函数y的导数由dy/dx= 2x给出。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/fa1274e9360be43018ffed6451d43388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4wj6-A9aWFfoyrw_JGqDQ.jpeg"/></div></div></figure><p id="795c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">曲线在一点的导数给出了曲线在该点的斜率。如果我们取x=3处曲线的斜率。斜率=dy/dx=2*3=6。正斜率表示x的增加将导致成本函数y的增加。因此，降低x的值将降低成本函数的值。</p><p id="6663" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，对于x=-2，曲线在x=-2处的斜率= dy/dx= -2*2 = -4。这里的负斜率表明，如果我们在x=-2处增加x的值，那么y值将减少。</p><p id="7be9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑这个成本函数的两种情况。</p><h2 id="6d42" class="jy jz hi bd ka kb kc kd ke kf kg kh ki iq kj kk kl iu km kn ko iy kp kq kr ks bi translated">案例1:</h2><p id="c96c" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">考虑开始时，我们设定参数x=5，然后y=5 +3=28。我们希望降低这一成本。我们必须调整参数x，使得y(成本函数)的值降低。所以将从x中减去x的导数，即dy/dx，但我们将dy/dx乘以0.1，因为我们不想过多地减少x的值，否则函数值将在另一侧(x的负侧)增加，而不是减少。我们姑且称之为0.1的价值学习率。</p><p id="900e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">learning_rate= 0.1，x = 5，y= 5 +3 = 28</p><p id="4d7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x = x - dy/dx *学习率</p><p id="1b03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x = 5-(2*5) *0.1 = 5- 1 = 4 …………(x更新为4)</p><p id="b48f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们计算成本函数:y = 4 +3 = 16+3 = 19 …(减少)</p><h2 id="2602" class="jy jz hi bd ka kb kc kd ke kf kg kh ki iq kj kk kl iu km kn ko iy kp kq kr ks bi translated">案例2:</h2><p id="0550" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">考虑开始时，我们设置了参数x= -6，然后y=6 +3 = 39</p><p id="b749" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x = x-dy/dx *学习率</p><p id="6084" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x = -6-(-6*2)*0.1 = -6+1.2 = 4.8</p><p id="985a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们计算成本函数:y = 4.8 +3 = 26…(减少)</p><h1 id="18d2" class="ky jz hi bd ka kz la lb ke lc ld le ki lf lg lh kl li lj lk ko ll lm ln kr lo bi translated">逻辑回归:</h1><p id="bf28" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">在这篇博客中，我们将使用单个神经元实现逻辑回归模型。该模型用于二元分类。它使用sigmoid函数，即sigma(z)= 1/(1+e^(-z)).此sigmoid函数返回值介于0和1之间。你可以尝试任何例子。</p><p id="1ac1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果z太大，e^(-z)值就会太小。因此sigma(z) =1(近似值)</p><p id="a4de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果z太小，e^(-z)值就会太大。因此sigma(z) = 0(近似值)</p><p id="9de7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以定义分类的阈值。如果sigma(z) &gt; 0.5，则输入示例属于class1，否则输入示例属于class0类型。这就是逻辑回归的工作原理。</p><p id="eb64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归即二元分类的代价函数由C(a，y)给出。其中y是包含值0或1的实际类别向量，a=sigma(z)。</p><p id="8356" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">C(a，y)=-log(a)* y-log(1–a)*(1-y)</p><h2 id="9ddd" class="jy jz hi bd ka kb kc kd ke kf kg kh ki iq kj kk kl iu km kn ko iy kp kq kr ks bi translated">成本函数证明:</h2><p id="660f" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">情况1:如果y=0，a=0.1。a非常接近y，并且使用阈值t=0.5，模型的预测类别将是我们想要的0。因此成本应该非常低。</p><p id="19f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">成本= C(0.1，0)=-log(1–0.1)*(1–0)=-log(0.9)=-(-0.04)= 0.04..(减少)</p><p id="25cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">情况2:如果y=0并且a=0.99，那么a根本不接近0，并且使用阈值=0.5，预测的类将是y=1，但是y是0。因此，与上一个示例相比，成本应该更高。</p><p id="075e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">成本= C(0.99，0)=-log(1–0.99)*(1–0)=-log(0.01)=-(-2)= 2..(高于上例)</p><p id="4f42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，对于m个例子，成本函数将是</p><p id="f69c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">C(a，y)=-1/m * summation of(y * log(a)+(1–y)* log(1-a))</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es lp"><img src="../Images/d6bacb7f8ce169abf41a5b88c28c81ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*orbqMT5yPFIx9sQORivr5A.jpeg"/></div></div></figure><p id="7a0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里X是我们的输入特征向量。w是权重向量，b是标量偏差。</p><p id="9a8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我假设你对人工神经网络(ann)有基本的理解，我将只解释数学细节。如果你理解了上面的神经网络框图，那么你就可以继续了。</p><p id="c2ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们回到衍生品。这里，C(a，y)是我们的成本函数，它是参数a和y的函数。函数“a”是参数z的函数，函数“z”是参数b和W的函数。最后，我们希望调整参数W和b，使成本函数C(a，y)值最小，这就是梯度下降算法的作用所在。</p><h1 id="5911" class="ky jz hi bd ka kz la lb ke lc ld le ki lf lg lh kl li lj lk ko ll lm ln kr lo bi translated">梯度下降:</h1><p id="bf3b" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">这里C不是参数W和b的直接函数，所以我们需要在导数中使用链式法则来获得C wrt b的导数即dC/dW和C wrt b的导数即dC/db。</p><p id="33d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设p = fun(q ), q = fun(r ),即p是q的函数，q是r的函数，则间接p是r的函数。因此，通过链式法则，我们可以计算dp/dr为dp/dr = dp/dq * dq/dr</p><p id="0f7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:即使我用符号d表示导数。实际上不是导数，是偏导数。因为当我计算对wrt W的导数时，我认为所有其他参数都是常数，当我计算对wrt b的导数时，我认为所有其他参数都是常数。这种类型的导数称为偏导数。</p><p id="cf88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将使用这个链式法则来计算dC/dW和dC/db。</p><p id="a8b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">z= WX+b，a =西格玛(z)，c=C(a，y)</p><p id="c127" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a) dC/dW = dC/da + da/dz + dz/dW</p><p id="1ecf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b) dC/db = dC/da + da/dz + dz/db</p><p id="a6e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的手写笔记中，我计算了这些导数。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es lq"><img src="../Images/fe7b01c0e3779fc088610d0cdc9c574f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EfVpxHumDfbMe7MD9DEMSg.jpeg"/></div></div></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es lr"><img src="../Images/c68cf0c024cbbdef4b57a593907c42ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MfrnMFG4-1nTnguEatxnXQ.jpeg"/></div></div></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es lr"><img src="../Images/02847f6e972ac84d77c640064b845419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EKMgm2005_i0xvg9Zb7RlA.jpeg"/></div></div></figure><p id="b05c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们将dC/dW和dC/db计算为</p><p id="3eab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dC/dW = (y-a) * X</p><p id="a2c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dC/db = (y-a)</p><p id="5b7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们之前在导数部分所讨论的，我们将更新参数W和b，以获得最小成本函数C值，如下所示</p><p id="8342" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">W = W-dC/dW *学习率</p><p id="f997" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b = b-dC/db *学习率</p><p id="6597" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将再次计算成本函数值，并再次更新W和b向量。在获得一定的精度或迭代一定次数后，我们可以停止梯度下降算法，我们的模型就准备好了。</p><h1 id="ceca" class="ky jz hi bd ka kz la lb ke lc ld le ki lf lg lh kl li lj lk ko ll lm ln kr lo bi translated">我们如何在代码中实现这一点？</h1><p id="0413" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">我们可以通过使用大量的for循环在代码中实现这一点。但是我们可以用numpy数组向量来代替，这比循环更有效。</p><h2 id="9eb8" class="jy jz hi bd ka kb kc kd ke kf kg kh ki iq kj kk kl iu km kn ko iy kp kq kr ks bi translated">正向传播实现:</h2><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/77c53461f31e67556e20fb16bd412149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jPDjA-zopzDfJ2hautCJtg.jpeg"/></div></div></figure><p id="ec7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里X是[n，m]个输入特征向量，它由m个例子组成，每个例子由n个特征组成。w是[1，n]向量，因此WX将是[1，m]向量。只要按照上面的文字说明。</p><h2 id="0cf7" class="jy jz hi bd ka kb kc kd ke kf kg kh ki iq kj kk kl iu km kn ko iy kp kq kr ks bi translated">反向传播实现:</h2><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/037308a8347e1b88b75faf19d38396a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D1QDX60EEXbJ0Lh67OxJgQ.jpeg"/></div></div></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/b68aca9d1879009136e570e27f02fcb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5B5MPkJW4yEgF6bWsBR3vg.jpeg"/></div></div></figure><h2 id="501f" class="jy jz hi bd ka kb kc kd ke kf kg kh ki iq kj kk kl iu km kn ko iy kp kq kr ks bi translated">代码</h2><ol class=""><li id="ca55" class="ls lt hi ih b ii kt im ku iq lu iu lv iy lw jc lx ly lz ma bi translated">我使用Heard Disease数据集进行训练。它包含一个target列，如果target=1人有心脏病，如果target=0人没有心脏病。该数据集中有13个要素。</li></ol><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es mb"><img src="../Images/8fccd7643de88c0bd9697748649a9531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KE60Ssm5q43d6GgVMRSFFw.png"/></div></div></figure><p id="7e2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">heartRawData变量中加载的数据。数据集中有303个条目。</p><p id="16bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3)与标签分离的特征</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es mc"><img src="../Images/3e8efef7921ced9c00eae30a43663543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O3OuiAmIYV86OVV-nZBO1Q.png"/></div></div></figure><p id="3275" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4)正向传播实现</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es md"><img src="../Images/561ffde60e3faa96c95f8e7b9cb69276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BObARkA2XQhpZYtjj0HfcQ.png"/></div></div></figure><p id="d54d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6)成本函数的实现</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es me"><img src="../Images/fae8e71b62d7efb52269452a93fbbb99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPu6xL00DpB9C7xvVQiU-Q.png"/></div></div></figure><p id="23ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7)梯度下降实现。可以看到，随着循环的迭代，成本在不断降低。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es mf"><img src="../Images/d5d76f6672ecdb298b1686541169804d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M3KyAKMQR7XfNjOEheAaTQ.png"/></div></div></figure><p id="0535" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">8)计算精度= 84.48 %</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es mg"><img src="../Images/b26a43f0f406770ea05c80a8402610a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eg9q2YYNz7MrjfRb-HVMIQ.png"/></div></div></figure><h1 id="9150" class="ky jz hi bd ka kz la lb ke lc ld le ki lf lg lh kl li lj lk ko ll lm ln kr lo bi translated">结尾:</h1><p id="bd43" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">在这个博客中，我们学习了导数，使用单个神经元的逻辑回归。在下一篇博客中，我将添加更多的层和每层中更多的神经元，朝着深度学习背后的数学方向前进。</p><p id="755c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谢谢大家！</p></div></div>    
</body>
</html>