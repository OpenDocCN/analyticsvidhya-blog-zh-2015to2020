<html>
<head>
<title>Support Vector Machine in R for beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向初学者的R语言支持向量机</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/support-vector-machine-in-r-for-beginners-94564aa2bb74?source=collection_archive---------3-----------------------#2019-10-14">https://medium.com/analytics-vidhya/support-vector-machine-in-r-for-beginners-94564aa2bb74?source=collection_archive---------3-----------------------#2019-10-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c2fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我在葡萄酒数据集上使用了支持向量机。数据集已经在我之前的<a class="ae jd" rel="noopener" href="/@dar.paharand/data-analysis-for-beginners-ff1a3d9389ae"> <strong class="ih hj"> <em class="je">新手数据分析</em> </strong> </a>的帖子里做了预处理。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jf"><img src="../Images/f6608df3a546836dff596a084c4a2439.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/1*BMEqI15_YDhKj3OCG0V31w.gif"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">来源:谷歌图片</figcaption></figure><p id="034f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，SVM是什么？</p><p id="992e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是一种受监督的<strong class="ih hj">机器学习算法</strong>，可用于分类或回归挑战。</p><p id="84a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主要目标是找到</p><ul class=""><li id="75d5" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated">线性可分模式的最佳超平面:</li><li id="f0c4" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">对于非线性可分模式，通过将原始数据转换到新空间，扩展到线性可分模式——</li></ul><p id="c441" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更多细节可以在R. Berwick的《支持向量机的白痴指南》中找到。</p><p id="fbfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回想一下<a class="ae jd" rel="noopener" href="/@dar.paharand/data-analysis-for-beginners-ff1a3d9389ae"> <em class="je">初学者数据分析</em> </a>，这两个特征<strong class="ih hj">色相</strong>、<strong class="ih hj">od 280/od 315 _ of _稀释_葡萄酒</strong>和<strong class="ih hj">脯氨酸</strong>揭示了数据的清晰分离，因此它们可能给出良好的聚类结果。因此，我首先将分析限制在这些要素上，然后将其扩展到整个数据集。</p><p id="5de8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">案例1 </strong>:使用两个特征对葡萄酒数据集进行分类:<strong class="ih hj">od 280/od 315 _ of _ delivered _ wine</strong>和<strong class="ih hj">脯氨酸</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es kf"><img src="../Images/4fd0c710693ebd28c48fcf1b105444f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yowxZnGkNEbIXELQoSg9LA.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">图1</figcaption></figure><p id="fb04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我使用<strong class="ih hj">支持向量机(SVM) </strong>和<strong class="ih hj">线性激活函数</strong>进行聚类。我将我的数据分为训练集和测试集:在<strong class="ih hj"> 178 </strong>个观察值中，<strong class="ih hj"> 91 </strong>用于训练，<strong class="ih hj"> 87 </strong>用于测试。</p><p id="e9e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要以下两个包:<strong class="ih hj"> e1070和dplyr。</strong></p><pre class="jg jh ji jj fd kk kl km kn aw ko bi"><span id="427f" class="kp kq hi kl b fi kr ks l kt ku">Twofeatures=dplyr::select(redwine,class,OD280_OD315_of_diluted_wines, Proline)<br/>Twofeatures[,-1]=scale(Twofeatures[,-1]) <br/>data2f=Twofeatures<br/>n=nrow(data2f)<br/>s=sample(n,91)<br/>train=data2f[s,]<br/>test=data2f[-s,]</span><span id="99ae" class="kp kq hi kl b fi kv ks l kt ku">svmfit=svm(class~.,data=train,kernel=”linear”,cost=.1,scale=F,type=”C-classification”)</span><span id="cc06" class="kp kq hi kl b fi kv ks l kt ku">plot(svmfit,test)</span><span id="a980" class="kp kq hi kl b fi kv ks l kt ku">pred=predict(svmfit,test[,-1],type=”class”)</span><span id="26a8" class="kp kq hi kl b fi kv ks l kt ku">CrossTable(pred,test[,1],prop.c = FALSE,prop.r = FALSE,prop.t = FALSE,prop.chisq = FALSE,dnn=c(“predicted Y”,”True Y”))</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es kf"><img src="../Images/4cf25f4a43386d2e29806cfbd8891dd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NaKsWsPlzQ2IpMapywcRmA.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">图2:在2个期货上使用线性激活函数的SVM</figcaption></figure><p id="f624" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它能够得到几乎90%的准确率，因此T42有10 %的分类错误。</p><pre class="jg jh ji jj fd kk kl km kn aw ko bi"><span id="5453" class="kp kq hi kl b fi kr ks l kt ku">cat(“ The accuracy of SVM classifier is = “,100*mean(pred==test[,1]),” %”)</span><span id="9938" class="kp kq hi kl b fi kv ks l kt ku">&gt; The accuracy of SVM classifier is = 89.65517 %</span></pre><p id="cdbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相应的混淆矩阵如下所示。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kw"><img src="../Images/0d9d3d129bccb57ddc01cf535230493c.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*ePQEma1jW3obzP47VUT_rg.png"/></div></figure><p id="483f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们在<strong class="ih hj"> svm函数</strong>中增加<strong class="ih hj"> cos </strong> t参数</p><pre class="jg jh ji jj fd kk kl km kn aw ko bi"><span id="05ac" class="kp kq hi kl b fi kr ks l kt ku">svmfit=svm(class~.,data=train,kernel=”linear”,<strong class="kl hj">cost=.3</strong>,scale=F,type=”C-classification”)</span><span id="2668" class="kp kq hi kl b fi kv ks l kt ku">plot(svmfit,test)</span><span id="cb39" class="kp kq hi kl b fi kv ks l kt ku">pred=predict(svmfit,test[,-1],type=”class”)</span><span id="26b8" class="kp kq hi kl b fi kv ks l kt ku">CrossTable(pred,test[,1],prop.c = FALSE,prop.r = FALSE,prop.t = FALSE,prop.chisq = FALSE,dnn=c(“predicted Y”,”True Y”))</span></pre><p id="4887" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果，精确度增加了<strong class="ih hj"> 3%，</strong></p><pre class="jg jh ji jj fd kk kl km kn aw ko bi"><span id="460b" class="kp kq hi kl b fi kr ks l kt ku">cat(“ The accuracy of SVM classifier is = “,100*mean(pred==test[,1]),” %”)</span><span id="2f41" class="kp kq hi kl b fi kv ks l kt ku">&gt; The accuracy of SVM classifier is = 93.10345 %. </span></pre><p id="3df8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，拥有正确的参数可以稍微提高精度(<strong class="ih hj">注意，这可以通过使用<strong class="ih hj"> </strong> e1070包中的<strong class="ih hj">调整</strong>功能在提供的参数范围</strong>内进行网格搜索来调整超参数，从而选择最佳参数集)。</p><h2 id="9bf2" class="kp kq hi bd kx ky kz la lb lc ld le lf iq lg lh li iu lj lk ll iy lm ln lo lp bi translated">案例2:使用所有数据执行分类</h2><p id="60a3" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">为此，我将<strong class="ih hj">首先应用PCA </strong>通过保留占最大可变性的PCs进行降维(这一主要步骤在我的帖子中完成)。注意:当我们有一个<strong class="ih hj">高维数据集(p &gt; &gt; n) </strong>时，这种方法很有用。为了完整起见，我们在这里使用了它。</p><p id="0efd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回想一下我在……上的帖子..PC1和PC2的可变性最大。然后，可以使用第一和第二主成分(PC1和PC2)来应用SVM。为此，我们将数据集分成两部分:<strong class="ih hj">训练:数据的2/3 </strong>和<strong class="ih hj">测试:数据的1/3 </strong>。不同的<strong class="ih hj">内核</strong>函数(线性、多项式、径向和sigmoid)与成本参数集<strong class="ih hj">一起使用。</strong></p><pre class="jg jh ji jj fd kk kl km kn aw ko bi"><span id="597f" class="kp kq hi kl b fi kr ks l kt ku">svm.linear=svm(class~.,data=train,kernel=”linear”,cost=.3,scale=F,type=”C-classification”)<br/>svm.linear %&gt;% plot(test)<br/>pred.linear=svm.linear %&gt;% predict(test[,-1],type=”class”)</span><span id="7ae8" class="kp kq hi kl b fi kv ks l kt ku">svm.poly=svm(class~.,data=train,kernel=”polynomial”,cost=.3,scale=F,type=”C-classification”)<br/>svm.poly %&gt;% plot(test)pred.poly=svm.poly %&gt;% predict(test[,-1],type=”class”)</span><span id="f6ef" class="kp kq hi kl b fi kv ks l kt ku">svm.radial=svm(class~.,data=train,kernel=”radial”,cost=.3,scale=F,type=”C-classification”)<br/>svm.radial %&gt;% plot(test)<br/>pred.radial=svm.radial %&gt;% predict(test[,-1],type=”class”)</span><span id="e5da" class="kp kq hi kl b fi kv ks l kt ku">svm.sigmoid=svm(class~.,data=train,kernel=”linear”,cost=.3,scale=F,t  ype=”C-classification”)<br/>svm.sigmoid %&gt;% plot(test)<br/>Pred.sigmoid=svm.sigmoid %&gt;% predict(test[,-1],type=”class”)</span></pre><div class="jg jh ji jj fd ab cb"><figure class="lv jk lw lx ly lz ma paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><img src="../Images/a9c1a4c45f30806ab6aafb546f32c365.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kmg2aSpxwKiQ440S7hyc5A.png"/></div></figure><figure class="lv jk lw lx ly lz ma paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><img src="../Images/ad4fb3e7c288c5c56a94b9136d5e1825.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*bMbv8TCnLL8G7QrnM3Fvlw.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx mb di mc md translated">图4: <strong class="bd kx">线性</strong>核(左)、<strong class="bd kx">多项式</strong>核(右)</figcaption></figure></div><div class="ab cb"><figure class="lv jk lw lx ly lz ma paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><img src="../Images/ef5caf79ba5cbca8b7ad71a8456d957f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*zkF6hR6Fr6rHa4CthnrOGQ.png"/></div></figure><figure class="lv jk lw lx ly lz ma paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><img src="../Images/512f8ce982f991fb6f9bf83aa821b98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*gwERhbRug_zBbU6GKwo8wQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx mb di mc md translated">图5: <strong class="bd kx">放射状</strong>内核(左)、<strong class="bd kx">乙状结肠</strong>内核(右)</figcaption></figure></div><p id="a368" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从这些图中，我们可以看到，具有多项式、<strong class="ih hj">线性和径向</strong>函数的SVM比具有sigmoid 函数的<strong class="ih hj"> SVM做得更好。事实上，这些言论被其相应的准确性所证实，</strong></p><pre class="jg jh ji jj fd kk kl km kn aw ko bi"><span id="61e4" class="kp kq hi kl b fi kr ks l kt ku">cat(“ The accuracy of SVM with linear classifier is = “,100*mean(pred.linear==test[,1]),” %”)</span><span id="06e6" class="kp kq hi kl b fi kv ks l kt ku">cat(“ The accuracy of SVM with polynomial classifier is = “,100*mean(pred.poly==test[,1]),” %”)</span><span id="bd77" class="kp kq hi kl b fi kv ks l kt ku">cat(“ The accuracy of SVM with radial classifier is = “,100*mean(pred.radial==test[,1]),” %”)</span><span id="383a" class="kp kq hi kl b fi kv ks l kt ku">cat(“ The accuracy of SVM with sigmoid classifier is = “,100*mean(pred.sigmoid==test[,1]),” %”)</span></pre><p id="f983" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给</p><pre class="jg jh ji jj fd kk kl km kn aw ko bi"><span id="f4be" class="kp kq hi kl b fi kr ks l kt ku">The accuracy of SVM with linear classifier is = 98.70%</span><span id="306e" class="kp kq hi kl b fi kv ks l kt ku">The accuracy of SVM with polynomial classifier is = 90.91%</span><span id="9861" class="kp kq hi kl b fi kv ks l kt ku">The accuracy of SVM with radial classifier is = 97.40 %</span><span id="3f9c" class="kp kq hi kl b fi kv ks l kt ku">The accuracy of SVM with sigmoid classifier is = 85.71%</span></pre><p id="4e6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">玩得开心……</p><h1 id="7c5b" class="me kq hi bd kx mf mg mh lb mi mj mk lf ml mm mn li mo mp mq ll mr ms mt lo mu bi translated">结论</h1><p id="9f70" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">我使用两种方法执行分类任务:</p><ol class=""><li id="fc8c" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc mv jx jy jz bi translated">探索数据并提取两个特征(变量),这两个特征给出了类别之间的清晰分离，然后应用SVM分类方法。</li><li id="e0d2" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc mv jx jy jz bi translated">在SVM之前，主成分分析首先应用于降维，尽管这里并不需要，但对未来的工作仍然有好处。</li></ol><p id="4bbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这两种情况下，结果都相当不错。</p><p id="80b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">仍有问题或意见，请在下面的部分留言。</p></div></div>    
</body>
</html>