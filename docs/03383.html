<html>
<head>
<title>Perceptron and Journey towards Artificial Neural Networks(ANN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">感知器与人工神经网络之旅</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/perceptron-and-journey-towards-artificial-neural-networks-ann-1af2f7349278?source=collection_archive---------17-----------------------#2020-01-28">https://medium.com/analytics-vidhya/perceptron-and-journey-towards-artificial-neural-networks-ann-1af2f7349278?source=collection_archive---------17-----------------------#2020-01-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a952" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">亲爱的读者你好…我们现在要讨论神经网络！</p><p id="d9fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们真正进入神经网络之前，我们应该知道神经网络是如何开始它的旅程的，以及它是从哪里开始的。我们开始吧！</p><p id="10e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感知器的想法是受我们大脑中通过电脉冲传递信息的生物神经元的启发。我们记得的所有甜蜜的小事和痛苦的经历，都是因为相互连接的神经元。</p><p id="781b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，<strong class="ih hj"> <em class="jd">“感知器是神经网络的一个构建模块。”</em>T3】</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/fbb5f5a6ed465cb2a027ff8014577663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qw61pp9p9CfaMdjJlHu7IA.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">生物神经元及其突触连接。</figcaption></figure><p id="19bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个成年人的大脑由1000亿个神经元和1000万亿个突触连接组成。</p><p id="efd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">科学家说，在研究了许多动物、鸟类和其他哺乳动物后，我们人类的神经元数量超过了我们的需求。</p><p id="40e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不幸的是，与其他生物相比，我们没有最大限度地利用它们，因为与我们相比，它们的神经元与身体的比例更低。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ju"><img src="../Images/c2300884642870f576515f0bd19737e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I0xwGRbpUNTTyIZFLMNJYQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">人工神经元-&gt;感知器(受生物神经元启发)</figcaption></figure><p id="3a4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">观察左边的图像:我们有输入、权重、传递函数、激活函数和损失函数(将在下面详细讨论)。</p><p id="e639" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些是根据生物神经元的灵感制定的，并创造了术语<strong class="ih hj"> <em class="jd">感知器</em> </strong>。</p><p id="4769" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">简单来说:</strong> <strong class="ih hj"> <em class="jd">无论何时我们执行任何任务，在执行之前或执行过程中，我们都会回忆或试图理解执行任务的程序(或)指令集，每个指令都会被赋予优先级/重要性，我们会尽量避免过度执行任务(或)以达到我们预期的结果。</em>T13】</strong></p><p id="2212" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">神经元和感知器之间的关联:</em> </strong></p><ol class=""><li id="f432" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated">指令=输入</li><li id="0ff1" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">优先级/重要性=权重</li><li id="9927" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">执行指令=应用传递函数</li><li id="1623" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">避免过度和执行不足=阈值(激活功能的作用)。</li></ol><p id="d133" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">很简单，不是吗？现在，从为输入分配权重到激活功能的最后一步，每一级都有大量的计算和变化。</p><p id="424f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还有…我们可以使用简单的感知器模型，轻松地对以下问题进行分类，而不是应用机器学习算法和进行如此多的计算:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kj"><img src="../Images/c73ce76a0619721dbcdee5934ab3cb3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*mNG1N_c7hTvXIK9i7NkhjA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">简单分类问题</figcaption></figure><p id="2c13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">感知器为什么会失败？</strong></p><p id="1ebc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">答案非常直观:当任务的复杂性增加时，感知器就会失效。</p><p id="2eb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的问题中:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kk"><img src="../Images/e0b106743dc686f06e5d3ceba623ade9.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*7fG5NpGb1ncTXY30HEOnlA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">非线性分类问题</figcaption></figure><p id="581b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于非线性空间，简单的感知器模型不能正确分类。</p><p id="8972" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是如果我们能在连接这些感知机的基础上建立一个网络呢？我们能更好地分类吗？答案是肯定的，我们将能够更好地分类！</p><p id="e292" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种想法导致了人工神经网络(ann)的诞生，它是完全连接的前馈网络。还有其他神经网络，如CNN和RNNs，这将在后面讨论，而不是在这里的这一个。</p><p id="13b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">单层/多层感知器模型:全连接前馈网络。</strong></p><p id="3391" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">全连接</strong>表示每个输入都连接到下一层的每个神经元。</p><p id="b5d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">前馈</strong>表示连接是正向的。</p><p id="d6d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ANN模型是单层和多层感知器模型。任何超过2层的网络都是深度神经网络。多达2层的网络是浅层网络。</p><p id="1a2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上面所承诺的，下面我们来谈谈:</p><p id="fdf8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入、权重、传递函数、激活函数和损失函数。</p><p id="5471" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来考虑这个等式:<strong class="ih hj">O =ψ(wx+θ)</strong></p><p id="543a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输入(x): </strong>输入是我们的数据集中的变量/列，我们用它们来训练我们的预测模型。</p><p id="c9c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">权重/特征/过滤器(w): </strong>用于训练模型的权重可以在所有输入中相等地给出，或者可以被选择为更少或更多地给出给输入。</p><p id="84b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定权重后会发生什么？</p><p id="3a5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算输入和权重的点积，并将其传递给传递函数。</p><p id="4445" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">传递函数:</strong>这里，我们的传递函数是每个输入及其权重的点积之和。该函数将该点的结果与偏差<strong class="ih hj">θ</strong>相加</p><p id="bca0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">偏差</strong>就像线性方程中加的截距。它是神经网络中的一个附加参数，用于调整输出以及神经元输入的加权和。此外，偏差值允许您将激活功能向左或向右移动。</p><p id="3106" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">ψ:ψ</strong>可以通过理解<strong class="ih hj">挤压/激活/阈值函数</strong>和<strong class="ih hj">损失函数来解释。</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kl"><img src="../Images/ec4806b946debaa4750335037cf1fc01.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*_UU9NaVAfpU5Zg33QU7e1Q.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">阈值/激活/挤压功能</figcaption></figure><p id="f5d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据上图，<strong class="ih hj">ψ</strong><strong class="ih hj"/>可以用上述任意一个函数代替。</p><p id="9819" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Sigmoid函数:</strong>我们在处理二元分类问题时使用该函数，即针对两个类别。</p><p id="0f5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Softmax函数:</strong>我们在处理多类分类问题时使用该函数，即针对两个以上的类。</p><p id="22e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">线性函数:</strong>该函数可用于回归问题。</p><p id="f5d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Tanh函数:</strong>该函数有助于更好地学习，在处理多层感知器模型时可以使用。</p><p id="c3d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">损失函数:</strong></p><p id="2a15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">平方损失、绝对损失、休伯损失、指数损失、对数损失、铰链损失、交叉熵损失、多类铰链损失。</strong></p><p id="1b8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">但最重要的是，如何选择合适的损失函数？</strong></p><p id="56aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理想的损失函数应该是:</p><p id="46c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">健壮:</strong>应该对异常值健壮，从而不会爆炸。</p><p id="1883" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">明确:</strong>多个系数值不应产生相同的误差。</p><p id="541c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">稀疏:</strong>应该尽量少用数据量。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es km"><img src="../Images/29286f67f39c4bcea812429f0a00b168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vceYimSQ_pEieId5ZCIt7w.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">损失函数</figcaption></figure><p id="5700" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">指数损失函数:</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kn"><img src="../Images/b55a9f7ec77404442f84a10db8594703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q6jB31t8_9jlwcqvej14nw.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><strong class="bd ko">二元分类:指数损失</strong></figcaption></figure><p id="44a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">物流损失函数:</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kp"><img src="../Images/732bd9c1bc0e668e055f1c78b2855503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qINimZjxkTxSCilsra1FXw.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><strong class="bd ko">二元分类:物流损失</strong></figcaption></figure><p id="d18f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">铰链损耗功能:</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kq"><img src="../Images/fb25d1575f5e838ebbdfda5bae43d3f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O-kIawkXXMpUjWCrCJyp7g.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><strong class="bd ko">二元分类:铰链损耗</strong></figcaption></figure><p id="a772" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多级铰链损耗:</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kr"><img src="../Images/10b97d65a577b8b11626eac228b68a65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99owhl1lttQvOG50Jh36Tg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">多级铰链损耗</figcaption></figure><p id="0cbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">soft max分类器的交叉熵损失:</strong></p><p id="b13b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">真实分布和预测分布之间的交叉熵定义为:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ks"><img src="../Images/b29acd2c29920e4184dd69a1a95bd488.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*giFIgLvVWEL7d27WHHP7GA.png"/></div></figure><p id="f672" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在交叉熵中，预测分数通过归一化转换为概率，正确类别的真实概率为1。因此，交叉熵损失写为:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kt"><img src="../Images/4b360f7e159ce079d91fa5badc13eaf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*t9Hvl7geTcDxifnRAzyQPw.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ku"><img src="../Images/ef4d40ea35fba22a89805eae1a359d55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*V1VnGIFdrEpmFOi3DU1NzQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><strong class="bd ko">问题和功能参考表</strong></figcaption></figure><p id="0a9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优化器和误差反向传播:</strong></p><p id="540b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化器用于减少损失。</p><p id="6045" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优化器</strong>和<strong class="ih hj">误差反向传播</strong>将被分开讨论，因为它们数量很多。</p><p id="f2fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中有梯度下降，阿达格拉德，阿达德尔塔，亚当，RAdam，批量梯度下降，迷你批量梯度下降:)</p><p id="6ebe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">误差反向传播</strong>是指必须更新权重，直到误差最小。</p><p id="b543" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">单层感知器模型:</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kv"><img src="../Images/c2fd0b124f579ca52114bd38981db654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*dspZTVSRk1bhqHCZhcGRcQ.png"/></div></figure><p id="450d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多层感知器模型:</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kw"><img src="../Images/399ee3e14e8ea0a5064c2dcf221bf57b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*LUx2tZOgJvk4nFCwIeXChw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">多层感知器模型图</figcaption></figure><p id="dd1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">隐藏层的几何金字塔规则:</strong></p><p id="00e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当<strong class="ih hj">输入</strong>有<strong class="ih hj"> m个节点</strong>而<strong class="ih hj">输出</strong>有<strong class="ih hj"> n个节点</strong>时，<strong class="ih hj">隐藏层</strong>应该有<strong class="ih hj">的平方根m×n</strong>个节点。</p><p id="abfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">网络拓扑:</strong></p><p id="9b96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在给定时间，网络中权值的<strong class="ih hj">个数为:H * (I + O) + H + O </strong>，其中H =隐网络的单元数，I =输入特征数，O =输出节点数。</p><p id="7034" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">举例:</strong>隐网络中6个输入，10个单元，2个输出节点在网络中的权重将为6 * (10 + 2) +6 + 2 = 80。</p></div><div class="ab cl kx ky gp kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="hb hc hd he hf"><p id="9716" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您阅读这篇文章到目前为止…下一集再见！</p></div></div>    
</body>
</html>