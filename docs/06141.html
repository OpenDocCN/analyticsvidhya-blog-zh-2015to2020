<html>
<head>
<title>YOLO[1506.02640]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLO</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/yolo-1506-02640-dbe968e87b46?source=collection_archive---------25-----------------------#2020-05-12">https://medium.com/analytics-vidhya/yolo-1506-02640-dbe968e87b46?source=collection_archive---------25-----------------------#2020-05-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="fa5b" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">实时目标检测</h2><div class=""/><div class=""><h2 id="7e03" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">你只看一次</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/f80faf2dddb54eb6f84fa9d3999f2e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*67y1492dP6qqYYo_.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">来源:<a class="ae jw" href="https://en.wikipedia.org/wiki/Object_detection" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Object_detection</a>作者:<a class="ae jw" href="https://commons.wikimedia.org/wiki/User:MTheiler" rel="noopener ugc nofollow" target="_blank"> Mtheiler </a></figcaption></figure><p id="d125" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我已经计划阅读主要的对象检测论文(虽然我已经浏览了大部分，但现在我将详细阅读它们，好到足以写一篇关于它们的博客)。这些论文与基于深度学习的对象检测相关。随时给建议或询问疑惑会尽我所能帮助大家。任何从这个领域开始的人都可以跳过许多这样的论文。当我看完所有的论文后，我也会写下它们的优先顺序/重要性。<br/>我写这篇博客是考虑到和我相似并且仍在学习的读者。虽然我会通过从各种来源(包括博客、代码和视频)深入理解论文来尽力写出论文的关键，但如果您发现任何错误，请随时在博客上指出或添加评论。我已经提到了我将在博客结尾涉及的论文列表。</p><p id="ada5" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">我们开始吧:)</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="15c9" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">YOLO是第一篇使用神经网络实现对象检测实时性能的论文。Yolo使用单个神经网络在一次评估中从完整图像预测边界框和类别概率。因此，YOLO是一个一级对象检测器，因为它在一个阶段预测输出，而不像我们之前研究的对象检测器，它有一个额外的阶段用于区域提议。与其他实时物体检测器相比，Yolo以45 FPS的速度工作，几乎有两倍的贴图。这使得YOLO成为许多应用的合理选择，并为一些新的应用打开了大门。</p><p id="1840" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">YOLO将输入图像分成S*S个网格。如果对象的中心落入网格单元，则该网格单元负责检测该对象。每个网格单元预测B个边界框和这些框的置信度得分(因此5个预测，框的中心(x，y)，宽度，高度和置信度得分)。这些置信度得分反映了模型对盒子包含对象的置信度，以及它认为盒子预测的准确性。每个网格单元还预测对象类别概率。因此，输出是维数为S*S*(B*5+C)的张量。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es la"><img src="../Images/5c8c8090979e1692c85301dd17f9c41c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*OdF80FJkt-gRnVWAaeBUoQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">来源:<a class="ae jw" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank"> YOLO论文</a>作者:Joseph Redmon</figcaption></figure><p id="5eb9" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">使用非最大抑制生成最终预测。</p><h2 id="0bc3" class="lb lc hi bd ld le lf lg lh li lj lk ll kg lm ln lo kk lp lq lr ko ls lt lu ho bi translated">网络</h2><p id="332e" class="pw-post-body-paragraph jx jy hi jz b ka lv is kc kd lw iv kf kg lx ki kj kk ly km kn ko lz kq kr ks hb bi translated">该网络非常简单，它有一个主干CNN网络，后面是一个全连接层，给出维数为S*S*(B*5+C)的输出(它是一个向量，后来被整形为张量)。YOLO使用两种不同的主干:更快的版本(快速YOLO 155 fps)和更精确的版本(45 fps)。该网络的架构类似于googlenet，精确版本有24层，快速版本有9层。</p><h2 id="f1d0" class="lb lc hi bd ld le lf lg lh li lj lk ll kg lm ln lo kk lp lq lr ko ls lt lu ho bi translated">培养</h2><p id="6e58" class="pw-post-body-paragraph jx jy hi jz b ka lv is kc kd lw iv kf kg lx ki kj kk ly km kn ko lz kq kr ks hb bi translated">上面讨论的主干网络首先在图像网络数据集上进行预训练。对于预训练，使用224*224的图像分辨率。由于物体检测通常需要更精细的信息，因此将其更改为448*448。边界框坐标(宽度和高度)在0和1之间标准化。所用的损失函数将在下一段中解释:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ma"><img src="../Images/1bc66ea2e08dc5b11d8d78441bcb9624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*YOXpo1c2yWhpDBOCLvt4Xg.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">YOLO损失函数</figcaption></figure><p id="25b5" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">上述等式中的前两个值是边界框坐标的回归损失。这里需要注意的两件事是λ_cord和w与h的平方根。平方根用于解决大小物体的相同重量误差问题，大盒子中的小偏差不如小盒子中的小偏差重要。λ_cord用于给定位和分类任务不相等的(λ_cord&gt;1，文中用5)权重。</p><p id="8270" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">第三个和第四个值是置信度得分。因为大多数网格实际上不包含物体，但是这些网格造成了压倒梯度的损失。因此λ_ noobj(λ_ noobj&lt; 1, used 0.5 in the paper) is used to reduce weightage because of grids not containing objects. The last value in the equation is for the probability score of each object class which is used only if the object is present in the grid.</p><p id="3280" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">Inference of YOLO is very easy, as the network is single-stage the image is resized, passed through the network and non-max suppression is applied to final output.</p><h2 id="64dc" class="lb lc hi bd ld le lf lg lh li lj lk ll kg lm ln lo kk lp lq lr ko ls lt lu ho bi translated">References</h2><ol class=""><li id="5241" class="mb mc hi jz b ka lv kd lw kg md kk me ko mf ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.02640.pdf</a></li></ol></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="ff55" class="mk lc hi bd ld ml mm mn lh mo mp mq ll ix mr iy lo ja ms jb lr jd mt je lu mu bi translated">论文列表:</h1><ol class=""><li id="51bd" class="mb mc hi jz b ka lv kd lw kg md kk me ko mf ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1312.6229.pdf" rel="noopener ugc nofollow" target="_blank"> OverFeat:使用卷积网络的综合识别、定位和检测</a>。[ <a class="ae jw" href="https://towardsdatascience.com/overfeat-review-1312-6229-4fd925f3739f" rel="noopener" target="_blank">链接到博客</a></li><li id="9073" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">丰富的特征层次，用于精确的对象检测和语义分割(RCNN)。</a> [ <a class="ae jw" rel="noopener" href="/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a">链接到博客</a> ]</li><li id="35e6" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池。</a> [ <a class="ae jw" rel="noopener" href="/@sanchittanwar75/review-spatial-pyramid-pooling-1406-4729-bfc142988dd2">链接到博客</a></li><li id="bb4d" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">快速R-CNN </a> [ <a class="ae jw" rel="noopener" href="/@sanchittanwar75/fast-rcnn-1504-08083-d9a968a82a70">链接到博客</a></li><li id="9baf" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">更快的R-CNN:利用区域提议网络实现实时目标检测。</a> [ <a class="ae jw" href="https://towardsdatascience.com/faster-rcnn-1506-01497-5c8991b0b6d3" rel="noopener" target="_blank">链接到博客</a></li><li id="8e72" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测。</a> ←你完成了这篇博客。</li><li id="fc9e" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank"> SSD:单次多盒探测器</a>。[博客链接]</li><li id="0381" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated">R-FCN:通过基于区域的完全卷积网络的目标检测。【博客链接】</li><li id="5ee9" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1612.03144.pdf" rel="noopener ugc nofollow" target="_blank">用于目标检测的特征金字塔网络。</a>【博客链接】</li><li id="7408" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated">DSSD:解卷积单粒子探测器。[博客链接]</li><li id="8a94" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">密集物体检测的失焦(视网膜网)。</a>【博客链接】</li><li id="16ab" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1804.02767.pdf" rel="noopener ugc nofollow" target="_blank"> YOLOv3:增量改进</a>。[博客链接]</li><li id="03f2" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1805.09300v3.pdf" rel="noopener ugc nofollow" target="_blank">狙击手:高效多尺度训练</a>。[博客链接]</li><li id="e75a" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1904.04514.pdf" rel="noopener ugc nofollow" target="_blank">标注像素和区域的高分辨率表示。</a>【博客链接】</li><li id="b39f" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1904.01355v5.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS:全卷积一级目标检测</a>。[博客链接]</li><li id="fa16" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1904.07850.pdf" rel="noopener ugc nofollow" target="_blank">物体为点</a>。[博客链接]</li><li id="fcd0" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated">CornerNet-Lite:高效的基于关键点的对象检测。【博客链接】</li><li id="0145" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1904.08189v3.pdf" rel="noopener ugc nofollow" target="_blank"> CenterNet:用于对象检测的关键点三元组</a>。[博客链接]</li><li id="9539" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1909.00700.pdf" rel="noopener ugc nofollow" target="_blank">训练时间友好的实时目标检测网络。</a>【博客链接】</li><li id="69b9" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated"><a class="ae jw" href="https://arxiv.org/pdf/1909.03625v1.pdf" rel="noopener ugc nofollow" target="_blank"> CBNet:一种用于目标检测的新型复合主干网络体系结构。</a>【博客链接】</li><li id="6373" class="mb mc hi jz b ka mv kd mw kg mx kk my ko mz ks mg mh mi mj bi translated">EfficientDet:可扩展且高效的对象检测。[博客链接]</li></ol><p id="7a31" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">和平…</p></div></div>    
</body>
</html>