<html>
<head>
<title>Understanding PCA and T-SNE intuitively</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">直观地理解主成分分析和T-SNE</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-pca-and-t-sne-intuitively-f8f0e196aee4?source=collection_archive---------3-----------------------#2020-03-17">https://medium.com/analytics-vidhya/understanding-pca-and-t-sne-intuitively-f8f0e196aee4?source=collection_archive---------3-----------------------#2020-03-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d224" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我希望提出一种直观的方式来理解降维技术，如PCA和T-SNE，而不要深究其背后的数学。</p><blockquote class="jd je jf"><p id="5618" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><strong class="ih hj">降维</strong></p><p id="3657" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">在现实世界中，我们经常会遇到维数非常高(数千)的数据集。如果你不知道维度是什么，它们是帮助识别数据的东西或者是数据的属性。为了前任。如果我们以一个人为例，我们可以用他/她的身高、体重、肤色、年龄等来表示他们的尺寸。尺寸和特征这两个词可以互换使用。降维非常重要，因为:</p><p id="0d41" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">人类最多只能看到3维空间(至少在外星人入侵之前)</p><p id="b51d" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">通常，在所有特征上训练ML模型在计算上将是昂贵的</p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jk"><img src="../Images/e5065347a5386164f8a1f7df324ce7a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8wRk4XgL0Y5y-p-5"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">迷失在更高维度来源:<a class="ae ka" href="https://www.cam.ac.uk/research/news/lost-in-high-dimensional-space-study-improves-the-cure-for-the-curse-of-dimensionality" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="a51c" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">主成分分析:</h1><p id="9caa" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">如前所述，在现实世界中，我们处理多维数据。降低维度是很有意义的。PCA是广泛使用的降维技术之一。为了理解PCA，让我们首先理解几个术语。</p><p id="8979" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">方差:它衡量我们的数据在任何给定维度上的分布情况。它在数学上可以定义为平均值的均方差。</p><p id="d062" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">协方差:它衡量我们的数据之间的线性关系，即如果x增加，y也增加，则值为正，如果x增加，y减少，则值为负，如果找不到线性关系，则值接近零。</p><p id="9c0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">协方差矩阵是对称矩阵，其中对角元素是特征的方差，而维度对的协方差是非对角元素。现在，用外行的话来说，我们想要减少维度，这样最大量的信息被保留。从几何角度来看，这意味着我们应该保留具有高方差(或具有最大数据分布)的特征。PCA就是这么做的！它会创建新要素，从而保留最大方差。PCA使用特征向量来实现这一点。在PCA中，我们找到协方差矩阵的特征向量和前n个特征向量(阅读新特征/维度),其中n的值取决于我们选择偏好的信息量。</p><blockquote class="jd je jf"><p id="318a" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">在PCA之后，我们得到一组相互正交的特征，这意味着它们是线性独立的。这是通过创建新要素来实现的，这些新要素是数据集中原始要素的线性组合。此外，当线性无关特征之间的协方差为零时，我们还得到非对角元素为零。</p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es le"><img src="../Images/a170ade8fb12da9dc4169ab02653bf00.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/0*jDPFNapv_Vi_c1Ei.gif"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">PCA源:<a class="ae ka" href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" rel="noopener ugc nofollow" target="_blank">链接</a>后沿绿线变换的特征</figcaption></figure><p id="4e77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">认证后活动应遵循的步骤:</p><ol class=""><li id="3f9f" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated">归一化数据集并计算其协方差矩阵x。</li><li id="1ece" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">求其特征向量和特征值。</li><li id="ad87" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">为了减少到k维，排序并选择对应于前k个特征值的特征向量。</li><li id="f356" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">将n维数据转换为新的k维数据。</li></ol><blockquote class="jd je jf"><p id="effd" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">注意:我们可以根据想要保留的方差百分比来选择我们想要的维数。例如，如果我们希望保留90%的信息，我们可以这样选择k:k个特征值之和/n个特征值之和= .9</p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lt"><img src="../Images/a8006ddbde7aea8f31c0869d985ec2c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*u3gwg4bEIIHBc48ncb7R7w.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">其中λI对应于第I个特征值</figcaption></figure><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lu"><img src="../Images/84527ea51758111e0386a13801dbaac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*Cs-IVhjUZvxJBDzwd1jtig.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">根据上述公式构建一个图表，并选择最佳的“k”</figcaption></figure><p id="c71d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="jg">PCA的局限性:</em> </strong></p><ol class=""><li id="5627" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated">当要素不相关时，保留的方差会相对较低。例如，如果一个二维数据集是圆形的，我们试图把它投影到一个轴上，只有50%的信息会被保留。</li><li id="fb0a" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">PCA考虑全局结构，因此邻域点/簇可能不会被保留。</li><li id="84e8" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">PCA可能会发现很难捕捉非线性关系。</li><li id="b309" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">PCA不能在不同尺度的数据集上工作(这可以通过标准化数据来克服)</li><li id="1255" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">PCA容易在数据中产生异常值。(除了最大化方差之外，PCA的替代公式是构建一个距离最小化的特征。这是使用离差的平方和来完成的，这对于异常值来说将非常高，从而支配了分量)</li></ol><p id="70ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">一个代码简单的例子:</strong></p><p id="31ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们不要从零开始实现PCA，因为我们不想深入研究数学。(但是，如果您选择查找输入数据的协方差矩阵，并使用来自<a class="ae ka" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.eigh.html" rel="noopener ugc nofollow" target="_blank"> Scipy </a>的eigh模块查找它们的特征向量。)下面的代码使用Scikit的PCA实现将神圣的<a class="ae ka" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据集的维数从784维减少到2维。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es lv"><img src="../Images/0228c94a01396b1f8436206c110cddb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mYApimPY_wUJQToijEGcNA.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">使用Scikit-learn对MNIST进行PCA分析</figcaption></figure><p id="6630" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然会有大量的信息丢失，但我们可以按照下面的第一个主成分来可视化数据。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lw"><img src="../Images/b48bfc75bc74782ee1c98bdda181f6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Zy9G2tFShHnCcZB8QdtmHw.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">沿着前两个主成分可视化的MNIST数据集。</figcaption></figure><h1 id="077f" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">SNE霸王龙:</h1><p id="39be" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">首先，T-SNE代表T-分布式随机邻域嵌入。虽然PCA是用于降维的最古老的技术之一，但是T-SNE是相对较新的(大约十年前！).T-SNE的关键是通过将高维数据拟合成概率分布，将高维数据转换成低维数据，使得邻域点保持在低维中。然而，当我们试图将分布拟合到低维空间中时，中心附近的点变得拥挤，这也被称为拥挤问题(拥挤是由于维数灾难而发生的。简而言之，在像超球体这样的高维空间中有更多的点，但是当减少到2-d平面时，就没有多少空间了，这意味着这些点在有限的空间中变得拥挤。为了绕过这个事实，算法使用了一个学生t分布(因此t-sne中的字母“t ”),它具有更宽的尾部。</p><blockquote class="jd je jf"><p id="9805" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">简而言之，该算法试图通过将数据拟合到学生t分布中来将点从高维嵌入到低维，从而保留点的邻域。</p></blockquote><p id="2de0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们试着理解t-sne的一些参数:</p><p id="3f86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">困惑:对应于我们在给定空间中选择保留的邻居数量</p><p id="b06c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">迭代次数:您希望算法运行的迭代次数(t-sne在每次迭代中不断移动点，并在一个确定的点停止)</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="ab fe cl lx"><img src="../Images/c17871b010669b7f6c0cf6d15b4af469.png" data-original-src="https://miro.medium.com/v2/1*5RyFnHaG661937C00NaBzA.gif"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">t-sne应用于高dim word2vec来源:<a class="ae ka" href="https://www.ibm.com/blogs/research/2017/11/interactive-supervision-tensorboard/" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><blockquote class="jd je jf"><p id="8e0c" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">注意:由于t-sne是一种迭代随机算法，对多个迭代和困惑值运行它并选择最适合我们数据集的算法总是更明智的。运行和发挥周围的t-sne参数<a class="ae ka" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank">在这里</a>。</p></blockquote><p id="ecdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="jg">t-SNE</em></strong>的注意事项/局限性:</p><ol class=""><li id="aae5" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated">由于它是一种非确定性算法，因此必须在改变参数值的情况下运行多次，并选择最适合我们数据集的算法。</li><li id="be90" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">t-sne中的簇大小没有任何意义，因为该算法操纵更密集和更稀疏的簇以适应更低维度的空间。</li><li id="c9f7" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">即使参数值适合我们的数据集，也要使用相同的值多次运行相同的算法，以确保形状不会改变。</li></ol><p id="98a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">一个代码简单的例子:</strong></p><p id="bd94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从Scikit Learn导入T-sne，并在数据集上运行算法以获得迭代和困惑的多个值。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ly"><img src="../Images/c10e3c1d493e80c08088d0b81bf54267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5hKh4Mue86uBv1F4GMZRoA.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">T-SNE对MNIST使用Scikit学习</figcaption></figure><p id="217a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是用T-SNE在二维空间显示的MINIST。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lz"><img src="../Images/9eb61f8db278208634574d17fb9b1777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*F1YAqMvOECTXIafq7VdOVg.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">使用T-SNE的二维空间上的MNIST</figcaption></figure><p id="df3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> PCA vs T-SNE: </strong></p><ol class=""><li id="51c6" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated">PCA致力于保存数据的全局结构，而T-SNE保存局部结构。</li><li id="f2de" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">PCA和T-SNE都产生难以解释的特征。</li><li id="31ce" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">当特征之间存在线性关系时，PCA工作得很好，而T-SNE即使在非线性数据集中也做得不错。</li><li id="e453" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">PCA是一种确定性算法(每次产生相同的输出)，而T-SNE在本质上是非确定性的(每次可能产生不同的输出)</li></ol><p id="2800" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ol class=""><li id="f225" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated"><a class="ae ka" href="http://www.appliedaicourse.com" rel="noopener ugc nofollow" target="_blank">www.appliedaicourse.com</a></li><li id="2dfe" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><a class="ae ka" href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/2691/making-sense-of-principal-component-analysis-features vectors-environments</a></li><li id="be66" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><a class="ae ka" href="https://mlexplained.com/2018/09/14/paper-dissected-visualizing-data-using-t-sne-explained/" rel="noopener ugc nofollow" target="_blank">https://ml explained . com/2018/09/14/paper-parsed-visualizing-data-using-t-SNE-explained/</a></li></ol><p id="a81c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其他资源:</p><ol class=""><li id="bf07" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated"><a class="ae ka" href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2014-10-Visualizing-MNIST/</a></li><li id="ac81" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated"><a class="ae ka" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank">https://distill.pub/2016/misread-tsne/</a></li></ol></div></div>    
</body>
</html>