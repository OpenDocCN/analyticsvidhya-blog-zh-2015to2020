<html>
<head>
<title>ML: Document comparison made simple</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML:文档比较变得简单</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml-document-comparison-made-simple-8b7e38c661df?source=collection_archive---------11-----------------------#2020-04-23">https://medium.com/analytics-vidhya/ml-document-comparison-made-simple-8b7e38c661df?source=collection_archive---------11-----------------------#2020-04-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="eb19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在学习机器学习的过程中，我开始遇到许多使用ML可以轻松解决的问题。所以当我开始研究自然语言处理时，我想到了建立一个比较文档的模型。这是一个简单的模型，当我遇到一些先进的模型时，我打算改进这个模型。说得够多了，让我们开始构建模型。</p><p id="41be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们必须安装几个python包才能让我们的模型工作。在命令提示符或终端中，给出以下命令来安装所需的软件包。您可以使用pip或conda进行安装。我正在使用pip来安装它。</p><p id="9c0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的简历可以是pdf或Docx格式。所以安装各自的包来处理文档。在这个项目中，我们将文档文本保存在excel表格中。所以我们需要将excel读取到panda的数据帧中，所以我们必须安装python包xlrd和pandas。</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="a69e" class="jm jn hi ji b fi jo jp l jq jr">pip install xlrd <br/>pip install pandas <br/>pip install PyPDF2<br/>pip install python-docx<br/>pip install nltk<br/></span></pre><p id="cfa8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们导入熊猫，从excel中获取数据并存储在dataframe中。我将excel文件命名为docs.xlsx。</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="c4f9" class="jm jn hi ji b fi jo jp l jq jr"><strong class="ji hj">import </strong>pandas <strong class="ji hj">as </strong>pd</span><span id="ac61" class="jm jn hi ji b fi js jp l jq jr">dataframe = pd.read_excel(<strong class="ji hj">'docs.xlsx'</strong>)</span></pre><p id="2660" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了数据帧中的数据，我们计算tf-idf，这是进一步计算余弦相似性所需要的。简单介绍一下tf-idf，它是对数据进行矢量化的方法之一，因为我们的模型都是数学模型，不能理解英语，我们需要将它们转换成向量，这些向量可以进一步馈送到ML模型来进行预测。在转换为向量之前，我们需要对数据进行预处理，以便减少停用词，从而使我们的模型变得健壮。因此，为了对数据进行预处理，我们需要执行一些已经存在于名为nltk的库中的操作。如您所见，我们已经安装了nltk包。我们只需要导入一些用于预处理数据的子包，这通常被称为规范化文本数据。下面是代码:</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="09b9" class="jm jn hi ji b fi jo jp l jq jr"><br/><strong class="ji hj">from </strong>nltk.util <strong class="ji hj">import </strong>ngrams<br/><strong class="ji hj">from </strong>nltk.corpus <strong class="ji hj">import </strong>stopwords<br/></span></pre><p id="57e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分析下面几行代码来理解ngrams，基本上你的n可以是你想要的数字，应该相应地选择。因此，我们决定在这种情况下，n的值对我们的任务来说是什么，2是恢复预测情况下n的值</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="2b4b" class="jm jn hi ji b fi jo jp l jq jr">&gt;&gt;&gt; from nltk import ngrams<br/>&gt;&gt;&gt; sentence = 'this is a foo bar sentences and i want to ngramize it'<br/>&gt;&gt;&gt; list(ngrams(sentence.split(),1))[('this',), ('is',), ('a',), ('foo',), ('bar',), ('sentences',), ('and',), ('i',), ('want',), ('to',), ('ngramize',), ('it',)]</span><span id="7931" class="jm jn hi ji b fi js jp l jq jr">&gt;&gt;&gt; list(ngrams(sentence.split(),2))<br/>[('this', 'is'), ('is', 'a'), ('a', 'foo'), ('foo', 'bar'), ('bar', 'sentences'), ('sentences', 'and'), ('and', 'i'), ('i', 'want'), ('want', 'to'), ('to', 'ngramize'), ('ngramize', 'it')]</span><span id="9bf3" class="jm jn hi ji b fi js jp l jq jr">&gt;&gt;&gt; list(ngrams(sentence.split(),3))<br/>[('this', 'is', 'a'), ('is', 'a', 'foo'), ('a', 'foo', 'bar'), ('foo', 'bar', 'sentences'), ('bar', 'sentences', 'and'), ('sentences', 'and', 'i'), ('and', 'i', 'want'), ('i', 'want', 'to'), ('want', 'to', 'ngramize'), ('to', 'ngramize', 'it')]</span></pre><p id="7bd1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以通过下面几行代码获得停用词。停用词是那些对文档的语义没有太大影响的词</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="f7ff" class="jm jn hi ji b fi jo jp l jq jr"><strong class="ji hj">from </strong>nltk.corpus <strong class="ji hj">import </strong>stopwords<br/>stopwords.words(<strong class="ji hj">'english'</strong>)</span></pre><p id="ff3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">词汇化和词干化是获取词根的过程。比如说。这两种方法都用来获取词根。比如吃，吃，被吃，吃了会导致吃。有不同的算法可用于词干分析，这里我们使用波特词干分析器。如果我们想要严格的词干，我们可以使用LancerStemmer，它是迭代词干。为了对单词进行词干处理，我们需要将单词单独传递给词干分析器，因此我们必须首先将文档转换成标记</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="fea5" class="jm jn hi ji b fi jo jp l jq jr"><strong class="ji hj">from </strong>nltk.stem <strong class="ji hj">import  </strong>PorterStemmer<br/><strong class="ji hj">from </strong>nltk.tokenize <strong class="ji hj">import </strong>sent_tokenize, word_tokenize</span><span id="b10c" class="jm jn hi ji b fi js jp l jq jr">words = nltk.word_tokenize(sentence)<br/>porter = PorterStemmer()<br/>words = []<br/><strong class="ji hj">for </strong>word <strong class="ji hj">in </strong>token_words:<br/>    words.append(porter.stem(word))<br/><strong class="ji hj">print(words)</strong></span></pre><p id="f07a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">小写字母和删除单个字符单词也是文本预处理的一部分。完成这些之后，我们的数据就可以建模了。</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="937b" class="jm jn hi ji b fi jo jp l jq jr">def lowerCaseWords(token_words):<br/>   words = []<br/>   for word in token_words:<br/>       if len(word) &gt; 1:<br/>           words.append(word.lower())<br/>return words </span></pre><p id="402a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来是使用sklearn库将我们的文本转换为tf-idf向量。tf-idf缩写为term frequency —逆文档频率。也就是说，它给出了一个单词对于一个文档有多重要。</p><p id="454d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数学上tf-idf定义为:</p><figure class="jd je jf jg fd ju er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es jt"><img src="../Images/c3704a2253fe6e03a9a9a16fe43f58b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/0*Dmgy5SAf16XOkCXl"/></div></div></figure><figure class="jd je jf jg fd ju er es paragraph-image"><div class="er es kb"><img src="../Images/e949c420b9a89487c268c99b702fc411.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/0*XW-HVm3HMtCE5FNg"/></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="2713" class="jm jn hi ji b fi jo jp l jq jr">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="5c72" class="jm jn hi ji b fi js jp l jq jr">vectorizer = TfidfVectorizer(stop_words="english", ngram_range=(1,2))</span><span id="c004" class="jm jn hi ji b fi js jp l jq jr">tfidf = vectorizer.fit_transform(raw_data)</span></pre><p id="5dad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们计算文档之间的余弦相似度。余弦相似性在数学上定义为:</p><figure class="jd je jf jg fd ju er es paragraph-image"><div class="er es kc"><img src="../Images/83d4f8214bbc76d1e6807f9bbf3185cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*iUWrxn_mxpxwFuh_G0EjUA.png"/></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">维基百科图片</figcaption></figure><p id="79dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里A和B是向量，在我们的例子中是doc a和doc b的tfids，所以在python中我们有许多实现余弦相似性的库，这里我使用sklearn的实现。以下是相同的代码:</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="8b60" class="jm jn hi ji b fi jo jp l jq jr">from sklearn.metrics.pairwise import cosine_similarity</span><span id="a7b8" class="jm jn hi ji b fi js jp l jq jr">cosine_similarity(query_tfidf, docs_tfidf).flatten()</span></pre><p id="af3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我故意忽略了一些让读者理解的步骤。。有任何问题请留下评论。很快将会在我的git hub个人资料上发布全部代码。<a class="ae kh" href="https://github.com/ams1234" rel="noopener ugc nofollow" target="_blank">https://github.com/ams1234</a></p><h1 id="9ba3" class="ki jn hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">快乐学习</strong></h1><p id="b228" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">问候，</p><p id="12c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">阿尔皮塔</p></div></div>    
</body>
</html>