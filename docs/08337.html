<html>
<head>
<title>Topic Modeling using Gensim-LDA in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中使用Gensim-LDA的主题建模</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920?source=collection_archive---------0-----------------------#2020-07-26">https://medium.com/analytics-vidhya/topic-modeling-using-gensim-lda-in-python-48eaa2344920?source=collection_archive---------0-----------------------#2020-07-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f7e60e2fe20a2326f7ddf0769799cd73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Iol-C47mo4deMFnE"/></div></div></figure><p id="284d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这篇博文是使用spaCy的自然语言处理的第2部分，主要关注主题建模。</p><p id="884e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一定要看看博客的第一部分，其中包括使用spaCy的各种预处理和特征提取技术。</p><h1 id="b87f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">什么是主题建模？</h1><p id="1975" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><strong class="is hj">主题建模</strong>是从大量文本中提取隐藏主题的技术。主题模型是包含文本信息的概率模型。</p><p id="efba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如:如果它是一个报纸语料库，它可能有像经济、体育、政治、天气这样的主题。</p><p id="6e40" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">主题模型对于文档聚类、组织大块文本数据、从非结构化文本中检索信息以及特征选择是有用的。<strong class="is hj"> <em class="ks">找到好的主题取决于文本处理的质量、主题建模算法的选择、算法中指定的主题数量。</em> </strong></p><p id="415b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有几种现有的算法可以用来执行主题建模。最常见的是潜在语义分析或索引(LSA/LSI)，分层狄利克雷过程(HDP)，<strong class="is hj"> <em class="ks">【潜在狄利克雷分配(LDA) </em> </strong>我们将在本文中讨论。</p><p id="677d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">LDA的主题建模方法是，将每个文档视为主题的集合，将每个主题视为关键字的集合。一旦为算法提供了主题数量，它所做的就是重新安排文档中的主题分布和主题中的关键字分布，以获得主题-关键字分布的良好组合。</p><p id="ebb4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">主题不过是主题中出现概率最高的关键词或单词的集合，这有助于识别主题是关于什么的。</p><h1 id="384d" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">安装依赖项</h1><blockquote class="kt ku kv"><p id="f0ed" class="iq ir ks is b it iu iv iw ix iy iz ja kw jc jd je kx jg jh ji ky jk jl jm jn hb bi translated"><em class="hi"> pip3安装空间</em></p><p id="9120" class="iq ir ks is b it iu iv iw ix iy iz ja kw jc jd je kx jg jh ji ky jk jl jm jn hb bi translated"><em class="hi"> python3 -m spacy下载en #语言模型</em></p><p id="9423" class="iq ir ks is b it iu iv iw ix iy iz ja kw jc jd je kx jg jh ji ky jk jl jm jn hb bi translated"><em class="hi"> pip3安装gensim #进行主题建模</em></p><p id="5042" class="iq ir ks is b it iu iv iw ix iy iz ja kw jc jd je kx jg jh ji ky jk jl jm jn hb bi translated"><em class="hi"> pip3安装pyLDAvis #用于可视化主题模型</em></p></blockquote><ul class=""><li id="2403" class="kz la hi is b it iu ix iy jb lb jf lc jj ld jn le lf lg lh bi translated">对于这个实现，我们将使用NLTK中的停用词。</li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="5e01" class="lr jq hi ln b fi ls lt l lu lv">import nltk<br/>nltk.download('stopwords')</span></pre><h1 id="3670" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">实施</h1><ul class=""><li id="54ef" class="kz la hi is b it kn ix ko jb lw jf lx jj ly jn le lf lg lh bi translated"><strong class="is hj">导入库</strong></li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="cb24" class="lr jq hi ln b fi ls lt l lu lv">import re<br/>import numpy as np<br/>import pandas as  pd<br/>from pprint import pprint# Gensim<br/>import gensim<br/>import gensim.corpora as corpora<br/>from gensim.utils import simple_preprocess<br/>from gensim.models import CoherenceModel# spaCy for preprocessing<br/>import spacy# Plotting tools<br/>import pyLDAvis<br/>import pyLDAvis.gensim<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><h1 id="1dc8" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">准备停用词</h1><ul class=""><li id="b1ef" class="kz la hi is b it kn ix ko jb lw jf lx jj ly jn le lf lg lh bi translated">您可以根据所使用的数据集或预处理后是否看到停用词来扩展停用词列表。您还可以使用<a class="ae jo" href="https://amueller.github.io/word_cloud/" rel="noopener ugc nofollow" target="_blank"> wordcloud </a>可视化清理后的语料库，并检查清理后的语料库中是否有任何添加噪音的单词或任何未使用的单词。</li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="4baa" class="lr jq hi ln b fi ls lt l lu lv"># NLTK Stop words<br/>from nltk.corpus import stopwords<br/>stop_words = stopwords.words('english')<br/>stop_words.extend(['from', 'subject', 're', 'edu', 'use'])</span></pre><h1 id="b154" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">加载数据集</h1><p id="502b" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我们将20-新闻组数据集。它包含大约11K个新闻组帖子，来自20个不同的主题。数据集可从<a class="ae jo" href="https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json" rel="noopener ugc nofollow" target="_blank"> newsgroup.json </a>获得。</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="dbfa" class="lr jq hi ln b fi ls lt l lu lv"># LoadDataset<br/>df=pd.read_json('https://raw.githubusercontent.com/selva86/datasets/ master/newsgroups.json')<br/>print(df.target_names.unique())<br/>df.head()</span></pre><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/da41a4477e8a568c9f418913dd903c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/0*NdLWV5sHEGzfKR9t.png"/></div></figure><h1 id="f05e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">删除电子邮件和换行符</h1><ul class=""><li id="57f4" class="kz la hi is b it kn ix ko jb lw jf lx jj ly jn le lf lg lh bi translated">正如你所看到的，数据集中有很多电子邮件和换行符。使用正则表达式删除它们。使用<strong class="is hj"> re </strong>模块的<strong class="is hj"> <em class="ks"> </em> sub() </strong>。在<strong class="is hj">re sub()<em class="ks"/></strong>中，在第一个参数中指定正则表达式模式，在第二个参数中指定新字符串，在第三个参数中指定要处理的字符串。</li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="29ec" class="lr jq hi ln b fi ls lt l lu lv"><em class="ks"># Convert to list</em> <br/>data = df.content.values.tolist()  <br/><em class="ks"># Remove Emails</em> <br/>data = [re.sub('\S*@\S*\s?', '', sent) <strong class="ln hj">for</strong> sent <strong class="ln hj">in</strong> data]  <br/><em class="ks"># Remove new line characters</em> <br/>data = [re.sub('\s+', ' ', sent) <strong class="ln hj">for</strong> sent <strong class="ln hj">in</strong> data]  <br/><em class="ks"># Remove distracting single quotes</em> <br/>data = [re.sub("<strong class="ln hj">\'</strong>", "", sent) <strong class="ln hj">for</strong> sent <strong class="ln hj">in</strong> data]  <br/>pprint(data[:1])</span></pre><ul class=""><li id="f187" class="kz la hi is b it iu ix iy jb lb jf lc jj ld jn le lf lg lh bi translated">文本看起来仍然凌乱，进行进一步的预处理。</li></ul><h1 id="cbe6" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">标记单词并清理文本</h1><p id="381a" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">使用gensims simple_preprocess()，设置deacc=True以删除标点符号。</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="0686" class="lr jq hi ln b fi ls lt l lu lv">def sent_to_words(sentences):<br/>  for sentence in sentences:<br/>    yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))            #deacc=True removes punctuations<br/>data_words = list(sent_to_words(data))<br/>print(data_words[:1])</span></pre><h1 id="0787" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">创建二元和三元模型</h1><p id="e001" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">二元词是两个在文档中频繁出现的词。三元组是三个经常出现的词。博客的<a class="ae jo" rel="noopener" href="/analytics-vidhya/natural-language-processing-using-spacy-in-python-part-1-ac1bc4ad2b9c"> <strong class="is hj">第一部分</strong> </a>中解释了许多其他技术，这些技术在NLP pipline中非常重要，值得您浏览该博客。<strong class="is hj">短语<em class="ks"> </em> </strong>的两个自变量是<strong class="is hj"> <em class="ks"> </em>最小计数<em class="ks"> </em> </strong>和<strong class="is hj">阈值</strong>。这些参数的值越高，单词就越难组合成二元模型。</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="bbd8" class="lr jq hi ln b fi ls lt l lu lv"># Build the bigram and trigram models<br/>bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) <strong class="ln hj"># higher threshold fewer phrases.</strong><br/>trigram = gensim.models.Phrases(bigram[data_words], threshold=100)<br/># Faster way to get a sentence clubbed as a trigram/bigram<br/>bigram_mod = gensim.models.phrases.Phraser(bigram)<br/>trigram_mod = gensim.models.phrases.Phraser(trigram)<br/># See trigram example<br/>print(trigram_mod[bigram_mod[data_words[0]]])</span></pre><h1 id="f7d3" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">删除停用词，建立二元模型和词条</h1><ul class=""><li id="6265" class="kz la hi is b it kn ix ko jb lw jf lx jj ly jn le lf lg lh bi translated">使用词汇化而不是词干化是一种实践，在主题建模中特别有用，因为词汇化的单词比词干化的更易于阅读。</li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="527f" class="lr jq hi ln b fi ls lt l lu lv"><em class="ks"># Define function for stopwords, bigrams, trigrams and lemmatization</em><br/><strong class="ln hj">def</strong> remove_stopwords(texts):<br/>    <strong class="ln hj">return</strong> [[word <strong class="ln hj">for</strong> word <strong class="ln hj">in</strong> simple_preprocess(str(doc)) <strong class="ln hj">if</strong> word <strong class="ln hj">not</strong> <strong class="ln hj">in</strong> stop_words] <strong class="ln hj">for</strong> doc <strong class="ln hj">in</strong> texts]<br/><br/><strong class="ln hj">def</strong> make_bigrams(texts):<br/>    <strong class="ln hj">return</strong> [bigram_mod[doc] <strong class="ln hj">for</strong> doc <strong class="ln hj">in</strong> texts]<br/><br/><strong class="ln hj">def</strong> make_trigrams(texts):<br/>    <strong class="ln hj">return</strong> [trigram_mod[bigram_mod[doc]] <strong class="ln hj">for</strong> doc <strong class="ln hj">in</strong> texts]<br/><br/><strong class="ln hj">def</strong> lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):<br/>    <em class="ks">"""https://spacy.io/api/annotation"""</em><br/>    texts_out = []<br/>    <strong class="ln hj">for</strong> sent <strong class="ln hj">in</strong> texts:<br/>        doc = nlp(" ".join(sent)) <br/>        texts_out.append([token.lemma_ <strong class="ln hj">for</strong> token <strong class="ln hj">in</strong> doc <strong class="ln hj">if</strong> token.pos_ <strong class="ln hj">in</strong> allowed_postags])<br/>    <strong class="ln hj">return</strong> texts_out</span></pre><h1 id="241c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">按顺序调用函数</h1><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="8f86" class="lr jq hi ln b fi ls lt l lu lv"><em class="ks"># Remove Stop Words</em><br/>data_words_nostops = remove_stopwords(data_words)<br/><br/><em class="ks"># Form Bigrams</em><br/>data_words_bigrams = make_bigrams(data_words_nostops)<br/><br/><em class="ks"># Initialize spacy 'en' model, keeping only tagger component (for efficiency)</em><br/><em class="ks"># python3 -m spacy download en</em><br/>nlp = spacy.load('en', disable=['parser', 'ner'])<br/><br/><em class="ks"># Do lemmatization keeping only noun, adj, vb, adv</em><br/>data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])<br/><br/>print(data_lemmatized[:1])</span></pre><h1 id="c091" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">创建主题建模所需的词典和语料库</h1><ul class=""><li id="a7f6" class="kz la hi is b it kn ix ko jb lw jf lx jj ly jn le lf lg lh bi translated">确保检查字典[id2word]或语料库是否干净，否则你可能得不到高质量的主题。</li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="969b" class="lr jq hi ln b fi ls lt l lu lv"><em class="ks"># Create Dictionary</em> <br/>id2word = corpora.Dictionary(data_lemmatized)  <br/><em class="ks"># Create Corpus</em> <br/>texts = data_lemmatized  <br/><em class="ks"># Term Document Frequency</em> <br/>corpus = [id2word.doc2bow(text) <strong class="ln hj">for</strong> text <strong class="ln hj">in</strong> texts]  <br/><em class="ks"># View</em> <br/>print(corpus[:1])</span></pre><blockquote class="kt ku kv"><p id="06b9" class="iq ir ks is b it iu iv iw ix iy iz ja kw jc jd je kx jg jh ji ky jk jl jm jn hb bi translated"><em class="hi">[(0，1)，(1，1)，(2，1)，(3，1)，(4，1)，(5，5)，(6，1)，(7，1)，(8，2)，(9，1)，(10，1)，(11，1)，(12，1)，(13，1)，(14，1)，(15，1)，(16，1)，(17，1)，(18，1)，(19，1)，(20，2)，(21，1)，(22，1)，(23，1)，(24，1)，(25，1)</em></p></blockquote><ul class=""><li id="0b3d" class="kz la hi is b it iu ix iy jb lb jf lc jj ld jn le lf lg lh bi translated">Gensim为文档中的每个单词创建唯一的id。它的映射<strong class="is hj">字_id </strong>和<strong class="is hj">字_频</strong>。例:(8，2)以上表示，word_id 8在文档中出现两次，以此类推。</li><li id="0496" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">这用作LDA模型的输入。</li></ul><p id="7f50" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您想查看哪个单词对应于给定的<strong class="is hj"> id，</strong>，那么将id作为一个键传递给dictionary。例如:id2word[4]。</p><ul class=""><li id="835d" class="kz la hi is b it iu ix iy jb lb jf lc jj ld jn le lf lg lh bi translated">通过执行下面的代码块，可以获得可读格式的语料库。</li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="e6ce" class="lr jq hi ln b fi ls lt l lu lv">[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]</span></pre><h1 id="abdd" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">构建主题模型</h1><p id="78da" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><strong class="is hj">&gt;LDA的参数</strong></p><ul class=""><li id="957f" class="kz la hi is b it iu ix iy jb lb jf lc jj ld jn le lf lg lh bi translated"><strong class="is hj"> <em class="ks"> Alpha </em> </strong>和<strong class="is hj"> <em class="ks"> Beta </em> </strong>是超参数——Alpha表示文档-主题密度，Beta表示主题-单词密度，<code class="du mf mg mh ln b">chunksize</code>是每个训练块中要使用的文档数量，<code class="du mf mg mh ln b">update_every</code>确定模型参数应该更新的频率，<code class="du mf mg mh ln b">passes</code>是训练遍数的总数。</li><li id="7aaa" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">衡量最佳主题数量的标准实际上取决于你所使用的语料库的种类、语料库的大小以及你希望看到的主题数量。</li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="e98c" class="lr jq hi ln b fi ls lt l lu lv">lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,<br/>                                           id2word=id2word,<br/>                                           num_topics=20, <br/>                                           random_state=100,<br/>                                           update_every=1,<br/>                                           chunksize=100,<br/>                                           passes=10,<br/>                                           alpha='auto',<br/>                                           per_word_topics=<strong class="ln hj">True</strong>)</span></pre><p id="6171" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">查看LDA模型中的主题</p><ul class=""><li id="a9d9" class="kz la hi is b it iu ix iy jb lb jf lc jj ld jn le lf lg lh bi translated">每个主题是关键词的组合，每个关键词对主题有一定的权重。</li><li id="c777" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">使用<strong class="is hj">LDA _ model . print _ topics()</strong>可以看到每个主题的关键词以及每个关键词的权重。</li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="6bc2" class="lr jq hi ln b fi ls lt l lu lv"># Print the keyword of topics<br/>pprint(lda_model.print_topics())<br/>doc_lda = lda_model[corpus]</span></pre><p id="ef1d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出:前5个主题-</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/5c700ee0a07bc393ceca07ce20707d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FFx2MHkOo3w_1dT7.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">图前5个主题</figcaption></figure><ul class=""><li id="b14c" class="kz la hi is b it iu ix iy jb lb jf lc jj ld jn le lf lg lh bi translated">您可以看到对主题有贡献的关键词相关的排名靠前的关键词和权重。</li><li id="b2c2" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">主题是在主题中出现概率最高的词，数字是词在主题分布中出现的概率。</li><li id="71f7" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">但是看关键词能猜出题目是什么吗？</li><li id="972d" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">你可以把topic-4概括为空格(在上图中)。每一个可能在特定的数字上有不同的主题，主题4可能不在现在的位置，它可能在主题10或任何数字上。</li></ul><h1 id="ed7f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">评估主题模型</h1><h2 id="7432" class="lr jq hi bd jr mn mo mp jv mq mr ms jz jb mt mu kd jf mv mw kh jj mx my kl mz bi translated">计算模型复杂度和<a class="ae jo" href="https://rare-technologies.com/what-is-topic-coherence/" rel="noopener ugc nofollow" target="_blank">一致性分数</a></h2><ul class=""><li id="0d3c" class="kz la hi is b it kn ix ko jb lw jf lx jj ly jn le lf lg lh bi translated">连贯分数和困惑度提供了一种方便的方法来衡量一个给定的主题模型有多好。</li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="1a26" class="lr jq hi ln b fi ls lt l lu lv"><em class="ks"># Compute Perplexity</em><br/>print('<strong class="ln hj">\n</strong>Perplexity: ', lda_model.log_perplexity(corpus))  <br/><em class="ks"># a measure of how good the model is. lower the better.</em><br/><br/><em class="ks"># Compute Coherence Score</em><br/>coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')<br/>coherence_lda = coherence_model_lda.get_coherence()<br/>print('<strong class="ln hj">\n</strong>Coherence Score: ', coherence_lda)</span></pre><p id="031a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出:</p><ul class=""><li id="cd78" class="kz la hi is b it iu ix iy jb lb jf lc jj ld jn le lf lg lh bi translated">复杂度越低，模型越好。</li><li id="ad9a" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">话题连贯性越高，话题就越有人情味可解读。</li></ul><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="d1a4" class="lr jq hi ln b fi ls lt l lu lv">Perplexity:  -8.348722848762439  <br/>Coherence Score:  0.4392813747423439</span></pre><h1 id="f1dd" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">可视化主题模型</h1><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="b9ca" class="lr jq hi ln b fi ls lt l lu lv"><strong class="ln hj"># Visualize the topics</strong><br/>pyLDAvis.enable_notebook()<br/>vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)<br/>vis</span></pre><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/59971cfd1981c1f580df142d322aba64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IfkDAs9lFfGQvvZ6.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">图4可视化主题模型</figcaption></figure><p id="0a7f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">左侧的每个气泡代表一个主题。气泡越大，主题就越普遍或占主导地位。好的主题模型将是分散在不同象限的相当大的主题，而不是聚集在一个象限。</p><ul class=""><li id="f1d8" class="kz la hi is b it iu ix iy jb lb jf lc jj ld jn le lf lg lh bi translated">有太多主题的模型会有很多重叠，小气泡聚集在图表的一个区域。</li><li id="ad96" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">如果你移动光标到不同的气泡上，你可以看到与主题相关的不同关键词。</li></ul><p id="9012" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">如何找到最佳话题数？</strong></p><ul class=""><li id="fef8" class="kz la hi is b it iu ix iy jb lb jf lc jj ld jn le lf lg lh bi translated">找到最佳主题数量的一种方法是建立许多具有不同主题数量值的LDA模型，并挑选一个给出最高一致性值的模型。</li><li id="1326" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">如果你看到相同的关键词在多个主题中重复出现，这可能是“k”太大的信号。</li><li id="03b0" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">有时主题关键词可能不足以理解主题的内容。因此，为了更好地理解主题，您可以找到给定主题贡献最大的文档，并通过阅读文档来推断主题。</li><li id="fef3" class="kz la hi is b it ma ix mb jb mc jf md jj me jn le lf lg lh bi translated">最后，我们需要了解主题的数量和分布，以便判断讨论的范围有多广。</li></ul><h1 id="c2fa" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">希望这个博客是有益的</h1><p id="b439" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><strong class="is hj"> <em class="ks">继续学习……..</em>T3】</strong></p><h1 id="61b0" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">谢谢你。</h1></div></div>    
</body>
</html>