<html>
<head>
<title>Word Embeddings: An Introduction to the NLP Landscape</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入:自然语言处理前景介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/word-embeddings-an-introduction-to-the-landscape-dcf20cf391a1?source=collection_archive---------4-----------------------#2019-09-19">https://medium.com/analytics-vidhya/word-embeddings-an-introduction-to-the-landscape-dcf20cf391a1?source=collection_archive---------4-----------------------#2019-09-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="31b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文旨在以简单的方式解释NLP的关键概念，即单词嵌入，以提供单词嵌入是什么、如何使用它们以及为什么它们是构建NLP模型的关键的高级概念。</p><h2 id="094d" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">先决条件:</strong></h2><p id="8782" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">—机器学习概念</p><p id="f584" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">—自然语言处理的基础</p><h2 id="1f69" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">单词嵌入:</strong></h2><p id="5273" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">单词嵌入是一种在D维向量空间中表示单词的方法，其中D可以由您选择。这种向量表示可以用于对单词执行数学运算、查找单词类比、执行情感分析等。</p><p id="8438" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">被广泛使用的最基本的嵌入是<strong class="ih hj"> One Hot Encoding </strong>技术，它通过将每个单词指定为一列来表示向量空间中的分类特征。这个独热编码向量的大小为N×V，其中N是观察值的数量，V是词汇表的大小。</p><p id="4add" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么其他单词嵌入方法优于一键编码？</strong></p><p id="07bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原因是一个热编码向量可以用来表示向量空间中的单词，但是单词的含义或上下文不能被它们捕获。例如，可以使用其他单词嵌入算法来寻找相似的单词，即在相同上下文中使用的单词或具有相似单词的单词，并且通过使用像欧几里德距离或余弦相似性这样的距离度量来计算该上下文相似性。</p><p id="9674" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于独热编码向量，只有一列具有值1，而其他列具有值0，因此任何两个独热字向量之间的欧几里德距离将总是sqrt(2)，因此所有字被认为是同样相似的。距离可以用欧几里德公式(较小= &gt;较近)或余弦距离(较大= &gt;较近)来度量。</p><p id="71a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一方面，单词嵌入捕捉单词的上下文相似性，即“猫”和“猫科动物”向量比“猫”和“飞机”具有更小的距离。在独热编码向量上使用单词嵌入的另一个原因是独热编码的维数随着唯一类别数量的增加而增加，或者用NLP术语来说，随着词汇表的增加而增加。而在单词嵌入中，我们可以选择输出维度，而不考虑vocab的大小。与BOW模型不同，一些高级单词嵌入算法，如BERT和ELMo，也能够处理“不好”等否定。</p><p id="0a71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建单词嵌入的算法:</p><ul class=""><li id="1297" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated">嵌入层</li><li id="f1e4" class="kd ke hi ih b ii km im kn iq ko iu kp iy kq jc ki kj kk kl bi translated">Word2Vec</li><li id="ce74" class="kd ke hi ih b ii km im kn iq ko iu kp iy kq jc ki kj kk kl bi translated">手套</li><li id="f2ef" class="kd ke hi ih b ii km im kn iq ko iu kp iy kq jc ki kj kk kl bi translated">快速文本</li><li id="5e82" class="kd ke hi ih b ii km im kn iq ko iu kp iy kq jc ki kj kk kl bi translated">工程与后勤管理局</li><li id="4ab5" class="kd ke hi ih b ii km im kn iq ko iu kp iy kq jc ki kj kk kl bi translated">伯特</li></ul><h2 id="0066" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">单词嵌入算法</strong></h2><p id="ae33" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">单词嵌入方法从文本语料库中学习预定义的固定大小词汇的实值向量表示。嵌入的学习过程可以作为某些任务(例如文档分类)的神经网络模型的一部分，或者可以使用文档统计以无监督的方式学习。</p><p id="229e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 1。嵌入层</strong></p><p id="6914" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个keras内置层，它使得使用嵌入作为神经网络的一部分变得很容易。该层基于语料库的词汇大小V和嵌入维度D生成权重/嵌入矩阵，如层定义中所指定的。该VxD权重矩阵在每次迭代时被更新，并且得到的权重包含单词在相应单词索引处的权重或嵌入。</p><p id="901f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该层可用于为一组指定的监督学习任务生成嵌入，或者可用于为您的特定用例学习单词的上下文表示。</p><p id="9c24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。Word2Vec </strong></p><p id="6871" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Word2Vec算法侧重于根据单词使用的上下文来构建单词的矢量表示。谷歌的预训练单词向量包含300万个单词和短语，他们从谷歌新闻数据集中训练了大约1000亿个单词，嵌入维数为300，可以下载并直接用于您的项目。</p><p id="a255" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有两种用于生成嵌入的模型架构:</p><ul class=""><li id="1251" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated">连续词袋，或CBOW模型，</li><li id="9275" class="kd ke hi ih b ii km im kn iq ko iu kp iy kq jc ki kj kk kl bi translated">连续跳格模型。</li></ul><p id="95ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CBOW模型通过基于使用上下文大小找到的上下文单词预测当前单词来学习嵌入，上下文大小是由用户选择的超参数。</p><p id="70ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">连续跳格模型通过从目标单词预测周围的上下文单词来学习嵌入。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/5dd2e8c555899687444d9c379558a8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*FtwB9ocTlQq9hHfoeIiDag.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">Word2Vec模型来自“向量空间中单词表示的有效估计”，2013年</figcaption></figure><p id="5414" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于上下文窗口的方法不注意语料库的同现统计，因此未能利用数据中的大量重复。关于Word2Vec及其内部工作的更多信息，可以参考<a class="ae ld" href="https://israelg99.github.io/2017-03-23-Word2Vec-Explained/" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="0f1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。手套</strong></p><p id="6192" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GloVe算法使用上下文计数方法来建立单词共现矩阵，并训练单词向量以基于它们的差异来预测共现比率。</p><p id="7a55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Word2Vec之前，像潜在语义分析(LSA)这样的矩阵分解技术被用来生成单词嵌入。在LSA，矩阵是“术语-文档”类型，即行对应于单词或术语，列对应于语料库中的不同文档。通过使用奇异值分解分解术语文档矩阵来生成词向量。与Word2Vec不同，由此产生的嵌入不能将单词类比表达成简单的算术运算。</p><p id="55bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一方面，GloVe使用固定窗口大小(当单词在固定窗口内一起出现时，它们被认为是共现的)使用局部上下文来计算共现矩阵。在此之后，GLoVe的目标是使用单词向量来预测共现率。</p><p id="4fcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Glove可能比word2vec更快地生成更好的嵌入，因为GloVe使用了全局共现统计以及局部上下文。关于手套背后的数学的更多细节，你可以参考<a class="ae ld" href="https://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="0fc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。快速文本</strong></p><p id="10bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">FastText使用n元字符拆分单词。与其他流行的通过给每个单词分配不同的向量来学习单词表示的模型相反，FastText基于skipgram模型，其中每个单词都被表示为一包字符n-gram。向量表示与每个字符n元语法相关联；单词被表示为这些表示的总和。</p><p id="0422" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种方法是对word2vec和GloVe的重大改进，原因有二:</p><p id="c81d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">推断词汇外单词的能力。例如，“英格兰”与“荷兰”相关，因为land在“lan”和“and”中都有表示。</p><p id="6040" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对拼写错误和错别字的鲁棒性。</p><p id="4b9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更多关于FastText内部工作方式的信息，可以参考<a class="ae ld" href="https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3" rel="noopener" target="_blank">这个</a>。</p><p id="199a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。ELMo(来自语言模型的嵌入)</strong></p><p id="f636" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ELMo表示是biLM的所有内部层的函数，即堆叠在每个最终任务的每个输入字上方的向量的线性组合。ELMo架构将字符串作为输入，使用字符级CNN生成原始单词向量。这些原始单词向量被传递到第一预训练双向语言模型(biLM)层，在该层提取的信息形成中间单词向量。这些中间单词向量作为输入被传递到第二预训练biLM，从该biLM提取的信息形成第二中间单词向量。</p><p id="eef6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过该层生成的三个字向量使用加权和进行组合，并形成ELMo字嵌入。以这种方式组合内部状态允许非常丰富的单词表示，即biLM是从字符而不是单词计算的，它捕捉单词的内部结构。仅ELMo表示法就能显著改善相对误差高达20%。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/242bd6bd74e19da65a160c71e9ed2d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BCXjVSwrMgEpDtJCDyj7EA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">ELMo的代表图。来源:<a class="ae ld" href="https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/" rel="noopener ugc nofollow" target="_blank">分析Vidhya博客</a></figcaption></figure><p id="c352" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ELMo可以从tensorflow hub导入模块直接使用。tfhub上的默认Elmo实现将字符串类型的标记作为输入，如果提供了完整的句子，它将根据空格对其进行拆分。因此，使用标准的文本预处理方法可能有助于提供更好的结果。由于高度复杂和深入的架构，使用ELMo的训练和预测是缓慢的。更多关于ELMo的信息，可以参考<a class="ae ld" href="https://mlexplained.com/2018/06/15/paper-dissected-deep-contextualized-word-representations-explained" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="53b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要了解如何在你的项目中使用ELMo，你可以参考<a class="ae ld" href="https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="2072" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">字符级嵌入</strong></p><p id="cf16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种技术使用ConvNets从字符级编码文本中提取信息。这是ELMo生成原始单词向量的第一步。已经表明，ConvNets可以直接应用于单词的分布式或离散嵌入，而不需要任何关于语言的句法或语义结构的知识，这使得它可以与传统模型竞争。有关使用ConvNets作为嵌入文本方式的更多信息，可以参考这篇<a class="ae ld" href="https://arxiv.org/pdf/1509.01626.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><p id="caf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 6。BERT(来自变压器的双向编码器表示)</strong></p><p id="953a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT是另一种最新的算法，广泛用于NLP任务。它使用词块嵌入，将单词拆分成它们的子单词单元，即书写变成书写+ ing。这种拆分有助于减少词汇量。BERT架构使用底层的变压器模型来保持对序列的关注。关于变压器型号的更多信息可以在<a class="ae ld" href="http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="f222" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT中的另一个不同之处在于，与ELMo不同，ELMo使用biLM，其中一个LSTM被馈送单词和该单词之前的上下文单词，而另一个LSTM被馈送目标单词和目标前面的上下文单词，BERT直接传递整个序列，即输入看起来像这样</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lj"><img src="../Images/2d864ce05f2baa3f6b32b8fe80664415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y7DETFdIDUF0yNU4"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">ELMo的前向和后向LSTM输入以及BERT的组合输入</figcaption></figure><p id="0e37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这有助于模型利用整个句子的信息做出更好的预测。BERT屏蔽了目标单词BERT已经被证明优于一般的单词嵌入以及ELMo。有关BERT的更多信息和深入分析，请参见此处的<a class="ae ld" href="https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/" rel="noopener ugc nofollow" target="_blank"/>。谷歌已经在维基百科上对伯特进行了预培训。</p><p id="ee88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要知道如何使用BERT查找单词嵌入，你可以参考这篇<a class="ae ld" href="https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><h1 id="3642" class="lk je hi bd jf ll lm ln jj lo lp lq jn lr ls lt jq lu lv lw jt lx ly lz jw ma bi translated">使用单词嵌入的不同方式</h1><h2 id="ab8c" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">1.学习嵌入</h2><p id="1cc3" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">可以从语料库中学习嵌入，但是需要大量的文本数据来确保学习到有用的嵌入。可以使用独立的语言模型算法(如Word2Vec、GLoVe等)来训练单词嵌入。在我们想要在多个模型中使用嵌入的情况下，或者我们可以将嵌入训练为特定于任务的模型(如分类)的一部分，这被证明是更有用的，这种方法的主要问题是学习到的嵌入仅特定于手边的任务，因此不能被重用。</p><h2 id="e670" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">2.重用预训练嵌入</h2><p id="c55a" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">研究人员使用上述算法训练的大多数单词嵌入都可以下载，并可以根据嵌入的许可证在项目中使用。</p><p id="b0f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您希望将这些嵌入用于已经为其进行了训练的常规任务，则可以通过在您的模型中将它们保持为不可训练来重用这些嵌入，或者您可以允许更新这些嵌入，从而为手头的任务提供更好的结果。</p><h1 id="c604" class="lk je hi bd jf ll lm ln jj lo lp lq jn lr ls lt jq lu lv lw jt lx ly lz jw ma bi translated">结论</h1><p id="5ef8" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在本文中，我们深入了解了单词嵌入及其用法，以及可以用来生成它们的不同语言模型。在后续文章中，我们将研究如何使用这些单词嵌入来生成文档嵌入。</p><p id="5457" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NLP的快速发展是一个活跃的研究领域，由于XLNet的引入，即使相对较新的技术如BERT也不再是最先进的，XLNet在20个NLP任务中已经超过了BERT。</p><h1 id="ce67" class="lk je hi bd jf ll lm ln jj lo lp lq jn lr ls lt jq lu lv lw jt lx ly lz jw ma bi translated">确认:</h1><p id="621d" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">我想借此机会感谢数据科学顾问Ankush Chandna对撰写本文的帮助。</p><h1 id="3518" class="lk je hi bd jf ll lm ln jj lo lp lq jn lr ls lt jq lu lv lw jt lx ly lz jw ma bi translated">参考</h1><p id="fa00" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated"><a class="ae ld" href="https://machinelearningmastery.com/what-are-word-embeddings/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/what-are-word-embedding/</a></p><p id="00f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ld" href="https://www.kdnuggets.com/2019/01/elmo-contextual-language-embedding.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2019/01/elmo-contextual-language-embedding . html</a></p><p id="c413" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ld" href="https://towardsdatascience.com/fasttext-under-the-hood-11efc57b2b3" rel="noopener" target="_blank">https://towards data science . com/fast text-under-the-hood-11 EFC 57 B2 B3</a></p><p id="a0d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">https://arxiv.org/abs/1607.04606快速文本<a class="ae ld" href="https://arxiv.org/abs/1607.04606" rel="noopener ugc nofollow" target="_blank"/></p><p id="5d0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ld" href="https://mlexplained.com/2018/04/29/paper-dissected-glove-global-vectors-for-word-representation-explained/" rel="noopener ugc nofollow" target="_blank">https://ml explained . com/2018/04/29/paper-parsed-GLoVe-global-vectors-for-word-representation-explained/</a>GLoVe</p><p id="ec8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ld" href="https://mlexplained.com/2018/06/15/paper-dissected-deep-contextualized-word-representations-explained/" rel="noopener ugc nofollow" target="_blank">https://ml explained . com/2018/06/15/paper-parsed-deep-contextized-word-representations-explained/</a>ELMo</p><p id="8fab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ld" href="http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/" rel="noopener ugc nofollow" target="_blank">http://ml explained . com/2017/12/29/attention-is-all-you-need-explained/</a>Transformer</p><p id="87a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ld" href="https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/" rel="noopener ugc nofollow" target="_blank">https://ml explained . com/2019/01/07/paper-parsed-BERT-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/</a>BERT</p><p id="2678" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ld" href="https://arxiv.org/pdf/1509.01626.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1509.01626.pdf</a>角色级转换</p></div></div>    
</body>
</html>