<html>
<head>
<title>Cool Mathematics of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的酷数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/cool-mathematics-of-neural-networks-3e62da62ebcf?source=collection_archive---------34-----------------------#2020-03-29">https://medium.com/analytics-vidhya/cool-mathematics-of-neural-networks-3e62da62ebcf?source=collection_archive---------34-----------------------#2020-03-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e675" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">问候一下，让我们看看神经网络背后的<strong class="ih hj">简单</strong>和<strong class="ih hj">有趣</strong>的数学。</p><h1 id="a5b9" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">第一部分(正向传播):- </strong></h1><p id="8795" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">所以，基本上我们可以认为神经网络是排列在不同层的许多<strong class="ih hj">神经元</strong>的集合。<br/>那么，我们知道了神经网络的基本概念，但是什么是<strong class="ih hj">神经元</strong>？</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/76fc233b6ced28cb78e2c9a148431a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2BcE8xBmjnEz0gD-ALgiA.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated"><strong class="bd jf">神经元</strong></figcaption></figure><p id="a812" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以认为一个神经元接受一些输入，对输入执行一些<strong class="ih hj">计算</strong>并产生输出(相当简单)。我们正在谈论的这些计算是什么？<br/>所以，数学上我们可以把神经元想象成，</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/779aefb653cc5a266d463d929ba1dedd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QmmRyS3jo19FdAl79vHDlQ.png"/></div></div></figure><p id="7cce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下图再次显示了神经元，但这次显示的是权重和逻辑函数(<strong class="ih hj"> sigmoid函数</strong>)。低权重会削弱信号，高权重会放大信号。</p><p id="3549" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">X =重量*输入(W*I)</p><p id="fabf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里每个神经元会分别找到<strong class="ih hj"> X </strong>和<strong class="ih hj"> y(X) </strong>(乙状结肠函数)<strong class="ih hj"> </strong>。<br/>如何计算y(X)？<br/>sigmoid函数，有时也称为逻辑函数，是</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kx"><img src="../Images/c60312379489e9bbd96184daa6c1af30.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*RG3pcbKDSk-ruKwkce3eXQ.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">Sigmoid函数</figcaption></figure><p id="6dcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是一个神经元如何计算它的输出:- <br/> 1)找到X <br/> 2)通过应用如上所示的公式找到y(X)3)一个神经元的输出= y(X)</p><p id="b3bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">一个三层的例子:- <br/> </strong>让我们举一个例子，计算一个三层神经网络的输出:)<br/>下图显示了一个三层神经网络的例子，每层有三个节点。为了保持图表清晰，没有标出所有的重量。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ky"><img src="../Images/073fa02801aeb509d52ce7169f0e4058.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8UWb-YhRVWO5TRB8slcwRA.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">三层神经网络</figcaption></figure><p id="0d76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一层是输入层，我们知道。最后一层是输出层，我们也知道。中间层称为隐藏层。<br/>那么我们开始计算吧，<br/>我们会用矩阵来降低复杂度。所以我们开始吧:)<br/> <strong class="ih hj"> 1) </strong>。我们可以看到三个输入分别是0.9、0.1和0.8。所以输入矩阵<strong class="ih hj"> I. </strong></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kz"><img src="../Images/df08ff8dcf53e3eb3dcbb86465c253db.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*3QnLAASCaokcQyr_C6tUhg.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated"><strong class="bd jf">输入矩阵I </strong></figcaption></figure><p id="c080" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2)。</strong>完成，现在下一步是权重去隐藏层。假设权重矩阵为<strong class="ih hj"> W(input_hidden) </strong>，因为权重是从输入层到隐藏层的。<br/>图表显示了本例中的一些(虚构的)权重，但不是全部。下面显示了所有这些，也是随机编造的。在这个例子中，它们没有什么特别的。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es la"><img src="../Images/c05076d3f60437e8ec96ed49b49a661c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*jPhokjTSWOaytPCefwcD3Q.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">权重矩阵(输入_隐藏)</figcaption></figure><p id="025c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3)。</strong>最后，我们也得到了权重。<br/>……现在我们需要计算X (X = W.I)如前所示。很简单，只要遵循公式，即X=W.I <br/>我们将X表示为<strong class="ih hj"> X(隐藏)</strong>，因为这些是隐藏节点的输入…<strong class="ih hj"> X(隐藏)= W(输入_隐藏)。我</strong></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lb"><img src="../Images/cf2925355d08c463f0f10beb320477b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*0BU4y7D6ybXNxZU1S-dOSg.png"/></div></figure><p id="1535" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Gr8伙计们，我们成功了，但是任务还没有结束。<br/>我们已经计算了隐藏层的输入。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lc"><img src="../Images/6144cd7eb15be6e7ff6fa811f4729f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rGGWeWZ4QKgq-_fl4ObhkQ.png"/></div></div></figure><p id="7fe3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4)。</strong>让我们计算一个隐藏层的输出，即<strong class="ih hj"> O(隐藏)。<br/> </strong>嗯，那么输出是多少呢？非常简单，输出是<strong class="ih hj"> y(X) </strong>我们前面已经看到了。所以让我们找到，<strong class="ih hj"> O(隐藏)= sigmoid(X(隐藏))</strong>即</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ld"><img src="../Images/1b4aab57d96ff86063d2c229902dc87a.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*6KotZlHBw7w5PPv65HqaEA.png"/></div></figure><p id="52dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们也得到了输出(万岁)。只有一件事需要考虑，我们得到了隐藏层的输出，但是输出层的输入和输出层的输出呢？<br/><strong class="ih hj">【EASYYY】</strong>只要按照我们之前做过的相同步骤:- <br/> 1)计算X <br/> 2)计算y(X) <br/>但是输出层的输入有哪些呢？<br/>输出层的输入是前一层的输出(有意义)</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es le"><img src="../Images/55991c4e22d6a5c6f847bebf4075c3da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mJ-_IljQi3TrNirWc2BvFA.png"/></div></div></figure><p id="78f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在你明白了，<br/>所以<strong class="ih hj">输出层的输入(I(output)) = O(hidden) <br/> </strong>那么，让我们找出X对于输出层= X(output) <br/> X = W.I，因此<br/> X(output) = W(hidden_output)。o(隐藏)</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lf"><img src="../Images/4858d376231a7f416969aa2ddf2f086c.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*kLlHMraALUSlJbSbwaSbFA.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">x表示输出层</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es lg"><img src="../Images/9f4535f70d5ecd40658a0c19b164c11f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j1pfPAZZKpAR5BiJydIoJg.png"/></div></div></figure><p id="d240" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">剩下的就是应用sigmoid激活函数，这很容易。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lh"><img src="../Images/2a2d60d99a57a2fc5314306ff84fa785.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*SoAn8Qm4E6t_p7SKjTNrPw.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">就是这样！我们得到了神经网络的最终输出。让我们也在图上展示一下</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es li"><img src="../Images/7111d1e3391b2102dfc4f7edf1dd04c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p59h9bxONWjH9aCp5RayEw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">万岁！万岁</figcaption></figure><ul class=""><li id="9f68" class="lj lk hi ih b ii ij im in iq ll iu lm iy ln jc lo lp lq lr bi translated"><strong class="ih hj"> **这整个过程称为正向传播*** </strong> <br/>，因为我们是从输入层到输出层正向传播</li><li id="8828" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated">因此，我们已经了解了神经网络如何产生输出(前向传播)。在我的第二部分中，我们将看到反向传播是如何工作的</li></ul></div></div>    
</body>
</html>