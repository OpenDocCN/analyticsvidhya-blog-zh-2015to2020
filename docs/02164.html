<html>
<head>
<title>My Probably Wrong Implementation of Back-Propagation and Stochastic Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我可能错误地实现了反向传播和随机梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/my-probably-wrong-implementation-of-back-propagation-and-stochastic-gradient-descent-eb17225b64c9?source=collection_archive---------21-----------------------#2019-12-03">https://medium.com/analytics-vidhya/my-probably-wrong-implementation-of-back-propagation-and-stochastic-gradient-descent-eb17225b64c9?source=collection_archive---------21-----------------------#2019-12-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b211272c29cee97f5254d80e592b2530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLdYUFsAn-EHzNnOpWYrfA.png"/></div></div></figure><p id="0933" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">作为机器学习，特别是神经网络的新学生，当我的算法表现不佳时，我经常感到困惑，我意识到这可能是因为我不明白这些算法在后台做什么。在这一顿悟之后，我自己开始尝试从数学的角度来理解这一切。在整篇文章中，我将着眼于一个2层网络，其中输入层有5个节点，隐藏层有3个节点，输出层有1个节点(注意下图中不包括偏置节点)。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jo"><img src="../Images/3bc5e6d9c676041388bff2f4c677e389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*RJLEyFHyGp8KGC-g_I4bvA.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">图1:</figcaption></figure><h2 id="eef0" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">Sigmoid激活函数</h2><p id="4b1f" class="pw-post-body-paragraph iq ir hi is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">激活函数是将相邻层彼此联系起来的函数，对于这个特定的例子，我使用了sigmoid激活函数，它由下式给出:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/44f256706a1a59e561c5ec0499e800a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*Bwo_BEUnADMTTy9114_oww.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">Sigmoid函数</figcaption></figure><p id="ca6f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">python中计算sigmoid函数的代码:</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="ky kz l"/></div></figure><p id="e4bb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面给出了sigmoid激活函数w.r.t. z的导数，我就不赘述了，但是我用了链式法则。当我们试图最小化我们的成本函数时，这将是很重要的。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es la"><img src="../Images/66c346466323be031541f51d95ea1ba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*5K2e-G-cet0Hc7DSl8DHmg.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">sigmoid函数的导数</figcaption></figure><h2 id="08c7" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">价值函数</h2><p id="a50a" class="pw-post-body-paragraph iq ir hi is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">我选择尝试最小化交叉熵成本函数。我使用了随机梯度下降——它一次查看我们训练集中的一个元素——不需要我对所有例子的成本函数求和。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/8f06dae675b569936028c4c21debc074.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*f86oE4GKb3jyYgn70ilQfQ.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">交叉熵代价函数</figcaption></figure><h2 id="50e8" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">前馈传播</h2><p id="b4bd" class="pw-post-body-paragraph iq ir hi is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">为了从输入层移动到隐藏层，我执行了前向传播，其中我取了θ1，它具有3X6的维度，并且包含我对权重的初始猜测，典型的随机值在[0，非常接近于零的小数字]之间，并且将其乘以(矩阵乘法)转置(x_i)，转置(x_i)是表示我们的训练集的第I个示例的向量，转置(x _ I)的维度是6X1(当包括bias (1)项时)。这个乘法产生了一个3X1的向量，然后我将sigmoid函数应用于向量的每个元素，得到一个a2</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/04eb4ec42d0c751d52923902961a6004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*zC2yW6fU_M_9HjQmMG6mzw.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">输入层到隐藏层</figcaption></figure><p id="ce48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我对a2重复了这个过程。我用1X4矩阵θ2乘以4X1矩阵a2(包括偏差(1)项),得到一个标量，然后应用sigmoid函数得到a3，即我们的预测。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/26ba788f46aaf05022d7e506eebbb3c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*bpWJd7BOcmKcB39IF_7V1g.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">隐藏层到输出层</figcaption></figure><p id="f6f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">python中用于执行前馈传播的函数和用于计算成本的函数。</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="ky kz l"/></div></figure><h2 id="f6c6" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">反向传播</h2><p id="f12b" class="pw-post-body-paragraph iq ir hi is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">这是我要用来寻找成本函数w . r . tθ1和θ2的梯度的过程。我应用链式法则来寻找这两个导数，并使用了上述sigmoid函数的结果。移调的🙈以确保梯度的尺寸与θ的梯度一致。注意，当用θ1计算导数时，我们忽略θ2的第一个元素和a2的第一个元素(偏置项)。</p><p id="9098" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我首先找到预测输出(假设)w.r.t对θ1和θ2的导数。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/d1ec78d341e7732869e42a566f428c0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*48ICA2KraH5jmkuOmKaR2Q.png"/></div></div></figure><p id="2ef2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我回到我的成本函数，应用链式法则，代入上面得到的结果。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/7f9033855c445c39cc0823ea17694fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-Maz-KAPHoz47EiGLo0kg.png"/></div></div></figure><p id="3e8e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对我的训练集的每个元素执行前馈传播和反向传播的python代码。</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="ky kz l"/></div></figure><p id="675f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用该公式给出的随机梯度下降来更新两个θ:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/f5cc6a51f2a107d57a03be0ddb4597be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*qd2R8U75AgTGNL9gVQVFFQ.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">梯度下降算法</figcaption></figure><p id="f83f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">用python更新thetas的代码</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="ky kz l"/></div></figure><p id="3a8d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如文章标题所暗示的，这种实现可能是错误的，如果能得到一些反馈，我将不胜感激。</p></div></div>    
</body>
</html>