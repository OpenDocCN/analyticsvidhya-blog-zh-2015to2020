# 关于集成学习方法我们需要知道什么——简单解释

> 原文：<https://medium.com/analytics-vidhya/what-we-need-to-know-about-ensemble-learning-methods-simply-explained-a285397a9ddd?source=collection_archive---------20----------------------->

![](img/55d8ea234c89ed8c132d73bf9453bde3.png)

Wylly Suhendra 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片。

*这篇文章的目的是介绍集成学习模型的各种概念。我将解释一些必要的关键，以便读者很好地理解相关方法的使用，并能够在需要时设计合适的解决方案。*

*学习模型中误差的主要原因是由于:*

*噪音，*

****偏差*** (预测值与实际值的平均差值，高偏差模型:表现不佳的模型，持续遗漏重要趋势)*

*和 ***方差*** (量化如果使用不同的训练数据，预测估计会有多大的不同，高方差模型:在训练数据上过度拟合，并且不能在看不见的数据上推广模型)。*

*[参见偏差-方差权衡](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)。*

*因此，首先让我们简要解释集成学习，以了解它如何针对这些类型的错误:*

*集成学习是将不同的弱学习者组合成一个预测模型的策略。它站在观念的投票上:一个所谓的“智慧的”的方法或“团结就是力量的*”。**

**主要思想是基于“更多的预测器可以比任何一个单独的预测器产生更好的模型”。**

**这可以通过简单的技术来实现，如最大投票(采用所有预测的模式，主要用于分类问题)、平均或加权平均，或者通过更复杂的计算来实现。**

**![](img/10c79abdaa191506e2eda154f90148df.png)**

**在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上由[Manuel n Geli](https://unsplash.com/@gwundrig?utm_source=medium&utm_medium=referral)拍摄的照片。合奏模型就像一个管弦乐队！:)**

**我们可以使用单个基础学习算法来产生**同质**基础学习器(相同类型的学习器)从而导致同质集成，或者使用**异质**学习器(不同类型的学习器)从而导致异质集成。**

**![](img/6658f6b586b8a4e2b6d68ed4279bc581.png)**

**集成模型结合了来自多个学习者的决策，以提高整体性能。**

**有四种高级类型的集合方法:**

**装袋、增压、堆垛、混合。**

**让我们了解这些先进技术背后的理念:**

**B***agging***:**B**ootstrap**AGG**regat**ING**ensemble 方法，该方法通过获取训练数据的自举子样本并构建 B 模型，在不同的子样本上进行训练。基础(弱)模型独立地建立在这些子集中的每一个上。它们在*平行*中独立运行。最终的预测将通过结合所有模型的结果来确定。**

**( [Bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) :带替换的随机抽样)。**

**Bootstrapping 通过为模型提供不同的数据子集来提供帮助。因此，减少这些模型给出相同结果的机会是有用的。**

**![](img/311a9da22c58b5cab0dc7c1c2b2d31fa.png)**

**随机森林:打包的一个小调整**

*****随机森林*** 遵循 bagging 技术，通过一个小的调整去树的相关性，产生一个非常强大的模型。我们做的一切都和装袋一样，但当我们构建树时，每次我们考虑分割时，都会选择一个随机的特征样本作为分割候选，而不是完整的特征集。当使用全部功能时，我们只是进行打包。**

***它适用于* ***高方差低偏差*** *车型。*如果问题是单个模型得到的性能非常低(高偏差)，那么 bagging 很少会得到更好的偏差。**

**Boosting***:***主要思想是通过使用来自先前构建的模型的信息来改进最终模型。Boosting 旨在减少偏差，同时保持方差较小。它通过缓慢增长来追求方差，通过将许多弱模型组合成一个“超级模型”来追求偏差。**

**增强可以应用于非树模型，但最常用于树。**

**在 Boosting 中，第一个算法在整个数据集上训练。然后依次建立后续算法，拟合前一算法的残差。每个学习器拟合前一个学习器的误差，每个阶段选择最优的学习器和权重。它对先前模型预测不佳的观测值给予较高的权重。嗯，我们都应该从错误中吸取教训(也许不是所有人，但大多数人)😏)。**

**提升是团队合作的结果。**

**损失函数和计算(伪)残差的方式取决于精确的 boosting 算法和如何设置学习参数λ。**

**这些是使用不同底层引擎的一些类型的增压:**

1.  **AdaBoost ( **Ada** 感受性 **Boost** ing):给错误预测的观察值分配权重**
2.  **梯度推进机器(GBM):使用梯度下降来用树更新
    模型，这有助于最小化我们的损失**
3.  **极限梯度提升机(XGBM):还是梯度提升，但是更巧妙！**
4.  **LightGBM:在大数据集方面优于 XGBM，但与 XGBM 相似，不同之处在于它拆分树的方式(LightGBM 是按叶方式，而其他 Boostings 是按层方式)**
5.  **CatBoost(**Cat**egory**Boost**ing):使用类别和目标之间的
    统计关系处理类别变量**

**Boosting 不同于 Bagging，Bagging 以并行格式拟合多个独立的模型，每棵树都将在不同的数据子集上创建。两者都降低了单个估计的方差，因为它们组合了来自不同模型的多个估计，所以结果可能是具有更高稳定性的模型。请记住，如果单一模型的难度过大，那么装袋是更好的选择。**

**S 也叫*叠加概括。*堆叠结合多个模型，基于完整的训练集训练基础层模型，然后在基础层模型的输出上训练元模型作为特征。基础层通常由不同的学习算法组成，因此堆叠集成通常是异构的。**

**所以，首先我们把训练数据分成 K 倍，就像 K 倍交叉验证一样。然后，我们对不同的基本型号重复以下步骤:**

**在 K-1 个部分上拟合基础模型，并对第 K 个部分进行预测。我们对训练数据的每一部分都这样做，并且基础模型在整个训练数据集上拟合，以计算它在测试集上的性能。**

**然后，我们使用来自训练集的预测作为第二级模型的特征。最后，使用第二层模型对测试集进行预测。**

**![](img/d259e1b1a19b471282c35522eade4511.png)**

**罗伯特·阿纳施在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片。堆叠级别:)**

**B lending:类似于堆叠的方法，但是仅使用来自训练集的维持(验证)集来进行预测，并且仅在维持集上进行预测。与堆叠相比，它更简单，信息泄露的风险也更低。**

**因此，首先我们将训练集分成训练集和验证集。然后在训练集上训练基本模型，并只在验证集和测试集上进行预测。验证预测是进行最终模型预测的特征。**

# **外卖食品**

**在这篇文章中，我写了集成学习的基本概述和一些主要概念:引导，打包，随机森林，提升，堆叠和混合。在这些模型的背后，还有很多代码和数学。但这将是一个良好的开端！**

**这篇文章的主要观点如下:**

*   **集成学习使用多个模型(弱学习器/基础模型)来解决同一个问题，然后将它们组合起来以获得更好的性能**
*   **在 Bagging(引导聚集)中，在训练数据的不同子样本(引导)上训练平行独立的相同基础模型，然后在某种“平均”过程中聚集。它适用于高方差低偏差模型。**
*   **在 Boosting 中，第一个模型在整个数据集上进行训练。然后，它依次建立后续模型，并拟合前一个模型的残差。它通过缓慢增长来追求方差，通过将许多弱模型组合成一个“超级模型”来追求偏差。**
*   **在堆叠中，基于完整的训练集训练基础层模型，然后基于基础层模型的输出作为特征训练元模型。**
*   **在混合中，它类似于堆叠，但仅使用训练集中的维持集来进行预测，并且仅在维持集上进行预测。与堆叠相比，它更简单，信息泄露的风险也更低。**

**最后，集成学习可以使用不同的模型和方法来获得针对特定问题的更好的性能。我们只需要很好地理解问题，更有创造力。**