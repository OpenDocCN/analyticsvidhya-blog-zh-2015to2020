<html>
<head>
<title>Time Series Analysis using ARIMA and LSTM(in Python and Keras)-Part2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 ARIMA 和 LSTM 的时间序列分析(Python 和 Keras 版)-第二部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/time-series-analysis-using-arima-and-lstm-in-python-and-keras-part2-74a79636568?source=collection_archive---------5-----------------------#2020-10-09">https://medium.com/analytics-vidhya/time-series-analysis-using-arima-and-lstm-in-python-and-keras-part2-74a79636568?source=collection_archive---------5-----------------------#2020-10-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="5e24" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">时间序列分析的深度学习方法:LSTM RNN</h2></div><blockquote class="ix iy iz"><p id="343d" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><a class="ae jx" href="https://medium.com/p/f987e11f9f8c/edit" rel="noopener">【链接到第一部分】</a></p></blockquote><h2 id="7460" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">介绍</h2><p id="6a79" class="pw-post-body-paragraph ja jb hi jd b je kw ij jg jh kx im jj kj ky jm jn kn kz jq jr kr la ju jv jw hb bi translated"><a class="ae jx" href="https://www.kaggle.com/parijat2018/airline-passenger-lstm" rel="noopener ugc nofollow" target="_blank">【LSTM 内核】</a></p><p id="9dc8" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">LSTM(长短期记忆)是 RNN(递归神经网络)的一种特殊类型，而 RNN 是具有反馈(即，来自先前步骤的递归输入)的 FFNN(前馈神经网络)。让我们首先了解 RNN 是如何从 FFNN 进化而来的。我只是假设你理解一个 FFNN 和休息，我们将从那里建立。</p><h2 id="01dd" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">FFNN 到 RNN 跃迁</h2><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lb"><img src="../Images/39caa7ec7b25bcf164c42e7a04d2eb6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HPlLKFDWQHTxcEoCLiqZmw.gif"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 30:典型的 FFNN</figcaption></figure><p id="dd81" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">图 30 显示了一个典型的 FFNN。</p><p id="f63c" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">让我们在图 31 的动画中看到整个变换(FFNN-&gt;FFNN 折叠模型-&gt;RNN 折叠模型)。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lb"><img src="../Images/5188bdca7833199808866aed9e3dc6b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1bAZOiLrIidxU9q2q6taaA.gif"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 31: FFNN 折叠模型和 RNN 转换</figcaption></figure><p id="bc53" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在让我们一部分一部分来看。</p><p id="54fd" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">首先，让我们画出并讨论一个 FFNN 的折叠模型。它将类似于图 32a 中的结构。</p><p id="5654" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">x 代表输入向量，Y 代表输出向量，h 代表隐藏层。Wx 表示将输入层连接到隐藏层的矩阵，Wy 表示将隐藏层连接到输出层的矩阵。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lr"><img src="../Images/1231fb3478517b2feb6ec2ccbfb31b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0j1LTzqLFqr7FQmngEAhNQ.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 32a: FFNN 折叠模型/图 32b: RNN 折叠模型</figcaption></figure><p id="69c7" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">FFNN 的公式为:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es ls"><img src="../Images/f032ed3bb73d4a2b86401ba71e10e879.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*zqhviqL6TInZ654FQEcYEA.png"/></div></figure><p id="fb6f" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">b 代表偏见，而</p><p id="7083" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">ф，代表 NLFs(非线性函数)</p><p id="6804" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">ф2 对于回归问题可以是 nothing，对于二进制问题可以是 Sigmoid，对于多类分类问题可以是 SoftMax。时间序列分析是一个回归问题，我们在输出层不需要 NLF。</p><p id="182c" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">如果我们用 FFNN 连接一个反馈环路，它将看起来像图 32b 中的结构，形成一个 RNN。简单！</p><p id="2fdb" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">注意，在图 32b 中，X、Y 和 h 标有 t，这表示在序列 t(在时间 t 的时间序列中)的输入、输出和隐藏层。存在附加矩阵 Wh，将序列 t(即 ht)处的隐藏层连接到序列 t+1(即 ht+1)处的隐藏层</p><blockquote class="ix iy iz"><p id="2c23" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">ht 是我们从序列 t 到 t+1 传递的短期记忆或反馈。</p></blockquote><p id="d4b9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">RNN 的方程式是:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lt"><img src="../Images/a6a140ea5eb9d168864d101a0095d9f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*g1kk2mb61_QKxkm_JQdx9Q.png"/></div></figure><p id="e80b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">ф2，同 FFNN 中提到的。</p><p id="895e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">最重要的是，与 RNN 的 FFNN 不同，在序列中，Yt 不仅依赖于 Xt，而且从先前序列 t-1 结转的记忆 ht-1，即 Yt，依赖于来自所有先前步骤的所有输入，即</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lu"><img src="../Images/80872717dac0e4adfa1b0e66da67a5f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*eliPcAlR6Khj3MqTE6Tvjg.png"/></div></figure><p id="2f32" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">让我们更详细地了解一下 RNN。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lv"><img src="../Images/65b214d62cf2145fa18c1a1924696d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OxyFVaqnleizU2KGzFfv5w.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 33: RNN 详细视图</figcaption></figure><p id="8bbb" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">图 33a 表示 RNN 中信息的顺序转换。</p><p id="7883" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">RNN 层也可以堆叠。在图 33a 中，红线显示了 L1 层和 L2 层之间的分界线。图层“L2”堆叠在 L1 的顶部。也可以堆叠更多层。所有层都独立工作，除了上层的输入来自其下一层。这里，Y 从 L1 到 L2，反过来，L2 生产 O。</p><p id="e2d8" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">基于问题的基数，Y 是被采用还是被忽略。如果需要，Keras API 可以输出所有的 Y。如果问题是多对一的，如情感分析，那么只需要最后的 Y(即，基于给定的句子，情感是积极的还是消极的)。这里，句子是顺序数据)。时间序列通常是多对多类型的，因为我们通常被给定一个数据跨度(许多)，我们希望预测一个即时跨度(许多)。此后，我们将把序列看作时间，就像我们在这里处理时间序列一样。现在，让我们把图 33a 中的黄色块(代表当时 RNN 的一切，t)转换成图 33d 中的 FFNN 等价物。输入层的蓝色圆圈(图 33d)代表从时间 t-1 开始的记忆(或反馈),我们将它与输入 Xt 一起输入到隐藏层 ht，最后得到 Yt (=ht)。该 ht 作为短期记忆被传递到下一个时间步骤 t+1(参见图 33a)。隐藏层的每个绿色圆圈代表一个神经元(带 NLF，tanh)，如图 33c 所示。简单 RNN 的箱形模型如图 33b 所示。</p><h2 id="1d42" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">RNN 到 LSTM 的过渡</h2><blockquote class="ix iy iz"><p id="a299" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">现在，下一个问题是，如果 RNN 带着记忆前进，那么为什么不在这里停下来，为什么需要一个先进的 LSTM 模型？</p></blockquote><p id="9181" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">原因是当我们使用梯度下降(或小批量梯度下降)在每个时期或小批量之后更新 W 和 b 时，我们需要在更新 Wh 时通过时间反向传播。由于 RNN 的数学构造，可以看出网络不能记住大约 10 步以上的后退，并开始遗忘。在反向传播过程中(求导时)，Wh 的系数变得接近于零，我们很久以前获得的信息丢失了。这被称为<em class="jc">消失梯度问题</em>，正因为如此，RNN 人无法进行长期记忆。RNN 只能对后退几步进行短期记忆。确切地说，RNN 的这个问题已经在 LSTM 解决了。在 LSTM，长期记忆贯穿始终。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lw"><img src="../Images/d13924750915ce508abba4c77640b0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hNQD4lNda3XIj-u_hhAg7g.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 34: LSTM 详细视图</figcaption></figure><p id="0cb2" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">要将图 33a 中的 RNN 网络转换为 LSTM，我们需要将隐藏层中的 RNN 单元转换为 LSTM 单元(参见图 34a)。RNN 细胞仅由一个双曲正切 NLF 组成，但 LSTM 细胞有σ(*3)、双曲正切(*2)、乘法(*3)和加法(*1)，这就形成了四个门，即遗忘、学习、记忆和使用。</p><blockquote class="ix iy iz"><p id="7f3f" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">ht 是短期记忆，Ct 是长期记忆。</p></blockquote><p id="fe96" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">如图 34b 所示，当 Ct-1 和 ht-1 进入 LSTM 单元时，遗忘门遗忘一些东西，学习门学习一些东西，然后使用遗忘门和学习门更新记忆门和使用门。使用门提供短期记忆，记忆门在时间 t 向 LSTM 网络提供长期记忆。LSTM 的数学构造允许携带长期记忆，并且在反向传播期间不会遭受消失梯度问题。为了让文章不那么数学化和直观，我没有加入方程式。图 34c 显示了一架 LSTM 的箱形模型。</p><p id="03d6" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在，让我们将 LSTM 应用于飞机数据集。这里应该提到的是，在 LSTM 分析中，不需要特别考虑季节性。</p><p id="b860" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">让我们将数据集分为训练和测试(类似于 ARIMA 分析)</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="f7bd" class="jy jz hi ly b fi mc md l me mf">df_train = df[:-24]<br/>df_test=df[-24:]<br/>trainset=df_train.iloc[:,1:] <br/># only taking the NOP (No.of passangers)column</span></pre><p id="7454" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">接下来，让我们对训练数据执行特征缩放，并查看缩放后的数据</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="94a7" class="jy jz hi ly b fi mc md l me mf">from sklearn.preprocessing import MinMaxScaler<br/>sc = MinMaxScaler(feature_range = (0,1))<br/>training_scaled = sc.fit_transform(trainset)</span><span id="70b2" class="jy jz hi ly b fi mg md l me mf">fig, (ax0,ax1) = plt.subplots(ncols=2, figsize=(20,8))<br/>ax0.set_title('Original Distributions')<br/>ax1.set_title('Distributions after MinMax scaler')<br/>sns.kdeplot(df_train['Nop'], ax=ax0)<br/>sns.kdeplot(pd.DataFrame(training_scaled,columns=['Nop'])['Nop'], ax=ax1)</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mh"><img src="../Images/601dea73113d54b48aeb9e24aa6c10d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JKgLlciob1WxR1JPfmWwRg.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 35:缩放前后的列车数据分布</figcaption></figure><p id="5e13" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">很高兴看到缩放后数据分布仍然完好无损。</p><p id="d7dd" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">在特征缩放的拟合和变换操作结束时，我们得到了 training_scaled 数组。</p><p id="8bd0" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在我们导入 Tensorflow 和 Keras。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="d8a2" class="jy jz hi ly b fi mc md l me mf">import tensorflow as tf<br/>keras=tf.keras    <br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import LSTM<br/>from keras.layers import Dropout</span></pre><p id="e025" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我保留了 LSTM 内核的结构，使其可以在脚本和笔记本模式下运行，并对滑动窗口做了更多的实验。我们稍后将讨论窗口大小实验。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="e73c" class="jy jz hi ly b fi mc md l me mf">window_max=4 # is the sliding window size  <br/># Tune this Hyper-parameter #  It's a monthly dataset<br/>trainError_MAPE=[]<br/>testError_MAPE=[]</span></pre><p id="3093" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我说的滑动窗口是什么意思？我们来讨论一下:</p><p id="c2e9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我们的<em class="jc">单变量时间序列</em>数据如下所示:</p><p id="0a16" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">[(t0，x0)，(t1，x1)，(t2，x2)，(t3，x3)，……………。，(tn，xn)]</p><p id="23d8" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">其中 tn =第 n 个时间点，xn =第 n 个时间点的变量值(在我们的例子中为 NOP)</p><p id="8b6a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在，为了训练 LSTM 算法，数据应该转换如下:</p><p id="6ef5" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">[(x0，x1，x2，x3)，(x4)]</p><p id="4b3b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">[(x1，x2，x3，x4)，(x5)]</p><p id="57dd" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">[(x2，x3，x4，x5)，(x6)]</p><p id="c27d" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi">………………….</p><p id="03a2" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">[(xn-3，xn-2，xn-1)，(xn)]</p><p id="2c5e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在，x_train =[(x0，x1，x2，x3)，(x1，x2，x3，x4)，(x2，x3，x4，x5)，…………，(xn-4，xn-3，xn-2，xn-1)]</p><p id="a24b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">y_train=[x4，x5，x6，…..，xn]</p><p id="f634" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">这里，窗口大小是 4(实际上是 5，因为 Python 索引是从 0 开始的)，我们以 1 的步幅滑动它。在窗口中，前 4 个元素进入 x_train，最后一个进入 y_train。我们将(x0，x1，x2，x3)反馈给 LSTM，并尝试预测 x4；我们将(x1，x2，x3，x4)反馈给 LSTM，并尝试预测 X5；我们馈(xn-4，xn-3，xn-2，xn-1)来预测 xn。在每个时段(或小批量)中，该算法利用预测值和实际值(y_tarin)计算误差，并使用梯度下降(或小批量梯度下降)进行反向传播，以更新 Wx、Wy 和 Wh，从而最小化下一个时段(或小批量)中的误差。这里我们将使用一个 32 的小批量，并在每个小批量后反向传播以更新 W。</p><p id="0cad" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我们正在从 training_scaled 数组构建 x_train 和 y_train。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="dc53" class="jy jz hi ly b fi mc md l me mf"># Preparing the training data for LSTM<br/>    <br/>    x_train = []<br/>    y_train = []</span><span id="4c37" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    for i in range(window,len(df_train)):<br/>        x_train.append(training_scaled[i-window:i, 0])<br/>        y_train.append(training_scaled[i,0])<br/>    x_train,y_train = np.array(x_train),np.array(y_train)</span></pre><p id="5ba8" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">此外，请注意 x_train 的维度:</p><p id="b4fe" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">x _ train . shape[0]=[len(training _ scaled)-window _ max+1](行数)，以及</p><p id="b6d8" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">x_train.shape[1]=[window_max](列)</p><p id="f6fc" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">y_train 的大小:x_train.shape[0] x 1(行 x 列)</p><p id="f7fe" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">这里讨论维度是有原因的，LSTM 以 3D 矩阵的形式接受输入。我们得到了一个 2D 训练矩阵。让我们把它转换成 3D:</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="bfbd" class="jy jz hi ly b fi mc md l me mf">x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))</span></pre><p id="ef01" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在形状是:(x_train.shape[0]，x_train.shape[1]，1)</p><p id="f0cf" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">这里，当前的窗口大小是 4。因此，我们借助前三个月的 NOP 值来预测每个第四个月的 NOP 值。您可以增加 window_max 变量，并注册测试和训练的映射，并决定窗口大小，因为较大的窗口大小意味着 LSTM 的较高内存保持力。达到一定的窗口大小后，它可能不会像预期的那样运行。这里还有一件事要提一下:</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="0d59" class="jy jz hi ly b fi mc md l me mf">keras.backend.clear_session()</span></pre><p id="e3b9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">当运行 for 循环时(在窗口实验期间)，我们将在每次运行中进行不同的训练，并且忘记除了期望的 MAPE 变量和循环计数器之外的一切。上面的代码块将在每次运行时释放内存。</p><p id="cb7e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在，让我们进入神经网络的主干，并通过执行一些超参数调整使其变得更强。首先，让我们建立神经网络。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="8947" class="jy jz hi ly b fi mc md l me mf"># %% [code]<br/>    regressor = Sequential()<br/>    regressor.add(LSTM(units = 50,return_sequences = True,input_shape = (x_train.shape[1],1)))<br/>    regressor.add(Dropout(0.2))</span><span id="e26a" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    regressor.add(LSTM(units = 50,return_sequences = True))<br/>    regressor.add(Dropout(0.2))</span><span id="ad25" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    # regressor.add(LSTM(units = 50,return_sequences = True)) # Activate it to see effect<br/>    # regressor.add(Dropout(0.2))</span><span id="d653" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    regressor.add(LSTM(units = 50))<br/>    regressor.add(Dropout(0.2))</span><span id="546a" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    regressor.add(Dense(units = 1)) <br/># only one output for regression value with NO Activation Function</span><span id="48ae" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    regressor.summary()</span></pre><p id="8f62" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">虽然我假设对 FFNN 的基本知识是先决条件，但我想在这里讨论一些事情。让我们从头开始浏览上面的代码块。第一个 LSTM 隐藏层有 50 个 LSTM 单元。我们通过仅提供 x_train 的最后两个维度来提及输入形状。需要注意的最重要的事情是，我们通过提供 return_sequences=True 来添加/堆叠 3 个以上的 LSTM 层。注意，最后一个 LSTM 层有一个<em class="jc">默认 return_sequences=False </em>。为每层增加 20%的下降，作为过度拟合的预防措施。倒数第二个 LSTM 图层在您的实验中被禁用。</p><p id="804c" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">最后，我们附加一个稠密层，它没有任何激活函数，只有一个输出作为它的回归问题。的。summary()函数汇总网络信息。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mi"><img src="../Images/2baca28cbba6ec2a8c0c81b257db48b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GXd1M18Uu8zcKxvsMv6xZg.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">35a: NN 摘要</figcaption></figure><p id="16aa" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在，下一步是引入优化器，声明学习率的初始值(学习率的合适值 lr，在拟合期间通过超参数调整找到)，并编译 NN 模型。Adam 在这里被用作优化器。</p><p id="c977" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">使用下面的 bcode 块，通过 Keras<em class="jc">learning-rate-schedular 函数进行 lr 调谐。</em>初始值是 1e-07，对于后续批次，它将被重新计算并通过 lr_schedule 变量传递给优化器。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="e867" class="jy jz hi ly b fi mc md l me mf"># %% [code]<br/>#     lr_schedule = keras.callbacks.LearningRateScheduler(   <br/>#         lambda epoch: 1e-7 * 10**(epoch / 20))</span><span id="f843" class="jy jz hi ly b fi mg md l me mf"># activate this code during lr tuning</span><span id="5c39" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>#     opt = keras.optimizers.Adam(lr=1e-7) <br/># use this code during lr tuning</span></pre><p id="03e9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在需要对模型进行编译，并提供优化器、损失类型和期望的指标(在我们的例子中是 MAPE)</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="b2f7" class="jy jz hi ly b fi mc md l me mf">regressor.compile(optimizer = opt, loss = 'mse',metrics=['mae','mape'])</span><span id="db3b" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',mode='min',patience=20)</span><span id="9b43" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='loss', mode='min', verbose=0, save_best_only=True)</span><span id="ba4d" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    hist = regressor.fit(x_train, y_train, epochs=150, batch_size=32,callbacks=[mc, lr_schedule,early_stopping])</span></pre><p id="dcbd" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我还使用了 Keras，如果在 20 次以上的迭代中损失值没有变化，就终止拟合过程的提前停止功能。它一直等到 20 次迭代后才终止。</p><p id="e94f" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">模型检查点功能保存损失最小的最佳模型，就像网格搜索一样。</p><p id="5446" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">在拟合期间，early_stopping、lr_schedule 和模型检查点作为列表传递给回调参数。</p><p id="3af0" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">最后，将训练数据、时期和小批量大小传递给。fit()方法。</p><p id="c46e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">拟合后，我们可以获得所有用于训练和验证(在此测试)的指标，如下所示:</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="5285" class="jy jz hi ly b fi mc md l me mf">hist.history.keys()</span></pre><p id="ee77" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我们可以绘制一些指标:纪元与损失图。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="65f1" class="jy jz hi ly b fi mc md l me mf">plt.plot(hist.history['loss'])<br/>plt.show()</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es mj"><img src="../Images/2a7ec9182908c0d5ac822046d873c321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*tlx16LcoU0HAkeLxyvFuxA.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">时代与损失图</figcaption></figure><p id="0e62" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">调谐 lr 时，我们还可以绘制 lr 与损耗的关系图。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="90a0" class="jy jz hi ly b fi mc md l me mf">%% [code]<br/>    ##use this part during lr design only<br/>    plt.semilogx(hist.history["lr"], hist.history["loss"])<br/>    plt.show()</span></pre><p id="b490" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我们可以保持窗口大小为 2、4 和 8，并比较每种情况下的最佳学习率值。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="2d3e" class="jy jz hi ly b fi mc md l me mf">window_max=4<br/>for window in range(4,window_max+1):</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mk"><img src="../Images/a37e369a470c49b3e00ffe2efed56718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qyhar7YMHA_B03LEo9WBEg.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 36:损耗与 lr 值的关系(左:窗口大小=8，右:窗口大小=4)</figcaption></figure><p id="0aab" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我对 4 和 8 进行了实验，发现在这两种情况下，1e-03 和 1e-02 之间都有一个稳定的区域。显然，lr 值取决于窗口大小。我保持窗口大小为 4，因为它给出了从 1e-03 到 1e-01 的宽稳定区域。我冻结 lr 值为 0.5e-02(这将导致窗口大小为 8 的坏结果，这是我的意图。)</p><p id="8b27" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">一旦确定了最佳 lr 值，就用下面的代码替换相应的代码。您可以保留这两项，并禁用其中一项。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="db88" class="jy jz hi ly b fi mc md l me mf">opt = keras.optimizers.Adam(lr=0.5e-2)<br/>hist = regressor.fit(x_train, y_train, epochs=100, batch_size=32,callbacks=[mc,early_stopping],verbose=0)</span></pre><p id="4c22" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在，编译并拟合最佳 lr 值和窗口大小为 4 的模型。</p><p id="f88b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">得到最终拟合的模型后，让我们检查火车 MAPE。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="f924" class="jy jz hi ly b fi mc md l me mf"># %% [code]<br/>    from keras.models import load_model<br/>    regressor=load_model('best_model.h5')</span><span id="9f6f" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    prediction_train=regressor.predict(x_train)</span><span id="ea0a" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    prediction_train = sc.inverse_transform(prediction_train)<br/>    #prediction_train</span><span id="e85c" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    prediction_train.shape</span><span id="4b5c" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    #y_train</span><span id="fd6d" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    y_train=sc.inverse_transform(y_train.reshape(-1,1))</span><span id="d784" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    y_train.shape</span><span id="e4b9" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    #metrics= regressor.evaluate(x_train,y_train)</span><span id="c342" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    #metrics</span><span id="a512" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    train_errors=errors(y_train,prediction_train)<br/>    train_errors</span></pre><p id="939f" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">在上面的代码块中，我们导入了最佳模型‘best _ model . H5’，用 x_train 进行预测。我们已经得到预测作为预测训练，并且逆变换预测训练和 y 训练，并且发现 8.756%的 MAPE</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="1738" class="jy jz hi ly b fi mc md l me mf"># Error Function<br/>def errors(actual,prediction):</span><span id="c652" class="jy jz hi ly b fi mg md l me mf">m=keras.metrics.MeanAbsolutePercentageError()<br/>    n=keras.metrics.MeanAbsoluteError()</span><span id="cedf" class="jy jz hi ly b fi mg md l me mf">m.update_state(actual,prediction)<br/>    n.update_state(actual,prediction)</span><span id="d5f3" class="jy jz hi ly b fi mg md l me mf">error=m.result().numpy() #  MAPE   <br/>    error1=n.result().numpy() # MAE</span><span id="1d8c" class="jy jz hi ly b fi mg md l me mf">return ({'MAE':error1 ,'MAPE':error})</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es ml"><img src="../Images/502b699e5f9d03c3ae59010abc25b714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*CzBHz2g9mCpm0dOTg4oR6g.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 37:列车数据的实际和预测图</figcaption></figure><p id="3ad8" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">图 37 显示了列车数据的实际和预测图。</p><p id="2d72" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">下一步是检查验证 MAPE。</p><p id="eea7" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">让我们准备好测试数据，并提供给 LSTM。</p><p id="c29d" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我们从训练集的后面获取一个窗口大小的数据，并将其附加到测试集的开头，以根据训练数据的最后三个值预测第一个测试数据点。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="2a0c" class="jy jz hi ly b fi mc md l me mf"># %% [code]<br/>inputs=pd.concat((df_train[-(window):],df_test),axis=0,ignore_index=True)</span></pre><p id="e716" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">让我们绘制测试数据。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mm"><img src="../Images/60f4df527ff5a6662f149f26d5c4f156.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHHUbbfcLDXZtMmYsgIa0g.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 38:原始训练和测试数据</figcaption></figure><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mn"><img src="../Images/26d0333e8918a702e6fb9e30fc6953f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HqSUtLbxJ7ijGQqRPEnY2Q.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 38a:(右:原始测试数据，左:为 LSTM 准备的测试数据)</figcaption></figure><p id="e4d9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">其余过程与训练数据相同。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="6eb7" class="jy jz hi ly b fi mc md l me mf"># %% [code]<br/>    testset=inputs['Nop'].values</span><span id="a03a" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    testset = testset.reshape(-1,1)</span><span id="4c00" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    testset_scaled = sc.transform(testset)<br/>    testset_scaled.shape</span><span id="acea" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    x_test=[]<br/>    y_test=[]</span><span id="0696" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    for i in range(window,len(inputs)):<br/>        x_test.append(testset_scaled[i-window:i,0])<br/>        y_test.append(testset_scaled[i,0])<br/>    x_test,y_test = np.array(x_test),np.array(y_test)</span><span id="f21b" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))<br/>    x_test.shape</span><span id="4b24" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    prediction_test = regressor.predict(x_test)</span><span id="884e" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    prediction_test = sc.inverse_transform(prediction_test)<br/>    prediction_test.shape</span><span id="507d" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    y_test = sc.inverse_transform(y_test.reshape(-1,1))</span><span id="84a0" class="jy jz hi ly b fi mg md l me mf"># %% [code]<br/>    test_errors=errors(y_test,prediction_test)<br/>    test_errors</span></pre><p id="a339" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">让我们看看验证图(图 39)。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es mo"><img src="../Images/b01cea4b13871b0ea9cadf8b8cea0761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*4FaeNRRqtprT22XwwShXXA.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 39:验证图</figcaption></figure><blockquote class="ix iy iz"><p id="88d7" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">LSTM 的验证 MAPE 为 8.763%。这比我们从 ARIMA 模型获得的 14.93%的 MAPE 好得多，但是 Auto Arima 技术是一种更快的方法来创建基线结果。</p></blockquote><p id="7a70" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">现在，该做窗口大小变化实验了(保持 lr 常数的最佳值为 0.5e-02)。让我们为一个范围(1，13)运行循环，并绘制结果。</p><pre class="lc ld le lf fd lx ly lz ma aw mb bi"><span id="192a" class="jy jz hi ly b fi mc md l me mf">plt.plot(*zip(*trainError_MAPE),label='Train Error')<br/>plt.plot(*zip(*testError_MAPE),label='Test Error')<br/>plt.legend(['Train Error', 'Test error'], loc='upper left')<br/>plt.xlabel('window size')<br/>plt.ylabel('MAPE')<br/>plt.title('Error')<br/>plt.show()</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es mp"><img src="../Images/a9349754f6f76a3792604272b689775f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*vWUXrl3jNcUdmktDf5IhYA.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图 40:窗口大小与 MAPE 图</figcaption></figure><p id="6993" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我们可以从图 40 中观察到，在窗口大小(WS)区域 1–4 中，模式是期望的，即验证损失&gt;训练损失。同样，在区域 6–8 中，它是一致的，并遵循所需的模式，但从 WS=9 开始，它迅速上升(尽管遵循所需的模式)。WS=5 和 10 显示了相似性，尽管幅度不同。WS=12 时，训练损失很小，验证损失很大。可能是太合身了。这些结果完美地证明了图 36 中的曲线，即，对于不同的窗口大小，我们得到不同的稳定 lr 区域，并且应该相应地选择优化的 lr 值，然后只决定最终的窗口大小。</p><p id="1051" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">我希望你喜欢它，谢谢你的时间。</p><p id="6104" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated"><strong class="jd hj"> <em class="jc">参考文献:</em> </strong></p><p id="6c8a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj kj jl jm jn kn jp jq jr kr jt ju jv jw hb bi translated">Udacity 免费课程:<a class="ae jx" href="https://classroom.udacity.com/courses/ud187" rel="noopener ugc nofollow" target="_blank">深度学习张量流介绍</a></p></div></div>    
</body>
</html>