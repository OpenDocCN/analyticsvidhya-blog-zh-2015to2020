<html>
<head>
<title>Behavior Regularized Offline Reinforcement Learning (BRAC)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">行为规范的离线强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/behavior-regularized-offline-reinforcement-learning-brac-42355d49af5d?source=collection_archive---------19-----------------------#2020-06-14">https://medium.com/analytics-vidhya/behavior-regularized-offline-reinforcement-learning-brac-42355d49af5d?source=collection_archive---------19-----------------------#2020-06-14</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><div class=""/><p id="0f09" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hh bi translated">在这篇文章中，我将简要介绍吴亦帆等人(2019)的论文<a class="ae jj" href="https://arxiv.org/abs/1911.11361" rel="noopener ugc nofollow" target="_blank">行为正规化离线强化学习</a></p><p id="53dd" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hh bi translated">在强化学习(RL)研究中，通常假设访问直接<strong class="in hp">在线</strong>与环境的交互，但在许多现实世界的情况下，对环境的访问是固定到记录经验的<strong class="in hp">离线</strong>数据集的。在这些情况下，标准RL算法的性能很差。</p><p id="d1e5" class="pw-post-body-paragraph il im ho in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hh bi translated">作者介绍了他们的框架，<strong class="in hp">行为规范的演员评论家，</strong>实证…</p></div></div>    
</body>
</html>