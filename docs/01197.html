<html>
<head>
<title>Mathematics behind Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树背后的数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mathematics-behind-decision-tree-73ee2ef82164?source=collection_archive---------2-----------------------#2019-10-08">https://medium.com/analytics-vidhya/mathematics-behind-decision-tree-73ee2ef82164?source=collection_archive---------2-----------------------#2019-10-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/23c0bdf8188cfea35a439ecec6fdf5c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/0*MxUzo0t_m7MfQBWw.png"/></div></figure><p id="484e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">基于嵌套if-else分类器的决策树。它是轴平行超平面的集合，它把区域分成一个超立方体。</p><p id="75a2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">决策树以树结构的形式建立分类或回归模型。随着树深度的增加，它将数据集分解成更小的子集。最终的结果是一个有<strong class="io hj">个决策节点</strong>和<strong class="io hj">个叶节点</strong>的树。决策节点(例如，Outlook)具有两个或更多分支(例如，晴天、阴天和雨天)。叶节点(例如Play)代表一个分类或决策。对应于最佳预测器的树中最顶端的决策节点被称为<strong class="io hj">根节点</strong>。决策树可以处理分类数据和数值数据。</p><h1 id="7d20" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">术语:</h1><ol class=""><li id="8d83" class="ki kj hi io b ip kk it kl ix km jb kn jf ko jj kp kq kr ks bi translated"><strong class="io hj">根节点:</strong>它代表整个总体或样本，并进一步分成两个或更多同类集合。</li><li id="3d11" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated"><strong class="io hj">拆分:</strong>是将一个节点分成两个或两个以上子节点的过程。</li><li id="e741" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated"><strong class="io hj">决策节点:</strong>当一个子节点分裂成更多的子节点时，则称之为决策节点。</li><li id="03f6" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated"><strong class="io hj">叶/端节点:</strong>没有子节点(没有进一步拆分)的节点称为叶或端节点。</li><li id="9d4a" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated"><strong class="io hj">剪枝:</strong>当我们通过移除节点(与分裂相反)来减小决策树的大小时，这个过程叫做剪枝。</li><li id="ae71" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated"><strong class="io hj">分支/子树:</strong>决策树的一个子部分称为分支或子树。</li><li id="86cc" class="ki kj hi io b ip kt it ku ix kv jb kw jf kx jj kp kq kr ks bi translated"><strong class="io hj">父节点和子节点:</strong>被划分为子节点的节点称为子节点的父节点，子节点是父节点的子节点。</li></ol><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es ky"><img src="../Images/d00076879de5e12923ad2c5887841f01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*utXHMy2ADF0Tndyy.png"/></div></div></figure><h1 id="8f14" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">决策树使用的算法:</strong></h1><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/6a535ed0162cc5717ca7b409982186d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*1OdXGJobL3a3okwOhsFgYw.png"/></div></figure><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es li"><img src="../Images/f2eabde48abf7cc60fa26f8ef7046579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sKWbeo9wSBtRH78J.png"/></div></div></figure><p id="2cae" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因为上述数据集在输出中包含两个类。首先找出输出中每一类的概率(P(y+)和P(y-))。</p><p id="09bb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">P(y+) = 9/14，P(y-)=5/14</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/bcc98e25d34e1f0d53041f34ce9a5695.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*Q2OqO2jeVAD2gkJo.png"/></div></figure><h2 id="f473" class="lk jl hi bd jm ll lm ln jq lo lp lq ju ix lr ls jy jb lt lu kc jf lv lw kg lx bi translated">属性:</h2><p id="f9f4" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated">如果类的平均分布大于熵=1。如果一个阶级完全占优，那么熵=0。</p><p id="4fb8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果样本是完全同质的，熵为零，如果样本被等分，则熵为一。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es mb"><img src="../Images/ccf2b39772707bf3f4210ac52c1b20b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sCprm9hs36uo1SUJ"/></div></div></figure><p id="7127" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于高斯分布，数据集分布广泛，因此熵最大，但小于均匀分布，因为所有值相等。</p><p id="edb9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在均匀分布的情况下，数据集是均匀分布的，因此具有最大熵</p><ul class=""><li id="76a3" class="ki kj hi io b ip iq it iu ix mc jb md jf me jj mf kq kr ks bi translated">对于尖峰分布，由于数据集分布不均匀，熵最小，接近于零。</li></ul><h2 id="38b0" class="lk jl hi bd jm ll lm ln jq lo lp lq ju ix lr ls jy jb lt lu kc jf lv lw kg lx bi translated">基尼杂质:</h2><figure class="kz la lb lc fd ij er es paragraph-image"><div class="ab fe cl mg"><img src="../Images/7079431772eb485bfdf4835fa4389f9d.png" data-original-src="https://miro.medium.com/v2/format:webp/0*-9wAZhCxOk2Bugoa.png"/></div></figure><figure class="kz la lb lc fd ij er es paragraph-image"><div class="ab fe cl mg"><img src="../Images/416004d55f6834555dfed09e95912517.png" data-original-src="https://miro.medium.com/v2/format:webp/0*RAkXb70j_sQondtA.png"/></div></figure><p id="c2c9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">基尼系数计算起来更快，因为squire比熵更容易计算，所以基尼系数计算起来更快。在对数计算的情况下所花费的时间比侍从要快得多。所以基尼快多了。</p><h1 id="9cd5" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">信息增益:</h1><p id="0fe6" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated">构建决策树就是要找到一个返回最高信息增益的属性(即最相似的分支)。首先，计算变量突变前的熵(0.94)。也不愿在休息后计算熵。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/7955a165a5d70d31e07280d4bf698bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*QEKhnu-JTyYxcnR7"/></div></figure><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/bce50af5697792681acc11c932f17a16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*bvvhfzF-2_uGqMVOlovbHQ.png"/></div></figure><p id="02c6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">计算变量中每个类的熵值。</p><h1 id="c9e1" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">DT的构造:</h1><p id="8aea" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated"><em class="mj">第一步</em>:计算目标的熵。</p><p id="91a7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="mj">第二步</em>:然后数据集被分割成不同的属性。计算每个分支的熵。然后按比例相加，得到分裂的总熵。从分割前的熵中减去得到的熵。结果是信息增加，或者熵减少。</p><p id="b2ed" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="mj">第三步</em>:选择信息增益最大的属性作为决策节点，按分支划分数据集，在每个分支上重复同样的过程。</p><p id="7fba" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="mj">步骤4a </em>:熵为0的分支为叶节点。</p><p id="eb27" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="mj">步骤4b </em>:熵大于0的分支需要进一步分裂。</p><p id="88e8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">分裂分类变量:</strong></p><p id="8ea6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">尝试打破使用所有特征，选择一个具有最大信息增益的特征。</p><p id="70ec" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">打破每个特征，找到每个节点的信息增益，并选择具有最大IG的节点。</p><p id="ed83" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">断开，直到它变成一个纯节点。</p><p id="b2ce" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果我们的点数很少，我们就不种树，因为过度适应会增加噪音。</p><p id="3b92" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果深度小于欠配合。</p><p id="4031" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">拆分数字特征:</strong></p><p id="b215" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">分类变量很容易被破坏，但数值变量却不容易。对于数字特征，首先将其按升序排序，使一个类(f1，f2…)每个f有两个数据集，因此计算每个f的ID。然后与每个值进行比较，作为潜在的阈值，找到具有最大IG值的f1，您将在该处分裂以形成决策树。</p><h2 id="d9c0" class="lk jl hi bd jm ll lm ln jq lo lp lq ju ix lr ls jy jb lt lu kc jf lv lw kg lx bi translated">功能标准化:</h2><p id="f1cd" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated">我们不必进行特性标准化，因为这里我们只关心小于或大于。它不是基于距离的方法。</p><p id="36d3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">注意:</strong>当有Pincode/zipcode这样的分类变量时。数据集非常高，因为所有的都是不同的。这完全取决于排序值，其中每个排序值都是潜在的阈值。在这种情况下，我们将分类变量转换为数值变量。通过转换成数值变量，摆脱数据稀疏。在这里，每个数字被分成两部分，而在其他情况下，大量的变量可能是无用的。</p><p id="3d8f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">(不需要规范化，不需要标准化)</p><h2 id="22be" class="lk jl hi bd jm ll lm ln jq lo lp lq ju ix lr ls jy jb lt lu kc jf lv lw kg lx bi translated">过度拟合和欠拟合</h2><p id="935b" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated">随着深度的增加，极少数作为纯点的概率增加。所以有过度拟合的可能。如果深度增加，可解释性就会降低。因此<strong class="io hj">使用交叉验证</strong>获得正确的深度。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es mk"><img src="../Images/a8fbda23ffbd4f3ac66e20e385834385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/0*KhBCq6Ibnwp3xfsq.png"/></div></div></figure><p id="ca5f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Underfit有很少的超立方体，而overfit有许多超立方体，这导致在超立方体中作为纯节点的点很少，即overfit。超立方体是一个立方体。</p><h2 id="5e31" class="lk jl hi bd jm ll lm ln jq lo lp lq ju ix lr ls jy jb lt lu kc jf lv lw kg lx bi translated">训练和运行时间复杂性</h2><p id="0945" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated">如果你有一个<strong class="io hj">大数据集</strong>，决策树可能不是一个好的选择。决策树适用于大型数据集，但维度较少。它也适用于低延迟需求。</p><p id="a723" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在平衡树的最好情况下，深度应该是在<em class="mj"> 𝑂 </em> (log <em class="mj"> 𝑁 </em> )O(log⁡N)，但是决策树做局部最优分裂而不太关心平衡。这意味着深度在<em class="mj"> 𝑂 </em> ( <em class="mj"> 𝑁 </em> )O(N)中的最坏情况是可能的——基本上是当每个分割简单地将数据分割成1个和n-1个样本时，其中n是当前节点的样本数。</p><h2 id="4341" class="lk jl hi bd jm ll lm ln jq lo lp lq ju ix lr ls jy jb lt lu kc jf lv lw kg lx bi translated"><strong class="ak">使用决策树进行回归</strong></h2><p id="eaa4" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated">不用IG，用MSE。计算每个节点的均方误差，取下一层的均方误差的等待和，选择减小的均方误差。</p><h2 id="da2c" class="lk jl hi bd jm ll lm ln jq lo lp lq ju ix lr ls jy jb lt lu kc jf lv lw kg lx bi translated">案例:</h2><p id="fc7a" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated"><strong class="io hj">不平衡数据集</strong>:影响熵/MSE计算。所以平衡一下</p><p id="79ea" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">大d </strong>:不好，避免一热编码。如果分类变量有很多特征。将它们转换成数字变量。</p><p id="429f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">多类分类:</strong>你不必做一节休止符(OVR)。熵在计算中已经考虑了多类。</p><p id="76d8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">决策面</strong>:非线性、轴平行超立方体。</p><p id="37ee" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">功能交互:</strong>像f1*f2/f1一样内置在DT中</p><p id="1d6e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">离群值:</strong>影响树并创建不稳定的树</p><p id="6e23" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">可解释性:</strong>深度不大时超可解释性</p><p id="a857" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">特性重要性</strong>:计算特性出现的次数是否很多，并因此增加很多。</p><p id="fe11" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了可视化:</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/1465f0e6971824be52059f036c1652e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*i318iJML8nWSTEKL2osh_A.png"/></div></figure><h1 id="cd03" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">参考:</h1><p id="393d" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix ly iz ja jb lz jd je jf ma jh ji jj hb bi translated">谷歌图片</p><p id="3649" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">应用人工智能</p><div class="mm mn ez fb mo mp"><a href="https://scikit-learn.org/stable/modules/tree.html" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">1.10.决策树-scikit-了解0.21.3文档</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">决策树(DTs)是一种用于分类和回归的非参数监督学习方法。目标是…</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">scikit-learn.org</p></div></div><div class="my l"><div class="mz l na nb nc my nd ik mp"/></div></div></a></div><div class="mm mn ez fb mo mp"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">sk learn . tree . decision tree classifier-sci kit-learn 0 . 21 . 3文档</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">class sk learn . tree . decision tree classifier(criteria = ' Gini '，splitter='best '，max_depth=None，min_samples_split=2…</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">scikit-learn.org</p></div></div><div class="my l"><div class="ne l na nb nc my nd ik mp"/></div></div></a></div><p id="3e97" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">#<a class="ae nf" href="http://homepage.cs.uri.edu/faculty/hamel/courses/2016/spring2016/csc581/lecture-notes/32-decision-trees.pdf" rel="noopener ugc nofollow" target="_blank">http://home page . cs . uri . edu/faculty/hamel/courses/2016/spring 2016/CSC 581/lecture-notes/32-decision-trees . pdf</a></p></div></div>    
</body>
</html>