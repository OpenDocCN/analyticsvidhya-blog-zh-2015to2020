<html>
<head>
<title>The need for activation function along with hidden layers in a Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在神经网络中需要激活函数以及隐藏层</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-need-for-activation-function-along-with-hidden-layers-in-a-neural-network-8b37b4bffa42?source=collection_archive---------4-----------------------#2020-01-30">https://medium.com/analytics-vidhya/the-need-for-activation-function-along-with-hidden-layers-in-a-neural-network-8b37b4bffa42?source=collection_archive---------4-----------------------#2020-01-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9bb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人工神经网络或连接主义系统是由构成动物大脑的生物神经网络模糊启发的计算系统。这种系统通过考虑例子来“学习”执行任务，通常没有用特定于任务的规则来编程。</p><p id="768d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，<strong class="ih hj">神经网络</strong>是一系列算法，通过模拟人脑运作方式的过程，努力识别一组数据中的潜在关系。<strong class="ih hj">神经网络</strong>能适应变化的输入；因此，<strong class="ih hj">网络</strong>无需重新设计输出标准就能产生最佳结果</p><p id="2b3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有许多类型的神经网络。著名的有</p><ul class=""><li id="d62e" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">前馈神经网络——人工神经元。</li><li id="7fd8" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">径向基函数神经网络。</li><li id="2ee8" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">多层感知器</li><li id="589b" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">卷积神经网络</li><li id="ae54" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">递归神经网络(RNN)——长短期记忆</li><li id="c3dc" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">模块化神经网络</li></ul><p id="6e24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有上述网络中的基本单元是一个感知器。那么什么是感知器呢？</p><p id="975b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感知机是在20世纪50年代和60年代由科学家弗兰克·罗森布拉特发明的，受到了沃伦·麦卡洛克和沃尔特·皮茨早期作品的启发。</p><p id="8e84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么感知机是如何工作的呢？感知器接受几个二进制输入，x1，x2，…x1，x2，…，并产生一个二进制输出:</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/984f6e0c33e3980a09f6476c638d32a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*Cg90x9kD1gY2Bk-Pwl_O5g.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">基本感知器</figcaption></figure><p id="58b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在所示的例子中，感知器有三个输入，x1，x2，x3。一般来说，它可以有更多或更少的输入。罗森布拉特提出了一个计算产量的简单规则。他引入了<em class="ke">权重</em>，w1，w2，…w1，w2，…，实数表示各自输入对输出的重要性。神经元的输出，00或11，取决于加权和<br/>∑<em class="ke">j</em>w<em class="ke">j</em>x<em class="ke">j</em>是否小于或大于某个<em class="ke">阈值</em>。就像权重一样，阈值是一个实数，它是神经元的一个参数。用更精确的代数术语来说:</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kf"><img src="../Images/02c8995d6157d3fcdbc6622d513c3857.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*1YUyDoZcszSbbb1-T6q4iQ.jpeg"/></div></figure><p id="488b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是感知器的工作原理！</p><p id="3adf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感知器由4部分组成。</p><p id="4837" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.输入值或一个输入图层</p><p id="10f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.权重和偏差</p><p id="afa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.净和</p><p id="dd1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.激活功能</p><p id="2513" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">权重显示了特定节点的强度。</strong></p><p id="c71b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">添加到净和的<strong class="ih hj">偏差值</strong>允许您上下移动激活函数曲线。</p><h1 id="00eb" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">那么激活函数基本上是什么呢？</h1><p id="97af" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">激活函数是决定神经网络输出的数学方程。<strong class="ih hj">该函数附属于网络中的每个神经元，并根据每个神经元的输入是否与模型的预测相关来确定它是否应该被激活(“触发”)。激活函数也有助于将每个神经元的输出标准化到1和0之间或-1和1之间的范围</strong></p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/52dee64cc14859bad401f93cd8c87bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k9pQ6T6claVVn39ZMcqlKg.png"/></div></div></figure><p id="ec08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激活函数是馈送当前神经元的输入和去往下一层的输出之间的数学“门”。它可以像阶跃函数一样简单，根据规则或阈值打开或关闭神经元输出。或者它可以是将输入信号映射成神经网络运行所需的输出信号的变换。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lo"><img src="../Images/50a72b7ac433748219ec118d6e680dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOzGW6CH7f1Mi3oh2z53sw.png"/></div></div></figure><p id="6efe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激活函数的<strong class="ih hj">主要</strong>目的是<strong class="ih hj">将非线性</strong>引入神经元的输出。我们都知道，一个神经网络包含许多隐含层，以不同的权重作为它们的参数。现在<strong class="ih hj">为什么我们需要非线性进入神经网络？</strong></p><p id="72cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">没有非线性激活函数的神经网络本质上只是线性回归模型。<strong class="ih hj">证明？</strong></p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lp"><img src="../Images/89696d37ced6c9ff4fcf296e4b8b85d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PjksM0dOimpaw-TBbRjNLA.png"/></div></div></figure><p id="35d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如果我们使用线性激活函数或不使用激活函数，神经网络的隐藏层将变得无用，因为两个或多个线性函数的组合本身就是一个线性函数</strong></p><blockquote class="lq"><p id="5af4" class="lr ls hi bd lt lu lv lw lx ly lz jc dx translated">使用非线性激活函数的另一个原因是不可能使用反向传播(梯度下降)来训练使用线性激活函数的模型——函数的导数是一个常数，与输入x无关。因此不可能回过头来了解输入神经元中的哪些权重可以提供更好的预测。</p></blockquote><p id="4ee5" class="pw-post-body-paragraph if ig hi ih b ii ma ik il im mb io ip iq mc is it iu md iw ix iy me ja jb jc hb bi translated">因此，非线性激活函数也被用作<strong class="ih hj">它们允许反向传播，因为它们具有与输入相关的导数函数。</strong></p><p id="5c65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激活功能对输入进行非线性转换，使其能够学习和执行更复杂的任务。</p><p id="e880" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激活功能有很多种，如<strong class="ih hj">乙状激活</strong> <strong class="ih hj">功能</strong>、<strong class="ih hj"> ReLu </strong>、<strong class="ih hj"> Softmax </strong>等。</p><p id="031c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以问题是非线性激活函数如何在网络中引起非线性？</p><p id="ae60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们以ReLu激活功能为例</p><p id="6061" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激活的阈值简单地为零:<em class="ke"> R(x) = max(0，x)，即如果x &lt; 0，R(x) = 0，并且如果x &gt; = 0，R(x) = x </em></p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mf"><img src="../Images/04ea58a7e5b26c39a4dc1940b2dc10b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*tZi4IbIPpQV5shwBs0BAsw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">ReLU函数</figcaption></figure><p id="01c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Relu输出不是一条直线，它在x轴弯曲。更有趣的一点是，<strong class="ih hj">这种非线性</strong>的后果是什么？简单来说，线性函数允许您使用直线来剖析特征平面。但是<strong class="ih hj">利用Relu的非线性，我们可以在特征平面上构建任意形状的曲线。</strong></p><p id="0579" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有了Relu，分类边界不再是一条穿过x-y位置的直线。x轴下方的部分<strong class="ih hj">被切掉</strong>，产生<strong class="ih hj"> <em class="ke">角度</em>区域</strong></p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es mg"><img src="../Images/f330a779e41de6b5a5e7a9da1fe325ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*YJ6bk9h8HcZI16pFA3SPXA.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">绘制函数max(0，-5–2x)</figcaption></figure><p id="9dea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以放入不同的权重来观察它如何影响决策区域。</p><p id="10dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">relu组，也称为网络的“隐藏层”，因为它们隐藏在输入和输出之间，产生这些角度决策区域</strong>。通过应用线性模型<strong class="ih hj">将所有这些角度区域加在一起</strong>来构建最终输出。</p><p id="141f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，在下图中，我们添加了四个ReLU函数<strong class="ih hj">ReLU(-4–2x+y)、ReLU(4+2x+y)、ReLU(4-x-2y)&amp;ReLU(9+2x-y)</strong></p><p id="efde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终输出是这个任意形状的决策边界，用绿色突出显示。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mh"><img src="../Images/ffbd971cfea46feece82ab03fb820de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wooQ69A11N8tmwDhmpQQsQ.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">绿色分类边界是近似的</figcaption></figure><blockquote class="lq"><p id="124b" class="lr ls hi bd lt lu mi mj mk ml mm jc dx translated">激活函数集合逼近任何曲线的这种精致能力是所有类型的神经网络模型的最终垫脚石</p></blockquote><p id="a6f7" class="pw-post-body-paragraph if ig hi ih b ii ma ik il im mb io ip iq mc is it iu md iw ix iy me ja jb jc hb bi translated"><strong class="ih hj"/><strong class="ih hj">隐藏层数</strong>越多<strong class="ih hj">越准确</strong>这个近似值越小<strong class="ih hj">越小</strong>就是封闭的区域或者决策边界。因此<strong class="ih hj">增加更多的隐藏层数</strong>总是会导致更好的预测(分类)。使用20–30或更多数量的ReLUs使决策边界更加“非线性”和“平滑”,从而改善分类。</p><p id="de8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总结就是神经网络有能力建立<strong class="ih hj"> <em class="ke">任意形状的分类边界</em> </strong>，这使得它们成为深度学习领域非常有用的工具。</p></div></div>    
</body>
</html>