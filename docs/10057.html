<html>
<head>
<title>Sentiment Analysis — Movies Reviews</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">情感分析—电影评论</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sentiment-analysis-movies-reviews-b299de4bb1aa?source=collection_archive---------2-----------------------#2020-10-03">https://medium.com/analytics-vidhya/sentiment-analysis-movies-reviews-b299de4bb1aa?source=collection_archive---------2-----------------------#2020-10-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7050" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用 sklearn 进行情感分析— 89%的准确率</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/de940ded417e817101cb719dc1d7ed96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eth5yLIvqfZh1njDwJiVGw.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片来源:<a class="ae jt" href="https://shichaoji.com/2017/04/17/sentiment-analysis-deployed/" rel="noopener ugc nofollow" target="_blank">https://Shi chaoji . com/2017/04/17/情操-分析-部署/ </a></figcaption></figure><p id="bc4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据分析不再关注数字数据，但声音、图像和文本也是提供有价值见解的重要信息。随着本文的分析，这是文本数据的重点。文本数据可以从多个来源收集，包括产品评论、评论、问答、社交网络帖子。在下一篇文章中，我们将抓取这些文本数据。</p><p id="874f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">情感分析怎么样？它是什么，如何用机器学习来分析它们？情感分析是自然语言中重要的文本分析应用之一，在商业和研究领域都有应用。这种分析是一种二元监督分类(我们可以考虑正面或负面的结果)，其中训练模型将学习正确的类别标签，然后使用这种学习来预测测试数据。</p><p id="543b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是一些情感分析的真实例子:<br/> -品牌监测使用情感分析来监测消费者对其品牌和产品的看法。<br/> -研究人员收集这些分析，以了解人们对特定事物的看法。<br/> -产品改进收集信息以确定消费者不满意或满意的地方。</p><h1 id="2e7d" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">这篇文章的大纲如下:</strong></h1><p id="735a" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">1.数据概述(分类问题)<br/> 2。文本预处理(http，小写等…) <br/> 3。单词嵌入技术(Bag of word，TF-IDF，Hashing) <br/> 4。分析技术(岭分类器和多项式分类器)5。选择模型并运行准确性测试(F1，精确度与召回率，混淆矩阵)</p><h1 id="df66" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">1.数据概述</h1><p id="713a" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在此分析中，我们将使用 50K IMBD 电影评论数据集，其中有 25K 正面评论和 25K 负面评论。数据可以在<a class="ae jt" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank">这里</a>找到！</p><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="8cc0" class="lc jv hi ky b fi ld le l lf lg">import pandas as pd<br/>import numpy as np</span><span id="e415" class="lc jv hi ky b fi lh le l lf lg">df = pd.read_csv('.../IMDB Dataset.csv')</span><span id="b1ad" class="lc jv hi ky b fi lh le l lf lg">df['sentiment'].value_counts()<br/>df.head(5)</span></pre><p id="fe8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们来看看在正面评论和负面评论中出现了什么词。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/a011fbed7e0089332bb1e4a863783164.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*vhKcBoO1ImbiRQ49F2ASQg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">正面评价</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/b36efae29a2db235a056462b976281d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*g1bO34nGm7c4cjL282fA9w.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">负面评论</figcaption></figure><p id="9213" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有了这些数据，我们将通过使用 sklearn 的 train_test_split 来拆分成训练集和测试集。</p><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="75eb" class="lc jv hi ky b fi ld le l lf lg">from sklearn.model_selection import train_test_split<br/>train, test = train_test_split(df)</span></pre><h1 id="c76a" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">2.文本处理</h1><p id="769e" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">为什么我们需要对文本进行预处理？</p><p id="540a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当人们写评论时，它可以是非正式的语言，例如，yours 和 urs。如果我们不教机器，它们会把它理解成不同单词。这只是文本分析面临的一个示例问题。其他问题可能包括拼写错误、大写字母等..我们需要通过进行文本预处理来处理这些问题，或者在将文本数据输入模型之前准备好文本数据。让我们看看如何一步一步地做到这些。</p><p id="fa1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.1 将情感转化为类别值<br/> </strong>在处理类别数据时，我们可以将这些文本类别转化为数字。例如，是和否类别可以变成 1 和 0。在这些情况下，我们有消极和积极的情绪。我们要把它们变成 0 和 1 的值。</p><p id="af6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.2 删除无文本和特殊字符<br/>T5】根据我们现有的审查数据，我们有&lt; br &gt;在上下文中不被视为文本。它没有任何意义，不会为模型提供任何有用的信息，因此必须将其删除。在其他文本分析场景中，文本数据可能包括网站链接、标签等。这些东西最好在我们运行模型之前从文本中删除。</strong></p><p id="e188" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.3 将所有文本转换为小写<br/> </strong>为了避免在培训过程中出现错误，例如“我们”和“我们”可能会以不同的方式学习，我们将所有大写字母的单词转换为小写字母。</p><p id="5a73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.4 去除换行符<br/> </strong>在很多编码语言中，当我们想拆分一行时，我们就写成“\n”。这有时可以在上下文中检测到，但是，它对我们的模型没有意义。因此，这需要被移除。</p><p id="c6b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.5 标记化</strong></p><p id="de0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个句子由许多单词组成，但并不是所有的单词都很重要。为了分析每个单词，我们需要为每个句子将单词拆分成单个单词。</p><p id="dca4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.6 删除停用词<br/> </strong>停用词是指“我”、“我们”、“我的”、“你”、“自己的”、“唯一的”等词。这些词不太可能代表特定的意思。模型可能认为这是噪声，因此我们将其移除以降低噪声水平。</p><p id="df7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.7 词汇化 vs 词干化<br/> </strong>我们知道‘studies’和‘study’之类的词是一回事，但机器不知道这一点。因此，我们应用词汇化来管理这些词。我们为什么不用词干？这是因为词干截断了字母表，只留下了单词的一部分，例如,“studies”会变成我们不想要的“studi”。</p><p id="dc53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，从第 2.5 步开始，我们把句子中的单词分割成单个单词，我们要把它恢复成原来的格式。</p><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="9116" class="lc jv hi ky b fi ld le l lf lg"><strong class="ky hj"># 2.1 Turn sentiment into categorical value</strong></span><span id="163f" class="lc jv hi ky b fi lh le l lf lg">from sklearn.preprocessing import LabelEncoder</span><span id="8b42" class="lc jv hi ky b fi lh le l lf lg">labelencoder = LabelEncoder()</span><span id="9787" class="lc jv hi ky b fi lh le l lf lg">train['sentiment'] = labelencoder.fit_transform(train['sentiment'])<br/>test['sentiment'] = labelencoder.fit_transform(test['sentiment'])<br/></span><span id="add3" class="lc jv hi ky b fi lh le l lf lg"><strong class="ky hj"># 2.2 Remove none text and special character</strong></span><span id="19e4" class="lc jv hi ky b fi lh le l lf lg">import re<br/>import string</span><span id="efe4" class="lc jv hi ky b fi lh le l lf lg">pattern = re.compile(r'&lt;br\s*/&gt;&lt;br\s*/&gt;&gt;*|(\-)|(\\)|(\/)')</span><span id="efbf" class="lc jv hi ky b fi lh le l lf lg">def preprocess_reviews(reviews):<br/>    reviews = [pattern.sub(" ",item) for item in reviews]<br/>    return reviews</span><span id="45fc" class="lc jv hi ky b fi lh le l lf lg">train_clean = preprocess_reviews(train['review'])<br/>test_clean = preprocess_reviews(test['review'])</span><span id="1986" class="lc jv hi ky b fi lh le l lf lg">train['review'] = train_clean<br/>test['review'] = test_clean</span><span id="b5a8" class="lc jv hi ky b fi lh le l lf lg">def remove_punctuation(input):<br/>    table = str.maketrans('','',string.punctuation)<br/>    return input.translate(table)</span><span id="57e3" class="lc jv hi ky b fi lh le l lf lg">train['review'] = train['review'].apply(remove_punctuation)<br/>test['review'] = test['review'].apply(remove_punctuation)</span><span id="43cd" class="lc jv hi ky b fi lh le l lf lg"><strong class="ky hj"># 2.3 Convert all text to lowercase</strong></span><span id="b060" class="lc jv hi ky b fi lh le l lf lg">train['review'] = train['review'].str.lower()<br/>test['review'] = test['review'].str.lower()</span><span id="0f53" class="lc jv hi ky b fi lh le l lf lg"><strong class="ky hj"># 2.4</strong> <strong class="ky hj">Remove line breaks</strong></span><span id="4fd9" class="lc jv hi ky b fi lh le l lf lg">def remove_linebreaks(input):<br/>    text = re.compile(r'\n')<br/>    return text.sub(r' ',input)</span><span id="d551" class="lc jv hi ky b fi lh le l lf lg">train['review'] = train['review'].apply(remove_linebreaks)<br/>test['review'] = test['review'].apply(remove_linebreaks)</span><span id="6de4" class="lc jv hi ky b fi lh le l lf lg"><strong class="ky hj"># 2.5 Tokenization</strong></span><span id="41f9" class="lc jv hi ky b fi lh le l lf lg">from nltk.tokenize import word_tokenize</span><span id="7ad6" class="lc jv hi ky b fi lh le l lf lg">train['review'] = train['review'].apply(word_tokenize)<br/>test['review'] = test['review'].apply(word_tokenize)</span><span id="d3e6" class="lc jv hi ky b fi lh le l lf lg"><strong class="ky hj"># 2.6 Remove stopword</strong></span><span id="1b83" class="lc jv hi ky b fi lh le l lf lg">from nltk.corpus import stopwords</span><span id="fe87" class="lc jv hi ky b fi lh le l lf lg">def remove_stopwords(input1):<br/>    words = []<br/>    for word in input1:<br/>        if word not in stopwords.words('english'):<br/>            words.append(word)<br/>    return words</span><span id="02e5" class="lc jv hi ky b fi lh le l lf lg">train['review'] = train['review'].apply(remove_stopwords)<br/>test['review'] = test['review'].apply(remove_stopwords)</span><span id="457e" class="lc jv hi ky b fi lh le l lf lg"># <strong class="ky hj">2.7 Lemmatization</strong></span><span id="586e" class="lc jv hi ky b fi lh le l lf lg">from nltk.stem import WordNetLemmatizer<br/>lem = WordNetLemmatizer()</span><span id="790d" class="lc jv hi ky b fi lh le l lf lg">def lemma_wordnet(input):<br/>    return [lem.lemmatize(w) for w in input]</span><span id="0b62" class="lc jv hi ky b fi lh le l lf lg">train['review'] = train['review'].apply(lemma_wordnet)<br/>test['review'] = test['review'].apply(lemma_wordnet)</span><span id="767b" class="lc jv hi ky b fi lh le l lf lg"># <strong class="ky hj">2.8 Combine individual words</strong></span><span id="d571" class="lc jv hi ky b fi lh le l lf lg">def combine_text(input):<br/>    combined = ' '.join(input)<br/>    return combined</span><span id="3c4c" class="lc jv hi ky b fi lh le l lf lg">train['review'] = train['review'].apply(combine_text)<br/>test['review'] = test['review'].apply(combine_text)</span></pre><h1 id="05b7" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">3.单词嵌入技术</h1><p id="1e81" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">现在，在我们使用机器学习来进一步实施情感分析之前，我们需要将这些文本转化为数值，这被称为单词的向量代表。要做到这一点，有很多技巧。让我们来看看其中的一些。</p><p id="ab55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3.1 单词袋<br/> </strong>这是最广为人知的将文本转化为数字的技术之一。这种技术考虑句子中的单词，忽略它们的顺序，因为它关注每个单词的频率。要应用这种技术，我们可以使用 sklearn 的 CountVectorizer。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lj"><img src="../Images/a5507f9104803bbb2213ee01b7c1af2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HjtNvZ-xvqy4MJpzPbT1Fw.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片来源:<a class="ae jt" href="https://www.programmersought.com/article/4304366575/" rel="noopener ugc nofollow" target="_blank">https://www.programmersought.com/article/4304366575/</a></figcaption></figure><p id="f51d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3.2 TF-IDF <br/> </strong>该技术计算单词在文档中的重要性。TF，术语频率，测量术语在每个文档中的频率。IDF(逆文档频率)衡量该术语在所有文档中的重要性。然后两个值相乘，TF*IDF，我们发现每个单词在文档中有多重要。要应用这种技术，我们可以使用 sklearn 的 TdidfVectorizer。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lk"><img src="../Images/4252209b1286c32c5901d0b36d52f00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Co00Ttks463HJHyTiTjMoA.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片来源:<a class="ae jt" rel="noopener" href="/shallow-thoughts-about-deep-learning/can-tfidf-be-applied-to-scene-interpretation-140be2879b1b">https://medium . com/shallow-thoughts-about-deep-learning/can-tfi df-be-applied-to-scene-interpretation-140 be 2879 B1 b</a></figcaption></figure><p id="6f69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3.3 哈希<br/> </strong>使用哈希，文本上下文将被转换为类似位大小值的哈希值，其中每个上下文将被计算并分配一个唯一值。这些算法是为防冲突而设计的，这意味着对同一文本产生不同值的可能性很低。为了应用这种技术，我们可以使用 sklearn 的 HashingVectorizer。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ll"><img src="../Images/6692a3660d06ffd20c43ee322a28e4a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EICrfMVka6B493jwPfofag.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片来源:<a class="ae jt" href="http://xeushack.com/on-hashes" rel="noopener ugc nofollow" target="_blank">http://xeushack.com/on-hashes</a></figcaption></figure><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="9c7b" class="lc jv hi ky b fi ld le l lf lg"><strong class="ky hj"># 3.1 Bag of words</strong><br/>from sklearn.feature_extraction.text import CountVectorizer</span><span id="7dc5" class="lc jv hi ky b fi lh le l lf lg">cv = CountVectorizer(binary=True)<br/>cv.fit(train)<br/>X_train_bow = cv.fit_transform(train['review'])<br/>X_test_bow = train['sentiment']<br/>Y_train_bow = cv.transform(test['review'])<br/>Y_test = test['sentiment']</span><span id="c2fa" class="lc jv hi ky b fi lh le l lf lg"><strong class="ky hj"># 3.2 TF-IDF</strong></span><span id="ef94" class="lc jv hi ky b fi lh le l lf lg">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="318b" class="lc jv hi ky b fi lh le l lf lg">vectorizer = feature_extraction.text.TfidfVectorizer(norm = None)</span><span id="3313" class="lc jv hi ky b fi lh le l lf lg">vectorizer.fit(train)<br/>X_train_tfidf = vectorizer.fit_transform(train['review'])<br/>X_test_tfidf = train['sentiment']<br/>Y_train_tdidf =vectorizer.transform(test['review'])</span><span id="d62e" class="lc jv hi ky b fi lh le l lf lg"><strong class="ky hj"><br/># 3.3 Hashing</strong></span><span id="11f4" class="lc jv hi ky b fi lh le l lf lg">from sklearn.feature_extraction.text import HashingVectorizer</span><span id="539b" class="lc jv hi ky b fi lh le l lf lg">hv = HashingVectorizer()</span><span id="f7b6" class="lc jv hi ky b fi lh le l lf lg">hv.fit(train)<br/>X_train_hash = hv.fit_transform(train['review'])<br/>X_test_hash = train['sentiment']<br/>Y_train_hash = hv.transform(test['review'])</span></pre><h1 id="2404" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">4.分析模型</h1><p id="d2ca" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">对于我们将要实现的不同模型，我们选择 F1 分数来决定一个模型如何优于另一个模型。F1 分数如下计算精确度和召回率的调和平均值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/6f5729be1d6017f05356e0723aca3bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*bf6uQbqSZFFU3R7DNQM0gQ.png"/></div></figure><p id="e69d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还对每种技术应用 5 倍交叉验证，并确定每倍的 F1 分数，平均这 5 倍的 F1 分数，并为我们的测试集选择平均 F1 分数最高的模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ln"><img src="../Images/b24af0edef3d9628252eebd445defde5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2LeMk1SZIATP6xfsjvNyvg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片来源:<a class="ae jt" href="https://www.datasciencecentral.com/profiles/blogs/cross-validation-in-one-picture" rel="noopener ugc nofollow" target="_blank">https://www . datascience central . com/profiles/blogs/cross-validation-in-one-picture</a></figcaption></figure><p id="d5b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4.1 岭分类器</strong> <br/>岭分类器是基于岭回归方法的计算，其中目标变量将变成-1 和 1 值。</p><p id="6dc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4.2 多项贝叶斯</strong> <br/>多项朴素贝叶斯或多项贝叶斯是朴素贝叶斯的一种模型，朴素贝叶斯是一种概率分类器。该模型需要整数特征计数，适合离散特征的分类问题。</p><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="b37e" class="lc jv hi ky b fi ld le l lf lg"><strong class="ky hj"># Rigde with bag of word</strong></span><span id="21ab" class="lc jv hi ky b fi lh le l lf lg">from sklearn import linear_model</span><span id="d4a8" class="lc jv hi ky b fi lh le l lf lg">alpha = [80.0, 90.0, 100.0, 110.0, 120.0]</span><span id="2a3f" class="lc jv hi ky b fi lh le l lf lg">for a in alpha:<br/>    ridge = linear_model.RidgeClassifier(a)<br/>    scores = model_selection.cross_val_score(ridge, X_train_bow, X_test_bow, cv=5, scoring='f1')<br/>    print("alpha: ",a)<br/>    print(scores)<br/>    print(np.mean(scores))<br/>    print('\n')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/a67dd3ecf6ebc9bfcc3109ec0adbfde8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*bKdHUpyFlV_Tap8I5iFmHw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">f1-单词袋的 5 倍交叉验证的分数</figcaption></figure><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="8178" class="lc jv hi ky b fi ld le l lf lg"><strong class="ky hj"># MultinomialNB with bag of word</strong></span><span id="d85e" class="lc jv hi ky b fi lh le l lf lg">from sklearn.naive_bayes import MultinomialNB</span><span id="5739" class="lc jv hi ky b fi lh le l lf lg">alpha = [1e-10, 1e-5, 0.1, 1.0, 2.0, 5.0]</span><span id="1d77" class="lc jv hi ky b fi lh le l lf lg">for a in alpha:<br/>    mnb = MultinomialNB(a)<br/>    scores = model_selection.cross_val_score(mnb, X_train_bow, X_test_bow, cv=5, scoring='f1')<br/>    print('alpha: ', a)<br/>    print(scores)<br/>    print(np.mean(scores))<br/>    print('\n')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/8c20de2771b68eb114f0260fb14b2b3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*iZKxPYupCjNv010WJ_ravA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">f1-对单词袋进行 5 倍交叉验证的得分</figcaption></figure><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="2c64" class="lc jv hi ky b fi ld le l lf lg"><strong class="ky hj"># Rigde with TF-IDF</strong></span><span id="4e10" class="lc jv hi ky b fi lh le l lf lg">alpha = [500.0, 1500.0, 2500.0, 3000.0]</span><span id="60d6" class="lc jv hi ky b fi lh le l lf lg">for a in alpha:<br/>    ridge = linear_model.RidgeClassifier(a)<br/>    scores = model_selection.cross_val_score(ridge, X_train_tfidf, X_test_tfidf, cv=5, scoring='f1')<br/>    print("alpha: ",a)<br/>    print(scores)<br/>    print(np.mean(scores))<br/>    print('\n')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/62d9b7eaeec138daeeb43a470e73b94a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*Yeac5jwYzXjOVX79yz8CZA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">f1-TF-IDF 5 倍交叉验证的分数</figcaption></figure><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="9e5a" class="lc jv hi ky b fi ld le l lf lg"><strong class="ky hj"># MultinomialNB with TF-IDF</strong></span><span id="530b" class="lc jv hi ky b fi lh le l lf lg">alpha = [175.0, 200.0, 225.0, 250.0, 300.0]</span><span id="3444" class="lc jv hi ky b fi lh le l lf lg">for a in alpha:<br/>    mnb = MultinomialNB(a)<br/>    scores = model_selection.cross_val_score(mnb, X_train, X_test, cv=5, scoring='f1')<br/>    print('alpha: ', a)<br/>    print(scores)<br/>    print(np.mean(scores))<br/>    print('\n')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/bfbb4474c2f1fe99ab0ac2d25ce95bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*dJ2bGTuWyGb8_RNwr5JJ-Q.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">f1-TF-IDF 五重交叉验证的得分</figcaption></figure><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="7272" class="lc jv hi ky b fi ld le l lf lg"><strong class="ky hj"># Rigde with Hash</strong></span><span id="fd69" class="lc jv hi ky b fi lh le l lf lg">alpha = [1.1, 1.2, 1.3, 1.4, 1.5, 2.0]</span><span id="8a49" class="lc jv hi ky b fi lh le l lf lg">for a in alpha:<br/>    ridge = linear_model.RidgeClassifier(a)<br/>    scores = model_selection.cross_val_score(ridge, X_train_hash, X_test_hash, cv=5, scoring='f1')<br/>    print("alpha: ",a)<br/>    print(scores)<br/>    print(np.mean(scores))<br/>    print('\n')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/39dd2602db584d00bf3c93bf85caefe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*U2VYEpWsxdh1TngMZC11yw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">f1-5 倍交叉验证的分数，用岭进行散列</figcaption></figure><p id="f9ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过所有这些训练，我们可以看到，通过使用具有 alpha 1.4 脊的散列，我们获得了最高的 F1 分数。因此，我们将选择这个 alpha 来预测测试集中的情感。</p><h1 id="b771" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">5.选择模型并运行精度测试</h1><p id="5683" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">为了测量预测的准确性，最简单的技术之一是准确性测试。这样，我们可以看到，例如，在模型需要预测的 100 次预测中，它正确预测了多少次。</p><p id="6fc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用的另一种技术是精确和回忆。让我们看看这些值是如何计算的。首先，参考混淆矩阵。我们需要理解以下术语及其计算。</p><p id="981a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">真阳性(TP) </strong>:当实际阳性被正确分类为阳性时。<br/> <strong class="ih hj">【真否定】</strong>:实际否定被正确归类为否定时。<br/> <strong class="ih hj">【假阳性(FP) </strong>:实际阴性被错误归类为阳性时。<br/> <strong class="ih hj">【假阴性(FN) </strong>:实际阳性被错误归类为阴性时。</p><p id="921a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，一旦我们知道了这些值，我们就可以计算精度和召回率，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/9fd78bc5ad1b0dfa6d0aaa9644552194.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*hLnBb47uMFlt51xUeyrHGQ.png"/></div></figure><p id="dd84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">精度，或正预测值，指的是在正预测总数中正确正预测的数量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/54be2e9642bab7d63bcc4a78c2388824.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*x3aQXTrvDxXcFcAplb5AIg.png"/></div></figure><p id="04ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回忆或敏感度指的是预测的正确阳性的数量与实际的真实阳性的数量之比。现在，让我们看看我们的模型是如何预测测试数据的。</p><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="abf4" class="lc jv hi ky b fi ld le l lf lg">from sklearn.metrics import accuracy_score</span><span id="fda7" class="lc jv hi ky b fi lh le l lf lg">ridge = linear_model.RidgeClassifier(1.4)<br/>ridge.fit(X_train_hash, X_test_hash)<br/>test['sentiment_pred'] = ridge.predict(Y_train_hash)</span><span id="4bf3" class="lc jv hi ky b fi lh le l lf lg">y_true = test['sentiment']<br/>y_pred = test['sentiment_pred']</span><span id="c1eb" class="lc jv hi ky b fi lh le l lf lg">accuracy_score(y_true, y_pred)</span></pre><p id="b1a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用这个模型，我们获得了 0.89256 的预测精度！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/513a407b9132c88cded2b83ae7094c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*u_EyEetQMsTh9jETgsAFhw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">实际情绪与预测情绪的示例</figcaption></figure><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="d1dc" class="lc jv hi ky b fi ld le l lf lg">from sklearn.metrics import classification_report,confusion_matrix </span><span id="a116" class="lc jv hi ky b fi lh le l lf lg">print(classification_report(y_true, y_pred, target_names = ['Bad Reviews','Good Reviews']))</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/5fdfdfe2a7d5285ab586d5f8c08fc708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*Ny2KqEi_ivKk_npCSnR1eA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">精确度、召回率、F1 分数报告</figcaption></figure><pre class="je jf jg jh fd kx ky kz la aw lb bi"><span id="4a03" class="lc jv hi ky b fi ld le l lf lg">import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="f285" class="lc jv hi ky b fi lh le l lf lg">cm = confusion_matrix(y_true, y_pred)</span><span id="bff5" class="lc jv hi ky b fi lh le l lf lg">plt.figure(figsize = (5,5))<br/>sns.heatmap(cm,cmap= "Blues", <br/>            linecolor = 'black', <br/>            linewidth = 1, <br/>            annot = True, <br/>            fmt='', <br/>            xticklabels = ['Bad Reviews','Good Reviews'], <br/>            yticklabels = ['Bad Reviews','Good Reviews'])<br/>plt.xlabel("Predicted")<br/>plt.ylabel("Actual")</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/e351cffda716dcd358671076cec971d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*_RA25PoYg31tilTc-qa39w.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">混淆矩阵</figcaption></figure><h1 id="5318" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">结论</strong></h1><p id="4178" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">利用 IMDB 电影评论数据，用两个主要模型 ridge 和 multinomialNB 对模型进行了训练，并使用 F1 评分对精度进行了比较。通过哈希嵌入技术和脊分类器，我获得了 89%的情感预测准确率。</p></div></div>    
</body>
</html>