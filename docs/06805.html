<html>
<head>
<title>Matrix Calculus for DeepLearning (Part3)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的矩阵演算(第三部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/matrix-calculus-for-deeplearning-part3-be3f35b7deb7?source=collection_archive---------35-----------------------#2020-06-02">https://medium.com/analytics-vidhya/matrix-calculus-for-deeplearning-part3-be3f35b7deb7?source=collection_archive---------35-----------------------#2020-06-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="fe4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2020年5月30日</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/9d18b9f3e57b4dee11c57aa6f17cda11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wlIDUPkKkzL1oNsC.png"/></div></div></figure><p id="c820" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在之前的博客中，我们已经研究了雅可比矩阵、元素运算、涉及单一表达式的导数、向量和归约以及链式法则。请通过那个，<a class="ae jp" href="https://kirankamath.netlify.app/blog/matrix-calculus-for-deeplearning-part1/" rel="noopener ugc nofollow" target="_blank">博客1 </a>，<a class="ae jp" href="https://kirankamath.netlify.app/blog/matrix-calculus-for-deeplearning-part2/" rel="noopener ugc nofollow" target="_blank">博客2 </a>。</p><h1 id="09d6" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">神经元激活的梯度</h1><p id="5cca" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">让我们计算单个神经网络计算单元的典型神经元激活相对于模型参数<em class="kt"> w </em>和b的导数</p><p id="da95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kt">激活</em> ( <strong class="ih hj"> x </strong> ) = max(0，<strong class="ih hj"> w </strong>。<strong class="ih hj"> x </strong> + b)</p><p id="5981" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这代表具有完全连接的权重和relu的神经元。让我们计算(<strong class="ih hj"> w </strong>的导数。<strong class="ih hj"> x </strong> + b ) wrt <strong class="ih hj"> w </strong>和b点积<strong class="ih hj"> w . x </strong>是元素的逐元素乘法的总和。总和的偏导数(<strong class="ih hj"> w </strong> ⊗ <strong class="ih hj"> x </strong>)可以通过使用中间向量变量的链式法则来计算</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ku"><img src="../Images/00ac98d6d86721fb71c2c7b3b5c9b3a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/0*cUTW9i5_TWlKlpvR.png"/></div></figure><p id="cf9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的图片使用了标量z上的max(0，z)函数调用，只是说将所有负z值视为0。最大值函数的导数是分段函数。当z ≤ 0时，导数为0，因为z是常数。当z &gt; 0时，max函数的导数正好是z的导数，也就是1。</p><p id="f022" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当激活函数将仿射函数输出z裁剪为0时，对任何权重w i的导数为零。当z &gt; 0时，就好像max函数消失了，我们只得到z对权重的导数。</p><h1 id="ee4d" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">神经网络损失函数的梯度</h1><p id="e09d" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">训练一个神经元需要我们对我们的损失或“成本”函数取关于我们的模型参数的导数，<strong class="ih hj"> w </strong>和b</p><p id="52e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要计算梯度权重和偏差</p><p id="de91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设X = [x 1，x 2，…，xN ] T (T表示转置)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kv"><img src="../Images/f22868ad6e5cdb899d7b87f91e38c3b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/0*Ts3pSausqdn_mIMo.png"/></div></figure><p id="1697" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果误差是0，那么梯度也是0，我们已经达到了最小损失。如果ei是某个小的正差值，梯度就是x方向的一小步，如果e 1很大，梯度就是那个方向的一大步。我们希望减少而不是增加损失，这就是为什么梯度下降递归关系采用梯度的负值来更新当前位置。</p><p id="cbcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看东西像一个向量的形状(长的或高的)，是变量标量还是向量，一个矩阵的维数。向量用粗体字母表示。读完这篇博客后，请阅读这篇文章，以获得更多的理解。</p><p id="1323" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文有独特的解释概念的方式，即从简单到复杂。当我们到达文章的结尾时，我们会自己解决，因为困难的表达是可以解决的，因为我们对简单的表达有深刻的理解。</p><p id="3c90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们从f(x)表示的简单参数的函数开始。其次，我们转向f(x，y，z)形式的函数。为了计算这种函数的导数，我们使用相对于特定参数计算的偏导数。第三，我们移动到作为f( <strong class="ih hj"> x </strong>)的输入参数向量的标量函数，其中f( <strong class="ih hj"> x </strong>)的偏导数被表示为向量。最后，我们看到<strong class="ih hj"> f </strong> ( <strong class="ih hj"> x </strong>)表示一组f( <strong class="ih hj"> x </strong>)形式的标量函数。</p><p id="3e9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是博客的最后一部分，第三部分。</p><p id="ab5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谢谢你。</p></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><p id="d7f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kt">最初发布于</em><a class="ae jp" href="https://kirankamath.netlify.app/blog/matrix-calculus-for-deeplearning-part3/" rel="noopener ugc nofollow" target="_blank"><em class="kt">https://kirankamath . netlify . app</em></a><em class="kt">。</em></p></div></div>    
</body>
</html>