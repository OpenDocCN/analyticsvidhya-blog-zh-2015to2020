<html>
<head>
<title>What exactly is meant by explainability and interpretability of AI?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能的可解释性和可解释性到底是什么意思？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-exactly-is-meant-by-explainability-and-interpretability-of-ai-bcea30ca1e56?source=collection_archive---------3-----------------------#2020-02-15">https://medium.com/analytics-vidhya/what-exactly-is-meant-by-explainability-and-interpretability-of-ai-bcea30ca1e56?source=collection_archive---------3-----------------------#2020-02-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="5ea8" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">了解可解释的人工智能术语</h2></div><p id="6de3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated">人工智能已经成为不同领域自动化的重要组成部分。然而，对于AI学习模型在现实生活中的部署，AI仍然需要克服“黑箱问题”。为了解决这个问题，目前，人工智能研究人员已经住在可解释人工智能(XAI)领域。XAI领域旨在为当前的人工智能学习模型配备透明性、公平性、可问责性和可解释性。一旦这些功能得到充分开发，它将导致负责任的AI，可用于不同领域的实际案例。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kc"><img src="../Images/4721b6973236409d57196921d86d7249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*0kZcel8dqArm9y2PA2yv7A.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">目标检测和分割</figcaption></figure><p id="4e63" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">目前，与人工智能的可解释性和可解释性相关的研究论文数量激增。然而，关于人工智能的可解释性和可解释性的确切含义，以及这些术语在意义上是否不同或多余，我们知之甚少。在这篇文章中，我提出了各种XAI的概念，这将有助于任何希望在XAI领域工作的人。</p><p id="a6bc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，XAI有助于确保训练数据集以及训练好的模型在决策时没有任何偏见。此外，XAI将有助于调试学习模型，以及提请注意各种敌对的干扰，这将导致错误的预测或决定。更重要的是，XAI将对学习模型建立的因果关系以及模型的推理给出一个深刻的见解。应该注意的一点是，通过使学习模型变得更复杂，其可解释性降低而性能提高；因此，在学习模型的性能和可解释性之间存在一种反比关系。在提出XAI技术时，应该关注学习系统的目标用户，以使学习系统值得用户信任。此外，用户的隐私也应该考虑在内。因此，要在现实生活中使用人工智能，首先，我们需要通过解释它的决定来使人工智能负责，并使它透明，从而为负责任或道德的人工智能构建基础。</p><p id="515f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们把重点放在定义负责任的人工智能和XAI社区中最常用的术语和术语上。</p><p id="0990" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">可理解性</em> </strong>指的是学习模型的特征，其中用户可以理解模型的功能(<em class="ko">模型如何工作</em>)，而无需关于学习模型中发生的内部过程的任何解释。它类似于AI上下文中的术语<strong class="iz hj"> <em class="ko">可懂度</em> </strong>。</p><p id="29c2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">可理解性</em> </strong>表示学习模型以用户可理解<em class="ko">的方式描述其所学知识的能力。</em></p><blockquote class="kp kq kr"><p id="ad53" class="ix iy ko iz b ja jb ij jc jd je im jf ks jh ji jj kt jl jm jn ku jp jq jr js hb bi translated">模型可理解性的概念源于Michalski的假设，即“计算机归纳的结果应该是给定实体的符号描述，在语义和结构上类似于人类专家观察相同实体可能产生的结果。这些描述的组成部分应该可以理解为单一的“信息块”，可以用自然语言直接解释，并且应该以综合的方式将定量和定性概念联系起来”——r . s . Michalski，归纳学习的理论和方法，载于《机器学习》, Springer，1983年，第83-134页。</p></blockquote><p id="7b16" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">可解释性</em> </strong>描述了学习模型的被动特征，指的是给定学习模型<em class="ko">对用户有意义</em>的程度。</p><p id="81c1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">可解释性</em> </strong>是学习模型的主动特征，描述学习模型所采取的过程，目的是<em class="ko">阐明</em>学习模型的内部工作。它与一个论点或解释的概念有关，在那里用户和决策者之间有一个界面。</p><p id="8cb5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">可以提出学习模型的事后解释，然而，学习模型的可解释性是从学习模型本身的设计中得出的特征。</p><p id="d88e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">透明性</em> </strong>是<strong class="iz hj"> <em class="ko"> </em> </strong>由学习模型在自身<em class="ko">可理解</em>时实现的，即不需要其他接口或过程。</p><p id="0592" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的定义可以清楚地看出，可理解性是XAI的关键，因为它在不同的术语中有不同程度的包含。可解释性和透明性是非常接近的概念，因为完全可解释性的学习模型最终将继承透明性的属性。注意，可理解性要求学习模型可理解性和人类可理解性，因此XAI技术应该集中于理解学习模型要执行的任务以及学习模型的用户。因此，这种以用户为中心的XAI概念使其成为一种以人为中心的人工智能努力。</p><p id="6c4d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过结合模型可解释性对用户的依赖性，XAI可以被重新定义如下:给定一个用户，<strong class="iz hj"> <em class="ko">可解释AI </em> </strong>是一个提供细节和理由以使其功能易于理解的实体。</p><h2 id="0d24" class="kv kw hi bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">XAI的目标(从最受欢迎到最不受欢迎)</h2><p id="55bf" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">信息量:</em> </strong>近年来XAI领域的大多数研究出版物的目标都是提取关于学习模型内部工作的信息或知识。</p><p id="1037" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">可移植性:</em> </strong>学习模型的可解释性导致其在不同应用中的重用。然而，请注意，并不是每个可转移的学习模型都是可以解释的。可转让性是在XAI领域进行研究的第二个最常用的理由。</p><p id="417c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">可访问性:</em> </strong>可解释的人工智能学习模型允许最终用户更加沉浸在调试和开发学习模型的过程中。可解释性也为非技术用户理解学习模型的内部工作提供了一个简单的入口。</p><p id="a297" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">信心:</em> </strong>要创建一个负责任的AI，一个可解释的学习模型必须提供关于其功能的信心。来自学习模型的鲁棒性、稳定性和可靠性要求导致需要评估学习模型的置信度。因此，在金融和医学领域中，在评估学习模型的置信度方面做了大量的研究工作。</p><p id="b3cf" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">公平性:</em> </strong>学习模型的可解释性强调用于训练学习模型的数据中普遍存在的偏差。<strong class="iz hj"> <em class="ko"> </em> </strong>由于XAI在其可解释的过程中涉及用户，所以由学习模型做出的预测是公平的变得至关重要，从而由涉及人类用户的学习模型做出的决策是公正的。因此，与XAI领域的公平性相关的研究出版物大多旨在实现一种道德的人工智能，并将人工智能用于社会公益。</p><p id="9f2a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">可信度:</em> </strong>对一个学习模型来说，量化可信度是一个非常困难的特性，它可以被看作是一个学习模型在面对一个问题时是否会像深思熟虑的那样起作用的置信度。学习模型的可解释性应该具有可信性的特征，然而并不是所有可信的学习模型都是可解释的。</p><p id="bd5b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">交互性:</em> </strong>以人为中心的人工智能驱动的交互系统将最终用户置于中心焦点，旨在为用户提供交互系统，以与执行预期任务的人工智能学习模型协作。</p><p id="51cf" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">因果关系:</em> </strong>关于XAI，因果关系是指找到学习模型的变量之间的因果关系。学习模型的可解释性导致确定其训练数据之间的相关性，最终导致发现因果关系。这种来自可解释人工智能的因果关系可以从因果推理技术获得的因果关系中得到验证。</p><p id="4b5c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ko">隐私意识:</em> </strong>非授权第三方理解学习模型内部工作的能力可能会危及原始训练数据的隐私；例如，由于学习模型的可解释性，在金融部门实现的学习系统可能导致其客户的私人信息的泄露。因此，保密性成为XAI领域的一个主要关注点。XAI研究人员对XAI隐私方面的关注非常薄弱，这给即将到来的XAI研究人员提供了机会。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="86fa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ko">希望这篇对可解释人工智能领域的新概念的简要介绍能激励读者在XAI寻找研究机会。</em></p><h2 id="fb4d" class="kv kw hi bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">参考:</h2><p id="1398" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">《可解释的人工智能(XAI):面向负责任的人工智能的概念、分类学、机遇和挑战》信息融合58(2020):82–115。</p><h2 id="cbe8" class="kv kw hi bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">进一步阅读</h2><p id="e294" class="pw-post-body-paragraph ix iy hi iz b ja lq ij jc jd lr im jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">廖，q .维拉，丹尼尔·格伦，莎拉·米勒。"质疑人工智能:为可解释的人工智能用户体验的设计实践提供信息."arXiv预印本arXiv:2001.02478  (2020)。</p></div></div>    
</body>
</html>