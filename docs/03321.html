<html>
<head>
<title>Build your semantic document search engine with TF-IDF and Google-USE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用TF-IDF和Google-USE构建你的语义文档搜索引擎</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/build-your-semantic-document-search-engine-with-tf-idf-and-google-use-c836bf5f27fb?source=collection_archive---------3-----------------------#2020-01-26">https://medium.com/analytics-vidhya/build-your-semantic-document-search-engine-with-tf-idf-and-google-use-c836bf5f27fb?source=collection_archive---------3-----------------------#2020-01-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es im"><img src="../Images/52dd1a38422cd37cd6a15649c623dbf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVAOE4YZHlckxwPdB0wkUQ.jpeg"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">来源:<a class="ae jc" href="http://inmyownterms.com/six-document-search-engines-use/" rel="noopener ugc nofollow" target="_blank"> inmyownterms </a></figcaption></figure><p id="5f82" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们来了解一下如何使用python语言来构建一个文档搜索引擎。</p><p id="af7a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在本帖中，我们将利用<a class="ae jc" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank"> 20newsgroup开源数据集</a>构建一个<strong class="jf hj">语义文档搜索引擎</strong>。</p><h1 id="1421" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">先决条件</h1><ul class=""><li id="9cbb" class="kz la hi jf b jg lb jk lc jo ld js le jw lf ka lg lh li lj bi translated"><a class="ae jc" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> Python 3.5 </a> +</li><li id="3709" class="kz la hi jf b jg lk jk ll jo lm js ln jw lo ka lg lh li lj bi translated"><a class="ae jc" href="https://pypi.org/project/pip/" rel="noopener ugc nofollow" target="_blank"> pip 19 </a> +或pip3</li><li id="e7fe" class="kz la hi jf b jg lk jk ll jo lm js ln jw lo ka lg lh li lj bi translated"><a class="ae jc" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK </a></li><li id="ed6e" class="kz la hi jf b jg lk jk ll jo lm js ln jw lo ka lg lh li lj bi translated"><a class="ae jc" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a></li><li id="f69c" class="kz la hi jf b jg lk jk ll jo lm js ln jw lo ka lg lh li lj bi translated"><a class="ae jc" href="https://www.tensorflow.org" rel="noopener ugc nofollow" target="_blank"> TensorFlow-GPU </a></li></ul><h1 id="f7f4" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">1.做好准备</h1><p id="1e42" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">对于这个职位，我们将需要上述先决条件<strong class="jf hj">，</strong>如果你还没有，请做好准备。</p><h1 id="f3c9" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">2.数据收集</h1><p id="ddb1" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">这里，我们使用20个新闻组数据集来分析给定输入关键词/句子输入的文本搜索引擎。</p><p id="b7bb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">20个新闻组数据集是大约11K个新闻组文档的集合，平均分布在20个不同的新闻组中。</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="55d0" class="lx kc hi lt b fi ly lz l ma mb">news = pd.read_json('<a class="ae jc" href="https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/newsgroups.json" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/newsgroups.json</a>')</span></pre><h2 id="d8fe" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">2.1数据清理:</h2><p id="9f43" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">在进入清理阶段之前，我们从文本中检索文档的主题。</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="e1d8" class="lx kc hi lt b fi ly lz l ma mb">for i,txt in enumerate(news['content']):<br/>    subject = re.findall('Subject:(.*\n)',txt)<br/>    if (len(subject) !=0):<br/>        news.loc[i,'Subject'] =str(i)+' '+subject[0]<br/>    else:<br/>        news.loc[i,'Subject'] ='NA'<br/>df_news =news[['Subject','content']]</span></pre><p id="abe0" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，我们从文本内容和数据集的主题中删除不需要的数据。</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="8376" class="lx kc hi lt b fi ly lz l ma mb">df_news.content =df_news.content.replace(to_replace='from:(.*\n)',value='',regex=True) ##remove from to email <br/>df_news.content =df_news.content.replace(to_replace='lines:(.*\n)',value='',regex=True)<br/>df_news.content =df_news.content.replace(to_replace='[!"#$%&amp;\'()*+,/:;&lt;=&gt;?@[\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except<br/>df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)<br/>df_news.content =df_news.content.replace(to_replace='\s+',value=' ',regex=True)    #remove new line<br/>df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space<br/>df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace</span></pre><h2 id="e5f6" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">2.2数据预处理</h2><p id="8fab" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">预处理是我们处理任何文本模型的主要步骤之一。在此阶段，我们必须查看数据的分布情况，需要什么技术以及应该清理多深。</p><h2 id="4e6b" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">小写字母</h2><p id="266d" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">将文本转换成小写形式。即'<strong class="jf hj">狗'</strong>变成'<strong class="jf hj">狗</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="0a78" class="lx kc hi lt b fi ly lz l ma mb">df_news['content']=[entry.lower() for entry in df_news['content']]</span></pre><h2 id="d20e" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">单词标记化</h2><p id="61f6" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">单词标记化是将句子分成单词形式的过程。</p><p id="9ffb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">“<strong class="jf hj">约翰在轨道</strong>上跑”→“<strong class="jf hj">约翰</strong>”、“<strong class="jf hj">是</strong>”、“<strong class="jf hj">跑</strong>”、“中的<strong class="jf hj">”、“<strong class="jf hj">中的</strong>”、“<strong class="jf hj">轨道</strong>”</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="f2bd" class="lx kc hi lt b fi ly lz l ma mb">df_news['Word tokenize']= [word_tokenize(entry) for entry in df_news.content]</span></pre><h2 id="476e" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">停止言语</h2><p id="054a" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">停用词是最常见的词，它不会给文档向量带来任何附加值。事实上，删除这些将增加计算和空间效率。NLTK库有一个下载停用词的方法。</p><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mp"><img src="../Images/c07e2a457bd4cedd5adaa29910de1fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*PdgWsOM1ep9Z2rfkQ6UJZA.png"/></div></figure><h2 id="d30d" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">单词词汇化</h2><p id="e608" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">引理化是一种将单词简化为单词的词根同义词的方法。与词干提取不同，词汇匹配确保缩减后的单词再次成为词典中的单词(同一种语言中的单词)。WordNetLemmatizer可用于对任何单词进行词汇化。</p><p id="6d13" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">即<strong class="jf hj">岩石→岩石，更好→好，语料库→语料库</strong></p><p id="f5c8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这里创建了wordLemmatizer函数来删除一个<strong class="jf hj">单字符</strong>、<strong class="jf hj">停用词</strong>和<strong class="jf hj">单词。</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="6375" class="lx kc hi lt b fi ly lz l ma mb"># WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun<br/>def wordLemmatizer(data):<br/>    tag_map = defaultdict(lambda : wn.NOUN)<br/>    tag_map['J'] = wn.ADJ<br/>    tag_map['V'] = wn.VERB<br/>    tag_map['R'] = wn.ADV<br/>    file_clean_k =pd.DataFrame()<br/>    for index,entry in enumerate(data):<br/>        <br/>        # Declaring Empty List to store the words that follow the rules for this step<br/>        Final_words = []<br/>        # Initializing WordNetLemmatizer()<br/>        word_Lemmatized = WordNetLemmatizer()<br/>        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.<br/>        for word, tag in pos_tag(entry):<br/>            # Below condition is to check for Stop words and consider only alphabets<br/>            if len(word)&gt;1 and word not in stopwords.words('english') and word.isalpha():<br/>                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])<br/>                Final_words.append(word_Final)<br/>            # The final processed set of words for each iteration will be stored in 'text_final'<br/>                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)<br/>                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)<br/>                file_clean_k=file_clean_k.replace(to_replace ="\[.", value = '', regex = True)<br/>                file_clean_k=file_clean_k.replace(to_replace ="'", value = '', regex = True)<br/>                file_clean_k=file_clean_k.replace(to_replace =" ", value = '', regex = True)<br/>                file_clean_k=file_clean_k.replace(to_replace ='\]', value = '', regex = True)<br/>    return file_clean_k</span></pre><p id="ff52" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">通过使用这个函数，花费了大约<strong class="jf hj"> 13小时</strong>的时间来检查和词条化20个新闻组数据集的11K个文档的单词。在下面找到这个单词的JSON文件。</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="533f" class="lx kc hi lt b fi ly lz l ma mb"><a class="ae jc" href="https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/WordLemmatize20NewsGroup.json" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/WordLemmatize20NewsGroup.json</a></span></pre><h2 id="1f4c" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">2.3数据已准备就绪</h2><p id="cefc" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">查看干净数据的示例-</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="c7ff" class="lx kc hi lt b fi ly lz l ma mb">df_news.Clean_Keyword[0]</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es mq"><img src="../Images/73e7a4167f0017cc75df9b2343e379a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Br5cASjTPcoN0J1QhXIYuA.png"/></div></div></figure><h1 id="59b8" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">3.文档搜索引擎</h1><p id="13a8" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">在这篇文章中，我们用三种方法来理解文本分析。</p><p id="3cf3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">1.使用<strong class="jf hj"> TF-IDF </strong>的文档搜索引擎</p><p id="0498" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">2.带有<strong class="jf hj">谷歌通用语句编码器</strong>的文档搜索引擎</p><h2 id="0bba" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">3.1使用<a class="ae jc" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>计算排名</h2><p id="8d5d" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">这是根据输入关键字/句子计算文档文本之间的相似性的最常见的度量。数学上，它测量的是在多维空间中投影的两个向量的角度b/w的余弦值。</p><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mr"><img src="../Images/4a415cd5889ab06dceaf78570f4384fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*nJT7q9nlDWgXllSHcI4ZJA.jpeg"/></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">要查询的黑白文档的余弦相似度</figcaption></figure><p id="178a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在上图中，空间中有3个文档向量值和一个查询向量。当我们计算3个文档的余弦相似度b/w时。最相似值将是三个文档中的D3文档。</p><h1 id="e6f5" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">1.使用TF-IDF的文档搜索引擎:</h1><p id="031e" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated"><a class="ae jc" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj"> TF-IDF </strong> </a>代表<strong class="jf hj">“词频—逆文档频”</strong>。这是一种计算每个单词的权重的技术，表示该单词在文档和语料库中的重要性。该算法主要用于信息检索和文本挖掘领域。</p><h2 id="60af" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">术语频率(TF)</h2><p id="156d" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">单词在文档中出现的次数除以文档中的总单词数。每个文档都有其词频。</p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es ms"><img src="../Images/87ac28bd18de9ce28e20848c7143bca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/0*0Uzik-cTMA-i6BUt.png"/></div></div></figure><h2 id="1a29" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">反向数据频率(IDF)</h2><p id="d827" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">日志中的文档数除以包含单词<strong class="jf hj"> <em class="mt"> w </em> </strong>的文档数。逆数据频率决定了语料库中所有文档中稀有词的权重。</p><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mu"><img src="../Images/2c4cee08c4949ce478536fe6f799f7fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/0*t2Uxb_43L3vjwDPm.png"/></div></figure><p id="4e3d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">最后，<strong class="jf hj"> TF-IDF </strong>就是TF乘以IDF。</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="9dc2" class="lx kc hi lt b fi ly lz l ma mb"><strong class="lt hj">TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)</strong></span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mv"><img src="../Images/44cf9d26b4295a628fe18837da398987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/0*yJm1bH6Ds0vFFyhP.png"/></div></figure><p id="d4a1" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们可以使用<a class="ae jc" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> Sklearn </a>提供的类，而不是自己手动实现<a class="ae jc" href="http://www.tfidf.com/" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>。</p><h2 id="fd7e" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">使用Sklearn的TfidfVectorizer生成TF-IDF</h2><p id="8e06" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">导入包:</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="651a" class="lx kc hi lt b fi ly lz l ma mb">import pandas as pd<br/>import numpy as np<br/>import os <br/>import re<br/>import operator<br/>import nltk <br/>from nltk.tokenize import word_tokenize<br/>from nltk import pos_tag<br/>from nltk.corpus import stopwords<br/>from nltk.stem import WordNetLemmatizer<br/>from collections import defaultdict<br/>from nltk.corpus import wordnet as wn<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span></pre><p id="cf72" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">TF-IDF</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="91b5" class="lx kc hi lt b fi ly lz l ma mb">from sklearn.feature_extraction.text import TfidfVectorizer<br/>import operator</span><span id="6839" class="lx kc hi lt b fi mw lz l ma mb">## Create Vocabulary<br/>vocabulary = set()</span><span id="8ab7" class="lx kc hi lt b fi mw lz l ma mb">for doc in df_news.Clean_Keyword:<br/>    vocabulary.update(doc.split(','))</span><span id="4b25" class="lx kc hi lt b fi mw lz l ma mb">vocabulary = list(vocabulary)</span><span id="c167" class="lx kc hi lt b fi mw lz l ma mb"># Intializating the tfIdf model<br/>tfidf = TfidfVectorizer(vocabulary=vocabulary)</span><span id="4e83" class="lx kc hi lt b fi mw lz l ma mb"># Fit the TfIdf model<br/>tfidf.fit(df_news.Clean_Keyword)</span><span id="0c87" class="lx kc hi lt b fi mw lz l ma mb"># Transform the TfIdf model<br/>tfidf_tran=tfidf.transform(df_news.Clean_Keyword)</span></pre><p id="06d3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">上面的代码已经创建了整个数据集的TF-IDF权重，现在必须创建一个函数来为输入查询生成一个向量。</p><h2 id="c045" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">为查询/搜索关键字创建向量</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="2394" class="lx kc hi lt b fi ly lz l ma mb">def gen_vector_T(tokens):</span><span id="57f3" class="lx kc hi lt b fi mw lz l ma mb">Q = np.zeros((len(vocabulary)))    <br/>    x= tfidf.transform(tokens)<br/>    #print(tokens[0].split(','))<br/>    for token in tokens[0].split(','):<br/>        #print(token)<br/>        try:<br/>            ind = vocabulary.index(token)<br/>            Q[ind]  = x[0, tfidf.vocabulary_[token]]<br/>        except:<br/>            pass<br/>    return Q</span></pre><h2 id="aae2" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">用于计算的余弦相似函数</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="7ee1" class="lx kc hi lt b fi ly lz l ma mb">def cosine_sim(a, b):<br/>    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))<br/>    return cos_sim</span></pre><h2 id="118f" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">余弦相似度黑白文档到查询函数</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="c4e8" class="lx kc hi lt b fi ly lz l ma mb">def cosine_similarity_T(k, query):<br/>    preprocessed_query = preprocessed_query = re.sub("\W+", " ", query).strip()<br/>    tokens = word_tokenize(str(preprocessed_query))<br/>    q_df = pd.DataFrame(columns=['q_clean'])<br/>    q_df.loc[0,'q_clean'] =tokens<br/>    q_df['q_clean'] =wordLemmatizer(q_df.q_clean)<br/>    d_cosines = []<br/>    <br/>    query_vector = gen_vector_T(q_df['q_clean'])<br/>    for d in tfidf_tran.A:<br/>        d_cosines.append(cosine_sim(query_vector, d))<br/>                    <br/>    out = np.array(d_cosines).argsort()[-k:][::-1]<br/>    #print("")<br/>    d_cosines.sort()<br/>    a = pd.DataFrame()<br/>    for i,index in enumerate(out):<br/>        a.loc[i,'index'] = str(index)<br/>        a.loc[i,'Subject'] = df_news['Subject'][index]<br/>    for j,simScore in enumerate(d_cosines[-k:][::-1]):<br/>        a.loc[j,'Score'] = simScore<br/>    return a</span></pre><h2 id="72cc" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">测试功能</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="1af0" class="lx kc hi lt b fi ly lz l ma mb">cosine_similarity_T(10,’computer science’)</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mx"><img src="../Images/710a08fd464236eb2387baadbad27831.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*iSxlgzrxMz9Epnp4WkhxBQ.png"/></div><figcaption class="iy iz et er es ja jb bd b be z dx translated"><strong class="bd kd">关于“计算机科学”单词的前5个相似性文档的结果</strong></figcaption></figure><h1 id="f133" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">2.带有谷歌通用句子编码器的文档搜索引擎</h1><h2 id="65fc" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">谷歌使用简介</h2><p id="5be7" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">预先训练的<a class="ae jc" href="https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html" rel="noopener ugc nofollow" target="_blank">通用语句编码器</a>在<a class="ae jc" href="https://www.tensorflow.org/hub/" rel="noopener ugc nofollow" target="_blank"> Tensorflow-hub </a>中公开。它有两种变化，即一种用<a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder-large/5" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj">变压器编码器</strong> </a>训练，另一种用<a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj">深度平均网络(DAN) </strong> </a>训练。它们是在大型语料库上预先训练的，可以用于各种任务(情感分析、分类等)。这两者在准确性和计算资源需求之间有一个折衷。虽然具有变换器编码器的方法具有更高的精度，但是它在计算上更昂贵。使用DNA编码的方法在计算上花费较少，并且准确性也稍低。</p><p id="f625" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这里，我们使用第二个丹通用句子编码器，可在这个网址:- <a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">谷歌使用丹模型</a></p><p id="c930" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">两个模型都以一个单词、句子或段落作为输入，并输出一个<strong class="jf hj"> 512 </strong>维度向量。</p><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es my"><img src="../Images/05ec3cbaab87c1fbf74e57a4ce29143b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*pGl0O-Z_Way5sT_U"/></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">一个原型语义检索管道，用于文本相似性。</figcaption></figure><p id="7e0b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在使用张量流枢纽模型之前。</p><p id="8afb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">先决条件:</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="aa83" class="lx kc hi lt b fi ly lz l ma mb">!pip install --upgrade tensorflow-gpu<br/> #Install TF-Hub.<br/>!pip install tensorflow-hub<br/>!pip install seaborn</span></pre><p id="c582" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在导入包:</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="5d1c" class="lx kc hi lt b fi ly lz l ma mb">import pandas as pd<br/>import numpy as np<br/>import re, string<br/>import os <br/>import tensorflow as tf<br/>import tensorflow_hub as hub<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn.metrics.pairwise import linear_kernel</span></pre><p id="b3dd" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">从调用直接URL的<a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank"> TensorFlow-hub </a>下载模型:</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="2851" class="lx kc hi lt b fi ly lz l ma mb">! curl -L -o 4.tar.gz "<a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed</a>"</span><span id="421a" class="lx kc hi lt b fi mw lz l ma mb">or<br/>module_url = "<a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder/4</a>"</span></pre><p id="92f3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">加载谷歌丹通用句子编码器</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="20a0" class="lx kc hi lt b fi ly lz l ma mb">#Model load through local path:</span><span id="5fb6" class="lx kc hi lt b fi mw lz l ma mb">module_path ="/home/zettadevs/GoogleUSEModel/USE_4"<br/>%time model = hub.load(module_path)</span><span id="7ce3" class="lx kc hi lt b fi mw lz l ma mb">#Create function for using model training<br/>def embed(input):<br/>    return model(input)</span></pre><h2 id="35c6" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">用例1:-单词语义</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="eed7" class="lx kc hi lt b fi ly lz l ma mb">WordMessage =[‘big data’, ‘millions of data’, ‘millions of records’,’cloud computing’,’aws’,’azure’,’saas’,’bank’,’account’]</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mz"><img src="../Images/7acadeae27507f5c1ac810032013f41c.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*DfKpd8bPS1PA-UugtEDCFA.png"/></div></figure><h2 id="61e6" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">用例2:句子语义</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="fd99" class="lx kc hi lt b fi ly lz l ma mb">SentMessage =['How old are you?','what is your age?','how are you?','how you doing?']</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es na"><img src="../Images/8f7eff0ac3646c46794abaec7fb68b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*-eUVfSMSsv8rmNoqPTmQ2w.png"/></div></figure><h2 id="dc53" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">用例3:单词、句子和段落语义</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="9f32" class="lx kc hi lt b fi ly lz l ma mb">word ='Cloud computing'</span><span id="02f2" class="lx kc hi lt b fi mw lz l ma mb">Sentence = 'what is cloud computing'</span><span id="43cb" class="lx kc hi lt b fi mw lz l ma mb">Para =("Cloud computing is the latest generation technology with a high IT infrastructure that provides us a means by which we can use and utilize the applications as utilities via the internet."<br/>        "Cloud computing makes IT infrastructure along with their services available 'on-need' basis." <br/>        "The cloud technology includes - a development platform, hard disk, computing power, software application, and database.")</span><span id="bab0" class="lx kc hi lt b fi mw lz l ma mb">Para5 =(<br/>    "Universal Sentence Encoder embeddings also support short paragraphs. "<br/>    "There is no hard limit on how long the paragraph is. Roughly, the longer "<br/>    "the more 'diluted' the embedding will be.")</span><span id="e369" class="lx kc hi lt b fi mw lz l ma mb">Para6 =("Azure is a cloud computing platform which was launched by Microsoft in February 2010."<br/>       "It is an open and flexible cloud platform which helps in development, data storage, service hosting, and service management."<br/>       "The Azure tool hosts web applications over the internet with the help of Microsoft data centers.")<br/>case4Message=[word,Sentence,Para,Para5,Para6]</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es nb"><img src="../Images/bec7f52e225b3494e8e4d2e82c630410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*ejcbdMkwG1nUHBMtYUyjTg.png"/></div></figure><h1 id="07fa" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">训练模型</h1><p id="c1e2" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">这里，我们以批处理方式训练数据集，因为生成数据集的图形需要很长的执行时间。因此，更好地训练批量数据。</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="4d0a" class="lx kc hi lt b fi ly lz l ma mb">Model_USE= embed(df_news.content[0:2500])</span></pre><p id="ee8a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">保存模型</strong>，以便重用模型。</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="677f" class="lx kc hi lt b fi ly lz l ma mb">exported = tf.train.Checkpoint(v=tf.Variable(Model_USE))<br/>exported.f = tf.function(<br/>    lambda  x: exported.v * x,<br/>    input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])</span><span id="0bf9" class="lx kc hi lt b fi mw lz l ma mb">tf.saved_model.save(exported,'/home/zettadevs/GoogleUSEModel/TrainModel')</span></pre><p id="9006" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">从路径:</strong>加载模型</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="5883" class="lx kc hi lt b fi ly lz l ma mb">imported = tf.saved_model.load(‘/home/zettadevs/GoogleUSEModel/TrainModel/’)<br/>loadedmodel =imported.v.numpy()</span></pre><p id="b930" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">文件搜索功能:</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="a49c" class="lx kc hi lt b fi ly lz l ma mb">def SearchDocument(query):<br/>    q =[query]<br/>    # embed the query for calcluating the similarity<br/>    Q_Train =embed(q)<br/>    <br/>    #imported_m = tf.saved_model.load('/home/zettadevs/GoogleUSEModel/TrainModel')<br/>    #loadedmodel =imported_m.v.numpy()<br/>    # Calculate the Similarity<br/>    linear_similarities = linear_kernel(Q_Train, con_a).flatten() <br/>    #Sort top 10 index with similarity score<br/>    Top_index_doc = linear_similarities.argsort()[:-11:-1]<br/>    # sort by similarity score<br/>    linear_similarities.sort()<br/>    a = pd.DataFrame()<br/>    for i,index in enumerate(Top_index_doc):<br/>        a.loc[i,'index'] = str(index)<br/>        a.loc[i,'File_Name'] = df_news['Subject'][index] ## Read File name with index from File_data DF<br/>    for j,simScore in enumerate(linear_similarities[:-11:-1]):<br/>        a.loc[j,'Score'] = simScore<br/>    return a</span></pre><p id="905c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">测试搜索:</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="ef59" class="lx kc hi lt b fi ly lz l ma mb">SearchDocument('computer science')</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es nc"><img src="../Images/27504e01cef8cb49cbf404063f2564cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*8Btzc5dq-HlTzCaFLw_jdQ.png"/></div></figure><p id="ad9a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">找到该项目的<a class="ae jc" href="https://github.com/zayedrais/DocumentSearchEngine" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj"/></a><strong class="jf hj"/></p><h1 id="d9e5" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">结论:</h1><p id="2213" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">在本教程的最后，我们得出结论，“谷歌通用句子编码器”模型提供了语义搜索结果，而TF-IDF模型不知道单词的意思。只是根据文档中可用的单词给出结果。</p><p id="ae5a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">一些参考文献:</strong></p><ul class=""><li id="d5ec" class="kz la hi jf b jg jh jk jl jo nd js ne jw nf ka lg lh li lj bi translated"><a class="ae jc" href="https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089" rel="noopener" target="_blank"> TF-IDF </a></li><li id="d614" class="kz la hi jf b jg lk jk ll jo lm js ln jw lo ka lg lh li lj bi translated"><a class="ae jc" href="https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html" rel="noopener ugc nofollow" target="_blank">谷歌使用</a></li></ul><h1 id="bcb0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">别忘了给我们你的👏！</h1></div></div>    
</body>
</html>