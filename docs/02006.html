<html>
<head>
<title>TensorFlow Embedding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流嵌入</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tensorflow-embedding-36ec186dbeb6?source=collection_archive---------24-----------------------#2019-11-25">https://medium.com/analytics-vidhya/tensorflow-embedding-36ec186dbeb6?source=collection_archive---------24-----------------------#2019-11-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/4aec3c53c036e19a59a9748281ad088f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9PiA3gjcnbAoUdWR"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">萨法尔·萨法罗夫在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="7626" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感恩节假期即将到来(下周四)！我想在网上给我买双鞋。我想到了一个问题:网上商店是如何将相似的商品组合在一起的？也许用了一些机器学习的方法？我决定去探索…！</p><p id="1a06" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">经过一些研究，我认为句子2 vec可能是解释项目相似性的一个好选择。https://tfhub.dev/google/universal-sentence-encoder/1和t-SNE可视化将有助于显示项目的相似性。</p><p id="ea8b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我从网上得到一些吸尘器的物品/upc描述。我的代码如下:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="d88c" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">from </strong>absl <strong class="jy hj">import </strong>logging<br/><br/><strong class="jy hj">import </strong>tensorflow <strong class="jy hj">as </strong>tf<br/><strong class="jy hj">import </strong>tensorflow_hub <strong class="jy hj">as </strong>hub<br/><strong class="jy hj">import </strong>matplotlib.pyplot <strong class="jy hj">as </strong>plt<br/><strong class="jy hj">import </strong>numpy <strong class="jy hj">as </strong>np<br/><strong class="jy hj">import </strong>os<br/><strong class="jy hj">import </strong>pandas <strong class="jy hj">as </strong>pd<br/><strong class="jy hj">import </strong>re<br/><strong class="jy hj">import </strong>seaborn <strong class="jy hj">as </strong>sns<br/><strong class="jy hj">import </strong>nltk<br/><strong class="jy hj">from </strong>sklearn.manifold <strong class="jy hj">import </strong>TSNE<br/><strong class="jy hj">import </strong>os<br/>os.environ[<strong class="jy hj">'KMP_DUPLICATE_LIB_OK'</strong>]=<strong class="jy hj">'True'<br/><br/><br/></strong>df= pd.read_csv(<strong class="jy hj">"vacumn_upcs_desc.csv"</strong>, encoding=<strong class="jy hj">"ISO-8859-1"</strong>)<br/>print(df.shape)<br/>df = df.drop_duplicates(subset=<strong class="jy hj">'upc_nbr'</strong>, keep=<strong class="jy hj">"first"</strong>)<br/>df = df.reset_index(drop=<strong class="jy hj">True</strong>)<br/>print(df.shape)<br/>print(df.columns)<br/><br/>print(df.head(3))<br/><br/>print(df.iloc[ 3, :][<strong class="jy hj">'upc_nbr'</strong>])<br/>print(<strong class="jy hj">"index="</strong>, df.loc[df[<strong class="jy hj">'upc_nbr'</strong>] == 1112000810].index)<br/><br/><em class="ki"># print(type(df['value'].tolist()))<br/><br/></em>print(<strong class="jy hj">"here 1"</strong>)<br/>module_url = <strong class="jy hj">"https://tfhub.dev/google/universal-sentence-encoder/2?tf-hub-format=compressed" </strong><em class="ki">#@param ["https://tfhub.dev/google/universal-sentence-encoder/2", "https://tfhub.dev/google/universal-sentence-encoder-large/3"]<br/># Import the Universal Sentence Encoder's TF Hub module<br/></em>embed = hub.Module(module_url)<br/>print(<strong class="jy hj">"here 2"</strong>)<br/><br/>messages = df[<strong class="jy hj">'value'</strong>].tolist()<br/><strong class="jy hj">with </strong>tf.Session() <strong class="jy hj">as </strong>session:<br/>  session.run([tf.global_variables_initializer(), tf.tables_initializer()])<br/>  message_embeddings = session.run(embed(messages))<br/>  print(<strong class="jy hj">"correlation of messages:"</strong>)<br/>  print(np.inner(message_embeddings, message_embeddings))<br/><br/>  <strong class="jy hj">for </strong>i, message_embedding <strong class="jy hj">in </strong>enumerate(np.array(message_embeddings).tolist()):<br/>    print(df.iloc[i,:])<br/>    print(<strong class="jy hj">"Message: {}"</strong>.format(messages[i]))<br/>    print(<strong class="jy hj">"Embedding size: {}"</strong>.format(len(message_embedding)))<br/>    message_embedding_snippet = <strong class="jy hj">", "</strong>.join(<br/>        (str(x) <strong class="jy hj">for </strong>x <strong class="jy hj">in </strong>message_embedding[:3]))<br/>    print(<strong class="jy hj">"Embedding: [{}, ...]\n"</strong>.format(message_embedding_snippet))<br/><br/><br/><strong class="jy hj">def </strong>get_features(texts):<br/>    <strong class="jy hj">if </strong>type(texts) <strong class="jy hj">is </strong>str:<br/>        texts = [texts]<br/>    <strong class="jy hj">with </strong>tf.Session() <strong class="jy hj">as </strong>sess:<br/>        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])<br/>        <strong class="jy hj">return </strong>sess.run(embed(texts))<br/><br/>print(<strong class="jy hj">"embedded..."</strong>)<br/><em class="ki">#print(get_features(df.iloc[ 3, :]['value']))<br/><br/># # pre-process/clean up texts<br/></em><strong class="jy hj">def </strong>remove_stopwords(stop_words, tokens):<br/>    res = []<br/>    <strong class="jy hj">for </strong>token <strong class="jy hj">in </strong>tokens:<br/>        <strong class="jy hj">if not </strong>token <strong class="jy hj">in </strong>stop_words:<br/>            res.append(token)<br/>    <strong class="jy hj">return </strong>res<br/><br/><strong class="jy hj">def </strong>process_text(text):<br/>    text = text.encode(<strong class="jy hj">'ascii'</strong>, errors=<strong class="jy hj">'ignore'</strong>).decode()<br/>    text = text.lower()<br/>    text = re.sub(<strong class="jy hj">r'http\S+'</strong>, <strong class="jy hj">' '</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r'#+'</strong>, <strong class="jy hj">' '</strong>, text )<br/>    text = re.sub(<strong class="jy hj">r'@[A-Za-z0-9]+'</strong>, <strong class="jy hj">' '</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"([A-Za-z]+)'s"</strong>, <strong class="jy hj">r"\1 is"</strong>, text)<br/>    <em class="ki">#text = re.sub(r"\'s", " ", text)<br/>    </em>text = re.sub(<strong class="jy hj">r"\'ve"</strong>, <strong class="jy hj">" have "</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"won't"</strong>, <strong class="jy hj">"will not "</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"isn't"</strong>, <strong class="jy hj">"is not "</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"can't"</strong>, <strong class="jy hj">"can not "</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"n't"</strong>, <strong class="jy hj">" not "</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"i'm"</strong>, <strong class="jy hj">"i am "</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"\'re"</strong>, <strong class="jy hj">" are "</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"\'d"</strong>, <strong class="jy hj">" would "</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"\'ll"</strong>, <strong class="jy hj">" will "</strong>, text)<br/>    text = re.sub(<strong class="jy hj">'\W'</strong>, <strong class="jy hj">' '</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r'\d+'</strong>, <strong class="jy hj">' '</strong>, text)<br/>    text = re.sub(<strong class="jy hj">'\s+'</strong>, <strong class="jy hj">' '</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"li"</strong>, <strong class="jy hj">""</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"br"</strong>, <strong class="jy hj">""</strong>, text)<br/>    text = re.sub(<strong class="jy hj">r"ul"</strong>, <strong class="jy hj">""</strong>, text)<br/>    text = text.strip()<br/>    <strong class="jy hj">return </strong>text<br/>print(<strong class="jy hj">"processed text..."</strong>)<br/>print(process_text(df.iloc[ 3, :][<strong class="jy hj">'value'</strong>]))<br/><br/><br/>data_processed = list(map(process_text, df[<strong class="jy hj">'value'</strong>].values))<br/>len(data_processed)<br/><br/>similarity_vectors = get_features(data_processed)<br/><br/><strong class="jy hj">def </strong>cosine_similarity(v1, v2):<br/>    mag1 = np.linalg.norm(v1)<br/>    mag2 = np.linalg.norm(v2)<br/>    <strong class="jy hj">if </strong>(<strong class="jy hj">not </strong>mag1) <strong class="jy hj">or </strong>(<strong class="jy hj">not </strong>mag2):<br/>        <strong class="jy hj">return </strong>0<br/>    <strong class="jy hj">return </strong>np.dot(v1, v2) / (mag1 * mag2)<br/><br/><strong class="jy hj">def </strong>test_similiarity(text1, text2):<br/>    vec1 = get_features(text1)[0]<br/>    vec2 = get_features(text2)[0]<br/>    print(vec1.shape)<br/>    <strong class="jy hj">return </strong>cosine_similarity(vec1, vec2)<br/><br/>print(<strong class="jy hj">'test similarity ....'</strong>)<br/>print(test_similiarity(data_processed[0], data_processed[2]))<br/><br/><br/><strong class="jy hj">def </strong>semantic_search(query, data, vectors, df):<br/>    query = process_text(query)<br/>    print(<strong class="jy hj">"Extracting features..."</strong>)<br/>    query_vec = get_features(query)[0].ravel()<br/>    res = []<br/>    <strong class="jy hj">for </strong>i, d <strong class="jy hj">in </strong>enumerate(data):<br/>        qvec = vectors[i].ravel()<br/>        sim = cosine_similarity(query_vec, qvec)<br/>        res.append((sim, d[:100], i, df.iloc[ i, :][<strong class="jy hj">'upc_nbr'</strong>]))<br/>    <strong class="jy hj">return </strong>sorted(res, key=<strong class="jy hj">lambda </strong>x : x[0], reverse=<strong class="jy hj">True</strong>)[0:4]<br/><br/>print(<strong class="jy hj">"similarity-search..."</strong>)<br/><br/>print(semantic_search(data_processed[102], data_processed, BASE_VECTORS,df))<br/><em class="ki">#67, 108, 133<br/><br/></em><strong class="jy hj">def </strong>tsne_plot(base_vectors, upc_list):<br/>    <em class="ki">"Creates and TSNE model and plots it"<br/>    </em>labels = []<br/>    tokens = []<br/><br/>    <strong class="jy hj">for </strong>i <strong class="jy hj">in </strong>range(len(upc_list)):<br/>        tokens.append(base_vectors[i])<br/>        labels.append(upc_list[i])<br/><br/>    tsne_model = TSNE(perplexity=40, n_components=2, init=<strong class="jy hj">'pca'</strong>, n_iter=2500, random_state=23)<br/>    new_values = tsne_model.fit_transform(tokens)<br/><br/>    x = []<br/>    y = []<br/>    <strong class="jy hj">for </strong>value <strong class="jy hj">in </strong>new_values:<br/>        x.append(value[0])<br/>        y.append(value[1])<br/><br/>    plt.figure(figsize=(16, 16))<br/>    <strong class="jy hj">for </strong>i <strong class="jy hj">in </strong>range(len(x)):<br/>        plt.scatter(x[i], y[i])<br/>        plt.annotate(labels[i],<br/>                     xy=(x[i], y[i]),<br/>                     xytext=(5, 2),<br/>                     textcoords=<strong class="jy hj">'offset points'</strong>,<br/>                     ha=<strong class="jy hj">'right'</strong>,<br/>                     va=<strong class="jy hj">'bottom'</strong>)<br/>    plt.show()<br/><br/>tsne_plot(similarity_vectors, df.upc_nbr)</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kj"><img src="../Images/a0fb81d5563fae5e41685e10df799ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6NVw_3bMB1iet8REv_4zqQ.png"/></div></div></figure><p id="b9ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我从Bissell挑选了一个upc(编号= 1112023060 ),显示在左中位置的红色高亮圆圈中。相似UPC前2名分别是:1112024622和1112019909。为了验证，</p><ul class=""><li id="475e" class="kk kl hi ix b iy iz jc jd jg km jk kn jo ko js kp kq kr ks bi translated">1112023060的描述是:比塞尔动力螺旋涡轮倒带无袋真空，1797:伟大的地毯，室内装潢和更多。螺旋分离系统。多气旋。自动软线倒带。涡轮刷工具:15英寸宽的清洁路径，25英寸的电源线5高度调节；大容量污物容器；螺旋污物分离系统；帮助过滤掉过敏原；型号# 1797。</li></ul><p id="c947" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1112024622和1112019909的upc描述分别为:</p><ul class=""><li id="665d" class="kk kl hi ix b iy iz jc jd jg km jk kn jo ko js kp kq kr ks bi translated">BISSELL PowerForce Helix无袋立式真空吸尘器，2191U:独家Helix性能系统；大容量污物罐；可清洗的过滤器；比塞尔无袋立式吸尘器包括除尘刷、缝隙工具和加长杆；仅重12磅；清洁路径宽度:13”；5表面高度设置；</li><li id="392e" class="kk kl hi ix b iy kt jc ku jg kv jk kw jo kx js kp kq kr ks bi translated">Bissell PowerForce Helix无袋立式真空吸尘器，多种颜色:非常适合地毯、室内装潢等，5种高度调节，Bissell PowerForce立式真空吸尘器有一个大容量的灰尘容器，可清洗的过滤器，无袋立式真空吸尘器有各种颜色可供选择；独有的灰尘分离系统捕获更多细小灰尘；螺旋灰尘分离系统有助于扩展吸力和清洁性能。</li></ul><p id="7d51" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在右上角红色突出显示的圆圈中，它显示UPC是Hoover品牌。</p><p id="4d18" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以sentence2vec可以从文本描述中给我展示有效的相似条目。下一步，我将为每个upc添加更多的特性来生成精细的嵌入。</p></div></div>    
</body>
</html>