<html>
<head>
<title>Overview of optimizers for DNN: when and how to choose which optimizer — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DNN优化器概述:何时以及如何选择哪个优化器—第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/overview-of-optimizers-for-dnn-when-and-how-to-choose-which-optimizer-part-2-16524dedbfd2?source=collection_archive---------32-----------------------#2020-04-13">https://medium.com/analytics-vidhya/overview-of-optimizers-for-dnn-when-and-how-to-choose-which-optimizer-part-2-16524dedbfd2?source=collection_archive---------32-----------------------#2020-04-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0322" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个故事里，我想<strong class="ih hj">回顾一下深度神经网络</strong>的优化方法的发展<em class="jd">【DNN】</em><strong class="ih hj">分享一下使用优化器</strong>的建议。</p><blockquote class="je jf jg"><p id="8f46" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">这是我故事的第二部分。如果你不熟悉<em class="hi"> DNN、</em>的优化者，请阅读<a class="ae jk" rel="noopener" href="/@shengfang/overview-of-optimizers-for-dnn-when-and-how-to-choose-which-optimizer-951084b72061">第一部分</a>。</p></blockquote><p id="b819" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">你能在第1部分和第2部分找到什么:</strong></p><ol class=""><li id="c0e1" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">从直观的角度简要回顾流行的优化。(第一部分)</li><li id="5d9b" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">流行的自适应优化器<em class="jd"> Adam </em>的缺点。(第二部分)</li><li id="6be3" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">关于联合使用不同优化器以获得更好性能的建议。(第二部分)</li></ol><p id="71b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">谁可能感兴趣:</strong></p><ol class=""><li id="15e2" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">想简单了解一下从<em class="jd"> SGD </em>到<em class="jd"> Nadam </em>的优化者。(第一部分)</li><li id="be95" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">想要练习如何使用它们。(第二部分)</li></ol></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><p id="7be8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">亚当是强大的自动调整学习速度。但是为什么许多研究人员在他们的论文中使用<em class="jd">新币</em>？<em class="jd">亚当</em>怎么了？如何同时利用<em class="jd">亚当</em>和<em class="jd"> SGD的优势？</em></p><p id="e6ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们先回顾一下Adam的缺点，然后谈谈联合使用<em class="jd"> Adam </em>和<em class="jd"> SGD </em>的做法。</p><h1 id="913d" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">亚当怎么了？</h1><blockquote class="je jf jg"><p id="23fe" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><strong class="ih hj">问题1:不收敛</strong></p></blockquote><p id="03d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文献[5]证明了<em class="jd"> Adam </em>在某些情况下会导致模型不收敛。让我们检查一下使用不同优化器时收敛的细节。</p><p id="2482" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在具有一阶动量的<em class="jd"> SGD </em>方法中，学习率是固定的。当模型收敛时，更新值变得接近0。</p><p id="b3f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于<em class="jd"> AdaDelta/RMSProp </em>和<em class="jd"> Adam </em>，二阶动量的波动会导致模型的不稳定。因此，本文建议如下过滤二阶动量:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es le"><img src="../Images/650d7752260b0a4b0bc4571502986f28.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*HIjFyBiIE9LXSMZXoDj9tQ.png"/></div></figure><p id="8bee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，更新值显示出总体下降的趋势。</p><blockquote class="je jf jg"><p id="d043" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><strong class="ih hj">问题2:局部最小值</strong></p></blockquote><p id="2def" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在文献[6]中，作者在<em class="jd"> CIFAR-10 </em>数据库上做了实验。Adam比<em class="jd"> SGD </em>收敛更快，但是<em class="jd"> SGD </em>性能更好。他们的结论是:<em class="jd"> Adam </em>的更新值太小，无法在训练结束时收敛到全局最小值。</p></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><h1 id="1c48" class="kg kh hi bd ki kj lm kl km kn ln kp kq kr lo kt ku kv lp kx ky kz lq lb lc ld bi translated">亚当还是SGD？</h1><p id="060b" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated"><em class="jd">亚当</em>和<em class="jd"> SGD </em>，哪个更好？很难说。</p><p id="fdb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于新从业者，最好用<em class="jd">亚当</em>，性能不错。<em class="jd"> Adam </em>具有自适应的学习速率，擅长学习稀疏数据的表示。</p><p id="9cb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然<em class="jd"> Adam </em>在训练开始时收敛速度很快，但专家们更喜欢使用<em class="jd"> SGD </em>或组合<em class="jd"> Adam </em>和<em class="jd"> SGD </em>，因为<em class="jd"> Adam </em>可能会导致模型不稳定或在训练结束时收敛到局部最小值。论文[6]的作者建议首先使用Adam进行快速收敛，然后使用SGD对模型进行微调。</p><p id="ff90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">怎样才能把<em class="jd">亚当</em>和<em class="jd"> SGD </em>结合起来？让我们关注以下两个问题:</p><ol class=""><li id="5e84" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">什么时候从<em class="jd"> Adam </em>切换到<em class="jd"> SGD </em>？</li><li id="b1d3" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">从<em class="jd">亚当</em>切换到<em class="jd">新币</em>时，我们应该使用什么样的学习速率？</li></ol><p id="0e49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">先回答第二个问题。我们回忆一下SGD和Adam的更新值:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es lw"><img src="../Images/8d0d9e6ab5d4cb0dc4db51053ac83cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*fGRfI7IUAMKQ12kc4sYhhA.png"/></div></figure><p id="c298" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们希望<em class="jd"> SGD </em>至少可以像<em class="jd"> Adam </em>一样更新参数。为此，我们将<em class="jd"> SGD </em> ( <em class="jd"> UV_SGD </em>)的更新值投射到<em class="jd"> Adam (UV_ADAM) </em>的更新值的方向。我们必须确保<em class="jd"> UV_SGD </em>在<em class="jd"> UV_ADAM </em>方向上的投影与<em class="jd"> UV_ADAM </em>的值相同。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es lx"><img src="../Images/6f70c8c6bd0a8148b63578711dba1db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*xo5NS3SJmpUNNalno2TIoQ.png"/></div></figure><p id="da0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据这个方程，我们可以计算出<em class="jd"> SGD </em>的学习率<em class="jd"> α </em>。作者将指数平均滤波器添加到学习率中以获得更稳定的值，标记为<em class="jd"> ƛ </em>。</p><p id="c73f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<em class="jd"> α </em>和<em class="jd"> ƛ </em>的帮助下，我们来回答第一个问题:什么时候从<em class="jd">亚当</em>切换到<em class="jd"> SGD </em>。正如作者在论文中提到的，当<em class="jd"> α </em>和<em class="jd"> ƛ </em>的绝对差值小于某个阈值时。是时候转换了。</p></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><h1 id="4de2" class="kg kh hi bd ki kj lm kl km kn ln kp kq kr lo kt ku kv lp kx ky kz lq lb lc ld bi translated">参考:</h1><ol class=""><li id="442c" class="jl jm hi ih b ii lr im ls iq ly iu lz iy ma jc jq jr js jt bi translated">南鲁德，梯度下降优化算法概述，<a class="ae jk" href="https://ruder.io/optimizing-gradient-descent/index.html" rel="noopener ugc nofollow" target="_blank">https://ruder.io/optimizing-gradient-descent/index.html</a></li><li id="c9a7" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">G.Hinton，N.Sricastava，K. Swersky，机器学习的神经网络，<a class="ae jk" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" rel="noopener ugc nofollow" target="_blank">http://www . cs . Toronto . edu/~ tij men/CSC 321/slides/lecture _ slides _ le C6 . pdf</a></li><li id="5adf" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><a class="ae jk" href="https://cs231n.github.io/neural-networks-3/#sgd" rel="noopener ugc nofollow" target="_blank">https://cs231n.github.io/neural-networks-3/#sgd</a></li><li id="8ed0" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><a class="ae jk" href="https://zhuanlan.zhihu.com/p/32230623" rel="noopener ugc nofollow" target="_blank">https://zhuanlan.zhihu.com/p/32230623</a></li><li id="5b07" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">Sashank J. Reddi，Satyen Kale，Sanjiv Kumar，关于亚当和超越的融合，<a class="ae jk" href="https://openreview.net/forum?id=ryQu7f-RZ" rel="noopener ugc nofollow" target="_blank">https://openreview.net/forum?id=ryQu7f-RZ</a></li><li id="1b84" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">Nitish Shirish Keskar，Richard Socher，通过从Adam切换到SGD提高泛化性能，<a class="ae jk" href="https://arxiv.org/abs/1712.07628" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1712.07628</a></li></ol></div></div>    
</body>
</html>