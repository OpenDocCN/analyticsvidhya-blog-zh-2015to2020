# Pytorch 或 Tensorflow，动态与静态计算图

> 原文：<https://medium.com/analytics-vidhya/dynamic-vs-static-computation-graph-2579d1934ecf?source=collection_archive---------0----------------------->

## 动态和静态计算图的区别

![](img/e333de734588392af464ae7e364978de.png)

萨法尔·萨法罗夫在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

使用静态计算图(如 Tensor Flow、CNTK)的框架和使用动态计算图(如 Pytorch 和 DyNet)的框架之间的主要区别在于，后者的工作方式如下，为每个训练样本从头开始构建不同的计算图，然后向前和向后传播，换句话说，用户可以自由地为每个输入样本使用不同的网络，但这当然会增加一些开销，但不要担心像 DyNet 这样的框架有一个优化的 C++后端和轻量级的图形表示。实验表明，DyNet 的速度比静态声明工具包更快或相当，而静态图框架只需定义一次图，然后优化图编译器就可以生成优化图，然后所有训练样本都可以输入到该图中。一方面，一旦编译完成，大型图可以在 CPU 或 GPU 上高效运行
，这使得它非常适合具有固定结构的大型图，其中只有输入在实例之间发生变化。然而，编译步骤本身可能是昂贵的，并且它使得界面使用起来更加麻烦

现在让我们详细了解一下两种范式之间的差异。

## 静态声明

静态声明遵循两个步骤:

> 计算架构的定义

在这个步骤中，用户定义他希望继续进行的图形的形状，例如取一个 16*16 的图像，并且把这个图像传递到 10 个卷积层，用这个确定的函数计算损失并且预测一个确定图像的类别。

> 计算的执行

在计算发生的这一步，用户重复地将数据传递给 16x16 矩阵，库执行先前声明的计算图。然后可以在测试时使用预测，或者在训练时可以计算损失并反向传播

优势

一个显而易见的事情是，图形一旦定义，就可以尽可能快地多次使用，因为实际上我们不会创建任何新的东西，所以这在大型数据集上非常有用，并且在训练和测试速度方面非常有用。第二，静态计算图可以用于跨计算设备池调度计算，因此可以共享计算成本。

不足之处

*   *不同的输入大小可能是一个问题，例如，如果您的输入不限于 16*16，那么定义相同计算的单一结构将会更加困难。*
*   可变结构的输入/输出:更复杂的情况是每个输入不仅有不同的大小，而且有不同的结构，例如你的数据可能有图像、文本和结构化的表格。

然而，如果我们可以在声明时声明一个具有未指定大小的输入的图，并让该图处理输入，因为 TensorFlow 提供了动态 rnn 操作，我们就可以避免这些困难。因此，虽然原则上可以用静态声明来处理可变架构，但在实践中仍然存在一些困难:

计算图实现的复杂性:

为了支持动态执行，计算图必须能够处理更复杂的数据类型(例如，可变大小的张量和结构化数据)，并且像流控制原语这样的操作必须作为操作可用。这增加了计算图形式和实现的复杂性，并减少了优化的机会。

调试难度:

虽然静态分析允许在声明期间识别一些错误，但是许多逻辑错误必须等到执行时才被发现(特别是当许多变量在声明时未被指定时)，这必然远离导致它们的声明代码。根本原因的位置和观察到的崩溃的位置的分离使得调试变得困难。

## 动态声明

这仅执行一步技术，因此回想一下我们的 16*16 图像的示例，加载该图像，然后将其传递到 2 个卷积层，然后计算训练阶段的损失，或者计算测试情况下的预测概率，为每个训练实例创建图表，因此它应该是轻量级的。

## 信用

*   [https://arxiv.org/abs/1701.03980](https://arxiv.org/abs/1701.03980)。
*   Yoav Goldberg 自然语言处理中的神经网络方法-Morgan & Claypool (2017)图书。