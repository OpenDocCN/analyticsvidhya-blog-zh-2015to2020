<html>
<head>
<title>Introduction and a detailed explanation of the k Nearest Neighbors Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k最近邻算法的介绍和详细说明</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-and-a-detailed-explanation-of-the-k-nearest-neighbors-algorithm-9f958d29dd83?source=collection_archive---------12-----------------------#2020-10-10">https://medium.com/analytics-vidhya/introduction-and-a-detailed-explanation-of-the-k-nearest-neighbors-algorithm-9f958d29dd83?source=collection_archive---------12-----------------------#2020-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0aa563c1a0cf1546854762a30ddf0fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0PRKtzuq4WJeUMiBpcSqqA.png"/></div></div></figure><h1 id="7287" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">kNN是什么？</h1><p id="3474" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">k近邻算法是最简单的机器学习算法之一。具体来说就是一个“分类”算法。但由于其通用程序，它也可用于特征选择、异常值检测(威尔逊编辑)和缺失值插补。它也被称为<em class="km">基于实例的学习</em>和<em class="km">懒惰学习</em>，因为在训练时它什么也不做！在kNN中，超参数是“k”。</p><h1 id="8952" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">kNN的工作</h1><p id="b49b" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">kNN有一个简单的工作机制。我会用4个步骤来解释。当一个测试点进来时，这就是我们在kNN中所做的，</p><ol class=""><li id="3ee7" class="kn ko hi jq b jr kp jv kq jz kr kd ks kh kt kl ku kv kw kx bi translated">固定k的值</li><li id="94dd" class="kn ko hi jq b jr ky jv kz jz la kd lb kh lc kl ku kv kw kx bi translated">通过欧几里德距离公式(或任何距离查找算法)找到k个最近的邻居</li><li id="6478" class="kn ko hi jq b jr ky jv kz jz la kd lb kh lc kl ku kv kw kx bi translated">给班级标签投票</li><li id="75ab" class="kn ko hi jq b jr ky jv kz jz la kd lb kh lc kl ku kv kw kx bi translated">预言；预测；预告</li></ol><p id="d7fc" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">让我用一个简单的例子来说明kNN。让我们假设我们的数据集有3个类别标签(A，B，C)。让我们将k的值固定为3，即我们找到3个最近的邻居。现在，当一个测试点进来时，我们在数据集中找到3个最近的邻居。假设我们的算法给出了3个最近的邻居A，A，C，因为测试点必须只属于一个类，我们必须只从A，A，C中选择一个，我们现在引入一个投票机制，因为A是2，C是1。“A”赢得了游戏，我们指定测试点属于类标签“A”。就这么简单！</p><p id="b1e1" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">现在，让我们用代码来看看详细的解释。</p><h1 id="feb6" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">kNN算法解释</h1><h2 id="adf6" class="lg ir hi bd is lh li lj iw lk ll lm ja jz ln lo je kd lp lq ji kh lr ls jm lt bi translated">0.导入所需的库</h2><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="6a14" class="lg ir hi lz b fi md me l mf mg"><strong class="lz hj">from sklearn import datasets<br/>import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import random as rnd<br/>import csv<br/>import random<br/>import math<br/>import operator<br/>%matplotlib inline</strong></span></pre><p id="67a2" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">让我们研究一下著名的虹膜数据集。不了解的话看看<a class="ae mh" href="http://archive.ics.uci.edu/ml/datasets/Iris/" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> <em class="km">虹膜数据集</em> </strong> </a>。第一步是加载数据集，然后将数据集分成训练数据和测试数据。</p><h2 id="23c9" class="lg ir hi bd is lh li lj iw lk ll lm ja jz ln lo je kd lp lq ji kh lr ls jm lt bi translated">1.加载数据集并分割数据集</h2><p id="fc6b" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">虹膜数据集可以通过两种方式加载。一种方法是从sklearn库加载，另一种方法是从本地桌面加载。让我们遵循第二种方法。</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="30f3" class="lg ir hi lz b fi md me l mf mg"><strong class="lz hj">def loadDataset(filename, split, trainingSet=[] , testSet=[]):<br/>  with open(filename, 'r') as csvfile:<br/>     lines = csv.reader(csvfile)<br/>     dataset = list(lines)<br/>     for x in range(len(dataset)-1):<br/>         for y in range(4):<br/>             dataset[x][y] = float(dataset[x][y])<br/>         if random.random() &lt; split:<br/>             trainingSet.append(dataset[x])<br/>         else:<br/>             testSet.append(dataset[x])</strong></span></pre><p id="562d" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">loadDataset是加载我们的数据集的函数。我们以读取模式打开CSV文件。因为我们的数据集是逗号分隔的值，所以我们使用CSV reader。我们运行一个循环，在每次迭代中，我们转换数据类型，并根据随机函数的返回值将数据点分成训练或测试。</p><p id="541b" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">默认情况下，数据集中的值作为字符串读取。但是值(萼片长度、萼片宽度、花瓣长度和花瓣宽度)应该是浮点类型。所以我们把它们转换成浮点型。</p><p id="0613" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">现在，我们必须将数据集分成两部分。我们随机分割数据集，并填充训练和测试数据集。</p><p id="317c" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">让我们调用这个函数。</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="3d02" class="lg ir hi lz b fi md me l mf mg"><strong class="lz hj">trainingSet=[]<br/>testSet=[]<br/>split = 0.67<br/>loadDataset('iris.csv', split, trainingSet, testSet)<br/>print('Train set is ', repr(len(trainingSet)))<br/>print('Test set is  ' , repr(len(testSet)))</strong></span></pre><p id="528b" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">完成这些后，我们可以看到我们的数据集已经成功地分为训练和测试。进入下一步。</p><h2 id="0012" class="lg ir hi bd is lh li lj iw lk ll lm ja jz ln lo je kd lp lq ji kh lr ls jm lt bi translated">2.定义距离和邻域函数</h2><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="2f24" class="lg ir hi lz b fi md me l mf mg"><strong class="lz hj">def euclideanDistance(instance1, instance2, length):<br/>    distance = 0<br/>    for x in range(length):<br/>        distance += pow((instance1[x] - instance2[x]), 2)<br/>    return math.sqrt(distance)</strong></span><span id="d8a8" class="lg ir hi lz b fi mi me l mf mg"><strong class="lz hj">def getNeighbors(trainingSet, testInstance, k):<br/>    distances = []<br/>    length = len(testInstance)-1<br/>    for x in range(len(trainingSet)):<br/>        dist = euclideanDistance(testInstance, trainingSet[x], length)<br/>        distances.append((trainingSet[x], dist))<br/>    distances.sort(key=operator.itemgetter(1))<br/>    neighbors = []<br/>    for x in range(k):<br/>        neighbors.append(distances[x][0])<br/>    return neighbors</strong></span></pre><p id="c89e" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我前面说过，在kNN算法中，我们用欧几里德原理来度量两点之间的距离。这个过程在“euclideanDistance”函数中完成。如果我们有3个维度，则公式变为√( x2 x1)+(y2 y1)+(z2z 1)，但在我们的数据集中，我们有4个维度/列(萼片长度、萼片宽度、花瓣长度和花瓣宽度)，因此我们在上面的公式中添加一个额外的项，然后返回距离。“长度”参数是数据集中的维数，即4。</p><p id="c7f4" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">在getNeighbors函数中，给定一个测试点、整个训练数据、值“k”，该函数应该返回该测试点在整个训练集中的k个最近邻。为了实现这一点，我们对整个训练数据集运行一个循环，在每次迭代中，我们找到测试点和训练点之间的距离。计算完所有距离后，我们按升序对距离进行排序，因为我们只需要前“k”个最近的邻居。我们返回最近的邻居。</p><h2 id="86a7" class="lg ir hi bd is lh li lj iw lk ll lm ja jz ln lo je kd lp lq ji kh lr ls jm lt bi translated">3.投票</h2><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="a2b6" class="lg ir hi lz b fi md me l mf mg"><strong class="lz hj">def getResponse(neighbors):<br/>    classVotes = {}<br/>    for x in range(len(neighbors)):<br/>        response = neighbors[x][-1]<br/>        if response in classVotes:<br/>            classVotes[response] += 1<br/>        else:<br/>            classVotes[response] = 1<br/>    sortedVotes = sorted(classVotes.items(), key=operator.itemgetter(1), reverse=True)<br/>    return sortedVotes[0][0]</strong></span></pre><p id="480f" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">和前面的例子一样，当我们有2个A和1个C时，我们选择了A作为测试点的类标签。在这个函数中，我们正是这样做的。在找到k个最近的邻居后，我们必须对类别标签进行投票。这可以通过填充classVotes字典轻松完成。填充完字典后，我们需要对字典进行排序，以便查看谁是赢家！我们回报胜利者！</p><h2 id="4606" class="lg ir hi bd is lh li lj iw lk ll lm ja jz ln lo je kd lp lq ji kh lr ls jm lt bi translated">4.找到我们模型的准确性</h2><p id="0319" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">仅仅创建一个模型是不够的，模型的准确性应该相当好。现在，准确性是(我们的模型做出的总预测除以正确的预测)。检验准确性很简单。如下所述，</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="a489" class="lg ir hi lz b fi md me l mf mg"><strong class="lz hj">def getAccuracy(testSet, predictions):<br/>    correct = 0<br/>    for x in range(len(testSet)):<br/>        if testSet[x][-1] == predictions[x]:<br/>            correct += 1<br/>    return (correct/float(len(testSet))) * 100.0</strong></span></pre><p id="6189" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我们所有的预测都存储在“预测”列表中。我们的正确预测存储在我们的测试数据中。因此，我们将预测的类别标签与原始类别标签进行比较，并最终返回准确度。</p><h2 id="569a" class="lg ir hi bd is lh li lj iw lk ll lm ja jz ln lo je kd lp lq ji kh lr ls jm lt bi translated">5.测试我们的模型</h2><p id="3793" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">重要的一步是“测试我们的模型”。我们在测试数据上测试我们的模型，并发现我们的模型的准确性。</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="c4cb" class="lg ir hi lz b fi md me l mf mg"><strong class="lz hj">predictions=[]<br/>k = 3<br/>for x in range(len(testSet)):<br/>    neighbors = getNeighbors(trainingSet, testSet[x], k)<br/>    result = getResponse(neighbors)<br/>    predictions.append(result)<br/>    print('Predicted is  ' + repr(result) + ' Actual is  ' + repr(testSet[x][-1]))<br/>accuracy = getAccuracy(testSet, predictions)<br/>print('Accuracy: ' + repr(accuracy) + '%')</strong></span></pre><p id="127e" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我们选择“k”的值为3。对于测试数据集中的每个测试点，我们找到它最近的邻居并投票。我们将所有的预测存储在“预测”列表中。最后，我们检查模型的准确性！</p><p id="e354" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">就这样，我们从头开始编写了kNN算法！！是不是很轻松很好玩？</p></div><div class="ab cl mj mk gp ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="hb hc hd he hf"><h1 id="4264" class="iq ir hi bd is it mq iv iw ix mr iz ja jb ms jd je jf mt jh ji jj mu jl jm jn bi translated">kNN的问题</h1><p id="2ebd" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">到目前为止，我们已经看到了kNN是如何工作的以及它可以用在什么地方，但是它有什么问题呢？简而言之，它有两个主要问题。</p><p id="52fb" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">第一个是“<em class="km">大计算时间</em>”。因为在训练时，它什么也不做，而在测试时，应该计算距离。距离的计算高度依赖于数据集中的特征/维度/列的数量。如果有更多的特征，那么距离的计算就变得很耗时。</p><p id="20eb" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">第二件事，“模型尺寸大”。因为在训练时间它什么也不做，在kNN中我们的模型是数据。数据= kNN中的模型。如果我们的数据庞大。然后我们的模型也很庞大，这是个问题。</p><p id="076a" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我希望你对kNN有所了解。如果您喜欢该内容，请点击“鼓掌”按钮，</p><p id="7c4c" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">和我联系-</p><blockquote class="mv"><p id="0fe3" class="mw mx hi bd my mz na nb nc nd ne kl dx translated"><code class="du nf ng nh lz b">LinkedIn : https://linkedin.com/in/bomma-pranay<br/>GitHub : https://github.com/Bomma-Pranay</code></p><p id="6b4e" class="mw mx hi bd my mz na nb nc nd ne kl dx translated"><code class="du nf ng nh lz b">--- By<br/>Bomma Pranay<br/>A Data Science Enthusiast</code></p></blockquote></div></div>    
</body>
</html>