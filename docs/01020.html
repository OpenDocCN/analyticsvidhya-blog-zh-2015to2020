<html>
<head>
<title>Autoencoders - Denoising Understanding!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器-去噪理解！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/autoencoders-denoising-understanding-b41315fd7fa?source=collection_archive---------9-----------------------#2019-09-25">https://medium.com/analytics-vidhya/autoencoders-denoising-understanding-b41315fd7fa?source=collection_archive---------9-----------------------#2019-09-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="aecc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过在MNIST数据集上训练一个模型来对数字进行分类，在现有的框架下确实是一件有趣的事情，将其投入生产将会非常棒。</p><p id="1f68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代号:<a class="ae jd" href="https://github.com/parmarsuraj99/Autoencoders" rel="noopener ugc nofollow" target="_blank">https://github.com/parmarsuraj99/Autoencoders</a></p><p id="6fb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道神经网络可以被视为<strong class="ih hj">【通用函数估值器】</strong>，意味着我们可以将它们映射到正确的标签上。这就是所谓的监督学习方法。</p><p id="bdcf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们没有标签呢？我们只剩下图像了？我们能用它们做什么？这越来越有趣了。我们可以训练一个网络来提高图像的分辨率，消除噪声，甚至生成新的样本。在某种程度上，将它们压缩到网络隐藏层的较低维度。</p><h1 id="0a3c" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">自动编码器</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kc"><img src="../Images/f651dcbcc4b0838d219ed07e23134466.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*uQOQpmY95tHJrwyhnozZVA.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated"><a class="ae jd" href="https://www.deeplearningbook.org/contents/autoencoders.html" rel="noopener ugc nofollow" target="_blank">深度学习书籍</a></figcaption></figure><blockquote class="ko kp kq"><p id="c4a7" class="if ig kr ih b ii ij ik il im in io ip ks ir is it kt iv iw ix ku iz ja jb jc hb bi translated">"自动编码器是一种神经网络，它被训练成试图将其输入复制到其输出."-深度学习书籍</p></blockquote><p id="c8e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它有一个学习输入表示的隐藏层。它可以被视为两部分网络:编码器部分，<em class="kr"> h=f(x) </em>和解码器部分<em class="kr"> r=g(h) </em>。目标是学习<em class="kr"> g(f(x))=x </em>。但是我们不希望它简单地接受一个输入并产生精确的输出。没用的！相反，我们希望它接近输出。也就是说，要学会用一种能够产生近似输出的方式来表示输入。也就是学习输入的中间表示。</p><p id="6b85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">传统上，它们被用于降维和主成分分析，但现在通过稍微改变架构，它们被视为生成模型的前沿。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kv"><img src="../Images/ba49c4e5ce6e322195a8a66b801c5d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*CxPztPOM3dZaetN2g4Okog.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">自动编码器的代表性体系结构</figcaption></figure><p id="b7df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们可以看到，我们正在尝试首先学习将输入映射到瓶颈，然后将瓶颈映射到输出。这是一个端到端的过程。</p><p id="58f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">编码器:</strong>取一个输入<strong class="ih hj"> <em class="kr"> x </em> </strong>(可以是图像、文字嵌入或语音数据)并产生一个输出<strong class="ih hj"> <em class="kr"> h </em> </strong>。例如，想象一个尺寸为32x32x1(HxWxC)的图像，它被缩小为3x1输出。<em class="kr">把这个想象成7zip之类的压缩软件。</em></p><p id="cf5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解码器:</strong>取一个输入<strong class="ih hj"> <em class="kr"> h </em> </strong>(密集表示)并产生一个输出<strong class="ih hj"> <em class="kr"> ~x. </em> </strong>例如，3x1矢量作为输入产生一个32x32x1的图像，它类似于<strong class="ih hj"> <em class="kr"> x. </em> </strong> <em class="kr">它就像从一个zip文件中恢复原始数据。</em></p><p id="937a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，我们可以使用什么类型的数据呢？这有什么应用？</p><ol class=""><li id="a9a6" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">我们已经看到，编码器可以产生低维数据，这可以用于<strong class="ih hj">降维</strong>，非常类似于主成分分析(PCA ),假设数据来自相同的域(相似的数据)。</li><li id="713b" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">它近似图像，所以我们可以将一个有噪声图像映射到一个去噪声图像。从而作为一个<strong class="ih hj">降噪器</strong>！</li><li id="a748" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">由于尺寸已经减少，我们可以更快地找到相似的图像比全尺寸的图像。这就像<strong class="ih hj">的语义哈希法。</strong></li><li id="cac5" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">以对抗方式训练的自动编码器可以用于<strong class="ih hj">生成</strong>目的。</li></ol><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lk"><img src="../Images/ec0274808a73e96cb870fe77046b4ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dSIqhTNMlA7cp-0z.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx translated">简单的自动编码器插图</figcaption></figure><p id="dfde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理想的自动编码器模型平衡了以下几点:</p><ul class=""><li id="f125" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lp lc ld le bi translated">对输入足够敏感以精确地重建。</li><li id="fd4a" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lp lc ld le bi translated">对输入不够敏感，模型不会简单地记忆或过度拟合训练数据。</li></ul><h1 id="f7cf" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">深度自动编码器</strong></h1><p id="0835" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">我们不应该局限于只使用一个隐藏层。这是一个深度完全连接的网络，它获取并处理扁平化的MNIST图像。</p><p id="cf0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们将使用MNIST数据集。这是手写数字的黑白28×28图像，因为它将易于构建和理解简单的自动编码器网络。</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="8e0a" class="ma jf hi lw b fi mb mc l md me">(x_train, y_train), (x_test, y_test) = mnist.load_data()<br/>x_train = x_train.astype('float32')/255.0<br/>x_test = x_test.astype('float32')/255.0<br/>x_train = x_train.reshape(len(x_train), (x_train.shape[1]*x_train.shape[2]))<br/>x_test = x_test.reshape(len(x_test), (x_test.shape[1]*x_test.shape[2]))</span></pre><p id="1d15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深层网络图像的扁平化和规范化加载与处理。</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="01cb" class="ma jf hi lw b fi mb mc l md me">encoding_dim = 32</span><span id="538c" class="ma jf hi lw b fi mf mc l md me">input_img = Input(shape = (784, ))<br/>encoded = Dense(128, activation='relu')(input_img)<br/>encoded = Dense(encoding_dim, activation='relu')(encoded)</span><span id="637a" class="ma jf hi lw b fi mf mc l md me">decoded = Dense(128, activation='relu')(encoded)<br/>decoded = Dense(784, activation='sigmoid')(decoded)</span><span id="56af" class="ma jf hi lw b fi mf mc l md me">autoencoder = Model(input_img, decoded)</span><span id="e2bf" class="ma jf hi lw b fi mf mc l md me">encoder = Model(input_img, encoded)<br/>encoded_input = Input(shape=(encoding_dim, ))<br/>decode_layer1 = autoencoder.layers[-2]<br/>decode_layer2 = autoencoder.layers[-1]<br/>decoder = Model(encoded_input, decode_layer2(decode_layer1(encoded_input)))</span><span id="0634" class="ma jf hi lw b fi mf mc l md me">hist = autoencoder.fit(x_train, x_train, epochs=10, validation_data=(x_test, x_test))</span></pre><p id="c415" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个简单的全连接网络，它采用784个扁平像素，并使其流经128个神经元，然后流经32个神经元。这32个神经元的激活是外部潜在空间的表征。然后由128和784个神经元顺序解码。然后我们再把它重塑成28x28的图像。</p><p id="647f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们试图迫使编码器以这样一种方式压缩图像，即解码器可以从这个编码表示(潜在空间)中重建它。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mg"><img src="../Images/96b83858883e5c1fd6b25891388666a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*YFr1iOnXzzNI0pMYkW3ccA.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">模型摘要</figcaption></figure><p id="6859" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然损耗在减少，但是这里需要注意的一点是，重建是有损耗的。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mh"><img src="../Images/6b97986a5fb56c3588dfdbd247d23182.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*TrXxirdWnrEx08yKBbTTeg.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">可视化。左:原始图像，中:特征可视化，右:重建</figcaption></figure><h1 id="e60b" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">使用卷积神经网络获得更好的结果</h1><p id="573a" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">CNN已经被证明非常擅长处理图像数据。因此，我们可以直接将图像馈送给CNN编码器和解码器，而不是将图像扁平化。通过使用它们，我们可以期望看到更好的结果。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mi"><img src="../Images/9c633017d302c3ef1db564e5d84c8922.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*muqVuf2_nUGb1z7nm67zOg.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx translated">潜在空间自动编码器</figcaption></figure><p id="81f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的想法是，通过使用过滤器学习更多的特征，通过学习更好的方式来表示潜在空间的输入，ConvNets学习得更好。</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="dc2a" class="ma jf hi lw b fi mb mc l md me">def CNN_AE():<br/>    input_img = Input(shape=(img_width, img_height, 1))<br/>    <br/>    # Encoding network<br/>    x = Conv2D(16, (3, 3), activation='relu', padding='same', strides=2)(input_img)<br/>    x = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(x)<br/>    encoded = Conv2D(32, (2, 2), activation='relu', padding="same", strides=2)(x)<br/># Decoding network<br/>    x = Conv2D(32, (2, 2), activation='relu', padding="same")(encoded)</span><span id="6dec" class="ma jf hi lw b fi mf mc l md me">x = UpSampling2D((2, 2))(x)<br/>    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)<br/>    x = UpSampling2D((2, 2))(x)<br/>    x = Conv2D(16, (3, 3), activation='relu')(x)<br/>    x = UpSampling2D((2, 2))(x)<br/>    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)<br/>    <br/>    encoder = Model(input_img, encoded)<br/>    <br/>   return Model(input_img, decoded), encoder</span></pre><p id="5d67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">完整代码在github repo中。可以在浏览器的colab中打开。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mj"><img src="../Images/07b262e9546a5ca18fb028ac66792d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*YZpPi3xDzwD00trn3r0Fcw.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx translated">潜在空间表征的CNN声发射结果</figcaption></figure><p id="6440" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以清楚地看到，与以前的深度AE相比，重建结果更好。我们可以看到潜在空间表征，它是输入的压缩表征。</p><p id="d613" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个图像都通过相同的编码器功能并被压缩。这导致更小尺寸的图像。我们可以用更少的计算来执行聚类。</p><h1 id="23ca" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">有趣的应用——图像去噪</h1><p id="f0f0" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">因为AE可以学习将图像表达到潜在空间中，并从中重建。它还可以学习去除图像中的噪声。例如，如果我们训练它将嘈杂的图像映射到清晰的图像，它可能会学习忽略噪声并进行重建。</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="7291" class="ma jf hi lw b fi mb mc l md me">noise_factor = 0.5<br/>x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) <br/>x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)</span><span id="0558" class="ma jf hi lw b fi mf mc l md me">x_train_noisy = np.clip(x_train_noisy, 0., 1.)<br/>x_test_noisy = np.clip(x_test_noisy, 0., 1.)</span><span id="78a6" class="ma jf hi lw b fi mf mc l md me">cnn_hist = model_cnn.fit(x_train_noisy, x_train, validation_data=(x_test_noisy, x_test))</span></pre><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mj"><img src="../Images/c576501f9aacfad52d89f92585aaf10f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*yZ2C59aKXARCA7qVnKflYQ.png"/></div></div><figcaption class="kk kl et er es km kn bd b be z dx translated">噪声图像重建</figcaption></figure><p id="1b20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在笔记本上，我已经把从前面的例子中学到的东西转移了。也就是说，使用相同的模型，这样它可以学习得更快。我们可以在上面的图中观察到表示的变化。这只是自动编码器的一个应用。</p><p id="ff2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它似乎工作得很好。如果您将此过程扩展到更大的ConvNet，您可以开始构建文档去噪或音频去噪模型。</p><h1 id="3126" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">序列到序列</h1><p id="ad45" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">如果您输入的是序列，而不是矢量或2D图像，那么您可能希望使用一种能够捕捉时间结构的模型(如LSTM)作为编码器和解码器。要构建基于LSTM的自动编码器，首先使用LSTM编码器将输入序列转换为包含整个序列信息的单个向量，然后重复这个向量<code class="du mk ml mm lw b">n</code>次(其中<code class="du mk ml mm lw b">n</code>是输出序列中的时间步长数)，并运行LSTM解码器将这个常量序列转换为目标序列。</p><h1 id="4e82" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">可变自动编码器</h1><p id="dbce" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">我们没有让神经网络决定如何在潜在空间中表示，而是对其进行了限制。意味着，限制潜在空间的表达，这样我们就可以用很少的参数进行重建。我们稍后将对此进行探讨。</p><h1 id="d904" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">参考</h1><div class="mn mo ez fb mp mq"><a href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">在Keras中构建自动编码器</h2><div class="mx l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">在本教程中，我们将回答一些关于自动编码器的常见问题，我们将涵盖代码的例子…</h3></div><div class="my l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">blog.keras.io</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ki mq"/></div></div></a></div><p id="541c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">https://www.deeplearningbook.org/contents/autoencoders.html<a class="ae jd" href="https://www.deeplearningbook.org/contents/autoencoders.html" rel="noopener ugc nofollow" target="_blank"/></p><p id="ac69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【https://github.com/parmarsuraj99/Autoencoders T4】</p></div></div>    
</body>
</html>