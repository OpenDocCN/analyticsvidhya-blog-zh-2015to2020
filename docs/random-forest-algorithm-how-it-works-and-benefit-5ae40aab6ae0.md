# 随机森林:工作原理和优势

> 原文：<https://medium.com/analytics-vidhya/random-forest-algorithm-how-it-works-and-benefit-5ae40aab6ae0?source=collection_archive---------13----------------------->

![](img/29fbb31b8d1f31b67cec6ab4b8cbde52.png)

[迈克尔·本茨](https://unsplash.com/@michaelbenz?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍照

机器学习的一大部分是分类——我们想知道一个观察值属于哪个类或组。对观察结果进行精确分类的能力对于各种业务应用程序非常有价值，例如预测特定用户是否会购买产品，或者预测给定的贷款是否会违约。

数据科学提供了大量的分类算法，如逻辑回归、支持向量机、朴素贝叶斯分类器和决策树。但是在分类器层次结构的顶端附近是随机森林分类器。

在这篇文章中，我们将研究随机森林算法是如何工作的，以及我是如何对这个算法如此着迷的。我最喜欢解释这种算法的方式是，一家公司通过多轮面试来聘用一名候选人。

比方说，你申请了亚马逊的数据分析师职位。像大多数公司一样，你不会只有一轮面试。你有多轮面试。每一次访谈都由独立小组主持。每个小组分别独立地评估候选人。一般来说，就连这些面试中问的问题也各不相同。

我们之所以有一个面试小组，是因为我们认为一个小组通常会比一个人做出更好的决定。这个委员会不是一群人。我们确保面试小组在每次面试涉及的话题、提问的类型以及许多其他细节方面有所多样化。你不会在每轮面试中都问同样的问题。

经过所有轮次的面试后，最终决定是选择还是拒绝候选人是基于每个小组的大多数决定。如果 5 个面试小组中，3 个推荐录用，2 个反对录用，我们倾向于继续选择候选人。

![](img/967633e86aaa08e59de2436f17ac00a6.png)

这里有两个关键词——**随机**和**森林**。随机森林或随机决策森林是决策树的集合。从单词 forest 来看，是树的集合，所以 random forest 是许多决策树的集合，比如 100 棵。

# **为什么叫随机？**

假设我们有一个 1000 行 30 列的数据集。该算法中有两个级别的随机性:

*   **在行级别**:这些决策树中的每一个都获得了训练数据的随机样本(比如 10%)，也就是说，这些树中的每一个都将在 1，000 行数据中随机选择的 100 行上进行独立训练。请记住，这些决策树中的每一个都是根据从数据集中随机选择的 100 行进行训练的，也就是说，它们在预测方面互不相同。
*   列级**:列级引入了第二级随机性。并非所有的列都被传递到每个决策树的训练中。假设我们只想将 10%的列发送到每棵树。这意味着随机选择的 3 列将被发送到每棵树。所以对于第一个决策树，可能列 C1，C2 和 C4 被选中。下一个 DT 将选择 C4、C5、C10 列，依此类推。**

**我来打个比方。现在让我们来理解面试选择过程是如何类似于随机森林算法的。面试过程中的每个小组实际上都是一棵决策树。每个小组给出一个结果，无论候选人是通过还是失败，然后这些结果中的大多数被宣布为最终结果。假设有 5 个小组，3 个说是，2 个说不是。最后的结论是肯定的。**

**类似的事情也发生在随机森林中。从每棵树中取出结果，并相应地声明最终结果。在分类和回归的情况下，分别使用投票和平均来预测。**

# **随机森林的缺点**

1.  ****随机森林在较小的数据集上训练得不好**，因为它无法选择模式。简而言之，假设我们知道 1 支笔价值 1 美元，2 支笔价值 2 美元，3 支笔价值 6 美元。在这种情况下，线性回归将很容易估计 4 支笔的成本，但随机森林将无法得出一个好的估计值。**
2.  ****随机森林存在可解释性问题。**你看不到或理解反应和自变量之间的关系。了解随机森林是一种预测工具，而不是描述工具。你得到了可变的重要性，但这在许多兴趣分析中可能是不够的，在这些分析中，目标可能是看到反应和独立特征之间的关系。**
3.  **当你训练多个决策树时，训练随机森林所花费的时间有时可能太长了。此外，在分类变量的情况下，时间复杂性呈指数增长。**
4.  **在回归问题的情况下，**响应变量可以取值的范围**由训练数据集中已经可用的值决定。与线性回归不同，决策树和随机森林不能采用训练数据之外的值。**

# **随机森林的好处**

1.  **由于我们使用了多个决策树**，因此偏差与单个决策树**的偏差相同。然而，方差减少，因此我们减少了过度拟合的机会。**
2.  **当你所关心的只是预测，而**想要一个快速而肮脏的出路**时，随机森林会前来解救。您不必太担心模型的假设或数据集中的线性度。**