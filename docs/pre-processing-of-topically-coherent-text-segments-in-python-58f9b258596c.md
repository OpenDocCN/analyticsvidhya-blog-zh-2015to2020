# Python ä¸­ä¸»é¢˜è¿è´¯æ–‡æœ¬æ®µçš„é¢„å¤„ç†ğŸ’¬

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/pre-processing-of-topically-coherent-text-segments-in-python-58f9b258596c?source=collection_archive---------4----------------------->

## å¦‚ä½•ä½¿ç”¨è‡ªç„¶è¯­è¨€å·¥å…·åŒ…é¢„å¤„ç†ä¸€ç»„æŠ„æœ¬å¹¶å°†å…¶è½¬æ¢æˆæ•°å­—è¡¨ç¤º

å®Œæ•´çš„ **Jupyter ç¬”è®°æœ¬**å’Œæ–‡ä»¶å¯åœ¨æˆ‘çš„ [**GitHub é¡µé¢**](https://github.com/maziarizadi/TextPreProcessingPy) è·å¾—ã€‚

## ä»‹ç»

æ–‡æœ¬æ–‡æ¡£ï¼Œå¦‚é•¿å½•éŸ³å’Œä¼šè®®è®°å½•ï¼Œé€šå¸¸ç”±ä¸»é¢˜è¿è´¯çš„æ–‡æœ¬ç‰‡æ®µç»„æˆï¼Œæ¯ä¸ªç‰‡æ®µåŒ…å«ä¸€å®šæ•°é‡çš„æ–‡æœ¬æ®µè½ã€‚åœ¨æ¯ä¸€ä¸ªä¸»é¢˜è¿è´¯çš„ç‰‡æ®µä¸­ï¼Œäººä»¬ä¼šæœŸæœ›å•è¯çš„ä½¿ç”¨æ¯”è·¨ç‰‡æ®µçš„ä½¿ç”¨è¡¨ç°å‡ºæ›´ä¸€è‡´çš„è¯æ±‡åˆ†å¸ƒã€‚**è‡ªç„¶è¯­è¨€å¤„ç†(NLP)** ï¼Œæ›´å…·ä½“åœ°è¯´ï¼Œå°†æ–‡æœ¬çº¿æ€§åˆ’åˆ†æˆä¸»é¢˜ç‰‡æ®µå¯ç”¨äºæ–‡æœ¬åˆ†æä»»åŠ¡ï¼Œä¾‹å¦‚ä¿¡æ¯æ£€ç´¢ä¸­çš„æ®µè½æ£€ç´¢ã€æ–‡æ¡£æ‘˜è¦å’Œè¯è¯­åˆ†æã€‚åœ¨å½“å‰çš„ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†å›é¡¾å¦‚ä½•ç¼–å†™ Python ä»£ç æ¥**é¢„å¤„ç†**ä¸€ç»„æŠ„æœ¬å¹¶ä¸”**å°†å®ƒä»¬è½¬æ¢æˆé€‚åˆè¾“å…¥åˆ°**ä¸»é¢˜åˆ†å‰²ç®—æ³•**ä¸­çš„æ•°å€¼è¡¨ç¤º**ã€‚

![](img/ebded4b8abcabc0f3892c045e78bf18f.png)

[å›¾åƒçš„æ¥æº](https://s3.amazonaws.com/codecademy-content/courses/NLP/Natural_Language_Processing_Overview.gif)

è¿™ç¯‡æ–‡ç« çš„æ¥æºæ¥è‡ªæˆ‘åœ¨è«çº³ä»€å¤§å­¦å®Œæˆçš„**æ•°æ®ç§‘å­¦ç ”ç©¶ç”Ÿæ–‡å‡­çš„ä¸€éƒ¨åˆ†ä½œä¸šã€‚æˆ‘ä¹Ÿåšäº†ä¸€äº›æ”¹åŠ¨ï¼Œè®©åŸæ¥çš„ä»»åŠ¡æ›´æœ‰è¶£ã€‚**

## ä»€ä¹ˆæ˜¯ä½¿ç”¨æ¡ˆä¾‹ï¼ŒNLP å¦‚ä½•æä¾›å¸®åŠ©ï¼Ÿ

ç°åœ¨æœ‰å¾ˆå¤šæ±‚èŒç½‘ç«™ï¼ŒåŒ…æ‹¬ seek.com.au å’Œ au.indeed.comã€‚è¿™äº›æ±‚èŒç½‘ç«™éƒ½ç®¡ç†ç€ä¸€ä¸ªæ±‚èŒç³»ç»Ÿï¼Œæ±‚èŒè€…å¯ä»¥æ ¹æ®å…³é”®è¯ã€è–ªæ°´å’Œç±»åˆ«æ¥æœç´¢ç›¸å…³çš„å·¥ä½œã€‚é€šå¸¸ï¼Œå¹¿å‘Šå·¥ä½œçš„ç±»åˆ«ç”±å¹¿å‘Šå•†(ä¾‹å¦‚ï¼Œé›‡ä¸»)æ‰‹åŠ¨è¾“å…¥ã€‚ç±»åˆ«åˆ†é…å¯èƒ½ä¼šå‡ºé”™ã€‚**å› æ­¤ï¼Œé”™è¯¯ç±»åˆ«çš„å·¥ä½œå°†æ— æ³•è·å¾—ç›¸å…³å€™é€‰ç¾¤ä½“çš„è¶³å¤Ÿæ›å…‰åº¦**ã€‚

éšç€æ–‡æœ¬åˆ†æçš„è¿›æ­¥ï¼Œè‡ªåŠ¨å·¥ä½œåˆ†ç±»å°†å˜å¾—å¯è¡Œï¼Œå¹¶ä¸”å¯ä»¥å‘æ½œåœ¨çš„å¹¿å‘Šå®¢æˆ·æä¾›åˆç†çš„å·¥ä½œç±»åˆ«å»ºè®®ã€‚è¿™æœ‰åŠ©äºå‡å°‘äººå·¥æ•°æ®è¾“å…¥é”™è¯¯ï¼Œå¢åŠ ç›¸å…³å€™é€‰äººçš„èŒä½æ›å…‰ç‡ï¼Œè¿˜å¯ä»¥æ”¹å–„æ±‚èŒç½‘ç«™çš„ç”¨æˆ·ä½“éªŒã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª**è‡ªåŠ¨æ‹›è˜å¹¿å‘Šåˆ†ç±»**ç³»ç»Ÿï¼Œå®ƒåœ¨ç°æœ‰çš„æ‹›è˜å¹¿å‘Šæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…·æœ‰æ ‡å‡†åŒ–çš„å·¥ä½œç±»åˆ«ï¼Œ**é¢„æµ‹æ–°è¾“å…¥çš„æ‹›è˜å¹¿å‘Šçš„ç±»åˆ«æ ‡ç­¾**ã€‚

> å½“å‰ç¤ºä¾‹æ¶‰åŠå¤„ç†å·¥ä½œå¹¿å‘Šæ–‡æœ¬æ•°æ®çš„ç¬¬ä¸€æ­¥ï¼Œå³ï¼Œå°†å·¥ä½œå¹¿å‘Šæ–‡æœ¬è§£ææˆæ›´åˆé€‚çš„æ ¼å¼ã€‚

æˆ‘ä»¬æä¾›çš„æ‹›è˜å¹¿å‘Šæ•°æ®åŒ…å«å¤§é‡ä»¥ç®€å• txt æ ¼å¼è¡¨ç¤ºçš„å†—ä½™ä¿¡æ¯ã€‚æˆ‘ä»¬åº”è¯¥å¯¹æ‹›è˜å¹¿å‘Šæ–‡æœ¬æ•°æ®è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†ï¼Œä»¥æé«˜åˆ†ç±»ç®—æ³•çš„æ€§èƒ½ã€‚

# é—®é¢˜é™ˆè¿°ğŸ’¡

æˆ‘ä»¬éœ€è¦ç¼–å†™ Python ä»£ç æ¥æå–ä¸€ç»„è¡¨ç¤ºæ¯ä¸ªæ‹›è˜å¹¿å‘Šå†…å®¹çš„å•è¯(ä¾‹å¦‚ï¼Œunigrams ),ç„¶å**å°†æ¯ä¸ªå¹¿å‘Šæè¿°è½¬æ¢ä¸ºæ•°å­—è¡¨ç¤º** : count vectorï¼Œå®ƒå¯ä»¥ç›´æ¥ç”¨ä½œè®¸å¤šåˆ†ç±»ç®—æ³•çš„è¾“å…¥ã€‚

## æˆ‘ä»¬å°†é‡‡å–ä»€ä¹ˆæ­¥éª¤ï¼Ÿ

*   æå–
    æ•°æ®æ–‡ä»¶`data.txt`ä¸­æ‰€æœ‰æ‹›è˜å¹¿å‘Šçš„ id å’Œæè¿°(çº¦ 500 æ¡æ‹›è˜å¹¿å‘Š)ã€‚
*   å°†è¿™äº›æ‹›è˜å¹¿å‘Šæ–‡æœ¬ä½œä¸ºç¨€ç–è®¡æ•°å‘é‡è¿›è¡Œå¤„ç†å’Œå­˜å‚¨ã€‚

ä¸ºäº†å®ç°ä¸Šè¿°ç›®æ ‡ï¼Œæˆ‘ä»¬å°†:

*   æ’é™¤é•¿åº¦å°äº 4 çš„å•è¯
*   ä½¿ç”¨æä¾›çš„åœç”¨è¯åˆ—è¡¨(å³åœç”¨è¯ _en.txt)åˆ é™¤åœç”¨è¯
*   åˆ é™¤åœ¨ä¸€ä¸ªæ‹›è˜å¹¿å‘Šæè¿°ä¸­åªå‡ºç°ä¸€æ¬¡çš„å•è¯ï¼Œå°†å…¶ä¿å­˜(æ— é‡å¤)ä¸ºä¸€ä¸ª`txt`æ–‡ä»¶(å‚è€ƒæ‰€éœ€çš„è¾“å‡º)
*   ä»ç”Ÿæˆçš„è¯æ±‡è¡¨ä¸­æ’é™¤è¿™äº›å•è¯
*   æ‰¾åˆ° 100 å¤šä¸ªå¹¿å‘Š
    æè¿°ä¸­å‡ºç°çš„å¸¸ç”¨è¯ï¼Œä¿å­˜ä¸º`txt`æ–‡ä»¶(å‚è€ƒæ‰€éœ€è¾“å‡º)
*   åœ¨ç”Ÿæˆçš„è¯æ±‡è¡¨ä¸­æ’é™¤å®ƒä»¬

æˆ‘ä»¬ä¸ä¼š:

*   ç”Ÿæˆå¤šè¯çŸ­è¯­(å³æ­é…ï¼Œåè¯çŸ­è¯­)

åœ¨æœ¬ç»ƒä¹ ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å°†è·å¾—ä»¥ä¸‹å‡ é¡¹è¾“å‡ºï¼ŒåŒ…æ‹¬å®ƒä»¬çš„è¦æ±‚:

1ã€‚ `vocab.txt`:åŒ…å«ä»¥ä¸‹æ ¼å¼çš„å•å­—è¯æ±‡:`word_string:integer_index`

*   è¯æ±‡è¡¨ä¸­çš„å•è¯å¿…é¡»æŒ‰å­—æ¯é¡ºåºæ’åˆ—ã€‚è¿™ä¸ªæ–‡ä»¶æ˜¯è§£é‡Šç¨€ç–ç¼–ç çš„å…³é”®ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œå•è¯ abbie æ˜¯è¯æ±‡è¡¨ä¸­çš„ç¬¬ 12 ä¸ªå•è¯(å¯¹åº”çš„ integer_index = 11)(æ³¨æ„ï¼Œä¸‹é¢çš„æ•°å­—å’Œå•è¯ä¸æ˜¯æŒ‡ç¤ºæ€§çš„)ã€‚

![](img/f2987425612b8e4ada501d5e72b8394b.png)

vocab.txt æ–‡ä»¶è¾“å‡ºæ ¼å¼

2ã€‚ `highFreq.txt`è¯¥æ–‡ä»¶åŒ…å«åœ¨ 100 å¤šä¸ªå¹¿å‘Šæè¿°ä¸­å‡ºç°çš„å¸¸ç”¨è¯ã€‚åœ¨è¾“å‡º`txt`æ–‡ä»¶ä¸­ï¼Œæ¯è¡Œåº”è¯¥åªåŒ…å«ä¸€ä¸ªå•è¯ã€‚å•å­—çš„é¡ºåºåŸºäºå®ƒä»¬çš„é¢‘ç‡ï¼Œå³åŒ…å«è¯¥è¯çš„å¹¿å‘Šçš„æ•°é‡ï¼Œä»é«˜åˆ°ä½ã€‚

3ã€‚ `lowFreq.txt`è¯¥æ–‡ä»¶åŒ…å«æŒ‰å­—æ¯é¡ºåºåœ¨ä¸€ä¸ªæ‹›è˜å¹¿å‘Šæè¿°ä¸­åªå‡ºç°ä¸€æ¬¡çš„å•è¯ã€‚åœ¨è¾“å‡ºçš„`txt`æ–‡ä»¶ä¸­ï¼Œæ¯è¡Œåº”è¯¥åŒ…å«ä¸€ä¸ªå•è¯ã€‚

4ã€‚ `sparse.txt`è¯¥æ–‡ä»¶çš„æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå¹¿å‘Šã€‚æ‰€ä»¥ï¼Œä»–ä»¬ä»`advertisement ID`å¼€å§‹ã€‚æ¯è¡Œçš„å…¶ä½™éƒ¨åˆ†æ˜¯ä»¥é€—å·åˆ†éš”çš„`word_index:word_freq`å½¢å¼çš„ç›¸åº”æè¿°çš„ç¨€ç–è¡¨ç¤ºã€‚è¡Œçš„é¡ºåºå¿…é¡»ä¸è¾“å…¥æ–‡ä»¶ä¸­å¹¿å‘Šçš„é¡ºåºç›¸åŒ¹é…ã€‚

**æ³¨:** `word_freq`è¿™é‡ŒæŒ‡çš„æ˜¯ unigram åœ¨ç›¸åº”æè¿°ä¸­çš„å‡ºç°é¢‘ç‡ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ–‡æ¡£ã€‚ä¾‹å¦‚ï¼Œåœ¨å¹¿å‘Š 12612628 çš„æè¿°ä¸­ï¼Œå•è¯ç¼–å· 11(æ ¹æ®ä¸Šé¢çš„ä¾‹å­æ˜¯â€˜abbie â€™)æ°å¥½å‡ºç°ä¸€æ¬¡(ç¼–å·ä¸æ˜¯æŒ‡ç¤ºæ€§çš„) :

![](img/764923683b48a9904acf1302cbb84f5a.png)

sparse.txt æ–‡ä»¶è¾“å‡ºæ ¼å¼

# â›³ï¸è§£å†³æ–¹æ¡ˆ

æ‰€ä»¥æˆ‘ä»¬æ€»æ˜¯ä»å¯¼å…¥æ‰€éœ€çš„åº“å¼€å§‹ã€‚é‰´äºè¿™é¡¹å·¥ä½œçš„æ€§è´¨ï¼Œéœ€è¦åšåˆ°ä»¥ä¸‹å‡ ç‚¹:

## å¯¼å…¥åº“

*   **æ­£åˆ™è¡¨è¾¾å¼**

ç¬¬ä¸€ä¸ªæ˜¯æ­£åˆ™è¡¨è¾¾å¼ï¼Œç®€ç§°ä¸º ReGexã€‚å¦‚æœä½ è¿˜æ²¡æœ‰ç”¨è¿‡å®ƒä»¬ï¼Œæˆ‘å¼ºçƒˆå»ºè®®ä½ æ‹¿èµ·å®ƒï¼Œåšä¸€äº›å¾ˆé…·çš„äº‹æƒ…ã€‚å†å¾€ä¸‹ï¼Œæˆ‘å·²ç»æä¾›äº†ä¸€äº›å¼€å§‹çš„ç»†èŠ‚ã€‚

```
# Regular Expressions (ReGeX)import re
```

*   **è‡ªç„¶è¯­è¨€å·¥å…·åŒ…**

NLTK æ˜¯æ„å»º Python ç¨‹åºæ¥å¤„ç†äººç±»è¯­è¨€æ•°æ®çš„é¢†å…ˆå¹³å°ã€‚å®ƒæä¾›äº†æ˜“äºä½¿ç”¨çš„ç•Œé¢ï¼Œå¦‚ WordNetï¼Œä»¥åŠä¸€å¥—ç”¨äºåˆ†ç±»ã€æ ‡è®°åŒ–ã€è¯å¹²åŒ–ã€æ ‡è®°ã€è§£æå’Œè¯­ä¹‰æ¨ç†çš„æ–‡æœ¬å¤„ç†åº“ï¼Œä»¥åŠå·¥ä¸šçº§ NLP åº“çš„åŒ…è£…å™¨ã€‚

`nltk.probability`æä¾›äº†è¡¨ç¤ºå’Œå¤„ç†æ¦‚ç‡ä¿¡æ¯çš„ç±»ï¼Œæ¯”å¦‚`FreqDist`ï¼Œæˆ‘ä»¬ç¨åä¼šç”¨åˆ°ã€‚

```
# Natural Language Toolkitimport nltkfrom nltk.probability import *from nltk.corpus import stopwords
```

*   **Itertools**

Python `itertools`æ¨¡å—æ˜¯å¤„ç†è¿­ä»£å™¨çš„å·¥å…·é›†åˆã€‚ç®€å•åœ°è¯´ï¼Œè¿­ä»£å™¨æ˜¯å¯ä»¥åœ¨`for`å¾ªç¯ä¸­ä½¿ç”¨çš„æ•°æ®ç±»å‹ã€‚Python ä¸­æœ€å¸¸è§çš„è¿­ä»£å™¨æ˜¯ listã€‚

```
# Functions creating iterators for efficient loopingimport itertoolsfrom itertools import chainfrom itertools import groupby
```

## è®©æˆ‘ä»¬å†™ä¸€äº›ä»£ç ğŸ”¥

æˆ‘ä»¬å…ˆå¯¼å…¥æ•°æ®ã€‚GitHub ä¸Šæœ‰ä¸€ä¸ªåä¸º`data.txt`çš„æ–‡ä»¶ä¾›ä½ å‚è€ƒã€‚æˆ‘æŠŠå®ƒä¿å­˜åœ¨æœ¬åœ°ç”µè„‘ä¸Šï¼Œå’Œæˆ‘çš„ Jupyter ç¬”è®°æœ¬æ–‡ä»¶æ”¾åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹é‡Œã€‚

åœ¨è¯»å–æ–‡ä»¶ä¹‹å‰ï¼Œä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œå¹¶å°†å…¶å‘½åä¸º`data`ã€‚

```
data = []
```

ç„¶åæˆ‘ä»¬ç®€å•çš„è¯»å–`data.txt`å¹¶ä¿å­˜åœ¨åˆ—è¡¨`data`ä¸­ã€‚ç¡®ä¿ä½ å®šä¹‰äº†**ç¼–ç æ ¼å¼** `utf8`ï¼Œå¦åˆ™ä½ å¯èƒ½ä¼šå¾—åˆ°ä¸€ä¸ªé”™è¯¯ã€‚

*   æ ·æœ¬è¯¯å·®:

```
UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 260893: character maps to <undefined>
```

å¦ä¸€ä¸ª**è€ƒè™‘äº‹é¡¹**æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨`.lower()`å‡½æ•°ç›´æ¥å°†æ–‡æœ¬è½¬æ¢æˆ lower ä»¥ä¿æŒä¸€è‡´æ€§ã€‚

```
with open('data.txt', encoding="utf8") as f:
    data = f.read().lower()
```

## æ ¼å¼åŒ–å’Œæ¸…ç†âœ‚ï¸ğŸ”¨ ğŸ“Œ

ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦å¼€å§‹æ ‡è®°æ–‡æœ¬çš„è¿‡ç¨‹ã€‚å°†ä¸€ä¸ªå­—ç¬¦åºåˆ—åˆ†æˆå‡ ä¸ªéƒ¨åˆ†çš„ä»»åŠ¡ç§°ä¸ºæ ‡è®°åŒ–ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»ç§»é™¤æ–‡æœ¬ä¸­çš„æ‰€æœ‰å™ªéŸ³ï¼Œæ¯”å¦‚/-*#@æˆ–ä»»ä½•å…¶ä»–éå•è¯å­—ç¬¦æˆ–å¤šä½™çš„ç©ºæ ¼ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºå¤§çš„`ReGex`å·¥å…·æ¥å®Œæˆã€‚

![](img/8ab2fa98859c724ca5cb350eee10ac93.png)

ä¸ºäº†ä½¿ç”¨ ReGex è¿è¡Œæ ¼å¼åŒ–ï¼Œéœ€è¦é‡‡å–ä¸¤ä¸ªæ­¥éª¤ï¼›

(1)åˆ›å»ºæ¨¡å¼ï¼Œ

(2)ä½¿ç”¨ Python ä»£ç è¿è¡Œæ¨¡å¼å¹¶æ‰¾åˆ°åŒ¹é…ã€‚

```
# (1) create a pattern for REGEX to find and keep matching words onlypattern = re.compile(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)?")# (2)tokenise the words: match the pattern to file's content 
# and tokenize the contenttokenised = pattern.findall(data)
```

![](img/467ab59e3a79f2cb8d4cc962944e379f.png)

å›¾åƒ[æ¥æº](https://www.google.com/url?sa=i&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwjf4MHO4YTnAhWLyDgGHW9YDI0QjRx6BAgBEAQ&url=http%3A%2F%2Fnlp.cs.tamu.edu%2F&psig=AOvVaw0kkE5JmXeahMHoc6Uvi0S9&ust=1579148397647336)

Regex ä¸Šæœ‰å¾ˆå¤šåœ¨çº¿èµ„æºï¼Œä½†æˆ‘å‘ç°æœ€æœ‰è¶£çš„æ˜¯ https://regex101.com/ã€‚å®ƒä¸ä»…å¯ä»¥å¸®åŠ©æ‚¨å°†æ–‡æœ¬ä¸æ¨¡å¼ç›¸åŒ¹é…ï¼Œè¿˜å¯ä»¥æä¾›ç®€çŸ­è€Œæœ‰ä»·å€¼çš„å†…å®¹ã€‚åœ¨å›¾ 1 ä¸­ï¼Œæˆ‘åœ¨ä»–ä»¬çš„é¡µé¢ä¸Šæä¾›äº†ä¸€ä¸ªç®€å•çš„åŠŸèƒ½åˆ—è¡¨ã€‚

![](img/b2a25f3088565f0816481c2465d7ee1b.png)

å›¾ 1ï¼Œ`[regex101](https://regex101.com/).com`æä¾›çš„åŠŸèƒ½

**å¯¹ Python æœ‰ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼èµ„æº:**

*   [Python æ­£åˆ™è¡¨è¾¾å¼æ•°æ®ç§‘å­¦æ•™ç¨‹](https://www.dataquest.io/blog/regular-expressions-data-scientists/)
*   [Python 3 re æ¨¡å—æ–‡æ¡£](https://docs.python.org/3/library/re.html)
*   [åœ¨çº¿æ­£åˆ™è¡¨è¾¾å¼æµ‹è¯•å™¨å’Œè°ƒè¯•å™¨](https://regex101.com/)

## **ç´¢å¼•æ ‡è®°åŒ–åˆ—è¡¨ğŸ“‡**

ç°åœ¨ï¼Œæˆ‘å·²ç»æ ¹æ®æ¯ä¸ªæ‹›è˜å¹¿å‘Šä¸­çš„`id`å’Œ`title`å¯¹ä»¤ç‰Œè¿›è¡Œäº†ç´¢å¼•:

```
# pass the length of the 'tokenised' series into a variabletokenised_len = len(tokenised) # indexing the tokens based on the position of "id" and "title"indexes = [i for i, v in enumerate(tokenised) if v=='id' and i+1 < tokenised_len and tokenised[i+1]=='title']
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»`**itertools**` [recipes](https://docs.python.org/3/library/itertools.html) åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°éå†ä»¤ç‰Œåˆ—è¡¨ï¼Œå¹¶ä¸”**åˆ›å»ºä¸€ä¸ªå­åˆ—è¡¨ï¼Œä»¥åŒ…æ‹¬ä»…ä¸ä¸€ä¸ªä½œä¸šå¹¿å‘Šç›¸å…³çš„ä»¤ç‰Œ**ã€‚è¾“å‡ºå°†æ˜¯ä¸€ä¸ªæ•°æ®å­—å…¸ã€‚

```
# from itertools recipes
def pairwise(iterable, fillvalue=None):
    """
       This function iterates through the list of tokens and 
       creates sub list to include tokens related to one job ad only
    """
    a, b = iter(iterable), iter(iterable)
    next(b, None)
    return itertools.zip_longest(a, b, fillvalue=fillvalue) # pairwise based on indexes in the last block and store in the 'tokenised' as a listtokenised = [tokenised[i:j] for i,j in pairwise(indexes)]
```

ä¸ºäº†åˆ›å»ºæ•°æ®å­—å…¸ï¼Œæˆ‘ä½¿ç”¨äº† Python `**itertools**`ã€‚ **Jason Rigdel** å¯¹ Python ä¸­çš„`**itertools**` è¿™ä¸€è¯é¢˜åšäº†å¾ˆå¥½çš„è§£é‡Šï¼Œå¹¶æä¾›äº†ä¸€ç»„ä¾‹å­ã€‚

*   [Python ITER tools æŒ‡å—](/@jasonrigden/a-guide-to-python-itertools-82e5a306cdf8)

ä½†æ˜¯ï¼Œè¿™ä¸ªåˆ—è¡¨åŒ…å«äº†å¾ˆå¤šåŠŸèƒ½è¯ï¼Œæ¯”å¦‚â€œtoâ€ã€â€œinâ€ã€â€œtheâ€ã€â€œisâ€ç­‰ç­‰ã€‚

> è¿™äº›åŠŸèƒ½è¯é€šå¸¸*å¯¹æ–‡æœ¬çš„è¯­ä¹‰æ²¡æœ‰å¤ªå¤§è´¡çŒ®*ï¼Œé™¤äº†åœ¨æ–‡æœ¬åˆ†æä¸­å¢åŠ æ•°æ®çš„ç»´åº¦ã€‚
> 
> å¦å¤–ï¼Œè¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„ç›®æ ‡é€šå¸¸æ˜¯å»ºç«‹ä¸€ä¸ªé¢„æµ‹åˆ†ç±»æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹æŠ¥å‘Šçš„å«ä¹‰æ¯”å¯¹è¯­æ³•æ›´æ„Ÿå…´è¶£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©åˆ é™¤é‚£äº›å•è¯ï¼Œè¿™æ˜¯ä½ çš„ä¸‹ä¸€ä¸ªä»»åŠ¡ã€‚

æˆ‘å°†é€šè¿‡ä¿ç•™é‚£äº›åŒ…å« 3 ä¸ªä»¥ä¸Šå­—ç¬¦çš„æ ‡è®°æ¥æ’é™¤æ‰€æœ‰å°‘äº 4 ä¸ªå­—ç¬¦çš„æ ‡è®°ï¼Œå¹¶å°†å…¶ä½™çš„æ ‡è®°æ·»åŠ åˆ°ä¸€ä¸ªåä¸º`to_remove`çš„åˆ—è¡¨ä¸­ã€‚è¿™ä¸ªåˆ—è¡¨å°†è¢«æ·»åŠ åˆ°é€šç”¨è‹±è¯­`stopwords`åˆ—è¡¨ä¸­ã€‚

```
tokenised = [[word if len(word) > 3 else "to_remove" for word in job] for job in tokenised]
```

## åˆ é™¤åœç”¨è¯âœ‚ï¸

åœç”¨è¯æºå¸¦*å°‘é‡è¯æ±‡å†…å®¹*ã€‚

> å®ƒä»¬ç»å¸¸æ˜¯è‹±è¯­ä¸­çš„åŠŸèƒ½è¯ï¼Œä¾‹å¦‚ï¼Œå† è¯ã€ä»£è¯ã€åŠ©è¯ç­‰ç­‰ã€‚åœ¨ NLP å’Œ IR ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä»è¯æ±‡è¡¨ä¸­æ’é™¤åœç”¨è¯ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å°†é¢ä¸´[ç»´åº¦è¯…å’’](https://towardsdatascience.com/the-curse-of-dimensionality-f07c66128fe1)ã€‚

ä¹Ÿæœ‰ä¸€äº›ä¾‹å¤–ï¼Œæ¯”å¦‚å¥æ³•åˆ†æåƒè§£æï¼Œæˆ‘ä»¬é€‰æ‹©ä¿ç•™é‚£äº›åŠŸèƒ½è¯ã€‚ä½†æ˜¯ï¼Œæ‚¨å°†é€šè¿‡ä½¿ç”¨ **NLTK** ä¸­çš„åœç”¨è¯åˆ—è¡¨æ¥åˆ é™¤ä¸Šé¢åˆ—è¡¨ä¸­çš„æ‰€æœ‰åœç”¨è¯ï¼Œå®ƒæ˜¯:

```
nltk.download('stopwords')stopwords_list = stopwords.words('english')
```

å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘å·²ç»åœ¨æˆ‘çš„ GitHub ä¸Šæä¾›äº†`stopwords_en.txt`æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥ä»é‚£é‡Œä¸‹è½½ã€‚æˆ‘ä»¬é¦–å…ˆå°†ä¸Šé¢åˆ›å»ºçš„`to_remove`åˆ—è¡¨æ·»åŠ åˆ°`stopwords_en.txt`æ–‡ä»¶ä¸­ï¼Œè¯»å–è¯¥æ–‡ä»¶ï¼Œç„¶åå°†å®ƒä»¬ä¿å­˜ä¸º`set()`ã€‚

```
# adding'to_removed' string to the list of stopwordsstopwords = []with open('stopwords_en.txt',"a") as f:
    f.write("\nto_remove") #\n to shift to next line with open('stopwords_en.txt') as f:
    stopwords = f.read().splitlines() #reading stopwords line and create stopwords as a list# convert stopwords into setstopwordsset = set(stopwords)
```

ä½ å¯èƒ½æƒ³çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬æŠŠ`stopwords`ä¿å­˜ä¸º`set`ã€‚è¿™æ˜¯ä¸ªå¥½é—®é¢˜â€¦â€¦Python`set`æ¯”`list`æ›´å¥½ï¼Œå› ä¸º`set`åœ¨æœç´¢å¤§é‡ ***å¯æ•£åˆ—*** é¡¹ç›®æ–¹é¢æ¯”åˆ—è¡¨è¿è¡Œå¾—å¿«å¾—å¤šã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªåä¸º`purifier()`çš„å‡½æ•°ï¼Œå®ƒé€šè¿‡ç§»é™¤`stopwords`æ¥å‡€åŒ–ä»¤ç‰Œï¼Œç„¶åè¿è¡Œ`tokenised`åˆ—è¡¨ã€‚

```
def purifier(tokenList,remove_token):
    """
        This function takes two input (list of current tokens 
        and list of tokens to be removed)
        The function converts the list into set to improve the 
        performance
        and return a list of sets each of which include purified 
        tokens and remove_token lists are removed
    """
    return [set(word for word in job if word not in remove_token) for job in tokenList]# running the 'purifier' functiontokenised = purifier(tokenised,stopwordsset)
```

![](img/fa8424bd4e4259c95abd7207204e6a97.png)

ç…§ç‰‡æ¥è‡ª shutterstock å›¾ä¹¦é¦†

æ¥ä¸‹æ¥æ˜¯åˆ é™¤åœ¨ä¸€ä¸ªæ‹›è˜å¹¿å‘Šæè¿°ä¸­åªå‡ºç°ä¸€æ¬¡çš„`words`ï¼Œå°†å®ƒä»¬(æ— é‡å¤)ä¿å­˜ä¸º txt æ–‡ä»¶(å‚è€ƒæ‰€éœ€è¾“å‡º)ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦ä»ç”Ÿæˆçš„è¯æ±‡è¡¨ä¸­æ’é™¤è¿™äº›å•è¯ã€‚

ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨`**chain()**`åŠŸèƒ½å°†æ‰€æœ‰æ‹›è˜å¹¿å‘Šä¸­çš„æ‰€æœ‰å•è¯åˆ—æˆä¸€ä¸ªåˆ—è¡¨ã€‚åœ¨â€œ[Python ITER tools](/@jasonrigden/a-guide-to-python-itertools-82e5a306cdf8)æŒ‡å—â€ä¸­ï¼Œæœ‰ä¸€ä¸ªå…³äº`chain()`å‡½æ•°å¦‚ä½•å·¥ä½œçš„å¾ˆå¥½çš„è§£é‡Šã€‚

```
stop_wrds_removed_words = list(chain.from_iterable([word for word in job] for job in tokenised))
```

å°†å•è¯åˆ—è¡¨è½¬æ¢ä¸ºé›†åˆä»¥åˆ é™¤é‡å¤é¡¹å¹¶åˆ›å»ºè¯æ±‡é›†åˆ

```
stop_wrds_removed_vocab = set(stop_wrds_removed_words)
```

æ¥ä¸‹æ¥æ˜¯é€šè¿‡`**FreqDisrt()**`å‡½æ•°ä¸­çš„å•è¯æ¥ç»Ÿè®¡ä»¤ç‰Œçš„æ•°é‡ã€‚

> FreqDist ç±»ç”¨äºå¯¹â€œé¢‘ç‡åˆ†å¸ƒâ€è¿›è¡Œç¼–ç ï¼Œå®ƒè®¡ç®—å®éªŒçš„æ¯ä¸ªç»“æœå‡ºç°çš„æ¬¡æ•°ã€‚å®ƒæ˜¯`nltk.probability`æ¨¡å—ä¸‹çš„ä¸€ä¸ªç±»ã€‚
> 
> æ ¹æ® [developedia](https://devopedia.org/text-corpus-for-nlp) çš„è¯´æ³•ï¼Œé€šå¸¸ï¼Œæ¯ä¸ªæ–‡æœ¬è¯­æ–™åº“éƒ½æ˜¯æ–‡æœ¬æºçš„é›†åˆã€‚å¯¹äºå„ç§ NLP ä»»åŠ¡ï¼Œæœ‰å‡ åä¸ªè¿™æ ·çš„è¯­æ–™åº“ã€‚æœ¬æ–‡å¿½ç•¥è¯­éŸ³è¯­æ–™åº“ï¼Œåªè€ƒè™‘æ–‡æœ¬å½¢å¼çš„è¯­æ–™åº“ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæ–‡æœ¬è¯­æ–™åº“æŒ‡çš„æ˜¯æ‰€æœ‰å·¥ä½œå¹¿å‘Šçš„ç»„åˆ(â€¦è€Œä¸æ˜¯æ¯ä¸ªå·¥ä½œå•ç‹¬)ã€‚

ä¸‹é¢çš„ line å‡½æ•°è®¡ç®—ä¸€ä¸ªå•è¯åœ¨æ•´ä¸ª`corpus`ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œè€Œä¸ç®¡å®ƒåœ¨å“ªä¸ª ad ä¸­ã€‚

```
fd = FreqDist(stop_wrds_removed_words)
```

## ä½é¢‘ä»¤ç‰Œ

ä¸ºäº†æ‰¾åˆ°ä¸å¤ªé¢‘ç¹çš„ä»¤ç‰Œï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªåªå‡ºç°è¿‡ä¸€æ¬¡çš„ä»¤ç‰Œåˆ—è¡¨ï¼Œå¹¶å°†è¯¥åˆ—è¡¨è½¬æ¢ä¸º`set`ä»¥æé«˜æ€§èƒ½ã€‚

```
once_only = set([k for k, v in fd.items() if v == 1])# sort the set into alphabetical orderonce_only = sorted(once_only) set(once_only)
```

ä¸ºäº†åˆ›å»º`lowFreq.txt`æ–‡ä»¶ï¼Œæˆ‘å·²ç»å°†åœ¨ä¸€ä¸ªæ‹›è˜å¹¿å‘Šæè¿°ä¸­å‡ºç°â€œä»…ä¸€æ¬¡â€çš„å•è¯çš„æ’åº`set`ä¿å­˜åˆ°ä¸€ä¸ªåŒåæ–‡ä»¶ä¸­ã€‚

```
out_file = open("lowFreq.txt", 'w')
for d in once_only:
    out_file.write(''.join(d) + '\n')
out_file.close()
```

![](img/d4ce25e2dfa65ae46764a298fa807449.png)

## é«˜é¢‘ä»¤ç‰Œ

åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘é‡å¤ä¸Šé¢ç›¸åŒçš„æ­¥éª¤ï¼Œä½†æ˜¯ï¼Œè¿™ä¸€æ¬¡çš„ç›®çš„æ˜¯æ‰¾åˆ°é«˜é¢‘è¯å¹¶æŠŠå®ƒä»¬ä¿å­˜åœ¨åä¸º`highFreq.txt`çš„æ–‡ä»¶ä¸­ã€‚
æˆ‘é¦–å…ˆé€šè¿‡è¿è¡Œæˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„`purifier()`å‡½æ•°ï¼Œä»ä»¤ç‰Œçš„`list`ä¸­ç§»é™¤`lowFreq`ä»¤ç‰Œã€‚

```
tokenised = purifier(tokenised,once_only)
```

ä¸‹ä¸€æ­¥æ˜¯åœ¨ç§»é™¤äº†`once_only`ä¸ªå•è¯ä¹‹ååˆ›å»ºä¸€ä¸ªæ–°çš„`list`ä¸ªå•è¯ã€‚

```
LowFreqRemoved_Words = list(chain.from_iterable([word for word in job] for job in tokenised)) LowFreqRemoved_vocab = set(LowFreqRemoved_Words) LowFreqRemoved_fd = FreqDist(LowFreqRemoved_Words)
```

å¯¹äºé«˜é¢‘è¯ï¼Œæˆ‘é€‰æ‹©äº† 100 ä¸ªé˜ˆå€¼ã€‚ä½ å¯ä»¥æ ¹æ®ä½ çš„å·¥ä½œç¯å¢ƒé€‰æ‹©ä»»ä½•é—¨æ§›ã€‚

```
highFreq = set([k for k, v in LowFreqRemoved_fd.items() if v > 100])
```

ç°åœ¨ï¼Œå°†åœ¨ 100 å¤šä¸ªæ‹›è˜å¹¿å‘Šæè¿°ä¸­å‡ºç°çš„é«˜é¢‘è¯çš„æ’åºåˆ—è¡¨ä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ã€‚

```
out_file = open("highFreq.txt", 'w')
for d in highFreq:
    out_file.write(''.join(d) + '\n')
out_file.close()
```

![](img/174bc5c558a8ce9268166aa2f6d7e370.png)

æˆ‘ä»¬å†æ¬¡è¿è¡Œ`purifier()`å‡½æ•°æ¥åˆ é™¤`highFreq`æ•°æ®é›†å¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„`list`ã€‚

```
tokenised = purifier(tokenised,highFreq) HighFreqRemoved_words = list(chain.from_iterable([word for word in job] for job in tokenised))HighFreqRemoved_vocab = set(HighFreqRemoved_words)
```

## æ³¨æ„

ä½ å¯èƒ½æƒ³çŸ¥é“åœ¨æˆ‘çš„ä»£ç ä¸­`words`å’Œ`vocab`åˆ—è¡¨æœ‰ä»€ä¹ˆä¸åŒï¼Œä¸ºä»€ä¹ˆæ¯æ¬¡æˆ‘åˆ›å»ºä¸€ä¸ª`words`åˆ—è¡¨ï¼Œç„¶åä¸€ä¸ª`vocab`ä¹Ÿè¢«åˆ›å»ºã€‚åŸå› è¿˜è¦è¿½æº¯åˆ° Python ä¸­`list`å’Œ`set`çš„åŒºåˆ«ã€‚ç„¶è€Œåº•çº¿æ˜¯åœ¨`vocab`ä¸­æ¯ä¸ªå•è¯åªè¢«åˆ—å‡ºä¸€æ¬¡ï¼Œè€Œ`words`å¯èƒ½æœ‰é‡å¤ã€‚

æ¥ä¸‹æ¥æ˜¯ä¸€ä¸ªç®€å•çš„æ£€æŸ¥ç‚¹ï¼Œç”¨äºæŸ¥çœ‹ä»¤ç‰Œçš„æçº¯è¿›åº¦:

```
print(f"Length of words: {len(stop_wrds_removed_words)}")print(f"Length of vocab: {len(stop_wrds_removed_vocab)}")print(f"Length of LowFreqRemoved_Words: {len(LowFreqRemoved_Words)}")print(f"Length of LowFreqRemoved_vocab: {len(LowFreqRemoved_vocab)}")print(f"Length of HighFreqRemoved_words: {len(HighFreqRemoved_words)}")print(f"Length of HighFreqRemoved_vocab: {len(HighFreqRemoved_vocab)}")
```

â€¦è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹è¾“å‡º:

```
Length of words: 474345
Length of vocab: 18619
Length of LowFreqRemoved_Words: 465779
Length of LowFreqRemoved_vocab: 10053
Length of HighFreqRemoved_words: 126491
Length of HighFreqRemoved_vocab: 9103
```

æ¥ä¸‹æ¥æ˜¯åˆ›å»ºä¸€ä¸ªåä¸º`vocab.txt`çš„æ‰€æœ‰è¯æ±‡çš„æ–‡ä»¶ã€‚

```
HighFreqRemoved_vocab = list(HighFreqRemoved_vocab) # list of final vocabsvocab = {HighFreqRemoved_vocab[i]:i for i in range(0,len(HighFreqRemoved_vocab))}
```

æ„å»ºä¸€ä¸ªå‡½æ•°æ¥åˆ›å»º`vocab.txt`æ–‡ä»¶ï¼Œæœ€åé€šè¿‡è°ƒç”¨ä»¥ä¸‹å‡½æ•°æ¥æ„å»ºå’Œæ’åºè¯¥æ–‡ä»¶:

```
def vaocab_output(file):
    with open (file, "a") as f:
        for key in sorted(vocab.keys()):
            f.write("%s:%s\n" % (key, vocab[key]))# calling the function to build the filevaocab_output("vocab.txt")
```

`vocab.txt`è¾“å‡º:

![](img/10261f935ac2fbfe2f500c1a974276b9.png)

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå‡ºäºç»ƒä¹ çš„ç›®çš„ï¼Œæˆ‘å°½é‡ä¿æŒä»£ç ç®€å•ã€‚ç„¶è€Œï¼Œå¯¹äºè¿™ä¸€æ­¥ï¼Œæˆ‘åˆ›å»ºäº†ä¸€æ®µæ›´å¤æ‚ä½†æ›´æœ‰æ•ˆçš„ä»£ç ã€‚è¯·åœ¨è¯„è®ºä¸­ç•™ä¸‹ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¿è¯ä»–ä»¬ä¼šå¾—åˆ°å›ç­”ã€‚

æœ€åçš„æ´»åŠ¨æ˜¯ä»¥é€—å·åˆ†éš”çš„`word_index:word_freq`çš„å½¢å¼ç¨€ç–è¡¨ç¤ºç›¸åº”çš„æè¿°å¹¶åˆ›å»ºæ–‡ä»¶`sparse.txt`ã€‚

```
data = {}
id = None
with open('data.txt', 'r',encoding="utf8") as f:
    for i, line in enumerate(f): # create the iteration in the range of imported file's length
        line = line.lower() 
        line = line.strip()
        if not line:
            continue
        section = line.split(':')[0] # define 'section' as a method to manupilate each line based on how the line begins
        content = ':'.join(line.split(':')[1:]).strip() # define 'content' a method to capture tokens
        if section == 'id': # id section:
            if id: # Error handle if theres some bad formatting: multiple ids
                raise ValueError('unable to parse file at line %d, multiple ids' % i)
            id = content[1:] # capture the job id
            if id in data.keys():# Error handle if theres some bad formatting: duplicates
                raise ValueError('unable to parse file at line %d, duplicate id' % i)
        elif section == 'description': #capture job description per each job ad
            if not id:# Error handle if theres some bad formatting: missing id
                raise ValueError('unable to parse file at line %d, missing id' % i)
            content = pattern.findall(line)
            content = [value for value in content if len(value) > 3] # remove short character token
            content = [value for value in content if value not in stopwordsset] # remove stopwords
            content = [value for value in content if value not in once_only] # remove lowFreq token
            content = [value for value in content if value not in highFreq] # remove highFreq tokens
            data[id] = content # creates data dictionary
            id = None
        elif section == 'title': # if the line start with 'title' do nothing
            continue
        else:
            raise ValueError('unable to parse file at line %d, unexpected section name' % i)
```

æœ€åæ„å»º`sparse.txt`æ–‡ä»¶ã€‚

```
with open('sparse.txt',"w") as f:
    for jobID,content in data.items(): # go through data dictionary created in the last block
        fd_parse = FreqDist(content) # count number of times each token occured in the same job ad
        tmp = "" # create a placeholder for word_index:word_freq
        for (x,y) in fd_parse.items(): # iterate through each frequencies
            tmp += f"{vocab[x]}:{y}," # build the dictionary of word_index:word_freq in the placeholder
        f.write(f"#{jobID},{tmp[:-1]}\n") # write in the file line by line
```

`sparse.txt`çš„è¾“å‡º:

![](img/ff7c05bdc84a1a0045b5fb1c98c0d7ba.png)

è¿›ä¸€æ­¥äº†è§£

*   [è®¡æ•°çŸ¢é‡å™¨ï¼Œtfidf çŸ¢é‡å™¨ï¼Œé¢„æµ‹ Kaggle ä¸Šçš„è¯„è®ºæ•™ç¨‹](https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments)
*   [å°†ä¸€ç»„æ–‡æœ¬æ–‡æ¡£è½¬æ¢æˆä¸€ä¸ªä»¤ç‰Œè®¡æ•°çŸ©é˜µ](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer)

â€”â€”ç»“æŸâ€”