<html>
<head>
<title>Tutorial on Text Classification (NLP) using ULMFiT and fastai Library in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python中的ULMFiT和fastai库进行文本分类(NLP)的教程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tutorial-on-text-classification-nlp-using-ulmfit-and-fastai-library-in-python-2f15a2aac065?source=collection_archive---------0-----------------------#2018-11-29">https://medium.com/analytics-vidhya/tutorial-on-text-classification-nlp-using-ulmfit-and-fastai-library-in-python-2f15a2aac065?source=collection_archive---------0-----------------------#2018-11-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3a56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自然语言处理(NLP)在当今世界无需介绍。这是最重要的学习和研究领域之一，在过去的十年里，人们对它的兴趣显著增加。NLP的基础知识广为人知，易于掌握。但是当文本数据变得巨大且无结构时，事情就变得棘手了。</p><p id="41a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是深度学习变得如此关键的地方。是的，我说的是针对NLP任务的深度学习——一条相对较少有人涉足的道路。DL已经证明了它在图像检测、分类和分割等计算机视觉任务中的有效性，但是像文本生成和分类这样的NLP应用长期以来一直被认为适合传统的ML技术。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/3a47a314e4ac019414733836cf053154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uT-xA2OiQHHknSAS.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:Tryolabs</figcaption></figure><p id="741c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习无疑对NLP产生了非常积极的影响，正如你将在本文中看到的那样。我们将关注迁移学习的概念，以及我们如何在NLP中利用它，使用流行的fastai库来构建极其精确的模型。在这个过程中，我还将向您介绍ULMFiT框架。</p><p id="413d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注- </strong> <em class="jt">本文假设基本熟悉神经网络、深度学习</em>和<em class="jt">迁移学习。如果你是深度学习的新手，我强烈推荐你先阅读以下文章:</em></p><p id="fffb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你是NLP的初学者，看看这个包含3个真实项目的<a class="ae ju" href="https://trainings.analyticsvidhya.com/courses/course-v1:AnalyticsVidhya+NLP101+2018_T1/about" rel="noopener ugc nofollow" target="_blank">视频课程</a>。</p><h1 id="7b44" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">目录</h1><ol class=""><li id="4c81" class="kt ku hi ih b ii kv im kw iq kx iu ky iy kz jc la lb lc ld bi translated">迁移学习的优势</li><li id="e082" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">NLP中预先训练的模型</li><li id="23a5" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">ULMFiT概述</li><li id="b5e3" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">理解问题陈述</li><li id="f836" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">系统设置:Google Colab</li><li id="fa06" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">用Python实现</li><li id="036e" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">下一步是什么？</li></ol><h1 id="10e3" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">迁移学习的优势</h1><p id="2578" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">我在简介里称赞了深度学习，也当之无愧。然而，任何事情都是有代价的，深度学习也不例外。深度学习的最大挑战是训练模型的海量数据需求。很难找到如此大规模的数据集，而且准备这样的数据集成本太高。对于大多数组织来说，想出它们是不可能的。</p><p id="84aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个障碍是运行高级深度学习算法所需的GPU成本高。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lm"><img src="../Images/eb28c197a7b40181a521fbca590de5e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Tos9T9dkBtIXcEDp.jpg"/></div></div></figure><p id="f9cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">幸运的是，我们可以使用预先训练的最先进的深度学习模型，并调整它们为我们服务。这就是所谓的迁移学习。它不像从零开始训练深度学习模型那样需要大量资源，即使在少量训练数据上也能产生不错的结果。当我们在一个相当小的数据集上实现我们的学习时，这个概念将在本文后面扩展。</p><h1 id="9d8a" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">NLP中预先训练的模型</h1><p id="55e2" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">预训练模型通过提供他们可以利用的现有框架，帮助数据科学家开始处理新问题。你不必总是从零开始建立一个模型，尤其是当别人已经投入了他们的辛勤工作和努力的时候！这些预训练模型在计算机视觉领域被证明是真正有效和有用的(<a class="ae ju" href="https://www.analyticsvidhya.com/blog/2018/07/top-10-pretrained-models-get-started-deep-learning-part-1-computer-vision/" rel="noopener ugc nofollow" target="_blank">查看本文</a>查看我们挑选的CV中前10名预训练模型)。</p><p id="031c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们的成功普遍归功于<a class="ae ju" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> Imagenet数据集</a>。它有超过1400万个带标签的图像，超过100万个图像还伴随着边界框。该数据集于2009年首次发布，此后成为最受欢迎的图像数据集之一。它导致了计算机视觉深度学习研究的几项突破，迁移学习就是其中之一。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/2793a1edeaae9ec46b93a4d5f8311ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*_KacPmm0vtkZ-qK7.png"/></div></figure><p id="72cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，在NLP中，迁移学习并不成功(至少与计算机视觉相比)。当然，我们有预先训练的单词嵌入，如word2vec、GloVe和fastText，但它们主要用于初始化神经网络的第一层。模型的其余部分仍然需要从头开始训练，并且需要大量的例子来产生良好的性能。</p><p id="7206" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，我们真正需要的是什么？像前面提到的计算机视觉模型一样，我们需要一个用于NLP的预训练模型，它可以进行微调并用于不同的文本数据集。预训练自然语言模型的竞争者之一是通用语言模型文本分类微调，或ULMFiT ( <a class="ae ju" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank"> Imagenet数据集</a>【cs。CL])。</p><p id="7379" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是如何工作的？它的应用有多广泛？如何才能让它在Python中发挥作用？在本文的其余部分，我们将通过解决一个文本分类问题来测试ULMFiT，并检查它的性能。</p><h1 id="7439" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">ULMFiT概述</h1><p id="55d0" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">由fast.ai的<a class="lo lp ge" href="https://medium.com/u/34ab754f8c5e?source=post_page-----2f15a2aac065--------------------------------" rel="noopener" target="_blank">杰瑞米·霍华德</a>和NUI Galway Insight Center的<a class="lo lp ge" href="https://medium.com/u/e3999e445181?source=post_page-----2f15a2aac065--------------------------------" rel="noopener" target="_blank"> Sebastian Ruder </a>提出的ULMFiT本质上是一种支持任何NLP任务的迁移学习并取得良好结果的方法。所有这些，都不需要从零开始训练模型。这引起了你的注意，不是吗？</p><p id="f8f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ULMFiT使用以下新技术实现了最先进的结果:</p><ul class=""><li id="be69" class="kt ku hi ih b ii ij im in iq lq iu lr iy ls jc lt lb lc ld bi translated">区别微调</li><li id="27b4" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc lt lb lc ld bi translated">倾斜三角形学习率，以及</li><li id="aef1" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc lt lb lc ld bi translated">逐步解冻</li></ul><p id="db2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该方法包括将在<a class="ae ju" href="https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset" rel="noopener ugc nofollow" target="_blank"> Wikitext 103数据集</a>上训练的预训练语言模型(LM)微调到新的数据集，以使其不会忘记之前学习的内容。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/b47fa78ad444103e64685cb5a86a8cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*FMSCQaVIhAYhRxuz.png"/></div></figure><p id="dc26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">语言建模可以被认为是自然语言处理的Imagenet的对应物。它捕捉语言的一般属性，并提供大量的数据，这些数据可以提供给其他下游的NLP任务。这就是为什么语言建模被选为ULMFiT的源任务。</p><p id="0814" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我强烈建议你通读乌尔姆菲特<a class="ae ju" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">的原始论文</a>，了解更多关于它的工作原理，杰瑞米和塞巴斯蒂安推导它的方式，并解析其他有趣的细节。</p><h1 id="6dcd" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">问题陈述</h1><p id="c4db" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">好了，足够的理论概念—让我们通过在数据集上实现ULMFiT来尝试一下，看看宣传是怎么回事。</p><p id="15e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的目标是微调预训练模型，并将其用于新数据集上的文本分类。我们将在此过程中实施ULMFiT。有趣的是，这个新数据非常小(&lt;1000 labeled instances). A neural network model trained from scratch would overfit on such a small dataset. Hence, I would like to see whether ULMFiT does a great job at this task as promised in the paper.</p><p id="92f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据集:</strong>我们将使用<a class="ae ju" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups" rel="noopener ugc nofollow" target="_blank"> sklearn.datasets </a>中的20个新闻组数据集。顾名思义，它包括来自20个不同新闻组的文本文档。</p><h1 id="39bd" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">系统设置:Google Colab</h1><p id="398a" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">我们将在Google <a class="ae ju" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> Colab </a>而不是本地机器上执行python实现。如果你以前从未在colab上工作过，那么就把这当作一个奖励吧！Colab，或Google Colaboratory，是一个运行Python的免费云服务。它最好的一点是它免费提供GPU和TPU，因此，它对于训练深度学习模型非常方便。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/5d862bc44d97763278e8105af6b0ecee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/0*Qnv9pAv4EnuT5EwF.png"/></div></figure><h2 id="4e3d" class="lw jw hi bd jx lx ly lz kb ma mb mc kf iq md me kj iu mf mg kn iy mh mi kr mj bi translated">Colab的一些主要优势:</h2><ul class=""><li id="be15" class="kt ku hi ih b ii kv im kw iq kx iu ky iy kz jc lt lb lc ld bi translated">完全免费</li><li id="eee9" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc lt lb lc ld bi translated">配备了相当不错的硬件配置</li><li id="23e1" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc lt lb lc ld bi translated">连接到您的Google Drive</li><li id="23a0" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc lt lb lc ld bi translated">非常好地与Github集成</li><li id="dd20" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc lt lb lc ld bi translated">在使用它的过程中，您会发现更多的特性..</li></ul><p id="9807" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，即使您的系统硬件规格非常普通也没关系，只要您有稳定的互联网连接，您就可以使用了。唯一的其他要求是，你必须有一个谷歌帐户。我们开始吧！</p><h1 id="fa68" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">用Python实现</h1><p id="c92d" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">首先，登录你的谷歌账户。然后选择“新PYTHON 3笔记本”。这个笔记本类似于你典型的Jupyter笔记本，所以如果你熟悉Jupyter环境的话，在上面工作不会有太大的麻烦。Colab笔记本看起来像下面的截图:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mk"><img src="../Images/d21785c04782b29b7a17107115ef558e.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/0*7Ag3XngYWUKYKaio.png"/></div></figure><p id="c613" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后进入<code class="du ml mm mn mo b">Runtime</code>，选择<code class="du ml mm mn mo b">Change runtime type</code>，然后选择<code class="du ml mm mn mo b">GPU</code>作为硬件加速器，免费使用GPU。</p><h1 id="91a8" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">导入所需的库</h1><p id="7183" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">大多数流行的库，如<em class="jt">熊猫</em>、<em class="jt"> numpy </em>、<em class="jt"> matplotlib </em>、<em class="jt"> nltk </em>和<em class="jt"> keras </em>，都预装了Colab。然而，PyTorch和fastai v1这两个库(我们在本练习中需要)需要手动安装。因此，让我们将它们加载到我们的Colab环境中:</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="1587" class="lw jw hi mo b fi mt mu l mv mw">!pip install torch_nightly -f <a class="ae ju" href="https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html" rel="noopener ugc nofollow" target="_blank">https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html</a> </span><span id="2ca9" class="lw jw hi mo b fi mx mu l mv mw">!pip install fastai</span><span id="4e9a" class="lw jw hi mo b fi mx mu l mv mw"># import libraries <br/>import fastai <br/>from fastai import * <br/>from fastai.text import * <br/>import pandas as pd <br/>import numpy as np <br/>from functools import partial <br/>import io <br/>import os</span></pre><p id="7de1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">导入我们之前下载的数据集。</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="afe0" class="lw jw hi mo b fi mt mu l mv mw">from sklearn.datasets import fetch_20newsgroups </span><span id="4bf4" class="lw jw hi mo b fi mx mu l mv mw">dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove= <br/>                             ('headers', 'footers', 'quotes'))</span><span id="e666" class="lw jw hi mo b fi mx mu l mv mw">documents = dataset.data</span></pre><p id="054b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们创建一个包含文本文档及其相应标签(新闻组名称)的数据框架。</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="ac0b" class="lw jw hi mo b fi mt mu l mv mw">df = pd.DataFrame({'label':dataset.target, 'text':dataset.data})</span><span id="2fd8" class="lw jw hi mo b fi mx mu l mv mw">df.shape</span></pre><p id="f34c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> (11314，2) </strong></p><p id="b237" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将通过从数据集中出现的20个标签中只选择2个来将这转换成二元分类问题。我们将选择分别对应于“comp.graphics”和“rec.sport.hockey”的标签1和10。</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="4c4b" class="lw jw hi mo b fi mt mu l mv mw">df = df[df['label'].isin([1,10])] <br/>df = df.reset_index(drop = True)</span></pre><p id="e7af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们快速地看一下目标分布。</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="cb59" class="lw jw hi mo b fi mt mu l mv mw">df['label'].value_counts()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es my"><img src="../Images/029d074d06d35e35babc9add6c7ea4d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LbiOQdRh7IhIkvbgZcvozQ.png"/></div></div></figure><p id="e35c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分布看起来相当均匀。在这种情况下，准确性是一个很好的评估指标。</p><h1 id="4cfa" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">数据预处理</h1><p id="d330" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">向模型提供干净的数据总是一个好的做法，尤其是当数据以非结构化文本的形式出现时。让我们通过仅保留字母并删除所有其他内容来清理我们的文本。</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="aa56" class="lw jw hi mo b fi mt mu l mv mw">df['text'] = df['text'].str.replace("[^a-zA-Z]", " ")</span></pre><p id="5d7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将从文本数据中删除停用词。如果你以前从未使用过停用词，那么你必须从<em class="jt"> nltk </em>包中下载它们，如下所示:</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="ca81" class="lw jw hi mo b fi mt mu l mv mw">import nltk nltk.download('stopwords') <br/>from nltk.corpus import stopwords </span><span id="aaec" class="lw jw hi mo b fi mx mu l mv mw">stop_words = stopwords.words('english')</span><span id="fcc9" class="lw jw hi mo b fi mx mu l mv mw"># tokenization <br/>tokenized_doc = df['text'].apply(lambda x: x.split()) </span><span id="2f27" class="lw jw hi mo b fi mx mu l mv mw"># remove stop-words <br/>tokenized_doc = tokenized_doc.apply(lambda x:[item for item in x if <br/>                                    item not in stop_words]) </span><span id="610f" class="lw jw hi mo b fi mx mu l mv mw"><br/># de-tokenization <br/>detokenized_doc = [] </span><span id="00e2" class="lw jw hi mo b fi mx mu l mv mw">for i in range(len(df)):<br/>    t =' '.join(tokenized_doc[i]) <br/>    detokenized_doc.append(t) </span><span id="a007" class="lw jw hi mo b fi mx mu l mv mw">df['text'] = detokenized_doc</span></pre><p id="545d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们按照60:40的比例将清理后的数据集分成训练集和验证集。</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="38fe" class="lw jw hi mo b fi mt mu l mv mw">from sklearn.model_selection import train_test_split </span><span id="abfd" class="lw jw hi mo b fi mx mu l mv mw"># split data into training and validation set <br/>df_trn, df_val = train_test_split(df, stratify = df['label'], <br/>                                  test_size = 0.4, <br/>                                  random_state = 12)</span><span id="c12b" class="lw jw hi mo b fi mx mu l mv mw">df_trn.shape, df_val.shape</span></pre><p id="f643" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ((710，2)，(474，2)) </strong></p><p id="e39c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">完美！</p><p id="dade" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在继续之前，我们需要分别为语言模型和分类模型准备数据。好消息是什么？使用fastai库可以很容易地做到这一点:</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="287b" class="lw jw hi mo b fi mt mu l mv mw"># Language model data <br/>data_lm = TextLMDataBunch.from_df(train_df = df_trn, valid_df = <br/>                                  df_val, path = "") </span><span id="ee3e" class="lw jw hi mo b fi mx mu l mv mw"># Classifier model data <br/>data_clas = TextClasDataBunch.from_df(path = "", train_df = df_trn, <br/>                                      valid_df = df_val,  <br/>                                      vocab=data_lm.train_ds.vocab, <br/>                                      bs=32)</span></pre><h1 id="682f" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">微调预训练模型并进行预测</h1><p id="e168" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">我们可以使用我们之前创建的<code class="du ml mm mn mo b">data_lm</code>对象来微调一个预先训练好的语言模型。我们可以创建一个学习对象“learn”，它将直接创建一个模型，下载预先训练好的权重，并准备好进行微调:</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="77fe" class="lw jw hi mo b fi mt mu l mv mw">learn = language_model_learner(data_lm, pretrained_model=URLs.WT103,  <br/>                               drop_mult=0.7)</span></pre><p id="7f22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个周期和循环动量允许模型以更高的学习速率被训练并且收敛得更快。<strong class="ih hj">单周期政策提供了某种形式的规范</strong>。我们不会深入研究它是如何工作的，因为这篇文章是关于学习实现的。然而，如果你希望了解更多关于单周期政策的信息，那么请随意参考Leslie Smith的这篇优秀论文——“<a class="ae ju" href="https://arxiv.org/abs/1803.09820" rel="noopener ugc nofollow" target="_blank">神经网络超参数的训练方法:第1部分——学习速率、批量大小、动量和权重衰减</a>”。</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="ccd0" class="lw jw hi mo b fi mt mu l mv mw"># train the learner object with learning rate = 1e-2 learn.fit_one_cycle(1, 1e-2)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mz"><img src="../Images/476876aaf9bb6c03a333244599053e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*H42845upPTW8VOFtheT4BQ.png"/></div></figure><p id="4943" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将保存这个编码器，以便稍后用于分类。</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="b640" class="lw jw hi mo b fi mt mu l mv mw">learn.save_encoder('ft_enc')</span></pre><p id="d620" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们使用之前创建的<code class="du ml mm mn mo b">data_clas</code>对象来构建一个带有微调编码器的分类器。</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="d0aa" class="lw jw hi mo b fi mt mu l mv mw">learn = text_classifier_learner(data_clas, drop_mult=0.7) learn.load_encoder('ft_enc')</span></pre><p id="69fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将再次尝试符合我们的模型。</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="ffc1" class="lw jw hi mo b fi mt mu l mv mw">learn.fit_one_cycle(1, 1e-2)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es na"><img src="../Images/f735a7264fe661cad8ae0a352a124e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*0H_Nk0TyJk8TeBJyzSvOyQ.png"/></div></figure><p id="1349" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">哇！我们的准确率有了巨大的提高，甚至验证损失也远远小于训练损失。在小数据集上，这是一个非常出色的表现。您甚至可以使用下面的代码从学习者对象中获得验证集的预测:</p><pre class="je jf jg jh fd mp mo mq mr aw ms bi"><span id="7649" class="lw jw hi mo b fi mt mu l mv mw"># get predictions <br/>preds, targets = learn.get_preds() <br/>predictions = np.argmax(preds, axis = 1) </span><span id="4813" class="lw jw hi mo b fi mx mu l mv mw">pd.crosstab(predictions, targets)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nb"><img src="../Images/af7724c9b164f6f06f4bbb8ff5c75c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*0OspilAaYe76_orGT3_EEA.png"/></div></figure><h1 id="30a2" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">下一步是什么？</h1><p id="e9bf" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">随着ULMFiT等方法的出现，我们正朝着更通用的NLP系统前进。这些模型将能够同时执行多项任务。此外，这些模式不仅仅局限于英语，还包括全球使用的其他几种语言。</p><p id="1909" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还有即将推出的技术，如ELMo，一种新的单词嵌入技术，以及BERT，一种新的语言表示模型，旨在通过联合调节所有层中的左右上下文来预训练深度双向表示。这些技术已经在许多NLP任务上取得了最先进的结果。因此，NLP的黄金时期刚刚到来，它就在这里停留。</p><h1 id="da99" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">结束注释</h1><p id="c8d6" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">我希望这篇文章对你有所帮助。然而，使用fastai库在ULMFiT中仍然有更多的东西可以探索，我鼓励你们去探索。如果你有任何建议，请在下面的评论区告诉我。此外，尝试在你选择的不同问题和领域上使用ULMFiT，看看结果如何。</p><p id="4d76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">代码:</strong>这里可以找到完整的代码<a class="ae ju" href="https://github.com/prateekjoshi565/ULMFiT_Text_Classification/blob/master/ULMFiT_fastai_Text_Classification.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="1ddf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jt">欢迎致电</em><strong class="ih hj"><em class="jt">prateekjoshi565@gmail.com</em></strong><em class="jt">联系我，进行一对一讨论。</em></p></div></div>    
</body>
</html>