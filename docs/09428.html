<html>
<head>
<title>Handling Categorical Features using Encoding Techniques in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 中的编码技术处理分类要素</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/handling-categorical-features-using-encoding-techniques-in-python-7b46207111ca?source=collection_archive---------4-----------------------#2020-09-06">https://medium.com/analytics-vidhya/handling-categorical-features-using-encoding-techniques-in-python-7b46207111ca?source=collection_archive---------4-----------------------#2020-09-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a7b3dbef2c043b3210d96e0a86188459.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WauOqoTz0H0jMJ66"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">马库斯·斯皮斯克在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="60ef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇文章中，我们将讨论机器学习中的分类特征，以及使用两种最有效的方法来处理这些特征的方法。</p><h1 id="c649" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">分类特征</strong></h1><p id="6354" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在机器学习中，特征可以大致分为两大类:</p><ul class=""><li id="0b7d" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lb lc ld le bi translated">数字特征(年龄、价格、面积等。)</li><li id="d8a1" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">分类特征(性别、婚姻状况、职业等。)</li></ul><p id="3728" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由一定数量的类别组成的所有这些特征被称为分类特征。分类特征可以分为两种主要类型:</p><ol class=""><li id="06f2" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lk lc ld le bi translated">名义上的</li><li id="1b15" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lk lc ld le bi translated">序数</li></ol><p id="3947" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">名义</strong>特征是指有两个或两个以上类别的特征，没有特定的顺序。比如性别有两个值，男性和女性，可以认为是一个名义上的特征。</p><p id="5a03" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">序数</strong>另一方面，特征有特定顺序的类别。例如，如果我们有一个名为 Level 的特性，其值为 high、medium 和 low，那么它将被视为一个序数特性，因为这里的顺序很重要。</p><h1 id="5f3e" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">处理分类特征</h1><p id="814d" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">因此，出现的第一个问题是，为什么我们需要分别处理分类特征？为什么我们不像数字特征一样简单地将它们作为输入传递给我们的模型呢？答案是，与人类不同，机器，尤其是机器学习模型，不理解文本数据。在将文本值输入模型之前，我们需要将它们转换成相关的数字。</p><p id="fb27" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将类别转换成数字的过程称为编码。最有效和最广泛使用的两种编码方法是:</p><ol class=""><li id="a35b" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lk lc ld le bi translated">标签编码</li><li id="12fe" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lk lc ld le bi translated">一个热编码</li></ol><p id="4552" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签编码</strong></p><p id="7ac8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">标签编码是将数字标签分配给要素中每个类别的过程。如果 N 是类别数，所有类别值将被分配一个从 0 到 N-1 的唯一数字。</p><p id="e730" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们有一个名为“颜色”的特征，其值为红色、蓝色、绿色和黄色，它可以转换为数字映射，如下所示</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="7535" class="lu ju hi lq b fi lv lw l lx ly"><strong class="lq hj">Category</strong> : <strong class="lq hj">Label</strong><br/>"red"    : 0<br/>"blue"   : 1<br/>"green"  : 2<br/>"yellow" : 3</span></pre><p id="aa98" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">注意:</strong>正如我们在这里看到的，为类别生成的标签不是标准化的，即不在 0 和 1 之间。由于这一限制，标注编码不应用于要素量起着重要作用的线性模型。由于基于树的算法不需要特征归一化，标签编码可以容易地用于这些模型，例如:</p><ul class=""><li id="c133" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lb lc ld le bi translated">决策树</li><li id="6a48" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">随机森林</li><li id="39c8" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">XGBoost</li><li id="fed1" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">LighGBM</li></ul><p id="8f82" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以使用 scikit-learn 的 LabelEncoder 类实现标签编码。我们将在下一节看到实现。</p><p id="9a70" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">一个热编码</strong></p><p id="585b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">标签编码的限制可以通过二进制化类别来克服，即仅使用 0 和 1 来表示类别。这里，我们通过大小为 N 的向量来表示每个类别，其中 N 是该特征中类别的数量。每个向量有一个 1，其余所有值都是 0。因此，它被称为一热编码。</p><p id="2be4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设我们有一个名为 temperature 的列。它有四个值:冷、冷、温、热。每个类别将表示如下:</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="f6ec" class="lu ju hi lq b fi lv lw l lx ly"><strong class="lq hj">Category</strong>        E<strong class="lq hj">ncoded vector<br/></strong>Freezing        0  0  0  1<strong class="lq hj"><br/></strong>Cold            0  0  1  0<strong class="lq hj"><br/></strong>Warm            0  1  0  0<strong class="lq hj"><br/></strong>Hot             1  0  0  0</span></pre><p id="608e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如您在这里看到的，每个类别由一个长度为 4 的向量表示，因为 4 是特性中唯一类别的数量。每个向量只有一个 1，其余所有值都是 0。</p><p id="bfa8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于一键编码可生成归一化要素，因此可用于线性模型，例如:</p><ul class=""><li id="9bc5" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lb lc ld le bi translated">线性回归</li><li id="a43c" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">逻辑回归</li></ul><p id="af69" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们已经对这两种编码技术有了基本的了解，为了更好的理解，让我们看看这两种编码技术的 python 实现。</p><h1 id="8c07" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">用 Python 实现</h1><p id="9a45" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在对分类特征应用编码之前，处理 NaN 值很重要。一种简单有效的方法是将 NaN 值作为一个单独的类别来处理。通过这样做，我们可以确保不会丢失任何重要信息。</p><p id="ba92" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，我们在处理分类特征时遵循的步骤是:</p><ol class=""><li id="93ea" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lk lc ld le bi translated">用新的类别填充 NaN 值(例如 NONE)</li><li id="4e8a" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lk lc ld le bi translated">对基于树的模型使用标签编码，对线性模型使用 hot 编码，将类别转换为数值。</li><li id="6af1" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lk lc ld le bi translated">使用数字和编码要素构建模型。</li></ol><p id="e40c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将在 kaggle 的 Dat 中使用一个名为<strong class="ix hj">猫的公共数据集。链接<a class="ae iu" href="https://www.kaggle.com/c/cat-in-the-dat-ii" rel="noopener ugc nofollow" target="_blank">这里</a>。这是一个包含大量分类特征的二元分类问题。</strong></p><p id="70a3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们将使用 scikit-learn 中的 StratifiedKFold 类创建 5 个折叠进行验证。KFold 的这种变体用于确保每个折叠中目标变量的比率相同。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="f845" class="lu ju hi lq b fi lv lw l lx ly">import pandas as pd<br/>from sklearn import model_selection</span><span id="ab8d" class="lu ju hi lq b fi lz lw l lx ly">#read training data<br/>df = pd.read_csv('../input/train.csv')</span><span id="ec68" class="lu ju hi lq b fi lz lw l lx ly">#create column for kfolds and fill it with -1<br/>df['kfold'] = -1</span><span id="9862" class="lu ju hi lq b fi lz lw l lx ly">#randomize the rows<br/>df = df.sample(frac=1).reset_index(drop=True)</span><span id="1802" class="lu ju hi lq b fi lz lw l lx ly">#fetch the targets<br/>y = df['target'].values</span><span id="870c" class="lu ju hi lq b fi lz lw l lx ly">#initiatre StratifiedKFold class from model_selection<br/>kf = model_selection.StratifiedKFold(n_splits=5)</span><span id="ebee" class="lu ju hi lq b fi lz lw l lx ly">#fill the new kfold column <br/>for f,(t_,v_) in enumerate(kf.split(X=df,y=y)):<br/>    df.loc[v_,'kfold'] = f</span><span id="17e0" class="lu ju hi lq b fi lz lw l lx ly">#save the new csv with kfold column<br/>df.to_csv('../input/train_folds.csv',index=False)</span></pre><p id="be6b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签编码</strong></p><p id="690c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，让我们定义在每个文件夹上运行训练和验证函数。在这个例子中，我们将对随机森林使用 LabelEncoder。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="ed90" class="lu ju hi lq b fi lv lw l lx ly">import pandas as pd<br/>from sklearn import ensemble<br/>from sklearn import metrics<br/>from sklearn import preprocessing</span><span id="8f09" class="lu ju hi lq b fi lz lw l lx ly">def run(fold):<br/>    #read training data with folds<br/>    df = pd.read_csv('../input/train_folds.csv')</span><span id="045f" class="lu ju hi lq b fi lz lw l lx ly">    #get all relevant features excluding id, target and kfold columns<br/>    features = [feature for feature in df.columns if feature not in ['id','target','kfold']]</span><span id="7adf" class="lu ju hi lq b fi lz lw l lx ly">    #fill all nan values with NONE<br/>    for feature in features:<br/>        df.loc[:,feature] = df[feature].astype(str).fillna('NONE')</span><span id="9bdf" class="lu ju hi lq b fi lz lw l lx ly">    #Label encoding the features<br/>    for feature in features:<br/>        #initiate LabelEncoder for each feature<br/>        lbl = preprocessing.LabelEncoder()</span><span id="2309" class="lu ju hi lq b fi lz lw l lx ly">        #fit the label encoder<br/>        lbl.fit(df[feature])</span><span id="fc87" class="lu ju hi lq b fi lz lw l lx ly">        #transform data<br/>        df.loc[:,feature] = lbl.transform(df[feature])</span><span id="5990" class="lu ju hi lq b fi lz lw l lx ly">    #get training data using folds<br/>    df_train = df[df['kfold']!=fold].reset_index(drop=True)<br/>    <br/>    #get validation data using folds<br/>    df_valid = df[df['kfold']==fold].reset_index(drop=True)</span><span id="1e46" class="lu ju hi lq b fi lz lw l lx ly">    #get training features<br/>    X_train = df_train[features].values<br/>    <br/>    #get validation features<br/>    X_valid = df_valid[features].values</span><span id="c591" class="lu ju hi lq b fi lz lw l lx ly">    #initiate Random forest model<br/>    model = ensemble.RandomForestClassifier(n_jobs=-1)</span><span id="56dc" class="lu ju hi lq b fi lz lw l lx ly">    #fit the model on train data<br/>    model.fit(X_train,df_train['target'].values)</span><span id="feed" class="lu ju hi lq b fi lz lw l lx ly">    #predict the probabilities on validation data<br/>    valid_preds = model.predict_proba(X_valid)[:,1]</span><span id="28f3" class="lu ju hi lq b fi lz lw l lx ly">    #get auc-roc score<br/>    auc = metrics.roc_auc_score(df_valid['target'].values,valid_preds)</span><span id="921a" class="lu ju hi lq b fi lz lw l lx ly">    #print AUC score for each fold<br/>    print(f'Fold ={fold}, AUC = {auc}')</span></pre><p id="b515" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，让我们调用这个方法来为每个文件夹执行 run 方法。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="b9b9" class="lu ju hi lq b fi lv lw l lx ly">if __name__=='__main__':<br/>    for fold_ in range(5):<br/>        run(fold_)</span></pre><p id="3319" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">执行这段代码将产生如下所示的输出。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="9aaf" class="lu ju hi lq b fi lv lw l lx ly">Fold =0, AUC = 0.7163772816343564<br/>Fold =1, AUC = 0.7136206487083182<br/>Fold =2, AUC = 0.7171801474337066<br/>Fold =3, AUC = 0.7158938474390842<br/>Fold =4, AUC = 0.7186004462481813</span></pre><p id="efaf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里需要注意的一点是，我们没有对随机森林模型进行任何超参数调整。您可以调整参数来提高验证的准确性。在上面的代码中要提到的另一件事是，我们使用 AUC ROC 分数作为验证的度量。这是因为目标值是有偏差的，准确性等指标不会给我们正确的结果。</p><p id="f681" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">一个热编码</strong></p><p id="e6b6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们来看一个带有逻辑回归的热编码的实现。</p><p id="e15b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是这种方法的 run 方法的修改版本。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="71c8" class="lu ju hi lq b fi lv lw l lx ly">import pandas as pd<br/>from sklearn import linear_model<br/>from sklearn import metrics<br/>from sklearn import preprocessing</span><span id="d16f" class="lu ju hi lq b fi lz lw l lx ly">def run(fold):<br/>    #read training data with folds<br/>    df = pd.read_csv('../input/train_folds.csv')</span><span id="7a7f" class="lu ju hi lq b fi lz lw l lx ly">    #get all relevant features excluding id, target and folds columns<br/>    features = [feature for feature in df.columns if feature not in ['id','target','kfold']]</span><span id="8153" class="lu ju hi lq b fi lz lw l lx ly">    #fill all nan values with NONE<br/>    for feature in features:<br/>        df.loc[:,feature] = df[feature].astype(str).fillna('NONE')</span><span id="d8cf" class="lu ju hi lq b fi lz lw l lx ly">    #get training data using folds<br/>    df_train = df[df['kfold']!=fold].reset_index(drop=True)<br/>    <br/>    #get validation data using folds<br/>    df_valid = df[df['kfold']==fold].reset_index(drop=True)</span><span id="ca9c" class="lu ju hi lq b fi lz lw l lx ly">    #initiate OneHotEncoder from sklearn<br/>    ohe = preprocessing.OneHotEncoder()</span><span id="3764" class="lu ju hi lq b fi lz lw l lx ly">    #fit ohe on training+validation features<br/>    full_data = pd.concat([df_train[features],df_valid[features]],axis=0)<br/>    ohe.fit(full_data[features])</span><span id="b67b" class="lu ju hi lq b fi lz lw l lx ly">    #transform training data<br/>    X_train = ohe.transform(df_train[features])<br/>    <br/>    #transform validation data<br/>    X_valid = ohe.transform(df_valid[features])</span><span id="82cf" class="lu ju hi lq b fi lz lw l lx ly">    #initiate logistic regression<br/>    model = linear_model.LogisticRegression()</span><span id="d628" class="lu ju hi lq b fi lz lw l lx ly">    #fit the model on train data<br/>    model.fit(X_train,df_train['target'].values)</span><span id="2612" class="lu ju hi lq b fi lz lw l lx ly">    #predict the probabilities on validation data<br/>    valid_preds = model.predict_proba(X_valid)[:,1]</span><span id="c372" class="lu ju hi lq b fi lz lw l lx ly">    #get auc-roc score<br/>    auc = metrics.roc_auc_score(df_valid['target'].values,valid_preds)</span><span id="bebc" class="lu ju hi lq b fi lz lw l lx ly">    #print AUC score for each fold<br/>    print(f'Fold ={fold}, AUC = {auc}')</span></pre><p id="1873" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">循环所有折叠的方法保持不变。</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="6aa0" class="lu ju hi lq b fi lv lw l lx ly">if __name__=='__main__':<br/>    for fold_ in range(5):<br/>        run(fold_)</span></pre><p id="f582" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这段代码的输出如下所示:</p><pre class="ll lm ln lo fd lp lq lr ls aw lt bi"><span id="c576" class="lu ju hi lq b fi lv lw l lx ly">Fold =0, AUC = 0.7872262099199782<br/>Fold =1, AUC = 0.7856877416085041<br/>Fold =2, AUC = 0.7850910855093067<br/>Fold =3, AUC = 0.7842966593706009<br/>Fold =4, AUC = 0.7887711592194284</span></pre><p id="4894" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们在这里看到的，一个简单的逻辑回归通过对分类特征应用特征编码给了我们相当好的准确性。</p><p id="53fb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这两种方法的实现中需要注意的一个区别是，LabelEncoder 必须分别适用于每个分类特征，而 OneHotEncoder 可以适用于所有特征。</p><h1 id="8812" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="0281" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">在这篇博客中，我讨论了机器学习中的分类特征，为什么处理这些特征很重要。我们还讨论了将分类特征编码成数字的两种最重要的方法，以及实现。</p><p id="0536" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我希望我已经帮助您更好地理解了这里涉及的主题。请让我知道你在评论中的反馈，如果你喜欢就给它一个掌声。<a class="ae iu" href="https://www.linkedin.com/in/sawan-saxena-640a4475/" rel="noopener ugc nofollow" target="_blank">这里的</a>是我的 Linkedin 个人资料的链接，如果你想连接的话。</p><p id="db60" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢阅读。:)</p></div></div>    
</body>
</html>