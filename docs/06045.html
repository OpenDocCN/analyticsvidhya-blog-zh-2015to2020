<html>
<head>
<title>Transforming 2D Video to 3D Animation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将2D视频转换为3D动画</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/transforming-2d-video-to-3d-animation-dc48cf1227f2?source=collection_archive---------2-----------------------#2020-05-10">https://medium.com/analytics-vidhya/transforming-2d-video-to-3d-animation-dc48cf1227f2?source=collection_archive---------2-----------------------#2020-05-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6b63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用深度学习的魔法</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/1672f83bd6e37a2175b6eec457290c03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4QLg2pOLqtpc8IKbS9rcrg.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">照片由<a class="ae jt" href="https://unsplash.com/images/things/dance?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae jt" href="https://unsplash.com/@aditya_ali?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Aditya Ali </a>拍摄</figcaption></figure><h1 id="25c9" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">问题陈述</h1><p id="19ee" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">我们正试图从2D视频中通过跟踪一个人的运动来渲染这个人的3D动画。</p><h1 id="90d0" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">我们为什么选择这种说法？</h1><p id="4585" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在3D图形中制作一个人的动画需要一个巨大的运动跟踪器来跟踪这个人的运动，并且手动制作每个肢体的动画也需要时间。我们的目标是提供一种节省时间的方法来做同样的事情。</p><h1 id="5202" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">我们是怎么解决的？</h1><p id="6d8e" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">我们对此问题的解决方案包括以下步骤:</p><ol class=""><li id="2664" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated"><strong class="ih hj"> 2D姿态估计:</strong>人体至少需要17个标志点来完整描述其姿态。</li><li id="8267" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated"><strong class="ih hj"> DeepSORT+FaceReID: </strong>追踪姿势的移动。</li><li id="2578" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated"><strong class="ih hj">提升2D到3D: </strong>我们从上一步得到的坐标在2D。为了让它们变成3D动画，我们需要将这些二维坐标映射到三维空间。</li><li id="ba5c" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated"><strong class="ih hj">渲染为3D: </strong>上一步检测到的这17个标志点的坐标，现在就是需要动画的3D角色四肢关节的位置。</li></ol><p id="b55a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们在文章的其余部分详细讨论这些步骤。</p><h1 id="f3ca" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">2D姿态估计</h1><p id="53a8" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">如上所述，只需指定17个关键的关键点(在深度学习社区中称为标志点)，就可以完全描述一个人的姿势。你可能已经猜到了，我们正在使用深度学习来估计人类的姿态(即在视频帧中跟踪人类的姿态)。在网上(我指的是在Github上)可以找到相当多的先进框架(比如PoseFlow和AlphaPose ),它们已经实现了相当精确的姿态估计。</p><ol class=""><li id="a1e5" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated"><strong class="ih hj"> PoseFlow: </strong>第一个框架是由<a class="ae jt" href="https://arxiv.org/pdf/1802.00977.pdf" rel="noopener ugc nofollow" target="_blank">修等人</a>开发的PoseFlow。PoseFlow算法的基本概述是，该框架首先通过最大化视频所有帧的整体置信度来构建姿势。下一步是删除使用非最大抑制(通常缩写为NMS)技术检测到的冗余姿态。</li><li id="10fe" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated"><strong class="ih hj"> AlphaPose: </strong>你可以在下面附上的GIF中看到，使用PoseFlow(在左边)估计的姿势在一些帧中有小故障。这就把我们带到了下一个框架:AlphaPose。AlphaPose是由<a class="ae jt" href="https://arxiv.org/pdf/1612.00137.pdf" rel="noopener ugc nofollow" target="_blank">郝-方舒等人</a>开发的。该框架在帧中检测到的人周围绘制边界框，并估计他们在每帧中的姿势。它还可以检测姿态，即使一个人被另一个人部分遮挡。</li></ol><div class="je jf jg jh fd ab cb"><figure class="ll ji lm ln lo lp lq paragraph-image"><img src="../Images/8b5640e10658c96396b2550d3991a6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*RG20ZiNApQdhiPNsLI_SjA.gif"/></figure><figure class="ll ji lm ln lo lp lq paragraph-image"><img src="../Images/a6c37da1fb331219f44f4207dc292181.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*E5ltnOC_bn-pdlL0TGiM7w.gif"/><figcaption class="jp jq et er es jr js bd b be z dx lr di ls lt translated">左边的PoseFlow。阿尔法姿势在右边。GIFs来源:【https://github.com/MVIG-SJTU/AlphaPose T4】</figcaption></figure></div><p id="c980" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">AlphaPose框架的代码可以在这里找到<a class="ae jt" href="https://github.com/MVIG-SJTU/AlphaPose" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="2fe3" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">DeepSORT + FaceReID</h1><p id="6f6e" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">我们已经使用Alpha Pose来检测视频中人的姿势。下一步是跟踪他们的运动，以便能够建立一个流畅的运动动画。DeepSORT框架的研究论文可以在<a class="ae jt" href="https://arxiv.org/pdf/1703.07402.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="437e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用DeepSORT和FaceReid边界框的输出，我们以下面的方式分离不同人的姿势。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/a4f4794d1654b798d69edf2d06d7e210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eJtCPnAU3QI8Ub7VZkv5UQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">在每一帧中围绕人绘制边界框</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/2413d6573e0540192b05c3c3e8c0e498.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*xx9nCKDiGW91R3Kg7Uo_Mw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">用于检查姿态关键点是否在边界框内的函数</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/885e7801a21d3da8660d8dc01c8007b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZEc0NTYpBWWAm6yZhG7Kzw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">循环分离不同人的姿势</figcaption></figure><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lx ly l"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">深度排序框架的实现</figcaption></figure><h1 id="945e" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">将2D提升到3D</h1><p id="e647" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">如上所述，我们在姿态估计步骤中获得的坐标是二维的(即，它们位于二维空间中)。但是为了实现我们3D动画的主要项目目标，这些坐标必须被映射到一个三维空间。这也是用…你猜对了！…深度学习！Github上有一个知识库，还有一篇<a class="ae jt" href="https://arxiv.org/pdf/1705.03098.pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a>被ICCV 2017年接受。可以在<a class="ae jt" href="https://github.com/una-dinosauria/3d-pose-baseline" rel="noopener ugc nofollow" target="_blank">这里</a>找到存储库的链接。</p><p id="d901" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">TL；DR:</strong>3d-pose-baseline知识库在Human3.6M数据集上训练他们的模型。该数据集包含17个不同场景中大约360万人的3d姿态及其相应的图像。简而言之，模型的输入是360万人的图像，期望的输出是数据集中存在的3d姿态。现在，可以建立和调整深度学习模型，直到它达到相当高的精确度。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lx ly l"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="https://github.com/una-dinosauria/3d-pose-baseline" rel="noopener ugc nofollow" target="_blank">三维姿态基线</a>库的结果</figcaption></figure><h1 id="b3d0" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">3D动画</h1><p id="04c6" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">一旦我们从姿势估计框架中获得了标志点的坐标，我们就可以将这些坐标输入到3D角色身体的每个肢体中。在这里，我们使用Unity作为3D动画环境来完成任务。</p><p id="a5f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每一帧的17个标志点的坐标存储在一个文本文件中，这个文本文件在Unity中使用C#读取。从文件中读取的坐标现在被重定位到3D人形模型中。这17个关键点与Unity内置的人形头像的身体关键点进行映射。</p><p id="fa9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，动画是使用Unity的反向运动学，骨骼旋转和四元数完成的。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ly l"/></div></figure><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ly l"/></div></figure><h1 id="8499" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">我们的结果</h1><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lx ly l"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">单人动画</figcaption></figure><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lx ly l"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">多人3d动画</figcaption></figure><h1 id="ec3a" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结论</h1><p id="e3b6" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">总之，我们能够实现视频中多人的姿势估计，并使用Unity等3D环境制作运动动画，同时还能保持视频中两个不同人之间的相对位置。</p><p id="60bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相关资源库的Github资源库链接如下:</p><ol class=""><li id="0af2" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated">AlphaPose/PoseFlow的Github知识库:<a class="ae jt" href="https://github.com/MVIG-SJTU/AlphaPose" rel="noopener ugc nofollow" target="_blank">https://github.com/MVIG-SJTU/AlphaPose</a></li><li id="f214" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated">深度排序的Github库:【https://github.com/nwojke/deep_sort T2】</li><li id="d519" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated">Github三维姿态基线库:【https://github.com/una-dinosauria/3d-pose-baseline T4】</li></ol><p id="1a68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相关研究论文的链接如下:</p><ol class=""><li id="7a34" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated">PoseFlow的研究论文:<a class="ae jt" href="https://arxiv.org/pdf/1802.00977.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1802.00977.pdf</a></li><li id="6657" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated">AlphaPose的研究论文:<a class="ae jt" href="https://arxiv.org/pdf/1612.00137.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.00137.pdf</a></li><li id="bdf6" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated">深度排序的研究论文:<a class="ae jt" href="https://arxiv.org/pdf/1703.07402.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1703.07402.pdf</a></li><li id="4460" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated">三维姿态基线研究论文:<a class="ae jt" href="https://arxiv.org/pdf/1705.03098.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1705.03098.pdf</a></li></ol><p id="5e2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们项目的Github资源库链接可以在<a class="ae jt" href="https://github.com/laxmaniron/3D-Animation" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="feef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读！如果你喜欢这篇文章和评论，别忘了鼓掌。</p><p id="b658" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此项目的合作者:</p><ol class=""><li id="ccd4" class="kx ky hi ih b ii ij im in iq kz iu la iy lb jc lc ld le lf bi translated">拉克斯曼·库马拉普:<a class="ae jt" href="https://github.com/laxmaniron" rel="noopener ugc nofollow" target="_blank">https://github.com/laxmaniron</a></li><li id="1738" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated">斯里苏亚·斯里干达:<a class="ae jt" href="https://github.com/SuryaSrikanth" rel="noopener ugc nofollow" target="_blank">https://github.com/SuryaSrikanth</a></li><li id="4317" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated">斯拉万·库马尔·维纳科塔:<a class="ae jt" href="https://github.com/JekyllAndHyde8999" rel="noopener ugc nofollow" target="_blank">https://github.com/JekyllAndHyde8999</a></li></ol></div></div>    
</body>
</html>