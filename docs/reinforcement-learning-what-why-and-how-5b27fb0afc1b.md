# 强化学习——什么，为什么，如何。

> 原文：<https://medium.com/analytics-vidhya/reinforcement-learning-what-why-and-how-5b27fb0afc1b?source=collection_archive---------14----------------------->

当谈到机器学习的类型和方法时，强化学习占有独特和特殊的地位。这是第三种类型的机器学习，一般来说可以称为“从经验中学习”。

在这篇文章中，我将尝试回答三个关于强化学习的基本但重要的问题。

# **什么是强化学习？**

让我们看一点细节，为此，我想从定义、直觉开始揭示三个方面，然后我将简单地谈谈 RL 代理的基本元素和特征。

![](img/3b2d673c89cba7bb812347501632fcdd.png)

**定义**

嗯，我们周围有各种各样的强化学习的定义，例如，它说，强化学习是学习做什么，如何将情况映射到行动，以便最大化奖励信号。

另一个版本稍微详细一些，说强化学习是一种 ML 技术，它涉及一个代理在一个环境中通过选择预定义的动作来行动，目标是最大化数字奖励。

![](img/53606f45919b2a767293f23ad5b8408f.png)

现在从各种各样的定义中，我们可以有这些关键词来定义强化学习，它是一种 *ML 类型*，它涉及一个*代理*在*环境*中交互，感知*状态*，采取*行动*，然后为环境采取的行动获得*奖励*，目的是使奖励最大化。*那么，这其中的学问在哪里呢？*

好吧，学习就是政策，让我们看看这个定义的一个实例。

例如，在下面的例子中，有一个智能体(这个机器人)可以使用它的传感器进行观察，然后根据策略选择动作并触发。通过采取这一行动，它从代理学习的环境中获得负回报，并因此更新它的策略。

![](img/45407cf75bbc4adb3c905f2f7c748d98.png)

代理现在知道，在这个特定的状态下，它需要避免选择火灾，因此选择其他行动。这种通过经验来强化学习的过程持续多次迭代，这些迭代被称为情节，直到代理学习到最优策略。

现在，我们来谈谈使用强化学习背后的直觉。

嗯，总的来说，如果我们看到定义人工智能与商业企业相关性的四个视角。这从感知开始，感知涉及获取数据、大数据/大型数据/流数据/批量数据、对数据进行分析，如报告等。

然后是推理，我们有统计建模，机器学习，深度学习。因此，如果我们看到目前，大多数行业都集中在这两个方面，即感知和推理，因为这些是目前最成熟的。

![](img/8d2ce3dbfaa5267bac08d19631e9c1ba.png)

然而，其他前瞻性的观点是决策和采取行动，我们看到强化学习属于决策类。

想象一下，当企业能够做出决策，而不仅仅是获得洞察力(从数据中推断出来)时，整个周转时间、效率和生产率将会成倍增长。

这就是强化学习的力量所在。

最后，让我们看看定义 RL 代理的基本元素是什么。RL 代理可以包括这些组件中的一个或多个，

A)决定代理行为的策略(强制策略)

b)一个价值函数，如果它存在，意味着政策是隐含的。

c)模型是一种主体自己对环境的理解，主体可以是无模型的，也可以是有模型的。

![](img/3da1884eb9a04f63b43df8f29e2e5f47.png)

以下是强化学习的典型特征:

首先，没有监督者，即强化学习代理不处理指导性反馈，而是由标量奖励信号确定的评估性反馈。

第二，有延迟的回报，或估计的回报，例如，考虑这个类比，我们可能有一个基于 RL 的下棋代理，现在一个特定的移动或移动序列，可能会导致赢或输，但不一定是在那个特定的移动之后。(其实那是即时 RL 或者 K 武装土匪的特例)

第三，在 RL 中，时间真的很重要，我们将在讨论 RL 中的不同方法时看到这一点。

第四，代理人的行为会影响后续状态，从而影响其获得的后续经验，这就是我们将看到探索和开发方法(也称为试验和搜索)的重要性的地方。

现在让我们快速看一下 RL 代理的关键元素的例子。

策略决定代理的行为，即它决定代理在特定状态下应该采取什么动作。例如，在这个迷宫中，如果一个代理在下一个单元，代理知道，它应该左转。代理最终学习最优策略，该策略对应于最佳可能(最优)动作的地图，该最佳可能(最优)动作对应于它以最大化其回报为目标而访问的州。

![](img/4ff6a83615482eae670e71229022f52f.png)

另一方面，价值函数有助于确定对未来回报的预测，并用于评估特定状态或对应于状态的行为的良好性/价值。换句话说，它回答了“在特定状态下有多好，或者在特定状态下采取任何行动有多好”这个问题。例如，在同一个迷宫示例中，由于对应于起点左侧的单元与其右侧的单元(值为-15)相比具有较小的值，代理知道它应该从具有-16 的单元移动到-15，并最终移动到具有最大值的单元，该最大值是最终状态(接近目标)并具有对应于-1 的值。

![](img/bba61bea41a6f3b8d926659edb2ac369.png)

然后，我们有模型，就像我们人类一样，那是我们自己对世界的理解。例如，迷宫的这个视图是代理自己感知的整个迷宫的内部视图，这就是为什么我们看不到所有的轨迹。

正如我前面提到的，一个代理可能有一个模型，也可能没有模型&完全学习探索和开发的方法。

这使学习成为学习和适应动态环境的一种独特而有效的方式。

现在，当我们理解了强化学习的不同方面，让我们来回答这个重要的问题，为什么我们还要费心去学习强化学习？

# 为什么要强化学习？

“为什么”这个概念背后可能会有很多原因，我有以下两个原因，

![](img/4a784eec186d72c6037353d011a773ef.png)

首先，RL 代理通过接受奖励和惩罚的连续过程进行学习，这使他们能够强健地接受训练并应对不可预见的环境。

让我们举这个例子，在强化学习的情况下，这都是关于经验学习的，这就是为什么我们说，让一个人尝尝鱼的味道，休息一下，他就会明白了。与监督学习不同，在监督学习中，我们需要通过数据标签等进行明确的教学/监督。

第二，如果我们看市场趋势，特别是从 Gartner 的人工智能炒作周期来看，2019 年，强化学习登上了浪潮，尽管它仍处于创新触发阶段，但这表明下一个地盘很快将成为现实，因此这是学习的最佳时机，并为 RL 等技术推动的巨大未来需求做好准备

# 如何强化学习？

这里有三种方法，我发现它们在强化学习中很有用。

首先，找一本书名为《强化学习导论》的书，作者是理查德萨顿和安德鲁巴尔托。这是关于这个话题的最佳读物之一。

![](img/bb85bdde3f5a9826e86985f3101f2e18.png)

第二，虽然有很多资源可供你参考，但首先你可以参考大卫·西尔弗的 RL 课程，这是一个大约 10 个视频的集合，端到端地处理 RL，主要处理萨顿和巴尔托书中的内容。因此，你甚至可以观看视频，然后阅读书中的相关章节，以确认对主题的理解。

此外，还有更多的资源，如 Udacity 的一个很好的对话式 RL 课程，而且你可以随时关注我的频道([认知](https://www.youtube.com/channel/UCt8BeNe9CKSaks6XhgFulNw?view_as=subscriber))。

第三，方面是让你的手脏起来，因为如果没有动手，这个主题将变得太大，这就是 openai.com 将帮助的地方。在那里，您可以参考 Gym 来复习示例、环境、培训您的 RL 代理、调整现有代理等。相信我，那会很有趣的。

注:观看相关视频并关注我的频道，了解关于[https://www.youtube.com/channel/UCt8BeNe9CKSaks6XhgFulNw](https://www.youtube.com/channel/UCt8BeNe9CKSaks6XhgFulNw?view_as=subscriber)的最新消息