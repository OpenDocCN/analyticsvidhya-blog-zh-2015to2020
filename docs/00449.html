<html>
<head>
<title>Momentum: A simple, yet efficient optimizing technique</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">动量:一种简单而有效的优化技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/momentum-a-simple-yet-efficient-optimizing-technique-ef76834e4423?source=collection_archive---------0-----------------------#2019-06-23">https://medium.com/analytics-vidhya/momentum-a-simple-yet-efficient-optimizing-technique-ef76834e4423?source=collection_archive---------0-----------------------#2019-06-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ad358a46cfae2417fb68a01caa7ab74b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jcyA66HdQsLDEswE"/></div></div></figure><p id="b402" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">先决条件:</strong>对神经网络和损失函数的基本了解</p><h2 id="3bd5" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">介绍</h2><p id="216e" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">在当今世界，一个飞速发展的世界<strong class="is hj"/><em class="ko">(记住这个词，将来会派上用场)</em>你肯定想了解最新的活动，无论是体育、新闻、科技、音乐等等。然而，如果没有使其成为可能的基本组件，目前的技术仍然无法发挥作用。没有晶体管的智能手机..呃？</p><p id="3729" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">可以对在相当多的先进深度学习模型中使用的最流行的优化技术之一进行类似的类比。在我们深入研究动量之前，我们将简要地看一下其他几个主题，在我们研究动量之前，你需要了解这些主题。</p></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h2 id="bfea" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">1.梯度下降</h2><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/3711896c6d9ca9abf2193db3fcd123d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*MuNyVyxnhLZFJ6fxxMrKRw.jpeg"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">均方误差的损失函数</figcaption></figure><p id="2a6e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设我们有一个由<strong class="is hj"> n </strong>个训练示例组成的训练数据集和一个必须训练的给定模型。让我们假设一个损失函数L，它有参数y和带帽子的y。上面的图像中给出了损失函数的示例，其中均方误差用作损失函数。y是假设函数<em class="ko"> h(W) </em>的结果，代表训练数据集的实际标签，带有帽子的y代表我们在数据集上训练时从模型获得的输出。</p><blockquote class="lf lg lh"><p id="e577" class="iq ir ko is b it iu iv iw ix iy iz ja li jc jd je lj jg jh ji lk jk jl jm jn hb bi translated"><strong class="is hj">损失函数几乎是学习模型的基础。在统计学中，典型的损失函数用于</strong> <a class="ae ll" href="https://en.wikipedia.org/wiki/Parameter_estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">参数估计</strong> </a> <strong class="is hj">，所讨论的事件是某个数据实例的估计值与真实值之差的函数。</strong></p></blockquote><p id="bca7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练模型通常从将模型的权重(参数)设置为一些随机初始值开始。随着迭代的进行，权重被更新和优化，以最小化损失。任何第j个参数的梯度下降公式如下:</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/08b0767c83b044696d7527ae793ee493.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*BahkeK4NyQ5KAcJOvfW-MQ.jpeg"/></div></figure><p id="9931" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">α <strong class="is hj"> </strong>表示梯度下降算法的学习率。它是用来控制我们在每次迭代中走多远。这是训练神经网络时需要调整的最重要的超参数。</p><p id="1b0a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">损失函数相对于相应的权重进行微分，以便获得损失函数相对于该权重的梯度值。这允许我们在最佳方向上更新权重。</p><p id="369f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图像中<strong class="is hj"> m </strong>的值可以是1，这种情况称为随机梯度下降(更新每个训练样本上的权值)或者任意数<strong class="is hj"> &gt; 1且≤n </strong>，这种情况称为批量梯度下降(每组m个训练样本训练一次权值)。</p><h2 id="abb0" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">2.简单移动平均线</h2><p id="c960" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">如果我们有一个序列<strong class="is hj"> V </strong>有<strong class="is hj"> n </strong>个元素，那么简单移动平均定义为:</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/acaaa62b55c741fe3f750f46e511a98b.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*I25nL5wr7WBRO1hexW5rVQ.jpeg"/></div></figure><h2 id="3ea5" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">3.指数移动平均线</h2><p id="477e" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">现在让我们取另一个序列<strong class="is hj"> S </strong>与<strong class="is hj"> n </strong>元素。指数移动平均线将新序列<strong class="is hj"> V </strong>定义为:</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/65ff3bd0912c6f5407cbc1008d17dbb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*YV-w1uOZGgzfh-5mvHg8kw.jpeg"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">V —新序列。S —原始序列。</figcaption></figure><p id="2203" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里β被称为<strong class="is hj">平滑常数</strong>。这是新序列的术语的结果:</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/a32481607873eff95e86bab8eb987d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*7ftz8UwKUvNUHKE3PJtZtQ.jpeg"/></div></figure><p id="775a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">解决更多的问题，</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/9a3f17aa9c63663ff318d4c74b800b3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*Lw1SnkwJM0M-l3ki6vQJuw.jpeg"/></div></figure><p id="447f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从这个等式中我们看到，新序列的第<em class="ko">个</em>号的值依赖于所有先前的值<em class="ko"> 1..来自S的所有值都被赋予一定的权重。这个权重是β乘以s的第<em class="ko">(t-I)</em>个值的<em class="ko"> i </em>的幂乘以<em class="ko"> (1- </em> β <em class="ko"> ) </em>，因为β小于1，所以当我们取β的某个正数的幂时，它会变得更小。因此S的<strong class="is hj">旧值得到小得多的权重</strong>，因此<strong class="is hj">对v的当前点的总体值贡献较少</strong>，S的<strong class="is hj">最近值得到较高权重</strong>并且<strong class="is hj">对v的当前值贡献较多</strong>。</em></p></div><div class="ab cl kp kq gp kr" role="separator"><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku kv"/><span class="ks bw bk kt ku"/></div><div class="hb hc hd he hf"><h2 id="2b2d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">4.动力</h2><p id="a61c" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">我们已经定义了一种方法来获得一些序列的“移动”平均值，它随着数据一起变化。我们如何将此应用于训练神经网络？</p><p id="ab68" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">他们可以对我们的梯度值进行平均。让我来解释一下动量是如何工作的以及为什么会工作。</p><p id="852b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，梯度下降并不能准确地为我们提供损失函数的方向，即损失函数的导数。因此，我们可能不会总是朝着最佳方向前进。这主要是因为损失函数的早期导数在更新权重的后期阶段充当噪声。这导致训练和收敛缓慢。</p><p id="bd1b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">动量技术帮助我们解决了收敛速度慢的问题。考虑这样一种情况，在丘陵地带的一个球试图到达最深的山谷。如果山坡在某个阶段非常陡峭，那么球就会获得很大的动量，并且能够以自己的方式通过小山坡。随着坡度的减小，球的动量和速度也减小，最终停在山谷的最深处。</p><blockquote class="lf lg lh"><p id="68fe" class="iq ir ko is b it iu iv iw ix iy iz ja li jc jd je lj jg jh ji lk jk jl jm jn hb bi translated">指数级！记得名字吗？？</p></blockquote><p id="0ad6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">动量技术通过引入表示速度的新变量<em class="ko"> V </em>和摩擦系数/平滑常数β来修改梯度下降法，这有助于控制<em class="ko"> V </em>的值，避免过冲最小值，同时允许更快的收敛。</p><p id="8875" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">回想一下我们之前讨论的指数移动平均线方程。我们可以将该方程与梯度下降更新步骤一起应用，以获得以下动量更新规则:</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/1cfff82ac0d29f0de337b36d94b3171b.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*-hF7uTQL8_u29uMDlFV6Jw.jpeg"/></div></figure><p id="292a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一种方法是忽略(1- β)项，这种方法不太直观。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/e99b1a20883e215583e28cbe1708445b.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*Rbtp8gEmPbCzf2_loSeL1A.jpeg"/></div></figure><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/65f72d1a5718e15cda5fd8cd2d980cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*j86pdyrG-4JqKiI3_ckzVQ.jpeg"/></div></figure><p id="ab62" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这与第一对等式基本相同，唯一的区别在于需要用<em class="ko"> ( </em> 1- β <em class="ko"> ) </em>因子来衡量学习率。</p><p id="3224" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">动量与梯度下降一起工作有两个主要原因:</p><ol class=""><li id="e42b" class="lu lv hi is b it iu ix iy jb lw jf lx jj ly jn lz ma mb mc bi translated">指数移动平均法有助于我们更加重视损失函数导数的最新值，并为我们提供比噪声计算更接近实际导数的更好估计。</li><li id="b587" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">有时，损失函数往往具有这样的结构</li></ol><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/000199b8e4919d7f7011bd2f6a551f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x0_9l2EDmWXxkqCNxKoWfA.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">病理曲线:图像来源:<a class="ae ll" href="https://bit.ly/2NkbnF9" rel="noopener ugc nofollow" target="_blank">https://bit.ly/2NkbnF9</a></figcaption></figure><p id="e6f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">蓝色区域代表一个类似峡谷的结构。</p><blockquote class="lf lg lh"><p id="f0fb" class="iq ir ko is b it iu iv iw ix iy iz ja li jc jd je lj jg jh ji lk jk jl jm jn hb bi translated">峡谷是一个表面在一个维度上比在另一个维度上弯曲得更陡的区域。在深度学习中，峡谷在局部最小值附近很常见，梯度下降很难引导它们。</p></blockquote><p id="e3f2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果在任何迭代中我们进入这个峡谷区域，损失函数可能会像下面一样不断地从峡谷壁上反弹。下面的这个区域被称为<em class="ko">病理弯曲</em>。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/3b35485d57ca071e042581e56932677a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JbNREzP23UDRtUyY4FrDIg.jpeg"/></div></div></figure><p id="00a6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有些人可能会说，“为什么你不降低学习速度呢？”</p><p id="0dd3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嗯，当您接近最小值时，这是有意义的，但是想想这样一个情况，您处于病理曲率中，要达到最小值，您还有很长的距离要走。这是一些动力有所帮助的地方。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/db282d880d99e57e4d979ed458c9e1d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fqE8GYHh7qzWaOHQTwVdmw.jpeg"/></div></div></figure><p id="36fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当梯度下降到达峡谷中段的某个点时，动量技术有助于识别最近的导数，从而以这种方式增强梯度下降的方向。在上图中，注意到每个梯度更新已经被分解为沿<em class="ko"> w1 </em>和<em class="ko"> w2 </em>方向的分量。如果我们将这些向量单独求和，它们沿<em class="ko"> w1 </em>方向的分量抵消，而沿<em class="ko"> w2 </em>方向的分量增强。在一次更新中，这被认为是<em class="ko"> w2 </em>的方向被增强，而<em class="ko"> w1 </em>分量被置零，导致更快地向最小值移动。</p><p id="5acc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在实践中，动量的值通常初始化在0.5左右，然后慢慢退火到0.9，并更接近于0.9。我也将张贴关于动量的实际模拟的文章。</p><h2 id="6ee2" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">结论</h2><p id="994a" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">动量背后的基本思想是通过在相关和最佳方向上加速梯度下降来减少收敛时间。这种技术用于各种类型的深度神经网络模型，其中必须减少噪声数据。要了解更多详细的数学知识，我建议你参考下面的文章</p><div class="ml mm ez fb mn mo"><a href="https://distill.pub/2017/momentum/" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab dw"><div class="mq ab mr cl cj ms"><h2 class="bd hj fi z dy mt ea eb mu ed ef hh bi translated">为什么动量真的有效</h2><div class="mv l"><h3 class="bd b fi z dy mt ea eb mu ed ef dx translated">这里有一个关于动量的流行故事:梯度下降是一个人走下山坡。他沿着最陡的路径…</h3></div><div class="mw l"><p class="bd b fp z dy mt ea eb mu ed ef dx translated">蒸馏. pub</p></div></div><div class="mx l"><div class="my l mz na nb mx nc io mo"/></div></div></a></div><h2 id="ba37" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">文章参考文献</h2><p id="934b" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">[1]<a class="ae ll" href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d" rel="noopener" target="_blank">https://towards data science . com/random-gradient-descent-with-momentum-a 84097641 a 5d</a></p><p id="92ec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[2]<a class="ae ll" href="https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/" rel="noopener ugc nofollow" target="_blank">https://blog . paper space . com/intro-to-optimization-momentum-rms prop-Adam/</a></p></div></div>    
</body>
</html>