<html>
<head>
<title>Incremental Online Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">增量在线学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/incremental-online-learning-9868861db880?source=collection_archive---------1-----------------------#2019-08-25">https://medium.com/analytics-vidhya/incremental-online-learning-9868861db880?source=collection_archive---------1-----------------------#2019-08-25</a></blockquote><div><div class="dt gx gy gz ha hb"/><div class="hc hd he hf hg"><figure class="hi hj fa fc hk hl es et paragraph-image"><div role="button" tabindex="0" class="hm hn di ho bf hp"><div class="es et hh"><img src="../Images/89b02ab744a682252ec8fc72994b8e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ya7OfSq8ydyRoVRr"/></div></div><figcaption class="hs ht eu es et hu hv bd b be z dy translated">照片由<a class="ae hw" href="https://unsplash.com/@reigraphy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Rei Kim </a>在<a class="ae hw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><div class=""/><p id="3a41" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia">摘要</strong></p><p id="a8a5" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">近年来，与完全数据可用性的传统假设相比，增量式和在线机器学习受到越来越多的关注，特别是在从实时数据流中学习的情况下。虽然有各种不同的方法可用，但通常仍然不清楚哪种方法适合于具体的任务，以及它们之间的比较表现如何。本文回顾了代表不同算法类的八种流行的增量方法。</p><p id="1e15" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia">简介</strong></p><p id="9f6e" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在我的<a class="ae hw" rel="noopener" href="/danny-butvinik/https-medium-com-dannybutvinik-online-machine-learning-842b1e999880?source=friends_link&amp;sk=b4c6815d2fe95263dd4f80aeea1ea258">上一篇文章</a>里，我讲的是在线机器学习入门。在这篇文章中，我将概述一些在线增量学习算法(或基于实例的增量学习)，也就是说，模型在每个实例到来时都进行学习。传统的批处理机器学习方法是同时访问所有数据，这种方法无法满足在给定时间内处理海量数据的要求，导致越来越多的累积数据无法处理。此外，它们不会不断地将新信息集成到已经构建好的模型中，而是有规律地从头开始重建新模型。这不仅非常耗时，而且可能导致模型过时。</p><p id="1294" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">要克服这种状况，需要在流式方案中实现向顺序数据处理的范式转变。这不仅允许我们在信息可用时立即使用信息，从而得到最新的模型，而且还降低了数据存储和维护的成本。</p><p id="148a" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">增量和在线算法自然适合这种方案，因为它们不断地将信息合并到它们的模型中，并且传统上以最小化处理时间和空间为目标。由于其连续大规模和实时处理的能力，它们最近获得了更多的关注，特别是在数据流的情况下。</p><p id="494f" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">增量算法也非常适合生产阶段以外的学习，这使得设备能够适应个别客户的习惯和环境。这对于金融机构来说尤其有趣，因为欺诈检测是金融犯罪的中心目的之一。<strong class="iy ia"> NICE Actimize </strong> —全球领先的犯罪监控、管理和预防公司，致力于实时检测欺诈行为。这是一个有数据流的时间序列环境。处理这样的任务是非常困难的，尤其是当你有高度不平衡的数据和与事实不符的问题时。只有极小比例的数据被认为是欺诈性的。其余的被认为是合法的，但不能保证合法数据中没有欺诈。这就是为什么试图用在线增量方法解决这个问题会改变这个领域的游戏规则。</p><p id="63e7" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">这里的主要挑战不是大规模处理，而是从少量数据中进行连续有效的学习。尽管在这种情况下，增量学习可以被云中的重复批量学习所取代，但这种解决方案有着严重的缺陷。需要与云的永久连接来提供随时模型，这并不总是可行的。此外，由于隐私原因，客户可能不愿意提供关于他们日常生活的数据。因此，以有效的方式在设备上直接学习仍然是非常可取的。文献中关于增量学习和在线学习的定义有很多模糊之处。一些作者将它们互换使用，而另一些作者以不同的方式区分它们。额外的术语，如终身学习或进化学习，也被用作同义词。我们将增量学习算法定义为在给定的训练数据流上生成的算法</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div role="button" tabindex="0" class="hm hn di ho bf hp"><div class="es et ju"><img src="../Images/515e9c2a1ac29fe0872478d9a0965817.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*ieQQbSNX3JkL0wVJ4GIxjA.jpeg"/></div></div></figure><p id="47ab" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">一系列模型</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et jz"><img src="../Images/2d59d264fb9cd581a1403c30017017d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*XyT5968rh6FCepl_8vt3ww.jpeg"/></div></figure><p id="a1b3" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在我们的例子中，</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et ka"><img src="../Images/fe18dd1b14dc4f99ec02b2d9a2e48afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*eWtKeO2oObcpQ5-Mg7RPgg.jpeg"/></div></figure><p id="56c6" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">和</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et kb"><img src="../Images/fe351173c22cb49472b0e905abbd0025.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*bnJufnf2OeCKUXHfz2UnIw.jpeg"/></div></figure><p id="5a55" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">一个模型函数仅仅依赖于</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et kc"><img src="../Images/98afc4d937de6950d76120049fc2147d.png" data-original-src="https://miro.medium.com/v2/resize:fit:134/format:webp/1*USqmzsl2j3SelSzK_GtZ9Q.jpeg"/></div></figure><p id="482e" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">还有最近的<strong class="iy ia"> <em class="kd"> p </em> </strong>的例子</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et ke"><img src="../Images/16c41f5626f1dcd3420f613c08e3e0f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:250/format:webp/1*lQXYB5Dq0aleHxghK-HtmQ.jpeg"/></div></figure><p id="504a" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">其中<strong class="iy ia"> <em class="kd"> p </em> </strong>受到严格限制。我们将在线学习算法指定为增量学习算法，其在模型复杂性和运行时间方面额外受限，能够在具有有限资源的设备上进行无止境/终身学习。</p><p id="d376" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">增量学习算法面临以下挑战:</p><ul class=""><li id="db95" class="kf kg hz iy b iz ja jd je jh kh jl ki jp kj jt kk kl km kn bi translated">该模型必须逐渐适应，即在没有完全再训练的基础上构建。</li><li id="c5d2" class="kf kg hz iy b iz ko jd kp jh kq jl kr jp ks jt kk kl km kn bi translated">保存先前获得的知识，并且没有<strong class="iy ia">灾难性遗忘</strong>的影响。</li><li id="d48f" class="kf kg hz iy b iz ko jd kp jh kq jl kr jp ks jt kk kl km kn bi translated">只允许维护有限数量的p训练示例。</li></ul><p id="e575" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">我们明确地假设数据被标记，但并不关注从未标记或部分标记的数据流中学习的关键场景。监督增量学习的设置可以应用于大多数预测场景。在这些情况下，在系统已经做出预测之后，真实的标签通常可以在一些延迟后被推断出来。</p><p id="71bb" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">必须根据给定任务的前提条件来选择算法，因为不可能存在在每种情况下都最佳执行的方法。到目前为止，已经发表了不同的有趣的增量学习算法，它们具有各种优点和缺点。然而，只有少数来源提供了关于它们的信息，因为基本上没有比较性的深入研究，根据最相关的标准对最流行的方法进行实验比较。文献中的广泛研究通常会导致所考虑的算法的原始出版物。在本文中，我分析了一些在线增量方法的核心特性。</p><p id="f242" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">我的重点在于在线增量算法的监督学习下的分类。我主要在静态数据集上执行评估(即假设流</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et kt"><img src="../Images/053869e5c4762b65cce84dda9e8339de.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*_NtvtFdS7YXcWxVQKmB_0Q.jpeg"/></div></figure><p id="9d8c" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">是独立同分布的)。然而，我在概念漂移的背景下对这些方法进行了简单的评估和讨论。</p><p id="a651" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia">算法</strong></p><p id="fcad" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia">增量支持向量机(ISVM) </strong>是SVM最流行的精确增量版本，于(<a class="ae hw" rel="noopener" href="/danny-butvinik/incremental-learning-with-support-vector-machines-isvm-7d1c41a394bc?source=friends_link&amp;sk=61fbda25a485031e53ac1c0c7b6b31e8">参见我在这里写的关于ISVM的文章</a>)推出。此外，对于该组支持向量，维护有限数量的示例，即所谓的“候选向量”。这些示例可以根据将来的示例提升为支持向量。候选向量集越小，丢失潜在支持向量的概率越高。如果候选向量集包含所有之前看到的数据，ISVM是一种无损算法，它会生成与相应的批处理算法相同的模型。</p><p id="203c" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">LASVM是一个在线近似SVM 解算器，在中提出。在另一种方式中，它检查当前处理的例子是否是支持向量，然后移除过时的支持向量。对于这两个步骤，它大量利用顺序方向搜索，因为它也是在顺序最小优化(SMO)算法中完成的。与ISVM相反，它不维护一组候选向量，而是仅将当前示例视为可能的支持向量。这导致一个近似的解决方案，但大大减少了训练时间。</p><p id="2130" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia">在线随机森林(ORF) </strong>是随机森林算法的增量版本。每当在一片叶子内收集到足够的样本时，通过添加分裂，预定数量的树就连续增长。不是计算局部最优分裂，而是根据极端随机树的方案测试预定数量的随机值。选择最能优化基尼系数的分割值。树集成由于其高准确性、简单性和并行化能力而非常受欢迎。此外，它们对特征缩放不敏感，并且可以容易地在实践中应用。</p><p id="9257" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia">增量学习矢量量化(ILVQ) </strong>是静态广义学习矢量量化(GLVQ)对动态增长模型的适应，在必要时插入新的原型。插入率由错误分类样本的数量决定。我们使用其中引入了原型放置策略的版本，该策略最小化了最近样本的滑动窗口上的损失。度量学习，如[38，39]中所述，也可用于进一步扩展分类能力。</p><p id="41ae" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia"> Learn++ (LPPCART) </strong>以预定义的大小分块处理传入的样本。对于每个组块，训练基本分类器的集合，并通过加权多数投票组合成“集合的集合”。与AdaBoost [41]算法类似，每个分类器都使用根据分布绘制的块样本子集进行训练，确保误分类输入的样本概率更高。LPP是一种与模型无关的算法，几种不同的基分类器如SVM、分类回归树(CART)和多层感知器已经被作者成功地应用。作为原作者，我们使用流行的CART作为基本分类器。基于块的训练模型固有地结合了取决于块大小的自适应延迟。这种算法最近被使用。</p><p id="b704" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia">增量极限学习机(IELM) </strong>将批量ELM最小二乘解重构为顺序方案。作为批处理版本，它通过随机化输入权重大大降低了训练的复杂性。网络是静态的，隐藏神经元的数量必须预先确定。这种方法能够逐个或分块处理数据，从而显著减少了总的处理时间。然而，输出权重的有效初始化需要至少与所使用的隐藏神经元的数量一样多的例子。</p><p id="23dd" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia">朴素贝叶斯(NBGauss) </strong>拟合每个类别的一个轴平行高斯分布，并将其用作朴素贝叶斯算法中的似然估计。稀疏模型允许在处理时间和存储器需求方面非常有效的学习。该算法从少量训练样本中高效学习，并已成功应用于现实世界中，如垃圾邮件过滤和文档分类。这种无损算法的主要缺点是特征的独立性假设以及它不能处理多模态分布。</p><p id="662c" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia">随机梯度下降(SGDLin) </strong>是一种通过最小化损失函数(如铰链损失或逻辑损失)来学习判别模型的有效优化方法。我们使用SGD通过最小化铰链损失函数来学习线性模型。最近在大规模学习的背景下复兴的SGD与线性模型相结合，对于在文本分类或自然语言处理领域中经常遇到的稀疏、高维数据表现得特别好。然而，每当需要非线性类边界时，线性模型就不适合，对于低维数据尤其如此。</p><p id="8b79" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">尽管不断有新版本的算法被提出，我们认为所选择的方法反映了各自家族的一般性质。因此，本文的结论普遍适用于相应算法的当前和未来的变化。这一点在两种支持向量机中表现得尤为突出，两种支持向量机的性能非常相似，不同之处在于LASVM由于其近似的特性而能够处理稍大的数据集。然而，这两种方法在大数据集或噪声数据集方面都有相同的缺点。最近在中提出的LASVM版本也有这些缺点，尽管程度稍弱，因为引入了减少支持向量数量的机制。已经提出了LPP和IELM算法的各种扩展。他们中的大多数都是通过引入遗忘机制来处理非平稳环境。然而，本文的主要焦点是静态环境中的增量学习，在静态环境中，遗忘是相当有害的，并且会降低性能。</p><p id="ed5b" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">此外，算法的基本原理和相应的优缺点仍然存在。在LPP的例子中，一方面是任意基本分类器的灵活性，另一方面是跨组块的有限知识集成。加速SGD收敛的方法见。然而，在我们的实验中，SGD算法获得的结果并不是因为SGD算法收敛缓慢，而是突出了线性模型的一般优势和局限性，例如模型复杂度低和线性类边界。</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et ku"><img src="../Images/d5bdff141082aeea7bddec8e7b840c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-nBvRyTqRwIO6li_Uuc29Q.jpeg"/></div><figcaption class="hs ht eu es et hu hv bd b be z dy translated">图1:离线模式下评估批处理算法的经典方案</figcaption></figure><p id="c654" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">监督分类的学习目标是预测目标变量y ∈ {1，.。。，c}给定一组特征。我们考虑两个diﬀerent评估设置，它们允许关于算法性能的diﬀerent方面的推断，并且一起提供甚至更深的洞察力。</p><p id="2981" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia"> Oﬀline设定</strong></p><p id="84c4" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在oﬀline设置中，批处理算法基于训练集生成模型<strong class="iy ia"> <em class="kd"> h </em> </strong></p><figure class="jv jw jx jy fe hl es et paragraph-image"><div role="button" tabindex="0" class="hm hn di ho bf hp"><div class="es et kv"><img src="../Images/b3e3c6570c2c118e686110f6f3463757.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*BWQkcwbPLUIDsoGKmq15Yw.jpeg"/></div></div></figure><p id="fdf6" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在随后的测试阶段，该模型被应用于另一组</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div role="button" tabindex="0" class="hm hn di ho bf hp"><div class="es et kw"><img src="../Images/6733e82990ede3a2265f3656689e66f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*T4paxOktQ-r9MoVw-1iD-g.jpeg"/></div></div></figure><p id="ed58" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">其标签是隐藏的。图1描述了这个过程。该模型预测一个标签</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et kx"><img src="../Images/df9401e9c362c784c8143f4f83a98f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*4NRp1r0iy4R3wJMJcbNWLg.jpeg"/></div></figure><p id="46ad" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">对于测试集中的每个点以及每个点的0–1损失</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et ky"><img src="../Images/86ab2ca19209eccc47ebad48087aba79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*ORh8YDPVDP5kw7RU1VuQ4w.jpeg"/></div></figure><p id="f5d3" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">是计算出来的。测试集上的平均准确度使得能够根据对看不见的例子的概括能力进行分析。</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div role="button" tabindex="0" class="hm hn di ho bf hp"><div class="es et kz"><img src="../Images/298f858bde1dc59e8a05922b0ae302c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1dGwHLgsJY2O3A_pOyCkuA.jpeg"/></div></div><figcaption class="hs ht eu es et hu hv bd b be z dy translated">图2:在oﬀ-line环境中测试增量算法的过程。值得注意的是，只有最后构建的模型用于预测。训练集中使用的所有数据都是从D训练集中获得的。</figcaption></figure><p id="6a4b" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在这种设置下，增量算法的评估是diﬀerent，如图2所示。它不是一次性访问所有训练数据，而是按照预先定义的顺序依次处理。该算法生成元组序列</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et la"><img src="../Images/4466d17ebae644184cbec5544a603f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*f_LUvDOuAFlGa98uKxlblQ.jpeg"/></div></figure><p id="a152" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">相应的模型序列</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et lb"><img src="../Images/12a2cb4d4581a3858e1c5622ef7957b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:202/format:webp/1*KyngUBDoYVd2LgPDj12i1g.jpeg"/></div></figure><p id="2003" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">因此，一个模型</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et lc"><img src="../Images/7be92f4338d6872cb72c500c7eb214b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:72/format:webp/1*cEm5eBuPYtm40szriJPskQ.jpeg"/></div></figure><p id="fda6" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">完全基于先前构建的模型和有限数量的<strong class="iy ia"> <em class="kd"> p </em> </strong>最近元组</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et ld"><img src="../Images/385b408284d126c50ccd51ec5fc05dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*IL8hysW5pVjvhp9tDSmEcQ.jpeg"/></div></figure><p id="3080" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">只有最后一个模型<strong class="iy ia"> <em class="kd"> hj </em> </strong>被应用于测试集以确定oﬀline精度ξ</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et le"><img src="../Images/25f4b1600c06cf91676f13b2071bd871.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*9w6PODaDvSObOCH85OhcDw.jpeg"/></div></figure><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et lf"><img src="../Images/505967c9002259a4d638918ede05215b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*Fs6zb3wl4QnglDX-VcreaA.jpeg"/></div><figcaption class="hs ht eu es et hu hv bd b be z dy translated">图3:在线学习计划。数据没有分成训练集和测试集。相反，每个模型随后预测一个示例，该示例随后用于构建下一个模型。</figcaption></figure><p id="b16d" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">因此，该设置只允许推断最后一个模型的泛化能力，而忽略所有前面的模型。这种评估在数据流方案中非常有用，例如，在数据流方案中，有大量的训练数据可用于持续地构建尽可能准确的模型。</p><p id="77c2" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated"><strong class="iy ia">在线设置</strong></p><p id="5bcf" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">数据流分类通常在在线设置中进行评估，如图3所示。潜在的无限序列</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et lg"><img src="../Images/450b532e4ddb545f53aa18e0a1382ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*3sJwvCIyskv-nrvznCgKLA.jpeg"/></div></figure><p id="9a30" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">元组的数量</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et lh"><img src="../Images/d78b2a756964c133f628a256d428961e.png" data-original-src="https://miro.medium.com/v2/resize:fit:214/format:webp/1*R8XVBECDmXSiOK2rCr8BrA.jpeg"/></div></figure><p id="0b71" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">一个接一个到达。由于<strong class="iy ia"> <em class="kd"> t </em> </strong>代表当前时间戳，学习的目的是预测相应的标签</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et li"><img src="../Images/dd4d81e4abdbef2cececa5415ceaafb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:84/format:webp/1*HTD3B4YUUQw9sSA5I8hAvg.jpeg"/></div></figure><p id="32cb" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">对于给定的输入</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et lj"><img src="../Images/6cea1b9ca9601ab41ee37f33ec990a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:90/format:webp/1*yaakbgMy_AnmVlQu9xky4w.jpeg"/></div></figure><p id="dab9" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">这应该是未知的。预测</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et lk"><img src="../Images/fdfa922bc7dccf8bbf7b078ec48b0a70.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*mMczbU9XPnVuX6HCqLSAoQ.jpeg"/></div></figure><p id="20fc" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">是根据先前学习的模型完成的</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et ll"><img src="../Images/91398f5207fc614d45c1ddb8d3200051.png" data-original-src="https://miro.medium.com/v2/resize:fit:140/format:webp/1*2emhFKj8Vz6EDvpsJ-wr1w.jpeg"/></div></figure><p id="bbda" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">后来，真正的标签被揭露和丢失</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et lm"><img src="../Images/b467b3a251f637e65bdfc229250a0151.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*zZvXTZDWbxjUqDOmCbEWEA.jpeg"/></div></figure><p id="e43c" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">下定决心。到当前时间<strong class="iy ia"> <em class="kd"> t </em> </strong>的序列的在线精度由下式给出</p><figure class="jv jw jx jy fe hl es et paragraph-image"><div class="es et ln"><img src="../Images/ac455aec313b68bdacfa65bf2a1702a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*ofC7cVWKJDV47XRUSaU4ZA.jpeg"/></div></figure><p id="a958" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">对先前设置的主要diﬀerence是所有中间模型都被考虑用于性能评估，但是它们中的每一个都只预测下面的例子。此外，用于训练和测试的数据不是严格分离的，而是每个实例最初用于模型测试，然后用于适应。</p><p id="7d1d" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">对于非平稳数据，高的在线精度不一定意味着模型的高泛化能力。例如，在标签的强自相关的情况下，简单地预测前一标签的算法实现了准确的结果，而无需学习数据中的任何结构。然而，对于i.i.d .数据，增量算法的在线精度通常与所有构建模型的平均概括能力相关。在线准确度是一个合理的评估标准，用于需要即时预测的任务，即使在少量训练样本之后。</p><p id="d78c" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">oﬀ-和在线两种精度的组合使得能够得出关于学习曲线的结论:在两个diﬀerent模型<em class="kd"> A </em>、<strong class="iy ia">T3】bt5】具有相同的oﬀline精度，但是<strong class="iy ia"> <em class="kd"> A </em> </strong>具有更高的在线精度的情况下，意味着<strong class="iy ia"><em class="kd"/></strong>平均比<strong class="iy ia"> <em class="kd"> B </em> </strong>收敛得更快，反之亦然。</strong></p><p id="58fa" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">因为这篇文章是对在线增量学习最流行的方法的一种概述，所以下一个系列的文章将更多地关注特定选择的算法。</p><p id="f68a" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">参考</p><p id="8e5c" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">[1] Viktor Losing等人，2018《增量在线学习:最先进算法的回顾与比较》。</p><p id="e85b" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">[2] R. Ade等人艾尔。，2013“增量学习方法:调查，国际。</p><p id="f1f3" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">[3] R .埃尔韦尔等人艾尔。，2011“非稳定环境中概念漂移的增量学习”。</p><p id="c972" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">[4] G. Ditzler等人，2013“从流不平衡数据中增量学习概念漂移。</p><p id="34c4" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">[5] Y. Ye等，2013《在线顺序极限学习机</p><p id="45a6" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">在不稳定的环境中。</p><p id="c3d6" class="pw-post-body-paragraph iw ix hz iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hc bi translated">[6] H. He等，2011“流数据的增量学习。</p></div></div>    
</body>
</html>