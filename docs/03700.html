<html>
<head>
<title>Deploying AI at the Edge with Intel OpenVINO- Part 3 (final part)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用英特尔OpenVINO在边缘部署人工智能-第3部分(最后一部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deploying-ai-at-the-edge-with-intel-openvino-part-3-final-part-664b92c75fde?source=collection_archive---------12-----------------------#2020-02-14">https://medium.com/analytics-vidhya/deploying-ai-at-the-edge-with-intel-openvino-part-3-final-part-664b92c75fde?source=collection_archive---------12-----------------------#2020-02-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/622b9a5635091fca341ba12cd76501a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D5GPm-kNpPEcwodcqxG9FA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">约书亚·梅洛在<a class="ae iu" href="https://unsplash.com/s/photos/engine?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="6a3a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我以前的帖子中，我已经介绍了OpenVINO，描述了如何在windows计算机中安装它，如何处理输入和输出，以及如何在model optimizer中获取或准备模型。我们现在处于最后一步，用推理引擎执行推理。让我们开始吧。这篇文章讨论的主题是，</p><ul class=""><li id="003f" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">推理机</li><li id="849f" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">向推理机提供模型</li><li id="f59a" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">检查不支持的层并使用CPU扩展(不推荐)</li><li id="fa3d" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">发送推理请求</li><li id="2540" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">处理输出</li><li id="8055" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">集成到应用程序中</li></ul><h1 id="885a" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">推理机</h1><p id="9592" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">推理引擎在模型上运行实际的推理。在<a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-1-51a09752fb4e">第1部分</a>中，我们从OpenVINO模型动物园下载了一个预训练模型，在<a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-2-1f1a9faa514b">第2部分</a>中，我们通过模型优化器将一些模型转换成IR格式。推理机只处理这种中间表示。在第2部分中，我们已经看到了模型优化器如何通过改进规模和复杂性来帮助优化模型。推理引擎提供进一步的基于硬件的优化，以确保使用尽可能少的硬件资源。因此，它有助于在物联网设备的边缘部署人工智能。</p><p id="5b26" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了更好地与硬件通信，推理机建立在C++之上。所以你可以直接在你的C++应用中使用引擎。还有一个python包装器可以在python代码中使用引擎。在这篇文章中，我们将使用python。</p><h1 id="db85" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">如何使用推理引擎</h1><p id="a7e6" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">以下是从头到尾使用推理引擎的步骤，</p><blockquote class="lk"><p id="bc46" class="ll lm hi bd ln lo lp lq lr ls lt js dx translated"><em class="lu">馈送模型&gt;检查任何不支持的层&gt;发送推理请求&gt;处理结果&gt;与您的应用程序集成</em></p></blockquote><p id="1cd1" class="pw-post-body-paragraph iv iw hi ix b iy lv ja jb jc lw je jf jg lx ji jj jk ly jm jn jo lz jq jr js hb bi translated">现在我们将看到每一步我们需要做的细节工作。在<a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-1-51a09752fb4e">第1部分</a>中，我们创建了两个python文件，一个名为<em class="ma"> app.py </em>，另一个名为<em class="ma"> inference.py </em>。我们将从那里继续代码。</p><h2 id="88f8" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">给模特喂食</h2><figure class="mq mr ms mt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/f8339c4ffb3c8d001f4520294b4d784b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xRLxuomgJ-0KgburxA0oyQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在<a class="ae iu" href="https://unsplash.com/s/photos/feed-bird?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae iu" href="https://unsplash.com/@tmokuenko?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Thibault Mokuenko </a>拍摄的照片</figcaption></figure><p id="3285" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们需要使用来自<em class="ma">“open vino . inference _ engine”</em>库中的两个python类。分别是<em class="ma">I core</em>和<em class="ma">I network</em>。<a class="ae iu" href="https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1IECore.html" rel="noopener ugc nofollow" target="_blank"> <em class="ma"> IECore </em> </a>和<a class="ae iu" href="https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1IENetwork.html" rel="noopener ugc nofollow" target="_blank"> <em class="ma"> IENetwork </em> </a>的文档对于使用类中的不同方法会非常有帮助，所以请查阅它们。</p><p id="b496" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ma">I network</em>保存从IR读取的关于模型网络的信息，并允许进一步修改网络。在必要的处理之后，它将网络提供给IECore，后者创建一个<a class="ae iu" href="https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1ExecutableNetwork.html" rel="noopener ugc nofollow" target="_blank">可执行网络</a>。</p><p id="9500" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将从在我们的<em class="ma">推论. py </em>文件中导入必要的库开始。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="82f2" class="mb ki hi mv b fi mz na l nb nc">from openvino.inference_engine import IECore<br/>from openvino.inference_engine import IENetwork</span></pre><p id="aa5c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我将创建一个名为<em class="ma">“load _ to _ IE”</em>的函数，它将接受一个参数model(models的位置<em class="ma"> *)。xml </em>文件),从中我们还将获得<em class="ma"> *的位置。bin </em>文件。此外，还有另一个名为“cpu_ext”的变量，我将在后面解释(例如，我正在使用我在<a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-1-51a09752fb4e">第1部分</a>中使用的同一人脸检测模型)。</p><p id="b94f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(<strong class="ix hj"> <em class="ma">更新:</em> </strong> <em class="ma">自OpenVINO 2020起，不再需要cpu扩展。如果您使用的是更高版本，请忽略所有关于cpu扩展的内容。我把它放在这里，以防有人正在使用旧版本，需要它作为参考)</em></p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="af86" class="mb ki hi mv b fi mz na l nb nc">cpu_ext_dll ="C:/Program Files (x86)/IntelSWTools/openvino_2019.3.379/deployment_tools/inference_engine/bin/intel64/Release/cpu_extension_avx2.dll"</span></pre><p id="7c42" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们定义这个函数。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="6ead" class="mb ki hi mv b fi mz na l nb nc">def load_to_IE(model):<br/>    # Getting the *.bin file location<br/>    model_bin = model[:-3]+"bin"</span><span id="dd31" class="mb ki hi mv b fi nd na l nb nc">    #Loading the Inference Engine API<br/>    ie = IECore()<br/>    <br/>    #Loading IR files<br/>    net = IENetwork(model=model_xml, weights = model_bin)</span></pre><p id="090a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(<strong class="ix hj"> <em class="ma">更新:</em> </strong> <em class="ma">像这样直接初始化IENetwork对象是不赞成的。使用下面的代码)</em></p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="24e7" class="mb ki hi mv b fi mz na l nb nc">def load_to_IE(model):<br/>    ....<br/>    net = ie.read_network(model=model, weights = model_bin)</span></pre><h2 id="65e4" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">检查不受支持的图层</h2><p id="382f" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">(<strong class="ix hj"> <em class="ma">更新:</em> </strong> <em class="ma">不适用于OpenVINO 2020及以后版本)</em></p><figure class="mq mr ms mt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/b45b4e65ae7d515ce2ccb3bb8dd9b111.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F6Iul2oo60-eNryMs0HR0w.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">兰迪·法特在<a class="ae iu" href="https://unsplash.com/s/photos/puzzle-piece?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="2374" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">即使在成功地将模型转换为IR后，CPU可能仍然不支持某些层。在这种情况下，我们可以使用CPU扩展文件来支持推理引擎中那些不受支持的层。在不同的操作系统中，CPU扩展文件的位置略有不同。例如，我找到了我的CPU扩展<em class="ma"> *。该位置的dll </em>文件:<em class="ma"> &lt;安装_目录&gt;\ open vino _ 2019 . 3 . 379 \ deployment _ tools \ inference _ engine \ bin \ Intel 64 \ Release。</em></p><p id="dbf0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">文件命名为<em class="ma">“CPU _ extension _ av x2 . dll”</em>。在linux中有几个扩展文件，而在Mac中只有一个。</p><p id="2ba5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">并非所有型号都需要CPU扩展。因此，首先我们将检查我们的模型是否需要它。继续“load_to_IE”函数内的代码。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="516e" class="mb ki hi mv b fi mz na l nb nc"># Listing all the layers and supported layers<br/>    cpu_extension_needed = False<br/>    network_layers = net.layers.keys()<br/>    supported_layer_map = ie.query_network(network=net,device_name="CPU")<br/>    supported_layers = supported_layer_map.keys()</span></pre><p id="2fe0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我解释一下代码。首先，我设置了一个标志，表明不需要cpu扩展。然后，我在“network_layers”变量中列出我们网络中所有层的名称。然后，我使用IECore类的<em class="ma">“query _ network”</em>方法，该方法返回当前配置中支持的层的字典。通过从字典中提取键，我创建了一个支持层的列表，并将其存储在“supported_layers”变量中。</p><p id="9cdc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我将遍历当前网络中的所有层，并检查它们是否属于受支持的层列表。如果所有层都出现在受支持的层中，则不需要cpu扩展，否则，我们将把我们的标志设置为false，并继续添加CPU扩展。在“load_to_IE”函数中继续下面的代码。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="9253" class="mb ki hi mv b fi mz na l nb nc"># Checking if CPU extension is needed   <br/>    for layer in network_layers:<br/>        if layer in supported_layers:<br/>            pass<br/>        else:<br/>            cpu_extension_needed =True<br/>            print("CPU extension needed")<br/>            break</span></pre><p id="6dcb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用IECore类的“add_extension”方法来添加扩展。继续相同函数中的代码。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="5f39" class="mb ki hi mv b fi mz na l nb nc"># Adding CPU extension<br/>    if cpu_extension_needed:<br/>        ie.add_extension(extension_path=cpu_ext, device_name="CPU")<br/>        print("CPU extension added")<br/>    else:<br/>        print("CPU extension not needed")</span></pre><p id="afbf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了安全起见，我们将再次使用相同的代码来查看在添加CPU扩展之后，现在是否支持所有的层。如果现在所有的层都被支持，我们可以进入下一步。但如果没有，我们将退出程序。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="8567" class="mb ki hi mv b fi mz na l nb nc">#Getting the supported layers of the network  <br/>    supported_layer_map = ie.query_network(network=net, device_name="CPU")<br/>    supported_layers = supported_layer_map.keys()<br/>    <br/># Checking for any unsupported layers, if yes, exit<br/>    unsupported_layer_exists = False<br/>    network_layers = net.layers.keys()<br/>    for layer in network_layers:<br/>        if layer in supported_layers:<br/>            pass<br/>        else:<br/>            print(layer +' : Still Unsupported')<br/>            unsupported_layer_exists = True<br/>    if unsupported_layer_exists:<br/>        print("Exiting the program.")<br/>        exit(1)</span></pre><p id="c76a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">既然我们确定所有的层都被支持，我们将把网络加载到我们的推理引擎中。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="0eab" class="mb ki hi mv b fi mz na l nb nc"># Loading the network to the inference engine<br/>    exec_net = ie.load_network(network=net, device_name="CPU")<br/>    print("IR successfully loaded into Inference Engine.")</span><span id="3fce" class="mb ki hi mv b fi nd na l nb nc">    return exec_net   #exec_net short for executable network</span></pre><h2 id="55c9" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">发送推理请求</h2><figure class="mq mr ms mt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nf"><img src="../Images/908e8796c5e019b0ed7134eae8a1f889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*592DRQTJysVPZMV1tSlSTg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://unsplash.com/@bradencollum?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">布拉登·科拉姆</a>在<a class="ae iu" href="https://unsplash.com/s/photos/start?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="753c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将把我们的推理请求发送到由我们的“load_to_IE”函数返回的可执行网络“exec_net”。有两种推断方法，<em class="ma">同步</em>和<em class="ma">异步。</em>在同步方法中，应用程序向引擎发送推理请求，除了等待推理请求完成之外什么也不做。另一方面，在异步方法中，当推理机执行推理时，应用程序可以继续其他工作。如果推理请求处理由于某种原因很慢，并且我们不希望我们的应用程序在推理完成时挂起，这是很有帮助的。因此，在异步方法中，当处理一帧上的推理请求时，应用程序可以继续收集下一帧并对其进行预处理，而不是在同步方法的情况下被冻结。</p><p id="5385" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们定义两个函数。一个函数将执行同步推理，另一个函数将执行异步推理。名为<em class="ma">“sync _ inference”</em>的同步函数将可执行网络和预处理后的图像作为参数。名为<em class="ma">“async _ inference”</em>的异步推理函数将再接受一个额外的参数<em class="ma">“request _ id”</em>，我将其默认为0，因为在本例中，我们将只发送一个请求，所以我们不需要为id而烦恼。当您使用这种方法使您的应用程序真正“异步”时(在我们的例子中，我们实际上是在等待IE完成推理，因为在我们简单的演示应用程序中没有其他事情要做)，您可能会在引擎完成对一个图像的推理之前，用不同的请求id一个接一个地输入几个图像。当引擎完成推理时，您将使用这些id提取相应的结果。因此，请确保在您的真实应用程序中为您的图像分配唯一的id。你可以阅读<a class="ae iu" href="https://docs.openvinotoolkit.org/latest/ie_python_api/classie__api_1_1ExecutableNetwork.html#a314107a6b50f0ebf65751569973c760a" rel="noopener ugc nofollow" target="_blank">文档</a>来了解更多关于异步方法的信息。处理这两种方法的输出是不同的。对于同步方法，我们可以直接返回结果。但是对于异步方法，我们将返回一个处理程序(由<em class="ma"> start_async </em>方法返回)。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="980a" class="mb ki hi mv b fi mz na l nb nc">def sync_inference(exec_net, image):<br/>    input_key = list(exec_net.input_info.keys())[0]<br/>    output_key = list(exec_net.outputs.keys())[0]<br/>    result = exec_net.infer({input_key: image})<br/>    print('Sync inference successful')<br/>    return result[output_key]</span><span id="ac0e" class="mb ki hi mv b fi nd na l nb nc">def async_inference(exec_net, image, request_id=0):<br/>    input_key = list(exec_net.input_info.keys())[0]<br/>    return exec_net.start_async(request_id, inputs={input_blob: image})</span></pre><p id="96bd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(在第1部分中，我们首先打印从<em class="ma"> sync_inference </em>函数返回的‘result’字典的键，然后硬编码该键以提取结果。我稍微修改了一下代码，这样我们就不再需要硬编码了。不同的型号对输入和输出键使用不同的名称。因此，通过以编程方式提取键名，我们使应用程序更适用于只有一个输入和一个输出的不同模型)</p><p id="b8fb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当异步推断完成时，可以使用请求id或处理程序提取结果，因此需要更多的处理。</p><h2 id="050c" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">处理输出</h2><figure class="mq mr ms mt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ng"><img src="../Images/bd8532a3e12d9ed1aca9e2ef24d0e6af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SQoz6oB8rbFkNkJxjsf1Og.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">杰西·拉米雷斯在<a class="ae iu" href="https://unsplash.com/s/photos/package?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="7bb7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们已经看到，同步方法直接给了我们推论的结果。在<a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-1-51a09752fb4e">第1部分</a>中，我们已经使用了这个结果并处理了我们的输出来绘制边界框。当然，根据我们希望应用程序做什么，处理过程会有所不同。为了从异步方法中获得推理结果，我们将定义另一个函数，我命名为“<em class="ma"> get_async_output”。这个函数将接受一个参数。<em class="ma">“async _ inference”</em>返回的句柄。</em></p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="4d23" class="mb ki hi mv b fi mz na l nb nc">def get_async_output(async_handler):<br/>    status = async_handler.wait()<br/>    output_key = list(async_handler.output_blobs.keys())[0]<br/>    result = async_handler.output_blobs[output_key].buffer<br/>    print('Async inference successful')<br/>    return result</span></pre><p id="e636" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ma">“wait”</em>函数返回加工状态。如果我们用参数0调用函数，它将立即返回状态，即使处理没有完成。如果您只是检查结果是否完整，这是很有用的，如果不完整，您可以继续其他过程。</p><h2 id="95b1" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">与您的应用程序集成</h2><figure class="mq mr ms mt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/0dd9de8b1a621f0ccf701e4cc2978d53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XXI-kg18liPn4XcfZmoqQQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">克里斯·里德在<a class="ae iu" href="https://unsplash.com/s/photos/code-python?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="5e43" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们已经完成了我们的推理引擎。现在，我们将在非常简单的<em class="ma"> app.py </em> python文件中使用我们的函数。首先，我们需要将刚刚在<em class="ma">推论. py </em>文件中定义的函数导入到我们的<em class="ma"> app.py </em>文件中。然后，在<em class="ma">“main”(<a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-1-51a09752fb4e">第一部分</a>中定义的</em>)中，我们将相应地调用这些函数，以便在我们的应用程序中使用它们。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="0bf7" class="mb ki hi mv b fi mz na l nb nc">from inference import preprocessing, load_to_IE, sync_inference, async_inference, get_input_shape, get_async_output</span><span id="67ce" class="mb ki hi mv b fi nd na l nb nc">def main():</span><span id="2516" class="mb ki hi mv b fi nd na l nb nc">    #*..................................*#</span><span id="3e7c" class="mb ki hi mv b fi nd na l nb nc">    exec_net = load_to_IE(model, model_bin)<br/>    <br/>    # Synchronous method<br/>    result = sync_inference(exec_net, image = preprocessed_image)<br/>    #result = result['detection_out'], we are don't need this anymore</span><span id="7177" class="mb ki hi mv b fi nd na l nb nc">    #*..................................*#</span><span id="02bd" class="mb ki hi mv b fi nd na l nb nc">    # or, Async method<br/>    async_handler=async_inference(exec_net,image=preprocessed_image)<br/>    result_async=get_async_output(async_handler)</span></pre><h2 id="7a13" class="mb ki hi bd kj mc md me kn mf mg mh kr jg mi mj kv jk mk ml kz jo mm mn ld mo bi translated">奖金！</h2><p id="1451" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">还记得吗，我们在第一部分中对所需的图像尺寸进行了硬编码？我们不需要再这样做了。在我们的<em class="ma"> "inference.py" </em>文件中定义这个新函数。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="9b7a" class="mb ki hi mv b fi mz na l nb nc">def get_input_shape(net):<br/>    input_key = list(net.input_info.keys())[0]<br/>    input_shape = net.input_info[input_key].input_data.shape<br/>    return input_shape</span></pre><p id="cd5f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，导入<em class="ma"> app.py </em>文件中的函数。并且，在<em class="ma">【main】</em>函数内添加这一行，而不是硬编码高度和宽度。</p><pre class="mq mr ms mt fd mu mv mw mx aw my bi"><span id="04ec" class="mb ki hi mv b fi mz na l nb nc">from inference import get_input_shape</span><span id="8d6e" class="mb ki hi mv b fi nd na l nb nc">def main():<br/>    #*.............*#<br/>    n, c, h, w = get_input_shape(exec_net)<br/>    preprocessed_image = preprocessing(image, h, w)</span></pre><p id="5bec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我已经将完整的代码上传到了github库中。如果您发现代码的任何部分令人困惑，请检查完整的代码。</p><h1 id="ab28" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">结论</h1><p id="d92d" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">英特尔OpenVINO toolkit拥有无限可能。巨大的模型动物园为开发人员提供了他们所需要的一切，以利用尖端的人工智能技术构建强大的边缘应用程序，而不必太担心培训。OpenVINO还可以毫不费力地将人工智能带到你的低功耗设备中。即使在一个与英特尔NCS配对的小raspberry pi中，您也可以通过OpenVINO使用AI制作出令人惊叹的应用程序。你的想象力在这里是极限。我希望我的帖子已经帮助你开始使用OpenVINO并制作你的edge应用程序。我目前正在与一个名为<a class="ae iu" href="http://hessix.github.io" rel="noopener ugc nofollow" target="_blank"> Hessix </a>的非营利组织合作，在那里我们应用深度学习来解决社会问题。我计划很快在那个项目中使用OpenVINO。完成后，我可能会在那里发布一篇关于我如何使用OpenVINO的文章。所以下次见，享受编码吧。</p><p id="6882" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(<em class="ma">链接到所有零件:</em> <a class="ae iu" rel="noopener" href="/@ahsan.shihab2/ai-at-the-edge-an-introduction-to-intel-openvino-toolkit-a0c0594a731c"> <em class="ma">零件0</em></a><em class="ma">&gt;</em><a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-1-51a09752fb4e"><em class="ma">零件1</em></a><em class="ma">&gt;</em><a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-2-1f1a9faa514b"><em class="ma">零件2</em></a><em class="ma">&gt;</em><a class="ae iu" rel="noopener" href="/@ahsan.shihab2/deploying-ai-at-the-edge-with-intel-openvino-part-3-final-part-664b92c75fde"><em class="ma">零件3 </em> </a> <em class="ma"> ) </em></p></div></div>    
</body>
</html>