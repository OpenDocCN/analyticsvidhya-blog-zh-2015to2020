<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-with-q-learning-implementation-in-python-ce9332b4438c?source=collection_archive---------16-----------------------#2020-05-27">https://medium.com/analytics-vidhya/reinforcement-learning-with-q-learning-implementation-in-python-ce9332b4438c?source=collection_archive---------16-----------------------#2020-05-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><p id="a913" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">除了有监督和无监督的学习算法，近年来人工智能最有趣和实践最多的领域之一包括强化学习。从制造机器人在现代游戏中与人类对抗，到数学计算中的协调运算研究，强化学习与其他学习算法一起发挥着至关重要的作用。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es if"><img src="../Images/e95b30337687e0e6f4a955af13059d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*mVmqPSO6Yr8Z4TV_AFtB4g.jpeg"/></div></figure><p id="0ac7" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">与监督学习被标记(结构化)的数据被呈现以采取最佳行动来使算法学习不同，强化学习更侧重于构建知识三角。从零先验知识开始，逐步增加，直到达到最大知识权重，以采取高回报的行动。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es in"><img src="../Images/4d4c5d7a24fc97323119a69f436b9fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j-CDObpecjP1_6SXzcV1vQ.jpeg"/></div></div></figure><p id="18ce" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">Q-Learning是一种强化学习算法，该算法学习一种策略，告诉代理在给定的环境下采取特定的行动。使用马尔可夫决策过程(MDP ), Q-Learning算法找到一个最优策略，以最大化从当前给定状态到下一个连续状态的单个奖励的数量，通过采取适当的行动产生最大的净总奖励。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es is"><img src="../Images/d392e6741811b09fdbb2eada988c548e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFP_Bn0aX4byRN0StYIuqg.jpeg"/></div></div></figure><p id="fbec" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(注意:每个行动都涉及一些与之相关的负成本，因此为了最大化净补偿，算法会用较少的步骤找到最佳路径)。</p><p id="604c" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">或许把Q学习算法组合成以下更简单的步骤:</p><p id="da74" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated"><strong class="hj it">第一步(初始化):</strong></p><p id="e60c" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">对于所有状态s和动作a，动作Q值被初始化为零:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es iu"><img src="../Images/b81766de6e10ab8c8278596c18780320.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*l3SR8Hx_KtR18yO1_D5PtQ.png"/></div></figure><p id="f9ad" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(a)我们从初始状态s0开始，</p><p id="d3f2" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(b)我们玩一个随机动作，</p><p id="e1d9" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(c)我们到达第一状态s1。</p><p id="abf2" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated"><strong class="hj it">步骤2(算法本身)</strong>:然后，对于每个t &gt; =1，我们在某些时期重复下面的步骤(这里我们实现1000个周期)</p><p id="79c9" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(a)我们从我们的可能状态中选择一个随机状态集</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es iv"><img src="../Images/06c86892ec235e0db5767b7f9950b367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*Hscl-PcF7rD7ULLvyR4tYg.png"/></div></figure><p id="33be" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(b)我们在at上玩一个任意的动作，可以导致下一个可能的状态，即，使得R(st，at)&gt;0:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es iw"><img src="../Images/1a2b3a619fbb6facd1affe27fb5d5e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O8vPJ7gimU9Id3D6gUeCQg.png"/></div></div></figure><p id="6162" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(c)我们到达下面的状态s(t+1)，我们得到奖励R (st，at)。</p><p id="1e95" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(D)我们计算时间差(T.D):</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es ix"><img src="../Images/c953f8ef784dcb5df12dc426bd0d3110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GH2GdVq5liHu56DW66EUsA.png"/></div></div></figure><p id="5aa7" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(e)我们应用贝尔曼方程更新Q值:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es iy"><img src="../Images/23838fcc8449f4a45e107df83200f9b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*CwZ0EuaGjKHiU2DLcYKBhQ.png"/></div></figure><p id="e4e8" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">现在让我们讨论简单的问题陈述，然后我们使用Python中的Q-Learning算法。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es iz"><img src="../Images/ba196dc57f8bc8220b7ed19c32f7950e.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*QzLIlr16qfqh3SkBPioMmg.png"/></div></figure><p id="1e0b" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">设A为我们启动机器人的初始状态，我们的最终目标是以最小的动作量到达状态D(从而找到最短的路线)。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es ja"><img src="../Images/31493a4854dec8648f58c5cc08b39597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*Z6700RmPxLYfzgGstzaoKw.png"/></div></figure><p id="4abe" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">现在让我们讨论起始状态下给定状态的奖励矩阵；对于机器人可以从给定状态移动到另一个状态的所有可能的物理路径，我们将奖励指定为一(1)，对于其余的，我们将奖励指定为零(0)。(尽管由于达到目标状态D是我们的机器人的动机，我们后来为D分配了更高的奖励，使其成为其优先级的最高值)。</p><p id="48f6" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">在实现Q-Learning时，我们遇到了一个术语叫做时间差异(T.D): <br/> T.D使Q-Learning算法能够根据其值更新自身。在更直接的意义上，T.D是从一个状态移动到另一个状态所获得的回报的总和，并且贴现因子乘以与动作相关的下一个状态的Q值和当前状态与其动作的Q值的最大值。<br/>最后，我们用前一状态的Q值和学习率(α)乘以所涉及的时间差来更新下一对应状态的Q值。<br/>因此时间差有助于防止Q值爆炸。<br/>超参数折现因子和学习率一般通过试错法获得。</p><p id="d339" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">Python中的实现:<br/>像所有其他机器学习程序一样，我们将NumPy作为np导入:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jb"><img src="../Images/afa3152556764029120c2cf76d593a38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UWXRYj1zfH9hMP0gfNOtkQ.png"/></div></div></figure><p id="8295" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">然后我们预定义两个参数:<br/> Gamma(贴现因子):<br/> Alpha(学习率):<br/>贴现因子:Q值在每个周期后递减的因子。<br/>学习速率:算法在每个周期后学习的速率。<br/>这里的循环是指算法从初始状态移动到最终状态的每个实例。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jc"><img src="../Images/4fa98434a036bfeb4b02906fb2f3bacf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eMb6h19Vq_uLHfKdb8XKpQ.png"/></div></div></figure><p id="7388" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">然后我们定义一个映射到索引和动作数组的状态字典。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jd"><img src="../Images/5a324f046e4fc99a9ff81795c17b87b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VSVHfP7LrqUxLlpdlGtBow.png"/></div></div></figure><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es je"><img src="../Images/e389c416b4d182f4a9965262f5649ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zo1gAtbM_xhqYQ7qEOCcWA.png"/></div></div></figure><p id="0633" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">然后为可能采取的行动定义奖励矩阵。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jf"><img src="../Images/1a18926e4859f746f0eb27353b568225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gFE4aB8mZAvYrWDnUe2A3w.png"/></div></div></figure><p id="ca2f" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">由于我们需要一个索引字典来映射到代码后面计算q值部分的状态，我们创建了一个新的字典来存储它们。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jg"><img src="../Images/390e9329f3fa5ea00bb0785fa01b6d69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HlHOXQR_OgErgeDi4MAuYA.png"/></div></div></figure><p id="93d5" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">现在我们直接应用上面讨论的算法。</p><p id="bb7b" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(a)让我们定义一个python函数路径，以初始状态和最终目标状态作为参数。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jh"><img src="../Images/11e375e454361a40f3dbed7c483a3ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVzYCC2ao9gbkGpoLAm4bQ.png"/></div></div></figure><p id="13f2" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(b)我们创建奖励矩阵的副本以创建显式的初始和目标奖励矩阵，而不是隐式地硬编码它们。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es ji"><img src="../Images/bb4245b8b7493da67e979a04f241c27a.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*VqEfbZLJxlNGjD8tjFEQFg.png"/></div></figure><p id="522a" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(c)我们在ending_state变量中获得目标状态的索引。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jj"><img src="../Images/5c7ac85929c97d6e22ce81d7172112f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6BpBd4Xnt_mfhEeb0nM8IA.png"/></div></div></figure><p id="63ff" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(d)我们将目标状态的最大奖励状态隐式定义为1000。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jk"><img src="../Images/f1c9c34f9fe5a0f86ac6c78457a71fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWNTLUdDsaBDcYvobe-SOg.png"/></div></div></figure><p id="831b" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(e)我们将Q值初始化为一个零数组。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jl"><img src="../Images/8f1d45a683e2f3825fd19f4dd2629a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ojodUlJpiDpwyg3Hd5T13Q.png"/></div></div></figure><p id="3d1f" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(f)对于训练，我们使用For循环测距1000个循环；随机的当前状态被初始化为与Q矩阵相同的大小，并且对于奖励矩阵中的每个动作，不止一个步骤被附加到playable_actions列表。下一个随机状态和相应的时间差异和Q值被计算。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jm"><img src="../Images/da9c5056357211d590eeda95b5487c1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q1xG7wwZXn9OeIwsRScKLg.png"/></div></div></figure><p id="acc4" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(g)最后，使用NumPy ArgMax函数计算数组中最大参数的Q值，并将其附加到包含作为第一个参数传递的起始位置的路由列表中。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jn"><img src="../Images/bc3ad62e1b320e8449657f9434ad692d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ca2KGazcD3tfQRCc2jLFRQ.png"/></div></div></figure><p id="1c3c" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">(f)路由列表作为对路由函数的回调返回，我们将起点和终点作为参数传递给路由函数，以获得我们自制的Q-Learning算法。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jo"><img src="../Images/fedccb9e43c6123bd4fcdfedf5d8a4dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XUdYbXsZBwIX905fM_Pqrg.png"/></div></div></figure><p id="95b4" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">输出</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jp"><img src="../Images/898cf871575c7b3d3353c9a0acd3d347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*qfvrT31uJQNbIwTWrmnddA.png"/></div></div></figure><p id="661f" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">最终函数看起来像:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="io ip di iq bf ir"><div class="er es jq"><img src="../Images/069c36775f6b87954bfdc28315eb7a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GW99YtxdhA4DjEgycCmjxg.png"/></div></div></figure><p id="131b" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">万岁，我们刚刚用Python实现了我们的第一个Q学习算法，就这么简单。现在，尝试将相同的Q-Learning算法应用于更复杂的现实生活问题，并创建一个应对市场挑战的出色解决方案。</p><p id="4fb5" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">前往我的GitHub repo查找完整代码:<a class="ae jr" href="https://github.com/vishaalsaravanan/Q_Learning/blob/master/q_learning.py" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/vishaalsaravanan/Q _ Learning/blob/master/Q _ Learning . py</a></p></div></div>    
</body>
</html>