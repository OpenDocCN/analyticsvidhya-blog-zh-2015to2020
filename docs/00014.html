<html>
<head>
<title>Understanding and coding Neural Networks From Scratch in Python and R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python和R从头开始理解和编码神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-and-coding-neural-networks-from-scratch-in-python-and-r-b8c760f0ad1c?source=collection_archive---------2-----------------------#2017-05-28">https://medium.com/analytics-vidhya/understanding-and-coding-neural-networks-from-scratch-in-python-and-r-b8c760f0ad1c?source=collection_archive---------2-----------------------#2017-05-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8f8e673c8c8f7d02e877081ed8f936f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KYQyUaMOWpfhQDzj.jpg"/></div></div></figure><h1 id="f470" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">介绍</h1><p id="41ca" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">您可以通过两种方式学习和实践一个概念:</p><ul class=""><li id="bf4c" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated">选项1: 你可以学习某一特定主题的全部理论，然后寻找应用这些概念的方法。所以，你阅读了整个算法是如何工作的，它背后的数学，它的假设，限制，然后你应用它。稳健但耗时的方法。</li><li id="4627" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated"><strong class="jq hj">选择2: </strong>从简单的基础开始，培养对主题的直觉。接下来，挑一个问题，开始解决它。在解决问题的同时学习概念。不断调整和提高你的理解。所以，你读了如何应用一个算法——出去应用它。一旦你知道如何应用它，用不同的参数、值、极限去尝试它，并发展对算法的理解。</li></ul><p id="adcf" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">我更喜欢第二种方法，用这种方法来学习任何新的话题。我也许不能告诉你算法背后的全部数学，但我可以告诉你直觉。根据我的实验和理解，我可以告诉你应用算法的最佳场景。</p><p id="f5d8" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">在我与人的互动中，我发现人们没有花时间去发展这种直觉，因此他们努力以正确的方式应用事物。</p><p id="60f1" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">在本文中，我将从头开始讨论神经网络的构建模块，并更多地关注开发这种直觉来应用神经网络。我们将用“Python”和“R”两种语言编码。到本文结束时，你将理解神经网络如何工作，我们如何初始化权重，以及我们如何使用反向传播来更新它们。</p><p id="3500" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">我们开始吧。</p><h1 id="97db" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">目录:</h1><ol class=""><li id="7c3d" class="km kn hi jq b jr js jv jw jz lf kd lg kh lh kl li ku kv kw bi translated">神经网络背后的简单直觉</li><li id="c2ab" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl li ku kv kw bi translated">多层感知器及其基础</li><li id="63f4" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl li ku kv kw bi translated">神经网络方法中涉及的步骤</li><li id="959d" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl li ku kv kw bi translated">神经网络工作方法的可视化步骤</li><li id="5553" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl li ku kv kw bi translated">使用Numpy (Python)实现神经网络</li><li id="d080" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl li ku kv kw bi translated">用R实现神经网络</li><li id="373f" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl li ku kv kw bi translated">[可选]反向传播算法的数学观点</li></ol><h1 id="3fed" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">神经网络背后的简单直觉</h1><p id="8df3" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">如果你是一名开发人员或者见过一个工作人员——你应该知道如何在代码中寻找bug。您可以通过改变输入或环境来激发各种测试用例，并寻找输出。输出中的变化为您提供了在哪里查找bug的提示——检查哪个模块，读取哪些行。一旦你找到了，你就进行修改，然后继续练习，直到你有了正确的代码/应用程序。</p><p id="5694" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">神经网络以非常相似的方式工作。它接受几个输入，通过来自多个隐藏层的多个神经元进行处理，并使用输出层返回结果。这个结果估计过程在技术上被称为“<strong class="jq hj">正向传播</strong>”。</p><p id="c025" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">接下来，我们将结果与实际输出进行比较。任务是使神经网络的输出尽可能接近实际(期望)输出。这些神经元中的每一个都对最终输出产生一些误差。你如何减少误差？</p><p id="bcb2" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">我们试图最小化对错误贡献更多的神经元的值/权重，这发生在返回神经网络的神经元并找到错误所在的时候。这个过程被称为“<strong class="jq hj">反向传播</strong>”。</p><p id="eb99" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">为了减少这些迭代次数以最小化误差，神经网络使用一种称为“梯度下降”的常见算法，这有助于快速有效地优化任务。</p><p id="9fdb" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">就是这样——这就是神经网络的工作方式！我知道这是一个非常简单的表示，但是它会帮助你以一种简单的方式理解事情。</p><h1 id="616f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">多层感知器及其基础</h1><p id="81e0" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">就像原子构成地球上任何物质的基础一样——神经网络的基本构成单位是感知器。那么，什么是感知机呢？</p><p id="37ff" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">感知器可以理解为接受多个输入并产生一个输出的任何东西。比如看下图。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/478e82f87ae45c563fa94ee5f0b90636.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/0*G1oRGW6Dej3Xkh-n.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">感知器</figcaption></figure><p id="770e" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">上面的结构接受三个输入并产生一个输出。下一个逻辑问题是，投入和产出的关系是什么？让我们从基本的方法开始，然后继续寻找更复杂的方法。</p><p id="1387" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">下面，我讨论了创建输入输出关系的三种方法:</p><ol class=""><li id="2f68" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl li ku kv kw bi translated"><strong class="jq hj">通过直接组合输入并基于阈值计算输出</strong>。例如:取x1=0，x2=1，x3=1，并设置阈值=0。所以，如果x1+x2+x3 &gt;为0，则输出为1，否则为0。你可以看到，在这种情况下，感知器将输出计算为1。</li><li id="32ee" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl li ku kv kw bi translated"><strong class="jq hj">接下来，让我们给输入增加权重。</strong>权重为输入赋予重要性。例如，将w1=2、w2=3和w3=4分别分配给x1、x2和x3。为了计算输出，我们将输入乘以各自的权重，并与阈值进行比较，如w1*x1 + w2*x2 + w3*x3 &gt;阈值。与x1和x2相比，这些权重赋予x3更多的重要性。</li><li id="c339" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl li ku kv kw bi translated">接下来，让我们添加偏差:每个感知器也有一个偏差，可以认为是感知器的灵活程度。它在某种程度上类似于线性函数<em class="ls"> y = ax + b的常数<em class="ls"> b </em>，它允许我们上下移动直线，以使预测与数据更好地吻合。没有b，直线将总是通过原点(0，0)，你可能得到一个较差的拟合。</em>例如，感知器可能有两个输入，在这种情况下，它需要三个权重。一个用于每个输入，一个用于偏置。现在，输入的线性表示看起来像w1*x1 + w2*x2 + w3*x3 + 1*b。</li></ol><p id="398c" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">但是，所有这些仍然是线性的，而感知器曾经是线性的。但这并不有趣。因此，人们想到将感知器进化成现在所谓的人工神经元。神经元对输入和偏置应用非线性变换(激活函数)。</p><h1 id="6993" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是激活函数？</h1><p id="04ad" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">激活函数将加权输入的和(w1*x1 + w2*x2 + w3*x3 + 1*b)作为自变量，并返回神经元的输出。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/a370d2be941c29d65708043fde47bab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/0*QrkYzzhhsY9mWKB0.png"/></div></figure><p id="be4f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">在上式中，我们将1表示为x0，b表示为w0。</p><p id="ad7e" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">激活函数主要用于进行非线性变换，使我们能够拟合非线性假设或估计复杂函数。有多个激活功能，像:“乙状结肠”，“Tanh”，ReLu和许多其他的。</p><h1 id="43a8" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">正向传播、反向传播和纪元</h1><p id="4c74" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">到目前为止，我们已经计算了输出，这个过程被称为“<strong class="jq hj">正向传播</strong>”。但是如果估计输出与实际输出相差很远(误差大)怎么办。在神经网络中，我们根据误差更新偏差和权重。这个权重和偏差更新过程被称为“<strong class="jq hj">反向传播</strong>”。</p><p id="780f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">反向传播(BP)算法的工作原理是确定输出端的损耗(或误差),然后将其传播回网络。权重被更新以最小化由每个神经元产生的误差。最小化误差的第一步是确定每个节点相对于最终输出的梯度(导数)。要获得反向传播的数学观点，请参考下一节。</p><p id="a725" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">这一轮前向和反向传播迭代被称为一次训练迭代，又名“<strong class="jq hj">时期</strong>”。</p><h1 id="67a1" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">多层感知器</h1><p id="d469" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">现在，让我们进入<strong class="jq hj">多层</strong>感知器的下一部分。到目前为止，我们只看到了一个由3个输入节点(即x1、x2和x3)组成的单层和一个由单个神经元组成的输出层。但是，出于实用目的，单层网络只能做这么多。MLP由堆叠在<strong class="jq hj">输入层</strong>和<strong class="jq hj">输出层</strong>之间的多层<strong class="jq hj">隐藏层</strong>组成，如下图所示。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/63d063181d0c6fbc5c7ed163cd4e7f6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xnzyeyCmmIwG4NAS.png"/></div></div></figure><p id="9c56" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">上图显示的只是一个绿色的隐藏层，但实际上可以包含多个隐藏层。对于MLP，需要记住的另一点是所有图层都是完全连接的，即一个图层中的每个节点(除了输入和输出图层)都连接到前一图层和后一图层中的每个节点。</p><p id="5d04" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">让我们继续下一个主题，这是一个神经网络的训练算法(以尽量减少误差)。在这里，我们将看看最常见的训练算法，称为<a class="ae lv" href="https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/" rel="noopener ugc nofollow" target="_blank">梯度下降</a>。</p><h1 id="a087" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">全批次梯度下降和随机梯度下降</h1><p id="05a1" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">梯度下降的两种变体通过使用相同的更新算法来执行更新MLP权重的相同工作，但是差异在于用于更新权重和偏差的训练样本的数量。</p><p id="4f6f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">顾名思义，全批次梯度下降算法使用所有训练数据点来更新每个权重一次，而随机梯度使用1个或更多(样本)但从不使用整个训练数据来更新权重一次。</p><p id="fb83" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">让我们用一个简单的例子来理解这一点，一个由10个数据点组成的数据集有两个权重<strong class="jq hj"> w1 </strong>和<strong class="jq hj"> w2 </strong>。</p><p id="c76b" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">整批:</strong>使用10个数据点(全部训练数据)，计算w1的变化(δw1)和w2的变化(δw2)，并更新w1和w2。</p><p id="077c" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj"> SGD: </strong>使用第一个数据点，计算w1(δw1)和w2(δw2)的变化，并更新w1和w2。接下来，当您使用第二个数据点时，您将处理更新后的权重</p><p id="a3b1" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">对于这两种方法的更深入的解释，你可以看一下<a class="ae lv" href="https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。</p><h1 id="6212" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">神经网络方法中涉及的步骤</h1><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/ba0d5aa257a21fd64ae654660fd8a152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0Ql4RWwCxmEl3P4R.png"/></div></div></figure><p id="ea59" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">让我们来看看神经网络的一步一步的构建方法(带有一个隐藏层的MLP，类似于上面显示的架构)。在输出层，我们只有一个神经元，因为我们正在解决一个二元分类问题(预测0或1)。我们也可以有两个神经元来预测这两类中的每一类。</p><p id="09f4" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">首先看一下宽泛的步骤:</p><p id="1385" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">0.)我们把输入和输出</p><ul class=""><li id="b92a" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated">x作为输入矩阵</li><li id="6543" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated">y作为输出矩阵</li></ul><p id="eb22" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">1.)我们用随机值初始化权重和偏差(这是一次性初始化。在下一次迭代中，我们将使用更新的权重和偏差)。让我们来定义:</p><ul class=""><li id="03c7" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated">wh作为隐藏层的权重矩阵</li><li id="de76" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated">bh作为隐藏层的偏置矩阵</li><li id="b142" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated">作为输出层的权重矩阵</li><li id="a088" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated">作为输出层的偏置矩阵</li></ul><p id="acad" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">2.)我们取输入和分配给输入和隐藏层之间的边的权重的矩阵点积，然后将隐藏层神经元的偏差添加到相应的输入，这被称为线性变换:</p><p id="9738" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">隐藏层输入=矩阵点积(X，wh) + bh</p><p id="308f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">3)使用激活函数(Sigmoid)执行非线性变换。Sigmoid将以1/(1 + exp(-x))的形式返回输出。</p><p id="2052" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">hidden layer _ activations = sigmoid(隐藏层输入)</p><p id="4fe8" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">4.2)对隐藏层激活执行线性变换(取带权重的矩阵点积并添加输出层神经元的偏差),然后应用激活函数(再次使用sigmoid，但您可以根据您的任务使用任何其他激活函数)来预测输出</p><p id="694d" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">output _ layer _ input = matrix _ dot _ product(hidden layer _ activations * wout)+bout<br/><em class="ls">output = sigmoid(output _ layer _ input)</em></p><p id="0129" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">以上所有步骤被称为“正向传播”</strong></p><p id="a8e9" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">5.)将预测与实际输出进行比较，并计算误差的梯度(实际-预测)。误差是均方损失= ((Y-t) )/2</p><p id="7e90" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">E = y输出</p><p id="fe72" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">6.2)计算隐藏层和输出层神经元的斜率/梯度(为了计算斜率，我们计算每个神经元在每个层的非线性激活x的导数)。sigmoid的梯度可以作为x * (1-x)返回。</p><p id="6470" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">slope _ output _ layer = derives _ sigmoid(output)<br/><em class="ls">slope _ hidden _ layer = derives _ sigmoid(hidden layer _ activations)</em></p><p id="4422" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">7.2)根据误差的梯度乘以输出层激活的斜率，计算输出层的变化因子(δ)</p><p id="dc96" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><em class="ls">d _输出= E *斜率_输出_图层</em></p><p id="6a55" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">8.)在这一步，错误将传播回网络，这意味着隐藏层的错误。为此，我们将采用输出层δ与隐藏层和输出层之间的边的权重参数的点积(wout。t)。</p><p id="0c96" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">error _ at _ hidden _ layer = matrix _ dot _ product(d _ output，wout。转置)</p><p id="8019" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">9.2)计算隐藏层的变化因子(δ),将隐藏层的误差乘以隐藏层激活的斜率</p><p id="eb14" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><em class="ls">d _ hidden layer = Error _ at _ hidden _ layer * slope _ hidden _ layer</em></p><p id="dbb1" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">10.2)更新输出和隐藏层的权重:网络中的权重可以根据为训练样本计算的误差来更新。</p><p id="528b" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">wout = wout+matrix _ dot _ product(hidden layer _ activations。转置，d _ output)* learning _ rate<br/>wh = wh+matrix _ dot _ product(X . Transpose，d_hiddenlayer)*learning_rate</p><p id="cb8e" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">learning_rate:权重更新的数量由称为学习率的配置参数控制)</p><p id="1b78" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">11.2)更新输出和隐藏层的偏差:网络中的偏差可以从该神经元的聚集误差中更新。</p><ul class=""><li id="30b4" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated">output_layer的偏差= output _ layer的偏差+output _ layer在行方向的增量之和* learning_rate</li><li id="0fb2" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated">隐藏层偏差=隐藏层偏差+行输出层增量之和*学习率</li></ul><p id="04f4" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><em class="ls"> bh = bh + sum(d_hiddenlayer，axis = 0)* learning _ rate<br/>bout = bout+sum(d _ output，axis=0)*learning_rate </em></p><p id="dd7e" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">从5到11的步骤被称为“反向传播”</strong></p><p id="d46f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">一个前向和后向传播迭代被认为是一个训练周期。正如我前面提到的，我们什么时候第二次训练，然后更新权重和偏差用于前向传播。</p><p id="c27e" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">上面，我们已经更新了隐藏层和输出层的权重和偏差，并且使用了全批次梯度下降算法。</p><h1 id="1a84" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">神经网络方法步骤的可视化</h1><p id="1a1d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">我们将重复上述步骤，并将输入、权重、偏差、输出、误差矩阵可视化，以理解神经网络(MLP)的工作方法。</p><p id="7690" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">注意:</p><ul class=""><li id="af44" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated">为了得到好的可视化图像，我在2或3位四舍五入了小数。</li><li id="2e06" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated">黄色填充单元格表示当前活动单元格</li><li id="93eb" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated">橙色单元格表示用于填充当前单元格值的输入</li></ul><p id="8e0a" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">步骤0: </strong>读取输入和输出</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/fd360d2a089e577b57f2a5a303c1f107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8Ncz80a91PjS4urO.jpg"/></div></div></figure><p id="072d" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">步骤1: </strong>用随机值初始化权重和偏差(有初始化权重和偏差的方法，但是现在用随机值初始化)</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/95e6cd3dd637161d007fc41c5201ea01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lT1WZp2cXYX_mkNc.png"/></div></div></figure><p id="8dcd" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">第二步:</strong>计算隐藏层输入:<br/>hidden _ layer _ input = matrix _ dot _ product(X，wh) + bh</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/410f759bf1cb8611bf6dd8ab06546167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8b_lonk18YIokEv2.png"/></div></div></figure><p id="6f1f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">步骤3: </strong>对隐藏的线性输入进行非线性变换<br/><em class="ls">hidden layer _ activations = sigmoid(hidden _ layer _ input)</em></p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/5ae83e530e056bb4267c8611e18e4ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vo01YKO5FtQ_CCiQ.png"/></div></div></figure><p id="7a5a" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">步骤4: </strong>在输出层执行隐藏层激活的线性和非线性变换</p><p id="6ce3" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">output _ layer _ input = matrix _ dot _ product(hidden layer _ activations * wout)+bout<br/><em class="ls">output = sigmoid(output _ layer _ input)</em></p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/f658a2fc17b846e5c0853366d618fdb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*n9RWK6DTL8OHjxUT.png"/></div></div></figure><p id="a362" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">第五步:</strong>计算输出层误差(E)的梯度<br/><em class="ls">E = y-输出</em></p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/fc3a530cd1538d9bbb0cab78e37e34d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K_cIwOw8ZKR1kUct.png"/></div></div></figure><p id="61d2" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">步骤6: </strong>计算输出和隐藏层的斜率<br/><em class="ls">Slope _ output _ layer = derives _ sigmoid(output)</em><br/><em class="ls">Slope _ hidden _ layer = derives _ sigmoid(hidden layer _ activations)</em></p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/4fbd3c501c0d2873852aecffd6cfedfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6cEMKW0VwOpvFi94.png"/></div></div></figure><p id="f286" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">第7步:</strong>计算输出层的增量</p><p id="1d8d" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><em class="ls">d _输出= E *斜率_输出_图层*lr </em></p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/65aa8be1ef97b238de89ec950951b812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1hg4ukxZaIaMEsZQ.png"/></div></div></figure><p id="94fd" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">步骤8: </strong>计算隐藏层的误差</p><p id="95eb" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">error _ at _ hidden _ layer = matrix _ dot _ product(d _ output，wout。转置)</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/ff8d2610868248863bc4de47f37f189f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Bw7NKPalf6YAA14B.png"/></div></div></figure><p id="a8f8" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">第九步:</strong>计算隐藏层的增量</p><p id="cdef" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><em class="ls">d _ hidden layer = Error _ at _ hidden _ layer * slope _ hidden _ layer</em></p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/53e19f4ecc1de3ee8255cb94791b62ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hNvFGYn3Q8FKnafV.png"/></div></div></figure><p id="4384" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">步骤10: </strong>更新输出层和隐藏层的权重</p><p id="58ef" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">wout = wout+matrix _ dot _ product(hidden layer _ activations。转置，d _ output)* learning _ rate<br/>wh = wh+matrix _ dot _ product(X . Transpose，d_hiddenlayer)*learning_rate</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/ec93ba303b76f2e2b67de82edb530316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ltEpBZLqri4NqfM_.png"/></div></div></figure><p id="a763" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">步骤11: </strong>更新输出和隐藏层的偏差</p><p id="fc8f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><em class="ls"> bh = bh + sum(d_hiddenlayer，axis = 0)* learning _ rate<br/>bout = bout+sum(d _ output，axis=0)*learning_rate </em></p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/858c71ee9f21372a4fb50dff72ca9e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JTsv3FoTsjPNv6Zp.png"/></div></div></figure><p id="3e49" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">上面，你可以看到仍然有一个不接近实际目标值的好的误差，因为我们只完成了一次训练迭代。如果我们将训练模型多次，那么它将是一个非常接近的实际结果。我已经完成了数千次迭代，我的结果接近实际目标值([[0.98032096][0.96845624][0.04532167]])。</p><h1 id="473f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">使用Numpy (Python)实现神经网络</h1><pre class="lk ll lm ln fd lx ly lz ma aw mb bi"><span id="fc25" class="mc ir hi ly b fi md me l mf mg">import numpy as np</span><span id="3c4e" class="mc ir hi ly b fi mh me l mf mg">#Input array<br/>X=np.array([[1,0,1,0],[1,0,1,1],[0,1,0,1]])</span><span id="e59a" class="mc ir hi ly b fi mh me l mf mg">#Output<br/>y=np.array([[1],[1],[0]])</span><span id="a71a" class="mc ir hi ly b fi mh me l mf mg">#Sigmoid Function<br/>def sigmoid (x):<br/>    return 1/(1 + np.exp(-x))</span><span id="ef76" class="mc ir hi ly b fi mh me l mf mg">#Derivative of Sigmoid Function<br/>def derivatives_sigmoid(x):<br/>    return x * (1 - x)</span><span id="4bb1" class="mc ir hi ly b fi mh me l mf mg">#Variable initialization<br/>epoch=5000 #Setting training iterations<br/>lr=0.1 #Setting learning rate<br/>inputlayer_neurons = X.shape[1] #number of features in data set<br/>hiddenlayer_neurons = 3 #number of hidden layers neurons<br/>output_neurons = 1 #number of neurons at output layer</span><span id="6b3e" class="mc ir hi ly b fi mh me l mf mg">#weight and bias initialization<br/>wh=np.random.uniform(size=(inputlayer_neurons,hiddenlayer_neurons))<br/>bh=np.random.uniform(size=(1,hiddenlayer_neurons))<br/>wout=np.random.uniform(size=(hiddenlayer_neurons,output_neurons))<br/>bout=np.random.uniform(size=(1,output_neurons))</span><span id="5a44" class="mc ir hi ly b fi mh me l mf mg">for i in range(epoch):<br/>    <br/>    #Forward Propogation<br/>    hidden_layer_input1=np.dot(X,wh)<br/>    hidden_layer_input=hidden_layer_input1 + bh<br/>    hiddenlayer_activations = sigmoid(hidden_layer_input)<br/>    output_layer_input1=np.dot(hiddenlayer_activations,wout)<br/>    output_layer_input= output_layer_input1+ bout<br/>    output = sigmoid(output_layer_input)</span><span id="1a5f" class="mc ir hi ly b fi mh me l mf mg">    #Backpropagation<br/>    E = y-output<br/>    slope_output_layer = derivatives_sigmoid(output)<br/>    slope_hidden_layer = derivatives_sigmoid(hiddenlayer_activations)<br/>    d_output = E * slope_output_layer<br/>    Error_at_hidden_layer = d_output.dot(wout.T)<br/>    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer<br/>    wout += hiddenlayer_activations.T.dot(d_output) *lr<br/>    bout += np.sum(d_output, axis=0,keepdims=True) *lr<br/>    wh += X.T.dot(d_hiddenlayer) *lr<br/>    bh += np.sum(d_hiddenlayer, axis=0,keepdims=True) *lr</span><span id="223d" class="mc ir hi ly b fi mh me l mf mg">print output</span></pre><h1 id="3a70" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">在R中实现神经网络</h1><p id="a673" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><code class="du mi mj mk ly b"># input matrix<br/> X=matrix(c(1,0,1,0,1,0,1,1,0,1,0,1),nrow = 3, ncol=4,byrow = TRUE)</code></p><p id="a29f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><code class="du mi mj mk ly b"># output matrix</code> <br/> <code class="du mi mj mk ly b">Y=matrix(c(1,1,0),byrow=FALSE)</code></p><p id="3af9" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><code class="du mi mj mk ly b">#sigmoid function</code><br/><code class="du mi mj mk ly b">sigmoid&lt;-function(x){</code><br/><code class="du mi mj mk ly b">1/(1+exp(-x))</code><br/>T6】</p><p id="57a8" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><code class="du mi mj mk ly b"># derivative of sigmoid function</code><br/><code class="du mi mj mk ly b">derivatives_sigmoid&lt;-function(x){</code><br/><code class="du mi mj mk ly b">x*(1-x)</code><br/>T3】</p><p id="6f50" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><code class="du mi mj mk ly b"># variable initialization</code><br/><code class="du mi mj mk ly b">epoch=5000</code><br/><br/><code class="du mi mj mk ly b">inputlayer_neurons=ncol(X)</code><br/><code class="du mi mj mk ly b">hiddenlayer_neurons=3</code><br/><code class="du mi mj mk ly b">output_neurons=1</code></p><p id="8467" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><code class="du mi mj mk ly b">#weight and bias initialization</code><br/><code class="du mi mj mk ly b">wh=matrix( rnorm(inputlayer_neurons*hiddenlayer_neurons,mean=0,sd=1), inputlayer_neurons, hiddenlayer_neurons)</code><br/><code class="du mi mj mk ly b">bias_in=runif(hiddenlayer_neurons)</code><br/><code class="du mi mj mk ly b">bias_in_temp=rep(bias_in, nrow(X))</code><br/><code class="du mi mj mk ly b">bh=matrix(bias_in_temp, nrow = nrow(X), byrow = FALSE)</code><br/><code class="du mi mj mk ly b">wout=matrix( rnorm(hiddenlayer_neurons*output_neurons,mean=0,sd=1), hiddenlayer_neurons, output_neurons)</code></p><p id="d172" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><code class="du mi mj mk ly b">bias_out=runif(output_neurons)</code><br/><code class="du mi mj mk ly b">bias_out_temp=rep(bias_out,nrow(X))</code><br/><code class="du mi mj mk ly b">bout=matrix(bias_out_temp,nrow = nrow(X),byrow = FALSE)</code><br/><code class="du mi mj mk ly b"># forward propagation</code><br/><code class="du mi mj mk ly b">for(i in 1:epoch){</code></p><p id="4ae7" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><code class="du mi mj mk ly b">hidden_layer_input1= X%*%wh</code><br/><code class="du mi mj mk ly b">hidden_layer_input=hidden_layer_input1+bh</code><br/><code class="du mi mj mk ly b">hidden_layer_activations=sigmoid(hidden_layer_input)</code><br/><code class="du mi mj mk ly b">output_layer_input1=hidden_layer_activations%*%wout</code><br/><code class="du mi mj mk ly b">output_layer_input=output_layer_input1+bout</code><br/><code class="du mi mj mk ly b">output= sigmoid(output_layer_input)</code></p><p id="80be" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><code class="du mi mj mk ly b"># Back Propagation</code></p><p id="b8cf" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><code class="du mi mj mk ly b">E=Y-output</code><br/><code class="du mi mj mk ly b">slope_output_layer=derivatives_sigmoid(output)</code><br/><code class="du mi mj mk ly b">slope_hidden_layer=derivatives_sigmoid(hidden_layer_activations)</code><code class="du mi mj mk ly b">Error_at_hidden_layer=d_output%*%t(wout)</code><code class="du mi mj mk ly b">d_output=E*slope_output_layer</code><code class="du mi mj mk ly b">Error_at_hidden_layer=d_output%*%t(wout)</code><br/><code class="du mi mj mk ly b">d_hiddenlayer=Error_at_hidden_layer*slope_hidden_layer</code><br/><code class="du mi mj mk ly b">wout= wout + (t(hidden_layer_activations)%*%d_output)*lr</code><br/><code class="du mi mj mk ly b">bout= bout+rowSums(d_output)*lr</code><br/><code class="du mi mj mk ly b">wh = wh +(t(X)%*%d_hiddenlayer)*lr</code><br/><code class="du mi mj mk ly b">bh = bh + rowSums(d_hiddenlayer)*lr</code></p><p id="d3dd" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><code class="du mi mj mk ly b">}</code><br/>T1】</p><h1 id="5468" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">[可选]反向传播算法的数学观点</h1><p id="65da" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">设Wi为输入层和隐藏层之间的权重。Wh是隐藏层和输出层之间的权重。</p><p id="c9f9" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">现在，<strong class="jq hj"> h=σ (u)= σ (WiX) </strong>，即h是u的函数，u是Wi和x的函数，这里我们把我们的函数表示为<strong class="jq hj"> σ </strong></p><p id="47ff" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj">Y =σ(u ')=σ(</strong>W<strong class="jq hj">h</strong>h<strong class="jq hj">)</strong>，即Y是u '的函数，u '是Wh和h的函数。</p><p id="15c9" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">我们将不断引用上述方程来计算偏导数。</p><p id="6844" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">我们主要感兴趣的是找到两个术语，∂E/∂Wi和∂E/∂Wh，即改变输入层和隐藏层之间的权重时的误差变化和改变隐藏层和输出层之间的权重时的误差变化。</p><p id="2088" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">但是为了计算这两个偏导数，我们需要使用偏导数的链式法则，因为E是Y的函数，Y是u '的函数，u '是Wi的函数。</p><p id="c9a7" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">让我们好好利用这个属性，计算梯度。</p><p id="633b" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">∂E/∂Wh = (∂E/∂Y).(∂Y/∂u').(∂u'/∂Wh)，……..(1)</p><p id="a6d8" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">我们知道E的形式为E=(Y-t)2/2。</p><p id="b8ea" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">所以，(∂E/∂Y)=)</p><p id="4d0a" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">现在，σ是一个sigmoid函数，并且具有σ(1- σ)形式的有趣微分。我敦促读者从他们的角度来验证这一点。</p><p id="4904" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">所以，(∂y/∂u')= ∂(σ(u ')/∂u'=σ(u ')(1-σ(u '))。</p><p id="2a53" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">但是，σ(u')=Y，所以，</p><p id="ead3" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">(∂Y/∂u')=Y(1-Y)</p><p id="93f8" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">现在，(∂u'/∂Wh)= ∂( Whh)/ ∂Wh = h</p><p id="22bf" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">替换我们得到的等式(1)中的值，</p><p id="2147" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj"> ∂E/∂Wh = (Y-t)。Y(1-Y)。h </strong></p><p id="3a7c" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">所以，现在我们已经计算了隐藏层和输出层之间的梯度。现在是我们计算输入层和隐藏层之间的梯度的时候了。</p><p id="e85e" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">∂E/∂Wi =(∂ E/∂ h)。(∂h/∂u).(∂u/∂Wi)</p><p id="9827" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">但是，(∂e/∂·h)=(∂e/∂y).(∂Y/∂u').(∂u'/∂h).在上面的等式中替换这个值，</p><p id="d4e5" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">∂E/∂Wi =[(∂E/∂Y).(∂Y/∂u').(∂u'/∂h)].(∂h/∂u).(∂u/∂Wi)……………(2)</p><p id="2c3a" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">那么，首先计算隐藏层和输出层之间的梯度有什么好处呢？</p><p id="140e" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">正如你在等式(2)中看到的，我们已经计算了∂E/∂Y和∂Y/∂u'，节省了我们的空间和计算时间。一会儿我们会知道为什么这个算法叫做反向传播算法。</p><p id="7481" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">让我们计算等式(2)中的未知导数。</p><p id="96c3" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">∂u'/∂h = ∂(Whh)/ ∂h =白色</p><p id="17c5" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">∂h/∂u = ∂( σ(u)/ ∂u= σ(u)(1- σ(u))</p><p id="95f3" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">但是，σ(u)=h，所以，</p><p id="c24f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">(∂Y/∂u)=h(1-h)</p><p id="aced" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">现在，∂u/∂Wi = ∂(WiX)/ ∂Wi = X</p><p id="a98e" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">替换等式(2)中的所有这些值，我们得到，</p><p id="2d0d" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj"> ∂E/∂Wi = [(Y-t)。Y(1-Y)。Wh】。h(1-h)。X </strong></p><p id="3fae" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">因此，现在我们已经计算了两个梯度，权重可以更新为</p><p id="080c" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj"> Wh = Wh + η。∂E/∂Wh </strong></p><p id="9a57" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><strong class="jq hj"> Wi = Wi + η。∂E/∂Wi </strong></p><p id="9730" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">其中<strong class="jq hj"> η </strong>是学习率。</p><p id="5d90" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">所以回到问题:为什么这个算法叫反向传播算法？</p><p id="a7e3" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">原因是:如果你注意到<strong class="jq hj"> ∂E/∂Wh </strong>和<strong class="jq hj"> ∂E/∂Wi </strong>的最终形式，你会看到(Y-t)项，即输出误差，这是我们开始的，然后将其传播回输入层进行权重更新。</p><p id="7383" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">那么，这种数学在代码中处于什么位置呢？</p><p id="ce26" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">hiddenlayer_activations=h</p><p id="2f43" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">E= Y-t</p><p id="c78c" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">坡度_输出_图层= Y(1-Y)</p><p id="7a2d" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">lr = η</p><p id="8341" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">坡度_隐藏_图层= h(1-h)</p><p id="ac1f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">wout = Wh</p><p id="ff4b" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">现在，你可以很容易地把代码和数学联系起来。</p><h1 id="308c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结束注释:</h1><p id="522a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">本文的重点是从头开始构建神经网络并理解其基本概念。我希望现在你理解了神经网络的工作原理，比如前向和后向传播是如何工作的，优化算法(完全批处理和随机梯度下降)，如何更新权重和偏差，Excel中每一步的可视化以及python和r中的代码。</p><p id="64ea" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">因此，在我即将发表的文章中，我将解释在Python中使用神经网络的应用，以及解决与以下相关的现实挑战:</p><ol class=""><li id="05c1" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl li ku kv kw bi translated">计算机视觉</li><li id="9a83" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl li ku kv kw bi translated">演讲</li><li id="8fe0" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl li ku kv kw bi translated">自然语言处理</li></ol><p id="75df" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated">我喜欢写这篇文章，并希望从你的反馈中学习。你觉得这篇文章有用吗？我将感谢你的建议/反馈。请随时通过下面的评论提出你的问题。</p></div><div class="ab cl ml mm gp mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hb hc hd he hf"><p id="7263" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz lc kb kc kd ld kf kg kh le kj kk kl hb bi translated"><em class="ls">原载于2017年5月28日</em><a class="ae lv" href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/" rel="noopener ugc nofollow" target="_blank"><em class="ls">www.analyticsvidhya.com</em></a><em class="ls">。</em></p></div></div>    
</body>
</html>