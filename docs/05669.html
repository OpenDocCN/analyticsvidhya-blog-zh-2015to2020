<html>
<head>
<title>JanataHack NLP Hackathon: Newbie to Public LB 1st place</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">JanataHack NLP黑客马拉松:公共LB第一名的新手</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/janatahack-nlp-hackathon-newbie-to-public-lb-1st-place-c0e819069f66?source=collection_archive---------19-----------------------#2020-04-28">https://medium.com/analytics-vidhya/janatahack-nlp-hackathon-newbie-to-public-lb-1st-place-c0e819069f66?source=collection_archive---------19-----------------------#2020-04-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5d4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">隔离时间还在继续，有效地度过一天是我脑海中唯一的目标。我偶然发现了JanataHack NLP黑客马拉松，想试一试。问题陈述是为了预测，评论者是否根据评论文本和其他信息推荐了特定的游戏。</p><p id="ff3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个问题引起了我的兴趣，尽管我以前在NLP方面没有任何经验。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/62ba4eefab19daf6335bb30e88d3ae4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EIcDvUGAFtr8waXkbkoSxQ.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">由<a class="ae jd" href="https://unsplash.com/@karla_rivera?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">卡拉·里维拉</a>在<a class="ae jd" href="https://unsplash.com/t/athletics?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="3135" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在关于NLP的文章中，我读到了N-grams、TF-IDF和许多其他传统的NLP技术。然后我偶然发现了杰瑞米·霍华德的<a class="ae jd" href="https://course.fast.ai/videos/?lesson=4" rel="noopener ugc nofollow" target="_blank"> fastai讲座视频</a>，他在视频中谈到使用fastai采用深度学习方法来解决NLP问题，也强调了迁移学习的使用。</p><p id="e064" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到了64个游戏的数据集，其中包括review_texts.csv(包含游戏评论)和game_overview.csv(包含每个游戏的概述)。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="f1f0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">第一次</h1><p id="bc8c" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">我们开始尝试在WikiText-103数据集上预处理的LSTM模型上应用Jeremy的迁移学习技术。使用game_overview.csv将LSTM模型微调为语言模型，并在训练数据集上训练为分类模型。这种方法让我们获得了<strong class="ih hj"> ~0.83磅</strong>(排行榜分数)。在实验过程中，局部CV(交叉验证分数)与LB密切相关。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="11ff" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">第二次</h1><p id="3833" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">为了进一步提高我们的LB排名，我们学习了预先训练的模型架构，如BERT、GPT-2、罗伯塔、XLM、DistilBert、XLNet、T5、CTRL等。通过<a class="ae jd" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">拥抱面部变形金刚</a>。</p><p id="4a16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">BERT</strong></a><strong class="ih hj"/>是一个双向转换器，用于对大量未标记的文本数据进行预训练，以学习一种可用于微调特定机器学习任务的语言表示。</p><p id="bb00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用预先训练好的BERT(基本无壳)，并遵循fastai的一次拟合循环方法，这很快使我们获得了<strong class="ih hj"> ~0.91磅</strong>，这比我们之前的成绩有了巨大的提高。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="99d7" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">最后阶段</h1><p id="9ea7" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">罗伯塔</strong> </a> <strong class="ih hj">。【RoBERTa是在脸书大学推出的稳健优化的BERT方法，它是对BERT的再培训，具有改进的培训方法、1000%以上的数据和计算能力。重要的是，RoBERTa使用160 GB的文本进行预训练，包括16GB的书籍语料库和BERT中使用的英语维基百科。附加数据包括<a class="ae jd" href="http://web.archive.org/web/20190904092526/http://commoncrawl.org/2016/10/news-dataset-available/" rel="noopener ugc nofollow" target="_blank"> CommonCrawl新闻数据集</a>(6300万篇文章，76 GB)，网络文本语料库(38 GB)，以及来自CommonCrawl的故事(31 GB)。</strong></p><p id="12c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换成RoBERTa (base)后提升了约0.01磅，我们的体重为<strong class="ih hj"> ~0.925磅</strong>。LB仍然与CV完全相关。</p><p id="5883" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RoBERTa(大号)大幅提升了约0.015磅，将我们推至约0.9397磅(T11)。这是我们最好的单一模型得分。<br/>我们的最佳LB分数(<a class="ae jd" href="https://datahack.analyticsvidhya.com/contest/janatahack-nlp-hackathon/#LeaderBoard" rel="noopener ugc nofollow" target="_blank">公众第一名</a>)为<strong class="ih hj"> 0.94235 </strong>是4个RoBERTa(大)模特的合奏。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><blockquote class="le"><p id="21b1" class="lf lg hi bd lh li lj lk ll lm ln jc dx translated">是什么让我们更上一层楼……</p></blockquote><ul class=""><li id="c7d2" class="lo lp hi ih b ii lq im lr iq ls iu lt iy lu jc lv lw lx ly bi translated">我们没有使用训练集中的所有训练信息，只是将“user_reviews”列输入到模型中。使用所有的培训信息可能会有所帮助。</li><li id="acc4" class="lo lp hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">为更多的时代而训练。我们训练每个罗伯塔最多6个时期。</li><li id="6862" class="lo lp hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">不同型号的组合。我们只集合了基于RoBERTa的模型。</li></ul></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="5d11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我们为黑客马拉松处理上述问题陈述的整体方法。<a class="ae jd" href="https://github.com/nainci/JanataHack-NLP-Hackathon" rel="noopener ugc nofollow" target="_blank">这里是</a>GitHub回购。</p><p id="b77a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为初学者，我们欢迎您的所有建议和评论。请在下面留下评论，以便进一步讨论。</p></div></div>    
</body>
</html>