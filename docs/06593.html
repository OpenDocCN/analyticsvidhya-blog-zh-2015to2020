<html>
<head>
<title>How to Handle Imbalanced Dataset ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何处理不平衡数据集？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-handle-imbalanced-dataset-b3dc05b85bf9?source=collection_archive---------8-----------------------#2020-05-27">https://medium.com/analytics-vidhya/how-to-handle-imbalanced-dataset-b3dc05b85bf9?source=collection_archive---------8-----------------------#2020-05-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6a0928818f44a85dc6580da58cff55d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*egANvpMCFxU5g-dBAXxnBQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来自谷歌</figcaption></figure><p id="340e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在我们进入正题之前。<strong class="iw hj">平衡数据集</strong>表示A类和B类的目标列应该处于<strong class="iw hj"> <em class="js"> 50:50 </em> </strong>的比率或者<strong class="iw hj"> <em class="js"> 60:40 </em> </strong>的比率。</p><p id="6b36" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">当我们将A类和B类的<strong class="iw hj"> <em class="js"> 80:20 </em> </strong>或<strong class="iw hj"> <em class="js"> 90:10 </em> </strong>视为<strong class="iw hj"> <em class="js">不平衡数据集</em> </strong>。如果我们有这样的数据集，模型会偏向<strong class="iw hj">和</strong>，导致<strong class="iw hj">模型过拟合</strong>。</p><p id="10f0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了避免这种情况，我们尝试对数据集进行采样。</p><ol class=""><li id="8af9" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated"><strong class="iw hj">什么是采样？</strong></li></ol><figure class="kd ke kf kg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kc"><img src="../Images/7e978182c4ae99fb2a0863d004b7bc13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xf9e1EaoK5n05izIFBouA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来自谷歌。</figcaption></figure><p id="31e4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">抽样意味着增加少数类记录或删除多数类记录，以使数据集成为平衡数据集。</p><p id="2f1e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">抽样可以应用于二进制或多类分类问题。</p><p id="1004" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 2。采样中有哪些技巧？</strong></p><ul class=""><li id="a21c" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated">过采样</li><li id="01ea" class="jt ju hi iw b ix ki jb kj jf kk jj kl jn km jr kh jz ka kb bi translated">欠采样</li><li id="1ef9" class="jt ju hi iw b ix ki jb kj jf kk jj kl jn km jr kh jz ka kb bi translated">组合过采样和欠采样</li><li id="485b" class="jt ju hi iw b ix ki jb kj jf kk jj kl jn km jr kh jz ka kb bi translated">添加类别权重</li></ul></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><blockquote class="ku kv kw"><p id="64ef" class="iu iv js iw b ix iy iz ja jb jc jd je kx jg jh ji ky jk jl jm kz jo jp jq jr hb bi translated"><strong class="iw hj">过采样:</strong></p></blockquote><ul class=""><li id="121b" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated"><strong class="iw hj">randomversample</strong>:它<strong class="iw hj">复制</strong>少数类记录</li><li id="a61b" class="jt ju hi iw b ix ki jb kj jf kk jj kl jn km jr kh jz ka kb bi translated"><strong class="iw hj"> SMOTE </strong> : It <strong class="iw hj">重采样</strong>少数类记录。</li></ul><blockquote class="ku kv kw"><p id="70d2" class="iu iv js iw b ix iy iz ja jb jc jd je kx jg jh ji ky jk jl jm kz jo jp jq jr hb bi translated"><strong class="iw hj"> RandomOverSample : </strong></p></blockquote><ul class=""><li id="c2e9" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated">它随机复制少数民族的记录。</li><li id="5d7f" class="jt ju hi iw b ix ki jb kj jf kk jj kl jn km jr kh jz ka kb bi translated">跟大家讲:目标有1和0..{1: 5, 0: 2}.应用RandomOverSampling后，少数类0变为{1: 5，0: 5}</li><li id="e8e1" class="jt ju hi iw b ix ki jb kj jf kk jj kl jn km jr kh jz ka kb bi translated">为了理解，让我们假设一个数据帧有几个目标为0，1的记录。</li></ul><pre class="kd ke kf kg fd la lb lc ld aw le bi"><span id="e30f" class="lf lg hi lb b fi lh li l lj lk">import pandas as pd <br/>data = [['tom',10,1], ['nick',11, 1], ['juli',10, 0], ['tommy',11, 1], ['hilton',11, 1], ['Mark',10, 0], ['Ani',11, 1]] <br/>df = pd.DataFrame(data, columns = ['Name','Class','Target']) <br/>df</span></pre><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/a8c6a3ed47f2a149ce5ee22a40daf224.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*w57A5-ZogK4uIEmF8eERCg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">朱莉和阿尼有0..现在1出现5次，0出现2次。</figcaption></figure><p id="2c77" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">应用RandomOverSampling后。</p><pre class="kd ke kf kg fd la lb lc ld aw le bi"><span id="2a16" class="lf lg hi lb b fi lh li l lj lk">from imblearn.over_sampling import RandomOverSampler</span><span id="fe4b" class="lf lg hi lb b fi lm li l lj lk">X = df.iloc[:, 0:2].values<br/>y = df.iloc[:, -1].values</span><span id="fa90" class="lf lg hi lb b fi lm li l lj lk">os =  RandomOverSampler(sampling_strategy='minority')<br/>X_new, y_new = os.fit_sample(X, y)<br/>print('Original dataset shape {}'.format(Counter(y)))<br/>print('Resampled dataset shape {}'.format(Counter(y_new)))</span><span id="cc8f" class="lf lg hi lb b fi lm li l lj lk">********************** OUTPUT *****************<br/>Original dataset shape Counter({1: 5, 0: 2})<br/>Resampled dataset shape Counter({0: 5, 1: 5})</span></pre><p id="c8ba" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">采样后，类0现在有5条记录。让我们看看数据帧。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/936de65406185da112aa6694035fadf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*yqcGhwq85glOiv8TN0BbzQ.png"/></div></figure><ul class=""><li id="941e" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated">RandomOverSampler(sampling _ strategy = ' minority ')在0.1到1之间更改采样策略，0.5表示50%的少数类被复制。0.8意味着80%的少数类被复制。</li></ul><blockquote class="ku kv kw"><p id="b3e0" class="iu iv js iw b ix iy iz ja jb jc jd je kx jg jh ji ky jk jl jm kz jo jp jq jr hb bi translated"><strong class="iw hj"> SMOTE </strong> : <strong class="iw hj">合成少数过采样技术</strong></p></blockquote><ul class=""><li id="3a27" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated">它综合少数民族的新例子，而不是复制记录。</li><li id="b634" class="jt ju hi iw b ix ki jb kj jf kk jj kl jn km jr kh jz ka kb bi translated">SMOTE采用k-最近邻，并在特征空间中找到最近的点。然后，它画一条线来连接所有最近的邻居，并在这条线内创建新点。</li></ul><figure class="kd ke kf kg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/f82e96fa9709cd05619c664fb001805c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DcKN4RdfH3ufT50kjAq-ug.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">红色圆圈是原始点，虚线是取n个最近邻点后画出的线，绿色圆圈表示从虚线新创建的新数据点。</figcaption></figure><pre class="kd ke kf kg fd la lb lc ld aw le bi"><span id="8461" class="lf lg hi lb b fi lh li l lj lk"><strong class="lb hj">from</strong> <!-- -->imblearn.over_sampling <strong class="lb hj">import</strong> <!-- -->SMOTE<br/>sm <strong class="lb hj">=</strong> <!-- -->SMOTE(random_state <strong class="lb hj">=</strong> <!-- -->42)<br/>X_res, y_res <strong class="lb hj">=</strong> <!-- -->sm.fit_resample(X_data, Y_data)</span></pre></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><blockquote class="ku kv kw"><p id="ad52" class="iu iv js iw b ix iy iz ja jb jc jd je kx jg jh ji ky jk jl jm kz jo jp jq jr hb bi translated"><strong class="iw hj">随机下样</strong>:</p></blockquote><ul class=""><li id="2d0c" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated">它从多数类标签中删除记录以匹配少数类标签。</li><li id="8433" class="jt ju hi iw b ix ki jb kj jf kk jj kl jn km jr kh jz ka kb bi translated">局限性是大部分数据都被删除了。被删除的记录可能具有有用的洞察力，或者不同的模式。通过检测，我们将丢失主要的重要记录。</li></ul><pre class="kd ke kf kg fd la lb lc ld aw le bi"><span id="5003" class="lf lg hi lb b fi lh li l lj lk">from imblearn.under_sampling import RandomUnderSampler</span><span id="3250" class="lf lg hi lb b fi lm li l lj lk">X = df.iloc[:, 0:2].values<br/>y = df.iloc[:, -1].values</span><span id="5cf0" class="lf lg hi lb b fi lm li l lj lk">os = RandomUnderSampler(sampling_strategy='majority')<br/>X_new, y_new = os.fit_sample(X, y)<br/>print('Original dataset shape {}'.format(Counter(y)))<br/>print('Resampled dataset shape {}'.format(Counter(y_new)))</span><span id="37bb" class="lf lg hi lb b fi lm li l lj lk">************ OUTPUT ***************************<br/>Original dataset shape Counter({1: 5, 0: 2})<br/>Resampled dataset shape Counter({0: 2, 1: 2})</span></pre><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/ea31dcebd1129d31008a9f777aca6841.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*Rg14oDpDsr-sPDC7HN8Cog.png"/></div></figure><blockquote class="ku kv kw"><p id="bc3f" class="iu iv js iw b ix iy iz ja jb jc jd je kx jg jh ji ky jk jl jm kz jo jp jq jr hb bi translated"><strong class="iw hj">组合过采样和欠采样:</strong></p></blockquote><ul class=""><li id="c384" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated">最好将过采样和欠采样结合在一起。</li><li id="3300" class="jt ju hi iw b ix ki jb kj jf kk jj kl jn km jr kh jz ka kb bi translated">首先对少数类标注应用50 %的过采样，然后对多数类标注应用20%或30%的欠采样。</li><li id="495f" class="jt ju hi iw b ix ki jb kj jf kk jj kl jn km jr kh jz ka kb bi translated">通过这样做，我们可能不会丢失主要的数据点，而是只丢失20%或30%的数据点。</li></ul><pre class="kd ke kf kg fd la lb lc ld aw le bi"><span id="ef04" class="lf lg hi lb b fi lh li l lj lk"># Perform Over Sampling<br/>over = RandomOverSampler(sampling_strategy=0.1)<br/>X, y = over.fit_resample(X, y)</span><span id="0f2f" class="lf lg hi lb b fi lm li l lj lk"># Perform Under Sampling</span><span id="e886" class="lf lg hi lb b fi lm li l lj lk">under = RandomUnderSampler(sampling_strategy=0.5)<br/>X, y = under.fit_resample(X, y)</span></pre><blockquote class="ku kv kw"><p id="0893" class="iu iv js iw b ix iy iz ja jb jc jd je kx jg jh ji ky jk jl jm kz jo jp jq jr hb bi translated"><strong class="iw hj">添加类权重:</strong></p></blockquote><p id="4178" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">代替添加或删除数据点，我们可以向少数类标签添加更多的权重<strong class="iw hj">。</strong></p><p id="4179" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们可以显式地添加权重，或者简单地指定class_weight="balanced "</p><pre class="kd ke kf kg fd la lb lc ld aw le bi"><span id="3f03" class="lf lg hi lb b fi lh li l lj lk">from sklearn.utils import class_weight<br/>class_weights = class_weight.compute_class_weight('balanced',np.unique(y),y)<br/>print(np.unique(y),class_weights)</span><span id="a689" class="lf lg hi lb b fi lm li l lj lk">[0 1] [1.75 0.7 ]</span></pre><p id="fe6c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">指定0:1.75和1:0.7的权重。</p><p id="b15b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">示例:</strong>随机森林分类器</p><p id="e77d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">RandomForestClassifier(n _ estimators = 50，class_weight = {0:0.5，1:1})</p><p id="8a38" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">(或)</p><p id="be9e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">RandomForestClassifier(n _ estimators = 50，class_weight ='balanced ')</p></div></div>    
</body>
</html>