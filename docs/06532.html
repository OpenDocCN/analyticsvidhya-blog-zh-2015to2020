<html>
<head>
<title>Spark Tuning and Debugging</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">火花调谐和调试</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/spark-tuning-and-debugging-fe32fded8454?source=collection_archive---------17-----------------------#2020-05-25">https://medium.com/analytics-vidhya/spark-tuning-and-debugging-fe32fded8454?source=collection_archive---------17-----------------------#2020-05-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7763" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在前一章中，我们讨论了<a class="ae jd" rel="noopener" href="/analytics-vidhya/spark-sql-and-dataframes-72e58fe90f94">结构化Spark API</a>，例如数据集、数据帧和Spark SQL。</p><p id="4604" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本章中，我们将研究如何调试和调整Spark程序以提高性能。</p><p id="77fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="je">火花设置:</em> </strong></p><p id="5f8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在执行spark作业时，为应用程序提供最佳资源非常重要。为此，您可以设置以下值:</p><p id="266e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je"> spark.executor.instances，spark.executor.cores，spark.driver.memory，spark.executor.memory，spark . yarn . executor . memory overhead</em></p><p id="52fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些配置可以在spark程序中设置，也可以在spark提交过程中设置，或者在默认spark配置文件中设置。</p><p id="1680" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="je">缓存/持久/检查点:</em> </strong></p><p id="3c5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每当你运行RDD的行动多次，它的重新计算将发生在默认情况下，即使是同一个RDD。所以如果你想在各种动作中多次使用同一个RDD，可以使用<strong class="ih hj"><em class="je">rdd . persist()</em></strong>持久化它，避免每次都要重新读取或者重新计算。如果您只使用一次RDD/数据集，则没有必要保留它。</p><p id="e5b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="je"> RDD.cache() </em> </strong>与persist相同，但具有默认存储级别，即它将数据存储在MEMORY_ONLY(仅用于RDD)和MEMORY_AND_DISK(用于数据帧)中。</p><p id="76ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你也可以<strong class="ih hj"> <em class="je">检查站()</em> </strong>你的RDD。对于检查点，不会维护RDD的沿袭，数据会以物理方式存储到HDFS，因此即使在Spark作业终止后，它仍将可用。在cache()/persist()的情况下存储沿袭，但是一旦作业完成，数据将被销毁。检查点将有助于处理长操作链的故障。</p><p id="8e24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有以下存储选项可供rdd持久存储:</p><p id="f6f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je"> MEMORY_ONLY </em> —作为未序列化的Java对象存储在JVM堆中。如果RDD不适合内存，一些分区将不会被缓存，并将在每次需要时动态地重新计算。</p><p id="7a7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je">MEMORY _ AND _ DISK</em>—JVM中反序列化的Java对象。如果RDD不适合内存，存储不适合磁盘的分区，并在需要时从那里读取它们。</p><p id="4627" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je"> MEMORY_ONLY_SER </em>和<em class="je"> MEMORY_AND_DISK_SER </em> —序列化的Java对象</p><p id="1e97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je"> DISK_ONLY_2 </em> —仅在磁盘上存储RDD分区，但在两个集群节点上复制每个分区</p><p id="baf0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Python中，存储的对象总是用pickle库序列化。</p><p id="395f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择存储选项将基于<strong class="ih hj">内存与CPU使用率</strong>。</p><p id="0215" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="je">火花UI: </em> </strong></p><p id="f2de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当您运行spark应用程序时，它还会在端口4040(默认)或任何下一个可用端口上启动Spark UI。Spark UI仅在Spark应用程序执行期间可用。此后，它将在历史服务器上可用(默认端口为18080)。处理应用程序日志以显示在历史服务器上需要一些时间。Spark UI为应用程序的重要信息提供了以下选项卡。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/b0e7a1cb4f13371ab736f4190007f168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SwjFcwhAlnGL3mlgWWD6Lg.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">Spark UI</figcaption></figure><p id="e573" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je">作业</em> —提供Spark应用程序中所有作业的摘要、事件时间表。单击特定作业后，您可以查看其状态、阶段、事件时间线和DAG可视化。</p><p id="62eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je">阶段</em> —提供所有工作的阶段细节。单击特定阶段，您可以看到其DAG可视化、事件时间表和摘要/聚合指标。</p><p id="f28f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je">存储</em> —存储选项卡显示rdd和数据帧的详细信息(但只显示持久化的数据帧，如果有的话)，如分区数量、存储级别等。</p><p id="ae43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je">环境</em> —该选项卡显示所有的配置变量。您可以在这里检查您在spark-submit或spark程序中设置的属性是否设置正确。用户没有明确设置的所有其他属性将由管理员设置，或者是spark.conf中的默认spark配置。</p><p id="e479" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je">执行者</em> —该选项卡显示为该应用程序创建的所有执行者的信息。它将有一个驱动程序和一个或多个其他执行者的信息，如内存，磁盘使用和洗牌。</p><p id="af42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je"> SQL </em> —显示SQL查询的详细信息，如持续时间、作业、DAG和执行计划(解析的逻辑计划、分析的逻辑计划、优化的逻辑计划和物理计划)。</p><p id="ca7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Dag、计划、任务、汇总指标、使用的内存、混洗数据大小、执行时间等有助于了解执行流程，以及它们是否是需要解决的瓶颈。如果Spark作业失败，您可以在YARN UI上查看详细的日志。</p><p id="80b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Rest APIs可从以下位置获得:<em class="je">https://&lt;Spark-server&gt;:4040/API/v1</em>用于Spark UI，以及<em class="je">https://&lt;history-server&gt;:18080/API/v1</em>用于历史服务器。</p><p id="fe8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">共享变量:</strong></p><p id="f507" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="je">广播</em> : </strong>广播一个变量有助于在每台机器上缓存那个变量(不可变的)，而不是把它的副本和任务一起发送出去。它可以用来复制大型数据集，以便使用高效的广播算法在许多Spark操作中重用它，从而降低网络成本。</p><p id="1544" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当SQL连接两个数据集/数据帧时，也可以使用广播。我们将给出一个提示，向集群的每个工作节点广播要加入的小数据集。</p><p id="7177" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="je">累加器:</em> </strong>累加一个变量(可变的)有助于将来自不同任务的数据添加/收集到一个共享的结果中。</p><p id="9498" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Broadcast join: </strong>使用pyspark . SQL . functions . Broadcast()，您可以显式地给SQL join一个提示，以便将特定的表广播为:</p><pre class="jg jh ji jj fd jv jw jx jy aw jz bi"><span id="3dcf" class="ka kb hi jw b fi kc kd l ke kf">broadcast(spark.table(“spark_table”)).join(spark.table(“new_table”), “key”)</span></pre><p id="e906" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了提高性能，应在火花作业中尽早应用过滤器。</p><p id="5097" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果从数据库中读取数据，将使用查询下推将过滤器推送到DB，并且可以使用谓词下推将数据并行导入到不相交谓词的各个分区中。</p><p id="2dd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还可以使用SQL和dataframes之类的高级结构化API来实现更好的内部存储和执行优化。</p><p id="9644" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，我们讨论了优化和监控Spark应用程序的不同方法。</p></div></div>    
</body>
</html>