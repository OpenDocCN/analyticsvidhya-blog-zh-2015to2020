<html>
<head>
<title>A Hands-On Introduction to Deep Q-Learning using OpenAI Gym in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python中的OpenAI Gym进行深度Q学习的实践介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-hands-on-introduction-to-deep-q-learning-using-openai-gym-in-python-b15d7d8597d?source=collection_archive---------0-----------------------#2019-04-18">https://medium.com/analytics-vidhya/a-hands-on-introduction-to-deep-q-learning-using-openai-gym-in-python-b15d7d8597d?source=collection_archive---------0-----------------------#2019-04-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="c98a" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="0295" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我一直对游戏很着迷。在紧张的时间表下执行一个动作似乎有无限的选择——这是一种令人兴奋的体验。没有什么比这更好的了。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kb"><img src="../Images/da63b3b7715a79f1a40175a561ae59a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6gEcgzWdUpALkCpf.png"/></div></div></figure><p id="8966" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">因此，当我读到DeepMind正在开发的令人难以置信的算法(如AlphaGo和AlphaStar)时，我就被迷住了。我想学习如何在自己的机器上制作这些系统。这让我进入了深度强化学习的世界。</p><p id="0217" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">即使你不喜欢游戏，深度学习也很重要。请查看当前使用Deep RL进行研究的各种功能:</p><p id="12c7" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">行业就绪的应用程序呢？这里有两个最常被引用的深度RL使用案例:</p><ul class=""><li id="5a8a" class="ks kt hi jf b jg kn jk ko jo ku js kv jw kw ka kx ky kz la bi translated">谷歌的云汽车</li><li id="09ed" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">脸书的地平线平台</li></ul><p id="e298" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">深度RL的范围是巨大的。这是进入这个领域并以此为职业的大好时机。</p><p id="a931" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">在这篇文章中，我的目标是帮助你迈出进入深度强化学习世界的第一步。我们将使用RL中最流行的算法之一，深度Q学习，来理解深度RL是如何工作的。还有锦上添花？我们将使用Python在一个很棒的案例研究中实现我们所有的学习。</p><h1 id="6b49" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">目录</h1><ol class=""><li id="1837" class="ks kt hi jf b jg jh jk jl jo lg js lh jw li ka lj ky kz la bi translated">Q-Learning之路</li><li id="0c1e" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">为什么要进行“深度”Q学习？</li><li id="15ae" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">深度Q学习简介</li><li id="3fa7" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">与深度学习相比，深度强化学习的挑战</li><li id="a1fb" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">体验回放</li><li id="626c" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">目标网络</li><li id="9aef" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">使用Keras &amp; Gym在Python中实现深度Q学习</li></ol><h1 id="52a5" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">Q-Learning之路</h1><p id="a850" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在深入深度强化学习之前，您应该了解一些概念。别担心，我会掩护你的。</p><p id="bcd5" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">我以前写过各种关于强化学习的具体细节的文章，介绍像多臂强盗、动态编程、蒙特卡罗学习和时间差分这样的概念。我建议按以下顺序浏览这些指南:</p><p id="12ce" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">这些文章足以从一开始就获得基础RL的详细概述。</p><p id="0dfe" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated"><em class="lk">然而，请注意，上面链接的文章绝不是读者理解深度Q学习的先决条件。在探索什么是深度Q学习及其实现细节之前，我们将快速回顾一下基本的RL概念。</em></p><h1 id="7275" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">RL代理-环境</h1><p id="3602" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">强化学习任务是关于训练一个<strong class="jf hj">代理</strong>，它与它的<strong class="jf hj">环境</strong>交互。代理通过执行<strong class="jf hj">动作</strong>到达被称为<strong class="jf hj">状态</strong>的不同场景。行动会带来积极和消极的回报。</p><p id="d975" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">代理人在这里只有一个目的——在一集里最大化它的总报酬。这一集是在环境中第一个状态和最后一个或最终状态之间发生的任何事情。我们通过经验强化代理学习执行最佳操作。这就是战略或<strong class="jf hj">政策</strong>。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es ll"><img src="../Images/791eeaf36dd6b5ed2dbdeddbb8f60328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GQGomAHdb8rUTzpz.png"/></div></div></figure><p id="534e" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">让我们以超受欢迎的PubG游戏为例:</p><ul class=""><li id="2f64" class="ks kt hi jf b jg kn jk ko jo ku js kv jw kw ka kx ky kz la bi translated">士兵是与环境互动的代理人</li><li id="d499" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">这些状态正是我们在屏幕上看到的</li><li id="4f51" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">一集是一个完整的游戏</li><li id="c921" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">动作有向前、向后、向左、向右、跳、蹲下、射门等。</li><li id="b791" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">奖励是根据这些行动的结果来确定的。如果士兵能够杀死一个敌人，那就需要一个积极的奖励，而被敌人击中则是一个消极的奖励</li></ul><p id="ba30" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">现在，为了杀死敌人或获得积极的奖励，需要一系列的行动。这就是延迟或推迟奖励的概念发挥作用的地方。RL的关键是学习执行这些序列，并最大化回报。</p><h1 id="fe50" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">马尔可夫决策过程(MDP)</h1><p id="660b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">需要注意的重要一点是，环境中的每个状态都是前一个状态的结果，而前一个状态又是前一个状态的结果。然而，存储所有这些信息，甚至对于具有短情节的环境，将变得不可行。</p><p id="72f1" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">为了解决这个问题，我们假设每个状态遵循一个马尔可夫性质，即每个状态仅依赖于前一个状态以及从该状态到当前状态的转换。查看下面的迷宫，更好地理解其工作原理背后的直觉:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lm"><img src="../Images/8f0f3f006e7ce2008fbdd1d459ef035b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8ugcEPhpSgATiMQn.png"/></div></div></figure><p id="6edb" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">现在，有两个场景，有两个不同的起点，代理通过不同的路径到达相同的倒数第二个状态。现在，代理通过什么路径到达红色状态并不重要。离开迷宫并到达最后一个状态的下一步是向右走。显然，我们只需要红色/倒数第二个状态的信息来找出下一个最佳行动，这正是马尔可夫属性所暗示的。</p><h1 id="5b41" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">q学习</h1><p id="0995" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">假设我们知道每一步每一个行动的预期回报。这基本上就像是代理的备忘单！我们的代理会准确地知道该执行哪个操作。</p><p id="43e6" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">它将执行一系列的行动，最终产生最大的总回报。这种总回报也称为Q值，我们将我们的战略正式化为:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es ln"><img src="../Images/9ff5e8b85c54bee94dad8513d3fef0c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OjPCeHuEoEIGGecv.png"/></div></div></figure><p id="d454" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">上面的等式表明，处于状态<em class="lk"> s </em>并执行动作<em class="lk"> a </em>所产生的Q值是即时回报r(s，a)加上下一状态<em class="lk">s’</em>可能的最高Q值。这里的Gamma是贴现因子，它进一步控制未来奖励的贡献。</p><p id="abe0" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">Q(s’，a)再次依赖于Q(s”，a)，于是Q(s”，a)将具有γ平方系数。因此，Q值取决于未来状态的Q值，如下所示:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lo"><img src="../Images/bfa42c029de0ed00ce078f660c036790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kALnvjz_MqStS7sr.png"/></div></div></figure><p id="13db" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">调整gamma的值会减少或增加未来奖励的贡献。</p><p id="560a" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">由于这是一个递归方程，我们可以从任意假设所有q值开始。有了经验，就会收敛到最优策略。在实际情况下，这是作为更新来实现的:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lp"><img src="../Images/661809be94cb915f806c147c854efcaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*j68EjSODP-3yfXtv.png"/></div></div></figure><p id="b48d" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">其中α是学习速率或步长。这仅仅决定了新获得的信息覆盖旧信息的程度。</p><h1 id="9156" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">为什么要进行“深度”Q学习？</h1><p id="ac68" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Q-learning是一个简单但非常强大的算法，可以为我们的代理创建一个备忘单。这有助于代理准确地计算出要执行的操作。</p><p id="8586" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">但是如果这个备忘单太长了怎么办？想象一个有10，000个状态，每个状态有1，000个动作的环境。这将创建一个包含1000万个单元格的表格。事情会很快失控！</p><p id="7c3b" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">很明显，我们不能从已经探索过的态中推断出新态的Q值。这提出了两个问题:</p><ul class=""><li id="8cac" class="ks kt hi jf b jg kn jk ko jo ku js kv jw kw ka kx ky kz la bi translated">首先，保存和更新该表所需的内存量会随着状态数量的增加而增加</li><li id="dc5a" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">第二，探索每个状态以创建所需的Q表所需的时间是不现实的</li></ul><p id="0c70" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">这里有一个想法——如果我们用机器学习模型(如神经网络)来近似这些Q值会怎么样？嗯，这就是DeepMind算法背后的想法，导致它被谷歌以5亿美元收购！</p><h1 id="84a6" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">深度Q-网络</h1><p id="afde" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在深度Q学习中，我们使用神经网络来逼近Q值函数。状态作为输入给出，所有可能动作的Q值作为输出产生。Q-learning和深度Q-learning之间的比较如下图所示:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lq"><img src="../Images/59e75def5c6c628254d9fad68b0ec5ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6p-EnwJgeyUbcoXo.png"/></div></div></figure><p id="6653" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">那么，使用深度Q学习网络(dqn)进行强化学习涉及到哪些步骤呢？</p><ol class=""><li id="6544" class="ks kt hi jf b jg kn jk ko jo ku js kv jw kw ka lj ky kz la bi translated">所有过去的经验都由用户存储在存储器中</li><li id="71ee" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">下一个动作由Q网络的最大输出决定</li><li id="ddf8" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">这里的损失函数是预测Q值和目标Q值的均方误差— Q*。这基本上是一个回归问题。然而，我们不知道这里的目标或实际值，因为我们正在处理一个强化学习问题。回到从贝尔曼方程导出的Q值更新方程。我们有:</li></ol><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lr"><img src="../Images/939e879f25557565e21edbe1864a3c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*EKXOQ8z0TAkvWQak.png"/></div></div></figure><p id="2568" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">绿色部分代表目标。我们可以说它在预测自己的值，但由于R是无偏的真实回报，网络将使用反向传播来更新其梯度，以最终收敛。</p><h1 id="ad93" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">与深度学习相比，深度学习面临的挑战</h1><p id="24e7" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">到目前为止，这一切看起来都很好。我们了解神经网络如何帮助代理学习最佳行动。然而，当我们将深度RL与深度学习(DL)进行比较时，存在一个挑战:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es ls"><img src="../Images/6e7028f06354673ecab642eacb12b24c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IDdM4DkPAOthBLam.png"/></div></div></figure><p id="d5c6" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">正如您在上面的代码中看到的，目标在每次迭代中不断变化。在深度学习中，目标变量不会改变，因此训练是稳定的，这对于RL来说是不正确的。</p><p id="817e" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">总之，我们经常依靠强化学习中的策略或价值函数来采样动作。然而，随着我们不断学习探索什么，这种情况经常会发生变化。随着游戏的进行，我们对状态和动作的基本真值有了更多的了解，因此输出也在变化。</p><p id="c313" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">因此，我们试图学习映射一个不断变化的输入和输出。但是解决办法是什么呢？</p><h1 id="5838" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">1.目标网络</h1><p id="a173" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">由于同一个网络正在计算预测值和目标值，这两者之间可能会有很大差异。因此，我们可以使用两个神经网络，而不是使用一个神经网络进行学习。</p><p id="98fc" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">我们可以使用单独的网络来估计目标。该目标网络具有与函数逼近器相同的架构，但是具有冻结的参数。对于每C次迭代(超参数)，来自预测网络的参数被复制到目标网络。这导致更稳定的训练，因为它保持目标函数固定(一段时间):</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lt"><img src="../Images/b394a79b888c0272c1eb7526a84225d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pEP_Azy3sA94bUtj.png"/></div></div></figure><h1 id="09fa" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">2.体验回放</h1><p id="4325" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">以上说法是什么意思？当状态/动作对在模拟或实际体验中出现时，系统不是在状态/动作对上运行Q-learning，而是将[状态、动作、奖励、下一状态]的发现数据存储在一个大表中。</p><p id="a88d" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">让我们用一个例子来理解这一点。</p><p id="95fd" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">假设我们试图构建一个视频游戏机器人，游戏的每一帧代表一个不同的状态。在训练期间，我们可以从最后100，000帧中随机抽取64帧来训练我们的网络。这将使我们得到一个子集，其中样本之间的相关性较低，并且还将提供更好的采样效率。</p><h1 id="884b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">把所有的放在一起</h1><p id="a8d8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">到目前为止我们学到的概念？它们结合在一起形成了深度Q学习算法，该算法用于在Atari游戏中实现人类水平的性能(仅使用游戏的视频帧)。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es lu"><img src="../Images/baf032ae07587ddb4d660c3e89a6681b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4jpojwBp6wycnf3U.png"/></div></div></figure><p id="c45e" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">我在下面列出了深度Q网络(DQN)涉及的步骤:</p><ol class=""><li id="0903" class="ks kt hi jf b jg kn jk ko jo ku js kv jw kw ka lj ky kz la bi translated">预处理并将游戏屏幕(状态s)输入到我们的DQN，它将返回状态中所有可能动作的Q值</li><li id="8dcf" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">使用ε-greedy策略选择一个操作。对于概率ε，我们选择随机动作<em class="lk"> a </em>，对于概率1-ε，我们选择具有最大Q值的动作，例如a = argmax(Q(s，a，w))</li><li id="7f4f" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">在一个州<em class="lk"> s </em>执行这个动作，然后移动到一个新的州<em class="lk">s’</em>领取奖励。这个状态s’是下一个游戏屏幕的预处理图像。我们将这个过渡存储在我们的重放缓冲器中作为&lt; s，a，r，s’&gt;</li><li id="f9ff" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">接下来，从重放缓冲区中随机抽取一些转换批次，并计算损失</li><li id="efa1" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka lj ky kz la bi translated">众所周知:</li></ol><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lv"><img src="../Images/67b777c99f43927e1d0c211c9f133cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/0*AGMlkLDglC776jIk.png"/></div></figure><p id="caf6" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">也就是目标Q和预测Q的平方差</p><p id="75f6" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">6.相对于我们的实际网络参数执行梯度下降，以最小化这种损失</p><p id="ad18" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">7.每C次迭代后，将我们的实际网络权重复制到目标网络权重</p><p id="c693" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">8.重复这些步骤达<em class="lk"> M </em>集</p><p id="a150" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">现在来看看实际情况，在<a class="ae lw" href="https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/" rel="noopener ugc nofollow" target="_blank"> Analytics Vidhya博客</a>上有一个很棒的python案例研究，使用Keras-rl库和Deep Q学习来解决Cartpole问题。看看这个。</p></div></div>    
</body>
</html>