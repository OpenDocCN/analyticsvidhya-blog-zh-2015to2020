<html>
<head>
<title>YOLOv3 Object Detection in TensorFlow 2.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2.x中的YOLOv3对象检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/yolov3-object-detection-in-tensorflow-2-x-8a1a104c46a8?source=collection_archive---------1-----------------------#2020-10-18">https://medium.com/analytics-vidhya/yolov3-object-detection-in-tensorflow-2-x-8a1a104c46a8?source=collection_archive---------1-----------------------#2020-10-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f0a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你只看一次(YOLO)是一个最先进的，实时对象检测系统，是令人难以置信的快速和准确。在本文中，我们介绍了对象检测的概念，YOLO算法，并在TensorFlow 2.0中实现了这样一个系统。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/051bc8e2c189d150f60eef32fd1f93b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lphzxAZMToiKwYaeiTou2Q.jpeg"/></div></div></figure><h1 id="b196" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">物体检测</strong></h1><p id="53fc" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">对象检测是一种计算机视觉技术，用于在图像或视频中定位对象的实例。它是监控系统、图像检索系统和高级驾驶辅助系统等应用背后的关键技术。</p><p id="12c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些系统不仅包括识别和分类图像中的每一个物体，还包括<em class="ks">通过在物体周围画出合适的边界框来定位每一个物体。</em></p><p id="c661" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对象检测有不同的算法，它们可以分为两组:</p><ol class=""><li id="a887" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated">基于分类的算法:这些算法分两个阶段工作。首先，我们从图像中选择感兴趣的区域。然后，我们使用卷积神经网络对这些区域进行分类。这个过程非常慢，因为我们必须对每个选定的区域进行预测。这类算法的例子是基于区域的卷积神经网络(RCNN)及其版本Fast-RCNN和Fast-RCNN。</li><li id="ffd4" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">基于回归的算法:我们不是从图像中选择感兴趣的区域(ROI ),而是在算法的一次运行中预测图像的类别和边界框。这类算法的例子是<strong class="ih hj"> YOLO(你只看一次)。</strong></li></ol><h1 id="68a2" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> YOLO </strong></h1><p id="ac1d" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">Joseph Redmon首先描述了YOLO模型。在2015年题为“<a class="ae lh" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的实时物体检测</a>”的论文中</p><p id="4f5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首次出版时(2016。)与R-CNN和FRCNN等系统相比，YOLO已经实现了最先进的mAP(平均精度)。另一方面，YOLO努力精确定位物体。然而，它学习对象的一般表示。在新版本中，在速度和准确性上都有一些改进。</p><blockquote class="li"><p id="7801" class="lj lk hi bd ll lm ln lo lp lq lr jc dx translated">我们将对象检测重新定义为一个单一的回归问题，直接从图像像素到边界框坐标和类别概率。</p></blockquote><p id="d1cb" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated">— <a class="ae lh" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测</a>，2015。</p><p id="a029" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单地说，我们把一幅图像作为输入，让它通过一个类似于CNN的神经网络，我们在输出中得到一个包围盒和类别预测的向量。</p><p id="4378" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解释预测向量</strong></p><p id="2af3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入图像被分成一个由单元组成的<em class="ks">S×S</em>网格。对于图像上出现的每个对象，一个网格单元被称为负责预测它的<strong class="ih hj"/><em class="ks"/><strong class="ih hj"/>。这是对象中心所在的单元格。</p><p id="a0dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个网格单元预测<em class="ks"> B </em>边界框以及<em class="ks"> C </em>类概率。边界框预测具有五个分量<em class="ks"> x，y，w，h，置信度</em>。<em class="ks"> (x，y) </em>坐标表示相对于网格单元位置的盒子中心。这些坐标被标准化为介于0和1之间。相对于图像尺寸，<em class="ks"> (w，h) </em>框的尺寸也被标准化为[0，1]。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lx"><img src="../Images/359072ddef0594d6dd6f9c294897ccdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*etbDtmj-pqQC2sxtiqC_Bw.png"/></div></div></figure><p id="228f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在包围盒预测中还有一个组件，即置信度得分。</p><blockquote class="li"><p id="7a57" class="lj lk hi bd ll lm ln lo lp lq lr jc dx translated"><em class="ly">形式上我们定义置信度为Pr(Object) * IOU(pred，truth)。如果该单元格中不存在任何对象，置信度得分应该为零。否则，我们希望置信度得分等于预测框和实际情况之间的交集(IOU)。</em></p></blockquote><p id="dd63" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated">— <a class="ae lh" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的实时物体检测</a>，2015。</p><p id="8bb3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个网格单元产生这些预测中的B个，因此总共有<em class="ks"> S x S x B * 5 </em>个与边界框预测相关的输出。</p><p id="40fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还需要预测类概率，<em class="ks"> Pr(Class(i) | Object)。</em>这个概率取决于包含一个对象的网格单元。这意味着如果网格单元上没有对象，损失函数将不会针对错误的类预测对其进行优化。该网络仅预测每个单元的一组类别概率，而不管总共有多少个框<em class="ks"> B、</em>使得<em class="ks"/>S x S x C产生类别概率。</p><p id="59a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，将类预测加到输出向量上，我们得到一个<em class="ks"> S x S x (B * 5 +C) </em>张量作为输出。</p><p id="1704" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">网络</strong></p><p id="9e88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">网络结构看起来像CNN，有卷积和max池层，最后是2个完全连接的层。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lz"><img src="../Images/bfe638aa8c1d4cbb7b6f64f166959947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ab_OTjlniZG77E9Tnv0xxg.png"/></div></div></figure><h2 id="750e" class="ma jq hi bd jr mb mc md jv me mf mg jz iq mh mi kd iu mj mk kh iy ml mm kl mn bi translated">损失函数</h2><p id="e89b" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们只希望其中一个边界框负责图像中的对象，因为YOLO算法为每个网格单元预测了多个边界框。为了实现这一点，我们使用损失函数来计算每个真阳性的损失。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mo"><img src="../Images/8fc9899941643785d4b714c131af104d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ipcAZNtO2O4oZTMsef2uYQ.png"/></div></div></figure><p id="4662" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该等式计算与预测边界框位置<strong class="ih hj"> <em class="ks"> (x，y) </em> </strong>相关的损失。该函数计算每个边界框预测值<strong class="ih hj"> ( <em class="ks"> j = 0)的总和..每个网格单元的b</em></strong><strong class="ih hj">(<em class="ks">I = 0)..S </em> ) </strong>。<strong class="ih hj"> <em class="ks"> 𝟙 obj </em> </strong>定义为:</p><ul class=""><li id="52d2" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc mp kz la lb bi translated">1，如果对象出现在网格单元<em class="ks"> i </em>中，并且第<em class="ks"> j </em>个边界框预测器“负责”该预测</li><li id="20ec" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc mp kz la lb bi translated">0，否则。</li></ul><blockquote class="li"><p id="11cb" class="lj lk hi bd ll lm mq mr ms mt mu jc dx translated"><em class="ly"> YOLO预测每个网格单元有多个边界框。在训练时，我们只希望一个边界框预测器负责每个对象。我们分配一个预测器来“负责”预测一个对象，基于哪个预测具有最高的当前IOU和地面真实值。</em></p></blockquote><p id="6677" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated">— <a class="ae lh" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测</a>，2015。</p><p id="2c66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">等式中的另一项:<strong class="ih hj"> <em class="ks"> (x，y) </em> </strong>是预测的包围盒位置，<strong class="ih hj"> <em class="ks"> (x̂，ŷ) </em> </strong>是来自训练数据的实际位置。</p><p id="1c88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">等式的第二部分，这是与预测的盒子宽度/高度相关的损失。</p><blockquote class="li"><p id="4c27" class="lj lk hi bd ll lm ln lo lp lq lr jc dx translated"><em class="ly">我们的误差指标应该反映大盒子中的小偏差不如小盒子中的小偏差重要。为了部分解决这个问题，我们预测边界框宽度和高度的平方根，而不是直接预测宽度和高度。</em></p></blockquote><p id="4ed5" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated">— <a class="ae lh" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测</a>，2015。</p><p id="53ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">等式的第三部分，这里我们计算与每个边界框预测器的置信度得分相关联的损失。<strong class="ih hj"> <em class="ks"> C </em> </strong>是置信度得分，而<strong class="ih hj"><em class="ks">ĉ</em></strong>是预测边界框与地面真实的交集。<strong class="ih hj"> <em class="ks"> 𝟙 obj </em> </strong>当单元格中有对象时等于1，否则等于0。<strong class="ih hj"> <em class="ks"> 𝟙 noobj </em> </strong>则相反。</p><p id="6d08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ks"> λ </em> </strong>参数用于损失函数的不同权重部分。这是增加模型稳定性所必需的。最高的惩罚是对于坐标预测(<strong class="ih hj"><em class="ks">λcoord</em></strong><em class="ks">= 5)</em>，最低的惩罚是对于没有对象存在时的置信度预测(<strong class="ih hj"><em class="ks">λnoobj</em></strong><em class="ks">= 0.5)</em>。</p><p id="8730" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">损失函数的最后一部分是分类损失，这看起来像是分类的正态平方和误差，除了<strong class="ih hj"> <em class="ks"> 𝟙 obj </em> </strong>项。</p><h1 id="055f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">YOLOv3</h1><p id="566a" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">YOLOv3 是一个实时的单阶段物体检测模型，它建立在<a class="ae lh" href="https://paperswithcode.com/method/yolov2" rel="noopener ugc nofollow" target="_blank"> YOLOv2 </a>的基础上，并做了一些改进。改进包括使用新的主干网络Darknet-53，该网络利用剩余连接，或者用作者的话说，“那些新奇的剩余网络东西”，以及对边界框预测步骤的一些改进，并使用三种不同的尺度来提取特征(类似于FPN)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mv"><img src="../Images/c1d9aa13c0a97b5eff88a4829f92eb2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yy5KaR0f58OPuJ80BNAzSA.png"/></div></div></figure><p id="4b44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原<a class="ae lh" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中提到，YOLOv3有53个卷积层叫做Darknet-53如下图所示，主要由卷积和残差结构组成。应当注意，最后三层Avgpool、Connected和softmax层用于在Imagenet数据集上进行分类训练。当我们使用Darknet-53图层从图片中提取特征时，这三个图层都没有用到。</p><p id="0fd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与ReseNet-101相比，Darknet-53网速是前者的1.5倍；虽然ReseNet-152和它的性能差不多，但是它需要2倍以上的时间。此外，Darknet-53还可以实现每秒最高的测量浮点运算，这意味着网络结构可以更好地利用GPU，从而使其更加高效和快速。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mw"><img src="../Images/f193f3011340c3fc583b34722315df2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*nsU7RrMMNRbZVdzsSC-rOw.png"/></div></div></figure><p id="3772" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">YOLO通过使用32、16和8的步幅以3种不同的尺度进行检测，以适应不同的物体尺寸。</p><p id="c98b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Yolo预测超过3个不同的尺度检测，因此如果我们输入一个大小为416x416的图像，它会产生3个不同的输出形状张量，13 x 13 x 255、26 x 26 x 255和52 x 52 x 255。</p><p id="910a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，当我们馈入大小为416x416的输入图像时，在进入Darknet-53网络后得到3个分支。这些分支经历一系列卷积、上采样、合并和其他操作。最终得到三幅大小不同的特征图，形状分别为[13，13，255]，[26，26，255]和[52，52，255]</p><p id="6831" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">残差网络用于缓解神经网络中因深度增加而导致的梯度消失问题，从而使神经网络更容易优化。</p><h1 id="61bf" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">TF2.0中的实现</h1><p id="9f06" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">这段代码的实现受到了罗卡斯·巴尔西斯的启发，他在自己的<a class="ae lh" href="https://pylessons.com/" rel="noopener ugc nofollow" target="_blank"> pylessons </a>站点上发表了一篇关于Yolov3实现的文章。</p><p id="1076" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设计用于在Python 3.7和TensorFlow 2.0上运行的代码可以在我的<a class="ae lh" href="https://github.com/anushkadhiman/YOLOv3-TensorFlow-2.x" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> GitHub库</strong> </a>中找到。</p><p id="b225" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的回购中，你会发现一个笔记本(。ipynb文件)，它是对图像和视频执行检测代码。</p><p id="2056" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先克隆我的资源库:<br/> <code class="du mx my mz na b">git clone https://github.com/anushkadhiman/YOLOv3-TensorFlow-2.x.git</code></p><p id="f2c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，安装需要的python包:<br/> <code class="du mx my mz na b">pip install -r ./requirements.txt</code></p><p id="4455" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，下载经过训练的yolov3.weights: <br/> <code class="du mx my mz na b">wget -P model_data <a class="ae lh" href="https://pjreddie.com/media/files/yolov3.weights" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/media/files/yolov3.weights</a></code></p><p id="84e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，测试Yolo v3检测:<br/> <code class="du mx my mz na b">python detection_demo.py</code></p><h2 id="86d1" class="ma jq hi bd jr mb mc md jv me mf mg jz iq mh mi kd iu mj mk kh iy ml mm kl mn bi translated">通过预先训练的模型进行检测</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nb"><img src="../Images/3d06ea76cbb89da485dfc84ab9135ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*aFZBri3yg2lxV5eovy9KnA.gif"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nc"><img src="../Images/40943304a0d4528d91b02848c7273faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BUjm6jJZAUJ825FDMvJlOg.jpeg"/></div></div></figure><h2 id="ce6f" class="ma jq hi bd jr mb mc md jv me mf mg jz iq mh mi kd iu mj mk kh iy ml mm kl mn bi translated">由定制训练模型进行检测</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nb"><img src="../Images/4f5f03301379b3589af48b515f339dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*uiv2-UbpBPDzMwUqVE-4qQ.gif"/></div></figure><p id="ba8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我的其他帖子:</strong></p><ol class=""><li id="7a55" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated"><a class="ae lh" rel="noopener" href="/analytics-vidhya/object-tracking-using-deepsort-in-tensorflow-2-ec013a2eeb4f">在TensorFlow 2中使用深度排序进行对象跟踪</a></li><li id="b4c1" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated"><a class="ae lh" rel="noopener" href="/analytics-vidhya/computer-vision-the-present-and-future-2c2c95fa81af">计算机视觉:现在和未来</a></li></ol><p id="0474" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ol class=""><li id="1e0a" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated"><a class="ae lh" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的实时物体检测</a>——约瑟夫·雷德蒙、桑托什·迪夫瓦拉、罗斯·吉斯克、阿里·法尔哈迪</li><li id="3fc1" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated"><a class="ae lh" href="https://arxiv.org/abs/1804.02767" rel="noopener ugc nofollow" target="_blank">约洛夫3:增量改进</a>——约瑟夫·雷德蒙，阿里·法尔哈迪</li><li id="767e" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">https://pylessons.com/——罗卡斯·巴尔西斯</li></ol></div></div>    
</body>
</html>