<html>
<head>
<title>The Recurrent Artificial Neuron</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归人工神经元</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-recurrent-artificial-neuron-ef67778e7a09?source=collection_archive---------19-----------------------#2020-03-09">https://medium.com/analytics-vidhya/the-recurrent-artificial-neuron-ef67778e7a09?source=collection_archive---------19-----------------------#2020-03-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="fa87" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">递归神经网络的构建模块</h2></div><p id="9789" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">人工神经网络介绍系列的前一篇文章解释了递归神经网络(RNN ),它构成了我们今天使用的任何翻译应用程序。今天，我们将讨论这个网络的每个神经元是如何工作的。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es jt"><img src="../Images/fca39565fb6c86f4c73b10c823f393a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*_QB4CjmSfIl6VgxWwXfqGg.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx translated">RNN单位[3]</figcaption></figure><p id="2948" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">递归人工神经元是RNN结构的最基本部分。它具有从激活层的输出到其线性输入的连接，并将输出加回到输入中。在任何时间步，相对于层输入的输出可以给出为:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kf"><img src="../Images/b70eeb0be5912944b88eed61cb5dd984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*dKZ339f1-GrevhxqvXKSlA.png"/></div></figure><p id="7e51" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<em class="kg"> y(t) </em>是时间<em class="kg"> t </em>的输出向量，<em class="kg"> x(t) </em>是时间<em class="kg"> t </em>的输入，<em class="kg"> y </em> ⁽ᵗ⁻ ⁾是前一时间步的输出，<em class="kg"> b </em>是偏置项，φ是激活，可以是tanh或ReLU。输入和输出分别有不同组的权重<em class="kg"> wˣ </em>和<em class="kg">wʸ</em>[1]。</p><p id="3b64" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以上可以以矢量化的形式针对整个层进行计算</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kh"><img src="../Images/70b01649e4f8c6895165a3383d35eb1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*1b5sM_TFIzzYfFv60xkv8A.png"/></div></figure><p id="e671" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中，<em class="kg"> Y⁽ᵗ⁾ </em>为大小为<em class="kg"> (m，n) </em>的输出矩阵，其中<em class="kg"> m </em>为批量实例数，<em class="kg"> n </em>为层单元数，<em class="kg"> X⁽ᵗ⁾ </em>为大小为<em class="kg"> (m，i) </em>的输入矩阵，<em class="kg"> i </em>为输入特征数，<em class="kg"> Wˣ </em>为输入权重矩阵 n) 和<em class="kg"> Wʸ </em>是大小为<em class="kg"> (n，n) </em>的前一步输出的权重矩阵。</p><p id="937e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在每一层，当前存储器<em class="kg"> hᵗ </em>和输出<em class="kg"> y'ᵗ </em>的最终值计算如下</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ki"><img src="../Images/dad3090cd51a35fa1ef90d57031f9fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*D5AOUxZPA4KAnifq7zwldg.png"/></div></figure><p id="985e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kg"> tanh </em>激活函数压缩<em class="kg"> -1 </em>和<em class="kg"> 1 </em>之间的结果，而<em class="kg"> softmax </em>是一个计算概率分布的激活函数。输出<em class="kg"> y'ᵗ </em>是一个与<em class="kg"> xᵗ </em>维数相同的向量，所有元素之和为<em class="kg"> 1 </em>。具有最高概率值的元素是下一个预测单词。</p><p id="f709" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随后，为了比较预测的准确性，将预测字<em class="kg"> y'ᵗ </em>与实际字<em class="kg"> yᵗ </em>进行比较。这是通过损失函数计算的，在这种情况下，交叉熵损失函数描述为</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kj"><img src="../Images/34867067de6db099ef8a813b9f564e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*LVg3ukq33YNPrflRXy_CAQ.png"/></div></figure><p id="4164" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该过程的最后一步是反向传播，其中算法向后遍历所有时间步骤，以更新网络的权重和偏差[2]。</p><p id="ce96" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RNN未能对长而复杂的序列做出好的预测的主要原因是消失/爆炸梯度阻止了有效的学习。这是因为在更新参数时，网络需要计算损失函数导数。关于同一组参数的该操作的多个实例可能导致导数值极大或极小，从而导致未定义的权重和偏差，并进而导致无意义的学习。这个问题通过引入LSTM网络得到了解决。</p><p id="c25e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就结束了我们关于递归神经网络的部分，其中我们回顾了网络的基本结构及其基本单元，递归人工神经元。我们还探讨了网络性能缺陷的原因，这可以归因于其固有的结构。在接下来的章节中，我们将介绍一种替代的长短期记忆网络，它解决了消失/爆炸梯度的问题。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><p id="d268" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[1]朱利安博士，2018。用Pytorch进行深度学习快速入门指南:学习用Python训练和部署神经网络模型，伯明翰:Packt。</p><p id="8250" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[2] Kostadinov，S. &amp; Safari，一家奥莱利传媒公司，2018年。递归神经网络与Python快速入门指南第1版。，Packt出版公司。</p><p id="dbad" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[3]中等。(2020).使用Penn Treebank进行语言建模。[在线]请访问:<a class="ae kr" href="https://towardsdatascience.com/language-modelling-with-penn-treebank-64786f641f6" rel="noopener" target="_blank">https://towards data science . com/language-modeling-with-Penn-tree bank-64786 f641 F6</a>[2020年3月3日访问]。</p></div></div>    
</body>
</html>