<html>
<head>
<title>Ordinary Least Squared (OLS) Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">普通最小平方(OLS)回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ordinary-least-squared-ols-regression-90942a2fdad5?source=collection_archive---------1-----------------------#2020-08-13">https://medium.com/analytics-vidhya/ordinary-least-squared-ols-regression-90942a2fdad5?source=collection_archive---------1-----------------------#2020-08-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="afd9" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">口译:</h2><div class=""/><figure class="ev ex ip iq ir is er es paragraph-image"><div class="er es io"><img src="../Images/37b2bc1bdedc436cde633a401ef0d707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*d3W4j-j-HiN5O8OOBshgrA.gif"/></div></figure><p id="ad0e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">OLS(普通最小二乘)回归是最简单的线性回归模型，也称为线性回归的基础模型。虽然它是一个简单的模型，但在机器学习中，它并没有被赋予太多的权重。OLS就是这样一个模型，它告诉你的不仅仅是整个模型的准确性。它还会告诉你每个变量的表现，如果我们有不想要的变量，如果数据中有自相关，等等。</p><p id="afc7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它也是更容易理解和更直观的技术之一，并且它为学习更高级的概念和技术提供了良好的基础。这篇文章解释了如何使用statsmodels Python包执行线性回归。</p><p id="82e4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">注:</strong>还有一种Logit回归，类似于Sklearn的Logistic回归，对分类问题有效。</p><p id="d9ce" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">OLS反映了X和y变量之间的关系，遵循简单的公式:</p><p id="bfb7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">y = b1X+B0 #简单线性</p><p id="efab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">𝑦 = b0 + b1X1 + b2X2…+𝜀#多线性</p><p id="f694" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在哪里</p><p id="432b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">b0 — y —截距</p><p id="7d53" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">b1、b2 —斜率</p><p id="0b24" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">x，X1，X2-预测值</p><p id="7772" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">y-目标变量</p><p id="3d63" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">OLS是一种估计量，其中b1和b0的值(来自上述等式)以这样的方式选择，即最小化观察的因变量和预测的因变量之间的差的平方和。这就是为什么它被命名为普通最小二乘法。</p><p id="135f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，当模型试图降低预测和实际之间的误差率时，这意味着它试图减少损失并更好地预测。你试图预测你的预测者对结果的影响。</p><p id="29cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">注意:</strong>理想情况下，在使用OLS计算模型构建之前，需要满足线性假设。本文的目的是解释OLS模型中的所有元素。</p><p id="6457" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们更好地理解这个例子，我取了一个简单的数据集——广告数据:</p><figure class="ju jv jw jx fd is er es paragraph-image"><div class="er es jt"><img src="../Images/6c55fbbf14db26156aa344a2823cbf48.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*R1zuK-fhBHTsXu98-NJETg.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">考虑中的数据。数据形状是200x4</figcaption></figure><p id="275d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在线性模型中，1变量的系数依赖于其他自变量。因此，如果数据减少或增加，将会影响整个模型。例如，假设在未来，我们也有另一种广告媒体，比如社交媒体，我们将不得不重新拟合和计算系数和常数，因为它们取决于数据集的维度。</p><p id="092c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您想查看多元线性回归的公式:</p><div class="kc kd ez fb ke kf"><a rel="noopener follow" target="_blank" href="/@nishigandha.sharma.90/multi-linear-regression-6ea7a16c9fe7"><div class="kg ab dw"><div class="kh ab ki cl cj kj"><h2 class="bd hs fi z dy kk ea eb kl ed ef hr bi translated">多元线性回归</h2><div class="km l"><h3 class="bd b fi z dy kk ea eb kl ed ef dx translated">多元线性回归(MLR)，也简称为多元回归，是一种统计技术，使用…</h3></div><div class="kn l"><p class="bd b fp z dy kk ea eb kl ed ef dx translated">medium.com</p></div></div><div class="ko l"><div class="kp l kq kr ks ko kt it kf"/></div></div></a></div><p id="a1ae" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以实际上，不断添加变量并检查它们的线性关系是不可行的。这个想法是使用以下两个步骤来选择最佳变量:</p><p id="5ca5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1.领域知识</p><p id="e3de" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.统计检验-不仅是参数和非参数检验，还检查自变量之间是否存在多重共线性以及与目标变量的相关性。</p><figure class="ju jv jw jx fd is er es paragraph-image"><div class="er es ku"><img src="../Images/b415d0e95a0a831b1977113dba8fff25.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*PSDaQSByHqgQtJHi6_4-yw.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">关联热图</figcaption></figure><p id="92e9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，我们快速检查数据的相关性，很明显，销售和电视广告有很强的相关性。比如说，广告的增加会导致销售额的增加，但是如果像报纸这样的媒体的读者群很低，也可能导致负相关。</p><p id="9fc5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，获得正确的变量是任何模型构建最重要的一步。这将有助于降低处理成本，同时建立正确的机器学习模型。</p><p id="32b0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">步骤1: </strong>导入库并添加常量元素。我们这样做是因为当所有其他包含的回归变量都设置为零时，我们希望我们的因变量取非零值。该值将显示为常数，即1。稍后当我们形成模型时，在我们的多元线性公式中，常数值的系数将是b0。</p><figure class="ju jv jw jx fd is er es paragraph-image"><div class="er es kv"><img src="../Images/210c803481aae968f13259b70546980a.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*B40b4BoQtrgyujw_dh2gwg.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">带有常数的x个变量</figcaption></figure><p id="aef4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">第二步:</strong>拟合X和y变量，检查汇总。现在让我们跑一跑，看看结果。我们将解释该汇总表的每一部分。</p><figure class="ju jv jw jx fd is er es paragraph-image"><div class="er es kw"><img src="../Images/3e8c1eaf0b9d0e9621d1bffd42571d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*cct3l1Xc0wJkJ_yklXfE6Q.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">OLS模型</figcaption></figure><p id="928a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了便于解释，我们将摘要报告分为4个部分。</p><p id="c647" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">第一节:</strong></p><p id="b5e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">总的来说，我们的模型表现良好，准确率达89%。让我们快速切入，首先从左上角部分开始:</p><figure class="ju jv jw jx fd is er es paragraph-image"><div class="er es kx"><img src="../Images/bebe4a08f57fb7ac706c2d46871da3b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*UqqOqgRmaD1WYHrSNHjivA.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">左上部分</figcaption></figure><p id="b3bd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这一部分给出了模型的基本细节，你可以阅读和理解，比如y变量是什么，模型是什么时候建立的等等。让我们来看看红色Df残差中突出显示的元素和Df型号:</p><p id="c6fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> Df残差:</strong>在我们理解这个术语之前，让我们先理解什么是Df和残差:</p><p id="5844" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ky"> Df </em>这里是自由度(Df ),表示在不违反任何约束的情况下，分析中可以变化的独立值的数量。</p><p id="3b9f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ky">回归中的残差</em>简单来说就是模型无法解释的误差率。它是数据点和回归线之间的距离。</p><p id="dc49" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">残差=(观察值)——(拟合值/期望值)</p><p id="5eb1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> <em class="ky"> df(残差)</em> </strong>是样本量减去被估计的参数个数，所以变成df(残差)= n — (k+1)或df(残差)= n -k -1。</p><p id="fe1b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，我们的计算是:</p><p id="841f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">200(总记录)-3(X变量的数量)-1(自由度)</p><p id="ba5c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> Df模型:</strong>数据中X个变量的简单数量，不包括常量变量3。</p><p id="88ca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">第二节:</strong></p><figure class="ju jv jw jx fd is er es paragraph-image"><div class="er es kz"><img src="../Images/366863ff03c4aceb43e85464e9bb27f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*YFzQdStOgr0D2JCKJtwOgw.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">右上部分</figcaption></figure><p id="dbf0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> R平方:</strong>它是因变量y的变化程度，由X中的因变量来解释。就像在我们的情况下，我们可以说，对于给定的X变量和一个多元线性模型，89.7%的方差由模型来解释。在回归分析中，这也意味着我们的预测值与实际值(即y. R2)接近89.7%，并达到0到1之间的值。</p><p id="ddf5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">R2分数的缺点是，X中变量的数量越多，R2倾向于保持不变，或者增加很小的数量。然而，新增加的变量可能重要，也可能不重要。</p><p id="30b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> <em class="ky"> R2 =模型解释的方差/总方差</em> </strong></p><p id="ba68" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> OLS模式:</strong>整体模式R2为89.7%</p><p id="298d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">调整的R平方:</strong>这解决了R2评分的缺点，因此被认为更可靠。R2没有考虑对模型不重要的变量。在一元线性回归中，R2值和调整后的R2值是相同的。如果模型中加入更多的无关紧要的变量，R2与调整后的R2之间的差距将会继续扩大。</p><p id="422f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> <em class="ky">调整后的R平方= 1—[(1—R2)*(n—1))/(n—k—1)]</em></strong></p><p id="7046" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中n —记录的数量，k是除常数外的重要变量的数量。</p><p id="4078" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> OLS模型:</strong>模型调整后的R2为89.6%，比R2低0.1个百分点。</p><p id="154d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">F-统计量和概率(F-统计量):</strong>此处方差分析应用于具有以下假设的模型:</p><p id="c552" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> H0: </strong> b1，b2，b3(回归系数)为0或者没有自变量的模型更好的拟合数据。</p><p id="61ac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> H1: </strong>系数(b1，b2，b3)中至少有一个不等于0，或者具有独立变量的当前模型比仅截距模型更适合数据。</p><p id="dfd7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">实际上来说，让所有的自变量的系数都为0是不太可能的，我们最终会拒绝零假设。然而，有可能每个变量本身都没有足够的预测性，没有统计学意义。换句话说，你的样本提供了足够的证据来断定你的模型是显著的，但不足以断定任何单个变量是显著的。</p><p id="d00f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"><em class="ky">F-统计量=解释方差/未解释方差</em> </strong></p><p id="23b6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> OLS模型:</strong>F-stat概率为1.58e-96，远低于0.05即orα值。它仅仅意味着得到至少一个非零值系数的概率是1.58e-96。</p><p id="0101" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">对数似然值:</strong>对数似然值是对任何模型的拟合优度的度量，或用于导出最大似然估计量。值越高，模型越好。我们应该记住，对数似然可能介于-Inf到+Inf之间。因此，绝对看值不能给出任何指示。估计量是通过求解获得的，也就是说，通过找到使观察样本的对数似然最大化的参数。</p><p id="cd06" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> AIC和BIC: </strong>阿凯克信息准则(AIC)和贝叶斯信息准则(BIC)是两种评分和选择模型的方法。</p><p id="cefe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> AIC = -2/N * LL + 2 * k/N </strong></p><p id="dce3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> BIC = -2 * LL + log(N) * k </strong></p><p id="2ba0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中N是训练数据集中示例的数量，LL是训练数据集中模型的对数似然，k是模型中参数的数量。</p><p id="e1d1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如上所述，分数被最小化，例如选择具有最低AIC和BIC的模型。</p><p id="f883" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">计算出的数量与AIC不同，尽管可以证明与AIC成比例。与AIC不同，BIC因模型的复杂性而对其进行更多的惩罚，这意味着更复杂的模型将具有更差(更大)的分数，从而更不可能被选中。</p><p id="cc92" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">第三节:</strong></p><p id="8225" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">低吠声..这是很多信息。我们还有两个部分要讲，让我们直接进入中心部分，这是总结的主要部分:</p><figure class="ju jv jw jx fd is er es paragraph-image"><div class="er es la"><img src="../Images/f2129e976d5775fa033bf6b14e3fd64e.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*bcuDjkzXvoTPcEbqYi8eiw.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">中央部分</figcaption></figure><p id="59ec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们知道，列<strong class="ix hs"> <em class="ky"> coef </em> </strong>是b0、b1、b2和b3的值。所以这条线的方程式是:</p><p id="ffe1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">y = 2.94 + 0.046 *(电视)+ 0.188*(广播)+ (-0.001)*(报纸)</p><p id="6337" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> <em class="ky"> Std err </em> </strong>是每个变量的标准误差，是变量离回归线的距离。</p><p id="e71a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> <em class="ky"> t和</em> P &gt; |t|: t <em class="ky"> </em> </strong>简单来说就是每个变量的t-stat值，假设如下:</p><p id="a7d0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> H0: </strong>斜率/系数= 0</p><p id="cdab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> H1: </strong>斜率/系数不= 0</p><p id="19eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在此基础上，它给出了t统计值，而<strong class="ix hs"> P &gt; |t| </strong>给出了P值。当alpha为5%时，我们测量变量是否显著。</p><p id="0047" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">【0.025，0.975】</strong>—在默认的5% alpha或95%置信区间，如果coef值位于该区域，我们说coef值位于可接受区域内。</p><p id="b3d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">查看p值，我们知道我们必须从列表中删除“报纸”,它不是一个重要的变量。在此之前，让我们快速解释一下模型的最后一部分。</p><p id="7e9c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">第四节:</strong></p><figure class="ju jv jw jx fd is er es paragraph-image"><div class="er es lb"><img src="../Images/3f654970caae081582ccdf5b35f7c7d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*NylhNM5exKSKvlFxGx23TA.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">底部截面</figcaption></figure><p id="781e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">综合的:</strong>他们测试一组数据中已解释的方差是否明显大于未解释的方差。这是对残差的偏斜度和峰度的测试。我们希望综合得分接近0，其概率接近1，这意味着残差遵循正态分布。</p><p id="9a64" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们的例子中，综合得分非常高，远远超过60，其概率为0。这意味着我们的残差或误差率不遵循正态分布。</p><p id="7c6d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">偏斜— </strong>其<strong class="ix hs"> </strong>数据对称性的一种度量。我们希望看到接近零的东西，表明残差分布是正态的。注意，这个值也驱动综合。</p><p id="1263" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到，我们的残差是负偏的，为-1.37。</p><p id="842c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">峰度— </strong>它是数据曲率的度量。更高的峰值导致更大的峰度。峰度越大，可以解释为残差在零附近的聚类越紧密，意味着模型越好，离群值越少。</p><p id="9b65" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">查看结果，我们的峰度是6.33，这意味着我们的数据没有异常值。</p><p id="8f60" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">德宾-沃森— </strong>德宾-沃森(DW)统计是对统计回归分析中残差的自相关性进行的测试。德宾-沃森统计值将总是在0和4之间。值为2.0意味着在样本中没有检测到自相关。</p><p id="ae53" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ky">德宾-沃森</em>值为2.084，非常接近于2，我们认为数据不具有自相关性。</p><p id="31ec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注:自相关，也称为序列相关，它是观测值之间的相似性，是它们之间时滞的函数。</p><p id="0e61" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">Jarque-Bera(JB)/Prob(JB)—</strong>JB score简单地用以下假设检验残差的正态性:</p><p id="1ed8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">H0:残差服从正态分布</p><p id="5a95" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">H1:残差不服从正态分布</p><p id="e9c8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Prob(JB) 非常低，接近于0，因此我们拒绝零假设。</p><p id="48ae" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs"> Cond。编号:</strong>条件编号用于帮助诊断共线性。共线性是指一个独立变量接近于一组其他变量的线性组合。</p><p id="78e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们的例子中，条件数是454，当我们减少变量时，让我们看看分数是如何减少的。</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><p id="1913" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">好了，我们的文章快结束了，我们已经看到了这个OLS模型中每一个元素的解释。最后一节，我们更新了OLS模型并比较了结果:</p><p id="0bd5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们看看我们的模型，只有p值为0.86的报纸高于0.05。因此，我们将在移除报纸后重建一个模型:</p><figure class="ju jv jw jx fd is er es paragraph-image"><div class="er es lj"><img src="../Images/4097dff0b4e52960f8095ff826b7d3dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*HAijoxf7RYlhbCksd1_kYQ.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">更新的OLS模型(移除报纸)</figcaption></figure><p id="02d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hs">请注意</strong>如前所述，每个变量的系数值相互依赖。因此，我们应该总是一列一列地删除，这样我们就可以测量差异。</p><p id="d853" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们拿走报纸时，我们的精度水平不会改变，但是系数已经更新。然而，AIC，BIC分数和Cond。没有。减少了，这证明我们提高了模型的效率。</p><p id="548d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">哇！我们终于到了这篇文章的结尾。我们在上面解释的OLS模型中有许多要素。我试图简化并阐明OLS总结的每一部分。</p><p id="1ed5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你有任何疑问，请在评论中提问，如果你发现有什么不对劲，请让我知道。</p><p id="6559" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你从这篇文章中学到了新东西，请鼓掌。</p><p id="dca7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">参考资料:</p><p id="5366" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1.<a class="ae lk" href="https://realpython.com/linear-regression-in-python/" rel="noopener ugc nofollow" target="_blank">https://realpython.com/linear-regression-in-python/</a></p><p id="5ade" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.<a class="ae lk" href="https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html" rel="noopener ugc nofollow" target="_blank">https://www . stats models . org/dev/examples/notebooks/generated/ols . html</a></p><p id="24cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm<a class="ae lk" href="http://www.stat.yale.edu/Courses/1997-98/101/linreg.htm" rel="noopener ugc nofollow" target="_blank"/></p><p id="bf24" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4.<a class="ae lk" href="https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/" rel="noopener ugc nofollow" target="_blank">https://statistics byjim . com/regression/ols-linear-regression-assumptions/</a></p></div></div>    
</body>
</html>