<html>
<head>
<title>How to get started using transformers for NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何开始使用NLP的变压器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/using-transformers-426e6b04addc?source=collection_archive---------28-----------------------#2020-12-22">https://medium.com/analytics-vidhya/using-transformers-426e6b04addc?source=collection_archive---------28-----------------------#2020-12-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4034" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当你可以自己尝试不同的方法时，这篇文章效果最好——在<a class="ae jd" href="http://deepnote.com/" rel="noopener ugc nofollow" target="_blank">deepnote.com</a>上运行我的笔记本来试试吧！</p><p id="801e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我喜欢变形金刚图书馆。这是迄今为止最容易开始使用变压器的自然语言处理，这是目前的出血边缘。</p><p id="90f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一步是从transformers库中获取模型和标记器</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="9f0a" class="jn jo hi jj b fi jp jq l jr js">import tensorflow as tf <br/>import tensorflow_hub as hub<br/>from transformers.modeling_tf_openai import TFOpenAIGPTLMHeadModel </span><span id="a01b" class="jn jo hi jj b fi jt jq l jr js">#this is the GPT transformer with additional layers added for easy language modeling</span><span id="f224" class="jn jo hi jj b fi jt jq l jr js">from transformers.tokenization_openai import OpenAIGPTTokenizer</span><span id="89be" class="jn jo hi jj b fi jt jq l jr js">import simpletransformers</span></pre><p id="53b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果下面的单元格需要一些时间，也不用担心！模型下载只需要一分钟。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="6153" class="jn jo hi jj b fi jp jq l jr js">model = TFOpenAIGPTLMHeadModel.from_pretrained("openai-gpt")</span><span id="8908" class="jn jo hi jj b fi jt jq l jr js">tokenizer = OpenAIGPTTokenizer.from_pretrained("openai-gpt")</span></pre><p id="28d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:GPT2是用于文本生成的较新的transformer模型，但是当我编写这段代码时，只有GPT支持文本生成</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="02c4" class="jn jo hi jj b fi jp jq l jr js">prompt_text = "&lt;Think of a piece of text to supply as a prompt!&gt;"</span><span id="743b" class="jn jo hi jj b fi jt jq l jr js">encoded_prompt = tokenizer.encode(prompt_text,add_special_tokens=False,return_tensors='tf')</span></pre><p id="85f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于transformers为GPT模型提供了一个标记器，这是一个比使用通用句子编码器更容易的解决方案。tensors返回pytorch类型，而不是tensorflow，因为我发现在这种情况下遇到的bug更少(这可能是库最初是为pytorch编写的结果)。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="336d" class="jn jo hi jj b fi jp jq l jr js">num_sequences = 1 # this is the number of different sequences/ sentences the generator will create given the prompt</span><span id="9a58" class="jn jo hi jj b fi jt jq l jr js">length = 15 # if you want to strictly follow the prompt for this question set the length to one. If you want to see how creative GPT can be feel free to amend</span></pre><p id="a308" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来点序列吧！</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="9156" class="jn jo hi jj b fi jp jq l jr js">generated_sequences = model.generate(<br/>input_ids=encoded_prompt,<br/>do_sample=True,<br/>max_length=length + len(encoded_prompt[0]),<br/>temperature=1.0,<br/>top_k=5,<br/>top_p=0.9,<br/>repetition_penalty=1.0)</span></pre><p id="337d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">牢房里有很多东西要打开。该表扬的地方要表扬——图书馆有很棒的文档，所以让我们打开包装吧。</p><p id="3833" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入id只是简单地指定输入是什么。</p><p id="7d53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Do_sample —这可以防止模型在每一步都贪婪地挑选最可能的单词。</p><p id="9f04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Max_len —我们想要的序列长度是多少？</p><p id="b262" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">温度——我发现这个非常有趣。这是衡量我们的模型在选词时会有多大风险的一个尺度。请随意调整这个！</p><p id="9edb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Top_k和top_p是相似的，因为它们限制了模型在从单词概率中随机采样之前解码时考虑的单词数量。</p><p id="1d39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重复惩罚是为了避免句子重复，没有任何真正有趣的东西。</p><p id="5c6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢<a class="ae jd" href="https://huggingface.co/transformers/main_classes/model.html?highlight=tfpretrained#transformers.TFPreTrainedModel" rel="noopener ugc nofollow" target="_blank">医生</a>和<a class="ae jd" rel="noopener" href="/voice-tech-podcast/visualising-beam-search-and-other-decoding-algorithms-for-natural-language-generation-fbba7cba2c5b">这篇</a>优秀的媒体文章帮助我理解了这里的事实。</p><p id="61fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于generated_sequences中的序列:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="0166" class="jn jo hi jj b fi jp jq l jr js">for sequence in generated_sequences:</span><span id="7342" class="jn jo hi jj b fi jt jq l jr js">text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)</span><span id="30d4" class="jn jo hi jj b fi jt jq l jr js">print(text)</span></pre><p id="23da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还有tada！我们已经预测了文本。然而，这并不是生成文本的唯一方式。到目前为止，最简单的方法是使用管道。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="ed1d" class="jn jo hi jj b fi jp jq l jr js">from transformers import pipeline</span><span id="32cb" class="jn jo hi jj b fi jt jq l jr js">generator = pipeline("text-generation")</span><span id="e017" class="jn jo hi jj b fi jt jq l jr js">text_1 = generator("Text generation is cool because", max_length=50)</span><span id="dcf1" class="jn jo hi jj b fi jt jq l jr js">text_1[0]['generated_text']</span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="aef5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于本文的其余部分，请点击<a class="ae jd" href="https://deepnote.com/project/e34c35b3-eeb8-4706-962c-57341bd0dafe#%2Ftext_generation.ipynb" rel="noopener ugc nofollow" target="_blank">这个</a>链接查看deepnote的完整版本</p></div></div>    
</body>
</html>