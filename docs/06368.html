<html>
<head>
<title>Introduction to the Gradient Boosting Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度推进算法简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-the-gradient-boosting-algorithm-c25c653f826b?source=collection_archive---------0-----------------------#2020-05-20">https://medium.com/analytics-vidhya/introduction-to-the-gradient-boosting-algorithm-c25c653f826b?source=collection_archive---------0-----------------------#2020-05-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/b6c6f83d68cb0a6a48e43b6f759ab183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Aj1Slh1c3aDjSYa2Em7QQ.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图片:Edusky</figcaption></figure><div class=""/><p id="7ffa" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Boosting算法是过去二十年中引入的最强大的学习思想之一。梯度推进是一种用于分类和回归问题的监督机器学习算法。它是一种集成技术，使用多个弱学习器来产生用于回归和分类的强模型。</p><h1 id="d9b0" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">直觉</strong></h1><p id="da1a" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">梯度推进依赖于直觉，即最佳可能的下一个模型，当与先前的模型结合时，最小化整体预测误差。关键的想法是从先前的模型到下一个模型设定目标结果，以最小化误差。这是另一个boosting算法(其他几个是Adaboost，XGBoost等。).</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kv"><img src="../Images/fb88694d9a151a69a13487cced1bfc7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rn-u1k5_8O4Vk7HQrPiX6w.png"/></div></div></figure><p id="8d5c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">梯度推进的输入要求:</strong></p><ol class=""><li id="3921" class="la lb hx iw b ix iy jb jc jf lc jj ld jn le jr lf lg lh li bi translated">要优化的损失函数。</li><li id="7ca6" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">弱学习者做预测(一般是决策树)。</li><li id="63a1" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">添加弱学习者以最小化损失函数的加法模型。</li></ol><h2 id="ebc1" class="lo jt hx bd ju lp lq lr jy ls lt lu kc jf lv lw kg jj lx ly kk jn lz ma ko mb bi translated">1.损失函数</h2><p id="a78d" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">损失函数基本上说明了我的算法是如何模拟数据集的。简单来说，就是实际值和预测值之间的差异。</p><p id="669f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">回归损失函数:</strong></p><ol class=""><li id="5c68" class="la lb hx iw b ix iy jb jc jf lc jj ld jn le jr lf lg lh li bi translated">L1损失或平均绝对误差(MAE)</li><li id="2332" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">L2损耗或均方误差(MSE)</li><li id="0047" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">二次损失</li></ol><p id="5ba7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">二元分类损失函数:</strong></p><ol class=""><li id="9e70" class="la lb hx iw b ix iy jb jc jf lc jj ld jn le jr lf lg lh li bi translated">二元交叉熵损失</li><li id="67ea" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">铰链损耗</li></ol><p id="2476" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">添加树时，使用梯度下降过程来最小化损失。</p><h2 id="d13c" class="lo jt hx bd ju lp lq lr jy ls lt lu kc jf lv lw kg jj lx ly kk jn lz ma ko mb bi translated">2.弱学习者</h2><p id="d6df" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">弱学习器是顺序使用的模型，以减少从先前模型产生的误差，并最终返回强模型。</p><p id="05fe" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">决策树在梯度提升算法中被用作弱学习器。</p><h2 id="8ae4" class="lo jt hx bd ju lp lq lr jy ls lt lu kc jf lv lw kg jj lx ly kk jn lz ma ko mb bi translated">3.加性模型</h2><p id="bf96" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">在梯度推进中，一次添加一个决策树(按顺序)，模型中现有的树不变。</p><h1 id="b0f9" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">逐步了解梯度增强:</h1><p id="06a0" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">这是我们的数据集。这里年龄，Sft。，地点是自变量，价格是因变量或目标变量。</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mc"><img src="../Images/4ad6f01fa7b667efa3fd3b82693f3e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*ULI9uquBUWfbJ20SNrU3dA.png"/></div></figure><p id="4af9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">步骤1 </strong>:计算目标变量的平均值/均值。</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es md"><img src="../Images/421288186fed21a75e002f99349e66c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*hPJy1islh56nnR5BH_Qeug.png"/></div></figure><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es me"><img src="../Images/dd81e8f24d3b479bc445ecf1c6536d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*FVi3Dn4UVnrp5x0yV8pU5Q.png"/></div></figure><p id="4c40" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">步骤2 </strong>:计算每个样本的残差。</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/64e297c4215faea0fe7d9cdeeb972659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*zAPsBPDiS82nLBFOOsL9gQ.png"/></div></figure><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="ab fe cl mg"><img src="../Images/e74144a57c664895d1381543e9960ca1.png" data-original-src="https://miro.medium.com/v2/format:webp/1*84jLMj8XMwLThJo8vBvk_g.png"/></div></figure><p id="b35e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">第三步:</strong>构建决策树。我们构建一个树，目标是预测残差。</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/4f33d2524ae8292734555d8c8ea00939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rRh2UY-HoGJeFfQ5RNVz2Q.png"/></div></div></figure><p id="6b4e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果有比叶节点更多的残差(这里是6个残差)，一些残差将在同一个叶内结束。当这种情况发生时，我们计算它们的平均值，并将其放入叶子中。</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/bb1b2b1e3afe42224213dd999c71db49.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*NyegHMJNCUDn3VVTMdEDUg.png"/></div></figure><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/78946810d6f2765df6c00311d8faff38.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*ilWs6-y7wrj66Tr0v1RqMA.png"/></div></figure><p id="ae14" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这棵树变成这样之后。</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mk"><img src="../Images/4e8cd2671fc5f17ddf818241fb233ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*VsALULZw3gEnpp4VATfPbA.png"/></div></figure><p id="db63" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">步骤4 </strong>:使用集合中的所有树预测目标标签。</p><p id="f9cd" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">每个样本通过新形成的树的决策节点，直到它到达给定的线索。所述叶子中的残差用于预测房价。</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/610d589e17b08e78003e81a62e482a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jgFL4ajDbulZ3ON098boSg.png"/></div></div></figure><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/c403872cbd2f5d253f5b8df77e2e4265.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*Bp01OWZAkGr67bqGVNUgdg.png"/></div></figure><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mn"><img src="../Images/bf50a29e378b0f093d044c51862492eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*81Rcj45XL8xFUm_p0ym5nQ.png"/></div></figure><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/40d20921a872400c6c762e19180bfd8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*6iN4MRQb1YQItPF-jqFeWw.png"/></div></figure><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/77b6fc47040bbc9b9c1333ceeea764f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*I8NnPSldKzcy7H8OUZpQFQ.png"/></div></figure><p id="696d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">上面对第2步</strong>中的残值(-338)和(-208)的计算</p><p id="f9fe" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">同样，我们将计算其他值的<strong class="iw hy">预测价格</strong></p><p id="eb66" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">注意:</strong>我们最初将0.1作为学习率。</p><p id="0f37" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">第五步</strong>:计算新的残差</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/2d6c92013e44f45c197ea8baa98a73d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*HwKAAuhdbIR6ihQQ2C9vRw.png"/></div></figure><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/2f1f71e9e398891d46488f93c481f68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*1CCravSOh8PMdV3VIzrNBw.png"/></div></figure><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es ms"><img src="../Images/9bd37c4acad8ceab0d2b351b8e16afeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*RjYwwnFyFJY4AVN0vWCrKw.png"/></div></figure><p id="ec4c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">价格分别为350和480时。</strong></p><p id="26f2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于我们的单叶平均值<strong class="iw hy"> (688) </strong>，我们得到下面的残差列。</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="ab fe cl mg"><img src="../Images/e74144a57c664895d1381543e9960ca1.png" data-original-src="https://miro.medium.com/v2/format:webp/1*84jLMj8XMwLThJo8vBvk_g.png"/></div></figure><p id="7ef8" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">通过我们的决策树，我们得到了下面的新残差。</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mt"><img src="../Images/0c3739eb32c885d1aff574d0c4f63347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUxAQ0qWyQ7D4yu-wyhETA.png"/></div></div></figure><p id="599e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">第6步</strong>:重复第3步到第5步，直到迭代次数与超参数指定的次数(估计数)相匹配</p><p id="ccaa" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">步骤7 </strong>:一旦训练完毕，使用集合中的所有树对目标变量的值进行最终预测。最终预测将等于我们在步骤1中计算的平均值加上组成森林的树木预测的所有残差乘以学习率。</p><p id="5ee3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里，</p><p id="1e7b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> LR </strong>:学习率</p><p id="0bd3" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> DT </strong>:决策树</p><figure class="kw kx ky kz fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mu"><img src="../Images/2f26b7d38537fcf51c17bd7351c2ee9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hOOvQbw9Jd_4yTc44wuPoA.png"/></div></div></figure><figure class="kw kx ky kz fd hk er es paragraph-image"><div class="er es mv"><img src="../Images/097733ebf396945f6eb4163490f2f570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*aKXj5kI4Yy2i6pXKysDS5g.png"/></div></figure><h1 id="64de" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">Python中的梯度推进代码实现</strong></h1><figure class="kw kx ky kz fd hk"><div class="bz dy l di"><div class="mw mx l"/></div></figure><h1 id="09e3" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">梯度增强的优势</h1><ol class=""><li id="0eb3" class="la lb hx iw b ix kq jb kr jf my jj mz jn na jr lf lg lh li bi translated">梯度推进算法的大部分时间预测精度偏高。</li><li id="4574" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">它提供了很大的灵活性，可以优化不同的损失函数，并提供了几个超级参数调整选项，使函数拟合非常灵活。</li><li id="9c73" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">大多数时候不需要数据预处理。</li><li id="6fbd" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">梯度推进算法适用于分类数据和数值数据。</li><li id="c677" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">处理缺失数据-不需要缺失值插补。</li></ol><h1 id="2371" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">梯度增强的缺点</strong></h1><ol class=""><li id="c9c8" class="la lb hx iw b ix kq jb kr jf my jj mz jn na jr lf lg lh li bi translated">梯度推进模型将继续改进，以尽量减少所有误差。这可能会过分强调异常值并导致过度拟合。必须用交叉验证来中和。</li><li id="aa10" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">它在计算上非常昂贵——GBM通常需要许多树(&gt; 1000)，这可能会耗尽时间和内存。</li><li id="c0e8" class="la lb hx iw b ix lj jb lk jf ll jj lm jn ln jr lf lg lh li bi translated">高灵活性导致许多参数相互作用并严重影响该方法的行为(迭代次数、树深度、正则化参数等)。).这需要在调优期间进行大范围的网格搜索。</li></ol><h1 id="a8b9" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">结论</strong></h1><p id="e498" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">梯度推进算法是非常广泛使用的机器学习和预测建模技术(在Kaggle和其他代码竞赛中首选)。</p><p id="bfcc" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">希望你喜欢我的文章！</p><p id="d00e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">想要连接:</p><p id="3072" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">联系方式:<a class="ae nb" href="https://www.linkedin.com/in/anjani-kumar-9b969a39/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/anjani-kumar-9b969a39/</a></p><p id="d2d2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果你喜欢我在Medium上的帖子，并希望我继续做这项工作，请考虑在<a class="ae nb" href="https://www.patreon.com/anjanikumar" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hy"> patreon </strong> </a>上支持我</p></div></div>    
</body>
</html>