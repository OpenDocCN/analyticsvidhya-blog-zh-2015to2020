<html>
<head>
<title>End-to-End ML in Tensorflow and Tensorflow Extended — 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow和tensor flow Extended-1中的端到端ML</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/end-to-end-ml-in-tensorflow-and-tensorflow-extended-1-db32b32334b7?source=collection_archive---------10-----------------------#2019-10-27">https://medium.com/analytics-vidhya/end-to-end-ml-in-tensorflow-and-tensorflow-extended-1-db32b32334b7?source=collection_archive---------10-----------------------#2019-10-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b4e035be0cc6bc30a5316bea0cd5a993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dKOQUM3MP6NozQHH"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">萨法尔·萨法罗夫在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="fc55" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一段时间以来，仍然有很多人问如何从Tensorflow或Keras部署训练好的模型，如何将Keras模型(h5)或检查点(ckpt)转换为冻结模型/原始缓冲区模型(pb)，或者如何将预训练模型转换为冻结模型。在所有这些之后，另一个问题将是我如何服务于这些模型，以及我如何从客户端进行推理。</p><p id="197a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有几种方法可以完成这些任务。服务可以使用基于Python的简单web框架Flask来完成，但是Flask有多可靠，不确定，有没有更好的方法来服务模型？是的，tensor flow-发球是关键。所有这些问题的答案就在这里，只需要很少几行代码和一个非常基本的模型。</p><p id="6722" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，我将遵循这个管道，创建一个非常基本的网络，导出TensorFlow模型，使用TensorFlow-serving提供服务，并创建一个客户端来进行推理。</p><p id="3b10" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们创建一个只有一个输入节点和一个输出节点的非常基本的网络，我们将在本系列的下一部分讨论复杂架构。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="4c54" class="kc kd hi jy b fi ke kf l kg kh">import tensorflow as tf</span><span id="7e3e" class="kc kd hi jy b fi ki kf l kg kh">input_placeholder = tf.placeholder(tf.int32, shape=[None, 1], name=’input_placeholder’)<br/>output_placeholder = tf.identity(tf.add(input_placeholder, 2), name=’output_placeholder’)</span></pre><p id="2879" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，首先，我们导入张量流并创建一个名为input_placeholder的双节点shape [None，1]，数据类型为integer32，input_placeholder的标识为output_placeholder，并添加了标量。</p><p id="a90b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将模型作为saved_model导出到导出目录，如下所示。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="585f" class="kc kd hi jy b fi ke kf l kg kh">import os</span><span id="73a9" class="kc kd hi jy b fi ki kf l kg kh">export_dir = “/tmp/tfmodeldir”<br/>version = "1"<br/>export_path = os.path.join(export_dir, version)</span><span id="d118" class="kc kd hi jy b fi ki kf l kg kh">sess = tf.Session()<br/>tf.saved_model.simple_save(sess, export_dir, inputs={"key":input_placeholder}, outputs={"keys":output_placeholder})</span></pre><p id="34b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面几行将以pb格式将模型保存在特定的目录中。您可以在特定位置检查模型。您可以从终端使用Tensorflow的命令行saved_model_cli查看模型的签名和期望版本。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="5d92" class="kc kd hi jy b fi ke kf l kg kh">saved_model_cli show — dir /tmp/tfmodeldir/1 — all</span><span id="7e7e" class="kc kd hi jy b fi ki kf l kg kh">MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:<br/><br/>signature_def['serving_default']:<br/>  The given SavedModel SignatureDef contains the following input(s):<br/>    inputs['key'] tensor_info:<br/>        dtype: DT_INT32<br/>        shape: (-1, 1)<br/>        name: input_placeholder:0<br/>  The given SavedModel SignatureDef contains the following output(s):<br/>    outputs['keys'] tensor_info:<br/>        dtype: DT_INT32<br/>        shape: (-1, 1)<br/>        name: output_placeholder:0<br/>  Method name is: tensorflow/serving/predict</span></pre><p id="90d1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，我们都做得很好。因此，让我们从命令行使用TensorFlow服务于保存的模型。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="9240" class="kc kd hi jy b fi ke kf l kg kh">tensorflow_model_server --port=850 --rest_api_port=8501 --model_name=simple_add --model_base_path=/tmp/tfmodeldir/</span><span id="a228" class="kc kd hi jy b fi ki kf l kg kh">2019-10-23 01:32:25.761732: I tensorflow_serving/model_servers/server.cc:82] Building single TensorFlow model file config:  model_name: simple_add model_base_path: /tmp/tfmodeldir/<br/>2019-10-23 01:32:25.761842: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.<br/>2019-10-23 01:32:25.761853: I tensorflow_serving/model_servers/server_core.cc:561]  (Re-)adding model: simple_add<br/>2019-10-23 01:32:25.862420: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: simple_add version: 1}<br/>2019-10-23 01:32:25.862486: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: simple_add version: 1}<br/>2019-10-23 01:32:25.862515: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: simple_add version: 1}<br/>2019-10-23 01:32:25.862566: I external/org_tensorflow/tensorflow/contrib/session_bundle/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: /tmp/tfmodeldir/1<br/>2019-10-23 01:32:25.862592: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /tmp/tfmodeldir/1<br/>2019-10-23 01:32:25.862792: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }<br/>2019-10-23 01:32:25.863032: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA<br/>2019-10-23 01:32:25.892224: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.<br/>2019-10-23 01:32:25.892303: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:212] The specified SavedModel has no variables; no checkpoints were restored. File does not exist: /tmp/tfmodeldir/1/variables/variables.index<br/>2019-10-23 01:32:25.892354: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 29757 microseconds.<br/>2019-10-23 01:32:25.892390: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:103] No warmup data file found at /tmp/tfmodeldir/1/assets.extra/tf_serving_warmup_requests<br/>2019-10-23 01:32:25.892505: I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: simple_add version: 1}<br/>2019-10-23 01:32:25.898323: I tensorflow_serving/model_servers/server.cc:324] Running gRPC ModelServer at 0.0.0.0:8500 ...<br/>[evhttp_server.cc : 239] RAW: Entering the event loop ...<br/>2019-10-23 01:32:25.900349: I tensorflow_serving/model_servers/server.cc:344] Exporting HTTP/REST API at:localhost:8501 ...</span></pre><p id="ea0d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以使用— tensorflow_model_server —帮助来检查参数定义，帮助将列出tensorflow服务中的几个参数。</p><p id="43b9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们程序中唯一剩下的部分是客户端部分。因为模型是用rest API和grpc API公开的。我们可以从客户端调用REST方法(具体来说是POST)。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="1053" class="kc kd hi jy b fi ke kf l kg kh">import requests<br/>import json</span><span id="cf1a" class="kc kd hi jy b fi ki kf l kg kh">endpoint = “<a class="ae iu" href="http://localhost:8501/v1/models/simple_add:predict" rel="noopener ugc nofollow" target="_blank">http://localhost:8501/v1/models/simple_add:predict</a>"</span><span id="69b0" class="kc kd hi jy b fi ki kf l kg kh">instance = [[11,1],[22,1],[44,1],[55,1]]</span><span id="459a" class="kc kd hi jy b fi ki kf l kg kh">headers = {“content-type”:”application-json”}<br/>data = json.dumps({“signature-name”:”serving_default”, “instances”: instance})<br/>response = requests.post(endpoint, data=data, headers=headers)<br/>prediction = json.loads(response.text)[‘predictions’]<br/>prediction</span></pre><p id="9ba2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">服务完模型后，HTTP的端点会是这样的格式<a class="ae iu" href="http://hostname:port/v1/models/model_name:predict." rel="noopener ugc nofollow" target="_blank">HTTP://hostname:port/v1/models/model _ name:predict。</a></p></div></div>    
</body>
</html>