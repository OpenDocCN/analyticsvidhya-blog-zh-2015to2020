<html>
<head>
<title>Understanding the GPT-2 Source Code Part 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解新GPT协议源代码第4部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-gpt-2-source-code-part-4-a5fbb89e5038?source=collection_archive---------0-----------------------#2019-05-22">https://medium.com/analytics-vidhya/understanding-the-gpt-2-source-code-part-4-a5fbb89e5038?source=collection_archive---------0-----------------------#2019-05-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e65a421434f0bd70afc8c5a9e1754d1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ceUS920Pza9WHJaWd-iebA.jpeg"/></div></div></figure><p id="cdd9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嗨！你可以分别在这里阅读第一部、第二部、第三部<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-1-4481328ee10b">，在这里</a>阅读<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-2-4a980c36c68b">，在这里</a>阅读<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-3-9796a5a5cc7c">。在这里，我试着说完model.py！我是新来的，所以如果有什么不清楚的地方，请告诉我！我将感谢反馈</a></p><h1 id="e2ef" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">变形金刚(电影名)</h1><p id="07e1" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在再次深入代码之前，我们必须先了解什么是变压器。不幸的是，它们不是变成汽车的机器人，但我认为它们同样令人兴奋！从根本上说，它们是一种对输入进行编码，然后输出输出的方法！我觉得下图描述的很好！</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/1610400f47ceb3f76f7f0552e7cff189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C6Z6i1_2EJn13jVEsAOkRQ@2x.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">图片感谢<a class="ae jo" href="https://towardsdatascience.com/@ardendertat" rel="noopener" target="_blank"> Arden Dertat </a>在这里写了一篇关于自动编码器<a class="ae jo" href="https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798" rel="noopener" target="_blank">的精彩文章</a></figcaption></figure><p id="cd5c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是，有一点要注意，这是自动编码器，而不是转换器。</p><p id="35d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然自动编码器使用传统的卷积神经网络，但处理文本数据的Transformers倾向于使用LSTMs，因此略有不同！</p><h1 id="3ec2" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">什么是LSTMs？</h1><p id="0b65" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">LSTMs代表长期短期记忆网络。我不会深入研究它，因为我认为许多其他文章在解释LSTMs方面做得很好。不过，我还是要深入探讨一下，因为我觉得这有点意思。</p><p id="7788" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">LSTM的每个区块接收两个输入。序列中前一个块和下一个字的信息。给定这两条信息，LSTMs计算一个输出和要传递给下一个单元的信息。我们使用LSTMs的原因是它们非常擅长记忆之前的信息和文本。</p><p id="3177" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在变形金刚中，通常的做法是将整个文本传递到一个LSTM网络中，这个网络就是前面提到的模块的网络。</p><h1 id="45cc" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">回到变形金刚</h1><p id="ddf0" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">现在，我们进退两难！如果我们简单地采用lstm的输出，并说这将是GPT-2的输出，这将是非常令人难过的，因为lstm的输出，就其性质而言，必须具有与输入相同或相似的长度，这是一个相当受限制的条件。此外，第一LSTM块的输出将只有第一个字来判断输出。因此，总的来说，这是一个坏主意。</p><p id="be24" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">解决这个问题的方法是变压器。还记得沿着块传递的数据吗？为什么我们不把文本放入LSTM网络，得到从最后一个LSTM街区传来的最后一条信息呢？然后，我们可以有一定把握地说，整个文本被编码到最后一个块输出的单个信息中(让我们称之为最后一个状态)。这就是所谓的编码网络。</p><p id="b35d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，转换器获取该信息，并将其传递给解码器网络。解码器，因为它从最后一个状态知道输入文本是什么，将开始输出标记。如果我们将这些令牌反馈给网络，它将继续输出令牌。我们可以通过在输出结束令牌时简单地停止来结束这些输出。</p><h1 id="2899" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">一个小问题</h1><p id="521f" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">虽然这听起来很不错，但它有一个根本问题，那就是它不工作！原因是最后一条信息，最后一个状态，事实上没能记住大部分文本。LSTMs原来是相当健忘的。因此，谷歌在论文<a class="ae jo" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">“注意力是你所需要的一切”</a>中提出了一个解决方案。</p><p id="cc84" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">解决方案是简单地将所有这些状态、沿着块传递的信息加在一起，同时对每个状态应用一个权重，并将其保存为上下文向量。这样，整个文本的信息可以被总结成一个向量，但没有健忘的倾向。权重从最后一个状态开始计算。然后，它被传递下去。</p><p id="e0ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">事实上，在论文中，OpenAI实际上取消了LSTMs，只是直接使用编码的单词，并将它们用作状态！</p><p id="c1ce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是当前变压器的方法，我们将在实际代码中看到。</p><h1 id="eb1f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">回到代码</h1><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="b9b4" class="lg jq hi lc b fi lh li l lj lk">presents = []<br/>pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer</span></pre><p id="0a17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码中的下一行如下。tf.unstack意味着张量沿着那个维度被分成一个列表。举个例子，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="6ec8" class="lg jq hi lc b fi lh li l lj lk">a = tf.placeholder("float", [1,2,3])</span></pre><p id="7629" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后做什么</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="14ce" class="lg jq hi lc b fi lh li l lj lk">tf.unstack(a, axis=1)</span></pre><p id="7fbc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">将输出</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="3bca" class="lg jq hi lc b fi lh li l lj lk">[tf.placeholder("float",[1,3]), tf.placeholder("float",[1,3])]</span></pre><p id="fd52" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，我们看到如果过去不存在，过去就变成了</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="0031" class="lg jq hi lc b fi lh li l lj lk">[None] * hparams.n_layer</span></pre><p id="38f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们可以得出结论，过去的第二维度的大小为hparams.n_layers。</p><h1 id="7c56" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">LSTM网络中的层是什么？(不需要)</h1><p id="ca2e" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">还记得不久前我说过LSTMs可以向下一个单元发出输出和信息吗？虽然这是事实，但您可能会怀疑1层LSTM是否足以产生足够复杂的输出。比方说，当我们观察常规神经网络时，我们倾向于在复杂的过程中使用多达10到12层。LSTMs中的层也是这样工作的。LSTM的下一层输入不是令牌，而是到下一个块的LSTM块的输出，然后获得输出本身并将其发送到下一层，依此类推。</p><h1 id="e6c3" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">对于注意力网络？</h1><p id="9469" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">对于注意力网络来说，</p><p id="5975" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从model.py中的代码来看</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="63e4" class="lg jq hi lc b fi lh li l lj lk">assert len(pasts) == hparams.n_layer<br/>for layer, past in enumerate(pasts):<br/>            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)<br/>            presents.append(present)<br/>        results['present'] = tf.stack(presents, axis=1)</span></pre><p id="cfde" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它似乎显示了网络的并行处理！</p><p id="5157" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下一个代码是</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="e36e" class="lg jq hi lc b fi lh li l lj lk">for layer, past in enumerate(pasts):<br/>    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)<br/>    presents.append(present)<br/>results['present'] = tf.stack(presents, axis=1)<br/>h = norm(h, 'ln_f')</span></pre><p id="50af" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个街区似乎是大部分工作进行的地方。</p><p id="1f50" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，让我们检查维度。h有一个维度[批次，序列，嵌入大小]，pasts有维度[不确定，n层，过去长度]</p><h1 id="ffe0" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">街区</h1><p id="bd75" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">现在，让我们研究一下方块函数。</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="1e26" class="lg jq hi lc b fi lh li l lj lk">def block(x, scope, *, past, hparams):<br/>    with tf.variable_scope(scope):<br/>        nx = x.shape[-1].value<br/>        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)<br/>        x = x + a<br/>        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)<br/>        x = x + m<br/>        return x, present</span></pre><p id="7272" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，在第一行中，一个名为nx的变量获取嵌入长度，然后将x和其他参数传递给一个名为attn的函数，这个函数很可能代表我们前面讨论过的注意！</p><h1 id="27f6" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">正常化</h1><p id="7ea3" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">首先，让我们看看norm，它用于在传递数据之前对数据进行规范化。</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="e9a8" class="lg jq hi lc b fi lh li l lj lk">def norm(x, scope, *, axis=-1, epsilon=1e-5):<br/>    """Normalize to mean = 0, std = 1, then do a diagonal affine transform."""<br/>    with tf.variable_scope(scope):<br/>        n_state = x.shape[-1].value<br/>        g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))<br/>        b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))<br/>        u = tf.reduce_mean(x, axis=axis, keepdims=True)<br/>        s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)<br/>        x = (x - u) * tf.rsqrt(s + epsilon)<br/>        x = x*g + b<br/>        return x</span></pre><p id="a6dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们直接研究代码之前，我想我应该澄清一下规范化意味着什么。它是使数据的平均值为0，标准差为1的过程。通常的做法是用平均值扣除，然后除以标准差。</p><p id="f32b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">许多机器学习算法都是这样做的，因为这往往会导致性能的提高！我不完全确定为什么会这样，但事情就是这样！</p><p id="2fe4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，当我们看到它在这个源代码中被规范化的方式时，非常有趣地看到，这里发生的不是任何简单的规范化。最初的部分，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="02d3" class="lg jq hi lc b fi lh li l lj lk">u = tf.reduce_mean(x, axis=axis, keepdims=True)<br/>s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)<br/>x = (x - u) * tf.rsqrt(s + epsilon)</span></pre><p id="9b8b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">挺标准的。u是平均值，s是由tf.reduce_mean计算的方差，它取第一个参数中所有值的平均值。</p><p id="8947" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">keepdims仅仅表示等级被保留。所以，基本上，如果它是一个二维数组，不是一个数字给出所有的平均值，而是两个数字给出两个轴的平均值。至少，这是我从阅读<a class="ae jo" href="https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean" rel="noopener ugc nofollow" target="_blank">文档</a>中得到的理解！</p><p id="b25a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">x通过减去平均值并乘以1/sqrt(标准偏差)进行归一化，1/sqrt由tf.rsqrt给出。ε用于避免s为0且tf.rsqrt变为无穷大的情况。</p><p id="eff4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">到目前为止，这是非常标准的，但是，当我们看到</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="278b" class="lg jq hi lc b fi lh li l lj lk">g = tf.get_variable('g', [n_state], initializer=tf.constant_initializer(1))<br/>b = tf.get_variable('b', [n_state], initializer=tf.constant_initializer(0))</span></pre><p id="19ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它们用来稍微改变x oh的值</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="5e4a" class="lg jq hi lc b fi lh li l lj lk">x = x*g + b</span></pre><p id="1cad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些值g和b可以被训练，因为它们是变量。因此，OpenAI在这里有效地做的是在训练开始之前操纵x，即数据，以便它被缩放，并在训练时对算法产生影响，我认为这非常有趣。它还允许数据保持不变，因为它是这样初始化的。</p><p id="5022" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们来看看大规模attn功能！</p><h1 id="3523" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">注意力</h1><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="d9d4" class="lg jq hi lc b fi lh li l lj lk">def attn(x, scope, n_state, *, past, hparams):<br/>    assert x.shape.ndims == 3  # Should be [batch, sequence, features]<br/>    assert n_state % hparams.n_head == 0<br/>    if past is not None:<br/>        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]</span><span id="0ad7" class="lg jq hi lc b fi ll li l lj lk">    def split_heads(x):<br/>        # From [batch, sequence, features] to [batch, heads, sequence, features]<br/>        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])</span><span id="a89e" class="lg jq hi lc b fi ll li l lj lk">    def merge_heads(x):<br/>        # Reverse of split_heads<br/>        return merge_states(tf.transpose(x, [0, 2, 1, 3]))</span><span id="8bec" class="lg jq hi lc b fi ll li l lj lk">    def mask_attn_weights(w):<br/>        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.<br/>        _, _, nd, ns = shape_list(w)<br/>        b = attention_mask(nd, ns, dtype=w.dtype)<br/>        b = tf.reshape(b, [1, 1, nd, ns])<br/>        w = w*b - tf.cast(1e10, w.dtype)*(1-b)<br/>        return w</span><span id="fb69" class="lg jq hi lc b fi ll li l lj lk">    def multihead_attn(q, k, v):<br/>        # q, k, v have shape [batch, heads, sequence, features]<br/>        w = tf.matmul(q, k, transpose_b=True)<br/>        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))</span><span id="2aa4" class="lg jq hi lc b fi ll li l lj lk">        w = mask_attn_weights(w)<br/>        w = softmax(w)<br/>        a = tf.matmul(w, v)<br/>        return a</span><span id="30af" class="lg jq hi lc b fi ll li l lj lk">    with tf.variable_scope(scope):<br/>        c = conv1d(x, 'c_attn', n_state*3)<br/>        q, k, v = map(split_heads, tf.split(c, 3, axis=2))<br/>        present = tf.stack([k, v], axis=1)<br/>        if past is not None:<br/>            pk, pv = tf.unstack(past, axis=1)<br/>            k = tf.concat([pk, k], axis=-2)<br/>            v = tf.concat([pv, v], axis=-2)<br/>        a = multihead_attn(q, k, v)<br/>        a = merge_heads(a)<br/>        a = conv1d(a, 'c_proj', n_state)<br/>        return a, present</span></pre><p id="6ea2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我知道这看起来很吓人。事实上，我自己看着这一堆代码也挺害怕的。但是我会试着把它分成小块，这样我们两个都能明白！</p><p id="e553" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们必须在顶部突出一些我们可以从代码中学到的要点！我们首先可以注意到，我们对x形状的预测是正确的！如给定的，是【批次，顺序，特征】。然而，对于过去的变量，维度比预期的要复杂得多。其实最后是【批次，2，头，顺序，特征】！</p><p id="2040" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了理解这两个单词的来源以及它们的含义，让我们回到理论上来。</p><h1 id="d9e5" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">LSTMs的隐藏状态(不需要)</h1><p id="0006" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">正如我之前提到的，LSTMs输出两件事。一个是输出，另一个是传递给下一个LSTM块的信息。然而，有一件事我没有提到，那就是输入和输出的维度。</p><h2 id="3e35" class="lg jq hi bd jr lm ln lo jv lp lq lr jz jb ls lt kd jf lu lv kh jj lw lx kl ly bi translated">投入</h2><ul class=""><li id="d466" class="lz ma hi is b it kn ix ko jb mb jf mc jj md jn me mf mg mh bi translated">LSTM的输入:[批量大小，序列长度，嵌入大小]</li></ul><h2 id="2dbe" class="lg jq hi bd jr lm ln lo jv lp lq lr jz jb ls lt kd jf lu lv kh jj lw lx kl ly bi translated">输出</h2><ul class=""><li id="a1fa" class="lz ma hi is b it kn ix ko jb mb jf mc jj md jn me mf mg mh bi translated">LSTM的输出:[批量大小，序列长度，嵌入大小]</li><li id="437a" class="lz ma hi is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh bi translated">传递到下一个lstm块的隐藏状态/信息:</li></ul><p id="91c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[批量大小，2，序列长度，嵌入大小]</p><p id="031d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，你可能想知道隐藏状态中的2是从哪里来的。实际上，当我们只看它时，隐藏状态的维数与LSTM的输出[batch_size，sequence_length，embed_size]相同。然而，由于我们也希望传递输出，我们将输出添加到最终隐藏状态的底部，从而形成新的隐藏状态，其维度为[batch_size，2，sequence_length，embed_size]。</p><p id="62ec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是，这仍然与[批次，2，头，序列，特征]不一样。问题是这些“头”是从哪里来的。这就是我们需要检查注意力的地方！</p><h1 id="e37c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">什么是头？</h1><p id="53e3" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我可能是错的，但对于官方的解释，看看这篇<a class="ae jo" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><p id="763e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基本上，还记得我说过注意通过对隐藏状态施加权重并相加来提高表现吗？事实证明这可能还不够！网络仍然很难仅仅从加权求和来理解正在发生的事情。</p><p id="190a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">诀窍是引入头像。每个头部是隐藏状态的加权总和，但是具有不同的权重。所以，从本质上来说，每个头看不同种类的隐藏状态。然后，这些被加在一起成为一个单一的上下文向量发送到解码器！这叫多头注意力。</p><p id="f92c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们可以说，过去的变量持有这些头！</p><h1 id="120a" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">返回代码</h1><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="26cf" class="lg jq hi lc b fi lh li l lj lk">with tf.variable_scope(scope):<br/>        c = conv1d(x, 'c_attn', n_state*3)<br/>        q, k, v = map(split_heads, tf.split(c, 3, axis=2))<br/>        present = tf.stack([k, v], axis=1)<br/>        if past is not None:<br/>            pk, pv = tf.unstack(past, axis=1)<br/>            k = tf.concat([pk, k], axis=-2)<br/>            v = tf.concat([pv, v], axis=-2)<br/>        a = multihead_attn(q, k, v)<br/>        a = merge_heads(a)<br/>        a = conv1d(a, 'c_proj', n_state)<br/>        return a, present</span></pre><p id="811e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">跳过所有的函数后，我们得到实际执行的代码。如果你还记得的话，n_state就是嵌入大小。变量作用域中调用的第一个函数称为conv1d。</p><p id="1d33" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Conv1d很可能代表一维卷积。</p><h1 id="b830" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">什么是一维卷积？</h1><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/78a66c735b9c5749125d56085d87d59a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sAOuHkmpQOe5-5FV66eHKQ.jpeg"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">非常感谢<a class="ae jo" href="https://www.youtube.com/channel/UC_vCQ-aJ2yoyMPAc-JFpS1A" rel="noopener ugc nofollow" target="_blank"> <br/>阿莫·乌姆巴卡</a>的图片和他的youtube教程<a class="ae jo" href="https://www.youtube.com/watch?v=ulKbLD6BRJA" rel="noopener ugc nofollow" target="_blank">这里</a></figcaption></figure><p id="6601" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">卷积是指有一个分配了权重的窗口，将其下方的元素乘以上方的权重，并产生一个数字(添加偏差后)。这个数字然后被发送到输出矩阵。然后，窗口移动它的位置，做同样的事情，并把它的输出发送到输出数组中被移动的位置！</p><p id="997c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如上图所示，当窗口为一维时，可以进行一维卷积！</p><p id="ee65" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们看看代码。</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="e33a" class="lg jq hi lc b fi lh li l lj lk">def conv1d(x, scope, nf, *, w_init_stdev=0.02):<br/>    with tf.variable_scope(scope):<br/>        *start, nx = shape_list(x)<br/>        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))<br/>        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))<br/>        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])<br/>        return c</span></pre><p id="b40a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里的x有维度[batch_size，sequence_size，embed_size]。因此，当调用shape_list()时，</p><p id="470f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">start被分配给[batch_size，sequence _ size ], NX是embed_size。这是通过在初始化它的时候在start前面加上*这个聪明的技巧来实现的，但是除非你知道指针之类的东西，否则我不认为这有什么意思。如果你感兴趣，请查看指针，了解它为什么有趣！</p><p id="bbd5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">w代表权重，用形状[1，nx，nf]和b初始化，b是偏差，设置为大小nf。如果我们回忆一下，nf是3*embed_size。我不确定为什么是3*embed_size，但是我很确定我们以后会发现的！</p><p id="bc64" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，输出c为:</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="654f" class="lg jq hi lc b fi lh li l lj lk">c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])</span></pre><p id="4609" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这对我来说很有趣，因为我不知道卷积可以通过简单的矩阵乘法和增加一个偏差来完成！我想，正如我解释的，我需要一个窗口，让它在矩阵中滑动。事实上，现在我看这个卷积，我注意到我可能在不知道的情况下做了相当多的卷积！</p><p id="8795" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于卷积，我通常使用Tensorflow的API，如tf.nn.conv2d和tf.nn.conv1d，因此我不太了解它的内部工作原理。事实上，我很好奇为什么OpenAI避免了这些预定义的功能，就像瘟疫一样，但是因为它很有教育意义，所以我没意见！</p><p id="ff7a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">反正现在的情况是x被重塑为[batch_size*sequence_size，embed_size]，权重被重塑为[embed_size(nx)，nf]。我不太确定为什么它一开始没有那个形状，但是不管怎样，让我们继续！然后将它们相乘得到[batch_size*sequence_size，nf]并加上大小偏差[nf]。最后，它被整形回[batch_size，sequence_size，nf]。</p><p id="b08e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我在网上简单查看时，我找不到一个数学证明来解释为什么会这样，但是我想一旦我找到并编辑这篇文章来收录它，我就会证明这一点！然而，现在，我只是想知道。</p><h2 id="b5f6" class="lg jq hi bd jr lm ln lo jv lp lq lr jz jb ls lt kd jf lu lv kh jj lw lx kl ly bi translated">q，k和v</h2><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="9fb2" class="lg jq hi lc b fi lh li l lj lk">q, k, v = map(split_heads, tf.split(c, 3, axis=2))</span></pre><p id="e6cd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这之后，c被分成3部分，这解释了为什么nf沿着nf轴乘以3。这产生了3个大小为[batch_size，sequence_size，embed_size]的张量，然后它们被映射到split_heads函数。让我们看看分裂头做了什么。</p><h2 id="8571" class="lg jq hi bd jr lm ln lo jv lp lq lr jz jb ls lt kd jf lu lv kh jj lw lx kl ly bi translated">分头_头</h2><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="97ff" class="lg jq hi lc b fi lh li l lj lk">def split_heads(x):<br/>        # From [batch, sequence, features] to [batch, heads, sequence, features]<br/>        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])</span></pre><p id="5f6b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">什么是分裂状态？让我们来了解一下！</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="7050" class="lg jq hi lc b fi lh li l lj lk">def split_states(x, n):<br/>    """Reshape the last dimension of x into [n, x.shape[-1]/n]."""<br/>    *start, m = shape_list(x)<br/>    return tf.reshape(x, start + [n, m//n])</span></pre><p id="98a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，基本上发生的事情是将[batch_size，sequence_size，embed_size]的传入形状转换为[batch_size，sequence_size，heads，embed_size/heads]。</p><p id="8627" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">显然，正在发生的是张量x变得适合用于头部！然而，当我们看裂头时，不仅如此，它还被调换了位置！</p><h2 id="6594" class="lg jq hi bd jr lm ln lo jv lp lq lr jz jb ls lt kd jf lu lv kh jj lw lx kl ly bi translated">置换</h2><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/2bc5efbfc46c32462126b9c2d91532e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*B2A4JEffuBj6njs6CLPRmA.jpeg"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">图片感谢<a class="ae jo" href="http://dreamsnmotion.com/word-of-the-day/2015/9/13/transpose" rel="noopener ugc nofollow" target="_blank">梦幻国度</a></figcaption></figure><p id="2b98" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如你在图中看到的，转置基本上是一个过程，其中每个索引以这样的方式反转，即索引(I，j)处的元素转到索引(j，I)处！</p><p id="0ea0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，当我们从另一个角度看这个问题时，轴被翻转了。x轴和y轴是翻转的！这也是tf.transpose做的事情。</p><p id="a9c2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从评论中我们可以看到，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="3f20" class="lg jq hi lc b fi lh li l lj lk">def split_heads(x):<br/>        # From [batch, sequence, features] to [batch, heads, sequence, features]<br/>        return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])</span></pre><p id="4974" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[batch_size，sequence_size，heads，embed_size]是由于tf.transpose的第二个参数，转换为[batch_size，heads，sequence_size，embed_size]。</p><p id="8b97" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在下一行中，虽然我不确定它为什么被称为present，但3个输出q、k和v中的两个被堆叠为</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="8768" class="lg jq hi lc b fi lh li l lj lk">present = tf.stack([k, v], axis=1)</span></pre><p id="ae92" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里的堆叠意味着引入了一个新的维度。因此，呈现具有维度[批量大小，2，头，序列大小，嵌入大小]</p><p id="716a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="88b3" class="lg jq hi lc b fi lh li l lj lk">if past is not None:<br/>            pk, pv = tf.unstack(past, axis=1)<br/>            k = tf.concat([pk, k], axis=-2)<br/>            v = tf.concat([pv, v], axis=-2)</span></pre><p id="55f8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为past也有维度[batch_size，2，heads，sequence_size，embed_size],所以当它被拆分时，它将具有与k、v和q相似的维度，即[batch_size，heads，past_sequence_size，embed_size]</p><p id="f6c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当沿着axis -2使用tf.concat将它与k和v连接起来时，本质上发生的是sequence_size变得更长！因此，正如在第3部分中提到的，由于过去的序列长度是过去的长度(到目前为止输出令牌的数量)，当连接时，k和v将具有维度[batch_size，heads，past _ sequence _ size+sequence _ size，embed_size]。</p><p id="f68d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这让我有点困惑，因为我不太确定给出sequence_size的X是从哪里产生的，但坦白地说，因为我们还没有涉及它，所以我还不知道是有道理的！所以，如果任何人和我一样有点困惑，请继续读下去，因为我很肯定我们以后可以解决这个困惑！</p><p id="66cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="b622" class="lg jq hi lc b fi lh li l lj lk">a = multihead_attn(q, k, v)</span></pre><p id="d515" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，一些注意力进来了。</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="f3f5" class="lg jq hi lc b fi lh li l lj lk">def multihead_attn(q, k, v):<br/>        # q, k, v have shape [batch, heads, sequence, features]<br/>        w = tf.matmul(q, k, transpose_b=True)<br/>        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))</span><span id="bef8" class="lg jq hi lc b fi ll li l lj lk">        w = mask_attn_weights(w)<br/>        w = softmax(w)<br/>        a = tf.matmul(w, v)<br/>        return a</span></pre><p id="dfa9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我想到的第一个令人困惑的行是tf.matmul，它带有参数transpose_b= True。我发现这很令人困惑，因为对于一个四维矩阵，当你转置它时，我认为维度的改变是转置所必需的。于是，我上网查了一下，到了<a class="ae jo" href="https://stackoverflow.com/questions/48100954/why-does-tf-matmula-b-transpose-b-true-work-but-not-tf-matmula-tf-transpos" rel="noopener ugc nofollow" target="_blank">这个栈溢出页面</a>。如果我理解正确的话，基本上正在发生的是，前两个维度被成批处理。</p><p id="9349" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以，它们保持不变。但是对于q和k的最后两个维度，虽然q对于每个批次都具有维度[序列，特征]，但是k对其进行了转置，因此它变成了[特征，序列]，并且由于q没有得到添加的过去长度，而k得到了，所以w的维度应该是[batch_size，heads，sequence_length，past _ sequence _ length+sequence _ length]。</p><p id="ac8e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在下一行，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="2a1e" class="lg jq hi lc b fi lh li l lj lk">w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))</span></pre><p id="b9c2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，基本上，w除以embed_size的平方根，当我查看<a class="ae jo" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>时，它被解释为“比例因子”，我认为这是有意义的。</p><p id="3314" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我知道上面很多关于q，k和v的内容没有太多的直观意义，至少对我来说没有，但很明显我们正在接近论文中的等式，也就是</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/28dcdc507702e727a096a56697fa87a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*pvZ-lkC9084jKQhhy_jK8A.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">来自《注意力是你所需要的全部》一文的等式</figcaption></figure><h1 id="12bf" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">试图理解这个等式</h1><p id="711d" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我知道这个等式不是最终结果，但因为我认为它足够接近，所以我会在继续之前尝试理解它。</p><p id="d0df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">目前我们从代码中知道的是</p><ul class=""><li id="6d81" class="lz ma hi is b it iu ix iy jb mq jf mr jj ms jn me mf mg mh bi translated">q、k和v将一维卷积应用于输入，h的维数为[batch_size，sequence_length，embed_size]，它是应用了一些噪声的令牌，并以维数[batch_size，heads，sequence_length，embed_size]结束</li><li id="490b" class="lz ma hi is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh bi translated">k和v很可能将过去的ks和vs连接在一起，从而产生一个更长的序列，即[batch_size，heads，sequence _ length+past _ sequence _ length，embed_size]</li><li id="c4c8" class="lz ma hi is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh bi translated">w，代表softmax函数中的表达式，维度为[batch_size，heads，sequence_length，sequence _ length+past _ sequence _ length]</li></ul><p id="a78d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">而且现在我觉得我从概念上理解了！</p><p id="d6c0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们先来了解一下softmax是什么！然后，我来解释一下上面的公式是干什么的！</p><h1 id="6c4c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">Softmax</h1><p id="bce9" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">Softmax是当人们想要进行概率分布时使用的函数。什么是概率密度函数，矩阵中所有元素的和是1，在0和1之间。例如，当我们将softmax函数应用于[1，1，1，1]时，我们得到[0.25，0.25，0.25，0.25]。</p><p id="a0c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这很简单，用e的每个元素的幂除以e的每个元素的幂之和。如下图所示</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/adad79c273ae765942027d45e477fdc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*L-5355UzqMquS_7B3l7sYQ.jpeg"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">感谢詹姆斯·d·麦卡弗里从<a class="ae jo" href="https://jamesmccaffrey.wordpress.com/2016/03/04/the-max-trick-when-computing-softmax/" rel="noopener ugc nofollow" target="_blank">这里</a>拍摄的图片</figcaption></figure><p id="fdd9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，没有维度或类似的变化。它只是放大了数字。虽然用这些值的和来除可能很诱人，但是使用softmax函数的一些优点如下！</p><ul class=""><li id="7256" class="lz ma hi is b it iu ix iy jb mq jf mr jj ms jn me mf mg mh bi translated">它也可以处理负元素，所有的元素都在0和1之间。</li><li id="f442" class="lz ma hi is b it mi ix mj jb mk jf ml jj mm jn me mf mg mh bi translated">像<a class="ae jo" href="https://stackoverflow.com/questions/17187507/why-use-softmax-as-opposed-to-standard-normalization" rel="noopener ugc nofollow" target="_blank">这个</a>栈溢出答案里提到的那样更容易训练！</li></ul><p id="1a16" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们回到解释上来。</p><h1 id="5c1e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">这个等式是做什么的？</h1><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/28dcdc507702e727a096a56697fa87a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*pvZ-lkC9084jKQhhy_jK8A.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">纸上的方程式</figcaption></figure><p id="146d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦我们知道softmax部分输出概率分布，我们现在可以从概念上看到Q、K和softmax都用于将权重应用于所有隐藏层的稍微变换版本:v。因此，这给出了我们想要的注意！现在，让我们回到代码上来！</p><h1 id="1547" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">面具</h1><p id="055d" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">应用比例因子(嵌入大小的平方根)后，代码中的下一行是</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="1c2c" class="lg jq hi lc b fi lh li l lj lk">w = mask_attn_weights(w)</span></pre><p id="f6d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我不记得在报纸上见过这个，但是让我们看看它是什么！</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="f815" class="lg jq hi lc b fi lh li l lj lk">def mask_attn_weights(w):<br/>        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.<br/>        _, _, nd, ns = shape_list(w)<br/>        b = attention_mask(nd, ns, dtype=w.dtype)<br/>        b = tf.reshape(b, [1, 1, nd, ns])<br/>        w = w*b - tf.cast(1e10, w.dtype)*(1-b)<br/>        return w</span></pre><p id="cd62" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们看到，sequence_length在这里称为dst_sequence，sequence _ length+past _ sequence _ length称为src_sequence。虽然我不是特别确定这些是什么意思，但是因为它们比较短，我想我会采用这个惯例。</p><p id="121b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">nd被分配给dst_sequence，ns被分配给src_sequence。现在让我们看看接下来要调用的attention_mask函数！</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="69ee" class="lg jq hi lc b fi lh li l lj lk">def attention_mask(nd, ns, *, dtype):<br/>    """1's in the lower triangle, counting from the lower right corner.</span><span id="eb60" class="lg jq hi lc b fi ll li l lj lk">Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.<br/>    """<br/>    i = tf.range(nd)[:,None]<br/>    j = tf.range(ns)<br/>    m = i &gt;= j - ns + nd<br/>    return tf.cast(m, dtype)</span></pre><p id="0c07" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，由于我从评论中不明白这个函数是做什么的，所以让我们来看看！</p><p id="93a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">I被分配给tf.range(nd)，它是从0到nd-1的值，带有另一个轴。[:，None]给出了另一个轴，所以I的维数是[nd，1]</p><p id="d743" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">j被分配给tf.range(ns)。nd是目的序列，而ns是src序列长度。</p><p id="8b1c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，一条有趣的线出现了。</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="2b1f" class="lg jq hi lc b fi lh li l lj lk">m = i &gt;= j - ns + nd</span></pre><p id="da62" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为我不明白这里发生了什么，所以我决定用numpy来测试一下！</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="25ba" class="lg jq hi lc b fi lh li l lj lk">a = np.arange(5)<br/>b = np.arange(7)</span></pre><p id="c7fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么，如果我做了</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="d1db" class="lg jq hi lc b fi lh li l lj lk">a &gt; b</span></pre><p id="5153" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为a和b的形状不同，所以出现了一个错误。然而，当我们这样做的时候</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="1bdc" class="lg jq hi lc b fi lh li l lj lk">a = np.arange(5)[:,None]<br/>b = np.arange(7)</span></pre><p id="e58a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="8b0a" class="lg jq hi lc b fi lh li l lj lk">array([[False, False, False, False, False, False, False],<br/>       [ True, False, False, False, False, False, False],<br/>       [ True,  True, False, False, False, False, False],<br/>       [ True,  True,  True, False, False, False, False],<br/>       [ True,  True,  True,  True, False, False, False]])</span></pre><p id="ad44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">被输出。当你把它看成一个表格时，这就变得很容易理解了！如果我们想象b的数字从0到6从左到右穿过各列，如果我们想象a的数字从0到4，看看比较结果如何，它给出相同的结果！下面的图片我认为将有助于说明这一点！</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/7f68efac35bd63da907b350068433f52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*zYyKK3qJABos4-I82bmBgA.png"/></div></figure><p id="6ed8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，如果我们看一看</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="ee9c" class="lg jq hi lc b fi lh li l lj lk">m = i &gt;= j - ns + nd</span></pre><p id="abd9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于j -ns+nd首先发生，j将一直保持nd-ns到nd的值。m将返回一个维数为[nd，ns]的掩码，其布尔值如上。</p><p id="64f2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，当它被返回时，它被转换为</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="a508" class="lg jq hi lc b fi lh li l lj lk">return tf.cast(m, dtype)</span></pre><p id="10ab" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，类型必须已经更改，并且它应该保存值0或1。现在这些评论有些道理了。基本上，这个函数返回了一个矩阵，其中左下角的三角形部分都是1！OpenAI解释了他们这样做的原因，简单地说，就是为了提高性能。</p><p id="c42d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，一旦归还，接下来会发生什么？</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="68db" class="lg jq hi lc b fi lh li l lj lk">b = tf.reshape(b, [1, 1, nd, ns])<br/>w = w*b - tf.cast(1e10, w.dtype)*(1-b)</span></pre><p id="2a34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">w目前的维度为[batch_size，heads，dst_length，src_length]。然后，通过在开始处增加维度来重塑b。然后，它们被繁殖，我发现这很有趣。*给出的这个乘法不是tf.Matmul，实际上是tf.multiply执行元素级乘法。</p><p id="5dc1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是什么意思？</p><p id="3bb3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">实质上，w的上半部分，由于w*b变成了0。之后，所有的信息都会消失！— tf.cast(1e10，w.dtype)*(1-b)允许它不为0(可能是由于某些错误)但是看到这样的决定很有趣！</p><p id="64e6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本博客精彩解释的基本论点是，如果我们看一下模型架构(这是你所需要的全部注意力的架构，不是GPT-2)，</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/42f094c7a26352d13d0d8c61ac357e41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*OKyOfBS4fXLZQZkaZ4okKw.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">感谢这个<a class="ae jo" href="http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/" rel="noopener ugc nofollow" target="_blank">神奇的博客</a></figcaption></figure><p id="7808" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">解码器的输出，如果没有屏蔽，可以访问输出中的未来文本。</p><p id="fed0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，Isamu，你可能会说，我们不再训练了，我们无法访问未来的文本，所以这个面具只会导致我们丢失数据！但是，幸运的是，情况并非如此。我回过头来看看step函数到底把什么作为标记，导致模型函数被调用。那就是下面的</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="47de" class="lg jq hi lc b fi lh li l lj lk">context_output = step(hparams, context[:, :-1])</span></pre><p id="54c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上下文[:，:-1]表示令牌。现在，如果我们回到面具的状态，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="5b0c" class="lg jq hi lc b fi lh li l lj lk">m = i &gt;= j - ns + nd</span></pre><p id="b352" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(j的大小为ns，I的大小为nd)并回想ns和nd是什么(ns =令牌的过去序列长度+序列长度，nd只是令牌的序列长度)</p><p id="fdbc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以重新审视面具会是什么！让我们首先将sequence_length限制为0，这是generate _ unconditional _ samples . py的情况。</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="3106" class="lg jq hi lc b fi lh li l lj lk">m = i ≥ j-past_sequence_length</span></pre><p id="b541" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这将是一个一维数组，大小为past_sequence_length，所有元素都为真！没有信息丢失！太棒了。</p><p id="0152" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">至于为什么面膜会起作用，我认为这个<a class="ae jo" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">博客帖子</a>解释得很好(图片也来自那里)</p><blockquote class="mw mx my"><p id="3467" class="iq ir mz is b it iu iv iw ix iy iz ja na jc jd je nb jg jh ji nc jk jl jm jn hb bi translated"><em class="hi">注意屏蔽下方显示了每个tgt单词(行)允许查看的位置(列)。单词被屏蔽是为了在训练中注意将来的单词。</em></p></blockquote><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/b9ee0ff900fde85fbd1bcdc06c7ef310.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*YVYPQ17laCdEAx4xx7IjFQ.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">图片感谢<a class="ae jo" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="7789" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我想我可以直观地理解，我不会声称我可以从数学上证明没有信息泄漏，但一旦我做到了，我一定会返回并编辑这篇文章并更新它！</p><p id="df5c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于interactive _ conditional _ samples . py，坦率地说，我还不太确定，但我想我现在直观地理解了这个掩码！</p><h1 id="cee1" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">返回代码</h1><p id="f0bc" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">接下来，像这样应用softmax函数，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="8bc7" class="lg jq hi lc b fi lh li l lj lk">w = softmax(w)</span></pre><p id="1907" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们研究softmax代码时，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="6554" class="lg jq hi lc b fi lh li l lj lk">def softmax(x, axis=-1):<br/>    x = x - tf.reduce_max(x, axis=axis, keepdims=True)<br/>    ex = tf.exp(x)<br/>    return ex / tf.reduce_sum(ex, axis=axis, keepdims=True)</span></pre><p id="843d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对传统softmax的一个有趣的改变是减去了最大值。我不能解释为什么，但我认为这很有趣，我也会尝试一下。我认为情况可能是这样的，softmax首先要求值很小/为负，并且很接近才能给出好的结果。这可以解释为什么w除以一个比例因子，为什么最大值被扣除，每个数字都是负数。可能目的是将分子缩放到1和0之间，但不管怎样，让我们继续！</p><p id="7103" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，w乘以v，给出注意</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="cc1f" class="lg jq hi lc b fi lh li l lj lk">a = tf.matmul(w, v)<br/>return a</span></pre><p id="6117" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">w具有维度[批次大小、头数、dst序列、src序列]</p><p id="15f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">并且v具有维度[批处理大小，头，src序列，嵌入大小]</p><p id="caad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，当成倍增加时，它们给予了具有维度的关注</p><p id="a55d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[批量大小，头数，dst序列，嵌入大小]</p><p id="717f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，在注意时，发生了以下操作！</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="506a" class="lg jq hi lc b fi lh li l lj lk">a = merge_heads(a)</span></pre><p id="aaa0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">merge_heads由下式给出</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="c6dc" class="lg jq hi lc b fi lh li l lj lk">def merge_heads(x):<br/>        # Reverse of split_heads<br/>        return merge_states(tf.transpose(x, [0, 2, 1, 3]))</span></pre><p id="a579" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，它将a的维度转换为[batch_size，dst_sequence，heads，embed_size]</p><p id="c114" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">merge_states由下式给出</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="acf4" class="lg jq hi lc b fi lh li l lj lk">def merge_states(x):<br/>    """Smash the last two dimensions of x into a single dimension."""<br/>    *start, a, b = shape_list(x)<br/>    return tf.reshape(x, start + [a*b])</span></pre><p id="b8e5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，a被整形为[batch_size，dst_sequence，heads*embed_size]</p><p id="c811" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，最后，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="e715" class="lg jq hi lc b fi lh li l lj lk">a = conv1d(a, 'c_proj', n_state)</span></pre><p id="a466" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">发生在。由于n_state是embed_size，注意力很好地转换为</p><p id="79d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[批量大小，dst序列，嵌入大小]</p><p id="a6b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">头部和重量很好地结合在一起！</p><p id="b0c4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们终于结束了关注。我们现在可以回到块函数了！</p><p id="ddf9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下一行是，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="f4c7" class="lg jq hi lc b fi lh li l lj lk">x = x + a</span></pre><p id="a493" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">简单回顾一下，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="6eee" class="lg jq hi lc b fi lh li l lj lk">a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)</span></pre><p id="9be4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">是我们得到a的代码。本质上，输入被加回输出。</p><p id="4d1b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这被称为剩余网络。虽然我个人对它们没有太多的经验，但它们所做的是将输入添加回输出，以便不丢失信息，并且它们非常令人惊讶地非常有效！</p><p id="28ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="76ac" class="lg jq hi lc b fi lh li l lj lk">m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)<br/>x = x + m</span></pre><p id="f61d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">赋范的x沿着mlp模型/函数传递，然后输出和x像以前一样加在一起！</p><p id="c810" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们来看看mlp函数！</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="6478" class="lg jq hi lc b fi lh li l lj lk">def mlp(x, scope, n_state, *, hparams):<br/>    with tf.variable_scope(scope):<br/>        nx = x.shape[-1].value<br/>        h = gelu(conv1d(x, 'c_fc', n_state))<br/>        h2 = conv1d(h, 'c_proj', nx)<br/>        return h2</span></pre><p id="33f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">mlp函数通过n_state应用一维卷积后，将输入x传递给名为gelu的函数。n_state的参数是4*nx，其中nx是嵌入大小。因此，函数gelu的输入维数为[batch_size，dst_length，embed_size*4]</p><p id="c78c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么，gelu函数内部是什么呢？</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="84e6" class="lg jq hi lc b fi lh li l lj lk">def gelu(x):<br/>    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))</span></pre><p id="54df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">GELU函数属于激活函数家族。激活函数所做的是将每个输入映射到-1和1或者0和1之间的某个位置。这导致神经网络更清楚地理解输出值。举个例子，当神经网络遇到100或-100这样的值时，它们的性能并不高。如果你对为什么专门选择GELU感兴趣，我推荐阅读这篇<a class="ae jo" href="https://arxiv.org/pdf/1606.08415.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。因为这是一个相当数学化的问题，我认为这个帖子现在不应该写那么久！如果你感兴趣，请告诉我，我会尽力解释。</p><p id="eda6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我不确定为什么大小被改为4*encode_size，但是在mlp函数中，gelu完成后，</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="7b2e" class="lg jq hi lc b fi lh li l lj lk">h2 = conv1d(h, 'c_proj', nx)<br/>return h2</span></pre><p id="48ce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">于是，大小最终又变成了[batch_size，dst_size，encode_size]了！</p><p id="9979" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，在添加到输入之后，函数输出</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="148f" class="lg jq hi lc b fi lh li l lj lk">x = x + m<br/>return x, present</span></pre><p id="1775" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后。现在的部分来自下面的attn函数！</p><pre class="kt ku kv kw fd lb lc ld le aw lf bi"><span id="e5cb" class="lg jq hi lc b fi lh li l lj lk">c = conv1d(x, 'c_attn', n_state*3)<br/>q, k, v = map(split_heads, tf.split(c, 3, axis=2))<br/>present = tf.stack([k, v], axis=1)</span></pre><p id="5d5e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">既然我已经讨论了model.py中的大部分理论，我想我现在就停下来。在下一篇文章中，我将尝试完成这个系列</p><h1 id="790c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">然后</h1><p id="9f49" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">如果你对最终到达这个传奇的结尾感兴趣，看看<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-5-87bbe21dd749">这个</a>！</p></div></div>    
</body>
</html>