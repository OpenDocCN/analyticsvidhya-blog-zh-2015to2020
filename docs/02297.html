<html>
<head>
<title>Building K-Nearest Neighbours(KNN) model without Scikit Learn: Easy Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无需Scikit Learn构建K近邻(KNN)模型:易于实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-k-nearest-neighbours-knn-without-using-scikit-learn-3905b4decc3c?source=collection_archive---------3-----------------------#2019-12-10">https://medium.com/analytics-vidhya/implementing-k-nearest-neighbours-knn-without-using-scikit-learn-3905b4decc3c?source=collection_archive---------3-----------------------#2019-12-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/ed656d961f94a6d1d1e24441e285bb9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*N0U82A1bvuvsp1u-.jpg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">为新来的穿红衣服的家伙找到K个最近的邻居并不难</figcaption></figure><blockquote class="iq ir is"><p id="cb50" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">k近邻(KNN)无疑是机器学习中我最喜欢的算法之一，因为它非常直观、简单易懂，同时也是一种需要学习的基本算法。</p></blockquote><p id="11a2" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">让我们看看定义是怎么说的</strong> : KNN是一种监督学习算法，它根据一个数据点(实例)的“k”个最近的实例属于哪个类，来估计该数据点(实例)属于一个类或另一个类的可能性。</p><blockquote class="iq ir is"><p id="a5b2" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这就像，告诉我你的密友，我就能知道你是谁！K '亲密朋友的数量，其中K是我们需要定义的整数。</p></blockquote><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es jv"><img src="../Images/226dc338a1efb610af1f1fe1409572bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QI4j3xa0ljVzEfpF.jpg"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">你有一群很酷的朋友吗？你也很酷！</figcaption></figure><p id="387e" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">看，KNN是一个通用的算法，可以用于分类和回归。它是一个<em class="iv"> </em> <strong class="iw hj"> <em class="iv">非参数</em> </strong>模型，这意味着它不像线性回归那样预先对数据做出假设，即数据必须是线性的。</p><p id="7c2a" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">此外，KNN被称为<strong class="iw hj"><em class="iv"/></strong>懒惰学习者，因为它在训练阶段不做任何事情，实际上，KNN没有这样的训练阶段，这意味着它使用训练数据点来做任何类型的<em class="iv">概括。</em>它只是记住所有的数据，或者只是保存所有的训练数据，然后在对新实例进行预测时使用这些数据。这意味着我们在训练过程中很快，但在测试阶段很慢。</p><p id="8822" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">那么让我们从KNN的实现开始吧。它实际上只涉及3个简单的步骤:</strong></p><ol class=""><li id="0de6" class="ke kf hi iw b ix iy jb jc js kg jt kh ju ki jr kj kk kl km bi translated">计算测试数据点和每个训练数据点之间的距离(欧几里德距离、曼哈顿距离等)。这是看谁近，谁远多少。</li><li id="cc65" class="ke kf hi iw b ix kn jb ko js kp jt kq ju kr jr kj kk kl km bi translated">对距离进行排序，并从中挑选K个最近的距离(前K个条目)。这将是K个与给定测试数据点最近的邻居。</li><li id="df3a" class="ke kf hi iw b ix kn jb ko js kp jt kq ju kr jr kj kk kl km bi translated">获取所选K个邻居的标签。最常见的标签(多数投票的标签)将是我们的测试数据点的预测标签。</li></ol><p id="4fe1" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">对测试集中的所有测试数据点重复上面的一切。</p></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><h1 id="c53b" class="kz la hi bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">现在让我们在不使用Scikit learn的情况下编写实现KNN的代码。</h1><p id="edc6" class="pw-post-body-paragraph it iu hi iw b ix lx iz ja jb ly jd je js lz jh ji jt ma jl jm ju mb jp jq jr hb bi translated">我们将使用iris数据集进行实施和预测。我假设你知道虹膜数据集。你可以在这里阅读这个非常受欢迎的数据集。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es md"><img src="../Images/b4c189bb512db6a61668d4ec142ee4ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NHgarBuqh9-a-3LV.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">Iris数据集中的三个类</figcaption></figure><p id="47ab" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">导入基本库</strong></p><pre class="jw jx jy jz fd me mf mg mh aw mi bi"><span id="686e" class="mj la hi mf b fi mk ml l mm mn">import numpy as np<br/>import scipy.spatial<br/>from collections import Counter</span></pre><p id="cb7d" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">从Sklearn </strong>加载鸢尾花数据集</p><pre class="jw jx jy jz fd me mf mg mh aw mi bi"><span id="e1c7" class="mj la hi mf b fi mk ml l mm mn">from sklearn import datasets<br/>from sklearn.model_selection import train_test_split</span><span id="d07c" class="mj la hi mf b fi mo ml l mm mn">iris = datasets.load_iris()</span><span id="75f6" class="mj la hi mf b fi mo ml l mm mn">X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 42, test_size = 0.2)</span></pre><p id="d89c" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><em class="iv">我们做了一个拆分，用于训练我们的算法，然后分别在测试数据上测试它。</em></p><p id="5528" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我们将定义一个“KNN”类，在这个类中我们将定义使我们的算法工作的每一个基本函数。我们将在我们的类中使用以下方法。</p><ol class=""><li id="99c1" class="ke kf hi iw b ix iy jb jc js kg jt kh ju ki jr kj kk kl km bi translated"><strong class="iw hj"> <em class="iv"> fit </em> </strong>:如前所述，它只是将数据保存在自身中，因为KNN不执行任何显式的训练过程。</li><li id="393c" class="ke kf hi iw b ix kn jb ko js kp jt kq ju kr jr kj kk kl km bi translated"><strong class="iw hj">距离</strong>:这里我们将计算欧几里德距离。</li><li id="251a" class="ke kf hi iw b ix kn jb ko js kp jt kq ju kr jr kj kk kl km bi translated"><strong class="iw hj">预测</strong>:在这个阶段，我们将使用完整的训练数据来预测测试实例的类。我们将用这种方法实现上面讨论的3步过程。</li><li id="156c" class="ke kf hi iw b ix kn jb ko js kp jt kq ju kr jr kj kk kl km bi translated"><strong class="iw hj"> Score </strong>:最后我们将有一个Score方法，根据测试数据计算我们模型的分数</li><li id="3942" class="ke kf hi iw b ix kn jb ko js kp jt kq ju kr jr kj kk kl km bi translated"><strong class="iw hj">那‘K’呢</strong>？:这里最重要的人是K，我们将在初始化KNN类的对象时将“K”作为参数传递(在<em class="iv"> __init__ </em>内)</li></ol><p id="e08d" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><em class="iv">这是我得到的一张关于所使用的各种距离度量的好图片:<br/>它们中的每一个都可以使用它们的Numpy内置函数很容易地计算出来，或者如果你愿意也可以直接编码。</em></p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="ab fe cl mp"><img src="../Images/becd03704504e4653243a2cf47253fb4.png" data-original-src="https://miro.medium.com/v2/format:webp/0*Rh1l2Sev-whPTZkQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">我们将在这里使用欧几里德距离</figcaption></figure><p id="ecf6" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我将首先编写完整的类，然后我们将讨论流程:</p><pre class="jw jx jy jz fd me mf mg mh aw mi bi"><span id="821b" class="mj la hi mf b fi mk ml l mm mn">class KNN:<br/>    def __init__(self, k):<br/>        self.k = k<br/>        <br/>    def fit(self, X, y):<br/>        self.X_train = X<br/>        self.y_train = y<br/>        <br/>    def distance(self, X1, X2):<br/>        distance = scipy.spatial.distance.euclidean(X1, X2)<br/>    <br/>    <strong class="mf hj">def predict(self, X_test):<br/>        final_output = []<br/>        for i in range(len(X_test)):<br/>            d = []<br/>            votes = []<br/>            for j in range(len(X_train)):<br/>                dist = scipy.spatial.distance.euclidean(X_train[j] , X_test[i])<br/>                d.append([dist, j])<br/>            d.sort()<br/>            d = d[0:self.k]<br/>            for d, j in d:<br/>                votes.append(y_train[j])<br/>            ans = Counter(votes).most_common(1)[0][0]<br/>            final_output.append(ans)<br/>            <br/>        return final_output</strong><br/>    <br/>    def score(self, X_test, y_test):<br/>        predictions = self.predict(X_test)<br/>        return (predictions == y_test).sum() / len(y_test)</span></pre><p id="159e" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><em class="iv">查看发生了什么:</em></p><blockquote class="iq ir is"><p id="b5b8" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将在为类“KNN”创建一个对象时传递K。</p><p id="8f05" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> Fit </strong>方法只是接受训练数据，没有别的。</p><p id="cf8d" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们使用<a class="ae mc" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html" rel="noopener ugc nofollow" target="_blank">scipy . spatial . distance . euclidean</a>来计算两点之间的<strong class="iw hj">距离</strong>。</p><p id="52b1" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">预测</strong>方法对每个测试数据点运行一个循环，每次计算测试实例和每个训练实例之间的距离。它将训练数据的距离和索引一起存储在2D列表中。然后，它根据距离对列表进行排序，然后更新列表，只保留列表中的K个最短距离(以及它们的索引)。</p><p id="3390" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然后，它取出对应于这K个最近数据点的标签，并使用计数器检查哪个标签占多数。该多数标签成为测试数据点的标签。</p><p id="eacc" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> Score </strong>方法只是将我们的测试输出与他们的实际输出进行比较，来发现我们预测的准确性。</p></blockquote><p id="8ec8" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">太棒了。就是这样。真的就这么简单！现在让我们运行我们的模型，并在我们之前分开的测试数据上测试我们的算法。</p><pre class="jw jx jy jz fd me mf mg mh aw mi bi"><span id="6244" class="mj la hi mf b fi mk ml l mm mn">clf = KNN(3)<br/>clf.fit(X_train, y_train)<br/>prediction = clf.predict(X_test)</span><span id="daf8" class="mj la hi mf b fi mo ml l mm mn">for i in prediction:<br/>    print(i, end= ' ')</span></pre><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/d091e68da270e993eb666feae50a081a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*eM04OgMfxViDT6_x5EWuHQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">这是我们对X_test的预测标签</figcaption></figure><p id="1af2" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">那是我们对测试数据的预测！我们不介意将它与实际的测试标签进行比较，看看我们做得如何。让我们看看。</p><pre class="jw jx jy jz fd me mf mg mh aw mi bi"><span id="3563" class="mj la hi mf b fi mk ml l mm mn">prediction == y_test</span></pre><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es mr"><img src="../Images/384ab4ee6650091a5ed431a7a5530618.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y7csmGHdLGXw60FEHUhR8w.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">似乎每个输出都被正确预测了。</figcaption></figure><p id="2b11" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">所以！我们走在正确的道路上。因为每个预测都是正确的，所以我们的预测显然会得到满分。</p><pre class="jw jx jy jz fd me mf mg mh aw mi bi"><span id="0bf4" class="mj la hi mf b fi mk ml l mm mn">clf.score(X_test, y_test)</span></pre><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="er es ms"><img src="../Images/ba27d831cbc657843840058aa1e2a3d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n2ax924uRB_oZjniaR2EkA.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">百分百得分！</figcaption></figure><p id="bf36" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我们完事了。真的就这么简单利落。KNN有一个非常基本和极简的方法，非常直观，但它是一个强大和通用的机器学习算法，可以解决各种各样的问题。</p><p id="d20e" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">那都是乡亲们！:)</p></div></div>    
</body>
</html>