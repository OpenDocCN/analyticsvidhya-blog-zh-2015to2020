<html>
<head>
<title>Intuitive Intro To Gaussian Processes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高斯过程的直观介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/intuitive-intro-to-gaussian-processes-328740cdc37f?source=collection_archive---------2-----------------------#2019-09-29">https://medium.com/analytics-vidhya/intuitive-intro-to-gaussian-processes-328740cdc37f?source=collection_archive---------2-----------------------#2019-09-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f2b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">去年，在我计算机科学学士学位的最后一年，我修了两个机器学习的硕士模块。在一次讲座中，教授用七张幻灯片的方程介绍了高斯过程。不用说，没有人知道发生了什么事。</p><p id="055f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章的核心是我关于高斯过程的笔记，它的解释帮助我对它们如何工作有了更直观的理解。大部分内容来源于<a class="ae jd" href="https://youtu.be/4vGiHC35j9s" rel="noopener ugc nofollow" target="_blank">南多·德·弗雷塔斯的精彩讲座</a>和这篇<a class="ae jd" href="http://bridg.land/posts/gaussian-processes-1" rel="noopener ugc nofollow" target="_blank">博客文章</a>。希望对别人有用。</p><p id="bf45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我也在<a class="ae jd" href="https://iq.opengenus.org/intuitive-introduction-to-gaussian-process/" rel="noopener ugc nofollow" target="_blank"> OpenGenus IQ </a>发表了这篇文章。</p><h1 id="a145" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">什么是高斯过程？</h1><p id="58c0" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">高斯过程是一种非参数模型，可用于表示函数的分布。</p><p id="ee53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们来分解一下这个定义。</p><h2 id="5183" class="kh jf hi bd jg ki kj kk jk kl km kn jo iq ko kp js iu kq kr jw iy ks kt ka ku bi translated">模型是参数化的还是非参数化的意味着什么？</h2><h2 id="a9dd" class="kh jf hi bd jg ki kj kk jk kl km kn jo iq ko kp js iu kq kr jw iy ks kt ka ku bi translated"><strong class="ak">参数模型</strong></h2><p id="0ace" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">参数模型假设数据分布(输入点、图像等的集合。)可以完全根据有限的一组参数θ来定义。例如，在简单的线性回归中，参数是方程y = mx + c中的m和c。</p><p id="bb74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使得如果模型被给定参数，则未来预测独立于数据。下图举例说明了这一点，其中x是未来预测，参数是θ，D是观测数据。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es kv"><img src="../Images/bbeea80b761e08a5a9b6970047c6239c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_vwMPsqRk_84GVMYFUiVEQ.png"/></div></div></figure><p id="9d40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着有限的参数集θ包含了你需要知道的关于数据的一切。</p><blockquote class="lh"><p id="3b55" class="li lj hi bd lk ll lm ln lo lp lq jc dx translated"><strong class="ak">即使数据量是无界的，模型的复杂度也是有界的。因此，参数模型不是很灵活。</strong></p></blockquote><p id="472e" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">这些模型的一些例子是多项式回归、逻辑回归和混合模型。</p><p id="c49b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我喜欢通过把参数模型想象成一个自满自大的人来记住这一点。不管你给他们什么新信息，他们总是会得出相同的结论，因为他们的思维模式是固定的。</p><h2 id="a48e" class="kh jf hi bd jg ki kj kk jk kl km kn jo iq ko kp js iu kq kr jw iy ks kt ka ku bi translated"><strong class="ak">非参数模型</strong></h2><p id="f32a" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">这些模型假设<strong class="ih hj">数据分布</strong> <strong class="ih hj">不能用一组有限的参数</strong>来定义。</p><p id="ba12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些类型的模型通常是通过假设一个无限维的θ来定义的，我们可以把它看作一个函数(我将在下面进一步解释)。</p><p id="038d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过这样做，θ能够从数据D中捕获的信息量可以随着数据量的增长而增长。使得这些类型的模型比参数模型更加灵活。</p><p id="e359" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">高斯过程(GP)就是这些模型之一。<br/>它们很酷，因为它们允许我们:<br/> -对任何黑箱函数建模<br/> -对不确定性建模(在信心很重要的情况下很有价值，比如医疗保健)</p><p id="e857" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们知道了GPs的非参数化意味着什么，接下来我们来看看如何使用高斯函数建模。</p><h1 id="6170" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">使用高斯函数建模</h1><blockquote class="lh"><p id="d5df" class="li lj hi bd lk ll lm ln lo lp lq jc dx translated">关键思想:任何函数都可以用无限维多元高斯分布来建模</p></blockquote><p id="a67c" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">这是一个如此混乱的说法，简直是在乞求被拆封。</p><h2 id="7c84" class="kh jf hi bd jg ki kj kk jk kl km kn jo iq ko kp js iu kq kr jw iy ks kt ka ku bi translated">高斯的基础</h2><p id="00de" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">好的第一步是了解什么是高斯(正态)分布及其含义。</p><p id="6080" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你们中的许多人对下面显示的正态分布的钟形曲线非常熟悉。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es lw"><img src="../Images/8910d59455a3a8697c171d5069c0580e.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*SplVeTHr41z-P4PHbkT0iw.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">高斯分布(来源:<a class="ae jd" href="https://amsi.org.au/ESA_Senior_Years/SeniorTopic4/4f/4f_2content_3.html" rel="noopener ugc nofollow" target="_blank">https://amsi . org . au/ESA _ Senior _ Years/Senior topic 4/4f/4f _ 2 content _ 3 . html</a>)</figcaption></figure><p id="185a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">和它的方程一样，下面显示的是多元高斯分布方程。其中，μ是均值，σ是协方差矩阵。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es mb"><img src="../Images/7306501a295a2482fa9394a10bda58a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*uI2AmjyiNHpKIPLftsVWPw.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">一个<em class="mc"> k </em>维随机向量的多元正态分布(来源:维基百科)</figcaption></figure><p id="8e6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">高斯分布通常用于描述任何一组聚集在平均值周围的相关随机变量。</strong></p><p id="5766" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我下面的草图中，蓝色代表2D高斯。蓝点代表平均值(sigma ),它描述了高斯分布的中心位置。适马是协方差矩阵，它描述了每个随机变量相对于其他随机变量的变化情况。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es md"><img src="../Images/7cfbf7d18c5b74e520a142dc54371e21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*5BPpjLq-4jqFk3ginTeShg.png"/></div></figure><p id="3b24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们对高斯函数的组成有了一些了解，让我们看看如何使用高斯函数来得到一些点，最终得到一个函数。</p><h2 id="5b76" class="kh jf hi bd jg ki kj kk jk kl km kn jo iq ko kp js iu kq kr jw iy ks kt ka ku bi translated">我们如何从高斯函数中得到点或函数？</h2><p id="43a5" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">一旦我们通过定义均值和协方差矩阵建立了高斯模型，我们就可以用它来产生数据(也称为抽取样本)。</p><p id="d5d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一种常见的采样技术是反向CDF(累积分布函数)采样。如果我们知道分布的CDF，我们可以产生一个随机样本。简单来说，就是产生一个介于0和1之间的随机(均匀分布)数，并将其与逆CDF一起使用，从高斯分布中获得一个值。再一次，我在下面的草图中展示了这一点。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es me"><img src="../Images/3b211a7837ae588a846846fb74fe0a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0r-k56YbwPt-AKkVsBaMA.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">反向CDF采样图</figcaption></figure><p id="5a45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以现在你可能会说“这和函数有什么关系！！!"。</p><p id="41ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们有一个2D高斯函数，我们用刚刚学过的技巧对它进行采样，我们得到2个点。我们可以画出这两点，然后把它们按顺序连接起来，形成一条线。你想做多少次都可以。在下面这张来自(<a class="ae jd" href="http://bridg.land/posts/gaussian-processes-1" rel="noopener ugc nofollow" target="_blank">http://bridg.land/posts/gaussian-processes-1</a>)的图片中，他们对高斯进行了10次采样，得到了10条线。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es mf"><img src="../Images/98d1c8ca43f30075cde326e7122204d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JcDZNtf7IokoUlUL4wZ6Ow.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">来自高斯的采样点(来源:<a class="ae jd" href="http://bridg.land/posts/gaussian-processes-1" rel="noopener ugc nofollow" target="_blank">http://bridg.land/posts/gaussian-processes-1</a></figcaption></figure><blockquote class="lh"><p id="62fc" class="li lj hi bd lk ll mg mh mi mj mk jc dx translated">这里的关键事实是，对于N-D高斯函数，如果你采样一次，就会得到N个点。</p></blockquote><p id="fabc" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">因此，如果对一个3D高斯样本进行采样，您将获得3个点，对一个30D高斯样本进行采样，您将获得30个点，如果对一个无限维高斯样本进行采样，您将获得无限多个点。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es ml"><img src="../Images/1d48a8a6e0c379f48c7bd4ce842461bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zUyY24RQLLDeyVLFS7iAqw.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">来自20D高斯的10个样本(来源:【http://bridg.land/posts/gaussian-processes-1】T2)</figcaption></figure><blockquote class="mm mn mo"><p id="9926" class="if ig mp ih b ii ij ik il im in io ip mq ir is it mr iv iw ix ms iz ja jb jc hb bi translated">随着维度的数量越来越接近无穷大，这意味着我们不需要把这些点连接起来，因为我们将有一个点来代表每一个可能的输入。</p></blockquote><p id="4255" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要从GP中抽取样本，你需要提供均值和协方差，然后神奇的事情就发生了。不要担心这和以前略有不同。这是因为在下面的例子中，我们从多元高斯分布(MVG)中抽取样本。想法仍然是一样的，我们可以生成点，并将其附加到一个采样向量。你使用分解，因为它允许我们得到类似于矩阵平方根的东西，这样我们可以把我们的MVG表示为单位MVG的变换。</p><pre class="kw kx ky kz fd mt mu mv mw aw mx bi"><span id="b5af" class="kh jf hi mu b fi my mz l na nb">def draw_samples(self, mean, covariance, num_samples=1):<br/> # Every time we sample a D dimensional gaussian we get D points <br/> all_samples = []</span><span id="b989" class="kh jf hi mu b fi nc mz l na nb"># SVD gives better numerical stability than Cholesky Decomp <br/> num_dimensions = len(mean)</span><span id="1593" class="kh jf hi mu b fi nc mz l na nb"> for _ in range(0, num_samples):<br/>   z = np.random.standard_normal(num_dimensions)</span><span id="a93d" class="kh jf hi mu b fi nc mz l na nb">   [U, S, V] = np.linalg.svd(covariance)<br/>   A = U * np.sqrt(S)</span><span id="b87e" class="kh jf hi mu b fi nc mz l na nb">   all_samples.append(mean + np.dot(A, z))</span><span id="414a" class="kh jf hi mu b fi nc mz l na nb">return all_samples</span></pre><p id="e27e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">对我来说，每一个可能的输入都得一分听起来很像一个函数。</strong></p><p id="d7c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是这些图真的很吵。这是因为在这些图表中，他们没有对这些点的相似程度施加任何限制。</p><p id="1fe8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了平滑函数(上面的线),我们需要一个被称为平方指数核的相似性度量(下图)。根据这个函数，如果两个输入点相等，那么它的值是1，否则它们之间的距离越远，它就越趋向于0。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es nd"><img src="../Images/071d199b57b817b1348108d57b99e3ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*SJ3JtrDP0DId3SeXCw6Tew.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">平方指数核函数(来源:维基百科)</figcaption></figure><pre class="kw kx ky kz fd mt mu mv mw aw mx bi"><span id="802b" class="kh jf hi mu b fi my mz l na nb">def kernel(self, a, b):<br/> # Squared Exponential Kernel Function for GP<br/> square_distance = np.sum((a — b) ** 2)<br/> return np.exp(-0.5 * square_distance)</span></pre><p id="2e0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用核函数填充高斯协方差矩阵意味着当您执行采样时，如果采样点在空间上接近，则它们之间应该不会有太大变化。产生看起来更好的线条(见下图),但是这并没有告诉我们如何实际建模一个函数。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es mf"><img src="../Images/620f02ef28f2a159393f396588f9b15e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MDfM3xXZ_ZESuAWtqkbnYA.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">10个样本来自具有内核的20D高斯(来源:<a class="ae jd" href="http://bridg.land/posts/gaussian-processes-1" rel="noopener ugc nofollow" target="_blank">http://bridg.land/posts/gaussian-processes-1</a></figcaption></figure><p id="980b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本节中，我们已经看到，可以对高斯过程进行采样以获得函数，从而使高斯过程成为函数的分布。</p><h1 id="e458" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">模拟一个函数</h1><p id="e740" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">让我们假设有一个函数，我们正试图从这个秘密函数给定一大堆数据来建模。</p><h2 id="083a" class="kh jf hi bd jg ki kj kk jk kl km kn jo iq ko kp js iu kq kr jw iy ks kt ka ku bi translated">我们如何培养一名全科医生？</h2><p id="02a7" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">我们有数据点，x，我们用它来构造协方差矩阵，它描述了给定数据点之间的关系。但重要的是要记住，分布是在标签上，y的。</p><pre class="kw kx ky kz fd mt mu mv mw aw mx bi"><span id="43ba" class="kh jf hi mu b fi my mz l na nb">def train(self):<br/> # GP training means constructing K<br/> mean = np.zeros(len(self.train_x))<br/> covariance = self.build_covariance(self.train_x, self.train_x)</span><span id="4b4b" class="kh jf hi mu b fi nc mz l na nb"> return mean, covariance</span></pre><p id="9780" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们想要预测一些新的数据集x*的标签y*的时候呢？</p><p id="a66f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是开始变得棘手的时候，因为我们需要从联合分布到条件分布。</p><p id="a9b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么？因为MV高斯分布被定义为联合分布。</p><p id="03a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更接近，我们可以做一个小把戏，将新的y*和x*添加到定义中，但只模拟p(y，y*| x，x*)。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es ne"><img src="../Images/c5ce4134814b1c3773930f731035aca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SWL63yR_V8M0ES2hLJel6w.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">高斯过程(来源:<a class="ae jd" href="http://bridg.land/posts/gaussian-processes-1" rel="noopener ugc nofollow" target="_blank">http://bridg.land/posts/gaussian-processes-1</a></figcaption></figure><p id="b4fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是我们想要p(y* | x，y，x*)。我们如何得到它？</p><p id="7e4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要以y*上的多元高斯为条件，因为我们只想要y*上的分布。对我们来说幸运的是，有人已经完成了所有的工作，弄清楚如何做<a class="ae jd" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions" rel="noopener ugc nofollow" target="_blank">这个</a>。它告诉我们如何更新均值和协方差，以便在给定x*和训练数据(x和y)的情况下预测y*。</p><pre class="kw kx ky kz fd mt mu mv mw aw mx bi"><span id="cc85" class="kh jf hi mu b fi my mz l na nb">def _predict(self, covariance_k):<br/> k = covariance_k<br/> k_star = self.build_covariance(self.train_x, self.test_x)<br/> k_star_star = self.build_covariance(self.test_x, self.test_x)</span><span id="3d09" class="kh jf hi mu b fi nc mz l na nb"> k_star_inv = k_star.T.dot(np.linalg.pinv(k))<br/> mu_s = k_star_inv.dot(self.train_y)<br/> sigma_s = k_star_star — np.matmul(k_star_inv, k_star)<br/> <br/> return (mu_s, sigma_s)<br/></span></pre><p id="ddd1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">新的平均值告诉我们y*值是多少，新的方差让我们得到一个可信的测量值。我们可以用这个新的均值和协方差来对函数进行采样。除了点的函数之外，我们还得到置信上限和置信下限的函数。</p><p id="43ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是Scikit-learn的一个示例结果。当你接近数据点(观察值)时，不确定性会变小。观察将任何预测的函数绑定到该区域。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es nf"><img src="../Images/aa0055732d0a9f31bba897132188e324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VX0ySfeNmR4RbtKDTdbgww.png"/></div></div></figure><h1 id="50b3" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">高斯过程的复杂性</h1><p id="3965" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">对于大小为N的数据集，复杂度如下:<br/>时间复杂度:θ(N)<br/>空间复杂度:θ(N)</p><p id="72bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">高斯过程是非常强大的，但是由于其巨大的时间复杂度，不幸的是在许多现实世界的应用中使用它们是不可行的。</p><p id="6fb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章中的所有代码都可以在我的GitHub知识库中找到<a class="ae jd" href="https://github.com/reido2012/ML-Methods" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="7a2a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">资源</h1><p id="fcc2" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">我强烈推荐下面这些不可思议的资源。<br/> <a class="ae jd" href="http://bridg.land/posts/gaussian-processes-1" rel="noopener ugc nofollow" target="_blank">高斯过程简介</a> <br/> <a class="ae jd" href="https://youtu.be/4vGiHC35j9s" rel="noopener ugc nofollow" target="_blank">南多·德·弗莱塔斯的高斯过程简介</a></p></div></div>    
</body>
</html>