<html>
<head>
<title>Pyspark equivalent of Pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pyspark相当于熊猫</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pyspark-equivalent-of-pandas-8912de7f9e39?source=collection_archive---------5-----------------------#2020-04-27">https://medium.com/analytics-vidhya/pyspark-equivalent-of-pandas-8912de7f9e39?source=collection_archive---------5-----------------------#2020-04-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d1d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为Pandas的狂热用户和Pyspark的初学者(我现在仍然是),我总是在寻找一篇关于Pyspark中Pandas的等价功能的文章或栈溢出帖子。我想我会为自己和任何对我有用的人创建一个。这也可能有点冗长。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/f2f8c3d87784b13baadb940836b249b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vh5HNC4vsZ0KW9tWdRRsw.png"/></div></div></figure><p id="0ed0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意:</strong>可能有一个更有效的版本，你可能需要查找，但这可以完成工作。</p><p id="48ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 1:通过引用列表将缺失的列添加到数据帧:</strong></p><p id="88c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设您有一个如下所示的数据帧，pandas中的数据帧命名为<code class="du jp jq jr js b">pandas_df</code>，spark中的数据帧命名为<code class="du jp jq jr js b">spark_df</code>:</p><pre class="je jf jg jh fd jt js ju jv aw jw bi"><span id="78de" class="jx jy hi js b fi jz ka l kb kc"> ---+---+---+---+<br/>|  A|  B|  C|  D|<br/>+---+---+---+---+<br/>| 24|  3| 56| 72|<br/>|  0| 21| 19| 74|<br/>| 41| 10| 21| 38|<br/>| 96| 20| 44| 93|<br/>| 39| 14| 26| 81|<br/>+---+---+---+---+</span></pre><p id="34e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了一个要添加到dataframe中的列列表，默认值为0。</p><p id="6de2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du jp jq jr js b">cols_to_add = ['Col1','Col2']</code></p><p id="57b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在pandas中，我们可以使用下面的<code class="du jp jq jr js b">reindex</code>功能:</p><pre class="je jf jg jh fd jt js ju jv aw jw bi"><span id="5a2b" class="jx jy hi js b fi jz ka l kb kc">print(pandas_df.reindex(columns=pandas_df.columns.union(cols_to_add,sort=False),fill_value=0))</span><span id="fe43" class="jx jy hi js b fi kd ka l kb kc">    A   B   C   D  Col1  Col2<br/>0  24   3  56  72     0     0<br/>1   0  21  19  74     0     0<br/>2  41  10  21  38     0     0<br/>3  96  20  44  93     0     0<br/>4  39  14  26  81     0     0</span></pre><p id="738e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Pyspark中，我们可以使用下面的<code class="du jp jq jr js b">lit</code>函数和<code class="du jp jq jr js b">alias </code>做同样的事情:</p><pre class="je jf jg jh fd jt js ju jv aw jw bi"><span id="f78a" class="jx jy hi js b fi jz ka l kb kc">import pyspark.sql.functions as F<br/>spark_df.select("*",<br/>        *[F.lit(0).alias(i) for i in cols_to_add]).show()</span><span id="a04f" class="jx jy hi js b fi kd ka l kb kc">+---+---+---+---+----+----+<br/>|  A|  B|  C|  D|Col1|Col2|<br/>+---+---+---+---+----+----+<br/>| 24|  3| 56| 72|   0|   0|<br/>|  0| 21| 19| 74|   0|   0|<br/>| 41| 10| 21| 38|   0|   0|<br/>| 96| 20| 44| 93|   0|   0|<br/>| 39| 14| 26| 81|   0|   0|<br/>+---+---+---+---+----+----+</span></pre><p id="13c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2:索引/子集化数据帧:</strong></p><p id="d969" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一些索引，我们想在这些索引中包含一个数据帧的子集。</p><p id="a3da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用上面同样的数据帧，我们可以使用<code class="du jp jq jr js b">.iloc[]</code>作为熊猫数据帧。假设起点和终点如下:</p><p id="0cc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du jp jq jr js b">start_row , end_row = 2,4</code></p><pre class="je jf jg jh fd jt js ju jv aw jw bi"><span id="6519" class="jx jy hi js b fi jz ka l kb kc">print(df.iloc[start_row-1:end_row])<br/>    <br/>    A   B   C   D<br/>1   0  21  19  74<br/>2  41  10  21  38<br/>3  96  20  44  93</span></pre><p id="478f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Pyspark，同样的事情可以通过分配一个<code class="du jp jq jr js b">row_number()</code>然后使用<code class="du jp jq jr js b">between</code>函数来实现。</p><pre class="je jf jg jh fd jt js ju jv aw jw bi"><span id="66cc" class="jx jy hi js b fi jz ka l kb kc">(spark_df.withColumn("Row",F.row_number()<br/>         .over(Window.orderBy(F.lit(0))))<br/>         .filter(F.col("Row")<br/>         .between(start_row,end_row)).drop("Row")).show()</span><span id="e904" class="jx jy hi js b fi kd ka l kb kc">+---+---+---+---+<br/>|  A|  B|  C|  D|<br/>+---+---+---+---+<br/>|  0| 21| 19| 74|<br/>| 41| 10| 21| 38|<br/>| 96| 20| 44| 93|<br/>+---+---+---+---+</span></pre><p id="a891" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3:熊猫和Pyspark列中值的条件赋值</strong></p><p id="4f31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们必须创建一个包含3个条件的条件列，其中:</p><p id="d4ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果A列小于20，赋值<code class="du jp jq jr js b">Less</code>，否则如果A列在20和60之间，赋值<code class="du jp jq jr js b">Medium</code>，否则如果A列大于60，赋值<code class="du jp jq jr js b">More</code>否则赋值<code class="du jp jq jr js b">God Knows</code></p><p id="6a29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在pandas中，推荐的方法是使用<code class="du jp jq jr js b">numpy.select</code>，这是一种矢量化的方法，而不是使用速度较慢的<code class="du jp jq jr js b">apply</code>。</p><pre class="je jf jg jh fd jt js ju jv aw jw bi"><span id="4ff4" class="jx jy hi js b fi jz ka l kb kc">import numpy as np<br/>cond1 , cond2 , cond3 = df['A'].lt(20) , df['A'].between(20,60) , df['A'].gt(60)<br/>value1 , value2 , value3 = 'Less' , 'Medium' , 'More'</span><span id="7e96" class="jx jy hi js b fi kd ka l kb kc">out = df.assign(New=np.select([cond1,cond2,cond3],[value1,value2,value3],default='God Knows'))<br/>print(out)</span><span id="1d2d" class="jx jy hi js b fi kd ka l kb kc">    A   B   C   D     New<br/>0  24   3  56  72  Medium<br/>1   0  21  19  74    Less<br/>2  41  10  21  38  Medium<br/>3  96  20  44  93    More<br/>4  39  14  26  81  Medium</span></pre><p id="c207" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Pyspark中，我们可以使用带有<code class="du jp jq jr js b">selectExpr</code>的SQL <code class="du jp jq jr js b">CASE</code>语句</p><pre class="je jf jg jh fd jt js ju jv aw jw bi"><span id="d5c5" class="jx jy hi js b fi jz ka l kb kc">expr = """CASE <br/>          WHEN A &lt; 20 THEN 'Less' <br/>          WHEN A BETWEEN 20 AND 60 THEN 'Medium'<br/>          WHEN A &gt; 60 THEN 'More'<br/>          ELSE 'God Knows'<br/>          END AS New"""<br/>spark_df.selectExpr("*",expr).show()</span><span id="d340" class="jx jy hi js b fi kd ka l kb kc">+---+---+---+---+------+<br/>|  A|  B|  C|  D|   New|<br/>+---+---+---+---+------+<br/>| 24|  3| 56| 72|Medium|<br/>|  0| 21| 19| 74|  Less|<br/>| 41| 10| 21| 38|Medium|<br/>| 96| 20| 44| 93|  More|<br/>| 39| 14| 26| 81|Medium|<br/>+---+---+---+---+------+</span></pre><p id="8e15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4:使用熊猫系列中的列表或Pyspark列中的数组:</strong></p><p id="f671" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有时，您可能会得到一个列表，如下所示:</p><pre class="je jf jg jh fd jt js ju jv aw jw bi"><span id="2cfc" class="jx jy hi js b fi jz ka l kb kc">  version   timestamp      Arr_col<br/>0      v1  2012-01-10  ['-A','-B']<br/>1      v1  2012-01-11  ['D-','C-']</span></pre><p id="c131" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这类列的任何操作，例如替换子字符串等。理想的方法是使用一个理解列表，这样我们就可以在熊猫中使用以下内容:</p><pre class="je jf jg jh fd jt js ju jv aw jw bi"><span id="2fb9" class="jx jy hi js b fi jz ka l kb kc">output = pandas_df.assign(Arr_col=<br/>                 [[arr.replace('-','')  for arr in i]<br/>                 for i in df['Arr_col']])</span><span id="f8e1" class="jx jy hi js b fi kd ka l kb kc">   version   timestamp Arr_col<br/>0      v1  2012-01-10  [A, B]<br/>1      v1  2012-01-11  [D, C]</span></pre><p id="47bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在PySpark 2.4+中，我们可以访问像<code class="du jp jq jr js b"><a class="ae ke" href="https://spark.apache.org/docs/latest/api/sql/index.html#transform" rel="noopener ugc nofollow" target="_blank">transform</a></code> <a class="ae ke" href="https://spark.apache.org/docs/latest/api/sql/index.html#transform" rel="noopener ugc nofollow" target="_blank"> </a>这样的高阶函数，所以我们可以像这样使用它们:</p><pre class="je jf jg jh fd jt js ju jv aw jw bi"><span id="1cf0" class="jx jy hi js b fi jz ka l kb kc">spark_df.withColumn("Arr_col",<br/>        F.expr("transform(Arr_col,x-&gt; replace(x,'-',''))")).show()</span><span id="49e0" class="jx jy hi js b fi kd ka l kb kc"># or for lower versions , you can use a udf:<br/>from pyspark.sql.types import ArrayType,StringType<br/>def fun(x):<br/>    return [i.replace('-','') for i in x]<br/>myudf = F.udf(fun,ArrayType(StringType()))<br/>spark_df.withColumn("Arr_col",myudf("Arr_col")).show()</span><span id="4701" class="jx jy hi js b fi kd ka l kb kc">+-------+----------+-------+<br/>|version| timestamp|Arr_col|<br/>+-------+----------+-------+<br/>|     v1|2012-01-10| [A, B]|<br/>|     v1|2012-01-11| [D, C]|<br/>+-------+----------+-------+</span></pre><p id="6bf7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读。希望这对你有用。我会试着在将来想出更多这样的场景。</p></div></div>    
</body>
</html>