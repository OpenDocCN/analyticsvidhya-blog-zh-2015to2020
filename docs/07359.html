<html>
<head>
<title>Tutorial on bucket_by_sequence_length API for efficiently batching NLP data while training.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练时高效批处理NLP数据的bucket_by_sequence_length API教程。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tutorial-on-bucket-by-sequence-length-api-for-efficiently-batching-nlp-data-while-training-20d8ef5219d7?source=collection_archive---------2-----------------------#2020-06-23">https://medium.com/analytics-vidhya/tutorial-on-bucket-by-sequence-length-api-for-efficiently-batching-nlp-data-while-training-20d8ef5219d7?source=collection_archive---------2-----------------------#2020-06-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6cb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我第一次接触到<a class="ae jd" href="https://www.youtube.com/watch?v=RIR_-Xlbp7s" rel="noopener ugc nofollow" target="_blank">bucket _ by _ sequence _ length</a>API是在2017 time 7:00 tensor flow Dev summit期间。<br/>该API最初位于包“tf.contrib.training”中，现在它已被移动到“tf.data.experimental”中。<br/>从那时起，该API经历了一些变化。我们将在本教程中讨论最新的API。<br/>本教程解释了使用API的必要性和重要性，随后展示了一个工作示例。</p><p id="c043" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们开始吧。</p><p id="9d32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑一下<a class="ae jd" href="https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection" rel="noopener ugc nofollow" target="_blank">讥讽检测</a>的任务。数据看起来像这样</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="ef7e" class="jn jo hi jj b fi jp jq l jr js">{<br/> “is_sarcastic”: 1,<br/> “headline”: “thirtysomething scientists unveil doomsday clock of hair loss”,<br/> “article_link”: “<a class="ae jd" href="https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205" rel="noopener ugc nofollow" target="_blank">https://www.theonion.com/thirtysomething-scientists-unveil-doomsday-clock-of-hai-1819586205</a>"<br/>}</span></pre><p id="2689" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个字典对象，其中“is _ antonic”是我们的目标，“headline”是我们的特征。</p><p id="9197" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在完整的数据集中，标题将具有不同的长度。这意味着训练一个模型来检测讽刺，我们将不得不使完整数据集中的“标题”的长度相同。这一步必须完成，因为模型的训练是分批进行的，每一批都应该具有相同的形状。</p><p id="0f4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使所有“标题”数据相同的一个方法是填充。将所有标题数据填充到数据库中标题数据的最大长度。用'<pad>'标记填充文本数据。</pad></p><p id="d9f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这在某种程度上是可行的，但是这并不节省内存。让我们对数据集进行一些分析。</p><p id="130a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，使用下面的代码加载JSON数据。从Kaggle下载v2数据到你的本地文件夹。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="04aa" class="jn jo hi jj b fi jp jq l jr js">df = pd.read_json(“./data/Sarcasm_Headlines_Dataset_v2.json”, lines=True)</span></pre><p id="7f0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们得到标题数据的最大长度。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="4844" class="jn jo hi jj b fi jp jq l jr js">print(‘maximum length of headline data is ‘, df.headline.str.split(‘ ‘).map(len).max())```<br/>We receive result `maximum length of headline data is 151`</span></pre><p id="d0c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们得到标题数据的最小长度。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="3d40" class="jn jo hi jj b fi jp jq l jr js">print(‘minimum length of headline data is ‘, df.headline.str.split(‘ ‘).map(len).min())`<br/>We receive result `minimum length of headline data is 2`</span></pre><p id="e594" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们也得到标题文本数据长度的平均值。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="8a6c" class="jn jo hi jj b fi jp jq l jr js">print(‘mean of the lengths of the headline data is ‘, df.headline.str.split(‘ ‘).map(len).mean())`<br/>The result is<br/>`mean of the lengths of the headline data is 10.051853663650023`</span></pre><p id="72f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的数据可以看出，如果我们将每个标题数据填充到151，我们将浪费大量的内存，对于少数数据，我们将拥有比实际单词更多的“<pad>”标记。</pad></p><p id="fb0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将我们带到API“bucket _ by _ sequence _ length ”,此方法更有效，因为它仍然将文本数据填充到相同的长度，但不是完整的数据集(即151个长度),而是针对单个批处理。这意味着一批中的每个标题长度相同，但每批的长度不同，这取决于该批中文本数据的最大长度。</p><p id="a828" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我曾试图自己实现这个API，但是我很难找到合适的文档和合适的例子。所以一旦我想通了，我想如果有更多的人知道并能在工作中使用它会更好。</p><p id="e0c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您阅读API文档，它说它返回一个转换函数，可以从文档传递到“tf.data.Dataset.apply”.“一个数据集转换函数，可以传递到tf.data.Dataset.apply。”</p><p id="3094" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着，首先，我们必须将数据帧转换为TF . data . dataset。tensor flow建议使用tf.data.Dataset API，因为它针对输入管道进行了优化。我们可以在数据集上进行多次转换，但在其他时间可以在数据集上进行更多转换。</p><p id="13be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是在开始之前，让我们先把文本转换成整数。为此，我们将使用TensorFlow标记器。我们将设置词汇大小、嵌入_dim以及词汇外令牌。我们也将设置batch_size，但是稍后您会看到我们也可以进行动态批处理。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="4183" class="jn jo hi jj b fi jp jq l jr js">vocab_size = 1000<br/>embedding_dim = 16<br/>oov_tok = “&lt;OOV&gt;”<br/>batch_size = 64</span></pre><p id="3d52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设置好以上参数后，让我们将文本转换成整数。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="0805" class="jn jo hi jj b fi jp jq l jr js"># Creating an instance of tokenizer<br/>tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok, lower=True)<br/># Creates and updates the internal vocabulary based on the text.<br/>tokenizer.fit_on_texts(df.headline)</span><span id="86dc" class="jn jo hi jj b fi jt jq l jr js"># Add padding token.<br/>tokenizer.word_index[‘&lt;pad&gt;’] = 0<br/>tokenizer.index_word[0] = ‘&lt;pad&gt;’</span><span id="265c" class="jn jo hi jj b fi jt jq l jr js"># Transforms the sentences to integers<br/>sentences_int = tokenizer.texts_to_sequences(df.headline)</span></pre><p id="c904" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们把标签放在一个列表中。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="5891" class="jn jo hi jj b fi jp jq l jr js">labels = df.is_sarcastic.values</span></pre><p id="a081" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们创建数据集，该数据集是为创建输入管道而推荐的。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="c00d" class="jn jo hi jj b fi jp jq l jr js"><br/># Using generator for creating the dataset.<br/>def generator():<br/>  for i in range(0, len(sentences_int)):<br/>    # creates x’s and y’s for the dataset.<br/>    yield sentences_int[i], [labels[i]]</span><span id="f774" class="jn jo hi jj b fi jt jq l jr js"># Calling the from_generator to generate the dataset.<br/># Here output types and output shapes are very important to initialize.<br/># the output types are tf.int64 as our dataset consists of x’s that are int as well as the labels that are int as well.<br/># The tensor shape for x is tf.TensorShape([None]) as the sentences can be of varied length.<br/># The tensorshape of y is tf.TensorShape([1]) as that consists of only the labels that can be either 0 or 1.<br/>dataset = tf.data.Dataset.from_generator(generator, (tf.int64, tf.int64),<br/> (tf.TensorShape([None]), tf.TensorShape([1])))</span></pre><p id="3516" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的数据集已经准备好了，现在让我们使用bucket_by_sequence_length API来生成批处理，并根据我们将提供的上限bucket大小填充我们的句子。让我们创建不同桶的上部长度。我们可以随心所欲地创建存储桶。我建议首先分析数据集，以了解您可能需要的不同存储桶。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="8c57" class="jn jo hi jj b fi jp jq l jr js"># These are the upper length boundaries for the buckets.<br/># Based on these boundaries, the sentences will be shifted to #different buckets.<br/>boundaries = [df.headline.map(len).max() — 850, df.headline.map(len).max() — 700, df.headline.map(len).max() — 500,<br/> df.headline.map(len).max() — 300, df.headline.map(len).max() — 100, df.headline.map(len).max() — 50,<br/> df.headline.map(len).max()]</span></pre><p id="3915" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还必须为不同的存储桶提供batch _ sizes。batch _ sizes的长度应为len(bucket_boundaries) + 1</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="ca00" class="jn jo hi jj b fi jp jq l jr js">batch_sizes = [batch_size] * (len(boundaries) + 1)</span></pre><p id="d92d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">bucket_by_sequence_length API还需要传递一个确定句子长度的函数。一旦API知道了句子的长度，就可以将它放入适当的桶中。在这里的理想场景中，您将创建不同大小的批，这取决于哪个存储桶包含的句子多还是少，但是这里我让所有存储桶的批大小保持不变。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="6592" class="jn jo hi jj b fi jp jq l jr js"># This function determines the length of the sentence.<br/># This will be used by bucket_by_sequence_length to batch them according to their length.<br/>def _element_length_fn(x, y=None):<br/> return array_ops.shape(x)[0]</span></pre><p id="daee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经准备好了调用bucket_by_sequence_length API所需的所有参数，下面是我们对API的调用。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="a65b" class="jn jo hi jj b fi jp jq l jr js"># Bucket_by_sequence_length returns a dataset transformation function that has to be applied using dataset.apply.<br/># Here the important parameter is pad_to_bucket_boundary. If this is set to true then, the sentences will be padded to<br/># the bucket boundaries provided. If set to False, it will pad the sentences to the maximum length found in the batch.<br/># Default value for padding is 0, so we do not need to supply anything extra here.<br/>dataset = dataset.apply(tf.data.experimental.bucket_by_sequence_length(_element_length_fn, boundaries,<br/> batch_sizes,<br/> drop_remainder=True,<br/> pad_to_bucket_boundary=True))</span></pre><p id="9202" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“边界”中的一个重要因素是拥有数据集句子的最大长度。如果你不知道，我会建议让pad_to_bucket_boundary = False</p><p id="6890" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们对数据集进行了适当的批处理和填充，使每个桶具有相同的形状，那么我们就可以分割数据集进行训练和测试。<br/>我无法找到比这里提供的答案更好的分割数据集的解决方案—【https://stackoverflow.com/a/58452268/7220545 T2】</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="2cea" class="jn jo hi jj b fi jp jq l jr js"># Splitting the dataset for training and testing.<br/>def is_test(x, _):<br/> return x % 4 == 0</span><span id="8394" class="jn jo hi jj b fi jt jq l jr js">def is_train(x, y):<br/> return not is_test(x, y)</span><span id="3ea1" class="jn jo hi jj b fi jt jq l jr js">recover = lambda x, y: y</span><span id="c53f" class="jn jo hi jj b fi jt jq l jr js"># Split the dataset for training.<br/>test_dataset = dataset.enumerate() \<br/> .filter(is_test) \<br/> .map(recover)</span><span id="fef2" class="jn jo hi jj b fi jt jq l jr js"># Split the dataset for testing/validation.<br/>train_dataset = dataset.enumerate() \<br/> .filter(is_train) \<br/> .map(recover)</span></pre><p id="796c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这一步之后，我们已经为训练和验证准备好了数据集。模型的训练超出了本教程的范围，但是我在<a class="ae jd" href="https://github.com/PratsBhatt/sarcasm_detection_with_buckets" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上提供了演示模型训练的代码。<br/>要运行该代码，您必须从ka ggle—<a class="ae jd" href="https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/RMI SRA/news-headlines-dataset-for-scara-detection</a>下载数据集，并将其粘贴到。/data文件夹。然后你就可以走了。</p><p id="b4d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望你能愉快地阅读这篇文章，并希望它对你有用。</p></div></div>    
</body>
</html>