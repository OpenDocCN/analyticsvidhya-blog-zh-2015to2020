<html>
<head>
<title>Linear Regression theory recap</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归理论概述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-theory-1736093dfd35?source=collection_archive---------36-----------------------#2020-12-30">https://medium.com/analytics-vidhya/linear-regression-theory-1736093dfd35?source=collection_archive---------36-----------------------#2020-12-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><h1 id="24ad" class="ie if hh bd ig ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb bi translated">问题陈述:</h1><p id="4e92" class="pw-post-body-paragraph jc jd hh je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ha bi translated">为了更好地理解线性回归，让我们看一个简单的例子，当我们只有一个因变量和一个自变量时</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ka"><img src="../Images/3c8b1c26470ec537254bd394f9cb37ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/0*q9C7G5zQFbiFaFlv"/></div><figcaption class="ki kj et er es kk kl bd b be z dx translated">实际Y值和线性线之间的距离最小</figcaption></figure><p id="3386" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">为了能够根据X值预测Y值，我们希望找到一条与所有点足够接近的最佳直线，以此作为可能的最佳直线。</p><p id="adce" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">由于我们寻找的是线性关系，拟合线将是直的，因此该线可以由以下函数定义:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es kr"><img src="../Images/0a91ed13a8ed38d6182e3596f5f613ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*BeKV8IFlzeXf4IKfLinfsQ.png"/></div></figure><p id="50a2" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">因为在我们的例子中只有一个独立变量，那么函数将是:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ks"><img src="../Images/dff31e4cc16bf4a9ccdf0cde98441091.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*dIazJl07ZLnPyZIELbaPLg.png"/></div></figure><p id="ec8c" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">表达最佳拟合线和实际Y值之间差异的一种方式是<strong class="je hi">最小二乘成本函数</strong>，定义如下:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es kt"><img src="../Images/2eb5406363fd5b26d1a012aecf12c7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*_YGc_jfOGL_lr_ASQzCoNQ.png"/></div></figure><p id="64e8" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">当使用这个特定的成本函数时，那么我们我们正在求解一个<strong class="je hi">普通最小二乘回归模型。</strong></p><p id="3986" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">现在我们的目标是<strong class="je hi">找到使这个函数J(θ)(差值)</strong>最小的最佳参数(θ)</p></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><h1 id="806a" class="ie if hh bd ig ih lb ij ik il lc in io ip ld ir is it le iv iw ix lf iz ja jb bi translated"><strong class="ak">解决方案(寻找最佳拟合线):</strong></h1><ol class=""><li id="ac5e" class="lg lh hh je b jf jg jj jk jn li jr lj jv lk jz ll lm ln lo bi translated"><strong class="je hi">批量梯度下降:</strong></li></ol><p id="4df2" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">最小化该函数并找到最佳θ的一种方法是使用<strong class="je hi">梯度下降</strong></p><p id="79f9" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">梯度下降是一种迭代算法，随着时间的推移，它将越来越接近最佳参数。</p><p id="1987" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">首先:我们开始一些随机的初始θ</p><p id="cec1" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated"><strong class="je hi">第二:</strong>我们重复执行这个迭代:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lp"><img src="../Images/61aade0c7f78ef5d47f102e900068c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*EjLBpsD21alMDZZph9fagQ.png"/></div></figure><p id="c99a" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated"><strong class="je hi">注释:</strong></p><ul class=""><li id="03a5" class="lg lh hh je b jf km jj kn jn lq jr lr jv ls jz lt lm ln lo bi translated">a := b表示将b的值赋给a</li><li id="d010" class="lg lh hh je b jf lu jj lv jn lw jr lx jv ly jz lt lm ln lo bi translated">α是学习率(后面会有更多解释)</li><li id="c596" class="lg lh hh je b jf lu jj lv jn lw jr lx jv ly jz lt lm ln lo bi translated">∂是偏导数符号</li></ul><p id="a72c" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated"><strong class="je hi">第三:</strong>让我们为更清晰迭代计算偏导数:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es lz"><img src="../Images/55f1ce11529581627237ca10a533d204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*cglcgU5RSOiZrSgmJ_1HQg.png"/></div></figure><p id="13f5" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">因此，对于单个训练示例，我们得到规则:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es ma"><img src="../Images/9e861b5ca3cad3cdb39c7df9d171e4a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*4LucaGuhkW3yVhfS8u134w.png"/></div></figure><p id="5eac" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">这条规则被称为<strong class="je hi"> LMS(最小均方)</strong>或<strong class="je hi"> Widrow-Hoff </strong>学习规则，因为更新的幅度与误差项(y(I)-hθ(x(I))成比例</p><p id="e543" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">因此，例如，如果我们遇到一个训练示例，在该示例中，我们的预测几乎与y (i)的实际值相匹配，那么我们发现几乎不需要改变参数；相反，如果我们的预测h θ (x(i))具有大的误差(即，如果它离y(i)非常远)，则将对参数进行较大的改变。</p><p id="37cf" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated"><strong class="je hi">第四:</strong>当有多个单个训练示例时，规则将是:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es mb"><img src="../Images/f5351033940a3d6bfdfcb6961ae7501f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*-CfX4H5cifQmkzhyZvCf4A.png"/></div></figure><p id="fd68" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">所以，这只是原始成本函数j的梯度下降。</p><p id="c2dc" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated"><strong class="je hi">标注提醒:</strong></p><ul class=""><li id="a2b5" class="lg lh hh je b jf km jj kn jn lq jr lr jv ls jz lt lm ln lo bi translated">θj中的小j从1到n，表示哪个参数</li><li id="5369" class="lg lh hh je b jf lu jj lv jn lw jr lx jv ly jz lt lm ln lo bi translated">I是数据样本行</li></ul><p id="480c" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">这种方法在每一步上查看整个训练集中的每个示例，称为批量梯度下降。</p><p id="854f" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">注意，虽然梯度下降通常易受局部极小值的影响，但是我们在这里为线性回归提出的优化问题只有一个全局最优值，而没有其他局部最优值；因此梯度下降总是收敛(假设学习率α不太大)到全局最小值。的确，J是一个凸二次函数。<br/>这是一个梯度下降的例子，它被运行以最小化二次函数<br/>:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es mc"><img src="../Images/3a0b1b20aed0e3485900206ceb6d5301.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*2p0v73OCvaEOmrZa7aD2yA.png"/></div></figure><p id="640e" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">上面显示的椭圆是二次函数的轮廓。另外<br/>显示的是梯度下降的轨迹，在<br/> (48，30)初始化。图中的x(用直线连接)标记了梯度下降经过的θ的连续<br/>值。</p></div><div class="ab cl ku kv go kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="ha hb hc hd he"><p id="436e" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated"><strong class="je hi"> 2。随机梯度下降:</strong></p><p id="94f8" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">批处理梯度下降在采取单个步骤之前必须扫描整个训练集-如果m很大，这是一个昂贵的操作-<strong class="je hi">随机梯度下降</strong>将数据分成小块，在每个步骤中，它只在特定的块上训练数据，同时循环这些步骤，因此它可以立即开始取得进展，并继续在它查看的每个示例中取得进展。</p><p id="8c27" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">通常，随机梯度下降比批量梯度下降更快地使θ“接近”最小值。</p><p id="eec9" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">但是注意，它可能永远不会“收敛”到最小值，参数θ会一直在J(θ)的最小值附近振荡；但是在实践中，接近最小值的大多数值将是真正最小值的相当好的近似值。</p><p id="013c" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">由于这些原因，特别是当训练集很大时，随机梯度下降通常优于批量梯度下降。</p><p id="17d9" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated"><strong class="je hi">随机梯度下降</strong>算法是:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es md"><img src="../Images/e391c09404ad6bf7aab3388c876d0b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*7xjLYzGk7n894KJ5xHZyBg.png"/></div></figure><p id="638c" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated"><strong class="je hi"> 3。正规方程:</strong></p><p id="f1b9" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">计算使成本函数最小的θ的另一种方法是使用一种分析方法，我们找到使成本函数的<strong class="je hi">导数等于零的θ。</strong></p><p id="a57d" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">其中，使成本函数最小的θ可通过下式获得:</p><figure class="kb kc kd ke fd kf er es paragraph-image"><div class="er es me"><img src="../Images/29f72f46f48b62ed617365f0b92b56e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*MXJvwQis1ACzWHVICKvcHg.png"/></div></figure><p id="3bb1" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">其中:</p><ul class=""><li id="0fc8" class="lg lh hh je b jf km jj kn jn lq jr lr jv ls jz lt lm ln lo bi translated">x是一个(m，n)矩阵，其中m是行数，n是列数</li><li id="6af8" class="lg lh hh je b jf lu jj lv jn lw jr lx jv ly jz lt lm ln lo bi translated">y是因变量的向量(我们想要预测的变量)</li></ul><p id="7531" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">不要忘记在第一列中添加一个带有[1]xm的列来表示X0</p><p id="f4fb" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">这种方法的缺点是，当n(要素数)很大(大于10K)时，速度很慢。</p><p id="0fc4" class="pw-post-body-paragraph jc jd hh je b jf km jh ji jj kn jl jm jn ko jp jq jr kp jt ju jv kq jx jy jz ha bi translated">参考资料:</p><ul class=""><li id="2dd8" class="lg lh hh je b jf km jj kn jn lq jr lr jv ls jz lt lm ln lo bi translated">安德鲁·吴(斯坦福大学)ML笔记。</li></ul></div></div>    
</body>
</html>