<html>
<head>
<title>Density Estimation: MLE, MAP, MOM, KDE, ECDF, Q-Q Plot, GAN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">密度估计:最大似然法、地图、矩量法、、、Q-Q图、甘</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/density-estimation-mle-map-mom-kde-ecdf-q-q-plot-gan-5161f84d28d7?source=collection_archive---------2-----------------------#2019-10-02">https://medium.com/analytics-vidhya/density-estimation-mle-map-mom-kde-ecdf-q-q-plot-gan-5161f84d28d7?source=collection_archive---------2-----------------------#2019-10-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/340813db127bb3e58606760ea7632e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvzAQU3wJ3Zxjf4O_UEj4g.png"/></div></div></figure><p id="7153" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">统计学围绕着从样本中对总体进行估计。密度估计是从样本中估计总体的概率密度函数。</p><h1 id="425a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">术语</h1><p id="d981" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated"><strong class="is hj">估计量:</strong>逼近感兴趣参数的数据函数。估计量是一个随机变量，因为它是随机样本的函数。</p><p id="2e65" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">一致估计量</strong>:当样本量趋于无穷大时，任何收敛到感兴趣的真实参数的估计量称为一致估计量。样本均值是一个一致的估计量。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/d988bac51646b2a68c4417e0bda083e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*lHOr6e666G81D-O3qVjLsw.png"/></div></figure><p id="7671" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">无偏估计量:</strong>任何期望值等于感兴趣的真实参数的估计量称为无偏估计量。样本均值是一个无偏估计量。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/a2a3c9ce071a65656514b1d164095259.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/format:webp/1*2a74zBR8AuCq2ZrlS-sYkA.png"/></div></figure><p id="2d23" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">参数方法:我们假设人口服从某种分布，并试图从现有数据中估计其参数。</p><p id="d0c1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">非参数方法</strong>:对人口分布不做任何假设。虽然参数方法只涉及估计几个参数，但非参数方法试图估计整个样本空间的密度。</p><p id="a60c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">显式密度估计:</strong>估计样本空间上的真实pdf或cdf。</p><p id="e2d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">隐式密度估计:</strong>不产生显式密度，但产生一个函数，可以从真实分布中抽取样本。</p><h1 id="7ed6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">显式密度估计</h1><h2 id="e644" class="kx jp hi bd jq ky kz la ju lb lc ld jy jb le lf kc jf lg lh kg jj li lj kk lk bi translated">最大似然估计</h2><p id="bdaf" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">最大似然法是一种参数方法，它使似然函数或对数似然函数最大化，对数似然函数是参数的函数。MLE是一个随机变量，因为它是根据随机样本计算的。MLE是一个相合估计量，在一定条件下，它渐近收敛于一个正态分布，其真参数为均值，方差等于Fisher信息矩阵的逆。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/b304aef1adea884c597f7ede47772aea.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*hvR_G_JmH6ppTuSFlUvSXg.png"/></div></figure><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/969fc2eccba02b9e74b8c628e9f73252.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*PrJ3iwFuoOgOrHpo7jMfSg.png"/></div></figure><p id="c5c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，如果我们假设我们的总体遵循正态分布，它由均值和方差参数化，MLE是样本均值和重标样本方差。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/fd675580d69f384a4b5ee4f3c03ec4e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*HHGA4ewJH05hjOuPRGfBQQ.png"/></div></figure><p id="a8eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">MLE的一致性</strong></p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/bb4562b7ad9e559c3e7124da2e13634e.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*RGyGQ5k_eUafTTJwMVd7Ng.png"/></div></figure><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/65840bbace9c66894954b021c0c6496f.png" data-original-src="https://miro.medium.com/v2/resize:fit:170/format:webp/1*PELLG5ZoHoILmwNSB5sc4A.png"/></div></figure><p id="1dc9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如上所述，MLE是一个一致的估计量，即随着样本量的增加，MLE接近真实参数，如上图所示。</p><h2 id="6c6c" class="kx jp hi bd jq ky kz la ju lb lc ld jy jb le lf kc jf lg lh kg jj li lj kk lk bi translated">最大后验估计</h2><p id="2ad0" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在计算其后验似然性时，映射考虑了我们对真实参数先验信念。如果我们的先验信念在参数空间上是一致的，MLE等于MAP。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/52ec0e777370d872577de5eb34e3e993.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/1*y7osFNx_XCxarTB0Q_Zt-w.gif"/></div></figure><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/400f73f191854cdfe5bbc71fddbff470.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*8hZU6ERphXS91Z3ydj1Z2w.png"/></div></figure><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/f64c11fdcf88c74187a5112420c7a990.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/1*R1s5DC6Q9B1whLQZoXUgVQ.gif"/></div></figure><h1 id="4cb4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">矩量法</h1><p id="a937" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">根据大数定律(LLN)，随着样本量趋于无穷大，平均值收敛于期望值。使用该法则，作为参数函数的群体矩被设置为等于样本矩以求解参数。对于正态分布，MLE和MOM都产生样本均值作为总体均值的估计。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/70145179a9435a0e4cab78f271e8c5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/1*7lNqqVkziIevVw1HqtDE0g.gif"/></div></figure><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/b63ecbd7efde6753c6941f5b9e60c99d.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/1*XQG1fZYDAvpyeAPMDPjpcw.gif"/></div></figure><h2 id="9c72" class="kx jp hi bd jq ky kz la ju lb lc ld jy jb le lf kc jf lg lh kg jj li lj kk lk bi translated">经验累积分布函数(ECDF)</h2><p id="6c95" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">描述数据生成分布(也称为人口分布)的另一种方法是估计其cdf，而不是pdf。ECDF是一致估计量、无偏估计量和非参数估计量。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/57f68f4fc212fd5d12c29a0ec2be278f.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*ZK80pf8uXzH8vYxaZNb8XA.png"/></div></figure><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/33b5e28063630140f96d7b86cdf8d546.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*yNXtdQoawqPPLEhpdv7L9w.png"/></div></figure><div class="ks kt ku kv fd ab cb"><figure class="lx ij ly lz ma mb mc paragraph-image"><img src="../Images/50620f5cc45eade93564ca7e05205f46.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*UY_8IfxkBZNd8e3ZWrbXpg.png"/></figure><figure class="lx ij ly lz ma mb mc paragraph-image"><img src="../Images/90bf8ed6f1a74b777f47524ded1b9ebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*41dy8JUMD4builw6eQCEmg.png"/></figure></div><p id="138d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随着样本量的增加，ecdf变得接近真实的cdf。</p><h2 id="9a87" class="kx jp hi bd jq ky kz la ju lb lc ld jy jb le lf kc jf lg lh kg jj li lj kk lk bi translated">核密度估计(KDE)</h2><p id="d58e" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">KDE是一种估计数据生成分布的概率密度函数的非参数方法。如果样本数据周围有许多数据点，KDE会为某些<code class="du md me mf mg b">x</code>分配高密度。一个数据点对某个<code class="du md me mf mg b">x</code>的贡献取决于它到<code class="du md me mf mg b">x</code>的距离和带宽。随着样本量的增加，KDE近似在一定条件下接近真实的概率密度函数。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/f4195e5d336740f6dbec65effaa063df.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*hlWEI-mDmuNWSSUDeOjG4A.png"/></div></figure><p id="7b5e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">核函数应该总是产生大于或等于零的值，并且应该在样本空间上积分为1。一些流行的核有均匀核、高斯核、双权重核等。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/4b71396f41909aad89506a96eee7e399.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*Vbeyh2cpNgC0uuAWYKawsg.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">带宽:斯科特，内核:高斯</figcaption></figure><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/fec6978a3a47cd9b5300b3d0f261ca01.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*AoUvaZr848PxLMt4EqdUEA.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">带宽的影响</figcaption></figure><p id="dad8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于带宽较大，它会高估周围数据点较少的点的密度，从而过度平滑曲线。另一方面，如果带宽非常小，它会产生虚假波动，并使数据过度拟合。</p><h1 id="174d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">Q-Q图</h1><p id="6183" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">绘制两个分布的分位数，通常是相对于具有已知参数的理论分布，如标准正态分布。如果样本是由与理论分布相同的数据分布生成的，q-q图看起来像一条直线。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/818ece87e9661e7d9fac397ac6f0e961.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*vFp0ddzwwkZCDyzyDhRx-A.png"/></div></figure><p id="33a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然Q-Q图不能估计真实的pdf，但它是检验我们对真实pdf的假设的一种快速简单的方法。</p><h1 id="445c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">隐式密度估计</h1><h2 id="46ee" class="kx jp hi bd jq ky kz la ju lb lc ld jy jb le lf kc jf lg lh kg jj li lj kk lk bi translated">开始</h2><p id="6336" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">GAN从潜在分布中随机抽取样本作为输入，并将其映射到数据空间。训练的任务是学习确定性函数，该函数可以有效地捕获数据中的相关性和模式，以便映射的点类似于从数据分布中生成的样本。下面我从<code class="du md me mf mg b">Isortropic Bivariate Gaussian</code>分布中生成了300个随机样本。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/56309e429bcf2f8e0efcae30a1ebbb02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZcaPAgElCItnp0oCiVS_yw.png"/></div></div></figure><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/ef38508cd755bbd3397041a3a40aab8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/1*R88gzkQao_dhm5lzlvKgIQ.gif"/></div></figure><p id="68a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当通过上述函数时，样本形成一个环。在GAN中，该功能实质上是发电机网络。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/7ddd5ff925e136cae40b3f0935fe373e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yuyeLSsEpObu8IskXPbc3w.png"/></div></div></figure><p id="1a0c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的例子表明，可能存在高容量函数，该函数能够对像图像这样的高维数据的数据分布进行建模。神经网络是最好的选择，因为它们是通用的函数逼近器。因此，在对图像的数据分布建模时使用深度神经网络。</p><p id="b43c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我刚刚介绍了文章中的主题。多读一些关于它们的书，加强你对它们的理解。在这里找到完整的笔记本<a class="ae mr" href="https://github.com/ajitsamudrala/Density-Estimation-Medium" rel="noopener ugc nofollow" target="_blank"/>。</p></div></div>    
</body>
</html>