<html>
<head>
<title>A Python solution to run Query in Google Cloud Dataproc using API and return back query results</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用API在Google Cloud Dataproc中运行查询并返回查询结果的Python解决方案</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-python-solution-to-run-query-in-google-cloud-dataproc-using-api-and-return-back-query-results-7c804fa5e08e?source=collection_archive---------8-----------------------#2019-11-24">https://medium.com/analytics-vidhya/a-python-solution-to-run-query-in-google-cloud-dataproc-using-api-and-return-back-query-results-7c804fa5e08e?source=collection_archive---------8-----------------------#2019-11-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="fef6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文原载于<a class="ae jd" href="https://burntbit.com/a-python-solution-to-run-query-in-google-cloud-dataproc-using-api-and-return-back-query-results/" rel="noopener ugc nofollow" target="_blank">大数据日报</a>，也可以在<a class="ae jd" href="https://www.linkedin.com/pulse/python-solution-run-query-google-cloud-dataproc-using-boning-zhang/?published=t" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上找到。源代码见<a class="ae jd" href="https://github.com/BoningZhang/Learning_Airflow/blob/master/common/utils/run_hive_query_gcp.py" rel="noopener ugc nofollow" target="_blank"> Git </a>。</p><p id="4fdc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在某些情况下，我们需要从本地运行Google Cloud Dataproc中的查询并返回查询结果，例如，我们需要根据Dataproc表中的查询结果向用户发送一些消息，或者我们需要根据dataproc表中的查询结果更新mysql表。</p><p id="cc31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们的hive表驻留在本地hadoop集群中，我们可以使用python中的subprocess运行hive/spark-sql查询，查询结果将以字符串形式从subprocess命令返回。因此，我们可以处理字符串结果。该解决方案可以按如下方式实施:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="6374" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是没有简单的解决方案可以从Google Cloud Dataproc API触发的查询中获取结果，因为API调用不会返回查询结果。假设如果我们通过调用dataproc API运行“<code class="du jl jm jn jo b">SELECT * FROM table_a</code>”，我们无法在dataproc作业的输出中获得查询结果。</p><p id="6e97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以这篇博客将实现一个使用API和output来运行dataproc查询的解决方案。至少有三种解决方案可以做到这一点。</p><p id="4010" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(1)从hive表的HDFS文件创建外部大查询表，因为大查询交互地运行，这意味着查询结果将包括在大查询作业的输出中。详见<a class="ae jd" href="https://cloud.google.com/bigquery/docs/running-queries" rel="noopener ugc nofollow" target="_blank">https://cloud.google.com/bigquery/docs/running-queries</a></p><p id="b06a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(2)如果我们的目标是基于dataproc的查询结果更新mysql表，那么我们可以通过<code class="du jl jm jn jo b">JDBC</code>将pyspark数据帧转储到mysql表中，并使用API将这个pyspark作业提交给dataproc。详见<a class="ae jd" href="https://stackoverflow.com/questions/46552161/write-dataframe-to-mysql-table-using-pyspark" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/46552161/write-data frame-to-MySQL-table-using-py spark</a></p><p id="474b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(3)使用云存储作为临时空间来转储查询结果。默认情况下，每个dataproc作业都会将其日志和输出转储到云存储的文件中。但是由于dataproc中的hive查询使用的是Beeline，所以输出文件中会有一些其他的日志信息，因此不容易得到清晰的查询结果。然而，我们可以使用类似的想法，将查询结果覆盖到云存储中的指定文件，然后从云存储中获取内容。本博客的以下部分将详细解释这个解决方案。假设查询结果相对较小，否则我们将不希望运行查询并以字符串形式返回结果，即使在本地hadoop集群中也是如此。</p><p id="d91d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我这里用的python是<code class="du jl jm jn jo b">python2.7</code>，用的是<code class="du jl jm jn jo b">google.cloud</code>、<code class="du jl jm jn jo b">apilcient</code>、<code class="du jl jm jn jo b">oauth2client</code></p><p id="94ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du jl jm jn jo b">run_hive_query_gcp()</code>是我们可以使用的函数run hql，它将以字符串形式返回查询结果。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="3aa5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Google Cloud文档显示使用环境变量export <code class="du jl jm jn jo b">GOOGLE_APPLICATION_CREDENTIALS="/home/user/Downloads/[FILE_NAME].json"</code>来获得认证</p><p id="04da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(<a class="ae jd" href="https://cloud.google.com/docs/authentication/production#obtaining_and_providing_service_account_credentials_manually" rel="noopener ugc nofollow" target="_blank">https://Cloud . Google . com/docs/authentic ation/production # getting _ and _ provide _ service _ account _ credentials _ manually</a>)，但是如果我们有几个不同的Google Cloud项目，就不太方便了，因为我们可能需要为不同的项目设置<code class="du jl jm jn jo b">GOOGLE_APPLICATION_CREDENTIALS</code>为不同的json key文件。相反，这里我使用了另一个包来设置身份验证，我们可以使用json密钥文件作为身份验证函数ServiceAccountCredentials()的输入参数。更多信息请参见<a class="ae jd" href="https://developers.google.com/analytics/devguides/config/mgmt/v3/quickstart/service-py" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/analytics/dev guides/config/mgmt/v3/quick start/service-py</a></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="73fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们向dataproc集群提交一个hive查询，并等待它的执行。在“<code class="du jl jm jn jo b">SELECT * FROM table_a</code>”子句之前，我们添加了sql来将查询结果覆盖到云存储中的一个文件。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="328c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在执行dataproc作业之后，我们使用云存储API将结果blobs作为字符串下载到云存储中。</p></div></div>    
</body>
</html>