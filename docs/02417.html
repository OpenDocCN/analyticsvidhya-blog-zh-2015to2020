<html>
<head>
<title>Web Scraping html table from Wiki</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">来自Wiki的Web抓取html表</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/web-scraping-html-table-from-wiki-9b18cf169359?source=collection_archive---------1-----------------------#2019-12-16">https://medium.com/analytics-vidhya/web-scraping-html-table-from-wiki-9b18cf169359?source=collection_archive---------1-----------------------#2019-12-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6977" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">用美汤从维基百科提取一个气候数据。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/8cefcc77d392c64e2baefd640c306c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*twfr6TACXlpXXeFiGY4b0g.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">网刮采用美汤。</strong></figcaption></figure><p id="1dcb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">“哪里有数据的硝烟，哪里就有生意的火。”—托马斯·莱德曼</strong>互联网每天都充斥着海量数据。这个<strong class="jq hj">数据</strong>在应用之前是没有用的。数据科学家的工作包括收集大量难以处理的数据，并将其转换为更有用的格式。但是如何从毫无价值的垃圾中刮出有价值的数据呢？</p><p id="c2da" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">接下来是<strong class="jq hj"> BeautifulSoup，它是一个流行的Python库</strong>，旨在快速从html垃圾中抓取有价值的数据。这种从网站下载大量信息的技术被称为“网络抓取”。 Beautiful Soup位于流行的Python解析器之上，如<strong class="jq hj">‘lxml’和‘html 5 lib</strong>’。你所需要的只是对HTML的了解，你可以从https://www.w3schools.com/那里学到这些。漂亮的Soup library解析数据，为你吐出树遍历的东西。因此，让我们看看如何从维基百科页面获取气候数据的HTML表格。</p><p id="7697" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这个过程只有很少的基本步骤。只需几行代码，所需的数据就能以您想要的任何格式生成。csv、xml和json。</p><ol class=""><li id="cdb0" class="kl km hi jq b jr js ju jv jx kn kb ko kf kp kj kq kr ks kt bi translated"><strong class="jq hj">装美汤:</strong> pip装美汤4</li><li id="304e" class="kl km hi jq b jr ku ju kv jx kw kb kx kf ky kj kq kr ks kt bi translated"><strong class="jq hj">导入所需的库:urllib </strong>是一个标准的Python库，因此不需要安装任何额外的东西来运行这段代码，它包含了跨web请求数据、处理cookies、甚至更改元数据(如标题和用户代理)的函数。</li></ol><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="f6d5" class="le lf hi la b fi lg lh l li lj"><strong class="la hj">import requests<br/>import urllib.request<br/>import pandas as pd<br/>import csv<br/>from bs4 import BeautifulSoup</strong></span></pre><p id="2113" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 3。获取您想要抓取的URL:</strong>本文将使用维基百科的URL来获取迪拜的气候数据，并使用下面的<strong class="jq hj">请求库来访问它。在我们开始之前，请注意从任何网站收集数据并不总是合法的。我们需要在着陆前检查状态码。200显示你可以继续下载。</strong></p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="7980" class="le lf hi la b fi lg lh l li lj"><strong class="la hj">url = 'https://en.wikipedia.org/wiki/Climate_of_Dubai'<br/>response = requests.get(url)<br/>print(response.status_code)<br/>[out]: 200 </strong></span></pre><p id="b3de" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 4。查看页面并详细检查:</strong>从页面中找到想要解析的数据。右键单击数据并选择inspect from chrome，查看数据中使用的所有HTML元素。下图显示了我们将使用Beautiful Soup处理的表格数据。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lk"><img src="../Images/850a90b0674a2f9bb0380801f7a48446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTsNGqqcCPiPAq6dVGAy-w.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">迪拜的气候数据</strong></figcaption></figure><p id="8119" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 5。</strong>让我们开始使用<strong class="jq hj">美汤</strong>解析杂乱的html数据，以获得结构化数据。在本例中，BeautifulSoup库中最常用的对象是BeautifulSoup对象“Soup”。为了缩短长度，我在这里避免了整个输出。但是输出显示了两个表，其中我们需要第二个表的类属性“<strong class="jq hj">可折叠”。</strong></p><p id="373b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">soup = beautiful soup(response . text，" html.parser") <br/> soup </strong></p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="5b1b" class="le lf hi la b fi lg lh l li lj">Out[3]:<br/>&lt;table class="<strong class="la hj">wikitable collapsible</strong>" style="width:100%; text-align:center; line-height: 1.2em; margin:auto;"&gt;<br/>&lt;tbody&gt;&lt;tr&gt;<br/>&lt;th colspan="14"&gt;Climate data for Dubai<br/>&lt;/th&gt;&lt;/tr&gt;<br/>&lt;tr&gt;<br/>&lt;th scope="row"&gt;Month<br/>&lt;/th&gt;<br/>&lt;th scope="col"&gt;Jan<br/>&lt;/th&gt;<br/>&lt;th scope="col"&gt;Feb<br/>&lt;/th&gt;<br/>&lt;th scope="col"&gt;Mar<br/>&lt;/th&gt;<br/>&lt;th scope="col"&gt;Apr<br/>&lt;/th&gt;<br/>&lt;th scope="col"&gt;May<br/>&lt;/th&gt;<br/>&lt;th scope="col"&gt;Jun<br/>&lt;/th&gt;</span></pre><p id="80b7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 5。提取表格的代码:</strong>使用这个BeautifulSoup对象，我们可以使用<strong class="jq hj"> findAll </strong>函数提取一个Python列表，该列表是通过只选择&lt;类中的文本找到的:“collapsible”&gt;。<strong class="jq hj"> find </strong>方法用于从页面中获取第一个表格，而<strong class="jq hj"> findAll </strong>方法获取所有表格。例:<strong class="jq hj"> findAll(标签，属性，递归，文本，限制，关键字)</strong></p><p id="2a70" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> find(标签，属性，递归，文本，关键字)。大多数时候，我们只需要标签和属性来获取数据。</strong></p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="0b42" class="le lf hi la b fi lg lh l li lj"><strong class="la hj">table = soup.findAll('table',{"class":"collapsible"})[0]<br/>values =[]<br/>#There are 12 rows out of which we  need only rows from 1 to 11.<br/>tr = table.findAll(['tr'])[1:11]</strong></span></pre><p id="6da8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 6。</strong> <strong class="jq hj">以所需格式存储数据:</strong>我们需要以. csv格式存储数据。所以使用&lt; th &gt;标签从表格中获取标题，使用&lt; td &gt;标签获取列中的数据。然后将获取的数据写入一个名为climate.csv的文件，并使用选项' wt '，newline = ' '用于避免两行之间出现任何换行符。如果文档没有指定编码，Beautiful Soup检测不到编码，那么就给出编码。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="6255" class="le lf hi la b fi lg lh l li lj"><strong class="la hj">csvFile = open("climate.csv",'wt',newline='',encoding='utf-8')<br/>writer = csv.writer(csvFile)  <br/>try:   <br/>        for cell in tr:<br/>            th = cell.find_all('th')<br/>            th_data = [col.text.strip('\n') for col in th]<br/>            td = cell.find_all('td')<br/>            row = [i.text.replace('\n','') for i in td]<br/>            writer.writerow(th_data+row)      <br/>        <br/>finally:   <br/>    csvFile.close()</strong></span></pre><p id="4b6a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">上述代码将输出一个<strong class="jq hj"> climate.csv </strong>文件，如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ll"><img src="../Images/28396c09fc9fc422937bcf384770254f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvpkv2Xe0JdlE8DDDzmLog.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn"> Climate.csv文件</strong></figcaption></figure><p id="a76c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们还可以将csv文件进一步读入<strong class="jq hj">熊猫</strong>，如下图所示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lm"><img src="../Images/0001771e620c8f77995cff72c133221f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Y9YnEhMbw3r99FxiT8Y_Q.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jn">将climate.csv读入熊猫</strong></figcaption></figure><p id="6937" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这一切都是为了把谷物从谷壳中分离出来。这是完整的代码。</p><pre class="iy iz ja jb fd kz la lb lc aw ld bi"><span id="73ca" class="le lf hi la b fi lg lh l li lj"><strong class="la hj">#import libraries</strong><br/><strong class="la hj">import</strong> requests<br/><strong class="la hj">import</strong> pandas <strong class="la hj">as</strong> pd<br/><strong class="la hj">import</strong> csv<br/><strong class="la hj">import</strong> urllib.request<br/><strong class="la hj">from</strong> bs4 <strong class="la hj">import</strong> BeautifulSoup</span><span id="a142" class="le lf hi la b fi ln lh l li lj"><strong class="la hj">url = '</strong><a class="ae kk" href="https://en.wikipedia.org/wiki/Climate_of_Dubai'" rel="noopener ugc nofollow" target="_blank"><strong class="la hj">https://en.wikipedia.org/wiki/Climate_of_Dubai'</strong></a><strong class="la hj"><br/>response = requests.get(url)<br/>print(response.status_code)<br/>soup = BeautifulSoup(response.text,"html.parser")<br/>table = soup.findAll('table',{"class":"collapsible"})[0]<br/>tr = table.findAll(['tr'])[1:11]<br/>csvFile = open("climate.csv",'wt',newline='', encoding='utf-8')<br/>writer = csv.writer(csvFile)  <br/>try:   <br/>        for cell in tr:<br/>            th = cell.find_all('th')<br/>            th_data = [col.text.strip('\n') for col in th]<br/>            td = cell.find_all('td')<br/>            row = [i.text.replace('\n','') for i in td]<br/>            writer.writerow(th_data+row)      <br/>        <br/>finally:   <br/>    csvFile.close()</strong></span></pre><p id="75db" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">不管你尝试过多少网页抓取代码，但是每次你做的时候，看到数据在5分钟内被轻松漂亮地抓取，程序员总是会兴奋不已，有点神奇；这就是美！现在继续尝试网页抓取和检查Scrapy和硒也。</p></div><div class="ab cl lo lp gp lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hb hc hd he hf"><p id="ff88" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我希望这篇文章内容丰富，增加了你的知识。如果你喜欢，请不要忘记点击下面的拍手图标。使用<a class="ae kk" href="https://github.com/Priya-raja/Data-Science/blob/master/Web%20Scrapping.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Priya-Raja/Data-Science/blob/master/Web % 20 screwing . ipynb</a>检查github中的代码</p></div></div>    
</body>
</html>