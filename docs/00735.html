<html>
<head>
<title>Reinforcement Learning — Basic Understanding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习—基本理解</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-basic-understanding-4fcb91ba4e4?source=collection_archive---------6-----------------------#2019-09-02">https://medium.com/analytics-vidhya/reinforcement-learning-basic-understanding-4fcb91ba4e4?source=collection_archive---------6-----------------------#2019-09-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6ca8" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用基于价值的方法理解强化学习背后的基本概念</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/f871f93dd77648425845722366927e09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jNBNnXW8htC9hIA7u_LznQ.png"/></div></div></figure><h1 id="2326" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">介绍</h1><p id="05cb" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">强化学习(RL)是一个机器学习领域，自2015年以来备受关注，此前谷歌的<a class="ae kx" href="https://deepmind.com/" rel="noopener ugc nofollow" target="_blank"> Deep Mind </a>团队展示了自学成才的DQN代理人学习走路、掌握雅达利游戏以及在游戏<a class="ae kx" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="noopener ugc nofollow" target="_blank">围棋</a>中击败亲人类选手。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ky kz l"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">观看DeepMind行走代理</figcaption></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="le kz l"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">DeepMind学习玩突围</figcaption></figure><p id="6e12" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">RL是自学成才的软件代理人背后的科学，他们探索一个以前未知的世界，并最终在那个世界中脱颖而出。令人难以置信的是，你如何将一个代理引入一个充满敌意和完全未知的环境，在那里，代理只知道它可以执行哪些可用的操作，但仅此而已，并通过反复试验发现随着每次连续移动而改进的策略，达到超人的性能。</p><blockquote class="lk ll lm"><p id="3fc9" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">自出生以来，我们不都是探索我们世界的代理人吗？难道婴儿不是在出生时完全无助的代理人，对他们的周围环境一无所知，并通过使用他们的大脑，处理记忆和经验来增加他们在世界上的成功(奖励)，并逐步掌握生活中的特定任务吗？</p></blockquote><p id="cd74" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">本文将试图给出一些基本的理解(<em class="ln">省略了数学</em>)，这样你就可以开始你的强化学习科学之旅，探索这个奇妙的不断发展的科学领域。</p></div><div class="ab cl lr ls gp lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="hb hc hd he hf"><h1 id="fe09" class="jj jk hi bd jl jm ly jo jp jq lz js jt io ma ip jv ir mb is jx iu mc iv jz ka bi translated">强化学习的生物学类比</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es md"><img src="../Images/fb3ac4ea224becfd0a5d0aafb4624346.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n4ZjmPxyS2vZiBLriOqIIw.jpeg"/></div></div></figure><p id="0d32" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">强化学习是基于试错法，通过评估不同的行动，从那些我们已经认为工作良好。</p><p id="eecf" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">随着时间的推移，在几次尝试产生对主体的状态/动作策略有益的进化变异失败后，我们可能会发现一个对主体在环境中的表现有益的变异。</p><blockquote class="lk ll lm"><p id="29cc" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">在生物学中，“这种政策变异”被称为“突变”。</p></blockquote><p id="f89e" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">地球上的生命是一个活生生的证据，证明了当你有数十亿年的时间与你的基因玩耍时，你可以通过突变实现什么。</p><p id="0f8b" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">大多数突变是有害的(甚至是致命的)，但有时一些突变可能会变成有益的，并导致物种的改进/进化。</p><p id="779c" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">正是由于这样的突变；我们的DNA形成了新的分支，将我们与先前的祖先物种区分开来。</p><blockquote class="me"><p id="a50e" class="mf mg hi bd mh mi mj mk ml mm mn kw dx translated">我们基因组中偶尔的“探索”( DNA突变)是物种进化的基石</p></blockquote><p id="fa7b" class="pw-post-body-paragraph kb kc hi kd b ke mo ij kg kh mp im kj kk mq km kn ko mr kq kr ks ms ku kv kw hb bi translated"><em class="ln">类似地，在RL中,“进化类比”是代理对每种情况(状态)采取的行动选择的改进。</em></p><blockquote class="lk ll lm"><p id="cdeb" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">我们，连同所有现在和过去的有机体，只不过是第一个有机体的进化变异分支。我们共同的祖先……一个古老的原核细胞。</p><p id="028d" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">只有1.5%的基因让我们成为人类！</p><p id="63e1" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">我们与黑猩猩有98.5%的DNA相同，与狗有3/4的DNA相同，与果蝇有1/2的DNA相同，与水仙花有1/3的DNA相同，然而我们在每个方面都是如此不同！</p></blockquote><blockquote class="me"><p id="f706" class="mf mg hi bd mh mi mt mu mv mw mx kw dx translated">如果在生物学中，DNA突变决定了基因表达的改变，同样地，在RL中，Q表突变决定了行为选择的改变。</p></blockquote><h1 id="aa59" class="jj jk hi bd jl jm jn jo jp jq jr js jt io my ip jv ir mz is jx iu na iv jz ka bi translated">强化学习—关键术语和概念</h1><p id="c5ac" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">假设你在某个外国城市醒来，你离开酒店，没人会说英语，你的手机没电了(所以你不能使用地图)，你很饿。你需要尽快去餐馆！</p><p id="22c8" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><strong class="kd hj">环境</strong>:所有城市街区都是一样的(网格世界)。环境就是你将要与之互动的东西。</p><blockquote class="lk ll lm"><p id="abcf" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated"><strong class="kd hj"> RL简而言之:</strong>你将从环境的某个随机状态开始<em class="hi">(在我们网格世界中的随机区块)</em>，对环境的状态执行动作<em class="hi">(走到网格中的某个新区块)</em>， 环境将以一个新的状态<em class="hi">(你在网格上结束的一个新的块)</em>和一些奖励来作出响应，这些奖励表明你的动作<em class="hi">(移动到这个新的块)</em>对于前一个状态<em class="hi">(你决定采取这个特定方向的前一个块)</em>是多么有益。</p></blockquote><p id="50dc" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><strong class="kd hj">动作</strong>:在这个网格世界中，你可以执行的整套动作(<strong class="kd hj">动作集</strong>)是移动[ <em class="ln">北</em>、<em class="ln">南</em>、<em class="ln">东</em>、<em class="ln">西</em> ]。最初，你没有理由偏爱任何一种行为。你不知道对于你现有的状态(<em class="ln">你所站的街区</em>)哪一个动作(<em class="ln">比如向东还是向西</em>)能让你更接近你的目标(<em class="ln">餐厅</em>)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/c458a5bf8b2462b4e2b5b37688708899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*StGhNc3Zz5i1Joj3v5f0Pg.jpeg"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">你生活在一个未知的世界。您只能执行可用的操作。是探索的时候了！</figcaption></figure><p id="38e1" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">所以你不可避免地开始探索。你选择随机行动(<strong class="kd hj">等概率策略)</strong>在城市街区闲逛，直到你在某个餐馆结束。</p><p id="ffb0" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><strong class="kd hj">终点站状态</strong>:所以逛了几个街区，最后你到了一家餐厅。当你访问这样一个<strong class="kd hj">终端状态</strong>时<strong class="kd hj">剧集</strong>就终止了，你现在可以坐下来评估你如何能改进你的策略(<strong class="kd hj">政策</strong>)，以便在未来的剧集中你能更有效地实现你的目标。</p><p id="073a" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><strong class="kd hj">插曲</strong>:一个插曲，就是你每次从投胎到那个世界的那一刻，到你到达一个终结状态的旅程。你可能需要成千上万集才能真正理解世界动态，这样你才能逐步改进你的策略(政策)，直到你想出最好的一个。</p><p id="9aec" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><strong class="kd hj">奖励</strong>:在格子世界中游荡时，每行走一分钟，你获得-1的负奖励。走正常的路障给了你负1的奖励，而拥挤的路障给了你负3的奖励，因为要花更长的时间通过它们。到达一家餐馆会给你+10的正奖励。</p><p id="7238" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">你的<strong class="kd hj">累积奖励</strong>将每集所有正面和负面奖励汇总在一起。<em class="ln">消极奖励可以被认为是“惩罚”。</em></p><blockquote class="lk ll lm"><p id="31f0" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">非终点状态下的负回报是必要的，因为如果你最后只得到正回报，你的代理无论在城市中徘徊多久都会得到相同的总回报，所以它永远不会真正学习。</p></blockquote><p id="7d82" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">因此，只要我们还没有“到达终点状态”，就通过获得负奖励，我们激励代理人找到最快的出路，从而找到最小化惩罚的方法，以增加整体最终累积奖励。</p><blockquote class="me"><p id="32c7" class="mf mg hi bd mh mi mj mk ml mm mn kw dx translated">因此，增加累积奖励应该转化为解决强化学习问题！</p></blockquote><blockquote class="lk ll lm"><p id="c2f3" class="kb kc ln kd b ke mo ij kg kh mp im kj lo mq km kn lp mr kq kr lq ms ku kv kw hb bi translated">通过这种逻辑，我们假设通过增加总奖励导致RL的解，我们称之为“<strong class="kd hj">奖励假设。</strong>”</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nc"><img src="../Images/6ea241d1b2593b441e263533a28d5699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i08wgyHZz3YRgnvscuKotw.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">探索未知的世界</figcaption></figure><p id="60d5" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><strong class="kd hj">状态</strong>:每次你移动到一个新的街区，你都在探索环境(世界)允许你看到的东西。大多数时候，对世界的“看法”是相当有限的。通常，你无法知道十个街区后会发生什么，所以状态是环境允许你“看到”的信息，这样你就可以估计你的下一步行动应该是什么。</p><blockquote class="lk ll lm"><p id="9fac" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">状态可以被认为是你在给定的世界位置对环境的观察。但是状态不仅是关于世界的信息，也是关于你自己的信息(你也是世界的一部分)。</p><p id="297e" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">例如，如果你正在“学习如何走路”，状态可以是你在世界上的位置，地面信息，海拔等。但是状态也包括你的关节的方向，你的速度，等等。</p><p id="c660" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated"><strong class="kd hj">所以状态是你的代理在给定时间的“感官”。</strong></p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/036879e468423db3cb6bd5ecc4bab3f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gvdLl0YD4CJ-eoHZDQwvFg.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">状态是环境允许你感知的“视图”</figcaption></figure><p id="314f" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><strong class="kd hj"> Q-Table </strong>:为了从这些经验中学习，我们需要跟踪一些<em class="ln">日志</em>关于每个状态的每个行为有多少回报(或没有回报)。</p><blockquote class="lk ll lm"><p id="aa0b" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">Q表可以被视为一个二维矩阵，其中行反映状态，列反映动作。该矩阵中的分数代表每个行动对所有可能状态的目标的积极或消极影响。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ne"><img src="../Images/d8eb8cd60e2b5b5f52739519a7a5275a.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*Lgpzkc639rO8QLiZkI_QBA.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">一个简单的Q表，其中两个可能的动作被分为4个可能的状态</figcaption></figure><p id="35e0" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">因此，下次当您处于相同的状态时，查看该日志可能会有所帮助，它可以作为一个指示器，指示哪些操作似乎最适合该状态。这样，你将建立你的策略(<strong class="kd hj"> policy </strong>)，该策略将指示为每个州选择什么样的最佳行动。</p><p id="4262" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">此日志包含在牌桌的每个状态下，您之前所有经历的预期总奖励的平均值。</p><p id="cd34" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><strong class="kd hj">我们应该从一开始就相信我们的Q表吗？</strong> <strong class="kd hj">没有</strong>！最初，你的Q表上的分数并不能反映好的选择！这是因为你从<strong class="kd hj">等概率策略</strong>开始(通过采取随机行动并将其表现记录到Q表中进行探索)，显然，这些奖励分数还不应该被认为是准确的。</p><p id="89c0" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">因此，最初我们应该只是通过选择随机行动来探索，并不断用结果填充Q表。情节越多，我们就越应该相信我们的Q表，逐步降低选择的随机性，并根据我们的状态更频繁地选择Q表行动建议(<strong class="kd hj">利用</strong>)。</p><p id="c865" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">然而，即使经历了成千上万集，我们仍然应该不时地采取一些随机的行动来继续探索，也许会发现一些可能对特定状态更有益的行动。</p><p id="026b" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">通过在每集后逐渐降低对我们状态的探索(但绝不归零)，我们的q表将逐步调整，让我们提出一个<strong class="kd hj">最优策略(π*) </strong>，在最短的可能时间内将我们带到餐馆。</p><p id="3d8e" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">这就是<strong class="kd hj">ε-贪婪政策</strong>。因此，epsilon-greedy-policy表示，在大多数情况下，我们将遵循我们的q-table建议，为每个州选择最高评级的行动，但总是允许有一小部分机会选择随机行动。在我们选择了这样一个随机的选择(而不是q表建议)之后，我们应该严格遵循所有剩余步骤的政策，以检查这种“偏离政策”是否有益。</p><p id="caa3" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">然后，我们在我们的Q表中记录由于这个随机状态/动作对的“迂回”而导致的改善或恶化，并继续进行可能…几千次！那么Q-Table将保存关于每个状态下每个动作做得有多好的更精确的分数！</p><p id="a678" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><em class="ln"/><strong class="kd hj"><em class="ln">贪婪策略</em> </strong> <em class="ln">总是盲目地遵循q表的指示，而</em><strong class="kd hj"><em class="ln">ε贪婪策略</em> </strong> <em class="ln">大多遵循q表，但允许不时地进行一些“随机选择”，以观察它如何影响代理的得分。</em></p><blockquote class="lk ll lm"><p id="89b1" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">这意味着<strong class="kd hj">贪婪策略</strong>(跟随q表而不探索)不是有益的，因为我们永远不会进一步探索(因此停止学习)，而<strong class="kd hj">ε贪婪策略</strong>将在大部分时间跟随我们的策略，但偶尔会选择随机行动来检查它是否有利于我们的代理实现其目标。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es md"><img src="../Images/5c3a327219315c3dcb46283f0e90373e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9yfIN4BjYNYazMJBKFe0Mg.jpeg"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">“贪婪政策”就像盲人摸象——你需要“ε贪婪政策”来允许一些探索！</figcaption></figure><blockquote class="me"><p id="2dc5" class="mf mg hi bd mh mi mt mu mv mw mx kw dx translated">ε贪婪策略不会盲目引导代理(像贪婪策略一样)。</p></blockquote><blockquote class="lk ll lm"><p id="f1cc" class="kb kc ln kd b ke mo ij kg kh mp im kj lo mq km kn lp mr kq kr lq ms ku kv kw hb bi translated">ε-Greedy-Policy允许在原始策略的基础上进行随机发现，引入增加累积奖励的新策略，并最终导致最优策略(π*)。</p></blockquote></div><div class="ab cl lr ls gp lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="hb hc hd he hf"><h1 id="d1d9" class="jj jk hi bd jl jm ly jo jp jq lz js jt io ma ip jv ir mb is jx iu mc iv jz ka bi translated">状态+行动→奖励+下一个状态(SARS)</h1><p id="6213" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated"><em class="ln">不要惊慌，这个SARS事件是我们一直在关注的，它更多的是一个回顾而不是一些新概念！</em></p><p id="09e3" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">正如我们到目前为止所看到的，环境提供给我们的投射，(<em class="ln">包括我们的力学——就像我们的关节或速度</em>)是我们感觉到的<strong class="kd hj">状态</strong>。</p><p id="aff7" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">对于每个状态，我们说有一组可用的<strong class="kd hj">动作</strong>。</p><p id="8930" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">通过在给定的状态下执行一些动作，我们前进到下一个状态，同时我们得到一些积极或消极的奖励，因此我们可以评估我们的动作在前一个状态下是否有益。</p><p id="462d" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">这是代理在环境中前进的“循环”,直到它达到终止状态。</p><ol class=""><li id="ec5c" class="nf ng hi kd b ke lf kh lg kk nh ko ni ks nj kw nk nl nm nn bi translated">S1+A1 →R1+S2</li><li id="2fd1" class="nf ng hi kd b ke no kh np kk nq ko nr ks ns kw nk nl nm nn bi translated">S2+A2 →R2+S3</li><li id="9efc" class="nf ng hi kd b ke no kh np kk nq ko nr ks ns kw nk nl nm nn bi translated">S3+A3 →R3+S4</li><li id="74c5" class="nf ng hi kd b ke no kh np kk nq ko nr ks ns kw nk nl nm nn bi translated">…一直到St(终端状态)</li></ol><p id="12d3" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">每一集所有奖励(R1+R2+R3…+Rt)的总和就是那一集的<strong class="kd hj">累积奖励</strong>，我们希望它越高越好。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nt"><img src="../Images/085a91650f066c244dec02724b8a799c.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*sHaQ307wLk0BDaztufS9JA.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">基于当前状态，代理执行“动作”——环境返回“下一状态”和“回报”</figcaption></figure><blockquote class="lk ll lm"><p id="c8e6" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated"><strong class="kd hj">状态/行动/奖励</strong>:所以如果我们学习走路，我们的左脚在前面(这是我们的状态)，我们的行动也是将右脚向前移动(我们的行动)，那么我们最终会摔倒(我们的新状态)，我们会得到一个负面的奖励，以便理解我们将双脚向前移动的行为会对我们学习如何走路的努力产生负面影响。</p><p id="2b61" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">因此，对于给定状态(其中左脚在前面)这个经历<strong class="kd hj">即使是负面的，也提供了一些有价值的(不要做)信息。</strong></p><p id="600a" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">因此，让我们在Q表中写下这一状态/动作对的最差分数！</p></blockquote></div><div class="ab cl lr ls gp lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="hb hc hd he hf"><h1 id="1496" class="jj jk hi bd jl jm ly jo jp jq lz js jt io ma ip jv ir mb is jx iu mc iv jz ka bi translated">间断的还是连续的？</h1><p id="fcc6" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">到目前为止，我们看到的是一个<strong class="kd hj">阶段性任务</strong>。这样的任务有一个明确的结束状态(在我们的例子中，结束状态是到达一家餐馆——相当于走出一个迷宫)。</p><p id="0980" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">最终状态(结束状态)可能并不总是意味着好的事情(比如达到我们的目标)。在自动驾驶汽车的类比中，我们可以通过到达目的地(正最终状态奖励)或崩溃(负最终状态奖励)来终止我们的模拟。在一盘棋中，我们可以以赢或输等方式结束。</p><p id="0a73" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">然而，有些任务可能没有终点，但会永远继续下去。这些被称为<strong class="kd hj">连续任务</strong>。</p><h1 id="4def" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">贴现率γ</h1><p id="38fb" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">现在，我们说，当我们填充Q表时，我们在表中的每个状态/行动单元写下，如果我们采取特定行动，然后从那时起坚持我们的政策，我们在该状态下的估计总回报是多少。</p><p id="1ca9" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">这个“总估计回报”是通过考虑所有(预期的)未来移动回报(在过去的生活中，在那个状态下采取相同的行动并记录所有回报直到某个终端状态后，我们已经总结出的<em class="ln">的平均值)来计算的。</em></p><p id="617f" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">但是，我们现在做的一个动作对于500个状态/动作来说应该是决定性的吗？</p><blockquote class="lk ll lm"><p id="206a" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated"><em class="hi">折现率γ的存在是为了让更近的奖励比更远的(未来)奖励对我们的即时反应更有意义。</em></p><p id="54ca" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">在决定采取行动时，将你的注意力主要放在眼前结果的重要性上，而不是放在长远的结果上，这是很有意义的。T9】</p><p id="f8fc" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated"><strong class="kd hj">例子</strong>:如果你学习如何走路，更重要的是尽最大努力采取行动，这样你现在就不会摔倒，而不是考虑到50米外的路会很滑而采取不同的行动。是的，因为你知道在不久的将来道路会很滑，你的政策应该包括“减速”，但现在最重要的是你要避开人行道上的裂缝，这样你才能保持平衡，不会马上摔倒。</p></blockquote><blockquote class="me"><p id="6c1d" class="mf mg hi bd mh mi mt mu mv mw mx kw dx translated">所以贴现率γ使得我们对未来看得越多，未来的回报就越不重要(所以我们把大部分注意力放在当前的情况上)</p></blockquote><p id="1ef3" class="pw-post-body-paragraph kb kc hi kd b ke mo ij kg kh mp im kj kk mq km kn ko mr kq kr ks ms ku kv kw hb bi translated">然而，一些简单的偶发任务，如到达餐馆，不需要应用贴现率γ。</p><p id="bb43" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><strong class="kd hj">那么这个折现率γ到底适用于哪里呢？</strong></p><p id="a29b" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">贴现率γ(γ)是一个你总是应用于连续任务的东西，但不一定应用于间断任务。</p><p id="4fed" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">众所周知，通过q表，我们记录了每个状态下所有行为的预期总回报。</p><p id="1cb0" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">这意味着，对于大多数偶发任务来说，只需平均该状态/行动的总回报，并将其记录到我们的q表中。但是要记录从那个状态开始的总报酬，一直到结束，这就自动暗示我们已经到达了某个最终状态St，否则就没有定义的“总报酬”。</p><p id="7579" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">但是在连续的任务(永远不会结束的任务)中，如果未来是无限的，我们就不能保持这样的日志(因此，如果没有“结束”，就没有“总回报”)。现在贴现率γ将被用作<em class="ln">衰减因子</em>，通过它我们考虑越来越少的未来预期回报，直到它们消失。我们利用这一点来“看到”未来的某个时刻，并能够处理连续的任务，而不是看到一个无限的未来，从而从一开始就无限地冻结我们的代理。</p><blockquote class="lk ll lm"><p id="6aa6" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">正如我们所说，在情节任务中使用折现率γ是可能的，但很多时候我们可以不使用它，特别是对于不涉及许多重合状态的简单任务。</p></blockquote><h1 id="c29e" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">马尔可夫决策过程(MDP)</h1><p id="a44d" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">这些状态、行动、奖励、它们的机制(被称为<strong class="kd hj">【一步动力学】</strong>)连同贴现率(γ)一起定义了一个<a class="ae kx" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank"><strong class="kd hj">【MDP】</strong></a><strong class="kd hj">【马尔可夫决策过程】。</strong></p><p id="b120" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">仅向代理提供可用动作的集合和折扣率γ。所有剩下的MDP部分都由代理商通过探索来发现。因此，一个好的q表创建将提供一个好的估计，让代理更接近于理解潜在的(未知的)MDP是什么。</p><blockquote class="lk ll lm"><p id="7c14" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">一个<strong class="kd hj">有限MDP </strong>是有限状态自动机(或<a class="ae kx" href="https://en.wikipedia.org/wiki/Finite-state_machine" rel="noopener ugc nofollow" target="_blank">有限状态机</a>)的概率(随机)等价物——如果你来自计算机科学领域的话。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nu"><img src="../Images/2db3d74e4775c910e0e0cb320c8088ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDyJJsM8JzcK0GTcTeFDEQ.jpeg"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">回收机器人的MDP图(*来源:Udacity)</figcaption></figure><p id="8e5c" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">在上面的例子中，我们看到了回收机器人的<strong class="kd hj"> MDP图</strong>。这里给出了两种可能的电池状态[高、低]。我们还被赋予了一组动作(搜索、等待、充电)。在这个“随机”模型中，我们还可以看到一个行为在什么情况下比另一个行为更受青睐，以及有什么样的回报。</p><p id="4000" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">例如，通过上面的MDP图，在高状态下，我们看到搜索有70% (.7)的机会将电池保持在相同的高状态，并提供4的奖励，而它有30% (.3)的机会将机器人的电池状态改为低，奖励为4。</p><blockquote class="lk ll lm"><p id="635f" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated"><strong class="kd hj"> <em class="hi">关于MDP</em></strong><em class="hi">的一个非常重要的观察是，你对一个状态的现有动作选择并不依赖于先前的动作/状态！</em></p></blockquote><p id="8c03" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">看上面的MDP！为您的状态“高”选择操作“搜索”，不受任何先前状态/操作的影响！它只取决于单步动态，给你一个机会，你会选择这个行动，而不是其他的！</p></div><div class="ab cl lr ls gp lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="hb hc hd he hf"><h1 id="58f7" class="jj jk hi bd jl jm ly jo jp jq lz js jt io ma ip jv ir mb is jx iu mc iv jz ka bi translated">深度强化学习(DRL)</h1><p id="f3b7" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">就这些吗？肯定不是！我们仅仅触及了强化学习的一些基本概念(<em class="ln">有意回避数学</em>)。在我们开始谈论深度强化学习之前，还有很多东西要看(<em class="ln">包括数学</em>)。</p><p id="6040" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">然而，<strong class="kd hj">许多人应该想知道神经网络在RL </strong>中的位置。我的意思是，我们可以用状态/动作日志填充q表，并使用它来根据我们现有的状态选择动作！那么，为什么要首先通过神经网络来增加额外的复杂性呢？</p><p id="a0fd" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">嗯，通常任务要复杂得多。这将意味着一个不切实际的大q表，或者我们的问题根本不能用一个经典的q表来表达！</p><p id="aa89" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">想象你打网球。球不能只来到你的右边或左边，所以它不能用简单的[左，右]状态来表示！它可以出现在你左右之间的任何地方，从上到下。这意味着我们处理可以是任何浮点值的实数(实数有无限的小数空间)来表示每个状态，将此转换为“无限值q表”(<em class="ln">，它根本无法定义</em>)。所以在连续的空间里，事物会分崩离析！</p><p id="44a2" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">一个解决办法是<strong class="kd hj">离散化</strong>我们连续的空间世界，并创建虚拟的正方形，以便将这个问题转化为可以用q表处理的网格世界问题。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nv"><img src="../Images/65e939b95c84b31a4ec0f639d9e159b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/1*K9kcrFcphlmXGl0cO3G6Ag.gif"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">离散化:将连续数据转换成网格数据，这样我们就可以通过q表来处理问题</figcaption></figure><p id="bb3e" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">然而，这并不总是可行的，正如我们之前提到的，即使我们执行离散化，这也可能产生不切实际的大q表，其需要同样不切实际的大量情节，以便有效地填充每个动作/状态对的日志条目。</p><p id="905a" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">因此，在这里我们可以使用<strong class="kd hj">函数逼近</strong>来处理这样的问题，它可以将状态对应到动作。</p><p id="ff0e" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><strong class="kd hj">深度神经网络</strong>定义为<strong class="kd hj">通用函数逼近器</strong>。</p><p id="ad1c" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">为了看到大图(<em class="ln">如果你来自深度学习背景</em>)——借助神经网络(NN)——在图像分类问题中，我们将<strong class="kd hj">像素转化为标签</strong>。</p><p id="8d94" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">嗯，在强化学习的情况下，想法是把<strong class="kd hj">状态变成</strong>动作。</p><pre class="iy iz ja jb fd nw nx ny nz aw oa bi"><span id="68e9" class="ob jk hi nx b fi oc od l oe of"><strong class="nx hj">In image classification problems... </strong><br/>Pixels -&gt; [NN] -&gt; Probabilities over Labels</span><span id="be62" class="ob jk hi nx b fi og od l oe of"><strong class="nx hj">In regression problems like stock exchange prediction</strong><br/>data -&gt; [NN] -&gt; Prediction of tomorrow's stock value</span><span id="580c" class="ob jk hi nx b fi og od l oe of"><strong class="nx hj">In reinforcement learning... </strong><br/>Environment's State -&gt; [NN] -&gt; Action Values</span></pre><p id="356a" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">所以主要的想法是创建一个在无限实值状态集上近似q表的东西，而不是一个离散的状态集。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es oh"><img src="../Images/53b5869b3794f0a29019f24724b20fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_TNa54fr_LsLOllgIsrcw.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">在神经网络中，我们有:“输入=状态”和“输出=动作”</figcaption></figure><p id="562e" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">因为我们的神经网络权重(θ)是随机初始化的，所以它们可以被认为是等概率策略的一个实例。我们的政策反映了这种θ权重配置，所以我们将它称为π(θ)。因此，通过将θ权重改变为某个新的配置θ`,新策略将被称为π(θ` ),以此类推。</p><blockquote class="lk ll lm"><p id="0096" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">在深度学习中，<strong class="kd hj">分类</strong>回答“是什么东西”，而<strong class="kd hj">回归</strong>回答“多少”是什么东西。</p><p id="4e10" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">我们还说在我们的神经网络中“状态”进去(输入)，动作出来(输出)。</p></blockquote><p id="0238" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">我们可以将DQN视为一个<strong class="kd hj">回归模型</strong>，它将一个状态作为输入，并尝试为每个可能的动作输出近似奖励分数(很像经典的q表)。<em class="ln">这就是为什么很有可能使用误差函数，例如我们在回归网络中常用的</em> <strong class="kd hj"> <em class="ln"> MSE(均方误差)</em></strong><em class="ln">——如果你使用过NN，你就会知道我在说什么。</em></p><p id="b11d" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">但是，即使我们使用回归技术来训练这个神经网络，<strong class="kd hj">因为输出表示离散的动作得分</strong>，并且我们喜欢具有最高得分的动作，<strong class="kd hj">结果最终分类为</strong>。(很像从给定状态的q表中选择得分最高的动作)。</p><p id="8fca" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated"><em class="ln">所有这些澄清的原因是，人们有时想知道为什么处理这样的分类问题要使用MSE函数。</em> <strong class="kd hj"> <em class="ln">现在你知道你并没有真正分类</em> </strong> <em class="ln">，你只是把两步合二为一。获取一个状态的q表动作值，并选择最上面的动作(顺便说一句，不要忘记，有时您必须随机选择而不是最上面的动作来选择探索)。</em></p><blockquote class="lk ll lm"><p id="2751" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">重要的是要注意，当我们有一组离散的可能动作，并且我们根据最高动作得分来估计我们对单个动作比对其他动作的偏好程度时，就应用了所描述的对最受欢迎的动作进行评分的NN方法(因此我们针对每个状态的动作模拟类似于q表的得分)。</p><p id="1f77" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">因此，如果我们可以向北、向南、向东或向西移动，我们可能会有一个大小为4的softmax输出图层，我们可以看到4个可能的操作中哪一个得到最高分。</p><p id="561f" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">然而，如果我们的输出动作应该包括多个变化(例如，将10个机器人关节同时移动10个不同的幅度作为单个动作响应)，我们就不再处理动作概率，因此我们的输出层可以由10个双曲正切函数(例如)组成，为10个可用关节中的每一个提供变化幅度。</p></blockquote><h2 id="40c5" class="ob jk hi bd jl oi oj ok jp ol om on jt kk oo op jv ko oq or jx ks os ot jz ou bi translated">简而言之，DRL…</h2><p id="d3ae" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">在深度强化学习中，你用神经网络来表示Q表。你试图通过改变神经网络的权重(梯度下降)来保持你对行动如何影响你的状态的世界的估计。</p><p id="5eb8" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">因此，通过函数逼近，我们试图预测一个给定状态的最佳拟合动作的概率，从而模拟一个q表。</p><blockquote class="lk ll lm"><p id="5d5d" class="kb kc ln kd b ke lf ij kg kh lg im kj lo lh km kn lp li kq kr lq lj ku kv kw hb bi translated">传统的离线分类训练(向您提供训练数据集)与在线训练的最大区别在于，在线训练是由您在探索的每一步中构建数据集，因为它会生成由数据集评估的结果。数据集的这种过渡创建会对神经网络造成明显的问题，并且可以用各种技术来处理。</p></blockquote><h1 id="c6be" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">基于价值与基于策略的方法</h1><p id="b3a6" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">正如本文的副标题所述，本文提供了一种基于价值的方法。</p><p id="179b" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">是的，到目前为止，我们一直在研究这种基于价值的方法，通过使用q表(直接或通过深度学习估计)来估计最优策略(π*)。</p><p id="902c" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">通过使用基于策略的方法直接改变神经网络的权重，并研究预期回报如何提高，也可以在没有状态/值函数的一些q表参考的情况下搜索这样的策略。</p><p id="90f7" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">然后用<strong class="kd hj">渐变上升</strong>(对，上升，不是错别字！)，通过将神经网络权重调整到直接似乎产生更好的估计总体回报的政策，是另一种方法。</p><p id="ba58" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">当然，基于策略的方法应该有自己的一篇文章，所以我们在这里不再深入探讨。</p><p id="d6ea" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">请记住，到目前为止，我们所看到的是理解基于价值的方法的非常早期的方法，它利用了q表。</p><p id="dd40" class="pw-post-body-paragraph kb kc hi kd b ke lf ij kg kh lg im kj kk lh km kn ko li kq kr ks lj ku kv kw hb bi translated">如果它只是烤你的面条，要知道一些疯狂的科学将基于价值和基于政策的方法结合成所谓的<strong class="kd hj">演员-评论家-方法</strong>。</p></div></div>    
</body>
</html>