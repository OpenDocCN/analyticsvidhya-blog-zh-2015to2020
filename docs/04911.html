<html>
<head>
<title>Story Telling for Linear Discriminant Analysis(LDA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性判别分析(LDA)的故事讲述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/story-telling-for-linear-discriminant-analysis-lda-a53261afc2a9?source=collection_archive---------12-----------------------#2020-04-05">https://medium.com/analytics-vidhya/story-telling-for-linear-discriminant-analysis-lda-a53261afc2a9?source=collection_archive---------12-----------------------#2020-04-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5e92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性判别分析是一种机器学习算法，在<strong class="ih hj">分类</strong>任务中被<strong class="ih hj">用作预处理</strong>步骤，以便我们能够<strong class="ih hj">减少数据集</strong>中与每个类别相关的特征数量。使用LDA的主要影响是它最大化了数据集中使用的每个类之间的距离</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/da873069f52cf299e888403e2f8f6b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*kQ6fR0HrbMzk57dZzoEfsg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jp">(PIC-1)</strong>→该图显示了如何使用<strong class="bd jp"> LDA </strong>将<strong class="bd jp"> q </strong> &amp; <strong class="bd jp"> △ </strong>类的二维数据点投影到<strong class="bd jp">蓝色1D线</strong>上，其中<strong class="bd jp">最大化q &amp; △类之间的间距</strong></figcaption></figure><p id="b3f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要注意的是，LDA类似于<a class="ae jq" rel="noopener" href="/analytics-vidhya/reduce-the-curse-of-n-n-to-our-choice-our-choice-71c0322405cd?source=friends_link&amp;sk=8601372811f93f7650db7502a6e0488e"> <strong class="ih hj"> PCA </strong> </a>(主成分分析)，但主要区别在于<strong class="ih hj"> PCA是一种无监督学习技术</strong>，而<strong class="ih hj"> LDA是</strong> <strong class="ih hj">监督学习技术</strong>。现在，我们将借助一个例子来理解LDA是如何工作的。这个例子很容易理解，但是有点长。所以只要耐心地把它读出来，几分钟后一切都会变得清晰明了。</p><h2 id="d027" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">LDA的工作:-</h2><p id="fcb3" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">我们将使用几个来自UCI机器学习知识库<a class="ae jq" href="https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> HABERMAN </strong> </a>数据集的行和列的例子。因此，在取出几个示例后，我们会收到这样一个数据集:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kq"><img src="../Images/54f2762c91ae3f31f4685894ff269042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*IrmBs4BHq76eaLN-4UFQLw.png"/></div></figure><p id="1299" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LDA的主要目标是从数据集的当前n维特征中获得新的特征集(<strong class="ih hj"> k </strong>),其中，<strong class="ih hj"> k ≤(n-1)。在我们从Haberman数据集使用的例子中，n的值是3，因为在数据集中有3个特征列。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kr"><img src="../Images/f3615f7d2b19d24003685dd03564f1ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FiTnoDpaJOTbByP8TYWnDw.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">表1</figcaption></figure><h2 id="2fef" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">步骤1: →首先我们将找到类内散布Matrix(S𝓌)和类间散布矩阵(Sᵦ)</h2><p id="16e5" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">从数据集我们得到推论:→</p><p id="1f1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从而Sᵦ = S₁ + S₂，其中<strong class="ih hj"> S₁ </strong> &amp; <strong class="ih hj"> S₂ </strong>是类C₁ &amp; C₂ 的<strong class="ih hj">协方差矩阵</strong></p><p id="44a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们需要得到每个类的每个特征的平均值。因此，</p><p id="69f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">μ</strong>₁=(31+31+33+62)/4 = 39.25</p><p id="6249" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> μ </strong> ₂ = (59 + 65 + 58 + 62)/4 =61</p><p id="bfc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> μ </strong> ₃ = (2 + 4+ 10 + 6)/4 = 5.5</p><p id="3e57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在S₁由公式给出=<strong class="ih hj">σₓϵ𝒸₁(x-μ)(x-μ)ᵀ</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kw"><img src="../Images/19966b1491d159d4fc2e4f4da20d1b3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*19iDeYSAW8E4BgoQa0cYcQ.jpeg"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kx"><img src="../Images/6e4d36c90b6fdc464ab22a707cf4e08d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5MguK2nPSVTJXPjBHar7w.jpeg"/></div></div></figure><p id="f9db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，我们将计算S₂</p><p id="95cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">μ</strong>₁=(38+41+42+62)/4 = 45.75</p><p id="8c66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">μ</strong>₂=(69+60+69+65)/4 = 65.75</p><p id="7208" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> μ </strong> ₃ = (21 + 23+ 1 + 19)/4 = 16</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es ky"><img src="../Images/668dfdc1325cb235e77741d1f71555cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lIf2mBxHYT7_CsabUglawQ.jpeg"/></div></div></figure><p id="4fea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，当我们计算这4个矩阵的每个位置的平均值时，我们得到:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kz"><img src="../Images/f8d0164ca70f450050d35e1bf47249c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kYsZi9Sd9AVvZceH1MRK3w.png"/></div></div></figure><p id="7c22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，类间散布矩阵(Sᵦ) = <strong class="ih hj"> S₁ + S₂ = </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es la"><img src="../Images/40dc9716052d8ef0bea1c8a6496ec008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ZLBrhGYxWSxbSpe7mivlA.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">Sᵦ显示了类间散布矩阵的值</figcaption></figure><p id="e07e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，Matrix(S𝓌)=在课堂上分散开来</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lb"><img src="../Images/1ac11c3dc3697e98e12e68749b121cb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ye0hWYDKjo5CD48Z2Wv__g.jpeg"/></div></div></figure><h2 id="85f1" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">步骤2: →现在我们将根据公式计算igen矢量&amp; igen值</h2><p id="ee36" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated"><strong class="ih hj">(s𝓌)⁻sᵦ*<em class="lc">w =</em>λ<em class="lc">w</em></strong><em class="lc"/>其中<em class="lc">、</em><strong class="ih hj"><em class="lc">w</em></strong><em class="lc"/>是投影向量&amp; <strong class="ih hj"> λ </strong>是Igen向量(标量值)。</p><p id="1c6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= |<strong class="ih hj">(s𝓌)⁻sᵦ-λ<em class="lc">I</em></strong>| = 0其中，<strong class="ih hj"> <em class="lc"> I </em> </strong>为单位矩阵。</p><p id="f3c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于这里的计算太多，所以我只是从这里给出计算的方向。</p><p id="2ba3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在给上面的方程赋值后，我们将得到一个三次方程，解这个方程我们将得到3个<strong class="ih hj"> λ，</strong>的值，其中我们必须选择<strong class="ih hj"> λ </strong>的<strong class="ih hj">最高</strong> <strong class="ih hj">值</strong>，它将输出到<strong class="ih hj">最优投影</strong>。</p><p id="e741" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">得到<strong class="ih hj"> λ </strong>的标量值后我们将<strong class="ih hj"> λ </strong>代入方程中，<strong class="ih hj">(s𝓌)⁻sᵦ*<em class="lc">w =</em>λ<em class="lc">w，</em> </strong> <em class="lc">得到</em> <strong class="ih hj"> <em class="lc"> W </em> </strong> <em class="lc">的值作为</em><strong class="ih hj"><em class="lc"/>3×1</strong>矩阵。</p><p id="84cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lc">或者，我们可以利用这个等式直接计算出</em> <strong class="ih hj"> λ </strong> <em class="lc">的值。</em></p><p id="0265" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注:→(s𝓌)⁻*(μ</strong>₁<strong class="ih hj">]⁻ᵀ</strong>-<strong class="ih hj">【μ</strong>₂<strong class="ih hj">]⁻ᵀ)，其中μ </strong> ₁ &amp; <strong class="ih hj"> μ </strong> ₂代表数据集中每个类的3个平均值，为1 &amp; 2，因此，<strong class="ih hj"> μ </strong> ₁ = [39.25，61，5.5] &amp;</p><h2 id="80a8" class="jr js hi bd jp jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">步骤3: →计算实际的1D矢量</h2><p id="df01" class="pw-post-body-paragraph if ig hi ih b ii kl ik il im km io ip iq kn is it iu ko iw ix iy kp ja jb jc hb bi translated">我们将通过简化<strong class="ih hj">(s𝓌)⁻*(μ</strong>₁<strong class="ih hj">]⁻ᵀ</strong>—<strong class="ih hj">【μ</strong>₂<strong class="ih hj">]⁻ᵀ)</strong>获得的矩阵值将是一个<strong class="ih hj">3×1</strong>矩阵，其转置将与C₁ &amp; C₂每个类的<strong class="ih hj">4×3</strong>特征矩阵的转置相乘【参见表1】。最后，我们将得到一个1×4的矩阵，用于C₁ &amp; C₂。</p><p id="0b06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们，假设C₁ = [x₁，x₂，x₃，x₄] &amp; C₂ = [y₁，y₂，y \，y \]，那么关于绘图(x \，y \)；(x₂,y₂)；(x₃,y₃)我们将在二维图上得到一条直线，代表表1 中给出的二维平面上C₁和C₂类的特征点。最后，我们可以使用任何分类算法(如(<strong class="ih hj"> PIC-1 </strong>)来画出区分C₁ &amp; C₂类别的特征点的判定边界。</p><p id="e8f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是LDA算法的工作原理，我希望你已经掌握了这个算法的工作原理。如果您对此有任何疑问，请在回复部分告诉我，直到他们喜欢学习为止:)</p></div></div>    
</body>
</html>