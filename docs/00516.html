<html>
<head>
<title>The Math Behind Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习背后的数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-math-behind-machine-learning-2e38d8718d17?source=collection_archive---------0-----------------------#2019-07-18">https://medium.com/analytics-vidhya/the-math-behind-machine-learning-2e38d8718d17?source=collection_archive---------0-----------------------#2019-07-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/234d06424a8625e1227fd0edbb6424b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A8uWQ-55WPeIyY16"/></div></div></figure><p id="5b33" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看看机器学习中的几种技术以及在这个过程中使用的数学主题。</p><h1 id="3678" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">1.线性回归</h1><p id="19b1" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在线性回归中，我们试图为一组给定的数据点找到最佳拟合线或超平面。我们使用一组参数作为权重，通过输入变量的线性组合来模拟线性函数的输出。</p><p id="2408" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过最小化残差平方和来找到参数。我们通过将残差平方和的导数的向量设置为零向量来找到临界点。通过二阶导数检验，如果残差平方和在临界点的Hessian是正定的，那么残差平方和在那里有一个局部最小值。</p><p id="9f81" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的过程中，我们使用了导数、二阶导数测试和Hessian，这些都是来自多变量微积分的概念。我们也可以使用线性代数找到最小化问题的解决方案。</p><p id="2727" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">设<strong class="is hj"> X </strong>是矩阵，其中行是我们的数据输入，从每行1开始，<strong class="is hj"> y </strong>是我们数据输出的向量。我们想要一个矢量<strong class="is hj"> β </strong>，使得<strong class="is hj"> Xβ </strong>接近于<strong class="is hj"> y </strong>。</p><p id="c5b2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">换句话说，我们想要一个向量β，使得Xβ和y之间的距离‖Xβ-y‖最小。使距离最小的向量β是Xβ是y在X的列空间上的投影。这是因为y在X的列空间上的投影是X的列空间中最接近y的向量。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/c93ef6b16e5a4dd456d39a41c373d088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a7_ljKXQqBpN-xsKMcLjSQ.jpeg"/></div></div></figure><p id="d4b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，我们利用欧几里德N-空间可以分成两个子空间的事实:</p><ol class=""><li id="5376" class="kw kx hi is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">X和的列间距</li><li id="a8d2" class="kw kx hi is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">X的列空间的正交补，</li></ol><p id="b431" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">事实是，欧几里得N-空间中的任何向量都可以分别唯一地写成X的列空间和X的列空间的正交补中的向量之和，从而推导出y-Xβ与X的列正交。</p><p id="8cb6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从这里，我们可以得出矩阵方程</p><blockquote class="lk ll lm"><p id="8076" class="iq ir ln is b it iu iv iw ix iy iz ja lo jc jd je lp jg jh ji lq jk jl jm jn hb bi translated">X^T Xβ=X^T y</p></blockquote><p id="6420" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果X^T X是正定的，那么X^T X的特征值都是正的。所以0不是X^T X的特征值。由此可见，X^T X是可逆的。然后，我们可以求解β的矩阵方程，结果与使用多元微积分得到的结果相同。</p><p id="332d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们刚刚讨论的解决方案中，使用了范数、投影、列空间、子空间、正交补、正交性、正定性、特征值和可逆性的概念。这些是来自线性代数的概念。</p><p id="044a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们还利用了欧几里德N-空间可以分解成两个子空间的事实，即X的列空间和X的列空间的正交补空间，并且欧几里德N-空间中的任何向量可以分别唯一地写成X的列空间和X的列空间的正交补空间中的向量之和。</p><h1 id="8204" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.分类</h1><p id="9152" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">让我们转向分类问题。在分类问题中，我们希望确定一个数据点属于哪个类。用于分类问题的方法之一是线性判别分析。</p><h2 id="4414" class="lr jp hi bd jq ls lt lu ju lv lw lx jy jb ly lz kc jf ma mb kg jj mc md kk me bi translated">2.1)线性判别分析</h2><p id="f549" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在线性判别分析中，我们估计<strong class="is hj"> Pr⁡(Y=k|X=x) </strong>，给定输入变量x为x，y为类k的概率，这叫做后验概率函数。一旦我们有了固定x的所有这些概率，我们选择概率Pr⁡(Y=k|X=x最大的类k。然后我们把x归为k类</p><p id="183f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用贝叶斯规则，我们可以根据π_k=pr⁡(y=k(Y=k的先验概率)和f _ k(x = x的概率，假定y = k)重写后验概率函数</p><p id="8612" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们假设X给定Y=k的条件分布是多元高斯分布N(μ_k，σ)，其中μ_k是类特定的均值向量，σ是X的协方差，所以f_k (x)可以用μ_k和σ来改写。</p><p id="baa4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们得到π_k、μ_k和σ的估计值，从而得到p_k (x)的估计值。我们根据估计的p_k (x)最大的类k对x进行分类。<br/>如果我们设置类k1的判别函数等于类k2的判别函数，我们得到一个分离类k1和k2的超平面。如果我们对每一对类都这样做，我们会得到一组将类彼此分开的分离超平面:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/169a1baba1de31828a60a22330582433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eNkI3ozsZARd_y6xXB7V_Q.jpeg"/></div></div></figure><p id="e290" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在线性判别分析中，我们使用后验概率函数、先验概率、贝叶斯规则、多元高斯分布、特定类别的均值向量和协方差，这些都是概率论中的概念。</p><h2 id="361c" class="lr jp hi bd jq ls lt lu ju lv lw lx jy jb ly lz kc jf ma mb kg jj mc md kk me bi translated">2.2)逻辑回归</h2><p id="3e8c" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">解决分类问题的另一种方法是逻辑回归。正如在线性判别分析中一样，我们想要估计Pr⁡(Y=k|X=x)并选择该概率最大的类k。我们直接估计概率，而不是像线性判别分析那样使用贝叶斯规则间接估计概率。</p><p id="9aa2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设只有0和1两个类，让p(x)=Pr⁡(Y=1|X=x).在逻辑回归中，我们假设对数比数是x分量的线性函数。假设对数比数是x分量的线性函数，参数为β_0，β_1，…，β_p，我们可以求解p(x)作为参数和x分量的函数。如果我们有参数β_0，β_1，…，β_p的估计值，我们就可以得到p(x)的估计值。</p><p id="f662" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们观测数据的概率是参数β_0，β_1，…，β_p的函数，称为似然函数。我们通过最大化似然函数来寻找参数的估计。最大化似然函数相当于最大化似然函数的对数。为了最大化对数似然函数，我们使用牛顿-拉夫森方法。</p><p id="b350" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对数似然函数L(β)是β=(β_0，β_1，…，β_p)的实值函数。所以l是从R^(p+1)到r的函数，而且，l是两次连续可微的。所以我们可以应用多元牛顿-拉夫森方法。</p><p id="d4fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在0和1两个类别的情况下，如果x的估计概率大于，我们将x分类为1类。设置(β_0) ̂+(β_1) ̂x_1+(β_2) ̂x_2=0给我们一个决策边界，对应的估计概率等于。下面是它可能的样子:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/7317913b9562893ddca7ca3fb4187cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HtRoQHO4hpy-u7x3ziZQZg.jpeg"/></div></div></figure><p id="c0d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">三角形属于0类，正方形属于1类；决策线以上的区域对应于类别0(其中估计概率小于)，决策线以下的区域对应于类别1(其中估计概率大于)。</p><p id="d2d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在逻辑回归中，我们使用似然函数，一个来自概率论的概念，和多元牛顿-拉夫森方法，一个来自多元微积分的概念。</p><p id="d615" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，我们将研究一种既能解决回归问题又能解决分类问题的方法。在人工神经网络中，我们使用线性和非线性函数的组合来模拟我们的输出函数。</p><h1 id="d46e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">3.神经网络</h1><p id="833a" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">输出函数可以用神经网络图来表示。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/691f4c2b78a271cf810d1e11ec17d6c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJmNnxiRflwldBVzqkiM-w.jpeg"/></div></div></figure><p id="cefd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">包括常数1在内的输入单元将形成输入层。我们采用输入单元的线性组合，包括常数1，然后对其应用激活函数h以获得新的单元。h是可微的(可能是非线性的)函数。我们这样做，比如说，M次；我们现在有M个隐藏单元，这些组成了一个隐藏层。看图，线性组合中的权重由连接两个单元的线段表示。我们可以继续这个过程，取前一层中单元的线性组合，并对每个线性组合应用激活函数，以创建新的隐藏单元，从而创建下一个隐藏层。在某一点上，我们有最后一层，称为输出层，我们为每个输出单元Y_k使用激活函数g _ k。g _ k是来自前一层的单元的所有线性组合的函数。</p><p id="ce18" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通常，激活函数h被选择为逻辑sigmoid函数或tanh函数。输出激活函数g_k将根据问题的类型而不同，无论是回归问题、二元分类问题还是多类分类问题。</p><p id="d8ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">到目前为止，我们已经构建了依赖于输入x的输出值Y_k，它包含一系列未知参数。我们现在的目标是使用我们的训练数据找到未知参数的值，使误差最小化。对于二元分类，我们通过最大化与我们的观察数据的概率相关联的似然函数来找到参数的估计；这相当于最小化所谓的交叉熵误差函数。类似地，对于多类分类，我们通过最大化与我们的观察数据的概率相关联的似然函数来找到参数的估计；这相当于最小化所谓的多类交叉熵误差函数。对于回归，我们通过最小化平方和误差函数找到参数的估计。</p><p id="0dde" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了最小化误差函数，我们使用梯度下降，这需要找到误差函数的梯度。为了找到误差函数的梯度，我们使用反向传播。</p><p id="1555" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在人工神经网络中，概率论中似然函数的概念用于分类问题。来自多变量微积分的梯度下降用于最小化误差函数。在反向传播过程中，使用多变量链规则。</p><h1 id="687e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">4.支持向量机</h1><p id="d8a5" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">让我们看看支持向量机解决分类问题的方法。这个想法是，我们有一堆数据点，比如说两个类，我们希望用一个决策边界来分隔它们。例如，数据点可能很容易被这样的线分开:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/f91949fd3cdf3014fff8b6fa57b0c139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x4_zQDddwic8UNWt2TW0qA.jpeg"/></div></div></figure><p id="b684" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果数据点可以很容易地用直线或超平面分离，我们找到离这些点尽可能远的分离超平面，这样就有很大的余量。这就需要最大化余量，最终成为一个凸优化问题。为了解决这个凸优化问题，我们使用拉格朗日乘子，一个来自多变量微积分的概念。一旦我们找到最大边缘超平面，我们可以根据点位于超平面的哪一侧来分类新点。这种分类点的方法被称为最大间隔分类器。</p><p id="e0ac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果数据点不能被超平面分离，我们仍然可以尝试找到一个超平面来分离大部分的点，但可能有一些点位于超平面的边缘内或错误的一侧。情况可能是这样的:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/b12e7fb62878a588835f0da152632f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uYUTv32g_3BSNrkB5D6Xbg.jpeg"/></div></div></figure><p id="ee17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">就像在最大间隔分类器的情况下，我们希望我们的超平面尽可能远离超平面正确一侧的每个点。因此，边缘上或边缘外但在超平面正确一侧的点将尽可能远离超平面。边缘内部但在超平面正确一侧的点将尽可能远离超平面，并尽可能靠近边缘边界。对于那些位于超平面错误一侧的点，我们希望这些点尽可能靠近超平面。</p><p id="41d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如在最大间隔分类器的情况下，我们想要最大化间隔，使得超平面的正确侧上的点离超平面尽可能远。</p><p id="3d56" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们不仅希望最大限度地提高利润，还希望最大限度地减少对利润的侵犯。这个问题变成了一个凸优化问题，并使用拉格朗日乘子来解决。</p><p id="1ac0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦我们找到分离超平面，称为软边超平面，我们可以根据点位于超平面的哪一侧来分类新点。这种对点进行分类的方法称为软边界分类器。</p><p id="3626" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果数据点不是线性可分的，并且分离两个类的决策边界似乎是非线性的，我们可以使用所谓的支持向量机，或支持向量机分类器。想法是考虑更大的特征空间，该更大的空间中的数据点与原始数据点相关联，并且将支持向量分类器应用于更大的特征空间中的该组新的数据点。这将在扩大的特征空间中给我们一个线性判定边界，但在原始特征空间中给我们一个非线性判定边界。通过将任何新点发送到更大的空间并使用线性判定边界来对其进行分类。以下是需要支持向量机的情况:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/90eb6471b5505d4f477d9fd66b75b8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jI7kAJrnmt1D6O6lv7P6rA.jpeg"/></div></div></figure><p id="3b7b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在解决软间隔分类器的凸优化问题的过程中，出现点积；在支持向量机的方法中，我们用一种叫做核的东西来代替点积。核本质上是一个函数，可以表示为输入值图像在某种变换h下的内积。这种用核代替点积的方法称为核技巧。</p><p id="95e9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">内核K应该是有效的内核；也就是说，应该存在对应于K的特征空间映射h。根据Mercer定理，K是对称半正定的就足够了。</p><p id="09c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在支持向量机方法中，扩大的特征空间可以是非常高维的，甚至是无限维的。通过直接使用内核，我们不必处理特征映射h或扩大的特征空间。</p><p id="62e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在支持向量机的方法中，我们看到使用了点积和对称半正定的概念；这些概念来自线性代数。为了解决凸优化问题，使用拉格朗日乘子；这个概念来自多变量微积分。</p><h1 id="6db4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结束注释</h1><p id="a52e" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在本文中，我们研究了机器学习技术背后的数学:线性回归、线性判别分析、逻辑回归、人工神经网络和支持向量机。</p><p id="c996" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">关于机器学习背后的数学的更多细节，请访问:<a class="ae mj" href="https://gum.co/VVZsI?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">机器学习的数学书籍</a></p></div></div>    
</body>
</html>