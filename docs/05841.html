<html>
<head>
<title>Spark’s Role In Big Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark在大数据中的角色</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sparks-role-in-big-data-9db392228638?source=collection_archive---------34-----------------------#2020-05-03">https://medium.com/analytics-vidhya/sparks-role-in-big-data-9db392228638?source=collection_archive---------34-----------------------#2020-05-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/552a85c0ab8e84e10030afea80f999c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MPM1WhO91Ssi2MbS.png"/></div></div></figure><p id="d0a6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">今天，大数据是一个热门话题。软件应用程序每天产生如此多的数据，与二十年前产生的数据量不可同日而语。此外，这些数据的传输速度很快，可以在几秒钟内接收数十亿字节的数据。大企业面临的共同问题是有效利用这些数据提高利润。</p><p id="c9ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文将描述典型问题如何变成大数据问题，以及Spark如何支持它。</p><p id="eb40" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您想到的第一个问题是，我们如何识别大数据问题。为此，我们有著名的<strong class="is hj">四V </strong>定义。任何至少具有以下属性之一的问题都将是大数据问题。</p><p id="dc47" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">卷</strong></p><p id="db64" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">速度</strong></p><p id="0ccd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">品种</strong></p><p id="8eb0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">准确性</strong></p><p id="47d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在大数据世界中，容量意味着万亿字节或千兆字节的数据。谈到速度，我们谈论的是每秒数百万个数据点。以不同结构出现的数据被视为大数据的变体。最后，准确性意味着我们在大数据世界中获得的不正确或有偏见的数据。</p><p id="1350" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">与数量相关的大数据问题的一个例子是分析数Pb的销售数据，以获得决策所需的洞察力。这么多数据无法加载到一台机器上进行分析。为了解决这个问题，分布式文件系统应运而生。</p><p id="c363" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">速度相关问题的一个例子是分析实时Twitter feed。在这种情况下，我们需要每秒分析数百万条推文。为了解决这一问题，流处理应运而生。</p><p id="4914" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们考虑多样性如何成为一个大数据问题时，我们需要考虑为我们提供数据的来源。大数据的多样性意味着不同的数据源以不同的格式发送数据。不同来源的数据结构不同，因此很难从不同来源的数据中提取所需的字段。</p><p id="55fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，大数据中的准确性意味着数据中的偏差和错误。例如，我们来看一个twitter feed。twitter feed中包含的信息有时可能不正确，有时对于特定社区可能是正确的。所以很难鉴别什么是真什么是假。</p><p id="e4d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们根据上面提到的<strong class="is hj">四个</strong> <strong class="is hj"> V的</strong>有了一个典型问题如何变成大数据问题的想法。因此，让我们来看看什么是Spark及其在大数据中的作用。</p><p id="298f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Spark </strong>是<strong class="is hj"> Apache </strong>软件基金会下的开源项目。它是一个运行在集群之上的计算系统。虽然Spark是用<strong class="is hj"> Scala </strong>编写的，但它支持像<strong class="is hj"> Java </strong>、<strong class="is hj"> Scala </strong>、<strong class="is hj"> Python </strong>和<strong class="is hj"> R </strong>这样的语言。Spark使用RDDs(弹性分布式数据集)作为底层数据结构。我们可以对这些RDD进行转换。例如，假设我们需要增加RDD中的一个字段并获得结果。然后Spark将创建一个新的RDD，其值递增。与此同时，Spark将使用下划线集群来满足任何计算要求，从而从旧的RDD创建新的。</p><p id="c8b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">与传统的分布式计算方法(如map-reduce)相比，Spark的速度更快，因为它具有内存处理行为。说到部署Spark作业，我们有几个选项，如<strong class="is hj">独立部署模式</strong>、<strong class="is hj"> Apache Mesos </strong>、<strong class="is hj"> Hadoop YARN </strong>和<strong class="is hj"> Kubernetes </strong>。在这些选项中，最常见的选项是在yarn集群上部署spark作业。典型的纱线簇将至少具有一个主节点和一个从节点。当我们在客户机模式下将一个Spark作业部署到一个yarn集群中时，它将类似于独立模式。该作业将在主节点上运行并执行计算。另一方面，如果我们在集群模式下将Spark作业部署到yarn集群中，主节点将执行协调作业，并使用从节点来执行计算。需要记住的一点是，一些在客户机模式下运行良好的作业有时在集群模式下无法运行，因为集群模式意味着我们使用分布式计算。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jo"><img src="../Images/a86b1a376a7c1b1ed03d67196f95d001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*yBJgn6VuyNVN-sHN.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">来源:<a class="ae jx" href="https://spark.apache.org/docs/latest/img/cluster-overview.png" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/latest/img/cluster-overview . png</a></figcaption></figure><p id="53ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">借助Spark，我们获得了解决大数据问题所必需的整套功能。在处理大型数据集时，<strong class="is hj"> SparkSQL </strong>工具将为开发人员提供传统的<strong class="is hj"> MySQL </strong>风格。<strong class="is hj"> MLlib </strong>机器学习库提供在Spark应用内部使用的机器学习算法。除此之外，Spark还提供了<strong class="is hj">数据集</strong>，这是rdd之上的一个抽象。这类似于我们熟悉的<strong class="is hj"> Python </strong>和<strong class="is hj"> R </strong>中的数据帧。</p><p id="4d28" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Spark最重要的功能是批处理和流处理。这两项功能允许解决大数据中与容量和速度相关的问题。这里需要记住的一点是，Spark流不是实时流。在Spark streaming中，我们拥有的是批处理。Spark将一组小批量流水线化为流。因此，如果你的需求是实时流，最好使用<strong class="is hj"> Apache Storm </strong>或<strong class="is hj"> Apache Flink </strong>。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jy"><img src="../Images/a08e465b01c984f30683f8c0789d5160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8854mXVIHpqjjdu-.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">来源:https://spark.apache.org/docs/latest/img/streaming-flow.png<a class="ae jx" href="https://spark.apache.org/docs/latest/img/streaming-flow.png" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="67a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，我要说的是，Spark提供了一个适合解决大数据问题的完整生态系统。为什么我这样说，是火花为我们提供了几乎一切。看，我们有批处理，实时处理(实际上它不是实时的，但对大多数情况来说已经足够了。)和一个机器学习库。此外，不要忘记Spark有一个非常好的开源社区，在那里你可以随时获得支持。</p></div></div>    
</body>
</html>