<html>
<head>
<title>An Airflow Sub-Dag to Sync Data from On-Premise Hadoop Cluster to Google Cloud Storage</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">气流子Dag将数据从本地Hadoop集群同步到Google云存储</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-airflow-sub-dag-to-sync-data-from-on-premise-hadoop-cluster-to-google-cloud-storage-230783fcf80a?source=collection_archive---------6-----------------------#2020-02-03">https://medium.com/analytics-vidhya/an-airflow-sub-dag-to-sync-data-from-on-premise-hadoop-cluster-to-google-cloud-storage-230783fcf80a?source=collection_archive---------6-----------------------#2020-02-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9fe6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个博客也可以在<a class="ae jd" href="https://www.linkedin.com/pulse/airflow-sub-dag-sync-data-from-on-premise-hadoop-cluster-boning-zhang" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>找到。</p><p id="8ad3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在正在将数据从本地Hadoop集群迁移到谷歌云平台(GCP)。由于我们的团队中有许多数据管道，以及对上游数据的许多内部和外部依赖，我们无法避免在内部Hadoop集群和Google云存储(GCS)之间来回复制数据的情况。例如，我们有一个非常关键和时间敏感的管道，向搜索引擎提交投标，我们希望将整个管道迁移到GCP。但是，它有一些上游依赖项，还没有迁移到GCP。因此，一旦新填充的上游数据准备就绪，我们需要首先将它们定期从本地集群同步到GCS，这样我们的投标渠道就可以完全在GCP运行。还有许多其他使用案例，我们需要定期将数据从本地集群同步到GCP。这个博客为此引入了一个气流子图，它封装了细节，可以很容易地被其他团队成员使用。</p><p id="2575" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我首先提供一个使用这个子dag来加载数据以显示其接口的例子。整个例子可以在<a class="ae jd" href="https://github.com/BoningZhang/Learning_Airflow/blob/master/DAGS/sync_data_to_gcp/dag.py" rel="noopener ugc nofollow" target="_blank"> Github </a>中找到。</p><p id="f4c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本例中，我们将两个表从本地集群导出到GCP。它们在字典中有定义。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="f933" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的“hql_dict”是我们需要提供给sub-dag的查询，它们将在那里运行。稍后我会详细介绍它们。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="b515" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个“export_table_params”是我们将在“hql_dict”的查询中使用的jinja参数。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="60b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">export_table_dataproc_config是gcp的dataproc的信息:“dataproc_cluster”是我们将用于在GCP创建表和加载数据的dataproc的名称，“region”是dataproc的区域，“gcp_conn_id”是与GCP项目的气流连接，可以按照<a class="ae jd" href="https://airflow.apache.org/docs/stable/howto/connection/gcp.html" rel="noopener ugc nofollow" target="_blank">指令</a>进行设置。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="a06d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是为了创建任务，通过应用我们定义的子dag export_to_gcp_dag来同步这两个表。如您所见，这三个参数(export _ table _ dict[op _ name][" hql _ dict "]，export_table_params，export_table_dataproc_config)在子dag中传递。</p><p id="e08d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我向您展示这个子dag是如何实现的，以及它将如何将数据从本地数据集群同步到GCP。sub-dag的实现可以在<a class="ae jd" href="https://github.com/BoningZhang/Learning_Airflow/blob/master/common/operators/export_on_premise_to_gcp_dag.py" rel="noopener ugc nofollow" target="_blank"> Github </a>中找到。</p><p id="f7e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此子dag主要运行三个查询，“export_data_hql”在本地Hadoop集群中运行。它在位于Google云存储中的本地集群中创建一个表，并将原始表中的查询结果插入到这个新表中。完成此操作后，本地集群中的数据将加载到GCS中。请注意，这里我们使用GCP的服务帐户授权本地集群访问GCS。此外，我们在表名中使用{ { params . gen _ date _ str _ nodash(execution _ date)} }，这样我们可以为同一个表运行多个具有不同日期分区的实例。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="56ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二步，“add_dataproc_partition_hql”将在GCP的dataproc集群中运行，以在GCP创建目标表，并将GCS中的数据加载到该表中。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="2254" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之后，我们运行第三个查询“drop_tmp_table_hql”来删除本地Hadoop集群中的临时表。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="9ed4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果进行了配置，将调用bash命令为GCS中的分区生成stamp文件。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure></div></div>    
</body>
</html>