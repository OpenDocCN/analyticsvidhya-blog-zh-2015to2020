<html>
<head>
<title>Breast cancer detection using deep neural network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度神经网络的乳腺癌检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/breast-cancer-detection-using-deep-neural-network-6691a472d7a7?source=collection_archive---------17-----------------------#2020-07-29">https://medium.com/analytics-vidhya/breast-cancer-detection-using-deep-neural-network-6691a472d7a7?source=collection_archive---------17-----------------------#2020-07-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="c873" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">(U-Net，更快的R-CNN)</h1><p id="933d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">案例研究</p><h1 id="27fc" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="3692" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">乳腺癌可以通过使用两种类型的图像来检测</p><p id="fed9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">1.乳房x光图像2。组织学图像</p><p id="dbfa" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们可以将深度学习技术应用于这两种类型的图像，但后一种(即组织学图像)产生更高的准确性，因为图像包含更高的细胞细节，并且具有高分辨率。因此，在这种情况下，我们要进行组织学成像。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kg"><img src="../Images/926e9219c0782c005df57844d5442607.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*fKODMu49t_dYOQIX84I0jA.jpeg"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">苏木精和伊红染色的乳腺组织学图像。</figcaption></figure><h1 id="8479" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">我们的任务</h1><p id="00f8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们的任务是检测图像中的有丝分裂细胞。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ks"><img src="../Images/f2c0f1d23f3df530ff6eb3810a12ca2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*G0nm1Fik39UnbAKANCfhIA.jpeg"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">有丝分裂细胞(左)，非有丝分裂细胞(右)</figcaption></figure><p id="5f2f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">有丝分裂计数是乳腺癌诊断的关键指标。</p><p id="5e22" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">许多机器学习和深度学习技术，如SVM，决策树，CNN等已经得到应用。</p><p id="f551" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">但是由于各种计算和数据相关的问题，很少有层数更多的深度学习模型得到应用。</p><p id="5e88" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在本案例研究中，我们将使用U-Net进行图像分割，然后使用更快的R-CNN进行对象检测。</p><h1 id="ee64" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">资料组</h1><p id="706b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将使用医学研究和竞赛中使用的两个数据集。</p><ol class=""><li id="77ce" class="kt ku hi jf b jg kb jk kc jo kv js kw jw kx ka ky kz la lb bi translated">2012年ICPR的有丝分裂数据集。l <a class="ae lc" href="http://ludo17.free.fr/mitos_2012/index.html" rel="noopener ugc nofollow" target="_blank">墨水</a></li></ol><p id="1db6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">该数据集包含具有300个有丝分裂细胞50幅图像。病理学家还在每个选择的图像中手工注释有丝分裂。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ld"><img src="../Images/c8debec5eab13df48bf6f1190392aae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xboTFawGD8iRT5z3jnYAKw.jpeg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">(H&amp;E)乳房图像(左)，带注释的图像(右)</figcaption></figure><p id="2886" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">黄色区域是有丝分裂细胞。还为每个图像提供了具有包含有丝分裂细胞区域的每个像素的x和y坐标的csv文件。</p><p id="6e4f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">2.来自ICPR 2014的有丝分裂数据集</p><p id="9adb" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">该数据集包含大约1000幅图像。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es li"><img src="../Images/b1e7836e574161b3c433e6a72ab8b536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i7AoGpuBfQ6mxg7IxK362g.jpeg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">(H&amp;E)乳房图像(左)，标有有丝分裂中心的图像(右)</figcaption></figure><p id="88bc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里黄点代表有丝分裂细胞的中心。</p><p id="780e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">该数据集的问题在于，与第一数据集不同，不提供逐像素注释，仅提供有丝分裂细胞的中心。</p><h1 id="e5ac" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">我们的方法</h1><p id="63b2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们的任务是识别有丝分裂细胞并在其周围创建包围盒。这是一种对象检测，其中对象是有丝分裂细胞，而不是猫、狗或人。</p><p id="2f3c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">为了训练对象检测算法，我们需要带有注释的图像。具体来说，我们需要对象的边界框坐标，如{x，y，width，height}</p><p id="ae84" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">由于我们数据集都不包含这些值，所以我们不能直接训练对象检测器。</p><p id="64d6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">用于已经有像素级注释的数据集1 </strong>。为了获得边界框坐标，我们将首先使用每个图像的csv文件，使用包含有丝分裂细胞的区域的每个像素的x，y坐标，创建图像的游程编码(RLE)。</p><p id="21e7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">游程编码在这里可以被解释为图像的掩码。其中有丝分裂细胞区域的像素为白色，其余为黑色。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es lj"><img src="../Images/b7cc0d73510a68aad2b89c4ed6c45221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P47IbsPd61iQOXnhUvAYtQ.jpeg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">带注释的图像(左)，RLE图像(右)</figcaption></figure><p id="a0d4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在创建图像的RLE之后，我们可以使用正常的图像处理技术来找到边界框坐标。</p><p id="743c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对于数据集2，我们没有像素注释。因此，我们不能使用与数据集1相同的技术。我们必须找到一个替代的方法来找到边界框坐标。</p><p id="7d7c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们将训练一个图像分割神经网络，它将分割有丝分裂细胞的区域，并将输出图像的掩模或RLE。</p><p id="db84" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">为了训练一个图像分割网络，我们需要像数据集1一样的带有遮罩的图像。因此，我们将使用数据集1训练图像分割模型。使用这个模型，我们将得到数据集2的掩模或RLE图像。</p><p id="a2d0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在获得数据集2的图像的RLE后，我们可以获得与数据集1相同的边界框坐标。</p><p id="0111" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在获得两个数据集的图像的边界框坐标之后，我们将训练对象检测网络。</p><p id="7c88" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对于图像分割，我们将使用U-Net，对于对象检测，我们将使用更快的R-CNN。训练后，我们的模型输出的图像应该像这样</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lk"><img src="../Images/ec39bbfecf60af1c099763f2d8b59e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*og5nN-uFfi8G7eALvRfi1Q.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">(H&amp;E)边界框中有有丝分裂细胞的乳房图像</figcaption></figure><h1 id="fbbe" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">什么是图像分割？</h1><p id="a6c3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">图像分割</strong>是将数字图像分割成多个片段的过程。更准确地说，图像分割是给图像中的每个像素分配标签的过程，使得具有相同标签的像素共享某些特征。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ll"><img src="../Images/40bd3a470f14463dc2cd6bf2492a0e7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t9pBS_o13AiwxjsOUndlcA.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">输入图像(左)，分段图像(右)</figcaption></figure><p id="8979" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对于图像分割，我们将使用称为U-Net的全卷积网络(FCN)。</p><h1 id="f60f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">什么是物体检测？</h1><p id="1e0f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">对象检测</strong>是在数字图像和视频中检测某类语义对象的实例(如人、建筑物或汽车)。定位是使用边界框完成的，如图所示。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es lm"><img src="../Images/723504ec3de330c95fefcd15f37fb635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W2ZO_ix7rEGiaEBju6W0Yw.jpeg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">输入图像(左)，输出图像(右)</figcaption></figure><p id="639c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对于物体检测，我们将使用更快的R-CNN。</p><h1 id="dc7f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">完全理解卷积网络(FCN)</h1><p id="a16d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">通常在用于图像分类的简单卷积神经网络(CNN)中，我们有一系列卷积层，接着是最大池层，最后是输出分类结果的全连接层。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ln"><img src="../Images/cafafd910a2bfe302006bf0a825b235a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3uGpJno2vEfRyQ9_iRhRqQ.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">分类</figcaption></figure><p id="db43" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">但是在图像分割中，我们希望对输入图像中的所有像素进行分类。为了达到以下目的，我们将使用卷积层代替全连接层，然后对其进行上采样以输入图像尺寸。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ln"><img src="../Images/e1c47b6b969263695e6138e559a8896d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*avZX61zKGEq_PZ7XmQwU0g.png"/></div></div></figure><p id="6680" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这种类型的网络被称为全卷积网络(FCN)</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es lo"><img src="../Images/852548fd362437045257f4224a334309.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1SiaMZmWt5bbSTYkKmj8aA.png"/></div></div></figure><p id="f067" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">上采样</strong>层与<strong class="jf hj"> </strong>卷积层相反。卷积是使输出尺寸变小的过程，而上采样是使输出尺寸变大的过程。</p><p id="5ae6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">有各种方法来进行上采样，但我们对使用转置卷积的<strong class="jf hj">上采样感兴趣</strong></p><h1 id="ffc8" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">转置卷积</h1><p id="773e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">转置卷积用于使用一些可学习的参数将输入特征映射上采样为期望的输出特征映射。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es lp"><img src="../Images/81ac3f54bc7a010d23b922bcfed0ab86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPaAQpNBphSEjpe_e5FAjQ.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">转置卷积运算</figcaption></figure><p id="b2de" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在这个过程中，每个输入单元与内核中的每个单元相乘。对所有的输入单元格都进行这个过程。在输出端，我们添加所有产生的上采样特征。</p><h1 id="5aa2" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">了解优信网</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es lq"><img src="../Images/ae89b4bdd9a5e4db969ad6b5249e3cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rmo71TyPxvLLM4FnlH5MDg.jpeg"/></div></div></figure><p id="d0d6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">它被称为U-Net，因为如图所示，它有一个“U”形结构。如果我们将架构分为两个对称部分，那么左边的部分称为收缩路径或下采样层。每个下采样层包含两个卷积和一个max pool，这是一般的卷积过程。</p><p id="dfc6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">右边部分是扩展路径或上采样层。每个上采样层包含两个上采样层(此处为<strong class="jf hj">转置卷积)</strong>和max pooling。在上图中，我们有五个这样的下采样和上采样层。当输入图像大时，这种层的数量可以增加，反之亦然。</p><p id="4176" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">网络的瓶颈部分在收缩和扩展路径之间，这些路径由两个卷积层和一个转置卷积(上采样层)组成。</p><p id="4afa" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在下采样路径和上采样路径之间也存在跳跃连接。这些跳跃连接旨在向上采样时向全局信息提供局部信息。</p><p id="9463" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">最后得到与输入图像具有相同维数的输出分割图像。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">优信网代码</figcaption></figure><p id="f599" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">代码分为三部分:下采样，瓶颈，上采样。</p><p id="ca8e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们有六层下采样。每层有两个卷积层和一个最大池层。同样，每个层都保存在一个列表中，这样我们就可以在跳过连接中使用它</p><p id="03ef" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在包含两个卷积层和一个上采样层(这里是转置卷积)的瓶颈层中</p><p id="ea9f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">那么我们有六个上采样层。每层有两个卷积层和一个上采样层(这里是转置卷积)。还应用跳过连接，其中我们将当前层与保存在列表中的来自下采样侧的对应层连接。</p><h1 id="1940" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">目标检测算法的发展</h1><h2 id="8591" class="lt ig hi bd ih lu lv lw il lx ly lz ip jo ma mb it js mc md ix jw me mf jb mg bi translated">简要说明</h2><h2 id="5dae" class="lt ig hi bd ih lu lv lw il lx ly lz ip jo ma mb it js mc md ix jw me mf jb mg bi translated">使用CNN的目标检测</h2><p id="6994" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对于对象检测，我们可以使用一种简单的方法来训练CNN分类器，并向网络提供图像的裁剪区域。但是这种方法的问题是裁剪区域中的对象可能具有不同的纵横比和空间位置。</p><h2 id="3cf6" class="lt ig hi bd ih lu lv lw il lx ly lz ip jo ma mb it js mc md ix jw me mf jb mg bi translated">基于R-CNN的目标检测</h2><p id="00f4" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">为了克服前面的问题，我们使用<strong class="jf hj">选择性搜索</strong>从图像中提取这些盒子(这些盒子被称为区域)。选择性搜索从一幅图像中提取大约2000个区域，称为<strong class="jf hj">感兴趣区域(ROI)。然后我们使用</strong> CNN提取每个感兴趣区域的特征，并使用支持向量机将这些区域分成不同的类别。边界框回归(<em class="mh"> Bbox reg </em>)也被用于预测每个识别区域的边界框。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mi"><img src="../Images/5ffbd02a3dcdbfe5caaa2abcbd483d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*lEQvzF1xtEa8kKy-ym5yUA.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">美国有线电视新闻网的运作</figcaption></figure><p id="3c40" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们不使用裁剪过的图像，而是使用区域，这就是为什么它被称为基于区域的卷积神经网络。</p><p id="f53c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这个网络的缺点是它非常慢，因为对于一个图像它产生大约2000个区域，因此对于N个图像N*2000是非常大的。</p><h2 id="961f" class="lt ig hi bd ih lu lv lw il lx ly lz ip jo ma mb it js mc md ix jw me mf jb mg bi translated">基于快速R-CNN的目标检测</h2><p id="3c94" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">以前方法的问题是对一幅图像运行CNN 2000次。为了克服这一点，我们将输入图像提供给CNN，CNN进而生成卷积特征图。使用这些图，提取提案的区域。然后，我们使用RoI pooling层将所有建议的区域重新调整为固定的大小，以便将其输入到一个完全连接的网络中进行分类，并同时使用softmax和线性回归层返回边界框</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es mj"><img src="../Images/319efa64399f9140090c838d44aa03f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0wy8Koyi_SKYqoVuU6Kwvg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">快速R-CNN的工作原理</figcaption></figure><p id="2112" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">快速R-CNN的问题是，它还使用选择性搜索作为建议方法来寻找感兴趣的区域，这是一个缓慢而耗时的过程</p><h1 id="2cbe" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">了解更快的R-CNN</h1><p id="f97c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将使用<strong class="jf hj">区域提议网络(RPN) </strong>，而不是使用选择性搜索来生成感兴趣的区域。RPN将图像特征图作为输入，并生成一组对象提议，每个提议都有一个对象性分数作为输出。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es mk"><img src="../Images/1b7eeecc3d88f6a95e694ab2986e35f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HnqwifSeOIcJlwlKUHNnLA.jpeg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">快速R-CNN的工作原理</figcaption></figure><p id="26d1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在上图中，如果我们用选择性搜索替换RPN网络，它将成为快速R-CNN。唯一的区别是快速R-CNN使用选择性搜索，而快速R-CNN使用RPN来生成感兴趣的区域。</p><p id="01ba" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">ROI池用于对非均匀大小的输入执行最大池，以获得固定大小的特征地图。这里的输入是来自CNN的特征图和来自RPN模型的感兴趣区域。</p><p id="9902" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在ROI合并之后，有两个完全连接的层，这两个层被连接到边界框回归器和用于输出的softmax分类器。</p><p id="5fcd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">代号</strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">美国有线电视新闻网(VGG-16)</figcaption></figure><p id="da77" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们已经使用预训练的VGG-16从图像生成特征地图。为此，我们从网络中移除了顶层。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ml"><img src="../Images/692863e16cea7b37589b12a2b2b5c714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*TFAZbvRqtwNdTtNr_IfmNQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">VGG-16结构</figcaption></figure><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">RPN的代码</figcaption></figure><p id="7e94" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">然后我们创建一个rpn网络，它从VGG-16模型中提取输入特征。它有一个回旋层分支成两个以上的回旋层。</p><p id="00f3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">x_class分类是否是一个对象。x_regr包含边界框坐标。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es mm"><img src="../Images/bde2850cb8bc1f1b6ee30f31cbdb7b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WAmn9jwmYcRyZSX24RmhdQ.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">RPN的结构</figcaption></figure><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="lr ls l"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">分类器代码</figcaption></figure><p id="94f6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">第一个ROI汇集(RoiPoolingConv)是通过VGG-16(base_layer)生成的特征图和从RPN网络创建的ROI(input _ ROIs)完成的。然后，它被传递到分类器网络，该分类器网络包含两个完全连接的层，该层被连接到边界框回归器(out_regr)和用于输出的softmax分类器(out_class)。</p><p id="d2f2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们不使用Conv2D，而是使用时间分布式。这是因为我们需要避免将几个图像合并成一个，因为所有图像的整个像素列表将被发送到第一层。为了避免这种情况，我们使用了时间分布。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mn"><img src="../Images/b7bab76924b13dfda022ffeb956d8523.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*GP10BynuQMwvpxLoivQMjA.jpeg"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">分类器的结构</figcaption></figure><p id="cf52" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">边界框将给出坐标，分类器将检测背景或前景。</p><h1 id="fc91" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">工作流程</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es mo"><img src="../Images/b7fcac309b10a53256fa2f9ce370426a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HgWEVZ23QRGCFmKHAliswQ.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">逐步过程</figcaption></figure><h1 id="d2d6" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结果</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es mp"><img src="../Images/a8ba8ad56d853fab02b36f6ad874c41f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2gdWdbJx8mdIasa_NlVeQ.jpeg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">两种HPF的有丝分裂检测结果</figcaption></figure><p id="239b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">该模型的精度为0.42，召回率为0.36，F值为0.37</p></div></div>    
</body>
</html>