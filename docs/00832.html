<html>
<head>
<title>Understanding Long-Short Term Memory</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解长短期记忆</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-long-short-term-memory-b2144ac64e82?source=collection_archive---------12-----------------------#2019-09-11">https://medium.com/analytics-vidhya/understanding-long-short-term-memory-b2144ac64e82?source=collection_archive---------12-----------------------#2019-09-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="20bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将看看递归神经网络(RNN)的类型，它可以克服简单的递归神经网络遭受的消失梯度问题。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/07894a1edf3aca706084dbd90a7eb3a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*06iqgU4zkVsPLkAWqzptfw.jpeg"/></div></div></figure><p id="ac02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">信息是新的电力、黄金、汽油、钻石…</p><p id="7261" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它已经成为21世纪最重要的财富，科技巨头们要么偷，要么买，要么要，却没有一个合适的免责声明来说明他们要用它做什么；有趣的是，它不仅在我们当前的社会中，而且在整个动物王国中一直扮演着重要的角色。</p><blockquote class="jp"><p id="0f1c" class="jq jr hi bd js jt ju jv jw jx jy jc dx translated">"一般来说，生活中最成功的人是拥有最多信息的人。"——本杰明·迪斯雷利</p></blockquote><p id="265e" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">早在一万年前，信息创造并开始发展其第一个<strong class="ih hj"> GMaps </strong>与被称为<em class="ke">鸽科</em>的鸟类，从物种<strong class="ih hj">鸽</strong>，进化到早期的<strong class="ih hj">家鸽</strong> ( <em class="ke">家鸽</em>)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kf"><img src="../Images/0e5702d10b412ed6b73451e2f006579b.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*KYjUiaogzFIbur4TAmIxzA.jpeg"/></div></figure><p id="7f4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">家鸽</strong>是由<strong class="ih hj">岩鸽</strong>演变而来，岩鸽是世界上最古老的家养鸟类，根据美索不达米亚楔形文字记载，鸟类的驯化可以追溯到5000年前。家鸽在和平时期和战争时期被大量使用，因为它们惊人的归巢能力，或者按照今天的标准，也许我应该说是谷歌地图的能力；足够有趣的是，如果与谷歌地图放在一起，自然构建导航系统领先几千年，肯定会在准确性、用户界面和UX方面胜出，只是在销售和营销方面会失败。</p><blockquote class="kg kh ki"><p id="9c2c" class="if ig ke ih b ii ij ik il im in io ip kj ir is it kk iv iw ix kl iz ja jb jc hb bi translated">寻的:能够在极长的距离内找到回家的路。伯德在竞技比赛中曾记录过长达1800公里的飞行。这整个系统不需要智能手机或任何其他电子设备(大脑除外)，只需要使用一个内部指南针就可以工作，这个指南针已经建立和发展了几千年。</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es km"><img src="../Images/20aeda43c1f8a17c5949b86762cb464f.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*jhepSIVqAl2IyQLHel1PxA.jpeg"/></div></figure><p id="4fe9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">研究人员认为，鸽子的这些归巢能力与<strong class="ih hj">磁接收</strong>一起工作，并使用“地图和指南针”模型，其中<strong class="ih hj">指南针</strong>功能允许鸟类确定方向，地图功能允许鸟类确定它们相对于目标地点(家庭阁楼)的位置。鸟类可以探测磁场来帮助它们找到回家的路。</p><blockquote class="kg kh ki"><p id="f7c1" class="if ig ke ih b ii ij ik il im in io ip kj ir is it kk iv iw ix kl iz ja jb jc hb bi translated"><strong class="ih hj">磁感应:</strong>是一种让生物体通过检测磁场来感知方向、高度或位置的感觉。</p></blockquote><p id="2bde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们可以有把握地说，它们通过接收、存储、处理和回忆<strong class="ih hj">信息</strong>来提供动力，利用它们的感官输入来捕捉信息，利用它们小小的大脑迷人的处理能力和记忆来使整个系统运转。</p><p id="73fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">信息是一种伟大的工具，当开发出适当的系统，以前所未有的方式存储、处理和分析数据时，信息看起来就像纯粹的魔法。它可以让动物知道巢穴的位置，家庭范围内导航的地标，过去在哪里找到食物和水，以及以前与另一种动物的社会互动结果如何，所有这些都是塑造未来行为的关键信息。</p><h1 id="b3eb" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">记忆是如何工作的？</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ll"><img src="../Images/3c9ffdfb783061788a63ecb1592162a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*RinghLNz9BEpfksqovnJHA.jpeg"/></div></figure><p id="2fbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">记忆始于感觉输入后大脑中的生化反应。</p><p id="9433" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有3种类型的内存:</p><ul class=""><li id="8937" class="lm ln hi ih b ii ij im in iq lo iu lp iy lq jc lr ls lt lu bi translated"><strong class="ih hj">短期</strong>——它是由你大脑神经递质(化学信使)水平在突触(一种允许一个神经元向另一个神经元传递电信号或化学信号的结构)的任何短暂变化触发的。</li><li id="6bb6" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated"><strong class="ih hj">中期</strong>——介于短期记忆和长期记忆之间，是记忆巩固阶段发生的地方(通常发生在睡眠期间)。</li><li id="1d7a" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated"><strong class="ih hj">长期</strong> —通过强化所学事件，将对未来至关重要的信息转移到长期记忆中。</li></ul><p id="73a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，回到人工智能，我们可以看到所有这些是如何与本文的主题相关联的。</p><h1 id="3a4d" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">这一切有什么联系？</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ma"><img src="../Images/16b9f5fc662ccd02bcbd664d23d9f046.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*-a-9sLWbPikMxNdtDfJIrg.gif"/></div></figure><p id="eac8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我之前的<a class="ae mb" rel="noopener" href="/datadriveninvestor/the-brief-history-of-nlp-c90f331b6ad7?source=your_stories_page---------------------------"> <strong class="ih hj">文章</strong> </a>中，我们简单介绍了几个用于序列处理的基本DL算法和方法，即:</p><ul class=""><li id="9bd9" class="lm ln hi ih b ii ij im in iq lo iu lp iy lq jc lr ls lt lu bi translated"><strong class="ih hj">单词嵌入</strong> —一种用于将人类语言映射到几何空间的方法</li><li id="b245" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated"><strong class="ih hj">递归神经网络(RNN) </strong> —一种具有记忆的神经网络。它处理数据序列，同时记录/记忆到目前为止所看到的内容，rnn的一些例子有:LSTM、GRU和BI-LSTM。</li><li id="316f" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated"><strong class="ih hj"> 1D修道院</strong> — <strong class="ih hj"> </strong>一维版的<a class="ae mb" rel="noopener" href="/@prince.canuma/beyond-neural-networks-bdc68980d474"><strong class="ih hj">2D/3D conv nets</strong></a><strong class="ih hj"/>用于计算机视觉领域。</li></ul><h1 id="7584" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">理解长短期记忆</h1><p id="320c" class="pw-post-body-paragraph if ig hi ih b ii mc ik il im md io ip iq me is it iu mf iw ix iy mg ja jb jc hb bi translated">在我的上一篇文章中，我们谈到了<strong class="ih hj">香草RNN </strong>，它是<strong class="ih hj"> Tensorflow Keras </strong>框架中的一层，称为SimpleRNN或RNN，如果你是我介绍的<strong class="ih hj"> Pytorch </strong>的忠实粉丝，它位于名为RNN(torch.nn.RNN)的神经网络(NN)包下。</p><p id="7f7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这不是唯一存在的RNN层，让我告诉你为什么。</p><p id="c5d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">香草RNN有一个主要问题:尽管它能够保留在先前时间步长(t)中看到的所有信息的记忆，其中t≥1；实际上，这样的长期依赖是不可能学会的。这是因为消失梯度问题影响了多层深度(多层堆叠)的神经网络，随着我们不断向网络添加层，网络变得不可训练，简单来说，学习能力随着层的增加而降低。</p><p id="23bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就像当我们学习新的话题时，我们需要不断地修改过去的信息，以便更好地学习，并防止我们忘记那些信息。</p><blockquote class="kg kh ki"><p id="5862" class="if ig ke ih b ii ij ik il im in io ip kj ir is it kk iv iw ix kl iz ja jb jc hb bi translated">对此的解决方案是将一些过去的信息注入/添加到后面的层，以便它可以修改它，并通过网络更好地传播学习信号，从而容易地训练更大的网络，并避免由网络的更深层获得的知识(<em class="hi">权重</em>)的退化。然而，当涉及到RNN时，我们必须计算出你希望过去的信息在多大程度上影响现在，因为记住一切没有帮助，不记住也没有帮助，所以我们必须有一个函数来决定从过去记住多少。</p></blockquote><blockquote class="jp"><p id="6437" class="jq jr hi bd js jt mh mi mj mk ml jc dx translated">“我们必须欢迎未来，记住它很快就会成为过去；我们必须尊重过去，记住它影响未来”——P·卡努马</p></blockquote><p id="e2e9" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">Hochreiter、Schmidhuber和Bengio在20世纪90年代早期研究了记住所有事情对RNNs的影响的理论原因。</p><p id="0f17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">长短期记忆</strong> (LSTM)和<strong class="ih hj">门控循环单元</strong> (GRU)层就是为了解决这个问题而设计的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mm"><img src="../Images/50a6982472e77cb4ca83ced537e6d95c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iaOE1EnJSb87fE8jRj5V7g.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">图1:LSTM的起点，来自F.chollet的《用python进行深度学习》</figcaption></figure><p id="f82f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SimpleRNN是LSTM的起点，但是LSTM层的设计考虑了一些因素；本质上，LSTM为以后保存信息，从而防止旧信号逐渐消失，它通过计算注入过去信息的百分比(概念上)来做到这一点。</p><h1 id="2d4b" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">潜入…</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mr"><img src="../Images/83c4f5f9e36dff6f66ca8aedb7cfbc8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/1*FoHn13tRGGmED67xf9FdHA.gif"/></div></div></figure><p id="cf18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们深入图1的细节，更好地理解LSTM单元内部的数据流。</p><p id="5287" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在单元格内有两个权重矩阵<strong class="ih hj"> W </strong>和<strong class="ih hj"> U </strong>(包含我喜欢称之为“知识”),它们用字母<strong class="ih hj"> o (Wo和Uo) </strong>表示<strong class="ih hj"> </strong> <em class="ke">输出</em> <strong class="ih hj">。</strong>我们还有<strong class="ih hj"> state_t </strong>，它是前一步t <em class="ke">的<em class="ke">输出</em>。</em></p><p id="7012" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，现在让我们添加一个额外的数据流，它携带跨时间步长(t)的信息。调用其在不同时间步长的值<em class="ke"> Ct，</em>其中<em class="ke"> C </em>代表<em class="ke">进位。</em> <strong class="ih hj"> <em class="ke"> </em> </strong>存储在Ct中的信息(时间t时所有输出的集合)将与输入(序列数据)和递归(先前输出)相结合，所有这些都是通过<strong class="ih hj">密集变换</strong>完成的:一个带有权重矩阵的点积，之后是偏置加法和激活函数的应用，它将影响发送到下一时间步的状态(通过激活函数和乘法运算)。</p><p id="6783" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请原谅我…</p><p id="501d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从概念上讲，进位数据流是调制下一个输出和或状态的一种方式。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ms"><img src="../Images/0c6c8adb5b85c2ffb305044c6cf0b834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hsNxb_fb8gc_CZfHvHk8JQ.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">图2:从简单到LSTM:增加进位跟踪</figcaption></figure><p id="fd16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算Ct(进位数据流)涉及三种不同的转换。三者都具有SimpleRNN单元的形式:</p><p id="8d8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> y =激活(dot(input_t，W) + dot(state_t，U) + b) </strong></p><p id="746a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是这三种变换都有自己的权重矩阵，您可以用字母I、f和k对其进行索引。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mt mu l"/></div></figure><p id="490d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单！！！看，没那么难。</p><h2 id="ec76" class="mv ko hi bd kp mw mx my kt mz na nb kx iq nc nd lb iu ne nf lf iy ng nh lj ni bi translated">LSTM的解剖</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nj"><img src="../Images/417bc091ae7a59663e1aeea822503095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iv2P5kL6tZIh_6T6nBKI_g.png"/></div></div></figure><p id="6cae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以解释这些操作中的每一个打算做什么，但是解释做不了多少，因为这些操作实际上做什么是由参数化它们的权重(知识)的内容决定的；并且重量是以端到端的方式学习的，从每一轮训练开始，使得不可能有特定目的的这个或那个操作。</p><p id="45dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重量不同的同一个细胞可以做非常不同的事情。</p><p id="0729" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RNN单元决定了您的假设空间，即您在训练过程中搜索良好模型配置的空间。<strong class="ih hj">因此，组成RNN单元的操作组合更好地解释为对您的搜索的一组约束，而不是工程意义上的设计。</strong></p><p id="bb24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于研究人员来说，似乎这种约束的选择——如何实现RNN单元的问题——最好留给优化算法，而不是人类工程师，即遗传算法或强化学习过程，它们将使用所有可用的计算能力来找到最佳约束集(超参数)，从而为特定问题产生最佳精度。在未来，这就是神经网络的构建方式。</p><h1 id="f1ff" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated"><strong class="ak">主要外卖</strong></h1><ul class=""><li id="55cf" class="lm ln hi ih b ii mc im md iq nk iu nl iy nm jc lr ls lt lu bi translated">LSTM细胞是为了让过去的信息在以后重新注入，通过计算我们允许过去的信息影响现在的信息的百分比，从而解决香草RNN的消失梯度问题。</li></ul></div><div class="ab cl nn no gp np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="hb hc hd he hf"><blockquote class="kg kh ki"><p id="4f8e" class="if ig ke ih b ii ij ik il im in io ip kj ir is it kk iv iw ix kl iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi">注意</em> </strong>:我特别为你准备了一个<strong class="ih hj"> colab笔记本</strong>，这样你就可以了解所有的概念，实际上看到它是如何从零开始的，以及如何使用内置在TF和Torch等著名框架中的生产就绪层。</p></blockquote><div class="nu nv ez fb nw nx"><a href="https://colab.research.google.com/drive/1IkG-iXsWtW2d23_BC4vNMDJ3CRsFeix1#scrollTo=gBo7Eb1_750Q" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab dw"><div class="nz ab oa cl cj ob"><h2 class="bd hj fi z dy oc ea eb od ed ef hh bi translated">谷歌联合实验室</h2><div class="oe l"><h3 class="bd b fi z dy oc ea eb od ed ef dx translated">编辑描述</h3></div><div class="of l"><p class="bd b fp z dy oc ea eb od ed ef dx translated">colab.research.google.com</p></div></div><div class="og l"><div class="oh l oi oj ok og ol jn nx"/></div></div></a></div></div><div class="ab cl nn no gp np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="hb hc hd he hf"><p id="8603" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您的阅读。如果你有任何想法，评论或批评，请在下面评论。</p><p id="f954" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在twitter上关注我的<a class="ae mb" href="https://twitter.com/CanumaGdt" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">卡努玛</strong> </a> <strong class="ih hj">，</strong>王子，这样你就可以随时了解AI领域的最新动态<strong class="ih hj">。</strong></p><h1 id="91c5" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">参考</h1><div class="nu nv ez fb nw nx"><a href="http://www.animalbehavioronline.com/memory.html" rel="noopener  ugc nofollow" target="_blank"><div class="ny ab dw"><div class="nz ab oa cl cj ob"><h2 class="bd hj fi z dy oc ea eb od ed ef hh bi translated">记忆</h2><div class="oe l"><h3 class="bd b fi z dy oc ea eb od ed ef dx translated">储存的信息在许多动物的生活中起着至关重要的作用。知道巢穴的位置，地标…</h3></div><div class="of l"><p class="bd b fp z dy oc ea eb od ed ef dx translated">www.animalbehavioronline.com</p></div></div></div></a></div><ul class=""><li id="8482" class="lm ln hi ih b ii ij im in iq lo iu lp iy lq jc lr ls lt lu bi translated">Francois Chollet用python进行深度学习</li></ul></div></div>    
</body>
</html>