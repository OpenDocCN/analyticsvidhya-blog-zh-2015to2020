<html>
<head>
<title>Demonstrating Calculation of TF-IDF From Sklearn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Sklearn的TF-IDF演示计算</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/demonstrating-calculation-of-tf-idf-from-sklearn-4f9526e7e78b?source=collection_archive---------0-----------------------#2020-04-21">https://medium.com/analytics-vidhya/demonstrating-calculation-of-tf-idf-from-sklearn-4f9526e7e78b?source=collection_archive---------0-----------------------#2020-04-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="eaef" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">python中sklearn的TF-IDF模块背后的数学逻辑解释</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/7c954db193117a025f36ec623062c133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5eNLy2lQuaG14yke.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图:1.1</figcaption></figure><h2 id="6a2d" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak">什么是文本数据的特征工程？</strong></h2><p id="1ca0" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">将原始文本数据转换成机器可理解的格式(数字)的过程称为文本数据的特征工程。机器学习和深度学习算法的性能和准确性从根本上取决于所使用的特征工程技术的类型。</p><p id="c295" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">在本文中，我们将看到使用TF-IDF的特征工程技术以及TF、IDF和TF-IDF的数学计算。读完这篇文章后，你会理解python中的<em class="lj">sk learn . feature _ extraction</em>包中的<em class="lj"> TfidfTransformer </em>等库背后的数理逻辑。</p><h2 id="90c7" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak"> TF(词频):</strong></h2><ul class=""><li id="7d28" class="lk ll hi kn b ko kp kr ks jy lm kc ln kg lo ld lp lq lr ls bi translated">词频就是一个单词在一个句子中出现的次数</li><li id="c2cd" class="lk ll hi kn b ko lt kr lu jy lv kc lw kg lx ld lp lq lr ls bi translated">TF基本上是捕捉单词的重要性，而不考虑文档的长度。</li><li id="2b95" class="lk ll hi kn b ko lt kr lu jy lv kc lw kg lx ld lp lq lr ls bi translated">一个词频为3、句子长度为10的词和句子长度为100的词是不一样的。在第一种情况下，它应该变得更加重要；这就是TF的工作。</li></ul><h2 id="c158" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak"> IDF(逆文档频率):</strong></h2><ul class=""><li id="03a9" class="lk ll hi kn b ko kp kr ks jy lm kc ln kg lo ld lp lq lr ls bi translated">每个单词的IDF是总行数与该单词所在的特定文档中的行数之比的对数。</li><li id="7eea" class="lk ll hi kn b ko lt kr lu jy lv kc lw kg lx ld lp lq lr ls bi translated">IDF将衡量一个术语的稀有程度。像“a”和“the”这样的词出现在语料库的所有文档中，但是稀有词并不在所有文档中。</li></ul><h2 id="b063" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="ak"> TF-IDF: </strong></h2><p id="d52a" class="pw-post-body-paragraph kl km hi kn b ko kp ij kq kr ks im kt jy ku kv kw kc kx ky kz kg la lb lc ld hb bi translated">它是TF和IDF的最简单的产品，因此上面提到了两个缺点，这使得预测和信息检索相关。</p><p id="4160" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">我们将使用以下包含五个文档的语料库:</p><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="8326" class="jn jo hi lz b fi md me l mf mg">docs = ['the cat see the mouse',<br/>      'the house has a tiny little mouse',<br/>       'the mouse ran away from the house',<br/>        'the cat finally ate the mouse',<br/>       'the end of the mouse story'<br/>       ]</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mh"><img src="../Images/56fdc0799e044a35e61cef7b6c42d845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nmxX1R2E-YSaozmCwMQyJg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图1.2</figcaption></figure><blockquote class="mi"><p id="cf83" class="mj mk hi bd ml mm mn mo mp mq mr ld dx translated"><strong class="ak">使用</strong><a class="ae ms" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction" rel="noopener ugc nofollow" target="_blank"><strong class="ak">TfidfTransformer</strong></a><strong class="ak"><em class="mt"/>从</strong><a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" rel="noopener ugc nofollow" target="_blank"><strong class="ak">sk learn . feature _ extraction</strong></a><strong class="ak">包中提取特征。</strong></p></blockquote><p id="1820" class="pw-post-body-paragraph kl km hi kn b ko mu ij kq kr mv im kt jy mw kv kw kc mx ky kz kg my lb lc ld hb bi translated">现在从sklearn.feature_extraction模块导入TfidfTransformer和CountVectorizer。</p><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="654d" class="jn jo hi lz b fi md me l mf mg">from sklearn.feature_extraction.text import TfidfTransformer<br/>from sklearn.feature_extraction.text import CountVectorizer</span></pre><p id="1ce2" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">CountVectorizer用于查找数据集的每个文档中的字数。也称为计算词频。要了解更多信息，请点击<a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" rel="noopener ugc nofollow" target="_blank">此处</a></p><p id="f82b" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">让我们看一个例子，</p><p id="f6a2" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">从上面的文档中，我们将使用计数矢量器拟合所有的句子，并获得每个文档的字数数组。</p><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="e193" class="jn jo hi lz b fi md me l mf mg">import pandas as pd<br/>cv = CountVectorizer()<br/>word_count_vector = cv.fit_transform(docs)</span><span id="0474" class="jn jo hi lz b fi mz me l mf mg">tf = pd.DataFrame(word_count_vector.toarray(), columns=cv.get_feature_names())<br/>print(tf)</span></pre><p id="8ea9" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es na"><img src="../Images/a84262f1915e1488abb076dd0e29c1c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*O5qw9If1bTsHG2BtZzEJyA.png"/></div></figure><p id="d5b1" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">现在声明TfidfTransformer()实例，用上面的word_count_vector拟合，得到IDF和最终的归一化特征。</p><p id="7c8b" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated"><strong class="kn hj">注:</strong>TfidfTransformer(<em class="lj">norm = ' l2 '</em>)默认使用' L2 '参数进行归一化，其中vector(document)元素的平方和为1，也称为欧氏范数。</p><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="419b" class="jn jo hi lz b fi md me l mf mg">tfidf_transformer = TfidfTransformer()<br/>X = tfidf_transformer.fit_transform(word_count_vector)<br/>idf = pd.DataFrame({'feature_name':cv.get_feature_names(), 'idf_weights':tfidf_transformer.idf_})<br/>print(idf)</span></pre><p id="02ed" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/3ac6c32ef2f5cdbb8ab328169876e7ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*bNSbzzyw2fVpLhbcos_rGQ.png"/></div></figure><p id="e452" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">最终特征向量是:</p><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="2ddd" class="jn jo hi lz b fi md me l mf mg">tf_idf = pd.DataFrame(X.toarray() ,columns=cv.get_feature_names())<br/>print(tf_idf)</span></pre><p id="0d74" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nc"><img src="../Images/5b300d68c9beece3eba662861dd5487c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*-bpwLYs6Bduo3390pntx7g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图1.3</figcaption></figure><p id="f7bf" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">对以上结果感到困惑？不要担心，让我们看看下面的矢量0的公式和计算。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/c671b400d430dda2fc232ead73f4746e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yYuR79ejxnVZKY5Ft6Bimg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图:1.4</figcaption></figure><p id="564c" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">在上面的文档集中，向量0是<code class="du ne nf ng lz b">docs[0]</code>，即<code class="du ne nf ng lz b">the cat see the mouse</code></p><p id="1d59" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">向量0的特征索引是:</p><p id="1985" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">{'the': 14，' see':12，' mouse':9，' cat':2} see索引<code class="du ne nf ng lz b">idf</code>数据帧。</p><p id="6015" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">因此，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nh"><img src="../Images/bc3128f960a4042cd439f3dc9e505549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*onoS_zQ8A40L4wzD4hOV_g.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ni"><img src="../Images/7883a574a6b334b2b0ca84f841fe66cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*PcJ_HLI4cfa6VoYaXprZoA.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nj"><img src="../Images/2097adb1615b4f04e7e67dd79f2d16c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*YnjAaEqu9Lq759fvLFCxaw.png"/></div></figure><p id="189e" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">这就是sklearn软件包计算规格化特征的方法。</p><p id="5990" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">现在，如果你想看到sklearn背后的实用方法。我将在不使用sklearn库的情况下演示手动方法。只需要几行代码，我相信这将有助于更好地理解TF-IDF计算，让我们继续吧。</p><p id="9e4e" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">为上述文件创建一个数据框架<code class="du ne nf ng lz b">df</code>，</p><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="d4e3" class="jn jo hi lz b fi md me l mf mg">df = pd.DataFrame({‘docs’: [‘the cat see the mouse’,<br/> “the house has a tiny little mouse”,<br/> ‘the mouse ran away from the house’,<br/> ‘the cat finally ate the mouse’,<br/> ‘the end of the mouse story’<br/> ]})</span><span id="84c8" class="jn jo hi lz b fi mz me l mf mg">print(df)</span></pre><p id="0cb8" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nk"><img src="../Images/efb5dc039903fa860a2337ad0c77b0a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*z-1WvDKbu4N22_SzB3Lffg.png"/></div></figure><p id="ce82" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">创建一个名为<code class="du ne nf ng lz b">tokens</code>的额外列，包含我对上述语料库中的每个文档进行标记所形成的单词列表。</p><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="2d75" class="jn jo hi lz b fi md me l mf mg">df[‘tokens’] = [x.lower().split() for x in df.docs.values] <br/>print(df)</span></pre><p id="868f" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/be32fcdbfd93459647b092b6dab8bcf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*OvTZPT7dE-9ouuNbKgTjYg.png"/></div></figure><p id="707b" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">现在，为了计算词频，在上面的dataframe列<code class="du ne nf ng lz b">tokens</code>上应用一个匿名函数，以便确定每行中每个单词的计数。用<code class="du ne nf ng lz b">0</code>填充<code class="du ne nf ng lz b">nan</code>值，最后使用sort_index()函数按列名对结果数据帧进行排序，如以下代码所示。</p><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="0e0e" class="jn jo hi lz b fi md me l mf mg">tf = df.tokens.apply(lambda x: pd.Series(x).value_counts()).fillna(0)   <br/>tf.sort_index(inplace=True, axis=1)<br/># 'a' feature doesn't seems good feature so, removing it from tf <br/>tf.drop('a', axis=1, inplace=True)<br/>tf</span></pre><p id="64f0" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/651cb393e83811c4a9256d95e2297b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*AEDzQYrGFq9kVxFaP-wpbQ.png"/></div></figure><p id="7b9d" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">现在，我们将使用与图1.2 中相同的公式计算IDF，即:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nn"><img src="../Images/5345f8c14135f5ee633309655c790ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*RrXNTHiLTUJj0JktBJUOKw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图:1.5</figcaption></figure><p id="3381" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">参考下面的代码实现上述公式，</p><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="c638" class="jn jo hi lz b fi md me l mf mg">import numpy as np<br/>idf = pd.Series([np.log((float(df.shape[0])+1)/(len([x for x in df.tokens.values if token in x])+1))+1 for token in tf.columns])<br/>idf.index = tf.columns<br/>print(idf)</span></pre><p id="d39e" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es no"><img src="../Images/5311e8a7e50614483ea14bc9f2d10f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*CpVar-mxHYWICD89co45Qg.png"/></div></figure><p id="90dc" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">要计算TF-IDF，只需将上面的<code class="du ne nf ng lz b">tf</code>数据帧和<code class="du ne nf ng lz b">idf</code>相乘，让我们看看下面的代码和最终结果。</p><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="1964" class="jn jo hi lz b fi md me l mf mg">tfidf = tf.copy()<br/>for col in tfidf.columns:<br/> tfidf[col] = tfidf[col]*idf[col]<br/> <br/>print(tfidf)</span></pre><p id="3177" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es np"><img src="../Images/9e5232c36a3ad0b24c76dec8833e85b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N_hH2r9qsV-vssXDyxIfQg.png"/></div></div></figure><p id="6eb9" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">现在，我们必须使用欧几里德范数来归一化上述结果</p><ol class=""><li id="aa3e" class="lk ll hi kn b ko le kr lf jy nq kc nr kg ns ld nt lq lr ls bi translated">参考<a class="ae ms" href="https://cdn-images-1.medium.com/max/800/1*yYuR79ejxnVZKY5Ft6Bimg.png" rel="noopener">图:1.3 </a>因此，为了实现这一点，我们将对上述<code class="du ne nf ng lz b">tfidf</code>数据帧中的一行的每个元素的平方和应用<code class="du ne nf ng lz b">numpy.sqrt()</code>，以获得<code class="du ne nf ng lz b">sqrt_vec</code>。</li><li id="7a87" class="lk ll hi kn b ko lt kr lu jy lv kc lw kg lx ld nt lq lr ls bi translated">使用数据帧<code class="du ne nf ng lz b"><a class="ae ms" href="https://pandas.pydata.org/pandas-docs/version/0.24.2/reference/api/pandas.DataFrame.div.html" rel="noopener ugc nofollow" target="_blank">DataFrame.div</a></code>的浮点除法将<code class="du ne nf ng lz b">tfidf </code>除以索引上的<code class="du ne nf ng lz b">sqrt_vec</code>。</li></ol><pre class="iy iz ja jb fd ly lz ma mb aw mc bi"><span id="0a7a" class="jn jo hi lz b fi md me l mf mg">sqrt_vec = np.sqrt(tfidf.pow(2).sum(axis=1))<br/>tfidf.div(sqrt_vec, axis=0)</span></pre><p id="d2ee" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nu"><img src="../Images/07c977c9549eecbd92bc2120f14245e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*O9GlAp2fX-njTIJ5oOc7nw.png"/></div></figure><p id="8916" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">如果你在<a class="ae ms" href="https://cdn-images-1.medium.com/max/800/1*-bpwLYs6Bduo3390pntx7g.png" rel="noopener">看到使用sklearn库的<code class="du ne nf ng lz b">tfidf</code>的输出:图1.3 </a>和上面的输出是一样的。这就是sklearn如何从给定的文本数据语料库中找到归一化的TF-IDF特征值的方式。</p><p id="afbf" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">在真实世界场景中，有大量的原始非结构化文本数据，因此为了处理这些数据，我们可以使用sklearn库中的Tf-IDF特征工程技术将原始文本数据转换为机器可读的格式。</p><p id="3c19" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">你可以在下面我的GitHub链接上引用一个真实的文本数据用例。</p><div class="nv nw ez fb nx ny"><a href="https://github.com/shubhamchouksey/Consumer-Complaint-Classification" rel="noopener  ugc nofollow" target="_blank"><div class="nz ab dw"><div class="oa ab ob cl cj oc"><h2 class="bd hj fi z dy od ea eb oe ed ef hh bi translated">shubhamchouksey/消费者投诉分类</h2><div class="of l"><h3 class="bd b fi z dy od ea eb oe ed ef dx translated">问题:消费者金融保护局每周都会发送数千份消费者关于金融…</h3></div><div class="og l"><p class="bd b fp z dy od ea eb oe ed ef dx translated">github.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om jh ny"/></div></div></a></div><p id="d981" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">参考:</p><div class="nv nw ez fb nx ny"><a href="https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558" rel="noopener follow" target="_blank"><div class="nz ab dw"><div class="oa ab ob cl cj oc"><h2 class="bd hj fi z dy od ea eb oe ed ef hh bi translated">python中从头开始的TF(词频)-IDF(逆文档频)。</h2><div class="of l"><h3 class="bd b fi z dy od ea eb oe ed ef dx translated">从头开始创建TF-IDF模型</h3></div><div class="og l"><p class="bd b fp z dy od ea eb oe ed ef dx translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="on l oj ok ol oh om jh ny"/></div></div></a></div><p id="d51f" class="pw-post-body-paragraph kl km hi kn b ko le ij kq kr lf im kt jy lg kv kw kc lh ky kz kg li lb lc ld hb bi translated">我希望这篇文章对你有用！如果你有任何问题，请在下面留言。</p><blockquote class="oo op oq"><p id="2dd5" class="kl km lj kn b ko le ij kq kr lf im kt or lg kv kw os lh ky kz ot li lb lc ld hb bi translated">您可以在<a class="ae ms" href="https://www.linkedin.com/in/insvk/" rel="noopener ugc nofollow" target="_blank"><strong class="kn hj">LinkedIn</strong></a><strong class="kn hj"/>上联系我，或者在<a class="ae ms" href="https://github.com/shubhamchouksey" rel="noopener ugc nofollow" target="_blank"> <strong class="kn hj"> GitHub </strong> </a>上关注我</p></blockquote></div></div>    
</body>
</html>