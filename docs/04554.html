<html>
<head>
<title>Gradient Descent and Stochastic Gradient Descent from scratch Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降和随机梯度下降从零开始Python</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-and-stochastic-gradient-descent-from-scratch-python-1cd93d4def49?source=collection_archive---------6-----------------------#2020-03-24">https://medium.com/analytics-vidhya/gradient-descent-and-stochastic-gradient-descent-from-scratch-python-1cd93d4def49?source=collection_archive---------6-----------------------#2020-03-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9fa5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我旨在解释如何通过生成简单的数据并应用逻辑回归对其进行分类来从头构建GD和SGD。为了优化/最小化损失，我们将使用GD和SGD。最后，我们将对收敛所需的时间、分类器的优化和绘制决策边界的利弊进行比较研究。我们开始吧！</p><p id="f2b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们继续之前，让我们了解一下梯度下降。它用来寻找某点x处的最小函数值，使得f(x) = 0。为了得到全局最小值:函数必须遵守一些约束。它必须是凸的和可微的。</p><p id="1041" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果函数是凸的，它一定会找到达到最小值的方法。如果是可微的，为了沿着那条路径到达最小值，在那一瞬间的梯度给出了下降的方向。当我们在某个点x对该函数求导时，就可以找到梯度</p><h1 id="56d9" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">伪代码</h1><p id="ec70" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">在机器学习方面，权重(w)是我们从最佳学习/描述我们数据的模型中获得的训练后的参数。x是我们手中的数据集，y是我们在数据中得到的标签，这是我们试图找到的答案，如果它是分类/回归问题的话。</p><ul class=""><li id="5547" class="kg kh hi ih b ii ij im in iq ki iu kj iy kk jc kl km kn ko bi translated">从随机权重开始。</li><li id="eac1" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">用这些权重计算预测值。</li><li id="09b9" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">检查你错过了多少。</li><li id="eeae" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">更新权重并迭代</li></ul><p id="3542" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当更新权重时，我们使用学习率，正如所讨论的，梯度给我们方向，但是下降多少由学习率决定。如果这个值很小，收敛速度就很慢，如果我们指定一个很大的值，可能会错过全局最小值。有多种方法来分配学习率的值，但我不会详细说明，因为它涉及数据的特征值和特征向量。</p><p id="8167" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">先决条件</strong>:我曾经对简单捏造的数据使用过逻辑回归，因此关于逻辑回归如何工作的知识将有助于理解代码。</p><p id="4a69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从简单中捏造数据。</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="2d9f" class="ld je hi kz b fi le lf l lg lh">mu = 0.5<br/>    sigma = 0.2</span><span id="83e2" class="ld je hi kz b fi li lf l lg lh">mu1 = -0.5<br/>    sigma1 = 0.2</span><span id="f908" class="ld je hi kz b fi li lf l lg lh">x_cor = np.random.normal(mu, sigma, 100)<br/>y_cor = np.random.normal(mu, sigma, 100)</span><span id="ec4d" class="ld je hi kz b fi li lf l lg lh">x1_cor = np.random.normal(mu1, sigma1, 100)<br/>y1_cor = np.random.normal(mu1, sigma1, 100)</span><span id="623d" class="ld je hi kz b fi li lf l lg lh">plt.scatter(x_cor,y_cor,s=10)<br/>plt.scatter(x1_cor,y1_cor,s=10)</span></pre><figure class="ku kv kw kx fd lk er es paragraph-image"><div class="er es lj"><img src="../Images/4a79063729c7c86b372ac2e877c69c73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*wDVBs3E1ukFnCRusGVSK2w.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">数据散点图</figcaption></figure><p id="3373" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们的目标是获得决策边界，使得这两个聚类类型的数据可以被算法分类。在我们开始之前，每个最大似然算法都有我们定义的损失函数。像MSE，RMSE，LogLoss等。</p><p id="a0c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在逻辑回归中，它致力于最小化该公式给出的负似然性(NLL)。</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="00f3" class="ld je hi kz b fi le lf l lg lh">L(f)  = -(y*log f(x) + (1 − y) log(1 − f(x)))</span></pre><p id="c545" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将把代码分成几个部分来解释:这是一个sigmoid函数，它总是位于0和1之间。由于我们的数据具有标签，如蓝色(1)和橙色(0)，因此，如果sigmoid函数值&gt; 0.5，我们可以将其归类为蓝色，否则归类为橙色。</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="1480" class="ld je hi kz b fi le lf l lg lh">def sigmoid(self,X):<br/>    # Activation function used to map any real value between 0 and 1<br/>    return 1 / (1 + np.exp(-X))</span></pre><p id="f156" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来是损失函数，这是上面提到的直接公式应用。我们将跟踪我们执行的每一次迭代的损失。请记住，我们的目标是将损失减少到理想的零。</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="8c00" class="ld je hi kz b fi le lf l lg lh">def cost_function(self,params, X, y):<br/>   # Computes the cost function for all the training samples<br/>   fx = self.sigmoid(X.dot(params))<br/>   cost = -np.sum(y * np.log(fx) + (1 - y)* np.log(1-fx))<br/>   return cost</span></pre><p id="c2ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来是梯度下降函数:神奇发生了。我将详细解释这一点。</p><ul class=""><li id="ebc6" class="kg kh hi ih b ii ij im in iq ki iu kj iy kk jc kl km kn ko bi translated">Params是权重，alpha是学习率，X是我们的原始数据，y是实际标签(1或0)。</li><li id="9cef" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">weight_new = weight_old + alpha *渐变。我希望你现在已经掌握了阿尔法和梯度的诀窍。阿尔法帮助采取步骤，梯度给方向。为什么我们要更新权重？这将帮助我们得到损失最小的函数的最小值。那是什么意思？该模型正在达到学习数据以便更好地对其进行分类的状态。</li><li id="76d7" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">sigmoid函数将是该元组的预测y_hat，然后通过对我们丢失的量取差并乘以我们的数据(X)来进行比较。我对整个数据集都这样做了，因为NumPy可以方便地轻松完成这样的操作。</li><li id="a041" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">一旦我们更新了权重，我们就计算损失函数，这一直持续到我们设置终止条件。我已经设置了要执行的迭代次数，也可以设置何时损失误差&lt; small_value say 1e-6</li><li id="49f7" class="kg kh hi ih b ii kp im kq iq kr iu ks iy kt jc kl km kn ko bi translated">I have kept tracked of cost_history which will be the plot.</li></ul><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="6a58" class="ld je hi kz b fi le lf l lg lh">def gradient_descent(self,params,X,y,iterations,alpha):<br/>   cost_history = np.zeros((iterations,1))<br/>   for i in range(iterations):<br/>     params = params+alpha*(X.T.dot(y-self.sigmoid(X.dot(params))))<br/>     cost_history[i] = self.cost_function(params,X, y)<br/>   return (cost_history, params)</span></pre><h1 id="c14b" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">Comparative Study</h1><figure class="ku kv kw kx fd lk er es paragraph-image"><div class="er es lr"><img src="../Images/493b7b6ed81061fee558e5b6ba2714dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*sFp8rhBBmY06Jw7x1Z1aiA.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">Results for GD and SGD</figcaption></figure><p id="c6ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Ignore the result for SGD, just to show a glimpse of Gradient descent Run time for 2000 iteration and alpha as 0.0001. Clearly seen, we started with a huge loss and slowly we are converging to zero value.</p><p id="7260" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">梯度下降和随机下降除了运行时间复杂度没有区别。GD对整个数据集进行多次迭代。SGD“统一”地只取数据集的子集，并运行相同的算法。</strong></p><figure class="ku kv kw kx fd lk er es paragraph-image"><div class="er es ls"><img src="../Images/644b5c262e6e86bc5aba236c63fdb068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*fO-f3_9GqFY5yPRd0jaIqQ.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">比较地块成本与迭代</figcaption></figure><p id="2a2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">一个问题？</strong></p><p id="8ea2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么SGD比GD需要更多的迭代次数来收敛。答案是，在GD中，整个数据都暴露在模型中，以便更深入地理解细微差别，而SGD是数据的子集，因此每个步骤获得的信息并不完整，无法在找到最小值的方向上采取决定性的决策步骤。</p><h1 id="7844" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">绘图结果</h1><p id="d319" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">最后，我们来绘制决策边界，如图所示。</p><figure class="ku kv kw kx fd lk er es paragraph-image"><div class="er es lt"><img src="../Images/8c994304049e2919f5350c554a18d8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*ShiyFJp4JLnW9WGnkBmqaw.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">SGD和GD的决策边界</figcaption></figure><p id="2f5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于如何绘制决策边界的简单逻辑。</p><ul class=""><li id="d09a" class="kg kh hi ih b ii ij im in iq ki iu kj iy kk jc kl km kn ko bi translated">y = MX+c——是一条线的eq，我们需要找到x和y的值，我们可以很容易地绘制出来。</li></ul><p id="2b15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">求x值，计算类2的最小值点和类1的最大值点，基本覆盖x轴极值点。</p><p id="0a96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要找到y值:正如您注意到的，weights有三个参数，weight[0]是截距，weight[1]是与class1相关联的参数，weight[2]是与class2相关联的参数</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="38a2" class="ld je hi kz b fi le lf l lg lh">y_values = - (weight[0] + weight[1]*x_values.T) / weight[2]</span></pre><p id="b5b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我有史以来第一篇关于媒介的文章，我希望我的想法被传达在纸上，如果有我解释不正确的地方，请让我知道并提出意见和建议。我会很乐意接受它。</p><h1 id="4622" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">结束</h1><p id="c13d" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">完整代码可以在<a class="ae lu" href="https://github.com/Darshansol9/GDvSGD_Python/" rel="noopener ugc nofollow" target="_blank">https://github.com/Darshansol9/</a>T2【GD-SGD _ from scratch _ Python找到</p><p id="4e05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢我的文章，我很感激放弃投票，并尽可能多地分享。</p></div></div>    
</body>
</html>