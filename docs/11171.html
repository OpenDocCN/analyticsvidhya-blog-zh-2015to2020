<html>
<head>
<title>Object Detection With OpenCV: Step by Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用OpenCV进行对象检测:一步一步</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/object-detection-with-opencv-step-by-step-6c49a9cc1ff0?source=collection_archive---------2-----------------------#2020-11-21">https://medium.com/analytics-vidhya/object-detection-with-opencv-step-by-step-6c49a9cc1ff0?source=collection_archive---------2-----------------------#2020-11-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0d98ae0cd5b7521d6a36feacced1a493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e2Yk8wOqDNzGYVJRvvhSbQ.jpeg"/></div></div></figure><p id="676f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">几个月前，我决定使用keras创建一个图像分类模型来检测武器等威胁。我现在决定把这个扩展到物体检测。</p><p id="c960" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">像这样的工具的目的是能够使用相机系统实时检测物体。</p><h2 id="1c06" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">目标检测与图像分类</h2><p id="5e65" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">在我们开始之前，我会稍微假设你已经知道物体检测和图像分类之间的区别，但这将作为一个快速回顾。</p><p id="76fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">图像分类</strong>是通过您的模型使用图像作为您的输入的过程，该模型检测给定图像中的相似性，以获得您想要的类别的输出。这将导致你的类名和概率分数的输出。</p><p id="6079" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">物体检测</strong>是通过模型使用图像和/或视频作为输入的过程，该模型检测任何物体。许多不同的对象检测方法都会发生这种情况。这将产生边界框、类名和概率分数的输出。</p><h2 id="9fac" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">走向</h2><p id="5bca" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">将会对将要使用的一些方法进行简要的解释，因为我不会对某些方法进行太多的详细说明，因为当涉及到对象检测时，你可以有许多关于一个主题/方法的博客。</p><p id="9e14" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我将一步一步地提供我的过程，但我不会覆盖我使用的已经建立的神经网络，它是从零开始创建的。也许我会在我为这个项目创建的神经网络上写另一篇博客。</p><p id="6eeb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个博客和项目的主要目标是使用真实世界的数据集/问题展示一个非常基本的对象检测形式。</p><h2 id="4a76" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">正在使用的数据</h2><p id="52ef" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">图像总数:3000</p><p id="adc3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">类别数:3 : {“突击步枪”:0，“手枪”:1，“无火器”:2}</p><p id="0cc2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">预先存在的神经网络:是</p><h2 id="acb2" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">进口</h2><p id="9a76" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">我使用的大部分进口产品来自tensorflow和keras。这些库将帮助加载我预先存在的卷积神经网络，并处理将用于通过对象检测模型的图像。</p><p id="8fc2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">OpenCV将是用于对象检测的库。</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="6b3d" class="jo jp hi kt b fi kx ky l kz la"># Neural Network<br/>from tensorflow.keras.applications import imagenet_utils<br/>from keras.preprocessing.image import load_img, img_to_array<br/>from tensorflow.keras.applications.mobilenet_v2 import preprocess_input<br/>from tensorflow.keras.utils import to_categorical<br/>from sklearn.preprocessing import LabelBinarizer<br/>from tensorflow.keras.models import load_model<br/>import keras<br/>import tensorflow as tf</span><span id="8396" class="jo jp hi kt b fi lb ky l kz la"># For measuring the inference time.<br/>import time<br/>import random</span><span id="57b5" class="jo jp hi kt b fi lb ky l kz la"># Computer Vision<br/>import cv2</span><span id="11fb" class="jo jp hi kt b fi lb ky l kz la"># Graphing<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="9e8f" class="jo jp hi kt b fi lb ky l kz la"># Math<br/>import numpy as np</span><span id="5796" class="jo jp hi kt b fi lb ky l kz la"># File handling<br/>import pickle<br/>from os import listdir<br/>from os.path import isfile, join<br/>import os</span></pre><h2 id="3b42" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">功能</h2><p id="d206" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">我将列出一些在整个项目中使用或可以使用的非常简单明了的函数，并且我将对与对象检测有直接联系的函数进行解释。</p><p id="1ddb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一个函数只是使用matplotlib显示图像:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="9616" class="jo jp hi kt b fi kx ky l kz la">def display_image(image_num):<br/>    """<br/>    Prints out picture of<br/>    the image that is selected<br/>    <br/>    After the else statement,<br/>    we try to use the file name<br/>    in oder to print out the image,<br/>    <br/>    while the first image is used to <br/>    print the image directly without a <br/>    filename.<br/>    """<br/>    try:<br/>        fig = plt.figure(figsize=(20, 15))<br/>        plt.grid(False)<br/>        plt.imshow(images[image_num])<br/>    except (RuntimeError, TypeError, NameError):<br/>        print("[INFO] Could not print image")<br/>        print("[INFO] trying something else...")<br/>    else:<br/>        print("[INFO] returning image...")<br/>        # Image path - getting images on file<br/>        image_paths = "Demo-Images/"<br/>        image_select = image_paths + ([f for f in listdir(image_paths) if isfile(join(image_paths, f))][image_num])  # Other way instead of the listdir function<br/>        img = plt.imread(fname=image_select)<br/>        plt.imshow(img)</span></pre><p id="444e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意:您的image_paths将取决于您对保存图像的目录的命名。</p><p id="524c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第二个函数用于预测您的输入图像，为您提供类别名称(突击步枪、手枪、无火器)和概率得分的输出:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="e007" class="jo jp hi kt b fi kx ky l kz la"># Prediction Function<br/>def predict(model, image_num):<br/>    # Image path - getting images on file<br/>    image_paths = "Demo-Images/"<br/>    image_select = image_paths + ([f for f in listdir(image_paths) if isfile(join(image_paths, f))])[image_num] # Other way instead of the listdir function<br/>    <br/>    img = load_img(image_select, target_size=(300, 300))  # Loading image<br/>    img = img_to_array(img)  # Transforming image to array<br/>    img = img / 255  # Normalizing Image<br/>    img = np.expand_dims(img, axis=0)  # Expanding dimensions<br/>    predict = cnn_model.predict(img)  # Predicting the image<br/>    pred_name = classes[np.argmax(predict)]  # Predicting the name<br/>    prediction = str(round(predict.max() * 100, 3))<br/>    print(display_image(image_num=image_num))<br/>    return prediction + '%', pred_name</span></pre><p id="22f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意:这是加载你预先存在的神经网络，并给你输入图像的结果。基于您从头开始构建CNN的方式，您将拥有不同的target_size参数值。</p><p id="82e4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第三个函数从根本上执行与上面的函数相同的任务，但是有一点小小的变化。第三个函数预测区域提议/边界框:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="ded5" class="jo jp hi kt b fi kx ky l kz la"># Prediction Function<br/>def predict_region_of_interest(model, proposals_used):<br/>    """<br/>    predicts region proposals<br/>    """<br/>    predict = model.predict(proposals_used)  # Predicting the image<br/>    for proposals in predict:<br/>        pred_name = classes[np.argmax(proposals)]  # Predicting the name<br/>    prediction = str(round(predict.max() * 100, 3))<br/>    print(pred_name)<br/>    return predict</span></pre><p id="d9db" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第四个函数计算IoU(交集/并集),这实质上是我们的对象检测模型的性能测量。IoU查看通过您的对象检测方法找到的建议的预测边界框/区域:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="e4a2" class="jo jp hi kt b fi kx ky l kz la">def compute_iou(boxA, boxB):<br/>    """<br/>    IOU is a form of <br/>    performance measurement<br/>    for our object detector.<br/>    """<br/>    # determine the (x, y)-coordinates of the intersection rectangle<br/>    xA = max(boxA[0], boxB[0])<br/>    yA = max(boxA[1], boxB[1])<br/>    xB = min(boxA[2], boxB[2])<br/>    yB = min(boxA[3], boxB[3])<br/>    # compute the area of intersection rectangle<br/>    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)<br/>    # compute the area of both the prediction and ground-truth<br/>    # rectangles<br/>    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)<br/>    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)<br/>    # compute the intersection over union by taking the intersection<br/>    # area and dividing it by the sum of prediction + ground-truth<br/>    # areas - the intersection area<br/>    iou = interArea / float(boxAArea + boxBArea - interArea)<br/>    # return the intersection over union value<br/>    return iou</span></pre><p id="2db5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第五个也是最后一个函数叫做非最大值抑制(NMS ),它清理边界框以返回概率最高的边界框:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/f1f76f3ee0dbc227c1cc1c77d0dc5dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Me9PqpjFGoL03G5l7qU6NA.png"/></div></div></figure><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="8a8f" class="jo jp hi kt b fi kx ky l kz la">#  Felzenszwalb et al.<br/>def non_max_suppression(boxes, overlapThresh):<br/>    # if there are no boxes, return an empty list<br/>    if len(boxes) == 0:<br/>        return []<br/>    # initialize the list of picked indexes<br/>    pick = []<br/>    # grab the coordinates of the bounding boxes<br/>    x1 = boxes[:,0]<br/>    y1 = boxes[:,1]<br/>    x2 = boxes[:,2]<br/>    y2 = boxes[:,3]<br/>    # compute the area of the bounding boxes and sort the bounding<br/>    # boxes by the bottom-right y-coordinate of the bounding box<br/>    area = (x2 - x1 + 1) * (y2 - y1 + 1)<br/>    idxs = np.argsort(y2)</span><span id="f82a" class="jo jp hi kt b fi lb ky l kz la"># keep looping while some indexes still remain in the indexes<br/>    # list<br/>    while len(idxs) &gt; 0:<br/>        # grab the last index in the indexes list, add the index<br/>        # value to the list of picked indexes, then initialize<br/>        # the suppression list (i.e. indexes that will be deleted)<br/>        # using the last index<br/>        last = len(idxs) - 1<br/>        i = idxs[last]<br/>        pick.append(i)<br/>        suppress = [last]</span><span id="a1ee" class="jo jp hi kt b fi lb ky l kz la"># loop over all indexes in the indexes list<br/>        for pos in range(0, last):<br/>            # grab the current index<br/>            j = idxs[pos]<br/>            # find the largest (x, y) coordinates for the start of<br/>            # the bounding box and the smallest (x, y) coordinates<br/>            # for the end of the bounding box<br/>            xx1 = max(x1[i], x1[j])<br/>            yy1 = max(y1[i], y1[j])<br/>            xx2 = min(x2[i], x2[j])<br/>            yy2 = min(y2[i], y2[j])<br/>            # compute the width and height of the bounding box<br/>            w = max(0, xx2 - xx1 + 1)<br/>            h = max(0, yy2 - yy1 + 1)<br/>            # compute the ratio of overlap between the computed<br/>            # bounding box and the bounding box in the area list<br/>            overlap = float(w * h) / area[j]<br/>            # if there is sufficient overlap, suppress the<br/>            # current bounding box<br/>            if overlap &gt; overlapThresh:<br/>                suppress.append(pos)<br/>        # delete all indexes from the index list that are in the<br/>        # suppression list<br/>        idxs = np.delete(idxs, suppress)<br/>    # return only the bounding boxes that were picked<br/>    return boxes[pick]</span></pre><h2 id="b35e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">选择性搜索</h2><p id="f94f" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">现在我们可以进入是什么让你的对象检测运行的话题。我们将使用选择性搜索这一主要模式来检索区域提案。</p><p id="1cc2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">选择性搜索是一种自动区域提议算法。它的工作原理是使用超像素算法对图像进行过度分割，特别是Felzenszwalb的超像素算法。从那里，选择性搜索寻求将超像素合并在一起，以找到图像中可能包含物体的区域。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/9e588a41ee8741d9b06d8c9d2109f113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HbDAcxIuUZKXsZ9U0hyrAQ.png"/></div></div></figure><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="05d7" class="jo jp hi kt b fi kx ky l kz la"># Setting a max amount of region proposals used when running selective search<br/>max_proposals = 2_000<br/>max_proposals_infer = 100  # Search for (1) gathering training data and (2) performing inference</span></pre><p id="53ee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于max_proposals_infer，您可以在图像中允许更多的区域提议以获得更好的结果。</p><p id="0df1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下一个代码将加载我们预先存在的卷积神经网络:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="76bc" class="jo jp hi kt b fi kx ky l kz la"># initialize the input dimensions to the network<br/>input_dimensions = (300, 300)  # 300 by 300 because that's what the CNN Model was tested on<br/># define the path to the output model<br/>model_path = "model_3.hdf5"<br/>cnn_model = keras.models.load_model(model_path)  # Loading CNN model from keras</span><span id="147f" class="jo jp hi kt b fi lb ky l kz la"># define the minimum probability required for a positive prediction<br/># (used to filter out false-positive predictions)<br/>min_probability = 0.90</span></pre><p id="1cf7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的代码将介绍我们OpenCV库中的选择性搜索类:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="6cd8" class="jo jp hi kt b fi kx ky l kz la"># initialize OpenCV's selective search implementation and set the<br/>ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()</span></pre><p id="261a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在接下来的代码行中，我将从我选择的目录中选择一个图像，并将其设置为我们的选择性搜索算法可以检测区域建议的基础图像:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="d1ff" class="jo jp hi kt b fi kx ky l kz la"># Input image in selective search</span><span id="00bc" class="jo jp hi kt b fi lb ky l kz la"># Image path - getting images on file<br/>image_num = 232<br/>image_paths = "Demo-Images/"<br/>image_select = image_paths + ([f for f in listdir(image_paths) if isfile(join(image_paths, f))][image_num])  # Other way instead of the listdir function</span><span id="8a59" class="jo jp hi kt b fi lb ky l kz la"># Making image compatible with imshow<br/>image = cv2.imread(image_select)</span><span id="bd4b" class="jo jp hi kt b fi lb ky l kz la"># load the input image (300x300) and preprocess it<br/>image = cv2.resize(image, input_dimensions)  # Increasing image means more regions</span><span id="8fa9" class="jo jp hi kt b fi lb ky l kz la"># Setting base image that will be used<br/>ss.setBaseImage(image)</span><span id="f72e" class="jo jp hi kt b fi lb ky l kz la"># Choosing which selective search<br/>ss.switchToSelectiveSearchQuality()</span></pre><p id="864a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，我使用上面的函数来显示图像，看看我们得到了什么:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/ac21418a4452eb924869ccfc627bc1a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XXim9T_vG1WEfOYCoc5_2g.png"/></div></div></figure><p id="1a16" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在下面的代码中，我们将运行算法来获取图像9中的区域:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="1a9f" class="jo jp hi kt b fi kx ky l kz la"># run selective search on the input image<br/>start = time.time()<br/>rects = ss.process()  # Run Selective Search<br/>end = time.time()</span><span id="ca7f" class="jo jp hi kt b fi lb ky l kz la"># show how along selective search took to run along with the total<br/># number of returned region proposals<br/>print(f"[INFO] selective search took {np.round(end - start, decimals=3)} seconds")<br/>print(f"[INFO] {len(rects)} total region proposals")</span></pre><p id="9e2a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的代码将向我们显示该算法选取的所有区域建议:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="25c5" class="jo jp hi kt b fi kx ky l kz la"># initialize the list of region proposals that we'll be classifying<br/># along with their associated bounding boxes<br/>proposals = []<br/>boxes = []<br/># loop over the region proposal bounding box coordinates generated by<br/># running selective search<br/>for (x, y, w, h) in rects[:max_proposals_infer]:<br/>    # extract the region from the input image, convert it from BGR to<br/>    # RGB channel ordering, and then resize it to the required input<br/>    # dimensions of our trained CNN<br/>    roi = image[y:y + h, x:x + w]<br/>    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)<br/>    roi = cv2.resize(roi, input_dimensions,<br/>        interpolation=cv2.INTER_CUBIC)<br/>    # further preprocess the ROI<br/>    roi = img_to_array(roi)<br/>    roi = preprocess_input(roi)<br/>    # update our proposals and bounding boxes lists<br/>    proposals.append(roi)<br/>    boxes.append((x, y, x + w, y + h))</span></pre><p id="8462" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的代码将向我们展示建议和边界框。我还使用了predict_region_of_interest函数来预测最有可能成为我们类中的对象的区域:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="660d" class="jo jp hi kt b fi kx ky l kz la"># convert the proposals and bounding boxes into NumPy arrays<br/>proposals = np.array(proposals, dtype="float64")<br/>boxes = np.array(boxes, dtype="int64")<br/>print(f"[INFO] proposal shape: {proposals.shape}")</span><span id="3468" class="jo jp hi kt b fi lb ky l kz la"># classify each of the proposal ROIs using fine-tuned model<br/>print("[INFO] classifying proposals...")<br/>proba = predict_region_of_proposals(model=cnn_model, proposals_used=proposals)  # Predicting the proposals for our desired object<br/>                                      # Result: 100 proposals 300 by 300 image with RGB color<br/># Probabilty of each proposal (Region of Proposals)<br/>print(f"[INFO] Probabiltiy Scores: {proba}")</span></pre><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/254c590bcf46f3b88e9d65179bf28f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R6kgfXiL2lGkMxi-WoqcSg.png"/></div></div></figure><p id="4e05" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面几行代码将过滤掉我们想要看到的预测/边界框。它将显示我们所选图像周围的边界框，该边界框具有我们进一步设置的最高/最低概率:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="0c7e" class="jo jp hi kt b fi kx ky l kz la"># Obtaining the label of the current prediction from the CNN<br/># Empty list to store proposals<br/>proposal_name_list = []<br/>for proposals in proba:<br/>    """<br/>    For each predicted proposal<br/>    attach the class names and <br/>    append it to a list.<br/>    """<br/>    pred_name = classes[np.argmax(proposals)]<br/>    proposal_name_list.append(pred_name)</span><span id="231c" class="jo jp hi kt b fi lb ky l kz la"># find the index of all predictions that are greater<br/># than the minimum probability<br/>print("[INFO] applying NMS...")</span><span id="1eeb" class="jo jp hi kt b fi lb ky l kz la"># Find the indexs where the main prediction label matches the overall image<br/># Get the index of the proposal that has the same class name as the overall image<br/>idxs = [i for i, x in enumerate(proposal_name_list) if x == pred_name]</span><span id="1ee2" class="jo jp hi kt b fi lb ky l kz la">boxes = boxes[idxs]<br/>proba = proba[idxs]</span><span id="7d39" class="jo jp hi kt b fi lb ky l kz la"># further filter indexes by enforcing a minimum prediction<br/>idxs = np.where(proba &gt;= min_probability)[0]<br/># probability be met<br/>boxes = boxes[idxs]<br/>proba = proba[idxs]</span></pre><h2 id="f3ba" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">查看我们的结果</h2><p id="b255" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">现在最后一部分是查看我们的结果。还可以使用plt.imshow()打印Jupyter Notebook中的图像。</p><p id="0be3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">前几行代码将向您展示我们的对象检测模型在不使用非最大值抑制算法的情况下运行图像后的图像:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="4efe" class="jo jp hi kt b fi kx ky l kz la"># clone the original image so that we can draw on it<br/>clone = image.copy()<br/># loop over the bounding boxes and associated probabilities<br/>for (box, prob) in zip(boxes, proba):<br/>    # draw the bounding box, label, and probability on the image<br/>    (startX, startY, endX, endY) = box<br/>    cv2.rectangle(clone, (startX, startY), (endX, endY),<br/>        (0, 255, 0), 2)<br/>    y = startY - 10 if startY - 10 &gt; 10 else startY + 10<br/>    text = f"{np.round(prob * 100, decimals=3)}%"<br/>    cv2.putText(clone, text, (startX, y),<br/>        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)<br/># show the output after *before* running NMS<br/>cv2.imshow("Before NMS", clone)<br/>cv2.waitKey(0);</span></pre><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/bb056b48c6bba97ae6789c77e089be80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eRiGGExwbulp36gEZiOUTQ.png"/></div></div></figure><p id="0393" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来的几行代码将向我们展示对象检测算法对所选图像做了什么，包括非最大抑制函数，使我们的算法返回单个边界框:</p><pre class="ko kp kq kr fd ks kt ku kv aw kw bi"><span id="9e19" class="jo jp hi kt b fi kx ky l kz la"># run non-maxima suppression on the bounding boxes<br/>boxIdxs = non_max_suppression_fast(boxes=boxes, overlapThresh=0.5)<br/># loop over the bounding box indexes<br/>for i in boxIdxs:<br/>    # draw the bounding box, label, and probability on the image<br/>    (startX, startY, endX, endY) = i # or boxes[0] will return 1 bb<br/>    cv2.rectangle(image, (startX, startY), (endX, endY),<br/>        (0, 255, 0), 2)<br/>    y = startY - 10 if startY - 10 &gt; 10 else startY + 10<br/>    text = f"{classes[np.argmax(prob)]}: {np.round(proba.max() * 100, decimals=1)}%"<br/>    cv2.putText(image, text, (startX, y),<br/>        cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)<br/># show the output image *after* running NMS<br/>cv2.imshow("After NMS", image)<br/>cv2.waitKey(0);</span></pre><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/226711db2a82433e60b7eb01fd8a61cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h1j0ShGV67iCi6sbcsRpBA.png"/></div></div></figure><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/9fff8aea5bc4614ffdeafe27847288e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zv-dmFDEr60h8vUDEZtfLw.png"/></div></div></figure><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/802e2811526a793e0a9dc25b3550c685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vcus9nKEZnDW_AijJZTMuQ.png"/></div></div></figure></div></div>    
</body>
</html>