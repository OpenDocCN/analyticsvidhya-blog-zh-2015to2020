<html>
<head>
<title>Quora Insincere Question Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Quora不真诚问题分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/quora-insincere-question-classification-eda-41add82a2d0b?source=collection_archive---------5-----------------------#2019-10-01">https://medium.com/analytics-vidhya/quora-insincere-question-classification-eda-41add82a2d0b?source=collection_archive---------5-----------------------#2019-10-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="92bb" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">检测有毒内容以改善在线对话</h2></div><p id="5b16" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇博客中，我将解释我如何通过机器学习和深度学习方法对Quora虚假问题数据集进行分类。</p><h1 id="e372" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated"><strong class="ak">问题概述:</strong></h1><p id="09e0" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">如今，任何主要网站都面临一个生存问题，那就是如何处理有毒和分裂性的内容。Quora希望正面解决这个问题，让他们的平台成为一个用户可以放心地与世界分享知识的地方。</p><p id="8f22" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae kq" href="https://www.quora.com/" rel="noopener ugc nofollow" target="_blank"> Quora </a>是一个让人们能够相互学习的平台。在Quora上，人们可以提出问题，并与贡献独特见解和高质量答案的其他人联系。一个关键的挑战是剔除不真诚的问题——那些建立在错误前提上的问题，或者那些旨在陈述而不是寻找有用答案的问题。</p><p id="b52e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Quora提出了一个竞赛，我们开发模型来识别和标记不真诚的问题。</p><h1 id="ed8b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">问题陈述:</h1><p id="1e7c" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">预测Quora上问的一个问题是否真诚。</p><h1 id="df61" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">评估指标:</h1><p id="8a0e" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">指标是预测目标和观察目标之间的F1分数。只有两个阶层，但积极阶层仅占总数的6%多一点。因此，目标是高度不平衡的，这就是为什么像F1这样的指标似乎适合这种问题，因为它考虑了测试的精确度和召回率来计算分数。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es kr"><img src="../Images/1daefdd0574445c5f02f2ed51a4eddce.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*B0UVzVktFU9uQ1t6_hfN-A.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated"><a class="ae kq" href="https://stackoverflow.com/questions/35365007/tensorflow-precision-recall-f1-score-and-confusion-matrix" rel="noopener ugc nofollow" target="_blank"> F </a> 1分(<a class="ae kq" href="https://stackoverflow.com/questions/35365007/tensorflow-precision-recall-f1-score-and-confusion-matrix" rel="noopener ugc nofollow" target="_blank">环节</a>)</figcaption></figure><h1 id="85d8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">数据概述:</h1><p id="471d" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">Quora提供了大量的训练和测试数据来识别不真诚的问题。训练数据由130万行和3个特征组成。</p><h1 id="b7bb" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">文件描述</h1><ul class=""><li id="2084" class="ld le hi iz b ja kl jd km jg lf jk lg jo lh js li lj lk ll bi translated">train.csv训练集</li><li id="2b8d" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated">test.csv —测试集</li><li id="394f" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated">嵌入</li></ul><h1 id="a9e0" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">数据字段</h1><ul class=""><li id="f52e" class="ld le hi iz b ja kl jd km jg lf jk lg jo lh js li lj lk ll bi translated">qid —唯一的问题标识符</li><li id="7e95" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated">问题_文本— Quora问题文本</li><li id="d0b6" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated">目标—标记为“不真诚”的问题的值为1，否则为0</li></ul><h1 id="b16f" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">探索性数据分析:</h1><h2 id="ede0" class="lr ju hi bd jv ls lt lu jz lv lw lx kd jg ly lz kf jk ma mb kh jo mc md kj me bi translated">负载训练和测试数据集:</h2><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="2d21" class="lr ju hi mg b fi mk ml l mm mn">train = pd.read_csv(“train.csv”)<br/>test=pd.read_csv(‘test.csv’)<br/>print(“Number of train data points:”,train.shape[0])<br/>print(“Number of test data points:”,test.shape[0])<br/>print(“Shape of Train Data:”, train.shape)<br/>print(“Shape of Test Data:”, test.shape)<br/>train.head()</span></pre><p id="cfef" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，加载训练和测试数据集。在这里，我也检查一组中的形状和数据点。</p><h2 id="bb1a" class="lr ju hi bd jv ls lt lu jz lv lw lx kd jg ly lz kf jk ma mb kh jo mc md kj me bi translated">数据点在输出类中的分布:</h2><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es mo"><img src="../Images/6b11a86085e587900dc21e4d6a45a529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSneK9jxOI58zMYwyWAzug.png"/></div></div></figure><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="8edb" class="lr ju hi mg b fi mk ml l mm mn">print(‘~&gt; Percentage of Sincere Questions (is_duplicate = 0):\n {}%’.format(100 — round(train[‘target’].mean()*100, 2)))<br/>print(‘\n~&gt; Percentage of Insincere Questions (is_duplicate = 1):\n {}%’.format(round(train[‘target’].mean()*100, 2)))</span><span id="2074" class="lr ju hi mg b fi mt ml l mm mn">#Output<br/>~&gt; Percentage of Sincere Questions (is_duplicate = 0):<br/>   93.81%<br/><br/>~&gt; Percentage of Insincere Questions (is_duplicate = 1):<br/>   6.19%</span></pre><p id="7878" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到，数据集是高度不平衡的，只有6.19%的无诚意问题。</p><h2 id="ad58" class="lr ju hi bd jv ls lt lu jz lv lw lx kd jg ly lz kf jk ma mb kh jo mc md kj me bi translated">基本特征工程:</h2><p id="07a2" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">我们可以添加一些功能，作为Quora虚假问题分类挑战的功能工程管道的一部分。</p><p id="da0f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面列出了我所包含的一些功能:</p><ul class=""><li id="f990" class="ld le hi iz b ja jb jd je jg mu jk mv jo mw js li lj lk ll bi translated"><strong class="iz hj">freq _ qid</strong>= qid的频率</li><li id="dd40" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated"><strong class="iz hj">qlen</strong>= qid长度</li><li id="f7de" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated"><strong class="iz hj"> n_words </strong> =问题字数</li><li id="a6cb" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated"><strong class="iz hj">数字字数</strong> =所讨论的数字字数</li><li id="704b" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated"><strong class="iz hj"> sp_char_words </strong> =有问题的特殊字符数</li><li id="5d4e" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated"><strong class="iz hj">唯一单词</strong> =问题中唯一单词的数量</li><li id="cc9a" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js li lj lk ll bi translated"><strong class="iz hj"> char_words </strong> =问题中的字符数</li></ul><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="6ada" class="lr ju hi mg b fi mk ml l mm mn">#Feature Engineering on Train Data<br/>train[‘freq_qid’] = train.groupby(‘qid’)[‘qid’].transform(‘count’) <br/>train[‘qlen’] = train[‘question_text’].str.len() <br/>train[‘n_words’] = train[‘question_text’].apply(lambda row: len(row.split(“ “)))<br/>train[‘numeric_words’] = train[‘question_text’].apply(lambda row: sum(c.isdigit() for c in row))<br/>train[‘sp_char_words’] = train[‘question_text’].str.findall(r’[^a-zA-Z0–9 ]’).str.len()<br/>train[‘char_words’] = train[‘question_text’].apply(lambda row: len(str(row)))<br/>train[‘unique_words’] = train[‘question_text’].apply(lambda row: len(set(str(row).split())))</span><span id="a08b" class="lr ju hi mg b fi mt ml l mm mn">#Feature Engineering on Test Data<br/>test[‘freq_qid’] = test.groupby(‘qid’)[‘qid’].transform(‘count’) <br/>test[‘qlen’] = test[‘question_text’].str.len() <br/>test[‘n_words’] = test[‘question_text’].apply(lambda row: len(row.split(“ “)))<br/>test[‘numeric_words’] = test[‘question_text’].apply(lambda row: sum(c.isdigit() for c in row))<br/>test[‘sp_char_words’] = test[‘question_text’].str.findall(r’[^a-zA-Z0–9 ]’).str.len()<br/>test[‘char_words’] = test[‘question_text’].apply(lambda row: len(str(row)))<br/>test[‘unique_words’] = test[‘question_text’].apply(lambda row: len(set(str(row).split())))</span></pre><p id="82d7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我添加了上面提到的特征，因为它们将帮助我们更好地评估我们的数据，以确定哪些特征是有用的，哪些是要丢弃/删除的。</p><h1 id="6be1" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">数据预处理:</h1><p id="31cd" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">文本数据并不完全干净，因此我们需要应用一些数据预处理技术。</p><p id="058a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据清理的预处理技术；</p><ol class=""><li id="a24c" class="ld le hi iz b ja jb jd je jg mu jk mv jo mw js mx lj lk ll bi translated">删除标点符号</li></ol><p id="ebaa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据中的特殊字符；我们将使用替换来删除这些字符</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="55c9" class="lr ju hi mg b fi mk ml l mm mn">puncts=[‘,’, ‘.’, ‘“‘, ‘:’, ‘)’, ‘(‘, ‘-’, ‘!’, ‘?’, ‘|’, ‘;’, “‘“, ‘$’, ‘&amp;’, ‘/’, ‘[‘, ‘]’, ‘&gt;’, ‘%’, ‘=’, ‘#’, ‘*’, ‘+’, ‘\\’, <br/> ‘•’, ‘~’, ‘@’, ‘£’, ‘·’, ‘_’, ‘{‘, ‘}’, ‘©’, ‘^’, ‘®’, ‘`’, ‘&lt;’, ‘→’, ‘°’, ‘€’, ‘™’, ‘›’, ‘♥’, ‘←’, ‘×’, ‘§’, ‘″’, ‘′’, <br/> ‘█’, ‘…’, ‘“‘, ‘★’, ‘”’, ‘–’, ‘●’, ‘►’, ‘−’, ‘¢’, ‘¬’, ‘░’, ‘¡’, ‘¶’, ‘↑’, ‘±’, ‘¿’, ‘▾’, ‘═’, ‘¦’, ‘║’, ‘―’, ‘¥’, ‘▓’, <br/> ‘ — ‘, ‘‹’, ‘─’, ‘▒’, ‘：’, ‘⊕’, ‘▼’, ‘▪’, ‘†’, ‘■’, ‘’’, ‘▀’, ‘¨’, ‘▄’, ‘♫’, ‘☆’, ‘¯’, ‘♦’, ‘¤’, ‘▲’, ‘¸’, ‘⋅’, ‘‘’, ‘∞’, <br/> ‘∙’, ‘）’, ‘↓’, ‘、’, ‘│’, ‘（’, ‘»’, ‘，’, ‘♪’, ‘╩’, ‘╚’, ‘・’, ‘╦’, ‘╣’, ‘╔’, ‘╗’, ‘▬’, ‘❤’, ‘≤’, ‘‡’, ‘√’, ‘◄’, ‘━’, <br/> ‘⇒’, ‘▶’, ‘≥’, ‘╝’, ‘♡’, ‘◊’, ‘。’, ‘✈’, ‘≡’, ‘☺’, ‘✔’, ‘<br/>’, ‘≈’, ‘✓’, ‘♣’, ‘☎’, ‘℃’, ‘◦’, ‘└’, ‘‟’, ‘～’, ‘！’, ‘○’, <br/> ‘◆’, ‘№’, ‘♠’, ‘▌’, ‘✿’, ‘▸’, ‘⁄’, ‘□’, ‘❖’, ‘✦’, ‘．’, ‘÷’, ‘｜’, ‘┃’, ‘／’, ‘￥’, ‘╠’, ‘↩’, ‘✭’, ‘▐’, ‘☼’, ‘☻’, ‘┐’, <br/> ‘├’, ‘«’, ‘∼’, ‘┌’, ‘℉’, ‘☮’, ‘฿’, ‘≦’, ‘♬’, ‘✧’, ‘〉’, ‘－’, ‘⌂’, ‘✖’, ‘･’, ‘◕’, ‘※’, ‘‖’, ‘◀’, ‘‰’, ‘\x97’, ‘↺’, <br/> ‘∆’, ‘┘’, ‘┬’, ‘╬’, ‘،’, ‘⌘’, ‘⊂’, ‘＞’, ‘〈’, ‘⎙’, ‘？’, ‘☠’, ‘⇐’, ‘▫’, ‘∗’, ‘∈’, ‘≠’, ‘♀’, ‘♔’, ‘˚’, ‘℗’, ‘┗’, ‘＊’, <br/> ‘┼’, ‘❀’, ‘＆’, ‘∩’, ‘♂’, ‘‿’, ‘∑’, ‘‣’, ‘➜’, ‘┛’, ‘⇓’, ‘☯’, ‘⊖’, ‘☀’, ‘┳’, ‘；’, ‘∇’, ‘⇑’, ‘✰’, ‘◇’, ‘♯’, ‘☞’, ‘´’, <br/> ‘↔’, ‘┏’, ‘｡’, ‘◘’, ‘∂’, ‘✌’, ‘♭’, ‘┣’, ‘┴’, ‘┓’, ‘✨’, ‘\xa0’, ‘˜’, ‘❥’, ‘┫’, ‘℠’, ‘✒’, ‘［’, ‘∫’, ‘\x93’, ‘≧’, ‘］’, <br/> ‘\x94’, ‘∀’, ‘♛’, ‘\x96’, ‘∨’, ‘◎’, ‘↻’, ‘⇩’, ‘＜’, ‘≫’, ‘✩’, ‘✪’, ‘♕’, ‘؟’, ‘₤’, ‘☛’, ‘╮’, ‘␊’, ‘＋’, ‘┈’, ‘％’, <br/> ‘╋’, ‘▽’, ‘⇨’, ‘┻’, ‘⊗’, ‘￡’, ‘।’, ‘▂’, ‘✯’, ‘▇’, ‘＿’, ‘➤’, ‘✞’, ‘＝’, ‘▷’, ‘△’, ‘◙’, ‘▅’, ‘✝’, ‘∧’, ‘␉’, ‘☭’, <br/> ‘┊’, ‘╯’, ‘☾’, ‘➔’, ‘∴’, ‘\x92’, ‘▃’, ‘↳’, ‘＾’, ‘׳’, ‘➢’, ‘╭’, ‘➡’, ‘＠’, ‘⊙’, ‘☢’, ‘˝’, ‘∏’, ‘„’, ‘∥’, ‘❝’, ‘☐’, <br/> ‘▆’, ‘╱’, ‘⋙’, ‘๏’, ‘☁’, ‘⇔’, ‘▔’, ‘\x91’, ‘➚’, ‘◡’, ‘╰’, ‘\x85’, ‘♢’, ‘˙’, ‘۞’, ‘✘’, ‘✮’, ‘☑’, ‘⋆’, ‘ⓘ’, ‘❒’, <br/> ‘☣’, ‘✉’, ‘⌊’, ‘➠’, ‘∣’, ‘❑’, ‘◢’, ‘ⓒ’, ‘\x80’, ‘〒’, ‘∕’, ‘▮’, ‘⦿’, ‘✫’, ‘✚’, ‘⋯’, ‘♩’, ‘☂’, ‘❞’, ‘‗’, ‘܂’, ‘☜’, <br/> ‘‾’, ‘✜’, ‘╲’, ‘∘’, ‘⟩’, ‘＼’, ‘⟨’, ‘·’, ‘✗’, ‘♚’, ‘∅’, ‘ⓔ’, ‘◣’, ‘͡’, ‘‛’, ‘❦’, ‘◠’, ‘✄’, ‘❄’, ‘∃’, ‘␣’, ‘≪’, ‘｢’, <br/> ‘≅’, ‘◯’, ‘☽’, ‘∎’, ‘｣’, ‘❧’, ‘̅’, ‘ⓐ’, ‘↘’, ‘⚓’, ‘▣’, ‘˘’, ‘∪’, ‘⇢’, ‘✍’, ‘⊥’, ‘＃’, ‘⎯’, ‘↠’, ‘۩’, ‘☰’, ‘◥’, <br/> ‘⊆’, ‘✽’, ‘⚡’, ‘↪’, ‘❁’, ‘☹’, ‘◼’, ‘☃’, ‘◤’, ‘❏’, ‘ⓢ’, ‘⊱’, ‘➝’, ‘̣’, ‘✡’, ‘∠’, ‘｀’, ‘▴’, ‘┤’, ‘∝’, ‘♏’, ‘ⓐ’, <br/> ‘✎’, ‘;’, ‘␤’, ‘＇’, ‘❣’, ‘✂’, ‘✤’, ‘ⓞ’, ‘☪’, ‘✴’, ‘⌒’, ‘˛’, ‘♒’, ‘＄’, ‘✶’, ‘▻’, ‘ⓔ’, ‘◌’, ‘◈’, ‘❚’, ‘❂’, ‘￦’, <br/> ‘◉’, ‘╜’, ‘̃’, ‘✱’, ‘╖’, ‘❉’, ‘ⓡ’, ‘↗’, ‘ⓣ’, ‘♻’, ‘➽’, ‘׀’, ‘✲’, ‘✬’, ‘☉’, ‘▉’, ‘≒’, ‘☥’, ‘⌐’, ‘♨’, ‘✕’, ‘ⓝ’, <br/> ‘⊰’, ‘❘’, ‘＂’, ‘⇧’, ‘̵’, ‘➪’, ‘▁’, ‘▏’, ‘⊃’, ‘ⓛ’, ‘‚’, ‘♰’, ‘́’, ‘✏’, ‘⏑’, ‘̶’, ‘ⓢ’, ‘⩾’, ‘￠’, ‘❍’, ‘≃’, ‘⋰’, ‘♋’, <br/> ‘､’, ‘̂’, ‘❋’, ‘✳’, ‘ⓤ’, ‘╤’, ‘▕’, ‘⌣’, ‘✸’, ‘℮’, ‘⁺’, ‘▨’, ‘╨’, ‘ⓥ’, ‘♈’, ‘❃’, ‘☝’, ‘✻’, ‘⊇’, ‘≻’, ‘♘’, ‘♞’, <br/> ‘◂’, ‘✟’, ‘⌠’, ‘✠’, ‘☚’, ‘✥’, ‘❊’, ‘ⓒ’, ‘⌈’, ‘❅’, ‘ⓡ’, ‘♧’, ‘ⓞ’, ‘▭’, ‘❱’, ‘ⓣ’, ‘∟’, ‘☕’, ‘♺’, ‘∵’, ‘⍝’, ‘ⓑ’, <br/> ‘✵’, ‘✣’, ‘٭’, ‘♆’, ‘ⓘ’, ‘∶’, ‘⚜’, ‘◞’, ‘்’, ‘✹’, ‘➥’, ‘↕’, ‘̳’, ‘∷’, ‘✋’, ‘➧’, ‘∋’, ‘̿’, ‘ͧ’, ‘┅’, ‘⥤’, ‘⬆’, ‘⋱’, <br/> ‘☄’, ‘↖’, ‘⋮’, ‘۔’, ‘♌’, ‘ⓛ’, ‘╕’, ‘♓’, ‘❯’, ‘♍’, ‘▋’, ‘✺’, ‘⭐’, ‘✾’, ‘♊’, ‘➣’, ‘▿’, ‘ⓑ’, ‘♉’, ‘⏠’, ‘◾’, ‘▹’, <br/> ‘⩽’, ‘↦’, ‘╥’, ‘⍵’, ‘⌋’, ‘։’, ‘➨’, ‘∮’, ‘⇥’, ‘ⓗ’, ‘ⓓ’, ‘⁻’, ‘⎝’, ‘⌥’, ‘⌉’, ‘◔’, ‘◑’, ‘✼’, ‘♎’, ‘♐’, ‘╪’, ‘⊚’, <br/> ‘☒’, ‘⇤’, ‘ⓜ’, ‘⎠’, ‘◐’, ‘⚠’, ‘╞’, ‘◗’, ‘⎕’, ‘ⓨ’, ‘☟’, ‘ⓟ’, ‘♟’, ‘❈’, ‘↬’, ‘ⓓ’, ‘◻’, ‘♮’, ‘❙’, ‘♤’, ‘∉’, ‘؛’, <br/> ‘⁂’, ‘ⓝ’, ‘־’, ‘♑’, ‘╫’, ‘╓’, ‘╳’, ‘⬅’, ‘☔’, ‘☸’, ‘┄’, ‘╧’, ‘׃’, ‘⎢’, ‘❆’, ‘⋄’, ‘⚫’, ‘̏’, ‘☏’, ‘➞’, ‘͂’, ‘␙’, <br/> ‘ⓤ’, ‘◟’, ‘̊’, ‘⚐’, ‘✙’, ‘↙’, ‘̾’, ‘℘’, ‘✷’, ‘⍺’, ‘❌’, ‘⊢’, ‘▵’, ‘✅’, ‘ⓖ’, ‘☨’, ‘▰’, ‘╡’, ‘ⓜ’, ‘☤’, ‘∽’, ‘╘’, <br/> ‘˹’, ‘↨’, ‘♙’, ‘⬇’, ‘♱’, ‘⌡’, ‘⠀’, ‘╛’, ‘❕’, ‘┉’, ‘ⓟ’, ‘̀’, ‘♖’, ‘ⓚ’, ‘┆’, ‘⎜’, ‘◜’, ‘⚾’, ‘⤴’, ‘✇’, ‘╟’, ‘⎛’, <br/> ‘☩’, ‘➲’, ‘➟’, ‘ⓥ’, ‘ⓗ’, ‘⏝’, ‘◃’, ‘╢’, ‘↯’, ‘✆’, ‘˃’, ‘⍴’, ‘❇’, ‘⚽’, ‘╒’, ‘̸’, ‘♜’, ‘☓’, ‘➳’, ‘⇄’, ‘☬’, ‘⚑’, <br/> ‘✐’, ‘⌃’, ‘◅’, ‘▢’, ‘❐’, ‘∊’, ‘☈’, ‘॥’, ‘⎮’, ‘▩’, ‘ு’, ‘⊹’, ‘‵’, ‘␔’, ‘☊’, ‘➸’, ‘̌’, ‘☿’, ‘⇉’, ‘⊳’, ‘╙’, ‘ⓦ’, <br/> ‘⇣’, ‘｛’, ‘̄’, ‘↝’, ‘⎟’, ‘▍’, ‘❗’, ‘״’, ‘΄’, ‘▞’, ‘◁’, ‘⛄’, ‘⇝’, ‘⎪’, ‘♁’, ‘⇠’, ‘☇’, ‘✊’, ‘ி’, ‘｝’, ‘⭕’, ‘➘’, <br/> ‘⁀’, ‘☙’, ‘❛’, ‘❓’, ‘⟲’, ‘⇀’, ‘≲’, ‘ⓕ’, ‘⎥’, ‘\u06dd’, ‘ͤ’, ‘₋’, ‘̱’, ‘̎’, ‘♝’, ‘≳’, ‘▙’, ‘➭’, ‘܀’, ‘ⓖ’, ‘⇛’, ‘▊’, <br/> ‘⇗’, ‘̷’, ‘⇱’, ‘℅’, ‘ⓧ’, ‘⚛’, ‘̐’, ‘̕’, ‘⇌’, ‘␀’, ‘≌’, ‘ⓦ’, ‘⊤’, ‘̓’, ‘☦’, ‘ⓕ’, ‘▜’, ‘➙’, ‘ⓨ’, ‘⌨’, ‘◮’, ‘☷’, <br/> ‘◍’, ‘ⓚ’, ‘≔’, ‘⏩’, ‘⍳’, ‘℞’, ‘┋’, ‘˻’, ‘▚’, ‘≺’, ‘ْ’, ‘▟’, ‘➻’, ‘̪’, ‘⏪’, ‘̉’, ‘⎞’, ‘┇’, ‘⍟’, ‘⇪’, ‘▎’, ‘⇦’, ‘␝’, <br/> ‘⤷’, ‘≖’, ‘⟶’, ‘♗’, ‘̴’, ‘♄’, ‘ͨ’, ‘̈’, ‘❜’, ‘̡’, ‘▛’, ‘✁’, ‘➩’, ‘ா’, ‘˂’, ‘↥’, ‘⏎’, ‘⎷’, ‘̲’, ‘➖’, ‘↲’, ‘⩵’, ‘̗’, ‘❢’, <br/> ‘≎’, ‘⚔’, ‘⇇’, ‘̑’, ‘⊿’, ‘̖’, ‘☍’, ‘➹’, ‘⥊’, ‘⁁’, ‘✢’]</span><span id="683f" class="lr ju hi mg b fi mt ml l mm mn">def clean_punct(x):<br/> for punct in puncts:<br/> if punct in x:<br/> x = x.replace(punct, ‘{}’ .format(punct))<br/> return x</span></pre><p id="5504" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.清理数字</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="deb9" class="lr ju hi mg b fi mk ml l mm mn">def clean_numbers(x):<br/> if bool(re.search(r’\d’, x)):<br/> x = re.sub(‘[0–9]{5,}’, ‘#####’, x)<br/> x = re.sub(‘[0–9]{4}’, ‘####’, x)<br/> x = re.sub(‘[0–9]{3}’, ‘###’, x)<br/> x = re.sub(‘[0–9]{2}’, ‘##’, x)<br/> return x</span></pre><p id="7d8b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.纠正拼写错误的单词</p><p id="0a31" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了更好的嵌入覆盖率，我们将使用拼写错误映射和正则表达式函数替换拼写错误的单词。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="509d" class="lr ju hi mg b fi mk ml l mm mn">mispell_dict = {‘colour’: ‘color’, ‘centre’: ‘center’, ‘favourite’: ‘favorite’, ‘travelling’: ‘traveling’, ‘counselling’: ‘counseling’, ‘theatre’: ‘theater’, ‘cancelled’: ‘canceled’, ‘labour’: ‘labor’, ‘organisation’: ‘organization’, ‘wwii’: ‘world war 2’, ‘citicise’: ‘criticize’, ‘youtu ‘: ‘youtube ‘, ‘Qoura’: ‘Quora’, ‘sallary’: ‘salary’, ‘Whta’: ‘What’, ‘narcisist’: ‘narcissist’, ‘howdo’: ‘how do’, ‘whatare’: ‘what are’, ‘howcan’: ‘how can’, ‘howmuch’: ‘how much’, ‘howmany’: ‘how many’, ‘whydo’: ‘why do’, ‘doI’: ‘do I’, ‘theBest’: ‘the best’, ‘howdoes’: ‘how does’, ‘mastrubation’: ‘masturbation’, ‘mastrubate’: ‘masturbate’, “mastrubating”: ‘masturbating’, ‘pennis’: ‘penis’, ‘Etherium’: ‘bitcoin’, ‘narcissit’: ‘narcissist’, ‘bigdata’: ‘big data’, ‘2k17’: ‘2017’, ‘2k18’: ‘2018’, ‘qouta’: ‘quota’, ‘exboyfriend’: ‘ex boyfriend’, ‘airhostess’: ‘air hostess’, “whst”: ‘what’, ‘watsapp’: ‘whatsapp’, ‘demonitisation’: ‘demonetization’, ‘demonitization’: ‘demonetization’, ‘demonetisation’: ‘demonetization’, <br/>‘electroneum’:’bitcoin’,’nanodegree’:’degree’,’hotstar’:’star’,’dream11':’dream’,’ftre’:’fire’,’tensorflow’:’framework’,’unocoin’:’bitcoin’,‘lnmiit’:’limit’,’unacademy’:’academy’,’altcoin’:’bitcoin’,’altcoins’:’bitcoin’,’litecoin’:’bitcoin’,’coinbase’:’bitcoin’,’cryptocurency’:’cryptocurrency’,‘simpliv’:’simple’,’quoras’:’quora’,’schizoids’:’psychopath’,’remainers’:’remainder’,’twinflame’:’soulmate’,’quorans’:’quora’,’brexit’:’demonetized’,‘iiest’:’institute’,’dceu’:’comics’,’pessat’:’exam’,’uceed’:’college’,’bhakts’:’devotee’,’boruto’:’anime’,‘cryptocoin’:’bitcoin’,’blockchains’:’blockchain’,’fiancee’:’fiance’,’redmi’:’smartphone’,’oneplus’:’smartphone’,’qoura’:’quora’,’deepmind’:’framework’,’ryzen’:’cpu’,’whattsapp’:’whatsapp’,<br/>‘undertale’:’adventure’,’zenfone’:’smartphone’,’cryptocurencies’:’cryptocurrencies’,’koinex’:’bitcoin’,’zebpay’:’bitcoin’,’binance’:’bitcoin’,’whtsapp’:’whatsapp’,‘reactjs’:’framework’,’bittrex’:’bitcoin’,’bitconnect’:’bitcoin’,’bitfinex’:’bitcoin’,’yourquote’:’your quote’,’whyis’:’why is’,’jiophone’:’smartphone’,‘dogecoin’:’bitcoin’,’onecoin’:’bitcoin’,’poloniex’:’bitcoin’,’7700k’:’cpu’,’angular2':’framework’,’segwit2x’:’bitcoin’,’hashflare’:’bitcoin’,’940mx’:’gpu’,<br/>‘openai’:’framework’,’hashflare’:’bitcoin’,’1050ti’:’gpu’,’nearbuy’:’near buy’,’freebitco’:’bitcoin’,’antminer’:’bitcoin’,’filecoin’:’bitcoin’,’whatapp’:’whatsapp’,‘empowr’:’empower’,’1080ti’:’gpu’,’crytocurrency’:’cryptocurrency’,’8700k’:’cpu’,’whatsaap’:’whatsapp’,’g4560':’cpu’,’payymoney’:’pay money’,‘fuckboys’:’fuck boys’,’intenship’:’internship’,’zcash’:’bitcoin’,’demonatisation’:’demonetization’,’narcicist’:’narcissist’,’mastuburation’:’masturbation’,‘trignometric’:’trigonometric’,’cryptocurreny’:’cryptocurrency’,’howdid’:’how did’,’crytocurrencies’:’cryptocurrencies’,’phycopath’:’psychopath’,\‘bytecoin’:’bitcoin’,’possesiveness’:’possessiveness’,’scollege’:’college’,’humanties’:’humanities’,’altacoin’:’bitcoin’,’demonitised’:’demonetized’,‘brasília’:’brazilia’,’accolite’:’accolyte’,’econimics’:’economics’,’varrier’:’warrier’,’quroa’:’quora’,’statergy’:’strategy’,’langague’:’language’,‘splatoon’:’game’,’7600k’:’cpu’,’gate2018':’gate 2018',’in2018':’in 2018',’narcassist’:’narcissist’,’jiocoin’:’bitcoin’,’hnlu’:’hulu’,’7300hq’:’cpu’,‘weatern’:’western’,’interledger’:’blockchain’,’deplation’:’deflation’, ‘cryptocurrencies’:’cryptocurrency’, ‘bitcoin’:’blockchain cryptocurrency’,}</span><span id="d127" class="lr ju hi mg b fi mt ml l mm mn">def _get_mispell(mispell_dict):<br/> mispell_re = re.compile(‘(%s)’ % ‘|’.join(mispell_dict.keys()))<br/> return mispell_dict, mispell_re</span><span id="8e6b" class="lr ju hi mg b fi mt ml l mm mn">mispellings, mispellings_re = _get_mispell(mispell_dict)<br/>def replace_typical_misspell(text):<br/> def replace(match):<br/> return mispellings[match.group(0)]<br/> return mispellings_re.sub(replace, text)</span></pre><p id="154c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4.消除收缩</p><p id="18d6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">缩写是我们用撇号写的单词。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="f39d" class="lr ju hi mg b fi mk ml l mm mn">contraction_dict = {“ain’t”: “is not”, “aren’t”: “are not”,”can’t”: “cannot”, “‘cause”: “because”, “could’ve”: “could have”, “couldn’t”: “could not”, “didn’t”: “did not”, “doesn’t”: “does not”, “don’t”: “do not”, “hadn’t”: “had not”, “hasn’t”: “has not”, “haven’t”: “have not”, “he’d”: “he would”,”he’ll”: “he will”, “he’s”: “he is”, “how’d”: “how did”, “how’d’y”: “how do you”, “how’ll”: “how will”, “how’s”: “how is”, “I’d”: “I would”, “I’d’ve”: “I would have”, “I’ll”: “I will”, “I’ll’ve”: “I will have”,”I’m”: “I am”, “I’ve”: “I have”, “i’d”: “i would”, “i’d’ve”: “i would have”, “i’ll”: “i will”, “i’ll’ve”: “i will have”,”i’m”: “i am”, “i’ve”: “i have”, “isn’t”: “is not”, “it’d”: “it would”, “it’d’ve”: “it would have”, “it’ll”: “it will”, “it’ll’ve”: “it will have”,”it’s”: “it is”, “let’s”: “let us”, “ma’am”: “madam”, “mayn’t”: “may not”, “might’ve”: “might have”,”mightn’t”: “might not”,”mightn’t’ve”: “might not have”, “must’ve”: “must have”, “mustn’t”: “must not”, “mustn’t’ve”: “must not have”, “needn’t”: “need not”, “needn’t’ve”: “need not have”,”o’clock”: “of the clock”, “oughtn’t”: “ought not”, “oughtn’t’ve”: “ought not have”, “shan’t”: “shall not”, “sha’n’t”: “shall not”, “shan’t’ve”: “shall not have”, “she’d”: “she would”, “she’d’ve”: “she would have”, “she’ll”: “she will”, “she’ll’ve”: “she will have”, “she’s”: “she is”, “should’ve”: “should have”, “shouldn’t”: “should not”, “shouldn’t’ve”: “should not have”, “so’ve”: “so have”,”so’s”: “so as”, “this’s”: “this is”,”that’d”: “that would”, “that’d’ve”: “that would have”, “that’s”: “that is”, “there’d”: “there would”, “there’d’ve”: “there would have”, “there’s”: “there is”, “here’s”: “here is”,”they’d”: “they would”, “they’d’ve”: “they would have”, “they’ll”: “they will”, “they’ll’ve”: “they will have”, “they’re”: “they are”, “they’ve”: “they have”, “to’ve”: “to have”, “wasn’t”: “was not”, “we’d”: “we would”, “we’d’ve”: “we would have”, “we’ll”: “we will”, “we’ll’ve”: “we will have”, “we’re”: “we are”, “we’ve”: “we have”, “weren’t”: “were not”, “what’ll”: “what will”, “what’ll’ve”: “what will have”, “what’re”: “what are”, “what’s”: “what is”, “what’ve”: “what have”, “when’s”: “when is”, “when’ve”: “when have”, “where’d”: “where did”, “where’s”: “where is”, “where’ve”: “where have”, “who’ll”: “who will”, “who’ll’ve”: “who will have”, “who’s”: “who is”, “who’ve”: “who have”, “why’s”: “why is”, “why’ve”: “why have”, “will’ve”: “will have”, “won’t”: “will not”, “won’t’ve”: “will not have”, “would’ve”: “would have”, “wouldn’t”: “would not”, “wouldn’t’ve”: “would not have”, “y’all”: “you all”, “y’all’d”: “you all would”,”y’all’d’ve”: “you all would have”,”y’all’re”: “you all are”,”y’all’ve”: “you all have”,”you’d”: “you would”, “you’d’ve”: “you would have”, “you’ll”: “you will”, “you’ll’ve”: “you will have”, “you’re”: “you are”, “you’ve”: “you have”}</span><span id="cd9e" class="lr ju hi mg b fi mt ml l mm mn">def _get_contractions(contraction_dict):<br/> contraction_re = re.compile(‘(%s)’ % ‘|’.join(contraction_dict.keys()))<br/> return contraction_dict, contraction_re</span><span id="5c78" class="lr ju hi mg b fi mt ml l mm mn">contractions, contractions_re = _get_contractions(contraction_dict)</span><span id="47af" class="lr ju hi mg b fi mt ml l mm mn">def replace_contractions(text):<br/> def replace(match):<br/> return contractions[match.group(0)]<br/> return contractions_re.sub(replace, text)</span></pre><p id="1ead" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">5.删除停用词</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="bb50" class="lr ju hi mg b fi mk ml l mm mn">stopword_list = nltk.corpus.stopwords.words(‘english’)<br/>def remove_stopwords(text, is_lower_case=True):<br/> tokenizer = ToktokTokenizer()<br/> tokens = tokenizer.tokenize(text)<br/> tokens = [token.strip() for token in tokens]<br/> if is_lower_case:<br/> filtered_tokens = [token for token in tokens if token not in stopword_list]<br/> else:<br/> filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]<br/> filtered_text = ‘ ‘.join(filtered_tokens)<br/> return filtered_text</span></pre><p id="c3fe" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">6.堵塞物</p><p id="5a06" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">词干提取是使用粗略的启发式规则将单词转换成其基本形式的过程。例如，一个规则可以是从任何单词的末尾去掉“s”，这样“cats”就变成了“cat”。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="d7e0" class="lr ju hi mg b fi mk ml l mm mn">from nltk.stem import SnowballStemmer<br/>from nltk.tokenize.toktok import ToktokTokenizer<br/>def stem_text(text):<br/> tokenizer = ToktokTokenizer()<br/> stemmer = SnowballStemmer(‘english’)<br/> tokens = tokenizer.tokenize(text)<br/> tokens = [token.strip() for token in tokens]<br/> tokens = [stemmer.stem(token) for token in tokens]<br/> return ‘ ‘.join(tokens)</span></pre><p id="ddc7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">7.词汇化</p><p id="7225" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">词汇化与词干化非常相似，但它的目的是仅当字典中存在基本形式时才删除词尾。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="d8c6" class="lr ju hi mg b fi mk ml l mm mn">from nltk.stem import WordNetLemmatizer<br/>from nltk.tokenize.toktok import ToktokTokenizer<br/>wordnet_lemmatizer = WordNetLemmatizer()<br/>def lemma_text(text):<br/> tokenizer = ToktokTokenizer()<br/> tokens = tokenizer.tokenize(text)<br/> tokens = [token.strip() for token in tokens]<br/> tokens = [wordnet_lemmatizer.lemmatize(token) for token in tokens]<br/> return ‘ ‘.join(tokens)</span></pre><p id="c9b2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们处理完文本，我们将应用下面给出的步骤来清理训练和测试数据上的文本。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="c74a" class="lr ju hi mg b fi mk ml l mm mn">def clean_sentence(x):<br/> x = x.lower()<br/> x = clean_punct(x)<br/> x = clean_numbers(x)<br/> x = replace_typical_misspell(x)<br/> x = remove_stopwords(x)<br/> x = replace_contractions(x)<br/> #x = preprocess(x)<br/> x = stem_text(x)<br/> x = lemma_text(x)<br/> x = x.replace(“‘“,””)<br/> return x</span></pre><h1 id="b184" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">提取特征的分析:</h1><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es my"><img src="../Images/7d518fa3367e50be3353366e67e2657e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s5KTizjxSrmJN_BHsj0BBA.png"/></div></div></figure><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es mz"><img src="../Images/36928a4cee0bce44f26093e31f7e0a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CK5wX5esF6SDEzZJa7GHPg.png"/></div></div></figure><p id="f806" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从词云我们可以看到，穆斯林、特朗普、黑人、印度人等是在不真诚的问题中出现很多的词。</p><p id="3487" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在应用数据预处理和清理之后，我们的文本就可以进行分类了。我应用了传统和深度学习方法进行分类。</p><p id="0255" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们先来了解分类的机器学习方法。</p><h1 id="4fa4" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">机器学习方法</h1><h1 id="4815" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">高级NLP文本处理:</h1><h2 id="a7e4" class="lr ju hi bd jv ls lt lu jz lv lw lx kd jg ly lz kf jk ma mb kh jo mc md kj me bi translated">1.单词包(计数矢量器)</h2><p id="f207" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">单词包(BoW)指的是描述文本数据中单词存在的文本表示。这背后的直觉是，两个相似的文本字段将包含相似种类的单词，因此将具有相似的单词包。此外，仅从文本中，我们可以了解一些关于文件的含义。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="eeb9" class="lr ju hi mg b fi mk ml l mm mn">cnt_vectorizer = CountVectorizer(dtype=np.float32,<br/> strip_accents=’unicode’, analyzer=’word’,token_pattern=r’\w{1,}’,<br/> ngram_range=(1, 3),min_df=3)</span><span id="3ab4" class="lr ju hi mg b fi mt ml l mm mn"># Fitting count vectorizer to both training and test sets (semi-supervised learning)<br/>cnt_vectorizer.fit(list(train.preprocessed_question_text.values) + list(test.preprocessed_question_text.values))<br/>X_train = cnt_vectorizer.transform(train.preprocessed_question_text.values) <br/>X_test = cnt_vectorizer.transform(test.preprocessed_question_text.values)<br/>y_train = train.target.values</span></pre><p id="b51b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">CountVectorizer将文本文档的集合转换为令牌计数的矩阵。为此，我选择n-gram范围为1–3，min_df为3来构建词汇表。</p><p id="e0b9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们为逻辑回归、朴素贝叶斯和LightGBM等机器学习模型运行这些功能。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="d8ae" class="lr ju hi mg b fi mk ml l mm mn"># Fitting a simple Logistic Regression<br/>clf = LogisticRegression(C=1.0)<br/>clf.fit(X_train,y_train)</span></pre><h2 id="6355" class="lr ju hi bd jv ls lt lu jz lv lw lx kd jg ly lz kf jk ma mb kh jo mc md kj me bi translated">2.术语频率—反向文档频率(TF-IDF)</h2><p id="5959" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated"><strong class="iz hj">词频(tf) </strong>:给出这个词在语料库中每个文档中的出现频率。它是单词在文档中出现的次数与该文档中单词总数的比率。它随着该单词在文档中出现的次数的增加而增加。每个文档都有自己的tf。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es na"><img src="../Images/0154e8f539928c3eb6c9345b7917691d.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*KXQSFhCjKN5AZYfrvhhknQ.png"/></div></div></figure><p id="14b1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">逆数据频率(idf): </strong>用于计算语料库中所有文档中稀有词的权重。在语料库中很少出现的单词具有高IDF分数。它由下面的等式给出。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es nb"><img src="../Images/916aab6fecef961767c4aaee3ff236d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*YvZY0NM5jVEovkFbS0pJ1g.png"/></div></figure><p id="0e8a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结合这两者，我们得出语料库中文档中的单词的TF-IDF分数(w)。它是tf和idf的产物:</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es nc"><img src="../Images/fd5df4bd5b3271521ca766738cc80546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*edwDqATNnIwZQlm1YEr3EA.png"/></div></figure><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es nd"><img src="../Images/915abb5508a1760b03fc5f5689488466.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*le-WpL12rdPV4OR0T4XmIA.png"/></div></figure><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="2a3c" class="lr ju hi mg b fi mk ml l mm mn">tfv = TfidfVectorizer(dtype=np.float32, min_df=3, max_features=None, <br/> strip_accents=’unicode’, analyzer=’word’,token_pattern=r’\w{1,}’,<br/> ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,<br/> stop_words = ‘english’)</span><span id="9cff" class="lr ju hi mg b fi mt ml l mm mn"># Fitting TF-IDF to both training and test sets (semi-supervised learning)<br/>tfv.fit(list(train.preprocessed_question_text.values) + list(test.preprocessed_question_text.values))<br/>X_train = tfv.transform(train.preprocessed_question_text.values) <br/>X_test_tfv = tfv.transform(test.preprocessed_question_text.values)<br/>y_train = train.target.values</span></pre><p id="b6c4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">TfidfVectorizer将原始文档集合转换为TF-IDF特征矩阵。为此，n-gram范围也是1-3，min_df是3，以建立词汇。</p><p id="4a39" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们再次为逻辑回归、朴素贝叶斯和LightGBM等机器学习模型运行这些功能。</p><h2 id="e8d9" class="lr ju hi bd jv ls lt lu jz lv lw lx kd jg ly lz kf jk ma mb kh jo mc md kj me bi translated">3.哈希功能(哈希矢量器)</h2><p id="e768" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">哈希矢量器被设计成尽可能地节省内存。矢量器不是将记号存储为字符串，而是应用<a class="ae kq" href="https://en.wikipedia.org/wiki/Feature_hashing" rel="noopener ugc nofollow" target="_blank">散列技巧</a>将它们编码为数字索引。这种方法的缺点是，一旦矢量化，就无法再检索要素的名称。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="f8a5" class="lr ju hi mg b fi mk ml l mm mn">from sklearn.feature_extraction.text import HashingVectorizer</span><span id="6da6" class="lr ju hi mg b fi mt ml l mm mn">hv = HashingVectorizer(dtype=np.float32,<br/> strip_accents=’unicode’, analyzer=’word’,<br/> ngram_range=(1, 3),n_features=2**10)<br/># Fitting Hash Vectorizer to both training and test sets (semi-supervised learning)<br/>hv.fit(list(train.preprocessed_question_text.values) + list(test.preprocessed_question_text.values))<br/>X_train = hv.transform(train.preprocessed_question_text.values) <br/>X_test_tfv = hv.transform(test.preprocessed_question_text.values)<br/>y_train = train.target.values</span></pre><p id="60e9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">HashingVectorizer将一组文本文档转换成一个标记出现的矩阵。2**10是我们希望在输出矩阵的一列中包含的特征数量。</p><p id="b298" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，我尝试了逻辑回归和LightGBM。</p><h2 id="bc7f" class="lr ju hi bd jv ls lt lu jz lv lw lx kd jg ly lz kf jk ma mb kh jo mc md kj me bi translated">4.Word2Vec功能(单词嵌入)</h2><p id="daf0" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated"><strong class="iz hj">单词嵌入</strong>是一种语言建模技术，用于将单词映射到实数向量。它用几个维度表示向量空间中的单词或短语。可以使用各种方法生成单词嵌入，如神经网络、共现矩阵、概率模型等。</p><p id="adbc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> Word2Vec </strong>由生成单词嵌入的模型组成。这些模型是具有一个输入层、一个隐藏层和一个输出层的浅层两层神经网络。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="9eea" class="lr ju hi mg b fi mk ml l mm mn">def load_glove_index():<br/> EMBEDDING_FILE = ‘glove.840B.300d/glove.840B.300d.txt’<br/> def get_coefs(word,*arr): return word, np.asarray(arr, dtype=’float32')[:300]<br/> f = open(EMBEDDING_FILE,encoding=”utf-8")<br/> embeddings_index = dict(get_coefs(*o.split(“ “)) for o in f)<br/> return embeddings_index</span><span id="575e" class="lr ju hi mg b fi mt ml l mm mn">embeddings_index = load_glove_index()</span><span id="2b83" class="lr ju hi mg b fi mt ml l mm mn">print(‘Found %s word vectors.’ % len(embeddings_index))</span><span id="e5f1" class="lr ju hi mg b fi mt ml l mm mn">from nltk.corpus import stopwords<br/>from nltk import word_tokenize</span><span id="e57d" class="lr ju hi mg b fi mt ml l mm mn">stop_words = stopwords.words(‘english’)<br/>def sent2vec(s):<br/> words = str(s).lower()<br/> words = word_tokenize(words)<br/> words = [w for w in words if not w in stop_words]<br/> words = [w for w in words if w.isalpha()]<br/> M = []<br/> for w in words:<br/> try:<br/> M.append(embeddings_index[w])<br/> except:<br/> continue<br/> M = np.array(M)<br/> v = M.sum(axis=0)<br/> if type(v) != np.ndarray:<br/> return np.zeros(300)<br/> return v / np.sqrt((v ** 2).sum())</span><span id="92dc" class="lr ju hi mg b fi mt ml l mm mn"># create sentence vectors using the above function for training and validation set<br/>xtrain = [sent2vec(x) for x in tqdm(train.preprocessed_question_text.values)]<br/>xtest_glove = [sent2vec(x) for x in tqdm(test.preprocessed_question_text.values)]</span></pre><p id="1d9b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，也尝试了逻辑回归和LightGBM。</p><p id="8471" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">既然我们已经详细了解了传统方法，现在让我们看看我是如何应用深度学习方法的。</p><h1 id="e88a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">深度学习方法</h1><p id="a268" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">我将解释我为这一分类尝试的深度学习模型，如TextCNN、BiLSTM和Attention。</p><p id="1900" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">注:</strong>对于所有模型，在输出层使用sigmoid作为激活函数，在编译模型时，使用Adam优化器和二元交叉熵作为损失函数。</p><h1 id="c700" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">1.TextCNN</h1><p id="4e95" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">TextCNN主要使用一维卷积层和max-over-time池层。假设输入的文本序列由<strong class="iz hj"> <em class="ne"> n </em> </strong>个单词组成，每个单词由一个<strong class="iz hj"> <em class="ne"> d </em> </strong>维单词向量表示。然后输入示例有宽度为<strong class="iz hj"> <em class="ne"> n </em> </strong>，高度为<strong class="iz hj"> <em class="ne"> 1 </em> </strong>，输入通道为<strong class="iz hj"> <em class="ne"> d </em> </strong>。textCNN的计算主要可以分为以下几个步骤:</p><ol class=""><li id="4100" class="ld le hi iz b ja jb jd je jg mu jk mv jo mw js mx lj lk ll bi translated">定义多个一维卷积核，并使用它们对输入执行卷积计算。具有不同宽度的卷积核可以捕获不同数量的相邻单词的相关性。</li><li id="1e5e" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js mx lj lk ll bi translated">对所有输出通道执行最大持续时间池，然后将这些通道的池输出值连接到一个向量中。</li><li id="cc1f" class="ld le hi iz b ja lm jd ln jg lo jk lp jo lq js mx lj lk ll bi translated">连接的矢量通过完全连接的图层转换为每个类别的输出。在这个步骤中可以使用脱落层来处理过拟合。</li></ol><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="877a" class="lr ju hi mg b fi mk ml l mm mn">filter_sizes = [1,2,3,5]<br/>num_filters = 36</span><span id="7a89" class="lr ju hi mg b fi mt ml l mm mn">inp = Input(shape=(maxlen,))<br/>x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False)(inp)<br/>x = Reshape((maxlen, embed_size, 1))(x)</span><span id="2b58" class="lr ju hi mg b fi mt ml l mm mn">maxpool_pool = []<br/>for i in range(len(filter_sizes)):<br/> conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),<br/> kernel_initializer=’he_normal’, activation=’relu’)(x)<br/> maxpool_pool.append(MaxPool2D(pool_size=(maxlen — filter_sizes[i] + 1, 1))(conv))</span><span id="c05c" class="lr ju hi mg b fi mt ml l mm mn">z = Concatenate(axis=1)(maxpool_pool) <br/>z = Flatten()(z)<br/>z = Dropout(0.1)(z)</span><span id="f6b9" class="lr ju hi mg b fi mt ml l mm mn">outp = Dense(1, activation=”sigmoid”)(z)</span></pre><p id="cdd1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2D卷积图层创建一个卷积核，它与图层输入进行卷积以产生输出张量。我选择了36作为输出空间的尺寸，以及一个指定conv2D窗口高度和宽度的过滤器尺寸列表。使用ReLu作为激活，he_normal作为内核初始化器。在此之上，应用最大池。</p><p id="12a1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该模型的f1值为0.6101。</p><h1 id="84f1" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">2.双向LSTM</h1><p id="7463" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">Keras通过<a class="ae kq" href="https://keras.io/layers/wrappers/#bidirectional" rel="noopener ugc nofollow" target="_blank">双向</a>层包装器支持双向LSTMs。</p><p id="f551" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个包装器将一个循环层(例如第一个LSTM层)作为参数。</p><p id="118f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它还允许您指定合并模式，即前向和后向输出在传递到下一层之前应该如何组合。</p><p id="d3f0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">双向RNN实际上就是两个RNN，其中一个向前馈送序列，而另一个向后馈送序列。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es nf"><img src="../Images/36f3224b3754470d2683e61f0f57800e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*5fFHZvcuJD3oFRy3-XTxvw.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">双向RNN</figcaption></figure><p id="4ccb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对双向RNN最简单的解释是，把RNN单元想象成一个黑盒，接受一个隐藏状态(向量)和一个字向量作为输入，给出一个输出向量和下一个隐藏状态。这个盒子具有一些权重，这些权重将使用损耗的反向传播来调整。此外，相同的单元格应用于所有单词，以便句子中的单词共享权重。这种现象被称为重量共享。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es ng"><img src="../Images/df5c031013efff97468bf333cd5aaca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X7KJ2_xNpWTV169REf7tkw.png"/></div></div></figure><p id="8134" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于长度为4的序列，如<strong class="iz hj">、【快速棕狐】、</strong>，RNN单元给出4个输出向量，这些向量可以连接起来，然后用作密集前馈架构的一部分。</p><p id="8e34" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在双向RNN中，唯一的变化是我们以通常的方式以及相反的方式阅读文本。因此，我们并行堆叠两个rnn，因此我们可以追加8个输出向量。</p><p id="4515" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们得到了输出向量，我们就将它们发送通过一系列密集层，最后通过softmax层来构建文本分类器。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="2a49" class="lr ju hi mg b fi mk ml l mm mn">inp = Input(shape=(maxlen,))<br/>x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable=False)(inp)<br/>x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)<br/>avg_pool = GlobalAveragePooling1D()(x)<br/>max_pool = GlobalMaxPooling1D()(x)<br/>conc = concatenate([avg_pool, max_pool])<br/>conc = Dense(64, activation=”relu”)(conc)<br/>conc = Dropout(0.1)(conc)<br/>outp = Dense(1, activation=”sigmoid”)(conc)<br/></span></pre><p id="76e7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里64是隐藏状态向量以及输出向量的大小(dim)。保留return_sequence我们需要整个序列的输出。那么这一层的输出维数是多少呢？<br/> 64*70(maxlen)*2(双向连接)</p><p id="065f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">注意:</strong> CuDNNLSTM是Keras中LSTM层的快速实现，只能在GPU上运行。</p><p id="8a97" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BiLSTM模型给出的F1值为0.6272。</p><h1 id="084b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">3.注意力模型</h1><p id="5b83" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">Dzmitry Bahdanau等人在他们的论文“<a class="ae kq" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">通过联合学习对齐和翻译的神经机器翻译</a>中提出了注意力，这是他们之前在编码器-解码器模型上的工作的自然延伸。</p><p id="06d9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意力被提出作为对编码器-解码器模型的限制的解决方案，该编码器-解码器模型将输入序列编码为一个固定长度的向量，从该向量中解码每个输出时间步长。当解码长序列时，这个问题被认为是更大的问题。</p><p id="2847" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意力被认为是一种对齐和翻译的方法。</p><p id="3135" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对齐是机器翻译中的问题，它确定输入序列的哪些部分与输出中的每个单词相关，而翻译是使用相关信息选择适当输出的过程。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="er es ng"><img src="../Images/acd1d83813655ca95a857646fee75e7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*utInurcth83o5ZsKaor2jQ.png"/></div></div></figure><p id="6abd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们希望为文本中的每个单词创建分数，这就是一个单词的注意力相似度分数。</p><p id="60dd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，我们从权重矩阵(W)、偏差向量(b)和上下文向量u开始。优化算法学习所有这些权重。在这一点上，我想强调我非常喜欢神经网络的一点——如果你不知道一些参数，让网络学习它们。我们只需要担心如何创建要调优的架构和参数。</p><p id="0e1b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后就是一系列的数学运算。请参见图中的详细说明。我们可以将u1视为RNN字输出的非线性。之后，v1是u1与上下文向量u的点积，u的乘幂。从直觉的角度来看，如果u和u1相似，v1的值将会很高。因为我们希望分数的总和为1，所以我们用v除以v的总和来得到最终分数s</p><p id="b151" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，这些最终得分乘以RNN单词输出，根据单词的重要性对其进行加权。然后将输出相加，并通过密集层和softmax发送，用于文本分类任务。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="a176" class="lr ju hi mg b fi mk ml l mm mn">inp = Input(shape=(maxlen,))<br/>x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)<br/>x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)<br/>x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)<br/>x = AttentionWithContext()(x)<br/>x = Dense(64, activation=”relu”)(x)<br/>x = Dense(1, activation=”sigmoid”)(x)</span></pre><p id="1f05" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">注意:</strong> CuDNNLSTM是Keras中LSTM层的快速实现，只能在GPU上运行。</p><p id="e08a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这款车型的F1得分为0.6305，比我尝试过的任何其他车型都要好。</p><h1 id="26ef" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">结果</h1><p id="b5bc" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">运行5重分层CV后的结果。</p><pre class="ks kt ku kv fd mf mg mh mi aw mj bi"><span id="7c72" class="lr ju hi mg b fi mk ml l mm mn">******************** Conventional Methods ********************<br/>+---------------------+-------------------+---------------------+<br/>|        Model        |     Vectorizer    |    Test F1-Score    |<br/>+---------------------+-------------------+---------------------+<br/>| Logistic Regression |  CountVectorizer  |  0.6061869265414231 |<br/>|     Naive Bayes     |  CountVectorizer  |  0.5411124898212416 |<br/>|       LightGBM      |  CountVectorizer  | 0.44805759949363083 |<br/>| Logistic Regression |  TFIDFVectorizer  |  0.5940808706791779 |<br/>|     Naive Bayes     |  TFIDFVectorizer  |  0.5029684543565248 |<br/>|       LightGBM      |  TFIDFVectorizer  | 0.44225094843332863 |<br/>| Logistic Regression | HashingVectorizer |  0.3491367081425039 |<br/>|       LightGBM      | HashingVectorizer |  0.3469309621513192 |<br/>| Logistic Regression |      Word2vec     |  0.5191204693754455 |<br/>|       LightGBM      |      Word2vec     |  0.4302154845215734 |<br/>+---------------------+-------------------+---------------------+<br/><br/><br/>******************** Deep Learning Methods ********************<br/>+-----------+--------------------+<br/>|   Model   |   Test F1-Score    |<br/>+-----------+--------------------+<br/>|  TextCNN  | 0.6101427665518363 |<br/>|   BiLSTM  | 0.6272316455189987 |<br/>| Attention | 0.6304589487057329 |<br/>+-----------+--------------------+</span></pre><h1 id="3ce7" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">未来的工作</h1><p id="42ec" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">对于上述所有模型，我没有进行超参数调整。您可以尝试通过使用<strong class="iz hj"> </strong>超视或网格搜索或随机搜索进行超参数调整来提高性能。</p><h1 id="75f8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">结论:</h1><p id="4bf8" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">从深度学习方法CuDNNLSTM with Attention获得的F1-score表现优于任何其他模型，给出了0.6304的分数。</p><p id="11e0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于这个问题的分类，我首先执行了必要的数据预处理和清理，如删除标点符号、缩写、停用词、替换拼写错误的单词、词干和对文本进行词条整理。</p><p id="28f7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">之后，我使用sklearn的文本特征提取方法(如计数矢量器、TF-IDF、哈希和Word2vec嵌入)执行了机器学习分类方法，如朴素贝叶斯、逻辑回归和LightGBM。</p><p id="31ff" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了获得更好的结果，我还使用TextCNN、有注意力和无注意力双向LSTM等模型进行了深度学习。</p><p id="bfbe" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一个编码<a class="ae kq" href="https://github.com/Priyanka2205/Quora-Insincere-Question-Classification" rel="noopener ugc nofollow" target="_blank">链接</a>到这个问题实现。</p><p id="4ddc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">也对<a class="ae kq" href="https://mlwhiz.com/" rel="noopener ugc nofollow" target="_blank">的mlwhiz </a>大声喊出来，他在这个项目上的博客帮助我实现了它。</p><p id="21d8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，感谢大家阅读我的博客。</p><h1 id="88cf" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">参考资料:</h1><div class="nh ni ez fb nj nk"><a href="https://www.kaggle.com/c/quora-insincere-questions-classification" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab dw"><div class="nm ab nn cl cj no"><h2 class="bd hj fi z dy np ea eb nq ed ef hh bi translated">Quora虚假问题分类</h2><div class="nr l"><h3 class="bd b fi z dy np ea eb nq ed ef dx translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="ns l"><p class="bd b fp z dy np ea eb nq ed ef dx translated">www.kaggle.com</p></div></div><div class="nt l"><div class="nu l nv nw nx nt ny kx nk"/></div></div></a></div><div class="nh ni ez fb nj nk"><a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab dw"><div class="nm ab nn cl cj no"><h2 class="bd hj fi z dy np ea eb nq ed ef hh bi translated">NLP学习系列:第1部分——深度学习的文本预处理方法</h2><div class="nr l"><h3 class="bd b fi z dy np ea eb nq ed ef dx translated">最近，我在Kaggle上发起了一个名为Quora问题不真诚挑战的NLP竞赛。这是一个NLP…</h3></div><div class="ns l"><p class="bd b fp z dy np ea eb nq ed ef dx translated">mlwhiz.com</p></div></div><div class="nt l"><div class="nz l nv nw nx nt ny kx nk"/></div></div></a></div><div class="nh ni ez fb nj nk"><a href="https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab dw"><div class="nm ab nn cl cj no"><h2 class="bd hj fi z dy np ea eb nq ed ef hh bi translated">Python |使用Word2Vec - GeeksforGeeks进行单词嵌入</h2><div class="nr l"><h3 class="bd b fi z dy np ea eb nq ed ef dx translated">单词嵌入是一种语言建模技术，用于将单词映射到实数向量。它代表单词…</h3></div><div class="ns l"><p class="bd b fp z dy np ea eb nq ed ef dx translated">www.geeksforgeeks.org</p></div></div><div class="nt l"><div class="oa l nv nw nx nt ny kx nk"/></div></div></a></div><div class="nh ni ez fb nj nk"><a href="https://mlwhiz.com/blog/2019/02/08/deeplearning_nlp_conventional_methods/" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab dw"><div class="nm ab nn cl cj no"><h2 class="bd hj fi z dy np ea eb nq ed ef hh bi translated">自然语言处理学习系列:第2部分-传统的文本分类方法</h2><div class="nr l"><h3 class="bd b fi z dy np ea eb nq ed ef dx translated">最近，我在Kaggle上发起了一个名为Quora问题不真诚挑战的NLP竞赛。这是一个NLP…</h3></div><div class="ns l"><p class="bd b fp z dy np ea eb nq ed ef dx translated">mlwhiz.com</p></div></div><div class="nt l"><div class="ob l nv nw nx nt ny kx nk"/></div></div></a></div><p id="7951" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae kq" href="https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/" rel="noopener ugc nofollow" target="_blank">https://mlwhiz . com/blog/2019/03/09/deep learning _ architectures _ text _ class ification/</a></p><div class="nh ni ez fb nj nk"><a href="https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab dw"><div class="nm ab nn cl cj no"><h2 class="bd hj fi z dy np ea eb nq ed ef hh bi translated">注意在编码-解码递归神经网络中是如何工作的</h2><div class="nr l"><h3 class="bd b fi z dy np ea eb nq ed ef dx translated">注意力是一种机制，其被开发来改善机器上的编码器-解码器RNN的性能</h3></div><div class="ns l"><p class="bd b fp z dy np ea eb nq ed ef dx translated">machinelearningmastery.com</p></div></div><div class="nt l"><div class="oc l nv nw nx nt ny kx nk"/></div></div></a></div><p id="cf11" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae kq" href="https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39" rel="noopener" target="_blank">https://towards data science . com/light-on-math-ml-attention-with-keras-DC 8 db C1 fad 39</a></p></div></div>    
</body>
</html>