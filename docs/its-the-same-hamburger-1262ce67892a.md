# æ˜¯åŒä¸€ä¸ªæ±‰å ¡ï¼ï¼

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/its-the-same-hamburger-1262ce67892a?source=collection_archive---------30----------------------->

> è‡ªç„¶è¯­è¨€å¤„ç†(ä¸‹)
> 
> ä»¥ä¸‹æ˜¯ NLP ç³»åˆ—æ–‡ç« çš„ä¸€éƒ¨åˆ†ã€‚(æŸ¥ [*ç¬¬ä¸€éƒ¨åˆ†*](/@azabou.sofiene/its-the-same-hamburger-983a7966acd8) *&* [*ç¬¬ä¸‰éƒ¨åˆ†*](/@azabou.sofiene/how-do-they-read-your-mind-c145d1b3de74) *)*

æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­çœ‹åˆ°çš„ï¼ŒNLP æä¾›äº†æœ‰è¶£çš„åŠŸèƒ½ï¼Œè¿™äº›åŠŸèƒ½æ­£åœ¨æ”¹å˜å½“ä»Šçš„è®¸å¤šè¡Œä¸šã€‚ç”µè„‘èƒ½åšé‚£ä¹ˆå¤šäº‹æƒ…ï¼Œè¿™å¾ˆé…·ï¼Œä½†æ˜¯å®ƒæ˜¯æ€ä¹ˆåšåˆ°çš„å‘¢ï¼Ÿå“¦ï¼Œæ˜¯çš„ï¼Œä½ çŒœå¯¹äº†ï¼Œæˆ‘ä»¬è¦å»ä¸€äº›ä¸¥è‚ƒçš„åœ°æ–¹..ä¸œè¥¿ï¼

# NLP æ¡†æ¶

æˆ‘ä»¬å°†é€æ­¥æ„å»ºä¸€ä¸ªè‡ªç„¶è¯­è¨€å¤„ç†æ¡†æ¶ï¼Œåœ¨æœ¬â€œæ•™ç¨‹â€ç»“æŸæ—¶ï¼Œæ‚¨å°†èƒ½å¤Ÿæ„å»ºè‡ªå·±çš„ NLP æ¨¡å‹ã€‚æˆ‘ä»¬å¼€å§‹å§ï¼

![](img/7031f194cf76380ba1e0eb96a2747440.png)

NLP æ¡†æ¶

é¦–å…ˆï¼Œæˆ‘ä»¬æ¥çœ‹è¿™æ®µæ–‡å­—ã€‚è¿™æ˜¯æ¯”å°”Â·ç›–èŒ¨çš„ä¸€å¥åè¨€ï¼Œä¹Ÿæ˜¯æˆ‘æœ€å–œæ¬¢çš„ä¸€å¥ã€‚å¦‚æœæˆ‘çš„ç”µè„‘èƒ½è¯»æ‡‚è¿™å¥è¯ï¼Œå°¤å…¶æ˜¯èƒ½â€œç†è§£â€å®ƒï¼Œé‚£å°±å¤ªæ£’äº†ï¼Œä¸æ˜¯å—ï¼Ÿè¦å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦é‡‡å–å‡ ä¸ªæ­¥éª¤ã€‚

![](img/c32aac6010175057d19c8eb2114b156e.png)

æ¯”å°”Â·ç›–èŒ¨â€”â€”å¾®è½¯åˆ›å§‹äººå…¼è‘£äº‹é•¿

# æ•°æ®é¢„å¤„ç†

æ•°æ®é¢„å¤„ç†è¢«è®¤ä¸ºæ˜¯è¿™é¡¹å·¥ä½œä¸­æœ€çƒ¦äººçš„éƒ¨åˆ†ï¼Œå› ä¸ºå®ƒåœ¨æŠ€æœ¯ä¸Šæ²¡æœ‰å¸å¼•åŠ›ï¼Œè€Œä¸”ç›¸å¯¹è´¹åŠ›ï¼Œä½†ä»ç„¶å¾ˆé‡è¦ã€‚æ•°æ®ç§‘å­¦å®¶ä¸­æœ‰ä¸€å¥åè¨€:â€œåƒåœ¾è¿›ï¼Œåƒåœ¾å‡ºâ€ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœä½ ç»™ä½ çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¾“å…¥è‚®è„çš„æ•°æ®ï¼Œå®ƒä¼šç›´æ¥æŠŠå®ƒæ‰”å›åˆ°ä½ é¢å‰(æŠ±æ­‰ğŸ˜Š)..æ¢å¥è¯è¯´ï¼Œå®ƒä¼šç»™ä½ æ— æ„ä¹‰çš„ç»“æœã€‚å¤Ÿå…¬å¹³å§ï¼Ÿè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿™éƒ¨åˆ†å·¥ä½œè¦ä¸¥è°¨çš„åšã€‚
é€šå¸¸ï¼Œåœ¨å¤„ç†ç»“æ„åŒ–æ•°æ®æ—¶ï¼Œæ•°æ®é¢„å¤„ç†å¾€å¾€æ¶‰åŠåˆ°åˆ é™¤é‡å¤æ•°æ®ã€ç©ºå€¼å’Œé”™è¯¯ã€‚å½“æ¶‰åŠåˆ°æ–‡æœ¬æ•°æ®æ—¶ï¼Œæœ‰è®¸å¤šå¸¸è§çš„æ•°æ®é¢„å¤„ç†æŠ€æœ¯ï¼Œä¹Ÿç§°ä¸ºæ–‡æœ¬æ¸…æ´—æŠ€æœ¯ã€‚
ä¸ºäº†åº”ç”¨é¢„å¤„ç†æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªéå¸¸å¼ºå¤§çš„ Python åº“: [**NLTK:è‡ªç„¶è¯­è¨€å·¥å…·åŒ…**](https://www.nltk.org/) ã€‚NLTK æä¾›äº†ä¸€å¥—æ–‡æœ¬å¤„ç†åº“ï¼Œç”¨äºåˆ†ç±»ã€æ ‡è®°åŒ–ã€è¯å¹²æå–ã€æ ‡è®°ç­‰ã€‚åšæŒä½ï¼Œæˆ‘ä»¬å°†åœ¨å‡ åˆ†é’Ÿåçœ‹åˆ°æ‰€æœ‰è¿™äº›åŠŸèƒ½ã€‚æ•¬è¯·æœŸå¾…ï¼

## å¥å­åˆ†å‰²

åŸºæœ¬ä¸Šï¼Œå®ƒæ˜¯æŠŠæˆ‘ä»¬çš„æ–‡æœ¬åˆ†æˆå•ç‹¬çš„å¥å­çš„è¡Œä¸ºã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä»¥è¿™ä¸ªç»“æŸ:

1.â€œæˆ‘èƒ½ç†è§£æƒ³è¦æ‹¥æœ‰æ•°ç™¾ä¸‡ç¾å…ƒï¼Œéšä¹‹è€Œæ¥çš„æ˜¯æŸç§è‡ªç”±ï¼Œæœ‰æ„ä¹‰çš„è‡ªç”±ã€‚â€2
2ã€‚â€œä½†æ˜¯ä¸€æ—¦ä½ åƒå¾—å¤ªå¤šï¼Œæˆ‘ä¸å¾—ä¸å‘Šè¯‰ä½ ï¼Œè¿™è¿˜æ˜¯åŒä¸€ä¸ªæ±‰å ¡ã€‚â€
3ã€‚*â€œæ¯”å°”Â·ç›–èŒ¨â€”â€”è‘£äº‹é•¿&å¾®è½¯åˆ›å§‹äººâ€*

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾æ¯ä¸ªå¥å­ä»£è¡¨ä¸€ä¸ªç‹¬ç«‹çš„æƒ³æ³•ã€‚å› æ­¤ï¼Œå¼€å‘ä¸€ä¸ªç†è§£å•ä¸ªå¥å­è€Œä¸æ˜¯æ•´ä¸ªæ®µè½çš„ç®—æ³•è¦å®¹æ˜“å¾—å¤šã€‚

## ç¬¦å·åŒ–

ç°åœ¨æˆ‘ä»¬æŠŠæ–‡æœ¬åˆ†æˆå¥å­ï¼Œè®©æˆ‘ä»¬åšå¾—æ›´å¥½ï¼ŒæŠŠå®ƒåˆ†æˆå•è¯ï¼Œæˆ–è€…æ›´å‡†ç¡®åœ°è¯´æ˜¯â€œè®°å·â€ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä»å¼•ç”¨çš„ç¬¬ä¸€å¥è¯å¼€å§‹:

â€œæˆ‘èƒ½ç†è§£æƒ³è¦æ‹¥æœ‰æ•°ç™¾ä¸‡ç¾å…ƒï¼Œéšä¹‹è€Œæ¥çš„æ˜¯æŸç§è‡ªç”±ï¼Œæœ‰æ„ä¹‰çš„è‡ªç”±ã€‚â€

åº”ç”¨æ ‡è®°åŒ–åï¼Œç»“æœå¦‚ä¸‹:

*"æˆ‘"ã€"èƒ½"ã€"æ‡‚"ã€"æƒ³"ã€"æœ‰"ã€"ç™¾ä¸‡"ã€"ç¾å…ƒ"ã€"æœ‰"ã€"æœ‰"ã€"æŸ"ã€"è‡ªç”±"ã€"æœ‰æ„ä¹‰"ã€"è‡ªç”±"ã€"é‚£ä¸ª"ã€"é‚£ä¸ª"ã€"æ¥äº†"ã€"æœ‰äº†"ã€"é‚£ä¸ª"ã€"é‚£ä¸ª"ã€‚"*

```
text = '''*I can understand wanting to have millions of dollars, thereâ€™s a certain freedom, meaningful freedom, that comes with that*. *But once you get much beyond that, I have to tell you, itâ€™s the same hamburger. Bill Gates â€” Chairman & Founder of Microsoft'''*#Import NLTK Library
import nltk#Segmentation
nltk.tokenize.sent_tokenize(text)#Tokenization
nltk.tokenize.word_tokenize(text)
```

## æ–‡æœ¬å‰¥ç¦»

å¦‚æœä½ å’Œæˆ‘æƒ³çš„ä¸€æ ·ï¼Œé‚£ä½ å°±é”™äº†..ä½†æ˜¯æˆ‘ä»¬è¿˜æ˜¯ä¼šè„±ä¸‹ä¸€äº›ä¸œè¥¿ã€‚
**ä½¿æ–‡æœ¬å°å†™:**è¿™æ˜¯ä¸€ç§æ ‡å‡†åŒ–æ£€æŸ¥ç‚¹ï¼Œä»¥é¿å…æˆ‘ä»¬è¦å¤„ç†çš„å­—ç¬¦æ•°ã€‚
**æ‰©å±•ç¼©å†™:**éæ­£å¼è‹±è¯­å……æ»¡äº†åº”è¯¥è¢«æ›¿æ¢çš„ç¼©å†™ï¼Œæ€»æ˜¯è¯•å›¾å°½å¯èƒ½åœ°ä½¿æˆ‘ä»¬çš„æ–‡æœ¬æ­£å¸¸åŒ–ã€‚
ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬çš„å¼•ç”¨ä¸­ï¼Œâ€œthere'sâ€å°†è¢«æ›¿æ¢ä¸ºâ€œthere isâ€ã€‚

æˆ‘åœ¨ [StackOverFlow](https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python) ä¸Šæ‰¾åˆ°äº†ä¸‹é¢ä½¿ç”¨çš„ **cList** ã€‚

```
###Make text lowercase & Expand contractions#Load English contracted/expanded words list from a .py file
from contractions import cList# Compile a regular expression pattern for matching 
import re
c_re = re.compile('(%s)' % '|'.join(cList.keys()))#Create a function to look for contractions and replace them with their full form
#Put text in lowercase to make sure all words are included
def expandContractions(text, c_re=c_re):
    def replace(match):
        return cList[match.group(0)]
    return c_re.sub(replace, text.lower())#Notice it's a bit grammatically incorrect, but it doesn't matter since we gonna remove the stopwords later
expanded_text = expandContractions(text)
```

æ³¨æ„è¿™æœ‰ç‚¹è¯­æ³•é”™è¯¯ï¼Œä½†æ˜¯æ²¡å…³ç³»ï¼Œå› ä¸ºæˆ‘ä»¬ç¨åä¼šåˆ é™¤åœç”¨è¯ğŸ˜‰

**åˆ é™¤æ ‡ç‚¹ç¬¦å·:**æ ‡ç‚¹ç¬¦å·ä»£è¡¨ä¸éœ€è¦çš„å­—ç¬¦ï¼Œè®©æˆ‘ä»¬å»æ‰å®ƒä»¬ã€‚

```
###Remove punctuations#Import string library
import string#Create a function to remove punctuation / special characters '!"#$%&\'()*+,-./:;<=>?#@[\\]^_`{|}~'
def clean_text(text):
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    return text
```

**æ‹¼å†™çº æ­£:**æƒ³æ³•å¾ˆç®€å•ï¼›æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå¤§è¯­æ–™åº“ä½œä¸ºå‚è€ƒæ¥çº æ­£æˆ‘ä»¬æ–‡æœ¬ä¸­çš„å•è¯æ‹¼å†™ã€‚
**åˆ é™¤åœç”¨è¯:**åœç”¨è¯æ˜¯è¢«è¿‡åº¦ä½¿ç”¨çš„è¯ï¼Œå¯¹æ¯ç¯‡æ–‡ç« æ‰€ä¼ è¾¾çš„ä¿¡æ¯æ²¡æœ‰é¢å¤–çš„é‡è¦ä¿¡æ¯ã€‚å¤§å¤šæ•°å¸¸è§çš„åœç”¨è¯æ˜¯é™å®šè¯(å¦‚ theï¼Œaï¼Œan)ï¼Œä»‹è¯(å¦‚ aboveï¼Œcrossï¼Œbefore)å’Œä¸€äº›å½¢å®¹è¯(å¦‚ goodï¼Œnice)ã€‚è®©æˆ‘ä»¬æŠŠä»–ä»¬èµ¶å‡ºå»ï¼

*æˆ‘èƒ½* **ç†è§£æƒ³è¦** *æ‹¥æœ‰* **ç™¾ä¸‡******ç¾å…ƒ*** **ï¼Œæœ‰ä¸€ç§* **ç¡®å®šçš„è‡ªç”±** *ï¼Œ* **æœ‰æ„ä¹‰çš„è‡ªç”±** *ï¼Œé‚£ç§* **ä¼´éšç€é‚£ç§è€Œæ¥** *ã€‚
*ä½†æ˜¯ä¸€æ—¦ä½ * **å¾—åˆ°è¿œè¿œè¶…å‡º** *çš„é‚£ä¸€ç‚¹ï¼Œæˆ‘å°±ä¸å¾—ä¸* **å‘Šè¯‰** *ä½ ï¼Œè¿™æ˜¯ä¸€æ ·çš„* **æ±‰å ¡** *ã€‚*
**æ¯”å°”ç›–èŒ¨***â€”â€”***è‘£äº‹é•¿** *&* **åˆ›å§‹äºº*****å¾®è½¯******

```
**###Remove stopwords#nltk.download('stopwords')
from nltk.corpus import stopwords#Create a function to remove stopwords
def remove_stopwords (sentence = None):
    words = sentence.split()
    stopwords_list = stopwords.words("english")
    clean_words = []
    for word in words:
        if word not in stopwords_list:
            clean_words.append(word)
    return ' '.join(clean_words);**
```

**![](img/cb58f73ca6fa12d6745a0633389b79be.png)**

****è¯æ€§è¿‡æ»¤:**ç›®çš„æ˜¯é€šè¿‡ç»™æ¯ä¸ªå•è¯æ·»åŠ æ ‡ç­¾æ¥è¯†åˆ«å…¶è¯æ±‡ç±»åˆ«:åŠ¨è¯ã€å½¢å®¹è¯ã€åè¯ã€å‰¯è¯ã€ä»£è¯ã€ä»‹è¯â€¦â€¦**

```
**###Part of Speech Tagger#nltk.download('averaged_perceptron_tagger')
import nltk
from nltk import pos_tag, word_tokenize#Create a function to pull out nouns & adjectives from text
def nouns_adj(text):
    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'
    tokenized = word_tokenize(text)
    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if 
    is_noun_adj(pos)] 
    return ' '.join(nouns_adj)#Return list of tuple [word; PoS]
tokens = word_tokenize(clean_text_sw)
tuple_list = nltk.pos_tag(tokens)**
```

**![](img/3fdfa5c4f3168647fa8123124bcdef82.png)**

**åœ¨å¤§å¤šæ•°è¯­è¨€ä¸­ï¼Œå•è¯å¯ä»¥ä»¥ä¸åŒçš„å½¢å¼å‡ºç°ã€‚å°†æ¯ä¸ªå•è¯æ›¿æ¢æˆå®ƒçš„åŸºæœ¬å½¢å¼ä¼šå¾ˆæœ‰è¶£ï¼Œè¿™æ ·æˆ‘ä»¬çš„è®¡ç®—æœºå°±å¯ä»¥ç†è§£ä¸åŒçš„å¥å­å¯èƒ½åœ¨è°ˆè®ºåŒä¸€ä¸ªæ¦‚å¿µã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬æ¥å¼•ç”¨æˆ‘ä»¬çš„è¯å§ï¼**

**å°±æˆ‘ä»¬è€Œè¨€ï¼Œâ€œæˆ‘å¯ä»¥ç†è§£æƒ³è¦æ‹¥æœ‰æ•°ç™¾ä¸‡ç¾å…ƒçš„æ„¿æœ›â€**

**å˜æˆâ€œæˆ‘å¯ä»¥ç†è§£ä¸º**ã€æƒ³è¦ã€‘**æœ‰**ã€ç™¾ä¸‡ã€‘****ã€ç¾å…ƒã€‘**â€**

```
**###Lemmatization#nltk.download('wordnet')#Lemmatize text with appropriate POS tag
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet#Create a function to map NLTK's POS tags to the format wordnet lemmatizer would accept
def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)#Create an instance of the WordNetLemmatizer()
lemmatizer = WordNetLemmatizer()#Create a function to return text after lemmatization
def lemmatize_text(text):
    lemm_text = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(clean_text_sw)]
    return ' '.join(lemm_text)**
```

****å‘½åå®ä½“è¯†åˆ«:**æ˜¯ä¸€ä¸ªç®—æ³•ä»¥ä¸€ä¸²æ–‡æœ¬(å¥å­æˆ–æ®µè½)ä½œä¸ºè¾“å…¥ï¼Œè¯†åˆ«ç›¸å…³åè¯(äººã€åœ°ç‚¹ã€ç»„ç»‡ç­‰)çš„è¿‡ç¨‹..)åœ¨é‚£ä¸²ä¸­æåˆ°çš„ã€‚**

```
**###Named Entity recognition#nltk.download('maxent_ne_chunker')
#nltk.download('words')
from nltk import ne_chunk#Create a function to tokenize and PoS your text
def NER(text):
    text = nltk.tokenize(text)
    text = nltk.pos_tag(text)
    return texttext_NER = NER(text)pos_list = ne_chunck(text_NER)**
```

**![](img/1853e97477652ca212141caaefa064a1.png)**

**æ¥çœ‹çœ‹å§ï¼**