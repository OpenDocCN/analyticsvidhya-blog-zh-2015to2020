<html>
<head>
<title>Boost your position on Leaderboard (Kaggle), How?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何提升你在排行榜上的位置？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/boost-your-position-on-leaderboard-kaggle-how-9c1f1a5d4663?source=collection_archive---------25-----------------------#2020-06-03">https://medium.com/analytics-vidhya/boost-your-position-on-leaderboard-kaggle-how-9c1f1a5d4663?source=collection_archive---------25-----------------------#2020-06-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="e12b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">使用XGBoost </strong></h1><p id="97ba" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">极端梯度推进算法是集成机器学习技术中最有效的算法之一。它是在2015年由<strong class="jf hj"> Tianqui Chen </strong>推出的，从那时起，它就因其预测性能和处理速度而成为机器学习黑客马拉松和竞赛的宠儿。它可用于监督学习任务，如回归、分类和排序。在本文中，我们将通过分解成以下内容来尝试理解它是如何工作的:</p><p id="1d15" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">升压|梯度升压|极端梯度升压</p><p id="e3c9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">这一切都始于助推… </strong></p><p id="1e8b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">Boosting(原称<em class="kg">假设boosting </em>)属于<strong class="jf hj">集成学习</strong>，符合<a class="ae kh" href="https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj"> <em class="kg">众智</em> </strong> </a> <strong class="jf hj"> <em class="kg">。</em></strong></p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ki"><img src="../Images/9e2814d3ee156c20b4106d887819ab2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XTdlhPYlgjCzAUmX"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">来源:互联网。</figcaption></figure><p id="f8fb" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">梯度推进</strong></p><p id="25f9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">它被称为梯度提升，因为它采用<a class="ae kh" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank">梯度下降</a>算法来<strong class="jf hj">最小化添加新模型时的损失函数</strong>。但是，它受到一次创建一个弱学习器(决策树)的限制，这使得该模型的计算成本很高，因为它是一个连续的过程。</p><p id="b129" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">为了克服这一点—</p><p id="325c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">极限梯度推进</strong>问世，其设计具有以下特点。</p><ol class=""><li id="71b5" class="ky kz hi jf b jg kb jk kc jo la js lb jw lc ka ld le lf lg bi translated">它通过并行创建提升树来支持<em class="kg">并行化</em>。</li><li id="864a" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">它实现了<em class="kg">分布式计算</em>方法，用于评估任何大型和复杂的模块/数据集。</li><li id="234d" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">它支持<em class="kg">正则化。</em></li><li id="e61b" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">缓存优化。</li></ol><p id="6aa6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在，让我们试着去理解它</p><p id="6184" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">正如我们所知，监督机器学习模型往往会找到最适合训练数据x <strong class="jf hj"> <em class="kg"> i </em> </strong> &amp;标签y <strong class="jf hj"> <em class="kg"> i </em> </strong>的参数(θ)，为了训练模型，我们需要定义目标函数来衡量模型与训练数据的拟合程度。</p><p id="ea82" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">任何监督机器学习算法的目标函数的显著特征是——它由两部分组成——<strong class="jf hj">损失函数和正则项</strong></p><p id="1037" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">obj(ө)=l(ө)+ω(ө)</p><p id="94b3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">l(ө)=<a class="ae kh" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank">损失函数</a> —训练损失衡量模型对训练数据的拟合程度。</p><p id="d7ee" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">ω(ө)=<a class="ae kh" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正则化(数学)</a> —衡量模型的复杂程度。</p><p id="0518" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">树木——(我们在学什么？)</strong></p><p id="bd39" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">XGBoost主要选择决策树集成模型，该模型主要包括分类树和回归树，具体取决于目标变量是连续变量还是分类变量。</p><p id="9233" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里有一个简单的购物车示例，它对某人是否喜欢一个假设的电脑游戏X进行分类，每个叶子值只包含一个分数。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es lm"><img src="../Images/601698a6d25a9263208ea42f8c6fcfaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Qv62wnA94KG0OEQs"/></div></div></figure><p id="a316" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">决策树集成:</p><p id="e473" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">大多数情况下，单个树不足以在数据集上提供更好的结果，因此我们构建多个树并将它们的预测相加。</p><p id="20be" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里有一个例子:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ln"><img src="../Images/2f0b0e6b23feb8298db0bef9976b523b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_LqLsCkoeYK4gC-d"/></div></div></figure><p id="8a0a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">数学上，一般树模型可以表示为:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es lo"><img src="../Images/2d1f94b804a8db1d19eb0fed37cab9d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*S8I4SBNR4MBP0Vx9"/></div></div></figure><p id="0a2a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">其中K =树的数量，当我们处理监督问题时，我们的目标函数可以写成:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es lp"><img src="../Images/0ed9bb4561ec09b2c51f6d2aaf3e2a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7sVu_d4cgT_bwLYs"/></div></div></figure><p id="5057" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">定义ω的可能方法？</p><ol class=""><li id="3ca0" class="ky kz hi jf b jg kb jk kc jo la js lb jw lc ka ld le lf lg bi translated">树中的节点数或深度</li><li id="4f33" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">叶权的L2范数</li></ol><p id="0368" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这定义了学习什么(目标和模型)，但现在来了一个非常重要的问题，即<strong class="jf hj">如何学习？</strong></p><p id="1d62" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">解决方案</strong> : <strong class="jf hj">加法训练(助推)</strong></p><p id="9cc3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">理解起来非常简单，其工作方式与它的名字所暗示的一样——从常量预测开始，每次都不断添加新的函数。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es lq"><img src="../Images/165a53848477eb3322d8e13ef62825c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4Tr-JQu4CR2YP7vN"/></div></div></figure><p id="0cfb" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在一个新的问题出现了，即如何决定添加哪个<strong class="jf hj"> f </strong>？答案很简单:只要优化目标！！</p><p id="bbd2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对于优化，如果我们考虑均方误差(MSE)作为我们的损失函数，目标变成:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ln"><img src="../Images/6b7c239bb740c32505f8bae293025e00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jLYrMmNU-BioWBr-"/></div></div></figure><p id="f17c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">极端梯度推进对回归和分类都使用二阶泰勒展开。泰勒近似的天才之处在于将它分成相对简单的部分。</p><p id="2af4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">目标函数的泰勒展开式是:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lr"><img src="../Images/ccb334a696ebf4abd438eeb41d8d590f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/0*wtAtBKQbNJo-p5gE"/></div></figure><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es ls"><img src="../Images/2978d4c9b92e5880785aaffb5ed7cafa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/0*25IueTgLaaLV3Afb"/></div></figure><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es lt"><img src="../Images/881c319b510e8f0a2fcf74817e5cdb6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Kyn8vlgBKjDKpKBA"/></div></div></figure><p id="9608" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在我们移除所有常数之后，步骤t处的特定目标函数变成:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lu"><img src="../Images/50a0820780057d167fd663d0e4341f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/0*CnwDw38oEsGaV1x5"/></div></figure><p id="a6c8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这成为我们对新树的优化目标。这个定义的一个重要优点是目标函数值只取决于gi和hi。</p><p id="4cdc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">上面讨论的部分为我们提供了XGBoost如何拟合训练数据以及如何进行预测的概念。</p><p id="b269" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在，让我们考虑另一个重要部分，即<strong class="jf hj">优化。</strong></p><p id="7185" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在不用深入研究数学，xgboost的正则化可以写成:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lv"><img src="../Images/d7c50937aab2310c2d1d1d174bcf2fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/0*eti8aQY7yse_bkZm"/></div></figure><p id="a626" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">术语𝛀 <strong class="jf hj"> </strong>惩罚了模型的复杂性。附加项有助于平滑最终学习的权重以避免过拟合，当𝛀 = 0时，目标退回到传统的梯度树提升。</p><p id="35bb" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">实际上不可能枚举所有可能的树结构，所以采用贪婪算法方法。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lw"><img src="../Images/c3841ffe0fbac81ce965f517fa72a692.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/0*7KoKC68dTGgP7fA0"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">来源:维基百科</figcaption></figure><p id="7a5e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">啊！我们已经走过了漫长的道路，现在让我们总结一下XGBoost。</p><ul class=""><li id="47e0" class="ky kz hi jf b jg kb jk kc jo la js lb jw lc ka lx le lf lg bi translated">在每次迭代中添加一个新的树。</li><li id="fdf3" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka lx le lf lg bi translated">在每次迭代开始时，计算gi和hi。</li><li id="3a3d" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka lx le lf lg bi translated">利用贪婪算法来生长一棵树。</li><li id="ed81" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka lx le lf lg bi translated">向模型中添加一棵树。</li></ul><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es ly"><img src="../Images/74e7d6b9c1a1078a026f49286bf5366e.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/0*_L_dbXjf7cqaqEeh"/></div></figure><p id="a9fa" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">其中，∊是一个步长或收缩量，以防止过度拟合。</p><p id="a7d5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以上就是我对XGBoost的看法。如果你有任何改进它的建议或者想要增加一些东西，请随时给我发消息到这个twitter账号<strong class="jf hj"> @DSsakshi。</strong></p><p id="1e78" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">免责声明:</strong></p><p id="ec26" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这个东西的灵感来自https://xgboost.readthedocs.io/en/latest/的<a class="ae kh" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>