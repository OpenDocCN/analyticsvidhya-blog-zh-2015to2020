<html>
<head>
<title>K-Nearest Neighbors Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k-最近邻算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbors-algorithm-7952234c69a4?source=collection_archive---------1-----------------------#2020-08-06">https://medium.com/analytics-vidhya/k-nearest-neighbors-algorithm-7952234c69a4?source=collection_archive---------1-----------------------#2020-08-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ih ii ij ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es ig"><img src="../Images/3c2ace4590ac900e4019393c0913939b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QPWeWP5FWVMlXNgu.png"/></div></div></figure><p id="84b6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp"> KNN是一种非参数的懒惰学习算法。非参数意味着没有对基础数据分布的假设。换句话说，从数据集确定的模型结构。这在实践中非常有用，因为大多数真实世界的数据集并不遵循数学理论假设。</em></p><p id="b2d2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp"> KNN是一种最简单、最传统的非参数样本分类技术。给定一个输入向量，KNN计算向量之间的近似距离，然后将尚未标记的点分配到其K-最近邻的类中。</em></p><p id="8f2a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">懒惰算法意味着它不需要任何训练数据点来生成模型。测试阶段使用的所有训练数据。这使得培训更快，测试阶段更慢，成本更高。昂贵的测试阶段意味着时间和内存。在最坏的情况下，KNN需要更多的时间来扫描所有的数据点，并且扫描所有的数据点将需要更多的内存来存储训练数据。</em></p><h1 id="2919" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">用于分类的K-NN</em></h1><p id="88c7" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">分类是一种监督学习。它指定了数据元素所属的</em><strong class="it hj"><em class="jp"/></strong><em class="jp">类，当输出有有限和离散值时最好使用。它还为输入变量预测了一个</em> <strong class="it hj"> <em class="jp">类</em> </strong> <em class="jp">。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es kt"><img src="../Images/e74f83359d942f0245666104896dbe0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*in8TsLRt_9yHK4uLa1ZVHg.png"/></div></figure><p id="a038" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">考虑给定的评论是肯定的(或否定的)，分类是关于如果我们给一个新的查询点确定(或预测)给定的评论是肯定的(或否定的)。</em></p><p id="92f7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">分类就是学习给定点的函数。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es ky"><img src="../Images/0e8b4a2e71b40fc3dc4f5593df372384.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*CYrc--s6ASclKKZL7Uu6Qw.png"/></div></figure><p id="c1cb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"><em class="jp">K-NN算法是如何工作的？</em>T29】</strong></p><p id="0419" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">在K-NN中，K是最近邻的个数。邻居数量是核心决定因素。如果类的数量是2，k通常是奇数。当K=1时，该算法称为最近邻算法。这是最简单的情况。假设P1是标签需要预测的点。首先，您找到离P1最近的一个点，然后将最近点的标签分配给P1。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es kz"><img src="../Images/cb72288b83bd9753f66c9e1db4d94383.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/0*hiqVoInIqFwNBsBc.png"/></div></figure><p id="8eb6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">假设P1是标签需要预测的点。首先，找到离P1最近的k个点，然后根据其k个邻居的多数投票对这些点进行分类。每个对象为它们的类投票，投票最多的类被作为预测。要查找最近的相似点，您可以使用距离度量来查找点之间的距离，例如欧几里德距离、汉明距离、曼哈顿距离和闵可夫斯基距离。</em></p><p id="a36a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp"> K-NN有以下基本步骤:</em> </strong></p><ol class=""><li id="164a" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated"><em class="jp">计算距离</em></li><li id="307b" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">寻找最近的邻居</em></li><li id="d6d8" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">投票给标签</em></li><li id="236d" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">取多数票</em></li></ol><p id="771c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"><em class="jp">K-NN的失败案例:</em> </strong></p><p id="d25e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp"> 1。当查询点远离数据点时。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es lo"><img src="../Images/77e0d0e43b17a7f8f4ee4deabf3dc764.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*bnXJWey9qPNdevBD-ihXfg.png"/></div></figure><p id="3552" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp"> 2。如果我们有混乱的数据集。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es lp"><img src="../Images/031a0a9d22835b977a2e00c382e7c1f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*bI2fwcjRPqVVbk0Rb1BOSA.png"/></div></figure><p id="7e2e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">对于上图显示的杂乱数据集，在上面的数据集中没有任何有用的信息。在这种情况下，算法可能会失败。</em></p><p id="cf56" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"><em class="jp">K-NN中的距离测度:</em> </strong> <em class="jp">机器学习中的距离测度主要有以下四种。</em></p><ol class=""><li id="e820" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated"><em class="jp">欧几里德距离</em></li><li id="fe83" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">曼哈顿距离</em></li><li id="4e6a" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">闵可夫斯基距离</em></li><li id="df23" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">海明距离</em></li></ol><blockquote class="lq lr ls"><p id="2b82" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><strong class="it hj"> <em class="hi">欧氏距离</em> </strong></p><p id="4c32" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><em class="hi">平面或三维空间中两点之间的欧几里德距离测量连接两点的线段的长度。这是表示两点间距离的最明显的方式。欧几里得距离标志着两点间最短的路线。</em></p></blockquote><p id="ad61" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">勾股定理可以用来计算两点之间的距离，如下图所示。如果点(x1，y1)(x1，y1)和(x2，y2)(x2，y2)在二维空间中，那么它们之间的欧几里德距离是</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es lw"><img src="../Images/8978eb78af757630e568710beeb18e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*Pp0fJb_ejUjd1snvQk4sTQ.png"/></div></figure><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es lx"><img src="../Images/7d5be4cd8110c80abe34502b7841871c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*kKOwgAm7eU7OoXQSEEhu3w.png"/></div></figure><p id="9a2a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">欧氏距离称为一个向量的L2范数。</em> </strong></p><p id="e712" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">范数是指两个向量之间的距离。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es ly"><img src="../Images/6264c47694fa034af7c3e104135f0616.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*2BKwQPGSWlvku8EIH0btAQ.png"/></div></figure><p id="f6b4" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">欧几里得距离一个原点的距离由</em>给出</strong></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es lz"><img src="../Images/4b3ff28a7d0f735ce19d32be60d53901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yIKOKX2g6J-Lc0jr.png"/></div></div></figure><blockquote class="lq lr ls"><p id="3e2a" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><strong class="it hj"> <em class="hi">曼哈顿距离</em> </strong></p><p id="d27c" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><em class="hi">两个向量(城市街区)之间的曼哈顿距离等于向量之间距离的1范数。所涉及的距离函数(也称为“度量”)也称为“出租车”度量。</em></p></blockquote><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es ma"><img src="../Images/60331766c3d2efe198ddd09f07cfd1d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*jpV6sai6_utRFyBPHB5wGA.png"/></div></figure><p id="553e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">两个向量之间的曼哈顿距离称为一个向量的L1范数。</em>T53】</strong></p><p id="1fa8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">在L2范数中，我们取元素向量之间的差的平方和，在L1范数中，我们取元素向量之间的绝对差的和。</em></p><p id="2c62" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">两点(x1，y1)与(x2，y2)之间的曼哈顿距离为:<br/></em><strong class="it hj"><em class="jp">| x1—x2 |+| y1—y2 |。</em> </strong></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mb"><img src="../Images/7ebe6df73651f9795cd3275f83bf3e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*irrvGKD2djlWOrjHm7ee8Q.png"/></div></figure><p id="f5ca" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">曼哈顿距离</em> </strong> <em class="jp"> </em> <strong class="it hj"> <em class="jp">由</em> </strong>给出</p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mc"><img src="../Images/efad5b232144d9eb067b90a0ca170de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*xOw-NUmU_Nhq8gxM2LwlqQ.png"/></div></figure><blockquote class="lq lr ls"><p id="c595" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><strong class="it hj"> <em class="hi">闵可夫斯基距离</em> </strong></p><p id="9b87" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><em class="hi">闵可夫斯基距离</em> <strong class="it hj"> <em class="hi"> </em> </strong> <em class="hi">是赋范向量空间中的度量。闵可夫斯基距离用于向量的距离相似性。给定两个或多个向量，求这些向量的距离相似度。</em></p></blockquote><p id="86a7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">闵可夫斯基距离称为向量的LP范数。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es md"><img src="../Images/d1b499affd80d59697cdcf37eba68eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*2_VeTPy1oZAmPrjFPZvfUA.png"/></div></figure><p id="d130" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp"> p！=0，P总是大于0(p &gt; 0) </em></p><h1 id="e9c7" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">欧几里得距离与闵可夫斯基距离</em></h1><p id="ebed" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">当</em> <strong class="it hj"> <em class="jp"> p = 2 </em> </strong> <em class="jp">时，闵可夫斯基距离与欧几里得距离相同。</em></p><h1 id="0025" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">曼哈顿距离闵可夫斯基距离</em></h1><p id="8829" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">当</em><strong class="it hj"><em class="jp">p = 1</em></strong><em class="jp">时，闵可夫斯基距离与曼哈顿距离相同。</em></p><blockquote class="lq lr ls"><p id="dc2d" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><strong class="it hj"> <em class="hi">海明距离</em> </strong></p><p id="b866" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><em class="hi">汉明距离是比较两个二进制数据串的度量。在比较两个等长的二进制字符串时，汉明距离是两个位不相同的位数。</em></p></blockquote><p id="3ed3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">它用于文本预处理，二元向量，布尔向量。</em></p><p id="e0ba" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">考虑x1，x2是布尔向量，</em></p><p id="7471" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp"> x1=[0，1，0，1，1，0，1] </em></p><p id="7f8c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp"> x2=[1，1，0，0，1，0，0] </em></p><p id="2216" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">汉明距离(x1，x2)=#二进制向量不同的位置/维数。</em></p><p id="71d1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">上例汉明距离(x1，x2)=3 </em></p><p id="1355" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">基因编码序列中使用的汉明距离。</em></p><blockquote class="lq lr ls"><p id="04b8" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><strong class="it hj"> <em class="hi">余弦距离和余弦相似度:</em> </strong></p></blockquote><p id="3d56" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">余弦相似性度量内积空间的两个向量之间的相似性。它通过两个向量之间的夹角余弦来测量，并确定两个向量是否大致指向同一方向。在文本分析中，它经常被用来度量文档的相似性。</em></p><p id="3b77" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">余弦相似性是有利的，因为即使两个相似的文档相距欧几里德距离很远(由于文档的大小)，它们仍有可能朝向更近。角度越小，余弦相似度越高。</em></p><p id="b940" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">余弦相似度和余弦距离的关系可以定义如下。</em></p><ol class=""><li id="6a00" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated"><strong class="it hj"> <em class="jp">两个向量之间的距离越大，相似度越小。</em>T55】</strong></li></ol><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es me"><img src="../Images/9bc2fdda3bd986d1432be353e0ffc24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*h6PTzEw696XvZQ-Bhv05eQ.png"/></div></figure><p id="8ea1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp"> 2。当两个向量之间的距离减小时，相似性增加。</em>T59】</strong></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mf"><img src="../Images/55af0e9d6a743055895835c465c2cc8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*TdHz89RkLs24NxDx--uUXw.png"/></div></figure><blockquote class="lq lr ls"><p id="efe3" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><strong class="it hj"> <em class="hi">余弦相似度和余弦距离:</em> </strong></p></blockquote><p id="9c0a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">余弦相似性说的是，要找到两点或向量之间的相似性，我们需要找到它们之间的角度。</em></p><p id="7f94" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">求余弦相似度和距离的公式如下:</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mg"><img src="../Images/f194b4bedb2664a8badc5801060e42af.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*7OhVc4GA8bI-B_sejt-zzg.png"/></div></figure><p id="3d3a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">余弦相似度= cosθ </em> </strong></p><p id="85da" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">余弦距离= 1-</em><strong class="it hj"><em class="jp">cosθ</em></strong></p><p id="3198" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">当余弦相似度(x1，x2)非常相似时余弦相似度(x1，x2)等于1。如果是非常不相似的余弦相似度(x1，x2)等于-1。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es mh"><img src="../Images/443f2e3a23453202f3c11cbeac28872e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJGeRJLT00gA-akgEXxkEg.png"/></div></div></figure><p id="41e4" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp"> θ </em> </strong> <em class="jp">是x1和x2之间的夹角。</em></p><p id="0e01" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">余弦距离使用两点之间的角度，而欧几里得距离使用两点之间的几何距离。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es md"><img src="../Images/d3d1df18b70ca383c65ec6b5f4d255f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/0*k_gd9bLwjdZi7yns.png"/></div></figure><p id="cd9a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">如果A和B都是单位向量，那么||A|| =||B||=1 </em></p><p id="7f61" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">比相似度，cos(</em><strong class="it hj"><em class="jp">θ</em></strong><em class="jp">)= A . B</em></p><p id="0963" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">欧氏距离和余弦距离的关系。</em> </strong></p><p id="c0dd" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">如果A和B都是单位向量，那么||A|| =||B||=1 </em></p><p id="41a2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">【欧氏距离(x1，x2)】的平方= 2(1-cos(</em><strong class="it hj"><em class="jp">θ</em></strong><em class="jp">)</em></p><p id="ef8d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">[欧几里德距离(x1，x2)]的平方=2余弦距离(x1，x2) </em></p><h1 id="fa90" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">K-NN算法的性能受三个主要因素的影响:</em></h1><ol class=""><li id="90e9" class="la lb hi it b iu ko iy kp jc mi jg mj jk mk jo lf lg lh li bi translated"><em class="jp"/><strong class="it hj"><em class="jp">距离函数</em> </strong> <em class="jp">或距离度量用于确定最近邻居。</em></li><li id="0743" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp"/><strong class="it hj"><em class="jp">决策规则用于从K个最近邻居中导出分类</em> </strong> <em class="jp">。</em></li><li id="360a" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp"/><strong class="it hj"><em class="jp">邻居数量</em> </strong> <em class="jp">用于分类新实例。</em></li></ol><h1 id="bb44" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">K-NN随K变化的决策面:</em></h1><p id="b9b4" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">当K=1时，判定曲线是锐边和非光滑曲线，分类器不出错。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es ml"><img src="../Images/73c928756f4e0a4a372bd0e868a0aefa.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*qWvNYsbbCmZcD4T6XWVkgQ.png"/></div></figure><p id="5697" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">当k=5决策面曲线光滑，分类器出现小错误时。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mm"><img src="../Images/98952dbdb9b8374807dccc75f1c5bfc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*xxE8pbETEQ9h80yqGzu2PA.png"/></div></figure><p id="dc3d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">在K-NN中，决策曲面的光滑度随着K的增加而增加。</em></p><p id="ed57" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">当k=n时，分类器给出每个查询点属于多数类。当K=n分类器产生更多错误时。</em></p><p id="6d77" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">我们用下图所示的一些玩具数据集玩了K-NN决策面。</em>T73】</strong></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mn"><img src="../Images/197508cae21328538986671cf98dc239.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*motYxpU5OuIGkrDAMp4cDA.png"/></div></figure><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mo"><img src="../Images/6d71d52bbd33659f9d7cfe633d724207.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*__1-Ut4sfPj-BSvMmnK5TA.png"/></div></figure><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mp"><img src="../Images/096128d5b23807c519cbc7310de2f814.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*z5QDa8uBNA-gOtk-6nNMoQ.png"/></div></figure><blockquote class="lq lr ls"><p id="9303" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><em class="hi">通过查看上面的图像，我们观察到随着k的增加，决策面将变得更加平滑。</em></p></blockquote><p id="8b98" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">要查看python中的完整代码和实现，请访问</em> <a class="ae mq" href="https://github.com/Sachin-D-N/Data_Science/blob/master/KNN_Decision_surface/knn_with_simple_code.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="it hj"> <em class="jp">此处</em> </strong> </a> <em class="jp">。</em></p><h1 id="c0f6" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">模型的过拟合和欠拟合</em></h1><p id="afbf" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">利用数据训练寻找给定数据的正确函数的过程称为拟合。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es mr"><img src="../Images/38481a377d3b31af244fdd534419c3a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jY8flCf4OOKgEGXOaT_jZA.png"/></div></div></figure><p id="1aff" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">当k=1时，模型过度拟合数据，因为我们的模型不会产生任何错误。</p><p id="2a9d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">当k=n时，由于我们的模型产生更多的错误，所以模型对数据拟合不足，它认为每个查询点都属于多数类。</p><p id="3827" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">过拟合和欠拟合之间的平衡很好，它会产生一些错误，因为机器学习不是完美的，犯点小错误是可以的。</p><p id="1253" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">但是你可能会想，我们怎么能确定我们的模型是欠拟合还是过拟合呢？</em>T5】</strong></p><p id="7139" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">答案是通过标绘法。</em></p><p id="0b83" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">我们希望我们的模型在交叉验证数据集上具有最大的准确性或最小的误差。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es ms"><img src="../Images/1c6852a5c62b5856c6193e36d884f2d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*CdaJC8BrhkvQ_4GUKAV1Pg.png"/></div></figure><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mt"><img src="../Images/39ed6abd04989e2a86028d3dc3fcf654.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*DTa0Da9sQEQF7LibxKa6mg.png"/></div></figure><blockquote class="lq lr ls"><p id="c348" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><strong class="it hj"> <em class="hi">训练错误:</em> </strong> <em class="hi">当一个训练好的模型再次对数据运行后返回</em> <strong class="it hj"> <em class="hi">错误</em> </strong> <em class="hi">时，就会发生训练错误。它开始返回错误的结果。顺便说一下，这里有一个逻辑假设，即您的训练集不会包括属于不同类别的相同训练样本，即冲突信息。然而，一些真实世界的数据集可能具有这种属性。</em></p><p id="6ce2" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><strong class="it hj"> <em class="hi">交叉验证错误:</em> </strong> <em class="hi">错误发生在选择最佳K时，通过使用交叉验证。</em></p></blockquote><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mu"><img src="../Images/07e6dbc129483e3fba871e172a016d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*0nf0I38UIXToQwAL1kP8mQ.png"/></div></figure><p id="1be4" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">当</em> <strong class="it hj"> <em class="jp">训练误差低</em> </strong> <em class="jp">和</em> <strong class="it hj"> <em class="jp">验证误差高</em> </strong> <em class="jp">时，我们面临上图所示的过拟合问题。</em></p><p id="b3bb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">当</em> <strong class="it hj"> <em class="jp">训练误差高</em></strong><em class="jp"/><strong class="it hj"><em class="jp">验证误差也高</em> </strong> <em class="jp">我们面临上图所示的欠拟合问题。</em></p><p id="774b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">我们选择我们的模型，因为一些训练误差和一些验证误差彼此接近，如上图所示最佳拟合。</em></p><h1 id="6000" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">如何找到最好的K？</em></h1><p id="8fe1" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">通过交叉验证，我们找到了最优k。</em></p><p id="1acf" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">交叉验证就是让函数看到一些数据，让函数看不到一些数据。</em></p><p id="f4ce" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">交叉验证(CV)是用于测试机器学习模型有效性的技术之一，它也是一种用于在数据有限的情况下评估模型的重采样程序。为了执行CV，我们需要保留一个样本/部分数据，这些数据不用于训练模型，以后我们使用这个样本进行测试/验证。</em></p><p id="cd3d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">下面是几个常用于CV的技巧。</p><ol class=""><li id="f861" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated"><strong class="it hj"> <em class="jp">训练_测试拆分方法</em> </strong> <em class="jp">。</em></li></ol><p id="50bc" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">在这种方法中，我们将完整的数据随机分为训练集和测试集。然后对训练集执行模型训练，并使用测试集进行验证，理想情况下将数据分成70:30或80:20。如果我们的数据很大，并且我们的测试样本和训练样本具有相同的分布，那么这种方法是可以接受的。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es mv"><img src="../Images/395246a2f19c96fe580ce5efc4ec5f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*28AQG-7vcPJ_L9nq-z-m5A.png"/></div></figure><p id="9310" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">我们可以使用切片将数据手动拆分为训练集和测试集，或者我们可以使用sci-kit-learn方法的</em><strong class="it hj"><em class="jp">train _ test _ split</em></strong><em class="jp">来完成此任务。完整的文档是</em> <a class="ae mq" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"> <em class="jp">这里的</em> </a> <em class="jp">。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es mw"><img src="../Images/de1804dd26e75ce52f8eb2211a111714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KOEU-G1goMHgfdbDxTCMYw.png"/></div></div></figure><h2 id="6332" class="mx jr hi bd js my mz na jw nb nc nd ka jc ne nf ke jg ng nh ki jk ni nj km nk bi translated"><strong class="ak"> <em class="if">在这个方法中有一个问题，</em> </strong></h2><p id="cb7d" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">使用这种方法，如果我们的数据有限，可能会出现较高的偏差，因为我们会错过一些没有用于训练的数据信息。</em></p><p id="d8dd" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">所以对于有限的数据，还有一种方法叫做</em> <strong class="it hj"> <em class="jp"> K倍交叉验证。</em>T25】</strong></p><blockquote class="lq lr ls"><p id="a360" class="ir is jp it b iu iv iw ix iy iz ja jb lt jd je jf lu jh ji jj lv jl jm jn jo hb bi translated"><strong class="it hj"> <em class="hi"> K倍交叉验证:</em> </strong></p></blockquote><p id="2bd9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp"> K-Fold很流行，也很容易理解，与其他方法相比，它通常会产生一个偏差较小的模型。因为它确保了原始数据集中的每个观察值都有机会出现在训练集和测试集中。如果我们的输入数据有限，这是最好的方法之一。该方法遵循以下步骤。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es nl"><img src="../Images/6f3a2da38580efecb88527d21e9b6bea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zLM9IS0wkpu5UYU8.png"/></div></div></figure><p id="2632" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">该方法遵循以下步骤。</em></p><ol class=""><li id="58e6" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated"><em class="jp">将整个数据集随机分成k个“折叠”</em></li><li id="1ba8" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">对于数据集中的每个k层，在数据集的k-1层上构建模型。然后，测试模型以检查第k次折叠的有效性</em></li><li id="6649" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">记录你在每个预测上看到的错误</em></li><li id="d09d" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">重复此操作，直到每个k折叠都作为测试集</em></li><li id="aa7c" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">你记录的k个误差的平均值被称为交叉验证误差，并将作为你的模型的性能指标</em></li></ol><p id="fd8b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">重复这个过程，直到每个K-fold都作为测试集。然后取你记录分数的平均值。这将是该模型的性能指标。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es nm"><img src="../Images/63ab76b97782f2baf053b19448ac01b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*u_OZmlb2NPLj9qsH1rnmPQ.png"/></div></figure><p id="1f74" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">这将花费更多的时间，但是，在这种方法中，我们使用了数据的每一部分进行训练，并对数据的每一部分进行交叉验证。</em></p><p id="f1f8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">要查看python中交叉验证的完整代码和实现，请访问这里的</em><a class="ae mq" href="https://github.com/Sachin-D-N/Data_Science/blob/master/KNN_Decision_surface/K_fold.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="it hj"><em class="jp"/></strong></a><em class="jp">。</em></p><h1 id="1dab" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">用于回归的K-NN</em></h1><p id="e29c" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">在回归中输出不再是小的有限集类的一部分，输出Yi属于实数(或)无限集类。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es nn"><img src="../Images/113cda2a0b8259d6b035511f466df465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*iQVno24bUB4Vg-zfyrG7Ug.png"/></div></figure><p id="034b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"><em class="jp">K-NN回归步骤如下。</em> </strong></p><ol class=""><li id="c80c" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated"><em class="jp">通过交叉验证(或)K重交叉验证找到给定点的最优-K。</em></li><li id="2feb" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">为给定的查询点找到K-最近邻。</em></li><li id="3eb8" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">取所有K近邻的平均值或中值。</em></li><li id="798f" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">中位数不太容易出现离群值。</em></li><li id="d88a" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">对于K-NN分类，我们采用K-最近邻的多数投票，对于K-NN回归，我们采用K-最近邻的均值(或中值)。</em></li><li id="c43e" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp"> K-NN是从分类到回归的简单扩展。</em></li></ol><h1 id="0a74" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak"> <em class="if">加权K-NN: </em> </strong></h1><p id="56cd" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">加权K-NN赋予各点权重的重要性。</em></p><p id="10a6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">加权K-NN </em> </strong> <em class="jp">是K近邻的修改版。…最简单的方法是采取多数投票，但是如果最近的邻居在距离上变化很大，并且最近的邻居更可靠地指示物体的类别，这可能是一个问题。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es no"><img src="../Images/10809c71cf0b0b4839ca4de598298157.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*eL6TNp6ZRZKysooKSeWhvQ.png"/></div></figure><p id="ac90" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">考虑上述图像，假设5-NN，然后通过距离测量，我们将得到3个正点和2个负点，通过采取多数投票，我们得出结论，给定的查询点是正的。</em></p><p id="0818" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">假设从查询点到5-NN的距离与下图相同。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es np"><img src="../Images/488a314ce30d068f8b9e1b66e5f03be0.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*aZ0CsgYNxE-wzJBymiE5WA.png"/></div></figure><p id="a139" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">但这并不能真实地与正点数相比，负点数与查询点数非常接近，所以要考虑5-NN个点的权重。</em></p><p id="3bd6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">点的权重和距离之间的关系由</em>给出</p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es nq"><img src="../Images/b001810b79cbfff76b172c3c317221f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*vznvIwo-R3-qUaBZL6KBWw.png"/></div></figure><p id="e122" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">当距离增加时，重量减少，距离减少时，重量增加。</em></p><p id="6412" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">现在考虑每个点的权重</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es nr"><img src="../Images/d0b6b42b4fdc9d707b1969a1cadee568.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*XyqcTKm95QQ6FIdmIkHw3Q.png"/></div></div></figure><p id="5f19" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">让我们考虑正类和负类的权重之和。</em></p><p id="b2e4" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">负类的权重是10+5 =5 </em></p><p id="c191" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">正类的权重为3.33+1.25+0.66= 5.24 </em></p><p id="cdd3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">考虑哪个类是最高权重，该类成为预测类，因此负类从加权K-NN成为我们的预测类。</em></p><h1 id="e025" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">k-NN的时间复杂度</em></h1><ol class=""><li id="f271" class="la lb hi it b iu ko iy kp jc mi jg mj jk mk jo lf lg lh li bi translated"><em class="jp">我们来看看k-NN的时间复杂度。我们在d维空间。</em></li><li id="54bf" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">为了简单起见，让我们假设我们已经处理了一些输入，我们想知道增加一个数据点的时间复杂度。</em></li><li id="6385" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">在训练的时候，k-NN只是简单的记住它看到的每个数据点的标签。<br/>这意味着再增加一个数据点是O(d)。</em></li><li id="12bb" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">测试时，我们需要计算新数据点和我们训练的所有数据点之间的距离。</em></li><li id="9e8b" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">如果NN是我们训练过的数据点的数量，那么我们训练的时间复杂度是O(dn)。对一个测试输入进行分类也是O(dn)。</li></ol><h1 id="aeef" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">K-NN的局限性</em></h1><ol class=""><li id="273d" class="la lb hi it b iu ko iy kp jc mi jg mj jk mk jo lf lg lh li bi translated"><em class="jp">大型存储要求。</em></li><li id="08dd" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp">计算密集型召回。</em></li><li id="93aa" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">K-NN的时间复杂度和空间复杂度一样多。</li><li id="580e" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp"> K-NN不适合互联网公司使用的低延迟应用。</em></li></ol><h1 id="3daf" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if"> K-D树</em></h1><p id="6d7f" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">由于K-NN的空间和时间复杂度较大，为了降低空间和时间复杂度，K-D树被发明出来。</em></p><p id="5a9e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">K-D树(也称为K维树)是二叉查找树，其中每个节点中的数据是空间中的K维点。简而言之，它是一种空间分区数据结构，用于组织K维空间中的点。</em></p><p id="0973" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">k-d树保证log2 n深度，其中n是集合中的点数。传统上，k-d树存储d维空间中的点，这些点相当于d维空间中的向量。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es ns"><img src="../Images/ac6e8e17569983924990676e4159a039.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*bMFks1N8a7_P30w5VfA7aQ.png"/></div></figure><p id="f7fa" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">选择X轴，将每个点投影到X轴上，然后计算中值，并使用中值分割数据。</em></p><p id="26b3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">然后选择Y轴并将每个点投影到Y轴上，然后计算中值并使用中值分割数据。</em></p><p id="dd36" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">重复上述步骤，交替变换轴，建立树。</em></p><p id="916a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">K-D树中的一个非叶节点将空间分成两部分，称为半空间。</em></p><p id="2c35" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">该空间左边的点由该节点的左子树表示，该空间右边的点由右子树表示。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es nt"><img src="../Images/686ca4f1abae5d35c7b8679ec5b32f01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B3WD04uHx_L330CdqrQKyg.png"/></div></div></figure><p id="1b6c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">如果只有一个点，用那个点形成一片叶子。否则，用一条垂直于其中一个轴的线将这些点分成两半。递归构造两组点的k-d树。将垂直于轴的点分割成最宽的分布。</p><h1 id="0bf6" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">使用K-D树查找最近的邻居</em></h1><p id="1d89" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">递归搜索，查找与查询相同单元格中的点。在返回时，搜索每个子树，在那里可能会找到比你已经知道的点更近的点。</em></p><p id="633b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">保留最接近找到的查询点的变量。一旦子树的边界框表明它们不能包含任何比查询点更近的点，就修剪子树。搜索子树以便最大化修剪的机会。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es nu"><img src="../Images/1bd49e3e525d72663ae23e8f6ebc915a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6xktMI6ihhGapzqo02CCow.png"/></div></div></figure><p id="f5b0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">为逐步实现构建K-D树访问</em> <a class="ae mq" href="https://courses.cs.washington.edu/courses/cse373/02au/lectures/lecture22l.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="it hj"> <em class="jp">此处</em> </strong> </a> <strong class="it hj"> <em class="jp">。</em>T41】</strong></p><h1 id="bf84" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">k-D树的时间复杂度</em></h1><p id="3844" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated">在一个合理的模型中，K-D树每次搜索的平均时间为O(log n)。(假设d很小)。k-d树的存储量是O(n)。假设d很小，预处理时间为O(n log n)。</p><h1 id="d2ce" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">k-D树的局限性</em></h1><ol class=""><li id="14a6" class="la lb hi it b iu ko iy kp jc mi jg mj jk mk jo lf lg lh li bi translated"><em class="jp">当d不小时——时间复杂度急剧增加。</em></li><li id="6c17" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">K-D树只适用于数据均匀分布且维数较小的情况。但是大多数真实世界的数据并不是均匀分布的。</li><li id="e9e3" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated"><em class="jp"> K-D树不是为K-NN发明的，它主要是为信息检索和计算机图形学发明的。</em></li></ol><h1 id="0ab8" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">区分位置哈希</em></h1><p id="795e" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated">寻找最近邻的相似点是一种很好的技巧。为此，它使用了哈希的概念。</p><p id="7143" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">你可能熟悉哈希表是如何构造的。如果没有，请点击此</em> <a class="ae mq" href="https://why-android.com/2018/03/26/gentle-introduction-to-hashing/" rel="noopener ugc nofollow" target="_blank"> <em class="jp">链接</em> </a> <em class="jp">快速复习哈希概念。这是一种非常有效的数据结构，允许我们在O(1)时间内执行操作。</em></p><p id="c138" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp"> LSH是一种基于哈希的算法，用于识别近似的最近邻居。在正常最近邻问题中，空间中有一堆点(我们称之为训练集),给定一个新点，目标是识别训练集中最接近给定点的点。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es nv"><img src="../Images/b54c6b211eaaac37e4d56734188f6999.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*zz4u8gei_6Hw-tvXfYAfvg.png"/></div></figure><p id="797c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">位置敏感哈希是一组技术，可显著加快数据的邻居搜索或近似重复检测。例如，这些技术可以用来以令人印象深刻的速度过滤掉重复的网页，或者从地理空间数据集中对附近的点进行近乎恒定时间的查找。</em></p><p id="10fa" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">哈希函数通常具有以下关键属性:</em></p><ol class=""><li id="5673" class="la lb hi it b iu iv iy iz jc lc jg ld jk le jo lf lg lh li bi translated">它们将某种类型的输入(如字符串或浮点数)映射到离散值(如整数)。</li><li id="003c" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo lf lg lh li bi translated">根据输入的关键属性，它们被设计成两个输入将产生不同或相同的散列输出。</li></ol><p id="a3c2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">它是如何工作的？</em>T29】</strong></p><p id="275b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">我们对每个可用的数据点应用哈希函数。这给了我们可以放置点的桶或钥匙。</em></p><p id="08c6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们尝试最大化碰撞，以便相似的点进入同一个桶。</p><p id="893a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">现在，如果我们获得一个未标记的查询点并应用哈希函数，那么它将转到存在相似点的同一个桶。</em></p><p id="72db" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">这使得查找查询点的最近邻居变得容易。随后，我们可以应用k-NN来预测它的类别。</em></p><h1 id="5e39" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">使用余弦相似度的位置敏感哈希</em></h1><p id="2e0d" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">我们试图解决的问题是，给定一个具有预分类数据点的数据集，预测一个新数据点的类别。</em></p><p id="c534" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp"> LSH是一种概率随机算法。因此，它并不完善，并广泛应用于计算机视觉。</em></p><p id="7539" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> <em class="jp">什么是余弦相似度？</em> </strong> <em class="jp"> <br/>高层次余弦相似度可以告诉我们两个点有多相似。为此，我们计算两个点的矢量表示，然后找出两个矢量之间的角度。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es nw"><img src="../Images/046c746529c0e7b03473e2e89b3af966.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/0*2SEGVnIoq2jX5ZJi.png"/></div></figure><p id="15f0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">向量a和b之间的相似度可以由它们之间的夹角余弦给出。</em></p><p id="2046" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">我们可以使用这个概念来计算数据点的哈希值。</em></p><p id="0925" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">现在我们知道了余弦相似性，我们可以用它来计算数据点的LSH值。为此，我们使用超平面来划分空间。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es nx"><img src="../Images/56b4f6479d05b42a286ad1d38f9cbfc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*p7jmTPT-VzpaRRXxbfm4sg.png"/></div></figure><p id="d8c4" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">为了简单起见，考虑一个具有X-Y轴的二维空间。我们可以使用3个平面(或)线π1、π2和π3将它分成几个区域。</em></p><p id="4967" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">因此，假设数据点“x1”、“x2”、“x3”、“x4”、“x5”将位于这些区域之一。对于每一个平面，我们可以用法向量的概念找到这个点所在的方向。</em></p><p id="f467" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">这样我们可以找到每个平面的值。对于每个平面，该值将是+1或-1。我们可以用这个来计算散列密钥。</em></p><p id="87d9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">如果同一个桶中的两个点对于一个超平面来说π比距离可以更近。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es ny"><img src="../Images/93497b3dfc713b5cb49803b79c4fc9fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*E7_beGgZevcSRZNzRamwVg.png"/></div></figure><p id="dc9d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">一旦我们有了哈希表，我们就可以用它来确定新数据点的键。然后找到最近的邻居。</em></p><p id="334c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">假设新点落在key =1的桶中。那么我们知道它离点很近。接下来，应用k-NN找到它的分类。</em></p><p id="13f7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">作为我们的距离度量，有些时候可能会错过余弦相似度中的最近邻居。为了解决这个问题，为所有点构建多个散列表，最后在桶中取公共的最近邻居。</em></p><p id="b15a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">随着超平面数量的增加，切片的数量增加多于每个切片的点数减少。</em></p><h1 id="780b" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">使用欧几里德距离的位置敏感散列法</em></h1><p id="bf36" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp">这与我们之前讨论的余弦相似性的位置敏感哈希(LSH)非常相似。我将在这里引用相同的内容，所以如果您在继续之前经历相同的内容会更好。</em></p><p id="bb8f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">不同之处在于我们计算哈希值的方式。正如我们所看到的，我们可以用平面来划分区域。在每个区域，我们可以有数据点。</p><p id="5065" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">将平面分成小部分，将每个数据点投影到平面上，对于每个数据点，获取沿每个平面的距离，并使用它来计算哈希值。</em></p><p id="839e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">垂直于平面投影所有点。相似的(或)更近的点应该到达相同的区域(或)桶。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es nz"><img src="../Images/9e0091689c7c2180e696b543f469c9a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*MjR7j2g6e2Tp1qU3MKdi5A.png"/></div></figure><p id="f5f0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">将整个区域分解成块(或)区域。</em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es oa"><img src="../Images/333f3eb1bf226f8a2ae0d4a87ad6a41a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*dHRee6IHTPb4fKiMB9ZtEA.png"/></div></figure><p id="c4db" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">如果两个点彼此靠近，那么它们很有可能映射到相同的区间。反之，如果点距离较远，则不太可能落入同一个桶</p><figure class="ku kv kw kx fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es ob"><img src="../Images/fab910a2527b271e829a5ca263dc4072.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*3gQFpjMXmqEN1HLCYaDErw.png"/></div></div></figure><p id="e8cb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">这样我们可以找到每个平面的值。对于每个平面，该值可以是正值，也可以是负值。我们可以用这个来计算散列密钥。</em></p><p id="8215" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">一旦我们有了哈希表，我们就可以用它来确定新数据点的键。然后找到最近的邻居。</em></p><p id="3702" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">假设新点落在key =1的桶中。那么我们知道它离点很近。接下来，应用k-NN找到它的分类。</em></p><p id="b821" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">有关</em> <strong class="it hj"> <em class="jp">区分位置哈希</em> </strong> <em class="jp">的更多信息，请访问</em> <a class="ae mq" href="https://cse.iitkgp.ac.in/~animeshm/algoml/lsh.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="it hj"> <em class="jp">此处</em> </strong> </a> <strong class="it hj"> <em class="jp">。</em>T25】</strong></p><h1 id="5c42" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if"> K-NN作为概率类标签</em></h1><p id="44d0" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated"><em class="jp"> KNN是一个非常简单的算法，具有被证明的错误率。</em></p><p id="9ef0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">考虑2类分类Y</em><strong class="it hj"><em class="jp">∈{</em></strong><em class="jp">0，1}为给定点X. </em></p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es oc"><img src="../Images/91793063289616f1ddc15b1e16abe6fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*cf3Ux6QwbU3o9NaFPmvW0g.png"/></div></figure><p id="2b69" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">假设7-NN (K=7)，查询点Xq属于4个假设为0的红色点作为负类标签，3个假设为1的蓝色点作为正类标签，那么通过7-NN的多数投票，将其归类为0。</em></p><p id="61cf" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">如果我们考虑概率方法，它给出了预测的确定性，而不是多数票。</p><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es od"><img src="../Images/610642a52e968c0845b5f46643208f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*gEGD9isdBW0WMXE3pGfZEg.png"/></div></figure><figure class="ku kv kw kx fd ik er es paragraph-image"><div class="er es oe"><img src="../Images/05bfdad437c6cf6e880a6ec3f44c3dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*qk7e4tzjxY08pn5CW2VzdA.png"/></div></figure><p id="bfee" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">考虑为负点P(Yq</em><strong class="it hj"><em class="jp">∈</em></strong><em class="jp">0)= 4/7 = 0.57</em></p><p id="ff85" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">考虑为正的点P(Yq</em><strong class="it hj"><em class="jp">∈</em></strong><em class="jp">1)= 3/7 = 0.42</em></p><p id="2775" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从上面的概率方法，我们得出结论，查询点属于负类的是57 %，而查询点属于正类的是42 %。</p><h1 id="dd8b" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><em class="if">K个最近邻的优劣</em></h1><ul class=""><li id="35f6" class="la lb hi it b iu ko iy kp jc mi jg mj jk mk jo of lg lh li bi translated"><em class="jp">简单的算法，因此容易解释预测。</em></li><li id="37b8" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo of lg lh li bi translated"><em class="jp">非参数so对底层数据模式不做任何假设。</em></li><li id="6e30" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo of lg lh li bi translated"><em class="jp">用于分类和回归。</em></li><li id="946e" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo of lg lh li bi translated"><em class="jp">与其他机器学习算法相比，最近邻的训练步骤要快得多。</em></li></ul><h1 id="cde2" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">K个最近邻的缺点</h1><ul class=""><li id="6c72" class="la lb hi it b iu ko iy kp jc mi jg mj jk mk jo of lg lh li bi translated"><em class="jp"> KNN的计算开销很大，因为它在预测阶段搜索新点的最近邻居</em></li><li id="6777" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo of lg lh li bi translated"><em class="jp">高内存需求，因为KNN必须存储所有数据点</em></li><li id="98bf" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo of lg lh li bi translated"><em class="jp">预测阶段成本非常高</em></li><li id="d206" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo of lg lh li bi translated"><em class="jp">对异常值敏感，准确性受噪声或无关数据影响。</em></li></ul><p id="39c0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">这是K近邻的小介绍。</em></p><h1 id="e253" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">参考</h1><ul class=""><li id="5552" class="la lb hi it b iu ko iy kp jc mi jg mj jk mk jo of lg lh li bi translated">应用人工智能</li><li id="ef40" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo of lg lh li bi translated">Coursera</li><li id="5ed8" class="la lb hi it b iu lj iy lk jc ll jg lm jk ln jo of lg lh li bi translated">数据营</li></ul><p id="844e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">感谢您的阅读和耐心。如果我的帖子有错误，请告诉我。如果你发现帖子中有什么错误或者有什么要补充的，让我们在评论中讨论吧...</em></p><p id="f46e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">快乐学习！！</p></div></div>    
</body>
</html>