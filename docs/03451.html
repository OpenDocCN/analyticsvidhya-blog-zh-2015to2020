<html>
<head>
<title>RVL-CDIP(Ryerson Vision Lab Complex Document Information Processing)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RVL-CDIP(瑞尔森视觉实验室复杂文件信息处理)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/rvl-cdip-ryerson-vision-lab-complex-document-information-processing-aa30b00a2b1e?source=collection_archive---------6-----------------------#2020-02-02">https://medium.com/analytics-vidhya/rvl-cdip-ryerson-vision-lab-complex-document-information-processing-aa30b00a2b1e?source=collection_archive---------6-----------------------#2020-02-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f74e590571bd070f1bd1a732d0ca46f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hdIAWI0rce-CMxgk"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">安妮·斯普拉特在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="7437" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">描述:</h1><p id="4d22" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">RVL-CDIP(瑞尔森视觉实验室复杂文档信息处理)数据集由16类400，000幅灰度图像组成，每类25，000幅图像。有320，000幅训练图像、40，000幅验证图像和40，000幅测试图像。调整图像大小，使其最大尺寸不超过1000像素。</p><p id="f242" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">以下是数据集中的类，以及每个类的示例:</p><p id="cd90" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">信函备忘录电子邮件文件夹表单手写发票广告<br/>预算新闻文章演示科学出版物问卷简历科学报告<br/>规格<br/>该数据集是IIT-CDIP测试集1.0的子集，可在此处公开获取。该数据集的文件结构与IIT集合中的文件结构相同，因此可以引用该数据集进行OCR和其他元数据。IIT-CDIP数据集本身就是传统烟草文档库的一个子集。</p><p id="7662" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">演职员表:<a class="ae iu" href="https://www.cs.cmu.edu/~aharley/rvl-cdip/" rel="noopener ugc nofollow" target="_blank">https://www.cs.cmu.edu/~aharley/rvl-cdip/</a></p><h1 id="f12b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">深度学习的使用:</h1><p id="925c" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">提出了一种用于文档结构学习的基于区域的深度卷积神经网络框架。这项工作的贡献包括基于区域的分类器的有效训练和文档图像分类的有效集成。通过在ImageNet数据集上从预训练的VGG16架构导出权重来使用初级“域间”迁移学习，以在整个文档图像上训练文档分类器。利用基于区域的影响建模的性质，使用二级“域内”转移学习来快速训练图像片段的深度学习模型。最后，利用基于堆叠概括的集成来组合基本深度神经网络模型的预测。所提出的方法在流行的RVL-CDIP文档图像数据集上实现了92.21%的最新精度，超过了现有算法设定的基准。</p><h1 id="ea54" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">数据来源:</h1><p id="0fb6" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">数据下载地点:<a class="ae iu" href="https://www.cs.cmu.edu/~aharley/rvl-cdip/" rel="noopener ugc nofollow" target="_blank">https://www.cs.cmu.edu/~aharley/rvl-cdip/</a>。我使用colab来完成这个项目。数据是使用curl-we-get软件直接在colab平台上下载的，然后提取并分发到16个类中。</p><h1 id="cd34" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">数据概述:</h1><p id="9098" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">该数据集由针对美国烟草公司的法律诉讼文件的扫描灰度图像组成，并被分为16个类别。数据集被细分为训练集、验证集和测试集，每个数据集分别包含320000、40000和40000幅图像。数据集已经被分割成train/dev/test，每个分割中分别有320k/40k/40k文件可用。未压缩时，数据集大小约为100GB。</p><h1 id="3a5e" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">映射到现实世界的ml问题:</h1><p id="a388" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">这是一个多类分类问题，对于给定的原始图像作为输入，我们需要预测它属于16类中的哪一类。</p><h1 id="46cb" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">绩效指标:</h1><p id="6cbb" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我使用准确度作为性能度量，因为所有的图像被分为16类，并且每类中的图像数量是均等分布的。</p><h1 id="d8c8" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">问题陈述:</h1><p id="7198" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">取一个raw图像作为输入，预测它属于哪个类作为输出。</p><h1 id="738d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">解决问题的第一步:</h1><p id="53e4" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">解决该问题的第一种方法是制作16个不同的文件夹，并将训练和交叉验证图像分发给各自的文件夹。下一步是使用图像数据生成器对这些图像分别进行整体、页眉、页脚、每隔一个图像序列的左右部分、验证和测试图像，并分别保存模型。接下来，执行所有五个模型的特征提取部分，并堆叠所有模型，然后使用堆叠的模型预测标签。最后一步是制作一个神经网络，并使用这个具有堆叠模型的神经网络作为神经网络拟合部分的输入。案例研究的最后一部分是将管道作为项目的一部分，即以原始图像作为输入，以hollistic、header、footer、left和right作为输入，使用现有的模型提取特征，并使用一些原始的看不见的图像来测试模型是否能预测类别。</p><h1 id="0335" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">rvl数据集的探索性数据分析:</h1><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/143fff33557a04bdee19ec0f0cd8d44d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D7a-bxC6kIx8IjT46jJ7gw.png"/></div></div></figure><p id="3936" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">首要任务是理解数据中实际提供给我们的是什么。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/70fe4d2cf26882e4b73c47bf28586fc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CKnvBd-3VY7ojoI99WNVFg.png"/></div></div></figure><p id="c22e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">从给定的图表中可以看出，数据是均匀分布的。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/e08f0a7f32fb8ccc069baafe624c52da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OqUtfVh9JoBDTFN_F8CaFw.png"/></div></div></figure><p id="7ca5" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">理解每幅图像的宽度。可以看出，图像的宽度在给定的范围内变化。</p><h1 id="09e9" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">我案例研究中的模型解释:</h1><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lb"><img src="../Images/7b87602399bb3d4ed568d92ea1b75d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O3HLlQTy1w67AMQUMsXb8w.png"/></div></div></figure><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/c8a4e32a1c0d1e99c91b34b91ac6d421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5bBCbwRSetd7pKsx2gGx3g.png"/></div></div></figure><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/6ec7ce747604ef84a70f45f55297fe40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8ZXsArfjvU5wZ3x1ozLKg.png"/></div></div></figure><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/cfc32de0b6da031ca4b661fb57b2b609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iQl2yWojcq4tkpT170DDOw.png"/></div></div></figure><p id="317e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">DCNNs是目前最流行的深度学习模型之一。为此，我使用了vgg16。VGG16模型在文档分类任务上比其他DCNN模型表现得更好。我们利用这些信息来选择VGG16模型作为我们的基本分类器模型。总体架构是通过Adam优化器完成的，Adam优化器用于训练，学习率衰减基于验证集的准确性进行调整。所使用的初始权重是从在ImageNet对象识别数据集上训练的模型转移来的。为了评估各种架构，使用ImageNet1K权重和随机权重初始化模型权重。我们使用32的小批量来容纳大的训练图像。它可能会更高，但测试集上的结果似乎保持良好。首先，完全连接的vgg16层被删除，最后两层是可训练的，并在此基础上增加三层。</p><p id="7684" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在最后一层，我使用了softmax，因为它是一个多类分类问题，损失函数是分类交叉熵，因为它是一个多类分类问题。在模型中使用下降来控制过拟合。Adam optimizer的性能优于rmsprop、adagrad和adadelta，因此我使用了Adam。之后，我使用tensorboard来了解模型的性能。之后，我使用了模型检查点来保存具有最佳权重的模型。保存模型权重后，我使用该模型在数据生成器上评估该模型。</p><p id="0a9b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">然后将模型序列化为json。在解决底部、右侧、左侧和顶部等其他部分时，我使用了保存的检查点中的类似模型。在对其他三个部分进行类似的训练后，我所做的是使用predict generator提取特征，并再次保存模型的特征。以这种方式，提取并保存所有五个部分——霍利斯蒂、左、右、整体和底部部分——的训练、测试、验证的所有特征。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/9ef219fb1060e95b0aa5cffd3e715382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OhSZ-TKlW-KkwgS67M9JnQ.png"/></div></div></figure><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/a4e10f80a94913186254e8e511855de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KRFcXOIuiR48PuzF99f2xQ.png"/></div></div></figure><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/888fe43f487cdd8f68d0dccd9babb388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mEx3DKjup34u48zQIWZkVg.png"/></div></div></figure><p id="9269" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们必须堆叠所有提取的特征并保存提取的特征，从而创建检查点。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/e95b19980f36c71130681d2428dcb218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lXyA30U6e2DOvQOZ2geKKw.png"/></div></div></figure><p id="e88f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">然后设计一个神经网络，根据提取的训练特征训练该模型，然后根据提取的测试特征评估该模型。准确率为91.85%。</p><h1 id="e48f" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">未来工作:</h1><p id="6876" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在未来，我可以使用分布式计算来提高模型的可使用性，我可以一次性运行整个任务，这在colab或入门级台式机/笔记本电脑中是不可能的，而不是保存模型，这将提高模型的效率。它可以使用数据并行性跨多个GPU进行训练。我还可以尝试用更多的历元来训练模型，这可以改进模型。</p><h1 id="335f" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考:</h1><p id="d3a1" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><a class="ae iu" href="https://www.cs.cmu.edu/~aharley/rvl-cdip/" rel="noopener ugc nofollow" target="_blank">-https://www . cs . CMU . edu/~ a Harley/rvl-cdip/</a></p><p id="ce04" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><a class="ae iu" href="https://paperswithcode.com/sota/document-image-classification-on-rvl-cdip" rel="noopener ugc nofollow" target="_blank">-https://papers with code . com/sota/document-image-class ification-on-rvl-cdip</a></p><p id="4546" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><a class="ae iu" href="https://github.com/jpcrum/Final-Project-Group5" rel="noopener ugc nofollow" target="_blank">-https://github . com/jpc rum/Final-Project-group 5</a></p><ul class=""><li id="dc50" class="le lf hi jv b jw kr ka ks ke lg ki lh km li kq lj lk ll lm bi translated">【https://www.cs.cmu.edu/~aharley/icdar15/ T4】</li></ul><h1 id="0d82" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">Github:</h1><div class="ln lo ez fb lp lq"><a href="https://github.com/Ilovemysr/rvl-cdip" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hj fi z dy lv ea eb lw ed ef hh bi translated">Ilovemysr/rvl-cdip</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">在GitHub上创建一个帐户，为Ilovemysr/rvl-cdip开发做出贡献。</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">github.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me io lq"/></div></div></a></div><h1 id="7f39" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">Linkedin:</h1><p id="7b42" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><a class="ae iu" href="https://www.linkedin.com/in/ishika-chatterjee-9a9966120/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/ishika-chatterjee-9a9966120/</a></p></div></div>    
</body>
</html>