<html>
<head>
<title>Quora Question Pairs Similarity Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Quora问题对相似性问题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/quora-question-pairs-similarity-problem-8e3ae90441f0?source=collection_archive---------1-----------------------#2020-06-04">https://medium.com/analytics-vidhya/quora-question-pairs-similarity-problem-8e3ae90441f0?source=collection_archive---------1-----------------------#2020-06-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b8e3b28245cd1d1b7750faa7885c1438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YulfXLgM6rhCtaJHDgPPtQ.png"/></div></div></figure><blockquote class="iq ir is"><p id="5997" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Quora是一个获取和分享任何知识的地方。这是一个提问的平台，可以联系那些提供独特见解和高质量答案的人。这使人们能够相互学习，更好地了解世界。每月有超过1亿人访问Quora，所以很多人问类似的问题也就不足为奇了。具有相同意图的多个问题会导致搜索者花费更多的时间来寻找问题的最佳答案，并使作者感到他们需要回答同一问题的多个版本。Quora重视规范问题，因为它们为积极的搜索者和作者提供了更好的体验，并从长远来看为这两个群体提供了更多的价值。</p></blockquote><p id="dd23" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><em class="iv">演职员表:卡格尔</em></p><p id="a33f" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">请点击此链接获取编码实现。<a class="ae jv" href="https://github.com/vedanshsharma/Quora-Questions-Pairs-Similarity-Problem" rel="noopener ugc nofollow" target="_blank">https://github . com/vedanshharma/Quora-Questions-Pairs-Similarity-Problem</a></p><h1 id="9cca" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">介绍</h1><p id="dae7" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">这个案例研究叫做<em class="iv"> Quora问题对相似性问题。在本案例研究中，我们将处理将quora中的重复问题配对的任务。更正式地说，以下是我们的问题陈述</em></p><ul class=""><li id="3054" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">确定Quora上问的哪些问题是已经问过的问题的重复。</li><li id="73b1" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">这对于立即提供已经回答的问题的答案可能是有用的。</li><li id="f25f" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">我们的任务是预测一对问题是否重复。</li></ul><p id="96f1" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">注意- </strong>我们在讨论问题的语义相似度。</p><p id="5d79" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">来源</strong>:<a class="ae jv" href="https://www.kaggle.com/c/quora-question-pairs" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/quora-question-pairs</a></p><p id="870d" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">让我们看看几个目标和约束。</p><ol class=""><li id="5562" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr ln lf lg lh bi translated">错误分类的代价可能非常高。</li><li id="9e8d" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr ln lf lg lh bi translated">我们需要一对问题重复的概率，以便我们可以选择任何选择阈值。</li><li id="cab0" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr ln lf lg lh bi translated">没有严格的延迟问题。我们可以用一毫秒以上的时间(比如说)来返回给定的一对问题相似的概率。</li><li id="4020" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr ln lf lg lh bi translated">可解释性是部分重要的。</li></ol><h1 id="0023" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">机器学习问题</h1><h2 id="045c" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">数据</h2><p id="fca0" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">数据保存在一个名为“Train.csv”的csv文件中，可以从kaggle本身(<a class="ae jv" href="https://www.kaggle.com/c/quora-question-pairs" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/quora-question-pairs</a>)下载。</p><ul class=""><li id="2d81" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">Train.csv包含5列:qid1、qid2、question1、question2、is_duplicate</li><li id="3dbd" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">Train.csv的大小— 60MB</li><li id="0133" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">Train.csv中的行数= 404，290</li></ul><p id="6530" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">“qid1”和“qid2”是各自问题的id，“问题1”和“问题2”是问题本身，“is_duplicate”是目标标签，对于不相似的问题为0，对于相似的问题为1。</p><p id="8cc7" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">这也可以认为qid1，qid2，question1，question 2’是x标签，而‘is _ duplicate’是y标签。</p><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/093d6f1cf1651eb82734da12d80e7075.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FJGqH38wy8DQfXiPs3XXww.jpeg"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">数据集的前五行。</figcaption></figure><h2 id="1637" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">性能指标</h2><p id="ceb8" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">这是一个二元分类问题，对于给定的一对问题，我们需要预测它们是否重复。唯一的修改是我们将使用概率分数来设置阈值。</p><p id="def3" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">指标-</p><ul class=""><li id="ba45" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">对数损失:<a class="ae jv" href="https://www.kaggle.com/wiki/LogarithmicLoss" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/wiki/LogarithmicLoss</a></li><li id="59b1" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">二元混淆矩阵</li></ul><p id="08cc" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">因为我们将处理概率得分，所以最好选择对数损失作为我们的度量。对数损失总是对概率分数的小偏差不利。二元混淆矩阵将为我们提供一些指标，如TPR、FPR、TNR、FNR、精度和召回率。</p><p id="ae99" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">来源:<a class="ae jv" href="https://www.kaggle.com/c/quora-question-pairs/overview/evaluation" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/quora-question-pairs/overview/evaluation</a></p><h2 id="6b9b" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">训练和测试分割</h2><p id="6706" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">一个更好的分割数据的方法是<strong class="iw hj">基于时间的分割，因为问题的类型会随着时间而变化</strong>。但是我们还没有得到给定的时间戳。因此，我们将通过以70:30或80:20的比例随机分割来构建训练和测试，无论我们选择什么，因为我们有足够的点来工作。</p><h1 id="ddb4" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">探索性数据分析</h1><p id="05e7" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">在本节中，我们将分析数据，以了解数据中发生的情况。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/2e573f808da6ab9f276a1f626ab6cfe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*xrjCy3Dz7_Y533hB7HfL7Q.png"/></div></figure><p id="606c" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">除了“问题1”和“问题2”分别有1个和2个空对象外，每个功能都有404290个非空值。我们将以不同的方式处理这些行。这是数据的高级视图。</p><h2 id="845d" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">数据点在输出类中的分布</h2><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/0f332592b7ecdfffcb97c5bc52fcb11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*MlcJOYTt2aysm0Czns7YkQ.png"/></div></figure><p id="81af" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我们有63.08%的非重复对和36.92%的重复对。我们有</p><ul class=""><li id="4891" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">独特问题的数量是537933</li><li id="f8a4" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">出现多于一个的独特问题的数量是111780，相当于所有独特问题的20.78 %。</li><li id="1e0a" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">单个问题重复的最大次数:157</li><li id="26e2" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">我们没有重复的问题。</li></ul><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/4b0821a7415ee4297e4bbafb270a3970.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*oUwnrUGsCwFUD0GSfn4UxQ.png"/></div></figure><p id="0342" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">现在我们来看看这个问题的分布。</p><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/7dead83f4dca591023e0e283e5ab5781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*drY1s1eg1sipuZS487EiVA.png"/></div></div></figure><p id="a1f6" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">这看起来像一个指数分布。</p><p id="31db" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">从上图可以清楚的看到<strong class="iw hj">大部分问题出现不到40次</strong>。我们很少有偶然出现超过60次的异常值，一个问题出现了157次的极端情况。</p><p id="2897" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">就空值而言，我们只是用一个空格来替换它们。</p><h2 id="7578" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">基本特征提取(清洁前)</h2><p id="fad5" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">在清理之前，我们将提取一些基本特征。这些功能可能会也可能不会解决我们的问题。</p><ul class=""><li id="dd51" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated"><strong class="iw hj">freq _ qid 1</strong>= qid 1的频率</li><li id="b988" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">freq _ qid 2</strong>= qid 2的频率</li><li id="8f0d" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">Q1 len</strong>= Q1的长度</li><li id="f4ff" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">q2len</strong>= Q2的长度</li><li id="d7aa" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj"> q2_n_words </strong> =问题2的字数</li><li id="d738" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj"> word_Common </strong> =(问题1和问题2中常见唯一词的数量)</li><li id="81d5" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj"> word_Total </strong> =(问题1总字数+问题2总字数)</li><li id="6901" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">word _ share</strong>=(word _ common)/(word _ Total)</li><li id="8027" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">freq _ Q1+freq _ Q2</strong>= qid 1和qid2的频率总和</li><li id="c944" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">freq _ Q1-freq _ Q2</strong>= qid 1和qid2的绝对频率差</li></ul><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/5f8324c1a14c28879127e94203ed0a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NYMkE_jX46GGLL2yvOfChw.png"/></div></div></figure><p id="77df" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">让我们分析一些特征</p><ul class=""><li id="22ff" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">单词共享</li></ul><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/d20ff1c4b1c89c409ce62ed1fba9e15d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*HUWArM8n7QRjeEj3iordFA.png"/></div></figure><ol class=""><li id="dc27" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr ln lf lg lh bi translated">标准化word_share的分布在最右侧有一些重叠，即有相当多的问题具有高的单词相似性</li><li id="5b6f" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr ln lf lg lh bi translated">当qid1和qid2重复(相近)时，它们的平均词份额和共有词数都较大</li></ol><p id="a2ea" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">这意味着该特征在分类中具有某些价值。</p><ul class=""><li id="8e9d" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">Word_Common</li></ul><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/bc3a8af9bde9ef180c3915e6a16f1cc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*Nkz3-7MNLGeYNQjNZOlfGQ.png"/></div></figure><p id="83a7" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">word_Common特征在相似和非相似问题中的分布高度重叠。因此，该特征不能用于分类。或者换句话说，我们可以说它的预测能力很小。</p><h2 id="08b3" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">文本预处理</h2><p id="ea36" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">在我们进入复杂的特征工程之前，我们需要清理数据。预处理的一些步骤包括-</p><ul class=""><li id="22e9" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">删除html标签</li><li id="f9d7" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">删除标点符号。</li><li id="5b37" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">执行词干分析，将屈折词(或有时衍生词)简化为词干、词根或词根形式的过程。</li></ul><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/a136ea8f6c3daa55fc2d4d5cb00f0842.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/0*bfRUk8loWm0yUVGQ.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">一些单词及其词干</figcaption></figure><ul class=""><li id="d669" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">删除停用词。一些停用词的例子有:“a”、“and”、“but”、“how”、“or”和“what”</li><li id="7d3d" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">扩展缩写，例如将“ll”替换为“will”，“n`t”替换为“not”，“$”替换为“dollar”等。</li></ul><h2 id="ea41" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">高级特征提取(自然语言处理和模糊特征)</h2><p id="17f0" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">我们将提取一些高级功能。首先，我们应该熟悉一些术语。</p><ul class=""><li id="ad7a" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated"><strong class="iw hj">令牌</strong>:将句子分割成一个空格，得到一个令牌</li><li id="913e" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">停止字</strong>:根据NLTK停止字。NLTK是我们正在使用的nlp库。</li><li id="a5c3" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">字</strong>:不是停止字的令牌</li></ul><p id="a555" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">现在让我们来看看它的特点。</p><ul class=""><li id="fcae" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated"><strong class="iw hj">CWC _ min</strong>:Q1和Q2常用字数与最小字数之比<br/> cwc_min =常用字数/ (min(len(q1_words)，len(q2_words))</li><li id="5d36" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">CWC _ max</strong>:common _ word _ count与Q1和Q2字数最大长度的比值<br/>CWC _ max = common _ word _ count/(max(len(Q1 _ words)，len(q2_words))</li><li id="6787" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">CSC _ min</strong>:Q1和Q2的公共停止计数与最小停止计数长度的比值<br/> csc_min =公共停止计数/ (min(len(q1_stops)，len(q2_stops))</li><li id="82da" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">CSC _ max</strong>:common _ stop _ count与Q1和Q2停车计数最大长度的比值<br/>CSC _ max = common _ stop _ count/(max(len(Q1 _ stops)，len(q2_stops))</li><li id="d398" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">CTC _ min</strong>:common _ token _ count与Q1和Q2令牌计数的最小长度之比<br/>CTC _ min = common _ token _ count/(min(len(Q1 _ tokens)，len(q2_tokens))</li><li id="d542" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">CTC _ max</strong>:common _ token _ count与Q1和Q2令牌计数最大长度之比<br/>CTC _ max = common _ token _ count/(max(len(Q1 _ tokens)，len(q2_tokens))</li><li id="d4be" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj"> last_word_eq </strong>:检查两个问题的首字是否相等<br/>last _ word _ eq = int(Q1 _ tokens[-1]= = Q2 _ tokens[-1])</li><li id="1500" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj"> first_word_eq </strong>:检查两个问题的首字是否相等<br/>First _ word _ eq = int(Q1 _ tokens[0]= = Q2 _ tokens[0])</li><li id="ce1b" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj"> abs_len_diff </strong> : Abs。长度差<br/>ABS _ len _ diff = ABS(len(Q1 _ tokens)—len(Q2 _ tokens))</li><li id="91eb" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj"> mean_len </strong>:两个问题的平均令牌长度<br/>mean _ len =(len(Q1 _ tokens)+len(Q2 _ tokens))/2</li><li id="95c5" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">fuzz _ ratio</strong>:<a class="ae jv" href="https://github.com/seatgeek/fuzzywuzzy#usage" rel="noopener ugc nofollow" target="_blank">https://github.com/seatgeek/fuzzywuzzy#usage</a><a class="ae jv" href="http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/" rel="noopener ugc nofollow" target="_blank">http://chair nerd . seat geek . com/fuzzywuzzy-fuzzy-string-matching-in-python/</a></li><li id="0399" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">fuzzy _ partial _ ratio</strong>:<a class="ae jv" href="https://github.com/seatgeek/fuzzywuzzy#usage" rel="noopener ugc nofollow" target="_blank">https://github.com/seatgeek/fuzzywuzzy#usage</a><a class="ae jv" href="http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/" rel="noopener ugc nofollow" target="_blank">http://chair nerd . seat geek . com/fuzzywuzzy-fuzzy-string-matching-in-python/</a></li><li id="bda2" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">token _ sort _ ratio</strong>:<a class="ae jv" href="https://github.com/seatgeek/fuzzywuzzy#usage" rel="noopener ugc nofollow" target="_blank">https://github.com/seatgeek/fuzzywuzzy#usage</a><a class="ae jv" href="http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/" rel="noopener ugc nofollow" target="_blank">http://chair nerd . seat geek . com/fuzzywuzzy-fuzzy-string-matching-in-python/</a></li><li id="d2da" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">token _ set _ ratio</strong>:<a class="ae jv" href="https://github.com/seatgeek/fuzzywuzzy#usage" rel="noopener ugc nofollow" target="_blank">https://github.com/seatgeek/fuzzywuzzy#usage</a><a class="ae jv" href="http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/" rel="noopener ugc nofollow" target="_blank">http://chair nerd . seat geek . com/fuzzywuzzy-fuzzy-string-matching-in-python/</a></li><li id="8fa1" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated"><strong class="iw hj">longest _ substr _ Ratio</strong>:Q1和Q2令牌计数的最长公共子串长度与最小长度之比<br/> longest_substr_ratio = len(最长公共子串)/ (min(len(q1_tokens)，len(q2_tokens))</li></ul><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/b72d0a488b48da0480091de3f1c02fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RrQ_Q37IACGd_Q6bB6nCLg.png"/></div></div></figure><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mu"><img src="../Images/0b19967af070a2e648228e382b4177a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o1-ji5sgtUjemfKxWX9ObQ.png"/></div></div></figure><h2 id="9586" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">分析提取的特征</h2><p id="fcb9" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">我们将首先创建单词云。词云是由在特定文本或主题中使用的<strong class="iw hj">个词</strong>组成的图像，其中每个<strong class="iw hj">词</strong>的大小表示其出现的频率或重要性。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/9376cd753d38227eb8f9573b646104b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*Kszs-rYvqdVWVNHYT6Q2fQ.png"/></div></figure><p id="d459" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">在这里，我们可以清楚地看到，诸如“唐纳德”、“特朗普”、“最佳”等词的大小更大，这意味着它们在重复的问题对中出现的频率很高</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/ad147d5650c1c2468f1885b5c1cf74a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*sHyuY8eO2rRbpRxEWthmLg.png"/></div></figure><p id="485e" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">在非重复问句对中，我们会看到像“不”、“印度”、“将要”等词。要注意的一点是，单词“best”即使在非重复对中也有相当大的频率，但是这里它的频率相当低，因为它的图像具有较小的尺寸。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/203dd7ee9c8af8323a1a208a521bed1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*qa5cNlvT2NFaTvcf0jrXdQ.png"/></div></figure><p id="10f9" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">这些是一些高级功能的配对图。我们可以观察到的一点是，几乎所有的图都有部分重叠。因此，我们可以得出结论，这些特征可以提供部分可分性。它们都提供了一些<strong class="iw hj">预测能力</strong>。</p><div class="md me mf mg fd ab cb"><figure class="my ij mz na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/af4ef3daface78982d8482cbd976844a.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*_tcKPBclVxzRSO2a9RefHA.png"/></div></figure><figure class="my ij ne na nb nc nd paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/a2bb1ae2aaefff573288958403e5ddb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*M-p-Td6nqwMm7XR7mqg-iA.png"/></div></figure></div><p id="92f9" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">类似地，token_sort_ratio和fuzz_ratio也提供了一些可分离性，因为它们的pdf有部分重叠。</p><h2 id="d3d5" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">形象化</h2><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/ab9e903e3236a48fb1aa87d1479df36e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*TlblF2lgePvfpKfBW745lQ.png"/></div></figure><p id="a714" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">这种可视化是通过使用困惑度= 30和max_iter = 1000的t-SNE对5000个数据点的样本执行维度缩减(由于计算资源的限制)而创建的。维数从15减少到2。如你所见，有几个突出显示的区域，在那里我们能够完全分离点。这意味着我们在正确的轨道上。</p><h2 id="2f9f" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">用tfidf加权词向量特征化文本数据</h2><p id="5a3d" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">存在于问题中的词语是有价值的。例如，我们注意到一些单词在重复问题对中出现的频率比非重复问题对更高(如“唐纳德·特朗普”)，反之亦然。现在，我们将使用<a class="ae jv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer" rel="noopener ugc nofollow" target="_blank"> tfidf </a>加权单词向量。TF–IDF或TFIDF是术语频率–逆文档频率的缩写，是一种对文本数据进行矢量化的数字方法，旨在反映一个词对集合或语料库中的文档有多重要。</p><ul class=""><li id="ad98" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">找到TF-IDF分数后，我们通过这些分数将每个问题转换为word2vec向量的加权平均值。</li><li id="e8c1" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">在这里，我们使用一个预先训练好的手套模型，它是免费的。<a class="ae jv" href="https://spacy.io/usage/vectors-similarity" rel="noopener ugc nofollow" target="_blank">https://spacy.io/usage/vectors-similarity</a></li><li id="51c4" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">它是在维基百科上训练的，因此，它在单词语义方面更强。</li></ul><p id="a3f4" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">对于每个问题，我们都会有一个96维的数字向量。</p><p id="e505" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">在将其与之前的特征(即nlp和简单特征)结合之后，数据的总维度将是221。</p><p id="5c52" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">首先是我们的高级nlp特征，然后是简单特征，最后是问题1和问题2的向量。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/bc69c8a2469ff33b15e87027cc25b557.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*f-wdBvF5HBpE8x6J-Pokog.png"/></div></figure><h1 id="29f8" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">训练模型</h1><h2 id="542d" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">构建随机模型(寻找最坏情况下的日志丢失)</h2><p id="2d3f" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">我们的关键性能指标“对数损失”是一个范围为<em class="iv"> (0，</em> ∞ <em class="iv">)的函数。</em>因此，我们需要一个随机模型来获得指标的上限。当给定x_i时，随机模型将随机产生1或0，其中两个标签是等概率的。</p><figure class="md me mf mg fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/7664f052fbaac37cd936082a168505e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*qDrsWfHilDC987-oND75oQ.png"/></div></figure><p id="a445" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">0.88是我们随机模型的对数损失值。对于我们的问题,“体面的”模型将具有不接近0.88的对数损失值。请注意，类0的数据点比类1的数据点多。</p><h2 id="89a5" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">超参数调整的逻辑回归</h2><p id="c0d9" class="pw-post-body-paragraph it iu hi iw b ix ku iz ja jb kv jd je js kw jh ji jt kx jl jm ju ky jp jq jr hb bi translated">由于我们的数据既不是高维(例如1000)也不是低维(例如30)，它在某种程度上位于221维的中间。因此，我们将首先尝试超参数调整的逻辑回归模型。我们将使用网格搜索。</p><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ni"><img src="../Images/a1d93a76c32e1f1315883ceda4bf11fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0zZtEkVEh_jKdBnRpZ8_IQ.png"/></div></div></figure><p id="5b81" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">我们得到的最佳alpha(超参数)为0.1，测试数据的对数损失约为0.4986，比随机模型稍好<strong class="iw hj">。我们在这个模型中观察到的几件事是-</strong></p><ul class=""><li id="2be4" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">该模型没有过度拟合的问题，因为它在训练中的对数损失和测试数据非常接近。它可能存在高偏差或不合适</li><li id="c935" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">模型能够很好地预测类别0，但在类别1的情况下表现不佳。</li><li id="0c1b" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">两个类的精度都在0.85左右，不是很高。</li><li id="dcbb" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">0类的召回率很高，但1类的召回率很低。</li></ul><h2 id="7134" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">具有超参数调谐的线性SVM</h2><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nj"><img src="../Images/3577818a9881ae6aeb76248fa33c10a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LYk5RZClSJaYT5yyGQi0mw.png"/></div></div></figure><ul class=""><li id="018c" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">与逻辑回归模型类似，线性SVM模型不会出现过拟合问题，因为它对训练数据和测试数据的对数损失非常接近。可能是偏高或不合适。</li><li id="c766" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">线性SVM也存在类似的精度和召回率问题。</li><li id="8330" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">我们在模型中没有观察到任何显著的改进，因为测试数据的对数损失仍然非常相似。</li></ul><h2 id="fc93" class="lo jx hi bd jy lp lq lr kc ls lt lu kg js lv lw kk jt lx ly ko ju lz ma ks mb bi translated">XGboost带超参数调整</h2><figure class="md me mf mg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nk"><img src="../Images/2fa7d37cb6753eb0b7ebfa487981f3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4voj0QlqKf9VFwICFwap1g.png"/></div></div></figure><ul class=""><li id="56cb" class="kz la hi iw b ix iy jb jc js lb jt lc ju ld jr le lf lg lh bi translated">训练损失和测试损失之间有很大的差异，这意味着我们的模型存在过度拟合的问题。我们的测试损失仍然比线性模型好。</li><li id="0e66" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">对于1类，我们在精确度和召回率上都有所提高。这意味着我们的模型即使对于类1也能够很好地执行。</li></ul><p id="8433" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">总之，XGboost的性能比线性模型好得多。这表明数据不是线性可分的，我们需要一个复杂的非线性模型，如XGboost。</p><h1 id="586e" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">参考</h1><ul class=""><li id="b3c8" class="kz la hi iw b ix ku jb kv js nl jt nm ju nn jr le lf lg lh bi translated">讨论:<a class="ae jv" href="https://www.kaggle.com/anokas/data-analysis-xgboost-starter-0-35460-lb/comments" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/anokas/data-analysis-xgboost-starter-0-35460-lb/comments</a></li><li id="69e5" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">Kaggle获胜方案及其他途径:<a class="ae jv" href="https://www.dropbox.com/sh/93968nfnrzh8bp5/AACZdtsApc1QSTQc7X0H3QZ5a?dl=0" rel="noopener ugc nofollow" target="_blank">https://www . Dropbox . com/sh/93968 nfnrzh 8 BP 5/aaczdtsapc 1 qst QC 7x 0h 3qz 5a？dl=0 </a></li><li id="042c" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">博客1:<a class="ae jv" href="https://engineering.quora.com/Semantic-Question-Matching-with-Deep-Learning" rel="noopener ugc nofollow" target="_blank">https://engineering . quora . com/Semantic-Question-Matching-with-Deep-Learning</a></li><li id="b9a8" class="kz la hi iw b ix li jb lj js lk jt ll ju lm jr le lf lg lh bi translated">博客2:<a class="ae jv" href="https://towardsdatascience.com/identifying-duplicate-questions-on-quora-top-12-on-kaggle-4c1cf93f1c30" rel="noopener" target="_blank">https://towards data science . com/identifying-duplicate-questions-on-quora-top-12-on-ka ggle-4c 1 cf 93 f1 C30</a></li></ul></div></div>    
</body>
</html>