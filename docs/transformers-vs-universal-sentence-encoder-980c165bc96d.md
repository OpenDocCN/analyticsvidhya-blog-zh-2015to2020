# Transformer-xl VS 通用句子编码器

> 原文：<https://medium.com/analytics-vidhya/transformers-vs-universal-sentence-encoder-980c165bc96d?source=collection_archive---------9----------------------->

## 声明:这篇文章纯粹是实验性的，你可能找不到实验的可靠理论。

在 2020 年，我们有很多选择，从 Word2Vec 这样的简单跳格模型到 transformers 这样的复杂编码器-解码器架构。正如你所知，LSTM 和 RNN 的编码器-解码器架构，或者变形金刚中实现的奇特的注意力机制，都不是以前没有的。我们也可以像 RNN 那样实施注意力，但是…