<html>
<head>
<title>I, Storytelling Bot</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我，讲故事机器人</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/i-storytelling-bot-3652812f3aff?source=collection_archive---------12-----------------------#2019-12-28">https://medium.com/analytics-vidhya/i-storytelling-bot-3652812f3aff?source=collection_archive---------12-----------------------#2019-12-28</a></blockquote><div><div class="dt gx gy gz ha hb"/><div class="hc hd he hf hg"><div class=""/><div class=""><h2 id="1ca8" class="pw-subtitle-paragraph ig hi hj bd b ih ii ij ik il im in io ip iq ir is it iu iv iw ix dy translated">一个小机器人如何通过机器学习来旋转叙事</h2></div><figure class="iz ja jb jc fe jd es et paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="es et iy"><img src="../Images/30a2fd404492fb4df9070a0df9a65a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7agie6XiiWFKEtjH"/></div></div><figcaption class="jk jl eu es et jm jn bd b be z dy translated">达斯汀·李在<a class="ae jo" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="a844" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><strong class="jr hk"> TLDR </strong>:我创建了一个简单的机器人，根据随机选择或输入的种子文本生成新文本。最终的预测候选是使用Keras/Tensorflow库的深度学习模型:使用单词标记化和预训练Word2vec (Gensim)的LSTM模型。该模型在一个数据集上进行训练，该数据集由来自<a class="ae jo" href="https://paizo.com/pathfinder" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">探路者</strong> </a>的免费短篇故事构建而成。</p><p id="4ca8" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">完整的代码和笔记本可以在<a class="ae jo" href="https://github.com/ringoshin/snaug" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">这里</strong> </a>找到。</p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h1 id="bb80" class="ks kt hj bd ku kv kw kx ky kz la lb lc ip ld iq le is lf it lg iv lh iw li lj bi translated">目录</h1><ol class=""><li id="9c50" class="lk ll hj jr b js lm jv ln jy lo kc lp kg lq kk lr ls lt lu bi translated"><a class="ae jo" href="#6b49" rel="noopener ugc nofollow">简介</a></li><li id="b7c7" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk lr ls lt lu bi translated"><a class="ae jo" href="#d415" rel="noopener ugc nofollow">模型建立和选择</a> <br/>步骤0。<a class="ae jo" href="#cfdf" rel="noopener ugc nofollow">项目文件夹格式化</a> <br/>步骤1。<a class="ae jo" href="#f928" rel="noopener ugc nofollow">原始数据来源</a> <br/>第二步。<a class="ae jo" href="#6bdf" rel="noopener ugc nofollow">数据准备</a> <br/>第三步。<a class="ae jo" href="#65f3" rel="noopener ugc nofollow">数据预处理</a>步骤4<br/>。<a class="ae jo" href="#4d3f" rel="noopener ugc nofollow">模特训练</a> <br/>第五步。<a class="ae jo" href="#2d86" rel="noopener ugc nofollow">样本文本预测</a> <br/>步骤6。<a class="ae jo" href="#1a18" rel="noopener ugc nofollow">型号选择</a>步骤7<br/>。<a class="ae jo" href="#28b0" rel="noopener ugc nofollow">文本可视化</a> <br/>第8步。<a class="ae jo" href="#f6c7" rel="noopener ugc nofollow">最终模型文本生成</a></li><li id="1db1" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk lr ls lt lu bi translated"><a class="ae jo" href="#ad4e" rel="noopener ugc nofollow">结论</a></li></ol></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h1 id="6b49" class="ks kt hj bd ku kv kw kx ky kz la lb lc ip ld iq le is lf it lg iv lh iw li lj bi translated">介绍</h1><p id="4302" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">这是我在<a class="ae jo" href="https://metis.kaplan.com.sg/" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk"> Metis </strong> </a> <strong class="jr hk">数据科学训练营</strong>的最后一个项目。我想尝试一些与我以前的作品不同的东西，我以前的作品涉及预测和图像分类。因此，在我的导师的指导下，我通过机器学习将注意力转向了创造力——文本生成机器人。</p><p id="112d" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">文本生成是许多NLP(自然语言处理)应用中的一种。它既有挑战性又很有趣。我们出发了…</p><figure class="iz ja jb jc fe jd es et paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="es et md"><img src="../Images/3816ac0e2a9ea8bf348620c01bf53973.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cNNxPdc-x9rF2-vs"/></div></div><figcaption class="jk jl eu es et jm jn bd b be z dy translated">凯利·西克玛在<a class="ae jo" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="0e7c" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h1 id="d415" class="ks kt hj bd ku kv kw kx ky kz la lb lc ip ld iq le is lf it lg iv lh iw li lj bi translated">模型建立和选择</h1><p id="1519" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">你会发现下面的步骤，我采取了建立最终的工作模式。为了避免重复，我有时可能只举例说明三个模型中的一个的细节。</p><p id="4c93" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h2 id="cfdf" class="me kt hj bd ku mf mg mh ky mi mj mk lc jy ml mm le kc mn mo lg kg mp mq li mr bi translated">步骤0。项目文件夹格式</h2><p id="bf53" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">我希望我的代码(和文档)尽可能地有条理，因此我利用了这里的<a class="ae jo" href="https://blog.godatadriven.com/write-less-terrible-notebook-code" rel="noopener ugc nofollow" target="_blank"><strong class="jr hk"/></a>和这里的<a class="ae jo" href="https://docs.python-guide.org/writing/structure/" rel="noopener ugc nofollow" target="_blank"><strong class="jr hk"/></a>中的一些建议。</p><p id="b636" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">我已经把常用的模块打包成库存放在<a class="ae jo" href="https://github.com/ringoshin/snaug/tree/master/lib" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">这里</strong> </a> <strong class="jr hk"> : </strong></p><ul class=""><li id="4831" class="lk ll hj jr b js jt jv jw jy ms kc mt kg mu kk mv ls lt lu bi translated">nlplstm _ class.py封装GPU使用的类库，以及使用Keras/Tensorflow LSTM模型的NLP类库</li><li id="9ab0" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated">data-common.py —数据加载、数据保存、数据预处理和文本生成相关功能</li><li id="5672" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated">text _ viz _ common.py使用空间库可视化生成文本的函数</li></ul><p id="8c9d" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">没有进入太多的细节，定制的LSTM类让我保持理智。我可以随时在Google Colab或任何云平台上使用GPU轻松训练我的所有模型(为了速度)，同时在我的笔记本电脑上单独重新加载和预测经过训练的模型，而无需强大的GPU(为了方便或作为一个更便宜的选择)。</p><figure class="iz ja jb jc fe jd"><div class="bz dz l di"><div class="mw mx l"/></div></figure><p id="7a74" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h2 id="f928" class="me kt hj bd ku mf mg mh ky mi mj mk lc jy ml mm le kc mn mo lg kg mp mq li mr bi translated"><strong class="ak">第一步。原始数据来源</strong></h2><p id="64bc" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">过去，我和儿子一起玩过几次<a class="ae jo" href="https://paizo.com/pathfinder" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">探路者</strong> </a>游戏。他总是玩家之一，而我作为DM(地下城主)运行这些游戏。一个DM戴着很多帽子，其中一个就是提供叙事让游戏继续下去。</p><p id="e063" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">不用说，我认为这是一个ML学习的机会:让我们教一个机器人如何根据它所学习的探路者故事来生成随机的、但有用的和相关的短篇故事。</p><p id="ea2f" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">幸运的是，我能够从Pathfinder网站下载免费的短篇冒险，这成为了我用来训练我的模型的数据集——全部15000个可用单词。</p><p id="dc73" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">你可以在 这里的<a class="ae jo" href="https://github.com/ringoshin/snaug/tree/master/data" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">子文件夹下找到下载的pdf。</strong></a></p><p id="029f" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h2 id="6bdf" class="me kt hj bd ku mf mg mh ky mi mj mk lc jy ml mm le kc mn mo lg kg mp mq li mr bi translated"><strong class="ak">第二步。数据准备</strong></h2><p id="ec9a" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">我应用了以下小步骤(手动/脚本混合)来产生适合我们的机器学习流程的单个数据集。</p><ul class=""><li id="708f" class="lk ll hj jr b js jt jv jw jy ms kc mt kg mu kk mv ls lt lu bi translated">将pdf转换成文本文件(你可以用任何免费的在线转换器来完成)</li><li id="d178" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated">去掉不包含任何有价值的故事元素的文本块</li><li id="6a5e" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated">将文本文件连接成一个单一的原始文本文件<a class="ae jo" href="https://github.com/ringoshin/snaug/blob/master/data/textgen_pathfinder_raw.txt" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">此处</strong> </a></li><li id="626d" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated">使用<a class="ae jo" href="https://github.com/ringoshin/snaug/blob/master/0%20clean_raw_text_data.py" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">这个</strong> </a>只保留合适的句子，保存到<a class="ae jo" href="https://github.com/ringoshin/snaug/blob/master/data/textgen_pathfinder.txt" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">这里</strong> </a>。</li></ul><p id="d775" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">生成的<em class="my">“text gen _ path finder . txt”</em>将用作源文件，用于创建训练以下模型所需的数据集。</p><p id="4a5a" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h2 id="65f3" class="me kt hj bd ku mf mg mh ky mi mj mk lc jy ml mm le kc mn mo lg kg mp mq li mr bi translated">第三步。数据预处理</h2><p id="c4c7" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">经过几次初步的探索性实验，我决定尝试三种不同的LSTM模型，然后选择最佳方案:</p><ul class=""><li id="7f9c" class="lk ll hj jr b js jt jv jw jy ms kc mt kg mu kk mv ls lt lu bi translated"><a class="ae jo" href="https://github.com/ringoshin/snaug/blob/master/1a%20snaug_train_char_token_model_save_weights.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="jr hk"/></a><strong class="jr hk"/>(<a class="ae jo" href="https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/" rel="noopener ugc nofollow" target="_blank">参考</a>)</li><li id="b30b" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated"><a class="ae jo" href="https://github.com/ringoshin/snaug/blob/master/1b%20snaug_train_word_token_model_save_weights.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">单词分词和单词嵌入</strong> </a> ( <a class="ae jo" href="https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/" rel="noopener ugc nofollow" target="_blank">参考</a>)</li><li id="bbf2" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated"><a class="ae jo" href="https://github.com/ringoshin/snaug/blob/master/1c%20snaug_train_pretrained_model_save_weights.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">分词和预训练Word2vec </strong> </a> ( <a class="ae jo" href="https://stackoverflow.com/questions/42064690/using-pre-trained-word2vec-with-lstm-for-word-generation" rel="noopener ugc nofollow" target="_blank">参考</a>)</li></ul><p id="91b4" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">我将仔细看看第三个模型是如何建立的，因为它采用了或多或少与前两个模型相同的技术(标记化、嵌入、LSTM/RNN、分类)。首先，我们需要导入这些库:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="807d" class="me kt hj na b fj ne nf l ng nh">from keras.preprocessing.text import Tokenizer<br/>from keras.utils import to_categorical</span><span id="2c2c" class="me kt hj na b fj ni nf l ng nh">import string<br/>import textwrap<br/>import pickle</span><span id="7a39" class="me kt hj na b fj ni nf l ng nh">from lib.nlplstm_class import (TFModelLSTMCharToken, TFModelLSTMWordToken, TFModelLSTMWord2vec) <br/>from lib.data_common import (load_doc, save_doc, clean_doc, prepare_char_tokens)<br/>from lib.data_common import (build_token_lines, prepare_text_tokens, load_word2vec)<br/>from lib.data_common import pathfinder_textfile, fixed_length_token_textfile</span></pre><p id="72f4" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">接下来，我们加载文档并用这个函数对其进行标记:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="31de" class="me kt hj na b fj ne nf l ng nh">def clean_doc(doc):<br/> # replace '--' with a space ' '<br/> doc = doc.replace('--', ' ')</span><span id="eb86" class="me kt hj na b fj ni nf l ng nh"> # replace '-' with a space ' '<br/> doc = doc.replace('-', ' ')</span><span id="3448" class="me kt hj na b fj ni nf l ng nh"> # split into tokens by white space<br/> tokens = doc.split()</span><span id="88b6" class="me kt hj na b fj ni nf l ng nh"> # remove punctuation from each token<br/> table = str.maketrans('', '', string.punctuation)<br/> tokens = [w.translate(table) for w in tokens]</span><span id="e306" class="me kt hj na b fj ni nf l ng nh"> # remove remaining tokens that are not alphabetic<br/> tokens = [word for word in tokens if word.isalpha()]</span><span id="c645" class="me kt hj na b fj ni nf l ng nh"> # make lower case<br/> tokens = [word.lower() for word in tokens]<br/> return tokens</span></pre><p id="53e1" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">将这些令牌组织成固定长度的令牌行并保存它:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="cf36" class="me kt hj na b fj ne nf l ng nh">def build_token_lines(tokens, length=50):<br/> length += 1<br/> lines = list()<br/> for i in range(length, len(tokens)):<br/>  # select sequence of tokens<br/>  seq = tokens[i-length:i]<br/>  # convert into a line<br/>  line = ' '.join(seq)<br/>  # store<br/>  lines.append(line)<br/> return lines</span></pre><p id="83af" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">实际上，这些固定长度的线将是用于训练模型的数据集。</p><p id="dd43" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">让我们使用上面的段落作为14个单词的玩具示例数据集，并让我们将固定长度设置为5个单词。然后，这个玩具示例模型将被训练，通过学习前面的5个单词来预测下一个单词。生成的玩具示例训练数据集如下所示:</p><figure class="iz ja jb jc fe jd"><div class="bz dz l di"><div class="mw mx l"/></div></figure><p id="5115" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">因此，这也是我们实际数据集的样子。然后，我们将把固定长度的文本标记转换成适合LSTM训练的格式:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="9b90" class="me kt hj na b fj ne nf l ng nh">def prepare_text_tokens(lines):<br/> # integer encode sequences of words<br/> tokenizer = Tokenizer()<br/> tokenizer.fit_on_texts(lines)<br/> sequences = tokenizer.texts_to_sequences(lines)</span><span id="53d5" class="me kt hj na b fj ni nf l ng nh"> # vocabulary size<br/> vocab_size = len(tokenizer.word_index)<br/> #print(tokenizer.word_index)</span><span id="91df" class="me kt hj na b fj ni nf l ng nh"> # split into X and y<br/> npsequences = np.array(sequences)<br/> X, y = npsequences[:,:-1], npsequences[:,-1]<br/> y = to_categorical(y, num_classes=vocab_size+1)<br/> seq_length = X.shape[1]<br/> <br/> return X, y, seq_length, vocab_size, tokenizer</span></pre><p id="3f53" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">第三种模式很特别，因为它使用预先训练的重量来启动训练。因此，我们需要通过使用固定长度的令牌行训练Gensim Word2vec模型来构建这组权重:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="d87d" class="me kt hj na b fj ne nf l ng nh">def load_word2vec(lines):<br/>  # split tokens up per line for Gensim Word2vec consumption<br/>  sentences = [line.split() for line in lines]</span><span id="996c" class="me kt hj na b fj ni nf l ng nh">  print('\nTraining word2vec...')<br/>  # workers=1 will ensure a fully deterministrically-reproducible run, per Gensim docs<br/>  word_model = Word2Vec(sentences, size=300, min_count=1, window=5, iter=100, workers=1)<br/>  pretrained_weights = word_model.wv.syn0<br/>  vocab_size, emdedding_size = pretrained_weights.shape<br/>  print('Result embedding shape:', pretrained_weights.shape)</span><span id="5b33" class="me kt hj na b fj ni nf l ng nh">return vocab_size, emdedding_size, pretrained_weights</span></pre><p id="c50a" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h2 id="4d3f" class="me kt hj bd ku mf mg mh ky mi mj mk lc jy ml mm le kc mn mo lg kg mp mq li mr bi translated">第四步。模特培训</h2><p id="42ee" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">一旦我们从上一步中收集了所有必要的组件，我们就可以定义将用于第三个模型的LSTM模型:</p><figure class="iz ja jb jc fe jd"><div class="bz dz l di"><div class="mw mx l"/></div></figure><p id="927b" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">我们添加到模型中的第一层是嵌入层。“vocab_size”是我们数据集中唯一单词的数量。一般来说，我看到的大多数文章都建议在50-300的范围内任意设置嵌入维数(“embedding_size”)。这也是我们将之前的“预训练权重”加载到模型中的地方。</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="18b8" class="me kt hj na b fj ne nf l ng nh">self.model.add(Embedding(input_dim=vocab_size,<br/>                         output_dim=embedding_size,<br/>                         weights=[pretrained_weights]))</span></pre><p id="a61b" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">现在，我们添加两个LSTM层，并定期与辍学夫妇。</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="c4f3" class="me kt hj na b fj ne nf l ng nh">self.model.add(self.select_LSTM(embedding_size,return_sequences=True))<br/>self.model.add(Dropout(0.2))<br/>self.model.add(self.select_LSTM(embedding_size))<br/>self.model.add(Dropout(0.2))</span></pre><p id="278e" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">我们用两个密集层来完成模型，后者是输出层。</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="7e82" class="me kt hj na b fj ne nf l ng nh">self.model.add(Dense(embedding_size, activation='relu'))<br/>self.model.add(Dense((vocab_size+1), activation='softmax'))</span></pre><p id="37d8" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">记住这一点，让我们通过定义所需的参数来创建上述类的对象:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="12d1" class="me kt hj na b fj ne nf l ng nh"># create new object that is an LSTM model using word tokenization<br/># and pre-trained Word2vec model from Gensim to generate text<br/>textgen_model_3 = TFModelLSTMWord2vec(use_gpu=True)</span><span id="318f" class="me kt hj na b fj ni nf l ng nh">textgen_model_3.define(vocab_size=vocab_size, <br/>                       embedding_size=emdedding_size, <br/>                       pretrained_weights=pretrained_weights)</span></pre><p id="d292" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">从根本上说，这是一个分类练习，因此将使用“<em class="my">分类交叉熵</em>”:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="67e1" class="me kt hj na b fj ne nf l ng nh"># compile model<br/>textgen_model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span></pre><p id="1ba2" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">我们现在可以训练模型并保存权重，以便我们可以从不同的云平台或本地机器重新加载这些权重:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="4df6" class="me kt hj na b fj ne nf l ng nh"># fit model<br/>history = textgen_model_3.fit(X, y, batch_size=128, epochs=50)</span><span id="2b07" class="me kt hj na b fj ni nf l ng nh"># serialize model weights to HDF5 and save model training history<br/>textgen_model_3.save_weights_and_history(fname_prefix="./model/pathfinder_wordtoken_w2v_model_50_epoch")</span></pre><p id="6a45" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">这是所有3个模型的预处理和模型训练过程的完整脚本:</p><figure class="iz ja jb jc fe jd"><div class="bz dz l di"><div class="mw mx l"/></div></figure><p id="eed5" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">各型号的Colab笔记本也有<a class="ae jo" href="https://github.com/ringoshin/snaug" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">此处</strong> </a>。</p><p id="b56f" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">最后但同样重要的是，我们还需要在每次成功完成训练后保存一次模型:</p><figure class="iz ja jb jc fe jd"><div class="bz dz l di"><div class="mw mx l"/></div></figure><p id="32f3" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">分别在<a class="ae jo" href="https://github.com/ringoshin/snaug/blob/master/2%20snaug_load_weights_save_models.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">这里</strong> </a>有Colab笔记本。</p><p id="0b26" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h2 id="2d86" class="me kt hj bd ku mf mg mh ky mi mj mk lc jy ml mm le kc mn mo lg kg mp mq li mr bi translated">第五步。样本文本预测</h2><p id="eecb" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">现在，我们已经训练了模型并保存了每个模型的副本，让我们对所有三个模型运行一些示例文本预测，并看看每个模型的表现如何。</p><p id="a872" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">对于模型1，它通过一次预测一个字母来生成文本，仅在达到所需的'<em class="my"> n_chars </em>'时返回完整的生成文本:</p><figure class="iz ja jb jc fe jd"><div class="bz dz l di"><div class="mw mx l"/></div></figure><p id="f0ac" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">对于模型2和3，它们将通过每次预测下一个单词来生成文本。当到达'<em class="my">n _ words</em>required '时，例程将返回生成的文本:</p><figure class="iz ja jb jc fe jd"><div class="bz dz l di"><div class="mw mx l"/></div></figure><p id="c46a" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">这两个文本生成例程都使用了一个相当巧妙的帮助函数来为每个预测增加一定程度的随机性。这是通过添加一个称为“温度”或“多样性”的超参数来实现的。</p><figure class="iz ja jb jc fe jd"><div class="bz dz l di"><div class="mw mx l"/></div></figure><p id="98cf" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">低温(接近零度)将返回预期的概率。温度越高，回归概率就越随机。这可以通过在玩具概率阵列上测试辅助函数得到最好的说明:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="8063" class="me kt hj na b fj ne nf l ng nh">preds=[0.05, 0.1, 0.35, 0.5]<br/>print([preds[sample_predict(preds,0.05)] for _ in range(10)])<br/>print([preds[sample_predict(preds,1)] for _ in range(10)])<br/>print([preds[sample_predict(preds,5)] for _ in range(10)])</span></pre><p id="45de" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">如果没有“温度”，预测将始终返回“0.5”作为最大概率。现在增加温度会引入一个你可以控制的令人兴奋的不确定性水平。</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="e4b2" class="me kt hj na b fj ne nf l ng nh">[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5] <br/>[0.5, 0.35, 0.35, 0.5, 0.5, 0.5, 0.1, 0.35, 0.5, 0.35] <br/>[0.05, 0.05, 0.35, 0.5, 0.05, 0.1, 0.5, 0.5, 0.35, 0.1]</span></pre><p id="26ee" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">对于模型1，我是这样运行的:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="6ffa" class="me kt hj na b fj ne nf l ng nh">temperature_table = [0, 0.7]</span><span id="8504" class="me kt hj na b fj ni nf l ng nh">for temperature in temperature_table:<br/>  generated = generate_seq_of_chars(textgen_model_1,<br/>              num_unique_char, char2indices, indices2char,<br/>              char_seed_text, maxlen, 300, temperature)</span><span id="ccc9" class="me kt hj na b fj ni nf l ng nh">  print("&gt;&gt; generated text (temperature: {})".format(temperature))<br/>  print(textwrap.fill('%s' % (generated), 80))<br/>  print()</span></pre><p id="19b1" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">结果如下:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="dab1" class="me kt hj na b fj ne nf l ng nh">&gt;&gt; generated text (temperature: 0)  comes a dissong creck abread to eaper the ring to hear the sandpoint in the wind a small captain now a points and styyengess-demer, and a scccentions from a for the rap, the beliening of shobthess gropp and pcs who elemental in surprised to hel make a for gite and stealsh with a worken of golds wit  </span><span id="00da" class="me kt hj na b fj ni nf l ng nh">&gt;&gt; generated text (temperature: 0.7)  combat—yagg, and is she robb as magnimar’s hork samp, and as not a points and following the beat of gold, in simpating the mapical mumber she wastreaks he enter the pcs may of sandpoint’s strypeled betore and to searing the maps nom a can grack fagilies she remares staight acamem, and for sceeters</span></pre><p id="cebc" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">对于模型2和模型3，我是这样运行的:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="4ac6" class="me kt hj na b fj ne nf l ng nh">temperature_table = [0, 1.0]<br/>for temperature in temperature_table:<br/>  generated = generate_seq_of_words(textgen_model_2, tokenizer,<br/>                       seq_length, word_seed_text, 100, temperature)</span><span id="6066" class="me kt hj na b fj ni nf l ng nh">  print("&gt;&gt; generated text (temperature: {})".format(temperature))<br/>  print(textwrap.fill('%s' % (generated), 80))<br/>  print()</span></pre><p id="13a0" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">模型2的结果是:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="deb1" class="me kt hj na b fj ne nf l ng nh">&gt;&gt; generated text (temperature: 0)<br/>or knowledge religion check defeating the undead is easier if the pcs extinguish the candle of the development section with the skeletons defeated the pcs can deal with the candle of night with a successful dc knowledge arcana or knowledge religion check the pcs learn this minor magic item cannot be extinguished save by snuffing the flame with live flowing blood hazelindra adds that the pcs can<br/>keep the candle as long as they do not tell the academy of her connection to this situation the cemetery is half a mile west of the town and is accessible via a</span><span id="f46d" class="me kt hj na b fj ni nf l ng nh">&gt;&gt; generated text (temperature: 1.0)<br/>or knowledge religion check defeating the undead is easier if the pcs extinguish the candle of the development section with the skeletons defeated the pcs can deal with the candle of night with a successful dc knowledge arcana or knowledge religion check the pcs learn this minor magic item cannot be extinguished save by snuffing the flame with live flowing blood hazelindra adds that the pcs can<br/>keep the candle as long as they do not tell that about a long plum sized ruby calling it the fire of versade savasha versade has decided to display it publicly for the</span></pre><p id="6316" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">对于模型3:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="1a47" class="me kt hj na b fj ne nf l ng nh">&gt;&gt; generated text (temperature: 0)<br/>or knowledge religion check defeating the undead is easier if the pcs extinguish the candle of the development section with the skeletons defeated the pcs can deal with the candle of night with a successful dc knowledge arcana or knowledge religion check the pcs learn this minor magic item cannot be extinguished save by snuffing the flame with live flowing blood in order for the pcs to put out<br/>its flame and prevent more undead from rising from graves along their path back to sandpoint they must douse the candle in blood from an open wound dealing at least points</span><span id="fc34" class="me kt hj na b fj ni nf l ng nh">&gt;&gt; generated text (temperature: 1.0)<br/>or knowledge religion check defeating the undead is easier if the pcs extinguish the candle of the development section with the skeletons defeated the pcs can deal with the candle of night with a successful dc knowledge local or knowledge religion check defeating the undead is easier if the pcs extinguish the candle of the development section with the skeletons defeated the pcs can deal with the candle of night with a successful dc knowledge arcana or knowledge religion check the pcs learn this minor magic item cannot be extinguished save by snuffing the flame with live flowing blood in</span></pre><p id="486f" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">以下是为所有三种模型生成不同温度文本的完整代码:</p><figure class="iz ja jb jc fe jd"><div class="bz dz l di"><div class="mw mx l"/></div></figure><p id="4ef6" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">此处 有colab版本<a class="ae jo" href="https://github.com/ringoshin/snaug/blob/master/3%20snaug_load_models_generate_text.ipynb" rel="noopener ugc nofollow" target="_blank">T5。</a></p><p id="5e79" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h2 id="1a18" class="me kt hj bd ku mf mg mh ky mi mj mk lc jy ml mm le kc mn mo lg kg mp mq li mr bi translated">第六步。型号选择</h2><p id="680b" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">当我决定最终的候选模型时，我审查了几个因素，如训练时间、模型复杂性、模型准确性、生成的文本连贯性等。</p><figure class="iz ja jb jc fe jd es et paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="es et nj"><img src="../Images/ccfb6212c057b5c07eca9e745d38ab5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bFIU5TSRJ03xpVRVWmuiVQ.png"/></div></div></figure><p id="a274" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">最后，我选择了模型3，因为它比其他两个模型更快地获得了相似的精度/损失，从而使用了更少的资源。此外，当从样本文本生成进行评估时，生成的文本总体上是可以理解的。</p><p id="0427" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h2 id="28b0" class="me kt hj bd ku mf mg mh ky mi mj mk lc jy ml mm le kc mn mo lg kg mp mq li mr bi translated">第七步。文本可视化</h2><p id="5b8c" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">现在我们有了最终的候选模型，让我们通过使用<a class="ae jo" href="https://spacy.io/usage/linguistic-features#named-entities" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">空间NER </strong> </a>(命名实体识别)特性来添加可视化值。特别是，我必须生成新的命名实体来识别来自探路者幻想世界的特定域名。</p><p id="668b" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">第一步是收集新命名实体类型的列表和字典:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="94aa" class="me kt hj na b fj ne nf l ng nh">god_name_list = ['Erastil', 'Aroden', 'Desna', 'Sarenrae']<br/>race_name_list = ['Azlanti', 'Varisian', 'Thassilonian', 'Korvosan', 'Magnimarian']<br/>...<br/>sp_name_list = ['Burning', 'Hands']</span><span id="97d2" class="me kt hj na b fj ni nf l ng nh">entity_names = {'GOD': god_name_list, 'RACE': race_name_list, 'ORG': org_name_list, 'MOB': mob_name_list, 'PER': per_name_list, 'LOC': loc_name_list, 'SP': sp_name_list}</span><span id="bd8f" class="me kt hj na b fj ni nf l ng nh">god_labels = ['Erastil', 'Aroden', 'Desna', 'Sarenrae']<br/>race_labels = ['Azlanti', 'Varisian', 'Thassilonian', 'Korvosan', 'Magnimarian']<br/>...<br/>sp_labels = ['Burning Hands']</span><span id="e918" class="me kt hj na b fj ni nf l ng nh">entity_labels = {'GOD': god_labels, 'RACE': race_labels, 'ORG': org_labels, 'MOB': mob_labels, 'PER': per_labels, 'LOC': loc_labels, 'SP': sp_labels}</span></pre><p id="9305" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">接下来，我们在<a class="ae jo" href="https://spacy.io/usage/rule-based-matching#phrasematcher" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">短语匹配器</strong> </a>的帮助下，将新命名的实体更新为空间<a class="ae jo" href="https://spacy.io/usage/rule-based-matching" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">匹配器</strong> </a>:</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="f56a" class="me kt hj na b fj ne nf l ng nh">def get_matcher(nlp, entity_labels):<br/>  matcher = PhraseMatcher(nlp.vocab)</span><span id="4f6e" class="me kt hj na b fj ni nf l ng nh">  for entity, label_list in entity_labels.items():<br/>    entity_patterns = [nlp(text) for text in label_list]<br/>    matcher.add(entity, None, *entity_patterns)<br/>  <br/>  return matcher</span></pre><p id="ac85" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">完成后，我们现在可以使用更新的matcher和<a class="ae jo" href="https://spacy.io/api/top-level#displacy.render" rel="noopener ugc nofollow" target="_blank"><strong class="jr hk">displacy . render</strong></a>来突出显示Pathfinder特有的重要名称。</p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="c092" class="me kt hj na b fj ne nf l ng nh">doc = nlp(revised_text)<br/>matches = matcher(doc)<br/>spans = []</span><span id="ab6a" class="me kt hj na b fj ni nf l ng nh">for match_id, start, end in matches:<br/>  # get the unicode ID, i.e. 'COLOR'<br/>  rule_id = nlp.vocab.strings[match_id]  </span><span id="0b12" class="me kt hj na b fj ni nf l ng nh">  # get the matched slice of the doc<br/>  span = doc[start : end]                </span><span id="054a" class="me kt hj na b fj ni nf l ng nh">  # print(rule_id, span.text)<br/>  spans.append(Span(doc, start, end, label=rule_id))<br/>  doc.ents = spans</span><span id="043d" class="me kt hj na b fj ni nf l ng nh">print()<br/>print('-'*95)</span><span id="c2cc" class="me kt hj na b fj ni nf l ng nh">options = {"ents": ['GOD','MOB','PER','LOC','RACE','ORG','SP'],<br/>    "colors": {'GOD':'#f2865e','MOB':'#58f549','PER':'#aef5ef',<br/>    'LOC':'pink','RACE':'#edcb45','ORG':'#d88fff', 'SP':'pink'}}</span><span id="4d80" class="me kt hj na b fj ni nf l ng nh">print('Snaug_bot:')</span><span id="e809" class="me kt hj na b fj ni nf l ng nh">if using_notebook:<br/>  displacy.render(doc, style='ent', jupyter=True, options=options)<br/>else:<br/>  displacy.render(doc, style='ent', options=options)</span></pre><p id="ab73" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">实现这些的完整功能可以在<a class="ae jo" href="https://github.com/ringoshin/snaug/blob/master/lib/text_viz_common.py" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">这里</strong> </a>找到。</p><p id="393d" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h2 id="f6c7" class="me kt hj bd ku mf mg mh ky mi mj mk lc jy ml mm le kc mn mo lg kg mp mq li mr bi translated">第八步。最终模型文本生成</h2><p id="467f" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">万岁！我们现在已经准备好启动最终的工作模型。我添加了一个循环来检查用户输入。如果你只点击<enter>，它将从原始数据集中随机选取一个种子文本，并使用它来生成有希望的东西。要退出，只需键入“quit”并点击<enter>。</enter></enter></p><pre class="iz ja jb jc fe mz na nb nc aw nd bi"><span id="d10f" class="me kt hj na b fj ne nf l ng nh">text_input='random'<br/>while True:<br/>  text_input = input("Enter seeding text or hit &lt;ENTER&gt; to automate or 'quit' to exit: ")<br/>  <br/>  if text_input=='quit':<br/>    break<br/>  else:<br/>    if text_input=='':<br/>      text_input='random'<br/>      generate_and_visualize(lines, textgen_model, tokenizer,<br/>             seq_length, nlp, matcher, entity_names, entity_labels,<br/>             text_input=text_input)</span></pre><p id="b9d9" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">完整的colab笔记本可以在<a class="ae jo" href="https://github.com/ringoshin/snaug/blob/master/4%20snaug_final_model_generate_text_and_visualize.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hk">这里</strong> </a> <strong class="jr hk"> </strong>找到，这里有一个输出的例子:</p><figure class="iz ja jb jc fe jd es et paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="es et nk"><img src="../Images/63e7b9c4edca877bd5a4e90b20a19c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2YrH15IhuboP7g3uIjaMcQ.png"/></div></div></figure><p id="7581" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated"><a class="ae jo" href="#bb80" rel="noopener ugc nofollow"> ^ </a></p></div><div class="ab cl kl km gq kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hc hd he hf hg"><h1 id="ad4e" class="ks kt hj bd ku kv kw kx ky kz la lb lc ip ld iq le is lf it lg iv lh iw li lj bi translated">结论</h1><p id="b838" class="pw-post-body-paragraph jp jq hj jr b js lm ik ju jv ln in jx jy ma ka kb kc mb ke kf kg mc ki kj kk hc bi translated">这无疑是一个有趣的项目，因为我探索了许多不同的领域，有时甚至是不同的领域。看着这个简单的应用程序似乎完全靠自己来创造故事，真是太神奇了。我们添加的唯一成分是几个具有共同背景的短篇故事，一个用于文本生成的相当常规的NLP LSTM模型和一些时髦的空间功能，以使事情变得稍微明亮一些。</p><p id="5bce" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">总而言之，我们肯定可以通过以下扩展目标来改进该模型:</p><ul class=""><li id="2578" class="lk ll hj jr b js jt jv jw jy ms kc mt kg mu kk mv ls lt lu bi translated">自动化一些手动和半自动流程</li><li id="7d02" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated">尝试句子和文档嵌入</li><li id="261a" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated">研究如何用标点和其他符号生成合适的句子</li><li id="9321" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated">添加一个助手函数，使用户输入的文本更接近种子文本</li><li id="a633" class="lk ll hj jr b js lv jv lw jy lx kc ly kg lz kk mv ls lt lu bi translated">探索其他文本生成模型</li></ul><p id="156d" class="pw-post-body-paragraph jp jq hj jr b js jt ik ju jv jw in jx jy jz ka kb kc kd ke kf kg kh ki kj kk hc bi translated">我希望你喜欢阅读这篇文章，甚至可能找到对你自己的项目有用的东西！:)</p></div></div>    
</body>
</html>