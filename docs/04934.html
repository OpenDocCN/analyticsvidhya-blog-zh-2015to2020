<html>
<head>
<title>Review: Mask R-CNN (Instance Segmentation &amp; Human Pose Estimation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:掩模R-CNN(实例分割和人体姿态估计)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4?source=collection_archive---------2-----------------------#2020-04-06">https://medium.com/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4?source=collection_archive---------2-----------------------#2020-04-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="dc6e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">在实例细分方面优于<a class="ae ix" href="https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34" rel="noopener" target="_blank">跨国公司</a>和<a class="ae ix" href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2" rel="noopener" target="_blank">FCIS</a>；以及人体姿态估计中的<a class="ae ix" rel="noopener" href="/analytics-vidhya/review-cmupose-openpose-winner-in-coco-keypoint-detection-challenge-2016-human-pose-ccbdbc72b7dd"> CMU姿态</a>和<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/review-g-rmi-1st-runner-up-in-coco-keypoint-detection-challenge-2016-human-pose-estimation-6c8d250f62a0"> G-RMI </a></h2></div><p id="b3ec" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di">在</span>这个故事中，非常著名的<strong class="ja hj">面具R-CNN </strong>，由<strong class="ja hj">脸书艾研究(FAIR) </strong>进行回顾。Mask R-CNN很容易推广到<strong class="ja hj">实例分割</strong>、<strong class="ja hj">包围盒对象检测</strong>、<strong class="ja hj">人物关键点检测</strong>等很多任务。它是由FAIR的<strong class="ja hj"> Detectron </strong>开发的，这是一个支持众多研究项目的FAIR软件系统。</p><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es kd"><img src="../Images/a826354ddcae3b0d81e83c20182c8ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*X__XAcPrP7AIZoXhxPAR-Q.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">屏蔽R-CNN进行实例分割</strong>(图片来自<a class="ae ix" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">作者论文</a>)</figcaption></figure><p id="edc3" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">Mask R-CNN是文献中相当重要的基于深度学习的计算机视觉论文之一。它在每个任务上都超过了所有现有的单一模型参赛作品(当时)，包括COCO 2016挑战赛的获胜者。在COCO 2017挑战赛中，获胜者的网络也是基于Mask R-CNN。</p><p id="2761" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这是一篇<strong class="ja hj"> 2017 ICCV </strong>论文，引用超过<strong class="ja hj"> 5000次</strong>。并在2017 ICCV获得<strong class="ja hj"> <em class="kq">马尔奖</em> </strong>。(<a class="kr ks ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----61080a93bf4--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="980e" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">概述</h1><ol class=""><li id="9eac" class="lr ls hi ja b jb lt je lu jh lv jl lw jp lx jt ly lz ma mb bi translated"><strong class="ja hj">什么是实例分割？</strong></li><li id="14fe" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt ly lz ma mb bi translated"><strong class="ja hj">从</strong> <a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1"> <strong class="ja hj">快速R-CNN </strong> </a> <strong class="ja hj">，</strong> <a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba"> <strong class="ja hj">快速R-CNN </strong> </a> <strong class="ja hj">，</strong> <a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank"> <strong class="ja hj">快速R-CNN </strong> </a> <strong class="ja hj">，到屏蔽R-CNN </strong></li><li id="5826" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt ly lz ma mb bi translated"><strong class="ja hj">屏蔽R-CNN网络概述&amp;损失函数</strong></li><li id="6422" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt ly lz ma mb bi translated"><strong class="ja hj"> RoIAlign </strong></li><li id="7078" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt ly lz ma mb bi translated"><strong class="ja hj">头部结构细节</strong></li><li id="bea9" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt ly lz ma mb bi translated"><strong class="ja hj">实例分割结果</strong></li><li id="9f7d" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt ly lz ma mb bi translated"><strong class="ja hj">人体姿态估计(关键点检测)结果</strong></li></ol></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="b170" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated"><strong class="ak"> 1。什么是实例分段？</strong></h1><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mh"><img src="../Images/a82617792aa25d5b425f5fab9bf6c486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-QdabKB4R2ksPjULjLJbA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">(图片来自<a class="ae ix" href="https://www.slideshare.net/windmdk/mask-rcnn" rel="noopener ugc nofollow" target="_blank">作者的PPT </a>)</figcaption></figure><ul class=""><li id="038b" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated"><strong class="ja hj">分类</strong>:只对图像中的主要物体进行分类。</li><li id="d65f" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">分类+定位:我们也想知道主要物体的包围盒。</li><li id="7a2b" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><strong class="ja hj">物体检测</strong>:图像中有多个物体，我们想知道所有已知类的类和每个物体的包围盒。</li><li id="f4dc" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><strong class="ja hj">实例分割</strong>:对单个对象进行分类，并使用边界框对每个对象进行定位。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="e32d" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated"><strong class="ak"> 2。从</strong><a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1"><strong class="ak">R-CNN</strong></a><strong class="ak">，</strong> <a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba"> <strong class="ak">快R-CNN </strong> </a> <strong class="ak">，</strong> <a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank"> <strong class="ak">快R-CNN </strong> </a> <strong class="ak">，到屏蔽R-CNN </strong></h1><ul class=""><li id="7587" class="lr ls hi ja b jb lt je lu jh lv jl lw jp lx jt mp lz ma mb bi translated">要想很好的理解Mask R-CNN网络架构，最好从<a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1"> R-CNN </a>开始理解。</li><li id="60f0" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">(最好先对<a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1"> R-CNN </a>、<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba">快R-CNN </a>、<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank">更快R-CNN </a>有个基本了解。这里只是简单回顾一下。)</li></ul><h2 id="a9ec" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated"><strong class="ak"> 2.1。</strong> <a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1"> <strong class="ak"> R-CNN </strong> </a></h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ne"><img src="../Images/fa4cb038240acdd8b855c997c629b7d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*vRtAQxCvctFn_5w7CgHP1A.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1"> <strong class="bd kp"> R-CNN </strong> </a> <strong class="bd kp"> </strong>(图片来自<a class="ae ix" href="https://www.slideshare.net/windmdk/mask-rcnn" rel="noopener ugc nofollow" target="_blank">作者的PPT </a>)</figcaption></figure><ul class=""><li id="6d49" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">在<a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1"> R-CNN </a>中，在网络的底部，<strong class="ja hj">基于非深度学习的选择性搜索(SS)用于特征提取，以生成2k个区域提议。</strong></li><li id="4329" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><strong class="ja hj">每个区域建议被扭曲并通过卷积神经网络(CNN)，最后通过支持向量机(SVM)</strong>,输出分类和包围盒。</li><li id="4545" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">(如有兴趣，请阅读<a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1"> R-CNN </a>了解更多详情。)</li></ul><h2 id="68d6" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">2.2.<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba">快速R-CNN </a></h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nf"><img src="../Images/d419763ed86e4036c2c4c84289d3d3c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*E7-fjvnXovzTENHGKpOdAw.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba"> <strong class="bd kp">快速R-CNN </strong> </a> <strong class="bd kp"> </strong>(图片来自<a class="ae ix" href="https://www.slideshare.net/windmdk/mask-rcnn" rel="noopener ugc nofollow" target="_blank">作者的PPT </a>)</figcaption></figure><ul class=""><li id="2470" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">在<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba">快速R-CNN </a>中，<strong class="ja hj"> SS仍然用于生成2k区域提议。</strong></li><li id="64a8" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">但是，<strong class="ja hj">与</strong><a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1"><strong class="ja hj">R-CNN</strong></a><strong class="ja hj">不同，输入图像经过CNN进行特征提取，生成特征图。这些特征图被共享，用于根据之后的每个区域提议的RoI汇集。</strong></li><li id="edec" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">对于每个区域提案，将对提案执行投资回报池，以最终通过网络，即全连接(FC)层。并且不再使用SVM。</li><li id="21b9" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">最后，<strong class="ja hj">在全连接(FC)层的输出端输出分类和包围盒。</strong></li><li id="4bbc" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">但是区域提议部分仍然使用基于非深度学习的SS方法。</li><li id="a0a5" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">(如有兴趣，请阅读<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba">快速R-CNN </a>了解更多详情。)</li></ul><h2 id="780c" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">2.3.<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank">更快的R-CNN </a></h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ng"><img src="../Images/c2617369fa54dd818f77508df3ef5bc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*ofZtmGU1gmliAa27zZfD5g.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank"> <strong class="bd kp">更快R-CNN </strong> </a> <strong class="bd kp"> </strong>(图片来自<a class="ae ix" href="https://www.slideshare.net/windmdk/mask-rcnn" rel="noopener ugc nofollow" target="_blank">作者的PPT </a>)</figcaption></figure><ul class=""><li id="0c73" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">在<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank">快速R-CNN </a>、<strong class="ja hj">中，输入图像通过CNN。这些特征图将用于区域建议网络(RPN)以生成区域建议，并用于在后期生成RoI汇集的特征图。</strong></li><li id="1673" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">在这种情况下，不再使用<strong class="ja hj">SS</strong>。而是使用一个CNN。因此，整个网络是一个端到端的深度学习网络，这对于梯度传播以提高对象检测精度是必不可少的。</li><li id="e14e" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">与<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba"> Fast R-CNN </a>类似，对于每个区域提案，RoI pooling在提案上执行，以通过网络的末端，即完全连接的层。最后输出分类和包围盒。</li><li id="eb27" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">(如有兴趣，请阅读<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank">更快R-CNN </a>了解更多详情。)</li></ul><h2 id="593a" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">2.4.屏蔽R-CNN</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nh"><img src="../Images/285cffa001225bfa6eb320ce93c939c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*Tm5ia-bzsVq3lw8TJt08YA.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">屏蔽R-CNN </strong>(图片来自<a class="ae ix" href="https://www.slideshare.net/windmdk/mask-rcnn" rel="noopener ugc nofollow" target="_blank">作者的PPT </a>)</figcaption></figure><ul class=""><li id="6da2" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">在这个故事里，Mask R-CNN，架构非常接近<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank">更快的R-CNN </a>。<strong class="ja hj">主要区别在于，在网络的末端，有另一个头，即上图中的掩码分支，用于生成实例分割的掩码。</strong></li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="bfa2" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated"><strong class="ak"> 3。屏蔽R-CNN网络概述&amp;损失函数</strong></h1><h2 id="5484" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">3.1.两阶段架构</h2><ul class=""><li id="5e83" class="lr ls hi ja b jb lt je lu jh lv jl lw jp lx jt mp lz ma mb bi translated">采用两级架构，就像<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank">更快的R-CNN </a>。</li><li id="f9b9" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><strong class="ja hj">第一阶段</strong> : <strong class="ja hj">地区提案网络</strong>，产生地区提案或候选人。每个地区的提案将进入第二阶段。</li><li id="6eff" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><strong class="ja hj">第二阶段:对于每一个区域提议</strong>，第一阶段提出的特征图是根据区域汇集的RoI，并通过剩余的网络，<strong class="ja hj">输出类别、包围盒以及二进制掩码</strong>。</li><li id="14a3" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">更详细的网络体系结构在第5节中提到。</li></ul><h2 id="f62c" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">3.2.损失函数</h2><ul class=""><li id="d398" class="lr ls hi ja b jb lt je lu jh lv jl lw jp lx jt mp lz ma mb bi translated">因此，损失函数是多任务损失:</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es ni"><img src="../Images/a4eab6f33bcb0124b4a91df3806de30f.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*m25rMgZ0BJbH9tKhYf5_7Q.png"/></div></figure><ul class=""><li id="fd67" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated"><em class="kq"> Lcls </em>:分级损耗，同<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank">快R-CNN </a>。</li><li id="d8d7" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><em class="kq"> Lbox </em>:包围盒丢失，同<a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank">快R-CNN </a>。</li><li id="ceeb" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><strong class="ja hj"> <em class="kq"> Lmask </em> </strong>:二进制掩码丢失。该掩膜分支为每个RoI输出<em class="kq"> Km </em>，为<em class="kq">m</em>×m分辨率的<em class="kq"> K </em>二值掩膜，代表<em class="kq"> K </em>的类别数。</li></ul><h2 id="3c7c" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">3.3.Lmask</h2><ul class=""><li id="8448" class="lr ls hi ja b jb lt je lu jh lv jl lw jp lx jt mp lz ma mb bi translated"><strong class="ja hj">应用每像素sigmoid </strong>。</li><li id="6f1a" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><strong class="ja hj">平均二进制交叉熵损失</strong>用于<em class="kq"> Lmask </em>。</li><li id="44f5" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><em class="kq"> Lmask </em>仅占带有地面真实等级<em class="kq"> k </em>的掩膜的RoI的第<em class="kq"> k </em>个掩膜。其他类不会造成损失。</li><li id="1f05" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">因此，它不同于完全卷积网络(<a class="ae ix" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" rel="noopener" target="_blank"> FCN </a>)。在<a class="ae ix" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" rel="noopener" target="_blank"> FCN </a>中，使用了每像素softmax和多项式交叉熵损失。相比之下，这里的<strong class="ja hj">掩码和类预测被解耦为<em class="kq"> Lcls </em>和<em class="kq"> Lmask </em> </strong>。</li><li id="1eb4" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">为了提取掩模的空间结构，使用<a class="ae ix" href="https://towardsdatascience.com/review-fcn-semantic-segmentation-eb8c9b50d2d1" rel="noopener" target="_blank"><strong class="ja hj">FCN</strong></a><strong class="ja hj">预测每个RoI的<em class="kq"> m </em> × <em class="kq"> m </em>掩模。</strong>不使用FC层的好处是需要的参数更少。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="3857" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">4.罗伊Align</h1><h2 id="f00a" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">4.1.更快的R-CNN </h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es nj"><img src="../Images/a8142964b554a81b5e2505fb3eb290b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9J1t1lO5PTMKvj01rjkPcg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp"> RoIPool in </strong> <a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank"> <strong class="bd kp">更快R-CNN </strong> </a> <strong class="bd kp"> </strong>(图片来自<a class="ae ix" href="https://www.slideshare.net/windmdk/mask-rcnn" rel="noopener ugc nofollow" target="_blank">作者的PPT </a>)</figcaption></figure><ul class=""><li id="4e8b" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated"><a class="ae ix" href="https://towardsdatascience.com/review-faster-r-cnn-object-detection-f5685cb30202" rel="noopener" target="_blank">更快R-CNN </a>中RoIPool的一个例子如上图。</li><li id="4de1" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">首先，我们得到了图左侧的输入特征图。</li><li id="6160" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">根据区域建议，我们使用一个7×5的区域作为RoIPool的输入，输出2×2的特征图。</li><li id="b217" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">每个黑色矩形都被四舍五入为整数长度，以便以后合并。</li><li id="8280" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">对于输出特征图的每个值，它们对应于每个黑色矩形的最大值，这被称为最大池。</li></ul><h2 id="f478" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">4.2.掩模R-CNN中的RoIAlign</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es nk"><img src="../Images/f4b6ec464861f77c0eeb1d9bacafdbfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c0X-ybk4HsuxHoOYeFG_zw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp"> RoIAlign in Mask R-CNN </strong>(图片来自<a class="ae ix" href="https://www.slideshare.net/windmdk/mask-rcnn" rel="noopener ugc nofollow" target="_blank">作者的PPT </a></figcaption></figure><ul class=""><li id="e53b" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">掩膜R-CNN中RoIAlign的一个例子如上图。</li><li id="35b0" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">使用相同大小的黑色<strong class="ja hj">矩形</strong>，而不是将黑色矩形圆化为整数长度。</li><li id="553d" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><strong class="ja hj">基于特征地图值重叠的区域，使用双线性插值</strong>获得中间汇集特征地图，如图右下方所示。</li><li id="2fa6" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">然后<strong class="ja hj">在这个中间池化特征图</strong>上执行最大池化。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="faae" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated"><strong class="ak"> 5。头部结构细节</strong></h1><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es nl"><img src="../Images/ac1b11552abf46ca49cac9016f169978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IV6q-xtvxdLTg1h9Jk5lGg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">网络架构变体</strong>(图片来自<a class="ae ix" href="https://www.slideshare.net/windmdk/mask-rcnn" rel="noopener ugc nofollow" target="_blank">作者的PPT </a>)</figcaption></figure><ul class=""><li id="df24" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">3名骨干受审:<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8" rel="noopener" target="_blank">雷斯内特</a>、<a class="ae ix" href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac" rel="noopener" target="_blank">雷斯NeXt </a>和FPN。</li><li id="90bb" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><strong class="ja hj">左</strong>:在没有<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610" rel="noopener" target="_blank"> FPN </a>的情况下使用<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8" rel="noopener" target="_blank">ResNet</a>/<a class="ae ix" href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac" rel="noopener" target="_blank">ResNeXt</a>时，在分割成两个头之前先进行进一步的卷积。一个头部用于分类和边界框，一个头部用于遮罩。</li><li id="bab4" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated"><strong class="ja hj">右</strong>:当<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8" rel="noopener" target="_blank"> ResNet </a> / <a class="ae ix" href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac" rel="noopener" target="_blank"> ResNeXt </a>与<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610" rel="noopener" target="_blank"> FPN </a>配合使用时，网络直接分裂成两个头。一个头部用于分类和边界框，一个头部用于遮罩。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="4d11" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">6.<strong class="ak">实例分割结果</strong></h1><ul class=""><li id="0599" class="lr ls hi ja b jb lt je lu jh lv jl lw jp lx jt mp lz ma mb bi translated"><strong class="ja hj">数据集:COCO女士</strong>，80个类别，80k训练图像，35k val图像子集，5k消融实验图像。</li></ul><h2 id="ba1c" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">6.1.消融研究</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nm"><img src="../Images/3f7cc4abd3e66084438958d703c8af49.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*youRR9rsd9HjUiYhjmoBhA.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">主干架构</strong></figcaption></figure><ul class=""><li id="57d8" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">更好的主干带来预期的收益:更深的网络做得更好，<a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610" rel="noopener" target="_blank"> FPN </a>优于C4特性，<a class="ae ix" href="https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac" rel="noopener" target="_blank"> ResNeXt </a>在<a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8" rel="noopener" target="_blank"> ResNet </a>上有所改进。</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nn"><img src="../Images/0d50486f68b167e1985ae63b9d8fbf3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*LD4sEEIe7ke_MGfBivBlXw.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">多项式与独立掩码</strong></figcaption></figure><ul class=""><li id="a164" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">与多项式掩码(softmax)相比，通过perclass二进制掩码(sigmoid)去耦可获得更大的增益。</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es no"><img src="../Images/38ffbfa2bf182bb63a49b78343713b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*r4DORk9igWXfHTUx7adn5g.png"/></div></figure><ul class=""><li id="d798" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">RoIWarp最初用于<a class="ae ix" rel="noopener" href="/coinmonks/review-r-cnn-object-detection-b476aba290d1"> R-CNN </a>中，RoIPool最初用于<a class="ae ix" rel="noopener" href="/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba"> Fast R-CNN </a>中。</li><li id="faa7" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">RoIAlign层提高AP 3点，AP75提高5点。使用适当的对齐是造成RoI层之间大间隙的唯一因素。</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es np"><img src="../Images/ef82c8e551ebba5b5789aa0443f09be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*7ryOHnUKslVRz4AWQBDlpQ.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp"> RoIPool vs RoIAlign </strong></figcaption></figure><ul class=""><li id="2e91" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">使用步幅32的ResNet-50-C5。</li><li id="d5c3" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">失调比stride-16特性更严重，导致巨大的精度差距。</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nq"><img src="../Images/40a4a7bb5d9923710c8d1fffc2b02cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*PDWxCSqBfbvps-I6RFJvEw.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">遮罩分支变体使用</strong><a class="ae ix" href="http://ResNet" rel="noopener ugc nofollow" target="_blank"><strong class="bd kp">ResNet</strong></a><strong class="bd kp">-50-</strong><a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610" rel="noopener" target="_blank"><strong class="bd kp">FPN</strong></a></figcaption></figure><ul class=""><li id="4985" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">与MLP(多层感知器，使用FC层)相比，FCNs 改善了结果，因为它们利用了显式编码空间布局的优势。</li></ul><h2 id="75d7" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">6.2.定性结果</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es nr"><img src="../Images/0e138016d60af754d307decd93e3989b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fV446Gse6EuTgvi502rCHw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">屏蔽R-CNN对COCO的测试图片，使用</strong><a class="ae ix" href="http://ResNet" rel="noopener ugc nofollow" target="_blank"><strong class="bd kp">ResNet</strong></a><strong class="bd kp">-101-</strong><a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610" rel="noopener" target="_blank"><strong class="bd kp">【FPN】</strong></a><strong class="bd kp"/>(图片来自<a class="ae ix" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">作者论文</a>)</figcaption></figure><h2 id="90ba" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">6.3.SOTA方法比较</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es ns"><img src="../Images/341022528da2476c2c206a16ddc3a396.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZdteP1LIKzVF0YLOP3u3UQ.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">COCO测试开发上的实例分段掩码AP</strong></figcaption></figure><ul class=""><li id="e603" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated"><a class="ae ix" href="https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34" rel="noopener" target="_blank">跨国公司</a>和<a class="ae ix" href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2" rel="noopener" target="_blank"> FCIS </a>分别是2015年和2016年COCO细分挑战的获胜者。</li><li id="4625" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">Mask R-CNN胜过更复杂的<a class="ae ix" href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2" rel="noopener" target="_blank">FCIS</a>++++，后者包括多尺度训练/测试、水平翻转测试和OHEM。所有条目都是单一模型结果。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="ff5e" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated"><strong class="ak"> 7。</strong>人体姿态估计(<strong class="ak">关键点检测)结果</strong></h1><ul class=""><li id="de25" class="lr ls hi ja b jb lt je lu jh lv jl lw jp lx jt mp lz ma mb bi translated">Mask R-CNN框架可以很容易地扩展到人体姿态估计。</li><li id="2272" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">关键点的位置被建模为一个热点遮罩。屏蔽R-CNN预测<em class="kq"> K </em>个屏蔽，每个屏蔽对应一个<em class="kq"> K </em>关键点类型(如左肩、右肘)。</li></ul><h2 id="b0b0" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">7.1.消融研究</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nf"><img src="../Images/27a27b4ccd6d20632f20b7638f44a224.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*bgz-0QY301ZpzOEoN-IezQ.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp"> RoIPool vs RoIAlign </strong></figcaption></figure><ul class=""><li id="6932" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">同样，使用适当的对齐是造成RoI层之间大间隙的唯一因素。</li></ul><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nt"><img src="../Images/c7441ab4a54e9b83a69d73ddc0fce21e.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*RtMEtheTQut5JYNsFCdfpQ.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">关于人物类别的方框、遮罩和关键点的多任务学习，在minival上评估</strong></figcaption></figure><ul class=""><li id="6aa1" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated">添加关键点分支略微减少了方框/掩码AP，这表明尽管关键点检测受益于多任务训练，但它不会反过来帮助其他任务。</li><li id="8393" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">然而，联合学习所有三个任务能够使一个统一的系统有效地同时预测所有输出。</li></ul><h2 id="0c7c" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">7.2.定性结果</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es nu"><img src="../Images/0e9a4ad7331e60055e319b904e4e085a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*konpy3GauIeCiJiSKWAEbQ.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">COCO测试上的关键点检测结果使用掩膜R-CNN(</strong><a class="ae ix" href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8" rel="noopener" target="_blank"><strong class="bd kp">ResNet</strong></a><strong class="bd kp">-50-</strong><a class="ae ix" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610" rel="noopener" target="_blank"><strong class="bd kp">FPN</strong></a><strong class="bd kp">)</strong>(图片来自<a class="ae ix" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">作者论文</a>)</figcaption></figure><h2 id="6b15" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">7.3.SOTA方法比较</h2><figure class="ke kf kg kh fd ki er es paragraph-image"><div class="er es nv"><img src="../Images/fbd0723c5384e5c27de1a79bf472d277.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*SHSynjvS4tQZ3tA7nXWiEA.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">COCO测试开发上的关键点检测AP。</strong></figcaption></figure><ul class=""><li id="2491" class="lr ls hi ja b jb jc je jf jh mm jl mn jp mo jt mp lz ma mb bi translated"><a class="ae ix" rel="noopener" href="/analytics-vidhya/review-cmupose-openpose-winner-in-coco-keypoint-detection-challenge-2016-human-pose-ccbdbc72b7dd"> CMU姿势</a> +++是2016年使用多尺度测试和后处理的比赛冠军，而<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/review-g-rmi-1st-runner-up-in-coco-keypoint-detection-challenge-2016-human-pose-estimation-6c8d250f62a0"> G-RMI </a>也是2016年的亚军。</li><li id="2601" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">Mask R-CNN (62.7 APkp)比COCO 2016关键点检测获奖者高0.9分。</li><li id="8212" class="lr ls hi ja b jb mc je md jh me jl mf jp mg jt mp lz ma mb bi translated">使用掩码标签进行训练也有助于增加关键点检测AP。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="e9e9" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">文中还给出了使用Cityscape数据集进行语义分割的结果，并在附录中给出了COCO上的增强结果。如果有兴趣，请阅读文件了解更多详情。</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="f60b" class="mq lb hi bd kp mr ms mt lf mu mv mw lj jh mx my ll jl mz na ln jp nb nc lp nd bi translated">参考</h2><p id="cc4b" class="pw-post-body-paragraph iy iz hi ja b jb lt ij jd je lu im jg jh nw jj jk jl nx jn jo jp ny jr js jt hb bi translated">【2017 ICCV】【面具R-CNN】<br/><a class="ae ix" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">面具R-CNN </a></p><h1 id="b95a" class="la lb hi bd kp lc nz le lf lg oa li lj io ob ip ll ir oc is ln iu od iv lp lq bi translated">实例分割</h1><p id="f16a" class="pw-post-body-paragraph iy iz hi ja b jb lt ij jd je lu im jg jh nw jj jk jl nx jn jo jp ny jr js jt hb bi translated">[<a class="ae ix" rel="noopener" href="/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b?source=post_page---------------------------">SDS</a>][<a class="ae ix" href="https://towardsdatascience.com/review-hypercolumn-instance-segmentation-367180495979?source=post_page---------------------------" rel="noopener" target="_blank">Hypercolumn</a>][<a class="ae ix" href="https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339?source=post_page---------------------------" rel="noopener" target="_blank">deep Mask</a>][<a class="ae ix" href="https://towardsdatascience.com/review-sharpmask-instance-segmentation-6509f7401a61?source=post_page---------------------------" rel="noopener" target="_blank">sharp Mask</a>][<a class="ae ix" href="https://towardsdatascience.com/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------" rel="noopener" target="_blank">multipath net</a>][<a class="ae ix" href="https://towardsdatascience.com/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34?source=post_page---------------------------" rel="noopener" target="_blank">MNC</a>][<a class="ae ix" href="https://towardsdatascience.com/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92?source=post_page---------------------------" rel="noopener" target="_blank">instance fcn</a>][<a class="ae ix" href="https://towardsdatascience.com/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2?source=post_page---------------------------" rel="noopener" target="_blank">FCIS</a>][<a class="ae ix" rel="noopener" href="/analytics-vidhya/review-mask-r-cnn-instance-segmentation-human-pose-estimation-61080a93bf4">Mask R-CNN</a>]</p><h1 id="13f9" class="la lb hi bd kp lc nz le lf lg oa li lj io ob ip ll ir oc is ln iu od iv lp lq bi translated">人体姿态估计</h1><p id="4425" class="pw-post-body-paragraph iy iz hi ja b jb lt ij jd je lu im jg jh nw jj jk jl nx jn jo jp ny jr js jt hb bi translated">[ <a class="ae ix" href="https://towardsdatascience.com/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=post_page---------------------------" rel="noopener" target="_blank"> DeepPose </a> ] [ <a class="ae ix" href="https://towardsdatascience.com/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=post_page---------------------------" rel="noopener" target="_blank">汤普森NIPS'14 </a> ] [ <a class="ae ix" href="https://towardsdatascience.com/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------" rel="noopener" target="_blank">汤普森CVPR ' 15</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------">CPM</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/review-fcgn-fully-convolutional-google-net-human-pose-estimation-52022a359cb3">FCGN</a>][<a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/review-ief-iterative-error-feedback-human-pose-estimation-a56add160fa5">IEF</a>][<a class="ae ix" rel="noopener" href="/@sh.tsang/review-deepcut-deepercut-multi-person-pose-estimation-human-pose-estimation-da5b469cbbc3">deep cut&amp;DeeperCut</a>][<a class="ae ix" href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5" rel="noopener" target="_blank">纽维尔ECCV'16 &amp;纽维尔POCV'16 </a> ] [ <a class="ae ix" rel="noopener" href="/towards-artificial-intelligence/review-g-rmi-1st-runner-up-in-coco-keypoint-detection-challenge-2016-human-pose-estimation-6c8d250f62a0"> G-RMI</a></p><h1 id="1b54" class="la lb hi bd kp lc nz le lf lg oa li lj io ob ip ll ir oc is ln iu od iv lp lq bi translated"><a class="ae ix" rel="noopener" href="/@sh.tsang/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e">我之前的其他评论</a></h1></div></div>    
</body>
</html>