<html>
<head>
<title>ETL Pipeline using Spark SQL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Spark SQL的ETL管道</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/etl-pipeline-using-spark-sql-746bbfae4d03?source=collection_archive---------0-----------------------#2019-11-11">https://medium.com/analytics-vidhya/etl-pipeline-using-spark-sql-746bbfae4d03?source=collection_archive---------0-----------------------#2019-11-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c12a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本教程中，我们将创建一个ETL管道来读取CSV文件中的数据，转换它，然后将其加载到关系数据库(在我们的例子中是postgresql)和JSON文件格式。我们将向您展示使用sprak DataFrameReader和DataFrameWriter从不同来源读取和写入数据是多么简单。</p><h2 id="0378" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">以下是我们将在本教程中遵循的步骤:</h2><ul class=""><li id="d85e" class="jy jz hi ih b ii ka im kb iq kc iu kd iy ke jc kf kg kh ki bi translated">将数据集(csv)加载到Apache Spark</li><li id="f5fb" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">用Spark SQL分析数据</li><li id="0c07" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">将数据转换成JSON格式并保存到数据库中</li><li id="8e46" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">查询并将数据加载回Spark</li></ul><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ko"><img src="../Images/266d4944304551c265764d9caa04a87f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*Eti1MBI6xjJGR8CF5y-usg.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">Spark ETL管道</figcaption></figure><h1 id="2008" class="la je hi bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">数据集描述:</h1><p id="e045" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">自2013年以来，<a class="ae lu" href="https://www.cms.gov/openpayments/" rel="noopener ugc nofollow" target="_blank"> Open Payments </a>是一项联邦计划，收集药物和设备公司向医生和教学医院支付的费用信息，如差旅费、研究费、礼品费、演讲费和餐费。</p><p id="4e99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">文件:OP _ DTL _ GNRL _ pgyr 2018 _ p 06282019 . CSV:</strong></p><p id="24ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该文件包含2018计划年度报告的一般支付的数据集。一般付款是指支付给受保人(医生或教学医院)的款项或其他价值转移，但与研究协议或研究方案无关。</p><h2 id="8919" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">文件头:</h2><pre class="kp kq kr ks fd lv lw lx ly aw lz bi"><span id="bd0e" class="jd je hi lw b fi ma mb l mc md">$ head -1 OP_DTL_GNRL_PGYR2018_P06282019.csv</span><span id="6cf3" class="jd je hi lw b fi me mb l mc md">Change_Type,Covered_Recipient_Type,Teaching_Hospital_CCN,Teaching_Hospital_ID,Teaching_Hospital_Name,Physician_Profile_ID,Physician_First_Name,Physician_Middle_Name,Physician_Last_Name,,<br/>{..}<br/>,Indicate_Drug_or_Biological_or_Device_or_Medical_Supply_5,Product_Category_or_Therapeutic_Area_5,Name_of_Drug_or_Biological_or_Device_or_Medical_Supply_5,Associated_Drug_or_Biological_NDC_5,Program_Year,Payment_Publication_Date</span></pre><h2 id="1293" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">第一行:</h2><pre class="kp kq kr ks fd lv lw lx ly aw lz bi"><span id="e7be" class="jd je hi lw b fi ma mb l mc md">"NEW","Covered Recipient Physician",,,,"258145","HOWARD",,"SADINSKY",,"321 BOSTON POST RD",,"MILFORD","CT","06460-2574","United States",,,"Doctor of Osteopathy","Allopathic &amp; Osteopathic Physicians|Pediatrics","CT",,,,,"Mission Pharmacal Company","100000000186","Mission Pharmacal Company","TX","United States",13.78,"04/13/2018","1","In-kind items and services","Food and Beverage",,,,"No","No Third Party Payment",,"No",,,"No","521226951","No","Yes","Covered","Drug","Adrenocorticosteroid","Prednisolone 25","0178-0582-08",,,,,,,,,,,,,,,,,,,,,"2018","06/28/2019"</span></pre><h2 id="4011" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">在我们的示例中，我们将仅使用以下字段:</h2><pre class="kp kq kr ks fd lv lw lx ly aw lz bi"><span id="7a36" class="jd je hi lw b fi ma mb l mc md">{</span><span id="7f1a" class="jd je hi lw b fi me mb l mc md">"physician_id":"258145",</span><span id="8b4f" class="jd je hi lw b fi me mb l mc md">"date_payment":"04/13/2018 ",</span><span id="ff03" class="jd je hi lw b fi me mb l mc md">"record_id":"521226951",</span><span id="3290" class="jd je hi lw b fi me mb l mc md">"payer":"Mission Pharmacal Company",</span><span id="0132" class="jd je hi lw b fi me mb l mc md">"amount":13.78,</span><span id="5353" class="jd je hi lw b fi me mb l mc md">"Physician_Specialty":"Allopathic &amp; Osteopathic Physicians|Pediatrics ",</span><span id="9556" class="jd je hi lw b fi me mb l mc md">"Nature_of_payment":"Food and Beverage"</span><span id="1250" class="jd je hi lw b fi me mb l mc md">}</span></pre><h1 id="22af" class="la je hi bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">将csv文件中的数据读入数据帧:</h1><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mf"><img src="../Images/c078e0e2759a443a5d4bc130f7c4fdae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3mUnPb6hkfkxptMda8-iHg.png"/></div></div></figure><p id="69ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们使用SparkSession.builder函数创建一个SparkSession对象，然后我们对spark session对象调用read函数，这样我们就可以获得DataFrameReader对象。如您所见，我们使用函数csv来加载csv文件，而不是调用[ format("csv ")。load(paths:)，因为这个函数是在我们这里完成的，如果您检查这个函数的文档，就可以验证这一点:</p><pre class="kp kq kr ks fd lv lw lx ly aw lz bi"><span id="3fc6" class="jd je hi lw b fi ma mb l mc md">@scala.annotation.varargs<br/>def csv(paths: String*): DataFrame = format("csv").load(paths : _*)</span></pre><p id="f08b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们创建了一个UDF函数来Double，以确保列amount只包含double值，这样我们就可以将它转换为payment对象类型的数据集。</p><h1 id="371b" class="la je hi bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">转换成支付对象的数据集:</h1><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mk"><img src="../Images/9aa247aaa99d946fc3546046371e09a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jue76vJRB6RPBxPMAtzMRg.png"/></div></div></figure><p id="6404" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们使用spark sql函数在payment视图上执行sql查询，我们也可以直接使用dataframe df2来执行相同的查询，然后我们将其转换为payment数据集，注意，为了使用该语法，我们必须导入spark implicits。付款类也应该是一个案例类</p><pre class="kp kq kr ks fd lv lw lx ly aw lz bi"><span id="8689" class="jd je hi lw b fi ma mb l mc md">import <em class="ml">spark</em>.implicits._</span></pre><p id="2cfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们缓存新创建的数据集。</p><h1 id="66da" class="la je hi bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">使用spark数据集探索和查询未结支付数据:</h1><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mm"><img src="../Images/261a684504d756ce55ee894223197412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TThbOsFsHmi8ICt0-wIJ2A.png"/></div></div></figure><h1 id="19fa" class="la je hi bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">将数据集保存到json文件:</h1><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mn"><img src="../Images/fe930b79d3c178fadb43b7c27dc6fdb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D181f-CTXkOZK8BWaJvy5Q.png"/></div></div></figure><p id="2318" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将数据集导出到外部文件就像读取过程一样简单。这一次，我们调用write方法来获取DataFrameWriter，而不是read方法，我们指定写模式(这里我们指定overwrite来重新创建文件，如果它已经存在)，然后我们调用json方法并提供输出路径，同样，json与调用:</p><pre class="kp kq kr ks fd lv lw lx ly aw lz bi"><span id="95ad" class="jd je hi lw b fi ma mb l mc md">format("json").save(path)</span></pre><h1 id="5d3c" class="la je hi bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">将数据集保存到数据库:</h1><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mo"><img src="../Images/2e051c3dacd756fba35fa554101e4538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-eSdHRmEo6NyLXFc-ZXtw.png"/></div></div></figure><h1 id="4920" class="la je hi bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">源代码:</h1><pre class="kp kq kr ks fd lv lw lx ly aw lz bi"><span id="c87c" class="jd je hi lw b fi ma mb l mc md">import java.sql.DriverManager<br/><br/>import org.apache.log4j.{Level, Logger}<br/>import org.apache.spark.sql.{Dataset, SparkSession}<br/>import org.apache.spark.sql.functions.{<em class="ml">desc</em>, sum, udf}<br/><br/><br/>object ETL extends App {<br/><br/>  Logger.<em class="ml">getLogger</em>("org").setLevel(Level.<em class="ml">OFF</em>)<br/>  val <em class="ml">spark </em>= SparkSession.<em class="ml">builder</em>.appName("ETL pipeline").master("local[*]").getOrCreate()<br/>  import <em class="ml">spark</em>.implicits._<br/><br/>  /*<br/>     I - Read data from the csv file and create a temp view<br/>  */<br/><br/>  def strToDouble(str: String): Double = {<br/>    try{<br/>      str.toDouble<br/>    } catch {<br/>      case n: NumberFormatException =&gt; {<br/>        <em class="ml">println</em>("Cant cast " + str)<br/>        -1<br/>      }<br/>      }<br/>    }<br/>  val <em class="ml">toDouble </em>= <em class="ml">udf</em>[Double, String](<em class="ml">strToDouble</em>(_))<br/>  val <em class="ml">df </em>= <em class="ml">spark</em>.read.option("header","true").csv("/home/achraf/Desktop/labo/SparkLab/datasets/open_payments_csv/OP_DTL_GNRL_PGYR2018_P06282019.csv")<br/>  val <em class="ml">df2 </em>= <em class="ml">df</em>.withColumn("amount",<em class="ml">toDouble</em>(<em class="ml">df</em>("Total_Amount_of_Payment_USDollars")))<br/>  <em class="ml">df2</em>.createOrReplaceTempView("payments")<br/><br/><br/>/*<br/>    II - Transform into a dataset of payment object<br/> */<br/><br/>/*<br/>This class represent one row of our dataset, note that we selected only the fields that interest us<br/> */<br/><br/>  case class Payment(physician_id: String, date_payment: String, record_id: String, payer: String, amount: Double, physician_specialty: String, nature_of_payment: String)<br/><br/>  /*<br/>  Now we select fields that interest us from the temp view<br/>   */<br/>  val <em class="ml">ds</em>: Dataset[Payment] = <em class="ml">spark</em>.sql(<br/>    """select physician_profile_id as physician_id,<br/>      | Date_of_payment as date_payment,<br/>      | record_id as record_id,<br/>      | Applicable_manufacturer_or_applicable_gpo_making_payment_name as payer,<br/>      | amount,<br/>      | physician_specialty,<br/>      | nature_of_payment_or_transfer_of_value as nature_of_payment<br/>      | from payments<br/>      | where physician_profile_id is not null""".stripMargin).as[Payment]<br/>  <em class="ml">ds</em>.cache()<br/><br/>/*<br/>     III - Explore and query the Open Payment data with Spark Dataset<br/> */<br/><br/>  //print first 5 payment<br/>  <em class="ml">ds</em>.show(5)<br/><br/>  //print the schema of ds<br/>  <em class="ml">ds</em>.printSchema()<br/><br/>  // What are the Nature of Payments with reimbursement amounts greater than $1000 ordered by count?<br/>  <em class="ml">ds</em>.filter($"amount" &gt; 1000).groupBy("nature_of_payment")<br/>    .count().orderBy(<em class="ml">desc</em>("count")).show(5)<br/><br/>  // what are the top 5 nature of payments by count?<br/>  <em class="ml">ds</em>.groupBy("nature_of_payment").count().<br/>    orderBy(<em class="ml">desc</em>("count")).show(5)<br/><br/>  // what are the top 10 nature of payment by total amount ?<br/>  <em class="ml">ds</em>.groupBy("nature_of_payment").agg(<em class="ml">sum</em>("amount").alias("total"))<br/>    .orderBy(<em class="ml">desc</em>("total")).show(10)<br/><br/>  // What are the top 5 physician specialties by total amount ?<br/>  <em class="ml">ds</em>.groupBy("physician_specialty").agg(<em class="ml">sum</em>("amount").alias("total"))<br/>    .orderBy(<em class="ml">desc</em>("total")).show(5,false)<br/><br/>  /*<br/>      IV - Saving JSON Document<br/>   */<br/><br/>  // Transform the dataset to an RDD of JSON documents :<br/>  <em class="ml">ds</em>.write.mode("overwrite").json("/home/achraf/Desktop/labo/SparkLab/datasets/open_payments_csv/OP_DTL_GNRL_PGYR2018_P06282019_demo.json")<br/><br/>  /*<br/>      V - Saving data to database<br/>   */<br/>  val <em class="ml">url </em>= "jdbc:postgresql://localhost:5432/postgres"<br/>  import java.util.Properties<br/>  val <em class="ml">connectionProperties </em>= new Properties()<br/>  <em class="ml">connectionProperties</em>.setProperty("Driver", "org.postgresql.Driver")<br/>  <em class="ml">connectionProperties</em>.setProperty("user", "spark")<br/>  <em class="ml">connectionProperties</em>.setProperty("password","spark")<br/><br/>  val <em class="ml">connection </em>= DriverManager.<em class="ml">getConnection</em>(<em class="ml">url</em>, <em class="ml">connectionProperties</em>)<br/>  <em class="ml">println</em>(<em class="ml">connection</em>.isClosed)<br/>  <em class="ml">connection</em>.close()<br/><br/>  val <em class="ml">dbDataFrame </em>= <em class="ml">spark</em>.read.jdbc(<em class="ml">url</em>,"payment",<em class="ml">connectionProperties</em>)<br/>  <em class="ml">dbDataFrame</em>.show()<br/><br/>  <em class="ml">ds</em>.write.mode("append").jdbc(<em class="ml">url</em>,"payment",<em class="ml">connectionProperties</em>)<br/>  <em class="ml">dbDataFrame</em>.show()<br/><br/>}</span></pre><h1 id="c15b" class="la je hi bd jf lb lc ld jj le lf lg jn lh li lj jq lk ll lm jt ln lo lp jw lq bi translated">总结:</h1><p id="c530" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">在这篇博文中，您了解了如何将Open Payments CSV文件数据ETL到JSON，使用SQL进行探索，并使用Spark数据集存储到关系数据库。</p></div></div>    
</body>
</html>