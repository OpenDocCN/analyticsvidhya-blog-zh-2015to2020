<html>
<head>
<title>How Good (or Bad) is Traditional TF-IDF Text Mining Technique?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">传统的TF-IDF文本挖掘技术有多好(或多坏)？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-good-or-bad-is-traditional-tf-idf-text-mining-technique-304aec920009?source=collection_archive---------3-----------------------#2020-01-31">https://medium.com/analytics-vidhya/how-good-or-bad-is-traditional-tf-idf-text-mining-technique-304aec920009?source=collection_archive---------3-----------------------#2020-01-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a91e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗨，数据伙伴们，爱好者们和有志者们！</p><p id="b3f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于各种教科书、博客和在线论文中的知识和研究，为您提供一个关于TF-IDF技术的博客。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/407a5499ceaf9fbff18f09beaac823b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*vh4OZUCQxiLHz1Gm4g2i2w.jpeg"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae jp" href="https://www.pexels.com/search/concept/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="d37a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文本挖掘是从文本和web文档中进行高质量信息检索的领域中众所周知的技术。在这个领域中一个非常流行和流行的策略是<strong class="ih hj"> <em class="jq">向量化词频和逆文档频率</em> </strong> (TF-IDF)表示法。事实上，谷歌搜索引擎在搜索一个单词时也使用这种技术。它基于<strong class="ih hj"> <em class="jq">无监督学习技术</em> </strong>。TF-IDF将您的文档文本转换为一个单词包<strong class="ih hj"><em class="jq"/></strong>，然后为每个单词分配一个加权项，该加权项使用以下公式计算。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jr"><img src="../Images/7e9147fd21c3ba1fe534a1bc6ff51275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AAN2lWsIVP8W4FDsqm9RUw.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">帕拉维</figcaption></figure><p id="12b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jq">其中TF(t，d) =(文档d中术语/单词t的出现次数)/(文档d中的总术语数)和</em> </strong></p><p id="8350" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="jq"/></strong></p><p id="93ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是计算文档中术语的加权频率的传统方法，这种方法足够好并且简单。但是，它忽略了许多在处理文本时应该相关的细节。</p><h1 id="3dbc" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">传统方法的工作原理及其局限性:</h1><ol class=""><li id="ea5a" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc lb lc ld le bi translated">它基于单词在文档中的出现频率来计算单词的加权项，但是它不考虑单词在文本中的位置、语义以及与文档中其他单词的共现。</li><li id="d393" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">由于TF-IDF是一种无监督的特征选择技术，它并没有说明术语/单词与特定类别的相关性，而是仅受文档的限制。</li><li id="2c85" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">此外，逆文档频率具有不考虑在文档和整个文档语料库中更频繁出现的单词的辨别能力。这意味着根据传统的方法，如果一个词出现在每个文档中，IDF将赋予该词0的权重，因此，IDF认为该词与语料库中的其他术语没有太大的相关性。因此，如果上面的公式简单地说，TD*IDF得分(权重)越高，该术语就越稀有，反之亦然。理想情况下，IDF应该考虑那些在文档中更频繁出现的单词，以便它能够更好地将自己与群集中的其余单词(除了<strong class="ih hj"> <em class="jq">停用词</em> </strong>)进行分类。</li><li id="4623" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">这种技术直接在字数统计空间中计算文档相似性，因此对于大型文档来说非常慢。</li></ol><p id="78a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">许多研究人员在文本分类领域进行了工作，并提出了TF-IDF的多个修改版本来处理传统方法的局限性。已经进一步解释了这些改进的技术中的一些。</p><h1 id="d963" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">术语频率相对频率(TF-RF): </strong></h1><p id="823b" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">这是基于项的类间分散系数，计算如下:</p><p id="5d4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">T3W(I，j) = TF(i，j)* IDF(I)* D(I)</strong></p><p id="a1f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jq">其中D(i) = (1/n)∑(F(t，i) — avg(F(t，i)) </em> </strong></p><p id="3f58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，n是类别的数量，F(t，I)是具有术语t的文档的数量，并且属于术语I所属的类别。所以这个等式检验了一个术语对分类过程的贡献。如果一个项均匀分布在各个类中，那么类间离差D将会很低，反之亦然。</p><h1 id="b82d" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">基于修改逆文档频率的TF-IDF:</strong></h1><p id="7d00" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">计算方法如下:</p><p id="9e8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jq"> W(i，j) = TF(i，j) *修改后的IDF(i) </em> </strong></p><p id="a356" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jq">修改后的IDF(I)=</em></strong><strong class="ih hj"><em class="jq">Log10((整个语料库中的文档数+1)/包含术语/单词I的文档总数)</em> </strong></p><p id="746d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与传统的IDF不同，该系数不会忽略在所有文档中更频繁出现并且在特定文档中具有高TF的单词。但是需要特殊处理来处理所有的<strong class="ih hj"> <em class="jq">停用词</em> </strong>，因为它们在所有文档中出现的频率更高。</p><h1 id="807f" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">基于课程频率的TF-IDF:</h1><p id="6e4e" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">这种方法也是基于如上所述的修改的逆文档频率。除此之外，类别频率系数也被添加到最终等式中，如下所述:</p><p id="fb4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jq"> W(i，j) = TF(i，j) *修正IDF(i) *类频率</em> </strong></p><p id="5b29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jq">类频率= n(c(i，j))/N(c(i)) </em> </strong></p><p id="d405" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中n(c(i，j))表示具有术语j并且属于文档I所属的类别c(i)的文档的总数。N(c(i))表示c(i)类文件的总数。这种策略将使术语在类中的频率标准化，并根据它们的类频率赋予它们重要性。</p><p id="7da3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">研究人员和科学家已经实施了几个修改过的策略，你可以让他们通读所有可用的研究论文。</p><h2 id="203e" class="ln jx hi bd jy lo lp lq kc lr ls lt kg iq lu lv kk iu lw lx ko iy ly lz ks ma bi translated">快乐阅读！</h2><p id="0aa0" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">可以通过<a class="ae jp" href="https://www.linkedin.com/in/pallavi-ahuja-22590114/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我取得联系，并随时对博客中陈述的任何误导性信息发表评论:)</p></div></div>    
</body>
</html>