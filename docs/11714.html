<html>
<head>
<title>Activation Functions in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-in-neural-networks-811e782d37e8?source=collection_archive---------12-----------------------#2020-12-15">https://medium.com/analytics-vidhya/activation-functions-in-neural-networks-811e782d37e8?source=collection_archive---------12-----------------------#2020-12-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3a57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是激活功能？</strong></p><p id="03a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">激活函数</strong>是决定神经网络输出的数学方程。在计算其输入(xi)的“加权和(Wi)”后，将<strong class="ih hj">函数</strong>附加到网络中的每个神经元，添加一个偏差，并根据每个神经元的输入是否与模型的预测相关来确定是否应该激活<strong class="ih hj"/>(“触发”)。</p><p id="8168" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">任何激活函数的重要用途是将非线性属性引入我们的网络。</strong></p><p id="2367" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有的<strong class="ih hj"> <em class="jd">输入Xi的</em> </strong>乘以它们的<strong class="ih hj"> <em class="jd">权重Wi的</em> </strong>分配给每个环节并连同<strong class="ih hj"> <em class="jd">偏差</em> </strong> <strong class="ih hj"> <em class="jd"> b </em> </strong>一起求和。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/03b5b83564c61d3ebedcdfe196c688e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*JAXlg6AApL7TDmyrgMKSbQ.png"/></div></figure><p id="1dd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">激活功能类型:</strong></p><p id="4d98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性函数</p><p id="db69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非线性函数</p><p id="401e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">线性函数</strong></p><p id="a8f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性激活函数的形式为<strong class="ih hj"> y=mx+c. </strong>它采用输入(<strong class="ih hj"> <em class="jd"> Xi的</em> </strong>)，乘以每个神经元的权重(<strong class="ih hj"> <em class="jd"> Wi的</em> </strong>)，并产生与输入成比例的输出。函数的输出将不会被限制在任何范围之间。由于该函数遵循线性模式，因此用于回归问题。</p><p id="993f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">非线性函数- </strong></p><p id="b452" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非线性激活函数是最常用的激活函数。这使得模型很容易概括或适应各种数据，并区分输出。它允许模型在网络的输入和输出之间创建复杂的映射，这对神经元的学习和解决复杂的业务问题至关重要。</p><p id="92f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非线性函数的类型:</p><p id="7b41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">乙状结肠的</p><p id="737e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">双曲正切</p><p id="b829" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">热卢</p><p id="82f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">泄漏Relu</p><p id="68b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Swish Relu</p><p id="67da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ELU</p><p id="07e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预relu</p><p id="0f26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">乙状结肠:</strong></p><p id="dafc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">sigmoid函数(y)的输出始终介于0和1之间。</p><p id="46f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在浅层神经网络的情况下，用于隐藏层和输出层。</p><p id="2686" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Sigmoid函数f(x)的导数将总是在0到0.25之间。</p><p id="fddc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">遭受消失梯度问题。</p><p id="b725" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据不是以零为中心的，这使得收敛更加困难并且计算量很大。</p><blockquote class="jm jn jo"><p id="6a56" class="if ig jd ih b ii ij ik il im in io ip jp ir is it jq iv iw ix jr iz ja jb jc hb bi translated"><strong class="ih hj">用于分类问题。</strong></p></blockquote><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es js"><img src="../Images/bcd67a78154aa0b9915530a7b2ef0308.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*Wta74GthEHua8QY3OwKoVA.png"/></div></figure><p id="1d0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> TanH- </strong></p><p id="49f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Tanh将y的值在-1到1之间转换。</p><p id="efec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">双曲正切函数f(y)的导数总是在0和1之间。</p><p id="c0f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据以零为中心。</p><p id="dc55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">收敛更容易。</p><p id="c186" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度神经网络中的消失梯度问题。</p><blockquote class="jm jn jo"><p id="8e31" class="if ig jd ih b ii ij ik il im in io ip jp ir is it jq iv iw ix jr iz ja jb jc hb bi translated"><strong class="ih hj"> Tanh优于乙状结肠。</strong></p></blockquote><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jt"><img src="../Images/291896c0e6bb2bee71148bdf6c5c40e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*XVPhg0rx54C_rvqhk-PlOA.png"/></div></figure><p id="64b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ReLU:(整流线性单元)</strong></p><p id="186b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLU是已经流行的非线性激活函数</p><p id="0cbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLU转换0到max(x)之间的值。它将负值转换为0。</p><p id="7d31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">函数f(x)的导数将是0或1。</p><p id="1268" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决消失梯度问题。</p><p id="2f32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它计算速度更快，容易收敛。</p><p id="cf94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">面临死亡神经元的问题。(在反向传播期间，如果权重为负，则wold=wnew，并且该神经元是死神经元)</p><p id="a8c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据不是以零为中心的。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ju"><img src="../Images/7e70818d2fb5f19549f9dfdc7e9954a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*D39dBTcBLV3bfY0l-U8tcQ.png"/></div></figure><p id="345c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">泄漏的ReLU: </strong></p><p id="5919" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Leaky ReLU函数只不过是ReLU函数的改进版本。当x小于0时，我们没有将Relu函数定义为0，而是将其定义为x的一个小的线性分量。通常，对于x的负值，会引入0.01的值。</p><p id="12be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它解决了死神经元问题。</p><p id="cf06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y的值将是最大值(0.01*y到y)。</p><p id="f3c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y的导数将在0和1之间。</p><p id="7d08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这在计算上更容易。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jv"><img src="../Images/f0617112dec0b50f24f0d8bd48040ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*tDlacEHcTCuU0YXx6Wew3w.png"/></div></figure><p id="11da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ELU:(指数线性单位)</strong></p><p id="53c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">指数线性(ELU) </strong>有一个额外的阿尔法常数，它应该是正数。如果x的值小于0，则负数上加一个alpha值。alpha大于或等于0。</p><p id="44d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决死亡神经元的问题。</p><p id="1503" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据以零为中心。</p><p id="26b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算成本高</p><p id="240d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于阿尔法项导致收敛缓慢。</p><p id="4ea3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">导数永远不会等于0。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jw"><img src="../Images/d4c98f63eb0bc272119971f554ad75b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*4xfJ0qeZ3__UVeFvBZ9SZw.png"/></div></figure><p id="b19b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> PReLu(参数ReLU): </strong></p><p id="bb0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果x是0.01，它将是漏relu，如果α是0，它是relu。在PReLU中，alpha值是动态训练的，这使它不同于其他ReLU函数。</p><p id="f795" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">α的值一般在0到1之间。</strong></p><p id="a5af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解决死神经元问题。</strong></p><p id="841e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Swish ReLU:(一个自门控功能)</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/6aa10eefca562c64467594dec047185e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*TU_6cX-7bDJS8vByybUZ_A.png"/></div></figure><p id="a633" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Y=x*sigmoid(x) </strong></p><p id="947b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">适用于层数超过40层的深度神经网络。</p><p id="1396" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用于自动门控和LSTM。</p><p id="76c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这在计算上非常昂贵。</p><p id="7de0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决死神经元问题。</p><p id="25d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如何选择激活功能？</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jx"><img src="../Images/675cfa99de50670297e0af6abe80c928.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*blof6z8Hk6OFqU9ZuDHXnw.png"/></div></figure><p id="68e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择一个激活函数是一个超参数，是由试凑法决定的，请记住下面的参数</p><p id="c17b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性激活函数用于解决回归问题。</p><p id="603d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在浅层神经网络的情况下，Sigmoid用于分类问题。</p><p id="4e7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TanH和Sigmoid的组合用于深度神经网络。</p><p id="0a1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLU广泛应用于大多数神经网络的隐层中。向前发展，ReLU的各种变体可用于解决ReLU的死神经元问题。</p><p id="eb11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi">我希望我已经给了你一些关于激活函数的基本理解。</em> </strong></p><blockquote class="jm jn jo"><p id="3ff0" class="if ig jd ih b ii ij ik il im in io ip jp ir is it jq iv iw ix jr iz ja jb jc hb bi translated">让我们连接:</p></blockquote><p id="7b09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">领英:<a class="ae jy" href="https://www.linkedin.com/in/prerna-nichani" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/prerna-nichani</a></p><p id="5ea7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">感谢阅读！</em>T3】</strong></p><p id="39ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi"><strong class="ih hj"><em class="jd">Thankyou for Reading!</em></strong></p></div></div>    
</body>
</html>