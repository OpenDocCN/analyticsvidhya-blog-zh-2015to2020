<html>
<head>
<title>PCA: A Linear Transformation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析:一种线性变换</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pca-a-linear-transformation-f8aacd4eb007?source=collection_archive---------0-----------------------#2019-04-07">https://medium.com/analytics-vidhya/pca-a-linear-transformation-f8aacd4eb007?source=collection_archive---------0-----------------------#2019-04-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/d83c4cd340e86b7235696ea380f2376c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XzSCohIM5ABAFdKF3CwDTg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">太空旋转</strong>—<a class="ae hw" href="https://www.pexels.com/@kubiceknov?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">雅各布·诺瓦切克</a>摄自<a class="ae hw" href="https://www.pexels.com/photo/time-lapse-photo-of-stars-on-night-924824/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">佩克斯</a></figcaption></figure><div class=""/><h1 id="c064" class="iw ix hz bd hv iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h1><p id="2047" class="pw-post-body-paragraph jt ju hz jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">最近我一直在努力加深我对主成分分析的了解。特别是，我想了解为什么<strong class="jv ia">主成分是我们特征</strong>的协方差矩阵的特征向量(你会在这里找到答案)。</p><p id="5c33" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">此外，由于我真的喜欢将PCA视为线性变换，所以我寻找了一篇直奔主题的简短文章，但我没有找到。所以我最终读完了整篇论文。</p><p id="def7" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">不是每个人都有时间进行长篇阅读；因此，我决定用一篇简短的文章来分享我学到的东西。我的目的不是解释基本的线性代数概念(网上有很多好的资源)，而是提供一篇简洁的文章，提供你在其他文章中可能找不到的东西:我只是想填补一个小空白。</p><p id="8271" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果你的线性代数有点生疏，我建议在阅读这篇文章之前，先通读一下<a class="ae hw" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" rel="noopener ugc nofollow" target="_blank">线性代数的精髓</a>，这是一个<strong class="jv ia">YouTube上很棒的</strong>系列。</p><h1 id="1009" class="iw ix hz bd hv iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">什么是主成分分析？</h1><p id="4700" class="pw-post-body-paragraph jt ju hz jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">主成分分析让我们降低数据的维度。但是怎么做呢？嗯，它让我们发现我们的特征变化最大的方向，以便我们可以使用它们来转换我们的数据集。换句话说，PCA的目标是找到一种新的方式来表达数据，以便将<strong class="jv ia">特征冗余最小化</strong>，并且我们可以保留信号并过滤掉噪声。</p><p id="2bb3" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">PCA假设<strong class="jv ia">信号=方差</strong>(即我们忽略数据变化最小的方向)，方差可以分解为<strong class="jv ia">正交成分</strong>(即我们的新特征将具有零协方差)。最后，PCA使用<strong class="jv ia">线性</strong>变换来重新表达数据(即PCA与我们特征的非线性组合无关)。</p><h1 id="8fb5" class="iw ix hz bd hv iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">PCA作为线性变换</h1><p id="0653" class="pw-post-body-paragraph jt ju hz jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">让我们从我们的<strong class="jv ia">贬低的</strong>数据集<strong class="jv ia"> X </strong>开始，一个<em class="kw"> m </em>乘以<em class="kw"> n </em>矩阵<em class="kw"> </em>其中<em class="kw"> m </em>是特征的数量，<em class="kw"> n </em>是实例的数量。</p><p id="b66e" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">通过PCA，我们将正交变换<strong class="jv ia"> P </strong>应用于<strong class="jv ia"> X </strong>，以获得数据的新表示。我们称这种表现为<strong class="jv ia"> Y </strong>。要变换<strong class="jv ia"> X </strong>，我们只需将其与<strong class="jv ia"> P </strong>相乘，如下所示。</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es kx"><img src="../Images/53f1542c7b614fc871bf264a5482eb34.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*PAWrLWL9-AuIrR6XQElG4A.png"/></div></figure><figure class="ky kz la lb fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lc"><img src="../Images/c8ebdc29faa96731aab790c4e8571a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PuH0OdvJ9q5NHKCUQANX9Q.png"/></div></div></figure><p id="6a67" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">但是等等，什么是正交变换？<strong class="jv ia">正交变换</strong>是由<strong class="jv ia">正交矩阵</strong>表示的线性变换，其行和列是具有单位范数的正交向量。</p><p id="c532" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">好了，我们刚刚了解了PCA是一个线性变换，可以用一个特殊的矩阵来表示。但是，我们为什么要把<strong class="jv ia"> P </strong>乘以<strong class="jv ia"> X </strong>？</p><p id="cc5a" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">让我们看看上面的矩阵。<strong class="jv ia"> P </strong>的行称为主成分。<strong class="jv ia"> Y </strong>的每一列包含的元素是<strong class="jv ia"> P </strong>的<em class="kw">第j</em>行<em class="kw">和<strong class="jv ia"> X </strong>的<strong class="jv ia"> </strong> <em class="kw">列</em>之间的点积。这里的每个点积代表一个点从特征空间到主分量的投影(如果你不知道什么是投影，我建议你看“线性代数的本质”，这是我在上面赞助的)。</em></p><p id="75d7" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">因此，<strong class="jv ia"> Y </strong>包含原始特征在我们的主成分所跨越的空间上的投影，主成分是单位向量。换句话说，将<strong class="jv ia"> P </strong>乘以<strong class="jv ia"> X </strong>意味着将<strong class="jv ia"> X </strong>投影到由<strong class="jv ia"> P </strong>的行所跨越的空间上。</p><p id="3227" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">注意<strong class="jv ia">一旦我们知道</strong> <strong class="jv ia"> P，我们可以选择使用多少组件来跨越新的特征空间</strong>。这决定了我们将保留多少转换后的特征(即，我们将在模型或可视化中使用多少行<strong class="jv ia"> Y </strong>)。<strong class="jv ia">降维来了！</strong>但是我们应该如何选择要保留的组件呢？根据我们的假设，信号是方差。因此，我们将保留差异较大的组件。</p><h1 id="d692" class="iw ix hz bd hv iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">如何找到P</h1><p id="13c0" class="pw-post-body-paragraph jt ju hz jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">请记住，我们转换的目标是<strong class="jv ia">最大限度地减少冗余，以保留信号并滤除噪声。</strong></p><p id="4d2e" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">什么是冗余？嗯，如果我们可以将我们的特征集合<strong class="jv ia"> A </strong>表示为其他特征集合<strong class="jv ia"> B </strong>的线性组合，那么我们说属于集合<strong class="jv ia"> A </strong>的特征是多余的。我们知道协方差衡量特征之间线性关系的大小。因此，为了最小化冗余，我们将最小化协方差(使我们的特征正交化确实会使协方差项为零)。</p><p id="0c63" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">为了找到<strong class="jv ia"> P </strong>我们需要从<strong class="jv ia"> C </strong>，<strong class="jv ia"> </strong>的平方对称协方差矩阵<strong class="jv ia"> X </strong>开始，它的对角项是方差，非对角项是协方差。</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es ld"><img src="../Images/7193b632b13a74385836ce752c82e06b.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*9jzrN8yGJn7UMs9uRy_0Dg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">X的协方差矩阵</figcaption></figure><p id="a22c" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在，为了最小化冗余，我们的正交矩阵<strong class="jv ia"> P </strong>将不得不变换<strong class="jv ia"> X </strong>，使得<strong class="jv ia">D</strong>,<strong class="jv ia">Y、</strong>的协方差矩阵变成<strong class="jv ia"> </strong>对角线(没有协方差项)。但是什么是<strong class="jv ia"> D </strong>？是一个<strong class="jv ia"> C </strong>和<strong class="jv ia">P</strong>T30】的函数(从Y的协方差开始——只是用Y代替上面公式中的X——用铅笔推导)。</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es le"><img src="../Images/5bbedb2d730f3318ca16e41443d590e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*8TAT0F8fnxVPHGmY2D5JCg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">Y的协方差矩阵</figcaption></figure><p id="4aeb" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在如何选择<strong class="jv ia"> P </strong>到<strong class="jv ia">T35】对角化<strong class="jv ia"> D </strong>？</strong></p><p id="aa59" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">原来，任何对称矩阵都可以用其特征向量<strong class="jv ia"> E </strong>的正交矩阵和用特征值填充<strong class="jv ia"> </strong>的对角矩阵<strong class="jv ia"> V </strong>来分解。这与矩阵的<a class="ae hw" href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix" rel="noopener ugc nofollow" target="_blank">特征分解</a>的概念有关。</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es lf"><img src="../Images/12a3bb1110c183a164d9704dfc453bec.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*OSp9lLg9lXWtZK_e2EUZKA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">对称矩阵的特征分解</figcaption></figure><p id="5239" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们可以将<strong class="jv ia"> C </strong>代入<strong class="jv ia"> D </strong>的公式。</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es lg"><img src="../Images/21f98c118ef76d071004c0578abcce99.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*oPYlUukPkxomTCJuB3rNpw.png"/></div></figure><p id="a14a" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果我们选择<strong class="jv ia"> P </strong> = <strong class="jv ia"> E </strong> <strong class="jv ia">转置</strong>我们得到</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="er es lh"><img src="../Images/4942e96d69975987e994bde3b04c83a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*XdJh9d24mbH5MAPkJceX2Q.png"/></div></figure><p id="5855" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">记住正交矩阵的乘积是一个单位矩阵。</p><p id="67d7" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们刚刚发现<strong class="jv ia"> P </strong>的<strong class="jv ia">主成分是X </strong>的协方差矩阵的特征向量。</p><p id="3544" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">正如已经提到的，最重要的主要组件是那些我们的新特性变化最大的组件。如果我们想使用方差来排列我们的主成分，那么我们可以使用<strong class="jv ia"> D </strong>。事实上，<strong class="jv ia"> D </strong>的对角线包含了沿着我们新的正交轴的新特征的变化(我们选择保留的主要成分)。我写了“新轴”:是的，这种线性变换意味着基础的改变。</p><p id="d417" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">就是这样！</p></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><p id="69d1" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">万一你有兴趣，我看的论文是<a class="ae hw" href="https://arxiv.org/abs/1404.1100" rel="noopener ugc nofollow" target="_blank"> <em class="kw">主成分分析教程</em> </a> <em class="kw"> </em>作者黄邦贤·施伦斯。</p><p id="568c" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果你认为有什么可以改进的地方，请告诉我。</p><p id="c212" class="pw-post-body-paragraph jt ju hz jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">谢了。</p></div></div>    
</body>
</html>