<html>
<head>
<title>Word2Vec — CBOW &amp; Skip-gram : Algorithmic Optimizations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec — CBOW &amp; Skip-gram:算法优化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/word2vec-cbow-skip-gram-algorithmic-optimizations-921d6f62d739?source=collection_archive---------2-----------------------#2020-01-25">https://medium.com/analytics-vidhya/word2vec-cbow-skip-gram-algorithmic-optimizations-921d6f62d739?source=collection_archive---------2-----------------------#2020-01-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2e6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">易消化+小而脆:</p><p id="4ac5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Word2Vec所做的是给定一个单词，它返回一个向量，使得这些向量在语义上与相似的单词相似。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/928b95a7e6601d366c33b3eae96e94f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QKThZ7C9hdOaahxuJQR7Gg.png"/></div></div></figure><p id="a144" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们以下面的例子为例，如果猫是焦点词，其他环境是上下文词。上下文单词对于理解焦点单词非常有用，反之亦然。</p><p id="c870" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有两种Word2Vec算法<strong class="ih hj"> CBOW(连续包字)</strong> &amp; <strong class="ih hj"> skip-gram。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jp"><img src="../Images/9b445a3424443ec3201bb932f52bd92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GzpUBUh3uC17AEgL2g5v-Q.png"/></div></div></figure><h1 id="628e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">Word2Vec: CBOW</h1><p id="edcd" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">假设我们有大小为V的单词V的词汇表&amp;我们使用一键编码来表示每个单词，因此，给定任何单词，我们都可以有一个V维的二进制向量(因为我们在词汇表中有总共V个单词)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kt"><img src="../Images/b5f4296f756884e5fca3ed7fda86c9c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wsCn0s77LrJiBD2YD8Q6fQ.png"/></div></div></figure><p id="214d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一个热编码的k个上下文词向量，那么，CBOW的核心思想是给定上下文词，我们可以预测焦点词。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ku"><img src="../Images/ef3d01f7effa4a96848c8e984c624daa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cQb0LcT0ymMuKDs0ulWfPw.png"/></div></div></figure><p id="eb0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">k个上下文单词(每个上下文单词的维度为词汇v)连接到N个维度的隐藏层，如下所示。所以，现在我们的目标是预测维度v的焦点词向量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kv"><img src="../Images/db311848d69c6b75f82e70e3b100d981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UVXiR2bPVKYa8T_CFDqOSQ.png"/></div></div></figure><p id="e14d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了训练Word2Vec CBOW，我们可以创建焦点单词和上下文单词组合，并训练这个NN。在训练结束时，我们有了所有这些重量(上面用黄色表示)。所以对于每个单词，我们在权重N*V矩阵中有一个向量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jp"><img src="../Images/42d437a129a388f5cd62c52bd1e6cc4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4v7npl3SoMqtjJVBEWBXJQ.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kw"><img src="../Images/47a7306d73988fb4a2f79f7543b6a0a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QLUttawx3ROnHbMJsx2ycA.png"/></div></div></figure><p id="85e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Word2Vec: Skip-gram </strong></p><p id="7229" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在CBOW中，我们试图在给定上下文单词的情况下预测焦点单词，而在skip-gram中，我们试图做相反的事情，我们试图在给定焦点单词的情况下预测上下文单词(完全相反的任务)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kx"><img src="../Images/cdfdfa459e5f3162bfc4e5df3e99b80b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZqwL0RKmKy1ifEnRKOa4w.png"/></div></div></figure><p id="72cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们有维度v(词汇的大小)的焦点单词，其连接到大小为N的隐藏层，该隐藏层将进一步连接到softmax(每个softmax是v维度)以预测上下文单词。因此，在输出层，我们有k个softmax(上下文字数)，它可以被认为是k个多类分类器。</p><p id="cbf5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在要训练的参数的数量方面，但是在CBOW中，我们有一个softmax要训练，而在skip-gram中，我们有k个softmax，因此skip-gram比CBOW花费更多的时间，所以它在计算上更昂贵。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kx"><img src="../Images/c56d63fa0cc7c3fb9a45e1bef137d189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eEo5MGMrFbGC5zP8f0fzFQ.png"/></div></div></figure><p id="caf7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【CBOW之间的比较&amp;跳过程序:</p><ol class=""><li id="37df" class="ky kz hi ih b ii ij im in iq la iu lb iy lc jc ld le lf lg bi translated">CBOW比skip-gram训练起来相对更快(因为CBOW只需要训练一个softmax)。</li></ol><p id="4ab7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.CBOW更适合频繁出现的单词(因为如果一个单词出现得更频繁，它将有更多的训练单词需要训练)。</p><p id="16d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.Skip-gram速度较慢，但适用于比CBOW更小的数据量。</p><p id="2e94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.Skip-gram比CBOW更适用于出现频率较低的单词。</p><p id="0400" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.CBOW是一个比Skip-gram更简单的问题(因为在CBOW中，我们只需要在给定许多上下文单词的情况下预测一个焦点单词)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jp"><img src="../Images/ecd98ae656629ba6f99514fc9b658d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0nTnxfc6oINvKLtYlvb91A.png"/></div></div></figure><p id="76c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Skip-gram和CBOW，随着上下文单词数量的增加，N维向量表示更好(通常N维接近200/300)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lh"><img src="../Images/0c28ef06fcdd7f4d2b97870805511954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RDRK-mz7fDcYKSXSNLQVig.png"/></div></div></figure><p id="8acc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Skip-gram的情况下，我们使用下面的权重矩阵来获得上下文单词的向量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es li"><img src="../Images/17f7c87e0b398f2381d68c8228fff2c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Msytd895zJoN8WSNWOzLFg.png"/></div></div></figure><p id="4dc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这两种情况下，我们都有一个主要的问题，我们有太多的参数需要学习(k+1)(N*V)。在N=200，k=5 &amp; V=10k的情况下，我们有将近1200万个参数要训练&amp;这可能要花很长时间来训练，所以，我们需要以某种方式优化这种训练。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kx"><img src="../Images/cdba9f1109fb2b7aece00ef1bf800631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99n7mldvib0OU-Rca-20UA.png"/></div></div></figure><p id="4a1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">训练Word2Vec的算法优化:</strong></p><p id="5f76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们有数百万个权重要学习，这非常耗时，一种处理方法是分层softmax &amp;另一种是负采样。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lj"><img src="../Images/9254de0a55d132adc299d99b1d2a0ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ErguaWBxe2_kb9EIHy3MQ.png"/></div></div></figure><p id="dc05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">分级Softmax: </strong></p><p id="3719" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在CBOW &amp; Skip-gram是softmax，他们需要花很多时间来训练，所以，更困难的部分是优化softmax的训练。因此，核心思想是我们能否优化v-softmax，使其达到最优。softmax正在做的是v类分类，所以，我们可以修改它，使其达到最优。假设我们的词汇表有8个单词，我们可以把这些单词作为二叉树的叶节点，当我们把树分成两部分时，一半的单词在树的一边，另一半在树的另一边。</p><p id="e341" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，代替v-class分类器，我们说单词出现在二叉树的这一半中，还是单词出现在前四个或后四个中。让我们假设它在前四个单词中，然后我们可以划分前四个，看看这个单词是在前两个还是在后两个，等等。所以，我们用了3个激活单元来找出单词，而不是8个激活单元。因此，如果我们在softmax中有v个激活，我们将几乎需要一个v基数为2的激活的日志来找出焦点词概率。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jp"><img src="../Images/907f9441b019a3f22dba99089ee5bce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8TkIM2gTveHTV5_nZmJ3fw.png"/></div></div></figure><p id="1e43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">负采样:</strong></p><p id="d4e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一种基于统计/概率的技术，只需在每次迭代中更新词汇表之外的单词样本&amp;样本定义为—</p><ol class=""><li id="fd41" class="ky kz hi ih b ii ij im in iq la iu lb iy lc jc ld le lf lg bi translated">始终保持目标词&amp;</li><li id="5f42" class="ky kz hi ih b ii lk im ll iq lm iu ln iy lo jc ld le lf lg bi translated">在非目标单词中，它们的唯一样本被更新并由下面的公式给出，其中单词被选择的概率与它的频率成反比</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kx"><img src="../Images/c916997f9b6b53170edb5c29fc1099ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9H5VHUQwtNDhz8JA4__ERg.png"/></div></div></figure></div></div>    
</body>
</html>