<html>
<head>
<title>Python &amp; SnowNLP: Sentiment Analysis for the Chinese Language</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python和SnowNLP:中文情感分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/python-snownlp-sentiment-analysis-for-the-chinese-language-8d9cafd0447d?source=collection_archive---------8-----------------------#2020-04-24">https://medium.com/analytics-vidhya/python-snownlp-sentiment-analysis-for-the-chinese-language-8d9cafd0447d?source=collection_archive---------8-----------------------#2020-04-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/966f20a0cd0d2f2b23efcedbc716ef5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rEWFgUtE1j81r0AV4gf_-A.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx">拍摄于印刷出版工艺学校</figcaption></figure><p id="e80e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">三亚大学要求我创建一个脚本，从一个特定的URL获取所有的新闻，然后获取关键词、摘要和总体情绪。</p><p id="6c94" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Python是一种漂亮而强大的语言，非常适合这个任务。我将使用Python 3.7.1在卡特琳娜为这个项目。</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="6a87" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">各个击破</h1><p id="29b5" class="pw-post-body-paragraph iu iv hi iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr hb bi translated">提供的网址是<a class="ae lc" href="http://people.com.cn/" rel="noopener ugc nofollow" target="_blank">http://people.com.cn/</a>。说到代码，我总是用“划分&amp;征服”来处理任何问题。因此，我的第一个任务是，抓取并获得来自http://people.com.cn/<a class="ae lc" href="http://people.com.cn/" rel="noopener ugc nofollow" target="_blank">的所有链接。为此，我使用了</a><a class="ae lc" href="https://pypi.org/project/requests/" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj">请求库</strong> </a> <strong class="iw hj"> </strong>和<strong class="iw hj"><em class="ld"/></strong><a class="ae lc" href="https://pypi.org/project/bs4/" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">bs4</strong></a><strong class="iw hj"><em class="ld">。</em> </strong>此处代码抓取所有网址</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="789b" class="ln ka hi lj b fi lo lp l lq lr">def get_all_website_links(url):<br/><br/>    urls = set()<br/>    domain_name = urlparse(url).netloc<br/>    soup = BeautifulSoup(requests.get(url).content, "html.parser")</span><span id="5b7a" class="ln ka hi lj b fi ls lp l lq lr">    for a_tag in soup.findAll("a"):<br/>        href = a_tag.attrs.get("href")<br/>        if href == "" or href is None:<br/>            continue</span><span id="d7a1" class="ln ka hi lj b fi ls lp l lq lr">        href = urljoin(url, href)<br/>        parsed_href = urlparse(href)<br/>        # cleaning URL<br/>        href = parsed_href.scheme + "://" + parsed_href.netloc + parsed_href.path</span><span id="9c22" class="ln ka hi lj b fi ls lp l lq lr">        if not is_valid(href):<br/>            # not a valid URL<br/>            continue<br/>        if href in internal_urls:<br/>            # already in the set<br/>            continue<br/>        if domain_name not in href:<br/>            # external link<br/>            if href not in external_urls:<br/>                external_urls.add(href)<br/>            continue</span><span id="662a" class="ln ka hi lj b fi ls lp l lq lr">        urls.add(href)<br/>        internal_urls.add(href)<br/>    return urls</span></pre><p id="ce02" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我将所有抓取的URL保存到一个. txt文件中。使用循环，我使用<a class="ae lc" href="https://github.com/codelucas/newspaper" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">newspaper 3k</strong></a><strong class="iw hj">从每篇文章中提取内容。</strong></p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="9b28" class="ln ka hi lj b fi lo lp l lq lr">from newspaper import Article</span></pre><p id="92c6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">导入后，您获得URL，然后解析内容。</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="99cc" class="ln ka hi lj b fi lo lp l lq lr">url = line<br/>a = Article(url, language='zh') # Chinese<br/>a.download()<br/>a.parse()</span></pre><p id="a9d9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然后，解析完所有内容后，就到了施展魔法的时候了</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><h1 id="a95a" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">SnowNLP</h1><p id="6084" class="pw-post-body-paragraph iu iv hi iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr hb bi translated">首先，什么是NLP？</p><p id="4d5b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae lc" href="http://Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj">自然语言处理</strong> ( <strong class="iw hj"> NLP </strong> ) </a>是语言学、计算机科学、信息工程和人工智能的一个分支，涉及计算机和人类(自然)语言之间的交互，特别是如何编写计算机程序来处理和分析大量自然语言数据(摘自维基百科)</p><p id="5c91" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Snow是一个使用NLP的Python库，它与诸如中文之类的语言兼容。</p><p id="0c1a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，您必须通过SnowNLP类进行初始化，如下所示:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="f0a7" class="ln ka hi lj b fi lo lp l lq lr">from snownlp import SnowNLP </span><span id="eb39" class="ln ka hi lj b fi ls lp l lq lr">s = SnowNLP(u’我喜欢红包’)</span></pre><p id="9e36" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">建议以<strong class="iw hj"> u </strong>为前缀，表示这是<strong class="iw hj"> Unicode字符串</strong>。</p><p id="3633" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">汉语是一种复杂的语言，因为单词之间没有空格。这使得很难确定一个句子中的单词数，因为一个单词可以是一个或多个汉字的组合。因此，在进行任何自然语言处理时，都需要将整个文本拆分成单词。您可以很容易地使用以下命令来进行令牌化:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="6885" class="ln ka hi lj b fi lo lp l lq lr">from snownlp import SnowNLP </span><span id="0d31" class="ln ka hi lj b fi ls lp l lq lr">s = SnowNLP(u’我喜欢红包’)<br/>s.words</span></pre><p id="950d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这将会回来</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="7640" class="ln ka hi lj b fi lo lp l lq lr">['我', '喜欢', '红包']</span></pre><p id="85a0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果你想得到这些单词的标签(我说的标签是指这个单词是名词、副词、动词、形容词等等)，你可以使用下面的函数</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="4e71" class="ln ka hi lj b fi lo lp l lq lr">from snownlp import SnowNLP <br/>s = SnowNLP(u’我喜欢红包’)<br/>list(s.tags)</span></pre><p id="faaa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这将会回来</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="fb2e" class="ln ka hi lj b fi lo lp l lq lr">[('我', 'r'), ('喜欢', 'v'), ('红包’', 'n')]</span></pre><ul class=""><li id="bc09" class="lt lu hi iw b ix iy jb jc jf lv jj lw jn lx jr ly lz ma mb bi translated"><strong class="iw hj"> r </strong>:指代词。</li><li id="c5de" class="lt lu hi iw b ix mc jb md jf me jj mf jn mg jr ly lz ma mb bi translated"><strong class="iw hj"> v </strong>:指动词。</li><li id="44c2" class="lt lu hi iw b ix mc jb md jf me jj mf jn mg jr ly lz ma mb bi translated"><strong class="iw hj">名词</strong>:指名词。</li><li id="dfd8" class="lt lu hi iw b ix mc jb md jf me jj mf jn mg jr ly lz ma mb bi translated"><strong class="iw hj"> w </strong>:指标点符号。</li></ul><p id="08a9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">而且像这样，还有很多其他有用的功能。</p><h1 id="56fc" class="jz ka hi bd kb kc mh ke kf kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw bi translated">更深入</h1><p id="3218" class="pw-post-body-paragraph iu iv hi iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr hb bi translated">回到任务上来，现在我们需要获取关键字，为此我将使用:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="7a58" class="ln ka hi lj b fi lo lp l lq lr">s.keywords(10)</span></pre><p id="d9a6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">其中数字10表示关键字的数量。然后我们继续总结:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="3b1e" class="ln ka hi lj b fi lo lp l lq lr">s.summary(3)</span></pre><p id="d385" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">其中3将是摘要的最大数量的句子。</p><p id="f2bc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最后，我们将整篇文章分成句子，并使用强大的功能:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="533e" class="ln ka hi lj b fi lo lp l lq lr">s.sentiments</span></pre><p id="3be4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">出于解释的目的，我们总结出了一个更简单的版本</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="f4f7" class="ln ka hi lj b fi lo lp l lq lr">s = SnowNLP(a.text[:1000])</span><span id="8dfd" class="ln ka hi lj b fi ls lp l lq lr"># keywords<br/>print("The keywords are:")<br/>print(*s.keywords(5), sep=", ")<br/><br/># summary<br/>print("The summary is:")<br/>print(*s.summary(2), sep=", ")<br/>print(" ")<br/><br/>#sentiment<br/>sent = s.sentences<br/>for sen in sent:<br/>    s = SnowNLP(sen)<br/>    print(s.sentences)<br/>    print(s.sentiments)</span></pre><p id="e4c6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">变量<strong class="iw hj"> a </strong>是使用报纸上的文章读取URL的全部内容的结果</p><p id="a207" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这段代码将输出句子，然后输出一个0到1之间的数字。值输出范围从0到1，0表示负面情绪，1表示正面情绪。您还可以使用自定义文本数据集来训练您的模型。</p><p id="0927" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有了这个，我就能获取所需的大部分信息。</p><h1 id="f19b" class="jz ka hi bd kb kc mh ke kf kg mi ki kj kk mj km kn ko mk kq kr ks ml ku kv kw bi translated">感谢阅读</h1><p id="e5d8" class="pw-post-body-paragraph iu iv hi iw b ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn lb jp jq jr hb bi translated">这篇不起眼的文章演示了如何使用SnowNLP模块对简体中文进行情感分析。自然语言处理有更多的选择，每种语言都有其优缺点。对于中文(简体中文)来说，SnowNLP是最好的。</p><p id="ff02" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如有任何问题或意见，请访问<a class="ae lc" href="https://www.lastrescarlos.com/" rel="noopener ugc nofollow" target="_blank">https://www.lastrescarlos.com/</a></p></div></div>    
</body>
</html>