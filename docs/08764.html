<html>
<head>
<title>Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/natural-language-processing-bedb2e1c8ceb?source=collection_archive---------7-----------------------#2020-08-12">https://medium.com/analytics-vidhya/natural-language-processing-bedb2e1c8ceb?source=collection_archive---------7-----------------------#2020-08-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/34b7116f9af8edb5019023825ac3dca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*chxLVv_3qk47T3Of.png"/></div></div></figure><p id="3f51" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当智能设备理解我们告诉它们的内容时，我们最初不都感到惊讶吗？事实上，它也以最友好的方式回答了，不是吗？就像苹果的 Siri 和亚马逊的 Alexa 一样，当我们问天气、问路或播放某种类型的音乐时，它们都能理解。从那时起，我就想知道这些计算机是如何获得我们的语言的。这种久违的好奇心重新点燃了我，我想以一个新手的身份写一篇博客。</p><p id="70a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本文中，我将使用一个流行的叫做 NLTK 的 NLP 库。自然语言工具包(NLTK)是最强大的，也可能是最流行的自然语言处理库之一。它不仅拥有最全面的基于 python 的编程库，还支持最多的不同人类语言。</p><p id="53bd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">什么是自然语言处理？</strong></p><p id="86ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">自然语言处理(NLP)是语言学、计算机科学、信息工程和人工智能的一个子领域，涉及计算机和人类语言之间的交互，特别是如何训练计算机处理和分析大量自然语言数据。</p><p id="7a2c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">为什么非结构化数据类型的排序如此重要？</strong></p><p id="f8fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在时钟的每一秒，世界都会产生大量的数据！！，是啊，这真是令人难以置信！！大多数数据属于非结构化数据类型。文本、音频、视频、图像等数据格式是非结构化数据的典型例子。非结构化数据类型不会像关系数据库的传统行和列结构那样具有固定的维度和结构。因此，它更难分析，也不容易搜索。话虽如此，对于商业组织来说，找到应对挑战和抓住机遇的方法以获得洞察力并在竞争激烈的环境中取得成功也很重要。然而，在自然语言处理和机器学习的帮助下，这种情况正在迅速改变。</p><p id="85b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计算机和我们的自然语言混淆了吗？</p><p id="a663" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">人类语言是强有力的交流工具之一。我们使用的词语、语气、句子、手势都在传达信息。在一个短语中有无数种不同的组词方法。单词也可以有多种含义，理解人类语言的意图是一个挑战。语言悖论是一个自相矛盾的短语或句子，例如，“哦，这是我公开的秘密”，“你能自然地行动吗”，尽管这听起来很愚蠢，但我们人类可以理解并在日常生活中使用，但对于机器来说，自然语言的模糊性和不准确性是航行的障碍。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jo"><img src="../Images/089f0d492613511a37dd0408ec9f25b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/0*O60N3vJ7yMHXJ_pm.png"/></div></figure><p id="03e4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">最常用的 NLP 库</strong></p><p id="0c2e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在过去，只有那些在数学、计算机学习和自然语言处理中的语言学方面拥有卓越知识的先驱者才能成为 NLP 项目的一部分。现在，开发人员可以使用现成的库来简化文本的预处理，以便他们可以专注于创建机器学习模型。这些库只需要几行代码就可以实现文本理解、解释和情感分析。最受欢迎的 NLP 库有:</p><p id="00b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Spark NLP，NLTK，PyTorch-Transformers，TextBlob，Spacy，Stanford CoreNLP，Apache OpenNLP，Allen NLP，GenSim，NLP Architecture，sci-kit learn。</p><p id="f655" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jt">问题是我们应该从哪里开始，如何开始？</em></p><p id="83f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你有没有观察过孩子们是如何开始理解和学习语言的？是的，通过挑选每个单词和句子结构，对吧！让计算机理解我们的语言或多或少与它相似。</p><p id="116b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">预处理步骤:</strong></p><ol class=""><li id="0bd4" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated">句子标记化</li><li id="77e9" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">单词标记化</li><li id="bbbc" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">文本词汇化和词干化</li><li id="4037" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">停止言语</li><li id="f685" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">词性标注</li><li id="aa52" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">组块</li><li id="9d91" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">Wordnet</li><li id="0e26" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">词汇袋</li><li id="7832" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">TF-IDF</li></ol></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><ol class=""><li id="41b5" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated"><strong class="is hj">句子标记化(句子切分)<br/> </strong>要让计算机理解自然语言，第一步就是把段落分解成句子。标点符号是把句子分开的一种简单方法。</li></ol><pre class="jp jq jr js fd kp kq kr ks aw kt bi"><span id="2a1a" class="ku kv hi kq b fi kw kx l ky kz">import nltk<br/>nltk.download('punkt')</span><span id="105a" class="ku kv hi kq b fi la kx l ky kz">text = "Home Farm is one of the biggest junior football clubs in Ireland and their senior team, from 1970 up to the late 1990s, played in the League of Ireland. However, the link between Home Farm and the senior team was severed in the late 1990s. The senior side was briefly known as Home Farm Fingal in an effort to identify it with the north Dublin area."</span><span id="8df2" class="ku kv hi kq b fi la kx l ky kz">sentences = nltk.sent_tokenize(text)<br/>print("The number of sentences in the paragrah:",len(sentences))</span><span id="25f7" class="ku kv hi kq b fi la kx l ky kz">for sentence in sentences:<br/>print(sentence)</span><span id="60a4" class="ku kv hi kq b fi la kx l ky kz"><strong class="kq hj">OUTPUT:<br/></strong>The number of sentences in the paragraph: 3  </span><span id="7657" class="ku kv hi kq b fi la kx l ky kz">Home Farm is one of the biggest junior football clubs in Ireland and their senior team, from 1970 up to the late 1990s, played in the League of Ireland. </span><span id="8227" class="ku kv hi kq b fi la kx l ky kz">However, the link between Home Farm and the senior team was severed in the late 1990s. </span><span id="a7ff" class="ku kv hi kq b fi la kx l ky kz">The senior side was briefly known as Home Farm Fingal in an effort to identify it with the north Dublin area.</span></pre><p id="04d5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 2。到目前为止，我们已经分离出句子，下一步是将句子分解成单词，这些单词通常被称为记号。</strong></p><p id="2441" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在自己的生活中创造空间的方式有助于好的方面，同样地，单词之间的空间有助于在一个短语中把单词分开。我们也可以将标点符号视为独立的符号，因为标点符号也有其用途。</p><pre class="jp jq jr js fd kp kq kr ks aw kt bi"><span id="9ebf" class="ku kv hi kq b fi kw kx l ky kz">for sentence in sentences:<br/>words = nltk.word_tokenize(sentence)<br/>print("The number of words in a sentence:", len(words))<br/>print(words)</span><span id="4ea1" class="ku kv hi kq b fi la kx l ky kz"><strong class="kq hj">OUTPUT:<br/></strong>The number of words in a sentence: 32 <br/>['Home', 'Farm', 'is', 'one', 'of', 'the', 'biggest', 'junior', 'football', 'clubs', 'in', 'Ireland', 'and', 'their', 'senior', 'team', ',', 'from', '1970', 'up', 'to', 'the', 'late', '1990s', ',', 'played', 'in', 'the', 'League', 'of', 'Ireland', '.'] </span><span id="1ba4" class="ku kv hi kq b fi la kx l ky kz"> The number of words in a sentence: 18 <br/>['However', ',', 'the', 'link', 'between', 'Home', 'Farm', 'and', 'the', 'senior', 'team', 'was', 'severed', 'in', 'the', 'late', '1990s', '.']  </span><span id="bd33" class="ku kv hi kq b fi la kx l ky kz">The number of words in a sentence: 22 <br/>['The', 'senior', 'side', 'was', 'briefly', 'known', 'as', 'Home', 'Farm', 'Fingal', 'in', 'an', 'effort', 'to', 'identify', 'it', 'with', 'the', 'north', 'Dublin', 'area', '.']</span></pre><p id="0ea7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在程序中使用<code class="du lb lc ld kq b">word_tokenize()</code>或<code class="du lb lc ld kq b">sent_tokenize()</code>功能的先决条件是，我们应该下载<strong class="is hj"> punkt </strong>包。</p><p id="b9e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3。词干和文本词条化</strong></p><p id="3618" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在每个文本文档中，我们通常会遇到不同形式的单词，如 write，writes，writing，意思相同，基本单词相同。但是如何让计算机来分析这样的单词呢？<strong class="is hj"> <br/> </strong>这时就出现了文本词汇化和词干化。</p><p id="8319" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">词干化和文本词汇化是<strong class="is hj">规范化</strong>技术，它们提供了同样的想法，即把一个单词的词尾砍向核心单词。虽然他们都想解决同一个问题，但他们却以完全不同的方式去做。词干化通常是一个粗略的启发式过程，而词汇化是一个基于词汇的形态学基础词。让我们仔细看看！</p><p id="1719" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jt">词干化</em> -单词被简化为它们的词干。词干不必与基于词典的形态(最小单位)词根是同一个词根，它只是等于或小于单词的形式。</p><pre class="jp jq jr js fd kp kq kr ks aw kt bi"><span id="d823" class="ku kv hi kq b fi kw kx l ky kz">from nltk.stem import PorterStemmer</span><span id="0462" class="ku kv hi kq b fi la kx l ky kz">#create an object of class PorterStemmer<br/>porter = PorterStemmer()</span><span id="ba11" class="ku kv hi kq b fi la kx l ky kz">#A list of words to be stemmed<br/>word_list = ['running', ',', 'driving', 'sung', 'between', 'lasted', 'was', 'paticipated', 'before', 'severed', '1990s', '.']</span><span id="a1ae" class="ku kv hi kq b fi la kx l ky kz">print("{0:20}{1:20}".format("Word","Porter Stemmer"))</span><span id="464c" class="ku kv hi kq b fi la kx l ky kz">for word in word_list:<br/>print("{0:20}{1:20}".format(word,porter.stem(word)))</span><span id="7935" class="ku kv hi kq b fi la kx l ky kz">OUTPUT:<br/>Word                Porter Stemmer       <br/>running             run                  <br/>,                   ,                    <br/>driving             drive                <br/>sung                sung                 <br/>between             between              <br/>lasted              last                 <br/>was                 wa                   <br/>paticipated         paticip              <br/>before              befor                <br/>severed             sever                <br/>1990s               1990                 <br/>.                   .</span></pre><p id="54b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">词干提取并不像看起来那么简单:(<br/>我们可能会遇到两个问题，比如一个单词的<strong class="is hj">词干提取不足</strong>和<strong class="is hj">词干提取过度</strong>。</p><p id="9985" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jt">词汇化</em>-当我们认为词干是根据单词出现的方式来删减单词的最佳估计方法时，另一方面，词汇化似乎是一种更有计划地删减单词的方法。他们的字典过程包括解析单词。事实上，一个词的引理就是它的字典或标准形式。</p><pre class="jp jq jr js fd kp kq kr ks aw kt bi"><span id="0a50" class="ku kv hi kq b fi kw kx l ky kz">nltk.download('wordnet')<br/>from nltk.stem import WordNetLemmatizer<br/>wordnet_lemmatizer = WordNetLemmatizer()</span><span id="dd93" class="ku kv hi kq b fi la kx l ky kz">#A list of words to lemmatize</span><span id="c5a2" class="ku kv hi kq b fi la kx l ky kz">word_list = ['running', ',', 'drives', 'sung', 'between', 'lasted', 'was', 'paticipated', 'before', 'severed', '1990s', '.']</span><span id="eff4" class="ku kv hi kq b fi la kx l ky kz">print("{0:20}{1:20}".format("Word","Lemma"))</span><span id="bfc4" class="ku kv hi kq b fi la kx l ky kz">for word in word_list:<br/>      print ("{0:20}{1:20}".format(word,wordnet_lemmatizer.lemmatize(word)))</span><span id="ab3d" class="ku kv hi kq b fi la kx l ky kz"><strong class="kq hj">OUTPUT:<br/></strong>Word                Lemma                <br/>running             running             <br/> ,                   ,                    <br/>drives              drive                <br/>sung                sung                 <br/>between             between              <br/>lasted              lasted               <br/>was                 wa                   <br/>paticipated         paticipated          <br/>before              before               <br/>severed             severed              <br/>1990s               1990s                <br/>.                   .</span></pre><p id="b41d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果需要速度，那么使用词干会更好。但是在需要准确性的情况下，最好使用引理化。</p><p id="2f33" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 4。停止词<br/> ' </strong>中的'，' at '，' on '，' so '..etc 被认为是停用词。停用词在自然语言处理中并不起重要作用，但是停用词的去除在情感分析中必然起重要作用。</p><p id="e039" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">NLTK 附带了 16 种不同语言的停用词，它们包含停用词列表。</p><pre class="jp jq jr js fd kp kq kr ks aw kt bi"><span id="6c10" class="ku kv hi kq b fi kw kx l ky kz">from nltk.corpus import stopwords<br/>from nltk.tokenize import word_tokenize<br/>stop_words = set(stopwords.words('english'))</span><span id="64d4" class="ku kv hi kq b fi la kx l ky kz">print("The stop words in NLTK lib are:", stop_words)</span><span id="2794" class="ku kv hi kq b fi la kx l ky kz">para="""Home Farm is one of the biggest junior football clubs in Ireland and their senior team, from 1970 up to the late 1990s, played in the League of Ireland. However, the link between Home Farm and the senior team was severed in the late 1990s. The senior side was briefly known as Home Farm Fingal in an effort to identify it with the north Dublin area."""</span><span id="c311" class="ku kv hi kq b fi la kx l ky kz">tokenized_para=word_tokenize(para)<br/>modified_token_list=[word for word in tokenized_para if not word in stop_words]<br/>print("After removing the stop words in the sentence:")<br/>print(modified_token_list)</span><span id="1e0d" class="ku kv hi kq b fi la kx l ky kz"><strong class="kq hj">OUTPUT:<br/></strong>The stop words in NLTK lib are: </span><span id="801c" class="ku kv hi kq b fi la kx l ky kz">{'about', 'ma', "shouldn't", 's', 'does', 't', 'our', 'mightn', 'doing', 'while', 'ourselves', 'themselves', 'will', 'some', 'you', "aren't", 'by', "needn't", 'in', 'can', 'he', 'into', 'as', 'being', 'between', 'very', 'after', 'couldn', 'himself', 'herself', 'had', 'its', 've', 'him', 'll', "isn't", 'through', 'should', 'was', 'now', 'them', "you'll", 'again', 'who', 'don', 'been', 'they', 'weren', "you're", 'both', 'd', 'me', 'didn', "won't", "you'd", 'only', 'itself', 'hadn', "should've", 'than', 'how', 'few', 're', 'down', 'these', 'y', "haven't", "mightn't", 'won', "hadn't", 'other', 'above', 'all', "doesn't", 'isn', "that'll", 'not', 'yourselves', 'at', 'mustn', "it's", 'on', 'the', 'for', "didn't", 'what', "mustn't", 'his', 'haven', 'doesn', "you've", 'are', 'out', 'hers', 'with', 'has', 'she', 'most', 'ain', 'those', 'when', 'myself', 'before', 'their', 'during', 'there', 'or', 'until', 'that', 'more', "hasn't", 'o', 'we', 'and', "shan't", 'which', 'because', "don't", 'why', 'shan', 'an', 'my', 'if', 'did', 'having', "couldn't", 'your', 'theirs', 'aren', 'just', 'further', 'here', 'of', "wouldn't", 'be', 'too', 'her', 'no', 'same', 'it', 'is', 'were', 'yourself', 'have', 'off', 'this', 'needn', 'once', "wasn't", 'against', 'wouldn', 'up', 'a', 'i', 'below', "weren't", 'over', 'own', 'then', 'so', 'do', 'from', 'shouldn', 'am', 'under', 'any', 'yours', 'ours', 'hasn', 'such', 'nor', 'wasn', 'to', 'where', 'm', "she's", 'each', 'whom', 'but'} </span><span id="7ed9" class="ku kv hi kq b fi la kx l ky kz">After removing the stopwords in the sentence: <br/>['Home', 'Farm', 'one', 'biggest', 'junior', 'football', 'clubs', 'Ireland', 'senior', 'team', ',', '1970', 'late', '1990s', ',', 'played', 'League', 'Ireland', '.', 'However', ',', 'link', 'Home', 'Farm', 'senior', 'team', 'severed', 'late', '1990s', '.', 'The', 'senior', 'side', 'briefly', 'known', 'Home', 'Farm', 'Fingal', 'effort', 'identify', 'north', 'Dublin', 'area', '.']</span></pre><p id="db4f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 5。回想一下我们早期的英语语法课，我们还记得老师是如何围绕基本词类进行相关指导以进行有效交流的吗？是啊，美好的旧时光！！让我们也教电脑词性吧。:)</strong></p><p id="4886" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">八个词类分别是<em class="jt">名词、动词、代词、形容词、副词、介词、连词、</em>和<em class="jt">感叹词。</em></p><p id="4c7b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">词性标注是一种识别和分配句子中单词的词性的能力。有不同的标记方法，但我们将使用通用的标记样式。</p><pre class="jp jq jr js fd kp kq kr ks aw kt bi"><span id="8655" class="ku kv hi kq b fi kw kx l ky kz">nltk.download('averaged_perceptron_tagger')<br/>nltk.download('universal_tagset')<br/>pos_tag= [nltk.pos_tag(i,tagset="universal") for i in words]<br/>print(pos_tag)</span><span id="4160" class="ku kv hi kq b fi la kx l ky kz">[[('Home', 'NOUN'), ('Farm', 'NOUN'), ('is', 'VERB'), ('one', 'NUM'), ('of', 'ADP'), ('the', 'DET'), ('biggest', 'ADJ'), ('junior', 'NOUN'), ('football', 'NOUN'), ('clubs', 'NOUN'), ('in', 'ADP'), ('Ireland', 'NOUN'), ('and', 'CONJ'), ('their', 'PRON'), ('senior', 'ADJ'), ('team', 'NOUN'), (',', '.'), ('from', 'ADP'), ('1970', 'NUM'), ('up', 'ADP'), ('to', 'PRT'), ('the', 'DET'), ('late', 'ADJ'), ('1990s', 'NUM'), (',', '.'), ('played', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('League', 'NOUN'), ('of', 'ADP'), ('Ireland', 'NOUN'), ('.', '.')]</span></pre><p id="9c59" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">POS 标记的应用之一是分析反馈中的产品质量，通过对客户评论中的形容词进行排序，我们可以评估反馈的情绪。举例来说，您在我们这里购物感觉如何？</p><p id="eb67" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">6。组块<br/> 组块用于通过标记以下词类(POS)为句子添加更多结构。也称为浅层解析。由此产生的单词组被命名为“组块”没有这样的预定义规则来执行分块。</p><p id="4956" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">短语结构约定:</p><ul class=""><li id="abf9" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn le ka kb kc bi translated">s(句子)→ NP VP。</li><li id="fa92" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn le ka kb kc bi translated">NP →{限定词，名词，代词，专名}。</li><li id="6c1f" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn le ka kb kc bi translated">VP → V (NP)(PP)(副词)。</li><li id="2e31" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn le ka kb kc bi translated">PP →代词(NP)。</li><li id="d423" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn le ka kb kc bi translated">AP →形容词(PP)。</li></ul><p id="f051" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我从来没有享受过复杂正则表达式的美好时光，我曾经尽可能地远离它，但后来意识到，在数据科学中掌握正则表达式是多么重要。让我们从理解这个简单的实例开始。</p><p id="99b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们需要从句子中标记名词、动词(过去式)、形容词和并列连词。您可以使用下面的规则</p><p id="f5f7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">组块:{ <nn.> * <vbd.> * <jj.> * <cc>？}</cc></jj.></vbd.></nn.></p><pre class="jp jq jr js fd kp kq kr ks aw kt bi"><span id="0e52" class="ku kv hi kq b fi kw kx l ky kz">import nltk<br/>from nltk.tokenize import word_tokenize</span><span id="af14" class="ku kv hi kq b fi la kx l ky kz">content = "Home Farm is one of the biggest junior football clubs in Ireland and their senior team, from 1970 up to the late 1990s, played in the League of Ireland. However, the link between Home Farm and the senior team was severed in the late 1990s. The senior side was briefly known as Home Farm Fingal in an effort to identify it with the north Dublin area."</span><span id="ec67" class="ku kv hi kq b fi la kx l ky kz">tokenized_text = nltk.word_tokenize(content)<br/>print("After Split:",tokenized_text)<br/>tokens_tag = pos_tag(tokenized_text)<br/>print("After Token:",tokens_tag)</span><span id="ff19" class="ku kv hi kq b fi la kx l ky kz">patterns= """mychunk:{&lt;NN.?&gt;*&lt;VBD.?&gt;*&lt;JJ.?&gt;*&lt;CC&gt;?}"""</span><span id="536c" class="ku kv hi kq b fi la kx l ky kz">chunker = RegexpParser(patterns)<br/>print("After Regex:",chunker)<br/>output = chunker.parse(tokens_tag)<br/>print("After Chunking",output)</span><span id="fde9" class="ku kv hi kq b fi la kx l ky kz"><strong class="kq hj">OUTPUT:<br/></strong>After Regex: chunk.RegexpParser with 1 stages: RegexpChunkParser with 1 rules: &lt;ChunkRule: '&lt;NN.?&gt;*&lt;VBD.?&gt;*&lt;JJ.?&gt;*&lt;CC&gt;?'&gt; </span><span id="3539" class="ku kv hi kq b fi la kx l ky kz">After Chunking <br/>(S   (mychunk Home/NN Farm/NN)   is/VBZ   one/CD  of/IN   the/DT   <br/>(mychunk biggest/JJS)   <br/>(mychunk junior/NN football/NN clubs/NNS)   in/IN  <br/>(mychunk Ireland/NNP and/CC)   their/PRP$   <br/>(mychunk senior/JJ)   <br/>(mychunk team/NN)   ,/,   from/IN   1970/CD   up/IN   to/TO   the/DT   (mychunk late/JJ)   1990s/CD   ,/,   played/VBN   in/IN   the/DT   (mychunk League/NNP)   of/IN   (mychunk Ireland/NNP)   ./.)</span></pre><p id="9665" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">7 .<strong class="is hj">。Wordnet </strong></p><p id="665d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Wordnet 是一个 NLTK 语料库阅读器，一个英语词汇数据库。它可以用来生成同义词或反义词。</p><pre class="jp jq jr js fd kp kq kr ks aw kt bi"><span id="15a1" class="ku kv hi kq b fi kw kx l ky kz">from nltk.corpus import wordnet</span><span id="aecb" class="ku kv hi kq b fi la kx l ky kz">synonyms = []<br/>antonyms = []</span><span id="d8fb" class="ku kv hi kq b fi la kx l ky kz">for syn in wordnet.synsets("active"):<br/>        for lemmas in syn.lemmas():<br/>            synonyms.append(lemmas.name())</span><span id="f640" class="ku kv hi kq b fi la kx l ky kz">for syn in wordnet.synsets("active"):<br/>        for lemmas in syn.lemmas():<br/>            if lemmas.antonyms():<br/>                antonyms.append(lemmas.antonyms()[0].name())</span><span id="2e60" class="ku kv hi kq b fi la kx l ky kz">print("Synonyms are:",synonyms)<br/>print("Antonyms are:",antonyms)</span><span id="0ac9" class="ku kv hi kq b fi la kx l ky kz"><strong class="kq hj">OUTPUT:<br/></strong>Synonyms are: ['active_agent', 'active', 'active_voice', 'active', 'active', 'active', 'active', 'combat-ready', 'fighting', 'active', 'active', 'participating', 'active', 'active', 'active', 'active', 'alive', 'active', 'active', 'active', 'dynamic', 'active', 'active', 'active']</span><span id="8bcf" class="ku kv hi kq b fi la kx l ky kz"> Antonyms are: ['passive_voice', 'inactive', 'passive', 'inactive', 'inactive', 'inactive', 'quiet', 'passive', 'stative', 'extinct', 'dormant', 'inactive']</span></pre><p id="f999" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 8。单词袋<br/> </strong>单词袋模型将原始文本转化为单词，并计算单词在文本中的出现频率。</p><pre class="jp jq jr js fd kp kq kr ks aw kt bi"><span id="7b3f" class="ku kv hi kq b fi kw kx l ky kz">import nltk<br/>import re # to match regular expressions<br/>import numpy as np</span><span id="5b85" class="ku kv hi kq b fi la kx l ky kz">text="Home Farm is one of the biggest junior football clubs in Ireland and their senior team, from 1970 up to the late 1990s, played in the League of Ireland. However, the link between Home Farm and the senior team was severed in the late 1990s. The senior side was briefly known as Home Farm Fingal in an effort to identify it with the north Dublin area."</span><span id="ceda" class="ku kv hi kq b fi la kx l ky kz">sentences = nltk.sent_tokenize(text)<br/>for i in range(len(sentences)):<br/>  sentences[i] = sentences[i].lower()<br/>  sentences[i] = re.sub(r'\W', ' ', sentences[i])<br/>  sentences[i] = re.sub(r'\s+', ' ', sentences[i])</span><span id="c0eb" class="ku kv hi kq b fi la kx l ky kz">bag_of_words = {}<br/>for sentence in sentences:<br/>    words = nltk.word_tokenize(sentence)<br/>    for word in words:<br/>       if word not in bag_of_words.keys():<br/>          bag_of_words[word] = 1<br/>       else:<br/>          bag_of_words[word] += 1<br/>print(bag_of_words)</span><span id="bb05" class="ku kv hi kq b fi la kx l ky kz"><strong class="kq hj">OUTPUT:<br/></strong>{'home': 3, 'farm': 3, 'is': 1, 'one': 1, 'of': 2, 'the': 8, 'biggest': 1, 'junior': 1, 'football': 1, 'clubs': 1, 'in': 4, 'ireland': 2, 'and': 2, 'their': 1, 'senior': 3, 'team': 2, 'from': 1, '1970': 1, 'up': 1, 'to': 2, 'late': 2, '1990s': 2, 'played': 1, 'league': 1, 'however': 1, 'link': 1, 'between': 1, 'was': 2, 'severed': 1, 'side': 1, 'briefly': 1, 'known': 1, 'as': 1, 'fingal': 1, 'an': 1, 'effort': 1, 'identify': 1, 'it': 1, 'with': 1, 'north': 1, 'dublin': 1, 'area': 1}</span></pre><p id="4600" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 9。TF-IDF </strong></p><p id="cc4a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TF-IDF 代表<strong class="is hj">词频—逆文档频率</strong>。</p><p id="12a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">文本数据需要转换为数字格式，其中每个单词都以矩阵形式表示。给定单词的编码是向量，其中对应的元素被设置为 1，所有其他元素为零。因此 TF-IDF 技术也被称为<strong class="is hj">字嵌入</strong>。</p><p id="d155" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TF-IDF 基于两个概念:</p><p id="e11c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> TF(t) =(术语 t 在文档中出现的次数)/(文档中的总术语数)</strong></p><p id="e489" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> IDF(t) = log_e(文档总数/包含术语 t 的文档数)</strong></p><pre class="jp jq jr js fd kp kq kr ks aw kt bi"><span id="dfb6" class="ku kv hi kq b fi kw kx l ky kz">from sklearn.feature_extraction.text import TfidfTransformer<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>import pandas as pd</span><span id="4dd9" class="ku kv hi kq b fi la kx l ky kz">docs=["Home Farm is one of the biggest junior football clubs in Ireland and their senior team, from 1970 up to the late 1990s, played in the League of Ireland",<br/>"However, the link between Home Farm and the senior team was severed in the late 1990s",<br/>" The senior side was briefly known as Home Farm Fingal in an effort to identify it with the north Dublin area"]</span><span id="4aa4" class="ku kv hi kq b fi la kx l ky kz">#instantiate CountVectorizer()<br/>cv=CountVectorizer()</span><span id="0ac4" class="ku kv hi kq b fi la kx l ky kz"># this steps generates word counts for the words in your docs<br/>word_count_vector=cv.fit_transform(docs)</span><span id="99ce" class="ku kv hi kq b fi la kx l ky kz">tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)<br/>tfidf_transformer.fit(word_count_vector)</span><span id="5217" class="ku kv hi kq b fi la kx l ky kz"># print idf values<br/>df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=["idf_weights"])</span><span id="5f75" class="ku kv hi kq b fi la kx l ky kz"># sort ascending<br/>df_idf.sort_values(by=['idf_weights'])</span><span id="ad8d" class="ku kv hi kq b fi la kx l ky kz"># count matrix<br/>count_vector=cv.transform(docs)</span><span id="9298" class="ku kv hi kq b fi la kx l ky kz"># tf-idf scores<br/>tf_idf_vector=tfidf_transformer.transform(count_vector)</span><span id="ba80" class="ku kv hi kq b fi la kx l ky kz">feature_names = cv.get_feature_names()</span><span id="99b6" class="ku kv hi kq b fi la kx l ky kz">#get tfidf vector for the document<br/>first_document_vector=tf_idf_vector[0]</span><span id="d327" class="ku kv hi kq b fi la kx l ky kz">#print the scores<br/>df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=["tfidf"])<br/>df.sort_values(by=["tfidf"],ascending=False)</span><span id="9df9" class="ku kv hi kq b fi la kx l ky kz">tfidf<br/>of      0.374810<br/>ireland 0.374810<br/>the     0.332054<br/>in      0.221369<br/>1970    0.187405<br/>football 0.187405<br/>up      0.187405<br/>as      0.000000<br/>an      0.000000</span><span id="d6cf" class="ku kv hi kq b fi la kx l ky kz">and so on..</span></pre><p id="0bf8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些分数告诉我们什么？单词在文档中越常见，得分越低，越独特的单词得分越高。</p><p id="4a92" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">到目前为止，我们学习了清理和预处理文本的步骤。在这一切之后，我们可以用排序后的数据做什么呢？我们可以用这些数据进行情感分析，聊天机器人，市场情报。也许建立一个基于用户购买或商品评论或客户分类的推荐系统。</p><p id="e075" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计算机对人类语言的准确性仍然不如对数字的准确性。随着每天产生大量的文本数据，自然语言处理对于理解数据变得越来越重要，并被用于许多其他应用中。因此，有无尽的方法来探索自然语言处理。</p></div></div>    
</body>
</html>