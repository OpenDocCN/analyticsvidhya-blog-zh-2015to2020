<html>
<head>
<title>Paper summary: End-to-End Speech-Driven Facial Animation with Temporal GANS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文摘要:基于时间GANS的端到端语音驱动人脸动画</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/paper-summary-end-to-end-speech-driven-facial-animation-with-temporal-gans-7d5f3a623629?source=collection_archive---------14-----------------------#2019-12-23">https://medium.com/analytics-vidhya/paper-summary-end-to-end-speech-driven-facial-animation-with-temporal-gans-7d5f3a623629?source=collection_archive---------14-----------------------#2019-12-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a4f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总结论文:<a class="ae jd" href="https://arxiv.org/pdf/1805.09313.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1805.09313.pdf</a></p><ol class=""><li id="0af3" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated"><strong class="ih hj">是关于什么的:</strong></li></ol><p id="c5f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">提出了一个从语音生成人脸动画的系统。</p><blockquote class="jn jo jp"><p id="7ea3" class="if ig jq ih b ii ij ik il im in io ip jr ir is it js iv iw ix jt iz ja jb jc hb bi translated">语音驱动的面部动画是使用语音信号自动合成说话角色的过程。</p></blockquote><p id="60f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">系统的输入是语音信号，它自动合成一个以静止图像形式呈现的说话字符。</p><p id="52b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">点击此处查看生成的动画:</p><figure class="ju jv jw jx fd jy"><div class="bz dy l di"><div class="jz ka l"/></div></figure><blockquote class="jn jo jp"><p id="05ec" class="if ig jq ih b ii ij ik il im in io ip jr ir is it js iv iw ix jt iz ja jb jc hb bi translated">我们提出了一个系统，用于生成一个说话的头部视频，使用一个人的静态图像和一个包含语音的音频剪辑，…</p></blockquote><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es kb"><img src="../Images/b1e8439159af880ef84cad073c236831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1psIg99AjmhtXxCZwMuc1w.png"/></div></div></figure><p id="f6cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。索赔:</strong></p><ol class=""><li id="b2ec" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">不依赖任何手工制作的功能。</li><li id="62a0" class="je jf hi ih b ii ki im kj iq kk iu kl iy km jc jj jk jl jm bi translated">这是第一种能够直接从原始音频生成独立于主题的真实视频的方法。</li></ol><blockquote class="jn jo jp"><p id="efc5" class="if ig jq ih b ii ij ik il im in io ip jr ir is it js iv iw ix jt iz ja jb jc hb bi translated">我们的方法可以生成具有(a)与音频同步的嘴唇运动和(b)诸如眨眼和眉毛运动1的自然面部表情的视频。</p></blockquote><p id="d88c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。主旨:</strong></p><blockquote class="jn jo jp"><p id="5e27" class="if ig jq ih b ii ij ik il im in io ip jr ir is it js iv iw ix jt iz ja jb jc hb bi translated">我们通过使用具有两个鉴别器的时间GAN来实现这一点，这两个鉴别器能够捕捉视频的不同方面。</p></blockquote><p id="2c60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。善良的度量:</strong></p><p id="d466" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于以下因素评估生成的视频</p><ol class=""><li id="492f" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">他们的敏锐</li><li id="a72d" class="je jf hi ih b ii ki im kj iq kk iu kl iy km jc jj jk jl jm bi translated">重建质量，以及</li><li id="c4a0" class="je jf hi ih b ii ki im kj iq kk iu kl iy km jc jj jk jl jm bi translated">唇读准确度。</li></ol><blockquote class="jn jo jp"><p id="4890" class="if ig jq ih b ii ij ik il im in io ip jr ir is it js iv iw ix jt iz ja jb jc hb bi translated">最后，进行了用户研究，证实了时间GAN比基于静态GAN的方法产生更自然的序列。</p></blockquote><p id="76cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。相关工作:GAN基视频合成</strong></p><p id="eb59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GAN [25][20]在视频中的直接应用用3D卷积代替了2D卷积。这使得网络能够对时间依赖性进行建模，但是也需要固定长度的时间步长。</p><p id="e5f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MoCoGAN [24]使用基于RNN的发生器，为运动和内容提供独立的潜在空间。它使用2D和3D CNN分别判断帧和序列。在3D CNN鉴别器中使用滑动窗口方法来处理变长序列。</p><p id="bb56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 6。提议的架构:</strong></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es kn"><img src="../Images/20ac18c84a188780d14f959934e85fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bMxoh7PncqPxAhr-cUvzFw.png"/></div></div></figure><p id="39f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">提议的架构有三个组成部分:一个发电机和<strong class="ih hj">两个</strong>鉴别器。</p><ul class=""><li id="0825" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc ko jk jl jm bi translated"><strong class="ih hj">发生器:</strong>发生器网络的输入是单个图像和音频信号。音频信号被分成每个长度为0.16秒的重叠帧。</li><li id="d4e8" class="je jf hi ih b ii ki im kj iq kk iu kl iy km jc ko jk jl jm bi translated"><strong class="ih hj">帧鉴别器:</strong>帧鉴别器评估取自合成/真实序列的单个帧。这驱动发生器产生详细的帧。</li><li id="2ec2" class="je jf hi ih b ii ki im kj iq kk iu kl iy km jc ko jk jl jm bi translated"><strong class="ih hj">序列鉴别器:</strong>序列鉴别器评估序列-音频对，以确定它们是真实的还是合成的。这使得音频和视频同步，并鼓励面部表情的产生(例如眨眼)</li></ul><p id="8113" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7 .<strong class="ih hj">。组件的详细描述:</strong></p><p id="2b49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7.1发电机:</strong></p><p id="6a7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">生成器必须生成给定人员的动画(通过单张照片提供),就好像该人员在讲所提供的语音记录一样。因此，直觉上，它必须用自然的面部姿态将面部动画与语音信号对应起来。</p><p id="5f5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以把它想象成一个两部分的过程，第一部分是产生面部动画来匹配语音信号。第二部分是确保动画发生在提供的主题上。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es kp"><img src="../Images/c04d3bf0b952667bc7d167ddbc3f2112.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c9QenqNZXcqnjupyrB3VAg.png"/></div></div></figure><p id="571a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为此，发电机有三个子模块:</p><p id="868f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7.1.1身份编码器:</strong></p><p id="8fa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模块负责特征化所提供的主题，以便动画应用于该主题。该编码器由几层CNN组成，并产生主题的50维编码，称为z_id。</p><p id="b77b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7.1.2上下文编码器:</strong></p><p id="e585" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个时间步长，该模块对语音信号进行编码。使用(1D卷积+批量范数+ relu)的块对其进行编码；然后该编码信息被馈送到2层gru。GRU层的输出是该时间步长的语音信号的编码。这种编码被命名为z_c。</p><p id="5615" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7.1.3帧解码器:</strong></p><p id="1aa7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，帧解码器必须为语音的每个时间步长生成带有合适动画的主题图像帧。当然，它的输入将是z_id和z_c(编码身份和编码语音帧)。作者还使用10D高斯噪声z_n作为帧解码器的输入。这种噪声是由1层GRU网络产生的，其输入也是高斯噪声。在我看来，添加这种噪声的目的是为了在解码相同的主题和语音帧时在动画中有一些变化。</p><p id="09d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在身份编码器和帧解码器之间使用U-Net架构来保持主体身份。</p><p id="abdc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，在下图中，z_id是由identity encoder生成的，而z_c和z_n与它连接在一起。还要注意标识编码器和帧解码器之间的跳跃连接(绿色虚线)。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es kq"><img src="../Images/4158cad979102f445a6cdd978e8e15d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9vbt6y_z7i0StwWcCo1lw.png"/></div></div></figure><p id="6edd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7.2鉴别器:</strong></p><p id="b3cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们之前提到的，这个架构使用了两个鉴别器。帧鉴别器负责在所有时间步长内重建目标人脸；而序列鉴别器负责创建与语音/音频同步的自然且连贯的视频。</p><p id="b637" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们详细看看这两个模块的架构。</p><p id="dfd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7.2.1帧鉴别器:</strong></p><blockquote class="jn jo jp"><p id="16ef" class="if ig jq ih b ii ij ik il im in io ip jr ir is it js iv iw ix jt iz ja jb jc hb bi translated">帧鉴别器是一个6层CNN，它决定一个帧是否真实。使用这种鉴别器的对抗性训练确保了生成的帧是真实的。原始静止帧被用作该网络中的一个条件，在通道上与目标帧连接以形成如图3所示的输入。这将在框架上加强此人的身份。</p></blockquote><p id="a9b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7.2.2序列鉴别器:</strong></p><blockquote class="jn jo jp"><p id="796e" class="if ig jq ih b ii ij ik il im in io ip jr ir is it js iv iw ix jt iz ja jb jc hb bi translated">图3所示的序列鉴别器区分真实视频和合成视频。鉴别器在每个时间步长接收一个帧，该帧使用CNN编码，然后馈入2层GRU。在序列的末尾使用一个小的(2层)分类器来确定序列是否真实。音频作为条件输入添加到网络，允许该鉴别器对语音-视频对进行分类。</p></blockquote><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kr"><img src="../Images/686bbebdbe61504094d8079d1b9ea589.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*vJFKbAdRxwR0Uq2pfiNeWQ.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图3</figcaption></figure><p id="4466" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 8。培训</strong></p><p id="c049" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">帧鉴别器(D_img)在使用采样函数S(x)从视频x中均匀采样的帧上被训练。</p><p id="ed41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">序列鉴别器(D_seq)根据整个序列x和音频a对真实和虚假序列进行分类。</p><p id="d779" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们将交叉熵损失用于鉴别器。GAN的总损耗是每个鉴频器损耗的总和。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es kw"><img src="../Images/9d37f9044ba94538c201731ecc60d9db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fPyHzqVnSUnvod78ImaMuQ.png"/></div></div></figure><p id="b1f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">额外的像素损失(L_1)也被用于改善嘴部运动相对于音频的同步。仅针对图像的下半部分(嘴出现的地方)计算像素级重建损失。这种重建损失不适用于图像的上半部分，因为它阻碍了改进，因此阻碍了面部姿态(眉毛等)。).</p><p id="b7bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于地面真实帧F和尺寸为W×H的生成帧G，像素级重建损失为:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es kx"><img src="../Images/198766b834d3ac23f0f9ba586196f33c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*romGGXITpQAd8eUzuammsQ.png"/></div></div></figure><blockquote class="jn jo jp"><p id="5b3f" class="if ig jq ih b ii ij ik il im in io ip jr ir is it js iv iw ix jt iz ja jb jc hb bi translated">最终目标是获得满足等式3的最佳发电机G*。</p></blockquote><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kc kd di ke bf kf"><div class="er es ky"><img src="../Images/dbdbc987642c67042098eb713767cb81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9K17PvJLU22FCvWoFObUaA.png"/></div></div></figure><blockquote class="jn jo jp"><p id="a95b" class="if ig jq ih b ii ij ik il im in io ip jr ir is it js iv iw ix jt iz ja jb jc hb bi translated">λ超参数控制每个损耗因子的贡献，并根据验证集上的调整程序设置为400。</p><p id="a684" class="if ig jq ih b ii ij ik il im in io ip jr ir is it js iv iw ix jt iz ja jb jc hb bi translated">训练该模型，直到在10个时期的验证集上没有观察到重建度量的改善。</p></blockquote><p id="901c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 9。参考文献:</strong></p><p id="5034" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[20] M .斋藤、e .松本和s .斋藤。具有奇异值裁剪的时态生成对抗网。在IEEE计算机视觉国际会议(ICCV)，第2830-2839页，2017年。</p><p id="92af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[24] S. Tulyakov、M. Liu、X. Yang和J. Kautz。MoCoGAN:分解视频生成的运动和内容。arXiv预印本arXiv:1707.04993，2017。</p><p id="e426" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">25 c . von drick、H. Pirsiavash和A. Torralba。用场景动态生成视频。《神经信息处理系统(NIPS)进展》，613–621页，2016年。</p></div></div>    
</body>
</html>