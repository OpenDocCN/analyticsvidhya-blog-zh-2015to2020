<html>
<head>
<title>Deep Reinforcement Learning using OpenAIGym</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用OpenAIGym的深度强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-reinforcement-learning-using-openaigym-10d1a66c3e2?source=collection_archive---------31-----------------------#2020-05-12">https://medium.com/analytics-vidhya/deep-reinforcement-learning-using-openaigym-10d1a66c3e2?source=collection_archive---------31-----------------------#2020-05-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e9ed12c01c46cb6cb9ef387408f3244a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3r3ZE_O_F5BbYtJE"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">玛丽亚·卡斯泰利在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h2 id="aa8a" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">简介:</h2><p id="bf53" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">深度强化学习是当今深度学习的另一个热门话题，在机器人、自动驾驶汽车、游戏等领域都有有趣的用例。它涉及用强化学习架构封装深度神经网络，使软件定义的代理能够在虚拟环境中学习可能的最佳动作，以达到他们的目标。在本文中，我们将用简单的语言来介绍关于强化学习的一般信息，并通过由<a class="ae iu" href="https://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank">openaigam</a>提供的用例使用深度学习机制来实现它们。那么，让我们开始吧…</p><h2 id="9e97" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">强化学习:</h2><p id="7bfa" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Reinforcement_learning" rel="noopener ugc nofollow" target="_blank">强化学习</a>与其他学习问题不同，它与状态-行动对一起工作，这意味着在环境中的每个状态下采取行动，目标是获得最大回报。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/6759b5e5e2dc4bc701e729570cecd51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*bIOMgt0MHe82m9ytuWQ38g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图1:学习问题的分类</figcaption></figure><p id="b462" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">可以使用代理、环境、状态、行动和奖励的概念来进一步理解，所有这些我们将在下面看到。</p><p id="c7f3" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">代理人:采取行动的人。<strong class="jv hj"> </strong>例如:对于自动驾驶汽车，汽车是代理</p><p id="d890" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated"><strong class="jv hj">环境:</strong>这是一个代理存在和操作的世界</p><p id="be4f" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated"><strong class="jv hj">动作:</strong>是代理可以做出的所有可能动作的集合。一个<strong class="jv hj">动作</strong>几乎是不言自明的，但是应该注意的是，代理通常从一个离散的、可能的动作列表中进行选择。</p><p id="34a4" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated"><strong class="jv hj">状态:</strong>当一个指定的动作被触发时，环境发回的一个观察。因此它被代理相应地感知。</p><p id="e737" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">奖励:它是一种反馈措施，根据状态期间采取的行动来表示其成功或失败。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/64ab27afa7d780aaa2d8ad5a73970ed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*PkamzJmEyLF3whouR623zQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图2:强化学习的关键概念</figcaption></figure><p id="9758" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">上图显示了它是如何循环工作的，当代理采取行动时，它会从环境中获得奖励和新的状态。<strong class="jv hj">举个例子:</strong>在一辆<strong class="jv hj">自动驾驶汽车中</strong>当智能体左转时，如果没有左转可用，那么环境向我们提供了一个观察(状态)显示它以负奖励崩溃，反之亦然。</p><p id="f8c8" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">在训练中，我们运行它的多个插曲，并收集所有的奖励，直到它完成，在我们的ex中，直到汽车被撞毁或停在目的地。我们对收到的奖励进行汇总，并通过在每个时间步采取最佳行动来尝试最大化奖励。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/c349d54c1d961c7f4bb1a20ac3055741.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*FJxY49ggbG8ZgD3XvCdnRA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图3:总报酬</figcaption></figure><p id="9da9" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated"><strong class="jv hj">贴现因子:</strong><strong class="jv hj">贴现因子</strong>乘以代理人发现的未来奖励，以抑制这些奖励对代理人行动选择的影响。换句话说，我们对最新的奖励感兴趣。这由希腊字母γ<em class="la">γ表示。</em></p><p id="780c" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">现在我们的奖励函数变成了，</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/bce5a9b2a968a46f7ce04f7853e6185b.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*8aK6qsalCM0ZsgKYmHTqgg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图4:最终奖励函数</figcaption></figure><p id="114b" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">我们已经看到了所有必要的术语，一般程序，现在让我们把重点放在算法上。</p><p id="f5fc" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated"><strong class="jv hj"> Q-Function: </strong>指在当前状态下采取行动的代理人的长期收益。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/79d35a77138bb6ebdf81d1b72d138600.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*_dasX7YqtErNY35JPTz3Zw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图5: Q函数</figcaption></figure><p id="51ee" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">它捕捉了处于状态s的代理通过采取行动a可以获得的预期总未来报酬。因此，Q值较高意味着我们正在采取最佳行动，并且我们倾向于在这个过程中最大化Q值，这被称为<strong class="jv hj">策略</strong>。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/f701e400a1e95c43191090c956076529.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*Kqr9dxjXcvLVvpfhj9Cd_w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图6:政策</figcaption></figure><p id="6c18" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">这是一种策略，它通过取Q值的最大值来帮助我们找到最佳行动。采用这种策略，我们在每个状态下以期望的Q值采取正确的行动。该方法也被称为<strong class="jv hj">值学习方法。但是这种方法也有一些缺点，</strong></p><p id="f7be" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated"><strong class="jv hj">缺点:</strong></p><ol class=""><li id="0a73" class="le lf hi jv b jw kt ka ku jg lg jk lh jo li kn lj lk ll lm bi translated">只能为动作空间离散且较小的场景建模</li><li id="496c" class="le lf hi jv b jw ln ka lo jg lp jk lq jo lr kn lj lk ll lm bi translated">不能用于连续动作空间</li><li id="2160" class="le lf hi jv b jw ln ka lo jg lp jk lq jo lr kn lj lk ll lm bi translated">策略是确定性的，因为它是通过最大化Q函数来实现的，并且不能处理随机策略。</li></ol><p id="d00f" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">为了克服这些缺点，我们有了这个<strong class="jv hj">策略学习方法</strong></p><p id="1a11" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated"><strong class="jv hj">政策学习方法:</strong></p><p id="abe2" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">这种方法针对当前状态下可以采取的每个动作建立概率分布。我们可以从分布中抽取一个对应于某个动作的值。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/fd62d07ab97be5fd44ad1fb90e58b921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*2HQta86wC6PeMYkDSi4SfA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图7:政策梯度</figcaption></figure><p id="8c1d" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">如上图所示，我们可以训练一个神经网络将动作空间输出到一个概率分布中。通过这样做，我们不仅可以将该模型用于分类动作，还可以用于连续动作空间。例如，在自动驾驶汽车中，不仅仅是左转或右转，我们还可以根据我们在前视图中看到的物体来模拟我们应该移动的速度。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/77dcab86d73b6948395e4e3de1c0fafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*P7K2xpZ7O74JyEItYK_sig.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图8:连续动作空间</figcaption></figure><p id="c510" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">因此，在这种情况下，我们已经参数化了高斯的均值和方差，从中可以对动作进行采样。现在，它不再是绝对的，就像从左到右，而是一个连续的框架，它应该向右或向左移动多少。</p><p id="2e8f" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated"><strong class="jv hj">训练方法:</strong></p><ol class=""><li id="9f29" class="le lf hi jv b jw kt ka ku jg lg jk lh jo li kn lj lk ll lm bi translated">设置代理</li><li id="33fe" class="le lf hi jv b jw ln ka lo jg lp jk lq jo lr kn lj lk ll lm bi translated">在模拟中将策略作为情节运行多次，直到终止</li><li id="500e" class="le lf hi jv b jw ln ka lo jg lp jk lq jo lr kn lj lk ll lm bi translated">记录情节中的所有状态、提供的动作和奖励</li><li id="4bc3" class="le lf hi jv b jw ln ka lo jg lp jk lq jo lr kn lj lk ll lm bi translated">降低获得低回报的行动的概率</li><li id="998f" class="le lf hi jv b jw ln ka lo jg lp jk lq jo lr kn lj lk ll lm bi translated">增加导致高回报的行动的可能性。</li></ol><p id="d14a" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">我们将看到一个由OpenAIGym提供的游戏模拟的示例实现，名为<a class="ae iu" href="https://gym.openai.com/envs/SpaceInvaders-ram-v0/" rel="noopener ugc nofollow" target="_blank"> Space Invaders。</a></p><p id="e81c" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">首先，我们必须调用OpenAIGym提供的模拟环境。在我们的例子中是太空入侵者游戏。</p><pre class="kp kq kr ks fd lu lv lw lx aw ly bi"><span id="393a" class="iv iw hi lv b fi lz ma l mb mc">env = gym.make("SpaceInvaders-v0")<br/>env.seed(1)</span></pre><p id="cdbd" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">创建一个ConvNet模型，以捕捉游戏模拟环境在每个状态期间的变化，这些变化可以与收到的行动和奖励相关联。</p><pre class="kp kq kr ks fd lu lv lw lx aw ly bi"><span id="1ee4" class="iv iw hi lv b fi lz ma l mb mc">Conv2D=functools.partial(tf.keras.layers.Conv2D,padding="same",activation="relu")<br/>Flatten=tf.keras.layers.Flatten<br/>Dense=tf.keras.layers.Dense</span><span id="f7cc" class="iv iw hi lv b fi md ma l mb mc">def create_spaceInvaders_model():<br/>model = tf.keras.models.Sequential([</span><span id="c8fe" class="iv iw hi lv b fi md ma l mb mc">Conv2D(filters=16,kernel_size=7,strides=4),</span><span id="359e" class="iv iw hi lv b fi md ma l mb mc">Conv2D(filters=32,kernel_size=5,strides=2),</span><span id="2dda" class="iv iw hi lv b fi md ma l mb mc">Conv2D(filters=48,kernel_size=3,strides=2),<br/>Flatten(),<br/>Dense(units=64,activation="relu"),<br/>Dense(n_actions,activation=None)</span><span id="a179" class="iv iw hi lv b fi md ma l mb mc">])<br/>return model<br/>SpaceInvadersModel=create_spaceInvaders_model()</span></pre><p id="37fd" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">一旦我们准备好模型，我们必须运行剧集，记录状态、行动和奖励，并每次都用这些数据更新模型。因此，在所有连续的情节中，模型根据收到的奖励更新在该州采取行动的概率。通过这种方式，模型得到训练，并最终开始做得更好。</p><p id="9a3f" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">下面的代码显示了根据提供的观察在每一步采取的操作。这是一个随机选择的概率测量。</p><pre class="kp kq kr ks fd lu lv lw lx aw ly bi"><span id="651e" class="iv iw hi lv b fi lz ma l mb mc">def choose_action(model, observation):</span><span id="f91f" class="iv iw hi lv b fi md ma l mb mc">observation = np.expand_dims(observation, axis=0)</span><span id="dbf6" class="iv iw hi lv b fi md ma l mb mc">logits = model.predict(observation)</span><span id="790d" class="iv iw hi lv b fi md ma l mb mc">prob_weights = tf.nn.softmax(logits).numpy()</span><span id="4252" class="iv iw hi lv b fi md ma l mb mc">action = np.random.choice(n_actions, size=1, p=prob_weights.flatten())[0] </span><span id="c6f9" class="iv iw hi lv b fi md ma l mb mc">return action</span></pre><p id="6507" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">为了表示的目的，我们已经用很少的迭代训练了模型，我们还可以根据我们的GPU设置训练多达10，000次或更多的迭代，以获得更好的结果。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es me"><img src="../Images/0ad6763a3618a4d7cb592c1601265737.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*_KoqIK-ec1kksHov_c5peA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图9:运行统计</figcaption></figure><p id="4bbd" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated"><strong class="jv hj">输出:</strong></p><p id="12d4" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">我们可以看到代理现在正在环境中运行，我们可以调整我们的训练参数和迭代次数以获得更多分数。获得完整的代码<a class="ae iu" href="https://github.com/SeshadriSenthamaraikannan/ReinforcementLearning/blob/master/RL_OpenAIGym.ipynb" rel="noopener ugc nofollow" target="_blank">在这里。</a></p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/15d357ee72bdf593e7b7a402ae49209b.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/1*O3zfvzIn_bfQOxiX87gPsg.gif"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图10:输出</figcaption></figure><p id="0773" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated"><strong class="jv hj">参考:</strong></p><p id="7b9d" class="pw-post-body-paragraph jt ju hi jv b jw kt jy jz ka ku kc kd jg kv kf kg jk kw ki kj jo kx kl km kn hb bi translated">麻省理工学院深度强化学习文档</p></div></div>    
</body>
</html>