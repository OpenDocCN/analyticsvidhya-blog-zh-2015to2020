<html>
<head>
<title>YOLO: Engineering Challenges for Training and Deploying YOLO on Edge Device</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLO:在边缘设备上培训和部署YOLO的工程挑战</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/yolo-engineering-challenges-for-training-and-deploying-yolo-on-edge-device-8d753748d243?source=collection_archive---------10-----------------------#2020-06-17">https://medium.com/analytics-vidhya/yolo-engineering-challenges-for-training-and-deploying-yolo-on-edge-device-8d753748d243?source=collection_archive---------10-----------------------#2020-06-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="41c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">米提什·帕特尔，大卫·莎玛，尤利乌斯·贾赫迪</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/8e1fa824da2b78f982cc8acd24f39c6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ro3fspog1fXfs5SGJwGaTw.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">YOLO建筑</figcaption></figure><p id="5ffb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di"> Y </span> OLO(你只看一次)是一种新的对象检测方法，它使用单卷积神经网络，在一次评估中直接从完整图像中同时预测多个边界框和这些框的类别概率。该架构是高速的，可以每秒45帧或每秒155帧进行预测，对于较小的架构来说(<a class="ae kd" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">在最初的YOLO论文</a>中声称)。</p><p id="1657" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">YOLO最早由Redmon等人提出，在2016年CVPR上发表，并获得了人民选择奖。此后，人们对YOLO进行了大量改进(在撰写本文时，YOLOv4是最新版本)。YOLO的代码库，<a class="ae kd" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank"> darknet </a>是开源的，用C语言写的(<em class="jd">惊讶，</em>顺便说一下，《YOLO》的作者，Joseph Redmond在UW教一门很棒的<a class="ae kd" href="https://pjreddie.com/courses/computer-vision/" rel="noopener ugc nofollow" target="_blank">计算机视觉课程</a>)。从那以后，深度学习爱好者和研究人员开发了各种版本的YOLO，支持不同的深度学习(DL)平台，如Tensorflow、Pytorch和Caffe。</p><p id="a69f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">许多有用的帖子/论文解释了YOLO，如<a class="ae kd" href="https://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b" rel="noopener" target="_blank">yolov 3中的新功能</a>、<a class="ae kd" href="http://christopher5106.github.io/object/detectors/2017/08/10/bounding-box-object-detectors-understanding-yolo.html" rel="noopener ugc nofollow" target="_blank">了解YOLO </a>、<a class="ae kd" rel="noopener" href="/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088">YOLO的实时物体检测</a>等等。</p><p id="4e96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我们将重点关注工程挑战，并分享我们在数据集上训练YOLOv3以及在英特尔或Coral的边缘设备上传输模型的经验。虽然YOLO上不乏帖子，但我们希望这篇文章将有助于减少像我们这样试图接触YOLO的爱好者的部署时间，并作为解决我们遇到的一些<strong class="ih hj"> <em class="jd">问题</em> </strong>时刻的指南。</p><h1 id="7a52" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">图书馆选择</h1><p id="f922" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">有各种开源库随时可用，可用于在数据集上执行迁移学习。Joseph Redmond开发的第一个版本是用C写的，在GitHub上。YOLO还被移植到其他流行的DL库中，如Tensorflow、Pytorch和CAFFE。这里列出了一些流行的Github实现，但并不详尽:</p><ul class=""><li id="c3d8" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">由Joseph Redmond用C实现的Darknet</li><li id="6f1e" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">YOLO在<a class="ae kd" href="https://github.com/heartkilla/yolo-v3" rel="noopener ugc nofollow" target="_blank">实现了张量流</a></li><li id="c8cf" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">另一个版本的<a class="ae kd" href="https://github.com/shahkaran76/yolo_v3-tensorflow-ipynb" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a></li><li id="8e24" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">YOLO在<a class="ae kd" href="https://github.com/ayooshkathuria/pytorch-yolo-v3" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>实现</li><li id="90bf" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">YOLO在<a class="ae kd" href="https://github.com/hojel/caffe-yolo-model" rel="noopener ugc nofollow" target="_blank">韩妃雅</a>实施</li></ul><p id="ba75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的工作中，我们使用了由Alexey Bochkovskiy 维护的darknet <a class="ae kd" href="https://github.com/AlexeyAB/darknet" rel="noopener ugc nofollow" target="_blank">分叉代码库。该存储库提供:</a></p><ul class=""><li id="a68e" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">更少的兼容性问题，因为它是由作者主动维护的。</li><li id="d6c9" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">许多其他人都在关注和使用这个存储库，这帮助我们找到了我们面临的大多数问题的答案，包括设置环境、培训期间的错误以及对一些参数的理解。</li><li id="a14b" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">作者提供了一个有用的工具，可以为您的数据集(我们的例子)生成带标签的数据来训练模型。</li></ul><h1 id="d653" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">标签数据</h1><p id="edc9" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">训练YOLO或任何其他DL模型的第一个要求是有标记的数据。对于我们的应用程序，我们使用YOLO来检测外围视觉中的对象，因此对象的视点是倾斜的。此外，在80类预训练模型中，我们的应用程序感兴趣的一些对象的类是不可用的。</p><p id="a1c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了满足我们的需求，我们生成了一个带标签的数据集来训练YOLOv3模型(用不同的类生成标签是一个漫长而无聊的过程)。对于训练数据，我们使用了由阿列克谢·博奇科夫斯基开发的名为<a class="ae kd" href="https://github.com/AlexeyAB/Yolo_mark" rel="noopener ugc nofollow" target="_blank">YOLO _马克</a>的标记工具。该存储库提供了一个优秀的UI接口，可以在不同类的对象上绘制边界框，并生成符合使用darknet存储库训练YOLOv3所需格式的带标签数据集。</p><h1 id="ae5b" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">设置Conda环境的挑战</h1><p id="4fcc" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">我们利用conda为darknet建立了一个环境(也可以使用docker)。我们继续康达，因为这是我们所知道的；我们可以很容易地在Ubuntu 18.04 LTS上维护所需的依赖关系。需要注意的一点是OpenCV的版本。我们面临一些OpenCV版本的问题，因此我们建议至少使用OpenCV版本3.4.3。</p><p id="ab87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在设置GPU支持的环境时遇到的另一个问题是<code class="du lv lw lx ly b">gcc</code>和<code class="du lv lw lx ly b">g++</code>版本。NVCC要求<code class="du lv lw lx ly b">gcc</code>版本5，而LTS Ubuntu 18.04的默认版本是版本7。我们用<code class="du lv lw lx ly b">apt-get install</code>安装了<code class="du lv lw lx ly b">gcc</code>和<code class="du lv lw lx ly b">g++</code>版本5。一旦安装完毕，我们必须在CUDA中指向NVCC来使用<code class="du lv lw lx ly b">gcc</code>和<code class="du lv lw lx ly b">g++</code>版本5，这是通过在<code class="du lv lw lx ly b">/usr/local/cuda/bin/</code>文件夹中创建一个符号链接来完成的。</p><h1 id="83fb" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">培训:问题和预防措施</h1><p id="e4df" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">在设置好我们的环境并生成必要的标记数据之后，很明显下一步是训练模型。我们使用了Alexey Bochkovskiy提供的<a class="ae kd" href="https://github.com/AlexeyAB/darknet#how-to-train-to-detect-your-custom-objects" rel="noopener ugc nofollow" target="_blank"> <em class="jd"> README.md </em> </a>文件中的<em class="jd"> How to Train </em>部分，并在yolov3.cfg文件中对类的数量、批量大小、细分、最大批量、步骤和过滤器大小(这是必不可少的)进行了必要的更改。有了这些变化，我们就可以用我们的标注数据集来训练YOLO模型了。</p><h2 id="7bb1" class="lz kf hi bd kg ma mb mc kk md me mf ko iq mg mh ks iu mi mj kw iy mk ml la mm bi translated">分段故障错误</h2><p id="699b" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">在第一次训练模型时，我们遇到了分割错误。首先想到的是我们正在耗尽GPU内存，但事实并非如此。Darknet在利用GPU资源方面非常明智，不像其他一些框架。在深入研究GitHub的问题后，我们发现其他人也面临类似的问题。Github问题标签上发布的建议之一是确保边界框标签尺寸不小于0.01。我们发现许多标签属于这一类别，因此通过编程从训练数据中删除。使用上述方法解决了分割问题。</p><h2 id="fb5e" class="lz kf hi bd kg ma mb mc kk md me mf ko iq mg mh ks iu mi mj kw iy mk ml la mm bi translated">yolov3.cfg文件中不同参数的详细信息</h2><p id="9e03" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">中有许多可用的配置参数。cfg文件，可以对其进行调整以优化整体训练。用户需要了解<a class="ae kd" href="https://github.com/AlexeyAB/darknet/issues/279" rel="noopener ugc nofollow" target="_blank">这些参数</a>中的每一个，以及它将如何影响整体训练。</p><h2 id="298e" class="lz kf hi bd kg ma mb mc kk md me mf ko iq mg mh ks iu mi mj kw iy mk ml la mm bi translated">控制台输出的详细信息</h2><p id="9a6b" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">训练YOLO的时候，有一大堆<a class="ae kd" href="https://github.com/AlexeyAB/darknet/issues/636" rel="noopener ugc nofollow" target="_blank">行话</a>比如地域、avg IOU、class、obj、. 5R、. 75R都印在控制台上。我们发现，这些输出中的一些帮助我们决定更早地终止训练，而不是等待模型在整个50k时代中得到训练。</p><h1 id="cfdd" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">在边缘设备中部署YOLO模型</h1><p id="8977" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">有了你的YOLO模型，假设你想在一个30美元的小树莓上用10美元的相机运行它们。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/32f07213c2f05807e1f06f1d89f11c14.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/0*Sy34QRq9dPekxaA3"/></div></figure><p id="f09e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用YOLO或类似但不同的移动SSD(单次检测器)的部分优势是，它们通常很紧凑，非常适合移动或小型设备。事实证明你可以，这取决于你的设置，如果你想多花一点钱的话。首先，有了一个基本的Raspberry Pi和OpenCV，<a class="ae kd" href="https://docs.opencv.org/master/da/d9d/tutorial_dnn_yolo.html" rel="noopener ugc nofollow" target="_blank">你就可以加载你的模型并在本地运行它</a>。在YOLO-TINY模型上，你可以期望得到约200毫秒每帧。在YOLOv3上，我们最好的成绩是每帧12秒。为了让它运行得那么快，你需要<a class="ae kd" href="https://www.pyimagesearch.com/2018/09/26/install-opencv-4-on-your-raspberry-pi/" rel="noopener ugc nofollow" target="_blank">在RPi上构建OpenCV，并打开所有适当的加速和硬件标志</a>。简易安装没有成功。</p><p id="c41b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，如果你需要一些实时的东西，你可以通过一个加速器来实现，比如79美元的英特尔<a class="ae kd" href="https://software.intel.com/content/www/us/en/develop/hardware/neural-compute-stick.html" rel="noopener ugc nofollow" target="_blank">神经计算棒2 </a> (NCU2)或59美元的珊瑚USB加速器 (EdgeTPU)。两者都会给你超过20 FPS。NCS2使用英特尔的OpenVino框架，从安装到运行时相当复杂，但一旦你通过所有的咒语，就可以加载<a class="ae kd" href="https://github.com/PINTO0309/OpenVINO-YoloV3" rel="noopener ugc nofollow" target="_blank"> YOLOv3。或者，我们的首选方法是Coral Accelerator，它运行Tensorflow Lite (TF-Lite)。YOLOv3 tiny </a>的<a class="ae kd" href="https://github.com/guichristmann/edge-tpu-tiny-yolo" rel="noopener ugc nofollow" target="_blank">流程是将darknet <code class="du lv lw lx ly b">.weights</code>转换为Keras模型，然后将Keras模型转换为TF-Lite，然后为EdgeTPU编译TF-Lite。诀窍是确保EdgeTPU上的所有操作都可用。如果没有，检测将退回到CPU并停留在那里等待操作…你不希望这种情况发生。这听起来有点像瀑布，但是总的来说，比起OpenVino，我们更喜欢工作和维护TF代码。如果您有多个加速器，您可以将它们结合使用，以获得更快的FPS。虽然它们不会联合，但您可以在交错管道中为每个加速器线程化一个帧。</a></p><h1 id="0de7" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">结论</h1><p id="7b79" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">总之，通过使用工程技巧/方法(无论你怎么称呼它)，我们能够为我们的应用程序训练YOLOv3，并将其部署在加速的边缘设备上，并以实时性能运行。一旦你有了第一台便宜的计算机视觉设备，你就可以在任何地方嵌入它。我们希望这篇文章能帮助你们做同样的事情，更快更便宜地训练和部署YOLO。</p></div></div>    
</body>
</html>