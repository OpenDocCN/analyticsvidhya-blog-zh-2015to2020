<html>
<head>
<title>Solving my first regression problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解决我的第一个回归问题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/solving-my-first-regression-problem-ffbd76d9032e?source=collection_archive---------17-----------------------#2020-12-16">https://medium.com/analytics-vidhya/solving-my-first-regression-problem-ffbd76d9032e?source=collection_archive---------17-----------------------#2020-12-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5b30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">我如何开始我的数据科学之旅，以及从解决我的第一个回归问题中学到的东西</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/7fce5ff4a73f5ab909aec2412802abc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AUu2Nznfw5E3ee26"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">由<a class="ae ju" href="https://unsplash.com/@maxconacher?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">马克西米利安·科纳彻</a>在<a class="ae ju" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="b4db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它位于疫情的中部。我的朋友在我们的一次谈话中偶然提到了数据科学。作为一名计算机科学毕业生，我已经无数次听到ML、AI、DS等术语。出于某种原因，我总是放弃尝试这个领域。但是这个领域一直吸引着我。预测事物和计算机自己做决定的世界总让人感觉有点不可思议。由于封锁和旅行限制而感到无聊，我决定尝试一下。我开始在谷歌上搜索如何在数据科学的世界里做事。因此，我从每个千年都喜欢的来源——随机的Instagram视频——获得了动力，并在coursera上开设了吴恩达的机器学习入门课程。就这样，我开始了走向数据科学的旅程(双关语)。</p><p id="0a44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我，完成了成为数据科学家的第一步。重要的问题是，下一步是什么？我的朋友建议我应该选择数据科学中的一个领域，并尝试解决该领域中的一些基本问题。数据科学的各个领域包括回归、分类、自然语言处理、时间序列分析等..因为我总是发现计算机本身预测事物的行为非常迷人，所以我决定选择回归。</p><h1 id="c59c" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">概述</strong></h1><p id="50fb" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">在这篇文章中，我将带你经历我的旅程，并解释解决你的第一个回归问题的步骤。我们将实施以下步骤:</p><ol class=""><li id="ef91" class="ky kz hi ih b ii ij im in iq la iu lb iy lc jc ld le lf lg bi translated">挑选数据集</li><li id="09f9" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">数据预处理</li><li id="cfe8" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">实施回归算法，如线性回归(岭和套索)和XgBoost回归</li><li id="1051" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">选择正确的模型</li></ol><p id="1238" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:对于这个实现，我们将使用Python编程语言以及一些库，如pandas，matplotlib，sklearn等。</p><h1 id="cfe4" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">选择正确的数据集</strong></h1><p id="bd07" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">作为一个初学者，到目前为止，我不想让自己过多地参与数据工程和预处理部分。我开始在Kaggle上寻找回归问题的干净数据集。我发现了一组关于美国爱荷华州房价的数据，看起来很有趣，于是我决定尝试一下这个问题。数据集出现在<a class="ae ju" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview" rel="noopener ugc nofollow" target="_blank">这里</a>。有些特征是你所期望的，比如房子的面积、邻居、公共设施等。但是数据集也包含了深入每栋房子的特征，比如壁炉质量、车库类型、地下室状况等。</p><p id="0e8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择这个数据集有几个原因:</p><ol class=""><li id="4d3f" class="ky kz hi ih b ii ij im in iq la iu lb iy lc jc ld le lf lg bi translated">这是在Kaggle的“入门”部分，因此我知道这将是初学者友好的。</li><li id="ddb7" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">我检查了数据集，发现它很大程度上是完整的，因此只需要最少的预处理。</li><li id="a050" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">问题的描述表明，学习高级回归技术是理想的。</li></ol><p id="ac9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果上述任何一个因素对你来说不一样，我会建议你去找一个你自己选择的数据集。例如，如果您已经解决了一些回归问题，并且知道不同的库，那么您可以选择一个不干净的数据集来学习更多关于预处理技术的知识。</p><p id="197a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个特定的数据集，我们将使用pandas库的read_csv函数来导入数据集:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="3465" class="lr jw hi ln b fi ls lt l lu lv">import pandas as pd<br/>trainData = pd.read_csv('&lt;path_to_file&gt;')<br/>testData = pd.read_csv('&lt;path_to_file&gt;')</span></pre><h1 id="9d09" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">数据预处理</strong></h1><p id="9604" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">数据集包含具有字母数字字符、1位数、5位数、NAs等值的特征。因此，以一种格式获取数据是非常重要的，这种格式使得没有一个变量是偏斜的，并且算法尽最大能力执行。最初，作为一个初学者，我只听说过标准化。但是当我开始阅读关于标准化的指南时，我发现对数转换值也是一种减少变量值偏差的方法。对数变换是考虑对数值而不是变量的实际值的过程。对于这个问题，当我比较对数转换和归一化的结果时，对数转换的结果稍微好一些。因此，我们将在这里使用对数变换:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="f53e" class="lr jw hi ln b fi ls lt l lu lv"># Joining train and test data to preprocess, will separate out later</span><span id="07c0" class="lr jw hi ln b fi lw lt l lu lv">all_data=pd.concat((trainData.loc[:,'MSSubClass':'SaleCondition'],testData.loc[:,'MSSubClass':'SaleCondition']))</span></pre><p id="c5c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，在训练集中记录房价:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="6ca2" class="lr jw hi ln b fi ls lt l lu lv">#log transform the target values<br/>trainData["SalePrice"] = np.log1p(trainData["SalePrice"])</span></pre><p id="9836" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们找出偏斜度&gt; 0.75的训练变量。这些值也将进行对数转换:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="8c8a" class="lr jw hi ln b fi ls lt l lu lv">#log transform the target values<br/>trainData["SalePrice"] = np.log1p(trainData["SalePrice"])</span><span id="3e2a" class="lr jw hi ln b fi lw lt l lu lv">#extract features which are pure numeric<br/>numeric_feats = all_data.dtypes[all_data.dtypes != "object"].index</span><span id="4f6a" class="lr jw hi ln b fi lw lt l lu lv">#calculate skew of features using lambda functions<br/>skewed_feats = trainData[numeric_feats].apply(lambda x:skew(x.dropna()))</span><span id="eafd" class="lr jw hi ln b fi lw lt l lu lv">#extracting indexes features which have skew &gt; 0.75<br/>skewed_feats = skewed_feats[skewed_feats&gt;0.75]<br/>skewed_feats = skewed_feats.index</span><span id="79c4" class="lr jw hi ln b fi lw lt l lu lv">#log transform features<br/>all_data[skewed_feats] = np.log1p(all_data[skewed_feats])</span></pre><p id="ecdc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当前数据集包含非数字和数字值。为了使用各种可用的python库，我们需要将非数值转换为数值。在这里，我们将使用所谓的一热编码。一键式编码将一个变量分解成几个指标变量。关于一热编码的解释和一些例子，请查阅<a class="ae ju" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html." rel="noopener ugc nofollow" target="_blank">https://panases . py data . org/panases-docs/stable/reference/API/panases . get _ dummies . html .</a></p><p id="d50a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将把非数值转换成数值。这很容易通过使用熊猫函数get_dummies来完成:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="341b" class="lr jw hi ln b fi ls lt l lu lv">#Dummy Data<br/>all_data = pd.get_dummies(all_data)</span></pre><p id="d5ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将按该列的平均值填写所有NA值:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="e8a5" class="lr jw hi ln b fi ls lt l lu lv">#Fill NAs with mean of column<br/>all_data = all_data.fillna(all_data.mean())</span></pre><p id="f6d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后一步是将分割的预处理数据转换回训练和测试样本:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="b36e" class="lr jw hi ln b fi ls lt l lu lv">#creating matrices for sklearn<br/>X_train = all_data[:trainData.shape[0]]<br/>X_test = all_data[trainData.shape[0]:]<br/>y = trainData.SalePrice</span></pre><p id="d7b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为预处理的附加步骤，您可以尝试检查特征描述文件，如果您认为这些特征可能不是非常有用，则决定是否要从训练数据中删除一些特征。我会建议在实现套索线性回归后这样做，因为这将帮助你决定一个功能是否有用(稍后会有更多)。</p><p id="8ee1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">完成！数据现在是可用的格式。现在我们进入非常有趣的部分，回归算法</p><h1 id="4bde" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">回归算法</strong></h1><p id="400e" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">在以下方法中，我们将选择<em class="jd">均方根误差</em> (rmse)作为度量模型性能的指标。rmse应尽可能接近0。</p><ol class=""><li id="c5d9" class="ky kz hi ih b ii ij im in iq la iu lb iy lc jc ld le lf lg bi translated">线性回归-岭回归(L2正则化)</li></ol><p id="6653" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">岭回归是线性回归的变种。当数据集中的变量存在多重共线性时，通常采用岭回归。在岭回归中，我们通常会添加一定程度的偏差，以防止数据集的过拟合。岭回归之所以被称为L2正则化，是因为惩罚项的系数大小是平方的。要了解更多共线性，请阅读此处:<a class="ae ju" href="https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Ridge_Regression.pdf" rel="noopener ugc nofollow" target="_blank">https://NCSs-WP engine . net DNA-SSL . com/WP-content/themes/NCSs/pdf/Procedures/NCSS/Ridge _ rejection . pdf</a>并了解更多ridge regression watch的内部工作方式:<a class="ae ju" href="https://www.youtube.com/watch?v=Q81RR3yKn30&amp;ab_channel=StatQuestwithJoshStarmer" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=Q81RR3yKn30&amp;ab _ channel = statt questwith joshstarmer</a></p><p id="762d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于岭回归的python代码，我们首先实现一个方法，该方法为我们提供特定线性模型的交叉验证分数</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="da5a" class="lr jw hi ln b fi ls lt l lu lv">def rootMeanSquareError_CrossValidation(model):<br/>    rmse_negative= cross_val_score(model, X_train, y, scoring="neg_mean_squared_error", cv = 3)<br/>    rmse = np.sqrt(-rmse_negative)</span><span id="f6fe" class="lr jw hi ln b fi lw lt l lu lv">    return(rmse) # The returned rmse is array of 3 numbers as the cross validation is done 3 times, as signified by the cv=3 parameter</span></pre><p id="eed5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们实现学习模型。岭回归模型采用一个称为偏差程度的参数α。我们循环遍历一组阿尔法值，并绘制均方根误差与阿尔法值的关系图。</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="39e6" class="lr jw hi ln b fi ls lt l lu lv">alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 40]</span><span id="2c4b" class="lr jw hi ln b fi lw lt l lu lv">ridgeReg_cv = []</span><span id="9668" class="lr jw hi ln b fi lw lt l lu lv">for alpha in alphas:<br/>    temp = rootMeanSquareError_CrossValidation(Ridge(alpha)).mean()<br/>    ridgeReg_cv.append(temp)</span><span id="9eae" class="lr jw hi ln b fi lw lt l lu lv">ridgeReg_cv = pd.Series(ridgeReg_cv, index = alphas)<br/>ridgeReg_cv.plot(title = "Validation")<br/>plt.xlabel("alpha")<br/>plt.ylabel("rmse")</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lx"><img src="../Images/a3b2b0d28cb61d0f86f7937cd16e9153.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*i6zMV9x3OzLAAB_0-KWg8g.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">alpha vs rmse。作者图片</figcaption></figure><p id="c9eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们从该图中找出均方根误差的最小值:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="f994" class="lr jw hi ln b fi ls lt l lu lv">ridgeReg_cv.min()<br/>Output : 0.12834043288009075</span></pre><p id="a488" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看到rmse值是0.12834043288009075。我们稍后将使用该值与其他模型进行性能比较。</p><p id="997f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">万岁！我们实现了第一个回归模型。现在，进入下一个。</p><p id="4298" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.线性回归-套索回归(L1正则化)</p><p id="5ca4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与岭回归一样，Lasso回归也增加了一个偏差项，为学习过程增加偏差。这两种方法的主要区别在于，在Lasso回归中，偏差项具有系数的模。由于正则化程度为1，因此lasso回归称为L1正则化。由于套索回归的内在作用，变量的系数有可能变为零。这意味着在套索回归模型中，某个特征可能会从学习过程中被消除。岭回归就不是这样了。要了解更多，请观看<a class="ae ju" href="https://www.youtube.com/watch?v=NGf0voTMlcs&amp;ab_channel=StatQuestwithJoshStarmer" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=NGf0voTMlcs&amp;ab _ channel = StatQuestwithJoshStarmer</a></p><p id="a574" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在代码中，我们将使用LassoCv方法和一个alphas列表。我们还将使用上面定义的方法来找出均方根误差:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="3ec5" class="lr jw hi ln b fi ls lt l lu lv">model_lasso = LassoCV(alphas = [5, 1, 0.1, 0.01, 0.001, 0.0005]).fit(X_train, y)</span><span id="5738" class="lr jw hi ln b fi lw lt l lu lv">rootMeanSquareError_CrossValidation(model_lasso).mean()</span><span id="4fc7" class="lr jw hi ln b fi lw lt l lu lv">Output rmse: 0.1242464172089154</span></pre><p id="2771" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，Lasso模型的表现略好于岭回归模型。</p><p id="a7c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，Lasso回归可以帮助我们确定数据集中的哪些要素正在被实际使用。我们将挑选正在使用的前5个特征和Lasso回归模型最少使用的5个特征，并绘制它们:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="476d" class="lr jw hi ln b fi ls lt l lu lv">coef = pd.Series(model_lasso.coef_, index = X_train.columns)</span><span id="cf79" class="lr jw hi ln b fi lw lt l lu lv">importance = pd.concat([coef.sort_values().head(5),coef.sort_values().tail(5)])</span><span id="90bc" class="lr jw hi ln b fi lw lt l lu lv">matplotlib.rcParams['figure.figsize'] = (7.0, 8.0)</span><span id="5ae1" class="lr jw hi ln b fi lw lt l lu lv">importance.plot(kind = "barh")</span><span id="0c5f" class="lr jw hi ln b fi lw lt l lu lv">plt.title("Coefficients of features in the Lasso Model")</span></pre><p id="9c7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是输出:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ly"><img src="../Images/11038153ca20335b30aab8aba8a8136e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*w_Cgti9TYA2VOvSyQjDHKg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">作者图片</figcaption></figure><p id="5ed7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上图可以看出，最重要的功能是“GrLivArea ”,即居住面积平方英尺，最少使用的功能是“RoofMatl ”,即屋顶材料。如果你仔细想想，这是一种直觉，因为在买房子的时候，一个人会首先关注房子的居住面积以及周围的环境，并会把屋顶的材料放在次要位置。</p><p id="4764" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">耶！我们实现了第二个线性回归模型！</p><p id="cd1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.XGBoost</p><p id="2cd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我实现的第三个也是最后一个回归模型。在继续之前，我强烈建议您查看决策树和随机森林，因为XGBoost模型与这两个主题有点关系。</p><p id="f3d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">XGBoost是一个非常强大的模型。它使用了助推的概念。该算法不是训练彼此隔离的每个模型，而是迭代地创建某个预定义深度的新的随机森林，以从本质上纠正由先前模型产生的错误。XGBoost极其迷人！了解模型手表的内部运作:<a class="ae ju" href="https://www.youtube.com/watch?v=OtD8wVaFm6E" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=OtD8wVaFm6E</a></p><p id="ad9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谈到代码，我们首先将训练和测试数据转换成xgboost接受的格式。该算法的学习不同于岭回归或套索回归。在XGBoost中，我们首先使用xgb.cv方法超调各种参数，以获得可能的最佳模型。在这里，我用试错法调整了我的参数<em class="jd">最大深度、预计到达时间</em>和<em class="jd">最小体重</em>。要对最佳参数进行网格搜索，请查看<a class="ae ju" href="https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f" rel="noopener ugc nofollow" target="_blank">https://blog . Cambridge spark . com/hyperparameter-tuning-in-xgboost-4ff 9100 a3b2f</a>和<a class="ae ju" href="https://machinelearningmastery.com/tune-number-size-decision-trees-xgboost-python/" rel="noopener ugc nofollow" target="_blank">https://machinelingmastery . com/tune-number-size-decision-trees-xgboost-python/</a></p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="ecf4" class="lr jw hi ln b fi ls lt l lu lv">import xgboost as xgb</span><span id="d087" class="lr jw hi ln b fi lw lt l lu lv">xg_train = xgb.DMatrix(X_train, label = y)<br/>xg_test = xgb.DMatrix(X_test)</span><span id="193b" class="lr jw hi ln b fi lw lt l lu lv">params = {"max_depth":2, "eta":0.1, "min_child_weight":1}<br/>model = xgb.cv(params, xg_train,  num_boost_round=1000, early_stopping_rounds=50)</span><span id="d788" class="lr jw hi ln b fi lw lt l lu lv">print(model['test-rmse-mean'].min())</span><span id="254b" class="lr jw hi ln b fi lw lt l lu lv">Output rmse : 0.123212</span></pre><p id="67bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们已经学习了最佳参数，我们将使用XGBRegressor和我们的最佳参数</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="50f1" class="lr jw hi ln b fi ls lt l lu lv">final_model_xgb = xgb.XGBRegressor(min_child_weight=1,n_estimators=360, max_depth=2, learning_rate=0.1)</span><span id="7915" class="lr jw hi ln b fi lw lt l lu lv">final_model_xgb.fit(X_train, y)</span></pre><p id="3d3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">恭喜你！我们现在已经成功部署了我们的第三个回归模型！</p><h1 id="bdd7" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">挑选合适的型号</strong></h1><p id="48fc" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">为了选择正确的模型，我们通常可以选择均方根误差最小的模型，因为这是预测结果最接近真实值的模型。总结一下我们3个模型的rmse值:</p><ol class=""><li id="a934" class="ky kz hi ih b ii ij im in iq la iu lb iy lc jc ld le lf lg bi translated">岭回归:rmse = 0.12834043288009075</li><li id="8f55" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">Lasso回归:RMSE = 0.124586768686</li><li id="2b33" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">XGBoost : rmse = 0.123212</li></ol><p id="c3d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据性能指标，我选择XGBoost模型，因为它给了我最好的性能。</p><p id="e1ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我完成最终解决方案后，我发现还有另一种方法来选择最终模型。可以取两个或多个模型的加权平均值。为了说明这一点，我们将首先使用我们想要考虑的模型进行预测，然后找到它们的加权平均值:</p><pre class="jf jg jh ji fd lm ln lo lp aw lq bi"><span id="73f9" class="lr jw hi ln b fi ls lt l lu lv">lasso_predictions = np.expm1(model_lasso.predict(X_test))<br/>xgb_predictions = np.expm1(final_model_xgb.predict(X_test))</span><span id="a531" class="lr jw hi ln b fi lw lt l lu lv">preds = 0.35*xgb_predictions + 0.65*lasso_predictions</span></pre><p id="a956" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就是这样！简单回顾一下，我们刚刚实现了3个不同的回归模型，了解了这些算法的内部工作原理。我强烈建议任何开始其旅程的数据科学家参加这项活动。万事如意！</p></div></div>    
</body>
</html>