<html>
<head>
<title>NLP with Latent Semantic Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潜在语义分析的自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nlp-with-latent-semantic-analysis-b3de6e16ad7d?source=collection_archive---------4-----------------------#2020-01-18">https://medium.com/analytics-vidhya/nlp-with-latent-semantic-analysis-b3de6e16ad7d?source=collection_archive---------4-----------------------#2020-01-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0b25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主题建模是根据每个文档中出现的单词为语料库获取抽象主题的数学过程。它是一种无监督的机器学习模型，试图发现文档之间的文本相关性。有各种模型可用于执行主题建模，如潜在狄利克雷分配、潜在语义分析等。在这篇文章中，我们将着眼于潜在语义分析的功能和工作。我们还将研究这种方法背后的数学原理。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/994ec18221ada9f6570f1abecd2f6a17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*alcn-4iuz7b9NkMA"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">凯利·西克玛在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="e8f9" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">潜在语义分析</h1><p id="b0d3" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">潜在语义模型是一种统计模型，用于通过获得这些词之间的语义关系来确定文档集合和这些文档中存在的术语之间的关系。这里，我们从文本语料库中形成一个文档术语矩阵。列中的每一行代表文档中的唯一单词，每一列代表单个文档。该过程通过奇异值分解来实现。</p><h1 id="eb44" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated">模型背后的数学</h1><p id="24b5" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">潜在语义分析基于奇异值分解。这是一种将一个矩阵分解成三个矩阵的方法。让我们考虑一个要分解的矩阵A。然后将其分解成三个唯一的矩阵U、L和V，其中U和V是标准正交矩阵，L是奇异矩阵。如果一个矩阵的行列式不存在或不可逆，则该矩阵被定义为奇异矩阵。</p><p id="72d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，看看下面的矩阵。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lj"><img src="../Images/7b022d7872e9a80d199f50783dbd91bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tctmPjlqV66mR9s-I5bzMg.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">资料来源:datajango.com</figcaption></figure><p id="2471" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，X是术语-文档矩阵，它由每个文档中出现的所有单词组成。u和V转置矩阵是标准正交矩阵，其中每一行都是正交向量。s是一个奇异值对角矩阵，它的特征值沿着对角线出现。矩阵V转置的每一行代表主题，每一列中特定主题的值代表该单词在相应文档中的重要性和关系[注意:每一列代表一个独特的文档]。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="df49" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">Python实现</h1><p id="a1c8" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">下面给出了奇异值分解的Python实现。我们将使用SVD的scipy包来执行操作。首先让我们导入所需的包并定义我们的A矩阵。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="9bd4" class="lp kc hi ll b fi lq lr l ls lt">import numpy as np<br/>from scipy.linalg import svd<br/><br/>A = np.array([[1, 2], [3, 4], [5, 6]])<br/>print(A)</span></pre><p id="33c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们执行奇异值分解，通过因式分解获得我们需要的结果矩阵。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="b3b9" class="lp kc hi ll b fi lq lr l ls lt">U, S, VT = svd(A)<br/>print(U)<br/>print(S)<br/>print(VT)</span></pre><h1 id="a3bb" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated">文本分析</h1><p id="b438" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">现在，让我们通过一个实时项目检查来清楚地了解这种方法是什么。这里，我们将执行潜在语义分析来识别给定语料库的主题集群。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/94cae24823ed6871af27c5b4fcc10e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QrQRdUvVWLPSsA8m"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">照片由<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae jt" href="https://unsplash.com/@ratushny?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Dmitry Ratushny </a>拍摄</figcaption></figure><p id="3f8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们导入执行项目所需的所有包。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="1d57" class="lp kc hi ll b fi lq lr l ls lt">import re<br/>import numpy as np<br/>import pandas as pd<br/>from scipy import linalg, spatial<br/>from sklearn.cluster import KMeans<br/>from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD<br/>from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer, TfidfVectorizer)<br/>from sklearn.cluster import KMeans<br/><br/>from sklearn.utils.extmath import randomized_svd<br/><br/>from nltk.tokenize import word_tokenize<br/>from nltk.corpus import stopwords</span></pre><p id="4e19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将导入要分析的文本。这篇文章是从维基百科中摘录的关于小罗伯特·唐尼的内容。仅使用文本的一小部分摘录来清楚地理解工作过程。这个过程可以使用request和BeautifulSoup包扩展到大型文本。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="38e6" class="lp kc hi ll b fi lq lr l ls lt">corpus = [<br/><br/>          "With all of the critical success Downey had experienced throughout his career, he had not appeared in a blockbuster film. That changed in 2008 when Downey starred in two critically and commercially successful films, Iron Man and Tropic Thunder. In the article Ben Stiller wrote for Downey's entry in the 2008 edition of The Time 100, he offered an observation on Downey's commercially successful summer at the box office.",<br/>          "On June 14, 2010, Downey and his wife Susan opened their own production company called Team Downey. Their first project was The Judge.",<br/>          "Robert John Downey Jr. is an American actor, producer, and singer. His career has been characterized by critical and popular success in his youth, followed by a period of substance abuse and legal troubles, before a resurgence of commercial success in middle age.",<br/>          "In 2008, Downey was named by Time magazine among the 100 most influential people in the world, and from 2013 to 2015, he was listed by Forbes as Hollywood's highest-paid actor. His films have grossed over $14.4 billion worldwide, making him the second highest-grossing box-office star of all time."<br/>          <br/>          ]<br/><br/>stop_words = set(stopwords.words('english'))</span></pre><p id="49f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在这个语料库中有四个文档。我们还删除了停用词，其中包括一些最常见的重复单词，这些单词对句子的意义没有贡献。</p><p id="f510" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将使用分词器将整个文档拆分成单个单词。然后将单个标记编译到python字典filtered_text中。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="6276" class="lp kc hi ll b fi lq lr l ls lt">filtered_document= []<br/>filtered_text = []<br/><br/>for document in corpus:<br/>    <br/>    clean_document = " ".join(re.sub(r"[^A-Za-z \—]+", " ", document).split())<br/>    <br/>    document_tokens = word_tokenize(clean_document)<br/><br/>    for word in document_tokens:<br/>        if word not in stop_words:<br/>            filtered_document.append(word)<br/><br/>    filtered_text.append(' '.join(filtered_document))</span></pre><p id="2fc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将形成一个词频矩阵来统计语料库中不同文档中不同单词的用法。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="8b14" class="lp kc hi ll b fi lq lr l ls lt">vectorizer = CountVectorizer()<br/><br/>counts_matrix = vectorizer.fit_transform(filtered_text)<br/><br/>feature_names = vectorizer.get_feature_names()<br/><br/>count_matrix_df = pd.DataFrame(counts_matrix.toarray(), columns=feature_names)<br/>count_matrix_df.index = ['Document 1','Document 2','Document 3','Document 4']<br/><br/>print("Word frequency matrix: \n", count_matrix_df)</span></pre><p id="4b27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这段代码的输出打印了一个矩阵，显示了每个单词在每个文档中出现的频率。</p><p id="f87a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将查看获得的特征名称，并使用Kmeans算法通过无监督机器学习算法来识别密切相关的单词。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="4ded" class="lp kc hi ll b fi lq lr l ls lt">vectorizer = TfidfVectorizer(stop_words=stop_words,max_features=10000, max_df = 0.5,<br/>                                    use_idf = True,<br/>                                    ngram_range=(1,3))<br/><br/>X = vectorizer.fit_transform(filtered_text)<br/>print(X.shape)<br/>print(feature_names)<br/><br/>num_clusters = 4<br/><br/>km = KMeans(n_clusters=num_clusters)<br/>km.fit(X)<br/>    <br/>clusters = km.labels_.tolist()<br/>print(clusters)</span></pre><p id="17af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们使用SVD为语料库生成主题。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="cff0" class="lp kc hi ll b fi lq lr l ls lt">U, Sigma, VT = randomized_svd(X, n_components=10, n_iter=100, random_state=122)<br/><br/>svd_model = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=100, random_state=122)<br/><br/>svd_model.fit(X)<br/>    <br/>print(U.shape)<br/><br/>for i, comp in enumerate(VT):<br/>    terms_comp = zip(feature_names, comp)<br/>    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]<br/>    print("Cluster "+str(i)+": ")<br/>    for t in sorted_terms:<br/>        print(t[0])<br/>    print(" ")</span></pre><p id="2e85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从该方案获得的结果附后。这说明了话题是如何根据词与词之间的语义关系获得的。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="34a3" class="lp kc hi ll b fi lq lr l ls lt">Cluster 0: <br/>american<br/>age<br/>actor<br/>article<br/>ben<br/>billion<br/>called<br/><br/>Cluster 1: <br/>actor<br/>article<br/>ben<br/>billion<br/>called<br/>career<br/>changed<br/><br/>Cluster 2: <br/>american<br/>age<br/>actor<br/>among<br/>appeared<br/>blockbuster<br/>box<br/><br/>Cluster 3: <br/>american<br/>actor<br/>abuse<br/>article<br/>ben<br/>billion<br/>called</span></pre><h1 id="0317" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated">结论</h1><p id="2fec" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">在本文中，我们介绍了潜在语义分析及其python实现。我们还研究了奇异值分解数学模型。</p><h1 id="0216" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated">参考资料:</h1><p id="50eb" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">01.【https://en.wikipedia.org/wiki/Robert_Downey_Jr. T4】</p><p id="3bfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">02.<a class="ae jt" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Latent_semantic_analysis</a></p><p id="90e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">03.<a class="ae jt" href="https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python" rel="noopener ugc nofollow" target="_blank">https://www . data camp . com/community/tutorials/discovering-hidden-topics-python</a></p></div></div>    
</body>
</html>