# 可解释的人工智能——它与数据科学家有什么关系？

> 原文：<https://medium.com/analytics-vidhya/explainable-ai-what-is-it-why-is-it-important-41f062207235?source=collection_archive---------11----------------------->

## 用可解释的人工智能打开黑盒机器学习模型(XAI)

# 什么是可解释的人工智能(XAI)？

> “我的狗不小心撞倒了垃圾桶，在里面发现了旧的奶酪意大利面，现在它确信垃圾桶能提供无穷无尽的奶酪意大利面，一有机会就会打翻它。”

有时，你会看到你的机器学习(ML)模型也做同样的事情。

一个臭名昭著的例子是神经网络如何学会区分狗和狼。它并没有真正学会区分狗和狼，而是学会了所有狼的图片都以雪为背景，因为这是它们的自然栖息地，而不是以草为背景的狗。然后，该模型通过观察背景是雪还是草来区分这两种动物。

![](img/b61a4878c2814f34b1e6bd27d34f32b3.png)

可解释的人工智能

如果一只狗在雪地上，一只狼在草地上会怎么样？这个模型会做出错误的预测。

# 为什么可解释的人工智能很重要？

为什么有人会在意一个把狗和狼错误分类的模型？

> 一个简单的模型把狗错认成狼不可怕吧？但想象一下，一辆自动驾驶汽车错误地将一个人识别为物体，并从它身上碾过。

在自动驾驶汽车和机器人助手等应用中使用人工智能时，机器不仅应该像人脑一样学习，还应该像人类一样解释和推理决策。

实现这一点将是人工智能系统进化的一大飞跃，这将使人类能够更好地信任人工智能系统。XAI 是一个庞大的主题，也是学术界和工业界 AI/ML 研究的最热门的话题之一。

# 卷积神经网络的可解释性

带着理解 ML 模型如何学习的好奇心，我们冒险去理解深度神经网络如何工作。

**神经网络**长期以来被称为**、【黑盒模型】**，因为由于大量相互作用的非线性部分，无法理解它们是如何工作的。

*   从 MNIST 数字识别数据集开始，我们建立了一个简单的 CNN 模型，并可视化了如何训练 CNN 滤波器以及每个滤波器识别什么特征。
*   可视化每个完全连接层中的激活值，并识别每个数字激活的神经元集。
*   然后，我们使用激活最大化和显著图来解释输入图像的哪个部分对于模型正确分类输入是非常关键的。

Python Deep 学习包 **Keras** 提供了许多内置方法来帮助你可视化模型。这些都是已经存在的技术，代码可以在 Python 笔记本[这里](https://github.com/vinodhini8694/Visualizing-CNN-Filters-and-HiddenLayerActivations)找到。

> 注意:我们继续通过专有的方法和代码来理解 CNN，这里不能分享。

# 数据科学家的收获

*   尽管数据科学家通常会微调现有的 ML 算法来解决业务问题，但不将 ML 模型视为黑盒并试图理解其工作方式将会在他们的职业生涯中走很长的路。
*   这个项目帮助我揭开了 CNN 黑匣子里面是什么以及它是如何工作的。我打算探索像遮挡图和注意力这样的概念来更好地理解 ML 模型。

# 感谢阅读！

如果你喜欢我的工作，想支持我…

1.  支持我的最好方式是通过媒体[在这里](/@vinodhini.sd)关注我。
2.  在 LinkedIn 上关注我[这里](https://www.linkedin.com/in/vinodhinisd/)。
3.  请随意鼓掌，这样我就知道这篇文章对你有多大帮助了。