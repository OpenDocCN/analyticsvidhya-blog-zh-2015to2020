<html>
<head>
<title>Lasso and Ridge: the regularized Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">套索和山脊:正则化的线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/lasso-and-ridge-the-regularized-linear-regression-2d85384badf3?source=collection_archive---------12-----------------------#2020-09-22">https://medium.com/analytics-vidhya/lasso-and-ridge-the-regularized-linear-regression-2d85384badf3?source=collection_archive---------12-----------------------#2020-09-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="63df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归是执行回归任务的经典和最简单的线性方法。它让我们能够将原始数据用作工具，并执行预测性和规范性数据分析。但是，简单并不影响它的可用性。线性回归仍然是一种非常强大和广泛使用的算法，因为它提供了各种优势:</p><ul class=""><li id="dc63" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">它可用于相当大的数据集</li><li id="35dd" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">这是非常容易解释的:很容易理解为什么我们的模型对给定的输入给出了特定的输出。</li><li id="094f" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">在对要素拟合回归线性模型之前，不需要对要素进行缩放</li><li id="1a42" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">不需要担心调整参数，因为没有</li></ul><p id="2c54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，正如人们常说的，每一件伟大的事情都有其反面，这次也不例外。</p><p id="8048" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单的线性模型过于简单，使我们无法控制其复杂性。它只是试图实现一个目标，即使用合适的超平面来降低数据的均方根误差(RMSE)。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es jr"><img src="../Images/fe0d90e6da0357b15fe6d9d2616141e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*st1zZN7rHX8KYy_BZWYETg.png"/></div></figure><p id="bf3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单的线性模型易于欠拟合和过拟合。</p><p id="9d5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据中太多的随机性或特征与目标之间的复杂关系会导致<strong class="ih hj">欠拟合。</strong>此问题的一个可能解决方案是引入“新的”复杂多项式特征，以帮助模型更好地识别数据中的趋势。你可以在<a class="ae jz" rel="noopener" href="/analytics-vidhya/polynomial-regression-the-curves-of-a-linear-model-bef70876c998">https://medium . com/analytics-vid hya/polynomial-regression-the-curves-of-a-linear-model-bef 70876 c 998</a>找到更多关于多项式特性的信息。</p><p id="5473" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要学习的特征太多，样本数量相对较少，往往会导致<strong class="ih hj">过拟合</strong>。对于这个问题，我们有一些优雅的解决方案，如<strong class="ih hj">套索和山脊回归。</strong>使用这些工具，我们有意降低模型的复杂性，因为我们希望它能够更好地概括新数据，而不是学习每个数据点，包括离群值。</p><p id="aac1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在继续之前，我们需要熟悉一些术语。</p></div><div class="ab cl ka kb gp kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="hb hc hd he hf"><h1 id="7da1" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">正规化规范</strong></h1><p id="397e" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">通常有两组正则化规范被研究和实现。那两个是<strong class="ih hj"> L1正则化</strong>和<strong class="ih hj"> L2正则化</strong>。它们都有自己的策略来降低模型的复杂性，使用对具有高量值的权重进行惩罚的概念。</p><blockquote class="lk"><p id="f8c4" class="ll lm hi bd ln lo lp lq lr ls lt jc dx translated">L1范数:惩罚的基础是特征权重的绝对值之和。它试图获得一个稀疏解，其中大多数特征的权重为零。它可以有多种解决方案。本质上，L1范数执行特征选择，并且仅使用少数有用的特征来建立预测模型，并且完全忽略其余的特征。</p><p id="9674" class="ll lm hi bd ln lo lp lq lr ls lt jc dx translated">L2规范:处罚的基础是重量的平方和。它会尝试降低与所有要素相关联的权重值，从而降低每个要素对预测值的影响。因为它涉及平方项，所以在处理异常值时，它不是首选。它总是有一个独特的解决方案，处理复杂的数据集比L1规范更好。</p></blockquote></div><div class="ab cl ka kb gp kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="hb hc hd he hf"><h1 id="c244" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">套索回归</strong></h1><p id="7849" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">Lasso(最小绝对收缩和选择算子)回归不仅使用线性回归的基本概念，该概念涉及适当调整权重的选择以提高预测的可靠性，而且还有另一个要遵守的约束。它使用L1标准。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lu"><img src="../Images/345248e29cc15ef5dcdca483cd206613.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9HafC8pwsP6o1GmPc1szcQ.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">突出显示的术语是在套索回归(L1)中使用的惩罚。</figcaption></figure><p id="253a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看到新参数λ的引入。λ的值控制正则化的程度。λ的高值意味着更高的正则化程度；更简单的模型；更多的权值等于零。在λ=0的情况下，套索模型变得等同于简单的线性模型。λ的默认值为1。λ在sklearn线性模型中被称为<em class="md"> alpha </em>。</p><p id="0a15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看拉索回归的行动！</p><p id="d583" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用sklearn的<em class="md"> make_regression </em>方法创建了一个具有五个特征的合成数据集。我们还添加了5个具有随机值的新特征，它们将作为“无用”特征，我们希望它们具有非常低的系数值。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es me"><img src="../Images/62f9ec544d522c5be137d696c46ca5ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6xPcZYm2DHfKWDKY2MCkcQ.png"/></div></div></figure><p id="1f20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们使用数据集来训练各种线性模型。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mf"><img src="../Images/4a96c74920bcc01ff2f546bea5e44fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xvajFkb_dw_qnzBxrk49DQ.png"/></div></div></figure><p id="2795" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们看看系数如何随着<em class="md">α的值而变化。</em></p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mg"><img src="../Images/05456015c069ef3fb49c9d32767c351f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hnk0HdIGCctbmaC7O28tPA.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mh"><img src="../Images/f1e0b4f925ebd1e5403f8e1b37520585.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wrHsbM7E_Ni9ESC-Rsj9ag.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mi"><img src="../Images/29f672b1903740fb986b87036bb74a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hj7m8IqGBEFS4TAtF0GdVw.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mj"><img src="../Images/4650cbc96cbc67e61d248521d6be6a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ebkyPkIvL9_pWGJW9S1Aig.png"/></div></div></figure><p id="eef1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看到系数的值随着α而变化。随着阿尔法值的增加，系数的大小越来越接近零。事实上，在<em class="md"> alpha </em> =10时，五个随机特征中的四个恰好为零，正如我们所预期的那样。</p></div><div class="ab cl ka kb gp kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="hb hc hd he hf"><h1 id="8639" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">岭回归</strong></h1><p id="81a0" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">它还使用简单线性回归损失函数的修改版本。它使用L2范数进行正则化</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es lu"><img src="../Images/4568c1e6b96f85c8715398f03a0b9dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ipKl7ePx4eogmnKIXc4AGA.png"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">突出显示的术语是岭回归(L2)中使用的惩罚。</figcaption></figure><p id="ac19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与Lasso一样，参数λ控制正则化的量。λ的高值意味着更强的正则化；更简单的模型；重量较小。在λ=0时，岭的表现与简单线性模型相同。λ的默认值为1。</p><p id="f49b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">岭回归在模型简单性和训练集得分之间进行了权衡。</p><p id="d3cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看看<em class="md">α</em>对系数值的影响，</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mg"><img src="../Images/05456015c069ef3fb49c9d32767c351f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hnk0HdIGCctbmaC7O28tPA.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mk"><img src="../Images/61887f0df33a3d99f323aed1288a544f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vhu06Cf__QHh71c7MmzYqA.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mk"><img src="../Images/a97d1fc6316e23dbfb0eaf3afca9a6f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*quBMuooqFir5hG7ElH4syw.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es ml"><img src="../Images/5d7fddddf5e5ca7ca8eed193f1d8fdac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rU96NNxfPSQWdyM49pYlvg.png"/></div></div></figure><p id="f8d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在系数和<em class="md">α的关系中看到类似的趋势。</em>然而，在岭回归中，系数实际上不会达到零值。</p></div><div class="ab cl ka kb gp kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="hb hc hd he hf"><h1 id="705d" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">弹性网回归</strong></h1><p id="4929" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">ElasticNet回归是一个强大的算法，结合了套索和岭回归的力量。因此，它可以像Lasso一样执行特征选择，并像Ridge一样将系数推得更接近零，因为它必须遵守L1和L2惩罚。</p><p id="b408" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像套索和山脊一样，正则化的程度使用<em class="md"> alpha </em>参数进行调整。还有一个混合参数叫做<em class="md"> l1_ratio </em>，它允许我们调整l1和L2正则化的数量。</p><ul class=""><li id="e4e2" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><em class="md"> l1_ratio </em>在于[0，1]</li><li id="8ef8" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">其默认值为0.5</li><li id="2547" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><em class="md"> l1_ratio </em> = 0对应纯岭回归</li><li id="2ee6" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><em class="md"> l1_ratio </em> = 1对应纯套索回归</li></ul><p id="0140" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与Lasso和Ridge相比，我们看到在这种情况下系数被强烈地推到零。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mg"><img src="../Images/05456015c069ef3fb49c9d32767c351f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hnk0HdIGCctbmaC7O28tPA.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mm"><img src="../Images/77aaf44da02086172c2fdb0546b7caa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4L9kRxzUezma5JsMmsU6A.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mn"><img src="../Images/e15b51719809b4cdefedcb45fa12174e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WiRVzDE6QAnxt7V21ekJXw.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="er es mo"><img src="../Images/9293c83bf08bb51d7d1b56a3429ec797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ErTmKTTvAxMY0GRoKNWhpg.png"/></div></div></figure><p id="952f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你发现它能提供信息，请鼓掌！</p></div></div>    
</body>
</html>