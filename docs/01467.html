<html>
<head>
<title>Multivariate Linear regression from Scratch Using OLS (Ordinary Least Square Estimator)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用OLS(普通最小二乘估计器)从头开始多元线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multivariate-linear-regression-from-scratch-using-ols-ordinary-least-square-estimator-859646708cd6?source=collection_archive---------1-----------------------#2019-10-24">https://medium.com/analytics-vidhya/multivariate-linear-regression-from-scratch-using-ols-ordinary-least-square-estimator-859646708cd6?source=collection_archive---------1-----------------------#2019-10-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0337" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">几乎所有的机器学习算法都集中在学习函数上，学习函数可以描述输入(特征/自变量)和输出(目标变量/因变量)之间的关系。该函数的形式取决于所使用的算法。线性回归是最简单的机器学习算法之一，它使用线性函数来描述输入和目标变量之间的关系。多元(具有一个以上的变量/输入)线性回归的简单方程可以写成</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/bb24d487c925b97a21fc6ecdbd8aea71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*nSZ6tHDleGTcWhgbB3MUjA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">等式1</figcaption></figure><p id="329c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中β1，β2…βn是与特征<em class="jp"> x1，x2，…相关联的权重。xn </em>。β0是偏差项(所有特征都等于零时的y值)。ε是误差。我们的任务是减少这种错误。我们将使用最小二乘法来减少这个误差。上述方程可以用矩阵方程的形式写成如下</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jq"><img src="../Images/d762dd16639646e001a9ac01738b593f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/1*F6abOBjEz-X-ZwgDIw3fVw.gif"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">等式:2线性回归的矢量化方程</figcaption></figure><p id="515a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意输入矩阵中额外的1列。增加该列是为了补偿偏差项。另外，<em class="jp"> b(权重)</em>栏中增加了偏置项β0。<em class="jp"> x </em>矩阵的每一行代表一个观察或记录，每一列代表一个特征。<em class="jp"> x </em> 12表示第二个特征的第一个观察值。给定的ε方程可以写成</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jr"><img src="../Images/8828d5bec18ccd566b8517cd4e93ad01.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/1*t3Hm4YqITzz6FzB6mezOBw.gif"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">等式:3.4</figcaption></figure><p id="64cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的目标是最小化ε的平方值。普通最小二乘估计器(OLS)的思想包括选择b，使得误差平方和尽可能小。所以我们必须尽量减少</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es js"><img src="../Images/c9c5dd6801675bc167d3cc2aba2b7709.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/1*TOCAJrUkquDPClXcXNECOw.gif"/></div></figure><p id="16a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是误差平方和，也可以写成</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jt"><img src="../Images/acd2ceffe3e4d56817dd662180c9c44c.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/1*RiGF0Q1HNVLtDL86C9ynUg.gif"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">等式5</figcaption></figure><p id="27bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如同</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ju"><img src="../Images/5a052ba930010cbb6a9c0d5abeb70c9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/1*zQw6TLhwybv69HSDo8ZbrA.gif"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">方程式:6.7</figcaption></figure><p id="73fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jv"><img src="../Images/292872f3ff45dd8dbbfb266a5da9c741.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/1*g8NFgRxDVK97-QxQ1GL2tg.gif"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">等式:8，9，10，11，12</figcaption></figure><p id="4ab1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，我们已经计算了平方误差相对于权重b的偏导数，并使其等于零，这意味着我们正在计算误差函数的局部最小值。因为偏导数在函数的最小值和最大值处都为零，我们怎么能确定它是已经计算的函数的最小值呢？嗯，最小二乘法形成一个凸函数，对于偏导数，只返回局部最小值<a class="ae jw" href="https://math.stackexchange.com/questions/483339/proof-of-convexity-of-linear-least-squares" rel="noopener ugc nofollow" target="_blank"/>。更详细的推导，可以访问<a class="ae jw" href="https://economictheoryblog.com/2015/02/19/ols_estimator/" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jx"><img src="../Images/29ba45626a4a3141846818ab907ffc7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*aiNJL5Gc0GHmWtcCHNyMdg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">凸函数</figcaption></figure><p id="dbc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将转向使用OLS实现多变量线性回归。我们将使用Numpy进行代数运算</p><pre class="je jf jg jh fd jy jz ka kb aw kc bi"><span id="ca55" class="kd ke hi jz b fi kf kg l kh ki">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="7a3a" class="kd ke hi jz b fi kj kg l kh ki">def msee(actual, predicted):<br/>    sum_error = 0.0<br/>    for i in range(len(actual)):<br/>        prediction_error = predicted[i] - actual[i]<br/>        sum_error += (prediction_error ** 2)<br/>        mean_error = sum_error / float(len(actual))<br/>    return mean_error</span></pre><p id="7a50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Mse是评估指标，我们将使用这个函数来评估我们的模型。</p><pre class="je jf jg jh fd jy jz ka kb aw kc bi"><span id="c25d" class="kd ke hi jz b fi kf kg l kh ki">def Train(X,Y):<br/>    ''' With this function we are calculate the weights   '''<br/>    X.astype(float)<br/>    first=np.dot(X.T,X)<br/>    first.astype(np.float16)<br/>    inverse=np.linalg.inv(first)<br/>    second=np.dot(X.T,Y)<br/>    <br/>    b=np.dot(inverse,second)<br/>    return b</span></pre><p id="4c1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该函数是等式12的实现。注意np.linalg.inv计算矩阵的逆矩阵。</p><pre class="je jf jg jh fd jy jz ka kb aw kc bi"><span id="c80c" class="kd ke hi jz b fi kf kg l kh ki">def add_bias(x):<br/>    if (len(x.shape)==1):<br/>        x=x[:,np.newaxis]<br/>    b=np.ones((x.shape[0],1))<br/>    x=np.concatenate((b,x), axis=1)<br/>    return x</span></pre><p id="739e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个函数将1添加到我们的特征中，就像等式2中的一样</p><pre class="je jf jg jh fd jy jz ka kb aw kc bi"><span id="505e" class="kd ke hi jz b fi kf kg l kh ki">def predict(X,b):<br/>    return (np.dot(X,b))</span></pre><p id="b44c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预测函数将根据训练函数返回的矩阵b(权重)的值预测目标值。数据可以从<a class="ae jw" href="https://drive.google.com/file/d/1JWhNDqCLl4tAYWiINFYSQx6580zIw0pM/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>下载</p><pre class="je jf jg jh fd jy jz ka kb aw kc bi"><span id="4c62" class="kd ke hi jz b fi kf kg l kh ki">df=pd.read_csv('MpgData_with_Cateogeries.csv')<br/>col=df.columns<br/>we=df.to_numpy()<br/>we=we[:,0:8]<br/>we=we.astype(np.float64)<br/>df.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kk"><img src="../Images/bdf9eb746e8cd3d180b177776607660f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jbxw-iKUdL4SkY3eKUaYHg.png"/></div></div></figure><p id="a608" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些数据是关于汽车的不同属性，如mpg(每加仑英里数)、马力、重量、加速度、制造年份。我们已经删除了carname和category等分类列(非数字值)。我们将选择mpg作为我们的目标变量。</p><pre class="je jf jg jh fd jy jz ka kb aw kc bi"><span id="d9d9" class="kd ke hi jz b fi kf kg l kh ki">xtrain=we[:292,1:8]<br/>ytrain=we[:292,0]<br/>xtest=we[292:,1:8]<br/>ytest=we[292:,0]</span></pre><p id="54cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在总共392个观察值中，我们将使用292个作为训练数据，其余100个作为测试数据。xtrain是我们的训练输入，ytrain是我们的训练输出。因此，xtest是我们的测试输入，ytest是目标变量的测试部分。此外，第零列是我们的目标变量mpg。</p><pre class="je jf jg jh fd jy jz ka kb aw kc bi"><span id="1b4d" class="kd ke hi jz b fi kf kg l kh ki">for i in range(2,8):<br/>    x_train=add_bias(xtrain[:,0:i])<br/>    b=Train(x_train,ytrain)<br/>    train_predict=predict(x_train,b)<br/>    train_error=msee(ytrain,train_predict)<br/>    print('Training  Error for Multivariable regression using  {} variables is   {}  '.format(i,train_error))</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kp"><img src="../Images/6d7fdb4fb69b0eb69f98b756462ec933.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wo5Syh0i92oLrvPgVT9arQ.png"/></div></div></figure><p id="54f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，在循环内部的第一行，我们添加了一个偏差项。然后我们用训练函数计算b。之后，我们使用训练数据预测我们的目标变量。然后我们计算训练误差。注意，对于循环的每一次迭代，我们都在增加我们的训练变量。对于第一次迭代，我们只考虑两个变量。对于第二次迭代，我们考虑3个变量等等。随着我们不断增加变量的数量，我们的MSE(均方误差)继续下降，这很明显。现在我们将根据测试数据评估我们的模型</p><pre class="je jf jg jh fd jy jz ka kb aw kc bi"><span id="3ec0" class="kd ke hi jz b fi kf kg l kh ki">for i in range(2,8):<br/>    x_train=add_bias(xtrain[:,0:i])<br/>    x_test=add_bias(xtest[:,0:i])<br/>    b=Train(x_train,ytrain)<br/>    test_predict=predict(x_test,b)<br/>    test_error=msee(ytest,test_predict)<br/>    print('Testing Error for Multivariable regression using  {} variables is   {}  '.format(i,test_error))</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kq"><img src="../Images/b3ebcd15dc192c87be6af659af16ed81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*pjvjjelYv7le8yFiHi-7QQ.png"/></div></figure><pre class="je jf jg jh fd jy jz ka kb aw kc bi"><span id="c57b" class="kd ke hi jz b fi kf kg l kh ki">x_train=add_bias(xtrain)<br/>x_test=add_bias(xtest)<br/>b=Train(x_train,ytrain)<br/>test_predict=predict(x_test,b)<br/>plt.figure(figsize=(10,5))<br/>plt.title('Multivariate linear regression for Test data',fontsize=16)<br/>plt.grid(True)<br/>plt.plot(ytest , color='purple')<br/>plt.plot(test_predict , color='red'  )<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kr"><img src="../Images/68a3a9d7ae3c4efd21be2ae1ebfeced2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HV9wsnlTbfsX4xha8-ASVg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">预测测试数据和原始测试数据图</figcaption></figure><p id="d669" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们绘制了测试目标和模型预测的目标值，即预测数据。你可以找到完整的项目与CSV文件<a class="ae jw" href="https://github.com/ImtiazUlHassan/Multivariate-linear-regression-Using-OLS-from-Scratch" rel="noopener ugc nofollow" target="_blank">在这里</a></p></div></div>    
</body>
</html>