<html>
<head>
<title>Logistic Regression Revisited</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归再探</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-revisited-introduction-and-implementation-a74b730b4e1f?source=collection_archive---------10-----------------------#2020-11-18">https://medium.com/analytics-vidhya/logistic-regression-revisited-introduction-and-implementation-a74b730b4e1f?source=collection_archive---------10-----------------------#2020-11-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="afe1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归是分类问题最基本的机器学习算法之一。彻底理解它变得更加重要，因为它为更复杂的算法(如深度神经网络)奠定了坚实的基础。从技术上讲，我们可以说逻辑回归是神经网络的最简单形式。</p><h2 id="c5cc" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">什么是逻辑回归</strong></h2><p id="7f0f" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">逻辑回归是一种经典的二元分类算法。当所需输出为0或1时使用。如果输入由X表示，输出由y表示，那么逻辑回归的输出可以表示为:</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="3d30" class="jd je hi ki b fi km kn l ko kp">y^ = Probability(y=1 | X)</span></pre><p id="3679" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里y^ (y-hat)是预测值，介于0和1之间。在逻辑回归中使用Sigmoid激活函数来生成这种类型的输出。让我们接下来讨论这个。</p><h2 id="88eb" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">乙状结肠激活功能</strong></h2><p id="4e47" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">如果您熟悉线性回归，输入x和预测输出y^之间的关系定义为:</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="9240" class="jd je hi ki b fi km kn l ko kp">y^ = W*X + b </span></pre><p id="5f88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中W是权重，b是偏差。上述公式返回y^.的连续值。我们在逻辑回归中的目的是得到一个介于0和1之间的值。为了实现这一点，我们在线性输出之上应用sigmoid激活。所以输出变成了:</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="165d" class="jd je hi ki b fi km kn l ko kp">y^ = Sigmoid (W*X +b),<br/>where Sigmoid(z) = 1 / (1+e^-z)</span></pre><p id="4012" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当Z是非常大的数时，Sigmoid(z)变得接近1。当Z很小时(大负数)，Sigmoid(z)变得接近0。因此，Sigmoid函数的输出始终保持在0和1之间。</p><p id="d9a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定一个二元分类问题，我们的目标是找到w和b的最佳值，以最小化地面真实值y和预测值之间的差异。y^.损失函数用于此目的。让我们来了解如何。</p><h2 id="7c0a" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">损失函数</strong></h2><p id="4b51" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">损失函数被定义来衡量预测相对于地面真实值有多好。我们的目标是最小化损失函数。为了计算单个训练示例的损失L，我们使用如下定义的对数损失:</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="962d" class="jd je hi ki b fi km kn l ko kp">L( y^, y) = -(y.log(y^) + (1-y).log(1-y^))</span></pre><p id="1d0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(1)如果y = 0，那么损失将等于:-log(1-y^).y^应该接近0，这样log的输出就变成0。(因为log(1) = 0)</p><p id="612e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(2)如果y = 1，那么损失变成:-log(y^).y^应该接近1，这样输出就变成0。</p><p id="87aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于多个训练样本，我们计算成本函数，这基本上是所有这些样本的平均损失。如果我们有m个训练样本，成本J定义为:</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="e77b" class="jd je hi ki b fi km kn l ko kp">J(W,b) = 1/m (Σi=1 to m L(y^(i), y(i))</span></pre><p id="3237" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们的目标是最小化为所有训练样本计算的成本函数。为此，我们使用梯度下降算法。让我们看看如何。</p><h2 id="bc9e" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">梯度下降</strong></h2><p id="8642" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在梯度下降算法中，相对于成本函数J计算参数W和b导数，以使其最小化。现在什么是导数。</p><blockquote class="kq kr ks"><p id="16e9" class="if ig kt ih b ii ij ik il im in io ip ku ir is it kv iv iw ix kw iz ja jb jc hb bi translated">导数基本上是函数的斜率，它决定了向哪个方向移动以使函数最小。换句话说，导数是当从属特征被更新时，函数改变的程度。</p></blockquote><p id="4480" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在逻辑回归中，我们根据W和b参数最小化成本函数。J W r t W和b的导数分别表示为dW和d b。如果α是学习率，下面是在梯度下降中更新参数的等式:</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="bd29" class="jd je hi ki b fi km kn l ko kp">W = W - α.dW<br/>b = b - α.db</span></pre><p id="ccfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们通过一个例子来理解这一点。假设我们有两个输入特征x1，x2。相应的权重为w1、w2，偏差为b。因此，逻辑回归的输出可以写成以下语句:</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="0f03" class="jd je hi ki b fi km kn l ko kp">Z = w1.x1 + w2.x2 +b<br/>y^ = Sigmoid(z)</span></pre><p id="ddd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并且参数将被更新为:</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="cbf9" class="jd je hi ki b fi km kn l ko kp">w1 = w1 - α.dw1<br/>w2 = w2 - α.dw2<br/>b = b - α.db</span></pre><p id="6a88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们检查一下这个例子中梯度下降的单个步骤:</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="0dcc" class="jd je hi ki b fi km kn l ko kp">J=0, dw1=0, dw2=0, db=0</span><span id="f2da" class="jd je hi ki b fi kx kn l ko kp">for i=1 to m:<br/>  z(i) = W.x(i) +b<br/>  y^(i) = Sigmoid(z(i))<br/>  J+= -[y(i).log(y^(i) + (1-y(i).log(1-y^(i)]<br/>  dz(i) = y^(i) - y(i)<br/>  dw1+= x1(i).dz(i) #repated for each input feature<br/>  dw2+= x2(i).dz(i)<br/>  db+= dz(i)</span><span id="cbfc" class="jd je hi ki b fi kx kn l ko kp">J/= m  #averaing for m training samples<br/>dw1/= m<br/>dw2/= m<br/>db/ = m   </span></pre><p id="9210" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的实现中，有两个地方我们需要使用显式for循环。第一个循环用于m个训练示例，第二个循环用于n个输入特征。(在这个例子中我们只有两个特性，因此我们没有写for循环)。</p><p id="41a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实现中使用显式for循环会随着训练数据变大而使梯度下降变得极其缓慢。我们使用向量化矩阵乘法技术来解决这个问题。</p><h2 id="dfef" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">矢量化实现</strong></h2><blockquote class="kq kr ks"><p id="df26" class="if ig kt ih b ii ij ik il im in io ip ku ir is it kv iv iw ix kw iz ja jb jc hb bi translated">数组乘法的矢量化版本比使用for循环快大约300倍。</p></blockquote><p id="cfe6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Numpy dot()函数用于将两个数组相乘。我们将使用以下矢量化版本进行实施:</p><p id="f10e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x:跨m个训练样本的输入特征</p><p id="11f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y:m个训练样本的基础真实向量</p><p id="513d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">w:权重矩阵</p><p id="2d1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a:m个训练样本的预测向量</p><p id="62bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实施的胜利梯度下降将被定义为:</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="a284" class="jd je hi ki b fi km kn l ko kp">import numpy as np</span><span id="5ba1" class="jd je hi ki b fi kx kn l ko kp">Z = np.dot(W,X) + b<br/>A = Sigmoid(Z)<br/>dZ = A - Y<br/>dW = 1/m (X.dZ)<br/>db = 1/m (np.sum(dZ))</span><span id="3b54" class="jd je hi ki b fi kx kn l ko kp">W = W - α.dW<br/>b = b- α.db</span></pre><p id="153f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管我们已经移除了训练样本(m)和特征数量(n)的循环，但我们仍然需要梯度下降中迭代次数的for循环。</p><p id="32e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，让我们总结一下实施逻辑回归梯度下降的步骤:</p><ol class=""><li id="9153" class="ky kz hi ih b ii ij im in iq la iu lb iy lc jc ld le lf lg bi translated">计算当前损失</li><li id="8e92" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">计算当前梯度</li><li id="a091" class="ky kz hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">使用梯度和学习率更新参数</li></ol><p id="991a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，我们在本文中讨论了逻辑回归的工作。这是我即将发表的关于深度学习的文章中的第一篇。希望你觉得有用。</p><p id="b964" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你觉得这篇文章有用，请在评论中告诉我。我是一名数据科学爱好者和博客作者。你可以通过我的LinkedIn <a class="ae lm" href="https://www.linkedin.com/in/sawan-saxena-640a4475/" rel="noopener ugc nofollow" target="_blank">个人资料</a>联系我。</p><p id="a154" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读。</p></div></div>    
</body>
</html>