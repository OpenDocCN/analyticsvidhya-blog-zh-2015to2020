# 数据砖块上的端到端机器学习管道—第 5 部分

> 原文：<https://medium.com/analytics-vidhya/end-to-end-machine-learning-pipeline-on-databricks-part-5-c10273e2cd88?source=collection_archive---------10----------------------->

![](img/59e9dca05401a67c3aab73dad14ef618.png)

在[之前的博客](/@anveshrithaas/machine-learning-in-pyspark-part-4-5813e831922f)中，我们从 PySpark 中的机器学习开始，通过使用 Spark 的机器学习库实现一个线性回归模型来预测房价。

在这篇博客中，我们将创建一个端到端的机器学习管道。与前一篇博客中的示例不同，我们将围绕 Apache Spark- Databricks 构建一个基于云的统一数据分析平台，体验使用 Apache Spark 进行大规模数据处理、分析和机器学习的体验，就像数据科学家和工程师如何大规模应用高级分析技术和机器学习模型来解决现实世界的问题一样。Databricks 使我们能够在很短的时间内轻松构建、扩展和部署机器学习模型。它提供了全面管理的 Spark 集群、用于数据探索和可视化的交互式工作空间、生产流水线调度程序和一个促进基于 Spark 的应用的平台。

为了更好地理解这是如何工作的，让我们通过在 Databricks 平台上用 PySpark 创建一个端到端的机器学习管道来动手操作。首先将数据加载到数据帧中，并执行探索性数据分析，然后继续创建统一的管道，该管道由统称为管道阶段的**转换器**和**估算器**组合而成，最后是机器学习模型的评估。转换器是一种将一个数据帧转换成另一个数据帧的算法。它包括特征转换器和学习模型。估计器是一种算法，它通过在数据帧上实现 fit()方法来拟合或训练数据，从而生成一个作为转换器的模型。

![](img/1c158f31c38bf695cc64c6af1ab5875f.png)

来源:AIEngineering

在这篇博客中，我们将在 PySpark 中为一个银行营销用例构建一个机器学习管道。该数据集包含一家银行为说服客户订阅定期存款而开展的基于电话营销活动的信息，我们的任务是对其进行分析，并确定有助于我们得出结论的模式，以便制定未来策略让客户订阅定期存款。基本上，这将是一个二元分类问题，目标是预测客户是否会订阅(是或否)。我们将为此使用的数据集可以在[这里](https://www.kaggle.com/janiobachmann/bank-marketing-dataset)下载。

## 设置数据块环境

![](img/af7190b3d3ca16b4271c60e74a4ab295.png)

在我们开始构建管道之前，我们必须首先设置我们的数据块环境。Databricks 提供了一个免费的社区版，其中包含了处理大规模数据集的功能，这对于我们在这篇博客中将要做的事情来说已经足够了。

*   先去[官方 databricks 网站](https://databricks.com/try-databricks)。一旦你到达那里，填写你的详细资料注册并开始。
*   您需要在 databricks 平台免费试用版和社区版之间选择一个平台。虽然在 AWS 或 Azure 云平台上提供的 databricks platform 免费试用版提供了一些高级功能，如协作环境、无限集群、作业调度器、与 ML 框架的本机集成等，这些都是面向企业的，但 community edition 提供了对免费微集群(6GB，无工作节点)以及集群管理器和笔记本环境的访问，非常适合开始使用 Spark。对于本教程，社区版应该足够了。
*   一旦电子邮件验证完成，我们就可以开始了。在欢迎页面中，您会看到创建新笔记本、表格、集群等的选项。让我们从创建一个集群开始。

![](img/569a3e538b8834ffab7c14ccb26c596b.png)

*   要创建新的集群，请为集群命名，选择一个 databricks 运行时版本，然后单击 create cluster。

***注意:*** *在社区版中，集群会在两个小时的空闲期后自动终止。*

![](img/489a70ce47724438bfedc898c0f21ba7.png)

创建集群需要一些时间。当群集状态从“挂起”变为“正在运行”时，这意味着群集已创建并且当前正在运行。

![](img/a159b485f790549d3607806e2ecc6562.png)

当您单击您的集群时，您可以在顶部看到多个选项卡。当你点击库标签，会有一个选项'安装新的'，让你安装各种软件包和库，你会需要的。“笔记本”选项卡显示与群集一起运行的笔记本。您还可以在“事件日志”和“驱动程序日志”选项卡中查看日志信息。

![](img/7e987175f0f67574a78fadb7bf9bb47c.png)

创建集群后，让我们转到显示已经可用的数据的数据部分。如果我们想使用新数据，那么点击“添加数据”来加载数据。

![](img/a1c5ec86eb2bb96571b4e4d2e9b914e9.png)

点击“添加数据”后，您会看到各种添加数据的选项。您可以添加数据，只需从您的本地文件系统上传，或连接到亚马逊 S3 桶，或将数据上传到 Databricks 文件系统(DBFS)，或添加来自其他来源的数据，如亚马逊 Kinesis，Cassandra，Kafka，JDBC 等。通过单击在笔记本中创建表格，它将打开一个新的笔记本，并在其中加载数据。

![](img/f7f066151646249bbdbea1446957af5b.png)

获取数据的一种方式是连接 AWS 环境，并通过给予必要的权限将数据获取到 Databricks Spark 环境，从 S3 存储桶中提取数据。亚马逊 S3 只不过是 AWS 提供的一种存储服务，具有高度的可扩展性、安全性和性能，可以为各种各样的用例存储任意数量的数据。包括各行各业领先企业在内的多家公司都使用它来存储大量数据，并随时随地在网上检索这些数据。

## AWS 入门

![](img/2d3c895fd51ffd89d672f4ff5da7fb01.png)

对于我们将要创建的机器学习管道，让我们尝试从亚马逊 S3 桶中提取数据，以便了解数据科学家和数据工程师如何使用真实世界用例的云存储来处理大量数据。为此，我们必须创建一个 S3 桶，将数据上传到其中，将 Databricks 环境与 AWS 连接起来，并将数据从 S3 桶拉入我们的 Spark 环境。

**设置 AWS 账户**

为此，首先我们需要有一个 AWS 帐户。如果您没有，可以注册一个 AWS 免费层帐户(包括 12 个月的免费层访问)。

*   访问 [AWS 网站](https://aws.amazon.com)
*   点击“创建 AWS 帐户”,输入您的详细信息，然后点击“继续”。
*   接下来，选择帐户类型—专业或个人，开始填写您的联系信息，并通过单击“创建帐户并继续”进行下一步。
*   一旦这一步完成，它会问你支付卡的细节。即使是免费等级帐户也需要您输入这些详细信息。但是，除非您的使用超过 AWS 免费层限制，否则不会向您收费。
*   完成后，您将被要求通过电话号码验证来确认您的身份。输入您的电话号码，然后点击“发送短信”以接收该手机号码的验证码。
*   输入验证码以成功验证您的身份。
*   最后，从列出的三个计划中，选择自由层的基本计划。

就是这样。您的 AWS 免费层帐户已成功创建，现在您可以使用您的凭据登录 AWS 控制台并开始使用 AWS 服务。AWS 免费层帐户在亚马逊 S3 提供高达 5 GB 的存储空间。为了简单起见，我们将用于机器学习管道的数据是一个小数据集。但在现实世界中，数据科学家和分析师处理的数据甚至可能高达万亿字节。

**创建 S3 桶并存储数据**

现在我们已经设置了 AWS 帐户，让我们开始上传已经下载到亚马逊 S3 桶中的数据集。在此之前，我们必须创建一个 S3 桶。为此，

*   登录到您的 AWS 帐户，在 AWS 管理控制台中，在 AWS 服务下搜索 S3 并点击它。

![](img/3451b56436c156dce6fc9cf71bc8e8ed.png)

*   进入 S3 窗口后，单击“create bucket”创建一个新的存储桶，我们将在其中存储数据。

![](img/4c6620e19bd48db444e4cbd839dd2362.png)

*   为存储桶指定一个唯一的名称，并选择离您的数据块集群区域最近的区域，以最大限度地减少延迟。完成后，单击创建。您不需要担心配置选项或设置权限，在为集群命名并选择区域后，您可以直接创建集群。

![](img/bd3cffe17455c96c97ea3bc76552cbab.png)

*   现在，你会看到新创建的 S3 桶。点击桶。

![](img/b0f07e79bdb0c5b0715134e324122330.png)

*   在这里，点击上传按钮上传我们的数据集在这个 S3 桶。

![](img/9117c451cb725a6d9a1414d937de2939.png)

*   单击“添加文件”以添加数据集。选择您必须从本地文件系统上传的数据集。

![](img/1706f786e4540162bcd1cbad2f1595a3.png)

*   现在，您要上传的选定文件将出现在列表中。验证并上传以将其存储在 S3 存储桶中。上传需要一些时间。

![](img/9e30fda33bd8a3df024ee6e31eed1759.png)

一旦上传成功，你会看到它出现在屏幕上。现在，我们已经将数据存储在 S3 存储桶中。

![](img/00125626c3628ff742b235ec64be38f2.png)

现在，我们如何在 S3 之外获取这些数据？我们如何将它放入我们的 Databricks Spark 环境？一种方法是将 S3 存储桶权限设置为对所有人可见，这样任何人都可以访问它。更安全的方法是获得一个访问密钥 ID 和一个秘密的访问密钥，并在我们的 Spark 环境中使用它从 bucket 中提取数据。这就是我们在这里要做的。

*   要获取访问 ID 和密钥，请转到服务，搜索 IAM 并单击它。

![](img/38e00de5535a8fde2ddedc20f0bf4ab5.png)

*   到达后，在窗口左侧的菜单中点击“访问管理”下的“用户”选项。然后点击“添加用户”选项。

![](img/8b3788dc2ff681861a7d085b0658f550.png)

*   通过提供用户名来设置用户详细信息。然后选择访问类型为编程访问，以便获得访问密钥 ID 和密钥，它们可以在我们的 Spark 程序中用来访问来自 S3 存储桶的数据。然后点击“下一个权限”。

![](img/16168d8e0ba26fac6a728780dc867b59.png)

*   在这里，选择“直接附加现有保单”并搜索 S3。从显示的列表中，选择“AmazonS3FullAccess”选项以授予读写权限。对于只读权限，请选择“AmazonS3ReadOnlyAccess”选项。选择后，点击“下一个标签”。
*   跳过可选的“添加标签”步骤并继续。

![](img/b945820a2b6f858e30ab39a28352e48b.png)

*   查看一次，然后单击 Create user 以获取访问 ID 和密钥。

![](img/5121bcca49643726af62b33e4d5ea933.png)

在这里，您可以找到 S3 桶的访问密钥 ID 和秘密访问密钥，我们可以在 PySpark 程序中使用它们来访问数据。请注意，不应该共享秘密访问密钥，因为拥有秘密密钥的任何人都可以完全访问您的 S3 存储桶，并可以对其进行更改。

![](img/b2bd2cd1140d43972940a2b4fd4d060f.png)

虽然这只是为我们的管道获取数据的一种方式，但您没有必要这样做。如果您无法设置亚马逊 S3 存储桶，您完全可以将数据直接上传到 Databricks 环境，这是一种简单得多的方法。

## **将数据直接上传至 databricks 环境，无需 AWS(替代方法)**

*   正如我们已经看到的，在数据部分，当您单击“添加数据”时，您可以选择上传文件。选择该文件并浏览本地文件系统中要上传的文件。
*   上传后，单击 Create table with UI。然后选择要将数据附加到的群集。我将使用我们在开始时已经创建的集群。

![](img/c61b056163693312c91374981731bb01.png)

*   向下滚动，在“指定表属性”下，给出表名、文件类型、列分隔符(CSV 文件中为逗号)，选择“首行作为标题”、“推断模式”和“多行”，然后单击“创建表”。

![](img/1230d661f5a7f9f13d315803787f5283.png)

将显示该表的模式。

![](img/b54468e9db59a6d9bcbe6c278f3063be.png)

## 在 databricks 中创建新笔记本

现在让我们创建一个新的笔记本(类似于 Jupyter Notebbok ),以我们构建机器学习管道的 Spark 计划开始。

*   转到 Databricks 的主窗口，单击“新建笔记本”选项创建一个新笔记本。
*   为笔记本指定一个名称，选择语言(我们将使用 python ),然后选择要将笔记本连接到的集群。完成后，点击“创建”打开一个新笔记本。现在我们可以走了。

![](img/c0e860a35aed8de108c34addc0c1be13.png)

## 为 ML 管道编写我们的 Spark 程序

让我们开始编写我们的 Spark 程序。首先，让我们开始使用访问密钥 ID 和秘密访问密钥从 S3 存储桶导入数据。如果您已经在 Databricks 环境中直接上传了数据，而不是使用亚马逊 S3，那么跳过下面的代码片段，直接通过指定文件位置开始(参见下一个代码片段)。

当您运行该单元时，您将获得以下输出。

![](img/187d732fad523c277cf23e54bf4f9ea9.png)

***注意:*** *使用 Databricks notebook 时，不需要显式创建 SparkContext 或 SparkSession。请改为对 SparkContext 使用已定义的变量“sc”，对 SparkSession 使用“spark”，对 SQLContext 使用“sqlContext”。*

接下来，指定数据集位置和文件类型，并配置 CSV 选项，例如将数据集的第一行设置为标题、列分隔符、推断模式等。最后显示数据集。如果您已经在 databricks 环境中直接上传了数据集，那么首先将文件位置指定为“/file store/tables/bank marketing . CSV”

数据框被创建并显示，如图所示。

![](img/eef038c0944ffe677f63341454732017.png)

接下来，使用 *printSchema()* 打印 dataframe 的模式。

这里我们可以看到数据集有 17 个属性，并且显示了每个属性的数据类型。这里的'*存款*'是目标变量，有两个类别标签(是和否)，其余属性是自变量。

![](img/b339d5c7f607462858675d49b086a1bd.png)

数据帧的模式(输出)

然后，使用 *groupBy()* 和 *count()* 显示数据集中每个类标签的计数(Yes 和 No)，检查数据集是否平衡。

这给出了数据集中每个类标签下的示例数。我们可以看到，该数据集非常均衡，每个类标签中的实例数量几乎相等，5873 个实例的类标签为“否”，5289 个实例的类标签为“是”。

![](img/93c254257333107d0163226f1570e846.png)

正如我们在模式中已经看到的，该数据包含字符串和数字类型(整数)的变量。让我们使用 *dtypes()获取所有类型为 integer 的列。*

以下变量在数据帧中有数值。

![](img/78b18ff0fe38ff27eec8a0b346850707.png)

数字类型的列(输出)

***注:*** *我们也可以使用 to Pandas()将 Spark 数据帧转换为 Pandas 数据帧。我们可以使用 Pandas dataframe 获得更好的数据表格视图。*

*举例:*

```
df.select(numeric_features).toPandas().head(5)
```

![](img/402ba154378df5bcd7bba4299adffd70.png)

熊猫数据框的表格视图

除了数字类型列之外，其余的列都是分类变量。为了使它们适合模型学习，我们必须通过一键编码将分类变量转换为二进制表示。我们将首先使用 *StringIndexer* 将分类列转换为索引，然后将这些索引类别转换为一键编码变量。目标变量(标签)也使用 *StringIndexer* 编码成一个列向量。我们将分类变量的字符串索引器和一键编码器以及标签的字符串索引器附加到流水线阶段。

接下来，我们将使用 *VectorAssembler* 将一系列列组合成一个单独的向量列，将所有的独立变量转换成一个单独的特征向量。这将把自变量(特征)与目标变量(标签)分开。一个 *VectorAssembler* 接受数字、布尔和向量类型的输入列。这里，所有的独立变量(数字列+分类列)都被转换成一个称为 features 的向量。然后，这个向量汇编器被附加为流水线阶段。

一旦我们添加了管道的所有阶段，就该创建管道了。这里，我们用这些阶段创建管道，然后使管道适合数据帧并对其进行转换。我们将选择*‘标签’*(目标变量)和*‘特征’*(所有自变量的向量)作为数据帧的列。

![](img/be6b9c81118b98c10a6c250e71e511e2.png)

数据帧的模式(输出)

现在，模型的数据已经准备好了。让我们将其分为训练集和测试集，用于训练模型，然后在看不见的数据上测试模型。这里，我们以 80:20 的训练/测试分割比率分割数据。

现在是时候创建一个二元分类器，我们将选择逻辑回归模型进行分类。首先，我们将初始化模型，然后使其适合训练数据。

一旦在训练集上创建并拟合了模型，让我们通过绘制真阳性率对假阳性率来绘制该二元分类器的 ROC 曲线。我们通过把它转换成熊猫，然后进行绘图来做到这一点。

![](img/e7ee14395ebb442cdc9cbfbfc7b9d467.png)

现在，我们将使用两个性能指标，即准确性和 ROC 曲线下面积，来评估模型在训练数据上的性能。

![](img/933f1d1f201eff0191045c110d781b8a.png)

模型的训练精度为 0.8257，而 ROC 曲线下面积为 0.9033。但是根据训练数据评估模型性能并不是一个好主意。因此，我们将使用测试集进行预测，并在此基础上评估模型。

![](img/19f571a14c23fbcf4bf108071b501614.png)

模型的预测(输出)

![](img/315020fbc9fc52ec59af416c5c877ce9.png)

当对以前未见过的数据进行测试时，该模型的准确性证明为大约 82.4%，ROC 曲线下的面积为 0.9047。我们看到，模型在训练和测试数据上的性能没有太大差异。

**超参数调谐**

在构建逻辑回归模型时，我们为模型分配了默认的超参数。调整这些参数以找到最佳超参数可以极大地提高模型性能。为此，我们可以使用 *ParamGridBuilder* 和 *CrossValidator* 来执行网格搜索，这相当于 Sklearn 的 *GridSearchCV* 。

![](img/3ed1318a2733fea4cf2be9a8360cb1f4.png)

这是从指定超参数值的各种组合创建的最佳可能分类模型的 ROC 曲线下的面积。

该模型的性能还有改进的余地。您可以尝试通过执行更多的数据预处理步骤来改进模型，如缺失值插补、特征选择等。并将它们作为阶段添加到管道中。您可以在此访问此笔记本[。](https://github.com/Anveshrithaa/PySpark-ML-Pipeline)

在这篇博客中，我们学习了如何为我们的 Spark 应用程序使用基于云的统一分析平台，如何在亚马逊 S3 桶中存储数据并从 Spark 环境中访问数据，以及如何在 Spark 中构建端到端的机器学习管道。不仅仅是阅读，动手操作和摆弄它会让你有更好的理解。确保尝试为您选择的任何机器学习用例构建自己的管道。对数据进行探索性分析，对数据进行分析并做必要的数据预处理，最后建立并训练模型。最后，努力提高模型的性能。编码快乐！

查看本系列中的其他博客

[***第 1 部分 Apache Spark 入门***](/@anveshrithaas/getting-started-with-apache-spark-part-1-91b379204ae0)

[***第二部分 PySpark 简介***](/@anveshrithaas/introduction-to-pyspark-part-2-6d6113e31592)

[***第三部分—了解火花 RDDs***](/@anveshrithaas/understanding-spark-rdds-part-3-3b1b9331652a)

[***第四部分—用 PySpark 进行机器学习***](/@anveshrithaas/machine-learning-in-pyspark-part-4-5813e831922f)