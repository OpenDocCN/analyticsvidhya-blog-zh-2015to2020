<html>
<head>
<title>Evolution of NLP — Part 1 — Bag of Words, TF-IDF</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理的发展——第一部分——词汇袋，TF-IDF</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/evolution-of-nlp-part-1-bag-of-words-tf-idf-9518cb59d2d1?source=collection_archive---------6-----------------------#2020-07-23">https://medium.com/analytics-vidhya/evolution-of-nlp-part-1-bag-of-words-tf-idf-9518cb59d2d1?source=collection_archive---------6-----------------------#2020-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/61a2f5b1bdae27764ee0a8e58ea6f6ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUYcVRu4ivwN9y1JasXFww.png"/></div></div></figure><div class=""/><div class=""><h2 id="74ab" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">情感分类的NLP基础入门</h2></div><p id="694f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这是一系列博客中的第一篇，我试图谈论过去几年中自然语言处理<strong class="jk hu"><em class="ke"/></strong>任务建模技术的变化。从最基础的单词袋开始，我们接触到了当前的艺术水平(State变形金刚！我将尝试简单介绍一下算法本身，然后我们直接进入编码！我们将看到这一系列的帖子，NLP在这些年里发生了多大的变化，以及如何快速入门或赶上最新的变化。我希望你喜欢这次旅行；)</p><p id="9b03" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在这篇文章中，我们将重点关注使用简单的词袋和基于TF-IDF的模型，再加上系综决策树来获得最高的准确度分数！</p><p id="17f3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">你也可以找到这个关于Kaggle内核的教程—<a class="ae kf" href="https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf" rel="noopener ugc nofollow" target="_blank">NLP的进化—第1部分—单词包，TF-IDF </a>的完整代码！</p><h1 id="1ef8" class="kg kh ht bd ki kj kk kl km kn ko kp kq iz kr ja ks jc kt jd ku jf kv jg kw kx bi translated">理解数据</h1><p id="1ba7" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">我在这里使用了<a class="ae kf" href="https://datahack.analyticsvidhya.com/contest/janatahack-nlp-hackathon/True/#ProblemStatement" rel="noopener ugc nofollow" target="_blank"> JantaHack NLP Hackathon </a>数据集。该数据集主要由2015年至2019年期间收集的不同类型游戏的<strong class="jk hu"> Steam用户评论</strong>组成。这里的目标是根据用户评论来预测用户推荐还是不推荐游戏。所以，我们的目标本质上是<strong class="jk hu">情感分类。这将是我们整个系列的任务！</strong></p><p id="153e" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">综合训练和测试数据有25000多条评论。我们将只关注使用<em class="ke"> user_review </em>进行预测。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ld"><img src="../Images/b3987a5df7073aa93f3248db2296191f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kU1tJv4iOjEO91NAzMMGyg.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated"><a class="ae kf" href="https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf" rel="noopener ugc nofollow" target="_blank">用于我们分析的完整数据集—图片来自作者</a></figcaption></figure><p id="af78" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">让我们开始吧！对于这一系列教程，我将只使用评论，没有其他专栏。实际上，执行EDA可以更好地理解数据。</p><h1 id="b670" class="kg kh ht bd ki kj kk kl km kn ko kp kq iz kr ja ks jc kt jd ku jf kv jg kw kx bi translated">使用单词包、N-Grams、TF-IDF</h1><p id="ec89" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">下面的方法基本上涵盖了所有尝试NLP的人最先使用的一些工具。而且，随着时间的推移，出现了很多很多的库，比如SpaCy和NLTK，它们极大地简化了这种方法的使用。还有像Textblob这样的库，它们站在强大的NLTK的肩膀上，提供了一个更好更快的接口来执行许多NLTK操作等等。</p><p id="5420" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我将试着对这些方法和库做一个快速的概述，但是，我建议去每个库的网站(附在下面)了解它们的完整功能。</p><h1 id="4a2e" class="kg kh ht bd ki kj kk kl km kn ko kp kq iz kr ja ks jc kt jd ku jf kv jg kw kx bi translated">第一步。预处理</h1><p id="8458" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">清理用户评论！</p><ol class=""><li id="5bf3" class="lm ln ht jk b jl jm jo jp jr lo jv lp jz lq kd lr ls lt lu bi translated"><strong class="jk hu"> decontracted </strong>函数会将一般短语的简短形式转换成它们的较长版本</li><li id="c9cb" class="lm ln ht jk b jl lv jo lw jr lx jv ly jz lz kd lr ls lt lu bi translated"><strong class="jk hu"> lemmatize_with_postag </strong>函数将单词还原为它们的基本形式。我在这里使用了TextBlob库的实现，它构建在NLTK之上。请随意尝试其他功能。主要思想是减少额外的词汇——这不仅有助于计算，也有助于减少对某些关键词的过度拟合。还有其他类似词干化的方法，但一般不如词干化。</li><li id="aebf" class="lm ln ht jk b jl lv jo lw jr lx jv ly jz lz kd lr ls lt lu bi translated">进一步清理删除链接，标点符号等。</li></ol><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ma"><img src="../Images/f0a95bd07a0f26704483b7031272eca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPrVGMcVlAr59P8hRTMM0A.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated"><a class="ae kf" href="https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf" rel="noopener ugc nofollow" target="_blank">预处理前后—图片来自作者</a></figcaption></figure><h1 id="28e7" class="kg kh ht bd ki kj kk kl km kn ko kp kq iz kr ja ks jc kt jd ku jf kv jg kw kx bi translated">第二步。为ML构建数据</h1><h2 id="4b4c" class="mb kh ht bd ki mc md me km mf mg mh kq jr mi mj ks jv mk ml ku jz mm mn kw mo bi translated">使用计数矢量器(单词袋)</h2><p id="438e" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">简而言之，单词包表示某个单词在评论中出现的次数，不管它的顺序如何。要做到这一点，首先我们要为评论中出现的所有单词(或标记)创建一个字典(或词汇表)。词汇表中的每个标记随后被转换为一个列，它的<em class="ke"> </em> <strong class="jk hu"> <em class="ke">行【j】</em></strong>指示— <em class="ke">“该标记在</em><strong class="jk hu"><em class="ke">review【j】</em></strong><em class="ke">中出现了多少次？”</em></p><p id="a057" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我在下面使用了一个scikit-learn实现，但是也有其他库可以很好地处理这个问题。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mp"><img src="../Images/ef56dc817ad3b9b79cf600f728d66b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ilBY8juxaeM0TS5Ix_rLmw.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated"><a class="ae kf" href="https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf" rel="noopener ugc nofollow" target="_blank">使用词汇袋后的最终数据集——图片来自作者</a></figcaption></figure><p id="0d26" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">列名代表单词，行代表单个句子。如果句子中存在该标记，则相应列将具有值1，否则为0。</p><h2 id="1bb3" class="mb kh ht bd ki mc md me km mf mg mh kq jr mi mj ks jv mk ml ku jz mm mn kw mo bi translated">使用N-gram</h2><p id="4e1a" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">然而，有时重要的是单词的组合，而不仅仅是单词本身。示例—“不好”和“好”对于“好”令牌具有相同的标志。因此，在我们的语料库中找到这些可能影响评论整体意义的短语变得很重要。这就是我们所说的N-grams <br/>然而，寻找这些的成本随着词汇大小(v)的增加而多项式增长，因为本质上我们正在寻找潜在的<strong class="jk hu"> 𝑂(𝑉*𝑉) </strong>更坏的短语组合(其中v是词汇量)。</p><p id="7814" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在我们的实现中，我们限制为2和3克。我们进一步选择总共前3000个特征，根据它们在数据中的出现进行排序。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mq"><img src="../Images/796ec62676f1942e67e64eededb2513a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDD-9WM-WaftsgG1qgjULw.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated"><a class="ae kf" href="https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf" rel="noopener ugc nofollow" target="_blank">使用N元单词袋后的最终数据集—图片来自作者</a></figcaption></figure><p id="d3e4" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">与前面的想法相同，但是，这次我们在句子中寻找特定的n-gram序列。</p><h2 id="14e8" class="mb kh ht bd ki mc md me km mf mg mh kq jr mi mj ks jv mk ml ku jz mm mn kw mo bi translated">使用TF-IDF(术语频率-逆文档频率)</h2><p id="01ac" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">现在，如果您想知道什么是术语频率，它是一个词在文档中的相对频率，给出为(术语实例数/总实例数)。逆文档频率是包含该术语的文档的相对计数，以log(文档数/包含该术语的文档数)的形式给出。每个单词对其出现的文档的总体重要性等于<strong class="jk hu"> TF * IDF </strong></p><p id="0d5c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这将为您提供一个矩阵，其中每一列代表词汇表中的一个单词(至少在一个文档中出现的所有单词)，每一行代表一篇评论，如前所述。这样做是为了降低评论中频繁出现的词的重要性，从而降低它们在评论的总体评分中的重要性。</p><p id="e215" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">幸运的是，scikit-learn为您提供了一个内置的<strong class="jk hu"> TfIdfVectorizer </strong>类，它可以用几行代码生成TF-IDF矩阵。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mr"><img src="../Images/7e296fede25f9c1c5c518749957cb32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0RqDlj3Y7zZhvJ6xLcB5GQ.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated"><a class="ae kf" href="https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf" rel="noopener ugc nofollow" target="_blank">使用TF-IDF后的最终数据集—图片来自作者</a></figcaption></figure><p id="6de8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">同样，行构成句子，单个标记代表列。如果单词出现在句子中，基于上面讨论的方法，计算TFIDF得分并填入相应的列，否则值为0。</p><p id="5371" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">类似于<strong class="jk hu">计数矢量器</strong>，我们在这里也使用n元语法。</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mq"><img src="../Images/90797c4afa8e4faf40119daf0c855704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOfp4ao8gQFmTtbbk1Kcyg.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated"><a class="ae kf" href="https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf" rel="noopener ugc nofollow" target="_blank">使用N-grams TF-IDF后的最终数据集—图片来自作者</a></figcaption></figure><h1 id="6240" class="kg kh ht bd ki kj kk kl km kn ko kp kq iz kr ja ks jc kt jd ku jf kv jg kw kx bi translated">第三步。建模</h1><p id="9b21" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">让我们首先尝试一下只有TF-IDFs的数据集。</p><h2 id="e18e" class="mb kh ht bd ki mc md me km mf mg mh kq jr mi mj ks jv mk ml ku jz mm mn kw mo bi translated">使用LGBM进行培训</h2><p id="40f5" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">在很多任务中，我使用了轻量级GBM，因为它的高精度和速度，以及其他基于树的集合模型。请随意在此进行实验。XGBoost是一个不错的选择！</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ms"><img src="../Images/5969d926a0a0061bf050e02a083d3ed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pqlAs8UlOwf_lng0OhjRvQ.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated"><a class="ae kf" href="https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf" rel="noopener ugc nofollow" target="_blank">初始化和训练灯光GBM —图片来自作者</a></figcaption></figure><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mt"><img src="../Images/8f9add1fbdc15e88f64345a0a69352ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlLQ7RkU3Isb7E2yruKiBg.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated"><a class="ae kf" href="https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf" rel="noopener ugc nofollow" target="_blank">TF-IDF数据集上LightGBM的结果—图片来自作者</a></figcaption></figure><blockquote class="mu"><p id="13f3" class="mv mw ht bd mx my mz na nb nc nd kd dx translated">使用TF-IDF方法——结合单词标记和n-grams，我们得到大约83.3%的分数<strong class="ak"/></p></blockquote><p id="08ba" class="pw-post-body-paragraph ji jj ht jk b jl ne iu jn jo nf ix jq jr ng jt ju jv nh jx jy jz ni kb kc kd hb bi translated">现在，让我们用CountVectorizer数据集检查结果</p><figure class="le lf lg lh fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nj"><img src="../Images/6e649b362af05e96cc93a5710e3ec577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*keYU3EG3dKtzbLe9O7x_ig.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated"><a class="ae kf" href="https://www.kaggle.com/jainkanishk95/evolution-of-nlp-part-1-bag-of-words-tf-idf" rel="noopener ugc nofollow" target="_blank">单词袋数据集上LightGBM的结果—图片来自作者</a></figcaption></figure><blockquote class="mu"><p id="9128" class="mv mw ht bd mx my mz na nb nc nd kd dx translated">使用单词袋方法——结合单词标记和n-grams，我们得到了大约82.7%的分数</p></blockquote><h2 id="bfc3" class="mb kh ht bd ki mc nk me km mf nl mh kq jr nm mj ks jv nn ml ku jz no mn kw mo bi translated">准确度— 83.3%</h2><p id="105a" class="pw-post-body-paragraph ji jj ht jk b jl ky iu jn jo kz ix jq jr la jt ju jv lb jx jy jz lc kb kc kd hb bi translated">这是我们的基线分数！基于一袋话和TF-IDF，这是我们能得到的最好的。让我们尝试更强大的技术来进一步改进它！希望它能帮助你更好地理解TF-IDF和计数矢量器以及它在Python中的实现。</p><p id="bfab" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">有很好的在线资源，尤其是在<strong class="jk hu">数据营</strong>，那里的在线互动课程涵盖了我们在本笔记本中讨论的相同技术。请随意查看它们以获得更多的理解！在本系列的<a class="ae kf" rel="noopener" href="/@jainkanishk001/evolution-of-nlp-part-2-recurrent-neural-networks-af483f708c3d">第二部分——递归神经网络</a>中再见。</p></div></div>    
</body>
</html>