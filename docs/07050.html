<html>
<head>
<title>Machine Learning: Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:决策树</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-decision-trees-78d9eb31c42a?source=collection_archive---------27-----------------------#2020-06-11">https://medium.com/analytics-vidhya/machine-learning-decision-trees-78d9eb31c42a?source=collection_archive---------27-----------------------#2020-06-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3b92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个博客涵盖了另一个有趣的机器学习算法，称为决策树及其数学实现。</p><p id="fad5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在生活的每一个点上，我们都会做出一些决定来继续前进。同样，这种机器学习算法也会对提供的数据集做出相同的决策，并在每一步找出最佳的分割或决策，以提高准确性，做出更好的决策。这反过来有助于给出有价值的结果。</p><blockquote class="jd je jf"><p id="06f1" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">决策树是一种机器学习算法，它表示数据集的分层划分，以基于某些参数形成一棵树。</p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jk"><img src="../Images/7bfa5baa00b91d4c4ddd4c13b0f228aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-DtYJBHD1XWeH2uEssPLw.jpeg"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">决策树的示例</figcaption></figure><p id="9b41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们讨论一些与决策树相关的基本术语。</p><ol class=""><li id="b1f4" class="ka kb hi ih b ii ij im in iq kc iu kd iy ke jc kf kg kh ki bi translated">根节点:第一次分裂发生的起始节点称为根节点。换句话说，最顶端的节点被称为根节点。在上图中，“要做的工作”是根节点。</li><li id="8276" class="ka kb hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">内部节点:表示对属性进行测试的节点称为内部节点。它不分类也不持有阶级标签。它有助于进一步分裂，以实现叶节点。上图中，“Outlook？”是内部节点。</li><li id="9dd0" class="ka kb hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">叶节点:持有类标签的节点称为叶节点。在此节点之后，不能再进行进一步的拆分。</li><li id="2400" class="ka kb hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">分支:决策树中的分支代表在内部节点上进行的测试的结果。</li></ol><h1 id="005a" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">数学实现</h1><ol class=""><li id="943d" class="ka kb hi ih b ii lm im ln iq lo iu lp iy lq jc kf kg kh ki bi translated"><strong class="ih hj">熵</strong>:熵是指数据中的随机程度。所以，对于一个特定的节点，不管是根节点还是内部节点，我们都会计算熵。计算熵的公式是:</li></ol><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lr"><img src="../Images/70b3f13984a0c9bc000c471bc011b8d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*7BY3p6J9dbVw9BaXD4P3cg.png"/></div></figure><p id="ea52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑二元分类“是/否”。对于一个节点，有9个是和5个否。所以，熵是:</p><p id="d2d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">E =-(9/14)log(9/14)—(5/14)log(5/14)<br/>E = 0.94</p><p id="98d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以看出这是一种高度的随机性。熵的范围从0到1。所以，如果熵为0，那就是纯除法，如果熵为1，那就是不纯除法。熵应该尽可能低。</p><p id="9ce7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">信息增益</strong>:简单来说，信息增益就是比较拆分前后样本的熵。计算特定节点的信息增益的公式是:</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ls"><img src="../Images/835c1836ca9fa3553536c80ed55eeac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*360RxUgycSf63SX_mSrOkg.png"/></div></figure><p id="55cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，该算法计算所有可能分裂的信息增益，并找出最佳分裂。信息增益最高的部门是最好的。</p><p id="0693" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.<strong class="ih hj">基尼杂质</strong>:基尼杂质与熵相同，用于计算分裂的纯度。大多数情况下，基尼系数优于熵，因为基尼系数易于计算，基尼系数的取值范围为0-0.5。计算基尼系数的公式是:</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es lt"><img src="../Images/4f47b95fe7a62d66ce198279e578038f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*anRagu5fVuKygIq4hb3Bsw.png"/></div></div></figure><p id="d976" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从下图可以看出基尼系数杂质和熵的对比。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es lu"><img src="../Images/61134f09fd54759867d51e4427a4260f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rUNu0cuMydaw0aAvLLj4lQ.png"/></div></div></figure><p id="430b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，决策树需要借助熵或基尼系数来计算总体信息增益。因为基尼系数在计算上是有效的，所以它是优选的。</p></div></div>    
</body>
</html>