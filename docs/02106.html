<html>
<head>
<title>Reading large Datasets using pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用pandas读取大型数据集</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reading-large-datasets-using-pandas-ec2d99465887?source=collection_archive---------2-----------------------#2019-12-01">https://medium.com/analytics-vidhya/reading-large-datasets-using-pandas-ec2d99465887?source=collection_archive---------2-----------------------#2019-12-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5c3f62da53badbca47096c850bf6b2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QNj1qKHgorPg0w2SlrAz9g.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@technobulka?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">斯坦尼斯拉夫·康德拉蒂耶夫</a>在<a class="ae iu" href="https://unsplash.com/s/photos/library?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="170e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在数据科学中，我们可能会遇到需要读取大于系统内存的大型数据集的情况。在这种情况下，您的系统将在读取如此大量的数据时耗尽RAM/内存。这也可能导致jupyter笔记本的内核关闭或系统崩溃。为了避免这种情况，有一些非常好的技术可以帮助我们读取大型数据集。</p><p id="092d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于任何数据科学家来说，阅读CSV文件的python包的明显选择是pandas。在这篇博客中，我们将揭示一些很酷的技术来阅读这些数据集。</p><p id="e9cb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们深入研究这些技术之前，最好先设置好我们的环境并做好准备:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="5f84" class="kc kd hi jy b fi ke kf l kg kh">conda install -c conda-forge jupyterlab<br/>conda install -c anaconda pandas<br/>jupyter notebook</span></pre><h1 id="d20a" class="ki kd hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">数据</strong></h1><p id="47e8" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">我使用的是<a class="ae iu" href="https://www.kaggle.com/c/pubg-finish-placement-prediction" rel="noopener ugc nofollow" target="_blank"> PUBG完成布局预测(仅内核)</a> kaggle竞赛的数据集。我们将使用训练数据集进行分析。训练集包含440万行，总计700 MB数据！</p><h1 id="3d80" class="ki kd hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">方法</strong></h1><h2 id="d210" class="kc kd hi bd kj lk ll lm kn ln lo lp kr jg lq lr kv jk ls lt kz jo lu lv ld lw bi translated">使用正常的pandas方法读取数据集</h2><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="d563" class="kc kd hi jy b fi ke kf l kg kh">&gt;&gt;&gt;&gt; pd.read_csv('train_V2.csv')</span></pre><p id="2b9c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是读取csv文件的标准方法。现在让我们看看这段代码执行所花费的时间。使用<a class="ae iu" href="https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html" rel="noopener ugc nofollow" target="_blank"> IPython的神奇命令</a>:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="6c0c" class="kc kd hi jy b fi ke kf l kg kh">&gt;&gt;&gt;&gt; %time pd.read_csv('train_V2.csv')<a class="ae iu" href="https://gist.github.com/raisedrussian/b009caee3f117c65487452567affde99" rel="noopener ugc nofollow" target="_blank"><br/></a>CPU times: user 16.5 s, sys: 1.33 s, total: 17.8 s<br/>Wall time: 27.5 s</span></pre><p id="9996" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们花了大约27秒来阅读我们的训练集。我们仍然可以提高速度，减少停留时间。</p><p id="adde" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了计算这种方法消耗的内存量，我们可以使用IPython的<code class="du lx ly lz jy b"><a class="ae iu" href="https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html" rel="noopener ugc nofollow" target="_blank">memory_profiler</a></code>扩展中的一个神奇命令，名为<code class="du lx ly lz jy b">%memit</code></p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="18c8" class="kc kd hi jy b fi ke kf l kg kh">&gt;&gt;&gt;&gt; %memit pd.read_csv('train_V2.csv')</span><span id="5dda" class="kc kd hi jy b fi ma kf l kg kh">peak memory: 3085.73 MiB, increment: 3001.68 MiB</span></pre><p id="f47e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里的内存峰值表示<code class="du lx ly lz jy b">read_csv</code>功能消耗的内存量。对于700 MiB的训练数据，需要大约3gb的内存！让我们看看下一个方法消耗了多少内存。</p><h1 id="f22b" class="ki kd hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">使用数据类型修改方法从数据集读取数据</strong></h1><p id="1d27" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">这种方法只需要将数据集每一列的数据类型更改为占用内存较少的数据类型。在我们的示例中，让我们首先列出训练集中的所有数据类型。</p><p id="f799" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们首先需要知道训练集的当前数据类型是什么。我们不会再用传统的方法阅读训练集。但是我们将从训练集中抽取一些样本，并将其保存在另一个文件中。为此，请在jupyter笔记本单元格中键入以下命令，</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a549" class="kc kd hi jy b fi ke kf l kg kh">!head train_V2.csv &gt; sample_train.csv</span></pre><p id="4643" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du lx ly lz jy b">head</code>命令将打印<code class="du lx ly lz jy b">train_V2.csv</code>文件的前10行，并将其标准输出添加到<code class="du lx ly lz jy b">sample_train.csv</code>。现在我们已经得到了子样本训练集，因此我们现在可以通过普通的<code class="du lx ly lz jy b">pandas.read_csv</code>函数轻松读取数据集:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="8776" class="kc kd hi jy b fi ke kf l kg kh">sample_train = pd.read_csv('sample_train.csv')<br/>dict(zip(sample_train.columns,sample_train.dtypes))</span></pre><p id="4d20" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后一行将根据各自的数据类型给出列名的字典:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="cb46" class="kc kd hi jy b fi ke kf l kg kh">{'Id': dtype('O'),<br/> 'groupId': dtype('O'),<br/> 'matchId': dtype('O'),<br/> 'assists': dtype('int64'),<br/> 'boosts': dtype('int64'),<br/> 'damageDealt': dtype('float64'),<br/> 'DBNOs': dtype('int64'),<br/> 'headshotKills': dtype('int64'),<br/> 'heals': dtype('int64'),<br/> 'killPlace': dtype('int64'),<br/> 'killPoints': dtype('int64'),<br/> 'kills': dtype('int64'),<br/> 'killStreaks': dtype('int64'),<br/> 'longestKill': dtype('float64'),<br/> 'matchDuration': dtype('int64'),<br/> 'matchType': dtype('O'),<br/> 'maxPlace': dtype('int64'),<br/> 'numGroups': dtype('int64'),<br/> 'rankPoints': dtype('int64'),<br/> 'revives': dtype('int64'),<br/> 'rideDistance': dtype('float64'),<br/> 'roadKills': dtype('int64'),<br/> 'swimDistance': dtype('float64'),<br/> 'teamKills': dtype('int64'),<br/> 'vehicleDestroys': dtype('int64'),<br/> 'walkDistance': dtype('float64'),<br/> 'weaponsAcquired': dtype('int64'),<br/> 'winPoints': dtype('int64'),<br/> 'winPlacePerc': dtype('float64')}</span></pre><p id="7ce7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一般来说，每当数据集被加载并且pandas找到任何数字列时，默认情况下它被分配给int64或float64数据类型。由于缺省值是64位数据类型，所以您可以想象，对于超过数百万行的数据集，它将占用多么巨大的空间。为了便于理解，下表将显示int和float数据类型允许的值范围:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="ab3f" class="kc kd hi jy b fi ke kf l kg kh">int8<!-- -->  Byte    (-128 to 127)<br/>int16<!-- --> Integer (-32768 to 32767)   <br/>int32<!-- --> Integer (-2147483648 to 2147483647)   <br/>int64<!-- --> Integer (-9223372036854775808 to 9223372036854775807)<br/>float32 Half precision<br/>float64 Full precision</span></pre><p id="da67" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有关pandas中使用的数据类型的更多信息，请参考<a class="ae iu" href="https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html#basics-dtypes" rel="noopener ugc nofollow" target="_blank"> pandas </a>和<a class="ae iu" href="https://docs.scipy.org/doc/numpy/user/basics.types.html" rel="noopener ugc nofollow" target="_blank"> numpy </a>文档。</p><p id="61f0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">很明显，较大的数据类型将消耗更多的空间。在我们的场景中，有<code class="du lx ly lz jy b">19 int64, 6 float64 and 4 object</code> <em class="mb"> </em>数据类型列。现在让我们创建一个字典，它将包含<code class="du lx ly lz jy b">int16 and float16</code> <em class="mb"> </em>作为所有整数和浮点列的数据类型。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="bd5b" class="kc kd hi jy b fi ke kf l kg kh">dtype_list = list()<br/>for x in sample_train.dtypes.tolist():<br/>    if x=='int64':<br/>        dtype_list.append('int16')<br/>    elif(x=='float64'):<br/>        dtype_list.append('float16')<br/>    else:<br/>        dtype_list.append('object')<br/>        <br/>dtype_list = dict(zip(sample_train.columns.tolist(),dtype_list))<br/>dtype_list</span></pre><p id="4b8b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面的代码片段将帮助我们获得类似于下面的数据类型字典:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="8f1d" class="kc kd hi jy b fi ke kf l kg kh">{'Id': 'object',<br/> 'groupId': 'object',<br/> 'matchId': 'object',<br/> 'assists': 'int16',<br/> 'boosts': 'int16',<br/> 'damageDealt': 'float16',<br/> 'DBNOs': 'int16',<br/> 'headshotKills': 'int16',<br/> 'heals': 'int16',<br/> 'killPlace': 'int16',<br/> 'killPoints': 'int16',<br/> 'kills': 'int16',<br/> 'killStreaks': 'int16',<br/> 'longestKill': 'float16',<br/> 'matchDuration': 'int16',<br/> 'matchType': 'object',<br/> 'maxPlace': 'int16',<br/> 'numGroups': 'int16',<br/> 'rankPoints': 'int16',<br/> 'revives': 'int16',<br/> 'rideDistance': 'float16',<br/> 'roadKills': 'int16',<br/> 'swimDistance': 'float16',<br/> 'teamKills': 'int16',<br/> 'vehicleDestroys': 'int16',<br/> 'walkDistance': 'float16',<br/> 'weaponsAcquired': 'int16',<br/> 'winPoints': 'int16',<br/> 'winPlacePerc': 'float16'}</span></pre><p id="9da3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们已经有了包含较小数据类型的自定义字典，让我们看看如何在<code class="du lx ly lz jy b">read_csv</code> <em class="mb"> </em>函数中包含这个数据类型字典，以便pandas按照我们想要的方式读取我们的训练集，即包含较小数据类型的列。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="e762" class="kc kd hi jy b fi ke kf l kg kh">&gt;&gt;&gt;&gt; %time pd.read_csv('train_V2.csv',dtype=dtype_list)</span><span id="6111" class="kc kd hi jy b fi ma kf l kg kh">CPU times: user 13.4 s, sys: 667 ms, total: 14.1 s<br/>Wall time: 16.6 s</span></pre><p id="04cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">读取整个训练数据集大约需要16.6秒。如果我们将这个时间与我们的普通方法进行比较，那么读取训练数据集的速度大约增加了<strong class="ix hj">40%</strong>。</p><p id="924b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在是我们使用前面方法中相同的神奇命令再次计算内存消耗的时候了:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="14c4" class="kc kd hi jy b fi ke kf l kg kh">&gt;&gt;&gt;&gt; %memit pd.read_csv('train_V2.csv',dtype=dtype_list)</span><span id="c770" class="kc kd hi jy b fi ma kf l kg kh">peak memory: 1787.43 MiB, increment: 1703.09 MiB</span></pre><p id="5293" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，这种方法消耗了大约一半的内存，与我们的传统方法相比，这是非常好的。</p><h1 id="e726" class="ki kd hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated"><strong class="ak">使用块创建方法读取csv </strong></h1><p id="4452" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">该方法包括使用<code class="du lx ly lz jy b">read_csv</code> <em class="mb"> </em>函数中的<code class="du lx ly lz jy b">chunksize</code> <em class="mb"> </em>参数读取数据。让我们创建一个块大小，以便通过以下方法读取数据集:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="67ce" class="kc kd hi jy b fi ke kf l kg kh">&gt;&gt;&gt;&gt; chunk_size = 10**6<br/>&gt;&gt;&gt;&gt; chunk_size<br/>1000000</span></pre><p id="7ff6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们将数据集分成1000000个块。因此，我们的数据集将被分成大小为1000000的4个块，最后一个块的大小由熊猫智能计算。现在让我们计算一下这种方法需要多少时间:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="4f9a" class="kc kd hi jy b fi ke kf l kg kh">&gt;&gt;&gt;&gt; %timeit [chunk for chunk in pd.read_csv('train_V2.csv',chunksize=chunk_size)]</span><span id="af13" class="kc kd hi jy b fi ma kf l kg kh">29.4 s ± 2.26 s per loop (mean ± std. dev. of 7 runs, 1 loop each)</span></pre><p id="2be7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，对于每个数据块，加载数据需要大约30秒，即所有5个数据块总共需要<strong class="ix hj"> 150秒</strong>。</p><p id="bfa9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，此方法消耗的内存是:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="db98" class="kc kd hi jy b fi ke kf l kg kh">&gt;&gt;&gt;&gt; %memit [chunk for chunk in pd.read_csv('train_V2.csv',chunksize=chunk_size)]</span><span id="8b76" class="kc kd hi jy b fi ma kf l kg kh">peak memory: 1966.75 MiB, increment: 1880.20 MiB</span></pre><p id="a339" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">比较所有方法消耗的时间和内存:</p><div class="jt ju jv jw fd ab cb"><figure class="mc ij md me mf mg mh paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/38f7204655fac9665df854f5cf3b7423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*KV7NiNiBJuND9tNQfWByKw.png"/></div></figure><figure class="mc ij mi me mf mg mh paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/41688b1508e4f263d5cf81202debdacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*NC3TuxxJIsOGIcuBde0PSw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx mj di mk ml translated">越低越好</figcaption></figure></div><h1 id="bbdc" class="ki kd hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">结论</h1><p id="279d" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">一般来说，这完全取决于您试图读取的数据集。方法2是我们讨论过的在读取时改变数据帧中列的数据类型的方法，它肯定可以用在系统资源不足的情况下或者在竞争中。但我认为这种方法有一个警告，即当我们从大数据集转移到小数据集时，我们正在缩小可接受的数字范围，因此可能会有一些数据丢失。如果速度不是那么重要，并且您希望保持数据的完整性，那么方法3是非常可取的。</p><p id="0e62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢fast.ai 的<a class="ae iu" href="https://course18.fast.ai/lessonsml1/lesson3.html" rel="noopener ugc nofollow" target="_blank">机器学习入门课程和他们的</a><a class="ae iu" href="http://forums.fast.ai" rel="noopener ugc nofollow" target="_blank">牛逼论坛</a>激励我写这篇博客。</p><p id="f7ec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以在这篇博文中找到我的代码:</p><p id="dd67" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://github.com/keyurparalkar/Blogpost_excerpts/blob/master/Blogpost_0%20-%20Reading%20large%20Datasets%20using%C2%A0pandas.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/keyurparalkar/Blogpost _ excepts/blob/master/Blogpost _ 0% 20-% 20 reading % 20 large % 20 datasets % 20 using % C2 % A0 pandas . ipynb</a></p><p id="685d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请在评论区告诉我你有多喜欢这篇博文，或者你可以通过我的twitter账号<a class="ae iu" href="https://twitter.com/keurplkar" rel="noopener ugc nofollow" target="_blank"> @keurplkar </a>向我寻求建设性反馈</p></div></div>    
</body>
</html>