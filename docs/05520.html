<html>
<head>
<title>Deep Residual Learning for Image Recognition (ResNet)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于图像识别的深度残差学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-residual-learning-for-image-recognition-resnet-94a9c71334c9?source=collection_archive---------3-----------------------#2020-04-24">https://medium.com/analytics-vidhya/deep-residual-learning-for-image-recognition-resnet-94a9c71334c9?source=collection_archive---------3-----------------------#2020-04-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="cd0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本节中，我们将尝试了解与ResNet体系结构相关的一些基本概念，为什么它比VGG网络更好，它的工作原理及其优点。</p><p id="8f33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在试图理解什么是ResNet之前，首先让我们简单地理解什么是T2 VGG网以及为什么ResNet比VGG网好。</p><h1 id="4e61" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">了解VGG网(VGG 16):</h1><p id="f4d0" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">VGG代表牛津大学的<strong class="ih hj">视觉几何小组</strong>。它是由Simonyan和Zisserman为<strong class="ih hj"> ILSVRC (Image Net大规模视觉识别挑战赛)</strong> 2014比赛开发的。它由16个权重层(13个卷积层和3个全连接层)组成，仅具有3×3特征检测器或滤波器。这种架构的误差百分比约为7%。</p><p id="0e5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">VGG网络背后的概念类似于Alexnet，这意味着，随着网络深度的增加，我们将增加特征图或卷积的数量。简而言之，随着我们深入网络，特征地图的数量增加，因此网络变得更广。总共有<strong class="ih hj">1.38亿个</strong>参数。</p><p id="0278" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">VGG 16网络架构如下所示:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/146286df4bc5c0d377ea962fbf09fe62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Lg1i7wv1pLpzp2F4MLrvw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">VGG-16网络架构</figcaption></figure><p id="4db7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来源:<a class="ae kw" href="https://www.researchgate.net/figure/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only_fig3_322512435" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/Fig/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-The-only _ Fig 3 _ 322512435</a></p><h1 id="d356" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">VGG网络的问题以及我们为什么需要该网络:</h1><p id="e2f2" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">但是这个架构有一个问题。该架构中有许多层，因此有大量的参数。这增加了模型的复杂性。此外，随着神经网络深度的增加，精确度会达到饱和，然后在某个点之后开始快速下降。出乎意料的是，这种退化不是由过度拟合引起的。向这些深度模型添加越来越多的层会导致更高的训练误差，正如论文<a class="ae kw" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">中提到的用于图像识别的深度残差学习</a>中的实验所测试的。</p><p id="4baa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，简而言之，随着我们向深层神经网络添加更多层，训练误差会增加。使用下图可以理解这一点:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kx"><img src="../Images/74c1da233dea29451e0f1335a475f473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n_VSziXG6EbDjWHV82nW2Q.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">堕落</figcaption></figure><p id="5561" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个问题是<strong class="ih hj">消失梯度。</strong></p><p id="ace7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，什么是<strong class="ih hj">消失渐变呢？</strong>嗯，随着我们继续使用一些激活函数向我们的神经网络添加更多层，损失函数的梯度趋于零，这有效地防止了权重改变其值，从而使网络难以训练。在<strong class="ih hj">反向传播</strong>阶段，计算误差并确定梯度值。渐变被发送回隐藏层，并且权重被相应地更新。这个过程一直持续到到达输入层。当到达输入层时，梯度变得越来越小。因此，初始层的权重要么更新非常慢，要么保持不变。换句话说，网络的初始层不会有效地学习。因此，深度神经网络将发现难以收敛，这将妨碍模型的准确性，因此训练误差增加。这个问题在很大程度上通过标准化初始化来解决，即标准化网络的初始权重和<strong class="ih hj">批量标准化</strong>。</p><h1 id="855d" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">批量标准化:</h1><p id="17a1" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">这是一种用于训练非常深的神经网络的技术，该网络对每个小批量的层的输入进行标准化。这具有稳定学习过程和显著减少训练深度网络所需的训练时期的效果。这种方法允许我们不必太在意初始化。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ky"><img src="../Images/12568c3af88bbcd4f6da7e33c781a2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*RC7ChedkwnEVP8f9KjVTYA.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">批量标准化</figcaption></figure><h2 id="f45d" class="kz je hi bd jf la lb lc jj ld le lf jn iq lg lh jr iu li lj jv iy lk ll jz lm bi translated">还没明白吗？让我们试着用不同的方式来理解它…</h2><p id="bddc" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">它也可以定义为一种标准化网络输入的技术，应用于前一层的激活或直接输入。在这种方法中，我们确保每一层的预激活都是单位高斯分布。也就是说，我们通过减去平均值并除以标准差来标准化输入，从而使平均值和单位方差为零。因此，我们将在网络的每一层都有这种分布。因此，现在每批数据都来自相同的分布，即使它来自不同的远距离分布。那么我们如何计算这个均值和方差呢？答案就在题目本身'<strong class="ih hj">批量标准化</strong>，即我们取当前批量的均值和方差，进行标准化。</p><p id="03b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过批量标准化，我们可以确信我们的激活在隐藏层上的分布是相当相似的。如果这是真的，那么我们知道梯度应该有更宽的分布，而不是几乎全为零。</p><p id="f3a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是我们仍然有退化的问题。我们需要克服这一点，对不对？是的，当然！这就是我们使用<strong class="ih hj">深度剩余学习</strong>框架的原因。</p><h1 id="e1e9" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">重量初始化:</h1><p id="27a6" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">训练深度神经网络是复杂的，因为在训练期间，随着前几层的参数改变，每层输入的分布也改变。由于需要较低的学习率和仔细的参数初始化，这减慢了训练，并且使得训练具有饱和非线性的模型变得非常困难。</p><p id="a22d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练深度神经网络时，我们在开始时初始化的权重在训练网络的效率和准确性方面起着重要作用。权重不应被初始化为零，因为如果神经元的权重被初始化为相同，则神经元之间不存在不对称性。这是不可接受的。那么我们就来看看两个最流行的<strong class="ih hj">权重初始化(Xavier和何等人)</strong>技术:</p><ol class=""><li id="9f94" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated"><strong class="ih hj">泽维尔/格拉罗初始化:</strong></li></ol><p id="19a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Xavier初始化通过从具有零均值和特定方差的分布中提取权重来初始化网络中的权重。它通常与tanh激活一起使用。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lx"><img src="../Images/488bd2f52c2ad65fb1248d25666a9047.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*61lspEI3qhICDcJBhle_bw.png"/></div></figure><p id="a1f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，fan_in为输入数量。</p><p id="41ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。何初始化:</strong></p><p id="1c95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这类似于Xavier初始化，其系数乘以2。为了更快更有效地获得代价函数的全局最小值，在初始化权重时要记住前一层的大小。这导致受控的初始化，结果是更快和更有效的梯度下降。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ly"><img src="../Images/30fc0b35719a156092fe8ed134b632a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*Kmn-gN1NUkVdKAEbTIC-rQ.png"/></div></figure><p id="22e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经对什么是VGG网络及其面临的问题有了一个简要的概述，我们将进一步了解什么是ResNet以及它是如何工作的。</p><h1 id="a286" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">剩余网络(ResNet):</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lz"><img src="../Images/881e75c9b4790a0121e07168c94582f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*_T6ybgHUoxj8rJq_Dc1nsw.jpeg"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated"><em class="ma">图1:残差块</em></figcaption></figure><p id="c0bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">残差网络或ResNet与传统的深度神经网络相同，具有诸如卷积、激活函数或ReLU、池和全连接网络的层。但是这里唯一的区别是我们在层之间添加了一个<strong class="ih hj">身份连接</strong>或<strong class="ih hj">身份映射</strong>。</p><p id="76e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是等等！什么是<strong class="ih hj">身份映射</strong>？你可能知道一个单位矩阵，I，它只包含从左上角开始的对角线位置上的1和所有其他位置上的0。这个矩阵与任何其他矩阵相乘，假设A，将得到相同的矩阵，使得AI = A。这里也发生了同样的情况，对输入应用恒等映射将得到与输入相同的输出。</p><p id="e033" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是这种身份映射有什么用呢？它使反向传播信号能够从输出(最后)层到达输入(第一)层。与传统的卷积神经网络相比，这是残差神经网络的整个工作发生的地方。</p><p id="c5a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如图1所示，<strong class="ih hj"> F(x) </strong>是两个卷积(权重)层之间的<strong class="ih hj">残差函数</strong>或<strong class="ih hj">残差映射</strong>，可以说是残差块的输入(x)和输出(H(x))之差，如图1所示。所以，剩余函数F(x)可以写成:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mb"><img src="../Images/5044363bd734d9fef5415efc8a396700.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*PbzHNP_RKaQyuLAjlHya-A.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated"><em class="ma">剩余功能</em></figcaption></figure><p id="772f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，引入该函数的主要目的是，不是期望堆叠层学习函数<strong class="ih hj"> H(x) </strong>的近似值，这是我们在普通堆叠卷积神经网络中所做的，而是让层去逼近残差函数<strong class="ih hj"> F(x) </strong>。这意味着，在训练深度残差网络时，我们的主要目标是学习残差函数<strong class="ih hj"> F(x) </strong>，这将增加网络的整体精度。</p><h1 id="d238" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">残差函数如何帮助提高网络的精度？</h1><p id="3000" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我们知道，在正常堆叠深度神经网络的反向传播过程中，当我们进入输入层时，梯度往往会变为零，因此权重不会更新，因此网络不会学习权重。我们知道这种情况叫什么，对吧？是的，<strong class="ih hj">消失渐变</strong>问题。</p><p id="2a8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是当我们使用残差函数时，即使梯度趋向于变为零，即，即使<strong class="ih hj"> H(x) </strong>变为零，网络也将至少学习<strong class="ih hj"> x </strong>(因为<strong class="ih hj"> F(x) = H(x) — x </strong>)，即，它避免梯度消失。因此，梯度到达输入层，权重被更新，这有助于网络更好地学习，从而提高网络的精度。有趣…不是吗？</p><h1 id="39e7" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">ResNet架构</h1><p id="bb43" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">现在，让我们看看ResNet-34架构:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mc"><img src="../Images/9cf251fda5abdc8efcf83910f0f49ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aqmUx_ONo8KqKNEYsjM8eA.png"/></div></div></figure><blockquote class="md me mf"><p id="b883" class="if ig ln ih b ii ij ik il im in io ip mg ir is it mh iv iw ix mi iz ja jb jc hb bi translated"><em class="hi">VGG-19、34层平面神经网络与34层深度残差神经网络对比</em></p><p id="5182" class="if ig ln ih b ii ij ik il im in io ip mg ir is it mh iv iw ix mi iz ja jb jc hb bi translated"><em class="hi">虚线表示从一个残差块到另一个残差块的图像大小的变化，并且是可以通过使用1x1核来实现的线性投影。</em></p></blockquote><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mj"><img src="../Images/4aa3e326a9de8b8cc13a599d99255329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6jnc3TWzDcBJrooZcpKlsw.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated"><em class="ma">各种深度剩余网络架构</em></figcaption></figure><h1 id="73b9" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">剩余网络的优势:</h1><p id="e06d" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">在ImageNet和CIFAR-10数据集上进行了实验。</p><ol class=""><li id="231d" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">易于优化</li><li id="82ef" class="lo lp hi ih b ii mk im ml iq mm iu mn iy mo jc lt lu lv lw bi translated">训练误差不会随着神经网络深度的增加而增加，这与简单神经网络的情况不同，在简单神经网络中，我们只是不断地堆叠层。</li><li id="4696" class="lo lp hi ih b ii mk im ml iq mm iu mn iy mo jc lt lu lv lw bi translated">添加身份映射不会引入任何额外的参数。因此，计算复杂度不会增加。</li><li id="6842" class="lo lp hi ih b ii mk im ml iq mm iu mn iy mo jc lt lu lv lw bi translated">随着深度的增加，精度增益更高，从而产生比诸如VGG网络的其它先前网络好得多的结果。</li></ol><p id="e9a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更清楚地了解ResNet的实现及其实际工作方式，您可以访问我的<a class="ae kw" href="https://github.com/vighneshutamse/DL-ResNet-Deploy" rel="noopener ugc nofollow" target="_blank"> github </a>个人资料，我在其中部署了ResNet-18模型，该模型是在CIFAR-10数据集上训练的。</p><h1 id="538b" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">参考资料:</h1><p id="017c" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated"><a class="ae kw" href="https://www.quora.com/What-is-the-vanishing-gradient-problem" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/什么是消失渐变问题</a></p><p id="4ff1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【https://arxiv.org/pdf/1512.03385.pdf T4】</p><p id="7330" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kw" href="https://cv-tricks.com/keras/understand-implement-resnets/" rel="noopener ugc nofollow" target="_blank">https://cv-tricks.com/keras/understand-implement-resnets/</a></p></div></div>    
</body>
</html>