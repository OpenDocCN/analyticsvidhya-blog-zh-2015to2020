<html>
<head>
<title>Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-faa4795bc73c?source=collection_archive---------17-----------------------#2020-05-24">https://medium.com/analytics-vidhya/linear-regression-faa4795bc73c?source=collection_archive---------17-----------------------#2020-05-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/76158cb25b0b8dece31b4a6d99656481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktosZkYOLObRd2V5gDYqKg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://cdn.lynda.com/course/578082/578082-637199624574737751-16x9.jpg" rel="noopener ugc nofollow" target="_blank">https://cdn . Lynda . com/course/578082/578082-637199624574737751-16x 9 . jpg</a></figcaption></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="b0f6" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">回归可能是你第一次进入机器学习领域时学习的第一个机器学习算法，因为它简单有效，是更高级的机器学习算法(包括神经网络和推荐算法)的基础，并且相对容易理解。</p><p id="ab04" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">回归是一种统计方法，采用一组随机变量并确定它们之间的数学关系。换句话说，回归计算许多变量来预测结果或分数。</p></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="998f" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">今天，我们将从头开始实施线性回归，并绘制出最适合我们数据的直线。要继续下去，你应该有一些基本的python知识来理解代码和基本的数学知识。在线性回归中，我们简单地应用二维形式的公式<strong class="je hj"> y=mx + b </strong>，随着维度的增加，公式变得越来越复杂。因此，在我们的公式中,“y”是目标值，它取决于x坐标的值,“m”是斜率,“b”是y轴截距。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ka"><img src="../Images/9a95a73565fd953e0666daabff7d77d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6sBhtrGnyqmkwQDbZ489XQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://scontent.fdel3-1.fna.fbcdn.net/v/t1.0-9/50170539_10161259674865203_3294943719437893632_o.png?_nc_cat=103&amp;_nc_sid=da1649&amp;_nc_ohc=7RraV_NZlroAX85F3X_&amp;_nc_ht=scontent.fdel3-1.fna&amp;oh=bca8d6495836a1626748261478095a28&amp;oe=5EEE56E8" rel="noopener ugc nofollow" target="_blank">https://scontent . fdel 3-1 . FNA . FBC dn . net/v/t 1.0-9/50170539 _ 10161259674865203 _ 3294943719437893632 _ o . png？_ NC _ cat = 103&amp;_ NC _ sid = da 1649&amp;_ NC _ OHC = 7 rrav _ nzlroax 85 F3 x _&amp;_ NC _ ht = s content . fdel 3-1 . FNA&amp;oh = bca8d 6495836 a 1626748261478095 a 28&amp;OE = 5 eee 56 e 8</a></figcaption></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="901b" class="kk kl hi kg b fi km kn l ko kp">from statistics import mean<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from matplotlib import style</span><span id="3149" class="kk kl hi kg b fi kq kn l ko kp">xs= np.array([1, 2 ,3 ,4 ,5 ,6 ,7 , 8, 9], dtype= np.float64)</span><span id="3a41" class="kk kl hi kg b fi kq kn l ko kp">ys= np.array([5,4,6,5,6,7 ,8, 7 ,9], dtype= np.float64)</span></pre><p id="da2a" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">首先，我们导入必要的库并创建x和y numpy数组。然后我们使用matplotlib库可视化我们的数据。</p><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="bf13" class="kk kl hi kg b fi km kn l ko kp">plt.scatter(xs,ys) #scatter plot<br/>plt.show()</span></pre><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/02ab46150f062bad805e87d0ebeab335.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*LCHMw1vbc--RMnNShO01ew.jpeg"/></div></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="4803" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">现在我们将创建计算斜率和y轴截距的函数。</p><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="7f40" class="kk kl hi kg b fi km kn l ko kp">def best_slope(xs,ys):<br/>    x_bar = mean(xs)<br/>    y_bar = mean(ys)<br/>    m= (((x_bar * y_bar) - mean(xs*ys)) /<br/>        ((x_bar* x_bar) - mean(xs * xs)))<br/>    <br/>    return m</span></pre><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/c4bb3e37618caa1f55ce32197bd126dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*Z9eLjrM2vhlmlsWECCuaBw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://pythonprogramming.net/static/images/machine-learning/best-fit-slope.png" rel="noopener ugc nofollow" target="_blank">https://python programming . net/static/images/machine-learning/best-fit-slope . png</a></figcaption></figure><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="49db" class="kk kl hi kg b fi km kn l ko kp">def best_intercept(xs , ys):<br/>    y_bar= mean(ys)<br/>    x_bar = mean(xs)<br/>    m=best_slope(xs ,ys)<br/>    <br/>    b=(y_bar - (m*x_bar))<br/>    return b</span><span id="15be" class="kk kl hi kg b fi kq kn l ko kp">reg_line = [ (m*x) +b for x in xs]</span></pre><p id="f890" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">现在我们用m、x和b的值绘制回归线，也称为“最佳拟合线”</p><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="5440" class="kk kl hi kg b fi km kn l ko kp">plt.scatter(xs, ys)<br/>plt.plot(reg_line)<br/>plt.title(“lineofbestfit”)<br/>plt.show()</span></pre><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/009be6450ef0df46fd1f80a844a1db11.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*2bOevCG6nYdo82z6z7xkbg.png"/></div></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="affb" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">但是我们如何保证我们的公式预测的线真的是最佳拟合的线呢？我们可以通过计算最佳拟合线的精确度来做到这一点，最佳拟合线也被称为<strong class="je hj"> R或决定系数</strong></p><p id="86b8" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">我们通过计算数据点和回归线之间距离的平方来计算。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/98a368b1ba82c28b09092e470aad2404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*3m3nrygiqTVE0nexgovfhQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.myaccountingcourse.com/financial-ratios/images/r-squared-graph.png" rel="noopener ugc nofollow" target="_blank">https://www . my accounting course . com/financial-ratios/images/r-squared-graph . png</a></figcaption></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="b41b" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">这就是线性回归的工作原理，通过找到相关特征的系数来找到一条最佳查找线，从而预测输出值或目标值。</p></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="c86c" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">你可以查看我的github链接，我在那里提供了<em class="ku"> i </em> -python笔记本，其中包括我在这里使用的所有代码及其解释。我要感谢youtube频道“sentdex”以如此简单的方式解释了线性回归，您可以通过单击下面的链接观看视频。</p><figure class="kb kc kd ke fd ij"><div class="bz dy l di"><div class="kv kw l"/></div></figure><p id="7623" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">我的github链接:【https://github.com/akshayakn13/LinearRegression T4】</p><p id="bb10" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">建议总是受欢迎的。别忘了看看我的其他文章。</p><p id="d598" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><a class="ae iu" href="https://ak95ml.blogspot.com/2020/06/linear-regression.html" rel="noopener ugc nofollow" target="_blank">从零开始线性回归。</a></p><p id="ac84" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><a class="ae iu" href="https://www.blogger.com/blog/post/edit/1004924421828631592/5881650886724527591#" rel="noopener ugc nofollow" target="_blank">美汤刮痧</a></p><p id="f37c" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><a class="ae iu" href="https://www.blogger.com/blog/post/edit/1004924421828631592/5881650886724527591#" rel="noopener ugc nofollow" target="_blank">我如何开始我作为机器学习爱好者的旅程</a>。</p></div></div>    
</body>
</html>