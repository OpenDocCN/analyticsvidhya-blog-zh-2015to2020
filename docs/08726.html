<html>
<head>
<title>Vanishing Gradient Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">消失梯度问题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/vanishing-gradient-problem-in-deep-learning-dafb9caf2f3a?source=collection_archive---------16-----------------------#2020-08-10">https://medium.com/analytics-vidhya/vanishing-gradient-problem-in-deep-learning-dafb9caf2f3a?source=collection_archive---------16-----------------------#2020-08-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="195b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在 20 世纪 80 年代，当时的研究人员无法在 ANN 中找到深度神经网络，因为我们必须在每个神经元中使用 sigmoid，因为 ReLU 尚未发明。由于 sigmoid 激活函数，我们面临着一个被称为消失梯度问题的问题。</p><p id="c2ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来理解它实际上是什么。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/56c8a01db822fa96bdf28b64153b3387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Ygj4VUmbUjVMjt_TcArd2Q.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">简单的安</figcaption></figure><p id="44e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设输入为 X1、X2 和 X3，输出为 Yp，输入和输出的权重分别为{W11，W12，W13}、{W21，W22，W23}和{W31，W32，W33}。</p><p id="f9c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，对于第二层到第三层，权重分别为{O1，O2}。</p><p id="92bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算 W11 新公式是 W11 新=W11 旧-n(L·w·r·t·W11 旧的导数)，其中</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jp"><img src="../Images/8905cac6430cf8b93eb9a321a115c872.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*d1Rplcs2DGUZ1aZqrTc9zw.png"/></div></figure><p id="f023" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">即损耗，Y 是实际输出。</p><p id="e493" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">sigmoid 函数由下式给出</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jq"><img src="../Images/269fba6f421db3bf736ca83b97a48cb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hns9HNPE2jM5G7qZ4YPf2Q.png"/></div></div></figure><p id="fc5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并且 sigmoid 函数的值在 0 到 1 之间。当我们对函数求导时，值在 0 到 0.25 之间。</p><p id="3ead" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">怎么会？？</p><p id="56fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它的导数 w . r . t . x 是</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jv"><img src="../Images/08d4dd12b55e383faffbf927c1bf4872.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*_MIb-19vMHHXA9N3N7zRJA.png"/></div></figure><p id="779c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们取 x 的值为 0，那么 f'(x)为 0.25，当我们取 x 的值为无穷大时，那么 f'(x)为 0。这可以使用下图进行分析，其中蓝线代表 sigmoid 函数，红线代表 sigmoid 函数的导数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jw"><img src="../Images/3ed46d18d2126a6473210c0bf7b6b7a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YNb2iNYTzGnkqJhpwEb4mQ.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">Sigmoid 函数及其导数的图形。</figcaption></figure><p id="808d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着随着层数的增加，该导数的值变得越来越小，直到旧权重的值近似等于新权重的点。</p><p id="d724" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，如果我们想找到新 W11 和旧 W11 是 5，n 是 0.01，那么我们需要 W11 的导数等于 W11 的导数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jx"><img src="../Images/cbb5f72b1ad6398b1e4b4219148350cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*QTRRojPQyDqKOKxpdBce5g.png"/></div></figure><p id="e063" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设 O1 的导数的值是 0.25，O1 的 W11 的导数是 0.02。那么 W11 的导数是 0.05，这使得新值是 4.9995，大约是 5。</p><p id="5f0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习中的这类问题被认为是消失梯度下降。</p></div></div>    
</body>
</html>