<html>
<head>
<title>[ML from scratch] Logistic Regression — Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[ML从零开始]逻辑回归—梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml-from-scratch-logistic-regression-gradient-descent-63b6beb1664c?source=collection_archive---------5-----------------------#2019-09-29">https://medium.com/analytics-vidhya/ml-from-scratch-logistic-regression-gradient-descent-63b6beb1664c?source=collection_archive---------5-----------------------#2019-09-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f7fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将深入研究逻辑回归背后的数学，以及它与经典分类器支持向量机的不同之处。</p><p id="0c23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归是用于分类问题的模型。虽然“回归”在其名称中，但逻辑回归主要用于分类问题，尤其是二元分类。</p><p id="0da2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于二分类问题，目标是{0，1}个向量，分别对应负类和正类。</p><p id="f6f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归使用sigmoid函数作为输出，这是神经网络中常用的激活函数。它可以理解为给定<a class="ae jd" rel="noopener" href="/@giangtran240896/ml-from-scrach-linear-regression-normal-equation-gradient-descent-1af26b542c28">线性函数</a>的真类的条件概率。它具有以下形式:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/847156d7e9a42b2c434c44a8c45b655b.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*477fFnx-V9L6-dSeU07zeA.png"/></div></figure><p id="c7e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以及sigmoid函数的图形:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jm"><img src="../Images/79029d07d8ec9f1ecf88a4b829120d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXCBO-Wx5XhuY_OwMl0Phw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">sigmoid函数的图形。</figcaption></figure><p id="d049" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到它的上界是1，下界是0，这个性质确保我们输出一个概率。</p><p id="958c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">乙状结肠的导数:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jv"><img src="../Images/ac0ba3de5f67e8bdb66a1e32834af019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kc85As-QOMdf-SXJpPX-jw.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jw"><img src="../Images/6d7df918405856e715f7062d28a4cd95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*otMeRmhr4dsiRlmgA82IMQ.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jx"><img src="../Images/41c02187c8612ccd4720545f188b9f16.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*tiYupHLghvriXW2H_JjiKg.png"/></div></figure><p id="9800" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">直观地说，给定一个数据集，其中<strong class="ih hj"> X </strong>是一个特征矩阵，而<strong class="ih hj"> y </strong>是正类或负类的矢量标签，我们想要对数据点<strong class="ih hj"> Xi </strong>属于哪个数据点进行分类。这意味着，从视觉上，我们发现一条线/平面/超平面(决策边界)将我们的数据分成两个区域。这种直觉大多用于SVM的<a class="ae jd" href="https://en.wikipedia.org/wiki/Support-vector_machine" rel="noopener ugc nofollow" target="_blank">算法。</a></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jy"><img src="../Images/4a7d2b5acfbdf021c98b54a0434a8934.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*4xeOZVXqPwjEtW98APSM4g.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">SVM: best line将数据分为两个区域。</figcaption></figure><p id="672d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，逻辑回归直觉不同。它将数据点映射到一个更高的维度，例如:2维-&gt; 3维，新增加的维度对应类别的概率。默认情况下，数据点概率≥ 0.5的阈值为1级，否则为0级。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jz"><img src="../Images/592984e6b1e2290a17f56c7c8f5e7651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*YW_pEBtrVzPI0JKIUKwdfQ.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">logistic回归映射的形状来自2D -&gt; 3D。</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jz"><img src="../Images/be6c0a9374e802baefe5732619182b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*XJB2lzeuaD60qsIMFiyPFg.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">从自上而下的角度看。</figcaption></figure><p id="173c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一个特征矩阵和一个相应目标的向量:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ka"><img src="../Images/ccc5a79c954c523d6f69793266de1849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*n2bjbDTWEuXpd8b5sP3rCw.png"/></div></figure><p id="9a89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中N是数据点数，D是每个数据点的维数。</p><p id="e3b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过参数<strong class="ih hj"> w </strong>从<strong class="ih hj"> X </strong>映射到<strong class="ih hj"> y </strong>的线性变换<strong class="ih hj"> h </strong>:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kb"><img src="../Images/2d9391b56d3bd4e8f2841d6b2ac12a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*poI7SXHGn2TlkmhDNpumvQ.png"/></div></figure><p id="f317" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将s形函数<strong class="ih hj"> z </strong>的元素应用到<strong class="ih hj"> h </strong>:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kc"><img src="../Images/71cfdb4315405d01a929cbe07be0b3ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*FFsd-rAAzlhA-G26AfP3uw.png"/></div></figure><p id="fe50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于sigmoid输出概率，我们使用负对数似然来表示误差:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kd"><img src="../Images/d75f126f47bd98b8fed23be65dd81e98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*1KyqULUn5ZjdCUOPSZa7mg.png"/></div></figure><p id="fdb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中N是数据点的数量，yi是真实标签，zi是sigmoid的预测概率。我们希望将参数<strong class="ih hj"> w </strong>的损失降至最低。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ke"><img src="../Images/a1dbc68348b8f3c02668bb632af73bc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*12c68mkwf6-5VHILtm6iPA.png"/></div></figure><p id="da95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">令人惊讶的是，逻辑回归的<strong class="ih hj"> J </strong>相对于<strong class="ih hj">T5的<strong class="ih hj"> w </strong>的导数与线性回归的导数相同。唯一不同的是，线性回归的输出是<strong class="ih hj"> h </strong>是线性函数，而在logistic中是<strong class="ih hj"> z </strong>是sigmoid函数。</strong></p><p id="2731" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">找到导数后，我们使用梯度下降来更新参数:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es kf"><img src="../Images/7d4a022e565780d2264f52011cf35b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s-UND5G3T5wwEQ-NE3VBJQ.png"/></div></div></figure><p id="e635" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们确信它会在有限的步骤中收敛。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jz"><img src="../Images/3e4fa095bb140459cbdbee6e72b45863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Fgba4pEC3IQQmLX7-gAlTQ.gif"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">训练逻辑回归可视化。</figcaption></figure><p id="e808" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于逻辑回归实现，请在此处检查<a class="ae jd" href="https://github.com/giangtranml/ml-from-scratch/tree/master/logistic_regression" rel="noopener ugc nofollow" target="_blank"/>。</p></div></div>    
</body>
</html>