<html>
<head>
<title>Assumptions Of Linear Regression — How to Validate and Fix</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的假设—如何验证和修正</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/assumptions-of-linear-regression-how-to-validate-and-fix-ec1bd9149bc2?source=collection_archive---------9-----------------------#2019-10-27">https://medium.com/analytics-vidhya/assumptions-of-linear-regression-how-to-validate-and-fix-ec1bd9149bc2?source=collection_archive---------9-----------------------#2019-10-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/3b807bf1633591a37ffe8d25230ca5df.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/0*WNqBzvsf1mgL7Rfc"/></div></figure><h1 id="7a89" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">介绍</h1><p id="509f" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我假设你对线性回归有相当的理解。如果没有，我已经用python写了一个简单易懂的例子<a class="ae ki" href="https://machinelearningmind.com/2019/10/06/introduction-to-linear-regression-e-commerce-dataset/" rel="noopener ugc nofollow" target="_blank">这里</a>。在继续下一步之前，请阅读它。</p><p id="af1a" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">线性回归对数据做出某些假设，并在此基础上提供预测。自然地，如果我们不考虑这些假设，线性回归会用一个糟糕的模型惩罚我们(你不能真的责怪它！)<br/>我们将获取一个数据集，尝试拟合所有的假设，检查指标，并与我们没有处理假设的情况下的指标进行比较。<br/>那么，我们就直入主题吧。</p><h1 id="1ced" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">无假设的线性回归</h1><p id="5b83" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">让我们以著名的广告数据集为例。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/3f9ad4c0869bc81c316654bfba0cc541.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/0*yeokycx2gTvnYi3P"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">df.info()</figcaption></figure><pre class="kp kq kr ks fd kx ky kz la aw lb bi"><span id="33e6" class="lc in hi ky b fi ld le l lf lg"># Creating DataFrame out of Advertising.csv<br/>df = pd.read_csv("Advertising.csv")<br/>df.drop("Unnamed: 0", axis=1,inplace=True)</span><span id="0f36" class="lc in hi ky b fi lh le l lf lg"># Separating Independent and dependent variables<br/>X=df.drop(['sales'],axis=1)<br/>Y=df.sales</span><span id="a228" class="lc in hi ky b fi lh le l lf lg"># Fit Linear Regression<br/>lr = LinearRegression()<br/>model=lr.fit(X,Y)<br/>y_pred = model.predict(X)<br/>print("R-squared: {0}".format(metrics.r2_score(Y,y_pred)))</span></pre><p id="e01c" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated"><strong class="jm hj">输出:</strong> R的平方:0。58660 . 68868886861</p><pre class="kp kq kr ks fd kx ky kz la aw lb bi"><span id="f4c0" class="lc in hi ky b fi ld le l lf lg"># Plot the Residuals vs Fitted Values<br/>plt.scatter(ypred, (Y-ypred)) <br/>plt.xlabel("Fitted values") <br/>plt.ylabel("Residuals")</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es li"><img src="../Images/1ad2178e29baaf4ba3a86dc0409b5989.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/0*dmX_tqLuAsohltQG"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">发现模式</figcaption></figure><p id="7dcd" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">我们可以在残差与拟合值图中看到一种模式，这意味着模型没有很好地捕捉到数据的非线性。</p><p id="632a" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">现在，让我们继续假设，看看R平方值和残差与拟合值图是否有所改善。</p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="c8e8" class="im in hi bd io ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj bi translated">假设1:</h1><blockquote class="lv lw lx"><p id="aeac" class="jk jl ly jm b jn kj jp jq jr kk jt ju lz kl jx jy ma km kb kc mb kn kf kg kh hb bi translated"><em class="hi">因变量和自变量必须有线性关系。</em></p></blockquote><h2 id="9010" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">怎么查？</h2><p id="aa97" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">一个简单的数据图表可以帮助我们了解自变量和因变量之间是否存在线性关系。</p><h2 id="def3" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">怎么修？</h2><p id="6d1e" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">要修复非线性，可以对独立变量log(X)进行对数变换，或者进行其他非线性变换，如√X或X。</p><p id="0444" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">让我们绘制一个配对图来检查自变量和因变量之间的关系。</p><pre class="kp kq kr ks fd kx ky kz la aw lb bi"><span id="089d" class="lc in hi ky b fi ld le l lf lg">sns.pairplot(df)</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/e8644a129490c5076ba9306f2b0b5eaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/0*HrvYtvAMeL4o7L2u"/></div></figure><p id="66dd" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">我们可以在残差与拟合值图中看到一种模式，这意味着模型没有很好地捕捉到数据的<strong class="jm hj">非线性。</strong></p><p id="4534" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">一阶方程不能完全捕捉非线性，这将导致模型不符合标准。为了对变量进行平方并拟合模型，我们将使用具有多项式特征的线性回归。</p><pre class="kp kq kr ks fd kx ky kz la aw lb bi"><span id="bfab" class="lc in hi ky b fi ld le l lf lg">from sklearn.preprocessing import PolynomialFeatures <br/>  <br/>poly = PolynomialFeatures(degree = 2) <br/>X_poly = poly.fit_transform(X) <br/>  <br/>poly.fit(X_poly, Y) <br/>X_poly = sm.add_constant(X_poly)<br/>results = sm.OLS(Y,X_poly).fit()</span><span id="77ed" class="lc in hi ky b fi lh le l lf lg">print(results.summary())</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/5b2e8606b25663e9fc5a5b3084a0810b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/0*dt8v_WNWI8UygVHl"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">r平方:0.987，德宾-沃森:2.136</figcaption></figure></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="21c5" class="im in hi bd io ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj bi translated">假设2:</h1><blockquote class="lv lw lx"><p id="48fb" class="jk jl ly jm b jn kj jp jq jr kk jt ju lz kl jx jy ma km kb kc mb kn kf kg kh hb bi translated">残差中没有自相关。</p></blockquote><h2 id="8cab" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">怎么查？</h2><p id="0281" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">使用德宾-沃森测试。<br/> DW = 2将是这里的理想情况(无自相关)<br/> 0 &lt; DW &lt; 2 - &gt;正自相关<br/> 2 &lt; DW &lt; 4 - &gt;负自相关<br/> statsmodels的线性回归总结为我们提供了DW值以及其他有用的见解。</p><h2 id="a5c7" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">怎么修？</h2><ul class=""><li id="9031" class="mr ms hi jm b jn jo jr js jv mt jz mu kd mv kh mw mx my mz bi translated">添加一个相对于自变量滞后的列</li><li id="f085" class="mr ms hi jm b jn na jr nb jv nc jz nd kd ne kh mw mx my mz bi translated">将变量居中(用平均值减去列中的所有值)</li></ul><p id="e33e" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">我们可以看到，<strong class="jm hj"> Durbin-Watson :~ 2 </strong>(摘自上面的results.summary()一节)这似乎非常接近理想情况。所以，我们什么都不用做</p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="60f7" class="im in hi bd io ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj bi translated">假设3:</h1><blockquote class="lv lw lx"><p id="b5ce" class="jk jl ly jm b jn kj jp jq jr kk jt ju lz kl jx jy ma km kb kc mb kn kf kg kh hb bi translated">没有<a class="ae ki" href="https://www.investopedia.com/terms/h/heteroskedasticity.asp" rel="noopener ugc nofollow" target="_blank">异方差</a>。</p></blockquote><h2 id="52c2" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">怎么查？</h2><p id="fd45" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">残差对拟合值图可以判断是否存在异方差。<br/>如果该图显示漏斗形模式，那么我们说异方差存在。</p><blockquote class="lv lw lx"><p id="1460" class="jk jl ly jm b jn kj jp jq jr kk jt ju lz kl jx jy ma km kb kc mb kn kf kg kh hb bi translated"><em class="hi">残差只不过是实际值和拟合值之间的差异</em></p></blockquote><h2 id="99cf" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">怎么修？</h2><p id="651b" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我们可以对因变量进行非线性转换，如log(Y)或√Y。此外，您可以使用加权最小二乘法来处理异方差。</p><pre class="kp kq kr ks fd kx ky kz la aw lb bi"><span id="8945" class="lc in hi ky b fi ld le l lf lg">plt.subplots(figsize=(10,5))</span><span id="a7f5" class="lc in hi ky b fi lh le l lf lg">plt.subplot(1,2,1)<br/>plt.title("Before")<br/>plt.scatter(ypred1, (Y-ypred1))<br/>plt.xlabel("Fitted values")<br/>plt.ylabel("Residuals")</span><span id="d24e" class="lc in hi ky b fi lh le l lf lg">plt.subplot(1,2,2)<br/>plt.title("After")<br/>plt.scatter(ypred2, (Y-ypred2))<br/>plt.xlabel("Fitted values")<br/>plt.ylabel("Residuals")</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/ed3448d570f7c886d1220532f2a2f0ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/0*485-dAcUmZ42l244"/></div></figure><p id="4d70" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">在这里，我们绘制了基于假设的之前的<strong class="jm hj">和</strong>之后的<strong class="jm hj">的残差与拟合值的曲线图。我们在“之后”或“之前”部分没有看到类似漏斗的模式，所以没有异方差。</strong></p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="40a5" class="im in hi bd io ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj bi translated">假设4:</h1><blockquote class="lv lw lx"><p id="1c28" class="jk jl ly jm b jn kj jp jq jr kk jt ju lz kl jx jy ma km kb kc mb kn kf kg kh hb bi translated">没有完美的多重共线性。</p></blockquote><p id="c732" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">我已经写了一篇关于<a class="ae ki" href="https://machinelearningmind.com/2019/10/19/multicollinearity-how-to-fix-it/" rel="noopener ugc nofollow" target="_blank">多重共线性</a>以及如何修复的帖子。如果您不熟悉多重共线性，请阅读它。</p><h2 id="2dd4" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">怎么查？</h2><p id="c0ff" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在变量非常少的情况下，可以使用热图，但是在列数很大的情况下不太可行。<br/>另一种常见的检查方法是计算VIF(方差膨胀因子)值。<br/>如果VIF=1，非常少的多重共线性<br/> VIF &lt; 5，中度多重共线性<br/> VIF &gt; 5，极度多重共线性(这是我们必须避免的)</p><h2 id="592a" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">怎么修？</h2><p id="9ace" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">具有高度多重共线性的变量可以一起移除，或者如果您可以找出哪两个或更多变量彼此具有高度相关性，您可以简单地将这些变量合并为一个。确保VIF &lt; 5.</p><pre class="kp kq kr ks fd kx ky kz la aw lb bi"><span id="24e6" class="lc in hi ky b fi ld le l lf lg"># Function to calculate VIF<br/>def calculate_vif(data):<br/>    vif_df = pd.DataFrame(columns = ['Var', 'Vif'])<br/>    x_var_names = data.columns<br/>    for i in range(0, x_var_names.shape[0]):<br/>        y = data[x_var_names[i]]<br/>        x = data[x_var_names.drop([x_var_names[i]])]<br/>        r_squared = sm.OLS(y,x).fit().rsquared<br/>        vif = round(1/(1-r_squared),2)<br/>        vif_df.loc[i] = [x_var_names[i], vif]<br/>    return vif_df.sort_values(by = 'Vif', axis = 0, ascending=False, inplace=False)</span><span id="96a5" class="lc in hi ky b fi lh le l lf lg">X=df.drop(['sales'],axis=1)<br/>calculate_vif(X)</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/0f5cdaf6edc855af8e7cc62dbdfd904a.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/0*fjuJjZBA12ACKaVQ"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">VIF &lt; 5 for all Independent variables</figcaption></figure><p id="f8c4" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">Great! we have all VIFs&lt;5 . If you want to know what to do in case of higher VIF values, check <a class="ae ki" href="https://machinelearningmind.com/2019/10/19/multicollinearity-how-to-fix-it/#fixit" rel="noopener ugc nofollow" target="_blank">这个</a>出来。</p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="a22f" class="im in hi bd io ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj bi translated">假设5:</h1><blockquote class="lv lw lx"><p id="7ac0" class="jk jl ly jm b jn kj jp jq jr kk jt ju lz kl jx jy ma km kb kc mb kn kf kg kh hb bi translated">残差必须是正态分布的。</p></blockquote><h2 id="4c6d" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">怎么查？</h2><p id="3858" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">对残差使用分布图，看看它是否正态分布。</p><h2 id="277b" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">怎么修？</h2><p id="da59" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">如果残差不是正态分布，可以尝试因变量或自变量的非线性变换。</p><pre class="kp kq kr ks fd kx ky kz la aw lb bi"><span id="5416" class="lc in hi ky b fi ld le l lf lg">plt.subplots(figsize=(8,4))</span><span id="968a" class="lc in hi ky b fi lh le l lf lg">plt.subplot(1,2,1)<br/>plt.title("Before")<br/>sns.distplot(Y-ypred1 , fit=norm);<br/>plt.xlabel('Residuals')</span><span id="4036" class="lc in hi ky b fi lh le l lf lg">plt.subplot(1,2,2)<br/>plt.title("After")<br/>sns.distplot(Y-ypred2 , fit=norm);<br/>plt.xlabel('Residuals')</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/3817ba93c10d8d0cbc4741e793afa51b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/0*PwABLoAl11Xax9YM"/></div></figure><p id="8987" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">图中的黑线显示的是正态分布，蓝线显示的是当前的分布。</p><p id="85d7" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">“之前”部分显示分布从正态分布略微偏移，而“之后”部分几乎与正态分布一致。</p><p id="37eb" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">另一种我们可以确定的方法是使用Q-Q图(分位数-分位数)</p><pre class="kp kq kr ks fd kx ky kz la aw lb bi"><span id="0905" class="lc in hi ky b fi ld le l lf lg">plt.subplots(figsize=(8,4))</span><span id="1f6f" class="lc in hi ky b fi lh le l lf lg">plt.subplot(1,2,1)<br/>stats.probplot(Y-ypred1, plot=plt)</span><span id="3096" class="lc in hi ky b fi lh le l lf lg">plt.subplot(1,2,2)<br/>stats.probplot(Y-ypred2, plot=plt)<br/>plt.show()</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/48f14cb2890620349bd37e99a87259ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/0*xZtMfwXwQH3ZiDeA"/></div></figure><p id="8eec" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">在“之前”部分，您将看到剩余分位数并不完全遵循它应该遵循的直线，这意味着分布不正常。而在假设验证之后，我们可以看到剩余分位数遵循一条直线，这意味着分布是正态的。</p><p id="a984" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">这标志着假设验证的结束。现在让我们比较两个模型的指标。</p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="fb2b" class="im in hi bd io ip lq ir is it lr iv iw ix ls iz ja jb lt jd je jf lu jh ji jj bi translated">比较</h1><p id="96b5" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我们来对比一下这两款，看看有没有改进的地方。</p><h2 id="79a1" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">以前</h2><p id="7de0" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">r平方:<strong class="jm hj"> 0.8972 </strong></p><pre class="kp kq kr ks fd kx ky kz la aw lb bi"><span id="7b6c" class="lc in hi ky b fi ld le l lf lg">plt.title("Before")<br/>plt.plot(Y,Y, color="red")<br/>plt.scatter(ypred1, Y)<br/>plt.xlabel("Fitted values")<br/>plt.ylabel("Actuals")</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/eb34511da884abcbc9a0b1dccd8ab300.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/0*yTOJ6qPG1OxQZ0T0"/></div></figure><h2 id="edb4" class="lc in hi bd io mc md me is mf mg mh iw jv mi mj ja jz mk ml je kd mm mn ji mo bi translated">在...之后</h2><p id="cafe" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">r平方:<strong class="jm hj"> 0.987 </strong></p><pre class="kp kq kr ks fd kx ky kz la aw lb bi"><span id="d590" class="lc in hi ky b fi ld le l lf lg">plt.title("After")<br/>plt.plot(Y,Y, color="red")<br/>plt.scatter(ypred2, Y)<br/>plt.xlabel("Fitted values")<br/>plt.ylabel("Actuals")</span></pre><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/c337ac4c6f1eddf9fff67ab964e5a5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/0*tpIGTRVB8Zlfh8vQ"/></div></figure><p id="c2da" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">r平方值已得到改进，在上面的图中，我们还可以看到假设验证前后的实际值与拟合值。<br/>98%+以上的拟合值与实际值一致。这意味着模型能够捕捉数据集的非线性并从中学习。</p><h1 id="1124" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">结论</h1><p id="c183" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我们现在已经验证了线性回归的所有假设都被考虑到了，我们可以有把握地说，如果我们考虑到这些假设，我们可以期望得到好的结果。</p><p id="6ff1" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">所以，基本上，如果你的线性回归模型给出了低于标准的结果，请确保这些假设是有效的，如果你已经修正了数据以符合这些假设，那么你的模型肯定会有所改进。</p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><p id="c7e4" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">这个帖子到此为止！。</p><p id="70ae" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">这是我为Jupyter笔记本设计的关于线性回归的GitHub。寻找用于这篇文章的笔记本-&gt;<a class="ae ki" href="https://github.com/fahadanwar10/LinearRegression/blob/master/media-sales-linear-regression-verify-assumptions.ipynb" rel="noopener ugc nofollow" target="_blank">media-sales-linear-regression-verify-assumptions . ipynb</a><br/>请随时查看，并在回复中提出更多改进指标的方法。</p><p id="8f29" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">感谢您的阅读！</p><p id="69a3" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">相关职位:</p><div class="nl nm ez fb nn no"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/introduction-to-linear-regression-e-commerce-dataset-cfa65b2c1213"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">线性回归简介—电子商务数据集</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">在这篇文章中，我们将了解什么是线性回归，它背后的一点数学知识，并试图拟合一个…</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">medium.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc ik no"/></div></div></a></div><div class="nl nm ez fb nn no"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/multicollinearity-how-to-fix-it-905b110d1968"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">多重共线性-如何解决？</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">这篇文章将回答诸如什么是多重共线性？，由…产生的问题是什么</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">medium.com</p></div></div><div class="nx l"><div class="od l nz oa ob nx oc ik no"/></div></div></a></div></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><p id="d96a" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated"><em class="ly">原载于2019年10月27日</em><a class="ae ki" href="https://machinelearningmind.com/2019/10/27/assumptions-of-linear-regression-how-to-validate-and-fix/" rel="noopener ugc nofollow" target="_blank"><em class="ly">https://machinelearningmind.com</em></a><em class="ly">。</em></p></div></div>    
</body>
</html>