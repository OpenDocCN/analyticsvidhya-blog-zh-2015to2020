<html>
<head>
<title>Entropy Calculation, Information Gain &amp; Decision Tree Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">熵计算、信息增益和决策树学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/entropy-calculation-information-gain-decision-tree-learning-771325d16f?source=collection_archive---------0-----------------------#2020-01-02">https://medium.com/analytics-vidhya/entropy-calculation-information-gain-decision-tree-learning-771325d16f?source=collection_archive---------0-----------------------#2020-01-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="f0a6" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">简介:</h1><p id="c7b7" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">决策树学习是一种逼近离散值目标函数的方法，其中学习的函数被表示为if-else/then规则集，以提高人类的可读性。这些学习方法是归纳推理算法中最流行的，并已成功地应用于广泛的任务；从学习诊断医疗案例，到评估贷款申请人的信用风险。构建决策树最流行的算法是ID3(迭代二分法3)。其他的还有ASSISTANT和C4.5 .这些决策树学习方法搜索一个完全表达的假设空间(所有可能的假设)，从而避免了受限假设空间的困难。他们的归纳偏向是偏爱小树而不是长树。</p><h1 id="c00d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">何时使用决策树:</h1><p id="6b5d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">记住，有许多分类器可以根据训练样本对未知实例进行分类。我们必须通过查看训练示例来了解哪个分类器最适合数据集。如果问题特征看起来像以下几点，决策树是最有效的- <br/> 1)实例可以用属性-值对来描述。2)目标函数是离散值的。</p><h1 id="91da" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">决策树的构建:</h1><p id="39e2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">决策树通过从根节点到某个叶节点对实例进行排序来对实例进行分类。每个节点指定实例的某个<strong class="jf hj"> <em class="kb">属性</em> </strong>的一个测试，从该节点向下的每个分支对应于该属性的一个可能值。我们的基本算法ID3通过自顶向下构建决策树来学习决策树，从问题“应该在树根处测试哪个属性？”为了回答这个问题，使用统计测试来评估每个属性，以确定它单独对训练示例进行分类的效果如何。最佳属性被选为树的根。我们的下一个任务是找到root之后的下一个节点。在这种情况下，我们希望再次选择对训练示例分类最有用的属性。然后重复这个过程，直到我们找到叶节点。<br/>现在的大问题是，ID3如何衡量最有用的属性。答案是，ID3使用一种称为<strong class="jf hj"> <em class="kb">信息增益</em> </strong>的统计属性，该属性测量给定属性根据训练示例的目标分类将它们分开的程度。一旦我们在下一节中获得了关于<strong class="jf hj"><em class="kb"/></strong>}的一些知识，我们将更详细地讨论关于<strong class="jf hj"> <em class="kb">【信息增益】</em> </strong>。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/39c2a20eda9a017176e6fa2975421061.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ms56TR5y_0mqr7hiZ_O43g.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图1:打网球的数据集，将用于训练决策树</figcaption></figure><h1 id="c72f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">熵:</h1><p id="c2b4" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">为了精确地定义信息增益，我们首先定义一个在信息论中常用的度量，叫做熵。熵基本上告诉我们一组数据有多不纯。术语不纯在这里定义了非同质性。换句话说，我们可以说，“熵是同质性的度量。它向我们返回关于任意数据集的信息，即数据集有多不纯/不均匀。”<br/>给定一个样本/数据集S的集合，包含一些目标概念的正例和反例，S相对于这个布尔分类的熵是-</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ks"><img src="../Images/a8b53efeb071154c49b36604dd4f41b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*jjw7I4NVHedL0oexku9Z3Q.png"/></div></figure><p id="5d18" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">为了说明这个等式，我们将做一个例子来计算图1中数据集的熵。数据集有9个阳性实例和5个阴性实例，因此-</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ky"><img src="../Images/52444bf2d5bab1bb79e47a2a2bdfe7ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*rhhhxm4NJmyyARHdNJKSFQ.png"/></div></figure><p id="704e" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">通过仔细观察方程<strong class="jf hj"> 1.2 </strong>、<strong class="jf hj"> 1.3 </strong>和<strong class="jf hj">1.4；</strong>我们可以得出一个结论，如果数据集是完全同质的那么杂质是0，因此熵是0(等式<strong class="jf hj"> 1.4 </strong>)，但是如果数据集可以被平均分成两类，那么它是完全非同质的&amp;杂质是100%，因此熵是1(等式<strong class="jf hj"> 1.3 </strong>)。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kz"><img src="../Images/1c8b2ab035ac1d3442907af032675bbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*YLTiAJwpSCV1lrZjAqwd2w.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图2:熵图</figcaption></figure><p id="c9e3" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">现在，如果我们尝试将<strong class="jf hj"> <em class="kb">熵</em> </strong>绘制在一个图中，它将如图2所示。它清楚地表明，当数据集是同质的时，熵最低，而当数据集完全是非同质的时，熵最高。</p><h1 id="7cc4" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">信息增益:</h1><p id="6b7c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">给定熵是数据集集合中杂质的度量，现在我们可以度量属性在分类训练集方面的有效性。我们将使用称为<strong class="jf hj"> <em class="kb">信息增益</em> </strong>的度量，简单来说就是根据该属性对数据集进行分区所导致的<strong class="jf hj"> <em class="kb">熵</em> </strong>的预期减少。属性A相对于数据集S的集合的信息增益<strong class="jf hj"> <em class="kb"> (Gain(S，A) </em> </strong>定义为-</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es la"><img src="../Images/5b6076f8cb1ee88213ab694f47ad4e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*jDKg5JZqfuZQni4gJ2aE8g.png"/></div></figure><p id="afc1" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">为了更清楚，让我们使用这个等式，从图1的数据集中测量属性<strong class="jf hj"><em class="kb"/></strong>的<strong class="jf hj"> <em class="kb">信息增益</em> </strong>。数据集有14个实例，因此样本空间为14，其中样本有9个正实例和5个负实例。属性<strong class="jf hj">风</strong>可以具有值<strong class="jf hj">弱</strong>或<strong class="jf hj">强</strong>。因此，</p><p id="a0e0" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">值(风)=弱，强</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ky"><img src="../Images/0381f2f1c03be4aad5551631bb237722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*Qx_wI3bkywYfA92Ly6xBSg.png"/></div></figure><p id="233d" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">所以，<strong class="jf hj"> <em class="kb">信息增益</em> </strong>乘<strong class="jf hj">风</strong>属性为0.048。让我们通过<strong class="jf hj">展望</strong>属性来计算<strong class="jf hj"> <em class="kb">信息增益</em> </strong>。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lb"><img src="../Images/43c3c9b5cddce2df339ceb14785b9d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*Nm3XLtj3f8GbSrxoOZtCkA.png"/></div></div></figure><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lc"><img src="../Images/d45d4e45b6c0aea56e414367f98a9674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*_kent859zVW8RO6DquYbpg.png"/></div></figure><p id="2f9f" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">这两个例子应该使我们清楚，我们怎样才能计算出<strong class="jf hj"><em class="kb"/></strong>的信息增益。图1数据集的4个属性的信息增益是:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ld"><img src="../Images/64e0d559fb0f71351104efa6067c2e1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*X6sjXPwgUYIhFtWmEmhy5A.png"/></div></figure><p id="2248" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">记住，度量<strong class="jf hj"> <em class="kb">信息增益</em> </strong>的主要目的是找到对分类训练集最有用的属性。我们的ID3算法将使用属性作为它的根来构建决策树。然后，它将再次计算信息增益以找到下一个节点。据我们计算，最有用的属性是“展望”,因为它比其他属性给我们提供了更多的信息。所以，“展望”将是我们这棵树的根。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es le"><img src="../Images/c8181c6247666c53d764086037375e84.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*9sjQVqvPGlwv_dAMDtlfYQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图3:从ID3的第一阶段部分学习的决策树</figcaption></figure><p id="c391" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">图3显示了我们在ID3第一阶段学习的决策树。训练样本被排序到相应的后代节点。<strong class="jf hj">阴</strong>后代只有正实例，因此成为分类为<strong class="jf hj">是</strong>的叶节点。对于另外两个节点，问题又出现了，应该测试哪个属性？这两个节点将通过选择相对于实例的新子集具有最高信息增益<strong class="jf hj">的属性来进一步扩展。让我们在<strong class="jf hj"> Sunny </strong>后裔处找到应该测试的属性。</strong></p><p id="224a" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">图1中的数据集在第1天、第2天、第8天、第9天、第11天的值为<strong class="jf hj"> Sunny </strong>。所以这里的样本空间S=5。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lf"><img src="../Images/08bcc9badbaa11f9b688c6b087345ced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*yd_BE_fbCA-89dytm8Hv6Q.png"/></div></figure><p id="666f" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">我们现在可以按照测量<strong class="jf hj">增益(S，湿度)</strong>的相同方法来测量温度和风的信息增益。最后，我们会得到:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lg"><img src="../Images/c1ee4fb43727c908cdfe175be45f5f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*ZfRrWEakxpyB7jAtWL80vQ.png"/></div></figure><p id="131e" class="pw-post-body-paragraph jd je hi jf b jg kt ji jj jk ku jm jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">所以湿度在这个阶段给了我们最多的信息。<strong class="jf hj">【展望】</strong>后的节点在<strong class="jf hj">晴</strong>后代将是<strong class="jf hj">湿度</strong>。<strong class="jf hj">高</strong>后代只有反例<strong class="jf hj">正常</strong>后代只有正例。所以两者都成为叶节点，无法进一步展开。如果我们用同样的方法展开<strong class="jf hj"> Rain </strong>后代，我们会看到<strong class="jf hj"> Wind </strong>属性提供了最多的信息。我把这部分留给读者自己去计算。因此，我们最终的决策树如图4所示:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lh"><img src="../Images/a5a5eb88ce9ef0d7faf90ed3915c1521.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*CTbvWbQe86BTDfqOyzVZjg.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图4:通过ID3算法完全学习的决策树</figcaption></figure><h1 id="d642" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">决策树学习中的归纳偏差:</strong></h1><p id="c38c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">学习算法的归纳偏差(也称为学习偏差)是学习者在没有遇到输入的情况下用来预测输出的一组假设{Tom M. Mitchell，Machine Learning}。给定一组示例，可能有许多决策树与这些示例一致。ID3选择哪个决策树？ID3搜索策略(a)选择较短的树而不是较长的树，以及(b)选择将具有最高信息增益的属性放置在最接近根的树。由于ID3使用的属性选择启发式算法和它遇到的特定训练示例之间的微妙交互，很难精确描述ID3表现出的归纳偏差。然而，我们可以近似地将它的偏好描述为对<strong class="jf hj"> <em class="kb">的偏好。“短树优于长树，将高信息增益属性放置在靠近根的树优于那些不靠近根的树。”</em> </strong></p></div></div>    
</body>
</html>