<html>
<head>
<title>Sigmoid Function with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch的Sigmoid函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sigmoid-function-with-pytorch-99cb2209ad89?source=collection_archive---------2-----------------------#2020-02-01">https://medium.com/analytics-vidhya/sigmoid-function-with-pytorch-99cb2209ad89?source=collection_archive---------2-----------------------#2020-02-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/c7073437d5a071bebbd440222a032b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*KrA4Z-LEzIxvgeqQnD_1lA.png"/></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">PyTorch</figcaption></figure><div class=""/><div class=""><h2 id="6977" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">在这篇文章中，我将告诉你如何使用PyTorch计算sigmoid(激活)函数。</h2></div><h1 id="3b95" class="ji jj ht bd jk jl jm jn jo jp jq jr js iz jt ja ju jc jv jd jw jf jx jg jy jz bi translated">Sigmoid函数</h1><p id="c2c7" class="pw-post-body-paragraph ka kb ht kc b kd ke iu kf kg kh ix ki kj kk kl km kn ko kp kq kr ks kt ku kv hb bi translated">首先，我们需要知道什么是Sigmoid函数。Sigmoid函数在分类器算法中非常常用于计算概率。它总是返回一个介于0和1之间的值，这是一个事物的概率。</p><p id="4654" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">点击阅读更多关于sigmoid函数的详细信息<a class="ae lb" href="http://mathworld.wolfram.com/SigmoidFunction.html" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="ld le lf lg fd hk er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lc"><img src="../Images/52cd1615c4b9508eca4e4023c839c731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X-e-cTxVrYbLwcqqLHh75g.png"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">Sigmoid函数</figcaption></figure><h1 id="7852" class="ji jj ht bd jk jl jm jn jo jp jq jr js iz jt ja ju jc jv jd jw jf jx jg jy jz bi translated">PyTorch</h1><figure class="ld le lf lg fd hk er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ll"><img src="../Images/4d1545082e1b6dcc4338c6f0d5c66c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4br4WmxNo0jkcsY796jGDQ.jpeg"/></div></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">PyTorch徽标</figcaption></figure><p id="c767" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated"><a class="ae lb" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>是由脸书AI团队开发的深度学习框架。所有深度学习框架都有一个称为张量的主干。您可以将张量视为矩阵或向量，即一维张量是矩阵，二维张量是矩阵，三维张量是具有3个索引的数组，即三维张量中的RGB颜色代码。我们可以有n维张量。让我们看看我们将如何计算激活(带PyTorch的sigmoid函数)。</p><p id="2488" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">PyTorch张量可以加、乘、减等，就像Numpy数组一样。一般来说，使用PyTorch张量与使用Numpy数组的方式非常相似。</p><h1 id="6d61" class="ji jj ht bd jk jl jm jn jo jp jq jr js iz jt ja ju jc jv jd jw jf jx jg jy jz bi translated">让我们编码并理解它</h1><p id="1acf" class="pw-post-body-paragraph ka kb ht kc b kd ke iu kf kg kh ix ki kj kk kl km kn ko kp kq kr ks kt ku kv hb bi translated">让我们使用PyTorch的内置方法生成一些随机数据。</p><p id="1e3f" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">我们使用<a class="ae lb" href="https://pytorch.org/docs/stable/torch.html#torch.manual_seed" rel="noopener ugc nofollow" target="_blank"> torch.manual.seed() </a>，<a class="ae lb" href="https://pytorch.org/docs/stable/torch.html#torch.randn" rel="noopener ugc nofollow" target="_blank"> torch.randn() </a>，<a class="ae lb" href="https://pytorch.org/docs/stable/torch.html#torch.randn_like" rel="noopener ugc nofollow" target="_blank"> torch.randn_like() </a>生成随机数据。</p><pre class="ld le lf lg fd lm ln lo lp aw lq bi"><span id="a6c4" class="lr jj ht ln b fi ls lt l lu lv">import torch # we import torch(i.e pyTorch)</span><span id="ca67" class="lr jj ht ln b fi lw lt l lu lv">#Lets Generate some Random Data</span><span id="5cce" class="lr jj ht ln b fi lw lt l lu lv">torch.manual.seed(7) #set the seed<br/>features = torch.randn((1,5))<br/>#torch.randn takes tuple of dimensions and returns tensor of<br/>#that dimension, here 1x5 vector</span><span id="fcf3" class="lr jj ht ln b fi lw lt l lu lv">weights = torch.randn_like(features) <br/>#randn_like takes a tensor and return a random tensor of that size</span><span id="65a4" class="lr jj ht ln b fi lw lt l lu lv">bias = torch.randn((1,1))</span></pre><p id="bc58" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">现在我们已经生成了随机数据，让我们对sigmoid函数进行编码</p><pre class="ld le lf lg fd lm ln lo lp aw lq bi"><span id="a140" class="lr jj ht ln b fi ls lt l lu lv">def sigmoid(x):<br/>   """Sigmoid Activation Function<br/>      Arguments:<br/>      x.torch.tensor<br/>      Returns<br/>      Sigmoid(x.torch.tensor)<br/>   """<br/>   return 1 / (1+torch.exp(x))<br/>   #remember formula of sigmoid<br/>   #function, torch.exp returns tensor of exp of all values in it</span><span id="6293" class="lr jj ht ln b fi lw lt l lu lv">&gt;&gt;&gt; print(weights)<br/>  tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])<br/>&gt;&gt;&gt;print(features)<br/>  tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])<br/>&gt;&gt;&gt;print(bias)<br/>  tensor([[0.3177]])</span></pre><p id="6cc0" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated"><strong class="kc hu">第一种方法</strong></p><p id="c1b9" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">我们用sigmoid(w1x1+w2x2+…)计算sigmoid函数。+wnxn+b)即我们将权重和特征相乘，这是一个元素接一个元素的乘法，即w1x1 + w2x2 +…。+wnxn并在结果中添加偏差。我们将整个结果发送给激活函数，答案存储在y中。</p><pre class="ld le lf lg fd lm ln lo lp aw lq bi"><span id="e106" class="lr jj ht ln b fi ls lt l lu lv">y = activation(torch.sum(features * weight)+bias)</span><span id="2cbd" class="lr jj ht ln b fi lw lt l lu lv">&gt;&gt;&gt;print(y)<br/>  tensor([[0.1595]])</span></pre><p id="e072" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated"><strong class="kc hu">第二种方法</strong></p><p id="0cc8" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">第二种方法是矩阵乘法。我们使用torch.matmul()或torch.mm()方法将我们的特征向量与权重向量相乘。</p><p id="47d7" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">我们在PyTorch中一般用<a class="ae lb" href="https://pytorch.org/docs/stable/torch.html#torch.mm" rel="noopener ugc nofollow" target="_blank"> torch.mm() </a>或者<a class="ae lb" href="https://pytorch.org/docs/stable/torch.html#torch.matmul" rel="noopener ugc nofollow" target="_blank"> torch.matmul() </a>。这里我们简单地将两个向量相乘，并加入偏差。然后我们把它发送给激活函数，得到结果。这种方法有一个技巧，我们将会看到，当我们以后处理复杂的神经网络时，这将会给我们很大的帮助。让我们编码它，看看事情。我们将使用先前声明的变量和函数。</p><pre class="ld le lf lg fd lm ln lo lp aw lq bi"><span id="bb14" class="lr jj ht ln b fi ls lt l lu lv">&gt;&gt;&gt; print(weights)<br/>  tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])<br/>&gt;&gt;&gt;print(features)<br/>  tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])<br/>&gt;&gt;&gt;print(bias)<br/>  tensor([[0.3177]])</span><span id="f6f3" class="lr jj ht ln b fi lw lt l lu lv">y = activation(torch.mm(features , weight) + bias)<br/>print(y)</span></pre><p id="5b35" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">看起来很简单，对吧？但是这里有一个真正的问题。我们的两个向量都是5 x 1维的，这意味着它们不能相乘，也就是说，要乘向量/矩阵，第一个向量的列数必须等于第二个向量的行数，在我们的例子中，这是不可能的。它返回以下错误。</p><pre class="ld le lf lg fd lm ln lo lp aw lq bi"><span id="3c3c" class="lr jj ht ln b fi ls lt l lu lv">---------------------------------------------------------------------------<br/>RuntimeError                              Traceback (most recent call last)<br/>&lt;ipython-input-13-15d592eb5279&gt; in &lt;module&gt;()<br/>----&gt; y=activation(torch.mm(features, weights)+ bias))</span><span id="382a" class="lr jj ht ln b fi lw lt l lu lv">RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033</span></pre><p id="4895" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">这是一个非常常见的错误，也是需要处理的最重要的错误之一。我们必须重塑我们的张量，使它们能够繁殖。我们必须重塑我们的权重向量，以获得正确的维度。幸运的是，我们有一些函数要处理。我们可以使用<a class="ae lb" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape" rel="noopener ugc nofollow" target="_blank">weights . shape()</a>或者<a class="ae lb" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize" rel="noopener ugc nofollow" target="_blank"> weights.resize() </a>或者<a class="ae lb" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view" rel="noopener ugc nofollow" target="_blank"> weights.view( </a>)。你可以在官方文档中看到它们之间的细微差别。我们将使用weights.view()</p><pre class="ld le lf lg fd lm ln lo lp aw lq bi"><span id="22c5" class="lr jj ht ln b fi ls lt l lu lv">&gt;&gt;&gt;weights.view(5,1)<br/>tensor([[-0.8948],<br/>        [-0.3556],<br/>        [ 1.2324],<br/>        [ 0.1382],<br/>        [-1.6822]])</span></pre><p id="83ec" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">让我们现在编码</p><pre class="ld le lf lg fd lm ln lo lp aw lq bi"><span id="d3d1" class="lr jj ht ln b fi ls lt l lu lv">y = activation(torch.mm(features , weights.view(5,1))+ bias)</span><span id="0187" class="lr jj ht ln b fi lw lt l lu lv">&gt;&gt;&gt;print(y)<br/>  tensor([[0.1595]])</span></pre><p id="ea9c" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">这就是我们在PyTorch中计算激活函数的方法。当我们的神经网络中有多层即隐藏层时，它甚至更强大。</p></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="c1a4" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">想了解更多关于PyTorch深度学习的知识？</p><blockquote class="me mf mg"><p id="f9f9" class="ka kb mh kc b kd kw iu kf kg kx ix ki mi ky kl km mj kz kp kq mk la kt ku kv hb bi translated">【PyTorch深度学习简介</p></blockquote><p id="9a26" class="pw-post-body-paragraph ka kb ht kc b kd kw iu kf kg kx ix ki kj ky kl km kn kz kp kq kr la kt ku kv hb bi translated">对这个领域完全陌生？从这里开始。</p><blockquote class="me mf mg"><p id="6c9b" class="ka kb mh kc b kd kw iu kf kg kx ix ki mi ky kl km mj kz kp kq mk la kt ku kv hb bi translated"><a class="ae lb" rel="noopener" href="/machine-learning-digest/beginners-learning-path-for-machine-learning-5a7fb90f751a">‘机器学习的初学者学习路径</a></p></blockquote></div></div>    
</body>
</html>