<html>
<head>
<title>TensorFlow 2.0: tf.data API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2.0: tf.data API</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tensorflow-2-0-tf-data-api-eaa9889186cc?source=collection_archive---------5-----------------------#2019-12-18">https://medium.com/analytics-vidhya/tensorflow-2-0-tf-data-api-eaa9889186cc?source=collection_archive---------5-----------------------#2019-12-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/143d47a62426310bc56db59f3d861435.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*Pbqe35QVxqoaz29uudUsNQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">ETL过程</figcaption></figure><p id="0b35" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果您还记得TensorFlow 1.x中只有队列可用作<strong class="iw hj"> ETL </strong>管道(提取/转换/加载)的数据结构，以及有时管理一些约束和隐含陷阱是多么困难。但是现在有一个非常成功的模块组织了整个<strong class="iw hj"> ETL </strong>管道:<strong class="iw hj"> tf.data </strong>。</p><p id="48a8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我想简单介绍一下tf.data API的介绍和用法。</p><p id="24de" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">tf.data处理不同类型的输入数据:</p><ol class=""><li id="18d9" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">战斗支援车</li><li id="2625" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">NumPy</li><li id="531d" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">文本</li><li id="5581" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">形象</li><li id="65d1" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">熊猫。数据帧</li><li id="a24a" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">TF。文本</li><li id="0b4d" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">统一码</li><li id="77a5" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">tfRecord和TF。例子</li></ol><p id="0b1e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">每种类型都有自己的类和方法来处理它们。</p><p id="e175" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">tf.data中的两个重要阶段:</p><ol class=""><li id="9417" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">创建存储输入数据的<strong class="iw hj">数据存储器</strong>。所有数据元素都变成了张量对象:</li></ol><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="dacf" class="kp kq hi kl b fi kr ks l kt ku">dataset = tf.data.Dataset.from_tensor_slices()</span><span id="2cf6" class="kp kq hi kl b fi kv ks l kt ku">dataset = tf.data.Dataset.from_tensors()</span></pre><p id="d90d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.定义<strong class="iw hj">转换</strong>并将链<strong class="iw hj">操作</strong>应用于数据集:</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="3648" class="kp kq hi kl b fi kr ks l kt ku">dataset.map()</span><span id="4655" class="kp kq hi kl b fi kv ks l kt ku">dataset.batch()<br/>    ... </span></pre><p id="ba21" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于<strong class="iw hj">数字</strong>示例:</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="6710" class="kp kq hi kl b fi kr ks l kt ku">dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))<br/>dataset = dataset.map(map_func=preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)<br/>dataset = dataset.batch(batch_size=batch_size).prefetch(buffer_size=prefetch_buffer_size)</span></pre><p id="c35f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如你所看到的，这是一个有趣的时刻:这个连锁经营由哪些部分组成？</p><p id="8198" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们考虑一下最有用的函数。</p><ol class=""><li id="7b4f" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated"><strong class="iw hj">映射:</strong>将给定的变换函数应用于输入数据。允许并行处理该过程。</li></ol><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="f46c" class="kp kq hi kl b fi kr ks l kt ku">dataset.map(map_func=preprocess,<br/>                      num_parallel_calls=tf.data.experimental.AUTOTUNE)</span></pre><p id="b218" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="kw"> num_parallel_calls </em>应该等于可用于转换的进程数。</p><p id="675e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="kw">TF . data . experimental . auto tune</em>定义适当数量的空闲工作进程。</p><p id="d741" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.<strong class="iw hj">批处理:</strong>将数据集分割成给定大小的子集。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="6645" class="kp kq hi kl b fi kr ks l kt ku">dataset.batch(batch_size=batch_size)</span></pre><p id="ddb6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.<strong class="iw hj">重复:</strong>多次重复数据集。当数据结束并且训练过程应该继续时，这是有用的，然后<em class="kw">重复</em>功能从最开始开始并且训练继续<em class="kw">计数</em>次。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="ea0e" class="kp kq hi kl b fi kr ks l kt ku">dataset = dataset.repeat(count=NROF_REPETITIONS)</span></pre><p id="a86c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">4.<strong class="iw hj"> shuffle: </strong>训练数据输入管道非常重要的功能。但是这种混洗需要<strong class="iw hj">缓冲器siz </strong> e，它负责将要混洗的元素的数量。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="1508" class="kp kq hi kl b fi kr ks l kt ku">dataset = dataset.shuffle(buffer_size=len(IMAGE_PATHS))</span></pre><p id="1102" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">每次需要数据时，它都会从缓冲区中获取数据。之后，缓冲区被最新的元素填充到给定的缓冲区大小。</p><p id="2dca" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">5.<strong class="iw hj">缓存:</strong>允许缓存数据集的元素以备将来重用。缓存数据将存储在内存(默认)或文件中。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="00e7" class="kp kq hi kl b fi kr ks l kt ku">dataset = dataset.cache(filename=CACHE_PATH)</span></pre><p id="8f9c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">6.<strong class="iw hj">预取:</strong> TensorFlow从时间和内存效率上非常清晰的展示了如何获取火车流水线。</p><p id="fbb8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">那是以前:</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/808f157d0ce322148f51236e5184f4c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ni_D_tAXKeSjEtDv"/></div></figure><p id="97bb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在:</p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/4faf10c986d3f5c859e9a629137f9792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OGhNZ2ldK3gZlGV6"/></div></figure><p id="c33a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">预取</strong>不允许CPU空闲。当模型正在训练时<em class="kw">预取</em>在GPU忙碌时继续准备数据。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="e5e0" class="kp kq hi kl b fi kr ks l kt ku">dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)</span></pre><p id="1580" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">7.<strong class="iw hj">交错:</strong>从不同的文件中读取数据，并行化<strong class="iw hj"> </strong>这个过程。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="fc28" class="kp kq hi kl b fi kr ks l kt ku">dataset = dataset.interleave(map_func=parse_files, cycle_length=NROF_READERS, block_length=1,<br/>num_parallel_calls=tf.data.experimental.AUTOTUNE)</span></pre><p id="65ec" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="kw"> parse_files </em> —从给定文件中读取输入数据；</p><p id="73f8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="kw"> cycle_length </em> —将同时处理的输入元素数量；</p><p id="b7b9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">block _ length在循环到另一个输入元素之前，从每个输入元素产生的连续元素的数量。</p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><h2 id="ea3a" class="kp kq hi bd lf lg lh li lj lk ll lm ln jf lo lp lq jj lr ls lt jn lu lv lw lx bi translated">最佳实践</h2><ol class=""><li id="c0c8" class="js jt hi iw b ix ly jb lz jf ma jj mb jn mc jr jx jy jz ka bi translated"><strong class="iw hj">地图</strong>和<strong class="iw hj">批次</strong>的顺序在性能上是有道理的。</li></ol><ul class=""><li id="399d" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr md jy jz ka bi translated">使用<strong class="iw hj">地图</strong>然后<strong class="iw hj">批量</strong>当地图是昂贵的功能时。每个批次都将由数据集中显示顺序的元素构成。但是，如果在map中使用了<em class="kw"> num_parallel_calls </em>，则不会保证给定数据集中呈现的元素顺序。</li><li id="653d" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr md jy jz ka bi translated">使用<strong class="iw hj">批量</strong>然后<strong class="iw hj">映射</strong>当映射是廉价功能时。在这种情况下，<strong class="iw hj">矢量化</strong>地图功能并同时处理整批元素是有意义的。<em class="kw"> num_parallel_calls </em>不会影响元素顺序。</li></ul><p id="f58f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.使用数据输入管道末端的<strong class="iw hj">预取</strong>来防止CPU闲置。但是如果<em class="kw"> map_func </em>增加输出数据的数量<strong class="iw hj">预取</strong>，<strong class="iw hj">混洗</strong>和<strong class="iw hj">重复</strong>应该在最开始使用以节省内存。</p><p id="05c4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.使用<strong class="iw hj">缓存</strong>缓存数据，目的是不要花费时间进行数据预处理。</p><p id="c060" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">4.不要忘记在方法中使用<em class="kw"> num_parallel_calls </em>，如果它有这样的属性。</p></div></div>    
</body>
</html>