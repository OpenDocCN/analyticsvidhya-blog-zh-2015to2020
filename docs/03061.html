<html>
<head>
<title>Adversarial Attacks and Data Augmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对抗性攻击和数据增强</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/adversarial-attacks-and-data-augmentation-1d97296b2d0c?source=collection_archive---------20-----------------------#2020-01-14">https://medium.com/analytics-vidhya/adversarial-attacks-and-data-augmentation-1d97296b2d0c?source=collection_archive---------20-----------------------#2020-01-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1349" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">几周前，有人向我介绍了对抗性攻击，我努力寻找对抗性攻击和数据增强之间的明显区别。</p><p id="e23f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的困惑始于这两个原因:</p><ol class=""><li id="87a8" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">两者都对应数据转换。</li><li id="88e6" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">目的是使神经网络在这两种情况下都是健壮的。</li></ol><p id="77d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑应用于两种情况的高斯分布。我的主管给了我一个很好的解释</p><p id="52ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们在数据扩充的情况下向图像添加高斯时，它可以是任何高斯，就像图像的噪声一样。但是，当我们在一幅图像中添加一个高斯模型以应对敌对攻击时，它将是“特定的”高斯模型，该模型已经被裁剪，因此神经网络可能会失败。</p><p id="7d0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你没有得到它，不要担心，让我们一个接一个地去。</p><p id="2df5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据扩充</strong>也是数据转换，但它用于获得更多数据并训练一个稳健的模型。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es jr"><img src="../Images/24b928d069379ca02d9c420cd058b586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*qdxy0rNUncghp-7GAog36w.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">叠加在典型图像上的对抗性输入会导致分类器将熊猫误归类为长臂猿。Pic鸣谢:开放人工智能研究</figcaption></figure><p id="05a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">对立的例子</strong>是那些输入，其中输入图像受到最坏情况扰动的攻击(或变换),使得扰动的输入导致训练模型以高置信度输出不正确的答案。这项技术显示了恶意对手如何操纵输入数据，从而危及机器学习系统的安全性。</p><p id="3ade" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，在训练时应用数据扩充以使模型稳健。然而，对抗性攻击(精心定制)应用于图像，然后通过一个经过训练的模型发送，以检查其鲁棒性和安全性。</p><p id="a586" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kd">参考文献</em> </strong></p><ol class=""><li id="20c4" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><a class="ae ke" href="https://openai.com/blog/adversarial-example-research/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/adversarial-example-research/</a></li><li id="70d3" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><a class="ae ke" href="https://en.wikipedia.org/wiki/Data_preparation" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Data_preparation</a></li><li id="c1f7" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><a class="ae ke" href="https://en.wikipedia.org/wiki/Adversarial_machine_learning" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Adversarial_machine_learning</a></li></ol></div></div>    
</body>
</html>