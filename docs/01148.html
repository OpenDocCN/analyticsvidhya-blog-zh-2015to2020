<html>
<head>
<title>Logistic Regression using Single Layer Perceptron Neural Network (SLPNN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于单层感知器神经网络的逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-using-single-layer-perceptron-neural-network-slpnn-31757c792d5d?source=collection_archive---------1-----------------------#2019-10-04">https://medium.com/analytics-vidhya/logistic-regression-using-single-layer-perceptron-neural-network-slpnn-31757c792d5d?source=collection_archive---------1-----------------------#2019-10-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/02916a3686492410685b23a5fcfb82e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xKlftK1s0cchv7lT.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://blogs.nvidia.com/wp-content/uploads/2018/12/xx-ai-networks-1280x680.jpg" rel="noopener ugc nofollow" target="_blank">https://blogs . NVIDIA . com/WP-content/uploads/2018/12/xx-ai-networks-1280 x680 . jpg</a></figcaption></figure><h1 id="1e0f" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">10周的机器学习乐趣–第4–10周回顾</h1><h1 id="e57e" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h1><p id="fd90" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">欢迎来到ML挑战赛的最后回顾，这将涵盖第4到10周。作为一个快速的介绍，对于那些想要完成10周旅程的人来说，这里是所有以前帖子的链接:</p><p id="e0c2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">关于挑战的原始帖子:<a class="ae iu" href="https://www.linkedin.com/feed/hashtag/?keywords=%2310WeeksOfMachineLearningFun" rel="noopener ugc nofollow" target="_blank"># 10 weeksofmachinengfun</a></p><p id="dc1d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">往期回顾链接:<a class="ae iu" href="https://www.linkedin.com/post/edit/10-weeks-machine-learning-fun-week-1-retrospective-tzimopoulos" rel="noopener ugc nofollow" target="_blank">#第一周</a><a class="ae iu" href="https://www.linkedin.com/pulse/10-weeks-machine-learning-fun-week-2-retrospective-tzimopoulos/" rel="noopener ugc nofollow" target="_blank">#第二周</a><a class="ae iu" href="https://www.linkedin.com/pulse/10-weeks-machine-learning-fun-week-3-retrospective-part-evangelos/" rel="noopener ugc nofollow" target="_blank">#第三周</a></p></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><p id="0044" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">第4–10周现已结束，挑战也已结束！</p><p id="5efa" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我很高兴能走到这一步，也很兴奋能告诉你我学到的所有东西，但首先要做的是:快速解释一下为什么我要总结剩下的几周，而且是在完成后这么晚:</p><ul class=""><li id="9234" class="ld le hi jv b jw kr ka ks ke lf ki lg km lh kq li lj lk ll bi translated">我在<a class="ae iu" href="https://www.linkedin.com/pulse/10-weeks-machine-learning-fun-week-3-retrospective-part-evangelos/" rel="noopener ugc nofollow" target="_blank"> #Week3 </a>中为逻辑回归选择的方法(使用单层感知器神经网络的近似逻辑回归函数— SLPNN)花费了更长的时间来解释，无论是从数学还是从编码的角度来看，每周提供更新几乎是不可能的</li><li id="435e" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">此外，在那段时间里，我可能会偏离主题去理解一些数学知识，总体来说这是很好的学习方法，例如成本函数及其导数，最重要的是何时使用一个而不是另一个，以及为什么:)(更多信息请见下文)</li><li id="98dd" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">成本函数的导数:考虑到我在<a class="ae iu" href="https://www.linkedin.com/pulse/10-weeks-machine-learning-fun-week-3-retrospective-part-evangelos/" rel="noopener ugc nofollow" target="_blank"> #Week3 </a>中的方法，我必须确保反向传播链规则数学用于计算成本函数相对于权重的偏导数，与相同偏导数的分析计算数学完美结合。为了做到这一点，我不得不一遍又一遍地把笔写在纸上，直到它最终有了意义。一旦你用两种不同的方法验证了数学，你就可以完全相信它们了！</li><li id="ec88" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">事实上，我已经创建了一个手写的单页备忘单，显示了所有这些，我正计划单独发布，所以敬请关注。</li><li id="4561" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">最后，最初计划在第4-10周花在挑战上的相当一部分时间，被用于职业和个人生活中的现实生活优先事项。这正是在工作、项目、生活中发生的事情…你只需要处理好优先事项，回到你正在做的事情上，完成工作！所以我来了！</li></ul></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="81ad" class="iv iw hi bd ix iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js bi translated">数据集</h1><p id="6938" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在我们回到逻辑回归算法和我在<a class="ae iu" href="https://www.linkedin.com/pulse/10-weeks-machine-learning-fun-week-3-retrospective-part-evangelos/" rel="noopener ugc nofollow" target="_blank"> #Week3 </a>中离开的地方之前，我想谈谈所选的数据集:</p><h2 id="e456" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">玻璃数据集</h2><p id="ed59" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">使用该数据集有三个主要原因:</p><ol class=""><li id="10be" class="ld le hi jv b jw kr ka ks ke lf ki lg km lh kq mk lj lk ll bi translated">这个数据集已经被用于分类玻璃样品是否是一个“窗口”类型的玻璃，这是完美的，因为我的意图是解决一个二元分类问题</li><li id="4e03" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq mk lj lk ll bi translated">如数据集本身所述，尽管是一个精选数据集，但它确实来自真实生活用例:<em class="ml">“对玻璃类型分类的研究是由犯罪学调查推动的。在犯罪现场，留下的玻璃可以作为证据…如果鉴定正确的话！</em></li><li id="82a6" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq mk lj lk ll bi translated">最后，作为由Randy Lao和Harpreet Sahota以及令人惊叹的DataScienceDreamJob团队举办的技术技能研讨会的一部分，这意味着我可以使用参考资料来验证我的结果，并且不会迷路(尽管我还是迷路了…哈哈！).</li></ol><p id="ac08" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">数据汇总和特点</strong></p><p id="ce71" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">玻璃数据集由10列214行组成，9个输入要素和1个输出要素为玻璃类型:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/c0b74bbaf9642ab0cf447c650fcd3dcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*HCbAE5YrBjthsW-VlyvZmQ.png"/></div></figure><p id="a865" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">关于数据集的更多详细信息可以在补充记事本文件中的<a class="ae iu" href="https://archive.ics.uci.edu/ml/datasets/glass+identification" rel="noopener ugc nofollow" target="_blank">这里</a>找到。简而言之，玻璃数据集包括折光率(第2列)、每个玻璃样品的成分(每行)及其金属元素(第3-10列)和玻璃类型(第11列)。</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/ac655136bfe2ad415421f39268768d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*cviCHiBJqaXuDGR-dzOndQ.png"/></div></figure><p id="55c5" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">基于后者，玻璃类型属性11，可以使用该数据集尝试2种分类预测:</p><ul class=""><li id="0aa8" class="ld le hi jv b jw kr ka ks ke lf ki lg km lh kq li lj lk ll bi translated"><strong class="jv hj">窗</strong>(类型1-4)vs<strong class="jv hj">非窗</strong>(类型5-7)或</li><li id="1b64" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated"><strong class="jv hj">浮动</strong>(类型1 &amp; 3) vs <strong class="jv hj">非浮动</strong>(类型2 &amp; 4) vs <strong class="jv hj">不适用</strong>(类型5–7)</li></ul><p id="19be" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">第一个是经典的<em class="ml">二元分类问题</em>。第二个可以被视为具有三个类别的<em class="ml">多类别分类</em>问题，或者如果想要预测“浮动对静止”类型的眼镜，可以将剩余类型(非浮动、不适用)合并为单个特征。</p><p id="fbaa" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">例如，通过屏蔽原始输出特性的第一种方法的代码片段:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="d57a" class="lw iw hi mv b fi mz na l nb nc"># glass_type 1, 2, 3 are window glass captured as "0"<br/># glass_type 5, 6, 7 are non-window glass captured as "1"</span><span id="af0e" class="lw iw hi mv b fi nd na l nb nc">df['Window'] = df.glass_type.map({1:0, 2:0, 3:0, 4:0, 5:1, 6:1, 7:1})</span></pre><p id="3db4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">新设计的“窗口”输出:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/f22c81cec2be14b08c2a8338e2920daa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Ced4fW_mA-QaxQJDIglTKw.png"/></div></figure><p id="80e4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">包含所有输入和新输出的数据帧现在如下所示(包括浮点特性):</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/fc4b64e6203c057f2273a9493de97f28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*9I0Zyp0Mwtluah7EfbUK2g.png"/></div></figure><p id="50a9" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">展望未来，出于本文<strong class="jv hj">的目的，重点将是预测“窗口”输出</strong>。即哪些输入变量可用于预测玻璃类型是否为窗户。</p><h2 id="21cd" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">虹膜数据集</h2><p id="3ecd" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">最初，我不打算使用另一个数据集，但最终我转向home-sweet-home Iris来解决一些实现挑战，并通过使用更简单的数据集进行编码来测试我的假设。这一点，加上我对glass数据集所做的一些特性选择，证明对我所面临的所有问题都非常有用，最终能够正确地调整我的模型。</p><p id="2eb9" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">数据汇总及特点</strong></p><p id="9a88" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">对于虹膜数据集，我在这里借用了<a class="mm mn ge" href="https://medium.com/u/2d1b498ccdb5?source=post_page-----31757c792d5d--------------------------------" rel="noopener" target="_blank">马丁·佩拉罗洛</a> <a class="ae iu" rel="noopener" href="/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac">提出的一个非常方便的方法</a>将3种原始虹膜类型转化为2种，从而将此转化为<em class="ml">二元分类问题</em>:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="91fa" class="lw iw hi mv b fi mz na l nb nc">from sklearn import datasets<br/>iris = datasets.load_iris()<br/>X = iris.data[:, :2]<br/>y = (iris.target != 0) * 1</span></pre><p id="50a7" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">其给出了输入和输出变量的以下散点图:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/53297ea3df2ce4490ac55921fa03efc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*RWha94zduhkiev3ywSuJDA.png"/></div></figure></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="cc2b" class="iv iw hi bd ix iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js bi translated">单层感知器神经网络</h1><p id="0373" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">单层感知器是只有一个神经元的最简单的神经网络，也称为McCullock-Pitts (MP)神经元，它转换触发激活函数的输入的加权和，以生成单个输出。下面是这种神经网络的示例图，其中X为输入，θI为权重，z为加权输入，g为输出</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/8fdfb82578e1731fec148e705d37a08b.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*3D98kAjSgrbyLgx2k74a3g.png"/></div></figure><h2 id="d264" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">逻辑回归假设</h2><p id="947c" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了我们实验的目的，我们将使用这个单神经元NN来预测我们创建的窗口类型特征，基于输入是它所包含的金属元素，使用逻辑回归。因此，我们使用分类算法来预测值为0或1的二进制输出，表示我们假设的函数是Sigmoid函数，也称为逻辑函数。</p><p id="8c7b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">神经网络输入</strong></p><p id="321f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">神经网络的输入是输入Xi的加权和:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/7dd853ed822dda42e119c8c09f7e6927.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*X_AezwxUNqLh4lNV3dBQjw.png"/></div></figure><p id="011d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">激活功能</strong></p><p id="d0dd" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">使用激活函数对输入进行变换，激活函数生成从0到1的概率值:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/8374ecccec276eff7c3c70a572a36b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*jb_Hd7OCGP7gHyrLx1mvbg.png"/></div></figure><p id="f518" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">描述它的数学方程式是:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/60945192962b53fbd9a7ca0cc0749f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*n4KxVIEXv0ZkKRJPw5UTMQ.png"/></div></figure><p id="a77b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">实现它的代码片段:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="f8df" class="lw iw hi mv b fi mz na l nb nc">def sigmoid(self, x):<br/>        return 1 / (1 + np.exp(-x))</span></pre><p id="3ee4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">假设</strong></p><p id="e0c9" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果我们结合以上所有内容，我们可以为我们的分类问题制定假设函数:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nk"><img src="../Images/0f674307e3d4d7b297e1b9972d015f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*HRpn6TcV0A4tMZnXO1zTyw.png"/></div></div></figure><p id="29b4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">前馈回路</strong></p><p id="c608" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">因此，我们可以通过使用以下函数运行神经网络的前向循环来计算输出<em class="ml"> h </em>:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="5229" class="lw iw hi mv b fi mz na l nb nc">def feedforward(self,X):<br/>        #X = self.add_bias(X)<br/>        z = np.dot(X, self.w)<br/>        h = self.sigmoid(z)<br/>        return(h)</span></pre><h2 id="6676" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">成本函数</h2><p id="4dca" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">选择正确的成本函数至关重要，并且需要对正在解决的优化问题有更深入的理解。</p><p id="646c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最初，我假设最常见的优化函数之一，<strong class="jv hj"> <em class="ml">最小二乘</em> </strong>足以解决我的问题，因为我以前在更复杂的神经网络结构中使用过它，老实说，取预测输出与实际输出的平方差最有意义:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/12846841db6a3b59101e3b30de8716ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*wK3NCHnH5zcReateXMPryw.png"/></div></figure><p id="a80c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">不幸的是，这使我陷入困境和困惑，因为我无法将误差降至可接受的水平，并且查看数学和编码，它们似乎与我当时为获得一些帮助而研究的类似方法不匹配。</p><p id="79d5" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">拯救我的是<a class="mm mn ge" href="https://medium.com/u/592ce2a67248?source=post_page-----31757c792d5d--------------------------------" rel="noopener" target="_blank">吴恩达</a>关于逻辑回归成本函数的在线课程的讲义(<a class="ae iu" href="http://www.holehouse.org/mlclass/06_Logistic_Regression.html" rel="noopener ugc nofollow" target="_blank">第6章</a>)。底线是对于特定的分类问题，我使用了一个非线性函数作为假设，sigmoid函数。出于优化目的，sigmoid被认为是一个<strong class="jv hj">非凸函数</strong>，具有多个<em class="ml">局部最小值</em>，这意味着它不会总是收敛。</p><p id="8822" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这个问题的答案是使用凸逻辑回归成本函数，即<strong class="jv hj"> <em class="ml">交叉熵损失</em> </strong>，它可能看起来很长很吓人，但给出了一个非常简洁的梯度公式，如下所示:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nm"><img src="../Images/306a71dc5593d2103a96e1dab4553619.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*Yb7kWm-lbvsITW5M8PJz-w.png"/></div></figure><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="b6bc" class="lw iw hi mv b fi mz na l nb nc"># Defining the Cost function J(θ) (or else the Error)<br/># using the Cross Entropy function<br/>    def error(self,h, y):<br/>        error = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()<br/>        self.E = np.append(self.E, error)</span></pre><h2 id="fa85" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">梯度下降—分析计算</h2><p id="809d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">使用分析方法，这里的下一步将是计算梯度，这是每次迭代的步骤，通过它算法收敛到全局最小值，因此得名<strong class="jv hj">梯度下降</strong>。</p><p id="b767" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在数学术语中，这只是成本函数相对于权重的偏导数。即，在每次迭代中，计算权重的调整量(或增量):</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nn"><img src="../Images/93e8611b7001cc7fefac9b14fa46ba95.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*z-eJ_lLRkSAAp3dVBPEjVw.png"/></div></figure><p id="7b53" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">并在每次迭代中重复</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es no"><img src="../Images/438da7b78e4ddb26735a6032cb364f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*ujUAUSr0om9Zv-uAkR4TOQ.png"/></div></figure><h2 id="33b9" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">使用反向传播链规则的梯度下降</h2><p id="3da3" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在这里，我将使用反向传播链规则来得出梯度下降的相同公式。</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/8fdfb82578e1731fec148e705d37a08b.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*3D98kAjSgrbyLgx2k74a3g.png"/></div></figure><p id="62a4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如上图所示，为了计算成本函数相对于权重的偏导数，使用链式法则，可将其分解为3个偏导数项，如下式所示:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es np"><img src="../Images/7dcfb97d4f3f6741eb6099df9473e8f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*8CncwiwZkiyo5s9XUQNMaw.png"/></div></figure><p id="19f6" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">期限(1) </strong></p><p id="be2c" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果我们对J(θ)相对于h进行微分，我们实际上是把<em class="ml"> log(h) </em>和<em class="ml"> log(1-h) </em>的导数作为J(θ)的两个主要部分。稍微整理一下数学，我们得出了下面这个术语:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/84c493632bee7250169a39bca7e53364.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*ZxwUe6cmXWst8IRz1luz-w.png"/></div></figure><p id="a3eb" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">期限(2) </strong></p><p id="9e41" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">第二项是sigmoid函数的导数:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nq"><img src="../Images/3539cca73bcc8fc3b9b3fa9834feecdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*RFl_Gh5dOu8yDxDRKS_5Yg.png"/></div></figure><p id="4c99" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">期限(3) </strong></p><p id="13aa" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">第三项就是输入向量X:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nr"><img src="../Images/49947b497993c355be139a2ff52d3067.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/1*vU6Z2yTsD2e_pXX8rhdLPg.png"/></div></figure><p id="c7de" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如果我们在计算中用3项代替J’，我们最终会得到上面看到的使用分析方法的梯度的swift方程:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es ns"><img src="../Images/4bd855bfdb217ac0c26ece2bd6734837.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*AxtHE-XRze4ZWJ31NCKHIg.png"/></div></figure><p id="d792" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这作为神经网络类中的一个函数的实现如下:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="6276" class="lw iw hi mv b fi mz na l nb nc">def backprop(self,X,y,h):<br/>        self.delta_E_w = np.dot(X.T,h-y) / self.outputLayer<br/>        <br/>        # Store All weights throughout learning<br/>        self.w_list.append(self.w)<br/>        <br/>        # Adjust weights<br/>        self.w = self.w - eta * self.delta_E_w</span></pre><h2 id="5c3a" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">摘要-方程式</h2><p id="712e" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">作为总结，在我们的例子中计算梯度下降所涉及的全套数学如下:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/0631ac7b2cceb167d23a4e0602e152a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*5Sh348AN0kByAHVjmmKd6A.png"/></div></figure><h2 id="57cd" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">预测和分类</h2><p id="3ae5" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了根据任何新的输入预测输出，利用前馈回路实现了以下功能:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="7b63" class="lw iw hi mv b fi mz na l nb nc">def predict(self, X):<br/>        # Forward pass<br/>        pred = self.feedforward(X)<br/>        return pred</span></pre><p id="3cfc" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">如上所述，结果是预测的<em class="ml">概率</em>，即输出是任一窗口类型。为了将它转化为一个分类，我们只需要设置一个阈值(这里是0.5)并将结果向上或向下舍入，取最接近的值。</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="e233" class="lw iw hi mv b fi mz na l nb nc">def classify(self, y):<br/>        return self.predict(y).round()</span></pre></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="102f" class="iv iw hi bd ix iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js bi translated">神经网络训练和参数</h1><p id="dc05" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了训练神经网络，对于每次迭代，我们需要:</p><ul class=""><li id="b7de" class="ld le hi jv b jw kr ka ks ke lf ki lg km lh kq li lj lk ll bi translated">通过正向循环传递输入X来计算输出</li><li id="22a9" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">运行反向传播来计算权重调整</li><li id="da26" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">应用权重调整并在下一次迭代中继续</li></ul><p id="6515" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">要实现的功能如下所示:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="c6e5" class="lw iw hi mv b fi mz na l nb nc">def train(self,X,y):<br/>    for epoch in range(epochs):<br/>      # Forward pass<br/>      h = self.feedforward(X)<br/>                        <br/>      # Backpropagation - Calculate Weight adjustments and update weights<br/>      self.backprop(X,y,h)<br/>            <br/>      # Calculate error based on the Cross Entropy Loss function<br/>      self.error(h, y)</span></pre><p id="9b65" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">此外，下面是用于NN的参数，其中eta是学习率，代表迭代次数。</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="1659" class="lw iw hi mv b fi mz na l nb nc"># Learning Rate<br/>eta = 0.001</span><span id="ef64" class="lw iw hi mv b fi nd na l nb nc"># Number of epochs for learning<br/>epochs = 10000</span><span id="4912" class="lw iw hi mv b fi nd na l nb nc"># Input layer<br/>inputLayer = X.shape[1]</span><span id="3242" class="lw iw hi mv b fi nd na l nb nc"># Output Layer<br/>outputLayer = 1</span></pre><h2 id="2a9e" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">培训和选择功能</h2><p id="713f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">正如在数据集部分所讨论的，原始数据有9个原始特征，在为训练选择正确的特征时，正确的方法是使用变量和输出之间的散点图，并且通常将数据可视化，以更深入地理解和直观地了解起点。</p><h2 id="2f43" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">只有“Al”的输入向量</h2><p id="5f09" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">由于这是一个基于Randy Lao 对使用该glass数据集的逻辑回归的介绍的引导式实现，我最初使用了以下输入向量:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="d390" class="lw iw hi mv b fi mz na l nb nc"># Selecting Independent Variables<br/><strong class="mv hj">iv = ['al']</strong><br/># Define Input vector for training<br/>X = df[iv].values</span></pre><p id="8fc1" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这给出了输入和输出之间的以下散点图，表明可能存在可用于相应分类的估计的sigmoid函数:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/25807fce8f941a321f43e1e6bad81a8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*JDt5I103O2TzqOKJ-oL9wg.png"/></div></figure><p id="735e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在测试过程中，尽管证明仅使用一个特性很难将误差降低到非常小的值，如下所示:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nu"><img src="../Images/d9046c90febc1c41ceb005a482caa9cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*ZqwYp8myFk_diZLFTNMB-g.png"/></div></figure><h2 id="9fe2" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">具有5个特征的输入</h2><p id="acd8" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了减少误差，进一步的实验导致选择输入向量的5个特征配置:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="76e2" class="lw iw hi mv b fi mz na l nb nc"># Selecting Independent Variables<br/><strong class="mv hj">iv = ['ri','mg','al','k','ca']</strong><br/># Define Input vector for training<br/>X = df[iv].values</span></pre><p id="9376" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最后，运行NN训练的代码的主要部分如下:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="bc4a" class="lw iw hi mv b fi mz na l nb nc"># Initiate Single Perceptron NN<br/>SPNN = LogisticRegressionSinglePerceptronModel(eta, inputLayer, outputLayer)</span><span id="6bd0" class="lw iw hi mv b fi nd na l nb nc"># Train SPNN for Linear Regression Model<br/>%time SPNN.train(X, y)</span><span id="e660" class="lw iw hi mv b fi nd na l nb nc"># Plot Error<br/>SPNN.plot()</span><span id="3006" class="lw iw hi mv b fi nd na l nb nc"># Predict output based on test set<br/>pred = SPNN.predict(X)</span><span id="eb2b" class="lw iw hi mv b fi nd na l nb nc"># Generate classified output<br/>pred2 = SPNN.classify(X)</span><span id="f4a3" class="lw iw hi mv b fi nd na l nb nc"># Assess Model accuracy<br/>print("Minimum Error achieved:", min(SPNN.E))</span><span id="e46d" class="lw iw hi mv b fi nd na l nb nc"># SPNN weights<br/>SPNN.w</span></pre></div><div class="ab cl kw kx gp ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="hb hc hd he hf"><h1 id="5b6b" class="iv iw hi bd ix iy lr ja jb jc ls je jf jg lt ji jj jk lu jm jn jo lv jq jr js bi translated">结果</h1><h2 id="be15" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">结果和估价</h2><p id="ca83" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">代码在大约313毫秒内运行，并产生一条快速收敛误差曲线，最终值为0.15:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/a6033116d6fe5d376935b15c2a31c70e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*7K1XMVjjMnpsSx-xk_q4mw.png"/></div></figure><p id="e3f8" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">末尾的数组是可用于预测新输入的最终权重。</p><h2 id="b63f" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">预测产量</h2><p id="f53c" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">训练后的实际输出向量与预测输出向量的比较表明预测已经(大部分)成功:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nv"><img src="../Images/9e0099da511d34de0db3affef2296d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ic424GI8CYJFtSG8rro_dA.png"/></div></div></figure><h2 id="42e5" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">使用虹膜数据集分类的验证</h2><p id="be7a" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">鉴于神经网络类的通用实现，我能够为第二个数据集重新部署代码，即众所周知的Iris数据集。如前所述，这样做既是为了验证，也是为了解决我当时面临的一些数学和编码问题。</p><p id="c0a7" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">正如这篇文章的<em class="ml">虹膜数据集</em>部分所描述的，通过一个小的操作，我们已经将虹膜分类变成了二进制分类。</p><h2 id="07ec" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">SLPNN配置</h2><p id="8e35" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">对于虹膜数据集的新配置，我显著降低了学习速率和时期:</p><pre class="mp mq mr ms fd mu mv mw mx aw my bi"><span id="4dd2" class="lw iw hi mv b fi mz na l nb nc"># Learning Rate<br/>eta = 0.0005</span><span id="bc7a" class="lw iw hi mv b fi nd na l nb nc"># Number of epochs for learning<br/>epochs = 3000</span><span id="f566" class="lw iw hi mv b fi nd na l nb nc"># Input layer<br/>inputLayer = X.shape[1]</span><span id="d5ad" class="lw iw hi mv b fi nd na l nb nc"># Output Layer<br/>outputLayer = 1</span></pre><h2 id="7a50" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">误差曲线</h2><p id="ff8b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">正如所预期的，训练时间比Glass数据集小得多，并且该算法非常快地实现了小得多的误差。</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nw"><img src="../Images/b9d09adc81b2fc798b2bdad5d2f05ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*PdutksntUMmqjWu4J9KVLQ.png"/></div></figure><h2 id="d351" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">预测产量</h2><p id="3a5f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">根据数据集示例，我们还可以检查生成的输出与预期的输出，以验证结果:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/c05901db61c21efe7779c17b79ae315b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*mYF3E-yyz5pco29jYLTQRQ.png"/></div></figure><h2 id="f3bf" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">回归线</h2><p id="4533" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">基于预测值，绘制的回归线如下所示:</p><figure class="mp mq mr ms fd ij er es paragraph-image"><div class="er es nx"><img src="../Images/b8ba155124c9cda673501d91e16c49ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*eNnwWbmQIDhqzQpQAAF5Nw.png"/></div></figure><h1 id="04f0" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">结论和总结</h1><p id="81cc" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">作为总结，在这个实验中，我涉及了以下内容:</p><ul class=""><li id="20c5" class="ld le hi jv b jw kr ka ks ke lf ki lg km lh kq li lj lk ll bi translated">讨论了使用<em class="ml">逻辑回归</em>实现用于<em class="ml">二元分类</em>的<strong class="jv hj">单层感知器神经网络</strong></li><li id="76fe" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">详述了神经网络输入和激活函数背后的数学原理</li><li id="2242" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">分析了逻辑回归算法的假设和成本函数</li><li id="84c4" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">使用两种方法计算梯度:反向传播链规则和分析方法</li><li id="e4a9" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">使用两个数据集来测试该算法，主要的数据集是Glass数据集，虹膜数据集用于验证</li><li id="8cd3" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">展示结果，包括误差图、曲线图和比较输出，以验证调查结果</li></ul><h1 id="cce2" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">积压</h1><p id="ab4f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">根据以前的帖子，我一直在维护和管理一周内的积压活动，所以我可以在挑战完成后再去处理它们。这是完整的列表:</p><p id="9312" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">1.#week1 —手动实现其他类型的编码和至少一种类型，不使用库</p><p id="c1ef" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">2.# week1重构神经网络类，以便输出层大小可配置</p><p id="e710" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">3.# week2使用梯度下降求解线性回归示例</p><p id="3201" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">4.#week2 —将线性回归模型预测和计算应用于真实数据集(“广告”数据集或来自Kaggle的<a class="ae iu" href="https://www.kaggle.com/sohier/calcofi" rel="noopener ugc nofollow" target="_blank"> this </a>数据集)</p><p id="12a4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">5.#第3周——阅读最大似然估计(MLE)的分析计算，并使用它重新实现逻辑回归示例(无库)</p><p id="5a75" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">6.#week4_10 —添加更多关于逻辑算法实施的验证措施</p><p id="2e7d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">7.#week4_10 —使用sklearn库实现玻璃组分类，以比较性能和准确性</p><h1 id="bdb5" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">#生产力#效率#弹性</h1><p id="8759" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">完成这个为期10周的挑战后，我对自己解决数据科学问题的方法、我的数学和统计知识以及我的编码标准有了更多的信心。</p><p id="a603" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">话虽如此，我仍需要改进的3个方面是:</p><p id="6612" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">a)我解决数据科学问题的方法</p><p id="97ea" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">b)我的数学和统计知识以及</p><p id="96dc" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">c)我的编码标准</p><p id="1b89" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">哈哈…它永远不会结束，享受旅程，学习，学习，学习！</p><h2 id="7deb" class="lw iw hi bd ix lx ly lz jb ma mb mc jf ke md me jj ki mf mg jn km mh mi jr mj bi translated">关于我的旅程</h2><ul class=""><li id="7099" class="ld le hi jv b jw jx ka kb ke ny ki nz km oa kq li lj lk ll bi translated">正如介绍中提到的，我不久前开始了为期10周的挑战，但只能在前3周每周发表一次。这是每周都要做的事情:破解数学(我的方法是在可能的情况下不使用库来实现主要的ML算法)，实现和测试，每周日写下来</li><li id="3101" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">那毕竟是在两个阵营都有疯狂项目的时期，家庭和职业责任😊</li><li id="819b" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">尽管如此，我还是后退了一步，专注于理解概念和数学，取得真正的进步，即使这意味着进展会更慢，并且已经违反了我的规则。于是，我停止了出版，继续工作。</li><li id="2687" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">然后，我有一个计划好的家庭假期，我也很期待，所以在重新投入工作之前，我又休息了很长时间。</li><li id="97f4" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">这是你可能再也回不来了的临界点！</li><li id="d60a" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">但我做到了，并陷入了同样的问题，并继续，因为我真的很想得到这一行。</li><li id="3fc6" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">每周有4到5天早上4:30起床，这对于每周工作6到8小时至关重要</li><li id="cc16" class="ld le hi jv b jw lm ka ln ke lo ki lp km lq kq li lj lk ll bi translated">早上那么早意味着注意力100%集中。也就是说，6-8小时的净工作时间实际上意味着我每周要多工作1-2天！</li></ul><p id="4a54" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最后，我想说的最后一点是，我建议任何开始从事数据科学的人尝试类似的东西，看看他们能走多远，并推动自己。</p><p id="94ba" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我很想听听已经做了类似的事情或打算做的人的意见。</p><p id="6b3b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">此外，任何想要尝试我的代码的极客，给我一个欢呼，并乐意分享这一点，我仍然在整理我的GitHub帐户。</p><p id="ddca" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">给我留下你的评论和反馈，感谢你阅读了这么多。</p><p id="9ece" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">安吉洛（男子名）</p><p id="32b4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><a class="ae iu" href="https://www.linkedin.com/feed/hashtag/?keywords=%23machinelearning" rel="noopener ugc nofollow" target="_blank"># machine learning</a><a class="ae iu" href="https://www.linkedin.com/feed/hashtag/?keywords=%23datascience" rel="noopener ugc nofollow" target="_blank"># data science</a><a class="ae iu" href="https://www.linkedin.com/feed/hashtag/?keywords=%23python" rel="noopener ugc nofollow" target="_blank"># python</a><a class="ae iu" href="https://www.linkedin.com/feed/hashtag/?keywords=%23LogisticRegression" rel="noopener ugc nofollow" target="_blank"># logistic regression</a></p></div></div>    
</body>
</html>