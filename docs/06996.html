<html>
<head>
<title>Introduction to Crowd Density Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人群密度估计简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-crowd-density-estimation-bc60223f363b?source=collection_archive---------4-----------------------#2020-06-10">https://medium.com/analytics-vidhya/introduction-to-crowd-density-estimation-bc60223f363b?source=collection_archive---------4-----------------------#2020-06-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c3ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">转载自<a class="ae jd" href="https://www.katnoria.com/crowd-density/" rel="noopener ugc nofollow" target="_blank">原帖</a></p><p id="7131" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我们将建立一个基于简单网络架构的对象计数模型。尽管我们在这里使用了群体数据集，但类似的解决方案也可以应用于更有用的应用，例如计数细胞、作物、水果、树木、牛，甚至是野生濒危物种。</p><p id="c639" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在给定的图像中，有不同的方法来计数对象。可以利用基于R-CNN的模型进行对象检测，如下例所示</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/ccb67dd2594ca79706b03ac3d7276fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23YzzzHcoQVaEteggRGWjg.png"/></div></div></figure><p id="080a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这很好，但是当你有更多像图1这样的人时，你该怎么办呢？同样的假设成立吗？我们可以访问R-CNN及其变体所使用格式的带标签的数据集吗？</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jq"><img src="../Images/7763ee162ce4d41d19df34a0d5b19aea.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*Ss22BqwnxYXs81ghRIh04g.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">图1，来源:上海理工大学数据集</figcaption></figure><p id="f187" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我们将使用预先训练好的ConvNet作为主干，并使用回归头来计算人群，从而构建尝试解决这一问题的模型。</p><p id="bbca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">网络架构非常简单，我认为这可以称为人群密度估计任务的“Hello World”(如果您知道更简单的方法，请原谅我的无知)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jv"><img src="../Images/1ff433380d85cb93d57de0bf7264bcb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gpasTA39ujc0RLZZaKQZbQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">高水位流量</figcaption></figure><p id="0190" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们叠加输出，即图像上的密度图，我们可以看到每个人的头部被突出显示。这些突出显示的点是我们希望我们的模型学会估计的。为了得到总数，我们将这些点加在一起。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jw"><img src="../Images/e2c1c59e0ac74d73f01ab85957d479c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ir6CTyHLANdAkKmmnxIsMg.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">来自上海理工大学数据集的样本图像和密度图</figcaption></figure><h1 id="12d2" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">资料组</h1><p id="ae08" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">我们使用在这篇<a class="ae jd" href="https://www.katnoria.com/crowd-density/Single-Image%20Crowd%20Counting%20via%20Multi-Column%20Convolutional%20Neural%20Network" rel="noopener ugc nofollow" target="_blank">论文</a>中介绍的人群计数数据集。该数据集被称为“上海科技馆人群计数数据集”，它具有任意人群密度的图像以及目标标签。我们在数据集的A部分训练我们的模型。然而，为了方便起见，我们没有使用数据集提供的密度图，而是使用由<a class="ae jd" href="https://github.com/gjy3035/C-3-Framework" rel="noopener ugc nofollow" target="_blank"> C3框架</a>生成的经过处理的图。C3框架是一个极好的资源，它涵盖了多种网络架构及其在不同数据集上的性能。我鼓励你看看报纸和他们的回购协议。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es la"><img src="../Images/13b8722c376a62fb8880e42332faf94a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dENC42qIsnCi0wT0-odNzQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">训练集直方图</figcaption></figure><p id="04d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面我们展示了数据集中的一些样本图像。我们还在每张图片下方显示了相关的密度图。注释数据集一定是一项艰巨的任务。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lb"><img src="../Images/5f4a2bc6c15a5b9c677623796e825ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Z7LdakVPS5RtiivCYCyfg.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">数据集示例</figcaption></figure><h1 id="29c0" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">预处理</h1><p id="4fa2" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">在整个实施过程中，我们遵循C3框架使用的指导方针和技术。C3框架在PyTorch中使用了以下扩展/转换:</p><p id="f94f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">中央裁剪(至224) →随机翻转→按比例缩小→标签规格化(100) →总传感器→规格化</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jv"><img src="../Images/8f88954b11460ae35c9fc9c6d46ac990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mS_zdIwRZ5BcttSpPnZQPA.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">Pytorch中的图像转换</figcaption></figure><h1 id="a681" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">模型</h1><p id="4b0f" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">在本帖中，我们将使用VGG16作为模型的主干。一旦我们准备好了完整的培训和评估基础架构，我们就可以轻松地添加更强大的模型，并将其性能与基线模型进行比较。</p><h1 id="3fa1" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">基线模型</h1><p id="a569" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">作为我们的基线，我们将使用预训练的VGG16网络，然后是2个Conv层和上采样层，以匹配目标密度图(m x n)。回想一下，我们通过一个缩放因子来缩小输入图像和目标，所以最终的图层需要考虑到这一点。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lc"><img src="../Images/6c2baf504593ba58d80ba4bd32683112.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-MsW7-8LvRGdJVwF9RVmbA.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">GitHub上的代码:<a class="ae jd" href="https://github.com/katnoria/crowd-density/blob/master/src/models.py#L9" rel="noopener ugc nofollow" target="_blank"> models.py </a></figcaption></figure><h2 id="dda2" class="ld jy hi bd jz le lf lg kd lh li lj kh iq lk ll kl iu lm ln kp iy lo lp kt lq bi translated">估价</h2><p id="2a2c" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">我们将首先检查模型能够在多大程度上过度拟合训练数据。我们通过比较实际和预测的人群数量来可视化它的性能。每个选项卡显示给定输入尺寸的图像预测。模型越好，离对角线越近的点越多。正如您在下面的图中看到的，对于群组≤ 1000的图像，该模型做得相对更好。然而，随着人群数量的增加，它的性能开始受到影响。这是为什么呢？我留给你去发现(提示:和输入大小有关系😉)</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lr"><img src="../Images/f85ee1b6e21fbb8ea3c4a65ce807d651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FhSM_8iS6FwyhH317Mce9Q.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">参见<a class="ae jd" href="https://www.katnoria.com/crowd-density/" rel="noopener ugc nofollow" target="_blank">原文</a>查看三个不同标签的剧情</figcaption></figure><p id="13fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们回顾一下我们的模型得到的图像以及它遇到困难的图像。这里，我们显示了来自训练集和测试集的图像。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/566561aad35405e922fa67271641db82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LYbr97UL5-8gLdoyEYo4pQ.png"/></div></div></figure><p id="aefc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我注意到两件事:1)预测更好的图像只包含人2)图像中头部的方向。另一个重要的见解是，随着输入图像大小的增加，模型的性能开始变得更好。查看224x224和448x448选项卡中的图来确认这一点。</p><p id="1275" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来自测试集的样本似乎也证实了带有许多不同对象的图像(例如树和人)给模型带来了困难。最后一幅图像对人眼来说也是困难的。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ls"><img src="../Images/37f1f352ac17b70ba5b12177cbdd3769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iDJ0-5A9c5jJX_6x5qfWPA.png"/></div></div></figure></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="7491" class="jx jy hi bd jz ka ma kc kd ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku bi translated">带解码器的VGG16</h1><p id="59f4" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">我们现在转向另一个简单但更强大的模型，它也使用预先训练的VGG16作为主干。我们使用CONV和CONVTRANSPOSE层，C3的论文将这些层称为解码器。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mf"><img src="../Images/dc1499959c448a39be7a396d7be0fdd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99uwfVC1oYOdVjqr0SPr0Q.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">GitHub上的代码:<a class="ae jd" href="https://github.com/katnoria/crowd-density/blob/master/src/models.py#L49" rel="noopener ugc nofollow" target="_blank"> models.py </a></figcaption></figure><p id="0345" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们用与基线模型相同的超参数训练这个模型400个时期。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mf"><img src="../Images/40f6e39cab0dd9b83047943b20784c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZNNnNTn9Hw5BhTiPWJyX9w.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">参见<a class="ae jd" href="https://www.katnoria.com/crowd-density/" rel="noopener ugc nofollow" target="_blank">原文</a>查看不同标签中的情节</figcaption></figure><p id="a779" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据C3框架，这两个模型将具有相当的性能，但带有解码器的VGG16将生成更精确的密度图。我们可以在下面的表格和示例中看到这一点。我们的数字与C3框架报告的数字相差甚远，我认为这主要是因为他们使用了更高的输入量来训练他们的模型。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mg"><img src="../Images/aa084e6ae8fc0d129ef626327675c432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dtovmlyDIlTZomSOB4MS1A.png"/></div></div></figure><p id="2567" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们将两个模型生成的密度图叠加在给定的图像上。我们看到VGG16基线在实际计数的✅in项上是点，但是VGG16 +解码器生成更紧密的密度图。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mh"><img src="../Images/d00f57e1e7b6aaa98319749a720a7c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wRHEyyh9MNen8WFaKjnrWg.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">模型预测法</figcaption></figure><p id="e28e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以尝试增加输入大小，并自己摆弄模型。如果你能得到足够好的模型，你也许可以帮助回答谁的集会上有更多的人的问题🤣。代码可在我的<a class="ae jd" href="https://github.com/katnoria/crowd-density" rel="noopener ugc nofollow" target="_blank"> GitHub Repo </a>上获得。如果你没有注意到，如果你悬停在封面图像上，你会看到由VGG16解码器生成的密度图。</p><blockquote class="mi mj mk"><p id="99ea" class="if ig ml ih b ii ij ik il im in io ip mm ir is it mn iv iw ix mo iz ja jb jc hb bi translated">对于不同型号的更彻底的评估，查看我的补充职位<a class="ae jd" href="https://www.katnoria.com/crowd-density-eval/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></blockquote><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mp"><img src="../Images/1657a7ec7a2ced46e1c70809df180605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Ux92w7PZk0EedvGz3uC_Dg.gif"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">来源:https://www.katnoria.com/crowd-density-eval/<a class="ae jd" href="https://www.katnoria.com/crowd-density-eval/" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="1725" class="jx jy hi bd jz ka ma kc kd ke mb kg kh ki mc kk kl km md ko kp kq me ks kt ku bi translated">下一步怎么样</h1><p id="ac06" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">您可以尝试调整超参数，找到正确的学习速率和/或模型架构，以获得更好的性能。如果我想让它有用的话，下面是我下一步要尝试的事情:</p><ol class=""><li id="df17" class="mq mr hi ih b ii ij im in iq ms iu mt iy mu jc mv mw mx my bi translated">添加正规化</li><li id="08a1" class="mq mr hi ih b ii mz im na iq nb iu nc iy nd jc mv mw mx my bi translated">使用强大的主干，如Resnet变体</li><li id="1a11" class="mq mr hi ih b ii mz im na iq nb iu nc iy nd jc mv mw mx my bi translated">使用C3框架中的其他模型</li><li id="4425" class="mq mr hi ih b ii mz im na iq nb iu nc iy nd jc mv mw mx my bi translated">假设我们只有300个样本(就DL而言非常低)，您可以尝试U-Net，它在细胞分割等任务上表现很好</li><li id="c95a" class="mq mr hi ih b ii mz im na iq nb iu nc iy nd jc mv mw mx my bi translated">在空间上将图像划分为称为闭集的子区域，在闭集上训练模型，如论文“从开集到闭集:通过空间分治计数对象”所建议的。作者声称，这种方法泛化能力很好，可以在几个人群计数数据集上实现最先进的性能</li><li id="63fc" class="mq mr hi ih b ii mz im na iq nb iu nc iy nd jc mv mw mx my bi translated">使用“基于编码器-解码器的卷积神经网络，具有用于人群计数的多尺度感知模块”一文中强调的基于编码器-解码器的方法。他们声称他们的模型可以在密集和稀疏的人群中表现良好。</li></ol><h2 id="5392" class="ld jy hi bd jz le lf lg kd lh li lj kh iq lk ll kl iu lm ln kp iy lo lp kt lq bi translated">参考和链接</h2><ol class=""><li id="8bfa" class="mq mr hi ih b ii kv im kw iq ne iu nf iy ng jc mv mw mx my bi translated">C3框架:我从这篇论文和他们的代码[ <a class="ae jd" href="https://arxiv.org/abs/1907.02724" rel="noopener ugc nofollow" target="_blank">论文</a> | <a class="ae jd" href="https://github.com/gjy3035/C-3-Framework" rel="noopener ugc nofollow" target="_blank"> GITHUB </a> ]中学到了很多</li><li id="5399" class="mq mr hi ih b ii mz im na iq nb iu nc iy nd jc mv mw mx my bi translated">基于CNN的单一图像人群计数和密度估计的最新进展调查[ <a class="ae jd" href="https://arxiv.org/abs/1707.01202" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="1b3c" class="mq mr hi ih b ii mz im na iq nb iu nc iy nd jc mv mw mx my bi translated">U-Net:用于生物医学图像分割的卷积网络</li><li id="831e" class="mq mr hi ih b ii mz im na iq nb iu nc iy nd jc mv mw mx my bi translated">从开集到闭集:通过空间分而治之[ <a class="ae jd" href="http://arxiv.org/abs/1908.06473" rel="noopener ugc nofollow" target="_blank">链接</a>计数对象</li><li id="50fb" class="mq mr hi ih b ii mz im na iq nb iu nc iy nd jc mv mw mx my bi translated">基于编码器-解码器的卷积神经网络，具有用于人群计数的多尺度感知模块[ <a class="ae jd" href="https://arxiv.org/abs/2003.05586" rel="noopener ugc nofollow" target="_blank">链接</a></li></ol></div></div>    
</body>
</html>