<html>
<head>
<title>Complete Overview of Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树的完整概述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/complete-overview-of-decision-tree-51b03cf87193?source=collection_archive---------26-----------------------#2020-05-12">https://medium.com/analytics-vidhya/complete-overview-of-decision-tree-51b03cf87193?source=collection_archive---------26-----------------------#2020-05-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="40b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树算法是针对<strong class="ih hj">分类问题</strong>的一种监督学习。分类是通过添加标签(例如垃圾邮件/非垃圾邮件)将数据集划分为不同类别或组的过程</p><h1 id="d3b5" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak"> <em class="kb">决策树</em> </strong></h1><p id="b331" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">决策树是基于特定条件的决策的所有可能解决方案的图形表示。它对分类变量和连续因变量都有效。在这个算法中，我们将全部样本数据分成两个或更多的同类集合。</p><p id="b891" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Ex </strong> -</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/b0ac61bfb8d27fac957e20a344f25e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*SohJ0pLjzDq5Hl2O"/></div></figure><p id="9065" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kp">重要术语</em> </strong></p><ol class=""><li id="cafe" class="kq kr hi ih b ii ij im in iq ks iu kt iy ku jc kv kw kx ky bi translated"><strong class="ih hj">决策节点</strong>——也称为根节点。这是整个数据集算法的起点</li><li id="9ea8" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated"><strong class="ih hj">叶节点</strong> -(算法结束，从此节点无法分离)</li><li id="d257" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated"><strong class="ih hj">拆分</strong> -基于条件</li><li id="0106" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated"><strong class="ih hj">Branch/Sub</strong>-tree-拆分树或数据后</li><li id="042b" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated"><strong class="ih hj">修剪</strong>——去除不需要的分支(减少复杂性)</li></ol><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es le"><img src="../Images/058b45f70ea8bcfd6f47ac32890bd295.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/0*LpJheDrFtPs654mt"/></div></figure><h1 id="5879" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak"> <em class="kb">大车算法</em> </strong></h1><p id="c287" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">到目前为止一切顺利，但你可能会怀疑我们应该如何选择，如何选择最佳属性，从哪里开始？？</p><p id="4a25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个算法帮助你进入它。不要担心在任何时候被卡住。一直走到最后:)</p><p id="cde1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">购物车</strong> -分类和回归树算法</p><h2 id="5699" class="lf je hi bd jf lg lh li jj lj lk ll jn iq lm ln jr iu lo lp jv iy lq lr jz ls bi translated"><strong class="ak">如何决定在哪里拆分？？</strong></h2><p id="ae52" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">为此我们需要知道</p><ol class=""><li id="961e" class="kq kr hi ih b ii ij im in iq ks iu kt iy ku jc kv kw kx ky bi translated"><strong class="ih hj">基尼指数</strong> -用于构建决策树的杂质(随机性程度)的度量</li><li id="51d9" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated"><strong class="ih hj">信息增益</strong> -根据属性分割数据集后熵的减少。决策树的构建是关于寻找返回最高信息增益的属性</li><li id="6020" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated"><strong class="ih hj">方差减少</strong>——这是一种用于连续目标变量(回归问题)的算法。选择方差较低的分裂作为分裂总体(样本)的标准</li><li id="db14" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated"><strong class="ih hj">卡方— </strong>这是一种找出子节点和父节点之间差异的统计显著性的算法</li></ol><h2 id="c367" class="lf je hi bd jf lg lh li jj lj lk ll jn iq lm ln jr iu lo lp jv iy lq lr jz ls bi translated"><strong class="ak">熵- </strong></h2><p id="3644" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">这是一个衡量杂质(随机程度)的指标。这是算法的第一步</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lt"><img src="../Images/25db7095f993841536cb26a81e5ccb84.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/0*1KiNOOcYqkdMxSol"/></div></figure><blockquote class="lu"><p id="0410" class="lv lw hi bd lx ly lz ma mb mc md jc dx translated"><strong class="ak"> <em class="kb">熵(s)= -P(是)log(是)-P(否)log(否)</em> </strong></p></blockquote><h2 id="8ea7" class="lf je hi bd jf lg me li jj lj mf ll jn iq mg ln jr iu mh lp jv iy mi lr jz ls bi translated"><strong class="ak">信息增益</strong></h2><p id="15d0" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">它测量熵的减少，并决定哪个属性应该被选为决策节点。</p><p id="31cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">信息增益公式</p><blockquote class="lu"><p id="b470" class="lv lw hi bd lx ly mj mk ml mm mn jc dx translated"><strong class="ak"> I(S)=熵(S) — [(加权平均)*(熵(每个特征))] </strong></p></blockquote><p id="0c95" class="pw-post-body-paragraph if ig hi ih b ii mo ik il im mp io ip iq mq is it iu mr iw ix iy ms ja jb jc hb bi translated">将此公式应用于所有属性，并选择具有最大增益的属性。</p><p id="1220" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Ex- </strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/55cba8e427adf0b70f49839863a1bd9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*TMZwiCo5e4PYvMqb"/></div></figure><p id="f7bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，你可以看到人口根据多种属性被分为四个不同的组，以识别“他们是否会玩”。为了将人口分成不同的异质群体，它使用了各种技术，如基尼系数、信息增益、卡方、熵</p><h1 id="b2bb" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak"> <em class="kb">购物车的一些有用功能和优点</em> </strong></h1><ul class=""><li id="fafa" class="kq kr hi ih b ii kc im kd iq mt iu mu iy mv jc mw kw kx ky bi translated">CART是非参数的，因此不依赖于属于特定分布类型的数据。</li><li id="3544" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">输入变量中的异常值不会显著影响CART。</li><li id="9f4d" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">您可以放宽停止规则，使决策树“过度增长”,然后将树修剪回最佳大小。这种方法最大限度地降低了数据集中的重要结构因过早停止而被忽略的可能性。</li><li id="346d" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">CART结合了测试数据集测试和交叉验证，以更准确地评估拟合优度。</li><li id="a732" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">CART可以在树的不同部分多次使用相同的变量。这种能力可以揭示变量集之间复杂的相互依赖关系。</li><li id="14d3" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">CART可以与其他预测方法结合使用，以选择输入变量集。</li></ul><h1 id="9163" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">决策树的优点</strong></h1><ul class=""><li id="dc6e" class="kq kr hi ih b ii kc im kd iq mt iu mu iy mv jc mw kw kx ky bi translated">容易理解</li><li id="0b0a" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">易于生成规则</li><li id="9938" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">几乎没有要调整的超参数</li><li id="aad1" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">复杂的决策树模型可以通过可视化得到显著简化</li></ul><h1 id="8a96" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">决策树的缺点</strong></h1><ul class=""><li id="1f07" class="kq kr hi ih b ii kc im kd iq mt iu mu iy mv jc mw kw kx ky bi translated">可能会过度拟合</li><li id="0cc6" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">不容易处理非数字数据</li><li id="a69b" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">当有许多分类标签时，计算可能会很复杂</li></ul><h1 id="82cb" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">一些关键建议- </strong></h1><ul class=""><li id="b5c5" class="kq kr hi ih b ii kc im kd iq mt iu mu iy mv jc mw kw kx ky bi translated">我们可以使用任何分类算法来代替依赖于数据集的决策树算法。您可以应用不同的算法，并选择更准确的算法</li><li id="ab2c" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">如果因变量和自变量之间的关系由线性模型很好地近似，则线性模型优于基于树的模型</li><li id="e1d7" class="kq kr hi ih b ii kz im la iq lb iu lc iy ld jc mw kw kx ky bi translated">如果因变量和自变量之间存在高度非线性和复杂关系，则树模型执行分类回归模型</li></ul><p id="6613" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kp">这个skicit算法备忘单在用python实现时可能会有帮助</em></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/72dd969ae2d83490bf4c63e872b662ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*rk9NkS2C0scVfS3S"/></div></figure><h2 id="0fbe" class="lf je hi bd jf lg lh li jj lj lk ll jn iq lm ln jr iu lo lp jv iy lq lr jz ls bi translated"><strong class="ak"> <em class="kb">延伸阅读材料</em> </strong></h2><p id="74f9" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated"><a class="ae mx" href="https://alex.smola.org/drafts/thebook.pdf" rel="noopener ugc nofollow" target="_blank"><em class="kp">https://alex.smola.org/drafts/thebook.pdf</em></a></p><p id="c9f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mx" href="http://cs229.stanford.edu/notes/cs229-notes-dt.pdf" rel="noopener ugc nofollow" target="_blank">http://cs229.stanford.edu/notes/cs229-notes-dt.pdfT21</a></p><p id="50d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mx" href="https://www.dotnetlovers.com/article/214/decision-tree-analysis-with-example" rel="noopener ugc nofollow" target="_blank"> <em class="kp">决策树分析示例</em> </a></p><p id="73ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mx" href="https://www.youtube.com/watch?v=wr9gUr-eWdA&amp;list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;index=10" rel="noopener ugc nofollow" target="_blank"><em class="kp">https://www.youtube.com/watch?v=wr9gUr-eWdA&amp;list = ploromvodv 4 rmigqp 3 wxshtmggzqpvfbu&amp;index = 10</em>T3】</a></p><p id="91dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mx" href="https://www.elsevier.com/books/handbook-of-statistical-analysis-and-data-mining-applications/nisbet/978-0-12-374765-5" rel="noopener ugc nofollow" target="_blank"> <em class="kp">统计分析与数据挖掘应用手册</em> </a></p></div></div>    
</body>
</html>