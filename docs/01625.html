<html>
<head>
<title>Understanding And Implementing Neural Style Transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解和实现神经类型转移</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-and-implementing-neural-style-transfer-7d752d3cfe74?source=collection_archive---------13-----------------------#2019-11-04">https://medium.com/analytics-vidhya/understanding-and-implementing-neural-style-transfer-7d752d3cfe74?source=collection_archive---------13-----------------------#2019-11-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9e48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">神经风格转移(Neural style transfer)是一种使用深度学习方法将两个独立图像的风格和内容组合成一个图像的艺术方法。</em>T3】</strong></p><blockquote class="je jf jg"><p id="acb3" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">先决条件:对CNN和Keras的基本了解。</p></blockquote><div class="jk jl jm jn fd ab cb"><figure class="jo jp jq jr js jt ju paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/be5cee69f208849be8d6bc445ae2d44d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*h4IxtXcHG7b84upKkr7p_g.jpeg"/></div></figure><figure class="jo jp kb jr js jt ju paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/771ceefbd573e68e86359bbada3548c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*2yQdIoiDWXRj6QeIztF8lg.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx kg di kh ki translated">照片由<a class="ae kj" href="https://unsplash.com/@timwhybrow?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">蒂姆·怀布罗</a>在<a class="ae kj" href="https://unsplash.com/search/photos/vibrany?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上通过应用NST进行改造。</figcaption></figure></div><h1 id="fa56" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">那么什么是神经风格转移(NST)？？？</h1><p id="fcc5" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">根据维基百科，</p><blockquote class="je jf jg"><p id="5ae3" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">神经风格转移指的是一类软件算法，这些算法操纵数字图像或视频，以采用另一个图像的外观或视觉风格。</p></blockquote><p id="28bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们要研究的NST算法是基于Leon A. Gatys等人的论文<a class="ae kj" href="https://arxiv.org/pdf/1508.06576.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jd">一种艺术风格的神经算法</em> </a>。NST算法提供了一个基于深度神经网络的人工系统，可以创建高视觉质量的艺术图像。</p><p id="67b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">艺术类图像的创建是计算机视觉的一个分支，称为<a class="ae kj" href="http://ieeexplore. ieee.org/xpls/abs_all.jsp?arnumber=6243138" rel="noopener ugc nofollow" target="_blank"> <em class="jd">非真实感渲染</em> </a>，但这些方法涉及对图像像素的直接操作，而不是对图像的特征空间的操作。</p><p id="b8a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们处理图像的特征空间时，我们利用卷积神经网络(这里我们使用VGG-19体系结构),其已经在像Imagenet(1000个类)这样的大数据集上被训练用于对象检测任务。</p><h1 id="f3cf" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">为什么我们在NST中使用CNN？？</h1><figure class="jk jl jm jn fd jp er es paragraph-image"><div class="er es ln"><img src="../Images/189f25434c1f6ec15b1727d5d9b28dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*KEkiGmv1a20uzxvRG66HWQ.jpeg"/></div></figure><ol class=""><li id="9b76" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">CNN是一类深度神经网络，在图像处理任务中非常强大。CNN的每一层产生输入图像的特征图，这些特征图是输入图像的过滤版本。</li><li id="de30" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">我们可以使用每一层的特征图来重建输入图像。特征图是图像的多层表示，由CNN的层产生作为输出。</li><li id="97a9" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">CNN帮助在特征空间而不是像素空间中捕捉图像的表示。</li><li id="de76" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">CNN的高层捕捉图像的全局信息，而不是像素表示。全局表示捕获图像的<strong class="ih hj">高级内容</strong>。相比之下，较低层捕捉原始图像的精确像素值。因此，我们使用来自更高层的特征响应作为<strong class="ih hj">“内容表示”。</strong>这里我们用的是VGG-19的<strong class="ih hj">【con V5 _ 1】</strong>层的特征图，它是最顶层的褶积层。</li><li id="7343" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">通过构建多尺度特征空间，获得图像的<strong class="ih hj">【风格表征】</strong>。这个特征空间包含多层的不同滤波器响应之间的相关性。这里我们将累积使用<strong class="ih hj">【con v1 _ 1】【con v2 _ 1】【con v3 _ 1】【con v4 _ 1】【con V5 _ 1】</strong>的特征图。</li></ol><blockquote class="je jf jg"><p id="ace8" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">通过包括多层的特征相关性，我们获得了输入图像的静态、多尺度表示，其捕获了其纹理信息，但没有捕获全局排列。</p></blockquote><h1 id="a77d" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">NST的实施</h1><p id="2cd6" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">实现该算法的过程包括:</p><ol class=""><li id="638a" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">选择两个图像，其中一个是样式参考图像，另一个是内容基础/目标图像。如果选择的样式参考图像有更多的纹理和图案，我们会得到更好的结果。</li></ol><pre class="jk jl jm jn fd mc md me mf aw mg bi"><span id="6d41" class="mh kl hi md b fi mi mj l mk ml">from keras.preprocessing.image import load_img, img_to_array<br/>target_image_path = 'img/portrait.jpg'<br/>style_reference_image_path = 'img/transfer_style_reference.jpg'<br/>width, height = load_img(target_image_path).size<br/>img_height = 400<br/>img_width = int(width * img_height / height)</span></pre><div class="jk jl jm jn fd ab cb"><figure class="jo jp mm jr js jt ju paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/482ea9dbd0ca101c540ed9407855bfa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*0fk_Mk1PUa5n8NB-KdaKzg.jpeg"/></div></figure><figure class="jo jp mn jr js jt ju paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/5e02bdf299c3653ad999e190add4d61e.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*h4IxtXcHG7b84upKkr7p_g.jpeg"/></div><figcaption class="kc kd et er es ke kf bd b be z dx mo di mp ki translated">风格参考图像(梵高的《星空》)和内容基础图像。</figcaption></figure></div><p id="2a2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。</strong>选择一个网络(这里我们使用VGG19 ),该网络已经在一些数据集上进行了训练，用于一些特定的任务，如对象检测。我们使用已经训练好的网络的原因是，他们已经有了关于提取图像特征的合理想法，因为他们已经在像ImageNet这样的大型图像数据集上进行了训练。</p><p id="62bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。</strong>因此，我们通过将生成的图像通过网络来同时计算样式参考图像、目标/内容基础图像和生成的图像的层激活。初始生成的图像作为白噪声图像。网络的输入是样式参考图像、内容图像和组合图像的组合(最初是空白图像)。</p><pre class="jk jl jm jn fd mc md me mf aw mg bi"><span id="140f" class="mh kl hi md b fi mi mj l mk ml">from keras import backend as K<br/>target_image = K.constant(preprocess_image(target_image_path))</span><span id="f16a" class="mh kl hi md b fi mq mj l mk ml">style_reference_image = K.constant(preprocess_image(style_reference_image_path))</span><span id="5bc8" class="mh kl hi md b fi mq mj l mk ml">combination_image = K.placeholder((1, img_height, img_width, 3))<br/>input_tensor = K.concatenate([target_image,<br/>style_reference_image,<br/>combination_image], axis=0)</span><span id="45a0" class="mh kl hi md b fi mq mj l mk ml">model = vgg19.VGG19(input_tensor=input_tensor,<br/>weights='imagenet',<br/>include_top=False)<br/>print('Model loaded.')</span></pre><p id="0a7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，preprocess_image是一个辅助函数，我们使用它将图像转换为numpy数组，我们还使用imagenet的平均值和标准差进行归一化，因为VGG19是在imagenet数据集上训练的，所以我们必须确保我们是在相同的输入分布上训练网络。</p><p id="54a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。</strong>接下来我们定义损失函数</p><ul class=""><li id="1fb9" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc mr lu lv lw bi translated"><strong class="ih hj">内容损失函数:</strong>内容损失函数接受目标/内容图像和生成图像的最后一个卷积层生成的特征。然后，我们定义两个特征表示之间的平方误差损失。</li></ul><pre class="jk jl jm jn fd mc md me mf aw mg bi"><span id="f453" class="mh kl hi md b fi mi mj l mk ml">def content_loss(base, combination):<br/>     return K.sum(K.square(combination - base))</span></pre><ul class=""><li id="415a" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc mr lu lv lw bi translated"><strong class="ih hj">风格损失函数:</strong>如前所述，图像的风格表示是由conv层的特征之间的相关性形成的，我们使用定义为<code class="du ms mt mu md b">G = (A ^ T) A</code>的格拉姆矩阵。因此，风格损失采用风格参考图像和生成图像的gram矩阵，并执行平方损失误差。</li></ul><pre class="jk jl jm jn fd mc md me mf aw mg bi"><span id="5c61" class="mh kl hi md b fi mi mj l mk ml">def gram_matrix(x):<br/>    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))<br/>    gram = K.dot(features, K.transpose(features))<br/>    return gram</span><span id="a3dc" class="mh kl hi md b fi mq mj l mk ml">def style_loss(style, combination):<br/>    S = gram_matrix(style)<br/>    C = gram_matrix(combination)<br/>    channels = 3<br/>    size = img_height * img_width<br/>    return K.sum(K.square(S - C)) / (4. * (channels ** 2) * (size ** 2))</span></pre><ul class=""><li id="938b" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc mr lu lv lw bi translated"><strong class="ih hj">全变分损失:</strong>全变分损失是对生成的组合图像的像素进行操作的正则化损失。我们使用这种损失来鼓励生成图像的空间连续性，这将避免过度像素化的结果。</li></ul><pre class="jk jl jm jn fd mc md me mf aw mg bi"><span id="c058" class="mh kl hi md b fi mi mj l mk ml">def total_variation_loss(x):<br/>    a = K.square(<br/>x[:, :img_height - 1, :img_width - 1, :] -<br/>x[:, 1:, :img_width - 1, :])<br/>    b = K.square(<br/>x[:, :img_height - 1, :img_width - 1, :] -<br/>x[:, :img_height - 1, 1:, :])<br/>return K.sum(K.pow(a + b, 1.25))</span></pre><p id="c309" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。</strong>我们要最小化的损失是使用梯度下降优化技术对所有这三种损失的加权平均。</p><p id="43c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 6。</strong>在Francois Chollet的《用python进行深度学习的T4》一书中，他提到在最初的论文中，他们使用L-BFGS算法进行了优化，但老实说，当我阅读论文时，我没有找到任何关于该算法的参考资料(我可能忽略了)。但是我们将使用L-BFGS算法进行优化。你可以参考下面关于L-BFGS算法的博客。</p><div class="mv mw ez fb mx my"><a href="http://aria42.com/blog/2014/12/understanding-lbfgs" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hj fi z dy nd ea eb ne ed ef hh bi translated">数值优化:理解L-BFGS</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">数值优化是许多机器学习的核心。一旦定义了模型并有了数据集…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">aria42.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm jz my"/></div></div></a></div><p id="f06c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7。</strong>作者还提到，如果我们使用平均池而不是最大池，我们会得到更好的结果。出现这种情况的原因可能是由于丢弃了max pooling中的像素，这可能会导致信息丢失。</p><h1 id="32cb" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">实验</h1><h2 id="6dcd" class="mh kl hi bd km nn no np kq nq nr ns ku iq nt nu ky iu nv nw lc iy nx ny lg nz bi translated">1.使用空白图像作为内容图像。</h2><div class="jk jl jm jn fd ab cb"><figure class="jo jp oa jr js jt ju paragraph-image"><img src="../Images/1a031d3355cd6c009f7a4cc54da378ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*S5WiMF1FYvy3IJvoowgcFA.jpeg"/></figure><figure class="jo jp ob jr js jt ju paragraph-image"><img src="../Images/f483b84e859c8568fc18ccf55f475ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*da941Eq8b1rhdMMEKEFqhQ.png"/><figcaption class="kc kd et er es ke kf bd b be z dx oc di od ki translated">1)用作风格参考图像的梵高自画像。2)当内容图像是空白图像时生成的图像。</figcaption></figure></div><h2 id="9630" class="mh kl hi bd km nn no np kq nq nr ns ku iq nt nu ky iu nv nw lc iy nx ny lg nz bi translated">2.使用空白图像作为样式参考图像。</h2><div class="jk jl jm jn fd ab cb"><figure class="jo jp oe jr js jt ju paragraph-image"><img src="../Images/772be0f1d666429ffb4c768eaa67a170.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*OlvNo0NBdGgHMXoaxhJ41g.png"/></figure><figure class="jo jp oe jr js jt ju paragraph-image"><img src="../Images/7dca3712271b34a0c32490f030738fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*MYxLXGN9CMKv0rXbMzMknA.png"/><figcaption class="kc kd et er es ke kf bd b be z dx kg di kh ki translated">一个时期后和十个时期后的结果。</figcaption></figure></div><h2 id="8b21" class="mh kl hi bd km nn no np kq nq nr ns ku iq nt nu ky iu nv nw lc iy nx ny lg nz bi translated">3.使用黑色图像作为样式参考图像。</h2><p id="df25" class="pw-post-body-paragraph if ig hi ih b ii li ik il im lj io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">我们能注意到的一件事是背景的黑色化而不是脸部区域。如果样式参考图像是高度纹理化的，则NST算法转移是有效的。</p><figure class="jk jl jm jn fd jp er es paragraph-image"><div class="er es of"><img src="../Images/69ee749988868fdca8a91e8904c86df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*wOnWD4Ffu7XBNwg57M_OlQ.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">30个时期后生成的图像。</figcaption></figure><h1 id="3005" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">密码</h1><div class="mv mw ez fb mx my"><a href="https://github.com/shiv-u/neural_style_transfer" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hj fi z dy nd ea eb ne ed ef hh bi translated">shiv-u/neural_style_transfer</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">github.com</p></div></div><div class="nh l"><div class="og l nj nk nl nh nm jz my"/></div></div></a></div><h1 id="2417" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">参考</h1><ol class=""><li id="9a8b" class="lo lp hi ih b ii li im lj iq oh iu oi iy oj jc lt lu lv lw bi translated"><a class="ae kj" href="https://www.amazon.in/Deep-Learning-Python-Francois-Chollet/dp/1617294438/ref=sr_1_1?crid=6P8X7GZR6GDK&amp;keywords=deep+learning+with+python&amp;qid=1564554245&amp;s=gateway&amp;sprefix=deeplea%2Caps%2C307&amp;sr=8-1" rel="noopener ugc nofollow" target="_blank">Francois chollet用python进行深度学习。</a></li><li id="a10a" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">原文:<a class="ae kj" href="https://arxiv.org/pdf/1508.06576.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1508.06576.pdf</a>。</li></ol></div></div>    
</body>
</html>