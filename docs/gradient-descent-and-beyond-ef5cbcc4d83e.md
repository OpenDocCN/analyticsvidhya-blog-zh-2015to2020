# 了解梯度下降

> 原文：<https://medium.com/analytics-vidhya/gradient-descent-and-beyond-ef5cbcc4d83e?source=collection_archive---------3----------------------->

## 机器学习

## 让我们达到全局最小值

![](img/1d00a9ffbd2f1c818e2cc96211516a40.png)

梯度下降。图片来自[维基共享。](https://upload.wikimedia.org/wikipedia/commons/5/5b/Gradient_descent_method.png)

*优化算法是设计用来收敛到一个解的算法。这里的解决方案可以是通过最小化成本函数比如‘L’的局部最小值或全局最小值。*

# 这个成本函数是什么？

*成本函数是对我们的模型预测能力的一种衡量。成本函数的形状定义了我们的优化目标。*

如果成本函数的形状是凸函数，我们的目标是找到唯一的最小值。这相对简单，因为没有局部最小值，我们只需要收敛到全局最小值。

如果成本函数的形状不是凸函数，我们的目标是找到邻域中可能的最低值。

![](img/b674875529677ca0b91dc2c71e930433.png)

凸函数。来源[维基百科](https://en.wikipedia.org/wiki/Convex_function#/media/File:Grafico_3d_x2+xy+y2.png)。

# 梯度下降

*梯度下降是一种一阶迭代优化算法，用于最小化一个函数 L，常用于机器学习*和*深度学习。*

这是一个**一阶优化算法**,因为在每次迭代中，该算法采用一阶导数来更新参数。参数是指回归问题中的系数或神经网络的权重。这些参数由给出最陡上升方向的梯度更新。在每次迭代中，这是通过在为成本函数 L 计算的梯度的相反方向上更新参数来执行的。更新的大小由称为**学习速率α的步长决定。**

为了理解这个算法，让我们考虑一个人试图尽快到达谷底的情况。

很明显，我们沿着最陡的斜坡方向快速向下移动。现在的问题是，我们如何找到这个方向？梯度下降通过测量误差函数的局部梯度找到相同的结果，并且沿着梯度的相反方向前进，直到我们达到全局最小值。

![](img/468c6b35752c4804e4487a713032bb8a.png)

全局最小值。图片来源: [ML 词汇表](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html)。

如前所述，该算法计算成本函数 w.r.t 每个参数θ的梯度，这告诉我们我们的成本函数在我们当前位置(当前参数值)的斜率以及我们应该移动以更新我们的参数的方向。我们更新的规模由学习率控制。

最常用的学习率有:0.3，0.1，0.03，0.01，0.003，0.001。

## 学习率α

**高学习率**导致大步长。虽然有机会快速到达最底部，但我们有超过全局最小值的风险，因为山坡的坡度在不断变化。

![](img/f7ffa851a02a7abae8d2b760afac0b01.png)

学习率高。图片由作者提供。

**学习率低**导致步长小。因此，我们精确地向梯度的相反方向移动。这里的缺点是计算梯度所需的时间。所以我们要花很长时间才能收敛(到达最底部的点)。

![](img/363ea8b91851c510a1bd1027ae0997d5.png)

学习率低。图片由作者提供。

如前所述，我们的目标是达到全局最小值。但是，当我们的成本函数具有不规则曲线时(主要是在深度学习神经网络的情况下)，随着随机初始化的进行，人们可能会达到一个**局部最小值**，这不如全局最小值好。克服这个问题的一个方法是使用动量的概念。

![](img/06701b7ea6f42d57a37f41ccbe4d6bf4.png)

局部最小值问题。图片由作者提供。

*学习率会影响我们的模型收敛的速度。因此，正确的值意味着我们训练模型的时间更少。这是至关重要的，因为更少的训练时间意味着更少的 GPU 运行时间。*

## 正常化

在执行梯度下降之前，缩放我们所有的特征变量是重要的。特征缩放在所定义的成本函数的形状中起着巨大的作用。当执行归一化时，梯度下降算法快速收敛，或者成本函数的轮廓将变得更窄和更高，这意味着它将花费更长的时间来收敛。

归一化数据意味着对数据进行缩放，以达到(均值) **μ=0** ，且(标准差) **σ=1** 。

考虑 ***n*** 特征变量。一个实例 ***xᵢ*** 可以按如下比例缩放:

![](img/c316d18349d93ff0e66aacd93910b9e4.png)

正常化。图片由作者提供。

可以使用 Scikit-Learn 的 StandardScaler 类来执行特征缩放。

# 优化程序

在深入讨论细节之前，让我们先定义偏导数。

*在梯度下降中，我们针对每个参数计算成本函数的梯度。这意味着我们计算当任一参数改变时，成本函数改变多少。这叫做* ***偏导数。***

考虑具有参数θ的成本函数 L。它可以表示为 L(θ)。我们的目标是通过找到最佳参数θ值来最小化该成本函数。

1.  我们用随机值初始化参数θ。
2.  我们选择一个学习率α并执行特征缩放(归一化)。
3.  在算法的每次迭代中，我们计算成本函数相对于每个参数的梯度，并如下更新它们:

![](img/4b13ac45f87754e2f727acde6222fe06.png)

梯度(斜率)。图片由作者提供。

![](img/83f2d6301dbb091dccbe3c5eb99e76bb.png)

优化步骤。图片由作者提供。

*优化步骤中的负号表示我们在为成本函数 L 计算的梯度的相反方向上更新我们的参数，w.r.t .参数θ。*

**如果梯度小于 0** ，我们通过梯度乘以学习率α的值来增加参数。

**如果梯度大于 0** ，我们通过梯度乘以学习率α的值来减少参数。

重复上述步骤，直到成本函数收敛。现在，我们所说的收敛是指，成本函数的梯度等于 0。

![](img/3a33d26bff09693a264f1109357bb14e.png)

趋同。图片由作者提供。

# 梯度下降的类型

> ***批量渐变下降***

批量梯度下降在每个训练步骤使用整批训练数据。因此，对于较大的数据集来说，速度非常慢。

学习率是固定的。理论上，如果代价函数是凸函数，则保证达到全局最小值，否则在损失函数不是凸的情况下，达到局部最小值。

> ***【随机梯度下降】***

批量梯度下降在每次迭代训练中使用整批训练集。这在计算上是昂贵的。

随机梯度下降(此处随机意味着随机)，仅随机取单个实例(方差变大，因为我们对于训练的每次迭代仅使用 1 个实例)，并且在训练的每次迭代中使用相同的实例。这真的很快，但由于随机性，观察到不规则的模式。然而，如果成本函数是不规则的，这种随机性有时是有帮助的，因为它可以跳出局部最小值，比批量梯度下降有更好的机会找到全局最小值。

我们可以使用一个学习率调度器，设置一个初始的高值，然后随着时间的推移逐渐降低我们的学习率，这样我们就不会从最小值反弹回来。如果学习率降低得太快，你可能会陷入局部最小值，甚至会半途而废。如果学习率降低得太慢，你可能会在最小值附近跳很长时间，如果你过早停止训练，最终会得到一个次优的解决方案。

可以使用 scikit learning SGD regressor 类来实现这一点。

需要注意的一件重要事情是，我们必须确保我们的训练数据在每个时期的开始都是混洗的，这样，平均起来，参数就趋向于全局最优。如果数据没有被打乱，SGD 将不会接近全局最小值，而是逐个标签地优化。

> ***小批量梯度下降***

小批量梯度下降在称为小批量的小随机实例集上计算梯度。与随机梯度下降相比，我们有更好的机会更接近最小值，但它可能更难摆脱局部最小值。

批量大小可以是 2 的幂，如 32、64 等。

建议像在 SGD 的情况下那样混洗数据，以避免预先存在的顺序。

小批量梯度下降比批量梯度下降更快，因为每个训练步骤使用的训练样本数量更少。它也能更好地概括。

缺点是，由于噪声的存在，很难收敛，因为人们可能会在最小区域附近跳跃。这些振荡是我们需要学习率衰减的原因，以随着接近最小值而降低学习率。

# 超越一阶优化

如前所述，梯度下降是一种一阶优化算法，这意味着它只测量成本函数曲线的斜率，而不是曲率。(曲率是指曲线或曲面分别偏离直线或平面的程度)。

那么函数的性质或曲率是如何测量的呢？它由二阶导数决定。曲率影响我们的训练。

如果二阶导数等于 0，则称曲率是线性的。

如果二阶导数大于 0，则曲率向上移动。

如果二阶导数小于 0，则曲率向下移动。

# 结论

很好的理解梯度下降真的很重要。如上所述，选择正确的学习速度会更容易，但找到正确的学习速度很难。同时，找出衰变率是另一个大任务。在非凸成本函数的情况下，试图摆脱局部极小值，使用动量也是一项困难的任务，我将在另一篇文章中讨论。下一集见。

## 最初发表于 [machinelearningmaster](https://nvsyashwanth.github.io/machinelearningmaster/understanding-gradient-descent/) 。

> 嘿，如果你喜欢这篇文章，请点击拍手按钮，分享这篇文章，以示你的支持。关注我，获取更多关于机器学习、深度学习和数据科学的文章。下一场见！

# 在网络上找到我

[**GitHub 简介:**这是我分叉的地方](https://github.com/NvsYashwanth)

[**LinkedIn 简介:**联系分享职业动态](https://www.linkedin.com/in/nvsyashwanth/)

[**推特:**分享科技推特](https://twitter.com/YashwanthNvs)

# 谢谢你