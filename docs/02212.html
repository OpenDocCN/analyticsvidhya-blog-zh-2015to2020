<html>
<head>
<title>What happens after Bert ? Summarize those ideas behind</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特之后会发生什么？总结后面的那些想法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-happens-after-bert-summarize-those-ideas-behind-ee02f1eae5d9?source=collection_archive---------5-----------------------#2019-12-06">https://medium.com/analytics-vidhya/what-happens-after-bert-summarize-those-ideas-behind-ee02f1eae5d9?source=collection_archive---------5-----------------------#2019-12-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="b8f7" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">伯特巴特斯潘伯特XLM XLNet阿尔伯特罗伯塔T5 MTDNN GPT2 …</p></blockquote><h1 id="2e05" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">各种模式和思维已经让人目不暇接。他们想告诉我们什么？希望这篇文章会让你看完之后明白。</h1><p id="cb79" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">我们将从以下几点着手:</p><ul class=""><li id="bf4a" class="kn ko hi il b im in iq ir kh kp kj kq kl kr jg ks kt ku kv bi translated">增加覆盖面以改善掩蔽LM</li><li id="8997" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">下一句预测👎？</li><li id="18bc" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">其他的前期训练任务会更好吗？</li><li id="07d8" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">把它变小</li><li id="588f" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">多语言</li><li id="f3e1" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">模型越大，效果越好？</li><li id="b1f3" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">多任务处理</li></ul><p id="15d5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">预培训</p><p id="35ec" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">伯特的模型之所以如此不可思议，是因为它改变了一个NLP模型的训练方式。</p><p id="83f2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">使用大规模语料库训练语义模型，然后使用该模型做下游任务，如阅读理解/情感分类/ NER等</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lb"><img src="../Images/bda2da8b77dc81eccfece1558c0d533d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3iheHlsg7uDU7v5O.png"/></div></div></figure><p id="d5f2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">这也被Yann LeCun称为自我监督学习</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ln"><img src="../Images/78d07760c2d9f0b656371ce708546a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Lceh3DyLLW5zfTww.png"/></div></div></figure><p id="385a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">Bert使用基于Transformer编码器的多任务模型，通过task MaskedLM和NextSentencePrediction来捕获语义。</p><p id="da28" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated"><strong class="il hj">增加覆盖率提高MaskedLM </strong> <br/>在MaskedLM中，MASK是在WordPiece之后的one-pieces上执行的。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lo"><img src="../Images/7b8a1c74dbe6c6f205349413e4eb07d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*xlFKYAGI6m3NHca-WxVp9A.png"/></div></figure><p id="1957" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">当提供' ##eni '和' # # zation '时，不难得到guess 'tok ',而不是根据上下文猜测整个单词。</p><p id="36d8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">由于词本身和词与他人之间的关联不同，伯特可能无法了解词与词之间的关系。</p><p id="0b30" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">预测单词的一部分意义不大，预测整个单词可以更多地了解其语义。因此，扩大掩蔽的覆盖面势在必行:</p><p id="72c4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">对整个单词进行屏蔽—wwm <br/>对短语进行屏蔽—厄尼<br/>缩放到一定长度— Ngram屏蔽/ Span屏蔽</p><p id="8b27" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">短语级别需要提供相应的短语列表。提供这种人为添加的信息可能会扰乱模型，给它一个偏差。看来马克斯在更长的长度上应该是一个更好的解决方案，所以T5尝试不同的长度来得出这个结论:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lp"><img src="../Images/20833199a96aa53882436a6047d5a476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4UHmeccsjFOZO-xU.png"/></div></div></figure><p id="5f59" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">可见，增加长度是有效的，但并不意味着越长越好。SpanBert有一个更好的解决方案，通过概率抽样来减少屏蔽过长文本的机会。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lq"><img src="../Images/af7697cf9bf7be0366ef9238b7635f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*29J3-UThijS6ktn7.png"/></div></div></figure><p id="b29b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">斯潘伯特的实验结果:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lr"><img src="../Images/c30555d35bd98fa9b4ec2a86f01f2baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*P6MPNLGIneoaWpaI.png"/></div></div></figure><p id="7c64" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated"><strong class="il hj">改变蒙版比例</strong> <br/>谷歌的T5尝试不同的蒙版比例，探索最佳的参数设置是什么。令人惊讶的是，伯特最初的设定是最好的:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ls"><img src="../Images/699098b992e86269d370c8feff45ab3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3AtXFUVsNp9-odNg.png"/></div></div></figure><p id="bea3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated"><strong class="il hj">下一句预测👎？</strong> <br/> NSP通过预测两个句子是否有语境来学习句子层面的信息。从实验结果来看，并没有太大的改善，甚至在某些任务上有所下降。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lt"><img src="../Images/569b28bb56301ecb013eeeba62d0627a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dls9SMW54N4occxb.png"/></div></div></figure><p id="a616" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">NSP似乎不太管用！这成了大家围攻的地方，下面的报纸都踩在上面:XLNET/RoBERTa/ALBERT<br/>RoBERTa</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lu"><img src="../Images/035086b2623e83ac4f4d048c0b48a7b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JoXYKgOUTh1ZxWdM.png"/></div></div></figure><p id="d11c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">艾伯特</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lv"><img src="../Images/17d6518724977177724b89802e9aac24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K6K6AmlzpS3DTvYx.png"/></div></div></figure><p id="f781" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">XLNet</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lw"><img src="../Images/7163d3f8a1966a194b5aecf6f12bb4db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cPyt-HxVNkos-1Hx.png"/></div></div></figure><p id="dc80" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">它发现NSP带来更多的负面影响！这可能是由于NSP任务设计的不合理——从其他容易区分的文档中抽取负样本，结果不仅学习的知识少，而且噪音大。此外，NSP把输入分成两个不同的句子，长句样本的缺乏使得伯特的差表现在长句上。</p><p id="14a3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated"><strong class="il hj">其他预训任务</strong> <br/> NSP表现平平，有没有更好的预训方式？每个人都尝试了各种方法，我认为总结各种预训练任务的最好方法是Google的T5和FB的BART</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lx"><img src="../Images/ca9d49574f1f91d6ece0842fc35d35bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/0*_j_LnucqyhWMEdoe.png"/></div></figure><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ly"><img src="../Images/90a59238d8c0637984a779cac1bd89c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*N0W1ojLud73soO6D.png"/></div></div></figure><p id="d82d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">巴特尝试的方式</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lz"><img src="../Images/d42ccbf8ffffdc5ef9fbd08ffb66553c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vDXgFtV_9YM7IpaR.png"/></div></div></figure><p id="c116" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">通常，语言模型将被用作每个人的基线。</p><ul class=""><li id="1b90" class="kn ko hi il b im in iq ir kh kp kj kq kl kr jg ks kt ku kv bi translated">覆盖一些代币，预测覆盖的内容</li><li id="d8ef" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">打乱句子的顺序，预测正确的顺序</li><li id="7249" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">删除一些令牌，预测在哪里删除</li><li id="29eb" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">随机挑选令牌，之后，所有内容将移动到开头，并预测正确的开头在哪里。</li><li id="5d23" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">添加一些令牌并预测删除的位置</li><li id="55f2" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">替换一些令牌并预测它们在哪里被替换</li></ul><p id="1ae8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">实验结果如下:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ma"><img src="../Images/7af6aab618f8b8a6c5e3c0a1b54b59e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*M26zNldJckyGq6Rk.png"/></div></div></figure><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mb"><img src="../Images/09e4713fe1fcb26c0f30e64c929b7e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cgSWo49SSXGImXcg.png"/></div></div></figure><p id="e045" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">这些实验发现，MaskedLM是最好的预训练方法。为了更好的效果，更长的掩码和更长的输入句子似乎是更有效的改善方法。为了避免泄露屏蔽了多少单词，您只能标记一个屏蔽并预测一个或多个单词结果</p><h1 id="0373" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">轻量级选手</h1><p id="ce11" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">伯特的模型非常大。为了使运行时更快，另一个方向是轻量级模型。<br/>你可以压缩的所有方法伯特已经详细说明了这一点。<br/>方向是:</p><ul class=""><li id="c78c" class="kn ko hi il b im in iq ir kh kp kj kq kl kr jg ks kt ku kv bi translated">修剪-删除模型的部分，删除一些层，一些头部</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mc"><img src="../Images/91a8d7d73fd6a8d52262b667f2cd2ecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sRFKxC3NuNoKI1Yc.png"/></div></div></figure><ul class=""><li id="82e0" class="kn ko hi il b im in iq ir kh kp kj kq kl kr jg ks kt ku kv bi translated">矩阵分解-词汇/参数的矩阵分解</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es md"><img src="../Images/6b4c271be68e96e2ae2124b5ba9ad84e.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/0*VnoR5Bws1piaj9Yl.png"/></div></figure><ul class=""><li id="7aca" class="kn ko hi il b im in iq ir kh kp kj kq kl kr jg ks kt ku kv bi translated">知识的升华——伯特对其他小模型的“学习”</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es me"><img src="../Images/324d82d137fada9daaaa9d5f174aac6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TGkPIcvGft71VKqG.png"/></div></div></figure><ul class=""><li id="199b" class="kn ko hi il b im in iq ir kh kp kj kq kl kr jg ks kt ku kv bi translated">参数共享-在层之间共享相同的权重</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es mf"><img src="../Images/5a192248f2544a936bf7991c0e98a707.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/0*mIgKPXA3yfwKevqA.png"/></div></figure><p id="1f2d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">模型和效果可以参考原文<br/>http://mitch Gordon . me/machine/learning/2019/11/18/all-the-ways-to-compress-Bert . html</p><h1 id="37f3" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">多语言</h1><p id="70b8" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">不同语言的数据集非常不均衡。通常情况下，有大量的英语语言数据集，其他语言的数据相对较少。在繁体中文中，这个问题更严重。由于伯特的预训方式没有语言限制。将更多的语言数据放入预训练模型，希望它可以在下游任务中取得更好的结果。</p><p id="60e7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">Google发布的Bert-Multilingual就是一个例子。它在不增加任何中文数据的情况下，在下游任务上取得了接近中文模型的结果。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mg"><img src="../Images/78131d0d6d2509dd7f9a8293e7e46b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BaEbSrBW_APz9IYI.png"/></div></div></figure><p id="60ec" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">在基于多语言语言表征模型的跨语言迁移学习的零距离阅读理解中，发现多语言版本的Bert对SQuAD(英语阅读理解任务)和evaluation(汉语阅读理解任务)进行了微调。可以达到接近QANet的结果；而且多语言模型不把数据翻译成同一种语言，比翻译好！</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es mh"><img src="../Images/4862404c7c5c8f9410e952d13a9f0cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/0*rim_Neo64OndfCSt.png"/></div></figure><p id="94eb" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">以上结果表明，Bert已经学会了用不同的语言链接数据，无论是在嵌入中还是在transformer编码器中。预训练语言模型中的新兴跨语言结构想要了解bert如何连接不同的语言。<br/>首先，它使用TLM在同一个预训练模型中连接不同的语言:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mi"><img src="../Images/93ca83af7d5b70b8ebc54886ad5d8db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pXrlZB1mjLj7q-P-.png"/></div></div></figure><p id="cb8a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">然后，通过共享组件或不共享组件，它试图找出哪个部分对结果影响最大。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es mj"><img src="../Images/edd3ebe6f8ac0b5db9aa96b7e801c4f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/0*lN0-NyXXGM5Hwbr-.png"/></div></figure><p id="7aaf" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">模型之间的参数共享是成功的关键</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mk"><img src="../Images/c03aab8a5435c12d6951e24e1b8ba2c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0kCM_V61re9evYGm.png"/></div></div></figure><p id="e49c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">这是因为Bert了解单词的分布及其背后的上下文。在不同的语言中，同样的词义，语境的分布应该是接近的。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ml"><img src="../Images/f94609e484ec389828ade9cfe8fd9004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SOBODWt8Q2XPFykE.png"/></div></div></figure><p id="68cd" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">而Bert的参数就是学习其中的分布，使得多语言转移产生如此惊人的效果。</p><h1 id="e0fe" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">模型越大，效果越好？</h1><p id="0393" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">虽然Bert用过大模型，但直觉上，数据越多，模型越大，效果应该越好。这也可能是提高的关键:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mm"><img src="../Images/e87b3a40b290b8bd7809cf99a6a42810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*obVavwNy_aoVGusC.png"/></div></div></figure><p id="7bb1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">T5利用TPU和金钱的魔力将其归咎于峰会</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mn"><img src="../Images/16fbb476a143a6f8242eb72c2b1cc6c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*C-S4d_8mw-Mb62yq.png"/></div></div></figure><p id="85f5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">较大的型号似乎没有多大改进</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mo"><img src="../Images/eda3c1d3a3663b761c06ce90022f9b84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5HilfwNe2Wg3Jsxd.png"/></div></div></figure><p id="7ad0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">所以单纯增加模型并不是最有效的方法。使用不同的训练方法和目标也是提高成绩的一种方法。<br/>例如，ELECTRA使用了一种新的训练方法，让每个单词都参与进来，以便模型能够更有效地学习表征。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mp"><img src="../Images/5708678afc9b4d17f98eb632e912419d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cZexqd8QnE-Mhgmd.png"/></div></div></figure><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mq"><img src="../Images/786e3d6dd586559a236688766fbb9688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3q5S6ygn6yBdxTFl.png"/></div></div></figure><p id="92c7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">Albert使用参数共享来减少参数的数量，同时效果没有显著下降。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mr"><img src="../Images/f764dcfc79acd3167c034d4cf337fdf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hPePwgrLF_dmSiKP.png"/></div></div></figure><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ms"><img src="../Images/18725fde421e36019229b6d77d2a74c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fyqEAfNAIcWrRjhI.png"/></div></div></figure><h1 id="7e98" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">多任务处理</h1><p id="bf9d" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">Bert使用多任务进行预训练。不仅如此，我们还可以使用多任务进行微调。<strong class="il hj">用于自然语言理解的多任务深度神经网络(MTDNN) </strong>正在这么做。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mt"><img src="../Images/825f506bc536e1c30d9a21b499db882b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Hls7OcNlEKTulrLw"/></div></div></figure><p id="7dd0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">与MTDNN相比，GPT2更加激进:使用一种极端的语言模型来捕捉一切，无需微调，只需给出任务的信号，它就可以处理其余的事情。这令人印象深刻，但离成功还很远。</p><p id="7d94" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">T5让它成为一种平衡</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mu"><img src="../Images/81d41323151479c9f324bdad8a0f9c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Jl4nXT4FoF7WNdH_.png"/></div></div></figure><p id="5daa" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">Google的T5类似于GPT2，训练生成模型生成所有文本答案。也和MTDNN一样，在训练的时候，它会让模型知道它现在是在解决不同的任务，是一个训练/微调的模型。</p><p id="4119" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">如此大规模的预训练模型需要解决两个问题:不平衡数据的处理和训练策略。</p><p id="d899" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated"><strong class="il hj">处理不平衡数据</strong></p><p id="6a0c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">任务之间的数据量不同，这导致模型对于一些数据量小的任务表现不佳。<br/>减少大量数据的采样，增加少量数据的采样是解决方案之一。伯特如何进行多语言培训就是一个例子:</p><blockquote class="if ig ih"><p id="f789" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">为了平衡这两个因素，我们在预训练数据创建(以及单词块词汇创建)期间对数据进行了指数平滑加权。换句话说，假设一种语言的概率是P (L)，例如，P(英语)= 0.21意味着在将所有维基百科串联在一起后，我们的数据中有21%是英语。我们将每个概率乘以某个因子S，然后重新归一化，并从该分布中取样。在我们的例子中，我们使用S = 0.7。因此，像英语这样的高资源语言将被欠采样，而像冰岛语这样的低资源语言将被过采样。例如，在最初的发行版中，英语将比冰岛语多抽样1000倍，但平滑后，它只多抽样100倍。</p></blockquote><p id="41f4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated"><strong class="il hj">训练策略</strong></p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es mv"><img src="../Images/898951532b5f283197e20a7a88e3e07a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wSQMc7aNpxkXm2_h.png"/></div></div></figure><ul class=""><li id="3932" class="kn ko hi il b im in iq ir kh kp kj kq kl kr jg ks kt ku kv bi translated">无监督预训练+微调是指T5预训练后在各项任务上的微调结果</li><li id="db03" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">多任务训练是将T5预训练和所有任务一起训练，直接在每个任务上验证结果</li><li id="f984" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">多任务预训练+微调就是把T5预训练和所有任务放在一起训练，然后微调每个任务的训练数据，然后验证结果</li><li id="824e" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">留一法多任务训练是对T5预训练和目标任务以外的任务进行多任务训练，然后对目标任务的数据集进行微调，然后验证结果</li><li id="2766" class="kn ko hi il b im kw iq kx kh ky kj kz kl la jg ks kt ku kv bi translated">有监督的多任务预训练会直接对所有数据进行多任务训练，然后在每个任务上对结果进行微调</li></ul><p id="358e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">可以看出，在大量附属数据后，对特定数据进行微调，可以缓解大量数据预训练时数据不平衡的问题。</p><h1 id="9236" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">参考</h1><p id="8cfc" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated"><a class="ae mw" href="https://translate.googleusercontent.com/[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向变换器的预训练</a> <br/> <a class="ae mw" href="https://translate.googleusercontent.com/[https://arxiv.org/pdf/1910.13461.pdf](https://arxiv.org/pdf/1910.13461.pdf)" rel="noopener ugc nofollow" target="_blank"> BART:用于自然语言生成、翻译和理解的去噪序列间预训练</a> <br/> <a class="ae mw" href="https://translate.googleusercontent.com/[https://arxiv.org/pdf/1907.10529.pdf](https://arxiv.org/pdf/1907.10529.pdf)" rel="noopener ugc nofollow" target="_blank"> SpanBERT:通过表示和预测跨度来改进预训练</a> <br/> <a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://arxiv.org/pdf/1910.13461.pdf&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhhh9FSWEkToU5quuftH2FC9tW7iKw" rel="noopener ugc nofollow" target="_blank"> BART:用于自然语言生成、翻译、 和理解</a> <br/> <a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://arxiv.org/pdf/1901.07291.pdf&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhjXthAUBgZqpTiJkIBWHlz6Z9gtiw" rel="noopener ugc nofollow" target="_blank">跨语言语言模型预训练</a><br/><a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://github.com/ymcui/Chinese-BERT-wwm&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhgLdlW9SurkWQBCzsW0ZvDZBAAkMA" rel="noopener ugc nofollow" target="_blank">Chinese-BERT-wwm</a><br/><a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://arxiv.org/pdf/1906.08237.pdf&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhgDqYVug3IpRuCFpDvpeeSpw4D6CQ" rel="noopener ugc nofollow" target="_blank">XLNet:用于语言理解的广义自回归预训练</a> <br/> <a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://arxiv.org/pdf/1909.11942.pdf&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhhLXedA84D7bP3FSvhIdw95XM4nnQ" rel="noopener ugc nofollow" target="_blank"> ALBERT:用于语言表示的自监督学习的LITE BERT</a><br/><a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://arxiv.org/pdf/1907.11692.pdf&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhhcNQreUcfXZu2PzB71LKt6ZE3efA" rel="noopener ugc nofollow" target="_blank">RoBERTa:一种健壮优化的BERT预训练方法</a> <br/> <a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhhccjso5--kRIZkACBEfr0yO_nUuw" rel="noopener ugc nofollow" target="_blank">所有可以压缩BERT的方法</a>  <br/> <a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://openreview.net/forum%3Fid%3Dr1xMH1BtvB&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhgVDPWkdshWBYCS3FndriyDdnejrg" rel="noopener ugc nofollow" target="_blank"> ELECTRA:预训练文本编码器作为鉴别器而不是生成器</a><br/><a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://voidful.github.io/voidful_blog/implement/2019/12/02/bert-recent-update-2019/DistilBERT,%2520a%2520distilled%2520version%2520of%2520BERT:%2520smaller,%2520faster,%2520cheaper%2520and%2520lighter&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhg5MfHlaMPHmdgbskiGxqGtKv6Ppg" rel="noopener ugc nofollow" target="_blank">DistilBERT</a><br/><a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://arxiv.org/pdf/1910.10683.pdf&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhjzs-JN-qgIT8gAtr5IgGhnxlsq5Q" rel="noopener ugc nofollow" target="_blank">用统一的文本到文本转换器探索迁移学习的极限</a> <br/> <a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://arxiv.org/pdf/1909.09587.pdf&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhjDMaWy2a2r5sRC0eTV549RwrI1YQ" rel="noopener ugc nofollow" target="_blank">用多语言语言表征模型进行跨语言迁移学习的零距离阅读理解</a> <br/> <a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://arxiv.org/pdf/1911.01464.pdf&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhjUbwlEFoTz2BDyGG6Sgi6gCl787A" rel="noopener ugc nofollow" target="_blank">预训练语言模型中出现的跨语言结构</a> <br/> <a class="ae mw" href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=zh-CN&amp;sp=nmt4&amp;tl=en&amp;u=https://arxiv.org/pdf/1807.03819.pdf&amp;xid=17259,15700021,15700186,15700191,15700256,15700259,15700262,15700265,15700271,15700283&amp;usg=ALkJrhh0uLqqfENqA_TeuVAJ8s5xBLscPQ" rel="noopener ugc nofollow" target="_blank">通用转换器</a></p></div></div>    
</body>
</html>