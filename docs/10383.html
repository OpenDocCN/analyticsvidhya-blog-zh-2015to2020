<html>
<head>
<title>Decision Tree Explained…</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树解释了…</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-tree-explained-6441c7b96ec4?source=collection_archive---------18-----------------------#2020-10-16">https://medium.com/analytics-vidhya/decision-tree-explained-6441c7b96ec4?source=collection_archive---------18-----------------------#2020-10-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6eddf03ecb003801f85608b3da3993d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eXJ_aTJzNt7cIeu7.jpg"/></div></div></figure><p id="9637" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">决策树(DTs) </strong>是用于分类和回归的非参数监督学习方法。目标是根据数据特征中的一些决策规则创建模型。它还能够对数据集执行多类分类。</p><p id="e25e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在告诉你决策树算法如何工作之前，我想先给你介绍几个在算法中起重要作用的术语。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jo"><img src="../Images/0843db7d49d9bf20fb36045b738088e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*so6Rxcimcdz1SQw1H_MBHg.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">决策树的基本结构</figcaption></figure><ol class=""><li id="4585" class="jx jy hi is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated"><strong class="is hj">熵:</strong>是杂质的度量。它告诉我们一个特定的节点在分裂之前或之后有多不纯。它的值介于0和1之间。</li></ol><p id="f99b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">0表示分离是完全纯的，而1表示完全不纯的分离。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es kg"><img src="../Images/6b1f943dbd2e9a12d1bed1b486e0139b.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*qtd42hWGSE8laue28JMJ0Q.png"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated"><strong class="bd kh">熵(E) </strong></figcaption></figure><p id="ae9a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> S =给定属性的样本</strong></p><p id="9a82" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> p+ =正类概率</strong></p><p id="868f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> p_ =负类概率</strong></p><blockquote class="ki kj kk"><p id="a6f1" class="iq ir kl is b it iu iv iw ix iy iz ja km jc jd je kn jg jh ji ko jk jl jm jn hb bi translated">熵值越小，纯分裂越多。</p></blockquote><p id="7379" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.<strong class="is hj"> Gini杂质:</strong>它也是节点中杂质的度量，但与熵相比，发现它要容易得多。它在计算上也比熵有效得多。它的值介于0和0.5之间</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/9db4f75dc5bf455805ed3e04649965cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KcrKrWYokK9C5c3oTfuENg.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">基尼不纯度</figcaption></figure><p id="a25e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.<strong class="is hj">信息增益:</strong>它告诉我们一个特定的特征为我们提供了多少“信息”用于拆分。值越大，接收的信息越多。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5b0b19e8c19843a875ca4593fd4f1c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2CpXDgzKNzeJ3Tix.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">信息增益</figcaption></figure><p id="6ec7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> S(v) =分割后的样本</strong></p><p id="299a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> S =分割前的样本</strong></p><h2 id="41ce" class="kq kr hi bd kh ks kt ku kv kw kx ky kz jb la lb lc jf ld le lf jj lg lh li lj bi translated">决策树算法如何工作</h2><ol class=""><li id="719e" class="jx jy hi is b it lk ix ll jb lm jf ln jj lo jn kc kd ke kf bi translated">首先，它将创建深度为1的多个决策树，并找出最适合放置在根节点上的特征。这可以通过对每个特征使用信息增益来找到。具有最大信息增益的特征将被选择用于根节点。</li><li id="b34a" class="jx jy hi is b it lp ix lq jb lr jf ls jj lt jn kc kd ke kf bi translated">在修复根节点之后，我们将再次开始以与我们对根节点所做的相同的方式分割决策树。我们将找到剩余特征的最高信息增益，并将开始放置在内部节点中。</li><li id="215f" class="jx jy hi is b it lp ix lq jb lr jf ls jj lt jn kc kd ke kf bi translated">这个分裂过程继续进行，直到我们得到完全纯粹的分裂(或者所有样本都是“是”或者“否”)，或者直到我们已经实现了为我们的决策树选择的最大深度。</li></ol><blockquote class="ki kj kk"><p id="ec70" class="iq ir kl is b it iu iv iw ix iy iz ja km jc jd je kn jg jh ji ko jk jl jm jn hb bi translated">决策树容易过度拟合数据。为此，我们可以使用在叶节点所需的最小数量的样本或使用树的最大深度来修剪我们的决策树。</p><p id="eba9" class="iq ir kl is b it iu iv iw ix iy iz ja km jc jd je kn jg jh ji ko jk jl jm jn hb bi translated">如果一个类占主导地位，决策树也容易偏向。因此，在拟合决策树之前，需要平衡数据集。</p></blockquote><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ba565c64f323ca9836acb31a9c96ef9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IY-y0gGEqFknsC1v.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">决策图表</figcaption></figure><h2 id="f060" class="kq kr hi bd kh ks kt ku kv kw kx ky kz jb la lb lc jf ld le lf jj lg lh li lj bi translated">ID3 v/s推车</h2><p id="f45f" class="pw-post-body-paragraph iq ir hi is b it lk iv iw ix ll iz ja jb lu jd je jf lv jh ji jj lw jl jm jn hb bi translated">这是决策树内部使用的两种算法。</p><ol class=""><li id="3918" class="jx jy hi is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated">ID3:它使用熵来寻找节点的杂质。</li><li id="e4de" class="jx jy hi is b it lp ix lq jb lr jf ls jj lt jn kc kd ke kf bi translated">CART:它使用Gini杂质来寻找节点的杂质。</li></ol><p id="68dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">决策树很容易解释。它需要较少的数据准备。它可以处理数字和分类特征，尽管在数字特征上它运行缓慢。</p><p id="156a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有关DT(s)的更多信息，请查看:<a class="ae lx" href="https://www.youtube.com/watch?v=7VeUPuFGJHk" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=7VeUPuFGJHk</a></p><p id="4558" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你喜欢这篇文章，别忘了点击拍手图标。</p><p id="4e9b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">快乐学习。</p></div></div>    
</body>
</html>