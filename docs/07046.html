<html>
<head>
<title>K Nearest Neighbors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k个最近邻居</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbors-a5ea981b6bc9?source=collection_archive---------23-----------------------#2020-06-11">https://medium.com/analytics-vidhya/k-nearest-neighbors-a5ea981b6bc9?source=collection_archive---------23-----------------------#2020-06-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6573a9790c2744e266e2f1ee21e69487.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Tt3iPqM7L0OAxl6-0s2vw.jpeg"/></div></div></figure><blockquote class="iq ir is"><p id="4dd1" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">KNN属于<strong class="iw hj">监督学习</strong>算法家族。k最近邻是一种简单的算法，它存储所有可用的案例，并根据相似性度量对新案例进行分类。分类是通过对其邻居的多数投票来完成的。数据被分配给具有最近邻居的类。随着最近邻数量的增加，k值的精确度可能会增加。</p></blockquote><p id="05a5" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">KNN是一个非参数的，懒惰的学习算法。当我们说一种技术是非参数的，这意味着它没有对底层数据分布做任何假设。</p><blockquote class="jv"><p id="ba41" class="jw jx hi bd jy jz ka kb kc kd ke jr dx translated">在KNN算法中，没有明确的训练阶段，或者训练阶段非常少</p></blockquote><p id="c32b" class="pw-post-body-paragraph it iu hi iw b ix kf iz ja jb kg jd je js kh jh ji jt ki jl jm ju kj jp jq jr hb bi translated">让我们看看K最近邻算法是如何工作的:</p><p id="dac5" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">第一步</strong>:选择我们在算法中将要拥有的邻居的数量<strong class="iw hj"> K </strong>。在这里，<strong class="iw hj"> KNN </strong>中的‘T6】K是用于分类或(在连续变量/回归的情况下预测)测试样本的最近邻数。最常见的K的默认值是5。</p><figure class="kl km kn ko fd ij er es paragraph-image"><div class="er es kk"><img src="../Images/748e2f299cd5fd22b5fed69911621413.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*j1sIYfBxhAPmWvc3FPqc7g.jpeg"/></div></figure><p id="4196" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">第二步</strong>:根据<strong class="iw hj">欧氏距离</strong>，取新数据点的K个最近邻。我们还可以使用任何其他测量距离的公式，如曼哈顿距离或切比雪夫和汉明距离。</p><p id="7f9f" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">欧几里德距离:</strong>欧几里德距离或欧几里德度量是欧几里德空间中两点之间的“普通”直线距离。</p><figure class="kl km kn ko fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/bec33d0e94a4c6dbc13a07eb372943a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*F507EfvD_1GAA42iLcCANw.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">欧几里德距离公式</figcaption></figure><figure class="kl km kn ko fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/206f2aff92e7cfd1705d0a7b1e5030e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*lBA0dZXaJAbnPdfDrI66Aw.jpeg"/></div></figure><p id="5a51" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">第三步</strong>:在这K个邻居中，统计每一类中的数据点数，即属于一类的数据点与属于另一类的数据点的数量。</p><p id="4d33" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">当K的值很小时，我们就迫使我们的模型盲目地分类并预测结果。小K值提供了最灵活的拟合。另一方面，较大的K值将增加每个预测中的平均投票者，因此对异常值更有弹性。</p><figure class="kl km kn ko fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/f5b9448ee5d709603a66152dfcb785e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*AGyF-idDTifwxvWVrgw97w.jpeg"/></div></figure><p id="8863" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><strong class="iw hj">步骤4 </strong>:将新的数据点分配到我们计数最邻近的类别。</p><figure class="kl km kn ko fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/aac30fd9e9900b22704650bc54f20626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*VoC6c5TDIO-qFq3-m1oc8g.png"/></div></figure><p id="3fd1" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">在KNN，模型结构是由数据决定的。KNN也是一个懒惰的算法，这意味着它不使用训练数据点做任何推广。</p><h1 id="617b" class="kx ky hi bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">应用</h1><p id="7694" class="pw-post-body-paragraph it iu hi iw b ix lv iz ja jb lw jd je js lx jh ji jt ly jl jm ju lz jp jq jr hb bi translated"><strong class="iw hj">非常简单的实施</strong>，因为没有明确的培训阶段，或者培训非常少。<strong class="iw hj"> KNN </strong>可以胜过更强大的分类器，被用于各种<strong class="iw hj">应用</strong>，如经济预测、数据压缩和遗传学。</p><p id="9bbb" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">K-NN算法最常见和最广泛使用的一个例子是<strong class="iw hj">推荐系统。</strong></p><p id="2b59" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">K近邻机器学习算法到此为止。敬请关注更多博客。</p><p id="21ce" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated"><em class="iv">谢谢</em></p></div><div class="ab cl ma mb gp mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="hb hc hd he hf"><p id="d57c" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">KNN算法在乳腺癌数据集上的实现</p><blockquote class="iq ir is"><p id="58b0" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">数据集:<a class="ae mh" href="https://github.com/InternityFoundation/MachineLearning_Navu4/blob/master/Logistic%20Regression/breast_cancer.csv" rel="noopener ugc nofollow" target="_blank">乳腺癌</a>数据集</p></blockquote><p id="91bf" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je js jg jh ji jt jk jl jm ju jo jp jq jr hb bi translated">链接:<a class="ae mh" href="https://github.com/InternityFoundation/MachineLearning_Navu4/blob/master/KNN/KNN.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/internity foundation/machine learning _ navu 4/blob/master/KNN/KNN . ipyn</a></p></div></div>    
</body>
</html>