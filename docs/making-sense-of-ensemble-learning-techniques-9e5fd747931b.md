# 理解集成学习技术

> 原文：<https://medium.com/analytics-vidhya/making-sense-of-ensemble-learning-techniques-9e5fd747931b?source=collection_archive---------24----------------------->

## 在我的团队中，当谈到我们的机器学习(ML)架构时，集成学习方法是首选的武器。由于集成学习方法组合了多个基本模型，因此它们一起具有产生更精确的 ML 模型的更大能力。事实上，我们已经使用它们成功地解决了从优化 LTV(客户终身价值)到欺诈检测的各种问题。

![](img/62f93dba61e16cbaba26300b80814d73.png)

由 [kazuend](https://unsplash.com/@kazuend?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 在 [Unsplash](https://unsplash.com/s/photos/forest?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) 拍摄的照片

最初出现在 [KD 掘金队 2020 年 3 月](https://www.bigabid.com/blog/making-sense-of-ensemble-learning-techniques#)

让我们仔细看看集成学习对整个 ML 过程的重要性，包括偏差-方差权衡和三种主要的集成技术:bagging，boosting 和 stacking。这些强大的技术应该是任何数据科学家工具包的一部分，因为它们是随处可见的概念。此外，理解它们的底层机制是机器学习领域的核心。

# 集成学习方法综述

正如我们上面简要提到的，集成学习是一种 ML 范式，其中许多基础模型(通常被称为“弱学习者”)被组合和训练来解决同一问题。该方法基于这样的理论，即通过正确地将几个基本模型组合在一起，这些弱学习器可以被用作设计更复杂的 ML 模型的构建块。这些集成模型(被恰当地称为“强学习者”)一起实现了更好、更准确的结果。

换句话说，一个弱学习者仅仅是一个基础模型，单独来看，表现相当差。事实上，它的准确程度仅仅高于概率，这意味着它对结果的预测仅仅比随机猜测好一点点。这些弱学习者通常在计算上也很简单。通常，基础模型本身表现不佳的原因是因为它们要么有很高的偏差，要么有太多的方差，这使得它们很弱。

这就是集成学习的用武之地。这种方法试图通过将几个弱学习者结合在一起来减少一般错误。把集合模型想象成“两个脑袋比一个好”的数据科学版本。如果一个模型工作得很好，那么多个模型一起工作会做得更好。

# 关于偏差-方差权衡的一句话

在我们讨论三种主要的集成学习技术之前，有必要先介绍弱学习者的概念，以及它为什么会被称为弱学习者。正如我们上面提到的，原因可以归结为偏差或方差。更具体地说，ML 模型的预测误差，即训练模型和实际情况之间的差异，可以分解为以下各项之和:偏差和方差。例如:

*   **偏差导致的误差:**这是我们模型的预期预测值和我们旨在预测的精确值之间的差异。
*   **方差导致的误差:**这是一个指定数据点的模型预测的方差。

如果我们的模型过于简单，没有很多参数，那么它可能会有高偏差和低方差。相比之下，如果我们的模型有许多参数，那么它可能具有高方差和低偏差。因此，有必要找到正确的平衡，避免数据拟合不足或拟合过度。这种复杂性的权衡是方差和偏差之间存在权衡的原因。简单地说，一个算法不能同时更复杂和更简单。

# 集成学习技术——组合弱学习者

既然我们已经解决了偏差-方差权衡的问题，我们可以继续三个主要的集成技术:装袋、提升和堆叠。让我们分别看一下每种技术:

# 制袋材料

Bagging 试图在小样本人群中纳入相似的学习者，并计算所有预测的平均值。一般来说，bagging 允许你在不同的人群中使用不同的学习者。通过这样做，这种方法有助于减少方差误差。

# 助推

提升是一种迭代方法，它根据最近的分类来微调观察值的权重。如果一个观察值被错误地分类，该方法将在下一轮中增加该观察值的权重(在下一轮中将训练下一个模型),因此不容易被错误分类。类似地，如果一个观察被正确分类，那么我们将为下一个分类器减少它的权重。权重表示特定数据点的正确分类的重要性。这使得顺序分类器能够集中于先前错误分类的例子。一般来说，增强减少了偏差误差并形成了强预测模型。然而，有时他们可能会在训练数据上过度拟合。

# 堆垛

堆叠是组合不同模型提供的信息的一种巧妙方式。通过这种方法，我们使用任何类型的学习者来组合不同学习者的输出。结果可能是由我们使用的组合模型决定的偏差或方差的减少。

# 最后

总之，集成学习是关于组合多个基本模型以实现更有效和更准确的集成模型，该集成模型具有更强大的特性，因此性能更好。例证:集成学习方法已经成功地在具有挑战性的数据集上创造了记录，并且一直是 Kaggle 竞赛获奖作品的一部分。

值得注意的是，即使使用我们描述的三种主要集成技术——装袋、提升和堆叠——变体仍然是可能的，并且可以更好地设计以更有效地适应特定问题，如分类、回归、时间序列分析等。但首先，这需要我们完全理解手头的问题，并在解决问题的方法上有创造性！使用集成学习方法是着手解决任何问题的一个伟大的、有前途的方法。