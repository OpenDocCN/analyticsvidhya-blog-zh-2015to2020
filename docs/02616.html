<html>
<head>
<title>Data Science: NLP I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学:NLP I</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/data-science-nlp-i-1e18f146b3ef?source=collection_archive---------11-----------------------#2019-12-25">https://medium.com/analytics-vidhya/data-science-nlp-i-1e18f146b3ef?source=collection_archive---------11-----------------------#2019-12-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/893553379e48581089fdd775eefadf4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LZCLsl_U1obM5SrR5kuvFw.jpeg"/></div></div></figure><p id="d11e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 1，737，185，185 —活跃网站数量</strong></p><p id="613d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 239，6 B —每</strong> <a class="ae jo" href="http://statista.com" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">天</strong> </a>发送的邮件数量</p><p id="4874" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一个非常庞大的数据源，包含大量待分析的原始文本，足以证明自然语言处理在当今科技世界中的重要性…</p><p id="f636" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">欢迎来到自然语言处理及其应用入门，在这里我们将主要关注Python的NLTK库(自然语言工具包)以及如何使用它来分析一大堆句子。读完这篇博客后，你将能够从文本中提取一些含义，并使用提到的技术为你的机器学习模型提供输入。</p><p id="227b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">“自然语言处理</strong> ( <strong class="is hj"> NLP </strong>)是<a class="ae jo" href="https://en.wikipedia.org/wiki/Linguistics" rel="noopener ugc nofollow" target="_blank">语言学</a>、<a class="ae jo" href="https://en.wikipedia.org/wiki/Computer_science" rel="noopener ugc nofollow" target="_blank">计算机科学</a>、<a class="ae jo" href="https://en.wikipedia.org/wiki/Information_engineering_(field)" rel="noopener ugc nofollow" target="_blank">信息工程</a>、<a class="ae jo" href="https://en.wikipedia.org/wiki/Artificial_intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a>的一个子领域，涉及计算机与人类(自然)语言之间的互动，特别是如何给计算机编程，以处理和分析大量的<a class="ae jo" href="https://en.wikipedia.org/wiki/Natural_language" rel="noopener ugc nofollow" target="_blank">自然语言</a>数据”——维基百科说。例如，NLP可以用于语音识别、文本摘要、问题回答，甚至可以用于预测打字，这已经在我们的智能手机中应用，每当我们开始在键盘上键入一些东西时。Cortana和Siri等语音识别软件，或者Gmail收件箱中的垃圾邮件检测，都是NLP已经在捕捉我们生活的其他例子。了解NLP技术是有利的，因为前述领域预计也是2020年的技术行业趋势之一。现在让我们用Python来研究NLP吧！</p><h1 id="977b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">一、Python和自然语言处理简介</h1><p id="bc3e" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我不会给出太多关于Python本身的信息，但是，如果你没有它，你可以去python.org下载并安装它的最新版本。安装完成后，您将需要下载一些库，如Numpy、用于科学计算或访问文件的Pandas，以及用于进行自然语言处理技术的NLTK库，我们在这里将主要关注这些库。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="0a8e" class="lb jq hi kx b fi lc ld l le lf">pip install nltk</span></pre><p id="a846" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它可以包含在使用import NLTK的常用方法中，但是，应该下载其中的语料库和包。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="0fa6" class="lb jq hi kx b fi lc ld l le lf">import nltk<br/>nltk.download()</span></pre><h1 id="42c2" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">二。标记化</h1><p id="61e7" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">在很多情况下，运用“分而治之”的方法确实是我们解决问题的一种方式，对句子的分析也是如此。为了分析文本，我们将文本串分成更小的组成部分，即句子——句子标记化(也称为句子分割)，然后，我们再次将句子分成单词(也称为单词标记化)。基本上，每当我们遇到句号(split())时，首先想到的就是关于将文本拆分成句子。然而，在确定句子边界时，一些例外情况(经常使用)忽略了规则并困扰着我们，例如句号字符用作缩写标记(即等等。公元前)。因此，在许多情况下，建议使用库及其函数来完成句子和单词的分割任务。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="e524" class="lb jq hi kx b fi lc ld l le lf">nltk.sent_tokenize <br/>nltk.word_tokenize</span></pre><p id="8933" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">简单地说，从名字中可以明显看出谁做了什么…让我们看看维基百科关于阿塞拜疆的文章中的一个例子:</p><blockquote class="lg lh li"><p id="5d8a" class="iq ir lj is b it iu iv iw ix iy iz ja lk jc jd je ll jg jh ji lm jk jl jm jn hb bi translated">阿塞拜疆是一个<a class="ae jo" href="https://en.wikipedia.org/wiki/Landlocked" rel="noopener ugc nofollow" target="_blank">内陆</a>国家，位于<a class="ae jo" href="https://en.wikipedia.org/wiki/Eurasia" rel="noopener ugc nofollow" target="_blank">欧亚</a>的<a class="ae jo" href="https://en.wikipedia.org/wiki/Transcaucasia" rel="noopener ugc nofollow" target="_blank">南高加索</a>地区，处于<a class="ae jo" href="https://en.wikipedia.org/wiki/Eastern_Europe" rel="noopener ugc nofollow" target="_blank">东欧</a>和<a class="ae jo" href="https://en.wikipedia.org/wiki/Western_Asia" rel="noopener ugc nofollow" target="_blank">西亚</a>的十字路口。它东临里海，北接俄罗斯，西北接格鲁吉亚，西接亚美尼亚，南接伊朗。</p></blockquote><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="64c4" class="lb jq hi kx b fi lc ld l le lf">import  nltk</span><span id="5c8b" class="lb jq hi kx b fi ln ld l le lf">text = "Azerbaijan is a landlocked country in the South Caucasus region of Eurasia at the crossroads of Eastern Europe and Western Asia. It is bounded by the Caspian Sea to the east, Russia to the north, Georgia to the northwest, Armenia to the west and Iran to the south."</span><span id="84ba" class="lb jq hi kx b fi ln ld l le lf">sentences  = nltk.sent_tokenize(text)<br/>for sentence in sentences:<br/>    print (sentence, "\n")</span></pre><p id="3c9f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">将文本中的两个组件—句子视为<strong class="is hj">输出</strong>:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="b730" class="lb jq hi kx b fi lc ld l le lf">Azerbaijan is a landlocked country in the South Caucasus region of Eurasia at the crossroads of Eastern Europe and Western Asia.</span><span id="249a" class="lb jq hi kx b fi ln ld l le lf">It is bounded by the Caspian Sea to the east, Russia to the north, Georgia to the northwest, Armenia to the west and Iran to the s<br/>outh.</span></pre><p id="e538" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当谈到单词标记化在同一个例子中的应用时:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="6b39" class="lb jq hi kx b fi lc ld l le lf">import  nltk</span><span id="d315" class="lb jq hi kx b fi ln ld l le lf">text = "Azerbaijan is a landlocked country in the South Caucasus region of Eurasia at the crossroads of Eastern Europe and Western Asia. It is bounded by the Caspian Sea to the east, Russia to the north, Georgia to the northwest, Armenia to the west and Iran to the south."</span><span id="07b8" class="lb jq hi kx b fi ln ld l le lf">sentences  = nltk.sent_tokenize(text)<br/>for sentence in sentences:<br/>    print(nltk.word_tokenize(sentence), "\n")</span></pre><p id="6785" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">作为一个<strong class="is hj">输出</strong>，每个单词和标点符号都作为句子的一个不同部分被分开。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="0e0d" class="lb jq hi kx b fi lc ld l le lf">['Azerbaijan', 'is', 'a', 'landlocked', 'country', 'in', 'the', 'South', 'Caucasus', 'region', 'of', 'Eurasia', 'at', 'the', 'cros<br/>sroads', 'of', 'Eastern', 'Europe', 'and', 'Western', 'Asia', '.']</span><span id="a094" class="lb jq hi kx b fi ln ld l le lf">['It', 'is', 'bounded', 'by', 'the', 'Caspian', 'Sea', 'to', 'the', 'east', ',', 'Russia', 'to', 'the', 'north', ',', 'Georgia', '<br/>to', 'the', 'northwest', ',', 'Armenia', 'to', 'the', 'west', 'and', 'Iran', 'to', 'the', 'south', '.']</span></pre><h1 id="77db" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">三。用NLTK和Spacy进行词性标注</h1><p id="a94c" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">NLTK是一个很棒的工具，它有大量的功能，这使得它可以为高级NLP项目铺平道路。词性标注是文本分析中的一个重要步骤，我们知道句子结构和哪个单词与另一个单词相连，哪个单词源于哪个单词，最终找出单词之间的隐藏联系，这可以在以后提高我们的机器学习模型的性能。所有这些都通过NLTK和Spacy库得到缓解，它们都带有内置的ML算法，可以根据它们之前保存的数据库和句子中围绕<strong class="is hj">目标</strong> <strong class="is hj">单词</strong>的单词来识别POS标签。</p><p id="9c0c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">具体说一下NLTK pos标签函数，有一个特定的标签列表，在句子被记号化成单词后会被识别。</p><ul class=""><li id="46b4" class="lo lp hi is b it iu ix iy jb lq jf lr jj ls jn lt lu lv lw bi translated">CC并列连词</li><li id="8db8" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">CD基数</li><li id="6051" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">DT限定词</li><li id="042a" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">EX存在性there(比如:“存在”…想象成“存在”)</li><li id="25cd" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">FW外来词</li><li id="7457" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">在介词/从属连词中</li><li id="439b" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">JJ形容词‘大’</li><li id="333c" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">JJR形容词，比较级“更大”</li><li id="c8ba" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">JJS形容词，最高级“最大的”</li><li id="e7a3" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">LS列表标记1)</li><li id="7109" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">MD modal可能会</li><li id="9bfd" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">名词，单数‘书桌’</li><li id="9112" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">NNS名词复数‘书桌’</li><li id="81b0" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">NNP专有名词，单数‘哈里森’</li><li id="9b2e" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">NNPS专有名词，复数“美国人”</li><li id="4251" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">PDT预先确定“所有孩子”</li><li id="8ff2" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">词尾所有格父母的</li><li id="df14" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">PRP人称代词我，他，她</li><li id="3062" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">PRP$所有格代词我的，他的，她的</li><li id="e0f4" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">非常安静地，</li><li id="d145" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">RBR副词，比较级更好</li><li id="57be" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">RBS副词，最高级best</li><li id="3f57" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">RP粒子放弃</li><li id="a3ab" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">去，去商店。</li><li id="d1c1" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">呃感叹词</li><li id="5021" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">VB动词，基本形式take</li><li id="9cad" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">VBD动词，过去式，take</li><li id="dede" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">动词，动名词/现在分词</li><li id="78eb" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">VBN动词，过去分词taken</li><li id="608b" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">VBP动词，唱。当前，非3d拍摄</li><li id="84fd" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">VBZ动词，第三人称演唱。当前拍摄</li><li id="9c01" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">WDT疑问词which</li><li id="42e1" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">代词谁，什么</li><li id="1f89" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">WP$所有格wh-代词谁的</li><li id="c01e" class="lo lp hi is b it lx ix ly jb lz jf ma jj mb jn lt lu lv lw bi translated">wh-副词where，when</li></ul><p id="6939" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看一个具体的例子来更好地理解这个列表及其应用:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="7ae0" class="lb jq hi kx b fi lc ld l le lf">def pos_tagging(s):<br/>    s = re.sub(r'[^a-zA-Z0-9\s]', ' ', s)<br/>    words = nltk.word_tokenize(s)<br/>    return nltk.pos_tag(words)</span></pre><p id="4664" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个函数中，我们首先使用<a class="ae jo" href="https://docs.python.org/3/library/re.html" rel="noopener ugc nofollow" target="_blank"> regex </a>删除带有空格的字母数字函数(如果你不太了解Python中的regex，请查看链接中的文档)，然后句子被标记为单词。然后，这些函数使用“<em class="lj"> nltk.pos_tag </em>”函数返回每个单词的POS标签。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="3ec8" class="lb jq hi kx b fi lc ld l le lf">print(pos_tagging(“Can you buy me a Turkish tea?”))</span></pre><p id="4851" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">知道哪个特定标签属于哪个单词是很重要的，因为它在不同的上下文中是可变的。如在这个例子中，“Can”根据用法可以有几种含义；作为动词和名词的模型。</p><p id="9776" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的输出显示每个单词旁边都有不同的标签，be、情态动词“MD”、人称代词“PRP”、动词“VB”、限定词“DT”、形容词“JJ”和名词“NN”:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="16f0" class="lb jq hi kx b fi lc ld l le lf">[('Can', 'MD'), ('you', 'PRP'), ('buy', 'VB'), ('me', 'PRP'), ('a', 'DT'), ('Turkish', 'JJ'), ('tea', 'NN')]</span></pre><p id="d184" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我看来，空间在这个问题上的一个简单应用在这里不应该被忽略。对于spacy的实现，我们应该首先在命令行中安装它，然后安装我们将在POS标记中使用的en_core_web_sm spaCy English模型。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="07df" class="lb jq hi kx b fi lc ld l le lf">sp = spacy.load('en_core_web_sm')<br/>sentence = sp("I like to play football.I hated it in my childhood though")<br/>print(sentence.text)</span></pre><p id="590e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，导入英语模型并保存我们的句子，然后将其作为<em class="lj">句子打印到屏幕上。说到这个算法的应用:</em></p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="e7da" class="lb jq hi kx b fi lc ld l le lf">import spacy<br/>def pos_tagging_s():<br/>    sp = spacy.load('en_core_web_sm')<br/>    sen = sp("I like to play football. I hated it in my childhood though")<br/>    print(sen.text)<br/>    print(sen[1].pos_)<br/>    print(sen[1].tag_)<br/>    print(spacy.explain(sen[1].tag_))<br/>    for word in sen:<br/>        print("Word:", word.text, "\t","POS Tag:", word.pos_,"\t", "Tag for Word:", word.tag_,"Explanatation:", spacy.explain(word.tag_), "\n")</span></pre><p id="8257" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们看到pos_tagging_s函数的<strong class="is hj"> return </strong>中带有word.text的句子的文本后，我们跳转到word.pos_ function，它输出每个单词的词性标签。接下来是word.tag_ function，它返回标记的缩写形式，并使用spacy.explain(word.tag_)函数输出它的解释，其中给出了对前面编写的word tag的更多解释。</p><p id="c9e6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面代码的输出，例如，“我”有一个PRON的标签，它的标签词是“PRP”，这个缩写的解释是，“它是人称代词”:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="4429" class="lb jq hi kx b fi lc ld l le lf">Word: I          POS Tag: PRON   Tag for Word: PRP       Explanatation: pronoun, personal</span><span id="9bdd" class="lb jq hi kx b fi ln ld l le lf">Word: like       POS Tag: VERB   Tag for Word: VBP       Explanatation: verb, non-3rd person singular present</span><span id="e8bf" class="lb jq hi kx b fi ln ld l le lf">Word: to         POS Tag: PART   Tag for Word: TO        Explanatation: infinitival "to"</span><span id="4d0c" class="lb jq hi kx b fi ln ld l le lf">Word: play       POS Tag: VERB   Tag for Word: VB        Explanatation: verb, base form</span><span id="7249" class="lb jq hi kx b fi ln ld l le lf">Word: football   POS Tag: NOUN   Tag for Word: NN        Explanatation: noun, singular or mass</span><span id="c173" class="lb jq hi kx b fi ln ld l le lf">Word: .          POS Tag: PUNCT          Tag for Word: .         Explanatation: punctuation mark, sentence closer</span><span id="cc85" class="lb jq hi kx b fi ln ld l le lf">Word: I          POS Tag: PRON   Tag for Word: PRP       Explanatation: pronoun, personal</span><span id="b15c" class="lb jq hi kx b fi ln ld l le lf">Word: hated      POS Tag: VERB   Tag for Word: VBD       Explanatation: verb, past tense</span><span id="2967" class="lb jq hi kx b fi ln ld l le lf">Word: it         POS Tag: PRON   Tag for Word: PRP       Explanatation: pronoun, personal</span><span id="f67d" class="lb jq hi kx b fi ln ld l le lf">Word: in         POS Tag: ADP    Tag for Word: IN        Explanatation: conjunction, subordinating or preposition</span><span id="0b0a" class="lb jq hi kx b fi ln ld l le lf">Word: my         POS Tag: DET    Tag for Word: PRP$      Explanatation: pronoun, possessive</span><span id="b06c" class="lb jq hi kx b fi ln ld l le lf">Word: childhood          POS Tag: NOUN   Tag for Word: NN        Explanatation: noun, singular or mass</span><span id="9ab6" class="lb jq hi kx b fi ln ld l le lf">Word: though     POS Tag: SCONJ          Tag for Word: IN        Explanatation: conjunction, subordinating or preposition</span></pre><h1 id="28c5" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">四。词干化和词汇化</h1><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/3389c578c0ce68fe96ec45cb504826fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OTjdJlYF5vRIzpBfOw75KA.png"/></div></div><figcaption class="md me et er es mf mg bd b be z dx translated"><strong class="bd jr">词干</strong>和<strong class="bd jr">之间的<strong class="bd jr">差异</strong>词汇化</strong>示例(NLP)</figcaption></figure><p id="c52f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">词干是一个将单词缩减到其核心词根的概念。特别是，在分析文本中的词频的情况下，这种方法可以真正地工作。基本上，以“ed”或“ing”结尾的单词，例如，将被驱动回到它们的主词根。"词干" = &gt; "词干"</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="bef2" class="lb jq hi kx b fi lc ld l le lf">import  nltk<br/>from nltk.stem import PorterStemmer<br/>ps = PorterStemmer()<br/>print(ps.stem("stemming"))<br/>#output of the code will be <strong class="kx hj">stem</strong></span></pre><p id="f254" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这当然也适用于被分成单词的句子:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="bab9" class="lb jq hi kx b fi lc ld l le lf">sentence = "It is good to be a pythoner who is pythonly pythoning with python"<br/>words = nltk.word_tokenize(sentence)<br/>for word in sentence:<br/>    ps = PorterStemmer()<br/>    print(ps.stem(word))</span></pre><p id="6667" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它将输出:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f77a" class="lb jq hi kx b fi lc ld l le lf">It<br/>is<br/>good<br/>to<br/>be<br/>a<br/>python<br/>who<br/>is<br/>pythonli<br/>python<br/>with<br/>python</span></pre><p id="f5bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">显然，有时一些错误是真实的，如词干过多和词干过少。因此，在不同的句子中，PorterStemmer和WortNetLemmatizer可能不是最好的用法。</p><p id="95a1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">再来看<strong class="is hj">词汇化</strong>，在这里，我们再次将单词驱动到它们的根，然而，应该被驱动的单词在内部被修改，而不是像在词干部分那样，单词只在它们的<strong class="is hj">结尾</strong>添加了音节，而不是在中的<strong class="is hj">。在词汇化的同时，我们还应该提到词性标签，这样算法就可以根据<em class="lj"> WordNet </em>(英语的词汇数据库)知道它连接到了哪个单词。它保存了名词、动词、形容词和副词，它们被分组为认知同义词集(synsets)，每个同义词集表达一个不同的概念。</strong></p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="bfba" class="lb jq hi kx b fi lc ld l le lf">from nltk.stem import WordNetLemmatizer<br/>lemmatizer = WordNetLemmatizer()</span><span id="08c2" class="lb jq hi kx b fi ln ld l le lf"># For single word<br/>print(lemmatizer.lemmatize("good", pos='v'))<br/>print(lemmatizer.lemmatize("better", pos='a'))<br/>print(lemmatizer.lemmatize('drive,pos='v'))<br/>print(lemmatizer.lemmatize('drives',pos='v'))<br/>print(lemmatizer.lemmatize('drove',pos='v'))<br/>print(lemmatizer.lemmatize('driven',pos='v'))</span></pre><p id="49b4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出是:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="3d33" class="lb jq hi kx b fi lc ld l le lf">good<br/>good<br/>drive<br/>drove<br/>driven</span></pre><h1 id="0c69" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">动词 （verb的缩写）停用字词(移除)</h1><p id="8d25" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">当我们说话时，我们会使用大量的停用词——也就是我们字面上所说的“停止”。如果你不是在做高级的NLP，这些单词大部分是没用的，因为即使是一个字母对你的模型也很重要。想想像“一个”、“一个”、“这个”这样的词，甚至像“嗯”这样的停用词，即使是最熟练的演讲者也经常使用。让我们看看存储在nltk.corpus包中的停用词列表:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="4e3c" class="lb jq hi kx b fi lc ld l le lf">from nltk.corpus import stopwords<br/>print(stopwords.words(“english”))</span></pre><p id="2a0c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">nltk.corpus包中的停用词列表:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1b32" class="lb jq hi kx b fi lc ld l le lf">[‘i’, ‘me’, ‘my’, ‘myself’, ‘we’, ‘our’, ‘ours’, ‘ourselves’, ‘you’, “you’re”, “you’ve”, “you’ll”, “you’d”, ‘your’, ‘yours’, ‘yourself’, ‘yourselves’, ‘he’, ‘him’, ‘his’, ‘himself’, ‘she’, “she’s”, ‘her’, ‘hers’, ‘herself’, ‘it’, “it’s”, ‘its’, ‘itself’, ‘they’, ‘them’, ‘their’, ‘theirs’, ‘themselves’, ‘what’, ‘which’, ‘who’, ‘whom’, ‘this’, ‘that’, “that’ll”, ‘these’, ‘those’, ‘am’, ‘is’, ‘are’, ‘was’, ‘were’, ‘be’, ‘been’, ‘being’, ‘have’, ‘has’, ‘had’, ‘having’, ‘do’, ‘does’, ‘did’, ‘doing’, ‘a’, ‘an’, ‘the’, ‘and’, ‘but’, ‘if’, ‘or’, ‘because’, ‘as’, ‘until’, ‘while’, ‘of’, ‘at’, ‘by’, ‘for’, ‘with’, ‘about’, ‘against’, ‘between’, ‘into’, ‘through’, ‘during’, ‘before’, ‘after’, ‘above’, ‘below’, ‘to’, ‘from’, ‘up’, ‘down’, ‘in’, ‘out’, ‘on’, ‘off’, ‘over’, ‘under’, ‘again’, ‘further’, ‘then’, ‘once’, ‘here’, ‘there’, ‘when’, ‘where’, ‘why’, ‘how’, ‘all’, ‘any’, ‘both’, ‘each’, ‘few’, ‘more’, ‘most’, ‘other’, ‘some’, ‘such’, ‘no’, ‘nor’, ‘not’, ‘only’, ‘own’, ‘same’, ‘so’, ‘than’, ‘too’, ‘very’, ‘s’, ‘t’, ‘can’, ‘will’, ‘just’, ‘don’, “don’t”, ‘should’, “should’ve”, ‘now’, ‘d’, ‘ll’, ‘m’, ‘o’, ‘re’, ‘ve’, ‘y’, ‘ain’, ‘aren’, “aren’t”, ‘couldn’, “couldn’t”, ‘didn’, “didn’t”, ‘doesn’, “doesn’t”, ‘hadn’, “hadn’t”, ‘hasn’, “hasn’t”, ‘haven’, “haven’t”, ‘isn’, “isn’t”, ‘ma’, ‘mightn’, “mightn’t”, ‘mustn’, “mustn’t”, ‘needn’, “needn’t”, ‘shan’, “shan’t”, ‘shouldn’, “shouldn’t”, ‘wasn’, “wasn’t”, ‘weren’, “weren’t”, ‘won’, “won’t”, ‘wouldn’, “wouldn’t”]</span></pre><p id="36a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，既然我们确定了这些单词的“重要性”,我们可以把它们从课文和句子的单词列表中去掉:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1202" class="lb jq hi kx b fi lc ld l le lf">import  nltk<br/>from nltk.corpus import stopwords</span><span id="d478" class="lb jq hi kx b fi ln ld l le lf">txt = "It is good to be a pythoner who is pythonly pythoning with python"<br/>words = nltk.word_tokenize(txt)<br/>stop_words = set(stopwords.words("english"))</span><span id="c7c0" class="lb jq hi kx b fi ln ld l le lf">filtered_txt = [word for word in words if not word in stop_words]<br/>print(filtered_txt)</span></pre><p id="0b04" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它将通过省略诸如“是”、“到”、“是”、“一个”、“具有”这样的词来输出这个:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="9458" class="lb jq hi kx b fi lc ld l le lf">['It', 'good', 'pythoner', 'pythonly', 'pythoning', 'python']</span></pre><h1 id="8a4d" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">不及物动词N-Grams</h1><p id="c57c" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">N元语法是“来自给定文本或语音的<a class="ae jo" href="https://en.wikipedia.org/wiki/Sample_(statistics)" rel="noopener ugc nofollow" target="_blank">样本</a>的<em class="lj"> n </em>项的连续序列。根据应用可以是<a class="ae jo" href="https://en.wikipedia.org/wiki/Phoneme" rel="noopener ugc nofollow" target="_blank">音素</a>、<a class="ae jo" href="https://en.wikipedia.org/wiki/Syllable" rel="noopener ugc nofollow" target="_blank">音节</a>、<a class="ae jo" href="https://en.wikipedia.org/wiki/Letter_(alphabet)" rel="noopener ugc nofollow" target="_blank">字母</a>、<a class="ae jo" href="https://en.wikipedia.org/wiki/Word" rel="noopener ugc nofollow" target="_blank">单词</a>或<a class="ae jo" href="https://en.wikipedia.org/wiki/Base_pairs" rel="noopener ugc nofollow" target="_blank">碱基对</a>。<em class="lj">n</em>grams通常从<a class="ae jo" href="https://en.wikipedia.org/wiki/Text_corpus" rel="noopener ugc nofollow" target="_blank">文本</a>或<a class="ae jo" href="https://en.wikipedia.org/wiki/Speech_corpus" rel="noopener ugc nofollow" target="_blank">语音语料库</a>中收集。维基百科说。基本上，这些单词组合在一起表达一个“更好”的意思。根据这些组合的数量，n-gram有不同的叫法，其中当n=1时，它们是unigrams。同样，n=2是二元模型，当n=3时，我们称之为三元模型。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="be4a" class="lb jq hi kx b fi lc ld l le lf">San Francisco (is a 2-gram)</span><span id="5a81" class="lb jq hi kx b fi ln ld l le lf">The Three Musketeers (is a 3-gram)</span><span id="5f1a" class="lb jq hi kx b fi ln ld l le lf">Elvin stood up slowly (is a 4-gram)</span></pre><p id="c377" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了查看n元语法的应用，让我们来看一个关于NLP的句子，并从中生成n元语法(我将n=3，但是，您仍然可以使用函数中的参数)</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="8505" class="lb jq hi kx b fi lc ld l le lf">def generate_ngrams(s, n):<br/>    s = s.lower()<br/>    s = re.sub(r'[^a-zA-Z0-9\s]', ' ', s)<br/>    words = nltk.word_tokenize(s)<br/>    # The zip function to generate n-grams<br/>    ngrams = zip(*[words[i:] for i in range(n)])<br/>    #Return concatted words in ngrams  <br/>    return [" ".join(ngram) for ngram in ngrams]</span></pre><p id="f755" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们在输入中将文本作为一个字符串，并输入数字来标识n-gram的类型。然后，将所有字符串转换为小写，并提取带有空格的字母数字值。随后，我们将句子标记成单词，以对n元语法使用zip函数。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="017d" class="lb jq hi kx b fi lc ld l le lf">s = """Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions<br/>between computers and human (natural) languages"""</span><span id="3615" class="lb jq hi kx b fi ln ld l le lf">print(generate_ngrams(s, n=3))</span></pre><p id="ed9c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">具有上述输入值的该函数的输出为:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="0c72" class="lb jq hi kx b fi lc ld l le lf">['natural language processing', 'language processing nlp', 'processing nlp is', 'nlp is an', 'is an area', 'an area of', 'area of computer', 'of computer science', 'computer science and', 'science and artificial', 'and artificial intelligence', 'artificial intelligence concerned', 'intelligence concerned with', 'concerned with the', 'with the interactions', 'the interactions between', 'i<br/>nteractions between computers', 'between computers and', 'computers and human', 'and human natural', 'human natural languages']</span></pre><h1 id="fca2" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">七。刮痧(美丽的汤4)</h1><p id="8b2a" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">正如我们在本文前面提到的，互联网充斥着大数据，并且以令人难以置信的速度变得越来越大。这些内容大部分是由新闻门户、文章和类似维基的博客等网站提供的。“窃取”这些网站的内容，如果你需要一些文本来分析，这是一个最著名和最简单的选择。BeautifulSoup库是最常用的工具之一，通过其内置的功能来抓取任何网站。要安装BS4，请在cmd中键入以下命令。但是，我们还需要安装lxml库来解析HTML和xml文件，然后在HTML或XML的标签之间抓取文本:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="2ec9" class="lb jq hi kx b fi lc ld l le lf">pip install beautifulsoup4<br/>pip install lxml</span></pre><p id="c544" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们从《纽约时报》(<a class="ae jo" href="https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&amp;action=click&amp;pgtype=Homepage&amp;clickSource=story-heading&amp;module=first-column-region&amp;region=top-news&amp;WT.nav=top-news" rel="noopener ugc nofollow" target="_blank">nytimes.com</a>)上随机抓取一篇文章:</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="2cd9" class="lb jq hi kx b fi lc ld l le lf">from bs4 import BeautifulSoup<br/>import requests<br/>import re</span><span id="99b7" class="lb jq hi kx b fi ln ld l le lf">url = "<a class="ae jo" href="https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&amp;action=click&amp;pgtype=Homepage&amp;clickSource=story-heading&amp;module=first-column-region&amp;region=top-news&amp;WT.nav=top-news" rel="noopener ugc nofollow" target="_blank">https://www.nytimes.com/2018/08/13/us/politics/peter-strzok-fired-fbi.html?hp&amp;action=click&amp;pgtype=Homepage&amp;clickSource=story-heading&amp;module=first-column-region&amp;region=top-news&amp;WT.nav=top-news</a>"<br/>res = requests.get(url)<br/>html = res.content<br/>soup = BeautifulSoup(html, 'html5lib')<br/>entered_attr = input("enter attribute:")<br/>t = soup.findAll("p",attrs={"class": "css-exrw3m evys1bk0"})<br/>all_text = ""<br/>for te in t:<br/>    all_text += te.text</span><span id="3e26" class="lb jq hi kx b fi ln ld l le lf">print(all_text)</span></pre><p id="60ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，我们将请求发送到给定的URL，以抓取保存在上面代码所示的“html”变量中的网站内容。下面，<em class="lj"> findAll </em>函数用它的All标签抓取网站的所有内容。但是，作为参数，我们输入了“<em class="lj"> p </em>，这意味着将检索所有段落，然后，由于执行了函数“<em class="lj"> findAll </em>，将输出所有<em class="lj"> p- </em>值(段落)文本。为了根据属性过滤它，我们可以添加另一个参数，即“attrs”，它接受类<em class="lj">的属性值(假设您有一些关于HTML标签的信息并检查了浏览器的功能)。</em>由于函数返回的不是一段，而是几段，最后我们在all_text中汇总全部，最终打印出来。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="9ad8" class="lb jq hi kx b fi lc ld l le lf">WASHINGTON — Peter Strzok, the F.B.I. senior counterintelligence agent who disparaged President Trump in inflammatory text messages and helped oversee the Hillary Clinton email and Russia investigations has been fired for violating bureau policies, Mr. Strzok’s lawyer said Monday….</span></pre><blockquote class="lg lh li"><p id="37de" class="iq ir lj is b it iu iv iw ix iy iz ja lk jc jd je ll jg jh ji lm jk jl jm jn hb bi translated">由于该主题足够宽泛，可以一次输入全部内容，因此我将在这里总结一下<em class="hi">NLP第一部分</em>  <em class="hi">的介绍，</em>我们提到的关于<br/><strong class="is hj">NLP的简要介绍，标记化，使用NLTK和Spacy的词性标记，词干化和词条化，停用词及其移除，N-Grams和使用BS4的网站抓取。</strong></p><p id="02b0" class="iq ir lj is b it iu iv iw ix iy iz ja lk jc jd je ll jg jh ji lm jk jl jm jn hb bi translated"><strong class="is hj"> NLP第2部分</strong>将讲述关于NLP的更高级的概念，其中我们将构建ML模型来分析搜集的数据，对文本进行<strong class="is hj">情感分析，</strong>谈论<strong class="is hj"> Sklearn和</strong>构建<strong class="is hj">朴素贝叶斯分类器，</strong>进行<strong class="is hj">文本分类，文本矢量化:单词包和TF-IDF，文本摘要</strong>等等。</p></blockquote><p id="4d42" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="lj">感谢</em> <strong class="is hj"> <em class="lj">您的</em> </strong> <em class="lj">时间看完(如果您还在进行这个长的:)。如果有任何遗漏或错误，我随时欢迎建设性的批评。干杯！&lt;/&gt;/T40】</em></p><p id="0023" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Elvin Aghammadzada <br/>巴库工程大学计算机系学生</strong></p></div></div>    
</body>
</html>