<html>
<head>
<title>Sigmoid Neuron model, Gradient Descent with sample code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Sigmoid神经元模型，带样本代码的梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sigmoid-neuron-model-gradient-descent-with-sample-code-4919bfc9d4c4?source=collection_archive---------0-----------------------#2019-06-05">https://medium.com/analytics-vidhya/sigmoid-neuron-model-gradient-descent-with-sample-code-4919bfc9d4c4?source=collection_archive---------0-----------------------#2019-06-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ac49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在感知器模型中，限制是输出函数(二进制输出)中有非常剧烈的变化，这需要线性可分离的数据。然而，在大多数实际情况下，<strong class="ih hj">我们需要一个连续的输出</strong>。所以我们提出了一个<strong class="ih hj">的乙状结肠神经元模型</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/90ab1517e7173e6ae88d4de81b55f4b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*o3k4HVz3mMG7nx4X2ovJzg.png"/></div></figure><p id="8ebc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面显示的函数是一个sigmoid函数，它采用线性输入，产生平滑连续的输出(以红线显示)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jl"><img src="../Images/d1b92eb2bc38386141481898a86c6cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*i1QShO6rm5zzdDTQ_cCkpA.png"/></div></figure><p id="2111" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里红线是sigmoid模型的输出，蓝线是感知器模型的输出。无论输入数量多少，输出值都在[0，1]之间。随着总和不断变化，我们观察到红线上的不同值。</p><p id="7370" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">sigmoid模型可用于回归和分类问题。<strong class="ih hj">在回归的情况下，sigmoid函数的预测值是y值，无论是在分类问题中，首先使用sigmoid函数进行预测，然后确定对不同类别的预测y进行分类的阈值。</strong>阈值可以是0.5或预测y的平均值或任何值，具体取决于问题。</p><h2 id="750a" class="jm jn hi bd jo jp jq jr js jt ju jv jw iq jx jy jz iu ka kb kc iy kd ke kf kg bi translated">损失函数</h2><p id="705e" class="pw-post-body-paragraph if ig hi ih b ii kh ik il im ki io ip iq kj is it iu kk iw ix iy kl ja jb jc hb bi translated"><strong class="ih hj">交叉熵或平方损失</strong>用于乙状结肠神经元模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es km"><img src="../Images/01c04d5a94db0f04432df38520685f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*ANq46-cgM3-TVdfcFrOIcQ.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">这里cap y是预测值，y是实际值</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kr"><img src="../Images/b4fdf0383e802d0b5f2784b81728ef9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*Ub3EkHYeiu0LylNV_HE7tg.png"/></div></figure><h2 id="3e3b" class="jm jn hi bd jo jp jq jr js jt ju jv jw iq jx jy jz iu ka kb kc iy kd ke kf kg bi translated">学习算法(梯度下降背后的数学)</h2><p id="48d4" class="pw-post-body-paragraph if ig hi ih b ii kh ik il im ki io ip iq kj is it iu kk iw ix iy kl ja jb jc hb bi translated">通过改变系数(w)和偏差(b)的值，你将得到sigmoid函数族。</p><p id="f14f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里从随机的‘w’和‘b’值开始，计算损失，然后再次更新w和b，以此类推。最初，您可能从最差的sigmoid函数开始，但随着w和b的更新，它将达到最佳sigmoid函数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ks"><img src="../Images/7e6273a5176004c1e803bc9de5433df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*MpE2dWC7xNEp61sAMAOPBA.png"/></div><figcaption class="kn ko et er es kp kq bd b be z dx translated">彩色曲线是不同w和b值的不同sigmoid函数。</figcaption></figure><p id="3654" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，Pytorch/Tensorflow内置了这一功能，可以自动计算最佳w和b，并使损失最小化。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kt"><img src="../Images/a6b289a9f01b3a0473b4fe59596ac2a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*VQZVBo6eu0Cqchsnr-VumA.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="ab fe cl ku"><img src="../Images/c1c5a2bf2dd49e462918f58187ef60e9.png" data-original-src="https://miro.medium.com/v2/format:webp/1*iUbc4svOER2VNFOJkGQJBA.png"/></div></figure><p id="4025" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此后为了计算每对(w，b)的损失，我们需要找到等级-w和等级-b。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kv"><img src="../Images/db35d26580ed0ab980a3cc557a1f47a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PapY3ULfHWetQvfM5mQgJA.png"/></div></div></figure><p id="1b68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们用泰勒级数来计算(w，b)更新后的损失。计算出来的损失应该小于之前的。现在我们的目标是找到(w，b)使得三次近似泰勒级数的第二项产生负值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es la"><img src="../Images/fb4aa36e36b4024e952e8fffe4b91b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-pCaAM-FzIAJ_z0_lmrqSQ.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lb"><img src="../Images/acf6b0057c69338b0875be259aa960a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*Cb9VZi2-9iHC86c5Ll1FZA.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lc"><img src="../Images/a7473d4114b43a15f72374c4a0be86d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bims7qMFp4c9vkbqje28og.png"/></div></div></figure><p id="3a19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如何计算成绩</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es ld"><img src="../Images/d6c3ccfae9b623c662ca4aaece97b0b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I3LwCLJUaPS4SnRU8fBOeA.png"/></div></div></figure><p id="499d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">整体适应度函数如下所示</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es le"><img src="../Images/19d631721d6575b183d0725982e5d442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fx4j5WWBIuvJhCmgqOpMTg.png"/></div></div></figure><p id="d825" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">迭代直到满意表示</strong></p><ul class=""><li id="fc8c" class="lf lg hi ih b ii ij im in iq lh iu li iy lj jc lk ll lm ln bi translated">在迭代次数的预定值</li><li id="28c7" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">在损失值的预定值</li><li id="6443" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc lk ll lm ln bi translated">如果迭代后(w，b)的变化不大</li></ul><h2 id="705a" class="jm jn hi bd jo jp jq jr js jt ju jv jw iq jx jy jz iu ka kb kc iy kd ke kf kg bi translated"><strong class="ak">多变量情况下的示例</strong></h2><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lt"><img src="../Images/bd0d65d8af35153e0231c2f5cfc5e5f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lL9htJRMIo8kFdVXPMu3UQ.png"/></div></div></figure><pre class="je jf jg jh fd lu lv lw lx aw ly bi"><span id="b9d7" class="jm jn hi lv b fi lz ma l mb mc">#importing library<br/><strong class="lv hj">import</strong> <strong class="lv hj">numpy</strong> <strong class="lv hj">as</strong> <strong class="lv hj">np</strong><br/><strong class="lv hj">import</strong> <strong class="lv hj">matplotlib.pyplot</strong> <strong class="lv hj">as</strong> <strong class="lv hj">plt</strong><br/><strong class="lv hj">from</strong> <strong class="lv hj">mpl_toolkits</strong> <strong class="lv hj">import</strong> mplot3d<br/><strong class="lv hj">import</strong> <strong class="lv hj">matplotlib.colors</strong><br/><strong class="lv hj">import</strong> <strong class="lv hj">pandas</strong> <strong class="lv hj">as</strong> <strong class="lv hj">pd</strong><br/><strong class="lv hj">from</strong> <strong class="lv hj">sklearn.preprocessing</strong> <strong class="lv hj">import</strong> StandardScaler, MinMaxScaler<br/><strong class="lv hj">from</strong> <strong class="lv hj">sklearn.model_selection</strong> <strong class="lv hj">import</strong> train_test_split<br/><strong class="lv hj">from</strong> <strong class="lv hj">sklearn.metrics</strong> <strong class="lv hj">import</strong> accuracy_score, mean_squared_error<br/><strong class="lv hj">from</strong> <strong class="lv hj">tqdm</strong> <strong class="lv hj">import</strong> tqdm_notebook</span><span id="3852" class="jm jn hi lv b fi md ma l mb mc"><strong class="lv hj">#sigmoid class for two variable<br/>def</strong> sigmoid(x1, x2, w1, w2, b):<br/>  <strong class="lv hj">return</strong> 1/(1 + np.exp(-(w1*x1 + w2*x2 + b)))<br/></span><span id="5f03" class="jm jn hi lv b fi md ma l mb mc"><strong class="lv hj">#calculate loss<br/>def</strong> calculate_loss(X, Y, w_est, b_est):<br/>  loss = 0<br/>  <strong class="lv hj">for</strong> x, y <strong class="lv hj">in</strong> zip(X, Y):<br/>    loss += (y - sigmoid(x, w_est, b_est))**2<br/>  <strong class="lv hj">return</strong> loss</span><span id="bfe2" class="jm jn hi lv b fi md ma l mb mc"><strong class="lv hj">#=======================================<br/>#class</strong> <strong class="lv hj">SigmoidNeuron</strong>:<br/>  <br/>  <strong class="lv hj">def</strong> __init__(self):<br/>    self.w = <strong class="lv hj">None</strong><br/>    self.b = <strong class="lv hj">None</strong><br/>    <br/>  <strong class="lv hj">def</strong> perceptron(self, x):<br/>    <strong class="lv hj">return</strong> np.dot(x, self.w.T) + self.b<br/>  <br/>  <strong class="lv hj">def</strong> sigmoid(self, x):<br/>    <strong class="lv hj">return</strong> 1.0/(1.0 + np.exp(-x))<br/>  <br/>  <strong class="lv hj">def</strong> grad_w(self, x, y):<br/>    y_pred = self.sigmoid(self.perceptron(x))<br/>    <strong class="lv hj">return</strong> (y_pred - y) * y_pred * (1 - y_pred) * x<br/>  <br/>  <strong class="lv hj">def</strong> grad_b(self, x, y):<br/>    y_pred = self.sigmoid(self.perceptron(x))<br/>    <strong class="lv hj">return</strong> (y_pred - y) * y_pred * (1 - y_pred)<br/>  <br/>  <strong class="lv hj">def</strong> fit(self, X, Y, epochs=1, learning_rate=1, initialise=<strong class="lv hj">True</strong>, display_loss=<strong class="lv hj">False</strong>):<br/>    <br/>    <em class="me"># initialise w, b</em><br/>    <strong class="lv hj">if</strong> initialise:<br/>      self.w = np.random.randn(1, X.shape[1])<br/>      self.b = 0<br/>      <br/>    <strong class="lv hj">if</strong> display_loss:<br/>      loss = {}<br/>    <br/>    <strong class="lv hj">for</strong> i <strong class="lv hj">in</strong> tqdm_notebook(range(epochs), total=epochs, unit="epoch"):<br/>      dw = 0<br/>      db = 0<br/>      <strong class="lv hj">for</strong> x, y <strong class="lv hj">in</strong> zip(X, Y):<br/>        dw += self.grad_w(x, y)<br/>        db += self.grad_b(x, y)       <br/>      self.w -= learning_rate * dw<br/>      self.b -= learning_rate * db<br/>      <br/>      <strong class="lv hj">if</strong> display_loss:<br/>        Y_pred = self.sigmoid(self.perceptron(X))<br/>        loss[i] = mean_squared_error(Y_pred, Y)<br/>    <br/>    <strong class="lv hj">if</strong> display_loss:<br/>      plt.plot(loss.values())<br/>      plt.xlabel('Epochs')<br/>      plt.ylabel('Mean Squared Error')<br/>      plt.show()<br/>      <br/>  <strong class="lv hj">def</strong> predict(self, X):<br/>    Y_pred = []<br/>    <strong class="lv hj">for</strong> x <strong class="lv hj">in</strong> X:<br/>      y_pred = self.sigmoid(self.perceptron(x))<br/>      Y_pred.append(y_pred)<br/>    <strong class="lv hj">return</strong> np.array(Y_pred)</span></pre><h1 id="28ae" class="mf jn hi bd jo mg mh mi js mj mk ml jw mm mn mo jz mp mq mr kc ms mt mu kf mv bi translated">估价</h1><p id="390c" class="pw-post-body-paragraph if ig hi ih b ii kh ik il im ki io ip iq kj is it iu kk iw ix iy kl ja jb jc hb bi translated">分类问题的准确率</p><p id="93d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回归问题中的RMSE</p><p id="3c91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">===================================</p><p id="e523" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">======详细代码在下面的Github链接中给出=====</p><div class="mw mx ez fb my mz"><a href="https://github.com/ranasingh-gkp/padhaiAI/tree/master/Sigmoid%2C%20Gradient%20Descent" rel="noopener  ugc nofollow" target="_blank"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hj fi z dy ne ea eb nf ed ef hh bi translated">ranasingh-gkp/padhaiAI</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">在GitHub上创建一个帐户，为ranasingh-gkp/padhaiAI的发展做出贡献。</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">github.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn jj mz"/></div></div></a></div><h1 id="e656" class="mf jn hi bd jo mg mh mi js mj mk ml jw mm mn mo jz mp mq mr kc ms mt mu kf mv bi translated">参考</h1><ol class=""><li id="0426" class="lf lg hi ih b ii kh im ki iq no iu np iy nq jc nr ll lm ln bi translated">维基百科(一个基于wiki技术的多语言的百科全书协作计划ˌ也是一部用不同语言写成的网络百科全书ˌ 其目标及宗旨是为全人类提供自由的百科全书)ˌ开放性的百科全书</li><li id="1524" class="lf lg hi ih b ii lo im lp iq lq iu lr iy ls jc nr ll lm ln bi translated">四分之一实验室深度学习(特别感谢)</li></ol></div></div>    
</body>
</html>