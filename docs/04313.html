<html>
<head>
<title>Make your Apache Spark column based in-built functions more dynamic and avoid UDFs, using expressions.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用表达式使基于Apache Spark column的内置函数更加动态，并避免使用UDF。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/make-your-apache-spark-column-based-in-built-functions-more-dynamic-and-avoid-udfs-using-54486f1dbf47?source=collection_archive---------1-----------------------#2020-03-14">https://medium.com/analytics-vidhya/make-your-apache-spark-column-based-in-built-functions-more-dynamic-and-avoid-udfs-using-54486f1dbf47?source=collection_archive---------1-----------------------#2020-03-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ac9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> (Spark2.4和Python3) </strong></p><p id="3dbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="jd">Spark</em></strong><em class="jd"/><strong class="ih hj"><em class="jd">2.4+</em></strong>为Python和Scala提供了全面而健壮的API，允许开发者实现各种基于sql的函数，用于大规模操作和转换数据。正如在API中看到的，这些函数可以接受两种类型的输入；完整的<strong class="ih hj">列值(col) </strong>，或<strong class="ih hj">值</strong>(它们是<strong class="ih hj">静态的</strong>，不会因处理的每一行而改变)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/c99dd34bf46ed85aed0fa6fd7696aca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-2GDktS3fDMKMDgdBBnIQ.jpeg"/></div></div></figure><p id="91ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">PySpark API:</strong><a class="ae jq" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/latest/API/python/PySpark . SQL . html</a></p><h2 id="a8b1" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated"><strong class="ak"> <em class="km">例如:</em> </strong></h2><p id="eccb" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated"><strong class="ih hj">相关函数:</strong> ( <em class="jd">以2列为输入</em>)</p><p id="a5e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du ks kt ku kv b">pyspark.sql.functions.<strong class="ih hj">corr</strong></code> <strong class="ih hj"> ( <em class="jd">栏1 </em>，<em class="jd">栏2 </em> ) </strong></p><p id="4a86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">相关性将被计算到</em><strong class="ih hj"><em class="jd">col 1和col2 </em> </strong>的每行组合中</p><p id="7086" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">子串函数</strong> :(str( <strong class="ih hj">列</strong>)、pos( <strong class="ih hj">值</strong>)、len( <strong class="ih hj">值</strong>))</p><p id="62aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du ks kt ku kv b">pyspark.sql.functions.<strong class="ih hj">substring</strong></code> <strong class="ih hj"> ( <em class="jd"> str </em>，<em class="jd"> pos </em>，<em class="jd"> len </em> ) </strong></p><p id="70ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">因此，与可以接受col的str不同，pos和len是文字值，不会为每一行</em> <strong class="ih hj"> <em class="jd">改变</em> </strong> <em class="jd">。</em></p><blockquote class="kw kx ky"><p id="6e52" class="if ig jd ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated">这里，您可以使用<strong class="ih hj">表达式</strong>使您的子串函数<strong class="ih hj">更加动态，</strong>允许它<strong class="ih hj">将pos和len值作为整个列</strong>，而不是<strong class="ih hj">分配静态的文字值。</strong></p></blockquote><p id="d8bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你试图在<strong class="ih hj"> PySpark语法中给你的<strong class="ih hj"> pos和len位置</strong>分配列，如上图</strong>，<strong class="ih hj">所示，你会得到一个错误:</strong></p><pre class="jf jg jh ji fd lc kv ld le aw lf bi"><span id="49f6" class="jr js hi kv b fi lg lh l li lj">TypeError: Column is not iterable</span></pre><h1 id="bb83" class="lk js hi bd jt ll lm ln jx lo lp lq kb lr ls lt ke lu lv lw kh lx ly lz kk ma bi translated"><strong class="ak">什么是表情？</strong></h1><p id="8772" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">从表面上看，您可以看到它们允许您在spark代码中编写<strong class="ih hj"> sql类型语法</strong>。然而，当你深入观察时，你会发现它们有能力在内置功能中激发你的火花。</p><p id="ee5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">表达式有两种用法，<em class="jd">pypspark . SQL . functions . expr</em><strong class="ih hj"/>或者<em class="jd"> df。select expr</em>T6。</p><p id="f29f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">一些基本语法</strong>:</p><pre class="jf jg jh ji fd lc kv ld le aw lf bi"><span id="8bc6" class="jr js hi kv b fi lg lh l li lj">df.selectExpr("age * 2", "abs(age)").collect()</span><span id="260b" class="jr js hi kv b fi mb lh l li lj">df.select(expr("length(name)")).collect()</span></pre><blockquote class="kw kx ky"><p id="88e5" class="if ig jd ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated">除此之外，表达式基本上允许您用<strong class="ih hj">输入列值(col) </strong> <strong class="ih hj">来代替文字值</strong>，这在文档中显示的常用Pyspark api语法中是<strong class="ih hj">不可能</strong>做到的。</p></blockquote><p id="db79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我将提供使用表达式和内置函数的例子，以一种动态和可伸缩的方式处理现实世界的spark问题。</p><h2 id="22a3" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">示例1:</h2><p id="3d22" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">假设您有一个DataFrame(df ),它具有StringType的col1列，您必须<strong class="ih hj">解包</strong>并<strong class="ih hj">创建两个新列(日期和名称)</strong>。我们将使用一个更动态的<strong class="ih hj">子串</strong>函数来解包我们的列。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mc"><img src="../Images/727dad85a5415f2be1a49bc814c31401.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*XnxFNfICZt7FoCb7wAgHIg.png"/></div></figure><p id="9347" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以看到，日期字符串是每次<strong class="ih hj"> (9) </strong>的<strong class="ih hj">相同字符串长度</strong>，然而，用于名称的字符串长度是<strong class="ih hj">不断变化的(Mary:4，William:8，Benjamin:9)。</strong></p><p id="e5f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">pyspark api <strong class="ih hj">中的常规子串函数将使我们能够解包日期</strong>、<strong class="ih hj">，但是对于名称，我们将使用表达式</strong>来执行该函数。</p><pre class="jf jg jh ji fd lc kv ld le aw lf bi"><span id="94d9" class="jr js hi kv b fi lg lh l li lj">from pyspark.sql import functions as F<br/>df.withColumn(“Date”, F.substring(“col1”,1,9))\<br/>.withColumn("name, <strong class="kv hj">F.expr(“””substr(col1,10,length(col1))”””)</strong>).show()</span><span id="72a6" class="jr js hi kv b fi mb lh l li lj"><strong class="kv hj"><em class="jd">OR</em></strong></span><span id="6363" class="jr js hi kv b fi mb lh l li lj">from pyspark.sql import functions as F<br/>df.withColumn("Date", F.substring("col1",1,9))\<br/>.withColumn("Length", F.length("col1"))\<br/>.withColumn("Name", <strong class="kv hj">F.expr("""substr(col1,10,Length)""")</strong>)\<br/>.drop("Length").show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es md"><img src="../Images/a6320ccc9f85b1c489f4273bebb11ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*xMjl9ghvRtEP3gyETUY1xg.png"/></div></figure><p id="1699" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您在<strong class="ih hj">粗体中所看到的，</strong>这两个表达式<strong class="ih hj"> (F.expr) </strong>允许您向您的<strong class="ih hj">子串函数</strong>提供一个列<strong class="ih hj"> (length(col1)或Length column) </strong>，这基本上使得<strong class="ih hj">对每一行</strong> <strong class="ih hj">都是动态的，而无需使用UDF </strong>(用户定义的函数)。</p><blockquote class="kw kx ky"><p id="1dcc" class="if ig jd ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated">这将节省大量宝贵的<strong class="ih hj">计算资源</strong>和时间，因为<strong class="ih hj">赋予您spark内置函数</strong>到<strong class="ih hj">像UDF </strong>一样运行，而没有与UDFS 相关的所有<strong class="ih hj">计算开销。</strong></p></blockquote><p id="0298" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我在<em class="jd"> stackoverflow </em>上回答过一个<strong class="ih hj">类似的问题</strong>:<a class="ae jq" href="https://stackoverflow.com/questions/60426113/how-to-add-delimiters-to-a-csv-file/60428023#60428023" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/60426113/how-to-add-delimiters-to-a-CSV-file/60428023 # 60428023</a>。使用表达式的理由有点不同，因为字符串<strong class="ih hj">的长度没有改变</strong>，这是出于懒惰(我不想计算字符串的长度)。</p><h2 id="2c2b" class="jr js hi bd jt ju jv jw jx jy jz ka kb iq kc kd ke iu kf kg kh iy ki kj kk kl bi translated">示例2:</h2><p id="98c8" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">这个例子实际上直接来自我回答的一个stackoverflow问题:<a class="ae jq" href="https://stackoverflow.com/questions/60494549/how-to-filter-a-column-in-a-data-frame-by-the-regex-value-of-another-column-in-s/60494657#60494657" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/60494549/how-to-filter-a-column-in-a-data-frame-by-the-regex-value-of-another-column-in-s/60494657 # 60494657</a></p><p id="5327" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设您有一个DataFrame，其中有一个StringType类型的列(<strong class="ih hj">查询</strong>)，您必须对其应用一个<strong class="ih hj"> regexp_extract </strong>函数，并且您有另一个列(<strong class="ih hj"> regex_patt </strong>)，其中包含该regex的所有模式，<strong class="ih hj">逐行。</strong>如果您不知道如何使regexp_extract函数对每一行都是动态的，<strong class="ih hj">您将构建一个UDF，将两列</strong>作为输入，并计算每一行的正则表达式。<strong class="ih hj">(会很慢，成本效率低)</strong>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/cda674adf61d409e0c18b39da985aa82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1txDu5T3HpIeGykYb7RAMQ.png"/></div></div></figure><p id="0214" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">这个问题主要是想过滤掉与给定模式不匹配的行。</em></p><p id="69a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PySpark api有一个内置的<strong class="ih hj"> regexp_extract: </strong></p><pre class="jf jg jh ji fd lc kv ld le aw lf bi"><span id="126b" class="jr js hi kv b fi lg lh l li lj">pyspark.sql.functions.<strong class="kv hj">regexp_extract</strong>(<em class="jd">str</em>, <em class="jd">pattern</em>, <em class="jd">idx</em>)</span></pre><p id="5482" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，我<strong class="ih hj"> t只把str作为列</strong>，<strong class="ih hj">而不是模式</strong>。该模式必须在函数中指定为<strong class="ih hj">静态字符串值。因此，我们可以使用一个<strong class="ih hj">表达式向函数的模式部分</strong>发送一列:</strong></p><pre class="jf jg jh ji fd lc kv ld le aw lf bi"><span id="c348" class="jr js hi kv b fi lg lh l li lj">from pyspark.sql import functions as F<br/>df.withColumn("query1", <strong class="kv hj">F.expr("""regexp_extract(query, regex_patt)""")</strong>).filter(F.col("query1")!='').drop("query1").show(truncate=False)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mf"><img src="../Images/8840c8f8cea3be2214d68c776adec1a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mKBk7a6Af3NDPCAPoIyFpg.png"/></div></div></figure><p id="41a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">粗体显示的表达式</strong>允许我们逐行应用正则表达式<strong class="ih hj">并过滤掉不匹配的行<strong class="ih hj">，因此使用<strong class="ih hj">过滤器移除了第2行</strong>。</strong></strong></p><p id="7990" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如上所述</strong>，如果您试图将<strong class="ih hj"> regex_patt作为一列</strong>放入您的<strong class="ih hj">常用pyspark regexp_replace函数语法</strong>中，您会得到以下错误:</p><pre class="jf jg jh ji fd lc kv ld le aw lf bi"><span id="51bd" class="jr js hi kv b fi lg lh l li lj">TypeError: Column is not iterable</span></pre><p id="bb0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">例3: </strong></p><p id="ab84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设您有一个显示在下面的数据框架<strong class="ih hj">，它有一个<strong class="ih hj"> loan_date列(DateType) </strong>和days_to_make_payment列(<strong class="ih hj"> IntegerType </strong>)。您想要计算付款的最后日期<strong class="ih hj"/>，这基本上是<em class="jd">将天数列添加到日期列，以获得新的日期</em><strong class="ih hj"><em class="jd"/></strong><em class="jd">。</em></strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mg"><img src="../Images/b74424b2d5b6c1d89c2ba68039a919dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*hLwvb66s4ieNEunyhTV83A.png"/></div></figure><p id="1bce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以使用spark内置的<strong class="ih hj"> date_add </strong>函数来完成此操作:</p><pre class="jf jg jh ji fd lc kv ld le aw lf bi"><span id="999c" class="jr js hi kv b fi lg lh l li lj">pyspark.sql.functions.<strong class="kv hj">date_add</strong>(<em class="jd">start</em>, <em class="jd">days</em>)</span></pre><p id="1323" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">它返回开始后第天的日期。</em> <strong class="ih hj">然而</strong>，使用这个语法，它<strong class="ih hj">只允许我们把<em class="jd">开始</em>作为列</strong>，而把<strong class="ih hj"><em class="jd"/>作为静态整数值。</strong>因此，我们可以使用一个<strong class="ih hj">表达式将days_to_make_payment列作为<em class="jd"> days </em> </strong>发送到我们的函数中。</p><pre class="jf jg jh ji fd lc kv ld le aw lf bi"><span id="86c5" class="jr js hi kv b fi lg lh l li lj">from pyspark.sql import functions as F<br/>df.withColumn(“last_date_for_payment”, <strong class="kv hj">F.expr(“””date_add(Loan_date,days_to_make_payment)”””)</strong>).show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mh"><img src="../Images/82739c69feb0e2f595b37c4447b05a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UfW0rW48CmAkKak56YKpFw.png"/></div></div></figure><p id="b2d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我只想最后一次重申<strong class="ih hj"/>如果您使用常用的pyspark语法将<strong class="ih hj">天_到_付款</strong>到<em class="jd">天</em>写成这样:</p><pre class="jf jg jh ji fd lc kv ld le aw lf bi"><span id="e331" class="jr js hi kv b fi lg lh l li lj">from pyspark.sql import functions as F<br/>df.withColumn("last_date_for_payment", F.date_add(F.col("Loan_date"),F.col("days_to_make_payment"))).show()</span></pre><p id="7523" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">你会得到这个错误:</strong></p><pre class="jf jg jh ji fd lc kv ld le aw lf bi"><span id="42e8" class="jr js hi kv b fi lg lh l li lj">TypeError: Column is not iterable</span></pre><h1 id="a200" class="lk js hi bd jt ll lm ln jx lo lp lq kb lr ls lt ke lu lv lw kh lx ly lz kk ma bi translated">结论:</h1><p id="7920" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">Spark是大数据处理引擎的黄金标准，它有一个庞大的开源社区一直在为此做出贡献。它有太多的功能可以让你执行Pb级的转换。也就是说，当涉及到<strong class="ih hj">UDF(需要将数据从执行器的JVM移动到Python解释器)</strong>和<strong class="ih hj"> Joins(在分区/内核之间混洗数据)</strong>时，人们应该<strong class="ih hj">很好地意识到它的局限性</strong>，并且人们应该总是<strong class="ih hj">尝试将其内置函数推到它们的极限</strong>，因为它们对于大数据任务<strong class="ih hj"> <em class="jd">是高度优化和可伸缩的</em>。</strong></p></div></div>    
</body>
</html>