<html>
<head>
<title>Super Car-io Odyssey: My Favorite Papers from CVPR 2019</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超级汽车-io奥德赛:2019年CVPR我最喜欢的论文</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/super-car-io-odyssey-my-favorite-papers-from-cvpr-2019-ccdca524e25c?source=collection_archive---------1-----------------------#2019-06-26">https://medium.com/analytics-vidhya/super-car-io-odyssey-my-favorite-papers-from-cvpr-2019-ccdca524e25c?source=collection_archive---------1-----------------------#2019-06-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7ef7d3894d36007157c3d6a5573eb8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sDmUIExPCF90_tm3vNn1yQ.jpeg"/></div></div></figure><p id="17ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上周，2019年计算机视觉和模式识别大会(CVPR)在加利福尼亚州多云的长滩举行。长滩会议中心挤满了大约10，000人，包括一些世界上顶尖的计算机视觉研究人员，和一些激进的中型研究人员(即我)。</p><p id="e226" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我很幸运能够提交过去在那里实习的一篇论文，并坐在前排(但技术上是倒数第三排，因为会议挤满了人)一些计算机视觉领域非常酷和创新的工作。现在一切都结束了，我想分享一些我觉得特别酷的论文。</p><p id="0dc8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jp">免责声明</em>:<em class="jp">T5】我不是一个有远见的人；我实际上还不是一个什么都不是的人，因为我刚刚完成我的本科学业。所以，我强调的论文不是传统的计算机视觉论文。向那些真正想听全景分割或单目深度估计的读者道歉，但我们将转而钻研运动规划和损失函数！</em></p></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><p id="6e5c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_End-To-End_Interpretable_Neural_Motion_Planner_CVPR_2019_paper.html" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">端到端可解释的神经运动规划器</strong> </a></p><p id="c021" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是尤特隆托/优步·ATG大学拉奎尔·乌尔塔森团队的最新也是最伟大的作品，乌尔塔森甚至声称这是“她写过的最好的论文”Urtasun在自动驾驶汽车(SDC)方面的研究非常务实，而且根据我在该行业的有限经验，极具影响力。我是她的超级粉丝，所以我去了她在CVPR的每一次研讨会。事后看来，这可能不是最好的主意，因为我最终在四个不同的时间里听到了同一个演讲。</p><p id="d1a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">端到端驱动</strong></p><p id="f45f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Urtasun研究的“圣杯”是SDCs的端到端方法。这意味着一种将传感器数据作为输入，并输出命令来控制车辆的方法。这是过去所做的<a class="ae jo" href="https://arxiv.org/pdf/1604.07316.pdf" rel="noopener ugc nofollow" target="_blank">，但是有一些缺点:</a></p><ol class=""><li id="6557" class="jx jy hi is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated">缺乏可解释性。</li><li id="be53" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">无法组合不同的子任务来解决主任务。这使得该方法的样品效率非常低。</li><li id="3b70" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">在现实世界中很难找到工作，在现实世界中，一些驾驶规则是必要的。</li></ol><p id="351b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这就是SDC工业转向工程堆栈方法的原因，其中不同的模块，如感知、预测和规划器，将解决问题的不同子任务(分别检测和跟踪物体，估计它们去哪里，并生成安全的执行轨迹)；这些模块将被组合在一起，形成车辆的人工智能。可悲的是，这种方法也并不完美:</p><ol class=""><li id="7552" class="jx jy hi is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated">模块之间不共享计算。</li><li id="181d" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">难以测量模块之间的误差传播；如果你不知道车在哪里，估计车会去哪里自然会受到影响。从这些错误中恢复也很困难。</li></ol><p id="d6ad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Urtasun的小组通过建立一种既可解释(由于中级预测)又在现实世界中工作良好的端到端方法，抓住了两个世界的最佳之处。本文是她所在的小组对<a class="ae jo" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Luo_Fast_and_Furious_CVPR_2018_paper.html" rel="noopener ugc nofollow" target="_blank">以前的</a> <a class="ae jo" href="http://www.cs.toronto.edu/~wenjie/papers/intentnet_corl18.pdf" rel="noopener ugc nofollow" target="_blank">工作</a>的总结；然而，与其他许多系列(即《权力的游戏》)不同，这个结局是强烈的，令人满意的。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/b1d1821f96697af0648745b9067555c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RIyGip1O9wMIFyWMdz-G6Q.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">工程堆栈与端到端(Creds: me和powerpoint)</figcaption></figure><p id="5a61" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">一位可解读的策划人</strong></p><p id="3db7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">提出的神经运动规划器(NMP)将激光雷达点云和道路网络的高清地图作为输入，并输出时空“成本体”成本量听起来很优雅，但实际上它只是每个时间步的2D热图，低价值/成本区域是很好的位置。</p><p id="e497" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果<em class="jp"> cᵗ </em>是时间步长t的成本量，<strong class="is hj"> <em class="jp"> s </em> </strong>是轨迹，那么规划的轨迹就是优化的解:</p><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/b4203da2d6129fa0259478aff333aeb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Woi9F7ySYc1di13BhcNWMQ.png"/></div></figure><p id="8653" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文给出了一个很好的图表来总结这种方法:</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kv"><img src="../Images/364948e202f821a66b40ea306c1268ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RulL3QaPVRu0yBGfz9U4sQ.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">神经运动规划器概述:输入是通过骨干网络，并预测感知结果，以及成本量。成本卷评估轨迹并选择最佳轨迹。</figcaption></figure><ol class=""><li id="f40e" class="jx jy hi is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated">输入是激光雷达点云和高清道路图的3D张量拼接，通过主干卷积网络进行馈送。</li><li id="7097" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">中间表示是通过两个头输入的。第一个，感知头，是一个<a class="ae jo" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank"> SSD网络</a>，它对场景中的实体的边界框进行回归和分类，并预测这些框的未来运动<em class="jp"> T </em> - 1步进入未来。</li><li id="a134" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">第二个是成本体头，它是一个去卷积网络，可以生成未来<em class="jp"> T </em>步的成本体。</li><li id="7146" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">轨迹样本被生成(在我稍后将讨论的另一个奇特的方法中)，并且通过成本体积的最小成本样本被选择作为计划的轨迹。</li></ol><p id="770f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在我们能够实施自己的NMP并创办另一家自动驾驶公司之前，只需要回答两个问题并获得数十亿美元:(1)如何对轨迹进行采样？以及(2)网络针对哪些损失进行训练？不幸的是，我没有足够的钱，但我可以提供答案。</p><p id="99c0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> (1)采样轨迹</strong></p><p id="d1e0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嗯，我记得上一次做几何是在中学的时候计算六边形的面积，所以这部分对我来说有点复杂。轨迹本质上是形状和速度剖面的组合。高级流程非常简单:</p><ol class=""><li id="22a6" class="jx jy hi is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated">为回旋曲线采样参数(稍后将详细介绍)，并将结果曲线用作轨迹形状。</li><li id="23d7" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">确定速度曲线(假设加速度恒定)。</li><li id="32dc" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">根据速度分布图沿曲线移动以获得轨迹。</li></ol><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/0cf6c776f70131398872b197a66d647b.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*MBTTUobHdTr6VGrBT-o4Ig.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">获取时空轨迹。</figcaption></figure><p id="6230" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">不幸的是，回旋曲线不是六边形，所以我中学的几何在这里没有用。但是这条曲线比较有名，由参数方程给出:</p><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/89a15e2d61e496e0628dc134aa60b007.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*Dj9OGwrUmaO6ERnW64Yqww.png"/></div></figure><p id="10ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，<strong class="is hj"> s </strong> (ξ)本质上定义了一条2D曲线，由到起点<strong class="is hj"> s₀ </strong> <em class="jp">，</em> <strong class="is hj"> T₀ </strong>和<strong class="is hj"> N₀ </strong>是到<strong class="is hj"> s₀ </strong>的正切和法向矢量，<em class="jp"> a </em>是比例因子。曲线的实际形状由<em class="jp"> a </em>固定，因此步骤1包括对<em class="jp"> a </em>进行均匀采样。为了形象化它的效果，我在Mathematica中画了一些回旋曲线变化的图<em class="jp"> a: </em></p><div class="km kn ko kp fd ab cb"><figure class="ky ij kz la lb lc ld paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/b31e246c524fb19d6635d41fc3e9fd72.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*1WXCvSoPhqmTnV-83gjWFA.png"/></div></figure><figure class="ky ij le la lb lc ld paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/ff51b801f475546bf4a96a8dc1c541fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*Iv6FQgHg3WS8maKc_Ir1bw.png"/></div></figure><figure class="ky ij lf la lb lc ld paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/b06d4b0bb441e91234d318d9212a3d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*UUrT1_2o7qIogyYdxchAbw.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx lg di lh li translated">a=10，30，50的回旋曲线，其中ξ在范围[0，50]内</figcaption></figure></div><p id="456f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">作为基线形状，直线和圆弧也以恒定的概率被采样。一旦确定了曲线的形状，曲线上的起点就由初始方向盘角度确定。然后，步骤2中的速度剖面ξ'(t)确定ξ(t)。最后，在步骤3中，ξ(t)产生曲线上的点，这些点形成采样轨迹的路点。</p><p id="5ba1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> (2)多任务损失</strong></p><p id="2a9e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">用感知和规划头的组合损失来训练网络:</p><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/622e9a73752cc5b49a8fe518ff19b75c.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*9bnAInVGnAQk2hQtuDT3aQ.png"/></div></figure><p id="f022" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">感知损失是一种非常简单的监督损失。使用地面实况检测结果，感知头被训练来预测在预定义的锚盒处车辆的存在(分类损失)，以及在每个当前和未来时间步长回归每个盒的位置、形状和航向角(回归损失)。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/207086c126f4274999146cd47172f0f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*0gIv6Mh-m5Ku3JlIvLFEIA.png"/></div></figure><p id="5a18" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计划损失有点棘手，因为我们仅有的地面事实是执行的轨迹，它不能很好地映射到预测的成本量。一个天真的研究者(也就是我)会将地面真实轨迹离散化为网格位置，并使用位置上的交叉熵损失来训练成本体。事实上，我在<a class="ae jo" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Hong_Rules_of_the_Road_Predicting_Driving_Behavior_With_a_Convolutional_CVPR_2019_paper.html" rel="noopener ugc nofollow" target="_blank">自己的作品</a>中就是这么做的(又是无耻的插一句)。</p><p id="d800" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">相反，Urtasun使用最大边际损失，将地面事实视为正面例子，将随机采样的轨迹视为负面例子。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/399a9b0bce885ae0fa3f4e09872febc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NbZ2UnbxIaCOZ3Nmh4w_OA.png"/></div></div></figure><p id="8a10" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">损失比交叉熵要复杂得多。这里，c^是地面实况航路点，cᵢ是抽样航路点，dᵢ是地面实况和抽样航路点之间的距离(鼓励距离远的航路点具有高成本)，γᵢ是对违反交通规则的处罚；<em class="jp"> max </em>确保<em class="jp">T5】只优化最差的样本，以便训练成本量来区分好的、基本的真实轨迹和真正差的轨迹。</em></p><p id="8c8e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该论文通过了相当多的实验，包括测量中间预测(感知和预测任务)质量的度量标准。TLDR:他们很好。我感兴趣的是可视化的成本量和规划的轨迹:</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/63f35d727956b94960946c452acc8761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HV6dYPMCpq0McttyzWPosA.gif"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">彩色区域表示低成本，计划的轨迹用红色表示，地面实况用蓝色表示。</figcaption></figure><p id="317f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">成本体积似乎很好地处理了多模式场景，包括车道变换和在路口转弯。虽然我很容易被打动(尤其是如果有一只狗参与的话)，但我认为计划轨迹的复杂性无疑是非常酷的。</p><p id="17f2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">讨论</strong></p><p id="99dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我希望我没有破坏任何SDC行业的秘密，因为我说，在SDC的书库里，机器学习还有很大的空间。直到今天，(非常复杂的)试探法似乎比学习方法更好，所以看到端到端的深度学习方法达到最先进的效果是非常有希望的。然而，一个活跃的改进领域是处理未知领域/从错误中恢复(即“<a class="ae jo" href="https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf" rel="noopener ugc nofollow" target="_blank">匕首问题</a>”)，这似乎是强化学习(RL)可以介入的领域。但那可能只是因为我是一个狂热的追星族。</p></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><p id="159a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.html" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">一种通用的、自适应的鲁棒损失函数</strong> </a></p><p id="cdfe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以这可能是整个会议中我最喜欢的一篇论文。它完美地融合了基本的机器学习理论和为什么我没有想到的原创性。我不熟悉乔恩·巴伦的(他自己写的论文！)在此之前工作，但从现在开始我一定会跟着做。他的海报旁边也有一大群人，所以他很快就会成为一流的大规模研究人员，那种让你得到街头认可的人(如果那条街上碰巧有大规模研究人员的话)。</p><p id="30c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">损失函数太多</strong></p><p id="bae2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有一个伴随视频的<a class="ae jo" href="https://www.youtube.com/watch?v=BmNKbnF69eY&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">很好地解决了这个问题。我们正在处理经典的回归问题，在这里我们试图预测给定特征<em class="jp"> x </em>的<em class="jp"> y </em>。在这种情况下，大多数人转向L₂损失，但我们都知道它对异常值非常敏感。</a></p><div class="km kn ko kp fd ab cb"><figure class="ky ij ln la lb lc ld paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/69fa593cea23ba127f568ba07bf086b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*XwI2sPoPq_qlx_TPB7VAGg.png"/></div></figure><figure class="ky ij lo la lb lc ld paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/71d82d63b295774618933acb11ab441b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*Pm2WQLzAruJWIczei65KUA.png"/></div></figure></div><p id="0622" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你真的喜欢损失函数，你可以开始尝试一些你知道的其他函数，其中一些会比其他的更好:</p><div class="km kn ko kp fd ab cb"><figure class="ky ij lp la lb lc ld paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/0ef1342ef263f9f309d32e7dd21a2d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*N0TGgEqgO-ed7bH90RMIfA.png"/></div></figure><figure class="ky ij lq la lb lc ld paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/0e6f21c9c833ed61fefa0c2d9be109ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*X3Jz4Xfr61ylC7Q2TcSPdA.png"/></div></figure></div><div class="ab cb"><figure class="ky ij lr la lb lc ld paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/a5009fb47e91020e2e855ab24c82a81d.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*QlMLpFbNUtys7NEiBp_AOg.png"/></div></figure><figure class="ky ij ls la lb lc ld paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><img src="../Images/d4a6873a0cb290d7c659ae5482bf85f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*9QUFrWht0BfZGHNyBrmE5A.png"/></div></figure></div><p id="a176" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">不同损失之间的变化是离群值相对于内值的权重——也称为<em class="jp">鲁棒性</em>。现在，如果你在此之前不知道德国-麦克卢尔损失是什么(我猜是除了德国人和麦克卢尔之外的所有人)，你可能已经被卡住了。即使你知道，尝试所有这些不同的损失函数，最终得到一个好的，仍然是相当乏味的。</p><p id="4b2e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Barron引入了一个新的损失函数，当给定不同的参数时，它实际上再现了上面显示的所有损失；也许更重要的是，他给出了一种在训练过程中自适应调整这些参数的方法，以自动确定这种损失函数对于特定任务的最佳鲁棒性。</p><p id="e831" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">损失函数</strong></p><p id="96f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我是巴伦，我会称之为巴伦损失，因为有一个以我命名的损失函数在我的遗愿清单上相当靠前；但巴伦没我自恋，没留名。不管怎样，这是他的损失函数:</p><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/556cdee3f5dfc04c88fa137bc53add60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*eZwEoUB8uG__mzQp1Gsuyg.png"/></div></figure><p id="9700" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">公式中，α是决定函数鲁棒性的形状参数，<em class="jp"> c </em> &gt; 0是决定<em class="jp"> x </em> = 0附近二次碗(U形)大小的比例参数。</p><p id="2196" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可能会注意到α = 0，2时的奇点。然而，在极限情况下，我们得到的损失实际上近似于α = 2附近的L₂损失，以及α = 0附近的柯西损失。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/aae900fe0aee6f0a10fb73ad9bd755d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*qGCXz0XWCsFXB4V2VZXNTw.png"/></div></figure><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/f65f39efe91f1847ec09187e7863c4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*svvBnkHqUG6GfulW7MTnQQ.png"/></div></figure><p id="7564" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">事实上，该损失函数的一个非常好的特性是，它实际上再现了特定α的几个现有损失函数:L2损失(α = 2)、夏邦尼尔损失(α = 1)、柯西损失(α = 0)、吉曼-麦克卢尔损失(α= 2)和韦尔施损失(α=∞)。这一点在他最终版本的损失(消除了奇点)中显而易见:</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/b47a4f2fcc696b73f7d76158b7550e67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rw8ZBLNYH_-MhtWWSQvNBA.png"/></div></div></figure><p id="6c51" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">论文中还有一个很好的图形来展示它的形状是如何变化的:</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/d4e88f493aa7239626c2c2b607454a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rgt6XDfD9qkfuMy8tWq5vg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">各种α的损失函数(左)及其梯度(右)。请注意，随着α的降低，异常值的梯度下降到0，从而增加了稳健性。</figcaption></figure><p id="7405" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个损失函数实际上有很多很好的特性:</p><ol class=""><li id="68d6" class="jx jy hi is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated">平滑w.r.t | <em class="jp"> x </em> |和α。因此，我们可以进行基于梯度的优化，老实说，这是我目前所知道的全部方法。</li><li id="753e" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">单调增加w.r.t. | <em class="jp"> x </em> |和α。您可能还注意到，随着α的降低，稳健性会增加(离群值的权重较小/对于较小的α具有较小的梯度)。这意味着你可以做一些渐进的非凸优化:设置α很大，所以损失是凸的，然后退火α，所以凸性降低但鲁棒性增加，并且(希望)不会陷入局部最小值。</li><li id="a728" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">有界一阶和二阶导数。这意味着我们可以在优化过程中避免令人不快的渐变爆炸。</li></ol><p id="8613" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">自适应算法</strong></p><p id="de01" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果这篇论文的高潮是我们需要对α和<em class="jp"> c. </em>进行超参数搜索，那么所有这些都是毫无用处的。真正酷的是，这些参数实际上是免费的，可以与模型权重的其余部分一起求解。现在，天真地最小化<em class="jp"> ρ </em>(，<em class="jp"> α </em>，<em class="jp"> c </em>)是行不通的，因为我们可以只选择<em class="jp"> α </em>，<em class="jp"> c </em>尽可能小，让损失变得微不足道(回想一下<em class="jp"> ρ </em>随<em class="jp"> α </em>单调增加)。</p><p id="be74" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">需要引入一些折衷，因此<em class="jp"> α </em>不能任意小。这种权衡被很好地打包到对应于损失函数的概率分布的负对数似然(NLL)中。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/060ee8249b5666380cb0d7aee45800ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fL1ldi3LxP3uXGcyM8bdiQ.png"/></div></div></figure><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/44430e57c1ec9cd2639004f803b520ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*lqorj88GC7ZQ5Zq8TZUcyg.png"/></div></figure><p id="f4cd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我已经看够了使用概率图形模型的工作，对<a class="ae jo" href="https://www.deeplearningbook.org/contents/partition.html" rel="noopener ugc nofollow" target="_blank">配分函数</a> <em class="jp"> Z </em> ( <em class="jp"> α </em>)有点害怕了。但是也许这次它很好，很容易驾驭…</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/f0176b30c14a3206e10654dc2eeab793.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XEh5-21gEsJgbix66Gl9cA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx">:(</figcaption></figure><p id="5749" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">幸运的是，Barrons找到了一个解决方法，用一个<a class="ae jo" href="https://en.wikipedia.org/wiki/Cubic_Hermite_spline" rel="noopener ugc nofollow" target="_blank">三次hermite样条</a>插值来逼近<em class="jp">log</em><em class="jp">Z</em>(<em class="jp">α</em>)。在实践中，他做了另一个使样条连续可微的技巧，通过在<em class="jp"> α </em>的变换空间中工作(见他的<a class="ae jo" href="http://openaccess.thecvf.com/content_CVPR_2019/supplemental/Barron_A_General_and_CVPR_2019_supplemental.pdf" rel="noopener ugc nofollow" target="_blank">补充材料</a>)。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/427f33dda9a66cf80b9f3e950ad674d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ug7wwhjvcb5FpgTx0C5IGw.png"/></div></div></figure><p id="6cfa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">增加<em class="jp">日志</em> <em class="jp"> Z </em> ( <em class="jp"> α </em>)对新目标有什么作用？嗯，它有效地转移了原始损失函数，因此对于小的<em class="jp"> α，</em>来说，离群值的成本较低，而内联值的成本会增加。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/b8f42ef926c6675780d2ad7c9c2c88dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*JrIJKSEuPAQT54iU0eZd5w.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">NLLs(左)和PDF(右)。请注意，对于较低的α，内联体的成本较高，这阻止了任意降低α。</figcaption></figure><p id="af53" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这就产生了一种折衷，在这种情况下，将低成本分配给较大的残差会导致较小残差的惩罚，从而阻止优化像以前一样微不足道地减少损失。现在，<em class="jp"> α </em>，<em class="jp"> c </em>可以作为自由参数与其余模型权重一起优化！</p><p id="7088" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">实验</strong></p><p id="3562" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Barron表明，在各种任务中，只需用新的自适应损失函数替换原始模型的损失函数，就能显著提高性能。因为这是CVPR(还没有提到图像生成或深度估计)，两个这样的任务是(1)使用<a class="ae jo" href="https://arxiv.org/pdf/1312.6114.pdf" rel="noopener ugc nofollow" target="_blank">变分自动编码器</a> (VAEs)的图像生成，和(2)单目深度估计。</p><p id="80b2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用VAEs，Barrons将为输出图像中的每个RGB像素生成正态分布与使用<em class="jp"> α </em>、<em class="jp"> c </em>作为可训练参数生成新分布进行了比较。在数量上，验证集ELBOs也有所提高，从31，505提高到36，373(用小波代替像素)。此外，使用VAEs的样本是出了名的<a class="ae jo" href="https://arxiv.org/pdf/1606.04934.pdf" rel="noopener ugc nofollow" target="_blank"/><a class="ae jo" href="https://arxiv.org/pdf/1702.08658.pdf" rel="noopener ugc nofollow" target="_blank"/>模糊，但是使用新的输出分布会产生更清晰的图像。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/2472effc4c07cac3a8b8feb4f0af1f0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WiM9IMyglj8oBiYb2QQieg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">使用新的分布会产生更清晰的样本。</figcaption></figure><p id="1ee4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/cvpr17_sfm_final.pdf" rel="noopener ugc nofollow" target="_blank">无监督单目深度估计</a>使用成对的相关的、未标记的图像作为训练数据来估计深度和相机姿态。在该任务中，使用固定的拉普拉斯分布来最大化像素的可能性的原始目标被使用新的分布所取代。这样做将验证误差从0.407减小到0.332(也使用小波；我承认我不知道它们有那么有用)，并且做出明显更好看的深度估计。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/a58e16ea83894fc78e573248be627527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUPS7KCnK7zk0XXufYnUhg.png"/></div></div></figure><p id="8e31" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我非常积极地浏览了实验，但我想专注于理论上有趣的部分，而不是计算机视觉应用(我希望我不会因为这个评论被列入未来CVPRs的黑名单)。</p><p id="a811" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">讨论</strong></p><p id="8d33" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我认为巴伦的作品让人想起亚历克斯·肯德尔(who(m？)我也是的粉丝！该工作处理在多任务损失设置中自适应地缩放单个任务损失，而该工作松散地缩放单个数据的损失，以它们的残差为条件。它们都符合处理<em class="jp">随机不确定性— </em>由于训练数据中隐藏的解释变量导致的统计不确定性这一总体主题。</p><p id="9f39" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这项工作的一个限制是它只适用于回归任务(如果这也算一个限制的话，因为<em class="jp">很多</em>任务都涉及回归)。对于分类任务，可能存在相应的“广义”交叉熵或铰链损失，但是使这样的损失平滑可能是一个挑战。</p><p id="536d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">尽管如此，巴伦的作品还是让我非常兴奋；这绝对是我最近看到的最令人印象深刻的事情之一，我看到了这个:</p><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es me"><img src="../Images/bb8e201cda6b16c73783f4dba951b2e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/1*4sIf82vynFm6Stft9n2z0w.gif"/></div></figure><p id="6800" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">无论如何，在今年的CVPR上还有很多其他伟大的工作，包括一篇“<a class="ae jo" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Xin_A_Theory_of_Fermat_Paths_for_Non-Line-Of-Sight_Shape_Reconstruction_CVPR_2019_paper.html" rel="noopener ugc nofollow" target="_blank">最佳论文”，我仍在努力通过</a>。对我来说，这是一个非常令人兴奋的时刻，我终于用自己的研究努力取代了学校考试！</p></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><p id="f8b4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">参考文献</strong></p><ol class=""><li id="b27a" class="jx jy hi is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated">曾、、西蒙·索、阿巴斯·萨达特、、塞尔吉奥·卡萨斯、拉克尔·乌尔塔松。<em class="jp">端到端可解释的神经运动规划器。</em>IEEE计算机视觉和模式识别大会(CVPR)，2019，第8660–8669页。</li><li id="d628" class="jx jy hi is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">乔纳森·t·巴伦。<strong class="is hj"> <em class="jp"> </em> </strong> <em class="jp">一种通用的、自适应的鲁棒损失函数。</em>IEEE计算机视觉和模式识别大会(CVPR)，2019，第4331–4339页。</li></ol></div></div>    
</body>
</html>