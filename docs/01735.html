<html>
<head>
<title>Use torchtext and Transformer to create your quote language model step by step !</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用torchtext和Transformer逐步创建您的报价语言模型！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/use-torchtext-and-transformer-to-create-your-quote-language-model-step-by-step-95ffc0192e12?source=collection_archive---------14-----------------------#2019-11-11">https://medium.com/analytics-vidhya/use-torchtext-and-transformer-to-create-your-quote-language-model-step-by-step-95ffc0192e12?source=collection_archive---------14-----------------------#2019-11-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/eb752b6f5f2da36008f0aae7d99c70ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NjQgZIXiRUrvWHIJ"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">由<a class="ae iu" href="https://unsplash.com/@fossy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Fab Lentz </a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="1023" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你对深度学习，尤其是自然语言处理感兴趣，你可能知道文本数据的问题。我指的是你在创建模型之前就应该努力解决的问题。这些问题比如创建词汇，字符串的数值化，填充，批处理等等！有许多库可以帮助你处理这些困难的步骤，但是由于某些原因，没有一个能让我满意。一个最重要的原因是，我觉得它们并不完全兼容深度学习框架，比如pytorch。但是现在，多亏了pytorch，我们有了一个伟大的助手来解决我们的文本数据问题，是的<strong class="ix hj"> TorchText </strong>！！</p><p id="fadd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇文章中，我想向你展示如何使用torchtext轻松处理你的虚拟数据。此外，我将使用nn.transformer(最近已添加到nn中)来设计我的模型，以使transformer的优势受益。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="84e2" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">我们的任务</h1><p id="c2ca" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">我们有一个简单的txt文件，其中包含一些报价，我们想训练一个语言模型，可以生成报价！！你可以从<a class="ae iu" href="https://github.com/mmsamiei/just-practice-deep/blob/master/lava-language-model/text.txt" rel="noopener ugc nofollow" target="_blank">这里</a>下载一个示例txt文件！</p><h1 id="f9b9" class="ka kb hi bd kc kd ld kf kg kh le kj kk kl lf kn ko kp lg kr ks kt lh kv kw kx bi translated">我们模型的结构</h1><p id="7c17" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">我将使用一个变压器解码器来学习语言模型。记住，在这个任务中我们不需要变压器编码器。(你会看到的)</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="5016" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">履行</h1><p id="9dff" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">好的，让我们一步一步来！！</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="c42a" class="lr kb hi ln b fi ls lt l lu lv">import torch</span><span id="bbb3" class="lr kb hi ln b fi lw lt l lu lv">import torch.nn as nn</span><span id="4c22" class="lr kb hi ln b fi lw lt l lu lv">import numpy as np</span><span id="947d" class="lr kb hi ln b fi lw lt l lu lv">device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')</span></pre><p id="dc10" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如此简单！在本节中，我们刚刚导入并设置了我们的设备！</p><p id="c1a4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">好了，现在让我们开始研究我们的模型:</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="cefd" class="lr kb hi ln b fi ls lt l lu lv">class LM(nn.Module):<br/>  <br/>  def __init__(self, hid_size, vocab_size, n_head, n_layers, max_len, device):<br/>    super().__init__()<br/>    self.device = device<br/>    self.hid_size = hid_size<br/>    self.max_len = max_len<br/>    self.embedding = nn.Embedding(vocab_size, hid_size)<br/>    self.position_enc = nn.Embedding(self.max_len, self.hid_size)<br/>    self.position_enc.weight.data =self.position_encoding_init(self.max_len, self.hid_size)<br/>    self.scale = torch.sqrt(torch.FloatTensor([self.hid_size])).to(device)<br/>    self.layer_norm = nn.LayerNorm(self.hid_size)<br/>    self.decoder_layer = nn.TransformerDecoderLayer(d_model=hid_size, nhead = n_head)<br/>    self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=n_layers, norm=self.layer_norm)<br/>    self.fc = nn.Linear(hid_size, vocab_size)<br/>    self._init_weights()<br/>  <br/>  def forward(self, x):<br/>    sent_len, batch_size = x.shape[0], x.shape[1]<br/>    memory_mask = self.generate_complete_mask(sent_len)<br/>    tgt_mask = self.generate_triangular_mask(sent_len)<br/>    memory = torch.zeros(1, batch_size, self.hid_size, device=self.device)<br/>    temp = x<br/>    temp = self.embedding(temp)<br/>    pos = torch.arange(0,sent_len).unsqueeze(1).repeat(1,batch_size).to(self.device)<br/>    temp_pos_emb = self.position_enc(pos)<br/>    temp = temp * self.scale + temp_pos_emb<br/>    temp = self.decoder(temp, memory, tgt_mask=tgt_mask)<br/>    temp = self.fc(temp)<br/>    return temp<br/>def _init_weights(self):<br/>    for p in self.parameters():<br/>      if p.dim() &gt; 1:<br/>        nn.init.xavier_uniform_(p)<br/>def generate_triangular_mask(self, size):<br/>        r"""Generate a square mask for the sequence. The masked positions are filled with float('-inf').<br/>            Unmasked positions are filled with float(0.0).<br/>        """<br/>        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)<br/>        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)<br/>        return mask<br/>def generate_sequence(self, src):<br/>    #src = [sent_len]<br/>    src = src.unsqueeze(1)<br/>    #src = [sent_len, 1]<br/>    generate_step = 0<br/>    while generate_step &lt; 20:<br/>      out = self.forward(src)<br/>      #out = [sent_len + 1, 1, vocab_size]<br/>      out = torch.argmax(out[-1, :], dim=1) # [1]<br/>      out = out.unsqueeze(0) #[1,1]<br/>      src = torch.cat((src, out), dim=0)<br/>      generate_step += 1<br/>    src = src.squeeze(1)<br/>    return src<br/>  <br/>def position_encoding_init(self, n_position, d_pos_vec):<br/>    ''' Init the sinusoid position encoding table '''<br/>    # keep dim 0 for padding token position encoding zero vector<br/>    position_enc = np.array([<br/>        [pos / np.power(10000, 2*i/d_pos_vec) for i in range(d_pos_vec)]<br/>        if pos != 0 else np.zeros(d_pos_vec) for pos in  range(n_position)])<br/>    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i<br/>    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1<br/>    temp = torch.from_numpy(position_enc).type(torch.FloatTensor)<br/>    temp = temp.to(self.device)<br/>    return temp</span></pre><p id="1c2f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这么长但不那么难！我们一起来考察一下吧！</p><p id="7974" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们已经定义了我们模型<em class="lx"> LM </em>，它本身就有变压器。感谢pytorch，现在我们可以使用预构建的transformer模块，但不幸的是pytorch还没有实现位置嵌入，所以我们必须自己实现。那是关于<em class="lx"> __init__ </em>方法的，但是在forward中发生了什么呢？</p><p id="c9ed" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在forward中，首先我们得到字符串的张量——然后我们通过“注意力是你所需要的全部”一文中介绍的公式对它应用单词和位置嵌入。然后，我们将嵌入序列提供给转换器(在这一步，我们使用<em class="lx"> position_encoding_init </em>创建正弦位置嵌入)。</p><p id="03d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为解码器转换器需要由编码器转换器产生的内存，而我们这里没有任何编码器，所以我们将它的内存设置为零！！加油！别担心，这不会毁了我们的模型。问为什么？因为解码器变换器本身有残差，如果你关注零向量，你得到零向量，因为你有残差连接，零与你先前的结果相加，所以你还会有先前的结果！</p><p id="24c9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">之后，我们需要生成三角形掩码，以使一个vocab的隐藏状态只关注它的左上下文。我们使用<em class="lx"> generate_triangular_mask </em>制作这个遮罩。最后，我们实现了<em class="lx"> generate_sequence方法</em>来完成我们的输入句子。现在让我们带着<strong class="ix hj">火炬文本</strong>继续与您见面！</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="f1ca" class="lr kb hi ln b fi ls lt l lu lv">import torchtext<br/>from torchtext import data<br/>import spacy</span><span id="8e21" class="lr kb hi ln b fi lw lt l lu lv">my_tok = spacy.load('en')<br/>def spacy_tok(x):<br/>    return [tok.text for tok in my_tok.tokenizer(x)]<br/> <br/>TEXT = data.Field(lower=True, tokenize=spacy_tok)</span></pre><p id="5205" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这段代码中，我首先使用spacy tokenizer实现了一个tokenizer(我在这里的工作类似于一个包装器！)，你可以看到<em class="lx"> spacy_tok </em>是一个可以标记字符串的方法。重要的是文本，它是一个字段。Field是torchtext的一个类，通过它你告诉torchtext如何查看你的原始数据。例如，我已经告诉torchtext，我有一个字段文本，必须使用spacy_tok方法将其原始数据标记化。</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="3063" class="lr kb hi ln b fi ls lt l lu lv">from torchtext.datasets import LanguageModelingDataset</span><span id="fa0a" class="lr kb hi ln b fi lw lt l lu lv">my_dataset = LanguageModelingDataset("./text.txt", TEXT)</span></pre><p id="5a5b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后我用<strong class="ix hj"><em class="lx">LanguageModelingDataset</em></strong>导入我的txt文件，同时在文本视图中查看它！(pytorch  中有类似于<strong class="ix hj"><em class="lx">LanguageModelingDataset的类，以便为许多任务加载数据集，例如<strong class="ix hj"><em class="lx">tabular dataset</em></strong>、<strong class="ix hj"><em class="lx">translation dataset</em></strong>等等)</em></strong></p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="e2d9" class="lr kb hi ln b fi ls lt l lu lv">TEXT.build_vocab(my_dataset)</span></pre><p id="e755" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们已经像你看到的那样简单地创造了我们的词汇！</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="c2eb" class="lr kb hi ln b fi ls lt l lu lv">def make_train_iter(batch_size, bptt_len):<br/>  train_iter = data.BPTTIterator(<br/>    my_dataset,<br/>    batch_size=batch_size,<br/>    bptt_len=bptt_len, # this is where we specify the sequence length<br/>    device=device,<br/>    repeat=False,<br/>    shuffle=True)<br/>  print(len(train_iter))<br/>  return train_iter</span></pre><p id="8a38" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们创建一个方法，它给我们一个数据迭代器！我们通过包装<strong class="ix hj"> <em class="lx">数据来实现。BPTTIterator </em> </strong>！太神奇了！Pytorch有许多像BPTTIterator这样的迭代器，它们通过提供批量和处理过的数据来帮助你。BPTTIterator是专门用于语言建模的。它有一个名为bptt_len字段，对它起关键作用。现在让我们假设你有一篇这样的文章“权力越大，责任越大！”并将其传递给bptt，bptt_len = 3，然后它在迭代器中给出这个src和target对:</p><p id="ad67" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">权力大-&gt;权力大责任大<br/>-&gt;责任大！</p><p id="e9c9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这正是我们想要的语言模型训练。您也可以尝试其他torchtext迭代器。</p><p id="953f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们创建模型并计算其参数:</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="461a" class="lr kb hi ln b fi ls lt l lu lv">vocab_size = len(TEXT.vocab)<br/>hid_size = 16<br/>pf_size = 32<br/>n_head = 4<br/>n_layer= 1<br/>model = LM(hid_size, vocab_size, n_head, n_layer, pf_size, device).to(device)</span><span id="b42d" class="lr kb hi ln b fi lw lt l lu lv">def count_parameters(model):<br/>    return sum(p.numel() for p in model.parameters() if p.requires_grad)</span><span id="3d58" class="lr kb hi ln b fi lw lt l lu lv">print(f'The model has {count_parameters(model):,} trainable parameters')</span></pre><p id="aaf1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">之后，我们实现了NoamOpt，它是一个用于优化transformer中参数的优化器。前面已经介绍过<a class="ae iu" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lx">注意是你所需要的</em> </a>)</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="c702" class="lr kb hi ln b fi ls lt l lu lv">class NoamOpt:<br/>    "Optim wrapper that implements rate."<br/>    def __init__(self, model_size, factor, warmup, optimizer):<br/>        self.optimizer = optimizer<br/>        self._step = 0<br/>        self.warmup = warmup<br/>        self.factor = factor<br/>        self.model_size = model_size<br/>        self._rate = 0<br/>        <br/>    def step(self):<br/>        "Update parameters and rate"<br/>        self._step += 1<br/>        rate = self.rate()<br/>        for p in self.optimizer.param_groups:<br/>            p['lr'] = rate<br/>        self._rate = rate<br/>        self.optimizer.step()<br/>        <br/>    def rate(self, step = None):<br/>        "Implement `lrate` above"<br/>        if step is None:<br/>            step = self._step<br/>        return self.factor * \<br/>            (self.model_size ** (-0.5) *<br/>            min(step ** (-0.5), step * self.warmup ** (-1.5)))<br/>    <br/>    def zero_grad(self):<br/>        self.optimizer.zero_grad()</span></pre><p id="5c65" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">之后，我们实现了train_one_epoch和train方法</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="8ab3" class="lr kb hi ln b fi ls lt l lu lv">def train_one_epoch(model,train_iter, optimizer, criterion, clip):<br/>  epoch_loss = 0<br/>  model.train()<br/>  for batch in train_iter:<br/>    optimizer.zero_grad()<br/>    batch_text = batch.text<br/>    batch_target = batch.target<br/>    result = model(batch_text)<br/>    loss = criterion(result.view(-1, result.shape[-1]), batch_target.view(-1))<br/>    loss.backward()<br/>    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)<br/>    optimizer.step()<br/>    epoch_loss += loss.item()<br/>    return epoch_loss / len(train_iter)<br/>    print("epoch is {} loss is {}".format(epoch, epoch_loss / len(train_iter)))</span><span id="ca3b" class="lr kb hi ln b fi lw lt l lu lv">def train(model, train_iter, optimizer, criterion, clip, N_EPOCH):<br/>  for epoch in range(N_EPOCH):<br/>    epoch_loss = train_one_epoch(model, train_iter, optimizer, criterion, clip)<br/>    print("epoch is {} loss is {}".format(epoch, epoch_loss))</span></pre><p id="0689" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们训练…</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="9d07" class="lr kb hi ln b fi ls lt l lu lv">for i in range(1, 3):</span><span id="2ca3" class="lr kb hi ln b fi lw lt l lu lv">optimizer = NoamOpt(hid_size, 1, 2000,</span><span id="74c5" class="lr kb hi ln b fi lw lt l lu lv">torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))</span><span id="fce3" class="lr kb hi ln b fi lw lt l lu lv">criterion = torch.nn.CrossEntropyLoss()</span><span id="9f77" class="lr kb hi ln b fi lw lt l lu lv">train_iter = make_train_iter(4096, i)</span><span id="0186" class="lr kb hi ln b fi lw lt l lu lv">train(model, train_iter, optimizer, criterion, 1, 3)</span></pre><p id="c641" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，如果您想测试您的模型并生成报价，您可以使用下面的代码(编写一个表达式，以便模型完成它！)</p><pre class="li lj lk ll fd lm ln lo lp aw lq bi"><span id="77ab" class="lr kb hi ln b fi ls lt l lu lv">source_sentence = ["your","expression","word","by","word"] ## you must write its word in an array, because i'm so tired to complete it :)))<br/>print(source_sentence)<br/>model.eval()<br/>print(' '.join(source_sentence))<br/>print()<br/>x = TEXT.numericalize([source_sentence]).to(device).squeeze(1)<br/>generated_sequence =model.generate_sequence(x)<br/>words = [TEXT.vocab.itos[word_idx] for word_idx in generated_sequence]<br/>print(' '.join(words))</span></pre><p id="3e15" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以在<a class="ae iu" href="https://github.com/mmsamiei/just-practice-deep/blob/master/lava-language-model/lava.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>看到完整代码的笔记本</p><p id="51d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">这是我最后的一幕！我希望尽快给你写信！再见！</strong></p></div></div>    
</body>
</html>