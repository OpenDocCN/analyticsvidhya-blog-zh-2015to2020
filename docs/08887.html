<html>
<head>
<title>Top 4 Pre-Trained Models for Image Classification with Python Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python代码进行影像分类的前4个预训练模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/top-4-pre-trained-models-for-image-classification-with-python-code-a3cb5846248b?source=collection_archive---------17-----------------------#2020-08-17">https://medium.com/analytics-vidhya/top-4-pre-trained-models-for-image-classification-with-python-code-a3cb5846248b?source=collection_archive---------17-----------------------#2020-08-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="8ab9" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="e4ad" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">人脑可以很容易地识别和区分图像中的对象。例如，给定一只猫和一只狗的图像，在十亿分之一秒内，我们区分两者，我们的大脑感知这种差异。万一有机器模仿这种行为，那就是我们能得到的最接近人工智能的了。随后，计算机视觉领域的目标是模仿人类视觉系统——已经有许多里程碑突破了这方面的障碍。</p><p id="5cf8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">而且，现在的机器可以轻松区分不同的图像，检测物体和人脸，甚至生成不存在的人的图像！很迷人，不是吗？当我开始使用计算机视觉时，我的第一个经验是图像分类的任务。机器区分物体的这种能力带来了更多的研究途径——比如区分人。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/045dc4f921a4cde78606e2f632875756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UvHtpHa1qFWDORsj.jpg"/></div></div></figure><p id="5b94" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">计算机视觉的快速发展，以及由此延伸的图像分类，由于迁移学习的出现而进一步加速。简而言之，迁移学习允许我们使用一个预先存在的模型，在一个巨大的数据集上训练，用于我们自己的任务。因此降低了训练新的深度学习模型的成本，并且由于数据集已经过审查，我们可以确保质量。</p><p id="c3a9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在图像分类中，有一些非常受欢迎的数据集，它们被用于研究、行业和黑客马拉松。以下是一些突出的例子:</p><ul class=""><li id="0729" class="ks kt hi jf b jg kb jk kc jo ku js kv jw kw ka kx ky kz la bi translated"><a class="ae lb" href="http://image-net.org/download" rel="noopener ugc nofollow" target="_blank"> ImageNet </a></li><li id="805c" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated"><a class="ae lb" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">cifa</a></li><li id="bcef" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated"><a class="ae lb" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a></li></ul><p id="bde0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">还有很多。</p><p id="d45f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在本文中，我将介绍用于图像分类的前4个预训练模型，它们是最先进的(SOTA ),在行业中也广泛使用。可以更详细地解释各个模型，但是我将这篇文章限制为概述它们的架构并在数据集上实现它。</p><p id="c8d1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="lh">如果你想以结构化的形式学习计算机视觉，参考本课程- </em> <a class="ae lb" href="https://courses.analyticsvidhya.com/bundles/certified-computer-vision-masters-program?utm_source=blog&amp;utm_medium=top4_pre-trained_image_classification_models" rel="noopener ugc nofollow" target="_blank"> <em class="lh">认证计算机视觉硕士项目</em> </a></p><h1 id="2cb5" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">设置系统</h1><p id="01f2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">既然我们从猫和狗开始，那就让我们来看看猫和狗的图像数据集。Kaggle上的原始训练数据集有25000张猫和狗的图像，测试数据集有10000张未标记的图像。因为我们的目的只是为了理解这些模型，所以我选择了一个更小的数据集。您可以直接在Google Colab上运行这个和其余的代码——所以让我们开始吧！</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="7382" class="ln ig hi lj b fi lo lp l lq lr">!wget --no-check-certificate \<br/>    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \<br/>    -O /tmp/cats_and_dogs_filtered.zip</span></pre><p id="600e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们也导入基本库。此外，我将根据型号介绍未来的导入:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="a40d" class="ln ig hi lj b fi lo lp l lq lr">import os <br/>import zipfile <br/>import tensorflow as tf <br/>from tensorflow.keras.preprocessing.image import ImageDataGenerator <br/>from tensorflow.keras import layers <br/>from tensorflow.keras import Model <br/>import matplotlib.pyplot as plt</span></pre><h1 id="70cf" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">准备数据集</h1><p id="4ca8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将首先准备数据集并分离出图像:</p><ol class=""><li id="2bfc" class="ks kt hi jf b jg kb jk kc jo ku js kv jw kw ka ls ky kz la bi translated">我们首先将文件夹内容分为训练和验证目录。</li><li id="9a5e" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka ls ky kz la bi translated">然后，在每个目录中，为只包含猫图像的猫创建一个单独的目录，为只包含狗图像的狗创建一个单独的目录。</li></ol><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="3903" class="ln ig hi lj b fi lo lp l lq lr">local_zip = '/tmp/cats_and_dogs_filtered.zip'<br/>zip_ref = zipfile.ZipFile(local_zip, 'r')<br/>zip_ref.extractall('/tmp')<br/>zip_ref.close()<br/><br/>base_dir = '/tmp/cats_and_dogs_filtered'<br/>train_dir = os.path.join(base_dir, 'train')<br/>validation_dir = os.path.join(base_dir, 'validation')<br/><br/># Directory with our training cat pictures<br/>train_cats_dir = os.path.join(train_dir, 'cats')<br/><br/># Directory with our training dog pictures<br/>train_dogs_dir = os.path.join(train_dir, 'dogs')<br/><br/># Directory with our validation cat pictures<br/>validation_cats_dir = os.path.join(validation_dir, 'cats')<br/><br/># Directory with our validation dog pictures<br/>validation_dogs_dir = os.path.join(validation_dir, 'dogs')</span></pre><p id="0f72" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以下代码将让我们检查图像是否已正确加载:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="46c7" class="ln ig hi lj b fi lo lp l lq lr"># Set up matplotlib fig, and size it to fit 4x4 pics<br/>import matplotlib.image as mpimg<br/>nrows = 4<br/>ncols = 4<br/><br/>fig = plt.gcf()<br/>fig.set_size_inches(ncols*4, nrows*4)<br/>pic_index = 100<br/>train_cat_fnames = os.listdir( train_cats_dir )<br/>train_dog_fnames = os.listdir( train_dogs_dir )<br/><br/><br/>next_cat_pix = [os.path.join(train_cats_dir, fname) <br/>                for fname in train_cat_fnames[ pic_index-8:pic_index] <br/>               ]<br/><br/>next_dog_pix = [os.path.join(train_dogs_dir, fname) <br/>                for fname in train_dog_fnames[ pic_index-8:pic_index]<br/>               ]<br/><br/>for i, img_path in enumerate(next_cat_pix+next_dog_pix):<br/>  # Set up subplot; subplot indices start at 1<br/>  sp = plt.subplot(nrows, ncols, i + 1)<br/>  sp.axis('Off') # Don't show axes (or gridlines)<br/><br/>  img = mpimg.imread(img_path)<br/>  plt.imshow(img)<br/><br/>plt.show()</span></pre><p id="cf8e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在我们已经准备好了数据集，让我们进入模型构建阶段。我们将在该数据集上使用4种不同的预训练模型。</p><h1 id="f6d8" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">用于图像分类的预训练模型</h1><p id="72a8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在本节中，我们将介绍以下4种用于图像分类的预训练模型-</p><h2 id="bcf9" class="ln ig hi bd ih lt lu lv il lw lx ly ip jo lz ma it js mb mc ix jw md me jb mf bi translated">1.用于大规模图像识别的甚深卷积网络(VGG-16)</h2><p id="5818" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">VGG-16是用于图像分类的最流行的预训练模型之一。它是在著名的ILSVRC 2014大会上推出的，即使在今天，它仍然是一个值得超越的模型。VGG-16由牛津大学视觉图形小组开发，击败了当时的AlexNet标准，并很快被研究人员和业界用于图像分类任务。</p><p id="9dbb" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这是VGG 16号的建筑:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mg"><img src="../Images/7fa76b2b4d081ababfbc701890cb3c9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2Si3eHYCWhCpaxDX.jpg"/></div></div></figure><p id="e4cd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里有一个更直观的VGG-16模型的布局。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mh"><img src="../Images/d6d8bc36d0b22484f5fe7964364bd290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*eIvnY0Suz14Vr83Q.png"/></div></div></figure><p id="a65f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以下是模型的各层:</p><ul class=""><li id="2aa7" class="ks kt hi jf b jg kb jk kc jo ku js kv jw kw ka kx ky kz la bi translated">卷积层数= 13</li><li id="c4a1" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">池层= 5</li><li id="5e8a" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">密集层= 3</li></ul><p id="7766" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们详细探究这些层:</p><p id="c303" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 1 —输入</strong>:尺寸图像(224，224，3)。</p><p id="247e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 2 —卷积层Conv1: </strong></p><ul class=""><li id="b67a" class="ks kt hi jf b jg kb jk kc jo ku js kv jw kw ka kx ky kz la bi translated">con v1–1:64滤波器</li><li id="65a2" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con v1–2:64滤波器和最大池</li><li id="1918" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">图像尺寸:(224，224)</li></ul><p id="5369" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 3 —卷积层Conv2: </strong>现在，我们将滤波器增加到128个</p><ul class=""><li id="517a" class="ks kt hi jf b jg kb jk kc jo ku js kv jw kw ka kx ky kz la bi translated">输入图像尺寸:(112，112)</li><li id="a9d1" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con v2–1:128滤波器</li><li id="7b24" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con v2–2:128个滤波器和最大池</li></ul><p id="1ac9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 4 —卷积层Conv3: </strong>再次，将滤镜加倍至256，现在添加另一个卷积层</p><ul class=""><li id="3667" class="ks kt hi jf b jg kb jk kc jo ku js kv jw kw ka kx ky kz la bi translated">输入图像尺寸:(56，56)</li><li id="6040" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con v3–1:256滤波器</li><li id="3ab2" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con v3–2:256滤波器</li><li id="85ee" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con v3–3:256个滤波器和最大池</li></ul><p id="10e9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 5 —卷积层Conv4: </strong>与Conv3相似，但现在有512个滤波器</p><ul class=""><li id="ad4c" class="ks kt hi jf b jg kb jk kc jo ku js kv jw kw ka kx ky kz la bi translated">输入图像尺寸:(28，28)</li><li id="8057" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con v4–1:512滤波器</li><li id="26ed" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con v4–2:512滤波器</li><li id="51d4" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con v4–3:512滤波器和最大池</li></ul><p id="5929" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 6 —卷积层Conv5: </strong>与Conv4相同</p><ul class=""><li id="143b" class="ks kt hi jf b jg kb jk kc jo ku js kv jw kw ka kx ky kz la bi translated">输入图像尺寸:(14，14)</li><li id="cc04" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con V5–1:512滤波器</li><li id="0111" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con V5–2:512滤波器</li><li id="6d0e" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">con V5–3:512滤波器和最大池</li><li id="8310" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">这里的输出维数是(7，7)。此时，我们展平该层的输出以生成特征向量</li></ul><p id="b008" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 7 —全连接/密集FC1 </strong> : 4096个节点，生成大小为(1，4096)的特征向量</p><p id="526b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 8 —全连接密度FC2 </strong> : 4096个节点生成大小为(14096)的特征向量</p><p id="777a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 9 —全连接/密集FC3 </strong> : 4096个节点，为1000类生成1000个通道。这然后被传递到Softmax激活功能</p><p id="485a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 10 —输出层</strong></p><p id="cda0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">如您所见，该模型本质上是顺序的，并且使用了许多过滤器。在每个阶段，小的3 * 3过滤器被用来减少所有隐藏层使用ReLU激活函数的参数数量。即使这样，参数的数量也是1380亿个——这使得它比其他模型训练起来更慢、更大。</p><p id="cbe1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">此外，还有VGG16模型的变体，基本上是对它的改进，如VGG19 (19层)。你可以找到详细的解释</p><p id="9dc2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在让我们探索如何在我们的数据集上训练VGG-16模型</p><p id="ef0b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">步骤1:图像增强</strong></p><p id="7923" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">因为我们之前使用了一个小得多的图像数据集，所以我们可以通过增加这个数据和数据集大小来弥补它。如果您正在处理原始的较大数据集，则可以跳过这一步，直接开始构建模型。</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="0eed" class="ln ig hi lj b fi lo lp l lq lr"># Add our data-augmentation parameters to ImageDataGenerator</span><span id="4504" class="ln ig hi lj b fi mi lp l lq lr">train_datagen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)<br/><br/># Note that the validation data should not be augmented!<br/>test_datagen = ImageDataGenerator( rescale = 1.0/255. )</span></pre><p id="54f2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">第二步:训练和验证集</strong></p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="cf8f" class="ln ig hi lj b fi lo lp l lq lr"># Flow training images in batches of 20 using train_datagen generator<br/>train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))<br/><br/># Flow validation images in batches of 20 using test_datagen generator<br/>validation_generator = test_datagen.flow_from_directory( validation_dir,  batch_size = 20, class_mode = 'binary', target_size = (224, 224))</span></pre><p id="735e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">步骤3:加载基础模型</strong></p><p id="99a2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们将只使用基本模型，只对最后一层进行修改。这是因为这只是一个二元分类问题，而这些模型是为了处理多达1000个类别而构建的。</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="ae6f" class="ln ig hi lj b fi lo lp l lq lr">from tensorflow.keras.applications.vgg16 import VGG16<br/><br/>base_model = VGG16(input_shape = (224, 224, 3), # Shape of our images<br/>include_top = False, # Leave out the last fully connected layer<br/>weights = 'imagenet')</span></pre><p id="643f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">因为我们不需要训练所有的层，我们使它们不可训练:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="09ba" class="ln ig hi lj b fi lo lp l lq lr">for layer in base_model.layers:<br/>    layer.trainable = False</span></pre><p id="6cf6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">第四步:编译并拟合</strong></p><p id="8c1c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">然后，我们将建立最后一个全连接层。我刚刚使用了基本的设置，但是可以自由地尝试不同的dropout值，以及不同的优化器和激活函数。</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="fe62" class="ln ig hi lj b fi lo lp l lq lr"># Flatten the output layer to 1 dimension<br/>x = layers.Flatten()(base_model.output)<br/><br/># Add a fully connected layer with 512 hidden units and ReLU activation<br/>x = layers.Dense(512, activation='relu')(x)<br/><br/># Add a dropout rate of 0.5<br/>x = layers.Dropout(0.5)(x)<br/><br/># Add a final sigmoid layer for classification<br/>x = layers.Dense(1, activation='sigmoid')(x)<br/><br/>model = tf.keras.models.Model(base_model.input, x)<br/><br/>model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001), loss = 'binary_crossentropy',metrics = ['acc'])</span></pre><p id="ae8b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在，我们将根据之前创建的训练集和验证集来构建最终模型。请注意使用原始目录本身，而不是我下面使用的扩充数据集。我只使用了10个纪元，但是您也可以增加它们以获得更好的结果:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="58be" class="ln ig hi lj b fi lo lp l lq lr">vgghist = model.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mj"><img src="../Images/2c04e6739412a95726e244bcbccc8c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sXfBVDAeueI89M-yugDEXQ.png"/></div></div></figure><p id="67c9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">厉害！如您所见，我们仅用10个时期就实现了93%的验证准确性，并且没有对模型进行任何重大更改。这就是我们意识到迁移学习是多么强大，以及预先训练的图像分类模型是多么有用的地方。这里需要注意的是，与其他模型相比，VGG16需要花费很长时间来训练，当我们处理大型数据集时，这可能是一个缺点。</p><p id="027d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">话虽如此，我真的很喜欢这个模型的简单和直观。接受过ImageNet语料库培训的VGG 16的另一项显著成就是，它在ImageNet ILSVRC-2014中获得了第一名，从而巩固了其在图像分类顶级预培训模型列表中的地位。</p><h1 id="d290" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">2.开始</h1><p id="c99b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在为本文进行研究时，有一点是清楚的。就开发真正受欢迎的图像分类预训练模型而言，2014年具有标志性。虽然上述VGG-16在当年的ILSVRC中获得了第二名，但第一名不是别人，正是谷歌——通过它的模型谷歌浏览器(GoogLeNet)或“盗梦空间”(Inception，现在被称为“盗梦空间”)。</p><p id="4ab6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">原论文提出了Inceptionv1模型。它只有700万个参数，比当时流行的VGG和AlexNet模型要小得多。加上较低的错误率，你可以明白为什么它是一个突破性的模型。不仅如此，本文的主要创新也是另一个突破——盗梦空间模块。</p><p id="a784" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">正如可以看到的，简单地说，初始模块只对输入执行具有不同滤波器大小的卷积，执行最大池，并连接下一个初始模块的结果。1 * 1卷积运算的引入极大地减少了参数。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mk"><img src="../Images/409c208e2653a503bdae0985453544eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LLq33roQ395gfpMRjjZUjQ.png"/></div></div><figcaption class="ml mm et er es mn mo bd b be z dx translated">来源:纸张</figcaption></figure><p id="90ed" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">虽然Inceptionv1中的层数是22层，但是参数的大量减少使得它成为一个难以击败的强大模型。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mp"><img src="../Images/19d33e808ec5f087b6e62cb079ba3d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RaeKm6nwYhjQRLY5.png"/></div></div><figcaption class="ml mm et er es mn mo bd b be z dx translated">来源:纸张</figcaption></figure><p id="d415" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">Inceptionv2模型是对Inceptionv1模型的重大改进，它提高了精度，并进一步降低了模型的复杂性。在与Inceptionv2相同的文章中，作者介绍了Inceptionv3模型，并对v2做了一些改进。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mq"><img src="../Images/f722613a0635da08fe4e531d1eb8319b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wBEk0UbYry_7fW1cThyQ-w.png"/></div></div><figcaption class="ml mm et er es mn mo bd b be z dx translated">来源:纸张</figcaption></figure><p id="948e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以下是包括的主要改进:</p><ul class=""><li id="d91b" class="ks kt hi jf b jg kb jk kc jo ku js kv jw kw ka kx ky kz la bi translated">批次标准化简介</li><li id="645d" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">更多因式分解</li><li id="25db" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">RMSProp优化器</li></ul><p id="49dd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="lh">虽然本文无法对盗梦空间进行深入的解释，但您可以通读这篇详细讲述盗梦空间模型的综合性文章:</em> <a class="ae lb" href="https://www.analyticsvidhya.com/blog/2018/10/understanding-inception-network-from-scratch/?utm_source=blog&amp;utm_medium=top4_pre-trained_image_classification_models" rel="noopener ugc nofollow" target="_blank"> <em class="lh">战壕中的深度学习:从零开始理解盗梦空间网络</em> </a></p><p id="3727" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">正如你所看到的，层数是42层，相比之下VGG16只有16层。此外，Inceptionv3将错误率降低到仅4.2%。</p><p id="1fd3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们看看如何用python实现它-</p><p id="a431" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">第1步:数据增加</strong></p><p id="806a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">您会注意到，我并没有进行大规模的数据扩充。代码和以前一样。我刚刚改变了每个模型的图像尺寸。</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="8223" class="ln ig hi lj b fi lo lp l lq lr"># Add our data-augmentation parameters to ImageDataGenerator<br/>train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2,shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)<br/><br/>test_datagen = ImageDataGenerator( rescale = 1.0/255. )</span></pre><p id="5990" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">第2步:培训和验证生成器</strong></p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="b530" class="ln ig hi lj b fi lo lp l lq lr">train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (150, 150))</span><span id="2e08" class="ln ig hi lj b fi mi lp l lq lr">validation_generator = test_datagen.flow_from_directory(validation_dir, batch_size = 20, class_mode = 'binary', target_size = (150, 150))</span></pre><p id="bb41" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">第3步:加载基础模型</strong></p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="efe2" class="ln ig hi lj b fi lo lp l lq lr">from tensorflow.keras.applications.inception_v3 import InceptionV3</span><span id="167a" class="ln ig hi lj b fi mi lp l lq lr">base_model = InceptionV3(input_shape = (150, 150, 3), include_top = False, weights = 'imagenet')</span></pre><p id="5d18" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">第4步:编译并安装</strong></p><p id="70dc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">就像VGG-16，我们只会改变最后一层。</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="8a13" class="ln ig hi lj b fi lo lp l lq lr">for layer in base_model.layers:<br/>    layer.trainable = False</span></pre><p id="8a8c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们执行以下操作:</p><ul class=""><li id="6dfc" class="ks kt hi jf b jg kb jk kc jo ku js kv jw kw ka kx ky kz la bi translated">将基本模型的输出展平到一维</li><li id="3461" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">添加一个具有1，024个隐藏单元的完全连接层并重新激活</li><li id="583b" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">这一次，我们将采用0.2%的辍学率</li><li id="865d" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">添加最终完全连接的s形层</li><li id="1d7f" class="ks kt hi jf b jg lc jk ld jo le js lf jw lg ka kx ky kz la bi translated">我们将再次使用RMSProp，尽管您也可以尝试Adam优化器</li></ul><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="ad55" class="ln ig hi lj b fi lo lp l lq lr">from tensorflow.keras.optimizers import RMSprop<br/><br/>x = layers.Flatten()(base_model.output)<br/>x = layers.Dense(1024, activation='relu')(x)<br/>x = layers.Dropout(0.2)(x)<br/>x = layers.Dense(1, activation='sigmoid')(x)<br/><br/>model = tf.keras.models.Model(base_model.input, x)<br/><br/>model.compile(optimizer = RMSprop(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])</span></pre><p id="8da6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">然后，我们将拟合模型:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="a274" class="ln ig hi lj b fi lo lp l lq lr">inc_history = model.fit_generator(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mr"><img src="../Images/d0072c154ddca96c82e842d74d47380e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*m5xA3AI474T3YeLL.png"/></div></div></figure><p id="c8ca" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">结果，我们可以看到，我们在10个时期内获得了96%的验证准确性。还要注意，这个型号怎么比VGG16快很多。每个历元仅占用VGG16中每个历元时间的1/4。当然，您可以尝试不同的超参数值，看看它的性能有多好/差。</p><p id="fa3e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我真的很喜欢研究盗梦空间模型。虽然当时的大多数模型仅仅是连续的，并遵循模型越深越大的前提，它的性能就越好——Inception及其变体打破了这种模式。就像它的前辈一样，Inceptionv3在2016年CVPR奥运会上以3.5%的前五名错误率获得了冠军。</p><p id="33ff" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这里是这篇论文的链接:<a class="ae lb" href="https://arxiv.org/pdf/1512.00567v3.pdf" rel="noopener ugc nofollow" target="_blank">反思计算机视觉的初始架构</a></p><h1 id="4b37" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">3.ResNet50</h1><p id="7219" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">就像Inceptionv3一样，ResNet50并不是来自ResNet家族的第一款机型。最初的模型被称为剩余网络或ResNet，是2015年CV领域的又一个里程碑。</p><p id="0f48" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这种模型背后的主要动机是避免模型变得更深时的不良准确性。此外，如果你熟悉梯度下降，你会遇到消失梯度问题ResNet模型旨在解决这个问题。这是最早的变体的架构:ResNet34(ResNet50也遵循类似的技术，只是有更多的层)</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ms"><img src="../Images/e6b7ea69b3b4b63be3496f51918e560b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*ZKI-MorWlcO29sW1DmkpXw.png"/></div></figure><p id="1a03" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">您可以看到，从单个卷积层和最大池开始后，有4个类似的层，只是滤波器大小不同，它们都使用3 * 3卷积运算。此外，在每两次盘旋之后，我们会绕过/跳过中间的那一层。这是ResNet模型背后的主要概念。这些跳过的连接被称为“身份快捷连接”，并使用所谓的残余块:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mt"><img src="../Images/4d71268bb5da064b5c9f322010fbeb39.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/0*RVBdjfTmpinXaRsD.png"/></div></figure><p id="8b3a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">简而言之，ResNet的作者提出拟合残差映射比拟合实际映射容易得多，因此可以将其应用于所有层。另一个值得注意的有趣点是，ResNet的作者认为，我们堆叠的层越多，模型的性能就不会越差。</p><p id="dc0d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这与我们在《盗梦空间》中看到的相反，在某种意义上，它几乎类似于VGG16，只是将层堆叠在另一层之上。ResNet只是改变了底层映射。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mu"><img src="../Images/f73596ec7b1ccd823d3008baf8ec7bef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*phgQl_ufbHgEdC2L.png"/></div></div></figure><p id="303c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">ResNet模型有许多变体，其中最新的是ResNet152。以下是ResNet系列在所用层方面的架构:</p><p id="296c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在让我们在数据集上使用ResNet50:</p><p id="e6b2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">步骤1:数据扩充和生成器</strong></p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="1374" class="ln ig hi lj b fi lo lp l lq lr"># Add our data-augmentation parameters to ImageDataGenerator<br/><br/>train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)<br/><br/>test_datagen = ImageDataGenerator(rescale = 1.0/255.)<br/><br/>train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))<br/><br/>validation_generator = test_datagen.flow_from_directory( validation_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))</span></pre><h2 id="451c" class="ln ig hi bd ih lt lu lv il lw lx ly ip jo lz ma it js mb mc ix jw md me jb mf bi translated">步骤2:导入基础模型</h2><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="f774" class="ln ig hi lj b fi lo lp l lq lr">from tensorflow.keras.applications import ResNet50<br/><br/>base_model = ResNet50(input_shape=(224, 224,3), include_top=False, weights="imagenet")</span></pre><p id="f8c7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">同样，我们只使用基本的ResNet模型，所以我们将保持层冻结，只修改最后一层:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="3f40" class="ln ig hi lj b fi lo lp l lq lr">for layer in base_model.layers:<br/>    layer.trainable = False</span></pre><h2 id="ddcd" class="ln ig hi bd ih lt lu lv il lw lx ly ip jo lz ma it js mb mc ix jw md me jb mf bi translated">步骤3:构建和编译模型</h2><p id="1e08" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们编译模型，这次让我们尝试SGD优化器:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="52b0" class="ln ig hi lj b fi lo lp l lq lr">base_model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.0001), loss = 'binary_crossentropy', metrics = ['acc'])</span></pre><h2 id="1064" class="ln ig hi bd ih lt lu lv il lw lx ly ip jo lz ma it js mb mc ix jw md me jb mf bi translated">步骤4:拟合模型</h2><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="8bee" class="ln ig hi lj b fi lo lp l lq lr">resnet_history = base_model.fit(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)</span></pre><p id="9d2e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">查看<a class="ae lb" href="https://gist.github.com/purva91/b89d9fdd136135beb0f2843580f38d01" rel="noopener ugc nofollow" target="_blank">要点</a>上的代码。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mv"><img src="../Images/97c4b036b2103158cbcc37d5684c759b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kzlG3ARs94quJnA5.png"/></div></div></figure><p id="cb22" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以下是我们得到的结果-</p><p id="3372" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">您可以看到它在我们的数据集上的表现，这使得ResNet50成为使用最广泛的预训练模型之一。就像VGG一样，它也有其他变化，正如我们在上表中看到的。值得注意的是，ResNet不仅有自己的变种，还衍生出一系列基于ResNet的架构。其中包括ResNeXt、ResNet as an Ensemble等。此外，ResNet50是最受欢迎的型号之一，错误率约为5%</p><p id="c4d2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以下是论文链接:<a class="ae lb" href="https://arxiv.org/pdf/1512.03385v1.pdf" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a></p><h1 id="88f0" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">4.效率网</h1><p id="7f84" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们终于看到了这4款产品中在这个领域掀起波澜的最新型号，当然，它来自Google。在EfficientNet中，作者提出了一种新的缩放方法，称为<strong class="jf hj">复合缩放。</strong>简单来说就是:早期的模型，比如ResNet，遵循的是任意缩放维度和添加越来越多的层的传统方法。</p><p id="4898" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">然而，该论文提出，如果我们同时以固定的量来缩放维度，并且统一地这样做，我们会实现好得多的性能。缩放系数实际上可以由用户决定。</p><p id="e116" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">虽然这种缩放技术可以用于任何基于CNN的模型，但作者从他们自己的基线模型开始，称为EfficientNetB0:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mw"><img src="../Images/6c2950b8b04f2a552bcc53a7f3e2a392.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*OY3gMQGCTPnfKWHuluKYJQ.png"/></div></figure><p id="191c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">MBConv代表移动反向瓶颈卷积(类似于MobileNetv2)。他们还提出了具有以下比例系数的复合比例公式:</p><p id="ca02" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">该公式再次用于构建一系列高效网络——高效网络0至高效网络7。下面是一个简单的图表，显示了该系列与其他流行型号的性能对比:</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mw"><img src="../Images/d7e2f17665a2989b85a90080da20c563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*B1obmQ22sRJkYAmlTF2gTw.png"/></div></figure><p id="366f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">正如您所看到的，即使是基线B0模型也是以高得多的精度开始的，精度只会不断提高，而且参数也越来越少。例如，EfficientB0只有530万个参数！</p><p id="7b8d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">实现EfficientNet最简单的方法是安装它，其余的步骤与我们上面看到的类似。</p><h2 id="e928" class="ln ig hi bd ih lt lu lv il lw lx ly ip jo lz ma it js mb mc ix jw md me jb mf bi translated">安装效率网络:</h2><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="acf8" class="ln ig hi lj b fi lo lp l lq lr">!pip install -U efficientnet</span></pre><h2 id="3786" class="ln ig hi bd ih lt lu lv il lw lx ly ip jo lz ma it js mb mc ix jw md me jb mf bi translated">导入它</h2><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="78ce" class="ln ig hi lj b fi lo lp l lq lr">import efficientnet.keras as efn</span></pre><h2 id="c17c" class="ln ig hi bd ih lt lu lv il lw lx ly ip jo lz ma it js mb mc ix jw md me jb mf bi translated">步骤1:图像增强</h2><p id="58d2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将使用与VGG16和ResNet50相同的图像尺寸。到目前为止，您应该已经熟悉了增强过程:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="4dfb" class="ln ig hi lj b fi lo lp l lq lr"># Add our data-augmentation parameters to ImageDataGenerator<br/><br/>train_datagen = ImageDataGenerator(rescale = 1./255., rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)<br/><br/>test_datagen = ImageDataGenerator(rescale = 1.0/255.)<br/><br/>train_generator = train_datagen.flow_from_directory(train_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))<br/><br/>validation_generator = test_datagen.flow_from_directory( validation_dir, batch_size = 20, class_mode = 'binary', target_size = (224, 224))</span></pre><h2 id="dec9" class="ln ig hi bd ih lt lu lv il lw lx ly ip jo lz ma it js mb mc ix jw md me jb mf bi translated">步骤2:加载基本模型</h2><p id="36bf" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将使用B0版本的EfficientNet，因为它是8个版本中最简单的。我强烈建议您尝试其余的模型，但是请记住，模型会变得越来越复杂，这可能不是最适合简单的二元分类任务的。</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="7223" class="ln ig hi lj b fi lo lp l lq lr">base_model = efn.EfficientNetB0(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')</span></pre><p id="d513" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们再次冻结这些层:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="7675" class="ln ig hi lj b fi lo lp l lq lr">for layer in base_model.layers:<br/>    layer.trainable = False</span></pre><h2 id="0a22" class="ln ig hi bd ih lt lu lv il lw lx ly ip jo lz ma it js mb mc ix jw md me jb mf bi translated">步骤3:构建模型</h2><p id="95d4" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">就像Inceptionv3一样，我们将在最后一层执行这些步骤:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="04e2" class="ln ig hi lj b fi lo lp l lq lr">x = model.output<br/>x = Flatten()(x)<br/>x = Dense(1024, activation="relu")(x)<br/>x = Dropout(0.5)(x)<br/>predictions = Dense(1, activation="sigmoid")(x)<br/>model_final = Model(input = model.input, output = predictions)</span></pre><h2 id="a789" class="ln ig hi bd ih lt lu lv il lw lx ly ip jo lz ma it js mb mc ix jw md me jb mf bi translated">第四步:编译和安装</h2><p id="4b6e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们再次使用RMSProp优化器，不过在这里，我引入了一个衰减参数:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="8b01" class="ln ig hi lj b fi lo lp l lq lr">model_final.compile(optimizers.rmsprop(lr=0.0001, decay=1e-6),loss='binary_crossentropy',metrics=['accuracy'])</span></pre><p id="9ed9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们最终根据我们的数据拟合了模型:</p><pre class="kh ki kj kk fd li lj lk ll aw lm bi"><span id="b7e1" class="ln ig hi lj b fi lo lp l lq lr">eff_history = model_final.fit_generator(train_generator, validation_data = validation_generator, steps_per_epoch = 100, epochs = 10)</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mx"><img src="../Images/73ef1b00196f9d10a565ea8c5af62005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3CRZSBmyn5i9XOvHmYsJqg.png"/></div></div></figure><p id="5a3b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们做到了——仅在10个时期内，我们就在验证集上获得了高达98%的准确率。我强烈建议您尝试使用EfficientNetB7训练更大的数据集，并在下面与我们分享结果。</p><p id="1c75" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以下是论文的链接:<a class="ae lb" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientNet:反思卷积神经网络的模型缩放</a></p><h1 id="5582" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结束注释:</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es my"><img src="../Images/36ceed45c6f9da927d34794ff534a4f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aQM8OITH0EY37OLr.png"/></div></div></figure><p id="d428" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">总之，在本文中，我向您介绍了4种用于图像分类的顶级预训练模型。这里有一个方便的表格，供您参考这些型号及其性能:</p><p id="c7b4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我只提供了图像分类的前4个预训练模型以及如何实现它们的概述。然而，这是一个持续增长的领域，总是有新的模式可以期待，并进一步推动边界。我迫不及待地想探索这些新模型，我也敦促你在不同的数据集上用不同的参数尝试上述模型，并在下面的评论中与我们分享你的结果！</p><p id="66dc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">您也可以在我们的移动应用程序上阅读这篇文章。</p><p id="0f93" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="lh">原载于2020年8月17日</em><a class="ae lb" href="https://www.analyticsvidhya.com/blog/2020/08/top-4-pre-trained-models-for-image-classification-with-python-code/" rel="noopener ugc nofollow" target="_blank"><em class="lh">【https://www.analyticsvidhya.com】</em></a><em class="lh">。</em></p></div></div>    
</body>
</html>