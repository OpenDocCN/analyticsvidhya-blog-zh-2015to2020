<html>
<head>
<title>Predicting Car Prices Using Machine Learning Models-Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习模型预测汽车价格-Python</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/predicting-car-prices-using-machine-learning-models-python-701aaac23079?source=collection_archive---------3-----------------------#2020-03-14">https://medium.com/analytics-vidhya/predicting-car-prices-using-machine-learning-models-python-701aaac23079?source=collection_archive---------3-----------------------#2020-03-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="88d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">土耳其的二手车市场相当活跃。因此，预测汽车价格是高度可变的。在这篇文章中，机器学习模型进行了比较，并选择了价格预测的最佳模型。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/392b23574ef3292aed61f411abe00728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kzB1ktfAw-PyB1Cy2qkl2A.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><a class="ae ju" href="https://www.pinterest.at/pin/406449935099301288/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="5515" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练模型是通过按汽车品牌划分模型来执行的，而不是为了有效的优化过程和更好地利用计算机能力和时间而进行的单个大模型训练。在训练中使用了大约100，000个具有12个特征的数据。比较的机器学习模型有:</p><ul class=""><li id="4b60" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated">线性回归</li><li id="0e3f" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">山脉</li><li id="ff57" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">套索</li><li id="7bd4" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">弹性网</li><li id="c24c" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">k-最近邻</li><li id="cf8f" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">随机森林</li><li id="849e" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">XGBOOST</li><li id="ec13" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">梯度推进机</li></ul><p id="4586" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，用默认参数训练模型。然后，对这些模型进行超参数优化。两种情况下的参数值和估计值都被记录下来，以备以后查看。</p><p id="6c57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据预处理</strong></p><p id="0889" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过网络抓取方法(也许可以在另一篇文章中详细讨论)获得的数据是从数据库中下载的。当导入第一个过程库时，我们的数据被转换成数据帧，然后检查是否有丢失的数据。</p><pre class="jf jg jh ji fd kj kk kl km aw kn bi"><span id="0b73" class="ko kp hi kk b fi kq kr l ks kt">from pymongo import MongoClient<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import re<br/>from sklearn.model_selection import train_test_split, GridSearchCV,ShuffleSplit, cross_val_score<br/>from sklearn import model_selection<br/>from sklearn.neighbors import KNeighborsRegressor<br/>import statsmodels.api as sm<br/>from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV, ElasticNet<br/>from sklearn.metrics import mean_squared_error, mean_absolute_error<br/>import numpy as np<br/>from sklearn.preprocessing import StandardScaler, OneHotEncoder<br/>from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor<br/>import xgboost as xgb<br/>from xgboost import XGBRegressor<br/>import config<br/>import pickle<br/>from collections import defaultdict<br/>from operator import itemgetter<br/>import random<br/>import lightgbm as lgb<br/>from lightgbm import LGBMRegressor</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ku"><img src="../Images/c8ac9f983bbe00ebb5e3e360fb23b071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KSHx5m7EnOKrkd2MQfNMuQ.png"/></div></div></figure><p id="4416" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于缺失数据的数量很少，我们可以删除它们。有些数据是数据类型字符串，会转换为整数，如year(“1999”，“2018”)。此外，数据是通过删除价格和里程值中的点表达式来准备的。</p><pre class="jf jg jh ji fd kj kk kl km aw kn bi"><span id="e706" class="ko kp hi kk b fi kq kr l ks kt">"""Preprocessing of Price"""<br/>df["Fiyat"] = df["Fiyat"].str.replace(".","")<br/>df["Fiyat"] = [re.findall(r"(\d+)", str(x)) for x in df["Fiyat"]]<br/>df["Fiyat"] = df["Fiyat"].str[0]<br/>df["Fiyat"]  = df["Fiyat"].astype(int)</span><span id="e77b" class="ko kp hi kk b fi kv kr l ks kt">"""Preprocessing of Km and year"""<br/>df["Km"] = df["Km"].str.replace(".","")<br/>df["Km"]  = df["Km"].astype(int)<br/>df["Yıl"]  = df["Yıl"].astype(int)</span></pre><p id="6677" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上述数据准备之后，识别异常值检测。这个项目中的异常值检测主要是由于供应商输入了不正确的信息。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kw"><img src="../Images/57fd633566e7ddab1b128bed322e719e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TwgKDx0hpRhtWgmIYOAg7Q.png"/></div></div></figure><p id="5451" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面可以看到检测到的错误里程(Km)信息。这些里程值是错误的，尤其是在与年份(yl)和价格(Fiyat)信息进行比较时。这些数据可以被删除，因为检测到的异常值的数量很少。然而，在我们拥有的数据数量很少的情况下，我们可能不想删除这些数据。在这种情况下，可以将平均值或中值分配给异常值列(在这种情况下为里程列)。例如，我们只考虑了奥迪，但这些程序适用于每个汽车品牌。</p><p id="b119" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们简单考虑一下项目中使用的机器学习模型:</p><ul class=""><li id="23e1" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj"> <em class="jd">线性回归:</em> </strong></li></ul><p id="2384" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归用于确定因变量和一个或多个自变量之间的线性关系。线性回归有两种类型，简单线性回归和多元线性回归。由于本项目中有多个独立变量，因此使用了多元线性回归模型。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kx"><img src="../Images/9d626bcd371377813958b1e9814b4cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*24EYi86Eqy5t3GDsGpgKMg.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ky"><img src="../Images/c20ba74f1746b2ceebb81ba358d807ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDQeXRugC-dlWPTsq982dg.png"/></div></div></figure><pre class="jf jg jh ji fd kj kk kl km aw kn bi"><span id="91cf" class="ko kp hi kk b fi kq kr l ks kt">def linear_regression(self):<br/>        self.lr_model = LinearRegression()<br/>        self.lr_model.fit(self.X_train, self.y_train)<br/>        y_pred = self.lr_model.predict(self.X_test)<br/>        lr_rmse = np.sqrt(mean_squared_error(self.y_test,y_pred))<br/>        self.lr_rmse_per  =np.sqrt(np.mean(np.square(((self.y_test - y_pred) / self.y_test))))*100<br/>        self.lr_mae = mean_absolute_error(self.y_test, y_pred)<br/>        self.lr_mape =  np.abs((self.y_test - y_pred) / self.y_test).mean(axis=0) * 100<br/>        return lr_rmse</span></pre><ul class=""><li id="b755" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj"><em class="jd">K-最近邻:</em> </strong></li></ul><p id="217e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然K-最近邻算法(KNN)通常用于分类问题，但它在回归问题中也给出非常好的结果。KNN算法的工作原理是根据对象的邻近关系对其进行聚类。一个例子被它的邻居的多数投票分类；该样本被分配到其最近邻中的最近类，通过距离函数进行测量。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kz"><img src="../Images/e659d648ad06e13e0995656f4c5a6a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NjPk4SJ0V32LPyLXgl0iAA.png"/></div></div></figure><p id="4911" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还应注意，所有三个距离仅适用于连续变量。在分类变量的情况下，应该使用汉明距离。此外，当数据集中混合了数字变量和分类变量时，应该注意数字变量的标准化。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kx"><img src="../Images/89441b700b1fc3dea8b5504c0b646868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BxZax3i2wLvSD95kpUbIiw.png"/></div></div></figure><p id="8d82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然KNN算法创建了一个有效的机器学习模型，特别是针对包含数据的噪声，但对于大数据，有必要谨慎处理。</p><p id="29a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">项目代码:</p><pre class="jf jg jh ji fd kj kk kl km aw kn bi"><span id="1e55" class="ko kp hi kk b fi kq kr l ks kt">def knn(self):<br/>        knn_model = KNeighborsRegressor().fit(self.X_train, self.y_train)<br/>        y_pred = knn_model.predict(self.X_test)<br/>        self.knn_rmse = np.sqrt(mean_squared_error(self.y_test,y_pred))<br/>        #Model tuning <br/>   #=============================================================================<br/>        knn_params = {"n_neighbors": [2,5,10,20,30,40],<br/>                      "leaf_size": [15,30,60]}<br/>        n_neighbors_list = knn_params["n_neighbors"]<br/>        leaf_size_list = knn_params["leaf_size"]<br/>        self.knn_result_list = []<br/>        for i in range(len(n_neighbors_list)):<br/>            for j in range(len(leaf_size_list)):<br/>                temp_list = []<br/>                knn_model = KNeighborsRegressor(n_neighbors = n_neighbors_list[i], leaf_size = leaf_size_list[j])<br/>                knn_model.fit(self.X_train, self.y_train)<br/>                y_pred = knn_model.predict(self.X_test)<br/>                knn_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))<br/>                temp_list.extend((knn_rmse, n_neighbors_list[i], leaf_size_list[j]))<br/>                self.knn_result_list.append(temp_list)<br/>        self.knn_result_list.sort()<br/>        <br/>        self.knn_tuned = KNeighborsRegressor(n_neighbors = self.knn_result_list[0][1], leaf_size = self.knn_result_list[0][2])<br/>        self.knn_tuned.fit(self.X_train, self.y_train)<br/>        y_pred = self.knn_tuned.predict(self.X_test)<br/>        knn_tuned_rmse = np.sqrt(mean_squared_error(self.y_test,y_pred))<br/>        self.knn_tuned_rmse_per  =np.sqrt(np.mean(np.square(((self.y_test - y_pred) / self.y_test))))*100<br/>        self.knn_mae = mean_absolute_error(self.y_test, y_pred)<br/>        self.knn_mape =  np.abs((self.y_test - y_pred) / self.y_test).mean(axis=0) * 100<br/>        return knn_tuned_rmse</span></pre><ul class=""><li id="2ea3" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj"> <em class="jd">随机森林:</em> </strong></li></ul><p id="c944" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林模型，用于回归和分类问题，基于决策树模型。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es la"><img src="../Images/a03776f9258931d4455d8341618fa993.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*KaxPiPHk5G46X2UzZlvDww.png"/></div></figure><p id="a3ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是一种传统的方法，其最大的问题之一是在教育中过拟合。随机森林模型就是为了解决这个问题而发展起来的。该模型从数据集和属性集中进行随机选择。从上面可以看出，每个决策树使用自己的子集进行单独的预测。在最后一个阶段，我们通过对所获得的预测取平均值来达到我们的随机森林模型的最终预测值。</p><p id="5d0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的项目中，“max_depth”、“n_estimators”和“max_features”超参数用于对我们的随机森林模型进行调整操作。</p><pre class="jf jg jh ji fd kj kk kl km aw kn bi"><span id="f58d" class="ko kp hi kk b fi kq kr l ks kt">def random_forest(self):<br/>        <br/>        rf_model = RandomForestRegressor(random_state = 10).fit(self.X_train, self.y_train)<br/>        y_pred = rf_model.predict(self.X_test)<br/>        self.rf_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))<br/>        #Random Forest Tuning Model<br/>        rf_params = {"max_depth": [None, 2,5,9],<br/>                     "n_estimators": [10,100,500,1000],<br/>                     "max_features": ["auto",3,5]}<br/>        self.rf_result_list = []<br/>        max_depth_list = rf_params["max_depth"]<br/>        n_estimators_list = rf_params["n_estimators"]<br/>        max_features_list = rf_params["max_features"]<br/>        for i in range(len(max_depth_list)):<br/>            for j in range(len(n_estimators_list)):<br/>                for k in range(len(max_features_list)):<br/>                    temp_list = []<br/>                    rf_model = RandomForestRegressor(random_state = 10, max_depth = max_depth_list[i], n_estimators = n_estimators_list[j], max_features = max_features_list[k])<br/>                    rf_model.fit(self.X_train, self.y_train)<br/>                    y_pred = rf_model.predict(self.X_test)<br/>                    knn_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))<br/>                    temp_list.extend((knn_rmse, max_depth_list[i], n_estimators_list[j], max_features_list[k]))<br/>                    self.rf_result_list.append(temp_list)<br/>        self.rf_result_list.sort()<br/>        self.rf_tuned = RandomForestRegressor(random_state = 10, max_depth = self.rf_result_list[0][1], n_estimators = self.rf_result_list[0][2], max_features = self.rf_result_list[0][3])<br/>        self.rf_tuned.fit(self.X_train, self.y_train)<br/>        y_pred = self.rf_tuned.predict(self.X_test)<br/>        rf_tuned_rmse = np.sqrt(mean_squared_error(self.y_test,y_pred))<br/>        self.rf_tuned_rmse_per  =np.sqrt(np.mean(np.square(((self.y_test - y_pred) / self.y_test))))*100<br/>        self.rf_mae = mean_absolute_error(self.y_test, y_pred)<br/>        self.rf_mape =  np.abs((self.y_test - y_pred) / self.y_test).mean(axis=0) * 100<br/>        return rf_tuned_rmse</span></pre><ul class=""><li id="b360" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj"> <em class="jd">渐变助推机:</em> </strong></li></ul><p id="8aa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Boosting是一种迭代技术，其中预测器不是独立产生的，而是顺序产生的。因此，后续模型中的观测值出现的概率是不相等的，误差最大的观测值出现得最多。预测器可以从一系列模型中选择，如决策树、回归器、分类器等。因为新的预测器正在从先前预测器犯下的错误中学习，所以达到接近实际预测需要更少的时间/迭代。但是我们已经仔细选择了停止标准，否则它会导致训练数据的过度拟合。所以梯度推进的例子就是推进算法。</p><p id="3965" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们将均方误差(MSE)定义为损失，即:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lb"><img src="../Images/ce9df50b3e8da0d5aa619158a14b882f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mkuq0DtZ7EmYGnZTA6NZoQ.png"/></div></div></figure><p id="7e0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过使用梯度下降并基于学习率更新我们的预测，我们可以找到MSE最小的值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kx"><img src="../Images/f0d392ca50f3d6f7edfa97cd63e5f9d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lR9CLhWhWKDTMxPNghtOQg.png"/></div></div></figure><pre class="jf jg jh ji fd kj kk kl km aw kn bi"><span id="15d1" class="ko kp hi kk b fi kq kr l ks kt">def gbm(self):<br/>        gbm_model = GradientBoostingRegressor().fit(self.X_train, self.y_train)<br/>        y_pred = gbm_model.predict(self.X_test)<br/>        self.gbm_rmse = np.sqrt(mean_squared_error(self.y_test,y_pred))<br/>      <br/>        gbm_params = {"learning_rate" : [0.001, 0.01, 0.1],<br/>                      "max_depth": [3,5,8,50,100],<br/>                      "n_estimators" : [10,100,500,1000], <br/>                      "subsample" : [1,0.5,0.75]}<br/>        learning_rate_list = gbm_params["learning_rate"]<br/>        max_depth_list = gbm_params["max_depth"]<br/>        n_estimators_list = gbm_params["n_estimators"]<br/>        subsample_list = gbm_params["subsample"]<br/>        self.gbm_result_list = []<br/>        for i in range(len(learning_rate_list)):<br/>            for j in range(len(max_depth_list)):<br/>                for k in range(len(n_estimators_list)):<br/>                    for l in range(len(subsample_list)):<br/>                        temp_list = []<br/>                        gbm_model = GradientBoostingRegressor(learning_rate = learning_rate_list[i],<br/>                                                              max_depth = max_depth_list[j],<br/>                                                              n_estimators = n_estimators_list[k],<br/>                                                              subsample = subsample_list[l])<br/>                        gbm_model.fit(self.X_train, self.y_train)<br/>                        y_pred = gbm_model.predict(self.X_test)<br/>                        gbm_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))<br/>                        temp_list.extend((gbm_rmse, learning_rate_list[i], max_depth_list[j], n_estimators_list[k], subsample_list[l]))<br/>                        self.gbm_result_list.append(temp_list)<br/>        self.gbm_result_list.sort()<br/>        self.gbm_tuned = GradientBoostingRegressor(learning_rate = self.gbm_result_list[0][1],<br/>                                                   max_depth = self.gbm_result_list[0][2],<br/>                                                   n_estimators = self.gbm_result_list[0][3],<br/>                                                   subsample = self.gbm_result_list[0][4])<br/>        self.gbm_tuned.fit(self.X_train, self.y_train)<br/>        y_pred = self.gbm_tuned.predict(self.X_test)<br/>        gbm_tuned_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))<br/>        self.gbm_tuned_rmse_per =np.sqrt(np.mean(np.square(((self.y_test - y_pred) / self.y_test))))*100<br/>        self.gbm_mae = mean_absolute_error(self.y_test, y_pred)<br/>        self.gbm_mape =  np.abs((self.y_test - y_pred) / self.y_test).mean(axis=0) * 100<br/>        return gbm_tuned_rmse</span></pre><ul class=""><li id="0572" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj"> <em class="jd">极限梯度提升(XGBOOST): </em> </strong></li></ul><p id="f7d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然XGBoost类似于梯度增压机，但更有效。XGBoost在创建决策树的同时提供了并行化，使得决策树的创建速度更快。事实上，他可以做到这一点是基础学习者，而他可以在内部和外部循环之间切换。通常，当形成决策树的叶子时，外部循环计算内部循环属性。XGBoost以max_depth优先考虑深度，其复杂度显著提高了计算性能。XGBoost中的核外计算优化了可用磁盘空间，并在处理不适合内存的大型数据集时最大限度地利用了磁盘空间。XGBoost可以使用套索和脊正则化来防止过度拟合。为了加快计算速度，XGBoost可以利用CPU上的多个内核。这是可能的，因为其系统设计中的块结构。数据被分类并存储在称为块的内存单元中。与其他算法不同，这使得数据布局能够被后续迭代重用，而不是再次计算。XGBoost最大的一个优点就是在划分成树的同时，通过加权来使用数据集中的观测点，以区别于正确的点。</p><p id="6035" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的项目中，“max_depth”、“n_estimators”、“colsample_bytree”和“learning rate”超参数用于对我们的随机森林模型进行调整操作。</p><pre class="jf jg jh ji fd kj kk kl km aw kn bi"><span id="8429" class="ko kp hi kk b fi kq kr l ks kt">def xgboost(self):<br/>        X_train_v = self.X_train.values<br/>        X_test_v = self.X_test.values<br/>        xgb_model = XGBRegressor().fit(X_train_v, self.y_train)<br/>        y_pred = xgb_model.predict(X_test_v)<br/>        self.xgboost_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))<br/>        #Model Tuning<br/>        xgb_params ={"colsample_bytree": [0.6,0.9,1],<br/>                     "n_estimators" : [100,200,500,1000],<br/>                     "max_depth" : [4,5,6],<br/>                     "learning_rate": [0.1,0.3,0.5]}<br/>        self.xgb_result_list = []<br/>        colsample_bytree_list = xgb_params["colsample_bytree"]<br/>        n_estimators_list = xgb_params["n_estimators"]<br/>        max_depth_list = xgb_params["max_depth"]<br/>        learning_rate_list = xgb_params["learning_rate"]<br/>        for i in range(len(colsample_bytree_list)):<br/>            for j in range(len(n_estimators_list)):<br/>                for k in range(len(max_depth_list)):<br/>                    for l in range(len(learning_rate_list)):<br/>                        temp_list = []<br/>                        xgb_model = XGBRegressor(colsample_bytree = colsample_bytree_list[i],<br/>                                                 n_estimators = n_estimators_list[j],<br/>                                                 max_depth = max_depth_list[k],<br/>                                                 learning_rate = learning_rate_list[l])<br/>                        xgb_model.fit(X_train_v, self.y_train)<br/>                        y_pred = xgb_model.predict(X_test_v)<br/>                        xgb_rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))<br/>                        temp_list.extend((xgb_rmse, colsample_bytree_list[i], n_estimators_list[j], max_depth_list[k], learning_rate_list[l]))<br/>                        self.xgb_result_list.append(temp_list)<br/>        self.xgb_result_list.sort()<br/>        self.xgb_tuned = XGBRegressor(colsample_by_tree = self.xgb_result_list[0][1],<br/>                                      n_estimators  =self.xgb_result_list[0][2],<br/>                                      max_depth = self.xgb_result_list[0][3],<br/>                                      learning_rate = self.xgb_result_list[0][4])<br/>        self.xgb_tuned.fit(X_train_v, self.y_train)<br/>        y_pred = self.xgb_tuned.predict(X_test_v)<br/>        xgb_tuned_rmse = np.sqrt(mean_squared_error(self.y_test,y_pred))<br/>        self.xgb_tuned_rmse_per  =np.sqrt(np.mean(np.square(((self.y_test - y_pred) / self.y_test))))*100<br/>        self.xgb_mae = mean_absolute_error(self.y_test, y_pred)<br/>        self.xgb_mape =  np.abs((self.y_test - y_pred) / self.y_test).mean(axis=0) * 100<br/>        return xgb_tuned_rmse</span></pre><p id="5604" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">均方根误差取每个观察值和预测值的差值。你可以交换减法的顺序，因为下一步是求差的平方。这是因为负值的平方总是正值。但是要确保你一直保持同样的顺序。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lc"><img src="../Images/d9eab2bfbb0d5ad093ba6238cfcece59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jySPZv-mrcoJS-pgS6yPXA.png"/></div></div></figure><p id="3bcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之后，将所有值的总和除以观察次数。最后，我们得到一个RMSE值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ld"><img src="../Images/f002916b21773f558d6f67372ee1b758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8G2F7nBwnoXJ1Lh_HdmkA.png"/></div></div></figure><p id="7f4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用于比较所获得的估计的另一种方法可以是平均绝对误差(MAE)。当数据中存在极端观察值时，这是特别优选的。然而，如果我们想更强烈地反映异常值对系统的影响，那么选择RMSE值将是更正确的。在项目中发现并比较了这两个值。</p><h2 id="311f" class="ko kp hi bd le lf lg lh li lj lk ll lm iq ln lo lp iu lq lr ls iy lt lu lv lw bi translated">结论</h2><p id="181f" class="pw-post-body-paragraph if ig hi ih b ii lx ik il im ly io ip iq lz is it iu ma iw ix iy mb ja jb jc hb bi translated">对于模型比较，使用rmse值。土耳其里拉被用作货币。每个汽车品牌要比较的所有模型都经过训练，并确定最佳模型。因此，每个汽车品牌的最佳模式被确定。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mc"><img src="../Images/0dd87dddc15feb417779424514cd941e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ueW6ZlizSTXdBhCvqaETw.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es md"><img src="../Images/107f647d058a5a3efb051154b9d2d852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HEEtSij002pbYqDmCzjmig.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/13a29b798f5553dd21dbe68c3332a86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*osWl-T7zob8R1zsHkpd_5Q.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mf"><img src="../Images/3a6bfdcdbfe1c180289e3d6e0e3638aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w010el1JYsFlfrmBre9SPQ.png"/></div></div></figure><p id="c29f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上表格中可以看到所有模型的rmse值。此外，在表中名为“所选型号mae”的列中，所选的最佳型号具有mae值。</p><p id="312b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为模型比较的结果(最佳模型计数):</p><ul class=""><li id="5cf7" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated">梯度推进机(GBM) : 19</li><li id="bf7d" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">极端梯度增强(XGBoost) : 11</li><li id="6140" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">随机森林:7</li><li id="8d60" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">山脊:5</li><li id="ddc2" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">套索:2</li><li id="29b6" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">弹性网:1</li></ul><p id="9a59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看一下排序，可以看到选择GBM模型的最多。尤其是XGBoost和GBM型号比其他型号更有优势。然而，根据这些结果，得出GBM模型是最佳模型的结论是不正确的。不要忘记，每个模型在不同的数据类型和大小下都是有效的。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es md"><img src="../Images/bea45d98da13f6ff7c59084819694e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxEgufsQv7MWCuhi2vefPg.png"/></div></div></figure><p id="7a26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，在上表中可以看到GBM模型的默认参数的rmse值和调整参数的rmse值。如果列名中有“def”(GBM def max _ depth)，则它是默认参数，否则(“cho”)它是调整后的参数。所以我们已经看到了超参数优化是如何导致模型进化的</p><p id="6a67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">github:<a class="ae ju" href="https://github.com/ademakdogan" rel="noopener ugc nofollow" target="_blank">https://github.com/ademakdogan</a></p><p id="13ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">领英:<a class="ae ju" href="https://www.linkedin.com/in/adem-akdo%C4%9Fan-948334177/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/adem-akdo%C4%9Fan-948334177/</a></p><h2 id="8fb1" class="ko kp hi bd le lf lg lh li lj lk ll lm iq ln lo lp iu lq lr ls iy lt lu lv lw bi translated">参考</h2><ul class=""><li id="8e1d" class="jv jw hi ih b ii lx im ly iq mg iu mh iy mi jc ka kb kc kd bi translated"><em class="jd">数据科学、网络安全和IT应用的进步</em></li><li id="feef" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><a class="ae ju" href="https://www.statisticallysignificantconsulting.com/RegressionAnalysis.htm" rel="noopener ugc nofollow" target="_blank">https://www . statistically significant consulting . com/regression analysis . htm</a></li><li id="0b75" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><a class="ae ju" href="https://www.analyticsvidhya.com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/09/an-end-to-end-guide-to-understand-the-math-behind-xgboost/</a></li></ul></div></div>    
</body>
</html>