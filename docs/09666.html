<html>
<head>
<title>The SVM we need to know || The SVM we implemented.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我们需要知道的SVM | |我们实现的SVM。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-svm-we-need-to-know-the-svm-we-implemented-47740d65aa5b?source=collection_archive---------19-----------------------#2020-09-15">https://medium.com/analytics-vidhya/the-svm-we-need-to-know-the-svm-we-implemented-47740d65aa5b?source=collection_archive---------19-----------------------#2020-09-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es im"><img src="../Images/1dfbd201bbf8a8dc43fed950f8e3e550.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*39mKJ8B5aqYF_HFbqlvK5w.png"/></div></figure><p id="4df6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">嘿，伙计们，希望你们都做得很好，首先，这是我上两个博客的延续，在那里我们讨论了关于<a class="ae js" href="https://www.linkedin.com/posts/atul-mishra-5001_new-approach-to-learn-diabetes-prediction-activity-6699157069678874624-hbmj" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj"/></a><strong class="iw hj">的实际糖尿病预测，随后解释了</strong> <a class="ae js" href="https://www.linkedin.com/posts/atul-mishra-5001_why-when-what-logistic-regression-activity-6703335712151924736-kfUH" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hj">逻辑回归</strong> </a> <strong class="iw hj"> </strong>以及它如何帮助我们实际应用。</p><ul class=""><li id="42fd" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">支持和反馈非常友好，这让我对交付和帮助社区更有热情。</li></ul><p id="2c38" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这篇文章在很大程度上是对那篇文章的延续，因为在这里我们将讨论我们在<strong class="iw hj">定制管道中使用的另一个算法，SVM(支持向量机)。</strong></p><p id="867b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们必须理解<strong class="iw hj">什么和为什么</strong>正如我们对逻辑回归所做的那样，同样的事情也必须考虑到<strong class="iw hj"> SVM </strong>这里。我在想加入这个博客的<strong class="iw hj"> MNIST </strong>时尚数据集和我<a class="ae js" href="https://github.com/mishra-atul5001/Data-Science-and-ML-insights-Projects/blob/master/Fashion%20MNIST%20with%20Keras.ipynb" rel="noopener ugc nofollow" target="_blank">尝试过的DL模特</a>，但是那会偏离手头的主题，所以让我们开始吧。</p><h1 id="2cc3" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">什么是SVM？</h1><ul class=""><li id="e6c0" class="jt ju hi iw b ix la jb lb jf lc jj ld jn le jr jy jz ka kb bi translated">支持向量机是一种经典且通用的机器学习模型，可用于<strong class="iw hj">分类/回归</strong>甚至<strong class="iw hj">离群点检测。</strong></li><li id="bc42" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">它可以对<strong class="iw hj">线性</strong>以及<strong class="iw hj">非线性数据</strong>执行这些操作，但是当在目标变量和预测变量之间存在具有高<strong class="iw hj">非线性</strong>趋势的高维数据时，可以看到SVM模型的实际功效。我确实使用了<strong class="iw hj"> SVM模型</strong>进行<a class="ae js" href="https://github.com/mishra-atul5001/Data-Science-and-ML-insights-Projects/blob/master/Customer%20Sentiment%20Analysis%20--%20NLP%20--%20IMDB%20Reviews.ipynb" rel="noopener ugc nofollow" target="_blank">情感分析</a>，并且给了我一些不错的结果。</li><li id="bb02" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated"><strong class="iw hj">支持向量机</strong>背后的基本思想是在分隔两个或更多类的决策边界和训练实例之间拟合最宽/最大可能的余量。</li></ul><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lk"><img src="../Images/69e2927dbaa204417b4740c84ffe5cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHhx0N3mDWPH4iuI6GjR7Q.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated"><strong class="bd ke">可视化表示<strong class="bd ke">二进制SVM模型</strong>的</strong></figcaption></figure><ul class=""><li id="4346" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">现在上图属于2D的<strong class="iw hj">超平面概念。</strong>这里，<strong class="iw hj">边距</strong>中间的线是我们的<strong class="iw hj">超平面</strong>分隔两个不同的类。那么一条线的数学方程就是<strong class="iw hj"> ax+by+c = 0 </strong>对吧？</li><li id="abaa" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">因此，如果我们假设超平面是一条线，那么具有数据点的<strong class="iw hj">的左右边距变成如下等式:对于类<strong class="iw hj"> -1 </strong>为<strong class="iw hj"> ax+by+c &lt; 0 </strong>，对于类<strong class="iw hj"> 1 </strong>为<strong class="iw hj"> ax+by+c &gt; 0 </strong>。这个数学很重要，因为现在我们有2维，当我们进入3维或推广N维时会发生什么。</strong></li><li id="14aa" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">现在，让我们也接触一下<strong class="iw hj"> 3D平面</strong>概念，以便达到<strong class="iw hj"> N维</strong>通用性。所以，SVM把一个三维超平面称为<strong class="iw hj">“平面”。</strong>这里平面的方程变成了<strong class="iw hj"> ax+by+cz+d = 0。</strong>与此类似，向量边距变成了<strong class="iw hj">平面的正负</strong>方面。</li></ul><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lt"><img src="../Images/51a02240a4c971a69c6841cece4a5c0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NTwomGmCLNuC8N8SmvTmAQ.jpeg"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">从我的SVM笔记剪-&gt;三维平面</figcaption></figure><ul class=""><li id="681c" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">有谁能回答这个-&gt; <strong class="iw hj">三维空间中超平面</strong>的维数是多少？答案:<strong class="iw hj">特征数量-1 </strong>。3D平面中的特征有<strong class="iw hj"> {x，y，z} </strong>。</li><li id="3112" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">对于<strong class="iw hj">D(10/1000/百万)维</strong>，我们的最终等式为:<strong class="iw hj"><em class="lu">sum(x1 . w1+x2 . w2+x3 . w3+…………..+xd.wd)+c = 0 </em> </strong>，其中x1，x2，x3…..xd是<strong class="iw hj">尺寸</strong>，w1、w2、w3……wd是<strong class="iw hj">系数</strong>，c是我们的常数。</li><li id="ceb2" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">由该方程表示的模型称为<strong class="iw hj">线性鉴别器</strong>，是重要的技术面试问题之一。但到目前为止，我们只讨论了什么是SVM，并且只考虑了线性可分数据。现在，在SVM的<strong class="iw hj">为什么部分</strong>中，我们将看到<strong class="iw hj"> SVM对非线性可分数据</strong>的威力。</li></ul></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><h1 id="584e" class="kc kd hi bd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv lz kx ky kz bi translated">为什么是SVM？</h1><ul class=""><li id="561c" class="jt ju hi iw b ix la jb lb jf lc jj ld jn le jr jy jz ka kb bi translated">分类问题可以用<strong class="iw hj"> Logistic回归</strong>解决，但是哦等一下，Logistic回归不是更适合<strong class="iw hj">二元分类吗？如果我有2门以上的课怎么办？如果我有一个更高维的数据呢？如果我的数据不是线性的怎么办？</strong></li><li id="7570" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">在这种情况下，我们需要SVM的力量来产生一个平面/超平面，它可以更好地分离阶级。但是有一个技巧，你希望自己付出多少代价来错误地分类这些点，以便实现一个更好/稳定的模型？我们先来看一些非线性可分的例子，然后再来讨论这个问题。</li></ul><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es ma"><img src="../Images/317f3a3852aefaa8252653632e1f7fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eRgwlUlTUG_vVPDxjZmqcA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated"><strong class="bd ke">非线性趋势</strong>的方式和观点</figcaption></figure><ul class=""><li id="8a25" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">这些乱七八糟的趋势数据确实让我们在建模时有些困难，但这就是我们需要弄清楚的地方，我们是想要<strong class="iw hj">硬边际分类</strong>还是<strong class="iw hj">软边际分类</strong>。</li><li id="a114" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated"><strong class="iw hj">硬页边距分类</strong>规定所有数据点必须严格远离页边距，并位于页边距的右侧。</li></ul><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mb"><img src="../Images/69e2e917541d2923b5b900d3c4ac679f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*Qw9ziRXdAfx_9KVijnMlOw.jpeg"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated"><strong class="bd ke">硬保证金SVM </strong></figcaption></figure><ul class=""><li id="9b41" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">不利之处在于，这些硬边界分类器通常过拟合，并且仅当数据是线性可分的时才适用。</li><li id="86c5" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">除此之外，它们甚至对异常值<strong class="iw hj">敏感</strong>，这也导致了过度拟合的性质。</li><li id="3adb" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">我们还将讨论/比较<strong class="iw hj">硬利润和软利润</strong>。</li><li id="4993" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">为了克服硬边界分类所面临的所有挑战，为了更好地概括和有一个更可行的模型，使用了<strong class="iw hj">软边界分类</strong>。</li></ul><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mc"><img src="../Images/68b8ee46c36467f375d20fd664a2f254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*H48os45ADitrbqhKbndz_w.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated"><strong class="bd ke">软保证金SVM </strong></figcaption></figure><ul class=""><li id="0dfc" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">这里我们给出了模型，以错误分类一些数据点，这在硬保证金是禁止的。这赋予了模型以更时尚的方式拟合数据点的能力。</li><li id="bfdc" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">这里的目标是<strong class="iw hj">在保持边缘街道尽可能大和限制边缘违规之间找到一个好的平衡。</strong></li><li id="3958" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">现在，你可能会想，既然我们允许软利润模型执行一些错误分类，那么模型如何是最优的？嗯，从这个角度来想，<strong class="iw hj">你必须从A点旅行到B点，有多条路线可以到达它</strong>。路径<strong class="iw hj"> A-C-B </strong>可能比路径<strong class="iw hj"> A-D-B </strong>多3到4公里，但是这条路很难走，所以你要做的是，选择更合适的路径，这样你的旅程会变得舒适。</li><li id="d206" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">类似地，我们允许错误分类，但是我们也通过<strong class="iw hj">超参数调整</strong>和使用<strong class="iw hj">松弛变量</strong>来控制错误分类。松弛变量告诉你变量相对于<strong class="iw hj">边缘和超平面的位置。</strong></li><li id="6bed" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">让我们看看对比，继续前进！</li></ul><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es md"><img src="../Images/1fb3b84dec488b842f014d9278f5b9b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fBxmBJ4UD2I3F3srhxXByQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated"><strong class="bd ke">硬vs软</strong></figcaption></figure><p id="de9e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这对于我们来说很重要，因为只有这样，如果采访者问，你如何选择一个最好的SVM模式，我们才能引用。让我再引用一次:<strong class="iw hj">最好的SVM模型是在区分两个类的决策边界和训练实例之间具有最大余量的模型。</strong></p><p id="9831" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，我们在糖尿病预测实践中实现的SVM是硬边界分类器，因为我们有线性可分的数据点。包括这一点，你可能会奇怪，为什么我没有谈到正在SVM使用的<strong class="iw hj">内核</strong>。</p><ul class=""><li id="ba1a" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">很好地，当必须对非线性数据点使用软边界分类器时，通常会考虑核。他们需要自己一个详细的解释和实现，我将在接下来的文章中展示。但是我不会让你蒙在鼓里，所以简单地说一下，我会告诉你什么是不同种类的内核，以及它们看起来是怎样的。</li></ul><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es me"><img src="../Images/8f1c16b17b09b17149a61a162db7273c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y7ZB6xFODjcQJMwvu0FCCg.jpeg"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">SVM核</figcaption></figure></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><h1 id="d067" class="kc kd hi bd ke kf lv kh ki kj lw kl km kn lx kp kq kr ly kt ku kv lz kx ky kz bi translated">我想分享的几点:</h1><ul class=""><li id="8ad6" class="jt ju hi iw b ix la jb lb jf lc jj ld jn le jr jy jz ka kb bi translated">与逻辑回归分类器不同，SVM分类器<strong class="iw hj">不输出每个类别</strong>的概率。我的一个同事陈述了这一点，然后分享了一个链接，<strong class="iw hj">意为SVM也输出概率，但它一半是真的，一半是假的。</strong></li><li id="679a" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">为了澄清这个事情，让我这样引用:<strong class="iw hj">SVM模型输出一个测试/训练实例和一个决策边界之间的距离，这个距离可以作为一个置信度得分。但是，这个分数不能直接转换成班级概率。</strong>现在在<strong class="iw hj"> sklearn </strong>中，如果我们在创建SVM模型时设置<strong class="iw hj"><em class="lu">probability = True</em></strong>，那么在训练时，该模型使用SVM 模型上的<strong class="iw hj">逻辑回归来校准概率。这将把<strong class="iw hj"> <em class="lu"> predict_proba()和</em> </strong>方法注入到SVM。</strong></li></ul><h1 id="cb28" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">面试问题:</h1><ul class=""><li id="1a3a" class="jt ju hi iw b ix la jb lb jf lc jj ld jn le jr jy jz ka kb bi translated">为什么使用SVM时缩放输入很重要？<strong class="iw hj"> SVM试图拟合数据点和决策边界之间的最大街道/边距。因此，如果训练集没有缩放，那么它将倾向于忽略可能在一个类中非常远的小特征。</strong></li><li id="0449" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">一个<strong class="iw hj"> RBF </strong> SVM模型被训练，但是它倾向于过度拟合训练数据，我们能做些什么？如果RBF SVM模型对数据拟合不足，这将导致施加过多的限制，技术上称为正则化。所以可能会有太多的管制。现在，为了减少正则化，我们需要增加正则化超参数γ或C，或者同时增加两者。</li><li id="4d7b" class="jt ju hi iw b ix lf jb lg jf lh jj li jn lj jr jy jz ka kb bi translated">尝试加州数据集的SVM回归。<strong class="iw hj">锻炼尝试。</strong></li></ul><h2 id="314b" class="mf kd hi bd ke mg mh mi ki mj mk ml km jf mm mn kq jj mo mp ku jn mq mr ky ms bi translated">结论</h2><p id="3aea" class="pw-post-body-paragraph iu iv hi iw b ix la iz ja jb lb jd je jf mt jh ji jj mu jl jm jn mv jp jq jr hb bi translated">这就是我对这篇文章的全部看法。我知道这可能太理论化了，但是我试着用一种更技术化——非技术化的方式来表达，这样每个人都能理解SVM工作的要点，它们是什么，并且能传递相同的解释。</p><p id="df89" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">另外，在下一篇文章中，我想加入<strong class="iw hj">时尚MNIST数据集和DL模型</strong>或者<strong class="iw hj">继续ML并引入决策树/高斯朴素贝叶斯模型。所以请在评论区告诉我你对下一篇博客的看法和选择。在那之前，保持安全…保持卫生。</strong></p><p id="1ee3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">反馈始终是持续改进的输入。</p></div></div>    
</body>
</html>