<html>
<head>
<title>Adaptively changing the learning rate in conjunction with early stopping using Tensorflow 2.x</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Tensorflow 2.x结合早期停止自适应地改变学习速率</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/adaptively-changing-the-learning-rate-in-conjunction-with-early-stopping-using-tensorflow-2-x-642898323505?source=collection_archive---------9-----------------------#2020-07-17">https://medium.com/analytics-vidhya/adaptively-changing-the-learning-rate-in-conjunction-with-early-stopping-using-tensorflow-2-x-642898323505?source=collection_archive---------9-----------------------#2020-07-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="3edc" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">关于这篇文章</h1><p id="3a55" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">本文将解释如下两个。</p><ul class=""><li id="01b7" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">使用tensorflow2.x自适应地改变学习速率并结合早期停止的方法</li><li id="3150" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">引入结合早期停止的方法，自适应地改变批量大小而不是学习速率，这具有与学习速率衰减相同的效果。以及如何在tensorflow2.x中实现</li></ul></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="5e50" class="if ig hi bd ih ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc bi translated">什么是“学习率衰减”？</h1><p id="800b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">“学习率衰减”是一种常用的技术，用于提高深度学习的总体性能，其中学习率随着学习的进行而降低。</p><p id="f19b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">截至2020年7月，在我看来，学习率往往衰减到损失历史成为平台期时学习率的1/10到1/5。在某些情况下，要衰减的历元数在常用的CIFAR10和ImageNet中预先指定，因为它们的行为在某种程度上是已知的。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es lg"><img src="../Images/849063d6844ccfabed3ca3b8b7263cc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*sdnfN2DSP-zaYJVkKl7qgQ.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">取自[1]，学习率大约在32，000步左右衰减。</figcaption></figure></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="88e4" class="if ig hi bd ih ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc bi translated">我们应该何时对未知数据集进行衰减？</h1><p id="08bc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">那么未知数据的情况呢？对于CIFAR10和ImageNet，您非常了解其行为，可以使用特定的纪元编号来指定衰减点。但实际上，在大多数情况下，您应该使用行为未知的数据。在这种情况下，不可能知道学习率应该在什么时候衰减。</p><p id="6fb1" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">随着数据集、网络类型和优化方法的变化，行为会发生显著变化。(下图)在实践中测试各种实验条件时，您需要了解它们的行为，并在每种条件下分别对它们应用学习率衰减。但是手动做不太实际。</p><p id="7aa1" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">所以你需要一种机制，一旦学习收敛，比如提前停止，你可以自动降低学习速度。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es ls"><img src="../Images/5cf73ba80732610d8b21b8551d9566df.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*MaRXVtvC5XAADLxC7hbvjA.png"/></div></figure><h1 id="2f41" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">Tensorflow2.x上的提前停止+学习率衰减</h1><p id="a082" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Tensorflow 2.x推荐你这样一个train_step函数来训练网络。</p><figure class="lh li lj lk fd ll"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="23d7" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">如果我试图只改变优化器的学习率，如下所示，我们会在第23行得到一个错误。</p><figure class="lh li lj lk fd ll"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="f90b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">所以当我们改变学习率的时候，我们需要重新定义train_step函数。这样，我们可以没有任何误差地衰减学习速率。</p><figure class="lh li lj lk fd ll"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="1b9e" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">结合了提前停止和学习率衰减的训练脚本如下。</p><p id="a7eb" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">定义` _max_patience '为精度不提高的允许最大历元数，超过时学习率衰减(第58~65行)。完整的代码可以在这里找到。</p><figure class="lh li lj lk fd ll"><div class="bz dy l di"><div class="lt lu l"/></div></figure><h2 id="6c7d" class="lw ig hi bd ih lx ly lz il ma mb mc ip jo md me it js mf mg ix jw mh mi jb mj bi translated">结果</h2><p id="f00f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">训练结果如下:我用VGG16训练过CIFAR10，优化方法是SGD momentum。最初的学习率是0.001，我工作了两次，每提前停止一次，学习率就降低四分之一。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mk"><img src="../Images/00cea87ea84aa5f9b53e4d7d2d8ef117.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*keO7FIaW9YsYNr2fcdfMyA.png"/></div></figure><p id="0e3c" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">学习率的衰减发生在29，39个时期之后。30个历元的精度提升很大，但是第40个历元衰减的影响就没那么大了。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="6e43" class="if ig hi bd ih ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc bi translated">增加批量大小而不是降低学习速率</h1><p id="bc00" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">顺便说一句，增加批量而不是降低学习率，效果是一样的。根据这篇论文[2]，降低学习率提高了准确性，因为它与更新的可变性相关。即使你增加批量大小而不是降低学习率，更新的变化也在同样的规模上。因此认为可以达到相同的精度提高效果。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="er es ml"><img src="../Images/b4d6f54d9899aa904ea94336a0f82d2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G9CF3tksemOmPiO-NHreqg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">改编自[2]。学习率衰减和批量增加的比较；混合执行两者。</figcaption></figure><p id="3166" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">此外，Compressive Transformer是Transformer [3]的一种发展，据报道，使用学习率衰减会降低精度，但增加批量大小有助于提高精度。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="er es mq"><img src="../Images/7c5bbdeb8386d7d186d43ba35469fd01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-3_P4p7nyDD50uEusEgs3g.png"/></div></div></figure><p id="fa4c" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">它的实现如下。</p><p id="5074" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">第60行使用tf.data更改数据加载器的批量大小。注意，在第4行中，迭代器的创建是在创建train_step函数时重新配置的。(我不知道为什么，但如果你不在这里重新配置，你会得到一个错误。)</p><figure class="lh li lj lk fd ll"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="8e16" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">结果如下。学习率下降/批量增加都会出现准确度峰值。但是这些时间是不同的，因为我根据提前停车适应性地改变时间。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mk"><img src="../Images/0b01b1d55ebaad120efaf7c5fa586946.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*_SvSna23hi4TO7xqndXAEA.png"/></div></figure></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="37ea" class="if ig hi bd ih ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc bi translated">结论</h1><p id="6b1a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这篇文章中，我已经展示了学习率衰减的影响和批量增加的等效影响以及那些实现。我们经常遇到需要使用这种适应性方法的情况。希望这样的情况对你有所帮助。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><p id="975b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated"><strong class="jf hj">推特，我贴一句话的论文评论。</strong></p><p id="4309" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">https://twitter.com/AkiraTOSEI<a class="ae lv" href="https://twitter.com/AkiraTOSEI" rel="noopener ugc nofollow" target="_blank"/></p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="0e95" class="if ig hi bd ih ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc bi translated">参考</h1><ol class=""><li id="52cd" class="kb kc hi jf b jg jh jk jl jo mr js ms jw mt ka mu kj kk kl bi translated">何、、、任、。用于图像识别的深度残差学习。CVPR2016</li><li id="28a6" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mu kj kk kl bi translated">塞缪尔·史密斯，彼得-简·金德曼斯，克里斯·英。不要衰减学习率，增加批量。ICRL2018</li><li id="0acb" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mu kj kk kl bi translated">Jack W. Rae，Anna Potapenko，Siddhant M. Jayakumar，Timothy P. Lillicrap。</li></ol><p id="171b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">用于长程序列建模的压缩变换器。ICLR2020</p></div></div>    
</body>
</html>