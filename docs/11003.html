<html>
<head>
<title>Preventing Fatalities by playing Video Games (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过玩电子游戏预防死亡(第二部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/preventing-fatalities-by-playing-video-games-part-2-4e525dcf02e6?source=collection_archive---------22-----------------------#2020-11-12">https://medium.com/analytics-vidhya/preventing-fatalities-by-playing-video-games-part-2-4e525dcf02e6?source=collection_archive---------22-----------------------#2020-11-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7663" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">又名用虚拟世界训练自动驾驶</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/100b02c9f22cb889cb728f65b43ab260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Khv-22R9J3CNljsYW_tapg.jpeg"/></div></div></figure><p id="409c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae jp" rel="noopener" href="/@enrico.busto/preventing-fatalities-by-playing-video-games-part-1-4948d1da390e">之前的文章</a>中，我提出了训练自动驾驶系统的一些挑战以及克服这些挑战的两种方法:<strong class="ih hj">像素级自适应</strong>和<strong class="ih hj">特征级自适应</strong>，这两种方法都基于生成对抗网络。本文将解释它们是如何工作的，并展示我们的实验结果。</p><h1 id="64cf" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">像素级适应</h1><p id="40bd" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">第一种方法基于GAN的一种特殊架构，称为<strong class="ih hj">循环Gan </strong>。该模型可以从一组图像中捕获主要的风格特征，然后将它们应用于来自另一个领域的另一个图像集合。例如，您可以为模型提供两组图像。第一个包含马的照片，第二个包含斑马的照片。然后，您可以训练模型用斑马条纹给马的身体重新着色。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kt"><img src="../Images/38d773ee79721a32919deb1c17c0e82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*j5P8PSxxe4xOQggaOgPFTA@2x.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated"><a class="ae jp" href="https://www.tensorflow.org/tutorials/generative/cyclegan" rel="noopener ugc nofollow" target="_blank">来源:CycleGAN教程</a></figcaption></figure><p id="c5d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该cycle-gan架构由4个子模块组成:<strong class="ih hj">两个发生器和两个鉴别器</strong>。训练过程分为两个迭代阶段:在第一阶段，我们训练鉴别器模型，在第二阶段，训练发生器模型。</p><p id="317b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ky">训练鉴别器模型，以识别所提供的图像是否来自在</em>上对其进行训练的图片集。我们训练两个鉴别器模型，分别用于两组照片中的每一组。</p><p id="9c91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">保持前面的斑马/马的例子，第一个鉴别器必须指示照片是否包含马。另一方面，另一个鉴别器，如果它包含斑马。鉴别器模型是一个简单的卷积神经网络(CNN)。</p><p id="2a5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相反，<em class="ky">两个发生器的目标都是产生给定输入图像的新版本，该版本必须足够逼真以欺骗对手鉴别器</em>。发生器被实现为<strong class="ih hj">可变自动编码器(VAE) </strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kz"><img src="../Images/076835f2c9304b9ab033528cba0edb9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N-eQOq05ynVNSsRLwYE78w@2x.jpeg"/></div></div></figure><p id="e98f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经定义了生成器架构，我们需要定义它的行为。我们有两个主要目标。<em class="ky">第一个是忽悠鉴别者</em>。为了达到这个结果，我们使用了一个称为<strong class="ih hj">对抗性损失</strong>的损失公式:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es la"><img src="../Images/737c0a270e400faff16fce4c60a4d609.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*WW59YAy0_XoJ_vZFzeLGgg.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es la"><img src="../Images/9243b336658447933486393a1935ed3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*nl8aNyg8ZJ_qSHJwnf7yfA.png"/></div></figure><p id="2b14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ky">复制对方领域的风格是不够的</em>。第二个生成器的目标是维护原始图像的内容。<br/>当第一个生成器接收到一个图像输入并产生另一个图像作为输出时，这个新图像将被用作来自第二个生成器的输入，第二个生成器重新创建第一个生成器的原始输入。我们通过添加另一个称为<strong class="ih hj">周期损耗</strong>的损耗来获得这个结果。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es la"><img src="../Images/83270c72bc6e21cde100f21483ac0071.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Ezchf1G6vzJj_DM5VEH6BA.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lb"><img src="../Images/ce750fa9a3ed9e1501ebb67f88a8c7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*5Ih2HbMzZw2IV-SoEcOhJw@2x.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated"><a class="ae jp" href="https://towardsdatascience.com/a-gentle-introduction-to-cycle-consistent-adversarial-networks-6731c8424a87" rel="noopener" target="_blank">来源:循环一致对抗网络的温和介绍</a></figcaption></figure><p id="6395" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种损失背后的直觉是，它迫使生成器创建一个足够真实的新图像来欺骗鉴别器，同时保留来自原始输入的足够信息，以允许第二个生成器重新创建初始输入。</p><h2 id="9c2d" class="lc jr hi bd js ld le lf jw lg lh li ka iq lj lk ke iu ll lm ki iy ln lo km lp bi translated">第一次实验</h2><p id="f81a" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">我们已经创建了一个由真实图像和合成图像组成的新数据集。真实的图像来自<strong class="ih hj"> Cityscapes数据集</strong>，而其他图像来自一款名为<strong class="ih hj"> GTA V </strong>的视频游戏。<br/>然后，我们使用循环gan将真实图像样式应用到合成图像中。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/8bd9aa70474889e784d5b0aef31fb4a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*5duFj9WPtTx4iWOBDFDOLQ.png"/></div></figure><p id="f942" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于我们的实验，<strong class="ih hj">我们在GTA V数据集</strong>上训练了一个DeepLab V2模型，并且我们在<strong class="ih hj"> Cityscape测试集</strong>上进行了测试。我们称这个实验为“源”，我们得到了37，960万的分数。</p><p id="7034" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们在GTA V数据集的改编版本上重新训练了相同的网络，获得了45.57% MIoU 的分数。我们称第二个测试为“适应”。</p><p id="5d89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了量化改进，我们进行了<strong class="ih hj">最终测试</strong>。我们已经直接用cityscape数据集对模型进行了重新训练，并在cityscape测试集上进行了测试。我们获得了64.78%的结果。</p><p id="3ff5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ky">使用Cycle-gan进行域自适应，相对于仅使用来自原始域的数据，我们获得了大约20%的分类性能提升。</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lr"><img src="../Images/ff5672f2a3c6c786100e24ea51eda4be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IujfqG_QW_HBYDel5U7MIw@2x.png"/></div></div></figure><p id="8eb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看我的下一篇文章:<a class="ae jp" rel="noopener" href="/@enrico.busto/preventing-fatalities-by-playing-video-games-part-3-31761eeb2931"> <strong class="ih hj">【第三部分】</strong></a><strong class="ih hj"/>看看我们从特征级适应实验中得到的结果</p></div></div>    
</body>
</html>