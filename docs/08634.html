<html>
<head>
<title>Understanding Deep Learning requires rethinking Generalization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解深度学习需要重新思考泛化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-deep-learning-requires-rethinking-generalization-7e7759eb6035?source=collection_archive---------6-----------------------#2020-08-06">https://medium.com/analytics-vidhya/understanding-deep-learning-requires-rethinking-generalization-7e7759eb6035?source=collection_archive---------6-----------------------#2020-08-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7b4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将向您简要介绍由<strong class="ih hj">【张】</strong> ( <a class="ae jd" href="https://arxiv.org/pdf/1611.03530.pdf" rel="noopener ugc nofollow" target="_blank">链接此处</a>)撰写的论文“<strong class="ih hj">理解深度学习需要重新思考泛化</strong>”。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/5e7ac8dfdaf91a503a9e0e4d95f62667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iMamid2afdcV8GQu.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">卷积神经网络(CNN)</figcaption></figure><p id="7e70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我认为你对卷积神经网络(CNN)非常熟悉。如果没有，我建议你从维基百科上阅读。</p><p id="783a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文“<strong class="ih hj">理解深度学习需要重新思考泛化</strong>”在深度学习和机器学习研究社区引起了不小的轰动。这是一篇罕见的论文，似乎具有很高的研究价值——从在2017年<a class="ae jd" href="http://www.iclr.cc/doku.php?id=ICLR2017:main&amp;redirect=1" rel="noopener ugc nofollow" target="_blank">ICLR</a>获得三个<em class="ju">最佳论文</em>奖之一来看——但<em class="ju">也</em>可读。因此，它在<a class="ae jd" href="https://openreview.net/forum?id=Sy8gdB9xx&amp;noteId=Sy8gdB9xx" rel="noopener ugc nofollow" target="_blank"> OpenReview </a>上获得了所有ICLR 2017年提交作品中最多的评论。</p><h1 id="e161" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">关键要点</h1><ol class=""><li id="ccc9" class="kt ku hi ih b ii kv im kw iq kx iu ky iy kz jc la lb lc ld bi translated"><em class="ju">深度神经网络很容易拟合随机标签。</em></li><li id="1442" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><em class="ju">显式正则化可以提高泛化性能，但是对于控制泛化误差来说，显式正则化既不是必要的，也不是充分的。</em></li></ol><h1 id="ddc0" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">但是什么是<em class="lj">概括和概括曲线呢？</em>T25】</strong></h1><p id="1964" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated"><em class="ju">按照</em><a class="ae jd" href="https://developers.google.com/machine-learning/crash-course/generalization/video-lecture" rel="noopener ugc nofollow" target="_blank"><em class="ju">developers.google.com</em></a><em class="ju">，</em> <strong class="ih hj"> <em class="ju">“泛化是指你的模型对新的、以前未见过的数据进行适当适应的能力，从与用于创建模型的分布相同的分布中提取”</em> </strong> <em class="ju"> </em>。<em class="ju">简单来说就是<em class="ju">【训练误差】</em>和【测试误差】</em>的区别。</p><p id="329d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一条<a class="ae jd" href="https://developers.google.com/machine-learning/glossary#loss_curve" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"/></a>损失曲线，显示了<a class="ae jd" href="https://developers.google.com/machine-learning/glossary#training_set" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">训练集</strong> </a>和<a class="ae jd" href="https://developers.google.com/machine-learning/glossary#validation_set" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">验证集</strong> </a>。一般化曲线可以帮助您检测可能的<a class="ae jd" href="https://developers.google.com/machine-learning/glossary#overfitting" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">过度拟合</strong> </a>。例如，下面的泛化曲线表明过度拟合，因为验证集的损失最终会明显高于定型集。(<a class="ae jd" href="https://developers.google.com/machine-learning/glossary#generalization-curve" rel="noopener ugc nofollow" target="_blank">参见</a>)</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ln"><img src="../Images/574dcdbf87c23cdb05cd253cccb35692.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TpDNtms5TOBpQ82zzPyKhg.png"/></div></div></figure><h1 id="81af" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">但是什么是正规化呢？</h1><p id="0307" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">正则化对模型的复杂性施加惩罚，从而防止<a class="ae jd" href="https://developers.google.com/machine-learning/glossary#overfitting" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">过度拟合</strong> </a>。它确实在深度学习中扮演了一个有趣的角色。它就像一个调整参数，以减少模型的最终测试误差。(<a class="ae jd" href="https://developers.google.com/machine-learning/glossary#regularization" rel="noopener ugc nofollow" target="_blank">参见</a>)</p><p id="d6a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正则化有两种类型:</p><ol class=""><li id="10c4" class="kt ku hi ih b ii ij im in iq lo iu lp iy lq jc la lb lc ld bi translated">显式正则化(<strong class="ih hj"> <em class="ju">权重衰减</em> </strong>，<strong class="ih hj"> <em class="ju">丢失</em> </strong>和<strong class="ih hj"> <em class="ju">数据增强</em> </strong>)</li><li id="51ae" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">隐式正规化(<strong class="ih hj"> <em class="ju">提前停止，批量正规化</em> </strong> <em class="ju">和</em> <strong class="ih hj"> <em class="ju"> SGD </em> </strong>)</li></ol><blockquote class="lr ls lt"><p id="67ab" class="if ig ju ih b ii ij ik il im in io ip lu ir is it lv iv iw ix lw iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi">【深度神经网络轻松拟合随机标签】</em> </strong></p></blockquote><p id="6b56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们进行了随机化测试，其中他们在数据副本上训练了几个预训练的模型，其中真实标签被随机标签取代，令人惊讶的是，神经网络实现了0训练误差。</p><p id="5218" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们还为基于CIFAR10和ImageNet分类基准测试的几种不同的标准架构确定了这一事实，并得出结论:</p><ol class=""><li id="9b8e" class="kt ku hi ih b ii ij im in iq lo iu lp iy lq jc la lb lc ld bi translated"><em class="ju">神经网络的有效容量足以记忆整个数据集</em>。</li><li id="8531" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><em class="ju">训练时间</em>无明显变化。</li><li id="2859" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><em class="ju">随机化标签仅仅是一种数据转换，学习问题的所有其他属性保持不变</em>。</li></ol><p id="b6e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这些实验的基础上，他们还用<em class="ju">完全随机的像素</em>(例如高斯噪声)替换了真实图像，并观察到卷积神经网络继续以零训练误差拟合数据。</p><p id="8d0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这表明，尽管它们的结构不同，卷积神经网络可以适应<strong class="ih hj"> <em class="ju">随机噪声</em> </strong>。随着我们提高噪声水平，神经网络能够捕获数据中的剩余信号，同时使用蛮力拟合噪声部分。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lx"><img src="../Images/7269a49ead6946117d80e33043bf364f.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*TeYEP0hjBgIAYnpOhW5kcg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">显示了随着训练步骤衰减的各种实验设置的训练损失。</figcaption></figure><h2 id="0a0a" class="ly jw hi bd jx lz ma mb kb mc md me kf iq mf mg kj iu mh mi kn iy mj mk kr ml bi translated"><a class="ae jd" href="https://statistics.stanford.edu/events/implicit-and-explicit-regularization-deep-learning" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">显式正则化</strong> </a></h2><blockquote class="lr ls lt"><p id="9b2e" class="if ig ju ih b ii ij ik il im in io ip lu ir is it lv iv iw ix lw iz ja jb jc hb bi translated"><strong class="ih hj">“显式正则化可能会提高泛化性能，但对于控制泛化误差来说，它既不是必要的，也不是足够的。”</strong></p></blockquote><p id="8981" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正则化的显式形式，如<strong class="ih hj"> <em class="ju">权值衰减</em> </strong>、<strong class="ih hj"> <em class="ju">丢包</em> </strong>、<strong class="ih hj"> <em class="ju">数据增强</em> </strong>并不能解释神经网络的泛化误差。他们发现，正则化在深度学习中扮演着相当不同的角色。它看起来更像是一个调整参数，通常有助于改善模型的最终测试误差，但没有全部正则化并不一定意味着较差的泛化误差。</p><h2 id="c848" class="ly jw hi bd jx lz ma mb kb mc md me kf iq mf mg kj iu mh mi kn iy mj mk kr ml bi translated"><a class="ae jd" href="https://statistics.stanford.edu/events/implicit-and-explicit-regularization-deep-learning" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> <em class="lj">隐式正规化</em> </strong> </a></h2><blockquote class="lr ls lt"><p id="f4f3" class="if ig ju ih b ii ij ik il im in io ip lu ir is it lv iv iw ix lw iz ja jb jc hb bi translated">"<strong class="ih hj">对于线性模型，SGD总是收敛到具有小范数的解。因此，算法本身是隐式正则化解决方案。<em class="hi"/></strong></p></blockquote><ul class=""><li id="65c0" class="kt ku hi ih b ii ij im in iq lo iu lp iy lq jc mm lb lc ld bi translated">它只是让我们得出这样的结论:对于SGD如何在正则化它的同时影响一个模型，以及它如何影响一个模型的其他属性，还需要更多的理解。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mn"><img src="../Images/cda01a55008d0c78e2a767c3edaa09b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UUQufylYKsn1rvM5Xhq1pQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">CIFAR10数据集上各种模型的训练和测试精度(百分比)。比较了具有和不具有数据增加和重量衰减的性能。还包括拟合随机标签的结果。</figcaption></figure><p id="32f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，从我不断增长的深度学习经验来看，我发现他们的实验结果令人惊讶。也许这将是理解深度学习中泛化的一个有用的起点。</p><h2 id="66af" class="ly jw hi bd jx lz ma mb kb mc md me kf iq mf mg kj iu mh mi kn iy mj mk kr ml bi translated">感谢你的阅读，如果你喜欢这个故事，请为它鼓掌。</h2><p id="3589" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lk is it iu ll iw ix iy lm ja jb jc hb bi translated">在<strong class="ih hj">linkedIn</strong>:<a class="ae jd" href="https://www.linkedin.com/in/kalp-panwala-72284018a/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/kalp-panwala-72284018a</a>上和我联系</p><p id="b924" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<strong class="ih hj">推特</strong>:【https://twitter.com/PanwalaKalp T2】上关注我</p><p id="8529" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">跟着我上<strong class="ih hj">github:</strong><a class="ae jd" href="https://github.com/kpanwala" rel="noopener ugc nofollow" target="_blank">https://github.com/kpanwala</a></p><p id="0bc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你对这个故事有任何疑问或者任何改进的空间，那么给我发邮件到<strong class="ih hj">kpanwala33@gmail.com</strong>。</p></div></div>    
</body>
</html>