<html>
<head>
<title>Detecting Lanes using Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度神经网络检测车道</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/detecting-lanes-using-deep-neural-networks-eebf2d9e3603?source=collection_archive---------2-----------------------#2019-11-25">https://medium.com/analytics-vidhya/detecting-lanes-using-deep-neural-networks-eebf2d9e3603?source=collection_archive---------2-----------------------#2019-11-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="ff26" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">这篇文章解释了如何使用深度神经网络来检测高速公路车道。车道标志是公路上主要的静态组成部分。<strong class="il hj">他们指导车辆在公路上安全互动地行驶</strong>。车道检测也是自动驾驶中的一项重要任务，它为汽车的控制提供定位信息。它也用于<strong class="il hj"> ADAS(高级驾驶辅助系统)</strong>。</p></blockquote><p id="3076" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">对于车道检测的任务，我们有两个可用的开源数据集。一个是Tusimple数据集，另一个是CULane数据集。让我们简单看一下其中一个数据集。</p><h2 id="8965" class="jk jl hi bd jm jn jo jp jq jr js jt ju jh jv jw jx ji jy jz ka jj kb kc kd ke bi translated">t简单数据集</h2><p id="b92f" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it jh kh iw ix ji ki ja jb jj kj je jf jg hb bi translated">该数据集是作为Tusimple车道检测挑战赛的一部分发布的。它包含3626个视频剪辑，每个剪辑时长1秒。这些视频剪辑中的每一个都包含20帧，其中最后一帧被标注。这些视频是通过安装在汽车仪表板上的摄像机拍摄的。你可以在这里从<a class="ae kk" href="https://github.com/TuSimple/tusimple-benchmark/issues/3" rel="noopener ugc nofollow" target="_blank">下载数据集，或者访问</a><a class="ae kk" href="https://github.com/TuSimple/tusimple-benchmark/issues/3" rel="noopener ugc nofollow" target="_blank">https://github.com/TuSimple/tusimple-benchmark/issues/3</a>。</p><p id="301e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">目录结构如下图所示，</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es kl"><img src="../Images/60d7a12d15f109f345cd59d11d77ad58.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*GQ5X3ImxvZYVvb_YyFbxdQ.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">数据集目录结构</figcaption></figure><p id="b58c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">每个子目录包含20个连续图像，其中最后一帧带有注释。label_data_(日期)。json包含最后一帧的<strong class="il hj"> JSON格式</strong>的标签。JSON文件中的每一行都是一个包含键值的字典…</p><p id="107a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj"> raw_file: </strong>字符串类型。剪辑中的文件路径</p><p id="73d7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">车道:</strong>是车道列表的列表。每个列表对应于一个车道，并且内部列表的每个元素是地面真实车道的x坐标。</p><p id="c291" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj"> h_samples: </strong>对应车道的高度值列表。此列表中的每个元素都是地面真实车道的y坐标</p><p id="478c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">在该数据集中，最多有四条车道被标注——两条<strong class="il hj"> ego车道</strong>(车辆当前所在的两条车道边界)以及ego车道右侧和左侧的车道。所有的车道都以相等的高度间隔标注，因此h_samples只包含一个列表，其元素对应于车道中所有列表的y坐标。对于h_samples中的一个点，如果该位置没有车道，其对应的x坐标为-2。例如，JSON文件中的一行如下所示:</p><pre class="km kn ko kp fd kx ky kz la aw lb bi"><span id="2425" class="jk jl hi ky b fi lc ld l le lf">{<br/>  "lanes": [<br/>        [-2, -2, -2, -2, 632, 625, 617, 609, 601, 594, 586, 578, 570, 563, 555, 547, 539, 532, 524, 516, 508, 501, 493, 485, 477, 469, 462, 454, 446, 438, 431, 423, 415, 407, 400, 392, 384, 376, 369, 361, 353, 345, 338, 330, 322, 314, 307, 299],<br/>        [-2, -2, -2, -2, 719, 734, 748, 762, 777, 791, 805, 820, 834, 848, 863, 877, 891, 906, 920, 934, 949, 963, 978, 992, 1006, 1021, 1035, 1049, 1064, 1078, 1092, 1107, 1121, 1135, 1150, 1164, 1178, 1193, 1207, 1221, 1236, 1250, 1265, -2, -2, -2, -2, -2],<br/>        [-2, -2, -2, -2, -2, 532, 503, 474, 445, 416, 387, 358, 329, 300, 271, 241, 212, 183, 154, 125, 96, 67, 38, 9, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2],<br/>        [-2, -2, -2, 781, 822, 862, 903, 944, 984, 1025, 1066, 1107, 1147, 1188, 1229, 1269, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]<br/>       ],<br/>  "h_samples": [240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710],<br/>  "raw_file": "path_to_clip"<br/>}</span></pre><p id="9bdb" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">它说图像中有四个车道，第一个车道从(632，280)开始，第二个车道从(719，280)开始，第三个车道从(532，290)开始，第四个车道从(781，270)开始。</p><h2 id="97cd" class="jk jl hi bd jm jn jo jp jq jr js jt ju jh jv jw jx ji jy jz ka jj kb kc kd ke bi translated">数据集可视化</h2><pre class="km kn ko kp fd kx ky kz la aw lb bi"><span id="d303" class="jk jl hi ky b fi lc ld l le lf"># import required packages<br/>import json<br/>import numpy as np<br/>import cv2<br/>import matplotlib.pyplot as plt</span><span id="b13f" class="jk jl hi ky b fi lg ld l le lf"># read each line of json file<br/>json_gt = [json.loads(line) <strong class="ky hj">for</strong> line <strong class="ky hj">in</strong> open('label_data.json')]<br/>gt = json_gt[0]<br/>gt_lanes = gt['lanes']<br/>y_samples = gt['h_samples']<br/>raw_file = gt['raw_file']</span><span id="db19" class="jk jl hi ky b fi lg ld l le lf"># see the image<br/>img = cv2.imread(raw_file)<br/>cv2.imshow('image',img)<br/>cv2.WaitKey(0)<br/>cv2.destroyAllWindows()</span></pre><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/914f7e7adf1577eff40fa6c2925bef38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cMwXM1_81ya_G3O8xAi_Wg.jpeg"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated"><strong class="bd jm">图片1。</strong>来自数据集的原始图像</figcaption></figure><p id="f7c6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">现在看看图片上的JSON点可视化</p><pre class="km kn ko kp fd kx ky kz la aw lb bi"><span id="aab2" class="jk jl hi ky b fi lc ld l le lf">gt_lanes_vis = [[(x, y) <strong class="ky hj">for</strong> (x, y) <strong class="ky hj">in</strong> zip(lane, y_samples)<br/>                  <strong class="ky hj">if</strong> x &gt;= 0] <strong class="ky hj">for</strong> lane <strong class="ky hj">in</strong> gt_lanes]<br/>img_vis = img.copy()<br/><br/><strong class="ky hj">for</strong> lane <strong class="ky hj">in</strong> gt_lanes_vis:<br/>    cv2.polylines(img_vis, np.int32([lane]), isClosed=False,<br/>                   color=(0,255,0), thickness=5)</span></pre><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/1ed49420228a6e31635aa30c457456a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kXs1Cpdrbyk_ErW9VUdC6A.jpeg"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated"><strong class="bd jm">图片2。</strong>标记可视化图像</figcaption></figure><p id="a194" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">现在，我们已经理解了数据集，但是我们不能将上面的图像作为标签传递给神经网络，因为值从0到num_classes -1的<strong class="il hj"> <em class="ik">灰度图像将被传递给深度卷积神经网络，以输出包含预测车道的图像。</em> </strong>所以，我们需要为JSON文件生成标签图片。标签图像可以使用<strong class="il hj"> OpenCV </strong>通过绘制穿过JSON文件中的点的线来生成。</p><p id="1015" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">OpenCV有一个内置函数，可以通过一组点绘制多条线。这里可以使用OpenCV的折线方法。首先，使用numpy创建一个高度和宽度等于原始文件高度和宽度的全零掩码。<strong class="il hj">在训练过程中，可以缩小图像尺寸以减少计算量，但不要忘记保持相同的宽高比。</strong></p><h1 id="96dd" class="lm jl hi bd jm ln lo lp jq lq lr ls ju lt lu lv jx lw lx ly ka lz ma mb kd mc bi translated"><strong class="ak">生成标签</strong></h1><p id="d72c" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it jh kh iw ix ji ki ja jb jj kj je jf jg hb bi translated">标签应该是灰度图像。从JSON文件中为每个片段生成一个标签。首先，创建一个黑色像素的遮罩，其形状类似于JSON文件中的raw_file图像。现在，使用OpenCV的折线方法，使用JSON文件中的车道和h_samples在遮罩图像上绘制不同颜色的线(每条线对应于车道中的每条车道)。从三通道掩模图像生成灰度掩模图像，其值为类别号。同样，为JSON文件中的所有图像创建标签。您可以将图像及其标签调整为较小的尺寸，以减少计算量。</p><pre class="km kn ko kp fd kx ky kz la aw lb bi"><span id="3d87" class="jk jl hi ky b fi lc ld l le lf">mask = np.zeros_like(img)<br/>colors = [[255,0,0],[0,255,0],[0,0,255],[0,255,255]]</span><span id="b8a8" class="jk jl hi ky b fi lg ld l le lf"><strong class="ky hj">for</strong> i <strong class="ky hj">in</strong> range(len(gt_lanes_vis)):<br/>    cv2.polylines(mask, np.int32([gt_lanes_vis[i]]), isClosed=False,color=colors[i], thickness=5)</span><span id="b5b8" class="jk jl hi ky b fi lg ld l le lf"># create grey-scale label image<br/>label = np.zeros((720,1280),dtype = np.uint8)<br/>for i in range(len(colors)):<br/>   label[np.where((mask == colors[i]).all(axis = 2))] = i+1</span></pre><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/b2d27fd6958ead948dd7f7225713f857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zp9x6NEYT56JuKnI00nnFw.jpeg"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated"><strong class="bd jm">图三。</strong>生成的掩膜图像</figcaption></figure><h1 id="411f" class="lm jl hi bd jm ln lo lp jq lq lr ls ju lt lu lv jx lw lx ly ka lz ma mb kd mc bi translated">构建和训练模型</h1><p id="6654" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it jh kh iw ix ji ki ja jb jj kj je jf jg hb bi translated">车道检测本质上是一个图像分割问题。所以我在这个任务中使用了高效快速的ERFNET模型。ERFNET最初是针对语义分割问题提出的，但它也可以扩展到其他图像分割问题。你可以在这里查看它的论文。这是一个具有编码器、解码器和扩展卷积以及非瓶颈剩余层的CNN。模型架构见图1。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es md"><img src="../Images/6fc6385a5faf6b69bcf448b88579b814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F_rCNrc2NR08pc4v.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated"><strong class="bd jm">图一。</strong>模型架构</figcaption></figure><p id="eeca" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">构建并创建模型的对象。使用二进制交叉熵损失或自定义损失函数在上面创建的数据集上训练它足够数量的时期，从而最小化每像素误差。为了更好地使用内存，请创建一个数据集生成器，并在其上训练模型。生成器消除了将所有图像加载到内存中的负担(如果您的数据集很大，您应该使用生成器)，这种负担会导致所有内存耗尽，其他进程无法正常工作。图2显示了具有输入和输出维度的ERFNET的各层。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es me"><img src="../Images/5eb31261badff7f93c524174cec1874c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LAGJP4bJdY3m8qbkunX71A.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated"><strong class="bd jm">图二。</strong>具有输入和输出形状的模型层</figcaption></figure><h1 id="a7c7" class="lm jl hi bd jm ln lo lp jq lq lr ls ju lt lu lv jx lw lx ly ka lz ma mb kd mc bi translated">评估模型</h1><p id="914c" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it jh kh iw ix ji ki ja jb jj kj je jf jg hb bi translated">训练之后，使用下面的代码片段获得模型的预测。我已经在Pytorch中实现了这个。我使用color_lanes方法将模型的输出图像(两个通道的值作为类编号)转换为三个通道的图像。im_seg是图4中所示的最终叠加图像。</p><pre class="km kn ko kp fd kx ky kz la aw lb bi"><span id="f00c" class="jk jl hi ky b fi lc ld l le lf"># using pytorch</span><span id="455f" class="jk jl hi ky b fi lg ld l le lf">import torch<br/>from torchvision.transforms import ToTensor</span><span id="c12f" class="jk jl hi ky b fi lg ld l le lf">def color_lanes(image, classes, i, color, HEIGHT, WIDTH):<br/>    buffer_c1 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8)<br/>    buffer_c1[classes == i] = color[0]<br/>    image[:, :, 0] += buffer_c1</span><span id="bc0b" class="jk jl hi ky b fi lg ld l le lf">    buffer_c2 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8)<br/>    buffer_c2[classes == i] = color[1]<br/>    image[:, :, 1] += buffer_c2</span><span id="aace" class="jk jl hi ky b fi lg ld l le lf">    buffer_c3 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8)<br/>    buffer_c3[classes == i] = color[2]<br/>    image[:, :, 2] += buffer_c3<br/>    return image</span><span id="5176" class="jk jl hi ky b fi lg ld l le lf">img = cv2.imread('images/test.jpg') <br/>img = cv2.resize(img,(WIDTH, HEIGHT),interpolation = cv2.INETR_CUBIC)<br/>op_transforms = transforms.Compose([transforms.ToTensor()])<br/>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/>im_tensor = torch.unsqueeze(op_transforms(img), dim=0)<br/>im_tensor = im_tensor.to(device)</span><span id="cdc7" class="jk jl hi ky b fi lg ld l le lf">model = ERFNET(5)<br/>model = model.to(device)<br/>model = model.eval()<br/>out = model(im_tensor)<br/>out = out.max(dim=1)[1]<br/>out_np = out.cpu().numpy()[0]</span><span id="d953" class="jk jl hi ky b fi lg ld l le lf">out_viz = np.zeros((HEIGHT, WIDTH, 3))<br/>for i in range(1, NUM_LD_CLASSES):<br/>    rand_c1 = random.randint(1, 255)<br/>    rand_c2 = random.randint(1, 255)<br/>    rand_c3 = random.randint(1, 255)<br/>    out_viz = color_lanes(<br/>            out_viz, out_np,<br/>            i, (rand_c1, rand_c2, rand_c3), HEIGHT, WIDTH)<br/>instance_im = out_viz.astype(np.uint8)<br/>im_seg = cv2.addWeighted(img, 1, instance_im, 1, 0)</span></pre><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/4d5118d99d8d50e3d2180f2bb30660d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b3HiwPg69Rw0L5iIMQMXOw.jpeg"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated"><strong class="bd jm">形象4。</strong>最终预测图像</figcaption></figure><p id="9c4a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">感谢阅读…</p><h2 id="9220" class="jk jl hi bd jm jn jo jp jq jr js jt ju jh jv jw jx ji jy jz ka jj kb kc kd ke bi translated">参考</h2><ol class=""><li id="9f56" class="mf mg hi il b im kf iq kg jh mh ji mi jj mj jg mk ml mm mn bi translated">ERFNet:用于实时语义分割的有效残差分解ConvNet。</li><li id="50a3" class="mf mg hi il b im mo iq mp jh mq ji mr jj ms jg mk ml mm mn bi translated">基于细胞神经网络的车道检测和分类。</li><li id="63e4" class="mf mg hi il b im mo iq mp jh mq ji mr jj ms jg mk ml mm mn bi translated"><a class="ae kk" href="https://www.mdpi.com/sensors/sensors-19-00503/article_deploy/html/images/sensors-19-00503-g004.png" rel="noopener ugc nofollow" target="_blank">https://www . mdpi . com/sensors/sensors-19-00503/article _ deploy/html/images/sensors-19-00503-g004 . png</a></li></ol></div></div>    
</body>
</html>