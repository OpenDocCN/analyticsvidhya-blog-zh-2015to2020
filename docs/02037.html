<html>
<head>
<title>How to Convert Your Keras Model to ONNX</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将Keras模型转换为ONNX</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-convert-your-keras-model-to-onnx-8d8b092c4e4f?source=collection_archive---------0-----------------------#2019-11-27">https://medium.com/analytics-vidhya/how-to-convert-your-keras-model-to-onnx-8d8b092c4e4f?source=collection_archive---------0-----------------------#2019-11-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6f9d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">将Keras模型转换为ONNX格式，并提供一些说明</h2></div><blockquote class="ix iy iz"><p id="9999" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">被困在付费墙后面？点击<a class="ae jx" rel="noopener" href="/@clh0524/how-to-convert-your-keras-model-to-onnx-8d8b092c4e4f?source=friends_link&amp;sk=75278939091222853d4949fcf5bf2899">该好友链接</a>进入:)</p></blockquote><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es jy"><img src="../Images/d324547e180a0cbd017336237ff3240b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wwnExqe720PPHykHhs5Hqw.png"/></div></div></figure><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es kk"><img src="../Images/351f1d27c6506b3e8b4facee859e1b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g9utLOtCDXciZ1o3C5bcOA.png"/></div></div></figure><h1 id="775a" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">直觉</h1><p id="3a05" class="pw-post-body-paragraph ja jb hi jd b je ld ij jg jh le im jj lf lg jm jn lh li jq jr lj lk ju jv jw hb bi translated">我喜欢Keras的简单。用10分钟左右的时间，我就可以用优雅的代码，用它的顺序或函数式API，建立一个深度学习模型。然而，Keras总是以非常慢的速度加载它的模型。此外，我不得不使用另一个深度学习框架，因为系统约束或者仅仅是我的老板告诉我使用那个框架。虽然我可以用其他人的脚本将我的Keras模型转换到其他框架，但我仍然将我的模型转换到ONNX，以尝试它在AI工具中声称的互操作性。</p><h1 id="596a" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">ONNX是什么？</h1><p id="3f62" class="pw-post-body-paragraph ja jb hi jd b je ld ij jg jh le im jj lf lg jm jn lh li jq jr lj lk ju jv jw hb bi translated"><a class="ae jx" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>是“开放式神经网络交换”的缩写。ONNX的目标是成为一种表示深度学习模型的开放格式，以便我们可以轻松地在框架之间移动模型，它是由脸书和微软创建的。</p><h1 id="b018" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">将Keras模型转换为ONNX</h1><ol class=""><li id="3d71" class="ll lm hi jd b je ld jh le lf ln lh lo lj lp jw lq lr ls lt bi translated">从<a class="ae jx" href="https://github.com/Cuda-Chen/keras2onnx-example" rel="noopener ugc nofollow" target="_blank"> my GitHub </a>下载示例代码</li><li id="3606" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw lq lr ls lt bi translated">点击从<a class="ae jx" href="https://drive.google.com/file/d/1ouJ8xZzi6x2cEkojS3DC1Wy77zjBGP1c/view" rel="noopener ugc nofollow" target="_blank">下载预训练重量</a></li><li id="d3f0" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw lq lr ls lt bi translated">键入以下命令进行设置</li></ol><pre class="jz ka kb kc fd lz ma mb mc aw md bi"><span id="aa4b" class="me km hi ma b fi mf mg l mh mi">$ conda create -n keras2onnx-example python=3.6 pip<br/>$ conda activate keras2onnx-example<br/>$ pip install -r requirements.txt</span></pre><p id="cfc5" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">4.运行此命令将预训练的Keras模型转换为ONNX</p><pre class="jz ka kb kc fd lz ma mb mc aw md bi"><span id="6e74" class="me km hi ma b fi mf mg l mh mi">$ python convert_keras_to_onnx.py</span></pre><p id="484a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">5.使用ONNX运行时运行此命令进行推理</p><pre class="jz ka kb kc fd lz ma mb mc aw md bi"><span id="4f51" class="me km hi ma b fi mf mg l mh mi">$ python main.py 3_001_0.bmp</span></pre><p id="a0b3" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">它最终应该输出以下消息:</p><pre class="jz ka kb kc fd lz ma mb mc aw md bi"><span id="04d3" class="me km hi ma b fi mf mg l mh mi">...<br/>3_001_0.bmp<br/>    1.000  3<br/>    0.000  37<br/>    0.000  42<br/>    0.000  14<br/>    0.000  17</span></pre><h1 id="8259" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">一些解释</h1><p id="0fd7" class="pw-post-body-paragraph ja jb hi jd b je ld ij jg jh le im jj lf lg jm jn lh li jq jr lj lk ju jv jw hb bi translated"><code class="du mj mk ml ma b">convert_keras_to_onnx.py</code>将Keras <code class="du mj mk ml ma b">.h5</code>模型转换为ONNX格式，即<code class="du mj mk ml ma b">.onnx</code>。它的代码如下所示:</p><figure class="jz ka kb kc fd kd"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="f89a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">将Keras模型转换为ONNX有几点:</p><ol class=""><li id="26be" class="ll lm hi jd b je jf jh ji lf mo lh mp lj mq jw lq lr ls lt bi translated">记得导入<code class="du mj mk ml ma b">onnx</code>和<code class="du mj mk ml ma b">keras2onnx</code>包。</li><li id="957d" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw lq lr ls lt bi translated"><code class="du mj mk ml ma b">keras2onnx.convert_keras()</code>函数将keras模型转换为ONNX对象。</li><li id="9658" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw lq lr ls lt bi translated"><code class="du mj mk ml ma b">onnx.save_model()</code>功能是将ONNX对象保存到<code class="du mj mk ml ma b">.onnx</code>文件中。</li></ol><p id="3c5b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated"><code class="du mj mk ml ma b">main.py</code>利用ONNX模型推断鱼类图像。我把代码粘贴到这里:</p><figure class="jz ka kb kc fd kd"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="3b05" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">推理时有一些要点:</p><ol class=""><li id="7110" class="ll lm hi jd b je jf jh ji lf mo lh mp lj mq jw lq lr ls lt bi translated">记得导入<code class="du mj mk ml ma b">onnxruntime</code>包。</li><li id="d780" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw lq lr ls lt bi translated"><code class="du mj mk ml ma b">onnxruntime.InferenceSession()</code>函数加载ONNX模型。</li><li id="c5a6" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw lq lr ls lt bi translated">第34行的<code class="du mj mk ml ma b">run()</code>函数预测图像并返回预测结果。此外，<code class="du mj mk ml ma b">sess.run(None, feed)[0]</code>将第0个元素作为numpy矩阵返回。</li><li id="1ebe" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw lq lr ls lt bi translated"><code class="du mj mk ml ma b">np.squeeze(pred_onnx)</code>将numpy矩阵压缩成numpy向量，即去掉第0维，这样就可以得到每一类的概率。</li></ol><h1 id="24d5" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">推理时间</h1><h2 id="97b1" class="me km hi bd kn mr ms mt kr mu mv mw kv lf mx my kx lh mz na kz lj nb nc lb nd bi translated">总推理时间(负载模型+推理)</h2><p id="a5e5" class="pw-post-body-paragraph ja jb hi jd b je ld ij jg jh le im jj lf lg jm jn lh li jq jr lj lk ju jv jw hb bi translated">一些报告称ONNX在模式加载时间和推理时间上都运行得更快。因此，本节我在笔记本电脑上做了一个推理时间实验。</p><p id="46b7" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">该实验的硬件如下所示:</p><ul class=""><li id="071b" class="ll lm hi jd b je jf jh ji lf mo lh mp lj mq jw ne lr ls lt bi translated">CPU:酷睿i5–3230m</li><li id="adb2" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw ne lr ls lt bi translated">内存:16GB</li></ul><p id="ac04" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">软件和软件包如下所示:</p><ul class=""><li id="226c" class="ll lm hi jd b je jf jh ji lf mo lh mp lj mq jw ne lr ls lt bi translated">操作系统:CentOS 7.6</li><li id="301e" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw ne lr ls lt bi translated">编程语言:Python 3.6</li><li id="9de1" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw ne lr ls lt bi translated">包装</li><li id="ca75" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw ne lr ls lt bi translated">keras版本2.2.4</li><li id="41bf" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw ne lr ls lt bi translated">tensorflow版本1.13.1</li><li id="f42b" class="ll lm hi jd b je lu jh lv lf lw lh lx lj ly jw ne lr ls lt bi translated">onnxruntime</li></ul><p id="5b18" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">每个推理运行三次，以消除其他因素的误差，例如上下文切换。</p><p id="530c" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">对于使用Keras的推理，我的计算机运行时会产生以下结果:</p><pre class="jz ka kb kc fd lz ma mb mc aw md bi"><span id="ed35" class="me km hi ma b fi mf mg l mh mi">$ time python resnet50_predict.py 3_001_0.bmp<br/># run for three times<br/>...<br/>real    0m37.801s<br/>user    0m37.254s<br/>sys 0m1.590s<br/>...<br/>real    0m35.558s<br/>user    0m35.838s<br/>sys 0m1.362s<br/>...<br/>real    0m36.444s<br/>user    0m36.542s<br/>sys 0m1.418s</span></pre><p id="0d1f" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">推理时间约为(37.081+35.58+36.444)/3 = 36.37秒(四舍五入到小数点后第二位)。</p><p id="1656" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">相反，ONNX运行时的推理显示:</p><pre class="jz ka kb kc fd lz ma mb mc aw md bi"><span id="846d" class="me km hi ma b fi mf mg l mh mi">$ time python main.py<br/># run three times<br/>...<br/>real    0m2.576s<br/>user    0m2.919s<br/>sys 0m0.759s<br/>...<br/>real    0m2.530s<br/>user    0m2.931s<br/>sys 0m0.700s<br/>...<br/>real    0m2.560s<br/>user    0m2.944s<br/>sys 0m0.710s</span></pre><p id="da02" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">哇哦。多么巨大的进步啊！推理时间约为(2.576+2.530+2.560)/3 = 2.56秒。</p><blockquote class="ix iy iz"><p id="a9bc" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><em class="hi">用Keras推断的代码可以在我的GitHub repo </em>  <em class="hi">上找到</em> <a class="ae jx" href="https://github.com/Cuda-Chen/fish-classifier/tree/master/cnn" rel="noopener ugc nofollow" target="_blank"> <em class="hi">。</em></a></p></blockquote><h2 id="29a6" class="me km hi bd kn mr ms mt kr mu mv mw kv lf mx my kx lh mz na kz lj nb nc lb nd bi translated">推理时间(仅推理)</h2><blockquote class="ix iy iz"><p id="4116" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">编辑:我的一个朋友说我应该只测试Keras和ONNX之间的推理时间，因为我们实际上只加载一次模型。因此，我将只测试Keras和ONNX之间的推理时间，并将它分成两部分:</p><p id="79c7" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><em class="hi"> 1。keras(tensor flow由</em> <code class="du mj mk ml ma b"><em class="hi">pip</em></code> <em class="hi">安装)v.s. ONNX </em></p><p id="b27e" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><em class="hi"> 2。Keras(装有TensorFlow由</em> <code class="du mj mk ml ma b"><em class="hi">conda</em></code> <em class="hi">安装)v.s. ONNX </em></p></blockquote><p id="fcd4" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">当然，我写<code class="du mj mk ml ma b">comparison.py</code>是为了做对比测试，如下图所示:</p><figure class="jz ka kb kc fd kd"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="52d1" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">你可以看到我运行每种推理方法10次，取平均时间，我运行<code class="du mj mk ml ma b">comparison.py</code>三次以减少错误。</p><h2 id="673e" class="me km hi bd kn mr ms mt kr mu mv mw kv lf mx my kx lh mz na kz lj nb nc lb nd bi translated">keras(tensor flow由<code class="du mj mk ml ma b">pip</code>安装)v.s. ONNX</h2><p id="1ccb" class="pw-post-body-paragraph ja jb hi jd b je ld ij jg jh le im jj lf lg jm jn lh li jq jr lj lk ju jv jw hb bi translated">对比如下所示:</p><pre class="jz ka kb kc fd lz ma mb mc aw md bi"><span id="f011" class="me km hi ma b fi mf mg l mh mi">$ python comparison.py<br/>...<br/>Keras inferences with 0.8759469270706177 second in average<br/>ONNX inferences with 0.3100883007049561 second in average<br/>...<br/>Keras inferences with 0.8891681671142578 second in average<br/>ONNX inferences with 0.313812255859375 second in average<br/>...<br/>Keras inferences with 0.9052883148193359 second in average<br/>ONNX inferences with 0.3306725025177002 second in average</span></pre><p id="4938" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">我们发现Keras推理需要(0.88+0.87+0.91)/3 = 0.87秒，而ONNX推理需要(0.31+0.31+0.33)/3 = 0.32秒。ONNX和Keras之间的加速比为0.87/0.32 = 2.72倍。</p><h2 id="2796" class="me km hi bd kn mr ms mt kr mu mv mw kv lf mx my kx lh mz na kz lj nb nc lb nd bi translated">keras(tensor flow由<code class="du mj mk ml ma b">conda</code>安装)v.s. ONNX</h2><p id="e330" class="pw-post-body-paragraph ja jb hi jd b je ld ij jg jh le im jj lf lg jm jn lh li jq jr lj lk ju jv jw hb bi translated">等一下！<code class="du mj mk ml ma b">pip install tensorflow</code>安装TensorFlow，不优化英特尔处理器。所以让我们先移除TensorFlow，然后通过<code class="du mj mk ml ma b">conda</code>安装它(我安装的版本是<code class="du mj mk ml ma b">1.13.1</code>)。</p><p id="aeba" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">然后再次运行<code class="du mj mk ml ma b">comparison.py</code>:</p><pre class="jz ka kb kc fd lz ma mb mc aw md bi"><span id="1bc8" class="me km hi ma b fi mf mg l mh mi">$ python comparison.py<br/>...<br/>Keras inferences with 0.9810404300689697 second in average<br/>ONNX inferences with 0.604683232307434 second in average<br/>...<br/>Keras inferences with 0.8862279415130615 second in average<br/>ONNX inferences with 0.6059059381484986 second in average<br/>...<br/>Keras inferences with 0.9496192932128906 second in average<br/>ONNX inferences with 0.5927849292755127 second in average</span></pre><p id="874e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj lf jl jm jn lh jp jq jr lj jt ju jv jw hb bi translated">我们发现Keras的推理时间为(0.98+0.89+0.95)/3 = 0.94秒。与ONNX相比，它的推理时间为(0.60+0.61+0.59)/3 = 0.6秒。这是0.94/0.6 = 1.57倍的加速。有趣的是，通过<code class="du mj mk ml ma b">conda</code>安装TensorFlow后，Keras和ONNX都变得更慢。</p><h1 id="cab9" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">结论</h1><p id="5dab" class="pw-post-body-paragraph ja jb hi jd b je ld ij jg jh le im jj lf lg jm jn lh li jq jr lj lk ju jv jw hb bi translated">在这篇文章中，我将介绍ONNX，并展示如何将Keras模型转换为ONNX模型。我还演示了如何使用ONNX模型进行预测。希望你喜欢这篇文章！</p></div><div class="ab cl nf ng gp nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="hb hc hd he hf"><blockquote class="ix iy iz"><p id="d5eb" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">如果你有什么想法和问题要分享，请联系我<a class="ae jx" href="http://clh960524@gmail.com" rel="noopener ugc nofollow" target="_blank"><strong class="jd hj">clh 960524【at】Gmail . com</strong></a>。另外，你可以查看我的<a class="ae jx" href="https://github.com/Cuda-Chen" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中的其他作品。如果你像我一样对机器学习、图像处理和并行计算充满热情，请随时<a class="ae jx" href="https://www.linkedin.com/in/lu-hsuan-chen-78071b171/" rel="noopener ugc nofollow" target="_blank">在LinkedIn </a>上添加我。</p></blockquote></div></div>    
</body>
</html>