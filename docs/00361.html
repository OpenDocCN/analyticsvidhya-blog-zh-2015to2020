<html>
<head>
<title>Logistic Regression as a Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">作为神经网络的逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-as-a-neural-network-b5d2a1bd696f?source=collection_archive---------0-----------------------#2019-04-25">https://medium.com/analytics-vidhya/logistic-regression-as-a-neural-network-b5d2a1bd696f?source=collection_archive---------0-----------------------#2019-04-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/d09ebe75d44a7322a038e7797a2cefd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yiZzfiMW4CanEKAiME4xGw.png"/></div></div></figure><div class=""/><p id="4e24" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">逻辑回归是一种统计方法，用于因变量或输出为分类变量时的预测。当我们想知道一个特定的数据点是属于0类还是1类时，就要用到它。在逻辑回归中，我们需要找到<em class="jo">在给定输入向量<strong class="is hu"> <em class="jo"> x </em> </strong> <em class="jo">的情况下，输出为<em class="jo"> y=1 </em>的概率</em>。</em> <strong class="is hu"> <em class="jo"> y' </em> </strong>是输入为<strong class="is hu"> <em class="jo"> x </em> </strong>时的预测值。数学上它可以定义为:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es jp"><img src="../Images/3702dc2413513c740f2f2750316dbdcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*spAvH3aIC_FPYPwHrmYP4Q.png"/></div></figure><h1 id="89d3" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">数学模型</h1><p id="aa61" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated"><strong class="is hu">输入</strong>:<strong class="is hu"><em class="jo">X</em></strong><em class="jo"/>是输入矩阵的维数<em class="jo"/><strong class="is hu"><em class="jo">n</em></strong><em class="jo"/>X<strong class="is hu"><em class="jo">m</em></strong><em class="jo"/>其中<em class="jo"> n </em>是<strong class="is hu"> <em class="jo"> X </em> </strong> <em class="jo"> </em>和<strong class="is hu">中的特征数</strong></p><p id="dc29" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">参数</strong>:<strong class="is hu">:<em class="jo">W</em></strong><em class="jo"/>是维度<strong class="is hu"><em class="jo">n</em></strong>X<strong class="is hu"><em class="jo">1</em></strong>其中<em class="jo"> n </em>是<strong class="is hu"> <em class="jo"> X </em> </strong>中特征的个数。偏置<strong class="is hu"> <em class="jo"> b </em> </strong>有助于控制激活功能触发的值。</p><p id="7db6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">输出</strong>:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es kx"><img src="../Images/618676adc985f0bf0c33d24442196533.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*5HNOJAb3_XfO9uw0buDJgw.png"/></div></figure><h1 id="dd09" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">激活功能</h1><p id="58e2" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">激活函数对于人工神经网络学习和理解一些非常复杂的东西非常重要。它们将非线性特性引入网络。它们的主要目的是将节点的输入信号转换成输出信号。该输出信号现在被用作神经网络下一层的输入。上面使用的激活是<strong class="is hu"><em class="jo">s形</em> </strong> <em class="jo"> </em>激活功能。数学上，它可以定义为:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es ky"><img src="../Images/38341ebd7205789adda2246598a3f0f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*s5aP_gj-4Yp4M43rdJOldw.png"/></div></figure><h1 id="b61c" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">损失函数</h1><p id="dc34" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">损耗可以定义为实际输出和预测输出之间存在的误差。我们希望损失函数值尽可能低，因为它会减少损失，并且预测值会接近实际值。我们用来训练神经网络的损失函数因情况而异。因此，为我们的用例选择一个适当的损失函数是很重要的，以便神经网络被适当地训练。我们将用于逻辑回归的损失函数可以在数学上定义为:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es kz"><img src="../Images/3d6fa4459f7949be411caf9b49e62468.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*qslSnKVxP9dFSzRPLY633g.png"/></div></figure><p id="3603" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们研究一下为什么这个损失函数对逻辑回归有好处，</p><ol class=""><li id="f71f" class="la lb ht is b it iu ix iy jb lc jf ld jj le jn lf lg lh li bi translated">当<strong class="is hu"> <em class="jo"> y=1 </em> </strong> <em class="jo"> </em>时，损失函数等于<strong class="is hu"> <em class="jo"> L(y '，y) = -log y' </em> </strong> <em class="jo">。</em>由于我们希望损失函数值越小，<strong class="is hu"> <em class="jo"> log y' </em> </strong>的值应该越大，<strong class="is hu"> <em class="jo"> </em> </strong>的值应该越大，当<strong class="is hu"><em class="jo">【y '</em></strong>越大，即越接近1时，预测值和实际值就越接近。</li><li id="168e" class="la lb ht is b it lj ix lk jb ll jf lm jj ln jn lf lg lh li bi translated">当<strong class="is hu"> <em class="jo"> y=0 </em> </strong>时，损失函数等于<strong class="is hu"> <em class="jo"> L(y '，y) = -log(1-y') </em> </strong>。由于我们希望损失函数值更小，<strong class="is hu"><em class="jo">log(1-y’)</em></strong>的值应该更大，当<strong class="is hu"><em class="jo">y’</em></strong>越小，即接近0时，预测值和实际值就越接近。</li><li id="5a21" class="la lb ht is b it lj ix lk jb ll jf lm jj ln jn lf lg lh li bi translated">上述损失函数是凸的，这意味着它具有单个全局最小值，并且网络不会陷入非凸损失函数中存在的局部最小值。</li></ol><h1 id="bd37" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">价值函数</h1><p id="d1dd" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">损失函数用于训练过程中的每个输入训练示例，而成本函数用于一次迭代中的整个训练数据集。因此，基本上，成本函数是整个训练数据集上所有损失值的平均值。数学上它可以定义为:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lo"><img src="../Images/0431830ee602cd706fac5936894d9fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*dQtxnC4z3sCy-_jykwkIoQ.png"/></div></figure><p id="0c47" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上式中，<strong class="is hu"> <em class="jo"> m </em> </strong>是训练样本的总数。训练网络的目标是找到权重矩阵<strong class="is hu"> <em class="jo"> W </em> </strong>和偏差<strong class="is hu"> <em class="jo"> b </em> </strong>，使得成本函数<strong class="is hu"> <em class="jo"> J </em> </strong>的值最小。</p><h1 id="d068" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">梯度下降</h1><p id="ee10" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">权重矩阵<strong class="is hu"> <em class="jo"> W </em> </strong>被随机初始化。我们用梯度下降法最小化代价函数<strong class="is hu"> <em class="jo"> J </em> </strong>得到最优权重矩阵<strong class="is hu"> <em class="jo"> W </em> </strong>和Bias <strong class="is hu"> <em class="jo"> b </em> </strong>。梯度下降是一种寻找函数最小值的一阶迭代优化算法。我们将梯度下降应用于成本函数<strong class="is hu"> <em class="jo"> J </em> </strong>以最小化成本。数学上它可以定义为:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lp"><img src="../Images/ecb52505107f30c0ef0a9c45d0b996cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*twEt0A_YOnydoTN1chAOow.png"/></div></figure><p id="dafb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一个等式表示权重矩阵<strong class="is hu"><em class="jo"/></strong>的变化，而第二个等式表示偏差<strong class="is hu"> <em class="jo"> b </em> </strong>的变化。值的变化由学习率<strong class="is hu"><em class="jo">α</em></strong>和成本<strong class="is hu"> <em class="jo"> J </em> </strong>相对于权重矩阵<strong class="is hu"> <em class="jo"> W </em> </strong>和偏差<strong class="is hu"> <em class="jo"> b </em> </strong>的导数确定。我们重复<strong class="is hu"> <em class="jo"> W </em> </strong>和<strong class="is hu"> <em class="jo"> b </em> </strong>的更新，直到代价函数<strong class="is hu"> <em class="jo"> J </em> </strong>被最小化。现在让我们借助下图了解梯度下降的工作原理:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lq"><img src="../Images/7809711522c28047ef21ecb13b8045b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*347jeF4Qvh-qKLY2KGhmWw.png"/></div></figure><p id="09a1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">案例一</strong>。假设<strong class="is hu"> <em class="jo"> W </em> </strong>被初始化为小于达到全局最小值的值，那么在该点的斜率，即<strong class="is hu"> <em class="jo"> J </em> </strong>相对于<strong class="is hu"> <em class="jo"> W </em> </strong>的偏导数是负的，因此，根据梯度下降方程，权重值将<strong class="is hu"> <em class="jo">增加</em> </strong>。</p><p id="de82" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">案例二</strong>。假设<strong class="is hu"> <em class="jo"> W </em> </strong>被初始化为大于其达到全局最小值的值，则该点的斜率，即<strong class="is hu"> <em class="jo"> J </em> </strong>相对于<strong class="is hu"><em class="jo"/></strong>W的偏导数为正，因此，根据梯度下降方程，权重值将<strong class="is hu"> <em class="jo">减小</em> </strong>。</p><p id="1209" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">相应地，<strong class="is hu"> <em class="jo"> W </em> </strong>和<strong class="is hu"> <em class="jo"> b </em> </strong>都将达到它们的最优值，并且成本函数<strong class="is hu"> <em class="jo"> J </em> </strong>的值将被最小化。</p><h1 id="a231" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">使用梯度下降的逻辑回归</h1><p id="f766" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">到目前为止，我们已经了解了逻辑回归和梯度下降的数学模型。在本节中，我们将了解如何在逻辑回归环境中使用梯度下降来学习权重矩阵<strong class="is hu"> <em class="jo"> W </em> </strong>和偏差<strong class="is hu"> <em class="jo"> b </em> </strong>。让我们总结一下目前已知的所有方程。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/5cdba3bf0d9d124ac1cefd889c44fd89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*3RjHtr2jj0X1IcTHIAnoZA.png"/></div></div></figure><ol class=""><li id="8792" class="la lb ht is b it iu ix iy jb lc jf ld jj le jn lf lg lh li bi translated">第一个等式表示输入<strong class="is hu"> <em class="jo"> X </em> </strong>与权重矩阵<strong class="is hu"> <em class="jo"> W </em> </strong>和偏差<strong class="is hu"> <em class="jo"> b </em> </strong>的乘积。</li><li id="f970" class="la lb ht is b it lj ix lk jb ll jf lm jj ln jn lf lg lh li bi translated">第二个方程是引入非线性的<strong class="is hu"><em class="jo"/></strong>激活函数。</li><li id="1bd4" class="la lb ht is b it lj ix lk jb ll jf lm jj ln jn lf lg lh li bi translated">第三个等式是损失函数，其计算给定的<strong class="is hu"> <em class="jo"> Y </em> </strong>和预测的<strong class="is hu"><em class="jo">Y’</em></strong>之间的损失。</li></ol><p id="8f85" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些方程可以使用称为<strong class="is hu"> <em class="jo">计算图</em> </strong>的图来建模。为了简单起见，让我们考虑在给定的输入矩阵<strong class="is hu"> <em class="jo"> X </em> </strong>中有两个特征<strong class="is hu"> <em class="jo"> x1 </em> </strong>和<strong class="is hu"> <em class="jo"> x2 </em> </strong>。相应地在权重矩阵<strong class="is hu"><em class="jo">【W</em></strong>中会有2个权重<strong class="is hu"> <em class="jo"> w1 </em> </strong>和<strong class="is hu"> <em class="jo"> w2 </em> </strong>。上述场景的计算图可以定义为:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ls"><img src="../Images/c67dff02544ef13d1e8e6d41cc4481e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WEldn-B-mVrZJKLkOzFqCg.jpeg"/></div></div></figure><p id="68ee" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上图中，前向传播(黑色箭头)用于预测<strong class="is hu"><em class="jo"/></strong>，后向传播(红色箭头)用于更新权重<strong class="is hu"> <em class="jo"> w1 </em> </strong>和<strong class="is hu"> <em class="jo"> w2 </em> </strong>和偏差<strong class="is hu"> <em class="jo"> b </em> </strong>。正如我们在梯度下降中看到的，我们需要计算权重和偏差的导数来更新它们。使用计算图可以很容易地计算这些导数。由于损失<strong class="is hu"> <em class="jo"> L </em> </strong>，<strong class="is hu"> <em class="jo"> </em> </strong>依赖于<strong class="is hu"><em class="jo"/></strong>，首先<strong class="is hu"> <em class="jo"> </em> </strong>我们计算导数<strong class="is hu"><em class="jo">【da】</em></strong>即代表导数的<strong class="is hu"> <em class="jo"> L </em> </strong>相对于</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lt"><img src="../Images/b5fc6e4a3e5f5ca8317db949f90d37f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*OZxQ4LzQDC8MtKqg3csq1w.png"/></div></figure><p id="f3ac" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">之后，我们使用等式2计算代表<strong class="is hu"><em class="jo"/></strong>相对于<strong class="is hu"> <em class="jo"> z </em> </strong>的导数<strong class="is hu"> <em class="jo"> dz </em> </strong>。这可以使用链规则来完成，如下所示:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lu"><img src="../Images/51ab93361ccaa05596f578f94cb7917f.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*VHg-Sbpra7p_nNkfSpvwUg.png"/></div></figure><p id="c085" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">同样，我们可以利用等式1和链式法则求出所有导数<strong class="is hu"> <em class="jo"> dw1 </em> </strong>、<strong class="is hu"> <em class="jo"> dw2 </em> </strong>和<strong class="is hu"> <em class="jo"> db </em> </strong>。这些衍生产品的价值如下:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lv"><img src="../Images/de3572fafd81a2878a68035536e50c3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*kdsucuu23HsKX1nrAGYTDg.png"/></div></figure><p id="20dd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们已经找到了所有的导数，我们需要借助梯度下降来更新权重和偏差值，如下所示:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lw"><img src="../Images/b069d03ccb64db50e97f3532c1f02919.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*HpzhndOA_0Gmsnc229SfVA.png"/></div></figure><p id="2177" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">据此，我们将梯度下降应用于逻辑回归，并研究如何更新权重矩阵<strong class="is hu"><em class="jo">【W】</em></strong>和偏差<strong class="is hu"> <em class="jo"> b </em> </strong>以使损失<strong class="is hu"> <em class="jo"> L </em> </strong>最小化。</p><blockquote class="lx ly lz"><p id="5413" class="iq ir jo is b it iu iv iw ix iy iz ja ma jc jd je mb jg jh ji mc jk jl jm jn hb bi translated">我建议读者，如果他们有微积分的知识，他们应该做数学，自己算出上述方程，以便正确理解这个话题。</p></blockquote><p id="3e64" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意，上述所有计算都是针对单个训练示例实现的，但是在训练过程中，我们必须在单次迭代中将上述步骤应用于所有训练示例。让我们想想该怎么做。我们知道成本函数<strong class="is hu"> <em class="jo"> J </em> </strong>在数学上定义如下:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lo"><img src="../Images/0431830ee602cd706fac5936894d9fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*dQtxnC4z3sCy-_jykwkIoQ.png"/></div></figure><p id="e5ed" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在培训过程中，我们需要最小化成本<strong class="is hu"> <em class="jo"> J </em> </strong>。这可以通过最小化所有<strong class="is hu"> <em class="jo"> m </em> </strong>训练示例中的每个训练示例<strong class="is hu"> <em class="jo"> i </em> </strong>的损失<strong class="is hu"> <em class="jo"> L </em> </strong>来实现。由于损失函数<strong class="is hu"> <em class="jo"> L </em> </strong>本身取决于权重<strong class="is hu"><em class="jo"/></strong><strong class="is hu"><em class="jo">w2</em></strong>和<strong class="is hu"> <em class="jo"> b </em> </strong>，我们不得不相对于权重<strong class="is hu"><em class="jo"/><strong class="is hu"><em class="jo">w2</em></strong>最小化<strong class="is hu"> <em class="jo"> J </em> </strong></strong></p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es md"><img src="../Images/9be4df009b2a5149c63f8dbb734e8421.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*stwmxjWWkhK_tfILTtFrrw.png"/></div></figure><h1 id="9d8b" class="ju jv ht bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">摘要</h1><p id="ddd1" class="pw-post-body-paragraph iq ir ht is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">以上几节以逻辑回归为例解释了神经网络工作背后的数学原理。让我们总结一下如何将逻辑回归作为神经网络来实现，如下所示:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es me"><img src="../Images/37f7a138b64da3e351370887f940cf3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*mArhc9TkCjkZYvJAwCcsmQ.png"/></div></figure><p id="44c1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是在所有训练示例的单个训练步骤中完成的计算。在训练期间，我们需要多次重复执行上述所有步骤，根据任务的不同，重复次数从1000到1000k不等。当精度没有提高或者成本<strong class="is hu"> <em class="jo"> J </em> </strong>最小化时，我们可以停止训练过程，从而不再需要指定步数。随着硬件和软件的改进，由于深度学习的兴起，如此庞大的计算量所需的时间大幅减少。</p><p id="c776" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，在这个故事中，我们在逻辑回归的背景下研究了神经网络背后的数学。</p></div><div class="ab cl mf mg gp mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="hb hc hd he hf"><h2 id="b3ba" class="mm jv ht bd jw mn mo mp ka mq mr ms ke jb mt mu ki jf mv mw km jj mx my kq mz bi translated">参考</h2><ol class=""><li id="e23f" class="la lb ht is b it ks ix kt jb na jf nb jj nc jn lf lg lh li bi translated"><a class="ae nd" href="https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning" rel="noopener ugc nofollow" target="_blank"> Coursera —深度学习课程1 </a></li></ol></div><div class="ab cl mf mg gp mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="hb hc hd he hf"><p id="12da" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我要感谢读者阅读这个故事。如果你有任何问题或疑问，请在下面的评论区提问。我将非常乐意回答这些问题并帮助你。如果你喜欢这个故事，请关注我，以便在我发布新故事时获得定期更新。我欢迎任何能改进我的故事的建议。</p></div></div>    
</body>
</html>