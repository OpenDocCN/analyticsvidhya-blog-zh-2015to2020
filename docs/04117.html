<html>
<head>
<title>Custom Data Science Workspace using Docker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Docker定制数据科学工作空间</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/custom-data-science-workspace-using-docker-6629f5218ccd?source=collection_archive---------15-----------------------#2020-03-05">https://medium.com/analytics-vidhya/custom-data-science-workspace-using-docker-6629f5218ccd?source=collection_archive---------15-----------------------#2020-03-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="fb65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为一名数据工程初学者，我想尝试Spark和Hadoop。因为它们都可以在基于Unix的操作系统(Linux，macOS)上轻松工作，所以在我的笔记本上安装Ubuntu似乎是个好主意，这样我就可以在Linux和Windows之间双重启动了。</p><p id="a1a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，工作量很大。重启笔记本电脑做一些编程，再重启做剩下的事情，其实效率并不是很高。(我需要Windows是因为我在空闲时间做一些编辑，而那些程序只支持Windows。)</p><p id="d813" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我想要一个可以让我使用Linux所有优点的东西，同时保留Windows的灵活性来完成我的其他任务。</p><p id="a951" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">答案:码头集装箱</strong></p><p id="1abf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用Docker容器来运行Hadoop和Spark似乎很不错，因为我了解到HortonWorks Sandbox(现由Cloudera所有)是以Docker映像的形式提供的，它允许您使用几乎所有开箱即用的大数据工具，无需额外设置。</p><p id="cb88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">捕获物:大小</strong></p><p id="a04a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一切都没问题，直到我开始拉沙盒Docker图像。我在想为什么下载需要这么长时间，然后我注意到它的大小约为22gb…！</p><p id="8617" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">22 GB。这真的很大，尤其是对于一个只想在Spark和Hadoop上做些工作的人来说。所以我放弃了使用HortonWorks沙盒docker镜像的想法。</p><p id="92a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最终解决方案:DIY沙盒</strong></p><p id="1b0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我是一名工程师，我天生喜欢把东西放在一起，让它们为更大的目的而工作。我启动Docker，把最新的Ubuntu图片作为我的CDW(自定义数据科学工作空间)的基础</p><p id="ddaf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我研究了Hadoop、Spark和JDK的兼容版本。我瞄准了Hadoop 3.2.0和JDK 8的Spark 3.0.0。这意味着以后遇到问题的可能性很小。</p><p id="fdc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">两个小时后，我的带有Spark和Hadoop的Docker映像就可以使用了..！</p><p id="5da6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我确保所有的东西和命令都开箱即用，最终用户不需要配置任何东西。</p><p id="58a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">使用我的Docker图像的说明:</strong></p><p id="305b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">确保您的系统至少有8 GB的内存，并且Docker Desktop设置为使用4 GB的内存。我的沙箱大小只有2 GB，所以不用担心存储和下载问题；-)</p><p id="a159" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">启动Docker并运行以下命令:</p><blockquote class="jd je jf"><p id="596f" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">docker pull chandanshastri/cs _ ds _ v 1.21<br/>docker run—hostname csds—name cs _ ds-it—entry point/bin/bash chandanshastri/cs _ ds _ v 1.21:最新</p></blockquote><p id="6e87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将从我的图像创建一个容器，就是这样。不需要更多的步骤…！</p><p id="0bd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要开始在容器内工作，只需运行以下命令:</p><blockquote class="jd je jf"><p id="60e7" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">docker start cs _ ds<br/>docker exec-it cs _ ds/bin/bash</p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jk"><img src="../Images/04fc695ed99349ce7a4c73632e13a818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JE7h-pZD44ILXpIWATgzQw.png"/></div></div></figure><p id="4edd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您将从容器内部获得bash终端。将您的目录更改为/home。做一个‘ls’，你会看到Spark和Hadoop目录。</p><p id="6616" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以运行<strong class="ih hj"> <em class="jg"> sh csds.sh </em> </strong>来启动Hadoop DFS和YARN。它还会启动端口10000上的节俭服务器。您可以使用Beeline访问Apache Hive。</p><p id="634e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要使用Spark，您可以使用spark-shell ( Scala)或PySpark，为了易于使用和matplotlib可视化，我已经将它们配置为与Jupyter Notebook一起启动。如果你想认真的话，你也可以为Spark编写程序并使用spark-submit命令运行它们。</p><p id="d2c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为奖励，我配置了Apache气流，它也是开箱即用的。MySQL、Postgres、Apache server、PHP和Tensorflow(是)也是无需额外配置即可使用。</p><p id="f368" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果有什么需要密码，就按“321”。</p><p id="ee9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jg">一切正常。</em> </strong></p><p id="5225" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">而且用Visual Studio Code的远程开发工具味道最好。</p><p id="03a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:您可以在Windows上运行我的自定义Hadoop和Spark发行版。只需将它们从容器复制到您的本地路径。但是您需要设置环境变量并编辑*-site.xml文件，以便根据您的系统路径模式设置Hadoop名称节点和数据节点目录。我在Spark的bin目录下创建了一个定制的cmd文件，用于在Windows上启动thrift服务器。(默认实现在Windows上遇到了问题)</p><p id="a44a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是您将错过Hadoop生态系统的许多配置特性，目前只有Linux支持这些特性。</p></div></div>    
</body>
</html>