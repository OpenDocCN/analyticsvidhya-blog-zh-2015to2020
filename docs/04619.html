<html>
<head>
<title>Deep Reinforcement Learning for Ping Pong</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">乒乓球深度强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-reinforcement-learning-for-ping-pong-e11106465443?source=collection_archive---------10-----------------------#2020-03-26">https://medium.com/analytics-vidhya/deep-reinforcement-learning-for-ping-pong-e11106465443?source=collection_archive---------10-----------------------#2020-03-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f9c70f98b1f520ef033b658ba32e1287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cBQ1wjZBV8aS2L4O.png"/></div></div></figure><p id="1036" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">⭐️In在这篇文章中，你将实现一个人工智能程序(或者代理，如果你想更花哨的话！😆)可以从任何环境(在我们的例子中是游戏)中学习，根据它的行为分别给予奖励或惩罚。如果你是强化学习的初学者，这篇文章非常适合你，因为它试图涵盖强化学习的本质。代码和一个挑战链接已被附在下面，所以跟随直到结束..！</p><p id="860e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于我们的例子，我们使用一个游戏，它是(你猜对了！)<strong class="is hj">乒乓</strong>，作为我们的环境，由<a class="ae jo" href="https://gym.openai.com/envs/Pong-v0/" rel="noopener ugc nofollow" target="_blank"> OpenAI的</a>库提供，供我们AI使用。人工智能只能控制其中一个滑块(在我们的例子中是绿色滑块)。所有的程序都是用Python做的。</p><p id="42a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">⭐️By结束这篇文章，你就会明白:</p><ol class=""><li id="a02b" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn ju jv jw jx bi translated">理论上是神经网络。</li><li id="f945" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">强化学习算法之一<strong class="is hj">策略梯度</strong>。</li><li id="6187" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">为Pong构建一个可以打败所谓“计算机”的AI(硬编码跟随球，速度限制为滑块的最大速度)。</li></ol><p id="f333" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码和思想都深受安德烈·卡帕西(伟大的家伙！)博客帖子，我已经添加了一些我的部分，使它更具视觉效果</p><h1 id="de12" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">先决条件和背景阅读</h1><p id="e113" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">要继续学习，您需要了解以下内容:</p><p id="adf3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">👉基本Python(用于实现)</p><p id="06ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">👉反向传播的工作原理(虽然不是强制的，但更可取)</p><p id="b0aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">太好了！让我们开始吧。</p><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="lk ll l"/></div></figure><h1 id="46aa" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">首先，什么是神经网络？</h1><p id="e98a" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">简而言之，神经网络是通用函数逼近器，可以逼近任何函数，这意味着如果我们知道很少的输入及其相应的输出，我们可以将输入(比如X)映射到输出Y，并近似地找到函数是什么(即对于给定的X，给出Y的最接近的函数是什么)。因此，如果给神经网络任何新的输入X，它可以使用这个近似函数预测新的输出Y。对于我们的例子，我们必须找到一个函数，它接受图像的输入，即像素强度值的2D阵列，并输出滑块向上移动的正确概率。</p><blockquote class="lm ln lo"><p id="b52b" class="iq ir lp is b it iu iv iw ix iy iz ja lq jc jd je lr jg jh ji ls jk jl jm jn hb bi translated">你可能会想为什么呢？为什么不下来？按照惯例，我们将输出视为滑块上升的概率，如果您希望输出为滑块下降的概率，您可以这样训练它，即(如果滑块需要下降，则给出输出的高概率值，如果滑块需要上升，则给出低概率值)，您可以通过更改相应的奖励函数来实现这一点。</p></blockquote><p id="8a5d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数学上，</p><p id="2a70" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">单个神经元(变量)执行一个简单的任务:接收来自先前神经元的输入，评估它们的加权和，并对和应用sigmoid函数。一层是这些神经元的组合，这些神经元从上一层获取输入，并将输出传递给下一层。知道了正确的权重(为了找到加权平均值),这些神经元就能以惊人的精度逼近任何函数。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/523495edec1409e8c21aded49a04556d.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*lsRDBHVXFJaiOiipzrfO_Q.png"/></div></figure><p id="f879" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">神经网络的正确权重可以通过下式确定:</p><p id="b8ac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1)梯度下降</p><p id="90ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2)遗传算法</p><blockquote class="lm ln lo"><p id="0b26" class="iq ir lp is b it iu iv iw ix iy iz ja lq jc jd je lr jg jh ji ls jk jl jm jn hb bi translated">这只是对神经网络如何工作的一个非常简短和高层次的回顾，你可以查看下面参考资料中的链接，以获得更有趣和更深入的视频解释。</p></blockquote><p id="7423" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">今天最常用的算法是梯度下降法，它还有很多类型，你可以在网上找到很多关于它的内容。但是重点是我们将使用梯度下降。</p><h1 id="c92a" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">我们神经网络的结构:</h1><p id="ab4c" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">我们训练的神经网络包含三层输入层、一个隐藏层和一个输出层。</p><blockquote class="lm ln lo"><p id="75f2" class="iq ir lp is b it iu iv iw ix iy iz ja lq jc jd je lr jg jh ji ls jk jl jm jn hb bi translated">我们将使用一个简单的神经网络，它有一个输入层、一个隐藏层和一个输出层。它将输入作为当前状态的图像，并输出向上或向下移动滑块的概率，并根据最高概率做出决定。这就是所谓的政策网络，在我们的案例中证明了它的强大。</p></blockquote><h1 id="f8c2" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">训练时间！！！</h1><p id="3dca" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">首先让我们来看看我们正在试图解决的游戏。OpenAI的健身房库所做的是让游戏准备好，并在当前状态下捕捉游戏的截图，然后等待我们的调用以从该状态恢复游戏。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/cc1c508d2b9d673f8238d34fbc156a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/1*iFp9NqIVNze4aVKVPEM3Vg.gif"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">橙色滑块(左)是由我们控制的，绿色滑块(右)是我们的老敌人的电脑(只是跟踪球，有速度限制)</figcaption></figure><p id="0cf1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最初，随机权重被分配给网络(即，它是一个随机函数)。这个游戏在当前状态下的截图将被作为输入给我们的神经网络。(PS:截图/图像数组尺寸为210 x 160 x 3)。该输入图像首先被预处理为灰度图像(这将我们的操作减少了1/3)，滑块和球的颜色变为白色，背景变为黑色，然后将其裁剪为80 x 80大小的中间部分。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/3568648f823f625e2b91e1ccbcdc51f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*vnpAe_u1GODyMpZavWyHsw.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">这是预处理80x80黑白图像后的效果</figcaption></figure><p id="438a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是正如你可能已经注意到的，一个单独的帧并没有显示很多关于球和滑块运动的信息，正如在上面的图像中，你不能实际上说出球或滑块在下一帧会去哪里。为了解决这个问题，我们将两个连续帧的差异传递给模型，如下所示:</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/a97dafd0781dea47a71074d65cd24dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*gn4xw5JIDZqpmQFBoVbTRA.png"/></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">黑色部分显示球/滑块从移动到<strong class="bd kf">的位置，白色部分显示球/滑块从<strong class="bd kf">到</strong>的位置</strong></figcaption></figure><p id="9c21" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该阵列是扁平的(即，阵列的行一个接一个地堆叠以形成向量)，因此我们从大小为28×28的输入阵列获得784维的输入向量，并形成网络的输入层。隐藏层我们将保持为200个神经元，最终输出层由一个神经元组成，这给出了向上移动滑块的概率。</p><p id="e70c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最初，权重将是随机的，因此模型将输出随机概率，在该游戏中，将传递给它的所有帧都向上移动，假设它给出70%的帧向上移动的概率。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/e76e62253ce4cf03dfba6af42043196a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*VI4FqfCegb3AlLfFO0ue4A.png"/></div></figure><p id="514b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们向上移动。现在的问题是，我们不知道在那个时候提升是不是正确的选择。对于所有即将到来的下一帧，我们假设模型给出的概率是正确的，直到它最终失去或获得一个分数(当一个玩家比其他玩家早得20分时，他赢得了一场比赛)。如果最后，它在游戏中得分，我们可以说它从开始以来采取的所有行动都是正确的，我们鼓励它在未来采取这样的行动，给予它积极的奖励，并根据梯度下降积极地更新权重。如果它输了一场比赛，我们可以说它在这种情况下做的大多数移动都是错误的，我们通过给它一个负奖励并通过梯度下降负更新权重来阻止未来的这种移动。</p><p id="69df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们对成千上万个游戏反复应用上述过程，探索不同的可能场景，直到最终，模型学习要使用的正确权重，以便每次最大化奖励，从而在每个游戏之后改进模型，并更接近给定游戏的理想权重(或达到如上所述的理想函数)。</p><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="lk ll l"/></div></figure><blockquote class="lm ln lo"><p id="426d" class="iq ir lp is b it iu iv iw ix iy iz ja lq jc jd je lr jg jh ji ls jk jl jm jn hb bi translated">我们刚刚使用的算法被称为<strong class="is hj"><em class="hi"/></strong>。它是强化学习的重要算法之一。</p></blockquote><p id="78d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">而现在你玩AI机器人的pong准备接管世界了。！</p><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="lk ll l"/></div></figure><p id="8880" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong>在这里，我们只考虑一个输出(即，向上滑动的概率和向下滑动的概率),这是效率较低的，因为在任何时刻，神经网络要么向上，要么向下，但它不会停留在一个地方，即使它是正确的事情</p><h1 id="f0d5" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">让我们收集一些见解…</h1><p id="872b" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">我们将面临的第一个问题是确定超参数，如学习率、衰减率、隐含层的神经元数目等。我们没有足够的计算能力来尝试和运行(试错)不同超参数值的模型。我们当前的模型使用了与Andrej的博客相似的值。</p><h1 id="dce1" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">所以最后…</h1><p id="41c7" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">在近10，000场比赛后，我们的神经网络/人工智能代理/游戏机器人显示出相当好的结果，几乎每场比赛都击败了硬编码的人工智能(红色)。</p><p id="50b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们想知道模型实际上在学习什么，我们可以看看第一层中机器人的最终权重，将其展平并可视化。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/9ff0e8bcc31756bdd535a9ca04b5fe98.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/0*AIvP-D9B5sD_ADCt"/></div></figure><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/080cad641f4f5087e18123e8e5221332.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*9DlchnUg1hhMunwSm3P3lw.png"/></div></div><figcaption class="lv lw et er es lx ly bd b be z dx translated">这实际上是我们的网络了解到的</figcaption></figure><p id="121c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">黑白轨迹是由于球的运动造成的，因为我们在两个连续的帧之间取了一个差值。正如我们所看到的，模型学习了游戏的一些常见状态，并相应地做出反应，以在每场游戏中最大化奖励。</p><p id="b50e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是等等，这还没有结束，看看下面的挑战，让事情变得更🔥。上述实现的代码可从以下网址获得:</p><div class="md me ez fb mf mg"><a href="https://github.com/mstale007/Pong_Reinforcememnt_Learning_Policy_Gradients" rel="noopener  ugc nofollow" target="_blank"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hj fi z dy ml ea eb mm ed ef hh bi translated">mstale 007/Pong _ Reinforcememnt _ Learning _ Policy _ Gradients</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">我试图解释最简单的强化学习算法策略梯度来制作一个游戏机器人…</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">github.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu io mg"/></div></div></a></div></div><div class="ab cl mv mw gp mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hb hc hd he hf"><h1 id="c28d" class="kd ke hi bd kf kg nc ki kj kk nd km kn ko ne kq kr ks nf ku kv kw ng ky kz la bi translated">有用的链接:</h1><ol class=""><li id="2373" class="jp jq hi is b it lb ix lc jb nh jf ni jj nj jn ju jv jw jx bi translated">神经网络如何工作(英尺。3蓝色1棕色)</li></ol><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="nk ll l"/></div></figure><p id="7601" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.Andrej Karpathy关于强化学习的博文:</p><div class="md me ez fb mf mg"><a href="http://karpathy.github.io/2016/05/31/rl/" rel="noopener  ugc nofollow" target="_blank"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hj fi z dy ml ea eb mm ed ef hh bi translated">深度强化学习:来自像素的Pong</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">这是一篇姗姗来迟的关于强化学习(RL)的博文。RL很辣！你可能已经注意到计算机现在可以…</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">karpathy.github.io</p></div></div><div class="mp l"><div class="nl l mr ms mt mp mu io mg"/></div></div></a></div></div><div class="ab cl mv mw gp mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hb hc hd he hf"><h1 id="d230" class="kd ke hi bd kf kg nc ki kj kk nd km kn ko ne kq kr ks nf ku kv kw ng ky kz la bi translated">下一个挑战:</h1><p id="48ff" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">试着做一个乒乓球游戏，游戏中的两个玩家都由你的机器人控制，这样你的机器人会互相竞争，并试图比对方更好。如果提供足够的资源，这种具有更复杂神经网络的机器人如果应用于其他游戏，可以在任何游戏中击败任何人。</p><p id="4c88" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意:</strong> Gym不提供两个玩家控制，因此您可以从这里获取游戏代码:</p><div class="md me ez fb mf mg"><a href="https://github.com/llSourcell/pong_neural_network_live" rel="noopener  ugc nofollow" target="_blank"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hj fi z dy ml ea eb mm ed ef hh bi translated">llSourcell/pong _神经网络_直播</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">概述这是@Sirajology的Youtube直播会话的代码。在这个现场会议中，我建立了乒乓游戏…</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">github.com</p></div></div><div class="mp l"><div class="nm l mr ms mt mp mu io mg"/></div></div></a></div><p id="0dc7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你需要自定义你自己的游戏，使它可以被两个玩家控制。</p><p id="afdd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">挑战赛</strong>参考代码:</p><div class="md me ez fb mf mg"><a href="https://github.com/mstale007/Two_Playered_Pong_Using_Policy_Gradients" rel="noopener  ugc nofollow" target="_blank"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hj fi z dy ml ea eb mm ed ef hh bi translated">mstale 007/Two _ Playered _ Pong _ Using _ Policy _ Gradients</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">我做了一个双人乒乓游戏，游戏中的两个玩家都是独立的神经网络，他们不断地尝试…</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">github.com</p></div></div><div class="mp l"><div class="nn l mr ms mt mp mu io mg"/></div></div></a></div><p id="34e9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">查看我的GitHub库，找到很多这样的ML代码:</p><div class="md me ez fb mf mg"><a href="https://github.com/mstale007?tab=repositories" rel="noopener  ugc nofollow" target="_blank"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hj fi z dy ml ea eb mm ed ef hh bi translated">mstale007 -概述</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">受到启发，用数学和物理的美来揭示现实的本质🌌有抱负的科技企业家* ML…</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">github.com</p></div></div><div class="mp l"><div class="no l mr ms mt mp mu io mg"/></div></div></a></div><p id="fab7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">黑客快乐！！！</p></div></div>    
</body>
</html>