<html>
<head>
<title>How to build a vanilla neural network with Tensorflow?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用 Tensorflow 构建香草神经网络？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-build-a-neural-network-classifier-with-tensorflow-54e10dd87433?source=collection_archive---------8-----------------------#2020-10-29">https://medium.com/analytics-vidhya/how-to-build-a-neural-network-classifier-with-tensorflow-54e10dd87433?source=collection_archive---------8-----------------------#2020-10-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="0b9b" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">万物张量流</h2><div class=""/><figure class="ev ex ip iq ir is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es io"><img src="../Images/6f7caa6e965029de57b8ba3ca5442fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v7dZ_Q-3kDkjb9Qt"/></div></div><figcaption class="iz ja et er es jb jc bd b be z dx translated">艾莉娜·格鲁布尼亚克在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="808f" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">在我的<a class="ae jd" rel="noopener" href="/analytics-vidhya/linear-regression-with-tensorflow-161f35a6ef3f">上一篇文章</a>中，我谈到了占位符，变量，用 TensorFlow 定义回归的损失函数。在本文中，我将向您介绍我设计一个香草神经网络来构建分类模型的尝试。<br/>本项目使用的数据可以在<a class="ae jd" href="http://www.kaggle.com/dataset/67721cc8414c5b9a7c7d70f1e6af93ef67e715fe9e0bcba130ec0f2766ca7fc9" rel="noopener ugc nofollow" target="_blank">这里</a>找到。我们将试图预测一些美国人的年收入是否会超过 50000 美元。更不用说，这是一个二元分类问题。</p><p id="89ba" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">我将不讨论数据清理和预处理部分，因为这不在本文的范围之内。你可以在这个<a class="ae jd" href="https://github.com/Pranab1011/tensorflow_for_everything/tree/main/Neural%20network%20classifier" rel="noopener ugc nofollow" target="_blank"> GitHub 链接</a>中找到它们。</p><p id="4a18" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">在预处理、特征工程和特征选择之后，数据看起来像这样</p><figure class="kd ke kf kg fd is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es kc"><img src="../Images/15fff955e708d0200aebc11a350e5068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xBoMErBIPNURmReDHlOIDg.png"/></div></div></figure><p id="12b8" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">训练集包含大约 32000 个样本和 21 个预测变量。这些变量大多是二元特征。接下来，我们将设计一个接受这些特征的神经网络来给出我们的二进制输出</p><h2 id="834f" class="kh ki hi bd kj kk kl km kn ko kp kq kr jp ks kt ku jt kv kw kx jx ky kz la ho bi translated">解读普通神经网络</h2><p id="987a" class="pw-post-body-paragraph je jf hi jg b jh lb jj jk jl lc jn jo jp ld jr js jt le jv jw jx lf jz ka kb hb bi translated">简单而模糊地说，香草神经网络只是一堆连接在一起的逻辑回归。网络中的每个神经元接收来自前一层的输出，将它们馈送给自己的“逻辑回归”，并生成自己的输出，该输出再次被馈送给下一层。这种相互关联的逻辑回归允许模型学习传统模型无法学习的更深层次的特征。</p><p id="6fe0" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">为了建立这样一个网络，我们需要做的就是定义层数和每层中的神经元数量，并用一个数学关系将它们连接起来。对于反向传播，我们还需要定义一个损失函数来优化模型。</p><h2 id="bb69" class="kh ki hi bd kj kk kl km kn ko kp kq kr jp ks kt ku jt kv kw kx jx ky kz la ho bi translated">编写一个简单的前馈神经网络</h2><p id="39c1" class="pw-post-body-paragraph je jf hi jg b jh lb jj jk jl lc jn jo jp ld jr js jt le jv jw jx lf jz ka kb hb bi translated">首先，让我们从一个非常简单的网络开始:</p><ul class=""><li id="6c57" class="lg lh hi jg b jh ji jl jm jp li jt lj jx lk kb ll lm ln lo bi translated">两个输入特征</li><li id="68c3" class="lg lh hi jg b jh lp jl lq jp lr jt ls jx lt kb ll lm ln lo bi translated">两个隐藏层，每个都有两个神经元</li><li id="d820" class="lg lh hi jg b jh lp jl lq jp lr jt ls jx lt kb ll lm ln lo bi translated">Sigmoin 激活函数</li></ul><figure class="kd ke kf kg fd is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es lu"><img src="../Images/069238f87c4e8f13b50eb70c18a55f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HefWta0qsPW5e6aFBrSZzQ.png"/></div></div><figcaption class="iz ja et er es jb jc bd b be z dx translated">在<a class="ae jd" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">http://alexlenail.me/NN-SVG/index.html</a>设计的图形</figcaption></figure><p id="f72b" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">在 Tensorflow 中，我们需要在建立任何关系之前声明占位符和变量。<strong class="jg hs">占位符</strong>是 tensorflow 声明输入和输出特性的方式。在我们的例子中，X 和 Y 是占位符。在每个向前和向后传播步骤中，占位符值都被提供给网络。tensorflow 中的<br/> <strong class="jg hs">变量</strong>是经过训练的变量。在这种情况下，变量是权重和偏差。</p><p id="bced" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">我们首先声明如下占位符:</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="08cc" class="kh ki hi lw b fi ma mb l mc md">n_features = 2<br/>n_dense_neurons_1 = 2</span><span id="a4b1" class="kh ki hi lw b fi me mb l mc md">x = tf.placeholder(float, (None, n_features))<br/>y = tf.placeholder(dtype=float)</span></pre><p id="66e9" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">接下来，我们声明第一个隐藏层的权重和偏差，并将它们插入到 sigmoid 激活中</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="f35e" class="kh ki hi lw b fi ma mb l mc md">W1 = tf.Variable(tf.random_normal([n_features, n_dense_neurons_1]))<br/>b1 = tf.Variable(tf.zeros(n_dense_neurons_1))</span><span id="ec1b" class="kh ki hi lw b fi me mb l mc md">z1 = tf.add(tf.matmul(x, W1), b1)<br/>a1 = tf.sigmoid(z1)</span></pre><p id="41b9" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">然后，我们设置第二个隐藏层</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="8fb4" class="kh ki hi lw b fi ma mb l mc md">n_dense_neurons_2 = 2</span><span id="056e" class="kh ki hi lw b fi me mb l mc md">W2 = tf.Variable(tf.random_normal([n_dense_neurons_1, n_dense_neurons_2]))<br/>b2 = tf.Variable(tf.zeros(n_dense_neurons_2))</span><span id="70cd" class="kh ki hi lw b fi me mb l mc md">z2 = tf.add(tf.matmul(a1, W2), b2)<br/>a2 = tf.sigmoid(z2)</span></pre><p id="7714" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">和输出层</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="9846" class="kh ki hi lw b fi ma mb l mc md">n_dense_neurons_3 = 1</span><span id="fbd4" class="kh ki hi lw b fi me mb l mc md">W3 = tf.Variable(tf.random_normal([n_dense_neurons_2, n_dense_neurons_3]))<br/>b3 = tf.Variable(tf.zeros(n_dense_neurons_3))</span><span id="ad55" class="kh ki hi lw b fi me mb l mc md">z3 = tf.add(tf.matmul(a2, W3), b3)<br/>a3 = tf.sigmoid(z3)</span></pre><p id="8818" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">输出激活 a3 给我们一个介于 0 和 1 之间的值。我们可以选择一个阈值概率来得到最终的二元预测</p><h2 id="eec1" class="kh ki hi bd kj kk kl km kn ko kp kq kr jp ks kt ku jt kv kw kx jx ky kz la ho bi translated"><strong class="ak">一个稍微复杂的正则化网络</strong></h2><p id="1b1e" class="pw-post-body-paragraph je jf hi jg b jh lb jj jk jl lc jn jo jp ld jr js jt le jv jw jx lf jz ka kb hb bi translated">在我的项目中，我开发了一个更深层次的神经网络</p><ul class=""><li id="a8d1" class="lg lh hi jg b jh ji jl jm jp li jt lj jx lk kb ll lm ln lo bi translated">21 个特征，</li><li id="0db9" class="lg lh hi jg b jh lp jl lq jp lr jt ls jx lt kb ll lm ln lo bi translated">2 个隐藏层，分别包含 20 个和 10 个神经元</li><li id="2a65" class="lg lh hi jg b jh lp jl lq jp lr jt ls jx lt kb ll lm ln lo bi translated">辍学正规化，以打击过度拟合</li></ul><figure class="kd ke kf kg fd is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es mf"><img src="../Images/48dcf70f09ed7b7b631bc5518531fbfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAdEQmn2vwOa7yKdxT-JvA.png"/></div></div><figcaption class="iz ja et er es jb jc bd b be z dx translated">在<a class="ae jd" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">http://alexlenail.me/NN-SVG/index.html</a>设计的图形</figcaption></figure><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="4e5d" class="kh ki hi lw b fi ma mb l mc md">n_features = 21<br/>n_dense_neurons_1 = 20<br/><br/>x = tf.placeholder(float, (<strong class="lw hs">None</strong>, n_features))<br/>y = tf.placeholder(dtype=float)<br/><br/>W1 = tf.Variable(tf.random_normal([n_features, n_dense_neurons_1]))<br/>b1 = tf.Variable(tf.zeros(n_dense_neurons_1))<br/><br/>z1 = tf.add(tf.matmul(x, W1), b1)<br/>a1 = tf.sigmoid(z1)<br/><br/>drop_out_1 = tf.nn.dropout(a1, 0.5)</span><span id="79c5" class="kh ki hi lw b fi me mb l mc md">n_dense_neurons_2 = 10<br/><br/>W2 = tf.Variable(tf.random_normal([n_dense_neurons_1, n_dense_neurons_2]))<br/>b2 = tf.Variable(tf.zeros(n_dense_neurons_2))<br/><br/>z2 = tf.add(tf.matmul(drop_out_1, W2), b2)<br/>a2 = tf.sigmoid(z2)<br/><br/>drop_out_2 = tf.nn.dropout(a2, 0.5)</span><span id="b7d8" class="kh ki hi lw b fi me mb l mc md">n_dense_neurons_3 = 5<br/><br/>W3 = tf.Variable(tf.random_normal([n_dense_neurons_2, n_dense_neurons_3]))<br/>b3 = tf.Variable(tf.zeros(n_dense_neurons_3))<br/><br/>z3 = tf.add(tf.matmul(drop_out_2, W3), b3)<br/>a3 = tf.sigmoid(z3)</span><span id="7e8d" class="kh ki hi lw b fi me mb l mc md">n_dense_neurons_4 = 1<br/><br/>W4 = tf.Variable(tf.random_normal([n_dense_neurons_3, n_dense_neurons_4]))<br/>b4 = tf.Variable(tf.zeros(n_dense_neurons_4))<br/><br/>z4 = tf.add(tf.matmul(a3, W4), b4)<br/>a4 = tf.sigmoid(z4)</span></pre><p id="2005" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">这里的大部分代码保持不变，除了在每个隐藏层激活后，我们使用 tensorflow 的 nn.dropout()方法放置一个<strong class="jg hs"> dropout 层</strong>，将分数传递给要保留的输入，作为第二个参数。</p><h2 id="8c1c" class="kh ki hi bd kj kk kl km kn ko kp kq kr jp ks kt ku jt kv kw kx jx ky kz la ho bi translated">反向传播</h2><p id="7ce5" class="pw-post-body-paragraph je jf hi jg b jh lb jj jk jl lc jn jo jp ld jr js jt le jv jw jx lf jz ka kb hb bi translated">为了建立反向传播，我们首先需要定义损失函数。对于这个项目，我使用了二元交叉熵损失函数</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="d0d1" class="kh ki hi lw b fi ma mb l mc md">loss = tf.reduce_sum(-(y*tf.math.log(a4) + (1-y)*tf.math.log(1-a4)))</span></pre><p id="d153" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">和 Adams optimizer 来优化损耗</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="181b" class="kh ki hi lw b fi ma mb l mc md">optimizer = tf.train.AdamOptimizer(learning_rate=0.0001, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=<strong class="lw hs">False</strong>,<br/>    name='Adam')<br/>train = optimizer.minimize(loss)</span></pre><h2 id="c3bb" class="kh ki hi bd kj kk kl km kn ko kp kq kr jp ks kt ku jt kv kw kx jx ky kz la ho bi translated">培养</h2><p id="be45" class="pw-post-body-paragraph je jf hi jg b jh lb jj jk jl lc jn jo jp ld jr js jt le jv jw jx lf jz ka kb hb bi translated">为了在 tensorflow 中训练我们的模型，我们首先需要使用</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="cc44" class="kh ki hi lw b fi ma mb l mc md">sess = tf.Session()<br/>init = tf.global_variables_initializer()  <br/>sess.run(init)</span></pre><p id="f7c7" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">为了对任何给定的示例进行训练，我们需要将它提供给<em class="mg"> train </em>实例</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="79ba" class="kh ki hi lw b fi ma mb l mc md">sess.run(train, feed_dict = {x:x_example, y:y_example})</span></pre><p id="458b" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">我们分批喂我们的训练样本。我们遍历每个时期，遍历每个唯一的批次，并将其提供给上面声明的<em class="mg"> train </em>实例。</p><p id="27ae" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">为了预测，我们将测试数据传递给最后一个激活函数。在这种情况下，整个列车组通过。</p><pre class="kd ke kf kg fd lv lw lx ly aw lz bi"><span id="7398" class="kh ki hi lw b fi ma mb l mc md">training_epochs = 200<br/>batch_size = 16<br/>train_size = x_data.shape[0]<br/><br/><strong class="lw hs">for</strong> epoch <strong class="lw hs">in</strong> tqdm(range(training_epochs)):<br/>    X = x_data.copy()<br/>    Y = y_data.copy()<br/>    <strong class="lw hs">for</strong> batch <strong class="lw hs">in</strong> range(int(train_size/batch_size)+1):<br/>        <br/>        <strong class="lw hs">if</strong> X.shape[0] &gt; batch_size:<br/>            random_index = random.sample(list(range(X.shape[0])), batch_size)<br/><br/>            x_sample = np.array(X.loc[random_index, :])<br/>            y_sample = np.array(Y.loc[random_index])<br/><br/>            X = X[~X.index.isin(random_index)].reset_index(drop=<strong class="lw hs">True</strong>)<br/>            Y = Y[~Y.index.isin(random_index)].reset_index(drop=<strong class="lw hs">True</strong>)<br/>        <strong class="lw hs">else</strong>:<br/>            x_sample = np.array(X)<br/>            y_sample = np.array(Y)<br/>            <br/>        sess.run(train, feed_dict = {x:x_sample, y:y_sample})<br/>        <br/>    <strong class="lw hs">if</strong> (epoch+1)%10 == 0:<br/>        c_train = sess.run(loss, feed_dict = {x:np.array(x_data), y:np.array(y_data)})<br/>        training_pred = sess.run(a4, feed_dict = {x:np.array(x_data), y:np.array(y_data)})<br/>        training_threshold = np.median(training_pred)<br/>        training_pred = [1 <strong class="lw hs">if</strong> i&gt;training_threshold <strong class="lw hs">else</strong> 0 <strong class="lw hs">for</strong> i <strong class="lw hs">in</strong> training_pred]<br/>        training_accuracy = accuracy_score(y_data, training_pred)<br/>        training_f1 = f1_score(y_data, training_pred)<br/>        <br/>        print("Epoch", (epoch + 1), ": Train loss =", c_train/x_data.shape[0], "Training accuracy =", training_accuracy, 'f1_score:', training_f1)</span></pre><p id="f8f8" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">这总结了我在建立二元分类的神经网络模型方面的尝试。对于大多数实际情况，我们不使用神经网络进行简单的二元分类，因为它需要大量的处理能力和时间。然而，这给了我们一个更好的想法，比如说，面部情感分类器的基础是什么。</p><p id="5399" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">我希望你喜欢这篇文章。接下来是 RNN 和 LSTM 的 Tensorflow</p><p id="7d1f" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">在那之前，谢谢你，注意安全。</p><blockquote class="mh mi mj"><p id="7292" class="je jf mg jg b jh ji jj jk jl jm jn jo mk jq jr js ml ju jv jw mm jy jz ka kb hb bi translated">再多的钱也买不到一秒钟的时间— <em class="hi">托尼·斯塔克，MCU </em></p></blockquote></div></div>    
</body>
</html>