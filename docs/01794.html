<html>
<head>
<title>Detection of face Manipulated Videos using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度学习的人脸篡改视频检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/detection-of-face-manipulated-videos-using-deep-learning-6bca870f3a6a?source=collection_archive---------5-----------------------#2019-11-14">https://medium.com/analytics-vidhya/detection-of-face-manipulated-videos-using-deep-learning-6bca870f3a6a?source=collection_archive---------5-----------------------#2019-11-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5fad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated">数字内容中的王牌操纵正成为传播虚假信息的主要原因之一，在这种情况下，它还可以诽谤任何人或组织。在没有任何智能系统的帮助下，人眼识别被操纵的视频的机会甚至非常小。针对这个问题，我实现了一个基本的基于深度学习的分类器，可以最大限度地准确识别虚假视频。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es jn"><img src="../Images/0bca7b7c12a96f6eba17244856ff2d15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mRKf4RNBZkEHc1db.jpg"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">封面图片(<a class="ae kd" href="https://aer.eu/coding-for-ethics-with-ai/" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure></div><div class="ab cl ke kf gp kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="hb hc hd he hf"><h1 id="a9f5" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">理解问题</h1><p id="67d2" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">由于在视频/图像生成和处理方面有许多积极的研究正在发展，这些研究公然帮助解决了许多问题，同时这也导致了对数字内容的信任的丧失，它甚至可能通过传播虚假信息和制造假新闻来造成进一步的伤害。</p><p id="6cff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为了获得更多的理解，请看看下面的视频</strong></p><figure class="jo jp jq jr fd js"><div class="bz dy l di"><div class="lo lp l"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">面部操作视频示例(<a class="ae kd" href="https://www.youtube.com/watch?v=cQ54GDm1eL0" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="0710" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jm">注:</em> </strong> <em class="jm">请理解，我在此收录的视频，并非冒犯任何人。这只是数字内容如何失去信任的一个例子，只是为了解决当前背景下的问题</em></p><p id="e9b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">视频本身解释了名人被操纵的面部重现的威胁，而这些大多是通过实施一些人工智能技术来操纵或生成的。</p><p id="b86b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果机器能够学习检测被操纵的视频，那么它可能是解决这个问题的解决方案之一。</p><h1 id="1098" class="kl km hi bd kn ko lq kq kr ks lr ku kv kw ls ky kz la lt lc ld le lu lg lh li bi translated">目标</h1><p id="f7d6" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">考虑到这个实时问题，我们的目标是建立一个模型，使其能够识别给定的视频是真的还是假的。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es jn"><img src="../Images/83d1a74ee4218c6b50436451940ccdbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mhA0rSZSdc2i09i7bGlmCw.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">代表目标的流程图</figcaption></figure><p id="afac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型应该能够将视频与置信度值一起分类，显示它是假/真的原因并不重要，这意味着结果解释在这种情况下并不重要，因此我们可以使用深度学习技术轻松解决这个问题，测量精确度和日志损失。</p><h1 id="13fe" class="kl km hi bd kn ko lq kq kr ks lr ku kv kw ls ky kz la lt lc ld le lu lg lh li bi translated">数据采集</h1><p id="cb2a" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">为了让机器能够思考和学习，我们需要一些合理数量的数据来输入和评估模型。这里我使用的是FaceForensics++数据，包含真实的和经过处理的人脸视频。</p><p id="c5d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">一些数据背景:</strong></p><p id="99c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Faceforensics++数据是由<a class="ae kd" href="http://niessnerlab.org/" rel="noopener ugc nofollow" target="_blank">视觉计算小组</a>收集的，该小组是一个关于计算机视觉、计算机图形学和机器学习的活跃研究小组。该数据包含1000个原始(真实)视频，这些视频是从YouTube上有选择地下载的，以便所有视频都具有清晰的面部可视性(视频大多像新闻读者阅读新闻)。</p><p id="5ecd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些原始视频是通过使用3种最先进的视频处理技术进行处理的，如DeepFakes、FaceSwap、Face2Face。要了解更多的数据请参考<a class="ae kd" href="https://www.groundai.com/project/faceforensics-learning-to-detect-manipulated-facial-images/1" rel="noopener ugc nofollow" target="_blank">这篇论文</a>。</p><p id="b1e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我总共下载了100个原始视频(49个真实+ 51个虚假)，涵盖了所有类别，这些视频被提取成图像。要下载并提取图片，请浏览<a class="ae kd" href="https://github.com/ondyari/FaceForensics" rel="noopener ugc nofollow" target="_blank">Github</a>页面并仔细阅读说明。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lv"><img src="../Images/4faf514b14c4df2e5685ee7ab7635989.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*g9Xseg7ttE8tu_xs1zx87A.png"/></div></figure><h1 id="61d2" class="kl km hi bd kn ko lq kq kr ks lr ku kv kw ls ky kz la lt lc ld le lu lg lh li bi translated">探索性数据分析和预处理</h1><p id="4af3" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">在建立任何机器学习/深度学习模型之前，我们需要通过一些数据分析来理解数据。</p><p id="1816" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">让我们了解一下这些数据是如何组织的:</strong></p><ol class=""><li id="34f9" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated">正如我提到的，我已经下载了49个真实+51个虚假的视频，这些视频中的每一个都是根据其类别在以<strong class="ih hj"> original </strong>、<strong class="ih hj"> Deepfakes </strong>、<strong class="ih hj"> FaceSwap </strong>和<strong class="ih hj"> Face2Face </strong>命名的目录下分开的</li><li id="ac45" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">对于每个视频，创建每个文件夹，其中包含所有提取的图像序列。</li></ol><p id="2b52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.例如，如果视频名称为“485.mp4”，则创建一个名为“485”的目录，其中包含“485.mp4”的所有帧。请看下图，它显示了原始视频的目录结构，我们对Deepfakes、Face2Face和Faceswap数据遵循相同的结构。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es mk"><img src="../Images/6b0f1b08ef47416fd7b69f3daa816a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*661e3EnFg6XqgGWIdHViDA.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">数据收集后的示例目录结构</figcaption></figure><p id="36d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请随时访问我的Github帐户，我在这里解释的每一步都有编码</p><div class="ml mm ez fb mn mo"><a href="https://github.com/pothabattulasantosh/Detection-of-face-Manipulated-videos" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab dw"><div class="mq ab mr cl cj ms"><h2 class="bd hj fi z dy mt ea eb mu ed ef hh bi translated">pothabattulasantosh/检测面部操纵视频</h2><div class="mv l"><h3 class="bd b fi z dy mt ea eb mu ed ef dx translated">这是使用Faceforensics++数据检测被操纵的面部图像的实现代码，因为有很多…</h3></div><div class="mw l"><p class="bd b fp z dy mt ea eb mu ed ef dx translated">github.com</p></div></div></div></a></div><p id="a8eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">让我们观察每个类别的每张图片</strong></p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mx"><img src="../Images/1a9695461cd18cd665f5422d5c53bfb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tmpObnAD80NFh-D6tn6SZQ.png"/></div></div></figure><p id="cfc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">观察结果:</p><ol class=""><li id="12b6" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated">在每张图像中，面部清晰可见，面部和相机之间没有任何物体，所有这些图像都直接面向相机(当然，数据是以这种方式有选择地收集的)</li><li id="f7b5" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">按照目标，我们需要找到人脸处理过的图像，因此我们只对人脸部分感兴趣，忽略所有其他细节，如身体、背景等是个好主意。</li><li id="beee" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">因此，我们可以通过跟踪每个图像中的人脸并将其输入分类器来实现这一点，为了实现这一点，我们使用在python库<strong class="ih hj">‘dlib’</strong>中实现的人脸跟踪算法之一，您可以从<a class="ae kd" href="https://pypi.org/project/dlib/" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</li></ol><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es my"><img src="../Images/53d3ad7902247cb9264b86e6b46aeaff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*167kSz2-VgtxQ6xt6SMqZg.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">使用在<strong class="bd kn"> dlib中实现的人脸检测算法的人脸裁剪图像。</strong></figcaption></figure><h1 id="af2b" class="kl km hi bd kn ko lq kq kr ks lr ku kv kw ls ky kz la lt lc ld le lu lg lh li bi translated">数据分割</h1><p id="74c0" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">为了训练和评估模型，我们将数据分为训练、测试和CV，同时拆分数据，我们还需要考虑数据平衡，我们需要在预处理之前拆分数据。</p><p id="757b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经将20%的数据分割出来进行测试，剩下的80%用于训练和验证，请看下面的数据分割图，每个分割都经过了数据平衡处理。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es mz"><img src="../Images/4684ce74b261c7f014f4a3a55d716326.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*oHfL6D2XRVegokO5dbHhsg.png"/></div></figure><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es na"><img src="../Images/c4fae3dd1c29cd3262cb217ddd39385e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r4RJxhlHGyLEFUQM5JkYYw.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">带平衡的数据分割</figcaption></figure><p id="4443" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们不考虑整个视频序列，而是从每个视频中只取101帧，因为这减少了建模期间的计算量。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es nb"><img src="../Images/66a85c244ed5daacb4714d45c2c7523d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o5tth2CJvnECDxttr1Zsuw.png"/></div></div></figure><p id="2162" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上图所示，我们从第10帧开始仅取101帧，这也有助于减少数据冗余。</p><p id="c87d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看一些面部追踪图像的样本。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es nc"><img src="../Images/65a76955908ac5f9f1faf9cc49460aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*V7pvSvVrYkKjxx_kp9s3xA.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">采样从原始视频中提取的人脸序列图像</figcaption></figure><p id="bbf1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">简而言之，我们的管道是:</strong></p><ol class=""><li id="922d" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated">对于每个视频，我们取101个图像序列的集合。</li><li id="1a9c" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">应用人脸追踪器算法对每幅图像进行预处理，检测人脸区域像素。</li><li id="9c7e" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated">将人脸追踪图像输入分类器。</li></ol><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es nd"><img src="../Images/0b15746e6a3b93e47484028b03c5eacc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RSEvc6mTnUJf_Nnm0cTcUQ.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">人脸操纵视频检测的整个流程的想法(<a class="ae kd" href="https://www.groundai.com/project/faceforensics-learning-to-detect-manipulated-facial-images/1" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><h1 id="7637" class="kl km hi bd kn ko lq kq kr ks lr ku kv kw ls ky kz la lt lc ld le lu lg lh li bi translated">建模</h1><p id="835e" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">由于我试图将这个问题作为一个二进制分类问题，我们需要将标签0固定为真实，1固定为虚假(Deepfakes或Face2face或FaceSwap ),并且我们还要测量准确性和分类日志损失。</p><p id="b159" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们完成数据分析，选择(或至少猜测)哪种模型可能适用于该数据是非常重要的，这里我们有完整的图像数据，因此选择基于<strong class="ih hj">卷积神经网络(CNN) </strong>的架构是一个好主意</p><p id="94f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，再次选择完美的参数真的是一个巨大的挑战，参数可能包括层数，单位数，辍学率，激活，学习率等。为了确定这一点，我们需要花费更多的时间，并需要非常高的计算能力。</p><p id="ea54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，我们可以调整任何已经在类似的巨大数据集上训练和测试过的相关模型。例如，基于CNN架构的<strong class="ih hj">例外</strong>网络已经在<strong class="ih hj"> imagenet </strong>数据集上进行了训练和测试。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es ne"><img src="../Images/48ec9aac56e83f542557a9f6ec8300d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z5gTWNJjWEXyePRzabfGUA.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kn">例外网络架构(</strong> <a class="ae kd" href="https://www.semanticscholar.org/paper/Xception%3A-Deep-Learning-with-Depthwise-Separable-Chollet/5b6ec746d309b165f9f9def873a2375b6fb40f3d" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kn">来源</strong> </a> <strong class="bd kn"> ) </strong></figcaption></figure><p id="24ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了适应这种架构，我们需要相应地预处理和设置我们的数据。用3个颜色通道上相同大小(299X299)的归一化图像来训练Xception网络。因此，在输入模型之前，我们需要注意我们的数据也应该被标准化和预处理成相同的大小</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es nf"><img src="../Images/7bb6c6661524d75023372be3cef15ebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L2czVUFxJl-ae0Qz58bt9g.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">我们训练、验证和测试数据在预处理后成形</figcaption></figure><p id="44a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我使用imagenet权重初始化异常网络架构，并将通过使用softmax激活仅替换具有2个输出的最顶层来应用迁移学习。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es ng"><img src="../Images/7b1c26ae80caa5ef6af418a351fb8048.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*4JPzm-dMzsuRxPXaaEQkGA.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">用2个输出替换顶层</figcaption></figure><p id="0cbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将把它视为基础架构，为了训练它，我们采用了一种叫做贪婪分层预训练的技术</p><ol class=""><li id="f7ed" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated">首先，用我们3个时期的训练数据训练基础架构中的每一层</li></ol><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es nh"><img src="../Images/0dc14a8d473d541d8b251ac0f82278d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*aX1VsliyUDmpNbWazyjXtg.png"/></div></figure><p id="11d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.此后，移除最上面的层(<strong class="ih hj">密集(2) </strong>)并且从训练中固定剩余的层，并且通过在顶部添加新的(<strong class="ih hj">密集(2) </strong>)层来开始15个时期的训练和验证。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es ni"><img src="../Images/8bfa43be663b3c401b6955bb78ac00f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L0_FBe_-XYnvadH_GpJFWQ.png"/></div></div></figure><p id="0d12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了训练这个架构，我使用了Adam，学习率为0.0002，batch_size为16，其他所有参数保持Keras默认值。</p><h1 id="6ea9" class="kl km hi bd kn ko lq kq kr ks lr ku kv kw ls ky kz la lt lc ld le lu lg lh li bi translated">结果</h1><p id="0a7c" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">在所有训练阶段之后，以下是结果</p><ul class=""><li id="74ad" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc nj mc md me bi translated">在训练数据上，我们得到了100%的准确率，损失是0.058</li><li id="a850" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc nj mc md me bi translated">对于验证数据，我们得到了96.63%的准确率和0.133的损失</li><li id="6780" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc nj mc md me bi translated">对于测试数据(看不见的数据),我们达到了99.10%，损失为0.078</li></ul><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es nk"><img src="../Images/e2f30c1c6f0cf3c78d107c610b858797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*kZt7GVkEQxrhFV67PW81Fw.png"/></div></figure><p id="34c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">新视频结果</strong></p><figure class="jo jp jq jr fd js"><div class="bz dy l di"><div class="nl lp l"/></div></figure><figure class="jo jp jq jr fd js"><div class="bz dy l di"><div class="nl lp l"/></div></figure><h1 id="96dd" class="kl km hi bd kn ko lq kq kr ks lr ku kv kw ls ky kz la lt lc ld le lu lg lh li bi translated">结论</h1><p id="c740" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">我们只用了100个视频来建立一个分类器，并且取得了很好的准确率。如果你想进一步提高准确率，你可以尝试更多的视频。</p><p id="a0c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型现在学会了检测被操纵的图像，并且仅当图像被DeepFakes、Face2Face或FaceSwap操纵时才表现得非常好，在不久的将来，这些类型的操纵技术可能会增加，在这种情况下，我们应该用新数据重新训练该模型。</p><p id="53ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献:</strong></p><ol class=""><li id="3550" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated"><a class="ae kd" href="https://arxiv.org/abs/1901.08971v1" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1901.08971v1</a></li><li id="5446" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><a class="ae kd" href="https://machinelearningmastery.com/greedy-layer-wise-pretraining-tutorial/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/greedy-layer-wise-pre training-tutorial/</a></li><li id="8314" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><a class="ae kd" href="https://towardsdatascience.com/introduction-to-video-classification-6c6acbc57356" rel="noopener" target="_blank">https://towards data science . com/introduction-to-video-class ification-6 c 6 acbc 57356</a></li></ol><p id="cc5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请检查我的Github档案，以获得完整的代码</p><div class="ml mm ez fb mn mo"><a href="https://github.com/pothabattulasantosh" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab dw"><div class="mq ab mr cl cj ms"><h2 class="bd hj fi z dy mt ea eb mu ed ef hh bi translated">pothabattulasantosh -概述</h2><div class="mv l"><h3 class="bd b fi z dy mt ea eb mu ed ef dx translated">热门资料库11月12月1月2月3月4月5月6月7月8月9月10月11月1日星期三Fri在2个月内创建了5个提交…</h3></div><div class="mw l"><p class="bd b fp z dy mt ea eb mu ed ef dx translated">github.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr jx mo"/></div></div></a></div></div></div>    
</body>
</html>