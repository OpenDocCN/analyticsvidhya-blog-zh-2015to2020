<html>
<head>
<title>Activation Functions In Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-in-neural-network-55d1afb5397a?source=collection_archive---------8-----------------------#2020-10-01">https://medium.com/analytics-vidhya/activation-functions-in-neural-network-55d1afb5397a?source=collection_archive---------8-----------------------#2020-10-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/42ae48fde931f1258546368f676ab7cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TEhut1cV5r63K6LbvdoaaQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:互联网</figcaption></figure><p id="33d4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这个博客中，我们将了解在深度学习中最广泛使用的激活函数。在进入正题之前，让我们简要回顾一下神经网络的基本架构，并理解它的工作原理。</p><p id="a630" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了简单起见，考虑多层感知器。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es js"><img src="../Images/6cfc5ab006172fe7ba86d188ce00d1c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*OKOUfaFz7QyRIc_ncwK4VQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="29d1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们有一组输入。这些输入被发送到隐藏层，我们计算Z值，它是输入特征和一些权重的乘积。在此操作之后，我们使用激活函数来计算输出h。我们在z上使用sigmoid激活函数。然后将结果发送到下一层，在那里我们有一些权重和偏差，然后我们计算输出o。</p><p id="c6df" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">更多细节你可以参考我以前的博客来更好地理解<a class="ae jx" rel="noopener" href="/@gauravrajpal1994/introduction-to-neural-networks-1d111bb4649">https://medium . com/@ gauravrajpal 1994/introduction-to-neural-networks-1d 111 bb 4649</a></p><p id="a3d7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">问题来了，我们到底为什么需要激活功能。要解决这个问题，让我们更好地理解它。</strong></p><p id="3063" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一个主要原因是它不能捕捉数据中的复杂关系。此外，我们将只剩下输入和权重以及偏差的线性组合。</p><p id="43b8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们用<a class="ae jx" href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.79089&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" rel="noopener ugc nofollow" target="_blank"> TensorFlow游乐场</a>来形象化。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es js"><img src="../Images/8a1d7f511f72dafe4d57485041af4c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*oepbfGO75-o38M8pZD4AgQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="c0b3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在TensorFlow Playground中，我们可以看到左侧有多个数据选项。在中间，我们定义了我们的架构。我们有输入X1和X2和单一隐藏层，然后输出层。我们还可以看到一个选项来改变激活功能，学习率等。<strong class="iw hj">目标是创建一个决策边界，将橙色点与蓝色点分开</strong>。所以，让我们看看在激活函数为<strong class="iw hj">线性</strong>之后，结果是什么。</p><p id="4ea0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将在图中观察到，在不到100个时期内，它可以为我们选择的简单数据集创建最佳决策边界，并且训练和测试集损失为0。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es js"><img src="../Images/01d7b846745154e855ddb600b5975137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*OpUKY2YWJy4se-pAQQu1KA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="4ede" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们在张量流运动场中选择另一个数据集，其中决策边界不是那么容易创建的。它是一个循环，稍后当使用线性激活函数训练模型时，我们将会看到，即使在更多次数的时期之后，并且在训练和测试损失中没有改善，我们也不能在分离橙色和蓝色点的最佳决策边界处导出。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es js"><img src="../Images/555e225c5cb29629e6674c283464c170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*jpKEsaYfcMEYeI8fMl03-Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="2ec8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，让我们想象当我们选择sigmoid激活函数时的决策边界，保持一切不变。</p><p id="44aa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将看到，该模型能够很好地进行训练，并能够在不到200个时期内正确地对这两种类型的数据点进行分类。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es js"><img src="../Images/518726e4ccb244be2042cb39ee30f5a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*3FVDpAgRNJxDWgP4exfpAQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="6795" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，将激活函数从线性改为sigmoid增加了网络中的非线性，这使得网络足够强大以捕捉数据中的关系。希望现在你们都清楚激活功能的用法了吧？</p><p id="b895" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，既然你意识到了激活函数的重要性，那么让我们深入研究一下深度学习中使用的激活函数的类型。</p><h1 id="577d" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">激活功能的类型。</strong></h1><blockquote class="kw kx ky"><p id="a1cd" class="iu iv kz iw b ix iy iz ja jb jc jd je la jg jh ji lb jk jl jm lc jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="hi">线性激活功能</em> </strong></p></blockquote><p id="bfa9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">线性激活函数是最简单的激活函数。它没有捕捉到我们之前观察到的数据中的任何非线性。</p><p id="e489" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">线性激活函数的数学等式是<strong class="iw hj"> y = ax </strong>这表示对于任何输入x，输出将是x的a倍。</p><p id="8e38" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">假设a = 1，图表看起来会像这样，</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/34149cb005273e048c41c96c8cdb44e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMa3bPxLr2IlHu-uL2MsLw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="a761" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">基于上面观察到的结果，我们可以说输入可以是范围从(-无穷大，+无穷大)的任何值，输出也是如此。这是激活函数连续的条件之一。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/b7ab7880c9c74b344ecd464a6e09ad3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTs_qotQ3cGXBPN9VHwpMQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="e3b3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">激活函数的第二个条件是，“它在每一点都应该是可微的。”。让我们看看线性激活函数的导数。我们会看到，当我们对w . r . t . x求导时，我们会得到x的系数，即a。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/69d27c52adefb12143e56dff55868fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t8-YFEu1A88TXfFv9OCIJQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="9428" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是最简单的激活函数，不会捕获数据中的非关系。该函数常用于回归问题的输出层。考虑这样一个例子，我们需要预测基于年龄、经验和资格的收入。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/0837d1b3daf021f3a42e4c5c1889602b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fh1SBMHfwsese_8Tb2HAyw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><blockquote class="kw kx ky"><p id="5437" class="iu iv kz iw b ix iy iz ja jb jc jd je la jg jh ji lb jk jl jm lc jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="hi">乙状结肠激活功能</em> </strong></p></blockquote><p id="9ae3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是最常用的激活函数，我们之前也曾使用它来演示如何通过张量流操场捕捉数据中的非线性。</p><p id="b3e8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">sigmoid激活函数的数学方程如下:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/58f2af90fe77c5e7ea77000e8bd28242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tdZ0WxV0l_Mf6tuU6SA4Cw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="4066" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">sigmoid激活函数最好的部分是它将输出值限制在0和1之间。这些值通常被视为概率，因此sigmoid函数通常用于输出层，我们需要在此计算类的概率。此外，从上面的图表中我们可以看到，sigmoid激活函数在每个点都是连续且可微的。</p><p id="9370" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们看看SIGMOID激活函数的导数。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/86cb1e395ae34ce980e0e019062991ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6keYuNcx5Yzg9t6R7eOArQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="6e22" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上面的图中我们可以看到，曲线非常平坦，这意味着这个激活函数的梯度或导数值将非常小。</p><p id="739d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了更好地理解我们如何得到sigmoid激活函数的导数的步骤，你们都可以参考下面的链接。</p><div class="lj lk ez fb ll lm"><a href="https://towardsdatascience.com/derivative-of-the-sigmoid-function-536880cf918e" rel="noopener follow" target="_blank"><div class="ln ab dw"><div class="lo ab lp cl cj lq"><h2 class="bd hj fi z dy lr ea eb ls ed ef hh bi translated">Sigmoid函数的导数</h2><div class="lt l"><h3 class="bd b fi z dy lr ea eb ls ed ef dx translated">在本文中，我们将看到在人工智能中使用的Sigmoid函数的完整推导…</h3></div><div class="lu l"><p class="bd b fp z dy lr ea eb ls ed ef dx translated">towardsdatascience.com</p></div></div><div class="lv l"><div class="lw l lx ly lz lv ma io lm"/></div></div></a></div><blockquote class="kw kx ky"><p id="7c72" class="iu iv kz iw b ix iy iz ja jb jc jd je la jg jh ji lb jk jl jm lc jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="hi"> TANH激活功能</em> </strong></p></blockquote><p id="2180" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">TANH激活函数非常类似于sigmoid激活函数。我们可以说它是sigmoid激活函数的缩放版本。</p><p id="e987" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">双曲正切激活函数的数学方程如下:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/c4718963f0a56f9f088849e44ca5acdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*thTErTk4QOMHgNrqHoAuvg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="2306" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在双曲正切激活函数中，输出值在(-1，1)之间，而在sigmoid激活函数中，我们看到输出值范围在(0，1)之间。</p><p id="b3b8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">与<em class="kz">之差&amp;之差</em>之差</strong></p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/520fb9edf8d1194eae44d7ea8271aca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K6CPOphICfaNiVDMiyElCg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="acdc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上图我们可以看到，双曲正切函数在0附近的中心更陡。此外，该图清楚地表明<strong class="iw hj"> TANH是SIGMOID的缩放版本。</strong></p><p id="c419" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们看看双曲正切激活函数的导数。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/0647270cccd27135680d04449bdf7f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KEMiN1powp_MwhVc9MFTSw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="68bb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">与sigmoid激活函数相比，tanh激活函数的值相对较大。因此，就tanh而言，训练更快，因为梯度值将更大，并且更新权重将更快。</p><blockquote class="kw kx ky"><p id="6d1c" class="iu iv kz iw b ix iy iz ja jb jc jd je la jg jh ji lb jk jl jm lc jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="hi"> ReLU激活功能</em> </strong></p></blockquote><p id="027d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">ReLU代表<strong class="iw hj">Re</strong>ctived<strong class="iw hj">L</strong>iner<strong class="iw hj">U</strong>nit。它是深度学习中最常用的激活函数之一。</p><p id="d4f8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于所有负值，该函数返回0，对于任何大于0的值，该函数返回相同的输出。让我们看看下面的等式。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/606108f58fb81ba2159b4da17caf02e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DnObHoLfQtWCi7PcG4VeMw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="2779" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们可以看到，对于所有大于0的值，它的行为就像一个线性函数，也可以表示为max(0，x)，其中x是任何实数。同样清楚的是，任何负的权重输入值的结果都将是0，这意味着神经元在正向传播过程中没有被激活。因为只有一定数量的神经元被激活，所以ReLU激活函数与sigmoid和tanh激活函数相比在计算上是有效的。</p><p id="a467" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"><em class="kz">ReLU、TANH之差&amp;乙状结肠</em>T3】</strong></p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/9abbca598e0c306ac838c7bcbe3b553c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jVaURy8esAAxqDUCF-RTlQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="8d0e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">回到<strong class="iw hj"> TANH </strong>和<strong class="iw hj"> SIGMOID </strong>激活函数，我们看到它们在每个点都是可微的，但是回到<strong class="iw hj"> ReLU </strong>激活函数，我们看到它在x = 0点是不可微的。</p><p id="23b8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们看看ReLU激活函数的导数。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/f5a47931db98de31504184a6d7148f0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oedpnKPI6EEZTuL9RNvUEQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="c5fb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如我们看到的，对于所有大于0的值，它的导数是1，对于小于0的值，它是0。它的导数没有定义在x=0的值上。</p><p id="cbe6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">为了实现的目的，x=0处的导数的值被认为是0。</strong></p><p id="ca04" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这个函数还有一个这样的问题。一些神经元的导数仍然变为0，因此一些权重和偏差没有被更新。因此，为了解决这个问题，我们有另一个激活功能。</p><blockquote class="kw kx ky"><p id="e223" class="iu iv kz iw b ix iy iz ja jb jc jd je la jg jh ji lb jk jl jm lc jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="hi">漏热路激活功能</em> </strong></p></blockquote><p id="052f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Leaky ReLU是一个激活函数，它克服了ReLU层中遇到的缺点，即某些神经元的导数变为0。为了解决这个问题，它为x &lt; 0 instead of 0.</p><p id="ad4a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Let us look at equation below.</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/8c3dbe238fe4d6303a379ed833c41c8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*imxPwL1ljaowN2tW0OWMaw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Source : Analytics Vidhya</figcaption></figure><p id="b7bb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Lets look at the derivative of Leaky ReLU activation function.</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/bbe3ca1077e7c960550f4a3e1d1e12ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*90WHKzgXQHNvIOvut3UrnQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Source : Analytics Vidhya</figcaption></figure><p id="e547" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">So, when we calculate the derivative of leaky relu activation function it will be 0.01 for all values of x ≤ 0 and 1 for all values of x &gt; 0返回x的一个小值0.01。</p><blockquote class="kw kx ky"><p id="186f" class="iu iv kz iw b ix iy iz ja jb jc jd je la jg jh ji lb jk jl jm lc jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="hi"> SOFTMAX激活功能</em> </strong></p></blockquote><p id="fd12" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">SoftMax激活通常用于多类分类。</p><p id="6c71" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在进入为什么这个激活函数被用于多类分类之前，让我们首先理解什么是多类分类问题。例如:</p><p id="a307" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">考虑下图，对于每个观察值，我们有5个特征，目标变量有3个类别(类别1、类别2和类别3)</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/b3a0e61207a2ca3598ab26fb9ae5b7fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*9rdT2CSUqg5ucVKEZhutPw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="f457" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们为上面讨论的问题创建一个简单的神经网络。我们将看到输入图层中有5个输入要素。接下来我们有一个隐藏层，它有4个神经元。显然，我们可以增加神经元的数量和体系结构中的层数，但目前我们只考虑具有4个隐藏层的神经元。这些神经元中的每一个都使用输入、权重和偏置来计算Zij(第1层的第1个神经元，我们称之为Z11等等)表示的值Z。通过这些值，我们应用激活函数，并将结果发送到输出层。</p><p id="0f06" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在你能猜出输出层的神经元数量吗？？？</p><p id="4811" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果你猜对了，你是对的，因为我们数据集的目标变量中有3个类。每一个单独的神经元都会给你单独类的概率。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/7eaa82d216d6e1d021cae8ef2e7d209a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yDdvv7UcP3djBF7qti2wgQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="d197" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在上图中，我们可以看到输出层中的第1个神经元将给出它属于第1类的概率。类似地，第二个神经元将给出它属于类别2的概率，最后第三个神经元将给出它属于类别3的概率。</p><p id="cdf6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，假设我们使用输出图层的权重和偏差计算Z值，并应用sigmoid激活函数，已知sigmoid激活函数为我们提供0和1之间的值，我们将获得一些输出值。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/414934a4285fb14eaf4a5b57d725d989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*tpXiQHlPaUhHvXheGBGLjw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="d0b0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果我们更深入地思考，我们会发现在这种情况下我们会遇到两个问题。首先，如果我们应用阈值= 0.5，它会告诉我们输入层属于2个类(类1: 0.84，类2: 0.67)。其次，概率值是相互独立的(数据点属于第1类的概率不考虑其他2类的概率)。</p><p id="caa1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="kz">这就是SIGMOID激活函数对于多类分类问题不是首选的原因。所以我们用SOFTMAX激活函数代替SIGMOID。</em>T3】</strong></p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/dff82982b5edec93a556ad7dedec2dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RYyFDAvEOF1FvcsTUYA7XQ.png"/></div></div></figure><p id="e784" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用SoftMax激活我们可以获得相对概率，这意味着它使用目标中多个类的概率值来计算最终输出。</p><p id="e8af" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们看看SoftMax激活功能是如何工作的。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/b32a228553ae7decb9104252f4e13042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*URTTd_Lxrz8eX2C8MvqsmQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:互联网</figcaption></figure><p id="ef19" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">SoftMax函数将logit[2.0，1.0，0.1]转换为概率[0.7，0.2，0.1]，概率总和为1。</p><p id="a5d8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="kz">在深度学习中，术语logits层普遍用于神经网络的最后一个神经元层，用于分类任务，产生范围从[-无穷大，+无穷大]的实数形式的原始预测值。—维基百科</em></p><p id="c083" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">SoftMax通过获取每个输出的指数，然后通过这些指数的总和归一化每个数字，从而将<em class="kz"> logits </em>(多类分类神经网络的最后一个线性层的数字输出)转化为概率，因此整个输出向量加起来等于1。</p><p id="460f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们逐步考虑基本上发生了什么(假设):</p><p id="65a8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">步骤1:假设我们获得了输出层的以下值。</strong></p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/7c6b0ddbb112bfe9505b2b7e4c7f3f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*J3veqhZXDbkFyuLOCoLEEA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="c0e9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">步骤2:将SoftMax激活函数应用于这些神经元中的每一个。</strong></p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/035cdd3b0d25e02f98944d1dbd052849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EFtt_6fhL1CO9_hPxk94lw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:Vidhya分析</figcaption></figure><p id="5cf2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们必须注意，这些是属于各自类别的输入数据点的概率值。我们必须注意在这些情况下概率总和是1。因此，在这种情况下，很明显输入属于类别1。此外，如果任何类别的概率值改变，类别1的概率值也将改变。</p><p id="7a7a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这都是关于SOFTMAX激活功能的。希望你明白了？</p><h1 id="9cb6" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">如何为我们的神经网络选择激活函数？</h1><p id="c5bf" class="pw-post-body-paragraph iu iv hi iw b ix mq iz ja jb mr jd je jf ms jh ji jj mt jl jm jn mu jp jq jr hb bi translated">你们可能想知道，到目前为止，我们已经研究了各种激活函数，看了它们的数学方程和导数，理解了为什么它们有用的术语。在这一部分中，我们将探索哪一个激活函数可以用于我们的神经网络。</p><ol class=""><li id="bc28" class="mv mw hi iw b ix iy jb jc jf mx jj my jn mz jr na nb nc nd bi translated"><strong class="iw hj">线性激活功能</strong></li></ol><p id="8cea" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">它用于目标变量连续的输出层的<strong class="iw hj">回归</strong>类问题。正如我们已经讨论过的，线性激活函数不能捕捉数据中的非线性，因此最好在输出层使用它，而我们可以在隐藏层使用非线性函数，如RELU和双曲正切函数。</p><p id="20a9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.<strong class="iw hj">乙状结肠激活功能</strong></p><p id="bc0e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如我们所知，它返回0到1之间的值，这些值被视为输出类的概率。通常它用于<strong class="iw hj">二元分类问题</strong>，而我们可以在隐藏层使用其他激活函数。</p><p id="61a5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.<strong class="iw hj"> ReLU &amp; TanH激活功能</strong></p><p id="6591" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这些激活函数通常用于神经网络的<strong class="iw hj">隐藏层</strong>。事实上，ReLU激活功能比其他激活功能表现更好，是最受欢迎的选择。</p><p id="68f5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">4.<strong class="iw hj"> Softmax激活功能</strong></p><p id="a4c3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">与sigmoid激活函数相似，softmax激活函数返回每个类别的概率，用于输出层，最常用于<strong class="iw hj">多类别分类。</strong></p><p id="e968" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi ne translated">C 结论</p><p id="dc98" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这就结束了我们的讨论话题。希望你喜欢它，喜欢探索深度学习中使用的激活函数背后的理论概念。如果是的话，请喜欢它并为它鼓掌。</p><p id="d5bc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请务必在LinkedIn上联系我:<a class="ae jx" href="https://www.linkedin.com/in/gaurav-rajpal/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/gaurav-rajpal/</a></p><p id="0b0b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="kz">敬请关注优化器/损失函数和深度学习演示项目的进一步更新。</em> </strong></p><p id="c230" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">问候，</p><p id="919b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">高拉夫·拉杰帕尔·(gauravrajpal1994@gmail.com)</p></div></div>    
</body>
</html>