<html>
<head>
<title>HyperParameter tuning an SVM — a Demonstration using HyperParameter tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调谐SVM —使用超参数调谐的演示</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hyperparameter-tuning-an-svm-a-demonstration-using-hyperparameter-tuning-cross-validation-on-96b05db54e5b?source=collection_archive---------0-----------------------#2019-11-13">https://medium.com/analytics-vidhya/hyperparameter-tuning-an-svm-a-demonstration-using-hyperparameter-tuning-cross-validation-on-96b05db54e5b?source=collection_archive---------0-----------------------#2019-11-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1a86" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">MNIST数据集的交叉验证或如何使用SVM改进MNIST的一对一策略</h2></div><p id="cc8f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是一件棘手的事情，因为改进算法不仅棘手和困难，而且有时不会有结果，很容易导致沮丧(抱歉，我在扯下一半头发后自言自语)。</p><p id="d676" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">哇呼，开始吧！</p><p id="8041" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">SVM的是一个伟大的分类工具，几乎是足够好的数据集的标准，以获得高精度。</p><p id="0ecb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是改进它们可能有点棘手，但是今天我们将使用一些标准技术来改进它们。</p><p id="1de4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们挑选一个好的数据集，在此基础上进行分类，并对其使用一对一策略。</p><p id="aa16" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">你可能会问，什么是一对一战略？</strong></p><p id="0070" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">好吧，假设我训练一台机器去理解一碗水果中的苹果，里面还有橘子、香蕉和梨。</p><p id="2058" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，机器将首先学习如何找到一个苹果，然后与橙子、香蕉和梨进行比较，宣布它们不是苹果。</p><p id="3160" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样的算法可以用来查找香蕉、橙子和梨，这有助于分别查找或分类所有水果。</p><p id="8388" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是一种一对一的技术，我们计算一个类别的概率或分类，然后将其与其他类别进行比较，而不是只找到这是苹果，这是橙色，等等，我们选择这不是苹果，这是苹果，这不是苹果，等等。</p><p id="f185" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">关于数据集</strong></p><p id="3cc7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了演示这种技术，我们将使用MNIST技术，这是一个包含从0到9的数字字母的数据集。</p><p id="5f8f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用一对所有策略，我们首先发现，什么是1而不是1，什么是2而不是2，等等。然后用它来猜测我们作为测试提供的字母。</p><p id="7835" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">出于我们的目的，我们将保留一个训练集和一个测试集。</p><p id="522b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们更深入地研究代码——</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="dec9" class="kc kd hi jy b fi ke kf l kg kh"><em class="ki">import numpy as np</em></span><span id="d6e8" class="kc kd hi jy b fi kj kf l kg kh"><em class="ki">from sklearn.datasets import fetch_openml<br/>mnist = fetch_openml(‘mnist_784’, version=1, cache=True)</em></span><span id="48e4" class="kc kd hi jy b fi kj kf l kg kh"><em class="ki">X = mnist[“data”]<br/>y = mnist[“target”].astype(np.uint8)</em></span><span id="3d70" class="kc kd hi jy b fi kj kf l kg kh">X_train = X[:60000]<br/>y_train = y[:60000]<br/>X_test = X[60000:]<br/>y_test = y[60000:]</span></pre><p id="d88c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">#将数据集加载到X和y中，并将其分成训练和测试数据集。注意——我们也可以使用train_test_split来做到这一点。</p><p id="64fd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">是时候调用分类器并在数据集上对其进行训练了</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="cbe1" class="kc kd hi jy b fi ke kf l kg kh"><em class="ki">from sklearn.svm import LinearSVC</em></span><span id="41cf" class="kc kd hi jy b fi kj kf l kg kh"><em class="ki">lin_clf = LinearSVC(random_state=42)<br/>lin_clf.fit(X_train, y_train)</em></span><span id="a723" class="kc kd hi jy b fi kj kf l kg kh"><em class="ki">from sklearn.metrics import accuracy_score</em></span><span id="91bb" class="kc kd hi jy b fi kj kf l kg kh"><em class="ki">y_pred = lin_clf.predict(X_train)<br/>accuracy_score(y_train, y_pred)</em></span></pre><p id="18fd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">准确度分数达到89.5，这相当糟糕，让我们尝试扩展训练数据集，看看是否存在任何改进-</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="5c46" class="kc kd hi jy b fi ke kf l kg kh"><em class="ki">from sklearn.preprocessing import StandardScaler</em></span><span id="522f" class="kc kd hi jy b fi kj kf l kg kh"><em class="ki">scaler = StandardScaler()<br/>X_train_scaled = scaler.fit_transform(X_train.astype(np.float32))<br/>X_test_scaled = scaler.transform(X_test.astype(np.float32))</em></span><span id="552c" class="kc kd hi jy b fi kj kf l kg kh"><em class="ki">lin_clf = LinearSVC(random_state=42)<br/>lin_clf.fit(X_train_scaled, y_train)</em></span><span id="4ccb" class="kc kd hi jy b fi kj kf l kg kh"><em class="ki">y_pred = lin_clf.predict(X_train_scaled)<br/>accuracy_score(y_train, y_pred)</em></span></pre><p id="1b52" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">准确度得分为92.10，比以前有所提高，但仍然不够高。</p><p id="f874" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">我们能做得更多吗？</strong></p><p id="74d7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">是</p><p id="3527" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以使用内核</p><p id="aa49" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">什么是内核，我们为什么要使用它们？</p><p id="1151" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我在绘制我的模型后有一个图，它没有分离我的类，建议给我的模型添加更多的度数，以帮助它线性地分离类，但是这个练习的成本是增加特性和降低模型的性能，因此是内核。</p><p id="bfd6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">核是ML中的一种方式，通过增加数据集的多项式次数来增加算法的灵活性，而不增加特征或</p><p id="789f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">内核诡计(来源Aurelion Geron) </strong></p><p id="f5d3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它可以获得相同的结果，就好像您添加了许多多项式特征，即使是非常高次的多项式，而实际上不必添加它们。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="c59e" class="kc kd hi jy b fi ke kf l kg kh"><em class="ki">from sklearn.svm import SVC</em></span><span id="f648" class="kc kd hi jy b fi kj kf l kg kh"><em class="ki">svm_clf = SVC(gamma=”scale”)<br/>svm_clf.fit(X_train_scaled[:10000], y_train[:10000]) # We use an SVC with an RBF kernel</em></span><span id="9afa" class="kc kd hi jy b fi kj kf l kg kh">y_pred = svm_clf.predict(X_train_scaled)<br/>accuracy_score(y_train, y_pred)</span></pre><p id="c3d7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">准确率得分出来是94.5，现在好多了。</p><p id="35a3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意，我们只训练了实际数据集的1/6，这是因为该操作的性能成本很高，并且有许多超参数要优化，因为这对我们来说是可行的，我们来进行超参数优化。</p><p id="a050" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">什么是超参数调谐？</strong></p><p id="cce2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们定义分类器、回归器或任何算法时，超参数是[ <em class="ki">、SVC(gamma="scale") </em> ]括号中的东西。</p><p id="4da4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">超参数是算法的属性，当您增加或减少它们时，可帮助分类或回归数据集。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="0520" class="kc kd hi jy b fi ke kf l kg kh"><em class="ki">lin_clf = LinearSVC(random_state=42)</em></span></pre><p id="98b8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，random_state=42是一个超参数，有助于将种子状态设置为42，这有助于算法挑选相似的随机实例，这有助于给出相同实例的准确度分数。</p><p id="0294" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">类似地，每个超参数都是一个属性，有自己的功能。</p><p id="24aa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我向您展示一个技巧，通过使用ML找到超参数的最佳组合，并在多个实例上运行以检查分数。</p><p id="b350" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有一种称为交叉验证的技术，其中我们使用小数据集，并检查这些小数据集上超参数的不同值，并在多个小数据集上多次重复这一练习。然后就可以找到每个超参数的最佳值。</p><p id="afe3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">多个小集合的使用被称为交叉值得分，而使用随机超参数值的技术被称为随机搜索。</p><p id="4a06" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我用代码演示一下——</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="3724" class="kc kd hi jy b fi ke kf l kg kh">from sklearn.model_selection import RandomizedSearchCV<br/>from scipy.stats import reciprocal, uniform</span><span id="7474" class="kc kd hi jy b fi kj kf l kg kh">param_distributions = {"gamma": reciprocal(0.001, 0.1), "C": uniform(1, 10)}</span><span id="ef59" class="kc kd hi jy b fi kj kf l kg kh">#Adding all values of hyperparameters in a list from which the values of hyperparameter will randomly inserted as hyperparameter</span></pre><figure class="jt ju jv jw fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kk"><img src="../Images/37fc08f8c01dc077887cdfcd1fb16bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NkRf8RZVG0LB7CmhXfqkZQ.png"/></div></div></figure><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="6453" class="kc kd hi jy b fi ke kf l kg kh">rnd_search_cv.best_estimator_</span><span id="7cbd" class="kc kd hi jy b fi kj kf l kg kh">&gt; SVC(C=6.7046885962229785, cache_size=200, class_weight=None, coef0=0.0,<br/>  decision_function_shape='ovr', degree=3, gamma=0.004147722987833689,<br/>  kernel='rbf', max_iter=-1, probability=False, random_state=None,<br/>  shrinking=True, tol=0.001, verbose=False)</span></pre><p id="fbd1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个最佳估计量给出了最佳的超参数值，我们可以将这些值插入到我们的算法中，这些值是通过多个小集合上的性能分数计算出来的。</p><p id="f5b6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们有了最好的超参数，或者我们已经完成了超参数调整，我们可以在整个训练数据集上运行它，然后在测试数据集上运行它。</p><p id="6b91" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们走吧。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="64a6" class="kc kd hi jy b fi ke kf l kg kh">rnd_search_cv.best_estimator_.fit(X_train_scaled, y_train)</span><span id="ecdf" class="kc kd hi jy b fi kj kf l kg kh">y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)<br/>accuracy_score(y_train, y_pred)</span><span id="d0a4" class="kc kd hi jy b fi kj kf l kg kh">y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)<br/>accuracy_score(y_test, y_pred)</span></pre><p id="0431" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的准确度得分为97.2，这并不算优秀，但已经足够好了，算法也没有过度拟合。</p><p id="51d2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，请注意，我们将准确度得分从89.5提高到97，这是真正的胜利。</p><p id="9ea2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们首先调整输入，然后调整超参数。我们必须注意，训练60，000个数据点并不容易，可能需要很多时间，所以要有耐心。</p><p id="eb01" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您正在寻找相同的源代码。</p><p id="50bf" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">源代码&gt;<a class="ae ks" href="https://github.com/Madmanius/HyperParameter_tuning_SVM_MNIST" rel="noopener ugc nofollow" target="_blank">https://github . com/Madmanius/HyperParameter _ tuning _ SVM _ MNIST</a></p></div></div>    
</body>
</html>