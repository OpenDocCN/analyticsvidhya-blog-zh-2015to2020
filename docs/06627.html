<html>
<head>
<title>N-Gram Language Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">n元语言模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/n-gram-language-models-9021b4a3b6b?source=collection_archive---------5-----------------------#2020-05-28">https://medium.com/analytics-vidhya/n-gram-language-models-9021b4a3b6b?source=collection_archive---------5-----------------------#2020-05-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3b9a19fbf4bcc2f740718371635655ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qb_pDT6-YPmQRAorh0hpAQ.jpeg"/></div></div></figure><p id="41c1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本文中，我们将讨论语言建模，使用N元语言模型生成文本，并使用语言模型估计句子的概率。首先，什么是语言建模？</p><blockquote class="jo"><p id="98f0" class="jp jq hi bd jr js jt ju jv jw jx jn dx translated">语言建模只不过是预测下一个单词是什么的过程。</p></blockquote><blockquote class="jy jz ka"><p id="6c3b" class="iq ir kb is b it kc iv iw ix kd iz ja ke kf jd je kg kh jh ji ki kj jl jm jn hb bi translated"><strong class="is hj">语言模型基于文本示例或训练数据学习单词出现的概率</strong></p></blockquote><p id="3f90" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑一个单词序列</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="c05b" class="kt ku hi kp b fi kv kw l kx ky">x1, x2, x3, x4,...,xn</span></pre><p id="a663" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设你的词汇集中有V和m个单词</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="74b5" class="kt ku hi kp b fi kv kw l kx ky">V = {w1, w2, w3,...,wm}</span></pre><p id="9d48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计算序列中下一个单词x(n+1)的概率分布，其中x(n+1)可以是固定词汇集V中的任何单词w</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="fbc8" class="kt ku hi kp b fi kv kw l kx ky">Conditional Probability = P(x(n+1)|xn, x(n-1), x(n-2),...,x2,x1)</span></pre><blockquote class="jo"><p id="3def" class="jp jq hi bd jr js kz la lb lc ld jn dx translated">你也可以认为语言模型或者语言模型是一项为句子或序列分配概率的任务</p></blockquote><p id="fd4a" class="pw-post-body-paragraph iq ir hi is b it kc iv iw ix kd iz ja jb kf jd je jf kh jh ji jj kj jl jm jn hb bi translated">假设我们有一个句子</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="aed3" class="kt ku hi kp b fi kv kw l kx ky">sentence = 'I came by bus'</span></pre><p id="410f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它由4个单词组成</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="6a99" class="kt ku hi kp b fi kv kw l kx ky">tokens = ['I', 'came', 'by', 'bus']</span></pre><p id="135a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以根据我们的语言模型，这个句子的概率是所有单词基于它们之前的单词的所有条件概率的乘积。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="775d" class="kt ku hi kp b fi kv kw l kx ky">prob = product of all probabilities</span><span id="1828" class="kt ku hi kp b fi le kw l kx ky">p = P('I')*P('came'|'I')*P('by'|'came','I')*P('bus'|'by','came','I')</span></pre><p id="e65f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">概率→ p是我们的语言模型为句子提供的</p><h2 id="21ae" class="kt ku hi bd lf lg lh li lj lk ll lm ln jb lo lp lq jf lr ls lt jj lu lv lw lx bi translated">那么，如何学习语言模型呢？</h2><p id="dd1d" class="pw-post-body-paragraph iq ir hi is b it ly iv iw ix lz iz ja jb ma jd je jf mb jh ji jj mc jl jm jn hb bi translated">一种方法是通过N元语言模型。那么什么是N-gram呢？</p><p id="6e83" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">N-gram是N个连续单词的序列</p><p id="45fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑上面的句子和记号</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="17b8" class="kt ku hi kp b fi kv kw l kx ky">unigrams = 'I', 'came', 'by', 'bus'</span><span id="56fc" class="kt ku hi kp b fi le kw l kx ky">bigrams = 'I came', 'came by', 'by bus'</span><span id="9ddf" class="kt ku hi kp b fi le kw l kx ky">trigrams = 'I came by', 'came by bus'</span><span id="0d10" class="kt ku hi kp b fi le kw l kx ky">4-grams = 'I came by bus'</span></pre><p id="4756" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi md translated"><span class="l me mf mg bm mh mi mj mk ml di">T</span><strong class="is hj">T5】何假设一个n元单词LM是下一个单词只依赖于前n-1个单词 </strong></p><p id="24ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以新的条件概率是</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="382f" class="kt ku hi kp b fi kv kw l kx ky">P(x(t+1)|xt, x(t-1),...,x2,x1) = P(x(t+1)|xt, x(t-1),...,x(t-n+2)</span></pre><p id="8d8a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以一个单字LM，next单词不依赖于任何前面的单词</p><p id="55fb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设我们正在学习一个4克的LM</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="baab" class="kt ku hi kp b fi kv kw l kx ky">sentence = If you resort to making fun of someone’s appearance, you lost the __________</span></pre><p id="7d2e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为我们正在学习一个4个字母的单词，所以只有最后(4-1)个单词会影响下一个单词</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="8a6f" class="kt ku hi kp b fi kv kw l kx ky">sequence = 'you lost the'</span></pre><p id="95ab" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以下一个单词的概率</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="b7ba" class="kt ku hi kp b fi kv kw l kx ky">P(next_w|you lost the) = P(you lost the next_w)/P(you lost the)</span></pre><p id="4822" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设在我们的语料库中</p><p id="177b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">‘你输了’→发生了10000次</p><p id="188d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">‘你输了游戏’→出现2000次</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="d6a0" class="kt ku hi kp b fi kv kw l kx ky">P(game|sequence) = P(you lost the game)/P(you lost the)<br/>P(game|you lost the) = 2000/10000 = 0.2</span></pre><p id="e5ec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">‘你赌输了’→出现1000次</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="5dc1" class="kt ku hi kp b fi kv kw l kx ky">P(bet|you lost the) = 0.1</span></pre><p id="0144" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">‘你数错了’→出现了1000次</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="53ea" class="kt ku hi kp b fi kv kw l kx ky">P(count|you lost the) = 0.1</span></pre><p id="5980" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“你输了这场争论”→发生了500次</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="22ac" class="kt ku hi kp b fi kv kw l kx ky">P(argument|you lost the) = 0.05</span></pre><p id="1525" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以当你看原句时，单词“argument”比其他单词更有意义。</p><p id="d0a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果丢弃太多的上下文，预测的结果就不会那么好。</p><p id="2d71" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">缺点或限制之一是稀疏性问题</p><p id="da43" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果语料库中不存在“你丢失了”怎么办</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="f88a" class="kt ku hi kp b fi kv kw l kx ky">P(sequence) = 0</span></pre><p id="c5fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后你必须从4个字母的LM退回到三个字母的LM，以此类推</p><h1 id="5e26" class="mm ku hi bd lf mn mo mp lj mq mr ms ln mt mu mv lq mw mx my lt mz na nb lw nc bi translated">使用python在NLTK的Brown语料库的帮助下生成文本</h1><p id="8423" class="pw-post-body-paragraph iq ir hi is b it ly iv iw ix lz iz ja jb ma jd je jf mb jh ji jj mc jl jm jn hb bi translated">基本的想法是在4克LM的帮助下生成接下来的30个单词。如果4-gram LM在预测下一个单词时存在稀疏性问题，请退回到三元模型LM。如果同样的问题发生在三元模型上，退回到二元模型，如果发生在二元模型上，退回到一元模型。因为unigram不依赖于前面的单词，所以从单词库中随机选择一个单词。</p><ul class=""><li id="1af3" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">导入单词语料库</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="8dcc" class="kt ku hi kp b fi kv kw l kx ky">import numpy as np<br/>from nltk.corpus import brown</span><span id="db7e" class="kt ku hi kp b fi le kw l kx ky">words = list(brown.words())</span></pre><ul class=""><li id="e007" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">为这个句子生成接下来的30个单词或记号</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="ee06" class="kt ku hi kp b fi kv kw l kx ky">sentence = 'I am planning to____________________________________'</span><span id="7cb0" class="kt ku hi kp b fi le kw l kx ky">start_sentence = 'I am planning to'</span><span id="7371" class="kt ku hi kp b fi le kw l kx ky">tokens = start_sentence.split()</span></pre><ul class=""><li id="56f3" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">4克LM </strong></li></ul><p id="8fea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个模型返回下一个单词的所有可能的标记。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="5084" class="kt ku hi kp b fi kv kw l kx ky">def fourgram_model(self):<br/>        next_words = []</span><span id="3321" class="kt ku hi kp b fi le kw l kx ky">for i in range(len(words)-3):<br/>            if words[i] == tokens[-3]:<br/>                if words[i+1] == tokens[-2]:<br/>                    if words[i+2] == tokens[-1]:<br/>                        next_words.append(words[i+3])</span><span id="9740" class="kt ku hi kp b fi le kw l kx ky">next_words = next_words<br/>        return next_words</span></pre><p id="01b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为上述4-gram LM模型的结果中出现的所有不同单词生成概率。根据概率<strong class="is hj"> <em class="kb">选择前三个单词。</em>T3】</strong></p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="057d" class="kt ku hi kp b fi kv kw l kx ky">def get_top_3_next_words(next_words):<br/>        next_words_dict = dict()</span><span id="6dda" class="kt ku hi kp b fi le kw l kx ky">for word in next_words:<br/>            if not word in next_words_dict.keys():<br/>                next_words_dict[word] = 1<br/>            else:<br/>                next_words_dict[word] += 1</span><span id="ad6d" class="kt ku hi kp b fi le kw l kx ky">for i,j in next_words_dict.items():<br/>            next_words_dict[i] = np.round(j/len(next_words),2)</span><span id="cba8" class="kt ku hi kp b fi le kw l kx ky">#Sorting the probs in decreasing order and select the first three<br/>result = sorted(next_words_dict.items(), key = lambda k:(k[1], k[0]), reverse=True)[:3]</span><span id="fe5c" class="kt ku hi kp b fi le kw l kx ky">return result</span></pre><p id="b694" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">选择这三个单词中的一个，并将该单词添加到单词列表的末尾。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="426d" class="kt ku hi kp b fi kv kw l kx ky">[('use', 0.11), ('tour', 0.11), ('shelter', 0.11)]</span></pre><p id="b854" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，如果您从预测中选择“使用”,则更新后的令牌列表应该是</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="a644" class="kt ku hi kp b fi kv kw l kx ky">tokens = ['I', 'am', 'planning', 'to', 'use']</span></pre><p id="1c4f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，如果您使用一个4-gram LM来生成下一个单词，请使用标记列表的最后三个单词。</p><p id="4cbb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你面临稀疏性问题，那么使用标记列表的最后两个单词，并使用三元模型LM。</p><p id="c386" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你仍然面临稀疏性的问题，退回到一元。</p><p id="9149" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我只在这里发布了4克LM的代码。对于trigram、bigram、unigram自己试试或者<strong class="is hj"> <em class="kb">你可以在</em></strong><a class="ae nm" href="https://github.com/ambatiashok60/Medium/blob/master/Language%20Modeling/N-gram%20Language%20Model%20for%20text%20generation.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="is hj"><em class="kb">GitHub</em></strong></a>找到与这个文本生成问题相关的代码</p><p id="41ad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在生成所有30个单词之后，您的令牌列表应该看起来像这样</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="ebde" class="kt ku hi kp b fi kv kw l kx ky">['I', 'am', 'planning', 'to', 'use', 'the', 'Standard', 'Deduction', 'or', 'the', 'Tax', 'Table', ',', 'and', 'later', 'go', 'hungry', '?', '?', 'The', 'voice', 'had', 'music', 'in', 'it', '.', 'The', 'sequence', 'may', 'involve', 'a', 'sharp', 'contrast', ':']</span></pre><ul class=""><li id="b1ec" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">生成的文本看起来像</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="84b0" class="kt ku hi kp b fi kv kw l kx ky">I am planning to use the Standard Deduction or the Tax Table , and later go hungry ? ? The voice had music in it . The sequence may involve a sharp contrast :</span></pre><h1 id="2dab" class="mm ku hi bd lf mn mo mp lj mq mr ms ln mt mu mv lq mw mx my lt mz na nb lw nc bi translated">估计句子的N-gram概率</h1><p id="4a6a" class="pw-post-body-paragraph iq ir hi is b it ly iv iw ix lz iz ja jb ma jd je jf mb jh ji jj mc jl jm jn hb bi translated">让我们使用同一个棕色语料库来计算一个句子的N-gram概率。我们将按如下方式计算概率</p><ul class=""><li id="8085" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">检查在语料库中只出现过一次的单词</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="a7bb" class="kt ku hi kp b fi kv kw l kx ky">words = list(brown.words())</span><span id="78c6" class="kt ku hi kp b fi le kw l kx ky">words_dict = dict()</span><span id="4702" class="kt ku hi kp b fi le kw l kx ky">for word in words:<br/>    if word not in words_dict.keys():<br/>        words_dict[word] = 1<br/>    else:<br/>        words_dict[word] +=1</span><span id="d8f7" class="kt ku hi kp b fi le kw l kx ky">occured_once = [i for i,j in words_dict.items() if j ==1]</span></pre><blockquote class="jy jz ka"><p id="cb47" class="iq ir kb is b it iu iv iw ix iy iz ja ke jc jd je kg jg jh ji ki jk jl jm jn hb bi translated">在语料库中只出现一次的单词被认为是被称为OOV的单个特殊单词的实例。OOV指的是词汇之外</p></blockquote><ul class=""><li id="8ccd" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">将出现过一次的单词转换为特殊单词OOV的实例</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="cfaa" class="kt ku hi kp b fi kv kw l kx ky">for num, word in enumerate(words):<br/>    if word in occured_once:<br/>        words[num] = "OOV"</span></pre><ul class=""><li id="2e08" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated">对句子进行标记，检查标记是否在语料库中。</li></ul><blockquote class="jy jz ka"><p id="bf00" class="iq ir kb is b it iu iv iw ix iy iz ja ke jc jd je kg jg jh ji ki jk jl jm jn hb bi translated">如果词汇中没有这些标记，则将该特定标记视为OOV的实例</p></blockquote><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="952c" class="kt ku hi kp b fi kv kw l kx ky">sentence = "I have never given it much thought"</span><span id="1e6e" class="kt ku hi kp b fi le kw l kx ky">tokens = sentence.split()</span><span id="bd44" class="kt ku hi kp b fi le kw l kx ky">for num,token in enumerate(tokens):<br/>    if not token in words:<br/>        tokens[num] = "OOV"</span></pre><p id="310d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们已经用OOV实例更新了令牌和单词</p><ul class=""><li id="c54c" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">将记号和单词转换成小写</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="894c" class="kt ku hi kp b fi kv kw l kx ky">tokens = [token.lower() for token in tokens]</span><span id="56c8" class="kt ku hi kp b fi le kw l kx ky">words = [word.lower() for word in words]</span></pre><ul class=""><li id="2738" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">二元概率</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="7ea3" class="kt ku hi kp b fi kv kw l kx ky">Probability = P(2 token sequence | first token)</span><span id="f5c0" class="kt ku hi kp b fi le kw l kx ky">prob = P(w-1, w | w-1) = count(w-1, w)/count(w-1)</span></pre><ul class=""><li id="5db8" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">句子的标记</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="4c2b" class="kt ku hi kp b fi kv kw l kx ky">tokens = ['i', 'have', 'never', 'given', 'it', 'much', 'thought']</span></pre><ul class=""><li id="447c" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">句子的所有二元模型</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="81b5" class="kt ku hi kp b fi kv kw l kx ky">bigrams = []<br/>for i in range(len(tokens)-1):<br/>    bigrams.append((tokens[i],tokens[i+1]))</span></pre><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es nn"><img src="../Images/c57c17ccc7a8e83cfbbd1bfebeafb53a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*W8_c_HXoDh73hk1mTKhFKA.png"/></div><figcaption class="no np et er es nq nr bd b be z dx translated">各自的二元概率</figcaption></figure><ul class=""><li id="c0ac" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">计算单个二元模型概率的函数</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="8de8" class="kt ku hi kp b fi kv kw l kx ky">def bigram_probability(bigram,words):<br/>    count = [0,0]<br/>    for i in range(len(words)-1):<br/>        if (words[i] == bigram[0]):<br/>            count[1] += 1<br/>            if (words[i+1] == bigram[1]):<br/>                count[0] += 1<br/>    return count[0]/count[1]</span></pre><blockquote class="jy jz ka"><p id="57e2" class="iq ir kb is b it iu iv iw ix iy iz ja ke jc jd je kg jg jh ji ki jk jl jm jn hb bi translated"><strong class="is hj">一个句子的概率=所有二元模型概率的乘积</strong></p></blockquote><ul class=""><li id="03ba" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">句子的二元模型概率</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="f830" class="kt ku hi kp b fi kv kw l kx ky">def bigram_prob_sentence(tokens, bigrams):<br/>    prob = []<br/>    for bigram in bigrams:<br/>        p = bigram_probability(bigram,words)<br/>        prob.append(p)<br/>    return np.prod(prob)</span></pre><p id="e25e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以这句话使用二元模型LM的概率是<strong class="is hj">7.92268578305691 e-15</strong></p><p id="df55" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们尝试三元模型。</p><ul class=""><li id="cde9" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">三元模型概率</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="e1d2" class="kt ku hi kp b fi kv kw l kx ky">trigram = (w-2, w-1, w)</span><span id="6ad0" class="kt ku hi kp b fi le kw l kx ky">Probability = P(3 token sequence | first 2 token sequence)</span><span id="2aa0" class="kt ku hi kp b fi le kw l kx ky">prob = P(w-2, w-1, w | w-2, w-1) = count(w-2, w-1,w)/count(w-2, w-1)</span></pre><ul class=""><li id="145d" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">句子的三元模型概率</strong></li></ul><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es ns"><img src="../Images/b9c3d8b569ccff5380f23e7196b14ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*s3yldUG565CorJhCmViytQ.png"/></div><figcaption class="no np et er es nq nr bd b be z dx translated">三元模型概率</figcaption></figure><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="8d5d" class="kt ku hi kp b fi kv kw l kx ky">probability = (8/259) * (0/24) * (0/2) * (1/7) * (1/2) = 0</span></pre><ul class=""><li id="0a59" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">单字概率</strong></li></ul><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="17e1" class="kt ku hi kp b fi kv kw l kx ky">unigram = w</span><span id="3edc" class="kt ku hi kp b fi le kw l kx ky">Probabilty = P(w)</span><span id="f07b" class="kt ku hi kp b fi le kw l kx ky">prob = count(w)/total_words</span></pre><ul class=""><li id="e14e" class="nd ne hi is b it iu ix iy jb nf jf ng jj nh jn ni nj nk nl bi translated"><strong class="is hj">句子的单字概率</strong></li></ul><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/bc2537ccce4aa91c26943abaecd3d178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*L2w_-5PECuzNr9AZKphEUA.png"/></div><figcaption class="no np et er es nq nr bd b be z dx translated">单字概率</figcaption></figure><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="76da" class="kt ku hi kp b fi kv kw l kx ky">probability = product of all unigram probabilities</span><span id="a6eb" class="kt ku hi kp b fi le kw l kx ky">probability = 1.6139361322466987e-28</span></pre><blockquote class="jy jz ka"><p id="675f" class="iq ir kb is b it iu iv iw ix iy iz ja ke jc jd je kg jg jh ji ki jk jl jm jn hb bi translated"><strong class="is hj">你可以在</strong> <a class="ae nm" href="https://github.com/ambatiashok60/Medium/blob/master/Language%20Modeling/Estimating%20the%20N-Gram%20Probabilities%20of%20a%20sentence.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> GitHub </strong> </a>找到使用N-grams的句子概率估计的相关代码</p></blockquote></div></div>    
</body>
</html>