<html>
<head>
<title>Deep Reinforcement Learning for autonomous vehicles with OpenAI Gym, Keras-RL in AirSim simulator</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在AirSim模拟器中使用OpenAI Gym、Keras-RL进行自动驾驶车辆的深度强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-reinforcement-learning-for-autonomous-vehicles-with-openai-gym-keras-rl-in-airsim-simulator-196b51f148e4?source=collection_archive---------9-----------------------#2020-06-13">https://medium.com/analytics-vidhya/deep-reinforcement-learning-for-autonomous-vehicles-with-openai-gym-keras-rl-in-airsim-simulator-196b51f148e4?source=collection_archive---------9-----------------------#2020-06-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="cc54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如今自动驾驶汽车变得流行，深度强化学习也是如此。这篇文章可以为你提供一个想法，为你开始学习和试验自动驾驶汽车的深度强化学习建立环境。此外，我将在模拟器中分享我用<a class="ae jd" href="https://arxiv.org/pdf/1312.5602.pdf" rel="noopener ugc nofollow" target="_blank"> Deep-Q-Network </a>为自动驾驶汽车在赛道上做的第一次实验。</p><p id="623d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我会给你看一个我实验的简短演示。的。gif图片突出显示了训练一辆汽车在右转时自动驾驶大约12小时的步骤。在这个实验中，用于训练导航模型的输入是从安装在汽车头部前方的摄像机记录的图像帧。右下角的窗口显示了该摄像机的实时记录。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/5e88b6abd186b115cebaf60536087e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*VxBb1X1JnQFixpvn-wsiXA.gif"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">8倍的快动作剪辑突出了12个小时的训练DQN模型的汽车在右转。观察值是来自汽车前视摄像头的RGB帧。</figcaption></figure><p id="6c74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有3个主要组件用于进行此实验。那些是AirSim(运行在虚幻引擎上)，Open AI Gym，和Keras-RL。如果你还不熟悉的话，我在这里简单介绍一下这些主要组件。</p><ul class=""><li id="6933" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated"><a class="ae jd" href="https://github.com/microsoft/AirSim/" rel="noopener ugc nofollow" target="_blank"> AirSim </a>是在游戏引擎上运行的附加软件，比如<a class="ae jd" href="http://www.unrealengine.com" rel="noopener ugc nofollow" target="_blank">虚幻引擎</a> (UE)或Unity。AirSim是作为人工智能研究的平台开发的，用于试验深度学习、计算机视觉和自动驾驶汽车的强化学习算法。</li><li id="755a" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated"><a class="ae jd" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI Gym </a>提供了一系列游戏环境来玩和评估强化学习算法。</li><li id="ad1a" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated"><a class="ae jd" href="https://github.com/keras-rl/keras-rl" rel="noopener ugc nofollow" target="_blank"> Keras-RL </a>是基于<a class="ae jd" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>实现不同深度强化学习算法的python库。Keras-RL与OpenAI健身房合作。这意味着评估和试验不同的算法很容易。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ke"><img src="../Images/9bd9d99e2d96194eae99602fb469fe83.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*kuS2gHP7e4VOSzQyCu8i8w.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">系统图显示了组件及其连接。</figcaption></figure><p id="347a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于我的环境设置，我使用虚幻引擎作为游戏引擎，地图<a class="ae jd" href="https://www.unrealengine.com/marketplace/en-US/product/race-course-pack" rel="noopener ugc nofollow" target="_blank">赛道</a>提供了几条赛道。同时，AirSim提供汽车和控制汽车的API(例如:转向、加速)，以及记录图像的API。所以我可以说我已经有了一个可以玩的汽车驾驶游戏。</p><p id="7bfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步是将这个驾驶游戏连接到深度强化学习工具Keras-RL和OpenAI Gym。为此，首先创建了一个定制的OpenAI健身房环境，这个定制的健身房环境调用必要的AirSim APIs，如控制汽车或捕捉图像。由于Keras-RL与健身房环境一起工作，这意味着现在Keras-RL算法可以通过定制的健身房环境与游戏进行交互。</p><p id="41af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将上述组件结合在一起后，我已经有了一个框架来实验自动驾驶汽车的深度强化学习。在运行第一个实验之前，我需要做的一件额外的事情是在地图中定位赛道，因为在训练和评估期间将需要赛道的位置来确定赛车的奖励，但这种信息在地图中还不可用。为了确定赛道的位置，我手动在道路中心放置了一些路点，并测量了道路的宽度。有了路点列表，我就能大致找到路线。请注意，一旦游戏开始，这些路点将不可见。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es kf"><img src="../Images/e29de11fec96acc8f5f5c1b3974c771a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iW_5N9yBnqHmpjQI8cTbzQ.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">路点放在赛道的中央。计算奖励需要汽车的位置和2个最近的路点。</figcaption></figure><p id="431a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于设置环境的更多细节，您可以查看我的Github <a class="ae jd" href="https://github.com/hoangtranngoc/AirSim-RL" rel="noopener ugc nofollow" target="_blank">库</a>。</p><p id="3576" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将系统的所有部分连接在一起后，我做了一个快速实验，以确保系统正常工作。在这个实验中，我使用了一个简单的动作空间和一个简单的奖励函数。调整DQN的超参数还不是重点，因为我们总是可以在以后改进训练。我想看看汽车代理商如何学习右转，所以我选择了你在视频中看到的那段赛道。</p><p id="98ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在来说一下对于汽车代理商的学习过程。我把观察、行动和奖励的一般思想放到下图的强化学习的一般学习过程中。想象一下，DQN模型是汽车的大脑。从目前的观察来看，对于每一步，DQN模型都会告诉汽车应该采取哪种转向动作(这可以称为“策略”)。然后计算该行为的回报。此时，在汽车代理再次观察之前，DQN用当前{观察、动作、奖励}更新知识。循环(一集)运行，直到汽车脱离轨道或到达目的地。在这篇文章的最后，我将重点介绍定制健身房环境的step()函数的实现，您可以在代码中看到这个学习过程。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es kk"><img src="../Images/c3809b1c0932d556526cb8e1ad814ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Jvwi4XO5mndu-E6M8fB5Q.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">实验的强化学习过程。注意，在我们的例子中，状态是局部的，因为没有观察到整个环境。在每个时间步t，对于当前观察(St)，汽车代理采取DQN模型推荐的行动(At)。然后对该行为的奖励(Rt)进行评估。如果插曲还没有完成，DQN用{St，At，和Rt}更新知识。然后观察到新的状态，依此类推。</figcaption></figure><p id="1d0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的实验中，我选择了简单的标准来奖励每一步中的汽车代理，这鼓励汽车代理靠近赛道的中心行驶。当汽车代理商靠近中心时，奖励更多的分数，当远离中心时，奖励更少的分数。对于汽车代理的动作空间，基于0.3的最大转向值使用7个离散动作。这意味着在每一步中，汽车代理可以选择这些值中的一个从左向右驾驶{-0.3，-0.2，-0.1，0.0，0.1，0.2，0.3}。采取更好的行动(或改进政策)是汽车代理商在培训期间学到的，以最大化长期回报。</p><p id="f90d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我将展示在培训DQN期间记录的2个常用指标(每集平均奖励和每集平均行动值(Q))。和往常一样，每集平均奖励比较嘈杂，而每集平均Q值比较稳定。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kg kh di ki bf kj"><div class="er es kl"><img src="../Images/89b214eb540753091f05e9c4098b9c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5CR5Rs4cJ_R5aLctSSCo5Q.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">左边的图显示每集的平均奖励，而右边的图显示平均行动值(Q)。这两个指标的长期趋势都在增长。</figcaption></figure><p id="4441" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想了解更多细节，请查看我的GitHub回购。我会在未来添加更多的东西。<a class="ae jd" href="https://github.com/hoangtranngoc/AirSim-RL" rel="noopener ugc nofollow" target="_blank">https://github.com/hoangtranngoc/AirSim-RL</a></p><p id="eda1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望你了解我如何设置环境/框架来做自动驾驶汽车深度强化学习的第一个实验的基本想法。那么你开始你的第一个实验就没那么复杂了。随着游戏引擎，像虚幻引擎(这个<a class="ae jd" href="https://www.unrealengine.com/en-US/blog/a-first-look-at-unreal-engine-5" rel="noopener ugc nofollow" target="_blank">博客</a>)，使游戏中的场景更接近现实，模拟器中的实验和现实生活之间的差距将会缩短。</p><p id="9a06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我在这里发布了一个视频，总结了培训过程，并添加了一些信息。(提示:观看时启用声音；))</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="km kn l"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">动作由离散转向值区分，从-0.3(左)到0.3(右)(最大范围为[-1，1])。汽车的油门固定在0.75(最大是1)。奖励是根据到赛道中心的距离给出的。</figcaption></figure><p id="f5e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你正在这个领域寻找挑战，看看<a class="ae jd" href="https://aws.amazon.com/deepracer/" rel="noopener ugc nofollow" target="_blank"> AWS Deep Racer </a>。</p><p id="e470" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您的阅读！</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><p id="407b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(如果您想深入了解代码，这里是附录)</p><p id="0621" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我只是强调定制的OpenAI Gym环境是如何用__init()__和step()函数实现的。如前所述，您可以将step()函数中的what映射到上述学习过程中。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="e374" class="la lb hi kw b fi lc ld l le lf">class AirSimCarEnv(gym.Env):<br/>  def __init__(self):<br/>    ..<br/>    # <strong class="kw hj"><em class="lg">image_shape</em></strong> is (height, width, channels).<br/>    self.observation_space = <br/>      spaces.Box(low=0, high=255, shape=<strong class="kw hj">image_shape</strong>, dtype=np.uint8)<br/>    #I wrote a <strong class="kw hj"><em class="lg">CarAgent </em></strong>class<strong class="kw hj"><em class="lg"> </em></strong>to control car with AirSim APIs<br/>    self.<strong class="kw hj">car_agent </strong>= <strong class="kw hj"><em class="lg">CarAgent</em></strong>()</span><span id="8877" class="la lb hi kw b fi lh ld l le lf">  def step(self, action):<br/>    # <strong class="kw hj"><em class="lg">action </em></strong><em class="lg">is produced by DQN</em>, action is discrete<br/>    self.<strong class="kw hj">car_agent</strong>.move(<strong class="kw hj"><em class="lg">action</em></strong>)<br/>    # compute <strong class="kw hj"><em class="lg">reward </em></strong>based on state(position) of the car<br/>    car_state= self.<strong class="kw hj">car_agent</strong>.getCarState()<br/>    <strong class="kw hj">reward </strong>= self._compute_reward(car_state)<br/>    # check if the episode is done<br/>    car_controls = self.<strong class="kw hj">car_agent</strong>.getCarControls()<br/>    <strong class="kw hj">done </strong>= self._isDone(car_state, car_controls, reward)<br/>    # log info<br/>    info = {}<br/>    # <strong class="kw hj"><em class="lg">observation </em></strong>is RGB image from car's camera<br/>    <strong class="kw hj">observation </strong>= self.<strong class="kw hj">car_agent</strong>.observe()</span><span id="cb86" class="la lb hi kw b fi lh ld l le lf">    return <strong class="kw hj">observation, reward, done, info</strong></span><span id="451e" class="la lb hi kw b fi lh ld l le lf">  ...</span></pre></div></div>    
</body>
</html>