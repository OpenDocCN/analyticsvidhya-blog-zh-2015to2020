<html>
<head>
<title>K-Nearest Neighbors- Classification technique in Machine Learning.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的 k 近邻分类技术。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbors-classification-technique-in-machine-learning-2b8d22b99979?source=collection_archive---------18-----------------------#2020-12-28">https://medium.com/analytics-vidhya/k-nearest-neighbors-classification-technique-in-machine-learning-2b8d22b99979?source=collection_archive---------18-----------------------#2020-12-28</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="a95b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这篇博客重点介绍了 K-NN 算法的概况，如何选择 K 值，K-NN 算法的特点，失败案例，偏差-方差权衡，异常值的影响和利弊。</p><h1 id="7f61" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">机器学习中的分类是什么</h1><p id="6235" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">比方说，你生活在一个封闭的住房社会，你的社会有不同类型的废物分开垃圾箱:一个用于废纸，一个用于塑料垃圾，等等。你在这里做的基本上是将垃圾分类。因此，分类是给一个特定的项目分配一个“类别标签”的过程。在上面的例子中，我们给不同类型的垃圾贴上了“纸”、“金属”、“塑料”等标签。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kf"><img src="../Images/89a077d430069fdfda7f8e93430317b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*aFPDXEkdFAuUU5KmczzEXw.jpeg"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">废物的分类。</figcaption></figure><h1 id="a811" class="jc jd hh bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated">机器学习中的分类算法</h1><p id="63c9" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">现在我们知道了分类的确切含义，我们将学习机器学习中的分类算法:</p><ul class=""><li id="ab49" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated">k-最近邻(KNN)</li><li id="98fd" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">逻辑回归</li><li id="ca75" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">线性回归</li></ul><p id="7fba" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在这篇博客中，我们将只关注 K-NN。我们将只涵盖对该算法的总体理解。</p><h2 id="436e" class="lj jd hh bd je lk ll lm ji ln lo lp jm ip lq lr jq it ls lt ju ix lu lv jy lw bi translated"><strong class="ak">矩阵符号</strong></h2><p id="94f0" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">矩阵是一种按列和行组织数据的方式。矩阵写在括号[ ]内。</p><p id="6217" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">下图中的矩阵有两行和三列。</p><ul class=""><li id="ca47" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated">其<strong class="ig hi">尺寸</strong>为 2 ×3</li><li id="fe64" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">2 行 3 列</li><li id="1c86" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">下面矩阵的<a class="ae lx" href="https://www.mathwarehouse.com/algebra/matrix/#entry" rel="noopener ugc nofollow" target="_blank">条目</a>是 2，-5，10，-4，19，4。</li><li id="ab6d" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">这里每行代表一个数据点。</li><li id="930a" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">所有列代表数据集的要素。</li></ul><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es ly"><img src="../Images/c9427c0335df2326be284f1e5a18565e.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/1*t2yYRVyPMV01vz8Pz0mhPA.gif"/></div></figure><h2 id="13f1" class="lj jd hh bd je lk ll lm ji ln lo lp jm ip lq lr jq it ls lt ju ix lu lv jy lw bi translated">KNN 算法及其直觉</h2><p id="46a9" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated"><strong class="ig hi">第一步</strong>—<strong class="ig hi">—</strong>在 KNN 的第一步中，我们必须加载训练和测试数据。</p><p id="1318" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">步骤 2</strong>—接下来，我们需要选择 K 的值，即最近的数据点。k 可以是任何整数。</p><p id="8e5e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">步骤 3</strong>—对于测试数据中的每个点，执行以下操作</p><ul class=""><li id="d836" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated"><strong class="ig hi">3.1</strong>—借助任何方法计算测试数据和每行训练数据之间的距离，即:欧几里德距离、曼哈顿距离或汉明距离。计算距离最常用的方法是欧氏距离。</li><li id="2fed" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><strong class="ig hi">3.2</strong>—现在，根据距离值，将它们按升序排序。</li><li id="416d" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><strong class="ig hi">3.3</strong>—接下来，它将从排序后的数组中选择前 K 行。</li><li id="6eac" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><strong class="ig hi">3.4</strong>—现在，它将根据这些行中最常见的类别为测试点分配一个类别。</li></ul><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es lz"><img src="../Images/256a00aafaf02c8d8ffedf1f4392490d.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*glSM_6wTph3VzCbTzTt8WA.jpeg"/></div></figure><p id="3b6b" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated"><strong class="ig hi">寻找 K-NN 中 K 的最佳值</strong></p><ul class=""><li id="e790" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated">我们使用验证数据的误差图或准确度图来<strong class="ig hi">找到</strong>最有利的<strong class="ig hi"> K </strong>值。</li><li id="ae01" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">从图中，我们取误差最小的 K 值，在这种情况下，K 值约为。9(来自下图)。</li></ul><figure class="kg kh ki kj fd kk er es paragraph-image"><div class="er es ma"><img src="../Images/a0e96472f119a3c081470f925e1c94d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*jwBhyHsM7VtuBwJcwMQjBg.jpeg"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">验证数据的 K-误差图。</figcaption></figure><h2 id="a21a" class="lj jd hh bd je lk ll lm ji ln lo lp jm ip lq lr jq it ls lt ju ix lu lv jy lw bi translated">KNN 的特色</h2><ul class=""><li id="870a" class="kv kw hh ig b ih ka il kb ip mb it mc ix md jb la lb lc ld bi translated">k 最近邻算法被广泛用于测试更复杂的算法，如深度网络、SVM、CNN。</li><li id="69ce" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">KNN 是懒惰学习者的典型例子。它被称为<em class="me">懒</em>不是因为它表面上的简单，而是因为它不从训练数据中学习任何函数，而是记忆训练数据集。(在现实中，KNN 没有任何决定边界，这只是为了我们的理解目的。)</li><li id="39ee" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">我们可以使用 KNN 进行回归和分类。</li><li id="2ba7" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">这是一种非参数方法，这意味着它不假设任何先验数据分布。</li><li id="e640" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">当数据具有偶数个类时，我们通常选择 K 为奇数，当数据具有奇数个类时，我们通常选择 K 为偶数。</li><li id="04f5" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">对于较小维度的数据，我们通常使用欧几里德距离。</li></ul><h2 id="3188" class="lj jd hh bd je lk ll lm ji ln lo lp jm ip lq lr jq it ls lt ju ix lu lv jy lw bi translated">KNN 的失败案例</h2><div class="kg kh ki kj fd ab cb"><figure class="mf kk mg mh mi mj mk paragraph-image"><img src="../Images/dc90a023a097635db13ea7ed5f09f3fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*Im4E3VzwKNm0Z2oSLOON0g.png"/></figure><figure class="mf kk ml mh mi mj mk paragraph-image"><img src="../Images/5b95b71866364126b4837312de5596a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*RvoTob_BDcQgDl0jAwU1Rg.png"/><figcaption class="kr ks et er es kt ku bd b be z dx mm di mn mo translated">图二。随机传播的数据</figcaption></figure></div><ul class=""><li id="d539" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated">如图 1 所示。如果我们的查询点离最近的邻居非常远，那么它会将查询点分类到错误的类中。</li><li id="1446" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">如果数据是杂乱的或随机分布的，那么我们会得到错误的查询点输出。</li></ul><h2 id="f21d" class="lj jd hh bd je lk ll lm ji ln lo lp jm ip lq lr jq it ls lt ju ix lu lv jy lw bi translated"><strong class="ak">如果我们有不平衡的数据对 KNN 的影响</strong></h2><p id="ef53" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">它受到类别不平衡问题的影响，即它试图将每个查询点归类为主要类别。但是我们可以使用过采样和欠采样等技术来平衡数据。</p><h2 id="bb51" class="lj jd hh bd je lk ll lm ji ln lo lp jm ip lq lr jq it ls lt ju ix lu lv jy lw bi translated">过度拟合和欠拟合(偏差-方差权衡)</h2><p id="717b" class="pw-post-body-paragraph ie if hh ig b ih ka ij ik il kb in io ip kc ir is it kd iv iw ix ke iz ja jb ha bi translated">k 值小可能导致<strong class="ig hi">过拟合</strong>，k 值大可能导致<strong class="ig hi">欠拟合</strong>。</p><ul class=""><li id="c244" class="kv kw hh ig b ih ii il im ip kx it ky ix kz jb la lb lc ld bi translated"><strong class="ig hi">过拟合</strong>表示模型在训练数据上表现良好，但在新数据到来时表现不佳。</li><li id="b7cb" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated"><strong class="ig hi">欠拟合</strong>指的是对训练数据不好的模型，也不能推广到预测新数据。</li></ul><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es mp"><img src="../Images/ac2ae3dc58fd479aa4900aec2399950b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UCd6KrmBxpzUpWt3bnoKEA.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">偏差-方差权衡</figcaption></figure><h2 id="778c" class="lj jd hh bd je lk ll lm ji ln lo lp jm ip lq lr jq it ls lt ju ix lu lv jy lw bi translated">优势</h2><ul class=""><li id="4fb2" class="kv kw hh ig b ih ka il kb ip mb it mc ix md jb la lb lc ld bi translated">无培训期，易于实施</li><li id="3265" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">新的数据可以随时添加，它不会影响模型，因为没有训练期。</li><li id="2e5e" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">很好地处理多类问题</li></ul><h2 id="abb6" class="lj jd hh bd je lk ll lm ji ln lo lp jm ip lq lr jq it ls lt ju ix lu lv jy lw bi translated"><strong class="ak">缺点</strong></h2><ul class=""><li id="40ce" class="kv kw hh ig b ih ka il kb ip mb it mc ix md jb la lb lc ld bi translated">对噪音和缺失数据敏感</li><li id="277e" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">必须进行特征缩放。</li><li id="7a06" class="kv kw hh ig b ih le il lf ip lg it lh ix li jb la lb lc ld bi translated">无法获取特征重要性。</li></ul><p id="5b5c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">参考资料:</p><p id="ea96" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">谷歌图片</p><p id="11bb" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是我第一篇关于媒体的文章，如果你觉得这篇文章对你有帮助，请不吝留下你的评论。感谢阅读。</p></div></div>    
</body>
</html>