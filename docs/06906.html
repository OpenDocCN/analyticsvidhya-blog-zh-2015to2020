<html>
<head>
<title>Humans Image Segmentation with Unet using Tensorflow Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于张量流Keras的Unet人体图像分割</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/humans-image-segmentation-with-unet-using-tensorflow-keras-fd6cb43b06e5?source=collection_archive---------0-----------------------#2020-06-07">https://medium.com/analytics-vidhya/humans-image-segmentation-with-unet-using-tensorflow-keras-fd6cb43b06e5?source=collection_archive---------0-----------------------#2020-06-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="3f57" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">我们想做的是-</h1><p id="1e0d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们希望通过使用现有的库和资源来创建人类的分段(现在只对人类)。因此，我们将为此使用OCHuman数据集和Tensorflow。我们将在这篇文章中谈论所有这些事情。</p><p id="8f15" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们将图像和他们的面具，以模型和模型将产生一个分割的面具的人给定的图像。我们可以使用这些分割结果来人为模糊图像的背景，在自动驾驶汽车中，改变图像的背景，图像编辑，检测图像中的人，以及许多可能性。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/568c663d1783cedba09376deceb52e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CbWGzglXqdpxlcpLkCV6Ig.png"/></div></div></figure><p id="acce" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">下面的图片是唯一的第44次训练的结果，文章中有很多东西要讨论。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ks"><img src="../Images/1f5c32605745446bc0d59b8d29352bc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wZ9p0nKGfp4a1VPB3RK6XQ.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">我们模型的预测</figcaption></figure><h1 id="0ba9" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">首先，我给你看一下这篇文章的重点。</h1><ol class=""><li id="e938" class="kx ky hi jf b jg jh jk jl jo kz js la jw lb ka lc ld le lf bi translated"><strong class="jf hj">什么是图像分割</strong></li><li id="0384" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">什么是Unet，我们为什么使用Unet </strong></li><li id="b8f9" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">为什么TensorFlow </strong></li><li id="bf81" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">我们使用的数据集</strong></li><li id="2edd" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">我们如何预处理数据并创建自定义数据集</strong></li><li id="9e3e" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">使用不同的自定义数据集进行模型构建、训练和结果</strong></li><li id="9e6e" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">你可以尝试什么</strong></li><li id="e721" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">代码(GitHub) </strong></li></ol><h1 id="f5e8" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">1.什么是图像分割</h1><p id="81f9" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">图像分割是机器视觉的一个广泛的部分，在图像分割中我们把图像的每一个像素归为一类。</p><p id="15ba" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">例如:假设在下图中，我们高亮显示了猫的每个像素值。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ll"><img src="../Images/628b4a8ade84533c871de4991dd85fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3lxvj_N1iWeSrQ471PBuA.jpeg"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">来源:<a class="ae lm" href="https://towardsdatascience.com/u-net-b229b32b4a71" rel="noopener" target="_blank">https://towardsdatascience.com/u-net-b229b32b4a71</a></figcaption></figure><p id="2840" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">正如你在上面看到的，图像是如何变成两段的，一段代表猫，另一段代表背景。</p><h2 id="c2ba" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated"><strong class="ak">有两种类型的图像分割- </strong></h2><p id="c148" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb"> 1。语义分割:</em> </strong> <em class="mb">将每个像素分类成一个类别。</em></p><p id="c00d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">例如:如果图片中有三只猫，我们将它们归类为一个实例，即猫。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ll"><img src="../Images/628b4a8ade84533c871de4991dd85fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3lxvj_N1iWeSrQ471PBuA.jpeg"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">例子:<em class="mc">语义切分</em></figcaption></figure><p id="c12e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">T29】2。实例感知分割，也称为<em class="mb">同时检测:</em> </strong> <em class="mb">在实例感知分割中我们找出每个对象的单独实例。</em></p><p id="7abf" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">例如:如果图片中有三只猫，我们可以分别识别它们。</p><p id="c784" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">自动驾驶汽车是图像分割的最大例子之一。在自动驾驶汽车中，我们可能需要对每个对象进行分类(人、汽车、自行车、道路、树木等。)单独来说。嗯，关于自动驾驶汽车，有很多事情可以谈论，如果你也想了解它们，请告诉我。</p><h1 id="7ad6" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">2.什么是Unet，我们为什么使用Unet</h1><p id="e9f2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">首先说一下CNN。CNN从图像中学习特征，并将图像压缩成特征向量，我们可以在图像分类和其他事情中使用它。</p><p id="1693" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在，谈谈Unet- In分割，我们需要从CNN创建的特征向量中重建图像。因此，这里我们将特征图转换成一个向量，并从这个向量中重建一个图像。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es md"><img src="../Images/151574ee5aad6a00d6693d009c310386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Q3ODFV_t2WVXvm_09ExjA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">Unet</figcaption></figure><p id="6c25" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这个建筑看起来像一个U形。在这个架构中，我们有两个部分压缩和扩展。在压缩部分，我们有一些卷积层，最大池层。每个块之后的内核或特征图的数量加倍，以便架构可以学习复杂的结构。</p><p id="e7d3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">类似地，在扩展块中我们有CNN层和上采样层。扩展块的数量与压缩块的数量相同。</p><p id="ef6e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">图像分割中的损失计算？好吧，它在论文中被简单地定义了。</p><p id="a3b9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="mb">“结合交叉熵损失函数，通过最终特征图上的逐像素软最大值来计算能量函数。”</em></p><p id="733d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">我们用Unet是因为</strong>它可以重建图像。我们将一些图像作为特征，将它们相应的蒙版图像作为标签添加到模型中。由于Unet的重建功能，Unet也能够生成图像作为输出。这里我们使用的是监督学习方法。</p><h1 id="4b8c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">3.为什么选择张量流</h1><p id="a82d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们使用Tensorflow是因为该模型开发速度很快，无需更多地担心语法，更专注于网络的架构，并对模型进行微调。</p><p id="354c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">TensorFlow还提供了Keras，因此我们可以使用它的API来创建数据生成器、模型和微调等。非常容易。</p><p id="708a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">但是如果你愿意，你也可以使用Pytorch。在PyTorch中，您还需要关注您的代码，并且需要编写更多的代码。但是Pytorch的优点是你可以和张量一起玩，并且在<strong class="jf hj">训练时间</strong>里获得的性能高不了多少。</p><h1 id="6875" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">4.我们使用的数据集</h1><p id="7592" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们使用的数据集<a class="ae lm" href="https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman" rel="noopener ugc nofollow" target="_blank"> OCHuman </a>。该数据集聚焦于具有全面注释的严重遮挡的人类，包括<strong class="jf hj">边界框、人类姿势和实例遮罩</strong>。该数据集包含5081幅图像中的13360个精心标注的人类实例。平均每个人0.573 MaxIoU，OCHuman是与人类相关的最复杂和最具挑战性的数据集。</p><p id="3470" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">《Pose2Seg:无检测人体实例分割》中提出的数据集<a class="ae lm" href="http://www.liruilong.cn/projects/pose2seg/index.html" rel="noopener ugc nofollow" target="_blank">【project page】</a><a class="ae lm" href="https://arxiv.org/abs/1803.10683" rel="noopener ugc nofollow" target="_blank">【arXiv】</a>@ cvpr 2019。</p><p id="a632" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">你可以从<a class="ae lm" href="https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据集。</p><p id="54e4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在应用<strong class="jf hj">边界框、人类姿势和实例遮罩之后，来自数据集的样本图像- </strong></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es me"><img src="../Images/bbfe136af2e624898821582f4371a1a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BWFYnmKpwt4ubOZqxw-ocw.jpeg"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">样本图像OCHuman</figcaption></figure><p id="b3e3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">此数据集包含以下文件-</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mf"><img src="../Images/3677a57bb261b7c9e06cbc0cc05e7a42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WjRT1VH5aWFVsso0AsU5cg.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">数据集OCHuman</figcaption></figure><p id="5b35" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们将只使用images.zip和ochuman.json。</p><p id="0a40" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> Images.zip: </strong>内容大量图片<strong class="jf hj">没有</strong>任何包围盒，人类姿势，和实例遮罩。我们将提取它，我们将有一个名为“图像”的文件夹，其中包含图像，如-</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mg"><img src="../Images/5e7507247bcfe47f3bdaae602503cc07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m8jY4XFKu85o_eAu4Qmn3Q.png"/></div></div></figure><p id="f8ce" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> ochuman.json:这是一个json文件，包含“images”目录中与图像相关的信息(</strong>边界框、人物姿势和实例遮罩<strong class="jf hj">)。</strong></p><p id="5454" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">有另一个数据集<a class="ae lm" href="http://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO </a>可用于相同的任务，但我们不想使用它，因为它除了人之外还有其他类型的分割，可能需要做更多的预处理。嗯，大约是18 GB的数据集。而OCHuman只有700 MB左右。如果我们有一些结果，那么我们也可以使用COCO数据集尝试相同的模型或不同的模型进行进一步的训练。</p><p id="8a1c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在，你对我们的数据集、Unet和任务有了一个基本的概念。现在，请稍微了解一下我们的自定义数据集。</p><h1 id="0874" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">5.我们如何预处理数据并创建自定义数据集</h1><h2 id="3479" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">为什么我们创建了一个自定义遮罩(分段)？</h2><p id="01d8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我认为对于我们的任务，由数据集生成的分段不是很有用，所以我创建了自定义分段。因为我们希望将精确的分割掩模提供给模型，而不希望提供额外的或不相关的信息。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mh"><img src="../Images/85770debf2b04527e22693386e67a9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tg-2TnAVslHXxWAuYwKQcw.png"/></div></div></figure><p id="30b6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">您可以通过更改下面的“new_mask”函数中的值来尝试不同类型的分段。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/29553c3ddca288486a95f6a7b9eafb84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AKq_xHSdPJf6kBglA1aSqw.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">您可以生成不同种类的分段</figcaption></figure><p id="e286" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">现在，在继续之前，让我向你展示我们使用JSON文件生成这些图像的遮罩和姿态的API。</strong></p><p id="f6eb" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="mb">你可以去这个GitHub链接安装API。</em><a class="ae lm" href="https://github.com/liruilong940607/OCHumanApi" rel="noopener ugc nofollow" target="_blank">https://github.com/liruilong940607/OCHumanApi</a></p><p id="0879" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">注意:确保你已经下载了images.zip并解压为文件夹名“images”并且有“ochuman.json”。</em> </strong></p><h2 id="68aa" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">我们如何创造面具-</h2><p id="b63f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">你可以写出比这更好的代码，但是现在，这是我所拥有的-</p><p id="f475" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 5.1。安装API- </strong></p><pre class="kh ki kj kk fd mi mj mk ml aw mm bi"><span id="78a0" class="ln ig hi mj b fi mn mo l mp mq">git clone https://github.com/liruilong940607/OCHumanApi<br/>cd OCHumanApi<br/>make install</span></pre><p id="9262" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 5.2。首先导入所有需要的库- </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="3e89" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 5.3。读取ochuman.json文件- </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="5707" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们设置Fiter='segm '是因为我们想要图像的唯一分割。你可以使用不同的参数。API的GitHub repo上提到的所有细节。</p><p id="ae21" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">所以，现在我们有<strong class="jf hj">个总图像:4731个</strong>在<strong class="jf hj"> image_ids </strong>列表中，包含人类<strong class="jf hj">的分割。</strong></p><p id="230f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 5.4。这是一个帮助函数，它将帮助我们只为图像创建一个分割- </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="55fc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 5.5。我们创建的另一个助手函数，只是传递一个由ochuman API生成的原始图像和分割图像。</strong>该功能将创建一个黑白<strong class="jf hj">自定义蒙版</strong>。您可以更改append函数中的值来生成不同种类的图像。稍后，您可以将生成的图像提供给模型。</p><p id="5c75" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">阅读第9行和第11行的注释。</p><p id="6042" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> new_mask: </strong>如果你想创建一个黑色背景和白色人类面具，反之亦然，使用此功能。</p><p id="b1db" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> new_mask_clr: </strong>如果你想创建彩色图像。例如，紫色背景和黄色的人面具，然后使用这个功能。阅读第63、65、67和70、72和74行的注释。只需改变append函数中的值来改变颜色。我们使用BGR格式，因为OpenCV可以读取BGR格式的图像。默认颜色是紫色背景和黄色遮罩(人类)。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="fa9b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 5.6。一些参数- </strong></p><p id="c7c6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">更改这些参数可能需要更改代码中许多其他地方的值，请仔细理解代码的工作原理。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="e6bf" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 5.7。创建黑白分割- </strong></p><p id="57f2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">你可以使用“new_mask_clr”(用于紫色和黄色遮罩)函数来代替第9行的“new_mask”(用于黑白遮罩)。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="a4d1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这个函数的输出是:(2，512，512，3) (2，512，512，3)</p><p id="836d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">解释-这个函数将返回x和y。这里x是一个形状为(2，512，512，3)的普通图像，没有任何分割、边界框等。y是形状为(2，512，512，3)的黑白分割图像。</p><p id="544b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">你可以看到这里的输出，你可能会想，为什么所有的人类没有被分割？是因为数据集。在ochuman.json文件中，我们没有这个图像中其他人的分割。我们在图像中只有一个人的分割。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mt"><img src="../Images/796ebf82b14a7c30a26866065054694b.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*aA3VLhWrI8TyJfelRagAug.png"/></div></figure><p id="2946" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 5.8。现在生成所有的4731张图片- </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="0ab6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们将遍历所有4731张图片。我知道它有点硬编码，但它对数据生成部分来说很好。</p><p id="7866" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">上面代码的输出-</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es mu"><img src="../Images/a0ad4144ddd711eda9609d8eb40511f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*tD9AecVYZYeZ2LnxUNg0Ww.png"/></div></figure><h2 id="a9aa" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">5.9.把这些放在一起-</h2><p id="735a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">以上所有代码都可以在我的<a class="ae lm" href="https://github.com/Dipeshpal" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中找到。</p><p id="ae6f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><a class="ae lm" href="https://github.com/Dipeshpal/Image-Segmentation-with-Unet-using-Tensorflow-Keras/blob/master/img_create.ipynb" rel="noopener ugc nofollow" target="_blank">点击此处</a>查看这款笔记本。本笔记本仅用于自定义数据生成部分，培训笔记本是不同的笔记本。我正在使用谷歌Colab，所以你可能需要编辑一些东西，如更改目录等。</p><h1 id="e33f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">6.代码模型构建-</h1><p id="434b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将使用Unet进行训练，因为它能够重新生成图像。</p><p id="66ba" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">该模型可能学习其他东西，这意味着该模型可以学习输入图像到输出图像之间的颜色映射。它可能会学习一些颜色到另一些颜色的映射，所以这就是为什么我们创建了三个不同的数据集。我们将通过使用Unet的相同架构，将三种不同类型的图像数据集逐个输入到模型中。</p><p id="2646" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">使用上面第5点中的笔记本，我们创建了三个自定义数据集-</p><h2 id="e743" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">6.1.自定义数据集人类:黑色背景和白色物体-</h2><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mv"><img src="../Images/0cae614b17033d2996318f77e0c9ab9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*394kxkWJLdMXs6skgwZDLQ.png"/></div></div></figure><h2 id="87c7" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">6.2.自定义数据集人类:白色背景和黑色对象-</h2><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mw"><img src="../Images/2c0a339f4a26f5acf9012b0ff08a8ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11895VHlIEIdPhcSSzsj9Q.png"/></div></div></figure><h2 id="a8a7" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">6.3.自定义数据集人类:紫色背景和黄色对象-</h2><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mw"><img src="../Images/7e3dd11791e683a7a5f5c4d0b8d93aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pfMI9f9RhhaLs1mkcz8eWw.png"/></div></div></figure><p id="e946" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们还将讨论数据生成器和其他东西，但在此之前，让我们先了解一下模型和结果。您可以使用不同的参数，如激活、内核初始化、时期、图像大小等。我们将对上述三个数据集使用相同的模型。我们创建的架构如下所示-</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="faf1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">模型总结- </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="7dc3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">训练结果- </strong></p><p id="f2a4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">此输出结果是针对黑色背景数据集图像的。你可以看到损失从一个<strong class="jf hj">损失:0.5708减少到损失:0.3164 </strong>。验证损失从<strong class="jf hj"> val_loss: 0.5251下降到val_loss: 0.3122 </strong>。准确度没有大的变化。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><h1 id="d1fe" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结果-</h1><h2 id="6f12" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated"><strong class="ak">输出图像- </strong></h2><p id="033c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">您可以看到输出非常令人印象深刻，到44 epoch结束时，我们有以下结果。“epochNumber_x_input.jpg”是输入图像，“epochNumber_Y_truth.jpg”是遮罩输入图像(标签),“epochNumber_Y_predicted.jpg”是由模型生成的图像(预测图像)。</p><p id="195b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">结果6.1:黑色背景的图像- </em> </strong></p><p id="cb9b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">结果分析</strong>:你可能会注意到，在43预测图像(43_Y_predicted.jpg)中，你可以看到我们有一个只针对右边那个人的遮罩(43_Y_truth.jpg)。该模型能够分割右边的人和女孩，以及左边戴黑帽子的人。44纪元后，我们的谷歌实验室崩溃了。</p><p id="492b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">笔记本:</strong><a class="ae lm" href="https://github.com/Dipeshpal/Image-Segmentation-with-Unet-using-Tensorflow-Keras/blob/master/training_black_background.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Dipeshpal/Image-Segmentation-with-Unet-using-tensor flow-Keras/blob/master/training _ black _ background . ipynb</a></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ks"><img src="../Images/1f5c32605745446bc0d59b8d29352bc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wZ9p0nKGfp4a1VPB3RK6XQ.png"/></div></div></figure><p id="58c0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">结果6.2:白色背景的图像- </em> </strong></p><p id="f216" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">结果分析</strong>:43个纪元后Colab崩溃。这里的结果令人印象深刻。</p><p id="2555" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">笔记本:</strong><a class="ae lm" href="https://github.com/Dipeshpal/Image-Segmentation-with-Unet-using-Tensorflow-Keras/blob/master/training_white_background.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Dipeshpal/Image-Segmentation-with-Unet-using-tensor flow-Keras/blob/master/training _ white _ background . ipynb</a></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mx"><img src="../Images/8d94072c84c4a5fe482b84d2b21b51cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ANA-DJ33HHmofrUqA7UjpQ.png"/></div></div></figure><p id="55fe" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">结果6.3:图片带紫色背景- </em> </strong></p><p id="3992" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">结果分析</strong>:43个纪元后，colab再次崩溃。我们取得了以下成果。结果与黑色背景或白色背景的结果非常相似。</p><p id="5289" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">笔记本:</strong><a class="ae lm" href="https://github.com/Dipeshpal/Image-Segmentation-with-Unet-using-Tensorflow-Keras/blob/master/training_purple_background.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Dipeshpal/Image-Segmentation-with-Unet-using-tensor flow-Keras/blob/master/training _ purple _ background . ipynb</a></p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mv"><img src="../Images/7568ee07428a8cf150959104fef8711a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZMl9JbvqEng-6yx69exUQ.png"/></div></div></figure><h2 id="50b2" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">让我们来谈谈图像生成器、训练参数、回调、库和其他东西</h2><p id="867c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">代码解释了一切。我的GitHub上的其他内容。</p><p id="ae7c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以下<strong class="jf hj">培训代码</strong>对于所有笔记本都是相同的(对于我们已经创建的三个数据集)，唯一的变化是型号名称和目录。</p><p id="3988" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">导入OCHuman API- </em> </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="1068" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">其他进口- </em> </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="ad12" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">阅读JSON注释(标签)- </em> </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="fb7b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">参数和训练图像- </em> </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="c7ec" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">特性和标签- </em> </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="8c4a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">列车，有效并测试拆分- </em> </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="1cb7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">自定义Keras数据生成器- </em> </strong></p><p id="88b7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们将在训练时调用使用该函数，它将给出(返回)所需的一批图像。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="64f7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">可选，如果要打印" keras_generator_train_val_test "生成的图像-</em>-</strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="fa9e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">型号- </em> </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="7fd8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">训练时自定义回调生成中间输出- </em> </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="8509" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">获取模型并打印摘要- </em> </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><p id="0468" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mb">让火车- </em> </strong></p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mr ms l"/></div></figure><h2 id="baf6" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">让我们把它们放在一起-</h2><p id="5385" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">上面代码的GitHub这里是<a class="ae lm" href="https://github.com/Dipeshpal/Image-Segmentation-with-Unet-using-Tensorflow-Keras/blob/master/training_black_background.ipynb" rel="noopener ugc nofollow" target="_blank">这里是</a>。我使用Google Colab进行培训，因此您可能需要根据自己的情况更改目录。顺便说一下，所有的代码(自定义数据集生成器和训练)也可以在这篇文章的“代码GitHub”部分找到。</p><h1 id="e07f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">7.你可以尝试-</h1><h2 id="7ba7" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">7.1.使用Unet进行迁移学习</h2><p id="ec09" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">你也可以在Unet上尝试迁移学习，是的，你没听错，你也可以使用迁移学习。还有一个预先训练好的Unet模型，如vgg16或resnet50等。迁移学习将有助于Unet的图像压缩块快速学习和学习更多。也许我会在其他文章中讨论这个问题。为此，您可能需要使用这个<a class="ae lm" href="https://github.com/divamgupta/image-segmentation-keras" rel="noopener ugc nofollow" target="_blank"> Github repo </a> (Keras Unet预训练库)。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es my"><img src="../Images/3c9d6c1043a7b79dec64e71840e8a7eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*DEbE4EG0-FSR9MifaeA_DA.png"/></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">来源:<a class="ae lm" href="https://divamgupta.com/image-segmentation/2019/06/06/deep-learning-semantic-segmentation-keras.html" rel="noopener ugc nofollow" target="_blank">https://divamgupta . com/image-segmentation/2019/06/06/deep-learning-semantic-segmentation-keras . html</a></figcaption></figure><h2 id="6ba4" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">7.2.尝试灰度图像</h2><p id="f09f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">是的，您也可以尝试将灰度图像用作要素和标注。输入数据的维数将减少到(512，512，1)，因为灰度图像只有一个通道，而RGB图像有(512，512，3)个通道。所以，我的直觉是在色彩数据集(RGB)模型中可以学习一些色彩到色彩的映射。这可能是一个问题，所以你可以尝试灰度。但是请记住，在灰度图像中可能会出现相同的问题，因为输入要素和输入标注(掩膜)都是灰度，我不知道什么模型会学习，我没有尝试过。</p><h2 id="d882" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">7.3.一些其他架构或模型</h2><p id="3c05" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">有许多不同类型的模式可用，而不是使用U-Net，你可以使用R-CNN，FCN，VGG-16，ResNet等。您也可以增加或减少Unet或这些其他模型中的可训练参数。在Unet中分别增加或减少压缩或扩展块。</p><h2 id="1ccf" class="ln ig hi bd ih lo lp lq il lr ls lt ip jo lu lv it js lw lx ix jw ly lz jb ma bi translated">7.4.使用GAN(生成对抗网络)</h2><p id="9e5a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">是的，你可以用甘的。GAN又是一个需要讨论的广泛领域，所以我不会说太多。你可以使用类似GAN系统的编码器-解码器来生成你想要模型生成的图像。记住GANs需要大量的计算能力，你可能需要高端的GPU或者让你的Colab运行几天或者几周，但是你不能。在你自己的系统中，你可以，但你家里可能没有<strong class="jf hj">英伟达特斯拉K80 GPU </strong>。</p><p id="abcd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 7.5。你可以在这里评论或提及你所做的或创造的东西，这样其他人也可以了解新事物。</strong></p><h1 id="90bd" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"> 7。代码(GitHub) </strong></h1><p id="a102" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">您可以在本节中找到所有代码和有用的资源。你可以<strong class="jf hj">随意使用我的代码</strong>，如果你能提到<strong class="jf hj">我的工作</strong>的功劳，那将是可观的。</p><blockquote class="mz na nb"><p id="7eff" class="jd je mb jf b jg kb ji jj jk kc jm jn nc kd jq jr nd ke ju jv ne kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="hi">你可以在我的网站上了解更多:</em></strong><a class="ae lm" href="http://www.dipeshpal.com" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj"><em class="hi">www.dipeshpal.com</em></strong></a></p><p id="f4ea" class="jd je mb jf b jg kb ji jj jk kc jm jn nc kd jq jr nd ke ju jv ne kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="hi">你可以了解我更多:</em></strong><a class="ae lm" href="http://www.dipeshpal.in" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj"><em class="hi">www . dipeshpal . I</em></strong></a><strong class="jf hj"><em class="hi">n</em></strong></p><p id="db4f" class="jd je mb jf b jg kb ji jj jk kc jm jn nc kd jq jr nd ke ju jv ne kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="hi">你可以在YouTube上看我的科技视频:</em></strong><a class="ae lm" href="https://www.youtube.com/DIPESHPAL17" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj"><em class="hi">https://www.youtube.com/DIPESHPAL17</em></strong></a></p><p id="4c25" class="jd je mb jf b jg kb ji jj jk kc jm jn nc kd jq jr nd ke ju jv ne kf jy jz ka hb bi translated"><strong class="jf hj"><em class="hi">Linkedin:</em></strong><a class="ae lm" href="https://www.linkedin.com/in/dipesh-pal-a34952110/" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj"><em class="hi">https://www.linkedin.com/in/dipesh-pal-a34952110/</em></strong></a></p><p id="435c" class="jd je mb jf b jg kb ji jj jk kc jm jn nc kd jq jr nd ke ju jv ne kf jy jz ka hb bi translated"><strong class="jf hj"> <em class="hi">推特:</em></strong><a class="ae lm" href="https://twitter.com/dipesh_pal17" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj"><em class="hi">https://twitter.com/dipesh_pal17</em></strong></a></p><p id="cc09" class="jd je mb jf b jg kb ji jj jk kc jm jn nc kd jq jr nd ke ju jv ne kf jy jz ka hb bi translated"><strong class="jf hj"><em class="hi">GitHub:</em></strong><a class="ae lm" href="https://github.com/Dipeshpal" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj"><em class="hi">https://github.com/Dipeshpal</em></strong></a></p></blockquote><ol class=""><li id="bd0b" class="kx ky hi jf b jg kb jk kc jo nf js ng jw nh ka lc ld le lf bi translated"><strong class="jf hj">GitHub Code:</strong><a class="ae lm" href="https://github.com/Dipeshpal/Image-Segmentation-with-Unet-using-Tensorflow-Keras" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/Dipeshpal/Image-Segmentation-with-Unet-using-tensor flow-Keras</a>(你可以使用这个模块在你的系统上运行，但我会推荐你使用Google Colab)</li><li id="70f7" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">资源- </strong></li></ol><p id="5a0a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">用Unet迁移学习:<a class="ae lm" href="https://divamgupta.com/image-segmentation/2019/06/06/deep-learning-semantic-segmentation-keras.html" rel="noopener ugc nofollow" target="_blank">https://divamgupta . com/image-segmentation/2019/06/06/deep-Learning-semantic-segmentation-keras . html</a></p><p id="5acd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">使用Unet进行GitHub迁移学习:<a class="ae lm" href="https://github.com/divamgupta/image-segmentation-keras" rel="noopener ugc nofollow" target="_blank">https://github.com/divamgupta/image-segmentation-keras</a></p><p id="51ba" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">U-Net:用于生物医学图像分割的卷积网络:<a class="ae lm" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1505.04597</a></p><p id="9fb4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">奥丘曼(被遮挡的人类)数据集API:<a class="ae lm" href="https://github.com/liruilong940607/OCHumanApi" rel="noopener ugc nofollow" target="_blank">https://github.com/liruilong940607/OCHumanApi</a></p><p id="090a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">奥楚曼数据集:<a class="ae lm" href="https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman" rel="noopener ugc nofollow" target="_blank">https://cg.cs.tsinghua.edu.cn/dataset/form.html?dataset=ochuman </a></p><p id="1730" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">可可数据集:<a class="ae lm" href="http://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">http://cocodataset.org/#home</a></p><p id="efa6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">张量流:【http://tensorflow.org/ T2】</p><p id="c6b7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">对于任何学分，建议，改变，或任何事情，请在这里评论或联系我。</strong></p><blockquote class="ni"><p id="e184" class="nj nk hi bd nl nm nn no np nq nr ka dx translated">非常感谢你的阅读，如果你觉得这有帮助，请分享。</p></blockquote></div></div>    
</body>
</html>