<html>
<head>
<title>Re-sampling Imbalanced Training Corpus for Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向情感分析的不平衡训练语料重采样</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/re-sampling-imbalanced-training-corpus-for-sentiment-analysis-c9dc97f9eae1?source=collection_archive---------0-----------------------#2019-03-19">https://medium.com/analytics-vidhya/re-sampling-imbalanced-training-corpus-for-sentiment-analysis-c9dc97f9eae1?source=collection_archive---------0-----------------------#2019-03-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8f23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我将讨论过采样、欠采样和两者结合的技术，以平衡不平衡的训练数据集，解决在线Twitter情感分析的挑战。该挑战旨在检测人工判断的测试语料库中的仇恨和辱骂言论。包括培训和测试数据在内的挑战在此处<a class="ae jd" href="https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/" rel="noopener ugc nofollow" target="_blank">可用。</a></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/9fc7517c6a0c793c6ddeae8fa335a947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SIKIaS3Ww3SHHqOgOoFSBA.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">对不平衡数据集进行重新采样肯定会提高分类效果</figcaption></figure><p id="ab7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练语料库包含人工判断的推文，那些被认为是滥用的推文被标记为1，其他正常推文被标记为0。</p><p id="6fbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们首先导入必要的库来继续:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="8e76" class="jz ka hi jv b fi kb kc l kd ke">#import the necessary libraries for dataset preparation, feature engineering, model training<br/>from sklearn import model_selection, preprocessing, metrics, linear_model, svm<br/>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer<br/>from imblearn.over_sampling import BorderlineSMOTE, SMOTE, ADASYN, SMOTENC, RandomOverSampler<br/>from imblearn.under_sampling import (RandomUnderSampler, <br/>                                    NearMiss, <br/>                                    InstanceHardnessThreshold,<br/>                                    CondensedNearestNeighbour,<br/>                                    EditedNearestNeighbours,<br/>                                    RepeatedEditedNearestNeighbours,<br/>                                    AllKNN,<br/>                                    NeighbourhoodCleaningRule,<br/>                                    OneSidedSelection,<br/>                                    TomekLinks)<br/>from imblearn.combine import SMOTEENN, SMOTETomek<br/>from imblearn.pipeline import make_pipeline<br/>import pandas as pd, numpy, string<br/>from nltk.tokenize import WordPunctTokenizer<br/>from nltk.stem import PorterStemmer<br/>from bs4 import BeautifulSoup<br/>#Remove Special Charactors<br/>import re<br/>from nltk.tokenize import WordPunctTokenizer<br/>from nltk.stem import PorterStemmer<br/>from bs4 import BeautifulSoup</span></pre><p id="c453" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们需要导入培训和测试csv文件，并查看部分文本:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="ae08" class="jz ka hi jv b fi kb kc l kd ke">#Import Training and Testing Data<br/>train = pd.read_csv('train.csv')<br/>print("Training Set:"% train.columns, train.shape, len(train))<br/>test = pd.read_csv('test_tweets.csv')<br/>print("Test Set:"% test.columns, test.shape, len(test))</span><span id="eee7" class="jz ka hi jv b fi kf kc l kd ke">Training Set: (31962, 3) 31962<br/>Test Set: (17197, 2) 17197</span></pre><p id="ed34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练集由32K tweets组成，测试集由17K tweets组成。让我们来探索一下这两组的最高记录:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="61d1" class="jz ka hi jv b fi kb kc l kd ke">train.head()<br/>id      label   tweet<br/>1       0       @user when a father is dysfunctional and is                2       0       @user @user thanks for #lyft credit i can't                 3       0       bihday your majesty                 <br/>4       0       #model   i love u take with u all the time i               5       0       factsguide: society now    #motivation\</span><span id="4fa2" class="jz ka hi jv b fi kf kc l kd ke">test.head()<br/>id          tweet<br/>31963       #studiolife #aislife #requires #passion #dedic                 31964       @user #white #supremacists want everyone to s...                 31965       safe ways to heal your #acne!! #altwaystohe...                 31966       is the hp and the cursed child book up for res.              31967       3rd #bihday to my amazing, hilarious #nephew...</span></pre><p id="e34b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您注意到语料库取代了对@ user用户名的提及，文本还包括#标签、特殊字符和数字。让我们看看在训练语料库中辱骂推文占正常推文的百分比:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="b2d4" class="jz ka hi jv b fi kb kc l kd ke">#Percentage of Positive/Negative<br/>print("Positive: ", train.label.value_counts()[0]/len(train)*100,"%")<br/>print("Negative: ", train.label.value_counts()[1]/len(train)*100,"%")</span></pre><p id="0888" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">负面推文占数据集的93%，负面占7%:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="2a2a" class="jz ka hi jv b fi kb kc l kd ke">Positive:  92.98542018647143 %<br/>Negative:  7.014579813528565 %</span></pre><p id="13aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着我们有不平衡的训练数据集，这肯定会影响预测的准确性。为了使这个论点有效，我们将首先通过分割判断的训练数据来预测负面和正面的推文，然后将预测的标签与人类判断的标签进行比较。但在我们开始之前，让我们清除tweets中的数字、html/xml标签、特殊字符、空格以及词干，这些都在下面的代码块中完成:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="1bad" class="jz ka hi jv b fi kb kc l kd ke">porter=PorterStemmer()<br/>tok = WordPunctTokenizer()<br/>pat1 = r'@[A-Za-z0-9]+'<br/>pat2 = r'https?://[A-Za-z0-9./]+'<br/>combined_pat = r'|'.join((pat1, pat2))</span><span id="d80c" class="jz ka hi jv b fi kf kc l kd ke">def tweet_cleaner(text):<br/>    soup = BeautifulSoup(text, 'lxml')<br/>    souped = soup.get_text()<br/>    stripped = re.sub(combined_pat, '', souped)<br/>    try:<br/>        clean = stripped.decode("utf-8-sig").replace(u"\ufffd", "?")<br/>    except:<br/>        clean = stripped<br/>    letters_only = re.sub("[^a-zA-Z]", " ", clean)<br/>    lower_case = letters_only.lower()<br/>    # During the letters_only process two lines above, it has created unnecessay white spaces,<br/>    # I will tokenize and join together to remove unneccessary white spaces<br/>    words = tok.tokenize(lower_case)<br/>    #Stemming<br/>    stem_sentence=[]<br/>    for word in words:<br/>        stem_sentence.append(porter.stem(word))<br/>        stem_sentence.append(" ")<br/>    words="".join(stem_sentence).strip()<br/>    return words<br/>nums = [0,len(train)]<br/>clean_tweet_texts = []<br/>for i in range(nums[0],nums[1]):<br/>    clean_tweet_texts.append(tweet_cleaner(train['tweet'][i]))<br/>nums = [0,len(test)]<br/>test_tweet_texts = []<br/>for i in range(nums[0],nums[1]):<br/>    test_tweet_texts.append(tweet_cleaner(test['tweet'][i])) <br/>train_clean = pd.DataFrame(clean_tweet_texts,columns=['tweet'])<br/>train_clean['label'] = train.label<br/>train_clean['id'] = train.id<br/>test_clean = pd.DataFrame(test_tweet_texts,columns=['tweet'])<br/>test_clean['id'] = test.id</span></pre><p id="5118" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们将训练数据集分为训练和测试:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="1b0c" class="jz ka hi jv b fi kb kc l kd ke">#split the dataset into training and validation datasets <br/>train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train_clean['tweet'],train_clean['label'])<br/>#label encode the target variable <br/>encoder = preprocessing.LabelEncoder()<br/>train_y = encoder.fit_transform(train_y)<br/>valid_y = encoder.fit_transform(valid_y)</span></pre><p id="10c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在前面的代码中，请注意，我必须将标签列转换为一个数组，因为我现在将使用术语频率TF和逆文档频率IDF将推文转换为术语的加权向量，在这里阅读更多关于它们的信息<a class="ae jd" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="acd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有不止一个选项来评估术语的重要性，最好的方法之一是使用TFIDF，为了便于讨论，您可以尝试使用bag of words BOW。</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="dc8b" class="jz ka hi jv b fi kb kc l kd ke"># word level tf-idf<br/>tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=100000)<br/>tfidf_vect.fit(train_clean['tweet'])<br/>xtrain_tfidf =  tfidf_vect.transform(train_x)<br/>xvalid_tfidf =  tfidf_vect.transform(valid_x)</span></pre><h1 id="ede4" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">评估指标</h1><p id="0daa" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">准确性是一个很好的衡量标准，但只有当你有平衡的数据集，其中标签0和1被几乎相等地分解。因此，我们需要看看其他参数来评估模型的性能。在我们的模型中，我们可以得到很高的准确率，因为0约占语料库的93%。</p><p id="30f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个模型，我使用了<strong class="ih hj"> F1得分</strong>——这是精确度和召回率的加权平均值。</p><p id="d0bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="li"> F1得分= 2*(召回率*精确度)/(召回率+精确度)</em></p><p id="b01a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有关评估技术的更多详情，请点击查看<a class="ae jd" href="https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="0bba" class="jz ka hi jv b fi kb kc l kd ke">#Return the f1 Score<br/>def train_model(classifier, feature_vector_train, label, feature_vector_valid):<br/>    # fit the training dataset on the classifier<br/>    classifier.fit(feature_vector_train, label)<br/>    <br/>    # predict the labels on validation dataset<br/>    predictions = classifier.predict(feature_vector_valid)</span><span id="1871" class="jz ka hi jv b fi kf kc l kd ke">return metrics.f1_score(valid_y,predictions)</span></pre><h1 id="4407" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">分类器</h1><p id="0a84" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">我将使用线性分类器(逻辑回归LR)和支持向量机(SVM)分类器来预测负面和正面的推文。LR通过使用logistic/sigmoid函数估计概率来衡量分类因变量与一个或多个自变量之间的关系。</p><p id="8cee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SVM算法寻找在N维空间(N-特征的数量)中具有最大余量的超平面，该超平面清楚地分类数据点。更多关于这两种分类器的信息可以在<a class="ae jd" href="https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f" rel="noopener" target="_blank">这里</a>找到。</p><p id="3ce3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看基线结果(没有重新采样):</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="6950" class="jz ka hi jv b fi kb kc l kd ke">accuracyORIGINAL = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),xtrain_tfidf, train_y, xvalid_tfidf)<br/>print ("LR Baseline, WordLevel TFIDF: ", accuracyORIGINAL)<br/>accuracyORIGINAL = train_model(svm.LinearSVC(), xtrain_tfidf, train_y, xvalid_tfidf)<br/>print ("SVM Baseline, WordLevel TFIDF: ", accuracyORIGINAL)</span><span id="fe63" class="jz ka hi jv b fi kf kc l kd ke">#LR Baseline, WordLevel TFIDF:  0.534131736526946<br/>#SVM Baseline, WordLevel TFIDF:  0.6991701244813279</span></pre><h1 id="c0a1" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">重新采样训练语料</h1><p id="4574" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">平衡训练数据集有许多方法，我们将在本文中讨论:欠采样、过采样以及过采样和欠采样相结合。</p><p id="f6ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">欠采样</strong>通过减少多数类的大小来平衡数据集。当数据量足够时，使用这种方法。通过将所有样本保留在少数类中并在多数类中随机选择相等数量的样本。在我们的模型中，正面和负面的推文之间有巨大的差距，只有7%的推文是负面的，所以欠采样可能不会产生很大的结果。我们将在本文中看到一些场景。</p><p id="73cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">过采样</strong>在数据量不足时使用。它试图通过增加少数样本的大小来平衡数据集。不是去除多数样本，而是通过使用:重复、自举、<a class="ae jd" href="https://arxiv.org/pdf/1106.1813.pdf" rel="noopener ugc nofollow" target="_blank"> SMOTE </a>(合成少数过采样技术)或<a class="ae jd" href="https://scinapse.io/papers/2104933073" rel="noopener ugc nofollow" target="_blank"> ADASYN </a>(自适应合成采样)来生成新的少数样本。</p><p id="237a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将这两种方法结合起来是对数据集进行重采样的另一种方法。选择重采样技术取决于模型本身，对于我们的模型，我们将评估这三种方法。</p><h1 id="73f6" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">随机过采样</h1><p id="747a" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">下面的代码块使用Scikit learn过采样方法随机重复一些少数样本，并在数据集之间平衡样本数量。</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="8dfa" class="jz ka hi jv b fi kb kc l kd ke">#Random Over Sampling<br/>ros = RandomOverSampler(random_state=777)<br/>ros_xtrain_tfidf, ros_train_y = ros.fit_sample(xtrain_tfidf, train_y)</span><span id="4963" class="jz ka hi jv b fi kf kc l kd ke">accuracyROS = train_model(linear_model.LogisticRegression(random_state=0, solver=’lbfgs’,multi_class=’multinomial’),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf)<br/>print (“LR ORIGINAL, WordLevel TFIDF: “, accuracyROS)<br/>accuracyROS = train_model(svm.LinearSVC(),ros_xtrain_tfidf, ros_train_y, xvalid_tfidf)<br/>print ("SVM ROS, WordLevel TFIDF: ", accuracyROS)</span><span id="0f60" class="jz ka hi jv b fi kf kc l kd ke">#LR ROS, WordLevel TFIDF:  0.6822066822066821<br/>#SVM ROS, WordLevel TFIDF:  0.6995744680851064</span></pre><p id="0f65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从f1的分数结果可以清楚地看出，用这种简单的方法已经不太偏向于多数阶级了。但是如果我们把它应用到真实的测试数据中，我们会得到这个结果吗？你可以自己试试。</p><h1 id="a4d8" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">SMOTE过采样</h1><p id="3531" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">SMOTE是一种合成少数民族过采样方法，其中通过创建“合成”样本而不是从现有少数民族类创建新的随机少数民族样本来对少数民族类进行过采样。</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="0e74" class="jz ka hi jv b fi kb kc l kd ke">#SMOTE<br/>sm = SMOTE(random_state=777, ratio = 1.0)<br/>sm_xtrain_tfidf, sm_train_y = sm.fit_sample(xtrain_tfidf, train_y)</span><span id="95a7" class="jz ka hi jv b fi kf kc l kd ke">accuracySMOTE = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf)<br/>print ("LR SMOTE, WordLevel TFIDF: ", accuracySMOTE)<br/>accuracySMOTE = train_model(svm.LinearSVC(),sm_xtrain_tfidf, sm_train_y, xvalid_tfidf)<br/>print ("SVC SMOTE, WordLevel TFIDF: ", accuracySMOTE)</span><span id="8999" class="jz ka hi jv b fi kf kc l kd ke">#LR SMOTE, WordLevel TFIDF:  0.6848436246992782<br/>#SVC SMOTE, WordLevel TFIDF:  0.693288020390824</span></pre><p id="1b16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，SMOTE的得分略高于ROS，但这是针对训练/测试数据的，但对于真实的测试数据，这可能会发生，请使用test_clean数据集尝试一下。</p><h1 id="e11a" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">ADASYN:自适应合成采样(过采样)</h1><p id="e0f1" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">ADASYN相对于SMOTE的优势在于根据不同少数类样本的学习难度对它们使用加权分布，其中与那些较容易学习的少数类样本相比，为较难学习的少数类样本生成更多的合成数据。更多详情请点击<a class="ae jd" href="https://scinapse.io/papers/2104933073" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="55d1" class="jz ka hi jv b fi kb kc l kd ke">#ADASYN<br/>ad = ADASYN(random_state=777, ratio = 1.0)<br/>ad_xtrain_tfidf, ad_train_y = ad.fit_sample(xtrain_tfidf, train_y)<br/>accuracyADASYN = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),ad_xtrain_tfidf, ad_train_y, xvalid_tfidf)<br/>print ("LR ADASYN, WordLevel TFIDF: ", accuracyADASYN)<br/>accuracyADASYN = train_model(svm.LinearSVC(),ad_xtrain_tfidf, ad_train_y, xvalid_tfidf)<br/>print ("SVM ADASYN, WordLevel TFIDF: ", accuracyADASYN)</span><span id="a948" class="jz ka hi jv b fi kf kc l kd ke">#LR ADASYN, WordLevel TFIDF:  0.6666666666666666<br/>#SVM ADASYN, WordLevel TFIDF:  0.6818181818181819</span></pre><p id="2efc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f1得分结果再次低于之前的两种方法，但这仍然是在训练/测试语料库上。</p><p id="e9fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在之前的3个过采样示例中，<code class="du lj lk ll jv b"><a class="ae jd" href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.RandomOverSampler.html#imblearn.over_sampling.RandomOverSampler" rel="noopener ugc nofollow" target="_blank">RandomOverSampler</a></code>通过复制少数类的一些原始样本来过采样，而<code class="du lj lk ll jv b"><a class="ae jd" href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE" rel="noopener ugc nofollow" target="_blank">SMOTE</a></code>和<code class="du lj lk ll jv b"><a class="ae jd" href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.ADASYN.html#imblearn.over_sampling.ADASYN" rel="noopener ugc nofollow" target="_blank">ADASYN</a></code>通过插值在语料库中生成新样本。然而，用于生成新合成样本的样本是不同的。<code class="du lj lk ll jv b"><a class="ae jd" href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.ADASYN.html#imblearn.over_sampling.ADASYN" rel="noopener ugc nofollow" target="_blank">ADASYN</a></code>方法着重于生成与使用k-最近邻分类器(难以学习)错误分类的原始样本相邻的样本，而<code class="du lj lk ll jv b"><a class="ae jd" href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html#imblearn.over_sampling.SMOTE" rel="noopener ugc nofollow" target="_blank">SMOTE</a></code>的基本实现不会对使用最近邻规则分类的简单和困难学习样本进行任何区分。</p><p id="a886" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">值得一提的是，SMOTE可能会连接内点和外点，因此这种方法提供了三个额外的选项来生成样本。这些方法侧重于最佳决策函数边界附近的样本，并将生成与最近邻类方向相反的样本。我实验过<code class="du lj lk ll jv b">BorderlineSMOTE</code>和<code class="du lj lk ll jv b">SMOTENC.</code></p><p id="cf0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> SMOTE-NC </strong>通过合成新的少数样本来使用SMOTE方法，但通过执行针对分类特征的特定操作来略微改变新样本的生成方式。事实上，新生成样本的类别是通过挑选在生成期间出现的最频繁类别的最近邻来决定的:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="617d" class="jz ka hi jv b fi kb kc l kd ke">#SMOTENC<br/>smnc = SMOTENC(categorical_features=[0, 2], random_state=0)<br/>smnc_xtrain_tfidf, smnc_train_y = smnc.fit_sample(xtrain_tfidf, train_y)<br/>accuracySMOTENC = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),smnc_xtrain_tfidf, smnc_train_y, xvalid_tfidf)<br/>print ("LR SMOTENC, WordLevel TFIDF: ", accuracySMOTENC)<br/>accuracySMOTENC = train_model(svm.LinearSVC(),smnc_xtrain_tfidf, smnc_train_y, xvalid_tfidf)<br/>print ("SVM SMOTENC, WordLevel TFIDF: ", accuracySMOTENC)</span><span id="b2ac" class="jz ka hi jv b fi kf kc l kd ke">#LR SMOTENC, WordLevel TFIDF:  0.534131736526946<br/>#SVM SMOTENC, WordLevel TFIDF:  0.6978193146417445</span></pre><p id="69ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">边界线</strong> SMOTE将使用处于危险(弱判断)的样本生成新样本；</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="ac83" class="jz ka hi jv b fi kb kc l kd ke">#Borderline SMOTE<br/>bsm = BorderlineSMOTE()<br/>bsm_xtrain_tfidf, bsm_train_y = bsm.fit_sample(xtrain_tfidf, train_y)<br/>accuracyBSMOTE = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),bsm_xtrain_tfidf, bsm_train_y, xvalid_tfidf)<br/>print ("LR Borderline SMOTE, WordLevel TFIDF: ", accuracyBSMOTE)<br/>accuracyBSMOTE = train_model(svm.LinearSVC(),bsm_xtrain_tfidf, bsm_train_y, xvalid_tfidf)<br/>print ("SVM Borderline SMOTE, WordLevel TFIDF: ", accuracyBSMOTE)</span><span id="cae9" class="jz ka hi jv b fi kf kc l kd ke">#LR Borderline SMOTE, WordLevel TFIDF:  0.6799999999999999<br/>#SVM Borderline SMOTE, WordLevel TFIDF:  0.6947368421052631</span></pre><p id="b1f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SMOTE-NC和Borderline SMOTE似乎提高了两种分类器的性能，但大大提高了LR。</p><h1 id="d4b1" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">随机欠采样</h1><p id="ed31" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">随机欠采样是一种受控欠采样方法，其中可以定义要选择的样本数量。这种方法随机减少了多数类以取得平衡，让我们看看评价:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="3234" class="jz ka hi jv b fi kb kc l kd ke"># Random Under Sampling<br/>rus = RandomUnderSampler(random_state=0, replacement=True)<br/>rus_xtrain_tfidf, rus_train_y = rus.fit_sample(xtrain_tfidf, train_y)<br/>accuracyrus = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),rus_xtrain_tfidf, rus_train_y, xvalid_tfidf)<br/>print ("LR RUS, WordLevel TFIDF: ", accuracyrus)<br/>accuracyrus = train_model(svm.LinearSVC(),rus_xtrain_tfidf, rus_train_y, xvalid_tfidf)<br/>print ("SVC RUS, WordLevel TFIDF: ", accuracyrus)</span><span id="bb34" class="jz ka hi jv b fi kf kc l kd ke">#LR RUS, WordLevel TFIDF:  0.5045045045045046<br/>#SVC RUS, WordLevel TFIDF:  0.5091093117408907</span></pre><p id="2f8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以f1分数低于所有使用的过采样方法。让我们试试别的东西。</p><h1 id="e7ed" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">差点错过</h1><p id="407f" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">NearMiss方法是<a class="ae jd" href="https://imbalanced-learn.readthedocs.io/en/stable/index.html" rel="noopener ugc nofollow" target="_blank">不平衡学习</a>库的一部分，NearMiss执行启发式规则以选择样本。它根据多数类中样本与同一类中其他样本的距离，对这些样本执行欠采样。这种方法使用三种不同的变体:</p><p id="1165" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> NearMiss-1 </strong>保留来自多数类的样本，对于这些样本，少数类的<em class="li"> k </em> ( <a class="ae jd" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">可调超参数</a>)最近样本的平均距离是最小的。</p><p id="07e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> NearMiss-2 </strong>保留来自多数类的样本，其到少数类中<em class="li"> k </em>最远样本的平均距离最低。</p><p id="7e40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> NearMiss-3 </strong>为少数类中的每个样本选择<em class="li"> k </em>多数类中的最近邻居。所以欠采样率由<em class="li"> k </em>直接控制，不单独调整。</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="47f4" class="jz ka hi jv b fi kb kc l kd ke">#NearMiss<br/>for sampler in (NearMiss(version=1),NearMiss(version=2),NearMiss(version=3)):<br/>    nm_xtrain_tfidf, nm_train_y = sampler.fit_sample(xtrain_tfidf, train_y)<br/>    accuracysm = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),nm_xtrain_tfidf, nm_train_y, xvalid_tfidf)<br/>    print ("LR NearMiss(version= {0}), WordLevel TFIDF: ".format(sampler.version), accuracysm)<br/><br/>for sampler in (NearMiss(version=1),NearMiss(version=2),NearMiss(version=3)):<br/>    nm_xtrain_tfidf, nm_train_y = sampler.fit_sample(xtrain_tfidf, train_y)<br/>    accuracysm = train_model(svm.LinearSVC(),nm_xtrain_tfidf, nm_train_y, xvalid_tfidf)<br/>    print ("SVC NearMiss(version= {0}), WordLevel TFIDF: ".format(sampler.version), accuracysm)</span><span id="4093" class="jz ka hi jv b fi kf kc l kd ke">#LR NearMiss(version= 1), WordLevel TFIDF:  0.2734271303424476<br/>#LR NearMiss(version= 2), WordLevel TFIDF:  0.5236625514403291<br/>#LR NearMiss(version= 3), WordLevel TFIDF:  0.5541591861160982<br/>#SVC NearMiss(version= 1), WordLevel TFIDF:  0.3597020219936148<br/>#SVC NearMiss(version= 2), WordLevel TFIDF:  0.5224948875255624<br/>#SVC NearMiss(version= 3), WordLevel TFIDF:  0.5594149908592323</span></pre><p id="57f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，NearMiss-3的性能超过了其他两个版本。但是仍然低于过采样结果。</p><h1 id="50bb" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">Tomek链接删除</h1><p id="d650" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">如果一对样本属于不同的类并且是彼此最近的邻居，则称它们为Tomek链接。欠采样可以通过从数据集中移除所有tomek链接来完成。另一种方法是仅移除作为Tomek链接一部分的多数类样本。<a class="ae jd" href="https://imbalanced-learn.readthedocs.io/en/stable/under_sampling.html" rel="noopener ugc nofollow" target="_blank"> <em class="li">参考1 </em> </a> <em class="li">，</em> <a class="ae jd" href="https://arxiv.org/pdf/1608.06048.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="li">参考2 </em> </a></p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="804d" class="jz ka hi jv b fi kb kc l kd ke"># Under-Sampling TomekLinks<br/>tl = TomekLinks()<br/>tl_xtrain_tfidf, tl_train_y = tl.fit_sample(xtrain_tfidf, train_y)<br/>accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),tl_xtrain_tfidf, tl_train_y, xvalid_tfidf)<br/>print ("LR TomekLinks, WordLevel TFIDF: ", accuracy)<br/>accuracy = train_model(svm.LinearSVC(),tl_xtrain_tfidf, tl_train_y, xvalid_tfidf)<br/>print ("SVC TomekLinks, WordLevel TFIDF: ", accuracy)</span><span id="926a" class="jz ka hi jv b fi kf kc l kd ke">#LR TomekLinks, WordLevel TFIDF:  0.5358851674641147<br/>#SVC TomekLinks, WordLevel TFIDF:  0.6998961578400832</span></pre><p id="32ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看起来，从不同的类中移除最近邻确实改进了SVC分类器，使其优于所有以前的方法。</p><h1 id="e402" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">编辑过的最近邻(ENN)</h1><p id="2493" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">ENN通过移除其类别与其最近邻类别不同的样本来对多数类别进行欠采样。如果重复这个步骤，那么我们驱动一个新的方法<code class="du lj lk ll jv b">RepeatedEditedNearestNeighbours</code></p><p id="70c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一种驱动方法:<code class="du lj lk ll jv b">AllKNN</code>与<code class="du lj lk ll jv b">RepeatedEditedNearestNeighbours</code>略有不同，它通过改变内部最近邻算法的<em class="li"> k </em>参数，在每次迭代时增加它。后两种算法需要更长的处理时间。</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="40e4" class="jz ka hi jv b fi kb kc l kd ke">#ENN - EditedNearestNeighbours<br/>for sampler in (EditedNearestNeighbours(),<br/>         RepeatedEditedNearestNeighbours(),<br/>        AllKNN(allow_minority=True)):<br/>    enn_xtrain_tfidf, enn_train_y = sampler.fit_sample(xtrain_tfidf, train_y)<br/>    accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),enn_xtrain_tfidf, enn_train_y, xvalid_tfidf)<br/>    print ("LR {0}, WordLevel TFIDF: ".format(sampler), accuracy)<br/>for sampler in (EditedNearestNeighbours(),<br/>         RepeatedEditedNearestNeighbours(),<br/>        AllKNN(allow_minority=True)):<br/>    enn_xtrain_tfidf, enn_train_y = sampler.fit_sample(xtrain_tfidf, train_y)<br/>    accuracy = train_model(svm.LinearSVC(),enn_xtrain_tfidf, enn_train_y, xvalid_tfidf)<br/>    print ("SVM {0}, WordLevel TFIDF: ".format(sampler), accuracy)</span><span id="3a44" class="jz ka hi jv b fi kf kc l kd ke">#LR EditedNearestNeighbours:  0.5480093676814988<br/>#LR RepeatedEditedNearestNeighbours:  0.5727170236753101<br/>#LR AllKNN: 0.5547785547785548<br/>#SVM EditedNearestNeighbours:  0.7101303911735205<br/>#SVM RepeatedEditedNearestNeighbours:  0.7046332046332046<br/>#SVM AllKNN:  0.7131474103585658</span></pre><p id="7715" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">K-最近邻<a class="ae jd" href="https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/#what-is-knn" rel="noopener ugc nofollow" target="_blank"> KNN </a>算法是一种健壮且通用的分类器，经常被用作更复杂分类器的基准，如人工神经网络(ANN)和支持向量机(SVM)。尽管简单，KNN可以胜过更强大的分类器</p><p id="924f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">AllKNN应用KNN算法进行欠采样，使用SVM分类器时，其性能超过了所有重采样方法。KNN具有内存开销，并且需要处理时间。</p><h1 id="d7ed" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">浓缩最近邻(CNN)</h1><p id="5da4" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">CNN利用第一最近邻(1-NN)来迭代地决定样本是否应该保留在数据集中。与其他方法相比，通过CNN的欠采样可能较慢，因为它需要多次通过训练数据。CNN被认为是噪声敏感的，并且保留有噪声的样本。另一种驱动方法<code class="du lj lk ll jv b">OneSidedSelection</code>也使用1-NN，并使用上面讨论的<code class="du lj lk ll jv b">TomekLinks</code>来去除被认为有噪声的样本。同样由CNN驱动的<code class="du lj lk ll jv b">NeighbourhoodCleaningRule</code>使用<code class="du lj lk ll jv b">EditedNearestNeighbours</code>移除一些样本。此外，他们使用3个最近邻来移除不符合此规则的样本:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="0616" class="jz ka hi jv b fi kb kc l kd ke">#CNN - CondensedNearestNeighbor<br/>for sampler in (CondensedNearestNeighbour(random_state=0),<br/>        OneSidedSelection(random_state=0),<br/>        NeighbourhoodCleaningRule()):<br/>    nm_xtrain_tfidf, nm_train_y = sampler.fit_sample(xtrain_tfidf, train_y)<br/>    accuracysm = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),nm_xtrain_tfidf, nm_train_y, xvalid_tfidf)<br/>    print ("LR {0}, WordLevel TFIDF: ".format(sampler), accuracysm)</span><span id="d29b" class="jz ka hi jv b fi kf kc l kd ke">for sampler in (CondensedNearestNeighbour(random_state=0),<br/>        OneSidedSelection(random_state=0),<br/>        NeighbourhoodCleaningRule()):<br/>    nm_xtrain_tfidf, nm_train_y = sampler.fit_sample(xtrain_tfidf, train_y)<br/>    accuracysm = train_model(svm.LinearSVC(),nm_xtrain_tfidf, nm_train_y, xvalid_tfidf)<br/>    print ("SVM {0}, WordLevel TFIDF: ".format(sampler), accuracysm)</span><span id="055d" class="jz ka hi jv b fi kf kc l kd ke">#LR CondensedNearestNeighbour:  0.4289156626506024<br/>#LR OneSidedSelection:  0.5358851674641147<br/>#LR NeighbourhoodCleaningRule:  0.5541327124563447</span><span id="7a8c" class="jz ka hi jv b fi kf kc l kd ke">#SVM CondensedNearestNeighbour:  0.48195030473511485<br/>#SVM OneSidedSelection:  0.6998961578400832<br/>#SVM NeighbourhoodCleaningRule:  0.7075376884422111</span></pre><h1 id="80d4" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">结合欠采样和过采样</h1><p id="585d" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">过采样和欠采样还有其他方法，但如何将两者结合起来呢？Ajinkya More在本文中声称，SMOTE和ENN的结合为他的重新采样实验产生了最好的结果，让我们看看我们是否会达到同样的结果:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="f6b2" class="jz ka hi jv b fi kb kc l kd ke"># Re-Sampling SMOTEENN<br/>se = SMOTEENN(random_state=42)<br/>se_xtrain_tfidf, se_train_y = se.fit_sample(xtrain_tfidf, train_y)<br/>accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),se_xtrain_tfidf, se_train_y, xvalid_tfidf)<br/>print ("LR SMOTEENN: ", accuracy)<br/>accuracy = train_model(svm.LinearSVC(),se_xtrain_tfidf, se_train_y, xvalid_tfidf)<br/>print ("SVC SMOTEENN: ", accuracy)</span><span id="617a" class="jz ka hi jv b fi kf kc l kd ke">#LR SMOTEENN:  0.39669421487603307<br/>#SVC SMOTEENN:  0.47154471544715443</span></pre><p id="1170" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是另一种方法<strong class="ih hj"> SMOTE + Tomek链接移除</strong>在我们的实验中运行良好:</p><pre class="jf jg jh ji fd ju jv jw jx aw jy bi"><span id="88b0" class="jz ka hi jv b fi kb kc l kd ke"># Re-Sampling SMOTETomek<br/>se = SMOTETomek(random_state=42)<br/>se_xtrain_tfidf, se_train_y = se.fit_sample(xtrain_tfidf, train_y)</span><span id="8d23" class="jz ka hi jv b fi kf kc l kd ke">accuracy = train_model(linear_model.LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial'),se_xtrain_tfidf, se_train_y, xvalid_tfidf)<br/>print ("LR SMOTETomek: ", accuracy)<br/>accuracy = train_model(svm.LinearSVC(),se_xtrain_tfidf, se_train_y, xvalid_tfidf)<br/>print ("SVC SMOTETomek: ", accuracy)</span><span id="d44a" class="jz ka hi jv b fi kf kc l kd ke">#LR SMOTETomek:  0.6756756756756755<br/>#SVC SMOTETomek:  0.6876061120543293</span></pre><h1 id="ffda" class="kg ka hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">结论</h1><p id="c750" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">为不平衡语料库选择正确的重采样算法对于向分类器提供最佳训练数据集是重要的。在这个长示例中，我讨论了各种过采样、欠采样以及过采样和欠采样相结合的方法，并得出结论，最适合我们模型的方法是应用AllKNN欠采样算法。当在竞赛测试数据上测试代码时，f1的分数是:0 . 46866 . 38686868661</p><p id="c784" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经用<a class="ae jd" rel="noopener" href="/@muabusalah/twitter-hate-speech-sentiment-analysis-6060b45b6d2c">集合极度随机化的树</a>做了同样的练习，这超过了这个方法。</p><p id="188b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">完整的运行代码可以在<a class="ae jd" href="https://github.com/mabusalah/Resampling" rel="noopener ugc nofollow" target="_blank"> github </a>上获得。</p></div></div>    
</body>
</html>