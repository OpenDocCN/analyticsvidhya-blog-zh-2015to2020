<html>
<head>
<title>Catastrophic Forgetting in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的灾难性遗忘</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/catastrophic-forgetting-in-neural-networks-27fe097fc681?source=collection_archive---------7-----------------------#2019-12-22">https://medium.com/analytics-vidhya/catastrophic-forgetting-in-neural-networks-27fe097fc681?source=collection_archive---------7-----------------------#2019-12-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6db5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章是关于使用置换MNIST和MLP网络分析灾难性遗忘的。</p><p id="8a68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习方法已经在各个领域取得了许多突破，但它们受到学分分配和<strong class="ih hj">“遗忘问题”的困扰。</strong>随着时间的推移，深度学习系统的能力越来越强，但是标准的多层感知器(MLP)和传统的训练方法无法在不灾难性地忘记之前学习的训练数据的情况下处理增量学习新任务或类别。我们称这个问题为神经网络中的灾难性遗忘。解决这个问题是至关重要的，以便代理在现实生活中部署时不断学习和改进。简而言之，当你在任务A上训练了一个模型，并使用相同的权重来学习新的任务B时，那么你的模型会忘记关于任务A的学习信息。这意味着它会灾难性地忘记以前的信息。在本文中，我们将测量和分析神经网络中的灾难性遗忘。</p><p id="0fc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">概述</strong></p><ol class=""><li id="aac6" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">这次作业我们将使用MNIST发型。</li><li id="95f4" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">训练网络超过50个历元，以达到期望的性能/精度。</li><li id="d99c" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">现在测试任务A，使用相同的网络，然后在任务B上训练20个周期，现在测试任务A和B。对N个任务(N = 10)继续此操作。</li><li id="3f5d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">纪元总数[50+20+20+20+20+20+20+20+20+20]= 230</li></ol><p id="2c81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">设置</strong></p><p id="7247" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">编码是在Tensorflow 1.14中在Unix环境中使用渴望执行(Python 3.6)完成的。选择一个唯一的种子(使用名称和随机整数)来创建MNIST数据集的10个不同排列(图1 ),每个排列被视为一个不同的任务。同样对于每个排列，<br/>数据集具有49500个训练图像、10000个测试图像和5500个验证图像。我们在置换数据集上训练三种类型的MLPs (2层深度、3层深度、4层深度)。每个MLP为第一个任务训练50个时期，为另外9个任务(230个任务)训练20个时期。目的是最小化神经网络中的灾难性遗忘。如果MLP开始进一步的任务训练，它应该记住并保留以前训练的大部分任务的重量。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es jr"><img src="../Images/125e50d1f76b75487c4b70d76ca4960d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wTjASzXXB6YHIzrzUSZ6Wg.png"/></div></div></figure><p id="bb19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">性能度量</strong> <br/>四种不同的性能度量用于估计神经网络中的遗忘水平。下面讨论了本实验使用的所有指标。</p><ol class=""><li id="e68b" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">合成任务矩阵</strong></li></ol><p id="2739" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在模型完成对每个任务的学习之后，评估其验证和测试性能以形成结果任务矩阵。我们构建一个矩阵</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kd"><img src="../Images/f15c7f69583fc5bf004be31c05bdb525.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*ZWel7mcRDd4DTKHUV7HPrQ.png"/></div></figure><p id="59fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<em class="ke"> M_i，j </em>为模型在任务<em class="ke"> t_j </em>上训练后在任务<em class="ke"> t_i </em>上的分类准确率t。构建结果矩阵后，我们可以使用该矩阵计算其他绩效指标，如<br/> ACC、BWT、CBWT、TBWT。</p><p id="aa34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj"> ACC(平均准确率)得分</strong> <br/>直观来看，ACC得分是对最后一项任务训练完成后，每项任务获得的准确率的平均值(如等式所示，T为任务数)。根据[1]，ACC分数是评估深度神经网络中遗忘的最重要的度量。这个指标越大，模型越好。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kf"><img src="../Images/1a643ae4f8698c6f4bea75fcd274bdb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*4w8KW2lVZIzisp5WiuRXZQ.png"/></div></figure><p id="d0d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。BWT(反向迁移)得分</strong> <br/>反向迁移得分是如果模型在任务n (m &lt; n)上进行训练，则该模型在任务m上的性能如何受到影响的度量。这个指标的值越大越好。通常，由于深度学习模型的遗忘特性，我们会观察到负的反向迁移分数。如果后向迁移得分为正，这意味着该模型在新任务训练后能够提高旧任务的绩效。方程式中给出了BWT的公式。如果两个模型的ACC分数相似，通常选择具有较高BWT的模型。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kg"><img src="../Images/51ce7068fc2db22c8f06fa7422576f41.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*ua41gmPPGZb3y_Tse4AGBQ.png"/></div></figure><p id="f7a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。CBWT(累积反向迁移)分数</strong><br/>【2】提出了另外两个性能指标，因为上面提到的两个指标可能不足以评估神经网络中的遗忘。CBWT是BWT的延伸。CBWT评分的目的是测量整个连续学习过程中遗忘的总量，而不是像BWT只能做的那样简单地检查最后一行。等式中给出了计算CBWT的公式。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kh"><img src="../Images/e25abe5181be00159920caa6af039bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*PrwOJPEoLpx8w4ZtvfbHUQ.png"/></div></figure><p id="598e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中T是当前任务号，T是任务总数。</p><p id="19e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。TBWT(真实后向迁移)得分</strong> <br/>由[2]提出的另一个评估度量使用独立的分类器(G)来比较每个任务的准确度(在对所有任务的训练完成之后)。TBWT类似于BWT，但我们将每个任务的模型退化与黄金标准(即全负荷训练)进行比较。在我们的实验中，独立的分类器是随机森林分类器(有64个估计器，通常每个MNIST排列的准确率为90%)。等式中给出了计算TBWT的公式。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es ki"><img src="../Images/770ad1c9987aae91f35c5029b9ef6933.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*gDAKQBpUB5gtA0umUepFdw.png"/></div></figure><p id="b645" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中G_ii是在任务I上训练的独立分类器的准确度。</p><p id="0eef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最优训练模型</strong></p><p id="260b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本节中，您将看到各种超参数、损失函数和优化器的比较，以找到可以避免灾难性遗忘的最佳神经网络。</p><ol class=""><li id="7c40" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">学习率对比</strong></li></ol><p id="236a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人们注意到，如果学习率太高，我们通常可以在训练期间获得当前任务的非常高的准确度分数。然而，在那种情况下，神经网络对于先前网络的遗忘倾向是更具灾难性的。因此，我们需要找到最佳的学习速率来训练神经网络，以便遗忘不会是灾难性的，并且当前任务的测试精度不会太低。如图2所示(仅针对230个历元的任务1)，当学习率过高时，分类准确率低至30%。然而，当学习率太低时，网络几乎不会忘记。我们在当前任务上没有获得最佳的准确度(第一个任务的准确度在50个时期后只有70%)。因此，学习率被选择为0.0002，这是经验分析后观察到的最佳学习率。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es kj"><img src="../Images/78d2b3897c50ff0914eaf3bba24b44f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IK_qbIEUbNbEkh8IkRMRLA.png"/></div></div></figure><p id="06d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。优化者对比</strong></p><p id="36c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">三个不同的优化器(Adam、RMSProp和SGD)在这个实验中进行了比较，学习率为0.0002，以查看哪个优化器有助于网络避免灾难性的遗忘。在图3中，我们可以看到针对第一个任务(230个时期)训练的优化器的验证准确性的比较。如图所示，Adam和RMSProp比SGD更适合这个目的，因为SGD的精度急剧下降。然而，我们观察到，当用Adam或RMSProp训练时，ACC和BWT分数是差不多的。对于其余的实验，Adam被用作期望的优化器。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es kk"><img src="../Images/2f6cc1e7ce57563001c040a4f18b08d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mhBoKKEfm5u87Oy91rE1fg.png"/></div></div></figure><p id="9dc0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。损失函数比较</strong></p><p id="aa30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为这是一个分类任务，所以使用了具有softmax函数和一键编码的分类交叉熵。然而，它不是仅仅使用普通的损失函数，而是与各种正则化函数(L1、L2、L1 + L2)结合起来进行性能比较。在一些经验分析之后，我们使用非常小的β(10–7)系数进行正则化。结果如图4所示。我们可以看到交叉熵结合L1正则化可以更好地帮助避免某些任务的灾难性遗忘。然而，与普通交叉熵相比，L2正则化和混合正则化给出了较弱的性能。表1描述了不同损失函数的ACC和BWT分数(在所有任务的训练之后)。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es kl"><img src="../Images/11836ee91c0cae3e2162c1be2e1e13e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9QQIhVX94tek4jykKkUtmw.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es km"><img src="../Images/fe46bd9a237c17a6cca30e484989cc38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_cx76tvu2qAkz2RfZHLy7Q.png"/></div></div></figure><p id="f6fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。神经网络深度比较</strong></p><p id="b5ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">据观察，更深层次的神经网络更容易发生灾难性遗忘。如图5和表2所示，2层神经网络具有最好的ACC和BWT分数(3次运行的平均值)。其原因在于，改变更深MLP的权重可能会导致最终结果发生重大变化。当从4层MLP切换到2层时，ACC得分从0.62下降到0.54。如图5所示，对于4层网络的许多任务来说，下降幅度更大，但我们观察到2层网络的下降平稳。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kn"><img src="../Images/2c4ad9919ea19385a7e97a0717236856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*ABeG5i6Po5VfM__9Sbt_Tw.png"/></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es ko"><img src="../Images/4063eebd00551792b61b123b5458c018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QpEHpsobro86SA-xTj2pUQ.png"/></div></div></figure><p id="a1bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">4层网络的掉线</strong></p><p id="b202" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每一个具有不同脱落概率的层之后添加一个脱落层。在[1]中，提到添加丢失有助于神经网络减轻灾难性遗忘。然而，据观察，更高的概率并不总是导致更好的性能。事实上，当退出概率为0.1时，性能有所提高，之后，当退出概率更高时，性能开始下降。在图6中，我们可以看到，增加更高的概率意味着大多数任务没有达到最佳精度，这导致ACC分数下降。如图6所示，我们看到ACC和BWT分数在训练后急剧下降，退出概率为0.2。因此，我们坚持0.1的退出概率用于进一步评估。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es kp"><img src="../Images/02c26e61c8b131f9e35b85884a7c77ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFdnOPWILL6yc9CT63zVhw.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kq"><img src="../Images/d1dce3d81d455fb2ba32141d5fec020b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*CcHEMbx5sTYjKQ5a9nojcA.png"/></div></figure><p id="4492" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最佳绩效指标</strong></p><p id="cb73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">具有Adam优化器、L1正则化器、0.1下降率和0.0002学习率的2层深度神经网络被用作获得最佳性能的最佳模型。表4显示了通过使用该网络得到的任务矩阵。每个任务的验证准确性如图7所示。可以看出，在对所有10个任务进行训练后，大多数任务的准确率仍然在60%以上。接下来，我们根据前面提到的结果任务矩阵来评估性能指标。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es kr"><img src="../Images/a54dc1bb5ea01226a558d50f963d34c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N5TR4VRYf2GhZSQr-RFZwA.png"/></div></div></figure><p id="7d85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ACC得分:0.6535 </strong></p><p id="5d8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> BWT得分:-0.27 </strong></p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es ks"><img src="../Images/4da7c23632eb79a7bd6a68c173bddaaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*u_DJMbmQyKYGdlzk4clx9w.png"/></div></figure><p id="4c65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> TBWT分数(使用随机森林):-0.291 </strong></p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kt"><img src="../Images/46df550c09088da3fbba71abc2996e8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*j3zx2Q6zAt0D0LJrC1tPQg.png"/></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es ku"><img src="../Images/21298372d73b90bc8ba79bbebe0f9bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4lx18Z1AyzZiPQAz6WnPFQ.png"/></div></div></figure><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es kv"><img src="../Images/6f1241d9c2eee6bcb157596c71131582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9vyJTEtlrnoqWpybloaNFw.png"/></div></div></figure></div></div>    
</body>
</html>