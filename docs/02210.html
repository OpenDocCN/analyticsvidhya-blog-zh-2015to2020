<html>
<head>
<title>Facial KeyPoint Detection with Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pytorch人脸关键点检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/facial-keypoint-detection-with-pytorch-e9f94ab321a2?source=collection_archive---------3-----------------------#2019-12-06">https://medium.com/analytics-vidhya/facial-keypoint-detection-with-pytorch-e9f94ab321a2?source=collection_archive---------3-----------------------#2019-12-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="2840" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">建立和训练卷积神经网络来检测面部关键点。建在Pytorch上。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/88935677fc54b9b0fdaf0c5f3235f43a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*koroGpniWjcsH_BN.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">这里有一个VGG16的描述。让你的大脑进入状态。</figcaption></figure><p id="c22d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">注意:我已经移动到SubStack。我仍然会偶尔在Medium上发布一些东西，但是现在<a class="ae kj" href="https://antoniolinares.substack.com/people/32102170-antonio-linares" rel="noopener ugc nofollow" target="_blank">你可以在这里找到我的大部分新作品</a>。</p><h2 id="9b6e" class="kk kl hi bd km kn ko kp kq kr ks kt ku jw kv kw kx ka ky kz la ke lb lc ld le bi translated">介绍</h2><p id="ea2f" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">时间在流逝，我仍然对<a class="ae kj" rel="noopener" href="/@evolvia2/my-deep-learning-journey-d63404afc0e0">深度学习</a>充满热情。出于这个原因，我决定深入挖掘<strong class="jp hj">计算机视觉</strong>，以下简称CV的子领域，我认为它将在未来几十年重新定义人类生活。如果机器能看得见，它们将能为我们做很多事情。这里也有一些哲学上的问题，或者说很多，但那是另一篇文章。</p><p id="7a7b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">随着我对医学越来越感兴趣，我决定实现一个<strong class="jp hj">面部关键点检测器，</strong>它运行在一个相当简单的<em class="lk">卷积神经网络上，下文称为</em> CNN。这种软件可以让我们更早地发现疾病。我已经用<strong class="jp hj"> Pytorch </strong>建立了这个。</p><p id="dfe7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在不知所措的感觉出现之前(我经常遇到这种情况)，这里有一个快速的项目概要:</p><ol class=""><li id="976c" class="ll lm hi jp b jq jr jt ju jw ln ka lo ke lp ki lq lr ls lt bi translated">获取数据(带有相应关键点坐标的人脸图片)。</li><li id="d266" class="ll lm hi jp b jq lu jt lv jw lw ka lx ke ly ki lq lr ls lt bi translated">对它进行变换，以帮助网络学习。</li><li id="f389" class="ll lm hi jp b jq lu jt lv jw lw ka lx ke ly ki lq lr ls lt bi translated">定义网络。</li><li id="fa36" class="ll lm hi jp b jq lu jt lv jw lw ka lx ke ly ki lq lr ls lt bi translated">训练网络并测试它。</li><li id="5629" class="ll lm hi jp b jq lu jt lv jw lw ka lx ke ly ki lq lr ls lt bi translated">使用网络进行预测(在这种情况下，返回面部关键点)。</li></ol><h2 id="3cca" class="kk kl hi bd km kn ko kp kq kr ks kt ku jw kv kw kx ka ky kz la ke lb lc ld le bi translated">1 -获取数据</h2><p id="31c1" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">我正在使用<a class="ae kj" href="https://www.cs.tau.ac.il/~wolf/ytfaces/" rel="noopener ugc nofollow" target="_blank">这个</a>数据集。每个数据点都是一张人脸的图像及其对应的68个关键点。每个关键点是一个(x，y)坐标。每个数据点如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lz"><img src="../Images/f8d5df3b99c7469de6b404c570f0bc1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*IGdTc0xK_bG91xtwVYRbGQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">数据点示例</figcaption></figure><p id="9c0c" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果你有一点点深度学习的经验，你将已经知道这是走向哪里。图像(没有关键点)是三维像素阵列。关键点只是136个数字(68个坐标对)。我们基本上要教会网络正确预测136个数字。</p><p id="2ed0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">存储</strong>给定目录中的数据，例如:</p><p id="725b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">/data/training _ frames _ key points . CSV '</p><p id="dc38" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">任何给定的数据点看起来都像这样:</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="e08c" class="kk kl hi mb b fi mf mg l mh mi">Image name:  Luis_Fonsi_21.jpg<br/>Landmarks shape:  (68, 2)<br/>First 4 key pts: [[  45.   98.]<br/> [  47.  106.]<br/> [  49.  110.]<br/> [  53.  119.]]</span></pre><h2 id="3533" class="kk kl hi bd km kn ko kp kq kr ks kt ku jw kv kw kx ka ky kz la ke lb lc ld le bi translated">2—转换数据</h2><p id="668b" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">充分转换数据有助于神经网络学习。对于CV来说，几乎是<em class="lk">永远是</em>左右:</p><ol class=""><li id="94ed" class="ll lm hi jp b jq jr jt ju jw ln ka lo ke lp ki lq lr ls lt bi translated">降低图像的初始维数，将图像从RGB (3 2D阵列)转换为灰度(1 2D阵列)。</li><li id="5eeb" class="ll lm hi jp b jq lu jt lv jw lw ka lx ke ly ki lq lr ls lt bi translated">重新缩放和随机裁剪它。</li><li id="9abb" class="ll lm hi jp b jq lu jt lv jw lw ka lx ke ly ki lq lr ls lt bi translated">归一化像素值。</li><li id="3a81" class="ll lm hi jp b jq lu jt lv jw lw ka lx ke ly ki lq lr ls lt bi translated">将图像转换成一个<strong class="jp hj">张量</strong>数据类型。</li><li id="0686" class="ll lm hi jp b jq lu jt lv jw lw ka lx ke ly ki lq lr ls lt bi translated">将数据集加载到<strong class="jp hj">数据加载器</strong></li></ol><p id="5357" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在这个项目中，我以面向对象的方式处理了这一部分，最初为数据集定义了一个类，它实际上继承了torch.utils.data的<strong class="jp hj">数据集类</strong>。这允许以更加<strong class="jp hj">敏捷</strong>的方式处理数据。</p><p id="cdc6" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下面，您可以看到FacialKeypointsDataset类定义的代码。最值得注意的是，<strong class="jp hj"> __getitem__ </strong>方法只允许通过索引<em class="lk">访问类实例来检索任何给定的数据点。这在以后会有意义。</em></p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="30ba" class="kk kl hi mb b fi mf mg l mh mi">from torch.utils.data import Dataset, DataLoader</span><span id="0824" class="kk kl hi mb b fi mj mg l mh mi">class FacialKeypointsDataset(Dataset):<br/>    """Face Landmarks dataset."""</span><span id="5228" class="kk kl hi mb b fi mj mg l mh mi">def __init__(self, csv_file, root_dir, transform=None):<br/>        """<br/>        Args:<br/>            csv_file (string): Path to the csv file with annotations.<br/>            root_dir (string): Directory with all the images.<br/>            transform (callable, optional): Optional transform to be applied<br/>                on a sample.<br/>        """<br/>        self.key_pts_frame = pd.read_csv(csv_file)<br/>        self.root_dir = root_dir<br/>        self.transform = transform</span><span id="2ab0" class="kk kl hi mb b fi mj mg l mh mi">def __len__(self):<br/>        return len(self.key_pts_frame)</span><span id="51d2" class="kk kl hi mb b fi mj mg l mh mi">def __getitem__(self, idx):<br/>        image_name = os.path.join(self.root_dir,<br/>                                self.key_pts_frame.iloc[idx, 0])<br/>        <br/>        image = mpimg.imread(image_name)<br/>        <br/>        # if image has an alpha color channel, get rid of it<br/>        if(image.shape[2] == 4):<br/>            image = image[:,:,0:3]<br/>        <br/>        key_pts = self.key_pts_frame.iloc[idx, 1:].as_matrix()<br/>        key_pts = key_pts.astype('float').reshape(-1, 2)<br/>        sample = {'image': image, 'keypoints': key_pts}</span><span id="22f7" class="kk kl hi mb b fi mj mg l mh mi">if self.transform:<br/>            sample = self.transform(sample)</span><span id="a18b" class="kk kl hi mb b fi mj mg l mh mi">return sample</span></pre><p id="b389" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我以类似的方式定义了转换，定义了Normalize、Rescale、RandomCrop和ToTensor类。由于这种格式的缩进看起来很奇怪，我建议将代码复制到一个真正的编辑器中，以便更好地检查。</p><p id="9bd0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这是Normalize类。它所做的只是将图像转换为灰度，并归一化0到1范围内的像素值，包括0和1:</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="6d90" class="kk kl hi mb b fi mf mg l mh mi">import torch<br/>from torchvision import transforms, utils<br/># tranforms</span><span id="17e4" class="kk kl hi mb b fi mj mg l mh mi">class Normalize(object):<br/>    """Convert a color image to grayscale and normalize the color range to [0,1]."""</span><span id="5189" class="kk kl hi mb b fi mj mg l mh mi">def __call__(self, sample):<br/>        image, key_pts = sample['image'], sample['keypoints']<br/>        <br/>        image_copy = np.copy(image)<br/>        key_pts_copy = np.copy(key_pts)</span><span id="23f7" class="kk kl hi mb b fi mj mg l mh mi"># convert image to grayscale<br/>        image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)<br/>        <br/>        # scale color range from [0, 255] to [0, 1]<br/>        image_copy=  image_copy/255.0<br/>        <br/>        # scale keypoints to be centered around 0 with a range of [-1, 1]<br/>        # mean = 100, sqrt = 50, so, pts should be (pts - 100)/50<br/>        key_pts_copy = (key_pts_copy - 100)/50.0</span><span id="c50e" class="kk kl hi mb b fi mj mg l mh mi">return {'image': image_copy, 'keypoints': key_pts_copy}</span></pre><p id="6701" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这是Rescale类。它所做的只是调整图像的大小:</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="b67b" class="kk kl hi mb b fi mf mg l mh mi">class Rescale(object):<br/>    """Rescale the image in a sample to a given size.</span><span id="7e2e" class="kk kl hi mb b fi mj mg l mh mi">Args:<br/>        output_size (tuple or int): Desired output size. If tuple, output is<br/>            matched to output_size. If int, smaller of image edges is matched<br/>            to output_size keeping aspect ratio the same.<br/>    """</span><span id="366c" class="kk kl hi mb b fi mj mg l mh mi">def __init__(self, output_size):<br/>        assert isinstance(output_size, (int, tuple))<br/>        self.output_size = output_size</span><span id="c860" class="kk kl hi mb b fi mj mg l mh mi">def __call__(self, sample):<br/>        image, key_pts = sample['image'], sample['keypoints']</span><span id="4430" class="kk kl hi mb b fi mj mg l mh mi">h, w = image.shape[:2]<br/>        if isinstance(self.output_size, int):<br/>            if h &gt; w:<br/>                new_h, new_w = self.output_size * h / w, self.output_size<br/>            else:<br/>                new_h, new_w = self.output_size, self.output_size * w / h<br/>        else:<br/>            new_h, new_w = self.output_size</span><span id="cb95" class="kk kl hi mb b fi mj mg l mh mi">new_h, new_w = int(new_h), int(new_w)</span><span id="c02d" class="kk kl hi mb b fi mj mg l mh mi">img = cv2.resize(image, (new_w, new_h))<br/>        <br/>        # scale the pts, too<br/>        key_pts = key_pts * [new_w / w, new_h / h]</span><span id="980b" class="kk kl hi mb b fi mj mg l mh mi">return {'image': img, 'keypoints': key_pts}</span></pre><p id="a4ea" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这是RandomCrop类。几乎不言自明:</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="3aeb" class="kk kl hi mb b fi mf mg l mh mi">class RandomCrop(object):<br/>    """Crop randomly the image in a sample.</span><span id="2b74" class="kk kl hi mb b fi mj mg l mh mi">Args:<br/>        output_size (tuple or int): Desired output size. If int, square crop<br/>            is made.<br/>    """</span><span id="65ab" class="kk kl hi mb b fi mj mg l mh mi">def __init__(self, output_size):<br/>        assert isinstance(output_size, (int, tuple))<br/>        if isinstance(output_size, int):<br/>            self.output_size = (output_size, output_size)<br/>        else:<br/>            assert len(output_size) == 2<br/>            self.output_size = output_size</span><span id="85ec" class="kk kl hi mb b fi mj mg l mh mi">def __call__(self, sample):<br/>        image, key_pts = sample['image'], sample['keypoints']</span><span id="58a1" class="kk kl hi mb b fi mj mg l mh mi">h, w = image.shape[:2]<br/>        new_h, new_w = self.output_size</span><span id="c4c7" class="kk kl hi mb b fi mj mg l mh mi">top = np.random.randint(0, h - new_h)<br/>        left = np.random.randint(0, w - new_w)</span><span id="6713" class="kk kl hi mb b fi mj mg l mh mi">image = image[top: top + new_h,<br/>                      left: left + new_w]</span><span id="2025" class="kk kl hi mb b fi mj mg l mh mi">key_pts = key_pts - [left, top]</span><span id="cd35" class="kk kl hi mb b fi mj mg l mh mi">return {'image': image, 'keypoints': key_pts}</span></pre><p id="1a3f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">最后，这里是ToTensor类，它简单地将图像从数组转换成神经网络最喜欢的张量类型。</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="f77d" class="kk kl hi mb b fi mf mg l mh mi">class ToTensor(object):<br/>    """Convert ndarrays in sample to Tensors."""</span><span id="f15d" class="kk kl hi mb b fi mj mg l mh mi">def __call__(self, sample):<br/>        image, key_pts = sample['image'], sample['keypoints']<br/>         <br/>        # if image has no grayscale color channel, add one<br/>        if(len(image.shape) == 2):<br/>            # add that third color dim<br/>            image = image.reshape(image.shape[0], image.shape[1], 1)<br/>            <br/>        # swap color axis because<br/>        # numpy image: H x W x C<br/>        # torch image: C X H X W<br/>        image = image.transpose((2, 0, 1))<br/>        <br/>        return {'image': torch.from_numpy(image),<br/>                'keypoints': torch.from_numpy(key_pts)}</span></pre><p id="9f62" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">上面的代码看起来很长，很神秘，但事实并非如此。它所能做的就是让我们以一种非常简单的方式创建数据加载器，这些数据加载器可以输入到网络中进行训练，就像这样:</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="d4e9" class="kk kl hi mb b fi mf mg l mh mi">from torchvision import transforms, utils</span><span id="31c3" class="kk kl hi mb b fi mj mg l mh mi">#defining transformations using transforms.Compose([])</span><span id="b9fd" class="kk kl hi mb b fi mj mg l mh mi">data_transform =transforms.Compose(<br/>[<br/>    Rescale(250), RandomCrop(224), Normalize(), ToTensor()<br/>])</span><span id="4ef5" class="kk kl hi mb b fi mj mg l mh mi">#creating dataset, by instantiating FacialKeyPointsDataset<strong class="mb hj"> </strong>class</span><span id="3cc8" class="kk kl hi mb b fi mj mg l mh mi">transformed_dataset = FacialKeypointsDataset(csv_file='/data/training_frames_keypoints.csv',<br/>                                             root_dir='/data/training/',<br/>                                             transform=data_transform)</span></pre><p id="a2ae" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">data_transform包含要对数据进行的转换。通过将其作为FacialKeypointsDataset实例化的参数进行传递，可以生成一个具有适当转换的数据集。然后，为了能够将其输入网络，我们运行以下代码:</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="fe58" class="kk kl hi mb b fi mf mg l mh mi">batch_size = 10</span><span id="0e94" class="kk kl hi mb b fi mj mg l mh mi">train_loader = DataLoader(transformed_dataset, <br/>                          batch_size=batch_size,<br/>                          shuffle=True, <br/>                          num_workers=4)</span></pre><p id="f882" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">train_loader现在包含10批转换后的图像，以及相应的关键点。不管有没有面向对象的方法，完成这项工作的高级模式都是一样的:</p><ol class=""><li id="c8c8" class="ll lm hi jp b jq jr jt ju jw ln ka lo ke lp ki lq lr ls lt bi translated">创建数据集(通过适当的转换)</li><li id="2092" class="ll lm hi jp b jq lu jt lv jw lw ka lx ke ly ki lq lr ls lt bi translated">创建数据加载器(将数据集作为参数传入)</li><li id="7727" class="ll lm hi jp b jq lu jt lv jw lw ka lx ke ly ki lq lr ls lt bi translated">就这样</li></ol><h2 id="8e1f" class="kk kl hi bd km kn ko kp kq kr ks kt ku jw kv kw kx ka ky kz la ke lb lc ld le bi translated">3 —定义网络</h2><p id="3a8a" class="pw-post-body-paragraph jn jo hi jp b jq lf ij js jt lg im jv jw lh jy jz ka li kc kd ke lj kg kh ki hb bi translated">卷积神经网络已经被证明在图像处理方面非常有效。老话“一个图像胜过1000个单词”，从计算的角度来说，是非常正确的。图像中信息密集。</p><p id="16b5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">本质上，CNN<strong class="jp hj">以某种方式压缩信息</strong>，然后可以将其输入分类器(基本上是一个前馈神经网络)。通过告诉分类器应该输出什么(损失函数)，我们可以通过反向传播和梯度下降对卷积层中的权重进行调整。在某个阶段，我会发布一个关于这是如何工作的深入解释，但不是今天。</p><p id="fd86" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下面是模型当前版本的代码。我将在更新实际模型时更新代码。</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="df70" class="kk kl hi mb b fi mf mg l mh mi">#defining the cnn architecture</span><span id="d54a" class="kk kl hi mb b fi mj mg l mh mi">import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.nn.init as I</span><span id="5f89" class="kk kl hi mb b fi mj mg l mh mi">class Net(nn.Module):</span><span id="ee62" class="kk kl hi mb b fi mj mg l mh mi">def __init__(self):<br/>        super(Net, self).__init__()<br/>       <br/>        #defining maxpool block<br/>        self.maxpool = nn.MaxPool2d(2, 2)<br/>               <br/>        #defining dropout block<br/>        self.dropout = nn.Dropout(p=0.2)<br/>        <br/>        self.conv1 = nn.Conv2d(1, 32, 5)<br/>        <br/>        #defining second convolutional layer<br/>        self.conv2 = nn.Conv2d(32, 64, 3)<br/>        <br/>        #defining third convolutional layer<br/>        self.conv3 = nn.Conv2d(64, 128, 3)<br/>        <br/>        #defining linear output layer<br/>        self.fc1 = nn.Linear(128*26*26, 136)<br/>        </span><span id="9a6a" class="kk kl hi mb b fi mj mg l mh mi">def forward(self, x):<br/>        <br/>        #passing tensor x through first conv layer<br/>        x = self.maxpool(F.relu(self.conv1(x)))<br/>     <br/>        #passing tensor x through second conv layer<br/>        x = self.maxpool(F.relu(self.conv2(x)))<br/>        <br/>        #passing tensor x through third conv layer<br/>        x = self.maxpool(F.relu(self.conv3(x)))<br/>        <br/>        print(x.size())<br/>        #flattening x tensor<br/>        x = x.view(x.size(0), -1)<br/>        <br/>        #applying dropout<br/>        x = self.dropout(x)<br/>     <br/>        #passing x through linear layer<br/>        x = self.fc1(x)<br/>        <br/>        #returning x<br/>        return x</span></pre><p id="7c2b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">就训练它而言，代码相当简单。它包括通过网络传递张量，计算输出误差，然后通过反向传播和梯度下降调整权重。代码迭代地做这个<em class="lk">，</em>让模型更接近足够高的性能水平。同样，由于缩进在这种格式下会变得很奇怪，所以将它复制粘贴到代码编辑器中，以真正了解发生了什么。</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="cc2f" class="kk kl hi mb b fi mf mg l mh mi">def train_net(n_epochs):<br/># prepare the net for training<br/>    net.train()<br/>    for epoch in range(n_epochs):  # loop over the dataset multiple times</span><span id="9d26" class="kk kl hi mb b fi mj mg l mh mi">running_loss = 0.0<br/># train on batches of data, assumes you already have train_loader<br/>        for batch_i, data in enumerate(train_loader):<br/>            # get the input images and their corresponding labels<br/>            images = data['image']<br/>            key_pts = data['keypoints']<br/># flatten pts<br/>            key_pts = key_pts.view(key_pts.size(0), -1)<br/># convert variables to floats for regression loss<br/>            key_pts = key_pts.type(torch.FloatTensor)<br/>            images = images.type(torch.FloatTensor)<br/># forward pass to get outputs<br/>            output_pts = net(images)<br/># calculate the loss between predicted and target keypoints<br/>            loss = criterion(output_pts, key_pts)<br/># zero the parameter (weight) gradients<br/>            optimizer.zero_grad()</span><span id="76b9" class="kk kl hi mb b fi mj mg l mh mi"># backward pass to calculate the weight gradients<br/>            loss.backward()<br/># update the weights<br/>            optimizer.step()<br/># print loss statistics<br/>            running_loss += loss.item()<br/>            if batch_i % 10 == 9:    # print every 10 batches<br/>                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/10))<br/>                running_loss = 0.0<br/>    print('Finished Training')</span></pre><p id="af6d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">训练网络现在是定义损失标准和优化器函数、实例化网络(来自上面定义的<strong class="jp hj">网络类</strong>)、定义训练过程的历元数以及运行<strong class="jp hj"> train_net方法</strong>的问题。</p><p id="81e1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">你会注意到我使用了<a class="ae kj" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> MSE </a>作为损失函数。这是因为网络将输出值，而我们希望这些值尽可能接近实际的关键点(68个坐标对)。因此，损失函数必须具有类似“T11”的“T10”回归性质。</p><pre class="iy iz ja jb fd ma mb mc md aw me bi"><span id="6195" class="kk kl hi mb b fi mf mg l mh mi">#defining the loss criterion<br/>import torch.optim as optim<br/>criterion = nn.MSELoss()</span><span id="b9cd" class="kk kl hi mb b fi mj mg l mh mi">#defining the optimizer method, for gradient descent<br/>optimizer = optim.Adam(net.parameters(), lr = 0.0005)</span><span id="d48f" class="kk kl hi mb b fi mj mg l mh mi">#instantiating a network<br/>net = Net()</span><span id="102d" class="kk kl hi mb b fi mj mg l mh mi">#defining the number of epochs<br/>n_epochs = 10</span><span id="d2a0" class="kk kl hi mb b fi mj mg l mh mi">#training the network<br/>train_net(n_epochs)</span></pre><p id="2d13" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">多次反复试验让我意识到，收敛开始于大约0.005 的<strong class="jp hj">学习速率。太远低于或高于该速率，网络似乎学习得不好。也许在不久的将来，一篇深入学习率概念的文章会很有意思。</strong></p><p id="75d8" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">训练网络共<strong class="jp hj"> 10个历元</strong>，训练损耗收敛到0.05左右。为了减少层之间的协变量转移，我现在继续添加批量标准化，看看它是如何工作的。</p><p id="42b3" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">此帖子正在进行中。</strong></p></div></div>    
</body>
</html>