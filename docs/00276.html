<html>
<head>
<title>Importing Google BigQuery tables to AWS Athena</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将Google BigQuery表导入AWS Athena</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/importing-google-bigquery-tables-to-aws-athena-4901f6488ea8?source=collection_archive---------0-----------------------#2019-02-22">https://medium.com/analytics-vidhya/importing-google-bigquery-tables-to-aws-athena-4901f6488ea8?source=collection_archive---------0-----------------------#2019-02-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d7cff7d05a7e53fa53ed849f5b64e910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*518Z4MAe36ZqeLX2LTzeDA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/photos/WxRnkVoVl0o?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Josè Maria Sava </a>通过<a class="ae iu" href="https://unsplash.com/search/photos/bridge-sea?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="91a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作为一名数据工程师，你很可能正在使用领先的大数据云平台之一，如AWS、Microsoft Azure、Google Cloud来处理数据。此外，将数据从一个平台迁移到另一个平台可能是您已经面临或将在某个时候面临的事情。在本文中，我将展示如何将Google BigQuery表导入AWS Athena。如果您只需要一个工具列表来配合一些非常高级的指导，您可以快速查看一篇展示如何将单个BigQuery表导入Hive metastore 的<a class="ae iu" href="https://amazon-aws-big-data-demystified.ninja/2018/05/27/how-to-export-data-from-google-big-query-into-aws-s3-emr-hive/" rel="noopener ugc nofollow" target="_blank">帖子。在本文中，我将展示一种将完整的BigQuery项目(多个表)导入Hive和Athena metastore的方法。导入限制很少，例如，从已分区表导入数据时，不能导入单个分区。开始该过程前，请检查</a><a class="ae iu" href="https://cloud.google.com/bigquery/docs/exporting-data" rel="noopener ugc nofollow" target="_blank">限制</a>。</p><p id="1022" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了成功地将Google BigQuery表导入Athena，我执行了如下所示的步骤。当从Google BigQuery转储数据和模式并将它们加载到AWS Athena时，我使用了AVRO格式。</p><blockquote class="jt ju jv"><p id="3ad3" class="iv iw jw ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><a class="ae iu" href="#264f" rel="noopener ugc nofollow">第一步。将BigQuery数据转储到Google云存储中</a></p><p id="5452" class="iv iw jw ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><a class="ae iu" href="#3af9" rel="noopener ugc nofollow">第二步。将数据从谷歌云存储传输到S3自动气象站</a></p><p id="85c0" class="iv iw jw ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><a class="ae iu" href="#c089" rel="noopener ugc nofollow">第三步。从存储在S3的AVRO文件中提取AVRO模式</a></p><p id="5461" class="iv iw jw ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><a class="ae iu" href="#cc2d" rel="noopener ugc nofollow">第四步。使用步骤3 </a>中的模式，在AVRO数据的基础上创建配置单元表</p><p id="1a89" class="iv iw jw ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><a class="ae iu" href="#fbf3" rel="noopener ugc nofollow">第五步。从配置单元表中提取配置单元表定义</a></p><p id="521d" class="iv iw jw ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated"><a class="ae iu" href="#c6f6" rel="noopener ugc nofollow">第六步。使用步骤3和5的输出创建Athena表</a></p></blockquote><p id="783a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那么，尽管最终目标是将数据保存在Athena中，为什么我必须首先创建Hive表呢？这是因为:</p><ul class=""><li id="70d4" class="ka kb hi ix b iy iz jc jd jg kc jk kd jo ke js kf kg kh ki bi translated">Athena不支持使用<code class="du kj kk kl km b">avro.schema.url</code> <strong class="ix hj"> </strong>来指定表模式。</li><li id="bcdd" class="ka kb hi ix b iy kn jc ko jg kp jk kq jo kr js kf kg kh ki bi translated">Athena要求您在CREATE语句中显式指定字段名及其数据类型。</li><li id="51ce" class="ka kb hi ix b iy kn jc ko jg kp jk kq jo kr js kf kg kh ki bi translated">在<code class="du kj kk kl km b">avro.schema.literal</code>下，Athena还需要JSON格式的AVRO模式。</li><li id="145c" class="ka kb hi ix b iy kn jc ko jg kp jk kq jo kr js kf kg kh ki bi translated">您可以查看此AWS <a class="ae iu" href="https://docs.aws.amazon.com/athena/latest/ug/avro.html" rel="noopener ugc nofollow" target="_blank">文档</a>了解更多详情。</li></ul><p id="87c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，可以通过指向存储在S3上的AVRO模式文件来直接创建配置单元表，但是要在Athena中拥有相同的配置单元表，在CREATE TABLE语句中需要列和模式。克服这个问题的一个方法是首先从AVRO数据中提取模式，作为<code class="du kj kk kl km b">avro.schema.literal</code>提供。其次，对于CREATE语句所需的字段名和数据类型，基于存储在S3的AVRO模式创建配置单元表，并使用<code class="du kj kk kl km b">SHOW CREATE TABLE</code>来转储/导出包含字段名和数据类型的配置单元表定义。最后，通过组合提取的AVRO模式和配置单元表定义来创建Athena表。我将在随后的章节中详细讨论。</p><p id="bca0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了进行演示，我准备将下面的BigQuery表导入Athena。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/ba8d604b3fbb76a55e472b13a535a30b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UZY8kqDB55M-kaGjQdW8fA.png"/></div></div></figure><p id="264f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以，让我们开始吧！</p><h2 id="65cf" class="kx ky hi bd kz la lb lc ld le lf lg lh jg li lj lk jk ll lm ln jo lo lp lq lr bi translated">第一步。将BigQuery数据转储到Google云存储中</h2><p id="8f95" class="pw-post-body-paragraph iv iw hi ix b iy ls ja jb jc lt je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">借助Google cloud UI，可以将BigQuery数据转储到Google storage中。但是，如果您必须手动转储几个表，这可能会成为一项单调乏味的任务。为了解决这个问题，我使用了Google Cloud Shell。在Cloud Shell中，您可以将常规Shell脚本与BigQuery命令相结合，并相对快速地转储多个表。可以激活云壳，如下图所示。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/7f95e8e97f2d96478ee7263bfe9c996b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EJDzjJC1bhj5ZDLYRs-eDw.png"/></div></div></figure><p id="7c39" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在Cloud Shell中，下面的操作提供了BigQuery <code class="du kj kk kl km b">extract</code>命令，将“后端”数据集的每个表转储到Google云存储中。</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="e239" class="kx ky hi km b fi mc md l me mf">bq ls backend | cut -d ' ' -f3 | tail -n+3 | xargs -I@ echo bq --location=US extract --destination_format AVRO --compression SNAPPY &lt;dataset&gt;.@ gs://&lt;bucket&gt;@</span></pre><p id="0b1a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我的例子中，它打印:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="43ea" class="kx ky hi km b fi mc md l me mf">aftab_ansari@cloudshell:~ (project-ark-archive)$ bq ls backend | cut -d ' ' -f3 | tail -n+3 | xargs -I@ echo bq --location=US extract --destination_format AVRO --compression SNAPPY backend.@ gs://plr_data_transfer_temp/bigquery_data/backend/@/@-*.avro</span><span id="593f" class="kx ky hi km b fi mg md l me mf">bq --location=US extract --destination_format AVRO --compression SNAPPY backend.sessions_daily_phase2 gs://plr_data_transfer_temp/bigquery_data/backend/sessions_daily_phase2/sessions_daily_phase2-*.avro</span><span id="5de1" class="kx ky hi km b fi mg md l me mf">bq --location=US extract --destination_format AVRO --compression SNAPPY backend.sessions_detailed_phase2 gs://plr_data_transfer_temp/bigquery_data/backend/sessions_detailed_phase2/sessions_detailed_phase2-*.avro</span><span id="9645" class="kx ky hi km b fi mg md l me mf">bq --location=US extract --destination_format AVRO --compression SNAPPY backend.sessions_phase2 gs://plr_data_transfer_temp/bigquery_data/backend/sessions_phase2/sessions_phase2-*.avro</span></pre><p id="aed1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意:<code class="du kj kk kl km b">--compression SNAPPY</code>，这很重要，因为未压缩的大文件会导致<code class="du kj kk kl km b">gsutil</code>命令(用于向AWS S3传输数据)卡住。通配符(<strong class="ix hj"> * </strong>)使<code class="du kj kk kl km b">bq extract</code>将较大的表(&gt; 1GB)分割成多个输出文件。在Cloud Shell上运行这些命令将数据复制到下面的Google存储目录中。</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="b40b" class="kx ky hi km b fi mc md l me mf">gs://plr_data_transfer_temp/bigquery_data/backend/table_name/table_name-*.avro</span></pre><p id="055b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们做<code class="du kj kk kl km b">ls</code>来看看被抛弃的AVRO文件。</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="cd99" class="kx ky hi km b fi mc md l me mf">aftab_ansari@cloudshell:~ (project-ark-archive)$ gsutil ls gs://plr_data_transfer_temp/bigquery_data/backend/sessions_daily_phase2</span><span id="5b8c" class="kx ky hi km b fi mg md l me mf">gs://plr_data_transfer_temp/bigquery_data/backend/sessions_daily_phase2/sessions_daily_phase2-000000000000.avro</span></pre><p id="3af9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我也可以从UI中浏览并找到如下所示的数据。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/ceb4c75af2d6d3f724afdbfc7a194a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XTGvjRuaHOOTpI3HdYcD3A.png"/></div></div></figure><h2 id="375b" class="kx ky hi bd kz la lb lc ld le lf lg lh jg li lj lk jk ll lm ln jo lo lp lq lr bi translated">第二步。将数据从谷歌云存储转移到S3自动气象站</h2><p id="2cbb" class="pw-post-body-paragraph iv iw hi ix b iy ls ja jb jc lt je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">将数据从谷歌存储转移到S3自动气象站非常简单。首先，设置您的S3凭据。在云壳上，创建或编辑<code class="du kj kk kl km b">.boto</code>文件(<code class="du kj kk kl km b">vi ~/.boto</code>)并添加这些:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="98bf" class="kx ky hi km b fi mc md l me mf">[Credentials]<br/>aws_access_key_id = &lt;your aws access key ID&gt;<br/>aws_secret_access_key = &lt;your aws secret access key&gt;<br/>[s3]<br/>host = s3.us-east-1.amazonaws.com<br/>use-sigv4 = True</span></pre><p id="78bf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意:<strong class="ix hj">s3.us-east-1.amazonaws.com</strong>—必须与铲斗所在的地区相对应。</p><p id="80db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">设置凭证后，执行<code class="du kj kk kl km b">gsutil</code>将数据从Google Storage传输到AWS S3。例如:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="ed82" class="kx ky hi km b fi mc md l me mf">gsutil rsync -r gs://your-gs-bucket/your-extract-path/your-schema s3://your-aws-bucket/your-target-path/your-schema</span></pre><p id="a385" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将<strong class="ix hj"> <em class="jw"> -n </em> </strong>标志添加到上面的命令中，以显示将使用指定命令执行的操作，而无需实际运行这些操作。</p><p id="94e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本例中，为了将数据传输到S3，我使用了以下代码:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="84ba" class="kx ky hi km b fi mc md l me mf">aftab_ansari@cloudshell:~ (project-ark-archive)$ gsutil rsync -r gs://plr_data_transfer_temp/bigquery_data/backend s3://my-bucket/bq_data/backend</span><span id="db63" class="kx ky hi km b fi mg md l me mf">Building synchronization state…<br/>Starting synchronization…<br/>Copying gs://plr_data_transfer_temp/bigquery_data/backend/sessions_daily_phase2/sessions_daily_phase2-000000000000.avro [Content-Type=application/octet-stream]...<br/>Copying gs://plr_data_transfer_temp/bigquery_data/backend/sessions_detailed_phase2/sessions_detailed_phase2-000000000000.avro [Content-Type=application/octet-stream]...<br/>Copying gs://plr_data_transfer_temp/bigquery_data/backend/sessions_phase2/sessions_phase2-000000000000.avro [Content-Type=application/octet-stream]...<br/>| [3 files][987.8 KiB/987.8 KiB]<br/>Operation completed over 3 objects/987.8 KiB.</span></pre><p id="7495" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看数据是否被传输到S3。我从我的本地机器上验证了这一点:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="ce70" class="kx ky hi km b fi mc md l me mf">aws s3 ls --recursive  s3://my-bucket/bq_data/backend --profile smoke | awk '{print $4}'</span><span id="c089" class="kx ky hi km b fi mg md l me mf">bq_data/backend/sessions_daily_phase2/sessions_daily_phase2-000000000000.avro<br/>bq_data/backend/sessions_detailed_phase2/sessions_detailed_phase2-000000000000.avro<br/>bq_data/backend/sessions_phase2/sessions_phase2-000000000000.avro</span></pre><h2 id="fe31" class="kx ky hi bd kz la lb lc ld le lf lg lh jg li lj lk jk ll lm ln jo lo lp lq lr bi translated">第三步。从存储在S3的AVRO文件中提取AVRO模式</h2><p id="e3bd" class="pw-post-body-paragraph iv iw hi ix b iy ls ja jb jc lt je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">要从AVRO数据中提取模式，可以使用带有<code class="du kj kk kl km b">getschema</code>参数的Apache <code class="du kj kk kl km b">avro-tools-&lt;version&gt;.jar</code>。使用该工具的好处是，它返回的模式格式可以在创建Athena表时直接在<code class="du kj kk kl km b">WITH SERDEPROPERTIES</code>语句中使用。您会注意到，在转储BigQuery表时，每个表只有一个<code class="du kj kk kl km b">.avro</code>文件。这是因为数据量很小——否则，我会在每个表中得到几个文件。不管每个表是单个还是多个文件，对每个表的任何单个文件运行avro-tools来提取该表的模式就足够了。我下载了avro-tools的最新版本<code class="du kj kk kl km b">avro-tools-1.8.2.jar</code>。我首先将所有的<code class="du kj kk kl km b">.avro</code>文件从s3复制到本地磁盘:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="0c4a" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 tmpAftab]$ aws s3 cp s3://my-bucket/bq_data/backend/ bq_data/backend/ --recursive</span><span id="43d7" class="kx ky hi km b fi mg md l me mf">download: s3://my-bucket/bq_data/backend/sessions_detailed_phase2/sessions_detailed_phase2-000000000000.avro to bq_data/backend/sessions_detailed_phase2/sessions_detailed_phase2-000000000000.avro</span><span id="f7ce" class="kx ky hi km b fi mg md l me mf">download: s3://my-bucket/bq_data/backend/sessions_phase2/sessions_phase2-000000000000.avro to bq_data/backend/sessions_phase2/sessions_phase2-000000000000.avro</span><span id="e6cc" class="kx ky hi km b fi mg md l me mf">download: s3://my-bucket/bq_data/backend/sessions_daily_phase2/sessions_daily_phase2-000000000000.avro to bq_data/backend/sessions_daily_phase2/sessions_daily_phase2-000000000000.avro</span></pre><p id="3218" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Avro-tools命令应该是这样的，<code class="du kj kk kl km b">java -jar avro-tools-1.8.2.jar getschema your_data.avro &gt; schema_file.avsc</code>。如果您有几个AVRO文件，这可能会变得很乏味(实际上，我已经为一个有更多表格的项目这样做了)。我再次使用shell脚本来生成命令。我用以下内容创建了<code class="du kj kk kl km b">extract_schema_avro.sh</code>:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="86f5" class="kx ky hi km b fi mc md l me mf">schema_avro=(bq_data/backend/*)<br/>for i in ${!schema_avro[*]}; do<br/>  input_file=$(find ${schema_avro[$i]} -type f)<br/>  output_file=$(ls -l ${schema_avro[$i]} | tail -n+2 \<br/>    | awk -v srch="avro" -v repl="avsc" '{ sub(srch,repl,$9);<br/>    print $9 }')<br/>  commands=$(<br/>    echo "java -jar avro-tools-1.8.2.jar getschema " \<br/>      $input_file" &gt; bq_data/schemas/backend/avro/"$output_file<br/>  )<br/>  echo $commands<br/>done</span></pre><p id="1437" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">运行<code class="du kj kk kl km b">extract_schema_avro.sh</code>提供以下内容:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="fa6b" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 tmpAftab]$ sh extract_schema_avro.sh</span><span id="cab3" class="kx ky hi km b fi mg md l me mf">java -jar avro-tools-1.8.2.jar getschema bq_data/backend/sessions_daily_phase2/sessions_daily_phase2-000000000000.avro &gt; bq_data/schemas/backend/avro/sessions_daily_phase2-000000000000.avsc</span><span id="8e0d" class="kx ky hi km b fi mg md l me mf">java -jar avro-tools-1.8.2.jar getschema bq_data/backend/sessions_detailed_phase2/sessions_detailed_phase2-000000000000.avro &gt; bq_data/schemas/backend/avro/sessions_detailed_phase2-000000000000.avsc</span><span id="6a43" class="kx ky hi km b fi mg md l me mf">java -jar avro-tools-1.8.2.jar getschema bq_data/backend/sessions_phase2/sessions_phase2-000000000000.avro &gt; bq_data/schemas/backend/avro/sessions_phase2-000000000000.avsc</span></pre><p id="07d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">执行上述命令复制<code class="du kj kk kl km b">bq_data/schemas/backend/avro/</code>下提取的模式:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="5ae2" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 tmpAftab]$ ls -l bq_data/schemas/backend/avro/* | awk '{print $9}'</span><span id="14d0" class="kx ky hi km b fi mg md l me mf">bq_data/schemas/backend/avro/sessions_daily_phase2-000000000000.avsc<br/>bq_data/schemas/backend/avro/sessions_detailed_phase2-000000000000.avsc<br/>bq_data/schemas/backend/avro/sessions_phase2-000000000000.avsc</span></pre><p id="d622" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们也检查一下<code class="du kj kk kl km b">.avsc</code>文件中的内容。</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="04cf" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 tmpAftab]$ cat bq_data/schemas/backend/avro/sessions_detailed_phase2-000000000000.avsc</span><span id="be23" class="kx ky hi km b fi mg md l me mf">{<br/>"type" : "record",<br/>"name" : "Root",<br/>"fields" : [ {<br/>"name" : "uid",<br/>"type" : [ "null", "string" ]<br/>}, {<br/>"name" : "platform",<br/>"type" : [ "null", "string" ]<br/>}, {<br/>"name" : "version",<br/>"type" : [ "null", "string" ]<br/>}, {<br/>"name" : "country",<br/>"type" : [ "null", "string" ]<br/>}, {<br/>"name" : "sessions",<br/>"type" : [ "null", "long" ]<br/>}, {<br/>"name" : "active_days",<br/>"type" : [ "null", "long" ]<br/>}, {<br/>"name" : "session_time_minutes",<br/>"type" : [ "null", "double" ]<br/>} ]<br/>}</span></pre><p id="5085" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如您所见，该模式的形式可以直接在Athena <code class="du kj kk kl km b">WITH SERDEPROPERTIES</code>中使用。但是在Athena之前，我使用AVRO模式来创建Hive表。如果您想避免创建Hive表，您可以读取<code class="du kj kk kl km b">.avsc</code>文件来提取字段名和数据类型，但是您必须自己将数据类型从AVRO格式映射到Athena表创建DDL。映射任务的复杂性取决于表中数据类型的复杂程度。为了简单起见(也为了涵盖大多数简单到复杂的数据类型)，我让Hive为我做映射。所以我首先在Hive metastore中创建了这些表。然后我使用<code class="du kj kk kl km b">SHOW CREATE TABLE</code>来获取DDL的字段名和数据类型部分。</p><h2 id="cc2d" class="kx ky hi bd kz la lb lc ld le lf lg lh jg li lj lk jk ll lm ln jo lo lp lq lr bi translated">第四步。使用步骤3中的模式，在AVRO数据的基础上创建配置单元表</h2><p id="7c78" class="pw-post-body-paragraph iv iw hi ix b iy ls ja jb jc lt je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">如前所述，Hive允许使用<code class="du kj kk kl km b">avro.schema.url</code>创建表。因此，一旦从AVRO数据中提取了模式(<code class="du kj kk kl km b">.avsc</code>文件)，就可以创建如下的表:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="b924" class="kx ky hi km b fi mc md l me mf">CREATE EXTERNAL TABLE table_name<br/>STORED AS AVRO<br/>LOCATION 's3://your-aws-bucket/your-target-path/avro_data'<br/>TBLPROPERTIES ('avro.schema.url'='s3://your-aws-bucket/your-target-path/your-avro-schema');</span></pre><p id="4462" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，将提取的模式上传到S3，以便<code class="du kj kk kl km b">avro.schema.url</code> <strong class="ix hj"> </strong>可以引用它们在S3的位置:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="3627" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 tmpAftab]$ aws s3 cp bq_data/schemas s3://my-bucket/bq_data/schemas --recursive</span><span id="4763" class="kx ky hi km b fi mg md l me mf">upload: bq_data/schemas/backend/avro/sessions_daily_phase2-000000000000.avsc to s3://my-bucket/bq_data/schemas/backend/avro/sessions_daily_phase2-000000000000.avsc</span><span id="0a50" class="kx ky hi km b fi mg md l me mf">upload: bq_data/schemas/backend/avro/sessions_phase2-000000000000.avsc to s3://my-bucket/bq_data/schemas/backend/avro/sessions_phase2-000000000000.avsc</span><span id="e8c2" class="kx ky hi km b fi mg md l me mf">upload: bq_data/schemas/backend/avro/sessions_detailed_phase2-000000000000.avsc to s3://my-bucket/bq_data/schemas/backend/avro/sessions_detailed_phase2-000000000000.avsc</span></pre><p id="420b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在S3有了AVRO数据和模式之后，可以使用本节开头所示的模板创建Hive表的DDL。我使用了另一个shell脚本<code class="du kj kk kl km b">create_tables_hive.sh</code>(如下所示)来覆盖任意数量的表:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="fb89" class="kx ky hi km b fi mc md l me mf">schema_avro=$(ls -l bq_data/backend | tail -n+2 | awk '{print $9}')<br/>for table_name in $schema_avro; do<br/>  file_name=$(ls -l bq_data/backend/$table_name | tail -n+2 | awk \<br/>    -v srch="avro" -v repl="avsc" '{ sub(srch,repl,$9); print $9 }')<br/>  table_definition=$(<br/>    echo "CREATE EXTERNAL TABLE IF NOT EXISTS backend."$table_name"\<br/>\nSTORED AS AVRO""\<br/>\nLOCATION 's3://my-bucket/bq_data/backend/"$table_name"'""\<br/>\nTBLPROPERTIES('avro.schema.url'='s3://my-bucket/bq_data/\<br/>schemas/backend/avro/"$file_name"');"<br/>  )<br/>  printf "\n$table_definition\n"<br/>done</span></pre><p id="0bef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">运行该脚本会提供以下内容:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="00f0" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 tmpAftab]$ sh create_tables_hive.sh</span><span id="e920" class="kx ky hi km b fi mg md l me mf">CREATE EXTERNAL TABLE IF NOT EXISTS backend.sessions_daily_phase2<br/>STORED AS AVRO<br/>LOCATION 's3://my-bucket/bq_data/backend/sessions_daily_phase2' TBLPROPERTIES ('avro.schema.url'='s3://my-bucket/bq_data/schemas/backend/avro/sessions_daily_phase2-000000000000.avsc');</span><span id="3bad" class="kx ky hi km b fi mg md l me mf">CREATE EXTERNAL TABLE IF NOT EXISTS backend.sessions_detailed_phase2 STORED AS AVRO<br/>LOCATION 's3://my-bucket/bq_data/backend/sessions_detailed_phase2'<br/>TBLPROPERTIES ('avro.schema.url'='s3://my-bucket/bq_data/schemas/backend/avro/sessions_detailed_phase2-000000000000.avsc');</span><span id="e7bf" class="kx ky hi km b fi mg md l me mf">CREATE EXTERNAL TABLE IF NOT EXISTS backend.sessions_phase2<br/>STORED AS AVRO<br/>LOCATION 's3://my-bucket/bq_data/backend/sessions_phase2' TBLPROPERTIES ('avro.schema.url'='s3://my-bucket/bq_data/schemas/backend/avro/sessions_phase2-000000000000.avsc');</span></pre><p id="4561" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我在配置单元控制台上运行了上面的代码，以实际创建配置单元表:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="6b9c" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 tmpAftab]$ hive<br/>Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j2.properties Async: false</span><span id="5680" class="kx ky hi km b fi mg md l me mf">hive&gt; CREATE EXTERNAL TABLE IF NOT EXISTS backend.sessions_daily_phase2<br/>&gt; STORED AS AVRO<br/>&gt; LOCATION 's3://my-bucket/bq_data/backend/sessions_daily_phase2' TBLPROPERTIES ('avro.schema.url'='s3://my-bucket/bq_data/schemas/backend/avro/sessions_daily_phase2-000000000000.avsc');<br/>OK<br/>Time taken: 4.24 seconds</span><span id="f0d9" class="kx ky hi km b fi mg md l me mf">hive&gt;<br/>&gt; CREATE EXTERNAL TABLE IF NOT EXISTS backend.sessions_detailed_phase2 STORED AS AVRO<br/>&gt; LOCATION 's3://my-bucket/bq_data/backend/sessions_detailed_phase2'<br/>&gt; TBLPROPERTIES ('avro.schema.url'='s3://my-bucket/bq_data/schemas/backend/avro/sessions_detailed_phase2-000000000000.avsc');<br/>OK<br/>Time taken: 0.563 seconds</span><span id="d5c7" class="kx ky hi km b fi mg md l me mf">hive&gt;<br/>&gt; CREATE EXTERNAL TABLE IF NOT EXISTS backend.sessions_phase2<br/>&gt; STORED AS AVRO<br/>&gt; LOCATION 's3://my-bucket/bq_data/backend/sessions_phase2' TBLPROPERTIES ('avro.schema.url'='s3://my-bucket/bq_data/schemas/backend/avro/sessions_phase2-000000000000.avsc');<br/>OK<br/>Time taken: 0.386 seconds</span></pre><p id="6b29" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，我已经成功地创建了配置单元表。为了验证这些表是否有效，我运行了这个简单的查询:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="ff74" class="kx ky hi km b fi mc md l me mf">hive&gt; select count(*) from backend.sessions_detailed_phase2;<br/>Query ID = hadoop_20190214152548_2316cb5b-29f1-4416-922e-a6ff02ec1775<br/>Total jobs = 1<br/>Launching Job 1 out of 1<br/>Status: Running (Executing on YARN cluster with App id application_1550010493995_0220)<br/>----------------------------------------------------------------------------------------------<br/><strong class="km hj">VERTICES      MODE        STATUS  TOTAL  COMPLETED  RUNNING  PENDING  FAILED  KILLED<br/></strong>----------------------------------------------------------------------------------------------<br/>Map 1 .......... container     SUCCEEDED      1          1        0        0       0       0<br/>Reducer 2 ...... container     SUCCEEDED      1          1        0        0       0       0<br/>----------------------------------------------------------------------------------------------<br/><strong class="km hj">VERTICES: 02/02  [==========================&gt;&gt;] 100%  ELAPSED TIME: 8.17 s<br/></strong>----------------------------------------------------------------------------------------------<br/>OK<br/>6130</span></pre><p id="fbf3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以有效！</p><h2 id="d1f7" class="kx ky hi bd kz la lb lc ld le lf lg lh jg li lj lk jk ll lm ln jo lo lp lq lr bi translated">第五步。从配置单元表中提取配置单元表定义</h2><p id="0a5a" class="pw-post-body-paragraph iv iw hi ix b iy ls ja jb jc lt je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">如前所述，Athena要求您在<code class="du kj kk kl km b">CREATE</code>语句中明确指定字段名及其数据类型。在第3步中，我提取了AVRO模式，它可以用在Athena表DDL的<code class="du kj kk kl km b">WITH SERDEPROPERTIES</code>中，但是我还必须指定所有的恶魔名称和它们的(Hive)数据类型。现在我已经有了Hive metastore中的表，我可以通过运行<code class="du kj kk kl km b">SHOW CREATE TABLE</code>轻松地获得这些表。首先，为所有表准备配置单元DDL查询:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="1752" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 tmpAftab]$ ls -l bq_data/backend | tail -n+2 | awk '{print "hive -e '\''SHOW CREATE TABLE backend."$9"'\'' &gt; bq_data/schemas/backend/hql/backend."$9".hql;" }'</span><span id="69af" class="kx ky hi km b fi mg md l me mf">hive -e 'SHOW CREATE TABLE backend.sessions_daily_phase2' &gt; bq_data/schemas/backend/hql/backend.sessions_daily_phase2.hql;</span><span id="84a5" class="kx ky hi km b fi mg md l me mf">hive -e 'SHOW CREATE TABLE backend.sessions_detailed_phase2' &gt; bq_data/schemas/backend/hql/backend.sessions_detailed_phase2.hql;</span><span id="2a25" class="kx ky hi km b fi mg md l me mf">hive -e 'SHOW CREATE TABLE backend.sessions_phase2' &gt; bq_data/schemas/backend/hql/backend.sessions_phase2.hql;</span></pre><p id="e8e6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">执行上述命令复制<code class="du kj kk kl km b">bq_data/schemas/backend/hql/</code>下的配置单元表定义。让我们看看里面有什么:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="ba26" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 tmpAftab]$ cat bq_data/schemas/backend/hql/backend.sessions_detailed_phase2.hql</span><span id="b0cb" class="kx ky hi km b fi mg md l me mf">CREATE EXTERNAL TABLE `backend.sessions_detailed_phase2`(<br/>`uid` string COMMENT '',<br/>`platform` string COMMENT '',<br/>`version` string COMMENT '',<br/>`country` string COMMENT '',<br/>`sessions` bigint COMMENT '',<br/>`active_days` bigint COMMENT '',<br/>`session_time_minutes` double COMMENT '')<br/>ROW FORMAT SERDE<br/>'org.apache.hadoop.hive.serde2.avro.AvroSerDe'<br/>STORED AS INPUTFORMAT<br/>'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'<br/>OUTPUTFORMAT<br/>'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'<br/>LOCATION<br/>'s3://my-bucket/bq_data/backend/sessions_detailed_phase2'<br/>TBLPROPERTIES (<br/>'avro.schema.url'='s3://my-bucket/bq_data/schemas/backend/avro/sessions_detailed_phase2-000000000000.avsc',<br/>'transient_lastDdlTime'='1550157659')</span></pre><p id="2908" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，在Athena中创建AVRO表所需的所有构件都已经存在:</p><ul class=""><li id="b744" class="ka kb hi ix b iy iz jc jd jg kc jk kd jo ke js kf kg kh ki bi translated">字段名和数据类型可以从配置单元表DDL中获得(用于<code class="du kj kk kl km b">CREATE</code>语句的列部分)</li><li id="c6f6" class="ka kb hi ix b iy kn jc ko jg kp jk kq jo kr js kf kg kh ki bi translated">AVRO模式(JSON)可以从提取的<code class="du kj kk kl km b">.avsc</code>文件中获得(将在<code class="du kj kk kl km b">WITH SERDEPROPERTIES</code>中提供)。</li></ul><h2 id="1aa8" class="kx ky hi bd kz la lb lc ld le lf lg lh jg li lj lk jk ll lm ln jo lo lp lq lr bi translated">第六步。使用步骤3和5的输出创建Athena表</h2><p id="8f0f" class="pw-post-body-paragraph iv iw hi ix b iy ls ja jb jc lt je jf jg lu ji jj jk lv jm jn jo lw jq jr js hb bi translated">如果你还和我在一起，你已经做得很好了。我现在将执行最后一步，即创建Athena表。我使用下面的脚本组合<code class="du kj kk kl km b">.avsc</code>和<code class="du kj kk kl km b">.hql</code>文件来构建Athena表定义:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="14a9" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 tmpAftab]$ cat create_tables_athena.sh </span><span id="d15a" class="kx ky hi km b fi mg md l me mf"># directory where extracted avro schemas are stored<br/>schema_avro=(bq_data/schemas/backend/avro/*)<br/># directory where extracted HQL schemas are stored<br/>schema_hive=(bq_data/schemas/backend/hql/*)<br/>for i in ${!schema_avro[*]}; do<br/>  schema=$(awk -F '{print $0}' '/CREATE/{flag=1}/STORED/{flag=0}\<br/>   flag' ${schema_hive[$i]})<br/>  location=$(awk -F '{print $0}' '/LOCATION/{flag=1; next}\<br/>  /TBLPROPERTIES/{flag=0} flag' ${schema_hive[$i]})<br/>  properties=$(cat ${schema_avro[$i]})<br/>  table=$(echo $schema '\n' \<br/>    "WITH SERDEPROPERTIES ('avro.schema.literal'='\n"$properties \<br/>    "\n""')STORED AS AVRO \n" \<br/>    "LOCATION" $location";\n\n")<br/>  printf "\n$table\n"<br/>done \<br/>  &gt; bq_data/schemas/backend/all_athena_tables/all_athena_tables.hql</span></pre><p id="4ea2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">运行上面的脚本将Athena表定义复制到<code class="du kj kk kl km b">bq_data/schemas/backend/all_athena_tables/all_athena_tables.hql</code>。在我的案例中，它包含:</p><pre class="kt ku kv kw fd ly km lz ma aw mb bi"><span id="6b97" class="kx ky hi km b fi mc md l me mf">[hadoop@ip-10-0-10-205 all_athena_tables]$ cat all_athena_tables.hql</span><span id="08df" class="kx ky hi km b fi mg md l me mf">CREATE EXTERNAL TABLE `backend.sessions_daily_phase2`( `uid` string COMMENT '', `activity_date` string COMMENT '', `sessions` bigint COMMENT '', `session_time_minutes` double COMMENT '')<br/>ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'<br/>WITH SERDEPROPERTIES ('avro.schema.literal'='<br/>{ "type" : "record", "name" : "Root", "fields" : [ { "name" : "uid", "type" : [ "null", "string" ] }, { "name" : "activity_date", "type" : [ "null", "string" ] }, { "name" : "sessions", "type" : [ "null", "long" ] }, { "name" : "session_time_minutes", "type" : [ "null", "double" ] } ] }')<br/>STORED AS AVRO<br/>LOCATION 's3://my-bucket/bq_data/backend/sessions_daily_phase2';</span><span id="5d8c" class="kx ky hi km b fi mg md l me mf">CREATE EXTERNAL TABLE `backend.sessions_detailed_phase2`( `uid` string COMMENT '', `platform` string COMMENT '', `version` string COMMENT '', `country` string COMMENT '', `sessions` bigint COMMENT '', `active_days` bigint COMMENT '', `session_time_minutes` double COMMENT '')<br/>ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'<br/>WITH SERDEPROPERTIES ('avro.schema.literal'='<br/>{ "type" : "record", "name" : "Root", "fields" : [ { "name" : "uid", "type" : [ "null", "string" ] }, { "name" : "platform", "type" : [ "null", "string" ] }, { "name" : "version", "type" : [ "null", "string" ] }, { "name" : "country", "type" : [ "null", "string" ] }, { "name" : "sessions", "type" : [ "null", "long" ] }, { "name" : "active_days", "type" : [ "null", "long" ] }, { "name" : "session_time_minutes", "type" : [ "null", "double" ] } ] } ')<br/>STORED AS AVRO<br/>LOCATION 's3://my-bucket/bq_data/backend/sessions_detailed_phase2';</span><span id="a17a" class="kx ky hi km b fi mg md l me mf">CREATE EXTERNAL TABLE `backend.sessions_phase2`( `uid` string COMMENT '', `sessions` bigint COMMENT '', `active_days` bigint COMMENT '', `session_time_minutes` double COMMENT '')<br/>ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'<br/>WITH SERDEPROPERTIES ('avro.schema.literal'='<br/>{ "type" : "record", "name" : "Root", "fields" : [ { "name" : "uid", "type" : [ "null", "string" ] }, { "name" : "sessions", "type" : [ "null", "long" ] }, { "name" : "active_days", "type" : [ "null", "long" ] }, { "name" : "session_time_minutes", "type" : [ "null", "double" ] } ] }')<br/>STORED AS AVRO<br/>LOCATION 's3://my-bucket/bq_data/backend/sessions_phase2';</span></pre><p id="6e22" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我在Athena中运行上述脚本来创建表格:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/fbf72196b0504bb350d3c7e156dfa453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qr64XFilMPAwS8RAQ8tImw.png"/></div></div></figure><p id="79f6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是了。</p><p id="149d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感觉过程有点漫长。然而，这对我很有效。另一种方法是使用AWS Glue wizard来抓取数据并推断模式。如果你使用过AWS胶水向导，请在下面的评论区分享你的经验。</p></div></div>    
</body>
</html>