<html>
<head>
<title>Turkish Text Classification, A Fast, Easy and Naive Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">土耳其语文本分类，一个快速，简单和天真的方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/turkish-text-classification-a-fast-easy-and-naive-approach-d1b85609f7f3?source=collection_archive---------15-----------------------#2020-08-29">https://medium.com/analytics-vidhya/turkish-text-classification-a-fast-easy-and-naive-approach-d1b85609f7f3?source=collection_archive---------15-----------------------#2020-08-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/52e5918de8fb55f0bcba961e4b673def.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NMXCcBYz8dUsJtStAl3ccQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://www.analyticsvidhya.com/blog/2020/03/6-pretrained-models-text-classification/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2020/03/6-pre trained-models-text-class ification/</a></figcaption></figure><p id="4bee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">文本分类是使用机器学习对句子进行分类的任务，不需要人工。</p><p id="f34e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这项工作中，我尝试在土耳其数据集上进行快速文本分类。数据集很大，所以我的电脑无法处理所有的数据，这就是为什么我只取了40k行。这使得我的代码更快。数据集由投诉和它们的类别组成，我无法证明，但应该来自https://www.sikayetvar.com的<a class="ae iu" href="https://www.sikayetvar.com" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="e55c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">很久以前，我曾尝试用SGDClassifer完成类似的任务，所以我的第一个模型是SGDClassifer。SGDClassifer是一个很好的多类文本分类工具，所以当你开始这样的任务时，我强烈推荐你首先使用它。除此之外，我还尝试了LinearSVC。</p><p id="65a0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">需要导入的库:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="b322" class="kc kd hi jy b fi ke kf l kg kh">#Mains<br/>import numpy as np<br/>import pandas as pd<br/>import re<br/>import string</span><span id="4b4f" class="kc kd hi jy b fi ki kf l kg kh"><em class="kj">#Models<br/>from sklearn.linear_model import SGDClassifier<br/>from sklearn.svm import LinearSVC</em></span><span id="cb03" class="kc kd hi jy b fi ki kf l kg kh"><em class="kj">#Sklearn Helpers<br/>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer<br/>from sklearn.feature_extraction.text import TfidfTransformer<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.model_selection import GridSearchCV, train_test_split<br/>from sklearn.metrics import classification_report<br/>from sklearn.feature_selection import chi2</em></span><span id="8e64" class="kc kd hi jy b fi ki kf l kg kh"><em class="kj">#For plots<br/>from wordcloud import WordCloud<br/>import matplotlib.pyplot as plt</em></span><span id="eaaf" class="kc kd hi jy b fi ki kf l kg kh"><em class="kj">pd.set_option(‘display.max_colwidth’, -1)</em></span></pre><p id="7f47" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">读取数据集，首先让我们将文本列设置为较低，并删除一些土耳其字符，并且在每个句子的结尾都有devam noku，这意味着继续读取，这意味着句子没有完全爬行，我还更改了一些土耳其字符。这里要提到的一点是，数据大约有400k行，这对我的计算机来说太多了，所以我只使用了40k，但可以随意尝试更多。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="b60d" class="kc kd hi jy b fi ke kf l kg kh">df=pd.read_csv(‘ticaret-yorum.csv’)<br/>df=df.sample(n=40000, random_state=1)<br/>df[‘text’]=df[‘text’].str.lower()</span><span id="7592" class="kc kd hi jy b fi ki kf l kg kh">df[‘text_new’] = df[‘text’].str.replace(‘...devamını oku’, ‘’)</span></pre><p id="c298" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看32个类的值计数:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="77a4" class="kc kd hi jy b fi ke kf l kg kh">df[‘category’].value_counts().sort_index(ascending=False).plot(kind=’bar’, figsize=(12, 8))</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kk"><img src="../Images/392801e8a3436989c4e6690ae698331e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BsSTabiOb-F5UbOHKOIH9w.png"/></div></div></figure><p id="1670" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们在文本列中有品牌名称，我认为这可能会导致过度拟合，所以我去了www.şikayetvar.com的<a class="ae iu" href="http://Şikayetvar" rel="noopener ugc nofollow" target="_blank"/>获取品牌名称，这样我就可以将它们从数据集中排除。在某种程度上，你可能不得不使用更复杂的方法来寻找-排除品牌名称，作为一个提示，我可以建议你从句子中删除所有的第一个单词，因为它包含了品牌的名称。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="1fa5" class="kc kd hi jy b fi ke kf l kg kh">brands=pd.read_csv(‘<a class="ae iu" href="https://raw.githubusercontent.com/pytmar/Jupyter-Notebooks/master/turkish_brands%20-%20Sayfa1.csv'" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/pytmar/Jupyter-Notebooks/master/turkish_brands%20-%20Sayfa1.csv'</a>)<br/>brands.columns=[‘brand_name’]<br/>pat = r’\b(?:{})\b’.format(‘|’.join(list(brands[‘brand_name’].str.lower())))<br/>df[‘text_new’] = df[‘text_new’].str.replace(pat, ‘’)<br/>df.head(5)</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/3efffc470a510d7470f0f680658e5104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xc44pgv5xc-BVOlsyl4OAg.png"/></div></div></figure><p id="1f25" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然nltk模块有土耳其语的停用词，但它是有限的，所以我找到了一个很好的数据集，其中包含大约1000个土耳其语的停用词，在这里我删除了这些停用词:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="e732" class="kc kd hi jy b fi ke kf l kg kh">stop_words=pd.read_csv(‘<a class="ae iu" href="https://raw.githubusercontent.com/InJuxSanct/turkish-stopwords/master/src/lib/stopwords/raw-stopwords.txt'" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/InJuxSanct/turkish-stopwords/master/src/lib/stopwords/raw-stopwords.txt'</a>, sep=” “, header=None)<br/>stop_words.columns=[‘words_list’]<br/>pat2 = r’\b(?:{})\b’.format(‘|’.join(list(stop_words[‘words_list’].str.lower())))<br/>df[‘text_new2’] = df[‘text_new’].str.lower().str.replace(pat2, ‘’)<br/>df.sample(5)</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/76e68a2c9f7551705330e87e50879d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tW4ksP9qCnSLm1xxMBah0A.png"/></div></div></figure><p id="f67e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，删除标点符号任务:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="31f7" class="kc kd hi jy b fi ke kf l kg kh">df=df.dropna()<br/>PUNCT_TO_REMOVE = string.punctuation<br/>def remove_punctuation(text):<br/> return text.translate(str.maketrans(‘’, ‘’, PUNCT_TO_REMOVE))<br/>df[‘text_final’] = df[‘text_new2’].apply(lambda text: remove_punctuation(text)) </span></pre><p id="7b1d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们来做一个wordcloud的剧情:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="ff9d" class="kc kd hi jy b fi ke kf l kg kh">wordcloud = WordCloud(width=1000, height=500).generate(“+”.join(df[‘text_final’]))<br/>plt.figure(figsize=(15,8))<br/>plt.imshow(wordcloud, interpolation=’bilinear’)<br/>plt.axis(“off”)<br/>plt.show()</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/ac0979fb77959cc64cecca0116bd661a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2guohrIaDp2vJ7NA817ZBA.png"/></div></div></figure><p id="6bb9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们还可以显示特定类别的词云:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="0509" class="kc kd hi jy b fi ke kf l kg kh">wordcloud = WordCloud(width=1000, height=500).generate(“+”.join(df[‘text_final’].loc[df[‘category’]==’finans’]))<br/>plt.figure(figsize=(15,8))<br/>plt.imshow(wordcloud, interpolation=’bilinear’)<br/>plt.axis(“off”)<br/>plt.show()</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/05edd36f4528a81eb292817985da6947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XvTL6sye0CRfS2mGEmNAIg.png"/></div></div></figure><p id="393e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">分割数据集，如果你对精度不满意，你也可以尝试GridSearch。最后，对于文本分类，我更喜欢sklearn管道:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="4137" class="kc kd hi jy b fi ke kf l kg kh">X_train, X_test, y_train, y_test = train_test_split(df[‘text_final’], df[‘category’], random_state = 0)</span><span id="5a6c" class="kc kd hi jy b fi ki kf l kg kh">trial = Pipeline([(‘vect’, CountVectorizer()),<br/> (‘tfidf’, TfidfTransformer()),<br/> (‘clf’, SGDClassifier()),])</span><span id="1d08" class="kc kd hi jy b fi ki kf l kg kh">parameters = {<br/> ‘vect__max_df’: (0.5, 0.75, 1.0),<br/> ‘vect__max_features’: (None, 5000, 10000, 50000),<br/> ‘vect__ngram_range’: ((1, 1), (1, 2)), # unigrams or bigrams<br/> #’tfidf__use_idf’: (True, False),<br/> # ‘tfidf__norm’: (‘l1’, ‘l2’),<br/> ‘clf__max_iter’: (20,),<br/> ‘clf__alpha’: (0.00001, 0.000001),<br/> ‘clf__penalty’: (‘l2’, ‘elasticnet’),<br/> # ‘clf__max_iter’: (10, 50, 80),<br/>}<br/>grid_search = GridSearchCV(trial, parameters, n_jobs=-1, verbose=1)</span><span id="f22c" class="kc kd hi jy b fi ki kf l kg kh">grid_search.fit(X_train, y_train)<br/>print(“Best score: %0.3f” % grid_search.best_score_)<br/>print(“Best parameters set:”)<br/>best_parameters = grid_search.best_estimator_.get_params()<br/>print(best_parameters)</span></pre><p id="e489" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我尝试了一些最好的估计参数，但我没有努力:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="7738" class="kc kd hi jy b fi ke kf l kg kh">trial = Pipeline([(‘vect’, CountVectorizer(max_df=0.75, ngram_range=(1, 2))),<br/> (‘tfidf’, TfidfTransformer()),<br/> (‘clf’, SGDClassifier(loss=’modified_huber’,alpha=1e-05, max_iter=20, penalty=’elasticnet’)),])<br/>trial.fit(X_train, y_train)<br/>print(“Accuracy: “ + str(trial.score(X_test, y_test)))</span><span id="0a11" class="kc kd hi jy b fi ki kf l kg kh">output: Accuracy: 0.7904</span></pre><p id="d50d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">无任何调谐的线性SVC:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="2f4a" class="kc kd hi jy b fi ke kf l kg kh">trial2 = Pipeline([(‘vectorizer’,CountVectorizer()), <br/> (‘tfidf’, TfidfTransformer()),<br/> (‘classifier’, LinearSVC())])<br/>trial2.fit(X_train, y_train)<br/>print(“Accuracy: “ + str(trial2.score(X_test, y_test)))</span><span id="3ff6" class="kc kd hi jy b fi ki kf l kg kh">output: Accuracy: 0.7886</span></pre><p id="8207" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们来看看好的试验能预测什么，如果你想试试，去www.şikayetvar.com写一个句子，然后看这里:</p><p id="ceca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">SGD分类器:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="0e9d" class="kc kd hi jy b fi ke kf l kg kh">cv = CountVectorizer(ngram_range=(1,2))</span><span id="8ba7" class="kc kd hi jy b fi ki kf l kg kh">message=”””çocuğum doğduğundan beridir Molfix bel bantlı bezi çok güzeldi, taki 3 gün önce bir eczaneden<br/>Molfix 3 midi 6–11 kg 94 adetlik külot bez alana kadar. Aldığım bez daha 3 kez kullanımda sızdırma yapıyor<br/>ve halen külot bez sızdırmakta devam ediyor. İçinden daha fazla bir şey kullanamadık elimizde kaldı. Öyle <br/>bu geri kalanı iade veya değişim yapılmasını talep ediyorum. Molfix’e hiç yakışmadı. Gerçekten sürekli bez<br/>değiştirirken elbisede değiştirmek zorunda kalıyorduk. Normal bel bantlı bezle değiştirilirse sevinirim.”””</span><span id="f471" class="kc kd hi jy b fi ki kf l kg kh">data = [message]<br/>n=3<br/>probs = trial.predict(data)<br/>probs<br/>output:array(['anne-bebek'], dtype='&lt;U25')</span></pre><p id="2da2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性SVC:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="52fa" class="kc kd hi jy b fi ke kf l kg kh">message=”””çocuğum doğduğundan beridir Molfix bel bantlı bezi çok güzeldi, taki 3 gün önce bir eczaneden<br/>Molfix 3 midi 6–11 kg 94 adetlik külot bez alana kadar. Aldığım bez daha 3 kez kullanımda sızdırma yapıyor<br/>ve halen külot bez sızdırmakta devam ediyor. İçinden daha fazla bir şey kullanamadık elimizde kaldı. Öyle <br/>bu geri kalanı iade veya değişim yapılmasını talep ediyorum. Molfix’e hiç yakışmadı. Gerçekten sürekli bez<br/>değiştirirken elbisede değiştirmek zorunda kalıyorduk. Normal bel bantlı bezle değiştirilirse sevinirim.”””</span><span id="0c09" class="kc kd hi jy b fi ki kf l kg kh">data = [message]<br/>probs = trial2.predict(data)<br/>probs<br/>output:array(['anne-bebek'], dtype='&lt;U25')</span></pre><p id="9659" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你用predict_proba:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="38e5" class="kc kd hi jy b fi ke kf l kg kh">message=”””çocuğum doğduğundan beridir Molfix bel bantlı bezi çok güzeldi, taki 3 gün önce bir eczaneden<br/>Molfix 3 midi 6–11 kg 94 adetlik külot bez alana kadar. Aldığım bez daha 3 kez kullanımda sızdırma yapıyor<br/>ve halen külot bez sızdırmakta devam ediyor. İçinden daha fazla bir şey kullanamadık elimizde kaldı. Öyle <br/>bu geri kalanı iade veya değişim yapılmasını talep ediyorum. Molfix’e hiç yakışmadı. Gerçekten sürekli bez<br/>değiştirirken elbisede değiştirmek zorunda kalıyorduk. Normal bel bantlı bezle değiştirilirse sevinirim.”””</span><span id="2ebe" class="kc kd hi jy b fi ki kf l kg kh">data = [message]</span><span id="6f75" class="kc kd hi jy b fi ki kf l kg kh">pd.DataFrame(sorted(zip(trial.classes_, trial.predict_proba(data)[0])), columns=[‘class’, ‘probability’]).sort_values(by=’probability’, ascending=False).head(3)<br/>output: <br/><br/>class,     probability<br/>anne-bebek, 1.00<br/>alisveris,  0.0<br/>turizm,     0.0</span></pre><p id="e421" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">别忘了查看班级表演的分类报告:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a420" class="kc kd hi jy b fi ke kf l kg kh">y_pred=trial.predict(X_test)<br/>print(classification_report(y_test, y_pred))</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kn"><img src="../Images/c105b51cf34993634c534687998307e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x_zGDSqanv8LRNkpUkJGLA.png"/></div></div></figure><p id="1952" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有一个很好的方法来检查每个类最重要的uni和bigrams:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="5e83" class="kc kd hi jy b fi ke kf l kg kh">df[‘category_id’]=df[‘category’].factorize()[0]<br/>category_id_df = df[[‘text_final’, ‘category_id’]].drop_duplicates().sort_values(‘category_id’)<br/>category_to_id = dict(category_id_df.values)<br/>tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm=’l2', encoding=’latin-1', ngram_range=(1, 2))</span><span id="b34c" class="kc kd hi jy b fi ki kf l kg kh">features = tfidf.fit_transform(df.text_final).toarray()<br/>labels = df.category_id</span><span id="3fbf" class="kc kd hi jy b fi ki kf l kg kh">N = 2<br/>for Product, category_id in sorted(category_to_id.items()):<br/> features_chi2 = chi2(features, labels == category_id)<br/> indices = np.argsort(features_chi2[0])<br/> feature_names = np.array(tfidf.get_feature_names())[indices]<br/> unigrams = [v for v in feature_names if len(v.split(‘ ‘)) == 1]<br/> bigrams = [v for v in feature_names if len(v.split(‘ ‘)) == 2]<br/> print(“# ‘{}’:”.format(Product))<br/> print(“ . Most correlated unigrams:\n . {}”.format(‘\n . ‘.join(unigrams[-N:])))<br/> print(“ . Most correlated bigrams:\n . {}”.format(‘\n . ‘.join(bigrams[-N:])))</span></pre><p id="3160" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我不能展示全部结果，但这里是输出的一部分:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/0c1969ad4906eb0913ed3040271033dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lmKglzLSmie9I6ZU6X7HKg.png"/></div></div></figure><p id="bc1f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢您的阅读。请用不同的模型挑战自己，因为这是一个大数据，您可以尝试pyspark来完成这项任务。</p><p id="d4f6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">来源</strong>:</p><p id="197d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据:<a class="ae iu" href="https://www.kaggle.com/savasy/multiclass-classification-data-for-turkish-tc32" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/savasy/multi class-classification-data-for-Turkish-tc32</a></p><p id="a3ba" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">灵感:<a class="ae iu" href="https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5" rel="noopener" target="_blank">https://towards data science . com/multi-label-text-class ification-with-scikit-learn-30714 b 7819 C5</a></p><p id="48db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Jupyter笔记本:<a class="ae iu" href="https://github.com/pytmar/Jupyter-Notebooks/blob/master/medium_text_class_sklearn.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/pyt mar/Jupyter-Notebooks/blob/master/medium _ text _ class _ sk learn . ipynb</a></p></div></div>    
</body>
</html>