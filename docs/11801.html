<html>
<head>
<title>ML15: PyTorch — CNN on MNIST</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML15:py torch——CNN 关于 MNIST 的报道</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml15-56c033cc00e9?source=collection_archive---------8-----------------------#2020-12-19">https://medium.com/analytics-vidhya/ml15-56c033cc00e9?source=collection_archive---------8-----------------------#2020-12-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="554d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">计算机视觉领域的资深人士(准确率 99.07%)</h2></div><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="985a" class="jg jh hi jc b fi ji jj l jk jl">Read time: 20 min</span><span id="f7cd" class="jg jh hi jc b fi jm jj l jk jl">Complete code on Colab: <a class="ae jn" href="https://bit.ly/2KmLYK7" rel="noopener ugc nofollow" target="_blank">https://bit.ly/2KmLYK7</a></span></pre><p id="e70b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们在 CNN 对 MNIST 的测试数据上得到了<strong class="jq hj"> 99.07% </strong>的准确率，而在 ML14 中 MLP 对 MNIST 的测试数据只有<strong class="jq hj"> 98.13% </strong>的准确率。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><blockquote class="kr ks kt"><p id="9d5b" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated"><strong class="jq hj"> <em class="hi">轮廓</em></strong><em class="hi"><br/>(1)</em><a class="ae jn" href="#10e7" rel="noopener ugc nofollow"><em class="hi">CNN</em></a><em class="hi"><br/>(2)</em><a class="ae jn" href="#0168" rel="noopener ugc nofollow"><em class="hi">准备数据</em></a><em class="hi"><br/>(3)</em><a class="ae jn" href="#0168" rel="noopener ugc nofollow"><em class="hi">MNIST 的图像</em></a><em class="hi"><br/>(4)</em><a class="ae jn" href="#bbf5" rel="noopener ugc nofollow"><em class="hi">初始化 CNN </em> </a> <em class="hi"> </em><a class="ae jn" href="#78fd" rel="noopener ugc nofollow"><em class="hi"/></a><em class="hi"><br/>(8)</em><a class="ae jn" href="#0727" rel="noopener ugc nofollow"><em class="hi">模型保存</em></a><em class="hi"><br/>【9】</em><a class="ae jn" href="#8dca" rel="noopener ugc nofollow"><em class="hi">常见错误</em></a><em class="hi"><br/>(10)</em><a class="ae jn" href="#a2a8" rel="noopener ugc nofollow"><em class="hi">总结</em></a></p></blockquote></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><p id="6d7c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在<strong class="jq hj"> ML14 </strong>的基础上，我们不断前进。</p><div class="ky kz ez fb la lb"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/ml14-f03f75254934"><div class="lc ab dw"><div class="ld ab le cl cj lf"><h2 class="bd hj fi z dy lg ea eb lh ed ef hh bi translated">ML14:py torch——MNIST 的 MLP</h2><div class="li l"><h3 class="bd b fi z dy lg ea eb lh ed ef dx translated">图像分类的第一步(98.13%的准确率)</h3></div><div class="lj l"><p class="bd b fp z dy lg ea eb lh ed ef dx translated">medium.com</p></div></div><div class="lk l"><div class="ll l lm ln lo lk lp lq lb"/></div></div></a></div></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="10e7" class="lr jh hi bd ls lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">(1)美国有线电视新闻网</h1><ul class=""><li id="ac9b" class="mi mj hi jq b jr mk ju ml jx mm kb mn kf mo kj mp mq mr ms bi translated"><em class="ku"> CNN，卷积神经网络</em>，是一种<em class="ku"> FNN </em>。<em class="ku">全连接层</em>(或 MLP)<strong class="jq hj"><em class="ku">太复杂</em></strong><strong class="jq hj"><em class="ku">丢失所有空间信息</em> </strong>，而<em class="ku"> CNN </em>避免了前述问题，并利用卷积层和池层来产生计算机视觉中出色的真实世界结果。[1]</li><li id="1287" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated"><em class="ku">卷积神经网络</em>。<em class="ku">CNN</em>是<strong class="jq hj">与</strong> <em class="ku">完全连接的神经网络</em>完全不同，并且在许多任务中取得了最先进的性能。这些任务包括图像分类、对象检测、语音识别，当然还有句子分类。<strong class="jq hj"><em class="ku">CNN</em>的一个主要优点</strong>是，与<em class="ku">全连接层</em>相比，<em class="ku"> CNN </em>中的卷积层具有数量少得多的<strong class="jq hj">参数</strong>。这允许我们构建更深层次的模型，而不用担心内存溢出。此外，更深的模型通常会带来更好的性能。[2]</li><li id="e66b" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">我们研究了 MLPs 的主要缺点/限制之一——缺乏参数共享——并介绍了卷积网络架构作为一种可能的解决方案。最初为计算机视觉而开发的 CNN，<strong class="jq hj">已经成为 NLP </strong>中的中流砥柱，这主要是因为它们的<strong class="jq hj">高效实现</strong>和<strong class="jq hj">低内存需求</strong>。[3]</li><li id="ded8" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">简言之，</li></ul><ol class=""><li id="1db2" class="mi mj hi jq b jr js ju jv jx my kb mz kf na kj nb mq mr ms bi translated">FNN: MLP(多层感知器，也称为“全连接”网络)，CNN(卷积神经网络)</li><li id="2da9" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj nb mq mr ms bi translated">RNN: RNN(循环神经网络)，LSTM(长短期记忆)，GRU(门控循环单元)</li></ol><p id="80c4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">详细查看<strong class="jq hj"> ML04 </strong>了解更多相关神经网络理论。</p><div class="ky kz ez fb la lb"><a href="https://becominghuman.ai/ml04-ce0b172deb2b" rel="noopener  ugc nofollow" target="_blank"><div class="lc ab dw"><div class="ld ab le cl cj lf"><h2 class="bd hj fi z dy lg ea eb lh ed ef hh bi translated">ML04:从 ML 到 DL 再到 NLP</h2><div class="li l"><h3 class="bd b fi z dy lg ea eb lh ed ef dx translated">简明概念图</h3></div><div class="lj l"><p class="bd b fp z dy lg ea eb lh ed ef dx translated">becominghuman.ai</p></div></div><div class="lk l"><div class="nc l lm ln lo lk lp lq lb"/></div></div></a></div></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="0168" class="lr jh hi bd ls lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">(2) <em class="nd">准备数据</em></h1><h1 id="a1ba" class="lr jh hi bd ls lt ne lv lw lx nf lz ma io ng ip mc ir nh is me iu ni iv mg mh bi translated">(3)MNIST 的形象</h1><p id="a274" class="pw-post-body-paragraph jo jp hi jq b jr mk ij jt ju ml im jw jx nj jz ka kb nk kd ke kf nl kh ki kj hb bi translated">MNIST 是我们在<strong class="jq hj"> ML14 </strong>中见过的著名数据集。查看<a class="ae jn" href="https://merscliche.medium.com/ml14-f03f75254934" rel="noopener"> <strong class="jq hj"> ML14 </strong> </a>了解更多信息。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="bbf5" class="lr jh hi bd ls lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">(4)初始化 CNN</h1><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="f9d1" class="jg jh hi jc b fi ji jj l jk jl">class CNN(nn.Module):<br/>    def __init__(self):  <br/>        super(CNN, self).__init__()  <br/>        self.conv1 = nn.Sequential(  <br/>                     nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2),<br/>                     nn.ReLU(),<br/>                     nn.MaxPool2d(kernel_size=2) # (16,14,14)<br/>                     )</span><span id="c6e1" class="jg jh hi jc b fi jm jj l jk jl">self.conv2 = nn.Sequential( # (16,14,14)<br/>                     nn.Conv2d(16, 32, 5, 1, 2), # (32,14,14)<br/>                     nn.ReLU(),<br/>                     nn.MaxPool2d(2) # (32,7,7)<br/>                     )<br/>        self.out = nn.Linear(32*7*7, 10)</span><span id="7546" class="jg jh hi jc b fi jm jj l jk jl">def forward(self, x):<br/>        x = self.conv1(x)<br/>        x = self.conv2(x)<br/>        x = x.view(x.size(0), -1) # (batch, 32,7,7) -&gt; (batch, 32*7*7)<br/>        output = self.out(x)<br/>        return output</span></pre><ul class=""><li id="bb8d" class="mi mj hi jq b jr js ju jv jx my kb mz kf na kj mp mq mr ms bi translated"><em class="ku"> forward( ) </em>为正向传播。<em class="ku"> backward( ) </em>用于反向传播(BP)。在我们定义了<em class="ku">前进()</em>，<em class="ku">后退()</em>会自动采用函数<em class="ku">签名</em>。</li><li id="bd81" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">输入层-&gt;Conv2D (2D 卷积层)-&gt;激活器(Sigmoid) -&gt;平均池-&gt;全连接层-&gt;输出层(Softmax)</li><li id="e42f" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">CNN 通常使用池来处理图像。</li><li id="340d" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">池化减少了特性和参数，但是保留了数据的一些属性。有 3 种流行的池化方法——平均池化、最大池化和随机池化。</li><li id="59bc" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">观察活化剂 ReLU 是 PyTorch 中的一个层。</li></ul><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="f16d" class="jg jh hi jc b fi ji jj l jk jl"># Initialization<br/>cnn = CNN()<br/>print(cnn)</span><span id="7a5a" class="jg jh hi jc b fi jm jj l jk jl">params = list(cnn.parameters())<br/>print('-----')<br/>print(len(params))<br/>print(params[0].size())</span><span id="c0b1" class="jg jh hi jc b fi jm jj l jk jl">EPOCH = 3<br/>BATCH_SIZE = 50<br/>LR = 0.001 # 0.001 is recommended</span><span id="68b7" class="jg jh hi jc b fi jm jj l jk jl"># Optimizer = Adam<br/>optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)</span><span id="e343" class="jg jh hi jc b fi jm jj l jk jl"># Loss function = cross-entropy<br/>loss_function = nn.CrossEntropyLoss()</span></pre><figure class="ix iy iz ja fd nn er es paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="er es nm"><img src="../Images/5870c9ca677831279f7121d49953c7db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EE7Fw2jvWPnlkwpv93gMFA.png"/></div></div><figcaption class="nt nu et er es nv nw bd b be z dx translated">图 CNN 模型的结构。</figcaption></figure></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="f043" class="lr jh hi bd ls lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">(5)训练 CNN</h1><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="0c90" class="jg jh hi jc b fi ji jj l jk jl">record = [] # A container recording the training accuracies</span><span id="3ab8" class="jg jh hi jc b fi jm jj l jk jl">for epoch in range(EPOCH):</span><span id="ec6c" class="jg jh hi jc b fi jm jj l jk jl">train_rights = [] # Record the training accuracies</span><span id="0984" class="jg jh hi jc b fi jm jj l jk jl">for step, (x, y) in enumerate(train_loader):</span><span id="ec51" class="jg jh hi jc b fi jm jj l jk jl">b_x = Variable(x)  <br/>        b_y = Variable(y)</span><span id="0b42" class="jg jh hi jc b fi jm jj l jk jl"><strong class="jc hj">cnn.train() <br/>        # Tell PyTorch that the model is running in training mode (training)</strong></span><span id="5696" class="jg jh hi jc b fi jm jj l jk jl">output = cnn(b_x)  <br/>        loss = loss_function(output, b_y)  <br/>        optimizer.zero_grad() <br/>        # Indicate optimizer &amp; loss function.<br/>        # Bear in mind that one must zero the gradients last time before back propagation.<br/> <br/>        loss.backward()  <br/>        optimizer.step()</span><span id="03cd" class="jg jh hi jc b fi jm jj l jk jl">right = rightness(output, b_y) # (outputs, labels) = (correct numbers, all samples)<br/>        train_rights.append(right)</span><span id="b8fc" class="jg jh hi jc b fi jm jj l jk jl">if step % 200 == 0:</span><span id="0ca3" class="jg jh hi jc b fi jm jj l jk jl"><strong class="jc hj">cnn.eval() <br/>            # Tell PyTorch that the model is running in evaluation mode (validation/test)</strong></span><span id="f39a" class="jg jh hi jc b fi jm jj l jk jl">test_output = cnn(test_x)</span><span id="2833" class="jg jh hi jc b fi jm jj l jk jl">pred_y = torch.max(test_output, 1)[1].data.squeeze()<br/>            test_accuracy = (sum(pred_y == test_y).item() / test_y.size(0)) * 100</span><span id="0e0a" class="jg jh hi jc b fi jm jj l jk jl">train_r = (sum([tup[0] for tup in train_rights]), sum([tup[1] for tup in train_rights]))<br/>            training_accuracy = 100. * train_r[0].numpy() / train_r[1]</span><span id="75a0" class="jg jh hi jc b fi jm jj l jk jl">total_step = len(train_data)//BATCH_SIZE</span><span id="5952" class="jg jh hi jc b fi jm jj l jk jl">print('Epoch [{}/{}], Step [{:4}/{}], Loss: {:.4f} | training accuracy: {:6.2f} % | test accuracy:{:6.2f} %'.format(<br/>                 epoch+1, EPOCH, step+200, total_step, loss.data, training_accuracy, test_accuracy))<br/>            <br/>            record.append((100 - 100. * train_r[0] / train_r[1], 100 - test_accuracy))</span></pre><figure class="ix iy iz ja fd nn er es paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="er es nx"><img src="../Images/7d5ece3474f721c1eabc7d9fc4d05a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S2cc8PptDcscKDHFTVLtNg.png"/></div></div><figcaption class="nt nu et er es nv nw bd b be z dx translated">图 2:培训过程。</figcaption></figure><h2 id="6ad2" class="jg jh hi bd ls ny nz oa lw ob oc od ma jx oe of mc kb og oh me kf oi oj mg ok bi translated">model.train()和 model.eval()</h2><p id="5c36" class="pw-post-body-paragraph jo jp hi jq b jr mk ij jt ju ml im jw jx nj jz ka kb nk kd ke kf nl kh ki kj hb bi translated">顾名思义，这些函数<strong class="jq hj">告诉 PyTorch </strong>模型运行在<strong class="jq hj">训练模式(training) </strong>或<strong class="jq hj">评估模式(validation/test) </strong>。只有当您想要<strong class="jq hj">关闭或打开</strong>模块，例如<strong class="jq hj"> Dropout </strong>或<strong class="jq hj"> BatchNorm </strong>时，这才有一些效果<strong class="jq hj"> </strong>。[4]</p><p id="7ad5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">记住<em class="ku">train()</em>&amp;<em class="ku">eval()</em>会影响<strong class="jq hj">Dropout</strong>&amp;<strong class="jq hj">bact chnorm</strong>，尽管实际上我们在这个 CNN 模型中不做这两者(检查类<em class="ku"> CNN </em>)。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="b3b0" class="lr jh hi bd ls lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">(6)训练和测试数据的准确性</h1><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="9068" class="jg jh hi jc b fi ji jj l jk jl">import numpy as np</span><span id="40d8" class="jg jh hi jc b fi jm jj l jk jl">plt.figure(figsize = (13, 5.5))<br/>train_error, test_error = [k[0] for k in record], [k[1] for k in record]<br/># "record" records the (training error rate, test error rate) in each step<br/>plt.plot(list(range(1,19)), train_error, label= "Training data") <br/>plt.plot(list(range(1,19)), test_error, label= "Test data")</span><span id="b592" class="jg jh hi jc b fi jm jj l jk jl">plt.xticks(range(0, 19, 1))</span><span id="ddee" class="jg jh hi jc b fi jm jj l jk jl">plt.xlabel('Steps', fontsize = 'xx-large') # Change font size<br/>plt.ylabel('Error rate (%)')<br/>plt.ylabel('Error rate (%)', fontsize = 'xx-large') # Change font size<br/>plt.title('CNN', fontname='DejaVu Sans', fontsize = 'xx-large') # Change font size</span><span id="9b5f" class="jg jh hi jc b fi jm jj l jk jl">plt.grid() # Simply add grid by default<br/>plt.legend(fontsize = 'xx-large',  loc= 'upper right')<br/>plt.show()</span></pre><figure class="ix iy iz ja fd nn er es paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="er es ol"><img src="../Images/f2f3a1f476ea246918d2d21cdf57eda9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DmKmFrXmRjGvNXkk1Mf8Zg.png"/></div></div><figcaption class="nt nu et er es nv nw bd b be z dx translated">图 3:步骤中训练数据和测试数据的错误率。</figcaption></figure><figure class="ix iy iz ja fd nn er es paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="er es om"><img src="../Images/5063e6fbabfbde7c86ecde7ba67b7beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gTVgPxXaxn4Z-jz2dFt3tA.png"/></div></div><figcaption class="nt nu et er es nv nw bd b be z dx translated">图 4:训练数据和测试数据的最终模型精度。</figcaption></figure><ul class=""><li id="21b3" class="mi mj hi jq b jr js ju jv jx my kb mz kf na kj mp mq mr ms bi translated">训练准确率:98.73 %</li><li id="be56" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">测试准确度:<strong class="jq hj"> 99.07% </strong></li></ul></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="78fd" class="lr jh hi bd ls lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">(7)对试验数据的预测</h1><figure class="ix iy iz ja fd nn er es paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="er es on"><img src="../Images/3191b149f4bcd228d075fdcedb38b37a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mGVgFbWEt6JCORZE9-vYTg.png"/></div></div><figcaption class="nt nu et er es nv nw bd b be z dx translated">图 5:检查测试数据的预测类别(10 个类别)。</figcaption></figure></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="0727" class="lr jh hi bd ls lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">(8) <em class="nd">模型保存</em></h1><p id="ecf4" class="pw-post-body-paragraph jo jp hi jq b jr mk ij jt ju ml im jw jx nj jz ka kb nk kd ke kf nl kh ki kj hb bi translated">如<a class="ae jn" href="https://merscliche.medium.com/ml14-f03f75254934" rel="noopener">中所说<strong class="jq hj"> ML14 </strong>中所说</a>。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="8dca" class="lr jh hi bd ls lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">(9)常见的错误</h1><p id="12d4" class="pw-post-body-paragraph jo jp hi jq b jr mk ij jt ju ml im jw jx nj jz ka kb nk kd ke kf nl kh ki kj hb bi translated"><strong class="jq hj">忘记初始化模型</strong>是高估模型性能的常见错误。这里有一个这种错误的例子。我们得到了一个性能模型，在训练开始时表现得难以置信的好。</p><figure class="ix iy iz ja fd nn er es paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="er es oo"><img src="../Images/4de9486188c2ef5e334743e6dfdc755f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kF4ntcq_rkbwVeO20xvyg.png"/></div></div><figcaption class="nt nu et er es nv nw bd b be z dx translated">图 6:一个常见的错误——培训过程</figcaption></figure><figure class="ix iy iz ja fd nn er es paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="er es op"><img src="../Images/c138b8feaf95af4218c7a45d0458d3ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AkMNWLLqdgKKBx1VNsXzmw.png"/></div></div><figcaption class="nt nu et er es nv nw bd b be z dx translated">图 7:一个常见的错误——步骤中训练数据和测试数据的错误率。</figcaption></figure></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="a2a8" class="lr jh hi bd ls lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">(10)总结</h1><p id="a2c4" class="pw-post-body-paragraph jo jp hi jq b jr mk ij jt ju ml im jw jx nj jz ka kb nk kd ke kf nl kh ki kj hb bi translated">我们在 CNN 对 MNIST 的测试数据上得到了<strong class="jq hj"> 99.07% </strong>的准确率，而在<strong class="jq hj"> ML14 </strong> MLP 对 MNIST 的测试数据只有<strong class="jq hj"> 98.13% </strong>的准确率。</p><p id="4552" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">CNN 已经在各种比赛中证明了自己，如 ImageNet 大规模视觉识别挑战赛(ILSVRC)，Kaggle 上的比赛等等[2][3][7]。此外，还有一些经典和流行的 CNN(基于 CNN 的模型)如:</p><ul class=""><li id="a990" class="mi mj hi jq b jr js ju jv jx my kb mz kf na kj mp mq mr ms bi translated">LeNet-5</li><li id="429b" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">ImageNet-2010</li><li id="d5bf" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">VGGNet</li><li id="762c" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">谷歌网</li><li id="bb3d" class="mi mj hi jq b jr mt ju mu jx mv kb mw kf mx kj mp mq mr ms bi translated">雷斯内特</li></ul><p id="2e94" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">到目前为止，我们从:NN/DL 理论(<a class="ae jn" href="https://becominghuman.ai/ml04-ce0b172deb2b" rel="noopener ugc nofollow" target="_blank"> ML04 </a> ) = &gt;一个仅仅由 NumPy ( <a class="ae jn" href="https://becominghuman.ai/ml05-8771620a2023" rel="noopener ugc nofollow" target="_blank"> ML05 </a> ) = &gt;一个详细的 PyTorch 教程(<a class="ae jn" href="https://merscliche.medium.com/ml12-59d2a56737ac" rel="noopener"> ML12 </a> ) = &gt; NN 使用 py torch(<a class="ae jn" href="https://merscliche.medium.com/ml13-e52e251d41c5" rel="noopener">ML13</a>)=&gt;MLP 在 MNIST 使用 PyTorch ( <a class="ae jn" href="https://merscliche.medium.com/ml14-f03f75254934" rel="noopener"> ML14 </a> ) = 【T56 这一个。).</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="dbd5" class="lr jh hi bd ls lt lu lv lw lx ly lz ma io mb ip mc ir md is me iu mf iv mg mh bi translated">(11)参考文献</h1><p id="704d" class="pw-post-body-paragraph jo jp hi jq b jr mk ij jt ju ml im jw jx nj jz ka kb nk kd ke kf nl kh ki kj hb bi translated">[1] Subramanian，V. (2018)。用 PyTorch 进行深度学习。英国伯明翰:Packt 出版公司。</p><p id="c30c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[2]t . Ganegedara(2018 年)。用张量流进行自然语言处理。英国伯明翰:Packt 出版公司。</p><p id="9465" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[3] Chollet，F. (2018)用 Python 进行深度学习。纽约州纽约市:曼宁出版公司。</p><p id="66e0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[4]托马斯和帕西(2019 年)。PyTorch 深度学习动手。英国伯明翰:Packt 出版公司。</p><h2 id="a1e0" class="jg jh hi bd ls ny nz oa lw ob oc od ma jx oe of mc kb og oh me kf oi oj mg ok bi translated">(中文)</h2><p id="e7ba" class="pw-post-body-paragraph jo jp hi jq b jr mk ij jt ju ml im jw jx nj jz ka kb nk kd ke kf nl kh ki kj hb bi">[5] 張校捷 (2020)。深入淺出 PyTorch：從模型到源碼。北京，中國：電子工業。<br/>[6] 集智俱樂部 (2019)。深度學習原理與 PyTorch 實戰。北京，中國：人民郵電。<br/>[7] 邢夢來等人 (2018)。深度学习框架 PyTorch 快速开发与实战。北京，中國：電子工業。</p></div></div>    
</body>
</html>