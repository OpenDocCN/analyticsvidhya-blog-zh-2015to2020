<html>
<head>
<title>Logistic Regression — Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归—已解释</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-explained-c6bb5dc5842c?source=collection_archive---------20-----------------------#2019-11-25">https://medium.com/analytics-vidhya/logistic-regression-explained-c6bb5dc5842c?source=collection_archive---------20-----------------------#2019-11-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/08731abffb97cc95490b819ce0403261.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/0*sxPRAyDc-INRjEvC"/></div></figure><h1 id="b541" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">介绍</h1><p id="6025" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在这篇文章中，我们将探索和理解机器学习中的基本分类技术之一<a class="ae ki" href="https://machinelearningmind.com/2019/11/02/machine-learning-classification/" rel="noopener ugc nofollow" target="_blank"/>—<strong class="jm hj">逻辑回归</strong>。</p><p id="b2e3" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated"><em class="ko">原载于2019年11月25日</em><a class="ae ki" href="https://machinelearningmind.com/2019/11/25/logistic-regression-explained/" rel="noopener ugc nofollow" target="_blank"><em class="ko">https://machinelearningmind.com</em></a><em class="ko">。</em></p><ul class=""><li id="8de3" class="kp kq hi jm b jn kj jr kk jv kr jz ks kd kt kh ku kv kw kx bi translated"><strong class="jm hj">二元逻辑回归<em class="ko"> : </em> </strong>它只有两种可能的结果。示例-是或否</li><li id="a3d3" class="kp kq hi jm b jn ky jr kz jv la jz lb kd lc kh ku kv kw kx bi translated"><strong class="jm hj">多项逻辑回归</strong>:有三个或三个以上的名义类别。例子-猫，狗，大象。</li><li id="671b" class="kp kq hi jm b jn ky jr kz jv la jz lb kd lc kh ku kv kw kx bi translated"><strong class="jm hj">有序逻辑回归</strong> -它有三个或更多有序类别，有序意味着类别将按顺序排列。示例-用户评级(1–5)。</li></ul><p id="61d7" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">现在，让我们关注二元逻辑回归。</p><h1 id="4e84" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">逻辑回归</h1><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/08a81167a8c5ea4f2323f0df2803eb32.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/0*3SuBzufNVBH91BNQ"/></div></figure><p id="223c" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">逻辑回归模型用于计算一个关键预测因子的特定值的预测概率，通常是在所有其他预测因子保持不变的情况下。</p><p id="b500" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">让我们以预测患者是否有未来冠心病(CHD)的10年风险为例。</p><p id="87e6" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">可能的预测因素包括患者的心率、血压、吸烟者/非吸烟者等。。我们想预测患者未来10年是否有患冠心病的风险。我们可以有一个决策边界，例如:0.5，使得P(Y) &gt; 0.5，将患者分类为处于危险中，并且P(Y) &lt; 0.5, classify patient as not at risk.</p><p id="fb77" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">So basically, if a model has to succeed in predicting this, we need individual predictor ( <strong class="jm hj"> X </strong>对处于CHD危险中的患者的概率具有恒定的影响[ <strong class="jm hj"> P(Y) </strong> ]。但是，X对Y的概率的影响有不同的值，这取决于X的值。</p><p id="295b" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">这就是<strong class="jm hj">赔率</strong>出现的原因，因为赔率代表预测值X对Y发生的可能性的恒定影响。</p><h1 id="8dcc" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">可能性</h1><p id="6009" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">赔率告诉你一个结果的可能性。对于那些不熟悉赔率概念的人，这里有一个例子。</p><p id="8b72" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">假设Y代表有风险的患者，Z代表没有风险的患者，有人说事件Y的概率是1/3。这是什么意思？</p><p id="5059" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">4名患者中，1名有风险，3名无风险。所以患者处于危险中的几率是1:3。</p><p id="728f" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">这就是概率和赔率的区别。这种情况下的概率是P(Y) = 1/(3+1) = 1/4。<br/>概率考虑了所有结果(3无风险+ 1有风险)。</p><p id="3dfb" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">概率和赔率在数学上是相关的。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es li"><img src="../Images/10b41ba4a3f8f44cd47a72d498c7a2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/0*jikI9Lm4f3w8JDZA"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">患者有患冠心病风险的几率</figcaption></figure><p id="defc" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">如果<em class="ko"> P(Y) </em> =患者处于危险中的概率，那么1- <em class="ko"> P(Y) </em>将给出患者不处于危险中的概率</p><p id="d5fa" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">逻辑回归使用<strong class="jm hj"> Sigmoid函数</strong>来预测概率。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/fcc31aab40cd33b7527a7ae3e27807af.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/0*p7W9K9kUyxNH4G_v"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">Sigmoid函数</figcaption></figure><p id="fab5" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">Sigmoid函数的等式:1/(1+ <em class="ko"> e </em> ^-z)，其中</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/ed9c8b8169280c3a062f915ab5d0b910.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/0*QEXQ45GO-X4d4jO2"/></div></figure><p id="dfe3" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">现在让我们知道P(Y) = 1/(1+ <em class="ko"> e^ </em> -z)来推导log(赔率)</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/559ba36691174d702b64d7dfe384269b.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/0*e9ZzyRppg-e_Ecga"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">导出的对数(比值)</figcaption></figure><p id="cf0a" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">这里是x 1，x 2..代表预测因子β 1，β 2 …代表我们前面讲过的预测因子的<strong class="jm hj"> <em class="ko">常量效应</em> </strong>。</p><h1 id="d5b9" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">价值函数</h1><p id="2115" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">成本函数基本上量化了模型预测值和实际值之间的差异，给出了误差值。</p><p id="e98b" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">这里我们不能使用线性回归(MSE)的代价函数，因为h θ(x) [sigmoid function]是非线性函数，使用MSE将导致具有许多局部最小值的非凸函数。当存在许多局部最小值时，梯度下降将不能成功地找到全局最小值。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/1a4d7d253c1fcc341891d5d86929dcf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/0*g7iq6s50t-OBlGqg.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">成本函数J(θ)</figcaption></figure><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/a16d3937d694306e8126bda6137c1146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/0*YzMyYgKLwLVZze_J.png"/></div></figure><p id="bbfb" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">将上述函数组合成一个函数，就得到最终的成本函数。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/27ede08b434b9af1bbc6894c9cb33554.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/0*YebhDT1VVIDfr5KK.png"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">成本函数J(θ)</figcaption></figure><h2 id="b6f2" class="lt in hi bd io lu lv lw is lx ly lz iw jv ma mb ja jz mc md je kd me mf ji mg bi translated">使用sklearn的逻辑回归</h2><p id="3e69" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">sklearn实现了逻辑回归，这使得我们只需调用函数fit()和predict()来完成分类变得非常容易。</p><p id="473f" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">让我们以10年内有患冠心病风险的患者为例。</p><pre class="le lf lg lh fd mh mi mj mk aw ml bi"><span id="6e24" class="lt in hi mi b fi mm mn l mo mp">df = pd.read_csv("framingham.csv") <br/>df.info()</span></pre><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/25f27eb318f9807236e17013311d8d91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/0*IN_qRzmxGPQCcI_D"/></div><figcaption class="lj lk et er es ll lm bd b be z dx translated">df.info()</figcaption></figure><pre class="le lf lg lh fd mh mi mj mk aw ml bi"><span id="dffb" class="lt in hi mi b fi mm mn l mo mp">print("Percentage of People with heart disease: {0:.2f} %".format(100*df.TenYearCHD.value_counts()[1]/df.TenYearCHD.count()))</span></pre><p id="5900" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated"><strong class="jm hj">产量</strong>:心脏病患者比例:15.23 %</p><p id="b0c4" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">在执行清理操作并剔除统计上无关紧要的变量后，让我们创建新的数据框架并执行逻辑回归。</p><pre class="le lf lg lh fd mh mi mj mk aw ml bi"><span id="d4f1" class="lt in hi mi b fi mm mn l mo mp"># Creating new dataframe with statistically significant columns new_df = df[['male','age','cigsPerDay','prevalentHyp','diabetes','sysBP','diaBP','BMI','heartRate','ed__2.0','ed__3.0','ed__4.0', 'TenYearCHD']] </span><span id="8f46" class="lt in hi mi b fi mr mn l mo mp"># Splitting into predictors and label <br/>X = new_df.drop(['TenYearCHD'], axis=1) <br/>Y = new_df.TenYearCHD </span><span id="fc0f" class="lt in hi mi b fi mr mn l mo mp"># Splitting dataset into train and test in the ratio 7:3 x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=5)</span></pre><p id="cb39" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">现在我们已经准备好应用sklearn的逻辑回归函数了</p><pre class="le lf lg lh fd mh mi mj mk aw ml bi"><span id="21bb" class="lt in hi mi b fi mm mn l mo mp">logreg = LogisticRegression() <br/>logreg.fit(x_train,y_train) <br/>y_pred = logreg.predict(x_test)</span></pre><p id="be5e" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">y_pred保存因变量的所有预测值(TenYearCHD)。现在让我们检查模型的准确性。</p><pre class="le lf lg lh fd mh mi mj mk aw ml bi"><span id="e7d1" class="lt in hi mi b fi mm mn l mo mp">sklearn.metrics.accuracy_score(y_test,y_pred)</span></pre><p id="28ed" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated"><strong class="jm hj">输出</strong>:0.8648686868686</p><p id="8512" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">这意味着我们的模型能够正确分类86%的测试数据，这是一个很好的数字(这个值是否令人满意取决于业务案例)。如果这不令人满意，有优化技术来改进模型，或者可以使用不同的分类技术，这些将在以后的帖子中讨论。</p><p id="ace1" class="pw-post-body-paragraph jk jl hi jm b jn kj jp jq jr kk jt ju jv kl jx jy jz km kb kc kd kn kf kg kh hb bi translated">上面例子的完整笔记本可以在<a class="ae ki" href="https://www.kaggle.com/fahadanwar/logreg-smote" rel="noopener ugc nofollow" target="_blank"> <strong class="jm hj"> Kaggle </strong> </a>和<a class="ae ki" href="https://github.com/fahadanwar10/ml-classification/tree/master/LogisticRegression" rel="noopener ugc nofollow" target="_blank"> <strong class="jm hj"> Github </strong> </a>找到。</p></div></div>    
</body>
</html>