<html>
<head>
<title>Principle Component Analysis: Dimension Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析:降维</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/principle-component-analysis-dimension-reduction-dea987f9d38?source=collection_archive---------16-----------------------#2020-09-23">https://medium.com/analytics-vidhya/principle-component-analysis-dimension-reduction-dea987f9d38?source=collection_archive---------16-----------------------#2020-09-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="00dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated">每个人都知道，为了进行机器学习，需要更多的数据集，但你有没有问过自己，如果数据集中的特征数量变得更多，会不会有问题？更简单地说，这种冗余特性使得从探索性数据分析的数据集提取洞察力变得非常困难。与此同时，这样的数据集使用了大量的竞争资源。</p><p id="8bb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文将讨论主成分分析和降维，这在机器学习中处理大数据集时特别有用。让我们开始吧……..</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/5a145e542762c20afaa6f075fe34a552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aIovpznczyhFoxYrjvjJrw.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">来源:<a class="ae kc" href="https://kindsonthegenius.com/blog/dimensionality-reduction-and-principal-component-analysis-pca/" rel="noopener ugc nofollow" target="_blank">天才博客</a></figcaption></figure><blockquote class="kd ke kf"><p id="b0d5" class="if ig kg ih b ii ij ik il im in io ip kh ir is it ki iv iw ix kj iz ja jb jc hb bi translated">正如我在上面已经讨论过的，当数据集的规模变大时，会发现各种冗余特征，这通常构成了过多的困境，因此为了克服这些问题，数据集的维度被降低，这被称为降维。我们必须知道哪些特征更重要，哪些不太重要。识别重要特征的方法被称为<strong class="ih hj">主成分分析或 PCA。</strong>接下来我们来看看<strong class="ih hj"> PCA 的步骤。</strong></p></blockquote><p id="2b5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PCA 通常由一些重要步骤组成，如下所示:</p><p id="a93f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤 1:缩放数据集:</strong>缩放是一种等同数据集的方法。因此，数据集是相对无偏的[1]。假设一个数据集包含一些人的年龄和收入数据。年龄范围通常可以从 0 到 110 岁，而收入/工资可以高达几十万卢比。所以这两个特征的范围可是大不相同的，把它们放在同一个尺度上比较是不合理的。进行缩放是为了在相同的比例上比较这些不同范围的特征。这是主成分分析的重要一步。</p><p id="1959" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第二步:协方差矩阵:</strong>协方差矩阵来源于数据集各个变量之间的协方差[2]。更简单地说，我们可以通过协方差很容易地理解两个变量之间的关系。例如，两个随机变量 X 和 Y 之间的协方差可以通过应用以下公式(对于总体)来计算:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kk"><img src="../Images/613b053cfa1ebcd810512f3d0ef3061a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vGtCTy5qWY7m3JPqI3O2zg.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图 2:协方差公式。来源:<a class="ae kc" href="https://byjus.com/covariance-formula/" rel="noopener ugc nofollow" target="_blank">https://byjus.com/covariance-formula/</a></figcaption></figure><p id="cd6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第三步:识别特征值和特征向量:</strong>特征向量最简单的例子是变换，但它不改变方向。特征值通常由协方差矩阵确定。包含最多协方差的那些是保存最多信息的那些。</p><p id="2160" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第四步:主成分分析(PCA): </strong>确定特征向量和特征值后，我们将得到的值按降序排列。最大的特征值是最重要的，它形成了第一主成分。以这种方式，使用后续值顺序地形成下一个主分量。</p><p id="38a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第五步:降维:</strong>第一个主成分包含关于数据集最重要的信息，因此其他主成分逐渐包含重要信息。如果我们愿意，我们可以通过消除不太重要或根本没有作用的成分来降低维度[3]。</p><p id="2cf7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们在 Iris 数据集上动手实现主成分分析和降维。</p><p id="6dbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，导入基本库并加载 iris 数据集。</p><pre class="jn jo jp jq fd kl km kn ko aw kp bi"><span id="d820" class="kq kr hi km b fi ks kt l ku kv"><strong class="km hj">from</strong> <strong class="km hj">sklearn</strong> <strong class="km hj">import</strong> datasets<br/><strong class="km hj">import</strong> <strong class="km hj">matplotlib.pyplot</strong> <strong class="km hj">as</strong> <strong class="km hj">plt<br/></strong>import pandas as pd<br/>df = <a class="ae kc" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html#sklearn.datasets.load_iris" rel="noopener ugc nofollow" target="_blank">datasets.load_iris</a>()</span></pre><p id="9323" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设置特性和目标变量</p><pre class="jn jo jp jq fd kl km kn ko aw kp bi"><span id="747c" class="kq kr hi km b fi ks kt l ku kv">features=['sepal_length','sepal_width','petal_length','petal_width']<br/>x=df[features]<br/>y=df.species</span></pre><p id="1b73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我现在要缩放数据集。看看代码。</p><pre class="jn jo jp jq fd kl km kn ko aw kp bi"><span id="b771" class="kq kr hi km b fi ks kt l ku kv">from sklearn.preprocessing import MinMaxScaler</span><span id="078a" class="kq kr hi km b fi kw kt l ku kv">scaler = MinMaxScaler()<br/>scaler.fit(x)<br/>scaled = pd.DataFrame(scaler.transform(x),columns=x.columns)<br/>scaled.head()</span></pre><p id="ee3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">缩放数据后，您将看到如图 3 所示的数据集。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kx"><img src="../Images/8217ce73c42f0115b7fb2f60a3ae0bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vVfBXYTX2D7Vf6I7_xKBCQ.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图 3:缩放数据集</figcaption></figure><p id="21f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们的数据集中有 4 个特征变量，所以这个数据集的维数也将是 4。从这个数据集中，我们最多可以得到 4 个主成分。假设我们想看到这个数据集的所有主要组成部分，</p><pre class="jn jo jp jq fd kl km kn ko aw kp bi"><span id="f94d" class="kq kr hi km b fi ks kt l ku kv">from sklearn.decomposition import PCA<br/>pca = PCA(n_components=4)<br/>principalComponents = pca.fit_transform(scaled)<br/>principalDf = pd.DataFrame(data = principalComponents , columns = ['pc-1', 'pc-2', 'pc-3','pc-4'])<br/>principalDf</span></pre><p id="1535" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在将主成分加入目标变量。</p><pre class="jn jo jp jq fd kl km kn ko aw kp bi"><span id="7006" class="kq kr hi km b fi ks kt l ku kv">finalDf = pd.concat([principalDf, df[['species']]], axis = 1)<br/>finalDf.head()</span></pre><p id="e0ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一次，我们将通过绘图来了解每个组件包含多少信息，或者它在 4 个主要组件中有多重要。</p><pre class="jn jo jp jq fd kl km kn ko aw kp bi"><span id="4a55" class="kq kr hi km b fi ks kt l ku kv">import seaborn as sns<br/>dfexp = pd.DataFrame({'var':pca.explained_variance_ratio_,'PC':['PC1','PC2','PC3','PC4']})<br/>sns.barplot(x='PC',y="var", data=dfexp, color="c");</span></pre><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es ky"><img src="../Images/d469bed5804675ccf7c77bc767911e25.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*aAZuK0um56_OpXbkFe8RgQ.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图 4:每个组件的信息</figcaption></figure><p id="62e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从图 4 中，我们很容易理解，第一个两个分量比第三个和第四个分量重要。现在，如果我们用两个主要的主成分绘图，那么很容易理解这两个主成分如何解释数据集。让我们看看下面的代码:</p><pre class="jn jo jp jq fd kl km kn ko aw kp bi"><span id="c376" class="kq kr hi km b fi ks kt l ku kv">import matplotlib.pyplot as plt<br/>sns.lmplot( x="pc-1", y="pc-2",data=finalDf, <br/>fit_reg=False,  hue='species', legend=True,scatter_kws={"s": 80})</span></pre><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kz"><img src="../Images/10f8084bf4273913ce778e75a3ce7918.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*WJizrgL2AaSSYuvgoFXTrQ.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图 5:每个组件的信息</figcaption></figure><p id="94a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">完整代码可以在我的知识库找到:<a class="ae kc" href="https://github.com/eliashossain001/Boosting-Algorithm-/blob/master/Principle_Component_Analysis_Dimension_Reduction.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/eliashossain 001/Boosting-Algorithm-/blob/master/principal _ Component _ Analysis _ Dimension _ reduction . ipynb</a></p><p id="57b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ol class=""><li id="ada0" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lf lg lh li bi translated"><a class="ae kc" href="https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2020/04/feature-scaling-machine-learning-normalization-标准化/ </a></li><li id="d5a6" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><a class="ae kc" href="https://corporatefinanceinstitute.com/resources/knowledge/finance/covariance/" rel="noopener ugc nofollow" target="_blank">https://corporate finance institute . com/resources/knowledge/finance/协方差/ </a></li><li id="e817" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><a class="ae kc" href="https://www.sciencedirect.com/topics/computer-science/dimensionality-reduction" rel="noopener ugc nofollow" target="_blank">https://www . science direct . com/topics/computer-science/dimensionally-reduction</a></li></ol></div></div>    
</body>
</html>