<html>
<head>
<title>Scraping all COVID-19 Symptoms tweets using Python.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python抓取所有新冠肺炎症状推文。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/scraping-all-covid-19-symptoms-tweets-using-python-cce4eaeb369e?source=collection_archive---------13-----------------------#2020-04-10">https://medium.com/analytics-vidhya/scraping-all-covid-19-symptoms-tweets-using-python-cce4eaeb369e?source=collection_archive---------13-----------------------#2020-04-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="2979" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph"><a class="ae ge" href="https://towardsdatascience.com/in-depth-analysis/home" rel="noopener" target="_blank">深入分析</a></h2><div class=""/><div class=""><h2 id="c61e" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated"><strong class="ak">大多数新冠肺炎患者一直在twitter上写他们的康复之路，因此我们在这里详细介绍如何收集所有信息并将其存储为csv文件，以供决策和进一步分析</strong></h2></div><p id="0c39" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">Twitter是一座关于人们情绪的数据金矿。与其他社交媒体数据点和发布方式相比，在twitter上获取的信息更加结构化。Twitter也提供了一种检索这些信息的简单方法。</p><p id="ddd4" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">本文主要解释了如何使用Dmitry Mottl的GetOldTweets3在Python中快速简单地从Twitter中抓取所有与COVID症状相关的推文。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/6774d6c94f82ca78a65392dfd09ac56b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJrBreAakeKLxlKFrbplqQ.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">来源:推特</figcaption></figure><p id="aad5" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">如果你想直接跳到代码，你可以在我的GitHub <a class="ae ks" href="https://github.com/cheruiyot/Python_scrapetweets/blob/master/Get_Old_tweets_final.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>访问这个教程的Jupyter笔记本。该代码是项目描述的最终支出</p><p id="05c7" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">GetOldTweets3由Dmitry Mottl创建，是Jefferson Henrqiue的GetOldTweets-python的改进分支。这个包允许你检索大量的推文和一周前的推文。<br/>下图显示了可以检索到的与tweet相关的信息。</p><h1 id="57b9" class="kt ku hi bd kv kw kx ky kz la lb lc ld ix le iy lf ja lg jb lh jd li je lj lk bi translated">先决条件</h1><p id="3cd9" class="pw-post-body-paragraph jg jh hi ji b jj ll is jl jm lm iv jo jp ln jr js jt lo jv jw jx lp jz ka kb hb bi translated">使用GetOldTweets3不需要twitter的授权，但是你只需要pip安装这个库，然后你就可以马上开始了。您还需要pandas或pyspark来操作数据，这将在后面讨论。</p><p id="3d9f" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">为此，导入以下代码行:</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="c26b" class="lv ku hi lr b fi lw lx l ly lz"># Pip install GetOldTweets3 if you don’t already have the package<br/># !pip install GetOldTweets3</span><span id="cf00" class="lv ku hi lr b fi ma lx l ly lz"># Imports<br/>import GetOldTweets3 as got<br/>import pandas as pd</span><span id="5d6e" class="lv ku hi lr b fi ma lx l ly lz">import findspark<br/>findspark.init()<br/>findspark.find()<br/>import pyspark<br/>findspark.find()</span><span id="b9b0" class="lv ku hi lr b fi ma lx l ly lz">#alwaysimport this for every pyspark analytics</span><span id="7221" class="lv ku hi lr b fi ma lx l ly lz">from pyspark import SparkContext, SparkConf<br/>from pyspark.sql import SparkSession</span><span id="449c" class="lv ku hi lr b fi ma lx l ly lz">conf = pyspark.SparkConf().setAppName(‘appName’).setMaster(‘local’)<br/>#sc = pyspark.SparkContext(conf=conf)<br/>sc = SparkContext.getOrCreate(conf=conf)<br/>spark = SparkSession(sc)</span></pre><h1 id="590d" class="kt ku hi bd kv kw kx ky kz la lb lc ld ix le iy lf ja lg jb lh jd li je lj lk bi translated">从文本搜索查询中抓取推文</h1><p id="67e0" class="pw-post-body-paragraph jg jh hi ji b jj ll is jl jm lm iv jo jp ln jr js jt lo jv jw jx lp jz ka kb hb bi translated">为了查询所有的COVID症状，我主要关注三个项目:查询词、推文数量和位置</p><p id="45d8" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">有了以上三个变量，我能够确保检索到主要热门城市(伦敦、纽约和巴黎)的新冠肺炎症状。</p><p id="9deb" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">以下查询创建了一个包含巴黎附近所有Tweets的文本文件，并包含单词COVID症状:</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="77c9" class="lv ku hi lr b fi lw lx l ly lz">text_query = ‘COVID symptoms’<br/>count = 7000<br/>geocode=”Paris”</span><span id="e0ef" class="lv ku hi lr b fi ma lx l ly lz"># Creation of query object<br/>tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setMaxTweets(count).setNear(geocode)</span><span id="fe34" class="lv ku hi lr b fi ma lx l ly lz"># Creation of list that contains all tweets<br/>tweets = got.manager.TweetManager.getTweets(tweetCriteria)</span><span id="9ec9" class="lv ku hi lr b fi ma lx l ly lz"># Creating list of chosen tweet data<br/> text_tweets = [[tweet.date, tweet.text,tweet.id,tweet.username,tweet.geo] for tweet in tweets]</span></pre><h1 id="a7b9" class="kt ku hi bd kv kw kx ky kz la lb lc ld ix le iy lf ja lg jb lh jd li je lj lk bi translated">进一步操作tweets文件</h1><p id="837e" class="pw-post-body-paragraph jg jh hi ji b jj ll is jl jm lm iv jo jp ln jr js jt lo jv jw jx lp jz ka kb hb bi translated">为了在python dataframe中存储我们的数据，我们使用下面的代码片段:</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="c96c" class="lv ku hi lr b fi lw lx l ly lz">tweets_df = pd.DataFrame(text_tweets, columns = ['Datetime', 'Text','TweetID','username','geo'])</span></pre><p id="eb80" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">如果你是熊猫专家，你可以直接将文件写入csv。<br/>对我来说，我喜欢使用pyspark，以便利用pyspark的处理功能。<br/>我将上面的熊猫数据帧转换成pyspark，然后写入csv文件，如下所示:</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="796e" class="lv ku hi lr b fi lw lx l ly lz">tweets_df_spark.coalesce(1).write.save(path='C:\\Users\\brono\\First_batch\\Finalextract2.csv', format='csv', mode='append', inferSchema = True)</span></pre><p id="0601" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">请注意，我将我的推文附加到现有的，以便运行我的推文，并附加到一个已经存在的文件。</p><h1 id="ca44" class="kt ku hi bd kv kw kx ky kz la lb lc ld ix le iy lf ja lg jb lh jd li je lj lk bi translated">删除重复的已清理推文文件</h1><p id="f293" class="pw-post-body-paragraph jg jh hi ji b jj ll is jl jm lm iv jo jp ln jr js jt lo jv jw jx lp jz ka kb hb bi translated">为了把这些放在一起，我添加了下面几行代码来删除重复抓取了两次的tweets。<br/>我还添加了最后两行代码来读取追加文件，并在写入新文件夹之前删除重复的文件。<br/>请注意，每次运行代码时都会覆盖该文件中的数据</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="c62f" class="lv ku hi lr b fi lw lx l ly lz">Finaldf = spark.read.csv("C:\\Users\\brono\\First_batch\\Finalextract2.csv", inferSchema = True, header = True)<br/>Finaldf = Finaldf.dropDuplicates(subset=['TweetID'])<br/>    <br/>Finaldf.sort("TweetID").coalesce(1).write.mode("overwrite").option("header", "true").csv("C:\\Users\\brono\\First_batch\\Cleaned_data.csv")</span></pre><h1 id="99a9" class="kt ku hi bd kv kw kx ky kz la lb lc ld ix le iy lf ja lg jb lh jd li je lj lk bi translated"><strong class="ak">将代码放在一起</strong></h1><p id="c2e6" class="pw-post-body-paragraph jg jh hi ji b jj ll is jl jm lm iv jo jp ln jr js jt lo jv jw jx lp jz ka kb hb bi translated">我将上述内容编译成一个函数，如下所示:</p><pre class="kd ke kf kg fd lq lr ls lt aw lu bi"><span id="394d" class="lv ku hi lr b fi lw lx l ly lz">def text_query_to_csv(text_query, count):<br/>    # Creation of query object<br/>    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query).setMaxTweets(count).setNear(geocode)<br/>    #.setSince(newest_date1).setUntil(newest_date1)<br/>    # Creation of list that contains all tweets<br/>    tweets = got.manager.TweetManager.getTweets(tweetCriteria)</span><span id="e75c" class="lv ku hi lr b fi ma lx l ly lz"># Creating list of chosen tweet data<br/>    text_tweets = [[tweet.date, tweet.text,tweet.id,tweet.username,tweet.geo] for tweet in tweets]</span><span id="7edd" class="lv ku hi lr b fi ma lx l ly lz"># Creation of dataframe from tweets<br/>    tweets_df = pd.DataFrame(text_tweets, columns = ['Datetime', 'Text','TweetID','username','geo'])<br/>    <br/>    <br/>    # Createspark spark dataframe<br/>    tweets_df_spark = spark.createDataFrame(tweets_df)</span><span id="59d0" class="lv ku hi lr b fi ma lx l ly lz"># Converting tweets dataframe to csv file<br/>   <br/>    tweets_df_spark.coalesce(1).write.save(path='C:\\Users\\brono\\First_batch\\Finalextract2.csv', format='csv', mode='append', inferSchema = True)<br/>    # Read the appended file and remove duplicates and write the clean as one file<br/>    Finaldf = spark.read.csv("C:\\Users\\brono\\First_batch\\Finalextract2.csv", inferSchema = True, header = True)<br/>    Finaldf = Finaldf.dropDuplicates(subset=['TweetID'])<br/>    <br/>    Finaldf.sort("TweetID").coalesce(1).write.mode("overwrite").option("header", "true").csv("C:\\Users\\brono\\First_batch\\Cleaned_data.csv")</span></pre><p id="3a22" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">最终代码发布在Github<a class="ae ks" href="https://gist.github.com/cheruiyot/369e5d99489ef55558ce1d5df2087c64" rel="noopener ugc nofollow" target="_blank">https://gist . Github . com/cheruiyot/369 e5d 99489 ef 55558 ce 1 D5 df 2087 c 64</a></p><p id="9459" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">要访问已经收集的9000多条推文，你可以联系</p></div></div>    
</body>
</html>