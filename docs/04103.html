<html>
<head>
<title>Extracting keywords from COVID-19 news with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从新冠肺炎新闻中提取关键词</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/extracting-keywords-from-covid-19-news-with-python-13249571d37b?source=collection_archive---------1-----------------------#2020-03-05">https://medium.com/analytics-vidhya/extracting-keywords-from-covid-19-news-with-python-13249571d37b?source=collection_archive---------1-----------------------#2020-03-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="d40f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用Python和NLP技术，我们能够提取由冠状病毒(新冠肺炎)引起的新传染病的超过100，000篇文章和出版物的关键词。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b2410954f13d74060771334a11602d74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TeTvbwj1AxGUlqbP9SAbvw.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">使用Python从大量文本中提取关键词</figcaption></figure></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><p id="43ed" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">新型冠状病毒——或现在称为新冠肺炎——作为一种新的传染病正在影响整个世界。从不同来源的几条新闻中，我们能够应用一些NLP技术和框架。</p><p id="69b3" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在本文中，我们将提取每篇文章的<em class="kq">关键词</em>，并创建一个数据集，以使用一个应用了词性标注概念的函数来识别<em class="kq">关键词</em>。</p><p id="6b33" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">我们将使用spaCy和News API，这是一个很好的数据源，可以从网络上搜索和检索实时文章。本文将有几个部分:</p><ul class=""><li id="9014" class="kr ks hi jw b jx jy ka kb kd kt kh ku kl kv kp kw kx ky kz bi translated">设置</li><li id="03d8" class="kr ks hi jw b jx la ka lb kd lc kh ld kl le kp kw kx ky kz bi translated">编码</li><li id="69f3" class="kr ks hi jw b jx la ka lb kd lc kh ld kl le kp kw kx ky kz bi translated">结论</li><li id="1cfb" class="kr ks hi jw b jx la ka lb kd lc kh ld kl le kp kw kx ky kz bi translated">未来作品</li></ul><p id="7668" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">我们走吧！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lf"><img src="../Images/e56fb883a5e3105e646671564fa9efc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/0*R0juCegH0ywti4gI.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来做点编码吧！</figcaption></figure></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><h2 id="783c" class="lg lh hi bd li lj lk ll lm ln lo lp lq kd lr ls lt kh lu lv lw kl lx ly lz ma bi translated">设置— Google Colab</h2><p id="da3c" class="pw-post-body-paragraph ju jv hi jw b jx mb ij jz ka mc im kc kd md kf kg kh me kj kk kl mf kn ko kp hb bi translated">首先让我们了解一下本文中将要用到的所有东西。我首先在Google Colab上创建了这个笔记本——但是这可以用另一个IDE或Python笔记本来实现。我们将使用pip安装spaCy，只要我会说巴西葡萄牙语，我就会使用英语语言模型。</p><p id="8c39" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">新闻API也有一个Python库，我们可以用pip安装。</p><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="98cb" class="lg lh hi mh b fi ml mm l mn mo">!pip install spacy<br/>!pip install newsapi-python</span></pre><div class="mp mq ez fb mr ms"><a href="https://spacy.io/usage" rel="noopener  ugc nofollow" target="_blank"><div class="mt ab dw"><div class="mu ab mv cl cj mw"><h2 class="bd hj fi z dy mx ea eb my ed ef hh bi translated">安装空间空间使用文档</h2><div class="mz l"><h3 class="bd b fi z dy mx ea eb my ed ef dx translated">使用pip，spaCy发行版可以作为源包和二进制轮获得(从v2.0.13开始)。将模型下载到…</h3></div><div class="na l"><p class="bd b fp z dy mx ea eb my ed ef dx translated">空间. io</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng jh ms"/></div></div></a></div><p id="9a6a" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">之后，我们可以下载最大尺寸的spaCy英语语言模型。这个型号的文件大小约为800MB，你可以下载其他版本的中小版本。我们将使用下面的代码:</p><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="118d" class="lg lh hi mh b fi ml mm l mn mo">!python -m spacy download en_core_web_lg</span></pre><p id="3846" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">安装后，您需要导入空间库，我们将使用另一个库来帮助实现NLP分析。我们还必须导入空间模型，并通过一个我们称之为<em class="kq"> nlp_eng </em>的变量来加载它。</p><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="d4c7" class="lg lh hi mh b fi ml mm l mn mo">nlp_eng = en_core_web_lg.load()<br/>newsapi = NewsApiClient (api_key='PLACE_HERE_YOUR_API_KEY')</span></pre><p id="08c5" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这完成了我们的设置，之后我们可以做一些编码。</p></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><h2 id="108e" class="lg lh hi bd li lj lk ll lm ln lo lp lq kd lr ls lt kh lu lv lw kl lx ly lz ma bi translated">编码</h2><p id="0289" class="pw-post-body-paragraph ju jv hi jw b jx mb ij jz ka mc im kc kd md kf kg kh me kj kk kl mf kn ko kp hb bi translated">新闻API提供了一种简单的方法来研究大量不同来源的文章和出版物，我们可以免费获得API_KEY。当我们发送一个HTTP请求时，API最多返回100篇文章——您必须向dev帐户付费才能获得结果总数。</p><div class="mp mq ez fb mr ms"><a href="https://newsapi.org/docs" rel="noopener  ugc nofollow" target="_blank"><div class="mt ab dw"><div class="mu ab mv cl cj mw"><h2 class="bd hj fi z dy mx ea eb my ed ef hh bi translated">文档—新闻API</h2><div class="mz l"><h3 class="bd b fi z dy mx ea eb my ed ef dx translated">News API是一个简单的HTTP REST API，用于搜索和检索网络上的实时文章。它可以帮助你…</h3></div><div class="na l"><p class="bd b fp z dy mx ea eb my ed ef dx translated">newsapi.org</p></div></div></div></a></div><p id="2e98" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这些文章分成几页，每页有20篇文章。您必须实现某种类型的分页，才能免费获得总共100篇文章。我们对过去的最大搜索日期也有限制，即30天。</p><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="946c" class="lg lh hi mh b fi ml mm l mn mo">temp = newsapi.get_everything(q='coronavirus', language='en', from_param='2020-02-03', to='2020-03-03', sort_by='relevancy', page=pagina)</span></pre></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><p id="a829" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">当我在Google Colab上使用Python笔记本时，需要将数据集保存在我的Google Drive中，以便在发生任何糟糕的事情时维护数据。我用泡菜库保存了所有的文章。我们可以创建并保存。<em class="kq"> pckl </em>文件使用这些命令:</p><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="5034" class="lg lh hi mh b fi ml mm l mn mo">filename = 'articlesCOVID.pckl'<br/>pickle.dump(articles, open(filename, 'wb'))</span><span id="dc15" class="lg lh hi mh b fi nh mm l mn mo">filename = 'articlesCOVID.pckl'<br/>loaded_model = pickle.load(open(filename, 'rb'))</span><span id="b421" class="lg lh hi mh b fi nh mm l mn mo">filepath = '/content/path/to/file/articlesCOVID.pckl'<br/>pickle.dump(loaded_model, open(filepath, 'wb'))</span></pre></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><p id="317f" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">之后，我们将处理数据并转换Pandas数据框架中的文章字典。我们从新闻API获得的原始JSON非常丰富，我们可以对其进行清理，只使用<em class="kq">标题</em>、<em class="kq">日期</em>、<em class="kq">描述</em>和<em class="kq">内容</em>。</p><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="7b5e" class="lg lh hi mh b fi ml mm l mn mo">for i, article in enumerate(articles):<br/>    for x in article['articles']:<br/>        title = x['title']<br/>        description = x['description']<br/>        content = x['content']<br/>        dados.append({'title':titles[0], 'date':dates[0], 'desc':descriptions[0], 'content':content})</span><span id="ebec" class="lg lh hi mh b fi nh mm l mn mo">df = pd.DataFrame(dados)<br/>df = df.dropna()<br/>df.head()</span></pre><p id="4e8e" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在我们清理数据并创建一个Pandas DataFrame之后，我们将创建一个名为<strong class="jw hj"><em class="kq">get _ keywords _ eng</em></strong>的函数，该函数将接收<strong class="jw hj"> <em class="kq">文本</em> </strong>，并使用spaCy模型来识别与新闻的<em class="kq">关键字</em>相匹配的词性标注。</p><blockquote class="ni nj nk"><p id="1f67" class="ju jv kq jw b jx jy ij jz ka kb im kc nl ke kf kg nm ki kj kk nn km kn ko kp hb bi translated">词性标注是基于在文本或语料库中标记单词的概念，因为它对应于特定的定义及其上下文。还考虑与短语、句子或段落中的其他相关单词的关系来识别正确的标签。</p></blockquote><p id="060a" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">我们将提取出<em class="kq">动词</em>(动词)、一个<em class="kq">名词</em>(名词)和<em class="kq">专有名词</em> (PROPN)。</p><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="33a8" class="lg lh hi mh b fi ml mm l mn mo">if (token.text in nlp_eng.Defaults.stop_words or token.text in punctuation):<br/>  continue<br/>if (token.pos_ in pos_tag):<br/>  result.append(token.text)</span><span id="56c6" class="lg lh hi mh b fi nh mm l mn mo">return result</span></pre><p id="d5cc" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">创建好函数后，是时候将它应用到我们的数据帧中了。我们可以在DataFrame中添加一个名为<em class="kq"> keywords </em>的新列来接收函数的结果。</p><p id="f30d" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">我已经用文章的<em class="kq">标题</em>、<em class="kq">描述</em>和<em class="kq">内容</em>进行了测试。我得到的最好的结果是文章的<em class="kq">内容</em>，因为它的字数最多。</p><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="e853" class="lg lh hi mh b fi ml mm l mn mo">for content in df.content.values:<br/>    results.append([('#' + x[0]) for x in Counter(get_keywords_eng(content)).most_common(5)])</span><span id="7740" class="lg lh hi mh b fi nh mm l mn mo">df['keywords'] = results</span></pre><p id="d5b0" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">之后，我们可以将这些新数据插入到我们的数据帧中，并再次保存到我们的。<em class="kq"> pckl </em>文件。</p></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><h2 id="6003" class="lg lh hi bd li lj lk ll lm ln lo lp lq kd lr ls lt kh lu lv lw kl lx ly lz ma bi translated">结论</h2><p id="552f" class="pw-post-body-paragraph ju jv hi jw b jx mb ij jz ka mc im kc kd md kf kg kh me kj kk kl mf kn ko kp hb bi translated">现在我们有了一个数据集，其中包含了每篇文章中最常见的5个关键词，并与另外几篇关于新冠肺炎的文章连接在一起，是时候选择展示我们结果的最佳方式了。我选择了文字云，这是一种根据文字出现的频率显示文字的图片。</p><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="4216" class="lg lh hi mh b fi ml mm l mn mo">text = str(results)<br/>wordcloud = WordCloud(max_font_size=50, max_words=100, background_color="white").generate(text)<br/>plt.figure()<br/>plt.imshow(wordcloud, interpolation="bilinear")<br/>plt.axis("off")<br/>plt.show()</span></pre><p id="f0e4" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在我们的图片下方:</p></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><h2 id="a8f0" class="lg lh hi bd li lj lk ll lm ln lo lp lq kd lr ls lt kh lu lv lw kl lx ly lz ma bi translated">未来的工作</h2><p id="5392" class="pw-post-body-paragraph ju jv hi jw b jx mb ij jz ka mc im kc kd md kf kg kh me kj kk kl mf kn ko kp hb bi translated">当然，这项工作可以不断发展，得到更多的应用，而不仅仅是创建一个单词云。</p><p id="ef3c" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">您可以应用DataFrame的其他部分，或者创建另一种方法来连接多个字段。也许从推特或脸书获取数据会很棒。</p><p id="ae24" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">我的一项调查显示，我们有足够的数据集来训练新冠肺炎新闻的语言模型，并通过这些新闻获得一些问题的答案。我现在正在和<a class="ae no" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">伯特</a>和<a class="ae no" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">奥博奈·GPT-2</a>一起尝试——很快会有更多的结果！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es np"><img src="../Images/50be8e8a5801070810c45c825ceaabda.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*sckm-D_0AGzfd-39.gif"/></div></figure><p id="0022" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">感谢阅读，并随时评论和分享！</p><p id="c259" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在<a class="ae no" href="https://www.linkedin.com/in/gilvandroneto1991/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我保持联系。:)</p></div></div>    
</body>
</html>