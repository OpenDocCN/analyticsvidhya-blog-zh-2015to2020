<html>
<head>
<title>The rise of Attention in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中注意力的上升</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-rise-of-attention-in-neural-networks-8c1d57a7b188?source=collection_archive---------13-----------------------#2020-06-02">https://medium.com/analytics-vidhya/the-rise-of-attention-in-neural-networks-8c1d57a7b188?source=collection_archive---------13-----------------------#2020-06-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b276" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可能已经注意到，在过去的几年里，自然语言处理领域发生了一些非常特别的事情。首先，谷歌翻译达到了前所未有的翻译质量。你可能还会碰到<a class="ae jd" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">一些人工智能写的故事</a>关于在安第斯山脉发现的独角兽。</p><p id="4e46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">发生了什么事？<strong class="ih hj">注意力机制</strong>开始发挥作用了！</p><p id="d745" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于注意力的架构目前是最先进的NLP模型的基础。他们的主要想法是模仿人类在执行分类或预测任务时有选择地关注句子的某些部分的能力。这篇文章将讨论注意力机制的起源，它们的基本原理和NLP中最重要的基于注意力的模型(Transformer，BERT和gpt-2)。</p><p id="63a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一篇<strong class="ih hj">介绍性文章</strong>，旨在帮助那些第一次接触该主题并希望在深入研究科学文章或高级Tensorflow教程之前掌握主要观点的人。一些解释被有意简化了一点，以方便直觉。</p><h2 id="c1e3" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">注意力的起源</h2><p id="c876" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">为了解释注意力是如何诞生的，我们先从讨论一个相关的、更广为人知的概念开始:<strong class="ih hj">记忆</strong>。</p><p id="27a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">记忆的概念最初是在深度学习中引入的，以实现对<strong class="ih hj">顺序数据</strong>的建模——即，对于所有那些数据具有内在顺序并且在处理新人时有必要以某种方式记住过去的情况。这包括自然语言和时间序列。</p><p id="ad49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于记忆思想的最著名的网络是<strong class="ih hj">循环神经网络</strong>，它已经存在了几十年。以下是用于文本分类任务的典型递归神经网络架构-在这种情况下，将报纸文章标题(例如，“尤文图斯前锋罗纳尔多用跳跃挑战地心引力”)分类为足球、经济或时尚三个主题之一:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ke"><img src="../Images/f575fb88ebc394a0aad0aef98dfdbb02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ez5EckKEbrXaEnHuSjlLOg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">用于文本分类的递归神经网络</figcaption></figure><p id="37cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图中显示的绿色单元格实际上就是多次表示的<strong class="ih hj">同一个单元格(参数相同)</strong>。这被称为“展开的”网络表示，旨在促进对底层过程的可视化和理解。序列元素(在本例中是单词)由单元按顺序一个接一个地处理。该单元有自己的内部存储器，由向量<em class="ku"> h </em> ᵢ(也称为<strong class="ih hj">隐藏状态</strong>)编码，其元素初始化为0，然后在每次处理新字时更新。</p><p id="b5d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个展开的模式中，所有中间内存更新都被显式表示:<em class="ku">h</em>ᵢ<em class="ku">T3】是直到单词I的内存，因此，<em class="ku"> h </em> ₙ是整个<strong class="ih hj">序列内存</strong>，或者换句话说，是完整句子的压缩<strong class="ih hj">合成表示</strong>。</em></p><p id="68ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果单元机制被适当地设计和优化，如在LSTM和GRU单元的情况下，记忆向量将被有效地更新到<strong class="ih hj"> <em class="ku">记住</em> </strong>输入序列的相关部分，而<strong class="ih hj"> <em class="ku">忘记</em> </strong>不相关的部分。在下面的例子中，记忆会清晰地记住尤文图斯和c罗，但忘记跳跃和重力。</p><p id="920a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相关当然是相对于网络被训练做什么而言的。如果目标是其他一些奇怪的任务，比如预测所描述的动作中涉及的身体部位(腿、胳膊、头、腹部……)，那么记忆就会集中在“跳”这个词上。</p><p id="5959" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">传统上，ₙ被认为是唯一有尊严的人。中间存储器将被丢弃，并且只有最后一个存储器将被传递到后续层以执行预测或分类任务。</p><h2 id="bc4f" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">瓶颈:翻译</h2><p id="c33f" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">直到大约2015年，递归神经网络也是大多数NLP任务的最先进技术，包括翻译。</p><p id="a2ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该架构类似于主题分类的架构。同样，输入的句子(要翻译的句子)将通过网络，一次一个元素，并在最终的存储器/序列表示中编码<em class="ku"> h </em> ₙ <em class="ku">。</em>事实上，这个网络被称为<strong class="ih hj">编码器</strong>。</p><p id="890e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，ₙ <em class="ku"> </em>将被传递给另一个复杂的结构，即<strong class="ih hj">解码器</strong>，它将一次生成一个单词的翻译。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es kv"><img src="../Images/2b98e20d691295490861eacb0250b704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gYCKumH0nHKkxJicrYpgyA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">ₙ必须记住很多事情:目的地、旅行时间、旅行者的名字……</figcaption></figure><p id="0835" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这次不讨论解码器架构的细节(顺便说一下，它只是另一个递归神经网络)。重要的一点是<strong class="ih hj">解码者所能看到的关于初始句子的所有信息就是这个单一的合成表示<em class="ku"> h </em> ₙ </strong>。</p><p id="ab89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，这使得翻译对解码者来说相当困难，尤其是当文本开始变长的时候。例如，在生成翻译时，解码器没有办法动态地关注原始句子中的特定部分——这是人类会做的，看起来是一种更明智和有效的方法。</p><p id="616f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而在2015年，有人有了一个想法:“为什么不把所有的中间存储器都传给解码器？”。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es kx"><img src="../Images/861389d2e2fb58c22c8fc3de444f0a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pdEJyMlo9G0Vu4zuxspjCA.png"/></div></div></figure><p id="45cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一种非常直接的方式，允许解码器在生成翻译的特定部分时专注于输入的特定部分:这正是我们一直在寻找的。这是神经网络中<strong class="ih hj">第一种形式的注意力:只不过是在递归神经网络上添加了一个聪明的技巧，以帮助它完成翻译任务。</strong></p><p id="3ab8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个新架构的详细描述和Tensorflow教程可以在<a class="ae jd" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="bd31" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">语境化的词汇表征</h2><p id="2045" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">这种新结构之所以有效，是因为<em class="ku">h</em>ᵢ<em class="ku">T7】有效地充当了<strong class="ih hj">语境化的单词表征</strong>。事实上，由于它们在这种架构中的构造方式，我们可以预期,<em class="ku"> h </em> ᵢ将主要编码(或记住)单词I加上一些关于之前发生的事情——即，加上一些<em class="ku">上下文</em>。</em></p><p id="b628" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要了解这一点，首先考虑网络的低层会发生什么。单词在被传递给递归单元之前必须被转换成向量(当然，该单元不能处理裸露的单词！).这种转换是通过一个简单的<strong class="ih hj">嵌入层</strong>来执行的，它将每个单词(或者更好地说，它的一键编码)分别映射到一个连续的向量。在训练期间学习转换的参数。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ky"><img src="../Images/e6ecb84e57425ae33de56a590f7862b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6BsoZHfBiVthedMJDIEjMg.png"/></div></div></figure><p id="81a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于被单独映射，单词在此阶段看不到彼此。这意味着它们的初始表示(下图中示意为深绿色矩形)是<strong class="ih hj">上下文无关的</strong>，就像<a class="ae jd" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> word2vec </a>或其他类似的嵌入一样。</p><p id="6f02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，本质上，递归神经网络完成的是<strong class="ih hj">将这些初始的上下文无关的单词表示组合成上下文感知的单词表示</strong>:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es kz"><img src="../Images/496020509403f5fbb38e08330bc3b8bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sCa_kHGoDWijt3VhaM_QoQ.png"/></div></div></figure><p id="03cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对上下文感知表示的搜索确实是机器翻译中的一个关键点。例如，在这两个句子中</p><p id="3ff2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一条河岸</p><p id="b9eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一家商业银行</p><p id="646f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你只能通过看前面的单词来消除“bank”的歧义。使用递归结构，在每一步后面查看，<strong class="ih hj">“bank”的向量表示可以根据句子</strong>进行有效调整。这样，解码器接收到“bank”的无歧义表示，并且有助于将其导向适当的翻译。</p><p id="1112" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是当我们需要向前看来消除一个单词的歧义时，又该怎么办呢？</p><p id="f6d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">穿过 <strong class="ih hj"> <em class="ku">河</em> </strong>后，我来到了岸边</p><p id="0447" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ku">穿过</em> <strong class="ih hj"> <em class="ku">路</em> </strong>到达银行</p><p id="a612" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以只使用双向网络，一切正常。通过连接来自两个网络的隐藏状态来获得最终的单词表示:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es la"><img src="../Images/a2b21fb729cd6fb8cb2340bb026e8a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ThNGZT_mrEnvybukSxCqiQ.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">使用双向RNN，可以通过从两个方向(后面和前面)查看其他单词来消除单词的歧义。</figcaption></figure><p id="f658" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果单个层不足以获得令人满意的表示，我们可以通过将多个递归层相互堆叠来构建更复杂的架构:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es lb"><img src="../Images/aa7aa98d4c4d294e690e603f6e2d76b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lCDeQggDq6jbPFEI61EZvA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">具有两个堆叠层的双向RNN。第一层的隐藏状态作为输入传递给第二层。</figcaption></figure><h2 id="5191" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">循环架构的局限性</h2><p id="58bd" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">注意力机制的加入提高了循环神经网络的翻译能力。然而，递归架构在有效地将相关单词和<strong class="ih hj">连接起来并消除</strong>的能力方面仍然存在固有的局限性。如果相关的单词彼此相距很多位置，就像这种情况一样，会怎么样呢？</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es lc"><img src="../Images/0d662f2200ba8261a9c1608f7cad9421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sw4rSZrzXlapxIZdhYFS7w.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">如果两个相连的单词相距很远，RNN人将很难记住和编码这种关系。如果存在直接连接就好了…</figcaption></figure><p id="6af6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">即使使用双向网络来捕获双方的上下文，网络内存仍然很难记住“它”确实连接到了“博物馆”。在这两个词之间打开一个直接连接肯定会更有效率，但是如何在不放弃顺序架构的情况下做到这一点呢？</p><p id="8d68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实际上，放弃<strong class="ih hj">顺序架构</strong>在这里是可取的。被迫按顺序处理序列意味着<strong class="ih hj">缺乏并行性</strong>，这大大增加了训练时间，因此对可用于训练的文档数量造成了严重限制。但是大多数NLP模型需要大量的训练文档来学习语言的所有微妙之处，所以这是一个关键点。</p><p id="d18f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">顺序架构是一个负担，但似乎仍然是必要的:否则如何考虑句子中单词的顺序？当然，顺序很重要…</p><p id="3865" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">狮子追赶斑马并吃掉了它</p><p id="743e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ku">斑马追着狮子吃了它</em></p><p id="683d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">顺序不可知(或词袋)模型有时适用于文本分类或情感分析，但不适用于翻译任务！</p><h2 id="1ece" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">位置编码</h2><p id="8bf0" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">最后，有人提出了使用位置编码(或位置嵌入)的想法。关键的事实是，关于一个单词在句子中的位置的信息可以有效地编码在一个矢量中——更重要的是，一个低维的矢量。</p><p id="2ab6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人们可能本能地认为10000个分量的向量对于编码位置1到10000是必要的。实际上，如果我们使用下面的公式，需要少得多:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ld"><img src="../Images/8b348b3b55e35fd5d8f1b451d5ecf938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQeDfkLkS6U1NdZjOThgmg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">计算矢量编码位置I的第j个分量的公式</figcaption></figure><p id="fb47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参见<a class="ae jd" href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/" rel="noopener ugc nofollow" target="_blank">此链接</a>了解其工作原理的直观解释。在下图中，行对应于位置1到10000的渐进式位置编码，并通过热图表示。向量只有512个分量:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es le"><img src="../Images/66072ef2395c87fdf2d24cbae8a609c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5jhCwag09oSEKoM1DB8aig.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">显示位置1到10000的512个组件的位置编码(每行一个)的热图</figcaption></figure><p id="5286" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">放大到前10个位置，我们可以了解编码是如何随着位置的增加而逐渐变化的:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es lf"><img src="../Images/37c74784a45a9e0670915e97722f95fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ssK4XJpioUwdGlQrUIFy3w.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">同上，放大位置1到10</figcaption></figure><p id="9cb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么这些位置编码如此混乱？例如，通过将它们连接到初始的(上下文无关的)单词嵌入，我们可以摆脱顺序架构，并开放所有可能的连接。结果是一个全新的架构，循环细胞被一个<strong class="ih hj">注意力层</strong>所取代:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es lg"><img src="../Images/a35accac023ddd84e5bf3039d14c8d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKI9zg2KgGBMIuJ32rSagA.png"/></div></div></figure><p id="19bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实际上，甚至没有必要串联。将编码与初始单词嵌入相加就足够了(当然，两者必须具有相同的维数)。关于这一点的讨论见<a class="ae jd" href="https://github.com/tensorflow/tensor2tensor/issues/1591" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="cd73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在新的架构中，每个单词在构建自己的上下文感知表示时都会查看所有其他单词，并且每对单词之间都有<strong class="ih hj">直接链接</strong>。而且，培训是高度<strong class="ih hj">并行化的</strong>。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es lh"><img src="../Images/6b61a683205ed82cde8fd8ec9513026b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RP6NNpr160hMmyhGNanJpw.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">关注的RNN与只关注网络:全球架构比较。这是一个单层的简化表示；实际上，在这两种体系结构中，多个层相互堆叠。</figcaption></figure><p id="55ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与RNN一样，<strong class="ih hj">多个关注层</strong>通常堆叠在彼此之上，每个层的输出表示作为输入被传递到下一层。</p><h1 id="7b66" class="li jf hi bd jg lj lk ll jk lm ln lo jo lp lq lr jr ls lt lu ju lv lw lx jx ly bi translated">注意力层:它是如何工作的？</h1><p id="1271" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">由于这一创新，注意力不再只是RNNs的一个附加功能，而是一个新的独立架构。但是它到底是如何工作的呢——也就是说，在注意力层的绿色细胞里发生了什么？</p><p id="82a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像往常一样，第一步是通过一个简单的嵌入层将单词转化为初始的上下文无关矢量表示(结果在上图中表示为深绿色矩形)。这些初始的、上下文无关的表示直观地看起来像这样:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es lz"><img src="../Images/49cb956a37ef17c300a9688ece10cda7.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*oukaNgtIptO1iLhwmNxSew.png"/></div></figure><p id="35f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个简化的二维表示，但是实际上，如果您采用预先训练的word2vec嵌入，并使用PCA将它们投影到一个二维子空间上(单词嵌入通常有数百个维度)，您可以期望看到类似的东西。</p><p id="d187" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">银行介于货币和河流之间，也许更倾向于货币，因为这是它最常用的方式。<strong class="ih hj">注意层</strong>(浅绿色细胞)<strong class="ih hj">必须学会根据句子中的其他单词动态转换“银行”(例如)的表示</strong>，就像RNN已经做的那样:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ma"><img src="../Images/4d47bf9682c15b374ffa072e4d3cd223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c7jOmuJylf9_ITBns1xBKA.png"/></div></div></figure><p id="4353" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这确实与人类大脑中的歧义消除非常相似。rnn使用它们自己的内部机制实现了这一点，这里将不讨论。新的注意力层呢？</p><p id="d452" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们通过关注单个句子“<strong class="ih hj">我在河岸</strong>”，并观察单词“<strong class="ih hj"> bank </strong>”发生了什么变化(句子中的所有其他单词也发生了同样的变化)，来说明注意力机制的功能。</p><p id="ad23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，所有单词分别通过嵌入层，并被映射到它们最初的上下文无关表示，包括“bank”。</p><p id="f872" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，执行以下步骤:</p><p id="4a16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤1 </strong>:针对句子中的每个单词，计算<strong class="ih hj">与“银行”</strong>的相关程度。“相关”是指它可以以某种方式影响或帮助消除/更好地理解银行的含义，反之亦然。我们将在<em class="ku">数学细节</em>部分看到“相关度”是如何定量测量的。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es mb"><img src="../Images/31eebe9d9bac5d1792e92c701184fb39.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*mRaJu0r7sXk4m9nKShwzyQ.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">在这种情况下，<strong class="bd jg"> river </strong>是句子中唯一与<strong class="bd jg"> bank </strong>显著“相关”的单词</figcaption></figure><p id="4366" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第二步</strong>:计算完所有“相关度”后，计算银行对其他每个词的<strong class="ih hj">关注度</strong>。与句子 中的其他单词相比，与“bank”<em class="ku">高度相关的单词的注意力得分<strong class="ih hj">更高。</strong></em></p><p id="6488" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相同的两个单词之间的注意力可能因此根据特定的句子而改变。考虑下面的例子:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es mc"><img src="../Images/9e8babf5df4791c53d13c09b8367f5b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*W3SnukxoCSdk4fh-DHWZsg.png"/></div></figure><p id="58e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一句话中，“河”是唯一与“岸”高度相关的词，所以“岸”的注意力完全指向了“河”。相反，在第二句中，“铺设”也与“银行”高度相关(人们会在银行=河岸上铺设，而不是在银行=商业银行上铺设)，因此“银行”的注意力部分指向“河流”，部分指向“铺设”。这意味着在第一种情况下，“银行”对“河流”的注意得分高于第二种情况。<strong class="ih hj">注意力是一个有限的量，必须分布在所有相关的词中</strong>。</p><p id="fb20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第三步</strong> : <strong class="ih hj">将“银行”的表示移动到更接近那些具有最高注意力分数的单词的表示</strong>。物理上发生的事情是这样的(括号[ ]表示初始的、上下文无关的单词表示):</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es md"><img src="../Images/599287177a777e2875a3a962b2ffa4d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f29_M1wgzbylOvW7jMk3jQ.png"/></div></div></figure><p id="8d30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果这句话是“我在银行存了一千万”，</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es me"><img src="../Images/e3eb0aca666889f1232302235c5205a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m8lbvc5rG0BEYUs_sBq6Iw.png"/></div></div></figure><p id="130a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">即<strong class="ih hj">目标词</strong>(在这种情况下是bank)<strong class="ih hj">的最终表示是句子中所有其他词(包括词本身)的加权平均值，权重由注意力分数</strong>提供。这些分数被构造成介于0和1之间，总和为1。</p><p id="e091" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对句子中的所有其他单词重复相同的过程(实际上，所有上下文化的表示都是使用矩阵一次性计算的；参见<em class="ku">数学细节</em>。</p><p id="15ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意的一个重要方面是<strong class="ih hj">不是对称的</strong>。例如，考虑下面句子中的单词“river”和“bank”:“由于洪水，水在河岸上流动”。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es mf"><img src="../Images/89ea4c4a3df927d96a903f6b7310d75e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gdlsw44WiUH3wU7neSxgfg.png"/></div></div></figure><p id="4363" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有很多和“河”高度相关的词:除了“岸”，我们还有“水”、“流”、“洪”，表示这条河是涨水的河，而不是干涸的河。这在翻译中可能是相关的(可能存在一些异国语言，其中有一个单独且特定的词来表示“涨水的河”…)。反之，“岸”只与“河”直接相关。“水”、“洪水”等。就其本身而言，对我们理解“银行”的含义没有什么帮助。</p><p id="f2a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这暗示着“银行”会对“河流”给予很大的关注；反过来，“河”就不会那么关注“岸”，因为它的注意力会分布在所有相关的词中。</p><h2 id="070a" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">注意力可视化</h2><p id="612f" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在步骤2中计算的注意力分数可以存储在矩阵中，该矩阵的条目ij是单词I对单词j的注意力——这可以被看作是对单词j与消除/更好地理解单词I的含义有多相关的<strong class="ih hj">的度量。注意力得分矩阵通常用热图显示(较亮的方块表示较高的注意力得分):</strong></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es mg"><img src="../Images/8ac36e51a6a7583bc2e1c60642b00fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*MBAu5EBAAC6G-V0Rbt5iqw.png"/></div></figure><p id="376f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是仍然有一个未解决的问题:模型如何执行步骤1——也就是说，它如何知道两个单词何时“相关”?</p><p id="5d4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">秘密在于初始的、<strong class="ih hj">上下文无关的单词表示法</strong>的计算:它们必须合理地放置，以便<strong class="ih hj">相关的单词在几何上彼此接近</strong>。换句话说，必须在几何性质和语言性质之间建立对应关系。直觉上，左边的模式可行，右边的模式不可行:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es mh"><img src="../Images/b696e6486875a25bf5923438b7331f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ZCiewtrgnGJngyxmPpReA.png"/></div></div></figure><p id="be60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，我们不必手工制作这些初始表示——嵌入层会在训练过程中学习它们。</p><p id="dde0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与注意力不同，<strong class="ih hj">相关度</strong><strong class="ih hj">是对称的</strong>。</p><h2 id="2eb7" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">数学细节</h2><p id="a9d6" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">一般注意力层的输出由下面的公式给出(这是你将在所有文章中找到的公式；n表示嵌入维数):</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es mi"><img src="../Images/ba7e1c52f1a31e15f0035f9d257449c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*4jkp3S0frThl-ojBjUJjxA.png"/></div></figure><p id="e8da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个公式可能有点令人困惑，因为它非常笼统，除了我们刚刚检查的情况之外，还包括许多其他情况。在我们的例子中，三个矩阵重合，即Q = K = v。Q是形状矩阵(input_seq_len，embedding_dim)，其行包含由嵌入层计算的初始单词表示(=下层的深绿色矩形，包括位置编码)。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es mj"><img src="../Images/ccf22cda290e4f880bf2eb6ace208578.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VSBQ15MxFip7D1USLlVz2A.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">Q的条目是在训练中学会的。</figcaption></figure><p id="c902" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤1中的“相关性”分数计算如下</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es mk"><img src="../Images/6b97b364394dbeeb8e9473ef401dc8ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*wwGotcmR__Vd78HSHqd6VA.png"/></div></figure><p id="3347" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个形状为(input_seq_len，input_seq_len)的<strong class="ih hj">对称</strong>矩阵，其条目ij(或ji)提供了单词I和j“相关”程度的度量。这个度量就是两个矢量表示之间的点积。在两个向量被归一化的情况下，点积等于余弦相似度，但是在一般情况下不提供适当的距离。然而，作者选择不对行进行规范化，也许是为了给网络留下一些额外的灵活性(嵌入层仍然可以自由地学习规范化的向量，如果它认为合适的话)。</p><p id="b57d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了将相关性分数转化为注意力分数(步骤2)，您只需按行取softmax。在此之前，您用一个固定的归一化因子来划分相关度分数，以便获得一个“更温和”的softmax(否则，只有非常高度相关的单词才会获得显著的注意力分数)。生成的矩阵由0到1之间的数字组成，每一行的总和为1。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es ml"><img src="../Images/e0c69a9969a92b3b792c041abbe84c31.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*IaWixZuckdQE3G_PRh_xyA.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">注意力得分的非对称矩阵；shape =(输入序列长度，输入序列长度)</figcaption></figure><p id="077d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，将上面的矩阵乘以q。结果是matrix of shape (input_seq_len，embedding_dim ),其行包含新的单词表示，因此每个单词表示都是作为初始单词表示的加权平均值获得的。</p><p id="9503" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">补充几点:</p><ul class=""><li id="66bc" class="mm mn hi ih b ii ij im in iq mo iu mp iy mq jc mr ms mt mu bi translated">在经历步骤1、2和3之前，通过<strong class="ih hj">(学习的)仿射变换</strong>变换Q的行(或者，换句话说，Q通过具有线性激活的密集层)。</li><li id="179c" class="mm mn hi ih b ii mv im mw iq mx iu my iy mz jc mr ms mt mu bi translated">如前所述，<strong class="ih hj">多个关注层</strong>通常是相互堆叠的。这意味着步骤3的输出通过另一个学习的仿射变换进行变换，然后再次通过步骤1、2和3…等等。</li></ul><h2 id="5a93" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">不仅仅是语义</h2><p id="cc30" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">前面的例子可能会导致这样一种想法，即单词的表示只在非常严格的意义上捕捉单词的含义。这意味着只有名词、形容词和一些动词积极参与注意过程，而像冠词、连词等助词。将被视为无关紧要。</p><p id="6da1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">事实上，与基本的“词义”相比，语境化的单词表征更能模拟复杂的语言特性。毕竟，这并不奇怪。考虑下面两句话:</p><p id="e509" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ku">我不能忍受吃肉</em></p><p id="600c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ku">我看见一只熊在吃肉</em></p><p id="fd50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“有意义”的词在两个句子中是一样的:明确bear是“a bear”VS“to bear”的是“a”或“不可以”这样明显“无意义”的助词的存在。单词表示必须能够捕捉所有这些类型的关系。为了帮助注意力层完成这项具有挑战性的任务，引入了<strong class="ih hj">多头注意力</strong>。在实践中，这意味着多个单词表示系统(通常为8到16个)被并行计算，从而产生一组独立的注意力得分矩阵:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es et"><img src="../Images/fa3a751671477adb4f3f39c9ed5a6277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rhhFppdlfD11_OydKqLtsQ.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">多头自我关注。来源:<a class="ae jd" href="https://docs.dgl.ai/en/0.4.x/tutorials/models/4_old_wines/7_transformer.html" rel="noopener ugc nofollow" target="_blank">https://docs . dgl . ai/en/0.4 . x/tutorials/models/4 _ old _ wines/7 _ transformer . html</a></figcaption></figure><p id="6908" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这在某种程度上类似于在卷积神经网络中使用多个过滤器:<strong class="ih hj">每个注意力头都是为了捕捉特定的语言属性</strong>。最后，我们仍然可以通过连接不同头部计算的表示来获得唯一的上下文化单词表示。</p><h2 id="9ec6" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">应用的世界</h2><p id="6cd4" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">总而言之，注意力机制允许我们非常<strong class="ih hj">有效地计算任何句子中任何单词的上下文化向量表示</strong>。由于位置编码，我们甚至可以在单词表示中包含位置信息，而不需要顺序架构。</p><p id="1aa8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每当必须处理一组有限的相互作用的离散输入(无论是顺序的还是非顺序的)时，相同的机制都可以很好地工作，就像在视频游戏中一样。谷歌Deepmind最近发布了一个模型，<a class="ae jd" href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii#block-8" rel="noopener ugc nofollow" target="_blank"> AlphaStar </a>，它学会了如何玩星际争霸，并打败了顶级人类玩家。该模型首先以受监督的方式在历史游戏数据上进行训练，然后通过在强化学习框架内重复与自己对抗来开始学习全新的策略。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es na"><img src="../Images/6ded440371adbcb16a31af81d376f1cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tPiJmSNlQ9Q15FHNAwqYFw.png"/></div></div></figure><p id="ec1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">猜猜看，注意力机制在架构定义中发挥了作用，对各种游戏元素(角色、建筑等)之间的关系进行建模。基本的想法和以前一样:假设你想为你的角色决定最好的移动或者计算你的胜利几率。您可以首先使用简单的嵌入层将您的“角色”(以及所有其他游戏元素)映射到一个矢量，并获得它的初始上下文无关表示，编码属性如“我是猎人，我是人形，我有这些武器”。在注意力层，你的角色“注意”其他游戏元素，并动态调整其表现以考虑上下文。例如，它可能会注意到一条龙将要杀死它，或者一块大石头将要落到它的头上。角色的最终上下文感知表示可能会编码成类似于“我是一个猎人，一个类人动物……而且我一点也不富裕”的内容。将这个向量传递给最终回归层，我们可以发现在这种情况下获胜的几率非常低。这是一个非常简单的解释，但应该给出一个注意机制潜力的想法。</p><h1 id="69a8" class="li jf hi bd jg lj lk ll jk lm ln lo jo lp lq lr jr ls lt lu ju lv lw lx jx ly bi translated">破解翻译:变压器</h1><p id="3914" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">但是现在让我们回到<strong class="ih hj">翻译</strong>，这是我们最初的问题。我们已经放弃了“注意力RNN”这个最先进的技术。现在我们已经装备了强大的“注意力网络”,我们还能前进吗？</p><p id="dd0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所见，这种新的架构允许我们非常<strong class="ih hj">有效地消除任何句子和任何语言中的所有单词的歧义</strong>。此外，嵌入层提供了合理放置的初始上下文无关单词表示。如果是这种情况，从英语到意大利语应该很容易。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es lc"><img src="../Images/b318548a5845a350af867d414188e50f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JrDT9LgCzki3NATarZJdsQ.png"/></div></div></figure><p id="6799" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，有一个问题。例如，考虑一下，如果我们将所有的意大利图片旋转相同的角度会发生什么。我们得到的仍然是意大利语的完美有效的再现:单词之间的所有距离和关系都保持不变。然而，与他们的英国同行相比，根本没有什么<strong class="ih hj">结盟</strong>。事实上，如果两个语言模型被独立地训练，我们不能保证得到的表示在某种程度上是一致的。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nb"><img src="../Images/94866912cb12639d92169ddfd9ff4fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gvrAlLg8UH1tW5vJUqo0PA.png"/></div></div></figure><p id="a6b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，如何有效连接这两个世界呢？这正是变压器所要达到的效果。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es lc"><img src="../Images/691277812cf7059b7aa63d6afa8fc37b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bhCANo8ZuxpDbzVBKPHKwg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">我们有这个。如何让这两个世界沟通？</figcaption></figure><p id="d4b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Transformer于2017年推出，并立即成为最先进的翻译，为现代谷歌翻译奠定了基础。这是第一个完全基于注意力机制的翻译模型——事实上，这篇论文的标题很有说服力:<a class="ae jd" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>。它的名字来源于这样一个事实:转换器，嗯，<em class="ku">将句子从一种语言转换成另一种语言。</em></p><p id="288c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">必须为每个单向输入到目标语言的翻译任务训练不同的转换器模型:例如，需要三个不同的转换器模型来执行意大利语到英语、英语到意大利语和法语到英语的翻译。本教程展示了一个完整的Tensorflow / Keras实现。</p><p id="15b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们检查一个意大利语到英语的转换模型的架构。我们将省略一些架构上的细节(层规范化、遗漏……)来关注核心组件。转换器由两部分组成，编码器和解码器，就像以前基于递归网络的模型一样。不同的是，内在机制现在完全基于注意力。</p><h2 id="0412" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">训练变压器</h2><p id="8c58" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在训练期间，<strong class="ih hj">对源语言和目标语言</strong>中的相应句子分别被输入编码器和解码器。</p><p id="eb20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">编码器</strong>执行我们刚刚看到的确切步骤:它使用一个注意力层(或者更准确地说，多个堆叠的注意力层)来计算源句子中每个单词的<strong class="ih hj">上下文感知表示。从功能的角度来看，这正是基于RNN的体系结构的编码器部分所做的事情:区别仅仅在于网络内部机制的效率提高了。</strong></p><p id="1b6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解码器</strong>基本上与目标语言句子的编码器一样，但增加了第二个基于注意力的层，以某种方式<strong class="ih hj">将目标语言的单词表示与源语言的单词表示对齐</strong>。我们将这个附加层称为“<strong class="ih hj">交叉注意层</strong>”，以区别于我们目前所考虑的“自我注意”层。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nc"><img src="../Images/732a6b0a5f645d164ff65cbb7b38e9d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nvnWkxpRH5Pphn2K5n5KNw.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">Transformer架构:解码器将编码器的输出作为辅助输入，以生成目标语言中的上下文化单词表示，这些单词表示也在某种程度上与源语言中的单词表示“对齐”。</figcaption></figure><p id="4a85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们通过关注上面例子中的单词“pizza”来详细了解解码器内部发生了什么。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nd"><img src="../Images/d81344f631f99cb0793df931d9b57b56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uqYFJFRYoNOJ6-kB2HmShg.png"/></div></div></figure><p id="c5dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，目标句子中的所有单词都通过一个简单的嵌入层映射到初始的上下文无关表示(+位置编码)，包括“pizza”。</p><p id="0896" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<strong class="ih hj">自我关注层</strong>中，“pizza”应用前面段落中描述的机制，根据句子中的其他单词(Joe和likes)调整自己的表示。在这种情况下，Joe和likes对我们理解比萨饼没有什么帮助，所以我们说“比萨饼”的上下文感知表征(自我注意层的输出)只是</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es ne"><img src="../Images/ed455cfd93cf8403d7743124f167a597.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*mPbMHQo79E21WHM3dGMegw.png"/></div></figure><p id="3599" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(为了简单起见，我们省略了初步的仿射变换)。</p><p id="c6f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个输出然后被传递到<strong class="ih hj">交叉注意层</strong>，其机制类似于自我注意层的机制。就像自我注意层学习根据<em class="ku">同一个</em>句子中的其他单词来表达句子中的每个单词一样，交叉注意层学习根据<em class="ku">另一个</em>句子中的单词来表达句子中的每个单词——在这种情况下，它学习根据意大利语单词(或者更好地说，根据作为编码器输出而提供的意大利语单词的上下文感知表示来表达英语单词。在这种情况下，我们可能会看到这样的情况</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es nf"><img src="../Images/d319421b1d798e8467a48584136951f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*D1jzDPDzbjjFa-68V8H6Lg.png"/></div></figure><p id="1c8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对目标句子中的所有其他单词重复相同的过程。最终的解码器输出是意大利语单词的<strong class="ih hj">上下文感知表示，其也与英语单词</strong>的上下文感知表示有些“一致”。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ng"><img src="../Images/42c42950581e6de07ee9bb296c6bfbf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CUG7lSwUJBfXZkpb7mm3nQ.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">变形金刚:关注解码器的内部层</figcaption></figure><p id="6067" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与自我关注层一样，<strong class="ih hj">交叉关注分数</strong>可以绘制成矩阵，帮助我们可视化两种语言之间的<strong class="ih hj">对应关系</strong>。同样，我们可以使用<strong class="ih hj">多个注意力头</strong>来捕捉更多的语言属性。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nh"><img src="../Images/4636fdb9f8796f674b820fc7ddbe44a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aIEdLl6AUUwHrl3R8Voohw.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">英语到德语翻译任务的交叉注意分数(来源:<a class="ae jd" href="https://docs.dgl.ai/en/0.4.x/tutorials/models/4_old_wines/7_transformer.html" rel="noopener ugc nofollow" target="_blank">https://docs . dgl . ai/en/0.4 . x/tutorials/models/4 _ old _ wines/7 _ transformer . html</a>)</figcaption></figure><p id="312f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数学细节</strong></p><p id="fe4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，让我们深入一点数学细节。我们说解码器学习“根据意大利语单词表达英语单词”；让我们看看这在实践中是如何实现的。</p><p id="5565" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自我关注层的工作方式与编码器完全一样。交叉注意层的输出公式也是通常的公式</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es mi"><img src="../Images/ba7e1c52f1a31e15f0035f9d257449c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*4jkp3S0frThl-ojBjUJjxA.png"/></div></figure><p id="bd9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，这一次，K = V和Q不同。确切地说，Q的行包含由解码器的自我关注层计算的英语单词的上下文感知表示，而K的行包含意大利语单词的最终(上下文感知)表示，即编码器的输出:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ni"><img src="../Images/ba71b98b1bc87bc2563bf568edd84b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OPURu8X0p4WxaUn_bCrEdA.png"/></div></div></figure><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es kz"><img src="../Images/31f3675ab50691fb2fa8877ce7b1166a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_O_B77e3VJw2da5S-xC-Zw.png"/></div></div></figure><p id="ed28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你仔细观察这个公式，你会发现交叉注意层实际上输出了英语单词的新表示，作为意大利语单词(上下文感知表示)的加权平均值。</p><p id="ff16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如前所述，Q和K的行在被传递到交叉注意层之前都经历学习仿射变换。</p><h2 id="d7a0" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">变形金刚:一些结果</h2><p id="f73e" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">让我们来看看谷歌翻译的一些输出，看看变压器模型的力量。单词被正确地消除了歧义…</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nj"><img src="../Images/c931bf77856bef7ac9e380a3a4216241.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sOLlLktNnigusLE1qd5osA.png"/></div></div></figure><p id="aeea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">…甚至代词。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nk"><img src="../Images/4cbb50f2893f941bb71ba0d898da3b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PR4RnuesAYigiEvXX9P8lg.png"/></div></div></figure><p id="da4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">位置嵌入确实有效！</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nl"><img src="../Images/c2570895352f10f4d56d779edde54a9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQpcW0-3DRhrJuToMJRsgA.png"/></div></div></figure><p id="fac7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一切都很好，但是…等等！我们是不是忘记了什么？还有几件事需要澄清。</p><h2 id="47e6" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">目标是什么？</h2><p id="d1e2" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">首先，我们应该明确变形金刚的训练目标。在训练过程中，在目标语言中的每个句子的开头和结尾添加两个标记词[START]和[END](实际上，对源语言中的句子也是如此，但我们目前可以忽略这个细节)。得到的训练集是</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nm"><img src="../Images/ce52e9d7c288600b20e644742339db77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8IicQwZh3RIg8YKtO4hNpA.png"/></div></div></figure><p id="ff3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，变压器被训练来预测…嗯，只是将<strong class="ih hj">目标句子向前移动了一个位置</strong>。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nn"><img src="../Images/f49b91b97669ac1551722dcda0c67213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*od_DznyBY0ZOpdcPTEKIVg.png"/></div></div></figure><p id="90a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用和以前一样的图片，</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es no"><img src="../Images/2a6d1ab057bc038f9cdbfed8bbd33e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xygNDLot9EztUBOEpSeqUQ.png"/></div></div></figure><p id="a278" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">听起来像是一个微不足道的任务，对吗？</p><p id="4711" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于我们所看到的，解码器的内部结构应该如下:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es kx"><img src="../Images/8f0f486d4ae59bb0340fe553979542f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MbBwDK-W0e6-GhzmSP3oA.png"/></div></div></figure><p id="5833" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，解码器在预测例如“Joe”时，会这样做吗？</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es np"><img src="../Images/7fbdeae8fd75255570b174cd839f45a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQ4FhFpBtxrEbTG0AHRiJQ.png"/></div></div></figure><p id="2296" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其实不是，因为这个快捷方式在解码器里是非法的！</p><p id="e621" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">准确的说，<strong class="ih hj">所有的后路都被禁止</strong>。解码器中的自我关注层与编码器中的自我关注层有一个根本的区别:<strong class="ih hj">每个单词在构造自己的上下文感知表示时只能向后看</strong>。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nq"><img src="../Images/5939f97609a184be8002ed061ddd87b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xEPj_5mjNY3uc5afuoD4DA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">实际的解码器架构</figcaption></figure><p id="cb93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是<strong class="ih hj">被掩盖的自我关注</strong>的一种形式:一些元素无法关注另一些元素，即一些连接被关闭。这种类型的屏蔽称为<strong class="ih hj">前瞻屏蔽</strong>，因为每个单词只能“向后看”自己，而不能向前看。实际上，相关性矩阵的“非法”条目被设置为-inf，因此softmax将有效地忽略这些条目。注意力得分矩阵看起来像这样，一些元素被模糊了。因此，每个单词的注意力将只分布在“合法连接的”元素中。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es ml"><img src="../Images/26c6b101a9a0959d159210337346f657.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*x0WHSyseu8kJJCZpfVjFtA.png"/></div></figure><p id="d34a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您更仔细地检查解码器目标，您会意识到这不过是一种训练解码器为输入句子中的所有子句子<strong class="ih hj">预测下一个单词的简洁方法:</strong></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nr"><img src="../Images/66c4988e2da064ba7ba156a7b863a9f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*7Ph7FtPBivtSJ_VkKPdG-g.png"/></div></div></figure><p id="950f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们不需要一个接一个地传递所有的子句子，而是可以利用先行掩蔽技巧来传递整个句子，并在一个段落中预测所有的下一个单词。</p><p id="fee7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们可以说，转换器被训练成在目标语言中执行<strong class="ih hj">引导的下一个单词预测——由源语言中的句子引导。</strong></p><p id="c392" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，多对自我注意+交叉注意层在解码器中相互堆叠。</p><h2 id="d3b8" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">再讲几句关于掩蔽的话</h2><p id="d5ea" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">屏蔽的概念实际上相当普遍:原则上，任何连接都可以被宣布为非法，并从注意力分数计算中排除。这可以应用于自然语言处理之外的其他情况。例如，OpenAI最近发布了一个基于RL的模型,教两组木偶互相玩捉迷藏。如果您看一下模型架构，您会发现:</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nq"><img src="../Images/a528af1327aa444ad460957ef0301491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5yzLjl4df4VWy6CAtXhEg.png"/></div></div></figure><p id="9071" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与AlphaStar类似，游戏元素首先被映射到上下文无关的嵌入，然后通过在自我关注层“关注”彼此来改变它们的表示。还有，你看，自我关注层被屏蔽了！但这不再是一个前瞻掩蔽:类似于现实世界中会发生的事情，木偶只是不允许注意他们的视觉锥和视线之外的元素。</p><p id="99e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一种(不太令人兴奋，但仍然很重要)的遮罩形式是<strong class="ih hj">填充遮罩</strong>。编码器和解码器都将固定长度的序列作为输入。如果你想给他们传递比这个固定长度短的句子，你必须首先用一系列所谓的[PAD]标记来完成它们。例如，如果建立的固定长度是10，并且您想将通常的<em class="ku">Joe like pizza</em>传递给解码器，那么您必须首先将其转换为</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ns"><img src="../Images/b8097d5a6cb46166087621d35d1b4f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*htLoXcD8EencB6gNJM-lCA.png"/></div></div></figure><p id="cc75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(记住，您还必须添加[START]和[END]标记！).</p><p id="9363" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，当然，您不希望“Joe”和“pizza”在构造它们自己的上下文感知表示时关注[PAD]标记。因此，<strong class="ih hj">所有的[PAD]令牌必须在自关注和交叉关注层以及编码器和解码器中被屏蔽</strong>。填充掩码必须始终被定义并作为输入传递给转换器。</p><h2 id="a87b" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">生成翻译</h2><p id="0be0" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">好了，变形金刚已经训练好了！现在，我们如何从零开始为不在训练集中的句子生成新的翻译呢？</p><p id="05a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，要翻译的句子作为输入传递给编码器，而解码器只是将[START]标记作为输入。预测的单词是最终翻译的第一个单词。然后，将[START]标记和第一个生成的单词传递给解码器，解码器预测翻译的第二个单词。重复该过程，直到预测到[END]标记。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nt"><img src="../Images/a2056777505b97a3a4cf850569d319d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwNzZ3Wx-idxwZoRpLnsCA.png"/></div></div></figure><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nu"><img src="../Images/4226c4a5050634df61daf7ee985f0fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Ib0N4q1wmK7r0VXOL_4fQ.png"/></div></div></figure><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es kv"><img src="../Images/0dfdf86f02221659828ace2df0f6a284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-CjVNMfGe-PFDeMVU48Cow.png"/></div></div></figure><h1 id="2d92" class="li jf hi bd jg lj lk ll jk lm ln lo jo lp lq lr jr ls lt lu ju lv lw lx jx ly bi translated">单语任务呢？</h1><p id="4257" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">Transformer的诞生是为了解决翻译问题，但它推广到单语问题的潜力很明显。事实上,“拆卸”变压器，我们获得了两个目前NLP的最新模型:BERT和gpt-2。</p><h2 id="d305" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">伯特</h2><p id="4b20" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated"><a class="ae jd" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT </a>是一个仅适用于<strong class="ih hj">编码器的</strong>模型，也是几乎所有NLP任务的最新技术。(其实最初的BERT早已进化成更高级的型号，就像变形金刚早已进一步发展成重整器，gpt-2发展成gpt-3一样。然而，主要的基本思想保持不变)。</p><p id="590f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定一个输入句子，BERT使用自我注意机制计算出句子中所有单词的强大<strong class="ih hj">上下文感知表示。编码器部分在Transformer模型中的作用。然后，这些表示可以被传递到定制的附加层，以执行各种NLP任务，例如句子分类、情感分析、问题回答等。</strong></p><p id="2d44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实际上，BERT(像许多其他语言模型一样)使用<strong class="ih hj">记号</strong>而不是单词——也就是说，输入的句子被分割成记号，而不是单词。如果你看一下多语言BERT模型的<a class="ae jd" href="https://www.kaggle.com/soulmachine/pretrained-bert-models-for-pytorch/version/3?select=bert-base-multilingual-cased-vocab.txt" rel="noopener ugc nofollow" target="_blank">词汇文件</a> (vocab.txt)，你会看到的是多个字母表中的字符和符号的短序列列表，从拉丁语到汉语，其中一些会显得毫无意义。这有助于大大减少词汇量。为每个令牌计算一个上下文化的表示，并且之前讨论的所有原则仍然有效，除了它们在更抽象的级别上应用。</p><p id="daf5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">伯特接受的是什么任务训练？这一次，我们所做的是随机屏蔽掉输入句子中的一些单词(这在物理上意味着用标记单词[MASK]替换它们)，并训练模型重新预测它们。在例如全英文维基百科上这样做迫使模型学习重要的语言结构。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es nv"><img src="../Images/6b505e3e612fa117d28a0b14bd743472.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*hpPc2yNPEua-qnex0WMjyg.png"/></div></figure><p id="bd05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:这个“<strong class="ih hj">掩蔽语言预测</strong>”任务，即这个“掩蔽掉”然后重新预测输入文本中一些单词的过程，与之前讨论的掩蔽注意概念(即前瞻掩蔽、填充掩蔽……)没有任何关系！不幸的是，在术语上有一些重叠。</p><p id="1ff2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">伯特还接受了第二项稍微复杂一点的任务训练。我们将在“为分类任务微调BERT”这一节中讨论它。</p><p id="199c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT首字母缩写词，来自Transformer的双向编码器表示，现在应该很清楚了:<strong class="ih hj">单个单词的上下文感知表示</strong>是通过以双向<strong class="ih hj">方式查看文本中目标单词前后的所有其他单词来构建的。</strong></p><p id="db42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">伯特实际上不使用文字，而是使用记号。如果您查看BERT模型的词汇文件，</p><h2 id="33b4" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">gpt-2</h2><p id="f49d" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">gpt-2 取而代之的是一个只有<strong class="ih hj">解码器的</strong>型号。你可能想知道:这是什么意思，只有解码器？如果没有编码器，我们定义的解码器就没有任何意义——它甚至将编码器的输出作为侧面输入！</p><p id="d167" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其实“解码器专用”这个说法有点不合适。事实上，gpt-2中并没有保留整个解码器:交叉注意层被扔掉了。剩下的基本上是一个<strong class="ih hj">自我关注层，应用了前瞻遮罩</strong>。</p><p id="36d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目标与转换器相同:输入的句子向前移动一个位置。由于前瞻屏蔽的存在，这相当于一个重要的<strong class="ih hj">下一个单词预测</strong>任务。与Transformer中的完全一样，只是这一次，下一个单词预测任务是免费的，不再受源语言句子的“指导”。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es nw"><img src="../Images/9eb1d01c1dc03345653f5b4d67de2b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*Bk6ueDmr-cH55SQ2WSmJSQ.png"/></div></figure><p id="41c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，gpt-2必须在非常大的语料库上训练，例如全英文维基百科。正如你可能猜到的那样，与BERT的关键区别在于<strong class="ih hj">单词表示</strong>不再是双向的:它们仅仅通过<strong class="ih hj">查看</strong>后面的单词来构建。</p><p id="b2bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这似乎是一个缺点，因为在每个单词的最终表示中可以包含的上下文较少。事实上，gpt-2的第一个版本，名为gpt，在所有基准NLP任务中被BERT击败。然而，gpt-2(本质上是具有更多参数并在更大语料库上训练的gpt)在一项任务上仍然优于BERT:文本预测——相当于<strong class="ih hj">文本生成</strong>。关于BERT-vs-gpt挑战的搞笑报告可以在这里<a class="ae jd" href="https://blog.floydhub.com/gpt2/" rel="noopener ugc nofollow" target="_blank">找到</a>(寻找经理vs随机工程师的讨论)。</p><p id="a562" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一些<a class="ae jd" href="https://talktotransformer.com/" rel="noopener ugc nofollow" target="_blank">链接</a>可以让你尝试gpt-2的全部威力。生成文本的质量确实令人印象深刻。似乎放弃双向性并不总是一个缺点，毕竟！</p><h2 id="b949" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">微调gpt-2</h2><p id="c12f" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">经过预先训练的gpt-2模型(你可以免费下载并试用)倾向于生成非常<strong class="ih hj">通用的文本</strong>，已经在这样一个大型和异构的语料库上进行了训练。然而，在你选择的语料库上微调gpt-2并教它生成更具体类型或流派的文本是非常简单的。例如，您可以微调gpt-2来生成莎士比亚戏剧、恐怖故事或说唱歌曲——所有这些都只需要几行Python代码！</p><p id="3c85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这本<a class="ae jd" href="https://play.aidungeon.io/" rel="noopener ugc nofollow" target="_blank">互动书</a>也是一个很酷的例子，展示了通过对gpt2进行微调可以实现什么。</p><p id="8e16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不幸的是，gpt-2的预培训版本目前只有<strong class="ih hj">英语</strong>版本。然而，你可以利用<strong class="ih hj"> mtranslate </strong>库(调用谷歌翻译API)来生成几乎任何你选择的语言的高质量文本。</p><h2 id="c71e" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">针对分类任务微调BERT</h2><p id="7e8c" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">虽然gpt-2专门用于文本生成，但是BERT及其后代通常是所有其他NLP任务的最佳选择。我们现在将更深入地讨论如何针对定制分类任务微调BERT，这是一个非常简单的示例，也是工业环境中最常见的示例之一(例如，考虑FAQ匹配)。</p><p id="6b3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，首先，我们必须说一下伯特接受训练的第二项任务(除了“掩蔽词预测”任务)。这第二个任务叫做<strong class="ih hj">下一句分类</strong>。基本上，通过从训练语料库(例如维基百科)中选择句子对来建立训练集。有些句子对是由连续的句子组成的，而有些句子对是由两个随机选择的句子组成的。然后训练伯特预测哪些句子对是连续的，哪些不是。</p><p id="87f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每对中的两个句子并不是单独馈入网络的，而是由一个<strong class="ih hj">【SEP】token</strong>单词(也是加在第二个句子的末尾)串联连接的。此外，一个额外的<strong class="ih hj">【CLS】标记</strong>单词被添加在所得到的连接句子的开头。至关重要的是，最终的分类仅使用[CLS]令牌的输出表示来执行。事实上，[CLS]是“分类”的简称。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nx"><img src="../Images/812e5ab323e4c5cfeefd253f8fccb449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RMrcG1_7PUeCplF4RyMY-Q.png"/></div></div></figure><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ny"><img src="../Images/d946a774fc946ab1ae4fde2c782eefd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4TMMFNU6snTsmZ-WDDqixA.png"/></div></div></figure><p id="8bb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说，只有[CLS]的最终(上下文感知的)表示被传递到二进制分类层。对于这个次要任务来说，其他所有的表示基本上都被扔掉了(但是它们仍然用于掩蔽词预测任务)。</p><p id="8d83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种架构稍加修改就可以用于针对<strong class="ih hj">单句分类</strong>微调BERT。事实上，也可以将一个句子(而不是两个连接的句子)传递给BERT，只要它封装在[CLS]和[SEP]标记之间。然后可以将[CLS]令牌的输出表示传递给自定义分类层。通常，一个简单的密集(num_classes)图层后跟一个softmax就足够了，可能还需要在中间添加一个Dropout图层。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nz"><img src="../Images/f51ad7400fa121dadb04eb48302d97c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hcdKv9hlOHRM2DPVcTemmA.png"/></div></div></figure><p id="a6ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，一个简短的问答环节包含了我开始玩BERT时问自己的一些问题，以及我在几次搜索和讨论后找到的答案:</p><p id="8764" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">问:为什么只使用【CLS】标记进行句子分类？我很想将所有输出单词表示传递给分类层。</p><p id="7a58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ku">答</em>:原则上，BERT架构已经被设计成使得【CLS】令牌应该能够自己提供有效的句子表示(或句子嵌入)。如果是这种情况，包含其他输出表示只会增加计算负担，而不会提高性能。</p><p id="2b46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ku">问</em>:你说【CLS】令牌应该提供句子嵌入。然后，如果我下载一个预先训练好的BERT模型，给它输入我选择的一个句子(按照规定，封装在[CLS]和[SEP]标记之间)，并检索[CLS]的输出表示，我可以将它用作句子嵌入，例如计算句子之间的余弦距离，对吗？</p><p id="3696" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不幸的是，[CLS]令牌似乎并不适合这个特殊的用例。性能甚至比臭名昭著的基线还要差，基线采用句子中单个单词的word2vec表示的平均值作为句子表示。这可能是因为BERT没有在句子相似性任务上受过明确的训练——不像其他模型，如<a class="ae jd" href="https://arxiv.org/abs/1803.11175" rel="noopener ugc nofollow" target="_blank">通用句子编码器</a> (USE)，它们实际上更适合这个特定的用例。一种称为<a class="ae jd" href="https://arxiv.org/abs/1908.10084" rel="noopener ugc nofollow" target="_blank">句子的原始BERT模型的变体——BERT</a>也被提出来提高句子相似性的性能。一般来说，人们应该记住，不存在普遍“最佳”的自然语言模型:最佳选择取决于所研究的特定问题。当使用预训练模型进行迁移学习或其他应用时，一个良好的做法是始终检查原始模型已被训练的任务，并验证它们与目标任务的一致程度。</p><p id="b1ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ku">问</em>:如果我取所有的输出单词表示(不包括[CLS]和[SEP]标记)，将它们平均，并使用结果向量作为句子嵌入，会怎么样？</p><p id="2efb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ku"> A </em>:这种方法实际上在应用与上下文无关的单词表示时工作得很好，就像word2vec提供的那些(这是一个众所周知的嵌入基线的句子，用来验证更复杂的模型)。不幸的是，也许令人惊讶的是，当使用BERT的上下文感知表示时，实验结果并不令人满意。一个可能的原因是，这些表征往往是<em class="ku">非常</em>上下文感知的，即，被周围的上下文非常“污染”。在BERT中，单个单词和周围上下文之间的界限实际上相当松散，很难定义。出于同样的原因，使用BERT的单词表示来计算单词之间的余弦相似性或执行其他word2vec典型任务可能是危险的。</p><h2 id="d137" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">玩基于注意力的模型</h2><p id="f519" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">如果你现在想玩基于注意力的模型，这里有一些资源可供参考:</p><ul class=""><li id="aea3" class="mm mn hi ih b ii ij im in iq mo iu mp iy mq jc mr ms mt mu bi translated">有几个教程可以用来试验BERT和GPT-2；<a class="ae jd" href="https://huggingface.co/transformers/v2.1.1/index.html" rel="noopener ugc nofollow" target="_blank"> huggingface </a>为你可能需要的几乎任何东西提供pytorch代码和预训练模型。如果你对Keras框架比较熟悉，可以在我的<a class="ae jd" href="https://github.com/flowel1/attention-networks" rel="noopener ugc nofollow" target="_blank"> github </a>上查看笔记本。一个值得一试的有趣的库是ktrain，您可以使用它在定制分类任务上对BERT进行微调，只需几行代码。</li><li id="c6bb" class="mm mn hi ih b ii mv im mw iq mx iu my iy mz jc mr ms mt mu bi translated">我发现Transformer上的<a class="ae jd" href="https://www.tensorflow.org/tutorials/text/transformer" rel="noopener ugc nofollow" target="_blank"> Keras tutoria </a> l对于深入理解架构非常有启发性，包括文章中没有提到的一些细节(例如层规范化和脱落)。</li></ul><p id="3269" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">玩得开心！</p></div><div class="ab cl oa ob gp oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="hb hc hd he hf"><p id="2836" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[1] K. Cho等人，<em class="ku">使用统计机器翻译的RNN编码器-解码器学习短语表示</em>，2014年9月</p><p id="4ab3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2] I. Sutskever等人，<em class="ku">用神经网络进行序列间学习</em>，2014年9月</p><p id="b544" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3] D. Bahdanau，K. Cho，I. Benjo，<em class="ku">联合学习对齐和翻译的神经机器翻译</em>，2015年4月</p><p id="f991" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4] Y. Wu等，谷歌的神经机器翻译系统:弥合人工与机器翻译的鸿沟，2016年10月</p><p id="7718" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[5] A. Vaswani等人，<em class="ku">关注是你所需要的全部</em>，2017年6月</p><p id="71eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[6] A .拉德福德等，<em class="ku">通过生成性预训练提高语言理解</em>，2018年6月</p><p id="8948" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[7] J. Devlin等人，<em class="ku"> BERT:用于语言理解的深度双向转换器的预训练</em>，2019年4月</p></div></div>    
</body>
</html>