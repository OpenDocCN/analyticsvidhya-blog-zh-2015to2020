<html>
<head>
<title>Image Captioning with Attention: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意图像字幕:第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/image-captioning-with-attention-part-1-e8a5f783f6d3?source=collection_archive---------4-----------------------#2020-11-01">https://medium.com/analytics-vidhya/image-captioning-with-attention-part-1-e8a5f783f6d3?source=collection_archive---------4-----------------------#2020-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="179f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">第一部分概述了图像字幕的“编解码”模型及其在 PyTorch 中的实现</h2></div><div class="ix iy iz ja fd ab cb"><figure class="jb jc jd je jf jg jh paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><img src="../Images/d93fedbe2fa57112e57253bb3fdeda53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*br6B43K4CAWNWdxYjz8nBw.png"/></div></figure><figure class="jb jc jd je jf jg jh paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><img src="../Images/2468093ca6d6c804fa3c3b33943ea0a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*LhEmSbSd2zCO31V-1f_YLw.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx js di jt ju translated">来源:<a class="ae jv" href="https://cocodataset.org/" rel="noopener ugc nofollow" target="_blank">可可女士数据集</a></figcaption></figure></div><h1 id="bfa7" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">介绍</h1><p id="4371" class="pw-post-body-paragraph ko kp hi kq b kr ks ij kt ku kv im kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">在多年的研究中，图像字幕问题一直是深度学习中一个活跃而又成熟的话题。最终目标是描述图像的内容，将其映射为一系列单词。</p><p id="7814" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><a class="ae jv" href="https://arxiv.org/pdf/1810.04020.pdf" rel="noopener ugc nofollow" target="_blank">这份</a>综合调查可以作为各种方法、数据集和评估指标的重要参考。</p><p id="53e4" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">在这个简短的系列文章中，我将坚持使用一种特殊的技术，这种技术允许使用基于“注意力”的序列到序列模型来完成捕获生成任务。</p><p id="a5ba" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">最初，我从我的<a class="ae jv" href="https://www.udacity.com/course/computer-vision-nanodegree--nd891" rel="noopener ugc nofollow" target="_blank"> Udacity 纳米学位项目</a>中提取了一个基线模型，并在数据加载(添加了验证集)、训练(预定采样)和推断(波束搜索)方面进行了重大调整。</p><p id="763c" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">在实施过程中，我遵循了来自“<a class="ae jv" href="https://arxiv.org/pdf/1502.03044.pdf" rel="noopener ugc nofollow" target="_blank">展示、出席和讲述</a>”论文和这个全面的<a class="ae jv" href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" rel="noopener ugc nofollow" target="_blank">图像字幕教程</a>的最佳实践。</p><p id="6c4d" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">代码是用 PyTorch 框架写的，可以在这个<a class="ae jv" href="https://github.com/MakarovArtyom/Image-Captioning-with-Attention" rel="noopener ugc nofollow" target="_blank">原始回购</a>中找到。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><h1 id="c021" class="jw jx hi bd jy jz lw kb kc kd lx kf kg io ly ip ki ir lz is kk iu ma iv km kn bi translated">数据加载</h1><p id="c675" class="pw-post-body-paragraph ko kp hi kq b kr ks ij kt ku kv im kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">在接下来的研究中，我使用了<a class="ae jv" href="https://cocodataset.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hj"> MS COCO 数据集</strong> </a>来训练和验证该模型。</p><p id="0613" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">该数据集版本于 2014 年<strong class="kq hj">发布，包含:</strong></p><ul class=""><li id="ec41" class="mb mc hi kq b kr lk ku ll kx md lb me lf mf lj mg mh mi mj bi translated">82783<em class="mk">培训；</em></li><li id="d42f" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated">40504<em class="mk">验证；</em></li><li id="45d9" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated">40775 张<em class="mk">测试</em>图像；</li><li id="95c9" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated">5 <em class="mk">标题</em>用于每个培训和验证部分。</li></ul><p id="2836" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">让我们仔细看看数据加载。完整代码<code class="du mq mr ms mt b">data_loader.py</code>可在<a class="ae jv" href="https://github.com/MakarovArtyom/Image-Captioning-with-Attention/blob/master/data_loader.py" rel="noopener ugc nofollow" target="_blank">回购</a>中获得。</p><ol class=""><li id="d880" class="mb mc hi kq b kr lk ku ll kx md lb me lf mf lj mu mh mi mj bi translated">来自<code class="du mq mr ms mt b">data_loader.py</code>的<code class="du mq mr ms mt b">get_loader()</code>函数接收<code class="du mq mr ms mt b">mode</code>作为参数，它有三个可能的值:<code class="du mq mr ms mt b">'train'</code>、<code class="du mq mr ms mt b">'valid'</code>或<code class="du mq mr ms mt b">'test'</code>。</li><li id="4484" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mu mh mi mj bi translated">如果模式是<code class="du mq mr ms mt b">'train'</code>或<code class="du mq mr ms mt b">'valid'</code>，我们使用<code class="du mq mr ms mt b">CoCoDataset</code>类中的<em class="mk"> </em> <code class="du mq mr ms mt b"><em class="mk">get_indices()</em></code>方法，检索指定<code class="du mq mr ms mt b">batch_size</code>的图像批次和对应的<em class="mk">随机采样长度、</em>的字幕。</li></ol><pre class="ix iy iz ja fd mv mt mw mx aw my bi"><span id="fdf6" class="mz jx hi mt b fi na nb l nc nd">def get_indices(self):<br/>    # randomly select the caption length from the list of lengths<br/>    sel_length = np.random.choice(self.caption_lengths)<br/>    <br/>    all_indices = np.where([self.caption_lengths[i] == sel_length for i in np.arange(len(self.caption_lengths))])[0]<br/>    <br/>    # select m = batch_size captions from list above<br/>    indices = list(np.random.choice(all_indices, size=self.batch_size))<br/>    # return the caption indices of specified batch<br/>    return indices</span></pre><p id="7c36" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">3.索引将被传递给数据加载器，数据加载器返回关于<code class="du mq mr ms mt b">mode</code>的数据点:</p><ul class=""><li id="a205" class="mb mc hi kq b kr lk ku ll kx md lb me lf mf lj mg mh mi mj bi translated">图像，字幕 if<code class="du mq mr ms mt b">mode == 'train'</code>；</li><li id="69b1" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated">图像、字幕和<em class="mk">所有字幕</em>，如果<code class="du mq mr ms mt b">mode == 'valid'</code>对应一幅图像；</li><li id="20f2" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated">原始图像、预处理图像(如果有<code class="du mq mr ms mt b">mode == 'test'</code>)。</li></ul><p id="134c" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">为了在验证阶段计算 BLEU 评分，将需要字幕的整个语料库。</p><p id="dd07" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">我们将把<a class="ae jv" href="https://github.com/MakarovArtyom/Image-Captioning-with-Attention/blob/master/vocabulary.py" rel="noopener ugc nofollow" target="_blank">词汇表</a>存储在<code class="du mq mr ms mt b">vocab.pkl</code>文件中，并为验证和测试数据集指定<code class="du mq mr ms mt b">vocab_from_file == True</code>，以便从文件中加载词汇表。</p><h1 id="02ad" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">模型架构</h1><p id="bd5e" class="pw-post-body-paragraph ko kp hi kq b kr ks ij kt ku kv im kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">一般来说，该模型包括三个主要部分:</p><ul class=""><li id="374a" class="mb mc hi kq b kr lk ku ll kx md lb me lf mf lj mg mh mi mj bi translated">编码器(预先训练好的 CNN)；</li><li id="c68f" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated">注意力网络；</li><li id="534f" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated">解码器——可训练 RNN 模型。</li></ul><figure class="ix iy iz ja fd jc er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es ne"><img src="../Images/ad46cff27c03ea0459a23697837adb92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Kz4UAvDWwcO-C0AdyI6pg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图 1:“编码器-解码器”模型架构</figcaption></figure><h2 id="7207" class="mz jx hi bd jy nf ng nh kc ni nj nk kg kx nl nm ki lb nn no kk lf np nq km nr bi translated">1.编码器</h2><p id="1e2e" class="pw-post-body-paragraph ko kp hi kq b kr ks ij kt ku kv im kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">当前的实现(图 1)假设 Resnet-152 是一个由卷积构建模块组成的<em class="mk">特征提取器</em>，嵌入了快捷连接。</p><p id="4384" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">它接收一个 224x224 随机裁剪的应用了变形器(稍后描述)的图像样本，并提取 2048 个大小为 7x7 的特征地图。</p><p id="4a88" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">注意，我们从最后一个卷积块<em class="mk">获得特征向量，而不应用全连接层</em>。这允许注意力网络在解码期间具有选择性并聚焦于各种图像特征。</p><figure class="ix iy iz ja fd jc"><div class="bz dy l di"><div class="ns nt l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">模型编码器</figcaption></figure><p id="27c5" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq hj">详情</strong>:</p><ul class=""><li id="66b2" class="mb mc hi kq b kr lk ku ll kx md lb me lf mf lj mg mh mi mj bi translated">为了加载<em class="mk">预训练的</em> Resnet-152，我们使用 torchvision 的<code class="du mq mr ms mt b">models</code>子包:<code class="du mq mr ms mt b">models.resnet152(pretrained=True)</code>；</li><li id="0532" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated">使用最低卷积块的输出(忽略底部的自适应平均池和线性层):<code class="du mq mr ms mt b">models = list(resnet.childern())[:-2]</code>；</li><li id="9f54" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated">为了准备用于解码的特征，我们置换维度- <code class="du mq mr ms mt b">features.permute(0,2,3,1)</code>并对其进行整形。输出的大小为<code class="du mq mr ms mt b">(batch, 49, 2048)</code>。</li></ul><h2 id="77b4" class="mz jx hi bd jy nf ng nh kc ni nj nk kg kx nl nm ki lb nn no kk lf np nq km nr bi translated">2.<strong class="ak">注意</strong></h2><p id="d5f2" class="pw-post-body-paragraph ko kp hi kq b kr ks ij kt ku kv im kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">使用注意机制，我们强调图像中最重要的像素。</p><p id="b7dc" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">为了关注每个解码步骤的相关部分，注意力网络输出<em class="mk">上下文向量</em>，它是编码器输出(特征)的加权和。</p><p id="0c7c" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">为了产生上下文向量:</p><ul class=""><li id="0a4e" class="mb mc hi kq b kr lk ku ll kx md lb me lf mf lj mg mh mi mj bi translated">首先，我们用<em class="mk">评分函数</em>对传递给注意力网络的编码器的每个输出(特征)进行<strong class="kq hj">评分</strong>。</li><li id="432c" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated">然后我们<strong class="kq hj">得到概率</strong>，将<em class="mk"> softmax </em>函数应用于分数。这些值表示我们输入到解码器的每个特征向量的相关性。</li><li id="848a" class="mb mc hi kq b kr ml ku mm kx mn lb mo lf mp lj mg mh mi mj bi translated"><strong class="kq hj">计算加权和</strong>，将特征乘以相应的概率。</li></ul><p id="d157" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">我强烈推荐这个博客来深入了解不同类型的 attention⁵.</p><p id="d751" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">当前的模型采用软<a class="ae jv" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank"> Bahdanau(加法)</a>注意力，在 training⁶.期间使用<em class="mk">前馈网络</em>学习注意力分数</p><p id="9950" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">在<code class="du mq mr ms mt b">4.2</code>章节下的“<a class="ae jv" href="https://arxiv.org/pdf/1502.03044.pdf" rel="noopener ugc nofollow" target="_blank">展示、出席和讲述</a>”论文中很好地描述了针对图像字幕问题的软注意的使用，并且可以示意性地表示如下。</p><figure class="ix iy iz ja fd jc er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es nu"><img src="../Images/311a10c9d382c5b1983d549581526a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iWYfHK5B4p8C4U-WQqkZwg.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图 2:“软注意块”</figcaption></figure><figure class="ix iy iz ja fd jc"><div class="bz dy l di"><div class="ns nt l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">柔和的注意力</figcaption></figure><p id="8b37" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq hj">详情</strong>:</p><ul class=""><li id="d481" class="mb mc hi kq b kr lk ku ll kx md lb me lf mf lj mg mh mi mj bi translated">注意力分数<code class="du mq mr ms mt b">atten_score</code>是使用前馈网络计算的(符号可能因来源不同而不同，并与原始论文不同):</li></ul><figure class="ix iy iz ja fd jc er es paragraph-image"><div class="er es nv"><img src="../Images/1d267a2e2505d4dc49f154f443f3f4e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*QY7bfpYCfhTqLZOJfWBAGw.png"/></div></figure><ul class=""><li id="4d8a" class="mb mc hi kq b kr lk ku ll kx md lb me lf mf lj mg mh mi mj bi translated">接下来，我们应用 softmax 来计算概率<code class="du mq mr ms mt b">atten_weights</code>:</li></ul><figure class="ix iy iz ja fd jc er es paragraph-image"><div class="er es nw"><img src="../Images/5b7316fd1c54e9b8b0b3cd7a153fb6eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*ouHXXlaRjFNhpMYZeLjXsg.png"/></div></figure><ul class=""><li id="9bbf" class="mb mc hi kq b kr lk ku ll kx md lb me lf mf lj mg mh mi mj bi translated">最后，导出上下文向量:</li></ul><figure class="ix iy iz ja fd jc er es paragraph-image"><div class="er es nx"><img src="../Images/a5f3a48514188d1e9523b7f3338e1698.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*nKACIGy2Bwo6N5Yf582WgQ.png"/></div></figure><h2 id="af8d" class="mz jx hi bd jy nf ng nh kc ni nj nk kg kx nl nm ki lb nn no kk lf np nq km nr bi translated">3.解码器</h2><p id="fe04" class="pw-post-body-paragraph ko kp hi kq b kr ks ij kt ku kv im kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">在跳到解码器的架构之前，让我们来形式化一下图像字幕任务。</p><p id="14a5" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">给定输入特征图<strong class="kq hj"> X </strong>和长度为<strong class="kq hj"> T </strong>的目标标题<strong class="kq hj"> Y </strong>，模型学习精确预测序列 Y，计算对数概率<strong class="kq hj">P(Y | X)</strong></p><figure class="ix iy iz ja fd jc er es paragraph-image"><div class="er es ny"><img src="../Images/6493ee0d74f1ecbf5d28345f40ca0c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*ufy8RlF8p5GYktCeis7_ZA.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><a class="ae jv" href="https://arxiv.org/pdf/1506.03099" rel="noopener ugc nofollow" target="_blank"> <em class="nz">递归神经网络序列预测的预定采样</em> </a></figcaption></figure><blockquote class="oa ob oc"><p id="8a86" class="ko kp mk kq b kr lk ij kt ku ll im kw od lm kz la oe ln ld le of lo lh li lj hb bi translated">该模型学习一组参数<strong class="kq hj"> θ* </strong>，使正确序列的<em class="hi">对数似然</em>最大化。</p></blockquote><p id="671a" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">为了处理序列，我们使用<em class="mk"> LSTM(长-短时记忆)</em>单元，其输出隐藏状态(短时记忆)<strong class="kq hj"> h </strong>和单元状态(长时记忆)<strong class="kq hj"> c </strong>。</p><p id="092c" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">然后，我们将隐藏状态<strong class="kq hj"> h </strong>提供给一个全连接层，之后是 softmax，以便为字典中的所有标记计算<em class="mk">概率</em>。</p><p id="0937" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq hj">重要</strong>:如果我们用<em class="mk">交叉熵损失</em>进行训练，损失函数将 softmax 应用于输出，然后执行对数运算。</p><p id="05d7" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">因此，只要给定一个隐藏状态和前一个令牌，模型<em class="mk">就会学习生成序列中的下一个令牌。</em></p><p id="0ec9" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">逐步解码过程如下所示。</p><figure class="ix iy iz ja fd jc er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es og"><img src="../Images/26bfe74481effb4727e40ec6db70b2b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d9qu_Fm-xpfwVMi_abftpQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图 3:“解码器”</figcaption></figure><p id="bc48" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><strong class="kq hj">解码步骤:</strong></p><ol class=""><li id="20a4" class="mb mc hi kq b kr lk ku ll kx md lb me lf mf lj mu mh mi mj bi translated">从目标标题:<code class="du mq mr ms mt b">embed = self.embeddings(captions)</code>创建<strong class="kq hj">嵌入</strong>。这个向量的大小为<code class="du mq mr ms mt b">(batch, t, embed_dim)</code>，其中<code class="du mq mr ms mt b">t</code>对应于一个序列长度。</li></ol><figure class="ix iy iz ja fd jc er es paragraph-image"><div role="button" tabindex="0" class="ji jj di jk bf jl"><div class="er es oh"><img src="../Images/d622ca7525e83ff3b3e63e1e83b31d81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QuSDT84uqTOADZ0V_MzToQ.png"/></div></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">图 3:“嵌入”</figcaption></figure><p id="2044" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">2.使用全连接层<code class="du mq mr ms mt b">h,c = self.init_hidden(features)</code>初始化<strong class="kq hj">隐藏状态(h，c) </strong>:</p><blockquote class="oa ob oc"><p id="9654" class="ko kp mk kq b kr lk ij kt ku ll im kw od lm kz la oe ln ld le of lo lh li lj hb bi translated">LSTM 的初始存储状态和隐藏状态由通过两个独立的 MLP(init _ c 和 init_h)馈送的注释向量的<strong class="kq hj">平均值来预测。</strong></p></blockquote><figure class="ix iy iz ja fd jc"><div class="bz dy l di"><div class="ns nt l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">隐藏状态的初始化</figcaption></figure><p id="a94a" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">3.在训练的每一步执行<a class="ae jv" href="https://arxiv.org/abs/1506.03099" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hj">预定抽样</strong> </a>。我们将<em class="mk">采样概率</em>设置为从目标序列(所谓的<strong class="kq hj">教师强制</strong>)或输出样本中选择下一个输入令牌<strong class="kq hj"> ỹ </strong>的条件。</p><p id="bff8" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">计划抽样背后的主要思想是当目标标记不可用时，缩小教师强制训练和推理阶段之间的差距。</p><figure class="ix iy iz ja fd jc er es paragraph-image"><div class="er es oi"><img src="../Images/4ffc93a3786e506695b18d5beda552fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*RbdmH3lZbOweTgJ6lZ5cvg.png"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated"><a class="ae jv" href="https://arxiv.org/pdf/1906.04331" rel="noopener ugc nofollow" target="_blank">平行预定抽样</a></figcaption></figure><figure class="ix iy iz ja fd jc"><div class="bz dy l di"><div class="ns nt l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">设置采样概率</figcaption></figure><p id="03d0" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">如果采样<code class="du mq mr ms mt b">sample_prob</code>的概率<em class="mk">高于随机</em>，我们选择从输出<code class="du mq mr ms mt b">top_idx</code>中采样令牌，否则目标令牌将被通过。</p><figure class="ix iy iz ja fd jc"><div class="bz dy l di"><div class="ns nt l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">输出的缩放</figcaption></figure><p id="248d" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">利用采样温度<code class="du mq mr ms mt b">self.sample_temp</code>调整输出，在应用 softmax 之前放大输出。比如温度等于<code class="du mq mr ms mt b">0.5</code>会导致输出值更大，多样性更低。</p><p id="8596" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">这使得 LSTM 更挑剔，但是<a class="ae jv" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank"> <em class="mk">在其样本</em> </a> ⁵ <em class="mk">中更保守。</em></p><p id="64e4" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">4.将嵌入和上下文向量连接成到 LSTM 单元的<strong class="kq hj">单输入</strong>:</p><figure class="ix iy iz ja fd jc"><div class="bz dy l di"><div class="ns nt l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">LSTM 单元的级联输入</figcaption></figure><p id="af65" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">5.最后，我们将具有<code class="du mq mr ms mt b">p=0.5</code>的丢失正则化应用于隐藏状态<strong class="kq hj"> h </strong>，并将其提供给全连接层:</p><figure class="ix iy iz ja fd jc"><div class="bz dy l di"><div class="ns nt l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">获得最终输出</figcaption></figure><p id="0171" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">解码器的<strong class="kq hj">全码</strong>:</p><figure class="ix iy iz ja fd jc"><div class="bz dy l di"><div class="ns nt l"/></div><figcaption class="jo jp et er es jq jr bd b be z dx translated">模型解码器</figcaption></figure><h2 id="3140" class="mz jx hi bd jy nf ng nh kc ni nj nk kg kx nl nm ki lb nn no kk lf np nq km nr bi translated">后续步骤</h2><p id="4fb1" class="pw-post-body-paragraph ko kp hi kq b kr ks ij kt ku kv im kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">在第二部分，我将深入了解训练过程(超参数选择、验证度量)和字幕采样(贪婪、波束搜索解码)。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><h2 id="c35a" class="mz jx hi bd jy nf ng nh kc ni nj nk kg kx nl nm ki lb nn no kk lf np nq km nr bi translated"><em class="nz">参考</em>:</h2><p id="0737" class="pw-post-body-paragraph ko kp hi kq b kr ks ij kt ku kv im kw kx ky kz la lb lc ld le lf lg lh li lj hb bi translated">[1] Samy Bengio，Oriol Vinyals，Navdeep Jaitly，Noam Shazeer。(2015 年 9 月 23 日)。<em class="mk">用递归神经网络进行序列预测的预定采样。</em></p><p id="528f" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">[2]开尔文·徐、吉米·巴雷、瑞安·基罗斯、赵京贤、亚伦·库维尔、鲁斯兰·萨拉胡季诺夫、理查德·泽梅尔、约舒阿·本吉奥。(2016 年 4 月 19 日)。<em class="mk">展示、出席、讲述:视觉注意的神经图像字幕生成。</em></p><p id="295f" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">[3] MD. ZAKIR HOSSAIN，FERDOUS SOHEL，MOHD FAIRUZ SHIRATUDDIN，HAMID LAGA。(2018 年 10 月 14 日)。<em class="mk">对图像字幕深度学习的全面调查。</em></p><p id="5cda" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">[4] Dzmitry Bahdanau，Kyunghyun Cho，Yoshua Bengio。(2014 年 9 月 1 日)。<em class="mk">通过联合学习对齐和翻译的神经机器翻译。</em></p><p id="4f58" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">[5]安德烈·卡帕西。(2105 年 5 月 21 日)。<em class="mk">递归神经网络的不合理有效性。</em></p><p id="5acd" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">[6]莉莲翁。(2018 年 6 月 24 日)。<em class="mk">注意？立正！</em></p><p id="fcb2" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated">[7]萨加尔·维诺达巴布。一个<a class="ae jv" href="https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning" rel="noopener ugc nofollow" target="_blank"> PyTorch 图像字幕教程</a>。</p><p id="4c23" class="pw-post-body-paragraph ko kp hi kq b kr lk ij kt ku ll im kw kx lm kz la lb ln ld le lf lo lh li lj hb bi translated"><em class="mk">项目的 GitHub 库:</em><a class="ae jv" href="https://github.com/MakarovArtyom/Image-Captioning-with-Attention" rel="noopener ugc nofollow" target="_blank"><em class="mk">https://GitHub . com/MakarovArtyom/Image-Captioning-with-Attention</em></a></p></div></div>    
</body>
</html>