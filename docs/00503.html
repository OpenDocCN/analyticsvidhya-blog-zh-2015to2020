<html>
<head>
<title>Understanding Gated Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解门控递归神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/i-strongly-recommend-to-first-know-how-rnn-algorithm-works-to-get-along-with-this-post-of-gated-9b4bf4f0ced2?source=collection_archive---------0-----------------------#2019-07-11">https://medium.com/analytics-vidhya/i-strongly-recommend-to-first-know-how-rnn-algorithm-works-to-get-along-with-this-post-of-gated-9b4bf4f0ced2?source=collection_archive---------0-----------------------#2019-07-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7ca4" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">门控rnn比rnn好在哪里？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/5dec97493778baca73a532fb8d60f313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7II76vRaoensWJ7T"/></div></div></figure><h2 id="1bb0" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">先决条件</h2><p id="4c52" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp ju kq kr ks jy kt ku kv kc kw kx ky kz hb bi translated">我强烈建议首先了解一个递归神经网络算法如何与这个门控RNN氏症的帖子相处。</p><h2 id="4c4a" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated"><strong class="ak">门控RNN </strong></h2><p id="2943" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp ju kq kr ks jy kt ku kv kc kw kx ky kz hb bi translated">在进入细节之前，让我们先讨论一下从RNN到类似RNN的东西的必要性。</p><p id="6a70" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">在调整基本的RNN算法时，或多或少每个神经网络都面临一个障碍，即<strong class="kj hj">消失梯度</strong>的问题。消失梯度是多层感知器面临的一个问题，在误差率反向传播期间，当梯度本身非常小时，梯度永远不会到达初始层。</p><p id="a4f4" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">这在MLP中以稍微不同的方式处理，通过使用<strong class="kj hj">更好的初始化技术</strong>和<strong class="kj hj">更好的激活函数</strong>。但是，如果你观察，基本RNN只有一个具有多个折叠的层，因此通过时间的反向传播是通过采用<strong class="kj hj">长延迟来处理这个问题的建议解决方案。</strong></p><p id="0dbd" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">长延迟意味着误差以两种方式传播</p><p id="381a" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">(1)沿着记忆状态和</p><p id="2d77" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">(2)沿着输入路径。</p><p id="8e0f" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">然而，这里又有一个问题，当我们用长时间的延迟来解决问题时，有多少长时间的延迟必须考虑在内？没有预定义/指定的步骤来达到长延迟的最佳数量。相反，我们去尝试错误。此外，我们非常清楚，这种方式的算法可能会/可能不会给出其最佳性能。</p><p id="350a" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">这就是门控RNN消除长时间延迟问题的地方，它将一种叫做“门”的东西纳入画面。</p><p id="3d12" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">现在我们举一个例子来了解这个算法的应用，以获得更好的画面。</p><p id="e2fb" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">考虑这两种说法—</p><blockquote class="lf lg lh"><p id="f57d" class="kh ki li kj b kk la ij km kn lb im kp lj lc kr ks lk ld ku kv ll le kx ky kz hb bi translated">“天空是蓝色的”</p><p id="e384" class="kh ki li kj b kk la ij km kn lb im kp lj lc kr ks lk ld ku kv ll le kx ky kz hb bi translated">“我在法国长大，我非常喜欢这个地方。我说法语”。</p></blockquote><p id="e62b" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">在第一种情况下，在对文本进行分类时，算法不需要长时间记住天空(主题)。然而，在第二种情况下，法国必须在存储器状态中保留一段时间，以便算法预测该人讲法语，即，可能有许多句子，直到最后一个句子说“我讲法语”，但是算法必须知道直到何时保留该信息，以便预测一些事情。这是基本RNN的长期延迟失败的地方，门控RNN提供了一个更好的优势。</p><p id="48b3" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">在门控RNN中通常有三个门，即<strong class="kj hj">输入/写入</strong>门、<strong class="kj hj">保持/存储</strong>门和<strong class="kj hj">输出/读取</strong>门，因此该算法被命名为门控RNN。这些门负责允许/禁止来自相应状态的信号流。这意味着任何给定门的值在任何给定点都是0或1，这实质上意味着信号被允许或停止。</p><p id="2c06" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">让我们以任何一个州为例来清楚地了解这一点。在输入状态的任何给定点<strong class="kj hj"> t </strong>，输入门可以取值0或1。如果输入门值为0，则此时输入信息流被阻塞，反之亦然。(注意:其他两个门的情况相同)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lm"><img src="../Images/a309e70c2a3cd7869afd5d6749161bca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*aCzNCuJZEOali_JQ33ZYYQ@2x.jpeg"/></div></figure></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><p id="5e94" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">有人提出了一个想法，不是完全允许/禁止信息，而是说，如果允许一些数量，而不允许一些数量，则性能会更好，这实质上意味着gate的值介于0和1之间，而不仅仅是0或1。</p><p id="92ac" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">说了这么多，做了这么多，还有一件事要弄清楚，是什么/谁控制着这些门？</p><p id="66f3" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">让我们再举一个例子来了解谁控制着大门。假设我在同一段视频中给两个人分配任务，我让其中一个人计算视频中的人数，另一个人计算视频中的物品数量。这里我们可以清楚地看到，目标完全不同，这恰恰成为控制门的决定性因素，或者说输入控制门。</p><p id="4aad" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">因此，门的输出在0和1之间，本质上是sigmoid函数的输出，并由输入控制，这意味着每个门是具有一个隐藏层的神经网络，其输入是RNN算法的输入。因此，通过门的信息流由神经网络控制，神经网络将其输入作为当前状态和给定时间<strong class="kj hj"> t </strong>的存储状态。</p><p id="2642" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">通过这种方式，信息流得到控制，长时间延迟的问题在门控RNN中完全消除。</p><p id="a4ff" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">这就是RNN的基本建筑是如何建成的。基于这种架构，LSTM和GRU模型得以发展，其性能远远优于基本架构。</p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><p id="33b3" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated">我要写另一篇关于LSTM建筑的文章。</p><p id="6b18" class="pw-post-body-paragraph kh ki hi kj b kk la ij km kn lb im kp ju lc kr ks jy ld ku kv kc le kx ky kz hb bi translated"><strong class="kj hj">附言</strong>。由于这是我的第一篇文章，我愿意接受关于进一步调整这篇文章的建议。</p></div></div>    
</body>
</html>