<html>
<head>
<title>A Detailed Guide of YOLO on OpenCV Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenCV Python上的YOLO详细指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-detailed-guide-of-yolo-on-opencv-python-98890baa2d6?source=collection_archive---------4-----------------------#2019-11-30">https://medium.com/analytics-vidhya/a-detailed-guide-of-yolo-on-opencv-python-98890baa2d6?source=collection_archive---------4-----------------------#2019-11-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="0052" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">本文来源于<a class="ae jh" href="https://cuda-chen.github.io/programming/2019/11/29/a-detailed-guide-of-yolo-on-opencv-python.html" rel="noopener ugc nofollow" target="_blank">我的个人博客</a>。如果你被挡在付费墙外，按<a class="ae jh" rel="noopener" href="/@clh0524/a-detailed-guide-of-yolo-on-opencv-python-98890baa2d6?sk=de8b1e2523b1cc5a637db5a56a739ab7">这个好友链接</a>看这篇文章。</p></blockquote><h1 id="2258" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">介绍</h1><p id="a5dd" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">我在Weeview实习的时候，第一次听说OpenCV。在OpenCV的帮助下，我编写了桶形失真、摄像机标定和视频画中画程序的代码。(你可以在我的GitHub上看到<a class="ae jh" href="https://github.com/Cuda-Chen/lens-distortion/tree/master/LD" rel="noopener ugc nofollow" target="_blank">这里</a>，这里<a class="ae jh" href="https://github.com/Cuda-Chen/camera-Calibration" rel="noopener ugc nofollow" target="_blank">这里</a>，这里<a class="ae jh" href="https://github.com/Cuda-Chen/lens-distortion/tree/master/0percent" rel="noopener ugc nofollow" target="_blank">这里</a>)虽然我花了一些时间来掌握它，甚至被我的导师“注意到”我完全没有按计划进行(如此尴尬…)，但我最终学会了如何使用OpenCV，并感受到了它的强大。您可以将OpenCV集成到您现有的C++项目中，使其具有显著的功能和快速的处理速度。更重要的是，通过浏览它的源代码，您可以了解每种图像处理方法的理论和实现。</p><p id="0c89" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">久而久之，OpenCV自带大量功能，深度学习也不例外。从3.4版本开始，OpenCV逐渐增加了深度学习推理的功能。然而，它仍然没有通过其名为<code class="du ko kp kq kr b">DNN</code>的API提供训练深度学习模型。所以通常用OpenCV创建深度学习模型会存在以下工作流程:</p><p id="d8b1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">用其他框架训练你的模型-&gt; <br/>用OpenCV readNet类函数加载训练好的模型- &gt; <br/>进行预测</p><p id="65b2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">YOLO是一个著名的对象检测神经网络架构，我很佩服pjreddie可以用C和他的意志编码这样的网络。然而，我们在实践中通常会转换重量文件，因为我们的老板说我们必须使用某种编程语言。另外，我听说在推理YOLO的时候使用OpenCV <code class="du ko kp kq kr b">DNN</code>模块比使用pjreddie或者AlexeyAB的verions要快很多(这里<a class="ae jh" href="https://www.learnopencv.com/deep-learning-based-object-detection-using-yolov3-with-opencv-python-c/" rel="noopener ugc nofollow" target="_blank">见</a>)，尤其是在CPU上。综上所述，我写这篇文章作为使用OpenCV <code class="du ko kp kq kr b">DNN</code>模型推理YOLO的记录，并在pjreddic、AlexeyAB和OpenCV之间做了一个推理时间比较的实验。</p><h1 id="35e8" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">本文的目标</h1><p id="06f3" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">在本文中，我将:</p><ol class=""><li id="e1b6" class="ks kt hi il b im in iq ir ki ku kk kv km kw jg kx ky kz la bi translated">在OpenCV上做一个鱼YOLO物体检测的例子(你可以在你自定义的物体检测工作上随意复制粘贴我的代码)。</li><li id="f952" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg kx ky kz la bi translated">在pjreddid、AlexeyAB和OpenCV YOLO推理之间做一个执行时间实验。</li></ol><h1 id="81c0" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">准备</h1><ol class=""><li id="3bfa" class="ks kt hi il b im kg iq kh ki lg kk lh km li jg kx ky kz la bi translated">从<a class="ae jh" href="https://github.com/Cuda-Chen/fish-opencv-yolo-python" rel="noopener ugc nofollow" target="_blank">这里</a>克隆我的回购。</li><li id="d7d5" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg kx ky kz la bi translated">从<a class="ae jh" href="https://drive.google.com/file/d/1L6JgzbFhC7Bb_5w_V-stAkPSgMplvsmq/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">我的Google Drive </a>下载预训练的权重，并放入<code class="du ko kp kq kr b">yolo-fish</code>目录。</li><li id="45cd" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg kx ky kz la bi translated">创建<code class="du ko kp kq kr b">conda</code>虚拟环境并安装依赖项:</li></ol><pre class="lj lk ll lm fd ln kr lo lp aw lq bi"><span id="6ae3" class="lr jj hi kr b fi ls lt l lu lv">$ conda create -n fish-opencv-yolo-python python=3.6 pip <br/>$ conda activate fish-opencv-yolo-python<br/>$ pip install -r requirements.txt</span></pre><p id="2d00" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">4.激活虚拟环境。</p><p id="3cb8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi">5. Run prediction of <code class="du ko kp kq kr b">七星斑.jpg</code> with this command: <code class="du ko kp kq kr b">$ python yolo.py --image ./images/七星斑.jpg --yolo yolo-fish</code></p><p id="dade" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">如果没有任何问题，您应该会看到一个弹出窗口:</p><figure class="lj lk ll lm fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es lw"><img src="../Images/ade26158fa921818fe80680c907c7afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T1HOE6kM02XiT3-UGTERFg.jpeg"/></div></div></figure><h1 id="e465" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">说明</h1><p id="3f3e" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">当然，你想知道如何使用OpenCV进行YOLO物体检测。在这一部分，我将逐一写出要点。</p><h2 id="143c" class="lr jj hi bd jk me mf mg jo mh mi mj js ki mk ml jw kk mm mn ka km mo mp ke mq bi translated">读取重量</h2><p id="576c" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">OpenCV提供了许多类似于<code class="du ko kp kq kr b">readNet</code>的函数，以便于读取其他框架训练的权重。在版本4.1.1中，OpenCV支持以下框架格式:</p><ul class=""><li id="1baf" class="ks kt hi il b im in iq ir ki ku kk kv km kw jg mr ky kz la bi translated">咖啡</li><li id="bb38" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">张量流</li><li id="e4c1" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">火炬</li><li id="0336" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">黑暗网络</li><li id="9660" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">开放葡萄酒</li><li id="11f2" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">ONNX</li></ul><p id="c22f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">我将使用我预先训练的暗网模型，因此选择<code class="du ko kp kq kr b">readNetFromDarknet()</code>读取暗网权重。</p><h2 id="afeb" class="lr jj hi bd jk me mf mg jo mh mi mj js ki mk ml jw kk mm mn ka km mo mp ke mq bi translated">创建斑点(张量)</h2><p id="2415" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">在OpenCV的DNN模块中，它要求你的输入变换为一个blob，或者其他神经网络框架中的张量。为了创建一个斑点，我使用<code class="du ko kp kq kr b">blobFromImage()</code>函数来创建一个4维的斑点，如下所示:</p><pre class="lj lk ll lm fd ln kr lo lp aw lq bi"><span id="f93c" class="lr jj hi kr b fi ls lt l lu lv">blob = cv.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)</span></pre><p id="bf40" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">具体来说，<code class="du ko kp kq kr b">blobFromImage</code>的一些参数描述如下:</p><ul class=""><li id="73ce" class="ks kt hi il b im in iq ir ki ku kk kv km kw jg mr ky kz la bi translated"><code class="du ko kp kq kr b">scalefactor</code>:将图像的每个像素与其值相乘。我在这里设置了<code class="du ko kp kq kr b">1 / 255.0</code>，因为我想获得一个类型为<code class="du ko kp kq kr b">CV_32F</code>的blob(或者Python中的<code class="du ko kp kq kr b">numpy.float32</code>)。</li><li id="7a30" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated"><code class="du ko kp kq kr b">size</code>:将图像调整到一定大小。这里我设置了<code class="du ko kp kq kr b">(416, 416)</code>，因为YOLO想要接收这个大小。</li><li id="a0fb" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">虽然我的YOLO网络是用大小为<code class="du ko kp kq kr b">(608, 608)</code>训练的，但是我设置了上面的值，因为<code class="du ko kp kq kr b">(608, 608)</code>在OpenCV上不能正常工作。更多描述见琐事部分。</li><li id="5054" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated"><code class="du ko kp kq kr b">swapRB=True</code> : OpenCV将彩色图像保存为<code class="du ko kp kq kr b">BGR</code>而不是<code class="du ko kp kq kr b">RGB</code>格式，而YOLO想要接收<code class="du ko kp kq kr b">RGB</code>格式的图片。因此，我为此设置了<code class="du ko kp kq kr b">True</code>。</li><li id="e8d6" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated"><code class="du ko kp kq kr b">crop</code>:调整图像大小后裁剪图像。我不想裁剪图像，所以我把这个设置为<code class="du ko kp kq kr b">False</code>。</li></ul><p id="bef6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">在这个操作之后，我们得到一个具有<code class="du ko kp kq kr b">NCHW</code>格式的4-D斑点。</p><h2 id="ab6f" class="lr jj hi bd jk me mf mg jo mh mi mj js ki mk ml jw kk mm mn ka km mo mp ke mq bi translated">通过YOLO网络执行前向传递</h2><p id="356e" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">在我向前传递之前，我必须通过以下方式从我的YOLO模型中确定输出层名称:</p><pre class="lj lk ll lm fd ln kr lo lp aw lq bi"><span id="11cc" class="lr jj hi kr b fi ls lt l lu lv">ln = net.getLayerNames()<br/>ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]</span></pre><p id="3683" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">之后，我可以向前传球:</p><pre class="lj lk ll lm fd ln kr lo lp aw lq bi"><span id="44c1" class="lr jj hi kr b fi ls lt l lu lv">net.setInput(blob)<br/>start = time.time()<br/>layerOutputs = net.forward(ln)<br/>end = time.time()</span><span id="2344" class="lr jj hi kr b fi ms lt l lu lv"># show execution time information of YOLO<br/>print("[INFO] YOLO took {:.6f} seconds.".format(end - start))<!-- --> </span></pre><h2 id="5fa6" class="lr jj hi bd jk me mf mg jo mh mi mj js ki mk ml jw kk mm mn ka km mo mp ke mq bi translated">非极大值抑制</h2><p id="b81a" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">对于物体检测，我们通常使用非极大值抑制来选择最适合标记被检测物体位置的包围盒。虽然YOLO没有这样做，但我们可以反过来设计一个非极大值抑制:</p><pre class="lj lk ll lm fd ln kr lo lp aw lq bi"><span id="1f20" class="lr jj hi kr b fi ls lt l lu lv">idxs = cv.dnn.NMSBoxes(boxes, confidences, args["confidence"],<br/>        args["threshold"])</span></pre><p id="2c79" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">你要做的只是提交包围盒(<code class="du ko kp kq kr b">box</code>)、置信度、<code class="du ko kp kq kr b">confidences</code>、置信度阈值和NMS阈值。</p><h1 id="5098" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">推理时间比较</h1><p id="621f" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">在引言部分，我提到过YOLO物体检测比pjreddie和AlexeyAB写的darknet运行得快得多。因此，我想做一个实验来证明这个事实是真的。</p><h2 id="0474" class="lr jj hi bd jk me mf mg jo mh mi mj js ki mk ml jw kk mm mn ka km mo mp ke mq bi translated">规格</h2><p id="01b8" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">硬件规格如下所示:</p><ul class=""><li id="063d" class="ks kt hi il b im in iq ir ki ku kk kv km kw jg mr ky kz la bi translated">CPU:酷睿i5–3230m</li><li id="dfc5" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">内存:16GB</li></ul><p id="7076" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">我在这里列出了软件规格:</p><ul class=""><li id="0f49" class="ks kt hi il b im in iq ir ki ku kk kv km kw jg mr ky kz la bi translated">操作系统:CentOS 7.6</li><li id="dce7" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">OpenCV</li><li id="5810" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">皮雷迪暗网</li><li id="4031" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">阿列克谢耶布的暗网</li></ul><blockquote class="if ig ih"><p id="b6d1" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">你可以在下面的链接中找到pjreddie和AlexeyAB的作品:</p><p id="09ef" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">pjreddie:<a class="ae jh" href="https://github.com/pjreddie/darknet" rel="noopener ugc nofollow" target="_blank">https://github.com/pjreddie/darknet</a></p><p id="b29e" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">https://github.com/AlexeyAB/darknet</p></blockquote><p id="860c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">这里显示了一些受控变量:</p><ul class=""><li id="b52b" class="ks kt hi il b im in iq ir ki ku kk kv km kw jg mr ky kz la bi">Input image: 七星斑.jpg</li><li id="b380" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">YOLO输入尺寸:608 x 608</li><li id="e194" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">信心:0.25</li><li id="9868" class="ks kt hi il b im lb iq lc ki ld kk le km lf jg mr ky kz la bi translated">阈值:0.4</li></ul><p id="06a0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">我在下面做一个比较时间表:</p><figure class="lj lk ll lm fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mt"><img src="../Images/c94538aa58ab1e4efb4fe9d89433e45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kwfz95yO_z1DloUcpOuDUg.png"/></div></div></figure><p id="2787" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">你可以发现运行在OpenCV上的Darknet运行速度最快。</p><h1 id="de24" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">结论</h1><p id="a183" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">在本文中，我通过一个鱼对象图像的例子演示了如何使用OpenCV进行自定义数据YOLO对象检测。我还展示了在OpenCV中使用YOLO物体检测时的一些概要。最后，我做了一个推理时间比较，显示OpenCV版本运行最快。</p><h1 id="8c2e" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">琐事</h1><p id="2286" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">我发现使用416x416分辨率作为输入会产生比OpenCV中608x608分辨率更令人满意的结果。</p><p id="d01c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi">Taking DSC_0061.JPG and 七星斑.jpg as examples:</p><ol class=""><li id="ec67" class="ks kt hi il b im in iq ir ki ku kk kv km kw jg kx ky kz la bi translated">将分辨率设置为608x608</li></ol><div class="lj lk ll lm fd ab cb"><figure class="mu lx mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/57e71907368a70c02e590424cf05a70e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1408/format:webp/1*N8ubpPP1E2vJM9EPRBmwww.jpeg"/></div></figure><figure class="mu lx na mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/8d3262341905c9f3834f79e1b266651e.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*J_5Gd0ulEcik7itqWMbQbQ.jpeg"/></div></figure></div><p id="e63c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">2.将分辨率设置为416x416</p><div class="lj lk ll lm fd ab cb"><figure class="mu lx mv mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/bc5204a21a8e82a092711cdab6256276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1408/format:webp/1*T1HOE6kM02XiT3-UGTERFg.jpeg"/></div></figure><figure class="mu lx na mw mx my mz paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/586db9c1c53e47f17bbb21574f949897.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*fCQdyHNod4cjQyu_bDwPpg.jpeg"/></div></figure></div><p id="dd71" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ki iv iw ix kk iz ja jb km jd je jf jg hb bi translated">当分辨率设置为416x416时，您可以实现更理想的结果(边界框更精确地定位鱼)。</p><h1 id="ad38" class="ji jj hi bd jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf bi translated">特别感谢</h1><p id="66aa" class="pw-post-body-paragraph ii ij hi il b im kg io ip iq kh is it ki kj iw ix kk kl ja jb km kn je jf jg hb bi translated">如果<a class="ae jh" href="https://www.pyimagesearch.com/" rel="noopener ugc nofollow" target="_blank"> pyimagesearch </a>没有写一篇教你如何使用OpenCV进行YOLO物体检测的精彩文章，这篇文章就不会出现。你可以在这篇<a class="ae jh" href="https://www.pyimagesearch.com/2018/11/12/yolo-object-detection-with-opencv/" rel="noopener ugc nofollow" target="_blank">文章</a>中找到他的作品。</p></div></div>    
</body>
</html>