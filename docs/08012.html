<html>
<head>
<title>PYTORCH</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PYTORCH</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pytorch-part1-9a63a063de31?source=collection_archive---------20-----------------------#2020-07-14">https://medium.com/analytics-vidhya/pytorch-part1-9a63a063de31?source=collection_archive---------20-----------------------#2020-07-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/470fa88a4cc50c6f40527e95fa1b5a7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/0*MUM8AS7JmNz1x6T9.jpg"/></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">金色火炬</figcaption></figure><div class=""/><p id="cf6b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PyTorch在深度学习框架的列表中。它有助于通过提高计算速度和降低成本来加速深度学习模型的研究</p><p id="aa6e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在NumPy库中，我们有多维数组，而在PyTorch中，我们有张量。</p><p id="f4f3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用Python在PyTorch中训练模型，然后通过<strong class="is hu"> TorchScript </strong>将模型导出到Python不可用的生产环境中。</p><p id="0020" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PyTorch支持<strong class="is hu">多种类型的张量</strong>，包括:</p><p id="5b70" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">浮点处理器:32位浮点</p><p id="b00f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">DoubleTensor: 64位浮点数</p><p id="7206" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">半张量:16位浮点型</p><p id="b820" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">IntTensor: 32位int</p><p id="f9f6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">long tenser:64位整数</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="cc37" class="jx jy ht jt b fi jz ka l kb kc">import torch<br/>import numpy as np</span><span id="94b4" class="jx jy ht jt b fi kd ka l kb kc">a=np.array(1)<br/>b=torch.tensor(1)<br/><br/>a,b,type(a),type(b)</span><span id="d9a5" class="jx jy ht jt b fi kd ka l kb kc">(array(1), tensor(1), numpy.ndarray, torch.Tensor)</span></pre><p id="a9b9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> Numpy操作</strong></p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="9a55" class="jx jy ht jt b fi jz ka l kb kc">a=np.array(2)<br/>b=np.array(1)<br/><br/># addition<br/>print(a+b)<br/><br/># subtraction<br/>print(b-a)<br/><br/># multiplication<br/>print(a*b)<br/><br/># division<br/>print(a/b)</span><span id="e4e4" class="jx jy ht jt b fi kd ka l kb kc">3<br/>-1<br/>2<br/>2.0</span></pre><p id="1bab" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> Pytorch </strong></p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="f567" class="jx jy ht jt b fi jz ka l kb kc">a=torch.tensor(2)<br/>b=torch.tensor(1)<br/><br/># addition<br/>print(a+b)<br/><br/># subtraction<br/>print(b-a)<br/><br/># multiplication<br/>print(a*b)<br/><br/># division<br/>print(a/b)</span><span id="04c4" class="jx jy ht jt b fi kd ka l kb kc">tensor(3)<br/>tensor(-1)<br/>tensor(2)<br/>tensor(2)<br/><br/><br/>..\aten\src\ATen\native\BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.</span></pre><p id="352f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">矩阵初始化</strong></p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="2888" class="jx jy ht jt b fi jz ka l kb kc"># matrix of zeros<br/>a = np.ones((3,3))<br/>print(a)<br/>print(a.shape)</span><span id="2ff7" class="jx jy ht jt b fi kd ka l kb kc">[[1. 1. 1.]<br/> [1. 1. 1.]<br/> [1. 1. 1.]]<br/>(3, 3)</span><span id="bb92" class="jx jy ht jt b fi kd ka l kb kc"># matrix of zeros<br/>a = torch.ones((3,3))<br/>print(a)<br/>print(a.shape)</span><span id="cd68" class="jx jy ht jt b fi kd ka l kb kc">tensor([[1., 1., 1.],<br/>        [1., 1., 1.],<br/>        [1., 1., 1.]])<br/>torch.Size([3, 3])</span><span id="f94f" class="jx jy ht jt b fi kd ka l kb kc"># setting the random seed for pytorch<br/>torch.manual_seed(42)<br/># matrix of random numbers<br/>a = torch.randn(3,3)<br/>a</span><span id="56f8" class="jx jy ht jt b fi kd ka l kb kc">tensor([[ 0.3367,  0.1288,  0.2345],<br/>        [ 0.2303, -1.1229, -0.1863],<br/>        [ 2.2082, -0.6380,  0.4617]])</span><span id="c17f" class="jx jy ht jt b fi kd ka l kb kc"># setting the random seed for pytorch and initializing two tensors<br/>torch.manual_seed(42)<br/>a = torch.randn(3,3)<br/>b = torch.randn(3,3)</span><span id="9c3c" class="jx jy ht jt b fi kd ka l kb kc"># matrix addition<br/>print(torch.add(a,b), '\n')<br/><br/># matrix subtraction<br/>print(torch.sub(a,b), '\n')<br/><br/># matrix multiplication<br/>print(torch.mm(a,b), '\n')<br/><br/># matrix division<br/>print(torch.div(a,b), '\n')<br/><br/># original matrix<br/>print(a, '\n')<br/><br/># matrix transpose<br/>torch.t(a)</span><span id="f9f8" class="jx jy ht jt b fi kd ka l kb kc">tensor([[7., 7.],<br/>        [7., 7.]], grad_fn=&lt;AddBackward0&gt;) <br/><br/>tensor([[-5., -5.],<br/>        [-5., -5.]], grad_fn=&lt;SubBackward0&gt;) <br/><br/>tensor([[12., 12.],<br/>        [12., 12.]], grad_fn=&lt;MmBackward&gt;) <br/><br/>tensor([[0.1667, 0.1667],<br/>        [0.1667, 0.1667]], grad_fn=&lt;DivBackward0&gt;) <br/><br/>tensor([[1., 1.],<br/>        [1., 1.]], requires_grad=True) <br/><br/><br/><br/><br/><br/><br/>tensor([[1., 1.],<br/>        [1., 1.]], grad_fn=&lt;TBackward&gt;)</span><span id="9ccd" class="jx jy ht jt b fi kd ka l kb kc">#matrix mul<br/>print(torch.matmul(a,b), '\n')<br/><br/>print(a@b, '\n')<br/><br/>print(torch.mm(a,b), '\n')</span><span id="1120" class="jx jy ht jt b fi kd ka l kb kc">tensor([[12., 12.],<br/>        [12., 12.]], grad_fn=&lt;MmBackward&gt;) <br/><br/>tensor([[12., 12.],<br/>        [12., 12.]], grad_fn=&lt;MmBackward&gt;) <br/><br/>tensor([[12., 12.],<br/>        [12., 12.]], grad_fn=&lt;MmBackward&gt;)</span><span id="9eb3" class="jx jy ht jt b fi kd ka l kb kc">torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))</span><span id="03e6" class="jx jy ht jt b fi kd ka l kb kc">tensor(7)</span></pre><p id="60f8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">串联</strong></p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="ed61" class="jx jy ht jt b fi jz ka l kb kc"># initializing two tensors<br/>a = torch.tensor([[1,2],[3,4]])<br/>b = torch.tensor([[5,6],[7,8]])<br/>print(a, '\n')<br/>print(b, '\n')<br/>print(torch.cat((a,b)))<br/><br/>print(torch.cat((a,b),dim=0))<br/><br/>print(torch.cat((a,b),dim=1))</span><span id="dbc9" class="jx jy ht jt b fi kd ka l kb kc">tensor([[1, 2],<br/>        [3, 4]]) <br/><br/>tensor([[5, 6],<br/>        [7, 8]]) <br/><br/>tensor([[1, 2],<br/>        [3, 4],<br/>        [5, 6],<br/>        [7, 8]])<br/>tensor([[1, 2],<br/>        [3, 4],<br/>        [5, 6],<br/>        [7, 8]])<br/>tensor([[1, 2, 5, 6],<br/>        [3, 4, 7, 8]])</span></pre><p id="a3fd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">重塑张量</strong></p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="e751" class="jx jy ht jt b fi jz ka l kb kc"># setting the random seed for pytorch<br/>torch.manual_seed(42)<br/># initializing tensor<br/>a = torch.randn(2,4)<br/>print(a)<br/>a.shape</span><span id="c126" class="jx jy ht jt b fi kd ka l kb kc">tensor([[ 0.3367,  0.1288,  0.2345,  0.2303],<br/>        [-1.1229, -0.1863,  2.2082, -0.6380]])<br/><br/><br/><br/><br/><br/>torch.Size([2, 4])</span><span id="11a2" class="jx jy ht jt b fi kd ka l kb kc"># reshaping tensor<br/>b = a.reshape(1,8)<br/>print(b)<br/>b.shape</span><span id="4200" class="jx jy ht jt b fi kd ka l kb kc">tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229, -0.1863,  2.2082, -0.6380]])<br/><br/><br/><br/><br/><br/>torch.Size([1, 8])</span></pre><p id="c7d4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> Numpy到张量的转换</strong></p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="e843" class="jx jy ht jt b fi jz ka l kb kc"># initializing a numpy array<br/>a = np.array([[1,2],[3,4]])<br/>print(a, '\n')<br/><br/># converting the numpy array to tensor<br/>tensor = torch.from_numpy(a)<br/>print(tensor)</span><span id="7b8f" class="jx jy ht jt b fi kd ka l kb kc">[[1 2]<br/> [3 4]] <br/><br/>tensor([[1, 2],<br/>        [3, 4]], dtype=torch.int32)</span></pre><p id="5bcc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">张量到数字的转换</strong></p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="9d4d" class="jx jy ht jt b fi jz ka l kb kc">numpy= tensor.numpy()<br/>print(numpy)</span><span id="2872" class="jx jy ht jt b fi kd ka l kb kc">[[1 2]<br/> [3 4]]</span></pre><p id="95f7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">除了CharTensor之外，CPU上的所有张量都支持与NumPy相互转换。</p><p id="e8c2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> CUDA张量</strong></p><p id="5915" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">可以使用将张量移动到任何设备上。方法。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="0015" class="jx jy ht jt b fi jz ka l kb kc"># let us run this cell only if CUDA is available<br/># We will use ``torch.device`` objects to move tensors in and out of GPU<br/>if torch.cuda.is_available():<br/>    device = torch.device("cuda")          # a CUDA device object<br/>    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU<br/>    x = x.to(device)                       # or just use strings ``.to("cuda")``<br/>    z = x + y<br/>    print(z)<br/>    print(z.to("cpu", torch.double))       # ``.to`` can also change dtype together<br/>else:<br/>    print("No tensor operation using cuda")</span><span id="f99b" class="jx jy ht jt b fi kd ka l kb kc">No tensor operation using cuda</span></pre><p id="7abc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PyTorch使用了一种叫做<strong class="is hu">自动微分</strong>的技术。它记录我们正在执行的所有操作，并回放它以计算梯度。这种技术有助于我们在每个历元上节省时间，因为我们正在计算向前传递本身的梯度</p><p id="1291" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">将requires_grad指定为True将确保每当我们对其执行一些操作时，梯度都被存储。</p><p id="de1c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">c =均值(b)=ψ(a+5)/4</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="f1df" class="jx jy ht jt b fi jz ka l kb kc"># initializing a tensor<br/>a = torch.ones((2,2), requires_grad=True)<br/>a</span><span id="c412" class="jx jy ht jt b fi kd ka l kb kc">tensor([[1., 1.],<br/>        [1., 1.]], requires_grad=True)</span><span id="8e89" class="jx jy ht jt b fi kd ka l kb kc"># performing operations on the tensor<br/>b = a + 5<br/>c = b.mean()<br/>print(b,c)</span><span id="91c0" class="jx jy ht jt b fi kd ka l kb kc">tensor([[6., 6.],<br/>        [6., 6.]], grad_fn=&lt;AddBackward0&gt;) tensor(6., grad_fn=&lt;MeanBackward0&gt;)</span></pre><p id="4f2a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，c w r t a的导数是，因此梯度矩阵是0.25。L</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="b08e" class="jx jy ht jt b fi jz ka l kb kc"># back propagating<br/>c.backward()<br/><br/># computing gradients<br/>print(a.grad)</span><span id="f6b5" class="jx jy ht jt b fi kd ka l kb kc">tensor([[0.2500, 0.2500],<br/>        [0.2500, 0.2500]])</span></pre><p id="4722" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">自动签名的</strong>模块帮助我们计算正向通道本身的梯度，这节省了一个历元的大量计算时间。</p><p id="324f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> Optim模块:</strong>py torch中的Optim模块为构建神经网络时使用的大多数优化器预先编写了代码。我们只需要导入它们，然后就可以用它们来构建模型。</p><p id="3a1f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">nn模块PyTorch中的<strong class="is hu">自动签名模块</strong>帮助我们在模型中继续定义计算图形。但是，当我们处理复杂的神经网络时，仅仅使用自动签名的模块可能是低级的。</p><p id="0e15" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这些情况下，我们可以利用神经网络模块。这定义了一组函数，类似于神经网络的层，它从前一状态获取输入并产生输出。</p><p id="6ce9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">在PyTorch中从头开始构建神经网络</strong></p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="f959" class="jx jy ht jt b fi jz ka l kb kc">#Input tensor<br/>X = torch.Tensor([[1,0,1,0],[1,0,1,1],[0,1,0,1]])<br/><br/>#Output<br/>y = torch.Tensor([[1],[1],[0]])<br/><br/>print(X, '\n')<br/>print(y)</span><span id="58d5" class="jx jy ht jt b fi kd ka l kb kc">tensor([[1., 0., 1., 0.],<br/>        [1., 0., 1., 1.],<br/>        [0., 1., 0., 1.]]) <br/><br/>tensor([[1.],<br/>        [1.],<br/>        [0.]])</span></pre><p id="e916" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">将作为激活函数的<strong class="is hu"> sigmoid函数</strong>和将在反向传播步骤中帮助我们的sigmoid函数的导数:</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="a875" class="jx jy ht jt b fi jz ka l kb kc">#Sigmoid Function<br/>def sigmoid (x):<br/>    return 1/(1 + torch.exp(-x))<br/><br/>#Derivative of Sigmoid Function/<br/>def derivatives_sigmoid(x):<br/>    return sigmoid(x) * (1 - sigmoid(x))</span><span id="bf58" class="jx jy ht jt b fi kd ka l kb kc">#Variable initialization<br/>epoch=1000 #Setting training iterations<br/>lr=0.1 #Setting learning rate<br/>inputlayer_neurons = X.shape[1] #number of features in data set<br/>print(inputlayer_neurons,X)<br/>hiddenlayer_neurons = 50 #number of hidden layer neurons<br/>output_neurons = 1 #number of neurons in output layer</span><span id="b491" class="jx jy ht jt b fi kd ka l kb kc">4 tensor([[1., 0., 1., 0.],<br/>        [1., 0., 1., 1.],<br/>        [0., 1., 0., 1.]])</span><span id="96f3" class="jx jy ht jt b fi kd ka l kb kc">#weight and bias initialization<br/>wh=torch.randn(inputlayer_neurons, hiddenlayer_neurons).type(torch.FloatTensor)<br/>bh=torch.randn(1, hiddenlayer_neurons).type(torch.FloatTensor)<br/>wout=torch.randn(hiddenlayer_neurons, output_neurons)<br/>bout=torch.randn(1, output_neurons)</span><span id="2ec5" class="jx jy ht jt b fi kd ka l kb kc">for i in range(epoch):<br/>    #Forward Propogation<br/>    #y=mx+c<br/>    #Forward Propogation<br/>    hidden_layer_input1 = torch.mm(X, wh)<br/>    hidden_layer_input = hidden_layer_input1 + bh<br/>    hidden_layer_activations = sigmoid(hidden_layer_input)<br/><br/>    output_layer_input1 = torch.mm(hidden_layer_activations, wout)<br/>    output_layer_input = output_layer_input1 + bout<br/>    output = sigmoid(output_layer_input)<br/>    <br/>    #calculate the slope<br/>    #y-yhat<br/>    E = y-output<br/>    slope_output_layer = derivatives_sigmoid(output)<br/>    slope_hidden_layer = derivatives_sigmoid(hidden_layer_activations)<br/>    <br/>    #Error<br/>    d_output = E * slope_output_layer<br/>    Error_at_hidden_layer = torch.mm(d_output, wout.t())<br/>    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer<br/>    <br/>    #Calculate weights an biases at outout<br/>    wout += torch.mm(hidden_layer_activations.t(), d_output) *lr<br/>    bout += d_output.sum() *lr<br/>    <br/>    #Update weights<br/>    wh += torch.mm(X.t(), d_hiddenlayer) *lr<br/>    bh += d_output.sum() *lr</span></pre><p id="7855" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们正在计算输出，最后，在反向传播步骤中。</p><p id="1d49" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们正在计算误差。然后，我们将使用该误差更新权重和偏差。</p><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="a2a7" class="jx jy ht jt b fi jz ka l kb kc">print('actual :\n', y, '\n')<br/>print('predicted :\n', output)</span><span id="91fa" class="jx jy ht jt b fi kd ka l kb kc">actual :<br/> tensor([[1.],<br/>        [1.],<br/>        [0.]]) <br/><br/>predicted :<br/> tensor([[0.9992],<br/>        [0.9913],<br/>        [0.0087]])</span><span id="1236" class="jx jy ht jt b fi kd ka l kb kc"># importing the libraries<br/>import pandas as pd<br/>import numpy as np<br/>from skimage.io import imread<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import accuracy_score</span><span id="7db7" class="jx jy ht jt b fi kd ka l kb kc"># loading dataset<br/>train = pd.read_csv('train.csv')<br/>test = pd.read_csv('test.csv')</span><span id="412f" class="jx jy ht jt b fi kd ka l kb kc">train.head()</span></pre><figure class="jo jp jq jr fd hk er es paragraph-image"><div class="er es ke"><img src="../Images/d2d653cb1359cdd1e20d96c156ef06cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*PPmOMd_QWX7VWa5vA4I6iw.png"/></div></figure><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="dfff" class="jx jy ht jt b fi jz ka l kb kc"># random number generator<br/>seed = 128<br/>rng = np.random.RandomState(seed)</span><span id="09d7" class="jx jy ht jt b fi kd ka l kb kc"># print an image<br/>img_name = rng.choice(train['id'])<br/><br/>filepath = 'train/' + str(img_name) + '.png'<br/><br/>img = imread(filepath, as_gray=True)<br/>img = img.astype('float32')<br/><br/>plt.figure(figsize=(5,5))<br/>plt.imshow(img, cmap='gray')</span><span id="3279" class="jx jy ht jt b fi kd ka l kb kc">&lt;matplotlib.image.AxesImage at 0x203a40aecc8&gt;</span></pre><figure class="jo jp jq jr fd hk er es paragraph-image"><div class="er es kf"><img src="../Images/b5ad0a068b55a0945b7a7af9aa7e46f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*SUU4w6z3Yb7z1SjCrebLRw.png"/></div></figure><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="5f08" class="jx jy ht jt b fi jz ka l kb kc">train_img = []<br/>for img in train['id']:<br/>    image_path = 'train/' + str(img_name) + '.png'<br/>    img = imread(image_path, as_gray=True)<br/>    img = img.astype('float32')<br/>    train_img.append(img)<br/>    <br/>train_x = np.array(train_img)<br/>train_x.shape</span><span id="27cf" class="jx jy ht jt b fi kd ka l kb kc">(60000, 28, 28)</span><span id="f7d6" class="jx jy ht jt b fi kd ka l kb kc">#train dataset<br/>train_x = train_x/train_x.max()<br/>train_x = train_x.reshape(-1, 28*28).astype('float32')<br/>train_x.shape</span><span id="966e" class="jx jy ht jt b fi kd ka l kb kc">(60000, 784)</span><span id="2b49" class="jx jy ht jt b fi kd ka l kb kc">#test dataset<br/>train_y = train['label'].values<br/>train_y</span><span id="dffd" class="jx jy ht jt b fi kd ka l kb kc">array([9, 0, 0, ..., 3, 0, 5], dtype=int64)</span><span id="df7a" class="jx jy ht jt b fi kd ka l kb kc">#train and validation dataset<br/>train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size = 0.1, stratify = train_y)<br/>(train_x.shape, train_y.shape), (val_x.shape, val_y.shape)</span><span id="3431" class="jx jy ht jt b fi kd ka l kb kc">(((54000, 784), (54000,)), ((6000, 784), (6000,)))</span><span id="971b" class="jx jy ht jt b fi kd ka l kb kc">import torch<br/>from torch.autograd import Variable<br/>from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential<br/>from torch.optim import Adam</span><span id="c0bc" class="jx jy ht jt b fi kd ka l kb kc"># number of neurons in each layer<br/>input_num_units = 28*28<br/>hidden_num_units = 500<br/>output_num_units = 10<br/><br/># set remaining variables<br/>epochs = 20<br/>learning_rate = 0.0005</span><span id="fbcf" class="jx jy ht jt b fi kd ka l kb kc"># define model<br/>model = Sequential(Linear(input_num_units, hidden_num_units),<br/>                   ReLU(),<br/>                   Linear(hidden_num_units, output_num_units))<br/># loss function<br/>loss_fn = CrossEntropyLoss()<br/><br/># define optimization algorithm<br/>optimizer = Adam(model.parameters(), lr=learning_rate)</span><span id="2ac8" class="jx jy ht jt b fi kd ka l kb kc">train_losses = []<br/>val_losses = []<br/>for epoch in range(epochs):<br/>    avg_cost = 0<br/>    <br/>    x, y = Variable(torch.from_numpy(train_x)), Variable(torch.from_numpy(train_y), requires_grad=False)<br/>    x_val, y_val = Variable(torch.from_numpy(val_x)), Variable(torch.from_numpy(val_y), requires_grad=False)<br/>    pred = model(x)<br/>    pred_val = model(x_val)<br/><br/>    # get loss<br/>    loss = loss_fn(pred, y)<br/>    loss_val = loss_fn(pred_val, y_val)<br/>    train_losses.append(loss)<br/>    val_losses.append(loss_val)<br/><br/>    # perform backpropagation<br/>    loss.backward()<br/>    optimizer.step()<br/>    avg_cost = avg_cost + loss.data<br/><br/>    if (epoch%2 != 0):<br/>        print(epoch+1, avg_cost)</span><span id="2c5a" class="jx jy ht jt b fi kd ka l kb kc">2 tensor(2.3062)<br/>4 tensor(2.3107)<br/>6 tensor(2.3114)<br/>8 tensor(2.3124)<br/>10 tensor(2.3191)<br/>12 tensor(2.3263)<br/>14 tensor(2.3267)<br/>16 tensor(2.3228)<br/>18 tensor(2.3179)<br/>20 tensor(2.3136)</span><span id="b0c5" class="jx jy ht jt b fi kd ka l kb kc">plt.plot(train_losses, label='Training loss')<br/>plt.plot(val_losses, label='Validation loss')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="jo jp jq jr fd hk er es paragraph-image"><div class="er es kg"><img src="../Images/d476a06e11b9bf3ee662a39aaa8a114b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*Vj3jx8D2eqRTpeclO4Gfiw.png"/></div></figure><pre class="jo jp jq jr fd js jt ju jv aw jw bi"><span id="b529" class="jx jy ht jt b fi jz ka l kb kc"># get training accuracy<br/>x, y = Variable(torch.from_numpy(train_x)), Variable(torch.from_numpy(train_y), requires_grad=False)<br/>pred = model(x)<br/><br/>final_pred = np.argmax(pred.data.numpy(), axis=1)<br/><br/>accuracy_score(train_y, final_pred)</span><span id="c51c" class="jx jy ht jt b fi kd ka l kb kc">0.1</span><span id="ef52" class="jx jy ht jt b fi kd ka l kb kc"># get validation accuracy<br/>x, y = Variable(torch.from_numpy(val_x)), Variable(torch.from_numpy(val_y), requires_grad=False)<br/>pred = model(x)<br/>final_pred = np.argmax(pred.data.numpy(), axis=1)<br/><br/>accuracy_score(val_y, final_pred)</span><span id="bb41" class="jx jy ht jt b fi kd ka l kb kc">0.1</span><span id="69fa" class="jx jy ht jt b fi kd ka l kb kc"># loading test images<br/>test_img = []<br/>for img_name in test['id']:<br/>    image_path = 'test/' + str(img_name) + '.png'<br/>    img = imread(image_path, as_gray=True)<br/>    img = img.astype('float32')<br/>    test_img.append(img)<br/><br/>test_x = np.array(test_img)<br/>test_x.shape</span><span id="ef32" class="jx jy ht jt b fi kd ka l kb kc">(10000, 28, 28)</span><span id="f013" class="jx jy ht jt b fi kd ka l kb kc"># converting the images to 1-D<br/>test_x = test_x/train_x.max()<br/>test_x = test_x.reshape(-1, 28*28).astype('float32')<br/>test_x.shape</span><span id="3cb3" class="jx jy ht jt b fi kd ka l kb kc">(10000, 784)</span><span id="6911" class="jx jy ht jt b fi kd ka l kb kc"># getting the prediction for test images<br/>prediction = np.argmax(model(torch.from_numpy(test_x)).data.numpy(), axis=1)</span><span id="a0ce" class="jx jy ht jt b fi kd ka l kb kc">prediction</span><span id="b0a3" class="jx jy ht jt b fi kd ka l kb kc">array([2, 1, 7, ..., 1, 7, 2], dtype=int64)</span></pre></div></div>    
</body>
</html>