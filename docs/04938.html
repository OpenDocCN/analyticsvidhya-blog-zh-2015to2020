<html>
<head>
<title>Mushroom Dataset — Data Exploration and Model Analysis (OneHot Encoded)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">蘑菇数据集—数据探索和模型分析(OneHot编码)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mushroom-dataset-data-exploration-and-model-analysis-onehotvectr-encoded-c7fd304112dd?source=collection_archive---------6-----------------------#2020-04-06">https://medium.com/analytics-vidhya/mushroom-dataset-data-exploration-and-model-analysis-onehotvectr-encoded-c7fd304112dd?source=collection_archive---------6-----------------------#2020-04-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es im"><img src="../Images/c2656a6d6a88572ea472e91f31a7f9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Tgz10jZ7nXkeRbSL"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">克里斯里德在<a class="ae jc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="b766" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">本文将向您展示分类数据集的不同数据探索技术。我们将演示处理空值(如果有的话)的技术、分类值到数字特征的转换以及数据集的聚类趋势检查。</p><p id="32c8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果你要找整个代码库，可以在这里找到:<a class="ae jc" href="https://www.kaggle.com/nvnvashisth/classification-and-model-analysis-one-hot-vector" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj">【分类与模型分析(一热向量)</strong> </a> <strong class="jf hj">。</strong></p><p id="8b31" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">为了演示所有的方法，我们考虑了一个著名的<a class="ae jc" href="https://www.kaggle.com/uciml/mushroom-classification" rel="noopener ugc nofollow" target="_blank"> Kaggle竞赛</a>的数据集。该数据集的详细描述可在网站(Kaggle)上获得，该网站明确解释了各个特征。数据科学家必须了解数据的背景，才能开始围绕数据构建他/她的假设。</p><p id="aadb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们首先读取文件并查看数据的前5条记录。</p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es kb"><img src="../Images/e7ef3d4d12e61c38fc0a841c33d0fe1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfa4tBgh_cnJO6Jnm-eswg.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">读取蘑菇数据集并显示前5条记录</figcaption></figure><p id="f99f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们详细研究一下数据(数据清理和数据探索)</p><h2 id="c9f2" class="kc kd hi bd ke kf kg kh ki kj kk kl km jo kn ko kp js kq kr ks jw kt ku kv kw bi translated">数据清理和数据探索</h2><p id="cb1c" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">通常，数据科学家总是会检查数据集中是否存在“NA”值，我们在这里也是如此。此外，我们还将检查蘑菇需要分类的不同的可用类别。在我们的例子中，这些类是“有毒的”和“可食用的”。</p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es kb"><img src="../Images/6e4cced9ae7438f1256e347e8d2aec31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U0yK4yD_jgjYRliVn1QSzA.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">计算每列中可用的“NA”</figcaption></figure><p id="3d7d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们需要对机器学习模型可以接受的数据进行预处理。我们在数据集中有不同的类别，不可能使用这些类别进行可视化和模型分析。因此，我们需要将这些数据转换成数字数据。我们已经使用了来自sk learn<a class="ae jc" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj">label encoder</strong></a><strong class="jf hj"/>的great库将所有类别转换为数值。</p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es kb"><img src="../Images/dec9991a44e6dfaf10878d5e2c8dca86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mrB4Q-5ngo7DJeaj0ft6_g.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">LabelEncoder将所有类别转换成数值</figcaption></figure><p id="a4f2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在我们必须进行特征分析，这可以使用相关矩阵来完成。它提供了关于不同特征之间的正相关和负相关的良好信息。我们也可以使用箱线图来检查异常值。</p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es lc"><img src="../Images/eadd709e8b987e6964e0328091c56972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K1X2D4PwyEL7DZctth9ZNw.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">蘑菇数据集的异常值盒图</figcaption></figure><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es ld"><img src="../Images/459ef59795a03e9331d7d0ff67ce60fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zUXC6U9UI3xeV-0iuCVupA.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">描述特征间相关性的相关矩阵</figcaption></figure><p id="85d6" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在我们绘制了箱线图和相关矩阵之后，它给我们的感觉是数据不能被标记编码。这些方法使用不同方法，如均值、协方差和其他数学技术来寻找特征之间的关系。我们的数据集是分类数据，不需要计算平均值或方差。因此，从这篇<a class="ae jc" href="https://scikit-learn.org/stable/modules/preprocessing.html#encoding-categorical-features" rel="noopener ugc nofollow" target="_blank">文章</a>来看，这篇文章提供了关于如何根据dat将分类特征(名词或序数)转换为数字特征(OrdinalEncoder，OneHotEncoder)的最佳信息</p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es le"><img src="../Images/4fb69d192a36815bcea5d9fba42f2c9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tFTfJQaQf7iFP1mAT5Pkww.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">名义分类数据的一键编码器</figcaption></figure><p id="364e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">但是一键编码器总是有一些缺点。它显著增加了维度。因此，我们需要为我们的数据分析寻找可以减少这些维度的可能方法。我们进行了主成分分析(PCA ),结果如下。这里我们可以看到，最大方差由45个组件捕获。</p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es lf"><img src="../Images/7ed5e38a3e229a1cdf930828ae4b3e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4jsmHU9pNtYQ8DCsAcCwNQ.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">主成分分析(PCA)降低维数</figcaption></figure><p id="0532" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们还检查了我们的数据的趋势，这些数据可以在<a class="ae jc" href="https://en.wikipedia.org/wiki/Hopkins_statistic" rel="noopener ugc nofollow" target="_blank">霍普金的统计数据</a>的帮助下进行聚类(详细内容请点击此处)。可以清楚地看到，数据集确实是高度可聚类的，值为0.98。</p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es lg"><img src="../Images/1360f186fa0fb2fe2cc5f9b04ce07e70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7XXoQHQXBQf6dtQ47uD2g.png"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">蘑菇数据集的霍普金统计量</figcaption></figure><p id="c715" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们还寻找了<a class="ae jc" href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" rel="noopener ugc nofollow" target="_blank">侧影得分</a>(详细阅读请点击此处)，从图中可以清楚地看到，最高峰的值大于2。可以推断，我们可以有2个或更多个集群。我们将执行另一个名为<a class="ae jc" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">肘测试</a>的测试(要阅读详细内容，请单击此处)，这将为我们提供可能的集群数。</p><div class="in io ip iq fd ab cb"><figure class="lh ir li lj lk ll lm paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><img src="../Images/8d0d2728c67c36310623887420457b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*RN-dxZKlMR8jFpEIpnH3Ag.png"/></div></figure><figure class="lh ir ln lj lk ll lm paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><img src="../Images/4c4dc5d4c34fe9a1e604402253ba60a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*10kfjthwVd2SqPuintOfkw.png"/></div><figcaption class="iy iz et er es ja jb bd b be z dx lo di lp lq translated">肘法(可能聚类2)和剪影评分(2或更多聚类)</figcaption></figure></div><p id="c2d6" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">然后，我们使用<a class="ae jc" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank"> K-means </a>算法绘制了3个和5个聚类。实现如下所示。</p><div class="in io ip iq fd ab cb"><figure class="lh ir lr lj lk ll lm paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><img src="../Images/262aa66aca10486a3a8d185e78af5c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*Pz0W6C5pRGa4S-ukaFGrhA.png"/></div></figure><figure class="lh ir ls lj lk ll lm paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><img src="../Images/36616499fdc721948b0a264820f12d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*3sQijHmN6D8VWAEmaQMYPA.png"/></div></figure></div></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><h2 id="1cc0" class="kc kd hi bd ke kf kg kh ki kj kk kl km jo kn ko kp js kq kr ks jw kt ku kv kw bi translated">模型分析和比较</h2><p id="c687" class="pw-post-body-paragraph jd je hi jf b jg kx ji jj jk ky jm jn jo kz jq jr js la ju jv jw lb jy jz ka hb bi translated">模型选择是基于流行度进行的，并且经常在实际案例中使用。下面是一些常用的技巧</p><ul class=""><li id="d746" class="lt lu hi jf b jg jh jk jl jo lv js lw jw lx ka ly lz ma mb bi translated"><strong class="jf hj"> K-fold交叉验证:</strong>这是一种对有限数据样本进行重采样的方法，有助于评估机器学习模型。该方法有一个参数“K ”,它可以被认为是组的数量，其中80:20的比率分别用于训练和测试，并且对不同的组重复该操作。</li><li id="ebf5" class="lt lu hi jf b jg mc jk md jo me js mf jw mg ka ly lz ma mb bi translated"><strong class="jf hj">精度和召回:</strong>检查关于假阳性的真实事例比率(TPR/(TPR+FPR))总是明智的，也称为精度值。召回率可以说是算法正确分类的相关结果总数的百分比(TPR/(TPR+FNR))。</li><li id="97aa" class="lt lu hi jf b jg mc jk md jo me js mf jw mg ka ly lz ma mb bi translated"><strong class="jf hj"> F1评分:</strong>人们总是期待拥有精确度和召回率最高的模型。但是当不可能决定的时候，我们必须追求F1的分数，它是精确和回忆的调和。(2 *精度*召回/(精度+召回))</li><li id="2784" class="lt lu hi jf b jg mc jk md jo me js mf jw mg ka ly lz ma mb bi translated"><strong class="jf hj"> ROC: </strong>用TPR(真阳性率)对FPR(假阳性率)绘制接收器工作曲线(ROC)，其中TPR在y轴上，FPR在x轴上。</li></ul><p id="bf8e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">上述标准将用于所有选定的算法，最终算法将从中选出。</p><ul class=""><li id="7f64" class="lt lu hi jf b jg jh jk jl jo lv js lw jw lx ka ly lz ma mb bi translated">高斯朴素贝叶斯分类器</li><li id="ee98" class="lt lu hi jf b jg mc jk md jo me js mf jw mg ka ly lz ma mb bi translated">逻辑回归分类器</li><li id="3175" class="lt lu hi jf b jg mc jk md jo me js mf jw mg ka ly lz ma mb bi translated">决策树分类器</li><li id="bb22" class="lt lu hi jf b jg mc jk md jo me js mf jw mg ka ly lz ma mb bi translated">随机森林分类器</li><li id="d7c9" class="lt lu hi jf b jg mc jk md jo me js mf jw mg ka ly lz ma mb bi translated">xgBoost分类器</li><li id="2c72" class="lt lu hi jf b jg mc jk md jo me js mf jw mg ka ly lz ma mb bi translated">线性判别分类器</li><li id="e140" class="lt lu hi jf b jg mc jk md jo me js mf jw mg ka ly lz ma mb bi translated">高斯过程分类器</li><li id="1cd3" class="lt lu hi jf b jg mc jk md jo me js mf jw mg ka ly lz ma mb bi translated">Ada-boost分类器</li></ul><p id="f8a1" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在我们已经完成了数据探索，现在让我们继续进行模型分析。我们考虑了许多不同的模型进行分析。下面是用于蘑菇分类的不同机器学习算法的实现。</p><pre class="in io ip iq fd mh mi mj mk aw ml bi"><span id="23c8" class="kc kd hi mi b fi mm mn l mo mp"># Importing required classification algorithm</span><span id="27a8" class="kc kd hi mi b fi mq mn l mo mp">from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import cross_val_score<br/>from sklearn.metrics import roc_curve, auc, classification_report<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier<br/>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/>from sklearn.gaussian_process import GaussianProcessClassifier<br/>import xgboost<br/>from sklearn import tree<br/>from sklearn import metrics</span><span id="65c9" class="kc kd hi mi b fi mq mn l mo mp"># Creating object for each class and storing into the array<br/>classifiers = []<br/>nb_model = GaussianNB()<br/>classifiers.append(("Gaussian Naive Bayes Classifier",nb_model))<br/>lr_model= LogisticRegression()<br/>classifiers.append(("Logistic Regression Classifier",lr_model))<br/>dt_model = tree.DecisionTreeClassifier()<br/>classifiers.append(("Decision Tree Classifier",dt_model))<br/>rf_model = RandomForestClassifier()<br/>classifiers.append(("Random Forest Classifier",rf_model))<br/>xgb_model = xgboost.XGBClassifier()<br/>classifiers.append(("XG Boost Classifier",xgb_model))<br/>lda_model = LinearDiscriminantAnalysis()<br/>classifiers.append(("Linear Discriminant Analysis", lda_model))<br/>gp_model =  GaussianProcessClassifier()<br/>classifiers.append(("Gaussian Process Classifier", gp_model))<br/>ab_model =  AdaBoostClassifier()classifiers.append(("AdaBoost Classifier", ab_model))</span><span id="c882" class="kc kd hi mi b fi mq mn l mo mp"># Stores all the scores<br/>cv_scores = []<br/>names = []<br/>for name, clf <strong class="mi hj">in</strong> classifiers:<br/>    print(name)<br/>    clf.fit(X_train, y_train)<br/>    y_prob = clf.predict_proba(X_test)[:,1] <em class="mr"># This will give you positive class prediction probabilities  </em><br/>    y_pred = np.where(y_prob &gt; 0.5, 1, 0) <em class="mr"># This will threshold the probabilities to give class predictions.</em><br/>    print("Model Score : ",clf.score(X_test, y_pred))<br/>    print("Number of mislabeled points from <strong class="mi hj">%d</strong> points : <strong class="mi hj">%d</strong>"% (X_test.shape[0],(y_test!= y_pred).sum()))<br/>    scores = cross_val_score(clf, features, label, cv=10, scoring='accuracy')<br/>    cv_scores.append(scores)<br/>    names.append(name)<br/>    print("Cross validation scores : ",scores.mean())<br/>    confusion_matrix=metrics.confusion_matrix(y_test,y_pred)<br/>    print("Confusion Matrix <strong class="mi hj">\n</strong>",confusion_matrix)<br/>    classification_report = metrics.classification_report(y_test,y_pred)<br/>    print("Classification Report <strong class="mi hj">\n</strong>",classification_report)</span></pre><p id="6220" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在蘑菇数据集上执行上述代码后，我们收到了不同的算法结果，可以用来比较这些算法。最终的算法可以被认为是部署的首选算法。</p><div class="in io ip iq fd ab cb"><figure class="lh ir ms lj lk ll lm paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><img src="../Images/406fea03a8ea35ac09ce2c317581329d.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*yWYiZ90GMBuiXyMHOn7TCg.png"/></div></figure><figure class="lh ir mt lj lk ll lm paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><img src="../Images/d6890e2d564c57da6af65d8874a4dacd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*JJUDxHJHfNoAHCAqBHBF6Q.png"/></div></figure><figure class="lh ir ms lj lk ll lm paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><img src="../Images/c5cdfc8b551697235d23eb292fc51592.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*_ZjrxJBo6Utt6OPm1845kQ.png"/></div><figcaption class="iy iz et er es ja jb bd b be z dx mu di mv lq translated">高斯过程、Adaboost、LDA、逻辑回归和决策树分类器评估</figcaption></figure></div><div class="ab cb"><figure class="lh ir mw lj lk ll lm paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><img src="../Images/c084c369b7fd44e0103577e81e5ce6f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*7GbbirjhObXHYQyvI_twsQ.png"/></div></figure><figure class="lh ir mx lj lk ll lm paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><img src="../Images/b0f1f3abe1a1930ddeb87617f7fed3a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*9IYX7jOCHlbCHp1VEXKqyw.png"/></div><figcaption class="iy iz et er es ja jb bd b be z dx my di mz lq translated">朴素贝叶斯、随机森林、XG Boost分类器评估</figcaption></figure></div></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="7311" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">本文的主要内容是处理分类数据的不同技术，即LabelEncoder、OrdinalEncoder和OneHotEncoder。此外，我们还谈到了主成分分析，其中我们只考虑了117个成分中的2个成分，因此有效地降低了维度。后来，我们研究了不同的方法，这些方法可以告诉我们数据集中聚类的趋势。霍普金的统计学、剪影评分和肘方法向我们展示了走向聚类的道路。最后使用K-means聚类方法。最后，我们使用上述不同的评估方法进行模型分析。</p></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="b205" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果你喜欢数据探索和模型分析，那么请鼓掌，分享，评论反馈。敬请关注更多博客！</p></div></div>    
</body>
</html>