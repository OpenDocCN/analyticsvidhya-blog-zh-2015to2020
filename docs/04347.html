<html>
<head>
<title>Analyzing tweets’ semantic using Word Embedding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用词嵌入分析推文的语义</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/analyzing-tweets-semantic-using-word-embedding-9463e6fbeadb?source=collection_archive---------14-----------------------#2020-03-15">https://medium.com/analytics-vidhya/analyzing-tweets-semantic-using-word-embedding-9463e6fbeadb?source=collection_archive---------14-----------------------#2020-03-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/c7cbc9766643d868663796f38ff2794c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*9_YXw768vWdz78hY0DHnfw.jpeg"/></div></figure><p id="31f2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">社交媒体改变了世界，让个人有机会被倾听并对社会产生影响。在过去的几年里，我们目睹了越来越多始于社交媒体的运动、抗议和现象。</p><p id="484b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个机会也有不利的一面。社交媒体可以提供一个传播仇恨、种族主义、性别歧视的平台，并具有伤害他人的真实潜力。言论自由很重要，但有时，检测和阻止这类内容的发布是必须的。</p><p id="c082" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">监控社交媒体内容的问题在于，浏览每一篇帖子、每一条推文和每一张图片，并检查其语义是不可能的。据大卫·赛思称，每天有5亿条推文被发布。这就是机器学习算法发挥作用的地方。</p><p id="ed63" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用<a class="ae jk" href="https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech#train.csv" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>，我提取了分为仇恨推特和普通推特的推特。在这种情况下，仇恨推文被认为是带有种族主义和性别歧视背景的推文。我的意图是训练一个模型，它将有能力分析推文并有效地标记它。</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es jl"><img src="../Images/56b6bb6c310467206a17bda33f2b1283.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*NaGG8P5cwnEyOgxi7ckHrw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">数据分布包括29720条非仇恨推文和2242条仇恨推文</figcaption></figure><p id="3844" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一个大的挑战是否定语义可能是隐含的。像<a class="ae jk" rel="noopener" href="/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428">弓</a>这样的技巧在这种情况下可能没什么用。幸运的是，单词嵌入正是完成这项任务的工具。</p><p id="fa71" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">单词嵌入是一种NLP技术，其中每个单词被表示为具有n维的向量，该向量表示单词在向量空间中的投影。在训练模型时，给定单词的位置是根据它周围的其他单词来决定的。</p><p id="f84e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">用来创建这种模型的两种最常见的方法是:Word2Vec和GloVe。</p><p id="e5a5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我使用Word2Vec对我在数据集中的所有tweets中的单词训练了一个100维向量的模型(可以随意选择任何大小并测试结果)。</p><pre class="jm jn jo jp fd ju jv jw jx aw jy bi"><span id="daa7" class="jz ka hi jv b fi kb kc l kd ke"># training the model on clean and tokenized tweets<br/>model = Word2Vec(df['tokenized_tweets'], size = 100, window = 20, min_count = 2, negative = 20, iter=25, workers=multiprocessing.cpu_count())</span></pre><p id="6a91" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一旦你训练好了你的模型，你可以表演一些非常好的技巧，比如这个-</p><pre class="jm jn jo jp fd ju jv jw jx aw jy bi"><span id="9c1c" class="jz ka hi jv b fi kb kc l kd ke"># the model words vocabulary<br/>word_vectors = model.wv</span><span id="c683" class="jz ka hi jv b fi kf kc l kd ke">#finding the most similar words to a given word<br/>word_vectors.similar_by_word(“hate”)</span><span id="c571" class="jz ka hi jv b fi kf kc l kd ke">&gt;&gt;&gt;output</span><span id="6e8f" class="jz ka hi jv b fi kf kc l kd ke">[('hateful', 0.5506447553634644),<br/> ('stupid', 0.5302757024765015),<br/> ('blame', 0.5247147083282471),<br/> ('kill', 0.505715012550354),<br/> ('lie', 0.48834866285324097),<br/> ('murderer', 0.48504817485809326),<br/> ('violence', 0.48261135816574097),</span></pre><p id="c7db" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如你所见，在训练模型之后，使用模型词汇我们可以搜索单词和它们最近的邻居。搜索与“恨”最相似的词，我得到了一些很好(嗯，不是很好)的词，它们之间的关系非常清楚。</p><p id="747f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您可以使用PCA和Pyplot执行的另一个好技巧是绘制模型词汇的2D表示:</p><pre class="jm jn jo jp fd ju jv jw jx aw jy bi"><span id="0e67" class="jz ka hi jv b fi kb kc l kd ke">#create a plot for the 150 most frequent words<br/>X = model[model.wv.index2entity[:150]]</span><span id="f26e" class="jz ka hi jv b fi kf kc l kd ke">pca = PCA(n_components=2)<br/>result = pca.fit_transform(X)[:150]</span><span id="d835" class="jz ka hi jv b fi kf kc l kd ke"># create a scatter plot of the projection<br/>pyplot.figure(figsize=(25,15))<br/>pyplot.rcParams.update({'font.size': 15});<br/>pyplot.scatter(result[:, 0], result[:, 1]);<br/>words = list(model.wv.vocab)[:150]<br/>for i, word in enumerate(words):<br/> pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))</span></pre><p id="15b7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这创建了下面的图表，显示了一些单词以及它们与其他单词的交互(其中一些对我来说不太清楚):</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kg"><img src="../Images/5ac89a0e3fea64715464219b7ec01ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xQ3U0puFV0Vfyv8mpZFklg.png"/></div></div></figure><p id="9932" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以保存我们训练的模型，并将其用于其他NLP任务。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><p id="158d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我做的下一件事是将数据中的每个单词转换为一个常数，这样它将适合我要创建的神经网络模型。我还需要为模型提供具有相同列数的数据，因此，我创建了一个填充文档:</p><pre class="jm jn jo jp fd ju jv jw jx aw jy bi"><span id="c9af" class="jz ka hi jv b fi kb kc l kd ke">texts = df['clean_tweets']<br/>tokenizer = Tokenizer(nb_words=14225)<br/>tokenizer.fit_on_texts(texts)<br/>sequences = tokenizer.texts_to_sequences(texts)</span><span id="e731" class="jz ka hi jv b fi kf kc l kd ke">word_index = tokenizer.word_index<br/>print('Found %s unique tokens.' % len(word_index))<br/>&gt;&gt;&gt;<br/>Found 37391 unique tokens.</span><span id="18ea" class="jz ka hi jv b fi kf kc l kd ke">data = pad_sequences(sequences, maxlen=280)</span><span id="3f7b" class="jz ka hi jv b fi kf kc l kd ke"># split the data into a training set and a validation set<br/>VALIDATION_SPLIT = 0.2<br/>indices = np.arange(data.shape[0])<br/>np.random.shuffle(indices)<br/>data = data[indices]<br/>labels = labels[indices]<br/>nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])</span><span id="4897" class="jz ka hi jv b fi kf kc l kd ke">x_train = data[:-nb_validation_samples]<br/>y_train = labels[:-nb_validation_samples]<br/>x_val = data[-nb_validation_samples:]<br/>y_val = labels[-nb_validation_samples:]</span></pre><p id="7a28" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如您所看到的，我已经决定我的数据将包含14，225个最常用的单词，并将有280个列\单词(尽管tweets的长度被限制为280个字符，但为了安全起见，我想将其分为训练和验证子集)。</p><p id="fae8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，使用我创建的Word2Vec模型，我将训练一个嵌入矩阵，这是一个单词与其相应向量的字典，将用于NN模型中的嵌入层:</p><pre class="jm jn jo jp fd ju jv jw jx aw jy bi"><span id="2690" class="jz ka hi jv b fi kb kc l kd ke">embeddings_index = {}<br/>f = open('Word_2_Vec_sentiment_analysis.txt')<br/>for line in f:<br/>    values = line.split()<br/>    word = values[0]<br/>    coefs = np.asarray(values[1:], dtype='float32')<br/>    embeddings_index[word] = coefs<br/>f.close()</span><span id="a5ee" class="jz ka hi jv b fi kf kc l kd ke">print('Found %s word vectors.' % len(embeddings_index))<br/>&gt;&gt;&gt;<br/>Found 14226 word vectors.</span><span id="c71a" class="jz ka hi jv b fi kf kc l kd ke">embedding_matrix = np.zeros((len(word_index) + 1, 100))<br/>for word, i in t.word_index.items():<br/> embedding_vector = embeddings_index.get(word)<br/> if embedding_vector is not None:<br/>  embedding_matrix[i] = embedding_vector</span></pre><p id="e07b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们现在可以进入有趣的部分了！使用嵌入矩阵和我创建的填充数据，我可以训练一个神经网络模型，并开始对推文进行分类！</p><p id="e764" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于此任务，我将使用Keras库，它提供了一个嵌入式层，我将构建一个非常简单的顺序模型:</p><pre class="jm jn jo jp fd ju jv jw jx aw jy bi"><span id="85a5" class="jz ka hi jv b fi kb kc l kd ke">model_2 = Sequential()<br/>e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=data_length, trainable=False)<br/>model_2.add(e)<br/>model_2.add(Flatten())<br/>model_2.add(Dense(1, activation=’sigmoid’))</span><span id="cd5f" class="jz ka hi jv b fi kf kc l kd ke">opt = Adam(lr=0.00001)</span><span id="9e3c" class="jz ka hi jv b fi kf kc l kd ke">model_2.compile(loss=’binary_crossentropy’,<br/> optimizer=opt,<br/> metrics=[‘accuracy’])</span><span id="89c9" class="jz ka hi jv b fi kf kc l kd ke">model_2.summary()</span></pre><p id="b044" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们看看我们的模型总结:</p><pre class="jm jn jo jp fd ju jv jw jx aw jy bi"><span id="be8f" class="jz ka hi jv b fi kb kc l kd ke">Model: "sequential_55"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_56 (Embedding)     (None, 280, 100)          3739200   <br/>_________________________________________________________________<br/>flatten_40 (Flatten)         (None, 28000)             0         <br/>_________________________________________________________________<br/>dense_48 (Dense)             (None, 1)                 28001     <br/>=================================================================<br/>Total params: 3,767,201<br/>Trainable params: 28,001<br/>Non-trainable params: 3,739,200</span></pre><p id="c367" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们可以运行它并查看结果:</p><pre class="jm jn jo jp fd ju jv jw jx aw jy bi"><span id="3f34" class="jz ka hi jv b fi kb kc l kd ke">history = model_2.fit(x_train, y_train, validation_data=(x_val, y_val),<br/> epochs=50, batch_size=120)</span><span id="5239" class="jz ka hi jv b fi kf kc l kd ke">&gt;&gt;&gt;<br/>Train on 25570 samples, validate on 6392 samples<br/>Epoch 1/50<br/>25570/25570 [==============================] - 3s 103us/step - loss: 0.5978 - accuracy: 0.6869 - val_loss: 0.5253 - val_accuracy: 0.7894<br/>Epoch 2/50<br/>25570/25570 [==============================] - 2s 83us/step - loss: 0.4881 - accuracy: 0.8296 - val_loss: 0.4470 - val_accuracy: 0.8656<br/>Epoch 3/50<br/>25570/25570 [==============================] - 2s 84us/step - loss: 0.4281 - accuracy: 0.8799 - val_loss: 0.4003 - val_accuracy: 0.8944<br/>Epoch 4/50<br/>25570/25570 [==============================] - 2s 84us/step - loss: 0.3895 - accuracy: 0.9038 - val_loss: 0.3683 - val_accuracy: 0.9110<br/>Epoch 5/50<br/>25570/25570 [==============================] - 2s 85us/step - loss: 0.3615 - accuracy: 0.9142 - val_loss: 0.3443 - val_accuracy: 0.9182<br/>Epoch 6/50<br/>25570/25570 [==============================] - 2s 93us/step - loss: 0.3397 - accuracy: 0.9202 - val_loss: 0.3254 - val_accuracy: 0.9222</span></pre><p id="48ae" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我只捕获了50个纪元中的6个，但是正如你所看到的，从第四个纪元开始，我们达到了90%的准确率。随着纪元的增加，训练分数的准确性不断上升，而验证集的准确性分数保持在93%左右(一点也不差！)，但为了防止过度拟合，我们就此打住。</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/4873e24aeac82bbbf1533a08a350e8c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*yQdG9yfdMSzUFow22CzLSA.png"/></div></figure><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/591fc2f130c3c58feee3d4aea7d26f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*1s5YNBvWcESx5qhh6Nk4XQ.png"/></div></figure><p id="8f49" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以将我们的预测添加到原始数据框架中，以便更仔细地查看结果:</p><pre class="jm jn jo jp fd ju jv jw jx aw jy bi"><span id="b060" class="jz ka hi jv b fi kb kc l kd ke">#taking our predictions <br/>predictions = model_2.predict_classes(x_val)</span><span id="67f5" class="jz ka hi jv b fi kf kc l kd ke">#extracting our dataframe rows that match the validation set<br/>df_eval = df.ix[list(y_val.index.values)]</span><span id="1c0d" class="jz ka hi jv b fi kf kc l kd ke">#adding a new column with our predictions to the dataframe<br/>df_eval['predictions'] = predictions</span><span id="42aa" class="jz ka hi jv b fi kf kc l kd ke">#print only rows which we predicted as hate tweets<br/>df_eval[df_eval['predictions'] ==1]</span></pre><p id="73ad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">看了结果之后，似乎这个模型在分类这些数据方面做得很好。甚至一些被错误归类为仇恨的推文也有一些负面的内容。</p><p id="476a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">还有什么？我还使用了一个预先训练的Word2Vec模型(您可以在下面我的完整代码的链接中找到它)来做同样的预测，它提供了几乎相同的准确性分数，但学习速度更快，而且-将其他层与NN模型结合起来，并调整模型的参数可能会给你带来更好的结果。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><p id="c6e7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我希望你阅读这篇短文，我将非常高兴听到你的想法和想法，以改善这个NLP任务！</p><p id="9e72" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" href="https://github.com/Natialuk/Analyzing-tweets-semantic-using-Word-Embedding" rel="noopener ugc nofollow" target="_blank">在GitHub上链接到我的完整代码</a></p><h1 id="4993" class="ku ka hi bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">信用和证明人:</h1><div class="lr ls ez fb lt lu"><a href="https://www.kaggle.com/arkhoshghalb/twitter-sentiment-analysis-hatred-speech#train.csv" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">推特情感分析</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">检测仇恨推文，由分析Vidhya提供</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">www.kaggle.com</p></div></div><div class="md l"><div class="me l mf mg mh md mi ik lu"/></div></div></a></div><div class="lr ls ez fb lt lu"><a href="https://nlp.stanford.edu/projects/glove/" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">GloVe:单词表示的全局向量</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">GloVe是一种无监督学习算法，用于获取单词的矢量表示。培训在…进行</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">nlp.stanford.edu</p></div></div><div class="md l"><div class="mj l mf mg mh md mi ik lu"/></div></div></a></div><div class="lr ls ez fb lt lu"><a rel="noopener follow" target="_blank" href="/greyatom/an-introduction-to-bag-of-words-in-nlp-ac967d43b428"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">自然语言处理中的词袋介绍</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">这篇文章将带你深入自然语言处理。在你继续前进之前，确保你已经…</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">medium.com</p></div></div><div class="md l"><div class="mk l mf mg mh md mi ik lu"/></div></div></a></div><div class="lr ls ez fb lt lu"><a href="https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">如何用Keras使用单词嵌入层进行深度学习——机器学习掌握</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">单词嵌入提供了单词及其相关含义的密集表示。它们比…有所改进</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">machinelearningmastery.com</p></div></div><div class="md l"><div class="ml l mf mg mh md mi ik lu"/></div></div></a></div><div class="lr ls ez fb lt lu"><a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">在Keras模型中使用预先训练的单词嵌入</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">在本教程中，我们将带您了解使用预先训练的word解决文本分类问题的过程…</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">blog.keras.io</p></div></div></div></a></div><div class="lr ls ez fb lt lu"><a rel="noopener follow" target="_blank" href="/@jonathan_hui/nlp-word-embedding-glove-5e7f523999f6"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">NLP —单词嵌入和手套</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">BERT是为句子创建矢量表示的一个重要里程碑。但是它没有告诉我们…的确切设计</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">medium.com</p></div></div><div class="md l"><div class="mm l mf mg mh md mi ik lu"/></div></div></a></div></div></div>    
</body>
</html>