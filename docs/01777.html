<html>
<head>
<title>Learn how to build a Language Translator</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解如何构建语言翻译器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/learn-how-to-build-a-language-translator-648d30845681?source=collection_archive---------4-----------------------#2019-11-13">https://medium.com/analytics-vidhya/learn-how-to-build-a-language-translator-648d30845681?source=collection_archive---------4-----------------------#2019-11-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2214" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们都遇到过在线翻译服务。它将输入从一种语言翻译成另一种语言。但是你有没有想过这是怎么做到的？！通过这篇文章，让我们试着理解它是如何完成的，也让我们试着用python构建一个简单的翻译器。</p><p id="e3cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章的灵感来自于Keras 中的<a class="ae jd" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">介绍序列到序列学习的博客。</a></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/af48c87c04e448865d5aa0011fb99863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWa4HcuaMyEICJ-OaCA_Eg.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图片来源:pixabay</figcaption></figure><p id="31fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在构建翻译器之前，我们需要知道深度学习中的一些概念，所以让我们开始探索这些概念。</p><p id="9a19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是序列对序列学习？</strong></p><p id="702e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">序列对序列(seq2seq)就是创建模型，将序列从一个域转换到另一个域。例如，将<strong class="ih hj">中的文本从一种语言转换成另一种语言</strong>，或者将一个人的<strong class="ih hj">声音转换成另一个人的</strong>声音，等等。所以，问题是为什么我们不使用简单的LSTM或GRU层模型来达到这个目的呢？或者为什么我们为此目的使用特定的序列对序列模型技术？答案很简单，如果输入和输出长度相同，那么我们可以很容易地使用简单的LSTM和GRU层，但这里的<strong class="ih hj">输入和输出长度不同</strong>(以语言翻译为例，英语句子中的单词数将不等于法语中表示相同意思的句子的单词数)，因此我们被迫为此采用不同的方法。</p><p id="b777" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于输入和输出序列的长度不同，为了开始预测目标输出，要考虑整个输入序列。现在，让我们看看它是如何工作的。</p><p id="ec86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在顺序对顺序的方法中，我们遇到了两种不同的架构</p><ol class=""><li id="b8ac" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">培训架构</li><li id="05f6" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">测试架构(推理模型)</li></ol><p id="d244" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练和测试架构都有编码器和解码器。在这两种情况下，编码器架构保持相同，但解码器架构略有不同。</p><p id="605c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">培训架构</strong></p><p id="8f66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如图1所示，在训练架构中，我们有两个部分，编码器部分和解码器部分。<strong class="ih hj">编码器(RNN层)</strong>将每个字符(英文字符)作为输入，并将它们转换成某种隐藏的表示。所有这些隐藏的表示然后通过函数F传递，使得它产生<strong class="ih hj">一个编码向量</strong>。</p><p id="6760" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<strong class="ih hj">解码器(RNN层)</strong>中，从编码向量和起始序列字符(SOS，在我们的例子中我们使用' \t ')作为输入开始，然后神经网络通过某种方式更新其特征(称为教师强制)而被强制产生其相应的目标。从下一个时间步长开始，输入将是每个字符(在我们的例子中是法语字符)和先前的解码器状态。<strong class="ih hj">有效，解码器学习生成</strong> <code class="du ki kj kk kl b"><strong class="ih hj">targets[t+1...]</strong></code> <strong class="ih hj">给定</strong> <code class="du ki kj kk kl b"><strong class="ih hj">targets[...t]</strong></code> <strong class="ih hj">，<em class="km">以输入序列</em>为条件。</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kn"><img src="../Images/a5a3c5c9a183fc9f341862bc182e6afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nq20uZKoxAFvL8-iXohhcQ.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1: <strong class="bd ko">用于训练的序列对序列模型架构</strong></figcaption></figure><p id="0233" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">测试架构(推理模型)</strong></p><p id="e674" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如图2所示，测试架构(<strong class="ih hj">用于预测输出</strong>)也有两个部分，编码器部分和解码器部分。编码器(RNN层)作为训练架构中的编码器层工作(即，它逐个字符地接受输入并产生单个编码向量)。</p><p id="f176" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在<strong class="ih hj">解码器层(先前训练的网络)</strong>将编码向量和起始序列作为输入，并试图产生第一个目标字符(猜测输出)，从下一个时间步开始，编码器将先前预测的字符和解码器状态作为输入，并试图产生目标输出。重复该过程，直到预测到停止序列(EOS)(参见图2)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kp"><img src="../Images/a8cdd7e44c939544e77cbba5e29aae90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bVOctnZIuFI-iy00RzRtbA.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图2: <strong class="bd ko">用于测试的序列到序列模型架构</strong></figcaption></figure><h2 id="a33c" class="kq kr hi bd ko ks kt ku kv kw kx ky kz iq la lb lc iu ld le lf iy lg lh li lj bi translated">我们来编码吧！</h2><p id="8ef1" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">构建翻译器的第一步是建立一个安装了必要库的环境。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="a283" class="kq kr hi kl b fi lt lu l lv lw">pip install keras<br/>pip install tensorflow<br/>pip install numpy</span></pre><blockquote class="lx ly lz"><p id="df50" class="if ig km ih b ii ij ik il im in io ip ma ir is it mb iv iw ix mc iz ja jb jc hb bi translated">让我们创建一个文件名为train.py的文件</p></blockquote><p id="c4e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们导入库并定义参数</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="84f8" class="kq kr hi kl b fi lt lu l lv lw">from keras.models import Model<br/>from keras.layers import Input, LSTM, Dense<br/>import numpy as np</span><span id="20ed" class="kq kr hi kl b fi md lu l lv lw">batch_size = 64      # Batch size for training.<br/>epochs = 100         # Number of epochs to train for.<br/>latent_dim = 256     # Latent dimensionality of the encoding space.<br/>num_samples = 10000  # Number of samples to train on.</span></pre><p id="ac9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步是准备数据集。我们将使用成对英语句子及其法语翻译的数据集，你可以从manythings.org/anki下载。一旦我们下载了数据集，我们只需设置访问数据集的路径，如下所示。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="33bf" class="kq kr hi kl b fi lt lu l lv lw"># Path to the data txt file on disk.<br/>path = 'fra-eng/fra.txt'</span></pre><p id="8f50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在下一步是对数据进行矢量化。因此，为了对数据进行矢量化，我们从读取文本文件中的每一行开始，并将其附加到一个列表中。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="0a1d" class="kq kr hi kl b fi lt lu l lv lw">input_texts = []<br/>target_texts = []<br/>input_characters = set()<br/>target_characters = set()</span><span id="4ea7" class="kq kr hi kl b fi md lu l lv lw">with open(path, 'r', encoding='utf-8') as f:<br/>    lines = f.read().split('\n')<br/>print(lines[1:5])</span><span id="66ab" class="kq kr hi kl b fi md lu l lv lw"><strong class="kl hj">Output</strong></span><span id="17f7" class="kq kr hi kl b fi md lu l lv lw">['Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #509819 (Aiji)', 'Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) &amp; #4320462 (gillux)', 'Run!\tCours\u202f!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906331 (sacredceltic)', 'Run!\tCourez\u202f!\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) &amp; #906332 (sacredceltic)']</span></pre><p id="6eb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，对于本教程，我们只考虑数据集的前10000行。我们希望将英语文本作为输入文本，法语文本作为目标文本。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="6c45" class="kq kr hi kl b fi lt lu l lv lw">for line in lines[:10000]:<br/>    input_text, target_text, _ = line.split('\t')</span><span id="96c9" class="kq kr hi kl b fi md lu l lv lw">print(input_text,target_text)<br/><strong class="kl hj">Output<br/></strong>.......<br/>Someone called. Quelqu'un a téléphoné.<br/>Stay out of it. Ne t'en mêle pas !<br/>Stay out of it. Ne vous en mêlez pas !<br/>Stop grumbling. Arrête de râler.<br/>Stop grumbling. Arrête de ronchonner.<br/>Stop poking me. Arrête de m'asticoter !<br/>.......</span></pre><p id="a128" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要为目标文本定义开始序列字符和结束序列字符。我们使用'<em class="km">制表符</em>作为字符的开头，使用'<em class="km"> \n' </em>作为字符的结尾。</p><p id="0cab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="km">注意:下面使用相同的for循环(如上所示)。这样做是为了解释概念。新添加的行用</em> <strong class="ih hj"> <em class="km">粗体</em> </strong> <em class="km">字母书写。</em></p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="ef53" class="kq kr hi kl b fi lt lu l lv lw">for line in lines[: 10000]:    <br/>    input_text, target_text, _ = line.split('\t')<br/>    # We use "tab" as the "start sequence" character<br/>    # for the targets, and "\n" as "end sequence" character.<br/>    <strong class="kl hj">target_text = '\t' + target_text + '\n'<br/>    input_texts.append(input_text)<br/>    target_texts.append(target_text)</strong></span></pre><p id="c9a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们分别将英语文本填充到输入文本，将法语文本填充到目标文本列表。</p><p id="8a43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们想为英语文本和法语文本创建一个独特的字符列表。为此，我们将文本中所有唯一的字符添加到相应的列表中，如下所示。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="ae74" class="kq kr hi kl b fi lt lu l lv lw">for line in lines[: 10000]:<br/>    input_text, target_text, _ = line.split('\t')<br/>    # We use "tab" as the "start sequence" character<br/>    # for the targets, and "\n" as "end sequence" character.<br/>    target_text = '\t' + target_text + '\n'<br/>    input_texts.append(input_text)<br/>    target_texts.append(target_text)<br/>    <strong class="kl hj">for char in input_text:<br/>        if char not in input_characters:<br/>            input_characters.add(char)<br/>    for char in target_text:<br/>        if char not in target_characters:<br/>            target_characters.add(char)<br/>print(input_characters)<br/>print(target_characters)</strong></span><span id="bc05" class="kq kr hi kl b fi md lu l lv lw"><strong class="kl hj">output:</strong></span><span id="a804" class="kq kr hi kl b fi md lu l lv lw">{'?', 's', 'Q', 'o', 'v', '8', '0', 'R', 'L', 'n', 'T', 'I', 'H', ',', '9', 'B', 'W', 'l', 'm', 'A', ' ', 'f', 'U', 'k', 'y', '1', 'c', '5', 'V', 'O', 'h', ':', 'j', 'e', 'z', '$', '&amp;', 'C', 'q', 'M', '%', 'w', 'r', 'i', 'g', 'b', '-', '2', '7', 'P', 'Y', 'd', 'N', 'S', 'D', "'", '!', '6', '.', 'x', '3', 'F', 't', 'J', 'K', 'E', 'a', 'u', 'G', 'p'}</span><span id="5dc2" class="kq kr hi kl b fi md lu l lv lw">{'ô', '?', 'Q', 'o', 'ê', '0', 'à', 'm', 'f', '5', 'V', '«', 'O', 'j', 'e', '&amp;', '\xa0', 'M', 'i', '2', 'd', 'D', "'", 'ï', 'K', 'J', 'E', '1', 'è', 'À', 't', 'é', '\u202f', 'v', ')', 'B', 'œ', '’', 'l', '(', 'c', ':', '$', '\t', 'C', 'q', 'N', 'S', 'x', '3', 'p', '8', 'R', 'L', 'T', 'I', '9', 'É', 'A', 'k', 'y', 'û', 'z', 'r', '»', '-', 'P', 'Y', '!', '.', 'a', 'u', 'Ç', 's', 'ç', 'n', 'H', ',', 'U', ' ', 'Ê', 'ë', '\n', 'h', 'ù', '%', 'g', 'b', '\u2009', 'F', 'â', 'G', 'î'}</span></pre><p id="7fe4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们要定义一些将用于特征工程零件的参数。</p><p id="eca5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输入字符:</strong>输入字符的排序列表</p><p id="9a0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">目标字符:</strong>目标字符的排序列表</p><p id="d7ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> num_encoder_tokens: </strong>输入字符列表长度</p><p id="3e0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> num_decoder_tokens: </strong>目标字符列表长度</p><p id="d4c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">max _ encoder _ seq _ length:</strong>输入集合中最大长度文本的长度</p><p id="4f19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">max _ decoder _ seq _ length:</strong>目标集合中最大长度文本的长度</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="6890" class="kq kr hi kl b fi lt lu l lv lw">input_characters = sorted(list(input_characters))<br/>target_characters = sorted(list(target_characters))<br/>num_encoder_tokens = len(input_characters)<br/>num_decoder_tokens = len(target_characters)<br/>max_encoder_seq_length = max([len(txt) for txt in input_texts])<br/>max_decoder_seq_length = max([len(txt) for txt in target_texts])</span></pre><p id="101d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还需要一个字符来索引输入和目标字符列表的映射。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="52c2" class="kq kr hi kl b fi lt lu l lv lw">input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])<br/>target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])</span><span id="1448" class="kq kr hi kl b fi md lu l lv lw">print(input_token_index)<br/>print(target_token_index)</span><span id="46f0" class="kq kr hi kl b fi md lu l lv lw"><strong class="kl hj">Output<br/></strong>{' ': 0, '!': 1, '$': 2, '%': 3, '&amp;': 4, "'": 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '5': 13, '6': 14, '7': 15, '8': 16, '9': 17, ':': 18, '?': 19, 'A': 20, 'B': 21, 'C': 22, 'D': 23, 'E': 24, 'F': 25, 'G': 26, 'H': 27, 'I': 28, 'J': 29, 'K': 30, 'L': 31, 'M': 32, 'N': 33, 'O': 34, 'P': 35, 'Q': 36, 'R': 37, 'S': 38, 'T': 39, 'U': 40, 'V': 41, 'W': 42, 'Y': 43, 'a': 44, 'b': 45, 'c': 46, 'd': 47, 'e': 48, 'f': 49, 'g': 50, 'h': 51, 'i': 52, 'j': 53, 'k': 54, 'l': 55, 'm': 56, 'n': 57, 'o': 58, 'p': 59, 'q': 60, 'r': 61, 's': 62, 't': 63, 'u': 64, 'v': 65, 'w': 66, 'x': 67, 'y': 68, 'z': 69}<br/>{'\t': 0, '\n': 1, ' ': 2, '!': 3, '$': 4, '%': 5, '&amp;': 6, "'": 7, '(': 8, ')': 9, ',': 10, '-': 11, '.': 12, '0': 13, '1': 14, '2': 15, '3': 16, '5': 17, '8': 18, '9': 19, ':': 20, '?': 21, 'A': 22, 'B': 23, 'C': 24, 'D': 25, 'E': 26, 'F': 27, 'G': 28, 'H': 29, 'I': 30, 'J': 31, 'K': 32, 'L': 33, 'M': 34, 'N': 35, 'O': 36, 'P': 37, 'Q': 38, 'R': 39, 'S': 40, 'T': 41, 'U': 42, 'V': 43, 'Y': 44, 'a': 45, 'b': 46, 'c': 47, 'd': 48, 'e': 49, 'f': 50, 'g': 51, 'h': 52, 'i': 53, 'j': 54, 'k': 55, 'l': 56, 'm': 57, 'n': 58, 'o': 59, 'p': 60, 'q': 61, 'r': 62, 's': 63, 't': 64, 'u': 65, 'v': 66, 'x': 67, 'y': 68, 'z': 69, '\xa0': 70, '«': 71, '»': 72, 'À': 73, 'Ç': 74, 'É': 75, 'Ê': 76, 'à': 77, 'â': 78, 'ç': 79, 'è': 80, 'é': 81, 'ê': 82, 'ë': 83, 'î': 84, 'ï': 85, 'ô': 86, 'ù': 87, 'û': 88, 'œ': 89, '\u2009': 90, '’': 91, '\u202f': 92}</span></pre><p id="c011" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">特色工程</strong></p><p id="f1b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据维基百科的说法，<strong class="ih hj">特征工程</strong>是利用数据的<a class="ae jd" href="https://en.wikipedia.org/wiki/Domain_knowledge" rel="noopener ugc nofollow" target="_blank">领域知识</a>创建<a class="ae jd" href="https://en.wikipedia.org/wiki/Feature_(machine_learning)" rel="noopener ugc nofollow" target="_blank">特征</a>的过程，使<a class="ae jd" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>算法工作。特征工程是机器学习应用的基础，既困难又昂贵。</p><p id="f92c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，是生成特性的时候了。为了生成特征向量，我们使用了<strong class="ih hj">一键编码。</strong>要了解更多关于一键编码的信息，请观看此视频。</p><p id="a29b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解释:一键编码</p><p id="e1d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了生成特性，我们首先需要定义变量来存储独热编码数据。我们使用3D numpy数组来存储独热编码数据。第一个维度对应于我们考虑的样本文本的数量(这里是10000)。第二维表示最大编码器/解码器序列长度(这意味着样本中最长文本的长度),第三维表示input _ char ecter/target _ char ecter中存在的唯一字符的数量。</p><p id="b198" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用三个变量来存储数据。</p><ol class=""><li id="9eda" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">encoder_input_data:编码器输入数据存储一次性编码的输入文本(英文文本)数据。</li><li id="07dd" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">解码器输入数据存储一个热编码的输入文本(相应的法语文本)数据。</li><li id="5c84" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">decoder_target_data:解码器目标数据存储独热编码的目标数据(即对应于decoder_input_data要生成的数据)。</li></ol><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es me"><img src="../Images/b94576312d9a64a12d43ef7c9b994860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*BeZWO8HXwZanHq0dQ1esog.jpeg"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图3。解码器输入数据vs解码器目标数据(内部表示)</figcaption></figure><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="1740" class="kq kr hi kl b fi lt lu l lv lw">encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens),dtype='float32')</span><span id="4cb7" class="kq kr hi kl b fi md lu l lv lw">decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')</span><span id="5f87" class="kq kr hi kl b fi md lu l lv lw">decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')</span><span id="9f04" class="kq kr hi kl b fi md lu l lv lw">for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):<br/>    for t, char in enumerate(input_text):<br/>        encoder_input_data[i, t, input_token_index[char]] = 1.<br/>    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.<br/>    for t, char in enumerate(target_text):<br/>        # decoder_target_data is ahead of decoder_input_data by one timestep<br/>        decoder_input_data[i, t, target_token_index[char]] = 1.<br/>        if t &gt; 0:<br/>            # decoder_target_data will be ahead by one timestep<br/>            # and will not include the start character.<br/>            decoder_target_data[i, t - 1, target_token_index[char]] = 1.<br/>    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.<br/>    decoder_target_data[i, t:, target_token_index[' ']] = 1.</span></pre><p id="9878" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们已经准备好了特征向量，接下来我们需要将这些特征向量输入到相应的编码器和解码器模型中。</p></div><div class="ab cl mf mg gp mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="hb hc hd he hf"><h2 id="111b" class="kq kr hi bd ko ks kt ku kv kw kx ky kz iq la lb lc iu ld le lf iy lg lh li lj bi translated">构建模型</h2><p id="19d8" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated"><strong class="ih hj">编码器:</strong>首先我们为编码器定义输入序列。输入以逐个字符的方式被馈送到编码器。<strong class="ih hj">对于编码器_LSTM，我们设置了<em class="km"> return_state = True。这意味着我们在输入序列的末尾获得了最终的编码隐藏表示。</em></strong>该最终编码表示然后用于初始化解码器的状态。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="5c56" class="kq kr hi kl b fi lt lu l lv lw"># Define an input sequence and process it.<br/>encoder_inputs = Input(shape=(None, num_encoder_tokens))<br/>encoder = LSTM(latent_dim, return_state=True)<br/>encoder_outputs, state_h, state_c = encoder(encoder_inputs)<br/># We discard `encoder_outputs` and only keep the states.<br/>encoder_states = [state_h, state_c]</span></pre><p id="6cb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解码器:</strong>我们首先为解码器定义输入序列。第一个目标字符到解码器的输入将是<strong class="ih hj">最终编码状态</strong>和来自<strong class="ih hj">一位热码编码<em class="km">解码器_输入_数据</em> </strong> <em class="km">的第一个字符。</em>从下一步开始，解码器的输入将是<strong class="ih hj">单元状态、隐藏状态(隐藏表示)</strong>和来自<strong class="ih hj">独热编码解码器数据的下一个字符。</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mm"><img src="../Images/f3bb1b53db8cc709eebb7dba2f71ffb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sRFnVzJFl3YnCmuOCMe_Jg.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><strong class="bd ko">图4。解码器架构(培训)</strong></figcaption></figure><p id="8d0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，在解码器LSTM中，我们给出<strong class="ih hj"><em class="km">return _ sequences = True</em></strong>以及<strong class="ih hj"><em class="km">return _ state = True</em></strong>这意味着我们在每个时间步长考虑解码器输出和两个解码器状态。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="098c" class="kq kr hi kl b fi lt lu l lv lw"># Set up the decoder, using `encoder_states` as initial state.<br/>decoder_inputs = Input(shape=(None, num_decoder_tokens))<br/># We set up our decoder to return full output sequences,<br/># and to return internal states as well. We don't use the<br/># return states in the training model, but we will use them in inference.<br/>decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)</span><span id="23c0" class="kq kr hi kl b fi md lu l lv lw">decoder_outputs, _, _ = decoder_lstm(decoder_inputs,<br/>                                     initial_state=encoder_states)</span><span id="cd3d" class="kq kr hi kl b fi md lu l lv lw">decoder_dense = Dense(num_decoder_tokens, activation='softmax')</span><span id="ba9f" class="kq kr hi kl b fi md lu l lv lw">decoder_outputs = decoder_dense(decoder_outputs)</span></pre><p id="a000" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">培训</strong></p><p id="b7bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们要训练的模型非常简单。该模型将用RMSprop优化器训练100个时期。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="3d9e" class="kq kr hi kl b fi lt lu l lv lw">model = Model([encoder_inputs, decoder_inputs], decoder_outputs)</span><span id="fca6" class="kq kr hi kl b fi md lu l lv lw"># Run training<br/>model.compile(optimizer='rmsprop', loss='categorical_crossentropy',<br/>              metrics=['accuracy'])<br/>model.fit([encoder_input_data, decoder_input_data],       decoder_target_data,<br/>          batch_size=batch_size,<br/>          epochs=epochs,<br/>          validation_split=0.2)</span></pre><p id="93e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练我们的模型花了很多时间。让我们保存我们的进度:</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="0aab" class="kq kr hi kl b fi lt lu l lv lw"># Save model<br/>model.save('s2s.h5')<br/>print(model.summary())</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mn"><img src="../Images/d4b5c9e87f75b8e97a8cd522d3f84371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQdS7YomAK81_DSgtb4qnA.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><strong class="bd ko">型号总结</strong></figcaption></figure><p id="2b32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们将代码保存在<strong class="ih hj"> train.py </strong>中</p><h2 id="af5e" class="kq kr hi bd ko ks kt ku kv kw kx ky kz iq la lb lc iu ld le lf iy lg lh li lj bi translated">测试(推理模式)</h2><p id="4139" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">为了进行测试，我们首先将保存的文件(s2s.h5)加载到一个新的python文件中。让我们将新文件命名为predict.py</p><blockquote class="lx ly lz"><p id="3889" class="if ig km ih b ii ij ik il im in io ip ma ir is it mb iv iw ix mc iz ja jb jc hb bi translated">文件名:<strong class="ih hj"> predict.py </strong>(与train.py位置相同)</p></blockquote><p id="db6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们必须初始化(如<em class="km">输入_字符，输入令牌_索引，</em>等。)所有变量，如在train.py中所做的那样。这样做是为了重建用于预测的编码器-解码器模型。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="5794" class="kq kr hi kl b fi lt lu l lv lw">from __future__ import print_function</span><span id="4945" class="kq kr hi kl b fi md lu l lv lw">from keras.models import Model, load_model<br/>from keras.layers import Input, LSTM, Dense<br/>import numpy as np</span><span id="d40b" class="kq kr hi kl b fi md lu l lv lw">batch_size = 64  # Batch size for training.<br/>epochs = 100  # Number of epochs to train for.<br/>latent_dim = 256  # Latent dimensionality of the encoding space.<br/>num_samples = 10000  # Number of samples to train on.<br/># Vectorize the data.<br/>input_texts = []<br/>target_texts = []<br/>input_characters = set()<br/>target_characters = set()</span><span id="ed1c" class="kq kr hi kl b fi md lu l lv lw">path = 'fra-eng/fra.txt'<br/>with open(path, 'r', encoding='utf-8') as f:<br/>    lines = f.read().split('\n')<br/>for line in lines[: min(num_samples, len(lines) - 1)]:<br/>    input_text, target_text, _ = line.split('\t')<br/>    # We use "tab" as the "start sequence" character<br/>    # for the targets, and "\n" as "end sequence" character.<br/>    target_text = '\t' + target_text + '\n'<br/>    input_texts.append(input_text)<br/>    target_texts.append(target_text)<br/>    for char in input_text:<br/>        if char not in input_characters:<br/>            input_characters.add(char)<br/>    for char in target_text:<br/>        if char not in target_characters:<br/>            target_characters.add(char)</span><span id="b77a" class="kq kr hi kl b fi md lu l lv lw">input_characters = sorted(list(input_characters))<br/>target_characters = sorted(list(target_characters))<br/>num_encoder_tokens = len(input_characters)<br/>num_decoder_tokens = len(target_characters)<br/>max_encoder_seq_length = max([len(txt) for txt in input_texts])<br/>max_decoder_seq_length = max([len(txt) for txt in target_texts])</span><span id="88d5" class="kq kr hi kl b fi md lu l lv lw">print('Number of samples:', len(input_texts))<br/>print('Number of unique input tokens:', num_encoder_tokens)<br/>print('Number of unique output tokens:', num_decoder_tokens)<br/>print('Max sequence length for inputs:', max_encoder_seq_length)<br/>print('Max sequence length for outputs:', max_decoder_seq_length)</span><span id="11a6" class="kq kr hi kl b fi md lu l lv lw">input_token_index = dict(<br/>    [(char, i) for i, char in enumerate(input_characters)])<br/>target_token_index = dict(<br/>    [(char, i) for i, char in enumerate(target_characters)])</span><span id="82ad" class="kq kr hi kl b fi md lu l lv lw">encoder_input_data = np.zeros(<br/>    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),<br/>    dtype='float32')</span><span id="f98c" class="kq kr hi kl b fi md lu l lv lw">for i, input_text in enumerate(input_texts):<br/>    for t, char in enumerate(input_text):<br/>        encoder_input_data[i, t, input_token_index[char]] = 1.</span></pre><p id="2b58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们加载模型。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="1ed2" class="kq kr hi kl b fi lt lu l lv lw"># Restore the model and construct the encoder and decoder.<br/>model = load_model('s2s.h5')</span></pre><p id="b4ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">加载保存的模型后，让我们开始重建编码器。</p><p id="83bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="km"> encoder_input:这里，我们实际上指定了编码器接受的输入类型。</em>(即<em class="km">张量(" input_1:0 "，shape=(None，None，70)，dtype=float32))。类似地，我们定义编码器_输出、状态_h_enc(隐藏状态)和状态_c_enc(单元状态)(隐藏状态和单元状态是</em> <strong class="ih hj"> <em class="km">隐藏表示</em> </strong> <em class="km">)。</em></p><p id="c4aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，通过使用输入张量(编码器_输入)和输出张量(隐藏表示，即状态_h_enc，状态_c_enc ),我们创建了一个模型。这是通过使用keras中的Model()函数来完成的。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="c635" class="kq kr hi kl b fi lt lu l lv lw">encoder_inputs = model.input[0]   # input_1<br/>encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output   # lstm_1<br/>encoder_states = [state_h_enc, state_c_enc]<br/>encoder_model = Model(encoder_inputs, encoder_states)</span></pre><p id="b0b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在创建了编码器的模型之后，现在让我们来构建解码器。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="f18d" class="kq kr hi kl b fi lt lu l lv lw">decoder_inputs = model.input[1]   # input_2<br/>decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')<br/>decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')<br/>decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]<br/>decoder_lstm = model.layers[3]<br/>decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(<br/>    decoder_inputs, initial_state=decoder_states_inputs)<br/>decoder_states = [state_h_dec, state_c_dec]<br/>decoder_dense = model.layers[4]<br/>decoder_outputs = decoder_dense(decoder_outputs)<br/>decoder_model = Model(<br/>    [decoder_inputs] + decoder_states_inputs,<br/>    [decoder_outputs] + decoder_states)</span></pre><p id="6628" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们创建两个反向查找字典来解码反向序列。</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="008e" class="kq kr hi kl b fi lt lu l lv lw"># Reverse-lookup token index to decode sequences back to<br/># something readable.<br/>reverse_input_char_index = dict(<br/>    (i, char) for char, i in input_token_index.items())<br/>reverse_target_char_index = dict(<br/>    (i, char) for char, i in target_token_index.items())</span></pre><p id="825a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，让我们写一个预测函数。(解码功能)</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mo"><img src="../Images/afc0ca74c85e4fe11d3401778c37c253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VIgWl4sh9CoetdollINRPA.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图5。解码器工作|测试架构</figcaption></figure><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="1ade" class="kq kr hi kl b fi lt lu l lv lw"># Decodes an input sequence.  Future work should support beam search.<br/>def decode_sequence(input_seq):<br/>    # Encode the input as state vectors.<br/>    states_value = encoder_model.predict(input_seq)</span><span id="0c89" class="kq kr hi kl b fi md lu l lv lw"># Generate empty target sequence of length 1.<br/>    target_seq = np.zeros((1, 1, num_decoder_tokens))<br/>    # Populate the first character of target sequence with the start character.<br/>    target_seq[0, 0, target_token_index['\t']] = 1.</span><span id="2bd6" class="kq kr hi kl b fi md lu l lv lw"># Sampling loop for a batch of sequences<br/>    # (to simplify, here we assume a batch of size 1).<br/>    stop_condition = False<br/>    decoded_sentence = ''<br/>    while not stop_condition:<br/>        output_tokens, h, c = decoder_model.predict(<br/>            [target_seq] + states_value)</span><span id="329d" class="kq kr hi kl b fi md lu l lv lw"># Sample a token<br/>        sampled_token_index = np.argmax(output_tokens[0, -1, :])<br/>        sampled_char = reverse_target_char_index[sampled_token_index]<br/>        decoded_sentence += sampled_char</span><span id="097d" class="kq kr hi kl b fi md lu l lv lw"># Exit condition: either hit max length<br/>        # or find stop character.<br/>        if (sampled_char == '\n' or<br/>           len(decoded_sentence) &gt; max_decoder_seq_length):<br/>            stop_condition = True</span><span id="a6c4" class="kq kr hi kl b fi md lu l lv lw"># Update the target sequence (of length 1).<br/>        target_seq = np.zeros((1, 1, num_decoder_tokens))<br/>        target_seq[0, 0, sampled_token_index] = 1.</span><span id="e197" class="kq kr hi kl b fi md lu l lv lw"># Update states<br/>        states_value = [h, c]</span><span id="b9a0" class="kq kr hi kl b fi md lu l lv lw">return decoded_sentence</span></pre><p id="f91b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">！！！！！！！！让我们翻译！！！！！！</p><pre class="jf jg jh ji fd lp kl lq lr aw ls bi"><span id="e71c" class="kq kr hi kl b fi lt lu l lv lw">input_sentence = "How are you?"<br/>test_sentence_tokenized = np.zeros(<br/>    (1, max_encoder_seq_length, num_encoder_tokens), dtype='float32')<br/>for t, char in enumerate(input_sentence):<br/>    test_sentence_tokenized[0, t, input_token_index[char]] = 1.<br/>print("Input: ", input_sentence)<br/>print("Translated: ",decode_sequence(test_sentence_tokenized))</span><span id="4ba7" class="kq kr hi kl b fi md lu l lv lw"><strong class="kl hj">Output<br/></strong>Input:  How are you?<br/>Translated:  Comment allez-vous ?</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mp"><img src="../Images/455c100e8d3b0b0ec68be44feff5d432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y7YC4yGpC7v12LgbJ7NHjA.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">谷歌翻译截图</figcaption></figure><p id="f103" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献</strong></p><ol class=""><li id="778a" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><a class="ae jd" href="https://youtu.be/bBBYPuVUnug" rel="noopener ugc nofollow" target="_blank">Ananth Sankar博士利用编码器-解码器神经网络模型进行序列对序列学习</a></li><li id="c27b" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jd" href="https://nextjournal.com/gkoehler/machine-translation-seq2seq-cpu" rel="noopener ugc nofollow" target="_blank">使用序列对序列学习的机器翻译。</a></li><li id="07a1" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jd" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">十分钟介绍Keras中的序列对序列学习</a></li><li id="d886" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jd" href="https://github.com/simra/keras/blob/simra/s2srestore/examples/lstm_seq2seq_restore.py" rel="noopener ugc nofollow" target="_blank">如何在Keras中从加载的模型构造编码器？</a></li><li id="e664" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jd" href="https://towardsdatascience.com/neural-machine-translation-using-seq2seq-with-keras-c23540453c74" rel="noopener" target="_blank">神经机器翻译—使用seq2seq和Keras </a></li><li id="aef0" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jd" href="https://youtu.be/oF0Rboc4IJw" rel="noopener ugc nofollow" target="_blank">深度学习中的序列对序列(Seq2Seq)模型-人工智能期刊</a></li></ol></div></div>    
</body>
</html>