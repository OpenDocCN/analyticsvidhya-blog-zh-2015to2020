<html>
<head>
<title>NLP Libraries and Pretrained models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP库和预训练模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nlp-libraries-and-pretrained-models-94c9a53a295a?source=collection_archive---------5-----------------------#2019-11-04">https://medium.com/analytics-vidhya/nlp-libraries-and-pretrained-models-94c9a53a295a?source=collection_archive---------5-----------------------#2019-11-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/883a08d348ff602013ad3ea267d077dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BMDQeyWUvUY88RrMB2-fGA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Patrick Schneider 在<a class="ae iu" href="https://unsplash.com/collections/288927/books-libraries?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="fd2e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">本文是NLP <em class="jt">入门系列</em> <strong class="ix hj">的第三篇。</strong></em> 希望你已经涵盖了前两个。它们将为您提供进入NLP世界的起点。以下是这两者的链接:<br/> 1。<a class="ae iu" rel="noopener" href="/@ajeet.singh.ec14/everything-to-get-started-with-nlp-3ccfbb7405a2?source=friends_link&amp;sk=a66eaccadf8dcc00e4baf07d70ee1c33">NLP入门的一切。</a> <br/> 2。<a class="ae iu" rel="noopener" href="/@ajeet.singh.ec14/sentiment-classifier-using-tfidf-3ffce3f1cbd5?sk=cd23d564be7a9b6cef3504657cfe2cd9">使用Tfidf的情感分类器。</a></p><p id="8652" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将关注NLP库和预训练模型。这会非常有趣。所以系好安全带，让我们开始吧。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="5699" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi kb translated">NLP的世界是巨大的，需要花费大量的时间和资源才能掌握它。驯服它完全是另一回事。</p><p id="0ff1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是几年前的情景，但不是现在。感谢预先构建的NLP库和模型，它们让我们在巨大的NLP世界中的生活变得轻松。研究人员花费了多年的时间来制作这些库和模块。</p><p id="599b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于他们已经为我们设计了很多基准模型/库，而不是从头开始构建东西来解决类似的NLP问题，我们应该在我们的数据集上使用预先训练的模型/库。在情感分析(上一篇文章)中，您已经看到了scikit-learn库的例子，这是一个用于机器学习的python库。</p><p id="24e8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在NLP中有很多已经发生并仍在进行的研究。因此，自然地，已经存在大量的模型和库。有些很好，有些不太好。因此，为了使您选择哪个库/模型适合您的任务变得容易，我将解释最流行的模型和库。</p><p id="e02f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这只是库和模型的概述，请点击链接获取完整信息。</p><h1 id="7fca" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">1.NLP库</h1><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/c0c4c14b9258af37ff8849fc88e35200.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UvnkO0QBfQkz6EdhVotlzQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@alfonsmc10?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Alfons Morales </a>在<a class="ae iu" href="https://unsplash.com/s/photos/library?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="cb91" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有许多可用的NLP库。以下是前五名。</p><h2 id="1ee3" class="ln kl hi bd km lo lp lq kq lr ls lt ku jg lu lv ky jk lw lx lc jo ly lz lg ma bi translated">1.<a class="ae iu" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank">自然语言工具包(NLTK) </a></h2><ul class=""><li id="0cfd" class="mb mc hi ix b iy md jc me jg mf jk mg jo mh js mi mj mk ml bi translated">NLTK处于NLP世界的最前沿。它相当成熟，自2000年以来一直在发展。当使用Python进行语言处理时，人们广泛使用NLTK。</li><li id="fbbb" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js mi mj mk ml bi translated">NLTK是用非常好的编码实践编写的，相当容易使用。最重要的是，它带有数据集，你可以下载，使生活变得简单。</li><li id="55a1" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js mi mj mk ml bi translated">支持许多任务，如分类、标记化、词干提取、标记、解析等。刚接触NLP和Python的开发人员通常从NLTK开始。</li></ul><h2 id="4da9" class="ln kl hi bd km lo lp lq kq lr ls lt ku jg lu lv ky jk lw lx lc jo ly lz lg ma bi translated">2.S <a class="ae iu" href="https://spacy.io/" rel="noopener ugc nofollow" target="_blank"> paCy </a></h2><ul class=""><li id="0215" class="mb mc hi ix b iy md jc me jg mf jk mg jo mh js mi mj mk ml bi translated">可以说是NLP任务中第二著名的库。它非常容易使用，直观，非常强大。它自称为<em class="jt">工业级自然语言处理</em></li><li id="e294" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js mi mj mk ml bi translated">尽管相对较新，SpaCy提供了当今市场上最快的语法解析器。最重要的是，由于工具包是用Cython编写的，所以速度非常快。</li><li id="9892" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js mi mj mk ml bi translated">Spacy支持几乎所有的NLP特性。一些例子是NER，标记化。最重要的是，Spacy支持49种以上的语言，预先训练的单词向量，以及帖子标签等。</li></ul><h2 id="9fb3" class="ln kl hi bd km lo lp lq kq lr ls lt ku jg lu lv ky jk lw lx lc jo ly lz lg ma bi translated">3.<a class="ae iu" href="https://radimrehurek.com/gensim/" rel="noopener ugc nofollow" target="_blank"> Gensim </a></h2><ul class=""><li id="ad25" class="mb mc hi ix b iy md jc me jg mf jk mg jo mh js mi mj mk ml bi translated">一个相当专业的无监督语义建模库。它是高度优化的。因此，您可以期待高性能。</li><li id="bf6f" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js mi mj mk ml bi translated">说到语义分析和主题建模，Gensim是首选库。它速度快、可扩展且非常高效。</li><li id="fbc8" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js mi mj mk ml bi translated">Gensim广泛使用Numpy，这使得它非常快。除了速度，由于Numpy，内存也进行了高度优化。由于这两个因素，Gensim可以处理大量的数据。</li></ul><h2 id="39fb" class="ln kl hi bd km lo lp lq kq lr ls lt ku jg lu lv ky jk lw lx lc jo ly lz lg ma bi translated">4.<a class="ae iu" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a></h2><ul class=""><li id="41b7" class="mb mc hi ix b iy md jc me jg mf jk mg jo mh js mi mj mk ml bi translated">到目前为止，Python中最重要的机器学习库。您已经在情感分析文章中看到了这个库的用法。</li><li id="7d32" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js mi mj mk ml bi translated">这个库提供了各种文本预处理工具。我们已经使用了其中的一些。</li></ul><h2 id="5fbf" class="ln kl hi bd km lo lp lq kq lr ls lt ku jg lu lv ky jk lw lx lc jo ly lz lg ma bi translated">5.<a class="ae iu" href="https://pypi.org/project/polyglot/" rel="noopener ugc nofollow" target="_blank">多语种</a></h2><ul class=""><li id="067a" class="mb mc hi ix b iy md jc me jg mf jk mg jo mh js mi mj mk ml bi translated">Polyglot主要是为多语言应用程序设计的，因此与我们之前讨论过的其他库有很大不同。尽管它也提供了典型的NLP特性，但多语言性使它与众不同。</li><li id="444c" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js mi mj mk ml bi translated">它提供了同时处理各种语言的功能。虽然其他图书馆也有这样的功能，但不如Polyglot准确和先进。</li><li id="b369" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js mi mj mk ml bi translated">Polyglot的后端也依赖于Numpy，因此它也非常快。</li></ul><h1 id="357b" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">这些库的利与弊</h1><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/719955891fbba2695d054e512cc3edd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8yGYQBsfDaTdLcFG_oEqeg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图书馆的比较。<a class="ae iu" href="https://activewizards.com/blog/comparison-of-python-nlp-libraries/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="84db" class="kk kl hi bd km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld mw lf lg lh bi translated">如何使用这些库</h1><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/c88cba37912289a904f942190fd5e064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXpdlQHYUqs29mib0AfC8g.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">由<a class="ae iu" href="https://unsplash.com/@ilyapavlov?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">伊利亚·巴甫洛夫</a>在<a class="ae iu" href="https://unsplash.com/s/photos/coding?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="4ee6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">既然我们已经看到了这些库的优缺点，也比较了它们，那么让我们看看它们是如何工作的，以及我们如何使用它们。</p><p id="f9d3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将看到如何安装每个库，也将看到一个如何使用它们的简短示例。我们已经看到了<em class="jt"> scikit-learn </em>如何工作。所以我们不需要再讨论了。对于<em class="jt"> NLTK </em>、<em class="jt"> spaCy、</em>和<em class="jt"> Polyglot </em>我们将随机抽取一个文本，看看它们如何执行命名实体识别(NER)。即它们能够多好地识别一个单词是否是一个实体。对于<em class="jt"> Gensim </em>，我们将看到一个文档相似性的示例。关于这些库的所有其他用法，请查阅它们的文档。我已经在每个库的标题中给出了每个库的文档链接。</p><p id="47d3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们将使用<em class="jt"> NLTK </em>、<em class="jt"> spaCy </em>和<em class="jt"> Polyglot </em>做<strong class="ix hj"> NER </strong>(命名实体识别)<strong class="ix hj"> </strong>。</p><blockquote class="my mz na"><p id="3a23" class="iv iw jt ix b iy iz ja jb jc jd je jf nb jh ji jj nc jl jm jn nd jp jq jr js hb bi translated">请注意，我用的是Jupyter笔记本和Python 3。</p></blockquote><pre class="lj lk ll lm fd ne nf ng nh aw ni bi"><span id="aa70" class="ln kl hi nf b fi nj nk l nl nm">#We will take this paragraph to perform NER using all three libraries.</span><span id="16f4" class="ln kl hi nf b fi nn nk l nl nm">text = '''But Google is starting from behind. The company made a late push into hardware, and Apple’s Siri, available on iPhones, and Amazon’s Alexa software, which runs on its Echo and Dot devices, have clear leads in consumer adoption. I was born in India on 23/03/1996. Chennai is a coastal city.'''</span></pre><p id="55bc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">NLTK</p><pre class="lj lk ll lm fd ne nf ng nh aw ni bi"><span id="9d6d" class="ln kl hi nf b fi nj nk l nl nm">#installing NLTK. <br/>#If having any trouble go to this link : <a class="ae iu" href="https://pypi.org/project/nltk/" rel="noopener ugc nofollow" target="_blank">https://pypi.org/project/nltk/</a></span><span id="6708" class="ln kl hi nf b fi nn nk l nl nm">!pip install nltk</span><span id="c958" class="ln kl hi nf b fi nn nk l nl nm">#importing tokenizer, POS tagger and NER chunker<br/>from nltk import word_tokenize, pos_tag, ne_chunk</span><span id="b168" class="ln kl hi nf b fi nn nk l nl nm">#performing tokenization followed by POS tagging. Then extracting #NER.<br/>print(ne_chunk(pos_tag(word_tokenize(text))))</span></pre><p id="7bd1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es no"><img src="../Images/b6976f63d44ec1051ab6bd1a0f7c4c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lHfgMEUnGC0KTsogv7k2CA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">输出的一小部分</figcaption></figure><p id="a6ed" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出是令牌、POS标签对的形式。您可以看到NLTK已经将整个文本转换成了标记，然后执行了词性标记，接着提取命名实体。所有命名的实体都在括号中。<br/>比如<strong class="ix hj"> <em class="jt"> (GPE印度/NNP) </em> </strong>就是说<em class="jt">印度</em>是一个<em class="jt"> GPE(地点)</em>而它的<em class="jt"> POS </em>标签是一个<em class="jt">名词(专有，单数)</em>。<br/>有很多POS标签，要了解所有这些标签，请参考这个StackOverflow <a class="ae iu" href="https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk" rel="noopener ugc nofollow" target="_blank">答案。</a></p><h1 id="5a36" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">宽大的</h1><pre class="lj lk ll lm fd ne nf ng nh aw ni bi"><span id="de04" class="ln kl hi nf b fi nj nk l nl nm">#installing spaCy. <br/>#If having any trouble go to this link : <a class="ae iu" href="https://spacy.io/usage" rel="noopener ugc nofollow" target="_blank">https://spacy.io/usage</a></span><span id="ec45" class="ln kl hi nf b fi nn nk l nl nm">!pip install -U spacy</span><span id="66ce" class="ln kl hi nf b fi nn nk l nl nm">#To use spaCy you must have some language model. The model that we #need depends on our language and also on our usage. <br/>#Please refer the above spaCy installation link for full details. <br/>#We will download the Enlish model for our usage</span><span id="5c14" class="ln kl hi nf b fi nn nk l nl nm">!python -m spacy download en</span><span id="2757" class="ln kl hi nf b fi nn nk l nl nm">#To download English's small statistical model. Medium and Large #models are also available.</span><span id="4d58" class="ln kl hi nf b fi nn nk l nl nm">!python -m spacy download en_core_web_sm</span><span id="b7f0" class="ln kl hi nf b fi nn nk l nl nm">#importing spaCy, English's small statistical model and loading it</span><span id="b42d" class="ln kl hi nf b fi nn nk l nl nm">import spacy<br/>import en_core_web_sm<br/>nlp = en_core_web_sm.load()</span><span id="7613" class="ln kl hi nf b fi nn nk l nl nm">#performing NER over the same text.</span><span id="44dc" class="ln kl hi nf b fi nn nk l nl nm">doc = nlp(text)<br/>for entity in doc.ents:<br/>    print(entity, entity.label_)</span></pre><p id="280f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es np"><img src="../Images/d6b8842d1a1a9a6cfadb6c980d6dd684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SuMxXL4gPKx3PIZ294DA8w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">空间输出</figcaption></figure><p id="802e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在spaCy中，当我们执行NER时，只显示NER的输出。其他所有细节都被隐藏了。spaCy的NER的输出是令牌NER标签对的形式。例如，<strong class="ix hj"><em class="jt">23/03/1996</em></strong>表示<em class="jt"> 23/03/1996 </em>是一个日期，<em class="jt">日期</em>是它的<em class="jt"> NER </em>标签。</p><h1 id="5a14" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">懂得多种语言的</h1><pre class="lj lk ll lm fd ne nf ng nh aw ni bi"><span id="ce57" class="ln kl hi nf b fi nj nk l nl nm">#installing Polyglot and other important modlules. All the other #three modules must be present for the working of polyglot. <br/>#If having any trouble go to this link : <a class="ae iu" href="https://pypi.org/project/polyglot/" rel="noopener ugc nofollow" target="_blank">https://pypi.org/project/polyglot/</a></span><span id="9b21" class="ln kl hi nf b fi nn nk l nl nm">!pip install PyICU<br/>!pip install pycld2<br/>!pip install morfessor<br/>!pip install polyglot</span><span id="c415" class="ln kl hi nf b fi nn nk l nl nm">#Downloading other requirements like POS tagger, embeddings and NER #tagger</span><span id="dc90" class="ln kl hi nf b fi nn nk l nl nm">!polyglot download pos2.en<br/>!polyglot download embeddings2.en<br/>!polyglot download ner2.en</span><span id="2021" class="ln kl hi nf b fi nn nk l nl nm">poly_text = Text(text)<br/>print(poly_text.entities)</span></pre><p id="6486" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nq"><img src="../Images/13df4db16a5f1dbece2aa678170244f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z52j8az5teW4HokzcRrl4Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">多语种输出</figcaption></figure><p id="b2a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Polyglot只给出命名的实体作为输出，并隐藏所有其他细节。输出的形式是<strong class="ix hj"> <em class="jt"> NER标签(['token']) </em> </strong>。例如，<em class="jt"> I-ORG(['Google']) </em>意味着<em class="jt"> Google </em>是一个实体，I-ORG是它的<em class="jt"> NER </em>标签。</p><h1 id="c241" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">根西姆</h1><p id="74de" class="pw-post-body-paragraph iv iw hi ix b iy md ja jb jc me je jf jg nr ji jj jk ns jm jn jo nt jq jr js hb bi translated">对于Gensim，它不像上面三个那样工作，我们将从<a class="ae iu" href="https://radimrehurek.com/gensim/tutorial.html#id2" rel="noopener ugc nofollow" target="_blank"> Gensim docs </a>中取一个相似性查找的例子。有关Gensim的所有其他示例，请访问此<a class="ae iu" href="https://radimrehurek.com/gensim/tutorial.html#id2" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="5f88" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于我们的例子，我们将采用9个文档，并找到它们与样本文档的相似性。</p><pre class="lj lk ll lm fd ne nf ng nh aw ni bi"><span id="943d" class="ln kl hi nf b fi nj nk l nl nm">#installing Gensim<br/>#If having any trouble go to this link : <a class="ae iu" href="https://pypi.org/project/gensim/" rel="noopener ugc nofollow" target="_blank">https://pypi.org/project/gensim/</a></span><span id="8362" class="ln kl hi nf b fi nn nk l nl nm">!pip install gensim</span><span id="b288" class="ln kl hi nf b fi nn nk l nl nm">#importing gensim<br/>import gensim</span><span id="037b" class="ln kl hi nf b fi nn nk l nl nm">#First, let’s create a small corpus of nine documents and twelve #features.<br/>#From the sentiment analysis example we are familiar with how #documents are converted into vectors. This is same. <br/>#We have taken documents as vectors. Total number of features are #12.</span><span id="4593" class="ln kl hi nf b fi nn nk l nl nm">corpus = [[(0, 1.0), (1, 1.0), (2, 1.0)],<br/>           [(2,1.0), (3,1.0), (4,1.0), (5,1.0), (6,1.0), (8,1.0)],<br/>           [(1, 1.0), (3, 1.0), (4, 1.0), (7, 1.0)],<br/>           [(0, 1.0), (4, 1.0), (7, 1.0)],<br/>           [(3, 1.0), (5, 1.0), (6, 1.0)],<br/>           [(9, 1.0)],<br/>           [(9, 1.0), (10, 1.0)],<br/>           [(9, 1.0), (10, 1.0), (11, 1.0)],<br/>           [(8, 1.0), (10, 1.0), (11, 1.0)]]</span><span id="8cc2" class="ln kl hi nf b fi nn nk l nl nm">#Now we will convert the vectors in our corpus into tf-idf vectors.<br/>#We have already seen tf-idf vectors in sentiment analyses example.</span><span id="4c40" class="ln kl hi nf b fi nn nk l nl nm">from gensim import models<br/>tfidf = models.TfidfModel(corpus)</span><span id="9de5" class="ln kl hi nf b fi nn nk l nl nm">#Now we will create a sample document to calculate similarity with #all other documents in our corpus.<br/>#we will apply tf-idf vectorization over this document also.<br/>#In the output you can see the output as tf-idf vectors.</span><span id="d714" class="ln kl hi nf b fi nn nk l nl nm">sample_doc = [(0, 1), (4, 1)]<br/>print(tfidf[sample_doc])</span></pre><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es nu"><img src="../Images/1b811ca9fa8a2f75b2e87886a8581958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*RCbiWS6AW_U1vnB4jQSKJQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">tfidf输出</figcaption></figure><pre class="lj lk ll lm fd ne nf ng nh aw ni bi"><span id="be8a" class="ln kl hi nf b fi nj nk l nl nm">#let us transform the whole corpus via TfIdf and index it, in #preparation for finding similarity</span><span id="2012" class="ln kl hi nf b fi nn nk l nl nm">from gensim import similarities</span><span id="0e54" class="ln kl hi nf b fi nn nk l nl nm">index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=12)</span><span id="022d" class="ln kl hi nf b fi nn nk l nl nm">#Now calculate the similarity of our sample document against every #document in the corpus:</span><span id="1270" class="ln kl hi nf b fi nn nk l nl nm">sims = index[tfidf[sample_doc]]</span><span id="f1ed" class="ln kl hi nf b fi nn nk l nl nm">print(list(enumerate(sims)))</span></pre><p id="12ac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出:</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nv"><img src="../Images/b91e6a67b8005be48a271e311800db99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BV-9U3IjaBWqHkqxt0muwg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Gensim相似性输出</figcaption></figure><p id="d799" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出是具有文档编号和相似性的元组列表的形式。例如，<em class="jt">第一个文档(索引=0)的相似性得分为0.466=46.6%，第二个文档的相似性得分为19.1%，等等</em>。</p><p id="b75e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，根据TfIdf文档表示和余弦相似性度量，与我们的sample_doc最相似的是4号文档，相似性得分为77.8%。请注意，在TfIdf表示中，与sample_doc没有任何共同特征的任何文档(文档编号5-9)的相似性得分为0.0。</p><p id="2ce8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> scikit-learn </strong></p><p id="f4b3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为我们已经在上一篇文章中广泛地看到了它的用法，所以我在这里就不再赘述了。</p><h1 id="37d0" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">关于图书馆的结束注释</h1><p id="d7b0" class="pw-post-body-paragraph iv iw hi ix b iy md ja jb jc me je jf jg nr ji jj jk ns jm jn jo nt jq jr js hb bi translated">请注意，所有这些库并不像它们的文档所显示的那样工作良好。您需要尝试不同的库来检查哪一个满足您的目标。<br/>例如，在空间<strong class="ix hj"> <em class="jt">钦奈</em> </strong>不是一个实体，但在NLTK中却是。在NLTK<strong class="ix hj"><em class="jt">1996年3月23日</em> </strong>不是一个日期，但在空间中却是。</p><p id="ad7e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果任何库不能满足您的要求，那么您可以为自己的数据集训练库。训练图书馆可能会花费很多时间和资源，但它可以帮助很多。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="c6a2" class="kk kl hi bd km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld mw lf lg lh bi translated">2.迁移学习和NLP预训练模型</h1><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nw"><img src="../Images/4b3515250f9f758f1d9af24f0e7b475e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zx_Kph1XOoi5Cms2pDde7g.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@priscilladupreez?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">普里西拉·杜·普里兹</a>在<a class="ae iu" href="https://unsplash.com/s/photos/teaching?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="6be7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">训练库的替代方法可以是训练预训练模型。</p><p id="3b5a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">目录</strong></p><ol class=""><li id="4434" class="mb mc hi ix b iy iz jc jd jg nx jk ny jo nz js oa mj mk ml bi translated">什么是迁移学习？</li><li id="613c" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js oa mj mk ml bi translated">什么是预训练模型？</li><li id="599d" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js oa mj mk ml bi translated">为什么重要？</li><li id="cb38" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js oa mj mk ml bi translated">如何使用它们？</li><li id="0877" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js oa mj mk ml bi translated">有哪些最好的型号？</li></ol><p id="d463" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们逐一讨论所有这些主题</p><ol class=""><li id="5c09" class="mb mc hi ix b iy iz jc jd jg nx jk ny jo nz js oa mj mk ml bi translated"><strong class="ix hj">什么是迁移学习？</strong> <br/>迁移学习(Transfer learning)是一种机器学习方法，其中为一项任务开发的模型被重新用作第二项任务的模型的起点。这就像从某人那里学到一些东西，然后将这些知识用于其他任务。</li><li id="528f" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js oa mj mk ml bi translated"><strong class="ix hj">什么是预训练模型？</strong> <br/>预训练模型是其他人为了解决类似问题而创建的模型。这个模型最重要的因素是它的学习。它已经从训练中学到了什么。所以与其从头开始解决一个类似的问题，不如把它的学习作为一个起点。</li><li id="3df5" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js oa mj mk ml bi translated"><strong class="ix hj">为什么重要？特定模型的研究人员和作者已经投入数月甚至数年来训练一个模型。将其细化到已经实现基准结果的水平。既然已经完成了，为什么还要重复同样的步骤呢？我们唯一需要做的就是针对我们的特定用例对模型进行微调。<br/>比如你要造一辆自学习汽车。你可以花费数年时间从头开始构建一个像样的图像识别算法，或者你可以采用谷歌的Inception模型(一个预先训练的模型)，它基于ImageNet数据来识别这些图片中的图像。</strong></li><li id="78b3" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js oa mj mk ml bi translated"><strong class="ix hj">如何使用它们？</strong> <br/>请参考这篇<a class="ae iu" href="https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/" rel="noopener ugc nofollow" target="_blank">博客</a>，因为他们已经解释得很好了。你可以把这个作为我正在使用的预训练模型的例子。那就是利用别人的知识，而不是从零开始。</li><li id="e3d7" class="mb mc hi ix b iy mm jc mn jg mo jk mp jo mq js oa mj mk ml bi translated"><strong class="ix hj">有哪些最好的型号？</strong> <br/> ULMFiT，Transformer，谷歌的BERT，Transformer-XL</li></ol><p id="987d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要对所有这些进行概述，请浏览此<a class="ae iu" href="https://www.analyticsvidhya.com/blog/2019/03/pretrained-models-get-started-nlp/" rel="noopener ugc nofollow" target="_blank">链接</a>。在那里，您还可以找到更多资源来完全掌握预训练模型。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="c135" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我试图让你深入了解NLP的巨大世界。我希望你对NLP的工作原理有所了解。我希望你能利用这些知识去探索更多。</p><p id="07b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你喜欢这篇文章，那么你可以想按多少次拍手按钮就按多少次。还有，你可以在<a class="ae iu" href="https://www.linkedin.com/in/singhajeet23/" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">LinkedIn</strong></a><strong class="ix hj">上联系我，或者在</strong><a class="ae iu" href="https://github.com/AjeetSingh02" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">GitHub</strong></a><strong class="ix hj">上关注我。</strong></p><blockquote class="ob"><p id="2c73" class="oc od hi bd oe of og oh oi oj ok js dx translated">仅此而已。感谢阅读。快乐学习。</p></blockquote><figure class="om on oo op oq ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ol"><img src="../Images/b55b7e9c4a9c754cd2a2ae45bfdd3b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hUItmMX1gdBLya0AwsIW_Q.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">克拉克·蒂布斯在<a class="ae iu" href="https://unsplash.com/s/photos/success?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div></div>    
</body>
</html>