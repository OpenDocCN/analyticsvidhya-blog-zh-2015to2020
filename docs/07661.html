<html>
<head>
<title>The transfer learning experience with VGG16 and Cifar 10 dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">VGG16和Cifar 10数据集的迁移学习体验</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-transfer-learning-experience-with-vgg16-and-cifar-10-dataset-9b25b306a23f?source=collection_archive---------4-----------------------#2020-07-03">https://medium.com/analytics-vidhya/the-transfer-learning-experience-with-vgg16-and-cifar-10-dataset-9b25b306a23f?source=collection_archive---------4-----------------------#2020-07-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/aa8bbd13e9f2567ffe64191f45981816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*url4DmIvvByWRaOI.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">摘自<a class="ae iu" href="http://www.thebluediamondgallery.com/wooden-tile/t/transfer.html" rel="noopener ugc nofollow" target="_blank">http://www . the blue diamond gallery . com/wood-tile/t/transfer . html</a>尼克·扬森</figcaption></figure><p id="2e7e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">摘要</strong></p><p id="7577" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇博客中，我将讲述我如何通过使用迁移学习，使用Cifar-10获得超过88%的准确率(92% epoch 22)，我使用了VGG16，应用了非常低的恒定学习速率，并实现了上采样功能，以获得更多的数据点进行处理</p><p id="b509" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">简介</strong></p><p id="b268" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如今，我们正处于机器学习的大好时光，我们有许多著名的模型，它们都有很好的结果，可以快速准确地做出预测。因此，我们应该将这些工具应用到我们的日常预测中，关注我们模型的目标，而不仅仅是it的足迹。为此，我们需要了解我们的数据集，并尝试应用正确的模型，对数据集进行必要的预处理，并在必要时对那些著名的模型进行修正。</p><p id="109d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">材料和方法</strong></p><p id="9cb9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本练习中，我使用了:</p><p id="e053" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Keras 1.12</p><p id="4b36" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用GPU的Colab:对我来说是我见过的编译和训练模型的最佳选择(性价比高)。是Jupyter保存在drive或者上传到GitHub。</p><p id="9b05" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">VGG16模型:我选择这个模型是因为我认为，如果我使用dense121或resnet50这样的更深入的模型，这个模型的精度还不错，在这个实践中的结果非常好，我与dense121进行了比较，它们之间的精度差异只有0.08%。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/34e467658ecffe68df4485833bc3adf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*ExFbfl4s8PB3DlDrKa2fPg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">摘自<a class="ae iu" href="https://keras.io/api/applications/" rel="noopener ugc nofollow" target="_blank">https://keras.io/api/applications/</a></figcaption></figure><p id="02e4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Cifar 10数据集:<em class="jy">“由10类60000张32x32的彩色图像组成，每类6000张。有50000个训练图像和10000个测试图像。”</em></p><p id="bc32" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Upsampling2D:用于获取每个图像的更多数据点的方法</p><p id="0bf8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">冻结所有的VGG16模型:我试着在一些层上获得更高的精确度，但是训练的时间增加了很多，结果几乎是一样的。</p><p id="f51b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">恒定学习率:我尝试使用学习率衰减，但结果不太好，我将在后面讨论。</p><p id="4b43" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">结果</strong></p><p id="e2de" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">应用VGG16模型的结果增加了两层并具有常数学习</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jz"><img src="../Images/33ab18ac39731cb1060185966cef5c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*bYk84G84p-RTOhkhd875LQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">模型摘要</figcaption></figure><p id="0af1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随着学习率衰减:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ka"><img src="../Images/f39f1fe2a2cc93f3371459dd557efb38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qEmi7nYkaNgD2vOasahzUg.png"/></div></div></figure><p id="caf0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在恒定的学习速率下:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kb"><img src="../Images/cf984dd52e81bc2feedd94087c2366c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FO73CThOQrQBtrUkxJfCMA.png"/></div></div></figure><p id="ce7e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到，我用恒定的学习率得到92.05%，而不是用学习率衰减得到80.9%。我添加了2层ReLU激活和1层softmax。</p><p id="0ec0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">讨论</strong></p><p id="85e4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对我来说，最重要的是实现一个非常低的恒定学习率，这可能是因为模型是用“imagenet”训练的，应用梯度下降的步骤不应该很大，因为我们可能会进入一个不是真正最小值的区域(见图，模型应该试图获得最小值，但在某些情况下可能会卡在不是最小值的低点， 我们可以看到只有一个点试图向下)另一个重要的点是预处理，因为cifar 10有低分辨率的图像，我们不能从它们身上取很多点，为此，上采样对提高精度有很大帮助。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kc"><img src="../Images/96967d087b611c9d750ba1505e086182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aW9vVPLYyBZKlRAL.png"/></div></div></figure><p id="2927" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">参考书目</strong></p><p id="3971" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">吴恩达视频【https://www.youtube.com/watch?v=FQM13HkEfBk T2】index = 20&amp;list = plk DAE 6 sczn 6 GL 29 AOE 31 wdvwsg-KnDzF</p><p id="60ed" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Santiago VG<a class="ae iu" rel="noopener" href="/@svelez.velezgarcia/transfer-learning-ride-fa9f2a5d69eb">https://medium . com/@ svelez . velez Garcia/transfer-learning-ride-fa 9 f2a 5d 69 EB</a></p><div class="kd ke ez fb kf kg"><a href="https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a" rel="noopener follow" target="_blank"><div class="kh ab dw"><div class="ki ab kj cl cj kk"><h2 class="bd hj fi z dy kl ea eb km ed ef hh bi translated">在深度学习中通过真实世界的应用转移学习的综合实践指南</h2><div class="kn l"><h3 class="bd b fi z dy kl ea eb km ed ef dx translated">用知识转移的力量进行深度学习！</h3></div><div class="ko l"><p class="bd b fp z dy kl ea eb km ed ef dx translated">towardsdatascience.com</p></div></div><div class="kp l"><div class="kq l kr ks kt kp ku io kg"/></div></div></a></div><p id="d32a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Keras应用<a class="ae iu" href="https://keras.io/api/applications/" rel="noopener ugc nofollow" target="_blank">https://keras.io/api/applications/</a></p><p id="9158" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">附录</strong></p><pre class="ju jv jw jx fd kv kw kx ky aw kz bi"><span id="9ceb" class="la lb hi kw b fi lc ld l le lf">"""</span><span id="e517" class="la lb hi kw b fi lg ld l le lf">This script has the method</span><span id="c243" class="la lb hi kw b fi lg ld l le lf">preprocess_data(X, Y): and decay</span><span id="8882" class="la lb hi kw b fi lg ld l le lf">and use transfer learning with VGG16 model</span><span id="13b9" class="la lb hi kw b fi lg ld l le lf">"""</span><span id="e938" class="la lb hi kw b fi lg ld l le lf">import tensorflow.keras as K</span><span id="64ab" class="la lb hi kw b fi lg ld l le lf">import datetime</span><span id="721a" class="la lb hi kw b fi lg ld l le lf">def preprocess_data(X, Y):<br/>""" This method has the preprocess to train a model """</span><span id="290d" class="la lb hi kw b fi lg ld l le lf"># applying astype to change float64 to float32 for version 1.12</span><span id="501f" class="la lb hi kw b fi lg ld l le lf"># X = X.astype('float32')</span><span id="ed6f" class="la lb hi kw b fi lg ld l le lf">#using preprocess VGG16 method by default to scale images and their values</span><span id="03bc" class="la lb hi kw b fi lg ld l le lf">X_p = K.applications.vgg16.preprocess_input(X)</span><span id="a578" class="la lb hi kw b fi lg ld l le lf"># changind labels to one-hot representation</span><span id="0eea" class="la lb hi kw b fi lg ld l le lf">Y_p = K.utils.to_categorical(Y, 10)</span><span id="46c2" class="la lb hi kw b fi lg ld l le lf">return (X_p, Y_p)</span><span id="9340" class="la lb hi kw b fi lg ld l le lf">def decay(epoch):</span><span id="4293" class="la lb hi kw b fi lg ld l le lf">""" This method create the alpha"""</span><span id="1b61" class="la lb hi kw b fi lg ld l le lf"># returning a very small constant learning rate</span><span id="d6f0" class="la lb hi kw b fi lg ld l le lf">return 0.001 / (1 + 1 * 30)</span><span id="7a9a" class="la lb hi kw b fi lg ld l le lf">if __name__ == "__main__":</span><span id="0fa8" class="la lb hi kw b fi lg ld l le lf"># loading data and using preprocess for training and validation dataset</span><span id="fe02" class="la lb hi kw b fi lg ld l le lf">(Xt, Yt), (X, Y) = K.datasets.cifar10.load_data()</span><span id="66c8" class="la lb hi kw b fi lg ld l le lf">X_p, Y_p = preprocess_data(Xt, Yt)</span><span id="f9ec" class="la lb hi kw b fi lg ld l le lf">Xv_p, Yv_p = preprocess_data(X, Y)</span><span id="21f4" class="la lb hi kw b fi lg ld l le lf"># Getting the model without the last layers, trained with imagenet and with average pooling</span><span id="3728" class="la lb hi kw b fi lg ld l le lf">base_model = K.applications.vgg16.VGG16(include_top=False,</span><span id="e45e" class="la lb hi kw b fi lg ld l le lf">weights='imagenet',</span><span id="51f4" class="la lb hi kw b fi lg ld l le lf">pooling='avg',</span><span id="3c4f" class="la lb hi kw b fi lg ld l le lf">input_shape=(32,32,3)</span><span id="9f34" class="la lb hi kw b fi lg ld l le lf">)</span><span id="e456" class="la lb hi kw b fi lg ld l le lf"># create the new model applying the base_model (VGG16)</span><span id="8db1" class="la lb hi kw b fi lg ld l le lf">model= K.Sequential()</span><span id="d37d" class="la lb hi kw b fi lg ld l le lf"># using upsamplign to get more data points and improve the predictions</span><span id="a0f3" class="la lb hi kw b fi lg ld l le lf">model.add(K.layers.UpSampling2D())</span><span id="cc8b" class="la lb hi kw b fi lg ld l le lf">model.add(base_model)</span><span id="cde9" class="la lb hi kw b fi lg ld l le lf">model.add(K.layers.Flatten())</span><span id="3326" class="la lb hi kw b fi lg ld l le lf">model.add(K.layers.Dense(512, activation=('relu')))</span><span id="6257" class="la lb hi kw b fi lg ld l le lf">model.add(K.layers.Dropout(0.2))</span><span id="a1fe" class="la lb hi kw b fi lg ld l le lf">model.add(K.layers.Dense(256, activation=('relu')))</span><span id="a3db" class="la lb hi kw b fi lg ld l le lf">model.add(K.layers.Dropout(0.2))</span><span id="c8b5" class="la lb hi kw b fi lg ld l le lf">model.add(K.layers.Dense(10, activation=('softmax')))</span><span id="c198" class="la lb hi kw b fi lg ld l le lf"># adding callbacks</span><span id="0537" class="la lb hi kw b fi lg ld l le lf">callback = []</span><span id="06fd" class="la lb hi kw b fi lg ld l le lf">callback += [K.callbacks.LearningRateScheduler(decay, verbose=1)]</span><span id="7845" class="la lb hi kw b fi lg ld l le lf">#callback += [K.callbacks.ModelCheckpoint('cifar10.h5',</span><span id="6c59" class="la lb hi kw b fi lg ld l le lf">#                                         save_best_only=True,</span><span id="1db1" class="la lb hi kw b fi lg ld l le lf">#                                        mode='min'</span><span id="fc83" class="la lb hi kw b fi lg ld l le lf">#                                         )]</span><span id="0ef3" class="la lb hi kw b fi lg ld l le lf"># tensorboard callback</span><span id="932e" class="la lb hi kw b fi lg ld l le lf"># log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")</span><span id="e90b" class="la lb hi kw b fi lg ld l le lf"># callback += [K.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)]</span><span id="93a8" class="la lb hi kw b fi lg ld l le lf"># Compiling model with adam optimizer and looking the accuracy</span><span id="ad3a" class="la lb hi kw b fi lg ld l le lf">model.compile(optimizer='adam', loss='categorical_crossentropy',</span><span id="b1de" class="la lb hi kw b fi lg ld l le lf">metrics=['accuracy'])</span><span id="1e40" class="la lb hi kw b fi lg ld l le lf"># training model with mini batch using shuffle data</span><span id="a7ca" class="la lb hi kw b fi lg ld l le lf">model.fit(x=X_p, y=Y_p,</span><span id="6c25" class="la lb hi kw b fi lg ld l le lf">batch_size=128,</span><span id="0c2b" class="la lb hi kw b fi lg ld l le lf">validation_data=(Xv_p, Yv_p),</span><span id="2275" class="la lb hi kw b fi lg ld l le lf">epochs=30, shuffle=True,</span><span id="62ae" class="la lb hi kw b fi lg ld l le lf">callbacks=callback,</span><span id="8715" class="la lb hi kw b fi lg ld l le lf">verbose=1</span><span id="0905" class="la lb hi kw b fi lg ld l le lf">)</span></pre><p id="4d4f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://github.com/PauloMorillo/holbertonschool-machine_learning/blob/master/supervised_learning/0x09-transfer_learning/0-transfer.py" rel="noopener ugc nofollow" target="_blank">https://github . com/PauloMorillo/holbertonschool-machine _ learning/blob/master/supervised _ learning/0x 09-transfer _ learning/0-transfer . py</a></p></div></div>    
</body>
</html>