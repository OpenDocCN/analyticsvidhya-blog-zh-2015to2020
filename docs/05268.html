<html>
<head>
<title>Attention Mechanism in Deep Neural network Simplified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度神经网络中的注意机制简化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/attention-mechanism-in-deep-neural-network-simplified-6b3142b1be78?source=collection_archive---------23-----------------------#2020-04-15">https://medium.com/analytics-vidhya/attention-mechanism-in-deep-neural-network-simplified-6b3142b1be78?source=collection_archive---------23-----------------------#2020-04-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/f0e26a846e2c07115701bb102fb357fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*mkS6RuFZ5ZOCUlCEYLckKg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">DNN利用注意力将文本转化为图像</figcaption></figure><p id="5f3c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意力是以高分辨率观看图像或文字的特定部分，而以低分辨率观看其余部分的过程。假设我想预测这个句子中的下一个单词"<em class="jo">我在卡纳塔克邦出生并长大，因此我说流利的_ _ _ _ _ _ _ _ _ "</em>。要预测这个句子中的下一个单词，注意"<em class="jo">卡纳塔克邦</em>"并说"<em class="jo">卡纳达语</em>"是非常容易的，而不是必须查看整个句子并处理每个单词。注意力是一个重要的向量，它估计一个元素如何与其他元素紧密相关。</p><p id="78da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意力广泛应用于自然语言处理和计算机视觉任务，如机器翻译、图像字幕生成、微软的注意力甘等。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jp"><img src="../Images/954c182c95584432ebae1e7c7ddbe94c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*upOoEpcIB9AfaLmN0L1JNA.jpeg"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">seq2seq模型</figcaption></figure><p id="8fcd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">之前attention神经机器翻译是用编码器解码器模型或者sequence2sequrnce模型完成的，如上图所示。这里，编码器给出“编码器向量”或“上下文向量”,这是迄今为止它所看到的所有内容的汇总(这是编码器的最后隐藏状态),这是解码器的初始隐藏状态，该系统的主要缺点是，如果编码器做出了错误的汇总，解码器输出将是错误的，这发生在输入大小很长时，这被称为长距离依赖性问题。</p><p id="2360" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">注意力是如何工作的？</em>T9】</strong></p><p id="de5b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意力被提出作为seq2seq模型的问题的解决方案。这里，上下文向量通过在上下文向量和输入序列之间创建快捷方式来访问整个输入序列，并且这些快捷方式的权重对于不同的解码器单元是不同的。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es jy"><img src="../Images/4f01167be4007d3e7b9ccf129762ab9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*E3-9G7lQV2sMOp6wbrMvFA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">具有注意机制的编解码模型</figcaption></figure><p id="2ade" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图是bahda nau 2015年关于注意力的论文中提到的具有注意力机制的编码器-解码器模型。</p><p id="c041" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设我们要把一个句子从英语翻译成印地语，</p><p id="713c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“你好<em class="jo">我是杜鲁夫</em>”对“<em class="jo"> नमस्ते मैं ध्रुव हूँ </em>”。</p><p id="3179" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为此，输入序列x的长度为4，输出序列y的长度为4。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es jz"><img src="../Images/8d2caccde2473197eb453260f8250348.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*sYI6G6N92_UZ21UBXT2pcA.png"/></div></figure><p id="46bd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图2中的编码器是双向RNN，具有前向和后向隐藏状态，所有向量h1、h2..等。，在他们的工作中使用的基本上是编码器中前向和后向隐藏状态的串联，其中i = 1，2，3，4。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/ef898b3db53724dab27995b557e0f843.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*i6lao0UjO1V2tfNja5mlAg.png"/></div></figure><p id="e631" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">解码器网络的隐藏状态如下所示，</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/dc35ee7c9bb95197d48c0ca226ef2c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*F0ldAwmLKIuIJqFzQeGc8Q.png"/></div></figure><p id="412b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中ct是上下文向量，它是由比对分数加权的输入序列的隐藏状态的加权和。</p><p id="d656" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">align函数分配一个分数，该分数告诉位置I处的输入对于位置t处的输出有多重要，比方说对于必须将“<em class="jo"> Hello </em>”转换为“<em class="jo"> नमस्ते </em>”的第一解码器单元，上下文向量可以如下</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kc"><img src="../Images/ed96fce33508c3f1cbc3d907c86f2baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*dEg9gADTMWTjK_pRnA6k-Q.png"/></div></figure><p id="0cc6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">比对分数如下</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kd"><img src="../Images/98ec26cde054859820177a2755bb8267.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*1bLmliEQq4e1CdAdALCoiA.png"/></div></figure><p id="fbad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中第一个隐藏向量(“你好”)的重要性最高，这意味着当将“<em class="jo">你好</em>”转换为“<em class="jo"> नमस्ते </em>”时，网络更多地关注“你好”，而较少关注序列的其余部分。</p><p id="d42b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在原始论文中，利用具有单个隐藏层的FFN来计算对齐分数，并且分数函数如下给出，</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es ke"><img src="../Images/af2cc3dca35b22bdd245bf216412621b.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*VEPCSw-c8ppscQNKPJ-nTg.png"/></div></figure><p id="731c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中va和Wa是要学习的权重。</p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><p id="0ab0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">进一步阅读</p><p id="df75" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae km" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2018/06/24/attention-attention . html</a></p><p id="a9f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae km" href="https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/</a></p><p id="60ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae km" href="https://towardsdatascience.com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f" rel="noopener" target="_blank">https://towards data science . com/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c 9482 aecf 4 f</a></p><p id="50a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae km" href="https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346" rel="noopener" target="_blank">https://towards data science . com/understanding-encoder-decoder-sequence-to-sequence-model-679 e 04 af 4346</a></p><p id="7b1e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae km" href="https://www.youtube.com/watch?v=W2rWgXJBZhU" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=W2rWgXJBZhU</a></p></div></div>    
</body>
</html>