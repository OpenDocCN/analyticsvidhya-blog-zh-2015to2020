<html>
<head>
<title>Encoding Time Series as Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将时间序列编码为图像</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/encoding-time-series-as-images-b043becbdbf3?source=collection_archive---------0-----------------------#2018-10-14">https://medium.com/analytics-vidhya/encoding-time-series-as-images-b043becbdbf3?source=collection_archive---------0-----------------------#2018-10-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/27ef970a10f17ae17deeea2ff39159a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c_uEuLbwK7C7O7r_IQsYog.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="d18d" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">格拉米角视场成像</h2></div><p id="9a5b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">深度学习的热潮很大程度上是由它在计算机视觉和语音识别方面的成功推动的。然而，当涉及到时间序列时，建立预测模型可能会令人毛骨悚然(<a class="ae ke" href="https://www.coursera.org/lecture/neural-networks/why-it-is-difficult-to-train-an-rnn-kTsBP" rel="noopener ugc nofollow" target="_blank">递归神经网络很难训练</a>，研究不太适用，并且没有预先训练好的模型，1D-CNN可能不方便)。</p><p id="bcc0" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">为了利用计算机视觉的最新发展带来的技术和见解，我将介绍并讨论一种将时间序列编码为图像的方法:T2(Gramian)角场。</p><p id="fa72" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">除了数学的先决条件(最小-最大标量，点积和Gram矩阵)，这篇文章将包含以下方面的解释和解决方案:</p><ul class=""><li id="9bad" class="kf kg ht jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">为什么Gram矩阵结构是单变量时间序列的良好2D表示？</li><li id="c3ce" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">为什么克矩阵点积不能代表CNN的数据？</li><li id="0381" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">让Gram矩阵结构为CNN做好准备的操作是什么？</li></ul><p id="d15f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">以及以下内容的Python要点:</p><ul class=""><li id="917f" class="kf kg ht jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">格拉米角场计算的有效数字实现。</li><li id="9cda" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">使用matplotlib+movie editor生成gif。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="6cda" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu"> TL:DR: </strong>我们对数据进行极坐标编码，然后对结果角度进行类似Gram矩阵的运算。</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es la"><img src="../Images/88e99dc02aefddb55a47aebb08c6aad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/1*A0yHZ8GD47cQd1OACTiz6Q.gif"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">格拉米角场转换的各个步骤(见下面的GIF代码)</figcaption></figure><h1 id="dfeb" class="lj lk ht bd ll lm ln lo lp lq lr ls lt iz lu ja lv jc lw jd lx jf ly jg lz ma bi translated">数学先决条件</h1><p id="1c93" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">因为Gramian角域的数学本质上与内积和相应的Gram矩阵相关联，这里有一个提示:</p><p id="131b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这个方法的数学本质上与内积和相应的Gram矩阵联系在一起，我写了一个关于它的意义的简短提示。</p><h2 id="8c94" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">点积</h2><p id="c067" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">内积是两个向量之间的<strong class="jk hu">运算，其中<strong class="jk hu">测量它们的“相似度”</strong>。它<em class="mu">允许使用来自传统<em class="mu">欧几里德几何</em>的概念</em>:长度、角度、二维和三维的正交性。</strong></p><p id="4bbf" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在最简单的情况下(2D空间)，两个向量u和v之间的内积定义为:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es mv"><img src="../Images/ebfeb1ef582efe934bc47ab74de0f3eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*7_HS3YaNpwHBfSmnSyTQVA.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">2D空间中的点积</figcaption></figure><p id="ce8d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">可以看出:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es mw"><img src="../Images/db97fedd299f4c214a4a02cfb7492ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*we_-r0dEqLnSXU78tZbyVw.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">2D空间中的另类表达</figcaption></figure><p id="f400" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">因此，如果<em class="mu"> u </em>和<em class="mu"> v </em>的规格为<em class="mu"> 1 </em>，则我们得到:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es mx"><img src="../Images/2ea18ab587a349ea4ec8220d2a15adf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*xbxgTXvouS_z_u9rzeWwrQ.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">单位向量的内积由它们的角差θ来表征</figcaption></figure><p id="6343" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">因此，在处理<strong class="jk hu">单位向量</strong>时，它们的<strong class="jk hu">内积仅由<em class="mu"> u </em>和<em class="mu"> v </em>之间的角度θ </strong>(用弧度表示)来表征。此外，结果值位于[-1，1]内。</p><p id="0029" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在本文的其余部分，这些属性将被证明是有用的。</p><p id="c59f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">注:</strong>在欧几里得设置(维数n)中，两个向量<em class="mu"> u </em>和<em class="mu"> v </em>的内积正式定义为</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es my"><img src="../Images/f87ea7521f17d46b63396933d45ce921.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*HEj0qy_q2k6BpHmOjl8aFw.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated"><em class="mz"> u </em>和v之间的内积</figcaption></figure><h2 id="6b71" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">格拉姆矩阵</h2><p id="bedb" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">Gram矩阵在线性代数和几何中是一个有用的工具。其中，它经常用于计算一组向量的<em class="mu">线性相关性。</em></p><p id="177d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">定义:</strong>一组<em class="mu"> n </em>向量的<em class="mu"> Gram矩阵</em>是由每对向量的点积(<em class="mu">见相似度</em>)定义的矩阵。从数学上来说，这意味着:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es na"><img src="../Images/a69fefb3674531994769c1065e117277.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*RtbJowmRgoETCOoH6IjKxQ.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">一组n个向量的格拉姆矩阵</figcaption></figure><p id="f53f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">同样，假设所有2D向量的范数为1，我们得到:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es nb"><img src="../Images/491b93a52b09946a82f2f4722a70c742.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*lYbwYSLrPqYVbkI8XVySjw.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">单位向量的Gram矩阵</figcaption></figure><p id="20cc" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">其中φ<em class="mu">(I，j) </em>是矢量<em class="mu"> i </em>和<em class="mu"> j </em>之间的夹角。</p><blockquote class="nc nd ne"><p id="5756" class="ji jj mu jk b jl jm iu jn jo jp ix jq nf js jt ju ng jw jx jy nh ka kb kc kd hb bi translated"><strong class="jk hu">要点:为什么要使用克矩阵？</strong></p><p id="5639" class="ji jj mu jk b jl jm iu jn jo jp ix jq nf js jt ju ng jw jx jy nh ka kb kc kd hb bi translated">Gram矩阵保持了时间依赖性。由于时间随着位置从左上向右下移动而增加，因此时间维度被编码到矩阵的几何中。</p></blockquote><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ni"><img src="../Images/2eae19a8c87b78c1785be33f0112a661.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*xrmFSfNnX0m4Y5poN-Xtqw.gif"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">不同时间步长的已知GAF值</figcaption></figure><p id="0ff2" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">注:</strong>本文的动机也是直觉，单变量时间序列在某种程度上无法解释数据的共现和潜在状态；我们应该致力于找到替代的和更丰富的表达方式。</p><h1 id="cfe8" class="lj lk ht bd ll lm ln lo lp lq lr ls lt iz lu ja lv jc lw jd lx jf ly jg lz ma bi translated">天真的实施</h1><p id="6faa" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">让我们计算时间序列值的Gram矩阵:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es nj"><img src="../Images/2966b3c31b15a68e858fa41bbc4708f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*aRSdlEkCVwPWJcG87b5pAQ.png"/></div></figure><h2 id="deda" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">用最小-最大缩放器将系列缩放到[-1，1]</h2><p id="e7a4" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">为了使内积不会偏向具有最大值的观察值，我们需要调整数据:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es nk"><img src="../Images/cd3329c91e9fefa98ff61c5b8c4f323e.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*2kGHaVLv77fxfObkf-1gSA.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">最小-最大缩放</figcaption></figure><p id="450b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在本用例中，<em class="mu">标准定标器不是合适的</em> <em class="mu">候选</em>，因为其输出范围和产生的内积都可能超过[-1，1]。</p><p id="de8a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">然而，结合最小-最大缩放器，内积可以保持输出范围:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es nl"><img src="../Images/5f6084320537304ea31dd6c5ee6ed25c.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*IERV45qNvJ1sXe0mfi4nDA.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">点积保留了范围</figcaption></figure><p id="37cf" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">选择[-1，1]内的点积并不是无害的。作为神经网络的输入范围，范围[-1，1]即使不是必需的，也是非常理想的。</p><h2 id="2114" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">嘈杂的图像？</h2><p id="367d" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">现在时间序列已经缩放，我们可以<strong class="jk hu">计算成对点积</strong>并将它们存储在Gram矩阵中:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es nm"><img src="../Images/5d22ab4dfded1b700e797a73985ff119.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*MiyM95NJ7HnSFjNocYBDrg.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">标度时间序列的Gram矩阵</figcaption></figure><p id="0386" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">让我们通过检查G:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nn"><img src="../Images/ee0d42221000649f1fd781ddcbfc1f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ProGxA01maOVgDcK6EeNkQ.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">Gram矩阵值遵循高斯分布(时间序列是余弦分布)</figcaption></figure><p id="7787" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我们观察到两件事:</p><ol class=""><li id="a064" class="kf kg ht jk b jl jm jo jp jr kh jv ki jz kj kd no kl km kn bi translated"><strong class="jk hu">输出</strong>似乎<strong class="jk hu">遵循以0为中心的高斯分布</strong>。</li><li id="9906" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd no kl km kn bi translated"><strong class="jk hu">生成的图像有噪声</strong>。</li></ol><p id="100e" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">前者解释了后者</strong>，因为数据的高斯性越强，就越难将其与高斯噪声区分开来。</p><p id="5991" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">This will be a problem for our Neural Networks. Additionally, it has been established that <a class="ae ke" href="https://www.quora.com/Why-are-deep-neural-networks-so-bad-with-sparse-data" rel="noopener ugc nofollow" target="_blank">CNN work better with sparse data</a>.</p><h2 id="01fc" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">The origin of non-sparsity</h2><p id="1601" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">The Gaussian distribution is not very surprising. When looking at the 3D plot of the inner product values <em class="mu">z</em>, for every possible combination of (<em class="mu">x</em>, <em class="mu">y</em>) ∈ R², we obtain:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es np"><img src="../Images/515038deea475688f947fa79853c3555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mU3tWzMdXpT5bGQ9U3W7xQ.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">3D surface of the dot product</figcaption></figure><p id="2aa7" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">Under the assumption that <strong class="jk hu">the values</strong> of the time series follow a Uniform distribution [-1, 1], the Gram Matrix values follow a Gaussian-like distribution. Here are <strong class="jk hu">histograms of the outputs</strong> of the Gram Matrix valued for different time series lengths <em class="mu">n</em>:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nq"><img src="../Images/fcea7c3784f032cb13527ef6f2867de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-4P1xRryR_RYmSWt3CqlXA.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">Gram Matrix output for time series of length n (in red is the density of N(0, 0.33))</figcaption></figure><h1 id="d436" class="lj lk ht bd ll lm ln lo lp lq lr ls lt iz lu ja lv jc lw jd lx jf ly jg lz ma bi translated">Preliminary Encoding</h1><h2 id="ee30" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">Why do we need one?</h2><p id="e738" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">As univariate time series are in 1D and the dot product fails to distinguish the valuable informations from Gaussian noise, there is no other way to take advantage of “angular” relations than changing the space.</p><p id="b2c1" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">We must therefore encode the time serie into a space of at least 2 dimensions, prior to using Gram Matrix like constructs. To do so, we will construct a <strong class="jk hu">bijective mapping </strong>between our 1D time serie and a 2D space, so as not to lose any informations.</p><blockquote class="nc nd ne"><p id="6ff8" class="ji jj mu jk b jl jm iu jn jo jp ix jq nf js jt ju ng jw jx jy nh ka kb kc kd hb bi translated">This encoding is largely inspired from <strong class="jk hu">polar coordinates transformations</strong>, except in this case, the radius coordinate expresses time.</p></blockquote><h2 id="d0ee" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">Step 1: Scale the serie onto [-1, 1] with a Min-Max scaler</h2><p id="67e2" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">We proceed similarly than in the naïve implementation. Coupled with the Min-Max scaler, our polar encoding will be bijective, the use the <em class="mu">arccos</em> function bijective (see next step).</p><h2 id="4524" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated"><strong class="ak">Step 2: Convert the scaled time serie into “polar coordinates”</strong></h2><p id="a9a5" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">Two quantities need to be accounted for, the <strong class="jk hu">value of the time series</strong> and its <strong class="jk hu">corresponding timestamp</strong>. These two variables will be expressed respectively with the <strong class="jk hu">angle</strong> and the <strong class="jk hu">radius</strong>.</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es nr"><img src="../Images/7c5acf14437c3df5f4c9aed911249240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*HTAk4ptd0a9Tp9wFc5scXg.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">Polar Encoding for time series</figcaption></figure><p id="b391" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">Assuming our time series is composed of <strong class="jk hu"><em class="mu">N</em></strong> timestamps <strong class="jk hu"><em class="mu">t</em></strong> with corresponding values <strong class="jk hu"><em class="mu">x</em></strong><em class="mu">, then:</em></p><ul class=""><li id="5d00" class="kf kg ht jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">The angles are computed using <strong class="jk hu"><em class="mu">arccos(x). </em></strong>They lie within [0, ∏].</li><li id="91b9" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">半径变量的计算方法是:首先，我们将区间[0，1]分成<em class="mu"> N </em>个相等的部分。我们因此获得<em class="mu"> N+1 </em>定界<em class="mu">T5】点{0，…，1}。然后，我们丢弃0，并将这些点连续关联到时间序列。</em></li></ul><p id="65cb" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">从数学上来说，它可以解释为:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es ns"><img src="../Images/553af9ae3153ec63b668aa23b22626c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*_E5pxbxn6BlXBxcQZB7NSw.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">标度时间序列的2D编码</figcaption></figure><p id="3783" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这些编码有几个<strong class="jk hu">优点</strong>:</p><ol class=""><li id="1948" class="kf kg ht jk b jl jm jo jp jr kh jv ki jz kj kd no kl km kn bi translated">整个编码是<strong class="jk hu">双射</strong>(作为双射函数的组合)。</li><li id="91fe" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd no kl km kn bi translated">它<strong class="jk hu">通过<em class="mu"> r </em>坐标保存时间相关性</strong>。这将被证明是非常有用的[*]。</li></ol><h1 id="4e00" class="lj lk ht bd ll lm ln lo lp lq lr ls lt iz lu ja lv jc lw jd lx jf ly jg lz ma bi translated">时间序列的内积？</h1><p id="2cc4" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">现在我们在2D空间中，接下来的问题是我们如何使用内积运算来处理稀疏性。</p><h2 id="b5ca" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">为什么不是极坐标编码值的内积？</h2><p id="4d1a" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">2D极空间中的内积有几个限制，因为每个向量的范数已经针对时间依赖性进行了调整。更准确地说:</p><ul class=""><li id="42c0" class="kf kg ht jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">两个不同观测值之间的内积将偏向于最近的一个(因为范数随着时间而增加)。</li><li id="e5b0" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">当计算观察值本身的内积时，所得的范数也是有偏差的。</li></ul><p id="a123" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">因此，如果一个类似内积的运算存在，它应该仅仅依赖于角度。</p><h2 id="5f34" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">使用角度</h2><p id="4e1e" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">由于任何内积运算都无法将两个不同观察值的信息转换成一个值，因此我们无法将两个角度给出的信息保存在一起。我们必须做出一些让步。</p><p id="3836" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">为了最好地解释两个角度上的单个和联合信息，作者将<strong class="jk hu">定义为内积</strong>的替代运算:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es nt"><img src="../Images/06e2fac7434c4c1cf64f0b95b8368db7.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*5jioAqHCGmogS9JjpKgudg.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">自定义操作</figcaption></figure><p id="2ffe" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">其中<em class="mu"> θ </em>分别是从<em class="mu"> x </em>和<em class="mu"> y </em>编码的角度。</p><p id="79f8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">注:</strong>我选择了不同的符号而不是使用内积，因为这个操作不满足内积(线性，正定)的<a class="ae ke" href="https://en.wikipedia.org/wiki/Inner_product_space#Definition" rel="noopener ugc nofollow" target="_blank">要求。</a></p><p id="e866" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这导致了以下类似Gram的矩阵:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es nu"><img src="../Images/fe5a895f3d5bb8007585be0911b6f1a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*5Q2XaC-BqDimZ-2TKz9mbg.png"/></div></figure><blockquote class="nc nd ne"><p id="eb4e" class="ji jj mu jk b jl jm iu jn jo jp ix jq nf js jt ju ng jw jx jy nh ka kb kc kd hb bi translated">作者的动机是:“第二，与笛卡尔坐标相反，极坐标保留了绝对的时间关系。”</p></blockquote><h2 id="cf7d" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">优势</h2><ol class=""><li id="ff4f" class="kf kg ht jk b jl mb jo mc jr nv jv nw jz nx kd no kl km kn bi translated"><strong class="jk hu">对角线由缩放时间序列的原始值</strong>组成(我们将从深度神经网络学习的高级特征中近似重建时间序列。)</li><li id="022c" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd no kl km kn bi translated">时间相关性通过相对于时间间隔k的方向叠加的相对相关性来说明</li></ol><h2 id="e303" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated">朝向更稀疏的表示？</h2><p id="b2bc" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">现在让我们绘制格拉米角场值的密度:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ny"><img src="../Images/6b3d5f342550d16d41d67a2510469dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S22TPejs15CZpTdEiWA8lQ.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">The Gramian Angular Field is less noisy / sparser than the Gram Matrix.</figcaption></figure><p id="2a49" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">As we can see from the plot above, the Gramian Angular Field is much sparser. To explain this, let’s re-express <em class="mu">u</em> ⊕ <em class="mu">v</em> in Cartesian coordinates:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es nz"><img src="../Images/82213950616c1db2d3b66d27959c63ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*The8P4Pigt7h9LVsY-mgPQ.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">Converting back the expression in Cartesian coordinates</figcaption></figure><p id="d63e" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">We notice in the last term above that the <strong class="jk hu">newly constructed operation </strong>corresponds to a <strong class="jk hu">penalized version of the conventional inner product:</strong></p><figure class="lb lc ld le fd hk er es paragraph-image"><div class="er es oa"><img src="../Images/d8af9872a55709b79a24f34e47b9c3d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*CHzcDqJ9-HbtH6gKgxMsjw.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">New operation = Penalized Inner Product</figcaption></figure><p id="2424" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">Let’s gain some insights on the role of this penalty. Let’s first have a look at the 3D plot of the full operation:</p><figure class="lb lc ld le fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ob"><img src="../Images/7767a310e7afdbabd4d5407872286cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1w277Py5sl0wQPDKEI2mAQ.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">3D plot of the operation</figcaption></figure><p id="e7e7" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">As we can see:</p><ul class=""><li id="cebb" class="kf kg ht jk b jl jm jo jp jr kh jv ki jz kj kd kk kl km kn bi translated">The penalty shifts the mean output towards -1.</li><li id="36a2" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">The closer x and y are to 0, the larger is the penalty. The main consequence is that points which were closer to the gaussian noise … (with the dot) are (with the penalized dot)</li><li id="cd3a" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">For x = y: it is casted to -1</li><li id="bbc1" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">The outputs are easily distinguishable from Gaussian Noise.</li></ul><h2 id="0b4b" class="mg lk ht bd ll mh mi mj lp mk ml mm lt jr mn mo lv jv mp mq lx jz mr ms lz mt bi translated"><strong class="ak">Drawbacks</strong></h2><ul class=""><li id="6fdd" class="kf kg ht jk b jl mb jo mc jr nv jv nw jz nx kd kk kl km kn bi translated">With the main diagonal, however, the generated GAM is large due to the augmentation n ⟼n², where the length of the raw time series is n. The author suggests to reduce the size of the GAF by applying Piecewise Aggregation Approximation (Keogh and Pazzani 2000).</li><li id="b646" class="kf kg ht jk b jl ko jo kp jr kq jv kr jz ks kd kk kl km kn bi translated">It is not an inner product..</li></ul><h1 id="eaf7" class="lj lk ht bd ll lm ln lo lp lq lr ls lt iz lu ja lv jc lw jd lx jf ly jg lz ma bi translated">Show me the code!</h1><p id="8dcf" class="pw-post-body-paragraph ji jj ht jk b jl mb iu jn jo mc ix jq jr md jt ju jv me jx jy jz mf kb kc kd hb bi translated">A numpy implementation to convert univariate time series into an image and other python code used for this article can be found <a class="ae ke" href="https://github.com/devitrylouis/imaging_time_series" rel="noopener ugc nofollow" target="_blank">here</a>.</p><p id="6b22" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">As for the gif, I will release it soon (implemented with matplotlib, numpy and moviepy).</p><h1 id="f849" class="lj lk ht bd ll lm ln lo lp lq lr ls lt iz lu ja lv jc lw jd lx jf ly jg lz ma bi translated">Sources</h1><blockquote class="nc nd ne"><p id="4dec" class="ji jj mu jk b jl jm iu jn jo jp ix jq nf js jt ju ng jw jx jy nh ka kb kc kd hb bi translated">This blogpost is largely inspired from the <a class="ae ke" href="https://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10179/10251" rel="noopener ugc nofollow" target="_blank">detailed paper</a> Encoding Time Series as Images for Visual Inspection and Classification Using Tiled Convolutional Neural Networks, by Zhiguang Wang and Tim.</p><p id="be6d" class="ji jj mu jk b jl jm iu jn jo jp ix jq nf js jt ju ng jw jx jy nh ka kb kc kd hb bi translated">This paper also mentions another interesting encoding technique: Markov Transition Field. This encoding will not be covered in this article.</p></blockquote><div class="hh hi ez fb hj oc"><a href="https://arxiv.org/abs/1403.6382" rel="noopener  ugc nofollow" target="_blank"><div class="od ab dw"><div class="oe ab of cl cj og"><h2 class="bd hu fi z dy oh ea eb oi ed ef hs bi translated">[1403.6382] CNN Features off-the-shelf: an Astounding Baseline for Recognition</h2><div class="oj l"><h3 class="bd b fi z dy oh ea eb oi ed ef dx translated">Abstract: Recent results indicate that the generic descriptors extracted from the convolutional neural networks are…</h3></div><div class="ok l"><p class="bd b fp z dy oh ea eb oi ed ef dx translated">arxiv.org</p></div></div></div></a></div><div class="hh hi ez fb hj oc"><a href="https://arxiv.org/abs/1411.1792" rel="noopener  ugc nofollow" target="_blank"><div class="od ab dw"><div class="oe ab of cl cj og"><h2 class="bd hu fi z dy oh ea eb oi ed ef hs bi translated">[1411.1792] How transferable are features in deep neural networks?</h2><div class="oj l"><h3 class="bd b fi z dy oh ea eb oi ed ef dx translated">Abstract: Many deep neural networks trained on natural images exhibit a curious phenomenon incommon: on the first…</h3></div><div class="ok l"><p class="bd b fp z dy oh ea eb oi ed ef dx translated">arxiv.</p></div></div></div></a></div><p id="f63e" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><a class="ae ke" href="https://arxiv.org/pdf/1506.00327.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.00327.pdf</a></p><p id="2257" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><a class="ae ke" href="https://stats.stackexchange.com/questions/47051/sparse-representations-for-denoising-problems" rel="noopener ugc nofollow" target="_blank">https://stats.stackexchange.com/questions/47051/sparse-representations-for-denoising-problems</a></p></div></div>    
</body>
</html>