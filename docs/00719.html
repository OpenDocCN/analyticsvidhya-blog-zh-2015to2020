<html>
<head>
<title>Guide on Quantizing and Converting Model to Tensorflow Lite</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">量化和转换模型到Tensorflow Lite的指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mobile-inference-b943dc99e29b?source=collection_archive---------2-----------------------#2019-08-31">https://medium.com/analytics-vidhya/mobile-inference-b943dc99e29b?source=collection_archive---------2-----------------------#2019-08-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/2482c1ca89341323bb8e2060691238cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BUr03Y0PJrlB-ocEDH-ZIQ.jpeg"/></div></div></figure><div class=""/><p id="a90d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">近年来，在深度学习中，随着模型性能的提高，模型中的参数数量也在大量增加。例如— Inceptionv3包含大约23M个参数。所有这些都使得在移动设备上运行它们变得不可能。</p><p id="71a0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">幸运的是，这样做有几个选择——设计轻量级架构，或者将训练模型的权重保存在低精度类型中，如float16、uint8…</p><p id="0720" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，我们将在Tensorflow的帮助下专注于量化和转换模型的第二个选项。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="0fd9" class="jv jw ht bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">概观</h1><p id="eb53" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">在Tensorflow Lite中，有几个选项可用于获取移动优化模型</p><ol class=""><li id="1897" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated">从Tensorflow转换到Tensorflow Lite，而不对权重和激活类型进行任何修改。</li><li id="031f" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">转换模型并同时量化权重(使用量化算法中的默认范围)</li><li id="d602" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">做量化感知训练，这将学习每层的最小和最大范围的重量，只有在转换成tflite。</li></ol><h1 id="dc12" class="jv jw ht bd jx jy lm ka kb kc ln ke kf kg lo ki kj kk lp km kn ko lq kq kr ks bi translated">简单转换</h1><p id="e75c" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">在这种情况下，您的模型将在没有任何优化的情况下转换为tflite，您将能够在移动设备上使用它，在Tensorflow Lite框架内进行推理。</p><p id="b19d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面是一个从Keras模型转换的例子。</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="cc3c" class="ma jw ht lw b fi mb mc l md me">converter = tf.lite.TFLiteConverter.from_keras_model_file('model_keras.h5')<br/>tflite_model = converter.convert()<br/>tflite_model_quant_file = <strong class="lw hu">"./test.tflite"<br/></strong>tflite_model_quant_file.write_bytes(tflite_model)</span></pre><h1 id="6fed" class="jv jw ht bd jx jy lm ka kb kc ln ke kf kg lo ki kj kk lp km kn ko lq kq kr ks bi translated">转换+岗位培训量化</h1><p id="1305" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">在这种情况下，您可以选择将权重转换为uint8，这将减少模型的大小，但请注意，这可能会增加模型的延迟，因为在推理时间内，在对输入执行操作之前，层的权重将被转换回float32，总之，此操作可能会花费大量时间，具体取决于您的架构。从张量流的官方文档来看，量子化的公式是</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="cf43" class="ma jw ht lw b fi mb mc l md me">out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)<br/>if T == qint8: out[i] -= (range(T) + 1) / 2.0<br/>where <br/>range(T) = numeric_limits&lt;T&gt;::max() - numeric_limits&lt;T&gt;::min()</span></pre><p id="1f32" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，性能可能会下降，因为所有图层的max_range和min_range都有默认值。</p><p id="2c86" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一个使用训练后量化转换模型的示例。</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="a23d" class="ma jw ht lw b fi mb mc l md me">converter = tf.lite.TFLiteConverter.from_keras_model_file('model_keras.h5')<br/>converter.optimizations = [tf.lite.Optimize.DEFAULT]<br/>tflite_model = converter.convert()<br/>tflite_model_quant_file = <strong class="lw hu">"./test.tflite"<br/></strong>tflite_model_quant_file.write_bytes(tflite_model)</span></pre><h1 id="6be5" class="jv jw ht bd jx jy lm ka kb kc ln ke kf kg lo ki kj kk lp km kn ko lq kq kr ks bi translated">量化感知训练</h1><p id="8c18" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">为了解决下面提出的关于最小/最大值的默认范围的问题，Teansorflow创建了一个流，在构建图形的过程中，您可以在每一层中插入假节点，以模拟向前和向后传递中的量化效果，并在训练过程中分别学习每一层的范围。</p><p id="e1fb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在训练中启用此选项所需的全部工作是在构建主模型后，添加转换图形的内置方法，以包括假节点</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="5c65" class="ma jw ht lw b fi mb mc l md me"><strong class="lw hu">with </strong>tf.variable_scope(<strong class="lw hu">'quantize'</strong>):<br/>    output= model(x=image_tf, is_training=<strong class="lw hu">True</strong>, keep_prob=keep_prob_tf)<br/>tf.contrib.quantize.create_training_graph(quant_delay=0)</span></pre><p id="13f6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练后，在导出图形的阶段，您需要调用一个方法将图形转换为相应的仅推理版本，其中已经训练的假节点将被视为每个层的最小/最大范围。</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="5aca" class="ma jw ht lw b fi mb mc l md me"><strong class="lw hu">with </strong>tf.variable_scope(<strong class="lw hu">'quantize'</strong>):<br/>    output = model(x=input, is_training=<strong class="lw hu">False</strong>, keep_prob=1.)</span><span id="0bf1" class="ma jw ht lw b fi mf mc l md me">g = tf.get_default_graph()<br/>tf.contrib.quantize.create_eval_graph(input_graph=g)</span></pre><p id="7d6e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">之后，您可以在TensorflowLite转换器的帮助下转换模型，将推理选项和推理输入类型指定为uint8。</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="148c" class="ma jw ht lw b fi mb mc l md me">converter = lite.TFLiteConverter.from_session(session, [input], [output])<br/>converter.inference_type = tf.uint8<br/>converter.inference_input_type = tf.uint8<br/>input_arrays = converter.get_input_arrays()<br/>converter.quantized_input_stats = {input_arrays[0] : (0., 1.)}</span><span id="840d" class="ma jw ht lw b fi mf mc l md me">flatbuffer = converter.convert()</span><span id="276d" class="ma jw ht lw b fi mf mc l md me"><strong class="lw hu">with </strong>open(<strong class="lw hu">'test.tflite'</strong>, <strong class="lw hu">'wb'</strong>) <strong class="lw hu">as </strong>outfile:<br/>    outfile.write(flatbuffer)</span></pre><h1 id="80d4" class="jv jw ht bd jx jy lm ka kb kc ln ke kf kg lo ki kj kk lp km kn ko lq kq kr ks bi translated">量化感知训练中的陷阱(针对Tensorflow 1.14)</h1><ol class=""><li id="20ef" class="ky kz ht is b it kt ix ku jb mg jf mh jj mi jn ld le lf lg bi translated">不支持融合批处理规范，这是tf.layers.batch_normalization的默认选项。</li><li id="69e1" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">目前，train和eval图中的变量范围存在差异，因此需要手动将变量范围添加到图中。</li></ol><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="8a8e" class="ma jw ht lw b fi mb mc l md me"># the training stage<br/><strong class="lw hu">with </strong>tf.variable_scope(<strong class="lw hu">'quantize'</strong>):<br/>    output= model(x=image_tf, is_training=<strong class="lw hu">True</strong>, keep_prob=keep_prob_tf)<br/>tf.contrib.quantize.create_training_graph(quant_delay=0)</span><span id="bf2e" class="ma jw ht lw b fi mf mc l md me"># exporting stage<br/><strong class="lw hu">with </strong>tf.variable_scope(<strong class="lw hu">'quantize'</strong>):<br/>    output = model(x=input, is_training=<strong class="lw hu">False</strong>, keep_prob=1.)<br/>g = tf.get_default_graph()<br/>tf.contrib.quantize.create_eval_graph(input_graph=g)</span></pre><p id="29e6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.因为TensorflowLite转换器仅严格考虑输入和输出节点之间的节点，所以图中最后一层的伪节点(仅在实际节点之后)被忽略，这会导致tflite转换器出错。可以有一种变通方法，即添加一些无用的op，这不会损害图形的结果，例如</p><pre class="lr ls lt lu fd lv lw lx ly aw lz bi"><span id="3c2e" class="ma jw ht lw b fi mb mc l md me"><strong class="lw hu">with </strong>tf.variable_scope(<strong class="lw hu">'quantize'</strong>):<br/>    output = model(x=input, is_training=<strong class="lw hu">False</strong>, keep_prob=1.)<br/>    output = tf.maximum(output, -1e27)</span></pre></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="3d48" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae mj" href="https://github.com/lusinlu/tensorflow_lite_guide" rel="noopener ugc nofollow" target="_blank">示例的源代码</a></p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="535b" class="jv jw ht bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">参考</h1><p id="ccf6" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">[1]tensor flow Lite的官方文档，【https://www.tensorflow.org/lite T2】</p><p id="ee21" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[2]张量流量化的实现，<a class="ae mj" href="https://www.tensorflow.org/api_docs/python/tf/quantization/quantize" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/quantization/quantize</a></p></div></div>    
</body>
</html>