<html>
<head>
<title>Style Transfer using Neural Nets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用神经网络的风格转换</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/style-transfer-using-neural-nets-1b57d88ebb91?source=collection_archive---------14-----------------------#2020-07-28">https://medium.com/analytics-vidhya/style-transfer-using-neural-nets-1b57d88ebb91?source=collection_archive---------14-----------------------#2020-07-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9909" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在众多实现人工智能的应用程序或通过应用人工智能解决的无数日常问题中，风格转换是人们曾经想到的最迷人和最具创新性的想法之一。像文森特·梵高、莱昂纳多·达·芬奇、巴勃罗·毕加索、米开朗基罗等传奇艺术家创作的杰作至今仍与我们同在。这些艺术家的作品从每个艺术家的角度描绘了日常生活、历史事件和宗教主题的艺术版本。这些传奇艺术家的作品是独一无二的&amp;具有明确的绘画风格特征，如色彩、色调、笔触、纹理、形状、抽象程度等，这使得普通人很难复制他们的风格。现在想象一下将这种艺术风格应用到纽约的天际线图片、孟买海上车道的航拍照片或一张人物照片上。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/eb5b2b1cb0c6e62935d7fc056e995454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ld5_hOPgcLQOn5ICcnknlw.jpeg"/></div></div></figure><p id="487d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最早的作品之一叫做<a class="ae jp" href="https://arxiv.org/abs/1701.01036" rel="noopener ugc nofollow" target="_blank">揭开神经风格转移的神秘面纱</a>成功地将风格从一幅图像转移到另一幅图像。给定两个图像，艺术图像&amp;要进行风格化的图像，神经网络可以从艺术图像中检测风格&amp;将其应用到另一个图像。</p><p id="361c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经风格转移是如此具有开创性，以至于一群人在伦敦最负盛名的拍卖行之一以 50 万美元(43.2 万美元)的价格出售了一幅由 GAN(一种深度神经网络)制作的画作，从而赚了一大笔钱。</p><h1 id="ea25" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">什么是神经风格转移？</h1><p id="0737" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">神经风格转移算法通过操纵图像来采用另一个图像的风格和外观，从而生成图像的艺术版本。该算法应用处理 3 种不同图像的优化技术:</p><ol class=""><li id="cc49" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated">内容图像。</li><li id="935c" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">风格形象。</li><li id="9ba7" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">生成的输出图像。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lh"><img src="../Images/0fb2448e7983e8403f1035a7ed598715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mP8sWQdQtIFhlfz970N0QA.jpeg"/></div></div></figure><p id="fa32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Content Image:我们希望从中提取内容并转换样式的输入图像，顾名思义，该图像具有基本内容，如建筑物或需要进行样式化的人物。</p><p id="3784" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">样式图像:DNN 从中捕获样式并将其应用于内容的图像。</p><p id="7a8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">生成的图像:输出图像是内容图像的样式版本。生成的图像是包含内容图像中的内容部分(建筑物/人)的输出，并从样式图像中获取样式。</p><blockquote class="li lj lk"><p id="2baa" class="if ig ll ih b ii ij ik il im in io ip lm ir is it ln iv iw ix lo iz ja jb jc hb bi translated">与经典的深度学习任务不同，深度神经网络不是来回训练的，而是算法将优化应用于输出生成的图像，以看起来更像内容图像的艺术版本。DNN 的权重和偏差被冻结，同时减少了生成的输出图像与内容图像、生成的图像输出图像与风格图像之间的误差。</p></blockquote><h1 id="2f1c" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">入门指南</h1><p id="963f" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">如所讨论的，深度神经网络没有被训练来校正艺术输出图像。<strong class="ih hj">卷积神经网络仅用于从两幅输入图像中提取内容特征&amp;艺术风格(特征图)。</strong>在 ImageNet 数据集上训练的 VGG19 等预先训练的先进网络用于此目的&amp;网络权重/内核在整个过程中保持固定。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/e71e463033a7afe8f7e508ac03404a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*U6_b8aNUtOgdkRlP3b9Vpg.png"/></div></figure><p id="5cff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用预训练网络背后的想法是，它的内核可以在网络的某个级别捕获内容特征和风格特征。卷积神经网络在输入图像向前传播时提取其特征，从较低层的简单原始特征开始，映射较深层的复杂图案和设计。如果你对<em class="ll">【CNN 如何工作】</em>背后的东西没有清晰的认识，可以随意去读一读《CNN 眼中的世界<a class="ae jp" rel="noopener" href="/analytics-vidhya/the-world-through-the-eyes-of-cnn-5a52c034dbeb?source=friends_link&amp;sk=ad1957a5e5286073a97864b397431078">。</a></p><p id="c422" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过将内容图像和风格图像输入到预先训练的 VGG 网络，我们获得内容和风格图像的特征图。内容图像的输出特征图表示图像的实体(建筑物、汽车、人物等)&amp;风格图像的输出特征图表示图像的艺术元素，如笔触、颜色混合、纹理、色调等。</p><p id="6a0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">内容图像特征图是从网络的深层提取的，因为它们复制了复杂的模式、前景中的对象&amp;背景或图像本身中的实体。</strong></p><p id="018e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">风格图像特征图是为网络的中级或低级获取的，它捕捉图像中的纹理&amp;图案，在我们的情况下，将是笔触、颜色组合等。</strong></p><blockquote class="li lj lk"><p id="6221" class="if ig ll ih b ii ij ik il im in io ip lm ir is it ln iv iw ix lo iz ja jb jc hb bi translated">正如所讨论的，在神经类型转换中，卷积神经网络没有被训练。训练网络基本上是计算网络呈现的损失或误差&amp;应用优化算法来调整网络权重和偏差，以便减少网络的误差。</p></blockquote><p id="5ebe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出图像用随机像素值或噪声初始化。在损失函数中将输出图像特征图与内容和风格特征图进行比较，以公式化损失/误差。</p><p id="f0b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过使用优化算法来修改输出图像的像素值，该优化算法在每次迭代中最小化输出图像像素的损失/误差。</p><p id="0fc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，神经类型转移保持网络的权重和偏差固定，而不是通过在每次迭代中最小化损失或成本函数来迭代地修改输出图像。</p><blockquote class="li lj lk"><p id="6532" class="if ig ll ih b ii ij ik il im in io ip lm ir is it ln iv iw ix lo iz ja jb jc hb bi translated">注意:输出图像也通过网络传播，以获得其相应的目标/输出内容特征和目标/输出风格特征。<br/>将这些目标/输出特征图与损失函数中的输入内容&amp;风格特征图表示进行比较。</p></blockquote><h2 id="e763" class="lq jr hi bd js lr ls lt jw lu lv lw ka iq lx ly ke iu lz ma ki iy mb mc km md bi translated">艺术输出是如何产生的？</h2><p id="c373" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">损失函数由<strong class="ih hj">内容损失</strong>&amp;<strong class="ih hj">风格损失</strong>项组成，这些项确定输出艺术图像在从其各自的内容&amp;风格特征图中结合内容部分&amp;风格元素方面有多远或有多错误。</p><p id="2d9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如前所述，我们从网络的不同阶段获得了内容图像和风格图像的特征图。这些特征图有助于确定内容损失和风格损失。总损失是内容和风格损失的总和。</p><h2 id="a326" class="lq jr hi bd js lr ls lt jw lu lv lw ka iq lx ly ke iu lz ma ki iy mb mc km md bi translated">内容损失</h2><p id="a405" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated"><strong class="ih hj">基于直觉，具有相似内容的图像在网络的更深层将具有相似的特征地图表示。</strong>为此，我们通过 VGG19 网络&amp;传播输出图像(最初带有随机噪声),在产生内容图像特征图的相同深度获得目标/输出图像特征图。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es me"><img src="../Images/f713e1981d31871bd2ade4a68a834664.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*4ptPhCH1DMGVq5dEgytvBQ.png"/></div></figure><p id="236a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">内容损失估计内容图像和输出目标图像的特征图之间的均方误差。</p><ul class=""><li id="42d2" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc mf kz la lb bi translated">P^ <em class="ll"> l </em> —原始图像的特征图。</li><li id="c64e" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc mf kz la lb bi translated">F^ <em class="ll"> l </em> —生成图像的特征图。</li><li id="ccb5" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc mf kz la lb bi translated"><em class="ll"> l </em> —提取内容特征的图层<strong class="ih hj">。</strong></li></ul><p id="7aa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果内容图像和生成图像在更深的层具有相似的特征表示，则内容损失值将最小。</p><blockquote class="li lj lk"><p id="ca82" class="if ig ll ih b ii ij ik il im in io ip lm ir is it ln iv iw ix lo iz ja jb jc hb bi translated">内容损失计算输出和内容输入图像特征图之间的逐像素差异。</p></blockquote><h2 id="862d" class="lq jr hi bd js lr ls lt jw lu lv lw ka iq lx ly ke iu lz ma ki iy mb mc km md bi translated">风格丧失</h2><p id="282e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">样式损失类似于内容损失，只是在样式损失中估计误差的方式有所不同。计算风格损失的过程是:</p><ul class=""><li id="eb2c" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc mf kz la lb bi translated">与内容特征不同，风格(艺术)输入图像特征图是从 CNN 的所有层获得的。因此，输出图像的相应目标风格特征也从所有层获得。</li><li id="ea9e" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc mf kz la lb bi translated">我们获得了每一层的风格特征图的 gram 矩阵表示。</li></ul><p id="5a7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是格拉姆矩阵表示法？</strong></p><p id="e434" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该论文描述了风格信息被测量为风格特征图之间的相关量。Gram Matrix 恰恰做到了这一点。该过程计算层<em class="ll"> l. </em>的特征图之间的相关性</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mg"><img src="../Images/7fa387ce65961265863f22c162241a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mbeEuJLyw_2atq3pi1PgAA.png"/></div></div></figure><p id="fecb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为输入样式图像&amp;输出图像的所有指定<em class="ll">样式层</em>计算特征图之间的相关性。</p><p id="8318" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算风格损失</strong></p><p id="2221" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">风格损失被定义为生成图像和输入风格图像的特征图的 gram 矩阵表示之间的平方差。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mh"><img src="../Images/f9240cd7c4fd40a5b9ee35d2dff75ccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7uVwbMtuRu7QQ6_fDQPQtw.png"/></div></div></figure><p id="7730" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">A^ <em class="ll"> l — </em>输入风格图像的 gram 矩阵表示。</p><p id="c5a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">G^ <em class="ll"> l — </em>生成的图像在一层的克矩阵<em class="ll"> l. </em></p><p id="a046" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ll"> N- </em>特征图的数量或图层的过滤器数量<em class="ll"> l. </em></p><p id="1636" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">特征地图的高度和宽度的 m-乘积。</p><p id="f4d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">总风格损失是每个风格层误差的线性加权组合。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mi"><img src="../Images/42748979f08e701ac8781741f5697db3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fyaXWlLDfzfaHfGc86dhFg.png"/></div></div></figure><p id="9772" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">E <em class="ll"> l- </em>图层样式错误<em class="ll"> l. </em></p><p id="9f5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">w <em class="ll"> l- </em>对各层的重量贡献。这控制了特定层对损失的贡献程度。</p><h2 id="7cb4" class="lq jr hi bd js lr ls lt jw lu lv lw ka iq lx ly ke iu lz ma ki iy mb mc km md bi translated">全损</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mj"><img src="../Images/9e9b928c4eb2bfa5784749fa0b8d5174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K7btqkh4e-Chops1fg_rig.png"/></div></div></figure><p id="b2ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总损失是内容损失和风格损失的总和。<strong class="ih hj">这个损失函数是如何描述神经风格转移的？</strong></p><p id="4544" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">生成图像的内容必须与内容图像的内容相似&amp;生成图像的样式必须与样式图像的样式相似。<em class="ll">α</em>&amp;<em class="ll">β</em>是用于控制最终生成的图像上的表现内容&amp;风格表现的权重。</p><ul class=""><li id="7c0b" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc mf kz la lb bi translated">如果<em class="ll"> alpha &gt; beta </em>最终的图像将包含更多的内容特征&amp;更少的风格。</li><li id="be64" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc mf kz la lb bi translated">如果α<em class="ll">α&lt;β</em>最终图像将更具艺术性&amp;将捕捉更少的内容。</li></ul><p id="56f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">运行优化算法，如 SGD 或 Adam，以减少输出图像的总损失。</strong></p><p id="dd55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每次迭代中，优化算法在最小误差的方向上修改输出图像的像素值。</p><h2 id="02bc" class="lq jr hi bd js lr ls lt jw lu lv lw ka iq lx ly ke iu lz ma ki iy mb mc km md bi translated">总结流程</h2><ol class=""><li id="238a" class="kt ku hi ih b ii ko im kp iq mk iu ml iy mm jc ky kz la lb bi translated">加载输入内容图像<em class="ll">c</em>T52】样式图像<em class="ll"> s </em>。</li><li id="f169" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">加载一个预先训练好的网络，比如 VGG19。</li><li id="b542" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">指定提取内容要素地图和样式要素地图的图层。</li><li id="4e3a" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">获取内容图像<em class="ll">‘c’</em>特征图&amp;样式图像<em class="ll">‘s’</em>特征图。</li><li id="dd3e" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">获取风格特征的 Gram 矩阵表示。此外，指定每个图层的样式权重(常数值)。</li><li id="4eb3" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">用随机像素值初始化输出/生成的图像。</li><li id="8794" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">初始化优化器/优化算法，如 SGD/Adam/Adagrad，以最小化输出图像的误差 wrt。最后，将α和β初始化为一些常数值。</li><li id="04a7" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">通过 VGG19 网络传播输出图像，获得输出图像在内容层的特征图，即<strong class="ih hj">目标内容特征图。</strong></li><li id="5869" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">计算目标内容特征图与输入内容图像的特征图<em class="ll"> c </em>之间的内容损失。</li><li id="3a40" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">获取输出图像在每个风格层的风格特征图，又名<strong class="ih hj">目标风格特征图</strong>。</li><li id="3fa4" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">计算<strong class="ih hj">目标样式特征图的 Gram 矩阵。</strong></li><li id="026c" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">计算目标风格特征图和输入风格图像特征图的 Gram 矩阵之间的误差。</li><li id="8e8a" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">对每个样式层重复此操作，以获得整体样式损失。</li><li id="630f" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">计算总损失(内容损失+风格损失)。</li><li id="233f" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">运行优化器/优化算法以减少输出图像的总损失/误差。</li><li id="ffbe" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">重复步骤 8-15，直到总损耗达到最小值。</li></ol><h1 id="28aa" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">密码</h1><p id="9dcd" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">下面实现神经风格转移的代码是用 PyTorch 编写的，这是一个流行的深度学习库。</p><ol class=""><li id="ed06" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated">导入必要的模块</li></ol><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="4991" class="lq jr hi mo b fi ms mt l mu mv">from PIL import Image<br/>import matplotlib.pyplot as plt<br/>import numpy as np</span><span id="b62c" class="lq jr hi mo b fi mw mt l mu mv">import torch</span><span id="0bcd" class="lq jr hi mo b fi mw mt l mu mv">import torch.optim as optim<br/>from torchvision import transforms, models</span></pre><p id="0d82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.定义一个函数来加载输入图像(内容和样式)。使用 PIL 库打开图像，该库返回图像的 NumPy 表示。通过应用变换将图像转换为 torch 张量</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="f2ac" class="lq jr hi mo b fi ms mt l mu mv">def load_image(image_path,max_size=400,shape=None):<br/>  image = Image.open(image_path).convert('RGB')<br/>  if max(image.size) &gt; max_size:<br/>    size = max_size<br/>  else:<br/>    size = max(image.size)<br/>  if shape is not None:<br/>    size = shape</span><span id="38a3" class="lq jr hi mo b fi mw mt l mu mv">in_transforms = transforms.Compose([</span><span id="7265" class="lq jr hi mo b fi mw mt l mu mv">transforms.Resize((size,int(1.5 * size))),</span><span id="92d4" class="lq jr hi mo b fi mw mt l mu mv">transforms.ToTensor(),</span><span id="7f6b" class="lq jr hi mo b fi mw mt l mu mv">transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])</span><span id="383f" class="lq jr hi mo b fi mw mt l mu mv">image = in_transforms(image)[:3,:,:].unsqueeze(0)<br/>  return image</span></pre><p id="e750" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.创建一个方法将 torch 张量转换回 NumPy 表示，以便可以使用 Matplotlib 显示它。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="2336" class="lq jr hi mo b fi ms mt l mu mv">def im_convert(tensor):<br/>  image = tensor.to("cpu").clone().detach()<br/>  image = image.numpy().squeeze()<br/>  image = image.transpose(1, 2, 0)<br/>  image = image * np.array((0.229, 0.224, 0.225)) + np.array(<br/>    (0.485, 0.456, 0.406))<br/>  image = image.clip(0, 1)<br/>  <br/>  return image</span></pre><p id="17cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.定义从网络/模型的指定图层提取要素地图的方法。将特征地图存储在特征地图字典中。这种方法适用于内容和风格的图像。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="3e6a" class="lq jr hi mo b fi ms mt l mu mv">def get_features(image,model,layers=None):<br/>  if layers is None:<br/>    layers = {'0': 'conv1_1','5': 'conv2_1',<br/>              '10': 'conv3_1',<br/>              '19': 'conv4_1',<br/>              '21': 'conv4_2',  ## content layer<br/>              '28': 'conv5_1'}<br/>  features = {}<br/>  x = image<br/>  <br/>  for name,layer in enumerate(model.features):<br/>    x = layer(x)<br/>    if str(name) in layers:<br/>      features[layers[str(name)]] = x<br/>  <br/>  return features</span></pre><p id="2fc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.定义一种方法来计算所有风格特征图的 gram 矩阵表示。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="5e42" class="lq jr hi mo b fi ms mt l mu mv">def gram_matrix(tensor):<br/>  _,n_filters, h, w = tensor.size()<br/>  tensor = tensor.view(n_filters, h*w)<br/>  gram = torch.mm(tensor,tensor.t())</span><span id="0ff8" class="lq jr hi mo b fi mw mt l mu mv">return gram</span></pre><p id="5fd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.创建一个 VGG19 网络实例，并使用预先训练好的 ImageNet 权重对其进行初始化。禁用所有网络参数的梯度，因为不需要训练网络&amp;权重保持固定。您可以从<a class="ae jp" href="https://download.pytorch.org/models/vgg19-dcbb9e9d.pth" rel="noopener ugc nofollow" target="_blank">这里</a>下载重量。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="5ae2" class="lq jr hi mo b fi ms mt l mu mv">vgg = models.vgg19()<br/>vgg.load_state_dict(torch.load('vgg19-dcbb9e9d.pth'))</span><span id="6a97" class="lq jr hi mo b fi mw mt l mu mv">for param in vgg.parameters():<br/>  param.requires_grad_(False)</span></pre><p id="bf33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7.用平均池替换最大池层，以获得更好的结果。原因是 max-pool 通过移除信息，而 average pooling 通过在对像素值求平均的过程中包含信息来保留丢失的信息。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="9315" class="lq jr hi mo b fi ms mt l mu mv">for i,layer in enumerate(vgg.features):<br/>  if isinstance(layer,torch.nn.MaxPool2d):<br/>    vgg.features[i] =  torch.nn.AvgPool2d(kernel_size=2,stride=2,padding=0)</span></pre><p id="e85e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">8.检查您的机器是否有 GPU，以便 PyTorch 可以利用 GPU 来提高处理速度。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="06e3" class="lq jr hi mo b fi ms mt l mu mv">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>vgg.to(device).eval()</span></pre><p id="2025" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">9.加载输入内容和样式图像。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="3dde" class="lq jr hi mo b fi ms mt l mu mv">content = load_image('content image path').to(device)<br/>style = load_image('style image path').to(device)</span></pre><p id="81bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">10.从它们各自的输入图像中获得相应的内容和风格特征图。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="64d0" class="lq jr hi mo b fi ms mt l mu mv">content_features = get_features(content,vgg)<br/>style_features = get_features(style, vgg)</span></pre><p id="1b45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">11.获取风格特征图的 gram 矩阵表示。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="39fe" class="lq jr hi mo b fi ms mt l mu mv">style_grams = {<br/>layer : gram_matrix(style_features[layer]) for layer in style_features<br/>}</span></pre><p id="84cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">12.创建具有随机值的输出图像张量，其大小与内容图像相同。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="10a4" class="lq jr hi mo b fi ms mt l mu mv">target = torch.rand_like(content).requires_grad_(True).to(device)</span></pre><p id="3222" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">13.初始化从中提取样式要素地图的每个图层的样式权重值。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="c801" class="lq jr hi mo b fi ms mt l mu mv">style_weights = {'conv1_1': 0.75,<br/>                 'conv2_1': 0.5,<br/>                 'conv3_1': 0.2,<br/>                 'conv4_1': 0.2,<br/>                 'conv5_1': 0.2}</span></pre><p id="54a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">14.创建 Adam optimizer 实例，以优化输出目标张量的误差 wrt。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="b46f" class="lq jr hi mo b fi ms mt l mu mv">optimizer = optim.Adam([target], lr=0.01)</span></pre><p id="caa5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">15.为总损失函数指定 alpha(内容权重)和 beta(样式权重)值。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="4425" class="lq jr hi mo b fi ms mt l mu mv">content_weight = 1e4<br/>style_weight = 1e2</span></pre><p id="05c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">16.以指定的迭代次数运行样式传递循环。每次迭代计算内容损失和风格损失。应用 Adam 优化器来减少目标的总损失 wrt。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="972f" class="lq jr hi mo b fi ms mt l mu mv">for i in range(2000):<br/>  optimizer.zero_grad()<br/>  target_features = get_features(target, vgg)<br/>  <br/>  content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)<br/>  <br/>  style_loss = 0<br/>  for layer in style_weights:<br/>    target_feature = target_features[layer] <br/>    target_gram = gram_matrix(target_feature)<br/>    _,d,h,w = target_feature.shape<br/>    style_gram = style_grams[layer]</span><span id="381a" class="lq jr hi mo b fi mw mt l mu mv">layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)</span><span id="8554" class="lq jr hi mo b fi mw mt l mu mv">style_loss += layer_style_loss/(d*h*w)</span><span id="2d74" class="lq jr hi mo b fi mw mt l mu mv">total_loss = content_weight*content_loss + style_weight*style_loss</span><span id="371d" class="lq jr hi mo b fi mw mt l mu mv">total_loss.backward(retain_graph=True)</span><span id="31e3" class="lq jr hi mo b fi mw mt l mu mv">optimizer.step()</span><span id="89ef" class="lq jr hi mo b fi mw mt l mu mv">if i % 10 == 0:<br/>    total_loss_rounded = round(total_loss.item(),2)</span><span id="7487" class="lq jr hi mo b fi mw mt l mu mv">content_fraction = round(content_weight*content_loss.item()/total_loss.item(),2)</span><span id="ee40" class="lq jr hi mo b fi mw mt l mu mv">style_fraction = round(style_weight*style_loss.item()/total_loss.item(),2)</span><span id="eab2" class="lq jr hi mo b fi mw mt l mu mv">print('Iteration {}, Total loss: {} - (content: {}, style {})'.format(<br/>      i,total_loss_rounded, content_fraction, style_fraction))</span></pre><p id="5574" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">17.将输出张量转换回 NumPy 数组以显示它&amp;查看结果。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="7eed" class="lq jr hi mo b fi ms mt l mu mv">final_img = im_convert(target)<br/>plt.imshow(final_img)</span></pre><div class="je jf jg jh fd ab cb"><figure class="mx ji my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/156f7132dd810fb75f52d42337417d30.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*mIo2bjgYUf8XJ3IILeMTXg.jpeg"/></div></figure><figure class="mx ji nd mz na nb nc paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/4e9825cb1c822795a7411ef2182fd32a.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*ukkXaFRcKvMxI6cVA8SGng.jpeg"/></div></figure><figure class="mx ji ne mz na nb nc paragraph-image"><img src="../Images/f97a64ac37aa55e6125f5063a682a41e.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*YYsZ4l-qJydBIIECTbNFMw.png"/></figure></div></div></div>    
</body>
</html>