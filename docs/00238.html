<html>
<head>
<title>DataHack Radio #15: Exploring Reinforcement Learning with Xander Steenbrugge</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DataHack电台#15:与史云光·斯廷布鲁格探讨强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/datahack-radio-15-exploring-reinforcement-learning-with-xander-steenbrugge-a033e0bf020c?source=collection_archive---------2-----------------------#2019-01-06">https://medium.com/analytics-vidhya/datahack-radio-15-exploring-reinforcement-learning-with-xander-steenbrugge-a033e0bf020c?source=collection_archive---------2-----------------------#2019-01-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7ec8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">强化学习的全行业应用有哪些？为什么它的进展一直很慢？未来5年RL有什么值得期待的？在本期DataHack电台节目中找出答案！</em></p><h1 id="485b" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">介绍</h1><figure class="kc kd ke kf fd kg"><div class="bz dy l di"><div class="kh ki l"/></div></figure><blockquote class="kj kk kl"><p id="6a7e" class="if ig jd ih b ii ij ik il im in io ip km ir is it kn iv iw ix ko iz ja jb jc hb bi translated">“如果智能是一块蛋糕，监督学习将是蛋糕上的糖衣，强化学习将是蛋糕上的樱桃。”— Yann LeCun，卷积网络的创始人</p></blockquote><p id="17bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强化学习算法近年来一直在敲开工业应用的大门。他们最终会在2019年把门吹开吗？阻碍强化学习的最大障碍是什么？RL在未来会把我们带到哪里，有没有一个上限？</p><p id="2b20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在DataHack Radio上迎来了2019年的第15集，其中有史云光·斯廷布鲁格，他带领我们穿越了强化学习的广泛而复杂的世界。是的，以上这些问题在这一集里都得到了很好的处理。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kp"><img src="../Images/15058e88b3587d09c597cc553fa282e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*IiBwDqNZ5IhDD9zy.jpg"/></div></figure><p id="63e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">史云光善于将最复杂的话题分解成易于理解的概念，这是一笔真正无价的财富。我遇到史云光是因为他在YouTube上的热门频道“Arxiv Insights ”,当我在2018年DataHack峰会上看到他的直播时，我真的很欣赏他的演讲风格。在这一集里，他解释具有挑战性的主题的能力也得到了充分的展示。</p><p id="d991" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章旨在强调这一集讨论的关键方面，包括史云光关于强化学习的思想和相关主题。我鼓励你听完整集，其中史云光更详细地阐述了他的RL理论和想法。快乐聆听！</p><p id="b2c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">你可以在以下任何一个平台上订阅DataHack Radio，以便在新一集发布时收到通知，或者浏览我们的档案:</em></p><ul class=""><li id="353b" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc kx ky kz la bi translated"><a class="ae lb" href="https://soundcloud.com/datahack-radio" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">音云</strong> </a></li><li id="896c" class="ks kt hi ih b ii lc im ld iq le iu lf iy lg jc kx ky kz la bi translated"><a class="ae lb" href="https://www.analyticsvidhya.com/blog/category/podcast/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">分析Vidhya </strong> </a></li><li id="b7cf" class="ks kt hi ih b ii lc im ld iq le iu lf iy lg jc kx ky kz la bi translated"><a class="ae lb" href="https://itunes.apple.com/in/podcast/datahack-radio/id1397786677?mt=2" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> iTunes </strong> </a></li></ul><h1 id="f2b3" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">史云光·斯廷布鲁格的背景</h1><p id="cd06" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">史云光是比利时根特大学的土木工程师。他的整个教育背景都集中在电子学上，如制造晶体管、微电路等。他选择了编码，目的是让想法进入执行阶段比纯粹用电子设备要快得多。</p><p id="eecd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">毫不奇怪，史云光的硕士毕业论文(2015年完成)是关于可以进行脑电波(EEG)分类的脑机接口。你可能已经在YouTube上看到了这个系统的一个应用——当病人戴上一个与这个机制相连的耳机时，他/她可以用他/她的思想移动连接的计算机屏幕上的光标。</p><p id="e74f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于脑电信号数据含有大量噪声，因此需要大量的预处理工作。一旦数据被清理，史云光就会在将数据输入机器学习分类器之前进行手动特征提取。当史云光从事他的项目时，神经网络还处于相对初级阶段。现在有了同样的数据，他很乐意直接将CNN(卷积神经网络)应用于EEG信号。迷人的东西！</p><h1 id="56f2" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">进入强化学习的世界</h1><p id="7f78" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">史云光是一名机器学习顾问，他偶然看到了DeepMind的2015年<a class="ae lb" href="https://deepmind.com/research/dqn/" rel="noopener ugc nofollow" target="_blank">论文，其中他们介绍了DQN算法</a>。你可以用同样的算法玩任何类型的游戏？这一突破激起了史云光的兴趣，并引导他探索强化学习这一极其复杂的领域。这是他在一个非常简单的层面上对这项工作的看法:</p><blockquote class="kj kk kl"><p id="f8a1" class="if ig jd ih b ii ij ik il im in io ip km ir is it kn iv iw ix ko iz ja jb jc hb bi translated"><em class="hi">“这并不像看起来那么难。这是监督学习，但有一些调整。”—史云光</em></p></blockquote><p id="10d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你会问，真的这么简单吗？下面是史云光通过对比这两种类型的学习来解释他的思维过程的总结:</p><p id="c418" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">监督学习和强化学习之间的区别在于，对于RL，我们有一个在环境中四处移动的代理，它有能力采取行动(比如向特定方向移动)。这个代理可以是一个算法，或者一个人，或者一个物体。它采取的行动会影响来自环境的输入。只有代理经过几次迭代，我们才能知道它离实现最终目标还有多远。当谈到监督学习时，输入和输出从一开始就已经被很好地定义了。</p><blockquote class="kj kk kl"><p id="3806" class="if ig jd ih b ii ij ik il im in io ip km ir is it kn iv iw ix ko iz ja jb jc hb bi translated"><em class="hi">“强化学习系统可以学会做一些我们人类不知道如何做的事情。”—史云光</em></p></blockquote><h1 id="71b4" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">行业内强化学习的现状</h1><p id="210f" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">众所周知，强化学习的进展比其他领域要慢。史云光之前提到的执行阶段的想法在RL中花费了大量的时间。在学术界，这些代理是在模拟中训练的(像ATARI游戏环境)，因为这些算法非常“样本低效”。换句话说，我们需要给这些代理展示大量的例子，然后他们才能学到实质性的东西。</p><p id="9cbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们到达真实世界的设置时，数据量通常是稀疏的(许多数据科学家都会涉及到！).此外，我们需要根据需求将算法推广到不同的设置。这是阻碍强化学习渗透到商业产品和服务的两个主要挑战。</p><p id="afe4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">说到这里，史云光提到了一个非常酷的增强学习成功应用的用例——机器人农业。请收听播客，了解这项技术的具体工作原理。</p><blockquote class="kj kk kl"><p id="ba56" class="if ig jd ih b ii ij ik il im in io ip km ir is it kn iv iw ix ko iz ja jb jc hb bi translated">“我们正处于一场非常大的革命的开端，我们可以从硬编码机器人发展到智能学习机器人。”—史云光</p></blockquote><p id="5bf9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">播客中另一个有趣的金块——大多数研究仍然集中在单代理强化学习上(与多代理强化学习相比),因为那里仍然有太多的问题需要解决。</p><h1 id="b454" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">强化学习的挑战</h1><p id="b271" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">在强化学习的当前状态下，我们面临两个主要障碍:</p><ol class=""><li id="0ea5" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc lm ky kz la bi translated">有大量的框架和工具包存在监督学习。像预训练模型这样的东西让任何想了解特定技术如何工作的人的生活变得容易多了。但是在目前的情况下，拥有这样的强化学习几乎是不可能的。你能想象在RL的背景下迁移学习吗？每个人目前都在研究中使用自己的定制库和工具</li><li id="8394" class="ks kt hi ih b ii lc im ld iq le iu lf iy lg jc lm ky kz la bi translated">正如史云光上面提到的，大多数研究都是在模拟环境中进行的。在实际环境中用少得多的数据训练代理的问题仍然需要解决</li></ol><h1 id="3fb7" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">开始强化学习的资源</h1><p id="5ebb" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">强化学习是一个巨大的领域，包含多个主题和科目。现在，没有一个单一的平台能让你直接进入这个领域。根据史云光的说法，从头开始理解监督学习是一个好主意，因为强化学习建立在这个基础上。因此，在进入RL概念之前，先熟悉一下图像分类器是如何工作的。</p><p id="5b7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">史云光的学习之旅始于<a class="ae lb" href="http://karpathy.github.io/2016/05/31/rl/" rel="noopener ugc nofollow" target="_blank">Andrej Karpathy的这篇博客</a>，名为“来自像素的Pong”。这是一篇有点冗长的阅读，但清楚地说明了如何从监督学习到强化学习。如果你正在寻找一个更具视觉吸引力的指南，看看史云光的“强化学习入门”视频:</p><figure class="kc kd ke kf fd kg"><div class="bz dy l di"><div class="ln ki l"/></div></figure><p id="52a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是Faizan Shaikh的另一篇<a class="ae lb" href="https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/" rel="noopener ugc nofollow" target="_blank">优秀的初学者RL入门</a>。你也应该在RL上查看OpenAI的教育资源，名为<a class="ae lb" href="https://blog.openai.com/spinning-up-in-deep-rl/" rel="noopener ugc nofollow" target="_blank">旋转起来</a>。这是一个资源和主题的综合列表，对我个人来说非常有帮助。</p><h1 id="424a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">结束注释</h1><p id="5b01" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">不到50分钟，一个非常愉快和简洁的强化学习介绍。在这个播客之前，我对多代理强化学习知之甚少，所以这对我来说是一个非常酷的部分。我在上面分享的史云光的资源列表足以让你的手脏起来，所以我希望在不久的将来看到我们社区更多的人开始学习RL。</p><p id="d862" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">开启2019年的精致播客。DataHack Radio今年将推出更多内容，所以请戴上您的学习帽，在此之前，祝您收听愉快！</p></div><div class="ab cl lo lp gp lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hb hc hd he hf"><p id="3d53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">原载于2019年1月6日</em><a class="ae lb" href="https://www.analyticsvidhya.com/blog/2019/01/datahack-radio-reinforcement-learning-xander/" rel="noopener ugc nofollow" target="_blank"><em class="jd">www.analyticsvidhya.com</em></a><em class="jd">。</em></p></div></div>    
</body>
</html>