<html>
<head>
<title>Gradient Decent Animated</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">渐变体面动画</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-decent-animated-25f4bdd39109?source=collection_archive---------23-----------------------#2020-10-10">https://medium.com/analytics-vidhya/gradient-decent-animated-25f4bdd39109?source=collection_archive---------23-----------------------#2020-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/42474a5a153be677fb8836fcd0ae5829.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GqEabk9paWf3y2JF"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">卢卡斯·克拉拉在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><blockquote class="iv"><p id="1c8a" class="iw ix hi bd iy iz ja jb jc jd je jf dx translated">“我们的目标是找到最大价值优化的最佳点，在这一点上，愚蠢的风险与过度的谨慎相平衡。”史蒂文·j·鲍恩</p></blockquote><h1 id="2e24" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated">问题和直觉:</h1><p id="5f1f" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la jf hb bi translated">大多数数据科学算法实际上是优化问题，给定数据和我们定义的模型，我们试图找到“最适合数据”的最佳参数。为此，我们需要一种方法来测量误差，这是以<strong class="kg hj">损失函数</strong>的形式出现的。较高的值表明我们的参数估计很差，而较低的值表明我们做得很好。问题简化为简单地找到最小化该损失函数的参数。</p><p id="a817" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la jf hb bi translated">一种方法是在我们的空间中搜索所有可能的参数，并找到哪一个是最优的，这对于小问题可能有效，但是随着问题的规模增加，在合理的时间内不可能做到。</p><p id="703e" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la jf hb bi translated">幸运的是，我们可以使用一些简单的数学概念来大大减少解决问题所需的努力。这些概念中最核心的就是<strong class="kg hj">渐变。</strong></p><p id="4535" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la jf hb bi translated">简单来说，函数的梯度是一个指向最大增长方向的向量，所以如果我们反过来，我们会找到一个最小值，它可能不是最好的最小值。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/dd67534f91972551a0f0f831ca62cce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UckZvoQeAxsFnA9PafTrdA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者对梯度下降图像的说明</figcaption></figure><p id="cf8e" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la jf hb bi translated">在整篇文章中，我将使用一些动画来说明每种渐变体面的工作方式。这些例子是通过解决一个简单的线性回归问题产生的，以均方误差作为我们的损失函数。您可以在我的 Github repo 中获得完整的代码:</p><p id="5a1f" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la jf hb bi translated"><a class="ae iu" href="https://github.com/tariqmassaoudi/GradientDecentAnimated" rel="noopener ugc nofollow" target="_blank">https://github.com/tariqmassaoudi/GradientDecentAnimated</a></p><h1 id="f851" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr ll jt ju jv lm jx jy jz ln kb kc kd bi translated">算法:</h1><p id="066e" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la jf hb bi translated">梯度下降将从一个随机点开始。为了进行下一步，它将计算梯度，这将为我们提供方向，但我们仍然需要知道要采取的步骤的大小，即<strong class="kg hj">学习率</strong>的来源。</p><p id="0c92" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la jf hb bi translated">学习率是我们设置的一个系数，用于确定在梯度方向上走多远。低学习率会导致收敛缓慢，而高学习率会产生更快的<strong class="kg hj">收敛</strong>，但如果步长太大，会导致我们发散。</p><p id="31d9" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la jf hb bi translated">我们重复采取更多的步骤，直到我们达到最优。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/3cde422177d692d55047f86bdd1d186a.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/0*t7fTd6eaH0QxMRU1.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">梯度下降步骤</figcaption></figure><h1 id="644c" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr ll jt ju jv lm jx jy jz ln kb kc kd bi translated">梯度下降的类型</h1><h2 id="7871" class="lp jh hi bd ji lq lr ls jm lt lu lv jq kp lw lx ju kt ly lz jy kx ma mb kc mc bi translated">批量梯度下降:</h2><p id="032e" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la jf hb bi translated">这种类型的梯度下降将使用所有的训练数据来计算梯度。它需要稳定和准确的步骤，但在性能方面代价很高。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es md"><img src="../Images/d60588a087ab46c2d11601e2e693a6b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*Bl8EmB-0MVMR_EuEFa-ayw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">批量梯度下降误差与迭代次数的关系-作者图片</figcaption></figure><figure class="lh li lj lk fd ij"><div class="bz dy l di"><div class="me mf l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">批量梯度下降线性回归收敛—作者视频</figcaption></figure><figure class="lh li lj lk fd ij"><div class="bz dy l di"><div class="me mf l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">批量梯度下降误差收敛—作者视频</figcaption></figure><h2 id="9942" class="lp jh hi bd ji lq lr ls jm lt lu lv jq kp lw lx ju kt ly lz jy kx ma mb kc mc bi translated">随机梯度下降；</h2><p id="2249" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la jf hb bi translated">当 Batch 使用所有数据时，random 将选择一个随机点并计算该点的梯度。它不如批量替代方法准确，但速度极快。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/4c456a4c80f068e7371e166b7240e429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*uV4f1d68486wl1zRKFU1_A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">随机梯度下降误差与迭代次数的关系—图片由作者提供</figcaption></figure><figure class="lh li lj lk fd ij"><div class="bz dy l di"><div class="me mf l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">随机梯度下降线性回归收敛—作者视频</figcaption></figure><figure class="lh li lj lk fd ij"><div class="bz dy l di"><div class="me mf l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">随机梯度下降误差收敛—作者视频</figcaption></figure><h2 id="c263" class="lp jh hi bd ji lq lr ls jm lt lu lv jq kp lw lx ju kt ly lz jy kx ma mb kc mc bi translated">小批量梯度体面:</h2><p id="3453" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la jf hb bi translated">迷你批处理方法带来了两个世界的最佳效果，在每次迭代中，它会随机选择几个数据点，我们选择的点越多，每步就越精确。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/a06fa05c588390c0cb0d88c385a1e7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*GB897VLLfirwTwH__17anw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">小批量梯度下降误差与迭代次数的关系批量大小为 10-图片由作者提供</figcaption></figure><figure class="lh li lj lk fd ij"><div class="bz dy l di"><div class="me mf l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">小批量梯度下降线性回归收敛-作者视频</figcaption></figure><figure class="lh li lj lk fd ij"><div class="bz dy l di"><div class="me mf l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">小批量梯度下降误差收敛-作者视频</figcaption></figure><h1 id="3cd7" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr ll jt ju jv lm jx jy jz ln kb kc kd bi translated">如果我们设定一个大的学习率呢？</h1><p id="db08" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la jf hb bi translated">学习率设置太大会导致算法大步走，发散。</p><figure class="lh li lj lk fd ij"><div class="bz dy l di"><div class="me mf l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">梯度体面大学习率-视频作者</figcaption></figure><h1 id="0c3c" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr ll jt ju jv lm jx jy jz ln kb kc kd bi translated">摘要</h1><p id="cabc" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la jf hb bi translated">梯度下降是一种广泛应用于机器学习的优化算法。背后的主要思想是沿着梯度的相反方向走，直到你达到一个最小值。</p><p id="32b7" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la jf hb bi translated">感谢阅读！❤</p><p id="ea5b" class="pw-post-body-paragraph ke kf hi kg b kh lb kj kk kl lc kn ko kp ld kr ks kt le kv kw kx lf kz la jf hb bi translated">关注我，了解更多信息丰富的数据科学内容。</p></div></div>    
</body>
</html>