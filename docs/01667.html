<html>
<head>
<title>Processing data with spark-core using Pyspark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Pyspark的spark核处理数据</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/processing-data-with-spark-core-using-pyspark-4b9106ca355d?source=collection_archive---------7-----------------------#2019-11-07">https://medium.com/analytics-vidhya/processing-data-with-spark-core-using-pyspark-4b9106ca355d?source=collection_archive---------7-----------------------#2019-11-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="dbfe" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">spark-核心处理样本数据，以便更熟悉spark。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/6f6b6e25f50e73310bf14f214ff636ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WuBKaJzMThPdtmyqZVmuSA.png"/></div></div></figure><p id="b04a" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">嘿伙计们，</p><p id="a20d" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">再一次带着一篇文章回来，这篇文章会让你更好地理解所有的spark函数。因此，对于那些认为spark-core无法使用的人来说，学习如何使用spark函数进行数据转换和聚合会让你受益匪浅。所以让我们开始吧。</p><p id="6ade" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">在这篇文章中，我使用kaggle的谷物数据作为例子。你可以从下面的链接下载数据。</p><p id="1522" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated"><a class="ae jy" href="https://www.kaggle.com/crawford/80-cereals#cereal.csv" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/crawford/80-cereals#cereal.csv</a></p><p id="c1c5" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">因此，让我们从创建sparkcontext开始，并读取csv文件。</p><pre class="jn jo jp jq fd jz ka kb kc aw kd bi"><span id="0231" class="ke kf hi ka b fi kg kh l ki kj">from pyspark import SparkContext<br/>import os<br/>os.environ[‘PYSPARK_PYTHON’] = ‘/usr/bin/python3.6’ <br/>sc = SparkContext("local","spark-processing")<br/>rdd = sc.textFile("cereal.csv")<br/>rdd.map(lambda line: line.split(",")).collect()</span></pre><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kk"><img src="../Images/c1d5012b6358642313ae5544a344d95f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lloSXzzHzmMrk4z69Zb4dQ.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">用于收集的输出</figcaption></figure><pre class="jn jo jp jq fd jz ka kb kc aw kd bi"><span id="f9f9" class="ke kf hi ka b fi kg kh l ki kj"> rdd.count()<br/>79</span></pre><p id="28a9" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">Spark在RDD(弹性分布式数据集)上工作，该数据集是为并行计算而构建的，不像pandas dataframe。它实际上工作和地图和减少概念。所以每一个单独的函数都必须一行一行地映射，这与应用在熊猫数据帧中非常相似。这将返回带有值列表的rdd，使其可以按元素方式访问。</p><p id="c637" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">这些数据是关于不同品牌的谷物的。所以我们来整理一下，看看热量最多的10大品牌。</p><pre class="jn jo jp jq fd jz ka kb kc aw kd bi"><span id="dc05" class="ke kf hi ka b fi kg kh l ki kj">rdd1.map(lambda x:(x[0],int(x[3]))).sortBy(lambda x:x[1],ascending=False).take(10)</span></pre><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kp"><img src="../Images/dac2d14c36bf32304687c131853c9216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4hb9i_lOFGNzhUaiOUPlCA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">按降序排序</figcaption></figure><p id="4952" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">在上面的部分中，我们使用map函数来选择我们需要显示的列。可以显示全部排序后的数据。但是为了便于理解，我使用map只选取了这两列。SortBy函数与SortByKey不同，sort by key只允许对键进行排序。在这里您可以选择您自己选择的列进行排序，并选择前10个。</p><p id="8539" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated"><strong class="iq hj">分组和求和</strong></p><p id="a49d" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">groupby可以通过reduceby和aggregateby函数来执行，这样会更快更有效。由于aggregateBy是一个高级概念，我将在下一篇博客中介绍它。</p><pre class="jn jo jp jq fd jz ka kb kc aw kd bi"><span id="e065" class="ke kf hi ka b fi kg kh l ki kj">rdd1.map(lambda x:(x[1],int(x[3]))).groupByKey().mapValues(lambda x: sum(x)).sortBy(lambda x:x[1],ascending=False).collect()</span><span id="c648" class="ke kf hi ka b fi kq kh l ki kj">rdd1.map(lambda x:(x[1],int(x[3]))).reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],ascending=False).collect()</span></pre><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kr"><img src="../Images/088f321bd180aabec64edfcb9e3b734e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1gAsu3_8TqJIMGq9lpcz9A.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">groupbykey函数</figcaption></figure><p id="0c43" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">groupbykey函数按键对数据进行分组，然后使用mapvalues和sum对每个组的值应用sum。最后，您将根据合计值对其进行升序排序。</p><p id="ccd6" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">reducebykey也做同样的动作，这里它更像是cumulative sum，(accum，n)，其中accum用0初始化，并为每个键添加一个元素。例如</p><p id="01fc" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">第一步累积(' k '，[x1，…xn])</p><p id="dfd2" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">第二步缩减(' k '，sum(x1，，，xn))</p><p id="b1b8" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated"><strong class="iq hj">过滤</strong></p><pre class="jn jo jp jq fd jz ka kb kc aw kd bi"><span id="f60a" class="ke kf hi ka b fi kg kh l ki kj">lambda x:(x[0],x[15])).filter(lambda x:float(x[1]) &gt; 50.00).take(5)</span></pre><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es ks"><img src="../Images/12f4ff92fe9691118344d7e69c52968d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8gwhtaH9M5JV_QslEI2Gw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">过滤器</figcaption></figure><p id="e807" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">这就是这个职位的所有人。我将提出spark-core本身的高级功能。所以剩下的就是未完待续…..</p><p id="2451" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">希望你们都喜欢阅读。</p><p id="eee2" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">谢了。</p></div></div>    
</body>
</html>