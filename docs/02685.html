<html>
<head>
<title>Core Learning Algorithm of Artificial Neural Network: Back-propagation Demystified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工神经网络的核心学习算法:反向传播揭秘</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/demystifying-backpropagation-6d165a4d9bf3?source=collection_archive---------19-----------------------#2019-12-28">https://medium.com/analytics-vidhya/demystifying-backpropagation-6d165a4d9bf3?source=collection_archive---------19-----------------------#2019-12-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="55aa" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">打开黑盒</h2><div class=""/><div class=""><h2 id="45e8" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">通过一个简单的例子理解反向传播算法的基本过程</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/cc915ebe7e8f39570d6f8453b69b4465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W3EirSuXlkpV5TTy.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">[ <a class="ae jw" href="https://deeplizard.com/images/ai-cyborg-cropped-2.png" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="b594" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">动机</h1><p id="aff1" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">最近在一次ML面试中，我被要求解释反向传播算法是如何工作的。</p><p id="ff85" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">我没有准备好回答这个问题，也不知道从何说起。虽然我对它的工作原理有所了解，但那都是我想象出来的。我不知道从哪里解释，用什么例子。</p><p id="a4a9" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">因此，这篇文章背后的动机是以最简单的方式揭开反向传播算法的神秘面纱，以便像我们这样的ML初学者可以在采访中解释它，并理解该算法如何工作的底层过程。</p></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="4104" class="jx jy hi bd jz ka lx kc kd ke ly kg kh ix lz iy kj ja ma jb kl jd mb je kn ko bi translated">先决条件</h1><p id="2305" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">在开始之前，我假设您对人工神经网络的工作原理有所了解。</p><p id="b1dc" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">至少到你知道一个典型的神经网络结构是什么样子，以及输入层、隐藏层、突触、权重、偏差、输出层和成本函数的作用是什么的程度。😅</p></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="08fe" class="jx jy hi bd jz ka lx kc kd ke ly kg kh ix lz iy kj ja ma jb kl jd mb je kn ko bi translated">什么是反向传播？</h1><p id="80c8" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">如果你已经开始学习<strong class="kr hs">机器学习</strong>和<strong class="kr hs">人工神经网络</strong>，那么你可能会遇到术语<strong class="kr hs">反向传播</strong>。</p><p id="9fb0" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">让我们先来看看定义，了解它是什么，有什么作用:</p><blockquote class="mc md me"><p id="c004" class="kp kq mf kr b ks ll is ku kv lm iv kx mg ln la lb mh lo le lf mi lp li lj lk hb bi translated">反向传播是“误差反向传播”的缩写，是一种使用梯度下降的人工神经网络监督学习算法。—<a class="ae jw" href="https://brilliant.org/wiki/backpropagation/" rel="noopener ugc nofollow" target="_blank">Brilliant.org</a></p></blockquote><p id="2a3d" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">给定人工神经网络和误差函数，反向传播算法计算误差函数相对于网络权重的负梯度。</p><p id="8daa" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">它是人工神经网络如何学习背后的核心算法。</p></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><p id="c7a5" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">如果已经有一段时间了，那么您可能已经理解了反向传播的目的。</p><p id="b8dd" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">在这篇文章中，我们不去关注它是什么，而是去理解它实际上是如何工作的，以及它为什么如此棒。</p><p id="6802" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">让我们用最简单的例子来理解它。</p><h1 id="e127" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">它是如何工作的？—尽可能简单</h1><p id="e4bc" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">为了理解算法，让我们以具有一个输入层和一个输出层的简单人工神经网络为例。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mj"><img src="../Images/6acdec165540c54efa41e6d69e00732f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZjtwrS5jbtnZ9sdsmHigbA.png"/></div></div></figure><p id="8036" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">它有输入(<strong class="kr hs"> <em class="mf"> x) </em> </strong>与权重(<strong class="kr hs"><em class="mf">【w)</em></strong>)和一个输出(<strong class="kr hs"><em class="mf">【ŷ】)</em></strong>。</p><blockquote class="mc md me"><p id="8af2" class="kp kq mf kr b ks ll is ku kv lm iv kx mg ln la lb mh lo le lf mi lp li lj lk hb bi translated"><strong class="kr hs">注意:</strong>这里我们不打算使用任何隐藏层、激活函数或添加任何偏置单元。</p></blockquote><p id="8f4b" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">因为我们没有任何隐藏层，也没有使用任何激活函数，所以我们的输出(<strong class="kr hs"><em class="mf">【ŷ】</em></strong>)是输入<strong class="kr hs"> <em class="mf"> (x) </em> </strong>和权重<strong class="kr hs"> <em class="mf"> (w) </em> </strong>的乘积:</p><pre class="jh ji jj jk fd mk ml mm mn aw mo bi"><span id="57c3" class="mp jy hi ml b fi mq mr l ms mt"><strong class="ml hs"><em class="mf">ŷ</em></strong><em class="mf"> = x * w</em></span></pre><p id="d4a0" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">现在，假设我们的训练数据的输入(x)=1.5，预期输出(y)=0.5。</p><pre class="jh ji jj jk fd mk ml mm mn aw mo bi"><span id="633a" class="mp jy hi ml b fi mq mr l ms mt">+-----------+--------------------+<br/>|input(x)   | expected output(y) |<br/>+-----------+--------------------+<br/>| 1.5       | 0.5                |<br/>+-----------+--------------------+</span></pre><p id="41f1" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">给定输入1.5，我们期望我们的网络产生输出0.5。</p><p id="6ada" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">让我们用随机值<em class="mf"> w=0.8 </em>初始化我们的权重(w)，看看它会产生什么。</p><pre class="jh ji jj jk fd mk ml mm mn aw mo bi"><span id="4246" class="mp jy hi ml b fi mq mr l ms mt">1.5 * 0.8 = 1.2</span><span id="fb1a" class="mp jy hi ml b fi mu mr l ms mt">+-----------+--------------------+---------------+<br/>|input(x)   | expected output(y) | output(ŷ)     |<br/>+-----------+--------------------+---------------+<br/>| 1.5       | 0.5                | 1.2           |<br/>+-----------+--------------------+---------------+</span></pre><p id="a7e4" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">以目前的重量，你可以看到它产生1.2。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mv"><img src="../Images/18adbfc7b69771e111b4fcf104527311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*yY-i1B5ORRue03bGFvqbfA.png"/></div></figure><p id="bbfa" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">现在我们需要为我们的网络定义一个错误，通过这个错误我们可以知道它的性能有多差。</p><p id="dc98" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">为简单起见，让我们取实际输出和预期输出之间的差，然后求平方。</p><pre class="jh ji jj jk fd mk ml mm mn aw mo bi"><span id="fe54" class="mp jy hi ml b fi mq mr l ms mt">j = (ŷ - y)²</span></pre><p id="1cfc" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">既然我们已经有了误差函数(<em class="mf"> j </em>)，我们的工作就是最小化它。反向传播算法试图通过沿着误差函数下降来实现:这被称为<strong class="kr hs">梯度下降。</strong></p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mw"><img src="../Images/e456352a05f28db64fb102b4bea416c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gj1_Sj4jfh13eTAYSPXj-A.png"/></div></div></figure></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><p id="2d54" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated"><strong class="kr hs">梯度下降</strong>值得单独解释，但简单来说:它查看误差函数给定点处切线的斜率，并计算出哪个方向是下坡。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mx"><img src="../Images/c4346ee17fbd808f1ec2a42ec002970b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VGQsorogatzNk3LU"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">梯度下降[ <a class="ae jw" href="https://camo.githubusercontent.com/a401a48f5503c52004369148a784e779aa7e3411/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a70775049472d475748796150564d564747354f6841512e676966" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><p id="5f1d" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">现在，为了执行反向传播，我们需要计算误差函数(j)的导数(变化率)。</p><p id="e4b3" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">由于我们对网络没有太多的控制，我们将通过改变权重(w)来最小化我们的误差。</p><p id="ddcc" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">(j)相对于(w)的变化率表达式为:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es my"><img src="../Images/db631603345bf4e0b3221fe28826ee15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3sCD4UlRYOH-FQwQd-Vc1A.png"/></div></div></figure><p id="35e1" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">为了计算一阶导数，我们必须使用微分的链式法则。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mz"><img src="../Images/86be898f94696aed63d3548a0f1d7913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*czEs4okrB4y-kpBG.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">链式法则</figcaption></figure><p id="6b31" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">链式法则告诉我们如何在函数内部求导，而幂法则告诉我们降低指数2并乘以它。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es na"><img src="../Images/24eb36576760be5486fe7d9c005bf39c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*OakiLg3nuhhM9lnMXQMfug.png"/></div></figure><blockquote class="mc md me"><p id="a3e2" class="kp kq mf kr b ks ll is ku kv lm iv kx mg ln la lb mh lo le lf mi lp li lj lk hb bi translated"><strong class="kr hs">注意:</strong>这里我们取外部函数的导数，然后乘以内部函数。因为<em class="hi"> y=0.5 </em>是一个常数，常数的导数是0，并且它相对于<em class="hi"> w </em>不变，所以我们剩下dŷ/dw.</p></blockquote><pre class="jh ji jj jk fd mk ml mm mn aw mo bi"><span id="d5e2" class="mp jy hi ml b fi mq mr l ms mt">dj/dw = 2(ŷ - y)*d(ŷ)/dw</span></pre><p id="cc6c" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">自<strong class="kr hs"><em class="mf">ŷ</em></strong><em class="mf">= x . w:</em></p><pre class="jh ji jj jk fd mk ml mm mn aw mo bi"><span id="1c17" class="mp jy hi ml b fi mq mr l ms mt">dj/dw = 2(ŷ - y) *d(1.5 * w)/d(w) = 2(ŷ-y)*1.5 = 4.5w - 1.5</span></pre><p id="be4e" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">现在我们有了方程:<code class="du nb nc nd ml b">dj/dw = 4.5w-1.5</code>让我们执行梯度下降。</p><p id="ee5a" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">我们基本上要从旧的重量中减去误差函数相对于重量的变化率，同时，我们要将变化率乘以学习率(<em class="mf"> lr </em> ) = <em class="mf"> 0.1 </em>。</p><blockquote class="mc md me"><p id="fc16" class="kp kq mf kr b ks ll is ku kv lm iv kx mg ln la lb mh lo le lf mi lp li lj lk hb bi translated"><strong class="kr hs">注:</strong>学习率定义了我们想要走多大的一步。</p></blockquote><p id="b050" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">我们降低梯度的公式是:</p><pre class="jh ji jj jk fd mk ml mm mn aw mo bi"><span id="ff11" class="mp jy hi ml b fi mq mr l ms mt">w(new) := w(old)-lr(dj/dw)<br/>w(new) := w(old) - 0.1(4.5w(old) - 1.5)</span></pre><p id="4e1e" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">现在让我们用这个公式来计算我们的新重量:</p><pre class="jh ji jj jk fd mk ml mm mn aw mo bi"><span id="10a2" class="mp jy hi ml b fi mq mr l ms mt">+-----------+--------------------+<br/>|old weight | new weight         |<br/>+-----------+--------------------+<br/>| 0.8       | 0.59               |<br/>+-----------+--------------------+<br/>| 0.59      | 0.4745             |<br/>+-----------+--------------------+<br/>| 0.4745    | 0.410975           |<br/>+-----------+--------------------+<br/>| 0.410975  | 0.37603625         |<br/>+-----------+--------------------+<br/>| 0.37603625| 0.3568199375       |<br/>+-----------+--------------------+</span><span id="bc88" class="mp jy hi ml b fi mu mr l ms mt">+------------+-----------+<br/>|0.3568199375| 0.333333  |<br/>+------------+-----------+</span></pre><p id="5c11" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">我们已经成功训练了我们的神经网络。</p><p id="348b" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">我们网络的最佳权重是0.333。</p><pre class="jh ji jj jk fd mk ml mm mn aw mo bi"><span id="857c" class="mp jy hi ml b fi mq mr l ms mt">1.5 * 0.333 = 0.5</span><span id="3ae7" class="mp jy hi ml b fi mu mr l ms mt">+-----------+--------------------+---------------+<br/>|input(x)   | expected output(y) | output(ŷ)     |<br/>+-----------+--------------------+---------------+<br/>| 1.5       | 0.5                | 0.5           |<br/>+-----------+--------------------+---------------+</span></pre><h1 id="4ec4" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki iy kj ja kk jb kl jd km je kn ko bi translated">最后的话</h1><p id="be44" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">这就是反向传播的基本过程如何找到人工神经网络的最佳权重。</p><p id="5c3b" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">我们已经看到了没有任何激活函数或偏差的非常简单的网络的过程。然而，在现实世界中，过程也是相同的，但有点复杂，因为我们必须向后传播到许多具有非线性和偏置单元的隐藏层。</p><p id="65fe" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">为了采访和表达你的理解，我想这么多解释就足够了。</p><h2 id="7cbc" class="mp jy hi bd jz ne nf ng kd nh ni nj kh ky nk nl kj lc nm nn kl lg no np kn ho bi translated">不合你的胃口？</h2><p id="3a51" class="pw-post-body-paragraph kp kq hi kr b ks kt is ku kv kw iv kx ky kz la lb lc ld le lf lg lh li lj lk hb bi translated">如果这种解释也不能打动你的大脑，或者你想了解更多，那么你可以遵循<a class="ae jw" href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" rel="noopener ugc nofollow" target="_blank"> 3Blue1Brown </a>的<a class="ae jw" href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" rel="noopener ugc nofollow" target="_blank">神经网络</a>教程系列和Welch Labs的<a class="ae jw" href="https://www.youtube.com/watch?v=bxe2T-V8XRs&amp;list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU" rel="noopener ugc nofollow" target="_blank">神经网络揭秘</a>系列。</p><p id="0cef" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">他们还解释了一步一步的过程与漂亮的视觉表现。</p><p id="b80c" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">或者如果你想从不同的角度学习，那么<a class="ae jw" href="https://www.youtube.com/watch?v=6BMwisTZFr4" rel="noopener ugc nofollow" target="_blank">这里</a>是布兰登的一个很好的解释。</p><p id="c729" class="pw-post-body-paragraph kp kq hi kr b ks ll is ku kv lm iv kx ky ln la lb lc lo le lf lg lp li lj lk hb bi translated">希望你学到了一些东西😀。感谢阅读。</p></div></div>    
</body>
</html>