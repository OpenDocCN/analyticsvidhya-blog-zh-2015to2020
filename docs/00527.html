<html>
<head>
<title>Attention as an Adaptive TF-IDF Technique for Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意力作为深度学习的自适应TF-IDF技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/attention-as-adaptive-tf-idf-for-deep-learning-549cbee1a61a?source=collection_archive---------2-----------------------#2019-07-22">https://medium.com/analytics-vidhya/attention-as-adaptive-tf-idf-for-deep-learning-549cbee1a61a?source=collection_archive---------2-----------------------#2019-07-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7631" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">注意力就像深度学习的TF-IDF。注意力和TF-IDF都提升了一些单词的重要性。但是，虽然TF-IDF权重向量对于一组文档是静态的，但是注意力权重向量将根据特定的分类目标进行调整。注意力为那些影响分类目标的单词导出更大的权重，从而在深度学习黑盒中打开了进入决策过程的窗口… </em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/b7f6bb4321edae96b8c9deb7a3112506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vebIXxhkYg3tIto7"/></div></div></figure><p id="81ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意了！你父母告诉你了，对吗？也是好事。当我们集中注意力时，我们实际上可能会学到一些东西。深度学习也不例外！注意，对于同样的预测任务，一个浅层网络可能比一大堆层更聪明。或者它可能需要更少的数据来训练，或者使用更少的参数来获得相同的技能。就像通过在课堂上集中注意力来考试，而不是在考试前一天晚上苦读到凌晨！所有好的东西和天生的吸引力，对不对？</p><p id="acec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TF-IDF <em class="jd"> </em>长期以来，单词的权重一直是构建各种NLP任务的文档向量的主要方法。但是对于给定的文档库，不管分类目标是什么，TF-IDF向量都是固定的。然而，有一个文本语料库需要根据不同的目标进行分类是很常见的。可能是根据其内容的性质，即政治/科学/体育，或者可能是根据其适用的地方，即美洲/亚洲/欧洲，或者可能是根据情绪积极/中立/消极等。在每种情况下，影响特定分类目标的决策过程的单词/短语自然会不同。但是TF-IDF向量是静态的。因此，即使在分类目标改变时，为相同的预测向量产生不同的预测的全部负担基本上落在分类算法上。</p><p id="6e51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使深度学习技术能够有利地根据所讨论的NLP任务对不同的单词、<em class="jd">和</em>使用不同的权重，这就是<a class="ae jq" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">注意力机制</a>的目的。而且是自动的。注意力嵌入到整体优化过程中，并挑选出哪些单词/短语正在驱动特定的分类目标。因此，预测步骤的最终向量是为所讨论的目标定制的。这种额外的灵活性允许更好的预测性能。</p><p id="f55d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章的目的是说明在一个简单的多类、多标签文档分类任务的环境中注意力的好处。单词的顺序与这里的分类任务无关。我们在嵌入层的顶部实现了<a class="ae jq" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank"> Bahdanau Attention </a>。当屏蔽不存在时，使用Keras中的功能API。当需要掩蔽时，使用来自<a class="ae jq" href="https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d" rel="noopener ugc nofollow" target="_blank">克里斯特斯巴兹奥蒂斯</a>的定制注意层。然而，这对我们这里的图解问题没有影响。我们将在这里浏览一些代码片段，但是完整的代码可以从<a class="ae jq" href="https://github.com/ashokc/Attention-as-Dynamic-Tf-Idf-for-Deep-Learning" rel="noopener ugc nofollow" target="_blank"> github </a>下载。那就让我们继续吧。</p><h1 id="d841" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">1.注意识别重要的单词</h1><p id="4759" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">并非文档中的所有单词对其分类都很重要。我们可能有一个确定的想法。例如，出现像<em class="jd"> good </em>这样的词可能表示对一部电影的正面评价。短语<em class="jd">不好</em>将指示负面评价，而<em class="jd">不太好</em>可能指示中性，其中<em class="jd">如此如此好</em>回到正面，而<em class="jd">如此如此</em>指示中性。你可以用坏的来代替好的，也可以颠倒积极和消极的分类。虽然<em class="jd">很好</em>可能表示正面评价，但<em class="jd">不太好</em>可能是中性的。你明白了。实际上有无限多的常用单词和短语的组合，它们的净情感可以有不同的方式。试图用grep/regex(或者甚至用一个成熟的搜索引擎，使用复杂的布尔逻辑和slop短语)来解释它们都是徒劳的。此外，这些有影响力的词可能会因语料库的不同而有所差异。因此，在一个语料库上精心构建的脚本在另一个语料库上不会起作用。这种方法根本无法扩展。</p><p id="39b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相反，我们想要一台机器来为我们做这件事。给定一组训练文档，我们希望算法学习哪些单词及其组合正在驱动所指示的分类目标。这就是深度学习中<a class="ae jq" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">注意机制</a>的内容。它为算法提供了一种识别控制词和短语的方法，作为正在进行的分类任务的一部分。因此，根据分类任务，分别为相同的句子/文档自动导出单词/句子的不同权重。下面的图1用一个句子说明了这一点。为颜色(或动物，如果目标是通过动物参考分类的话)单词获得的较大注意力权重有助于驱使句子向量落入正确的颜色(或动物)桶中。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ku"><img src="../Images/cc05256d45423caa4fac4a5ecb88cbbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aAdFNZbxHja0XKf-.jpg"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图一。一个句子是词向量的加权和。注意机制根据分类目标给出不同的权重。对于给定的文档存储库，Tf-idf权重是静态的。上面的重量数字是为了说明的目的。</figcaption></figure><p id="2158" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个极好的附带好处是，我们对算法为什么将文档放入它所做的任何类中有了一些了解。这是巨大的，因为我们正在谈论可解释的人工智能。这里实现的单词级注意机制来自<a class="ae jq" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank"> Bahdanau等人的工作。艾尔。</a>另一个很好的参考文献是<a class="ae jq" href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" rel="noopener ugc nofollow" target="_blank">杨等人的，该文献将这一思想扩展到了句子级注意的层次。al </a>。</p><h1 id="a9ce" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">2.在喀拉斯实施Bahdanau关注</h1><p id="8d6f" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">一个句子是一系列单词。每个单词都是一定长度的数字向量——每个单词的长度都相同。单词的数字向量可以通过Keras中的嵌入层直接获得，也可以从外部源(如FastText)导入到模型中。注意力权重向量是按照<a class="ae jq" href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" rel="noopener ugc nofollow" target="_blank"> Yang等人提出的等式作为训练过程的一部分而获得的。al </a>。方程式很简单。这是我们的命名和索引惯例。</p><ul class=""><li id="5b1c" class="kz la hi ih b ii ij im in iq lb iu lc iy ld jc le lf lg lh bi translated"><em class="jd"> N_s </em>:任意一句话的最大字数</li><li id="3652" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><em class="jd"> N_a </em>:第一个<em class="jd"> tanh </em>密集层的任意/可选单元数</li><li id="c16b" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><em class="jd"> N_f </em>:每个词向量的长度</li><li id="a76f" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><em class="jd"> w_i </em>:句子中第I个单词的单词向量。</li><li id="eaab" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><em class="jd"> A^j_i </em>:第j个句子中第I个单词的注意权重</li><li id="88d2" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated">S_j :第j句</li></ul><p id="d450" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个句子的这些<em class="jd"> N_s </em>个词向量中的每一个首先通过具有<em class="jd"> N_a </em>个单元的<em class="jd"> tanh </em>密集层(<a class="ae jq" href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" rel="noopener ugc nofollow" target="_blank">杨等人a </a> l选择<em class="jd"> N_a </em>约为100)。接着是另一个具有1个单位的tanh密集层。第二个<em class="jd"> tanh </em>层的目的是将第一层的<em class="jd"> N_a </em> long向量减少为一个标量。我们当然可以在1个单元中只使用一个<em class="jd"> tanh </em>层，但是具有两个<em class="jd"> tanh </em>层，第一个层具有<em class="jd"> N_a </em> &gt; 1，这提供了更多的灵活性。此外，代替第二个<em class="jd"> tanh </em>层，也许可以在第一个<em class="jd"> tanh </em>层之后的矢量中累加<em class="jd"> N_a </em>值来获得这个标量——但这里没有尝试。在任何情况下，最终结果都是句子中的每个单词有一个标量值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ln"><img src="../Images/11373b7cdafdf7f10e612ecbfd74a5f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/0*MJCXOLmtA6fML2yv.jpg"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">等式1。按照NLP任务的总体目标，句子中的单词向量通过几个<em class="lo"> tanh </em>层回归到每个单词的标量权重。</figcaption></figure><p id="5b6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为第j个句子中的每个单词收集上面的<em class="jd"> v_i </em>产生了一个<em class="jd"> N_s </em>长向量<em class="jd"> v </em>，该向量在softmax上给出该句子的<em class="jd"> N_s </em>长注意力权重向量<em class="jd"> A^j </em>。然后，句子向量被简单地获得为注意力加权的单词向量的总和。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lp"><img src="../Images/e692492a4628ce3a41e7bfc6c5ae0a03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/0*GcdGoKM4DWKGL9po.jpg"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">等式2:用于分类的句子向量是注意力加权词向量的总和。很像那个句子的tf-idf向量。</figcaption></figure><h2 id="3192" class="lq js hi bd jt lr ls lt jx lu lv lw kb iq lx ly kf iu lz ma kj iy mb mc kn md bi translated">2.1无掩蔽</h2><p id="88d7" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">虽然等式很简单，但在Keras中实现注意力有点棘手。当不使用屏蔽时——也就是说，当每个句子都有完全相同的字数时，这就简单多了，我们可以用函数式API来实现它。如果我们走这条路，所有较短的句子都需要添加一个虚拟/未使用的单词(如图1中的<em class="jd"> zzzzz </em>)来使它们和最长的句子一样长。有点低效，但是对于简单的演示问题，比如这篇文章中的问题，这不是问题。下面的图2(来自<a class="ae jq" href="https://github.com/philipperemy/keras-attention-mechanism" rel="noopener ugc nofollow" target="_blank"> Philippe Rémy </a>的修改想法，以及来自<a class="ae jq" href="https://www.youtube.com/watch?v=oaV_Fv5DwUM" rel="noopener ugc nofollow" target="_blank">王菽一</a>的you-tube视频)显示了当不使用掩蔽时，我们需要在Keras中做些什么来实现注意。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es me"><img src="../Images/7feb690ef31ba6efe7c049d6506f52d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/0*cQv10Yd2YfoTy9pV.jpg"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图二。当不使用屏蔽时，使用功能API的注意的Keras实现。获取表示句子的单词向量数组，并产生注意力加权的句子向量，该向量可用于任何分类目标。所有的句子都有相同数量的单词(包括虚拟单词),没有屏蔽。</figcaption></figure><p id="54c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述内容可以定义为一个函数，它将一系列单词向量作为自变量，并按照等式2返回一个句子向量。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="mf mg l"/></div></figure><h2 id="74a9" class="lq js hi bd jt lr ls lt jx lu lv lw kb iq lx ly kf iu lz ma kj iy mb mc kn md bi translated">2.2带掩蔽</h2><p id="2632" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">克里斯特斯·巴兹奥蒂斯有一个注意力的实现，作为一个支持屏蔽的自定义层，这样我们就不必显式地在句子中添加虚拟单词。当我们想要使用屏蔽时，我们在模型中使用该实现。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="mf mg l"/></div></figure><p id="1faf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于我们的演示问题，它产生了与2.1节中没有屏蔽的实现相同的结果。</p><h1 id="303a" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">3.示例:注意多类、多标签分类</h1><p id="671f" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">现在剩下要做的就是把它应用到文本语料库中，并展示注意力在给定NLP任务目标的情况下动态识别重要单词的工作。重现结果的代码可以从<a class="ae jq" href="https://github.com/ashokc/Attention-as-Dynamic-Tf-Idf-for-Deep-Learning" rel="noopener ugc nofollow" target="_blank"> github </a>下载。</p><p id="0731" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">长度为15-25个单词的句子是通过从1000个独特的单词列表中随机选择单词来生成的，包括一些颜色和动物单词。一个句子最多可以有4个不同的颜色词和/或动物词。目的是根据颜色或包含的动物来对句子进行分类。当目标是按颜色分类时，一个句子属于它作为单词包含的所有颜色类别。同样，当分类目标是动物参考时。因此，无论分类标准是什么，我们都在讨论一个多类、多标签的分类问题。下面的代码片段生成文档。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="mf mg l"/></div></figure><p id="fce4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">句子的标签向量是number_of_colors + 1长(当按颜色分类时),其中1位于出现在句子中的颜色词的位置，否则为0。例如，我们使用7种彩虹颜色的VIBGYOR，因此标签向量的长度为8——包括“无”标签。</p><p id="10d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">构建整个Keras模型非常简单。我们定义了一个中间模型，这样我们就可以从名为“attention_vector_layer”的层中输出注意力权重。由于这是一个多类别、多标签的分类问题，我们在最终的密集层中使用sigmoid激活，并使用具有分类准确性的二元交叉熵作为度量。查看托拜厄斯·斯特巴克的文章和T2关于为什么这些是正确选择的讨论。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="mf mg l"/></div></figure><p id="e441" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型在有/无关注和有/无屏蔽的情况下运行。Keras制作的张量流图证实了我们在第2节中的模型。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mh"><img src="../Images/29620cce0e787f087d2e42b0c5afe3e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FgOGQFE7sD5A0m7R.jpg"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图二。有和没有注意的数据流，有和没有屏蔽的数据流。当掩蔽到位时，使用来自克里斯特斯Baziotis的注意层。</figcaption></figure><h1 id="131b" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">4.结果</h1><p id="9811" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">当目标是颜色和动物时，运行有/无注意/遮蔽的模拟。在所有四次运行中，使用了1300个句子，其中训练/有效/测试:832/208/260拆分。每当一个类别的预测概率对于一个句子大于0.5时，我们用那个类别来标记那个句子。图3示出了针对不同分类目标获得的一个这样的句子中的单词的注意力权重。这肯定是我们想看到的，因为重要的词已经被注意力机制自动<em class="jd">挑选出来，并给予比其他词更大的权重。这直接转化为一个句子的正确类别的更大概率，因为这些单词正在驱动分类决策。</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mi"><img src="../Images/433c6d437160961b4600e3584199785f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/0*BfvH9Ym6Dxi7I-yZ.jpg"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图3。注意机制识别控制分类目标的正确单词，并为这些单词获得更大的权重。因此，当分类目标改变时，相同单词的权重是不同的。这与独立于NLP任务的tf-idf加权不同。</figcaption></figure><p id="5ecb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于同一个句子，下面的图4显示了类别概率。注意，正确类别的概率接近1.0，其余类别的概率接近0.0。正是我们想要的。如果不加注意，我们会看到，对于不正确的类别<em class="jd">靛蓝</em>预测的概率高达0.75，而对于正确的类别<em class="jd">老虎</em>预测的概率低至0.05。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mj"><img src="../Images/67a1a9b57e49781d7e8725d5d5a0c78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*I3ghsviLZ1AMtB1g.jpg"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图4。当注意力到位时，控制分类的词的较大权重允许精确的分类..屏蔽对这个简单的问题没有任何影响。但是如果不加注意，预测会有额外的&amp;缺失的类。</figcaption></figure><p id="0691" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意分类对所有260个测试文档产生了100%准确的预测。也就是说，对于每个测试文档，预测了每个实际的类，并且只有实际的类(概率&gt; 0.5)。如果不注意，一些正确的类别会被遗漏(预测概率&lt; 0.5) and some wrong classes were predicted (predicted probability &gt; 0.5)，如图4所示。</p><h1 id="a33c" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">5.结论</h1><p id="105f" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">注意机制使得模型能够将较大的权重赋予对于给定分类任务重要的单词。这增强了分类的准确性，同时阐明了为什么特定的文档被分类到特定的桶中——这对于现实世界的应用程序来说是一条非常有用的信息。</p><p id="a0b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个例子中，单词的顺序与分类无关。因此不需要更花哨的递归层或卷积层。但是单词的顺序和单词的相对接近度对于真实文档中所传达的意思是很重要的。在接下来的文章中，我们将在LSTM(单向或双向，return_sequences=True)层上应用相同的注意机制，它可以更好地处理单词序列很重要的文档。</p></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><p id="6cee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">原载于2019年7月22日【http://xplordat.com】<a class="ae jq" href="http://xplordat.com/2019/07/22/attention-as-adaptive-tf-idf-for-deep-learning/" rel="noopener ugc nofollow" target="_blank"><em class="jd"/></a><em class="jd">。</em></em></p></div></div>    
</body>
</html>