<html>
<head>
<title>Deploying huggingface‘s BERT to production with pytorch/serve</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用pytorch/serve将huggingface的BERT部署到生产中</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deploy-huggingface-s-bert-to-production-with-pytorch-serve-27b068026d18?source=collection_archive---------0-----------------------#2020-04-25">https://medium.com/analytics-vidhya/deploy-huggingface-s-bert-to-production-with-pytorch-serve-27b068026d18?source=collection_archive---------0-----------------------#2020-04-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2fb5afe1be2c5e0d236a90546dd2f81b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FWNNcaRsmHIuDrDt.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">火炬服务建筑。图片首次出现在TorchServe 上的一篇<a class="ae iu" href="https://aws.amazon.com/blogs/machine-learning/deploying-pytorch-models-for-inference-at-scale-using-torchserve/" rel="noopener ugc nofollow" target="_blank"> AWS博客文章中。</a></figcaption></figure><p id="5a66" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">TL；DR: pytorch/serve是一个新的令人敬畏的框架，为生产中的torch模型提供服务。这个故事教你如何将它用于像伯特这样的<em class="jt">拥抱脸/变形金刚</em>模型。</p><p id="6ad1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">传统上，在生产中为<em class="jt"> pytorch </em>车型服务是<strong class="ix hj">挑战</strong>，因为过去没有标准框架可用于这项任务。这种差距使其主要竞争对手<em class="jt"> tensorflow </em>到<a class="ae iu" href="https://blog.exxactcorp.com/pytorch-vs-tensorflow-in-2020-what-you-should-know-about-these-frameworks/" rel="noopener ugc nofollow" target="_blank">在许多生产系统上保持强大的掌控力</a>，因为它在其<a class="ae iu" href="https://www.tensorflow.org/tfx/guide/serving" rel="noopener ugc nofollow" target="_blank"><em class="jt">tensor flow/serving</em></a>框架中为此类部署提供了坚实的工具。</p><p id="af89" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，如今大多数新的模型和方法倾向于首先在pytorch 中开发和提供，因为研究人员喜欢它的原型灵活性。这在研究实验室开发的最新技术和大多数公司通常部署到生产中的模型之间产生了差距。在自然语言处理(NLP)等快速发展的领域中，尽管像<a class="ae iu" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"><em class="jt">hugging face/transformers</em></a>这样的框架努力为两种框架提供模型兼容性，但这种差距仍然非常明显。在实践中，新方法的开发和采用往往首先发生在pytorch中，当框架和生产系统赶上并集成了tensorflow版本时，新的和更多改进的模型已经弃用了它。</p><p id="39bf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最近，<em class="jt"> pytorch </em>开发者发布了他们的新服务框架<em class="jt"> pytorch/serve </em>以一种直接的方式解决了这些问题。</p><h1 id="164a" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">TorchServe简介</h1><blockquote class="ks kt ku"><p id="68ba" class="iv iw jt ix b iy iz ja jb jc jd je jf kv jh ji jj kw jl jm jn kx jp jq jr js hb bi translated">TorchServe是一个灵活易用的工具，用于服务PyTorch模型。</p></blockquote><p id="cf75" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://pytorch.org/serve/" rel="noopener ugc nofollow" target="_blank"> TorchServe </a>(资源库:<a class="ae iu" href="https://github.com/pytorch/serve" rel="noopener ugc nofollow" target="_blank"> pytorch/serve </a>)是最近(撰写本文时的4天前)发布的框架，由<em class="jt"> pytorch </em>开发人员开发，允许简单高效地生产经过训练的pytorch模型。</p><p id="5f36" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我推荐阅读<a class="ae iu" href="https://aws.amazon.com/blogs/machine-learning/deploying-pytorch-models-for-inference-at-scale-using-torchserve/" rel="noopener ugc nofollow" target="_blank">这篇AWS博客文章</a>来全面了解<em class="jt">火炬服务</em>。</p><h1 id="79f4" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">服务变压器型号</h1><p id="ffec" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated"><em class="jt">hugging face/transformers</em>可以被认为是一个最先进的文本深度学习框架，并且已经显示出足够的灵活性，能够跟上这个快速发展的空间的快速发展。</p><p id="fc34" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于这是一个非常受欢迎的框架，拥有来自不同领域的许多活跃用户(Github上超过25k颗星)，因此毫不奇怪已经有人对使用TorchServe服务BERT和其他transformer模型感兴趣(例如这里的<a class="ae iu" href="https://github.com/pytorch/serve/issues/249" rel="noopener ugc nofollow" target="_blank"/>、这里的<a class="ae iu" href="https://github.com/pytorch/serve/issues/267" rel="noopener ugc nofollow" target="_blank"/>和这里的<a class="ae iu" href="https://github.com/pytorch/serve/issues/85" rel="noopener ugc nofollow" target="_blank"/>)。</p><p id="57d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个故事将解释如何用TorchServe服务你训练过的变形金刚模型。</p><h2 id="89a8" class="ld jv hi bd jw le lf lg ka lh li lj ke jg lk ll ki jk lm ln km jo lo lp kq lq bi translated">先决条件</h2><p id="ae05" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">为了避免这篇文章不必要的膨胀，我将做一个假设:你已经有一个训练过的BERT(或其他变形金刚句子分类器模型)检查点。</p><p id="b2e2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你没有，不要担心:我会提供一些参考指南，你可以根据这些指南很快找到自己的指南。</p><p id="67bc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">安装火炬服务器</strong></p><p id="4675" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">TorchServe为使用pip、conda或docker安装提供了简单的<a class="ae iu" href="https://github.com/pytorch/serve#install-torchserve" rel="noopener ugc nofollow" target="_blank">指南。目前，安装大致包括两个步骤:</a></p><ul class=""><li id="ec82" class="lr ls hi ix b iy iz jc jd jg lt jk lu jo lv js lw lx ly lz bi translated">安装Java JDK 11</li><li id="293e" class="lr ls hi ix b iy ma jc mb jg mc jk md jo me js lw lx ly lz bi translated">安装<em class="jt"> torchserve </em>及其python依赖项</li></ul><p id="594f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请仔细阅读上面链接的安装指南，以确保TorchServe已安装在您的计算机上。</p><p id="f905" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">训练一个拥抱脸伯特句子分类器</strong></p><p id="dbc6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有许多关于这方面的教程，由于我严重怀疑自己是否有能力补充现有的关于这一主题的知识，我只是给出一些我推荐的参考资料:</p><ul class=""><li id="03ad" class="lr ls hi ix b iy iz jc jd jg lt jk lu jo lv js lw lx ly lz bi translated"><a class="ae iu" href="https://blog.rosetta.ai/learn-hugging-face-transformers-bert-with-pytorch-in-5-minutes-acee1e3be63d" rel="noopener ugc nofollow" target="_blank">https://blog . Rosetta . ai/learn-hugging-face-transformers-Bert-with-py torch-in-5-minutes-acee 1 e 3 be 63d</a></li><li id="24cc" class="lr ls hi ix b iy ma jc mb jg mc jk md jo me js lw lx ly lz bi translated"><a class="ae iu" rel="noopener" href="/@nikhil.utane/running-pytorch-transformers-on-custom-datasets-717fd9e10fe2">https://medium . com/@ nikhil . utane/running-py torch-transformers-on-custom-datasets-717 FD 9 e 10 Fe 2</a></li></ul><p id="33e8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">获得训练过的BERT检查点的一个简单方法是使用<em class="jt"> huggingface </em> GLUE示例进行句子分类:</p><p id="7433" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://github.com/huggingface/transformers/blob/master/examples/run_glue.py" rel="noopener ugc nofollow" target="_blank">https://github . com/hugging face/transformers/blob/master/examples/run _ glue . py</a></p><p id="c5e8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">训练结束时，请确保将训练好的模型检查点(<em class="jt"> pytorch.bin </em>)、模型配置文件(<em class="jt"> config.json </em>)和tokenizer词汇文件(<em class="jt"> vocab.txt </em>)放在同一个目录下。在下面的内容中，我将使用一个经过训练的<em class="jt">“Bert-base-un cased”</em>检查点，并将其与标记化器词汇表一起存储在文件夹<em class="jt">中。/bert_model”。</em></p><p id="591c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作为参考，我的是这样的:</p><figure class="mg mh mi mj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/1e5fe706475ce15b53cd43a7cbd72d78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tlhd6CmMnw1rYTdHBp0_nQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">模型检查点文件夹，一些文件是可选的</figcaption></figure><p id="5cf3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">为我们的BERT模型定义TorchServe处理程序</strong></p><p id="196e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是salt: TorchServe使用<strong class="ix hj">处理程序</strong>的概念来定义服务模型如何处理请求。一个很好的特性是，在打包模型时，这些处理程序可以由客户端代码注入，从而允许大量的定制和灵活性。</p><p id="67eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是我为BERT/transformer分类器设计的一个非常基本的TorchServe处理程序的模板:</p><figure class="mg mh mi mj fd ij"><div class="bz dy l di"><div class="mk ml l"/></div></figure><p id="5a4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的负责人不做但你的负责人可能想做的几件事:</p><ul class=""><li id="8761" class="lr ls hi ix b iy iz jc jd jg lt jk lu jo lv js lw lx ly lz bi translated">文本的自定义预处理(这里我们只是进行标记化)</li><li id="b5fa" class="lr ls hi ix b iy ma jc mb jg mc jk md jo me js lw lx ly lz bi translated">伯特预测的任何后处理(这些可以添加到<em class="jt">后处理</em>功能中)。</li><li id="8935" class="lr ls hi ix b iy ma jc mb jg mc jk md jo me js lw lx ly lz bi translated">加载一组模型。实现这一点的一个简单方法是在<em class="jt">初始化</em>函数中加载额外的检查点，并在<em class="jt">推理</em>函数中提供集合预测逻辑。</li></ul><p id="3768" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">将训练好的关卡转换成火炬服务器MAR文件</strong></p><p id="7b2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">TorchServe使用一种叫做<a class="ae iu" href="https://pytorch.org/serve/model-archiver.html" rel="noopener ugc nofollow" target="_blank"> MAR(模型存档)</a>的格式来打包模型，并在它的模型库中对它们进行版本化。为了使它可以从TorchServe访问，我们需要将我们训练过的BERT检查点转换成这种格式，并在上面附加我们的处理程序。</p><p id="14e3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下命令可以实现这一目的:</p><pre class="mg mh mi mj fd mm mn mo mp aw mq bi"><span id="3ddb" class="ld jv hi mn b fi mr ms l mt mu">torch-model-archiver --model-name "bert" --version 1.0 --serialized-file ./bert_model/pytorch_model.bin --extra-files "./bert_model/config.json,./bert_model/vocab.txt" --handler "./<a class="ae iu" href="https://gist.github.com/MFreidank/3463d407a94ffe53d0d0daa137ad3973#file-transformers_classifier_torchserve_handler-py" rel="noopener ugc nofollow" target="_blank">transformers_classifier_torchserve_handler.py</a>"</span></pre><p id="f797" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该命令附加BERT模型的序列化检查点(<em class="jt">)。/Bert _ model/py torch _ model . bin</em>)添加到我们新的定制处理程序<em class="jt">中，transformers _ classifier _ torch serve _ handler . py</em>如上所述，并为配置和记号化器词汇表添加额外的文件。它生成一个TorchServe可以理解的名为<em class="jt"> bert.mar </em>的文件。</p><p id="e7bc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我们可以为我们的BERT模型启动一个TorchServe服务器(默认情况下它使用<a class="ae iu" href="https://pytorch.org/serve/rest_api.html" rel="noopener ugc nofollow" target="_blank">端口8080和8081 </a>),模型存储包含我们新创建的MAR文件:</p><pre class="mg mh mi mj fd mm mn mo mp aw mq bi"><span id="c05e" class="ld jv hi mn b fi mr ms l mt mu">mkdir model_store &amp;&amp; mv bert.mar model_store &amp;&amp; torchserve --start --model-store model_store --models bert=bert.mar</span></pre><p id="d717" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">就是这样！我们现在可以使用<a class="ae iu" href="https://pytorch.org/serve/inference_api.html" rel="noopener ugc nofollow" target="_blank">推理API </a>查询模型:</p><pre class="mg mh mi mj fd mm mn mo mp aw mq bi"><span id="999d" class="ld jv hi mn b fi mr ms l mt mu">curl -X POST http://127.0.0.1:8080/predictions/bert -T unhappy_sentiment.txt</span></pre><p id="7b3e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我的例子中，<em class="jt">是一个包含负面情绪例句的文件。我的模型正确地预测了该文本的负面情绪(类别0)。</em></p><p id="90e1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意，在<a class="ae iu" href="https://pytorch.org/serve/management_api.html#list-models" rel="noopener ugc nofollow" target="_blank">管理API </a>中有许多额外的有趣功能。例如，我们可以很容易地获得所有注册模型的列表，注册一个新的模型或新的模型版本，并动态地切换每个模型的服务模型版本。</p><p id="96cf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编码和服务愉快！</p></div></div>    
</body>
</html>