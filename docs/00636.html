<html>
<head>
<title>Neural Network Implementation: Derivatives, chain rule and multiplications.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络实现:导数、链式法则和乘法。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-network-implementation-derivatives-chain-rule-and-multiplications-818ea3aaf06e?source=collection_archive---------2-----------------------#2019-08-20">https://medium.com/analytics-vidhya/neural-network-implementation-derivatives-chain-rule-and-multiplications-818ea3aaf06e?source=collection_archive---------2-----------------------#2019-08-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4d2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们试图将重点放在神经网络背后的数学实现上。在阅读这篇博客之前，建议你应该对神经网络有一些基本的了解。</p><p id="4825" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经网络有两个主要阶段:</p><ol class=""><li id="ba66" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">正向传播</li><li id="ce6e" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">反向传播</li></ol><p id="9ea9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">神经网络(NN): </strong>神经网络是一种监督算法，其中我们有输入数据(自变量)和输出标签(因变量)。通过使用训练数据，我们将训练神经网络来预测输出变量。开始时，神经网络作出一些几乎是随机的预测。将这些预测与实际输出进行比较，并将误差计算为预测输出和实际输出之间的差值。我们的目标是训练神经网络来减少这个误差/成本函数。</p><p id="d69a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们借助一个例子来理解神经网络的数学实现。我们将要创建的神经网络具有以下可视化表示形式。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es jr"><img src="../Images/1dccf97df077e2886348c506cf4367f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P70d3yRwHqoMSea7uaJ5Iw.png"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">具有两个隐层的神经网络</figcaption></figure><p id="5311" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图表示一个有两个隐藏层的神经网络，每个层有4个神经元。在输入层，我们有两个神经元指示训练数据中的两个特征列。</p><ol class=""><li id="d0fe" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">正向传播:</strong>在神经网络的前馈步骤中，使用输入节点值、权重和偏差进行预测。<br/>数学上:<br/><em class="kh">zh21 = x1 w1+x2 w2+b<br/>ah21 = activation _ function(zh21)<br/></em>相似类型的计算将在每个节点发生，并且计算将向前传播直到最后一个节点</li><li id="cccb" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">反向传播(BP): </strong>一旦我们完成正向传播，我们在输出层得到预测为<em class="kh"> ao </em>。使用此输出和实际标签，成本函数/误差表示如下:</li></ol><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es ki"><img src="../Images/f77654c3d445d00abc3929b39d8e3394.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*NUTLS2nowCWy4aU1dnQWmQ.png"/></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">成本函数/误差</figcaption></figure><ul class=""><li id="1980" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc kj jj jk jl bi translated">我们的目标是微调网络参数以最小化误差项(成本函数)，以这种方式使预测更接近实际标签。</li><li id="4441" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated">如果你观察我们的神经网络，你会注意到我们只能控制权重和偏差。为了最小化成本，我们需要找到成本函数返回可能最小值的权重和偏差值。成本越小，我们的预测就越准确。这是一个<a class="ae kk" href="https://en.wikipedia.org/wiki/Optimization_problem" rel="noopener ugc nofollow" target="_blank"> <em class="kh">优化问题</em> </a>在这里我们要找到<a class="ae kk" href="https://en.wikipedia.org/wiki/Maxima_and_minima" rel="noopener ugc nofollow" target="_blank"> <em class="kh">函数极小值</em> </a> <em class="kh">。</em></li><li id="dbe9" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated">求函数的极小值，我们可以使用<a class="ae kk" href="https://www.youtube.com/watch?v=sDv4f4s2SB8&amp;t=722s" rel="noopener ugc nofollow" target="_blank"> <em class="kh">梯度下降</em> </a>算法。</li></ul><p id="64ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">反向传播的数学实现:</strong></p><ul class=""><li id="8dbd" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc kj jj jk jl bi translated">在神经网络的每一层引入的权重负责在预测中引入误差。因此，我们必须更新每一层的权重。在进行反向传播时，我们将从输出层到第一个隐藏层以相反的顺序更新权重。</li><li id="1286" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated">如上图所示，我们必须依次更新三层的权重，即输出层(BP_Phase1)、隐藏层2(BP_Phase2)和隐藏层1(BP_Phase3)</li><li id="0aef" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated">更新权重时，我们将使用梯度下降，如下所示:</li></ul><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es kl"><img src="../Images/d9e9a52605de130c2b37d15b8bedaf93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xePfEb3EupKbOmfxKbaGVA.png"/></div></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">梯度下降</figcaption></figure><ul class=""><li id="0d15" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc kj jj jk jl bi translated">在上面的等式中，<code class="du km kn ko kp b">J</code>是成本函数。基本上，上面的等式表示:找到成本函数相对于每个权重和偏差的偏导数，并从现有权重值中减去结果，以获得新的权重值。</li><li id="4b18" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated">现在，我们将一步一步地更新上面提到的所有三层的权重。</li></ul><p id="5609" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">渐变下降:</strong>请参考此<a class="ae kk" href="https://www.youtube.com/watch?v=sDv4f4s2SB8&amp;t=722s" rel="noopener ugc nofollow" target="_blank"> <em class="kh">视频</em> </a> <em class="kh"> </em>深入了解。</p><ul class=""><li id="ecf1" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc kj jj jk jl bi translated">对损失函数中的每个参数(截距/重量)求导数。</li><li id="4e74" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated">为参数选择随机值</li><li id="81d8" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated">将参数值代入导数</li><li id="ef7b" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated">计算步长:<strong class="ih hj">步长=斜率*学习率</strong></li><li id="1074" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated">计算新参数:<br/> <strong class="ih hj">新参数=旧参数-步长</strong></li><li id="841a" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated">重复最后三个步骤，直到步长非常小或达到最大步数。</li></ul><p id="912e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> A) BP阶段1: </strong>在这个阶段，我们将在输出层更新权重。在这里起作用的数学部分是导数、链式法则和乘法。</p><ul class=""><li id="6600" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc kj jj jk jl bi translated">我们将在该层计算成本函数w.r.t .权重的导数为<strong class="ih hj"><em class="kh">dcost _ dwo</em></strong><br/>由于我们没有这些项的直接值，我们将使用链式法则来计算它们，如下所示:</li></ul><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kq"><img src="../Images/1d48dd6a3be4052109b506a6e6a7906e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*Xrt0CQLuq-pW58Oj4lRGhg.png"/></div></figure><p id="c2a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们准备计算等式2。<br/>现在，我们可以使用上述术语更新输出层的权重:<br/><strong class="ih hj"><em class="kh">wo-= lr * dcost _ dwo</em></strong>，<br/>其中lr:学习速率。</p><p id="a2f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> B) BP阶段2: </strong>在这个阶段，我们将更新隐藏层2的权重。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kr"><img src="../Images/8bb3b683a59d7686dba996b9b4e0fdad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*la4ymC5ddBK3EcDocjBgIA.png"/></div></figure><p id="10dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们准备计算等式3。<br/>现在，我们可以使用上述术语更新输出层的权重:<br/><strong class="ih hj"><em class="kh"/>wh1-= lr * dcost _ dw h1</strong><br/>其中lr:学习速率。</p><p id="0bcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BP阶段3: 在这个阶段，我们将更新隐藏层1的权重。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es ks"><img src="../Images/cb11ced31a16b4d3fecb8b54a157ef83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*oGyhttGccBwQUhSmYmUvvg.png"/></div></figure><p id="c3c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们准备计算等式4。<br/>现在，我们可以使用上述术语更新输出层的权重:<br/><strong class="ih hj">wh2-= lr * dcost _ dwh 2</strong><br/>其中lr:学习速率。</p><p id="ee32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们更新了所有的权重，从而完成了我们的第一个纪元。对于下一个时期，将使用这些更新的权重，并且该过程将继续该数量的时期。</p><p id="bd7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kh">源代码:</em> </strong></p><figure class="js jt ju jv fd jw"><div class="bz dy l di"><div class="kt ku l"/></div><figcaption class="kd ke et er es kf kg bd b be z dx translated">Python中的完整脚本</figcaption></figure><p id="3183" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了保持篇幅小且集中于数学部分，我们没有详细解释所有的术语。</p><p id="53d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献</strong>:</p><ul class=""><li id="ffd5" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc kj jj jk jl bi translated"><a class="ae kk" href="https://stackabuse.com/creating-a-neural-network-from-scratch-in-python-adding-hidden-layers/" rel="noopener ugc nofollow" target="_blank">https://stack abuse . com/creating-a-neural-network-from-scratch-in-python-adding-hidden-layers/</a></li><li id="effd" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc kj jj jk jl bi translated"><a class="ae kk" href="https://www.youtube.com/watch?v=sDv4f4s2SB8" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=sDv4f4s2SB8</a></li></ul></div></div>    
</body>
</html>