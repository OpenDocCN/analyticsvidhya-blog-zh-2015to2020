<html>
<head>
<title>Ensemble Learning — Your Machine Learning savior and here is why (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习——你的机器学习救星，原因如下(第 1 部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ensemble-learning-your-machine-learning-savoir-and-here-is-why-part-1-78ef52c8c365?source=collection_archive---------24-----------------------#2020-12-14">https://medium.com/analytics-vidhya/ensemble-learning-your-machine-learning-savoir-and-here-is-why-part-1-78ef52c8c365?source=collection_archive---------24-----------------------#2020-12-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0ebb661f29ef49d26038ef0d21a91c40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6k6WNblevsVREQfKG5W3QQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">罗曼·贝斯的照片</figcaption></figure><p id="ffa8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">这是一系列以简单易懂的方式解释机器学习中的集成方法的帖子。在这篇文章中，我们将讨论异质系综。</em></p><p id="73f1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1/异质系综</p><p id="b83e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2/ <a class="ae iu" rel="noopener" href="/analytics-vidhya/ensemble-learning-bagging-random-forest-part-2-6cc81eb3470d">同质系综—装袋</a></p><p id="8bf0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3/同质系综—增强</p><p id="98f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作为一名机器学习爱好者和自学者，我写了以下帖子作为自己的学习材料，并愿意与我的学习者分享。我们的想法是展示不同的系综技术的全貌，何时使用，以及它们对我们的模型有什么影响。</p><p id="3691" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">集成是一种通过使用模型集合进行预测的机器学习技术。这就是为什么大多数时候，学习者倾向于在学习了所有其他经典算法之后才发现这种方法的原因。这种技术的优势源于“群体的智慧”这一理念，即:</p><blockquote class="ju jv jw"><p id="3816" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">“群体中信息的聚合，导致的决策，往往比群体中任何一个成员所能做出的决策更好”(维基百科，群体的智慧，2020)。</p></blockquote><p id="865e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">事实证明，在 Kaggle 竞赛中有几个使用这种建模方法的获奖预测模型。</p><p id="22db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">实现 Ensemble 有很大的好处。除了惊人的<em class="jt">高精度</em>之外，通过优化<em class="jt">并行计算</em>(例如 XBBoost，Light GBM)在更大的数据集上执行集成方法以减少训练时间和内存空间也是理想的。机器学习实践者最糟糕的噩梦之一是过度拟合，特别是对于包含噪声并且不遵循任何典型数据分布的真实世界数据集。在这种情况下，我们也可以使用系综<em class="jt">对抗高方差</em>(例如随机森林)。</p><p id="6e21" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们想要组合<em class="jt">不同的微调算法</em>以得出最佳可能的预测时，使用异质集成。投票和堆叠就是这种技术的例子。</p><h2 id="c3b6" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">硬投票</h2><p id="cc0c" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">此方法用于分类任务。它使用模式(选择多数类)组合多个估计量的预测，以选择最终结果。通过选择具有最大投票总数的类别，硬投票集成可以实现比仅使用单一算法的模型更好的预测。</p><p id="3762" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了有效，我们需要提供一组<em class="jt">奇数</em>(超过 3)和不同的估计量来产生他们自己的独立预测。</p><p id="b5b4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是一个使用硬投票集成的代码示例。请注意，算法列表是您的选择，只要它们是微调的、多样的和解决分类任务的。</p><figure class="la lb lc ld fd ij"><div class="bz dy l di"><div class="le lf l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">对分类任务使用硬投票的代码示例</figcaption></figure><h2 id="3cc2" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">软投票</h2><p id="2dbf" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">回归和分类任务都可以用于这项技术。为了执行软投票，我们对分类使用平均预测概率，对回归任务使用平均预测值。特征与硬投票相似，除了我们可以使用任何数量的估计器，只要有 2 个以上。我们也可以根据分类器对最终预测的重要性来分配权重，如硬投票。</p><figure class="la lb lc ld fd ij"><div class="bz dy l di"><div class="le lf l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">对回归任务使用软投票的代码示例</figcaption></figure><h2 id="2f2a" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">堆垛</h2><p id="5403" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">在尝试了硬投票和软投票但仍未达到预期结果后，这就是堆叠开始发挥作用的时候。该技术用于分类和回归任务。堆叠和其他两种异质集成方法的最大区别在于，除了基本学习器之外，我们还有一个额外的元分类器。</p><p id="6428" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，基础学习者将在数据集上进行训练和预测。然后，第二层上的元估计器将使用基础学习者层的预测作为下一步的新输入特征。请注意，元估计器将对采用新输入特征(X)的数据集进行训练和预测，新特征的数量等于基础学习者的数量，以及原始数据集的类标签(Y)。在这种情况下，我们不再使用位置估计来进行组合，而是使用可训练的学习器作为组合器本身。</p><p id="d963" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种方法的好处是元估计器可以有效地检查哪个基础估计器提供更好的预测，以及使用原始数据和新的输入特征直接参与最终预测。</p><figure class="la lb lc ld fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/97f1ccdc980bfdb615b3189234e8107c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*xxOq_gDKy8k1i0oyAHXGKg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:来自<a class="ae iu" href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/" rel="noopener ugc nofollow" target="_blank"> mlxtend </a>的打桩模型可视化</figcaption></figure><figure class="la lb lc ld fd ij"><div class="bz dy l di"><div class="le lf l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">对分类任务使用 Staking 方法的代码示例</figcaption></figure><p id="797e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">总的来说，当你已经建立了一个小的估计器集，单独训练和微调它们时，异质集成方法是一个很好的选择。请记住</p><blockquote class="ju jv jw"><p id="8f88" class="iv iw jt ix b iy iz ja jb jc jd je jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">”<em class="hi">只有当学习计划表现相当好时，投票才有意义。如果三个分类器中的两个做出严重错误的预测，我们就有麻烦了！”</em>(威滕，弗兰克，霍尔，&amp;帕尔，2016)。</p></blockquote><p id="155b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过应用简单和直观的集成技术，异构集成无疑提供了对整体性能的改进。在下一篇文章中，我们将深入探讨另一种由一些获奖算法组成的集成技术。</p><p id="c1b5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢阅读！</p><p id="58d4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不要害羞，如果你对我的更多帖子感兴趣，让我们联系:</p><p id="8879" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">中:【https://medium.com/@irenepham_45233】T4</p><p id="6294" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">insta gram:<a class="ae iu" href="https://www.instagram.com/funwithmachinelearning/" rel="noopener ugc nofollow" target="_blank">https://www.instagram.com/funwithmachinelearning/</a></p></div></div>    
</body>
</html>