<html>
<head>
<title>Adapting a real world data pipeline for my pet project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为我的宠物项目调整真实世界的数据管道</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/adapting-a-real-world-data-pipeline-for-my-pet-project-ca536d545409?source=collection_archive---------6-----------------------#2019-12-02">https://medium.com/analytics-vidhya/adapting-a-real-world-data-pipeline-for-my-pet-project-ca536d545409?source=collection_archive---------6-----------------------#2019-12-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/045f6735dd28148885250df9a45e09e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yXzzMGqet5oKrGfq"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">昆腾·德格拉夫在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="49a7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Gitlab有一个很酷的内部文化，默认情况下会把事情公开。这包括他们的整个<a class="ae iu" href="https://gitlab.com/gitlab-data/analytics" rel="noopener ugc nofollow" target="_blank">数据分析渠道</a>。我想熟悉他们的基础设施，所以我决定采用他们的工作，并根据我自己的目的进行调整。</p><p id="2520" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">前阵子，我写了一些代码从Fitbit、Trace(滑雪板跟踪应用程序)和一些降雪数据中提取数据，以便对去年的滑雪季节进行一些数据分析。我决定自动接收Fitbit和Trace数据，并使用DBT而不是熊猫来进行转换。</p><p id="f694" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(简化的)管道包括:</p><ul class=""><li id="bbae" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated"><a class="ae iu" href="https://gitlab.com/gitlab-data" rel="noopener ugc nofollow" target="_blank"> Gitlab </a>:所有代码、CI管道和图像库都托管在Gitlab中。</li><li id="02f4" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://cloud.google.com/kubernetes-engine/" rel="noopener ugc nofollow" target="_blank">谷歌Kubernetes引擎</a>:气流在GKE上运行，使用KubernetesPodOperators完成大部分任务。</li><li id="03c0" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://www.snowflake.com/" rel="noopener ugc nofollow" target="_blank">雪花</a>:数据仓库。存储从各种系统中提取的所有原始源数据以及由DBT创建的最终转换。</li><li id="9af6" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://airflow.apache.org/" rel="noopener ugc nofollow" target="_blank">气流</a>:管理与提取、编排、数据库管理和转换(通过DBT)相关的作业。</li><li id="60e2" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><a class="ae iu" href="https://www.getdbt.com/" rel="noopener ugc nofollow" target="_blank"> DBT </a>:处理数据转换、数据谱系的文档，以及测试来自源和转换的数据质量。</li></ul><h1 id="ae2e" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">设置回购</h1><p id="af8e" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">我创建了一个名为<code class="du lk ll lm ln b">snowboard-analysis</code>的Gitlab组，然后我将<a class="ae iu" href="https://gitlab.com/snowboard-analysis/analytics" rel="noopener ugc nofollow" target="_blank">分析</a>和<a class="ae iu" href="https://gitlab.com/snowboard-analysis/data-image" rel="noopener ugc nofollow" target="_blank">数据图像</a>回购分入其中。然后，我在<code class="du lk ll lm ln b">us-west1-a</code>中创建了一个名为<code class="du lk ll lm ln b">data-ops</code>的Kubernetes集群(参见“设置GKE”)，其中有来自<a class="ae iu" href="https://gitlab.com/groups/snowboard-analysis/-/clusters" rel="noopener ugc nofollow" target="_blank"> Gitlab group Kubernetes页面</a>的6个节点。我对名称和区域使用这些值，因为它们在一些地方是硬编码的，我不想改变它们。虽然5个节点就足够了，但是6个节点可以在同时运行多个任务时进行扩展。您也可以在GKE启用自动缩放来适应这一点。我还为CI 创建了一个<a class="ae iu" href="https://gitlab.com/groups/snowboard-analysis/-/settings/ci_cd" rel="noopener ugc nofollow" target="_blank"> group runner，以便能够在我向repo推送更改时运行CI/CD脚本。</a></p><p id="9f1c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在克隆了repos之后，我必须使用以下命令来更改硬编码的repo和docker注册表路径:</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="d00a" class="lw ki hi ln b fi lx ly l lz ma">find . -type f -exec sed -i "s/gitlab.com\/gitlab-data/gitlab.com\/snowboard-analysis/g" {} \;</span></pre><p id="47d4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我还删除了所有不必要的Dag、自定义提取、DBT模型和测试、雪花角色、用户和数据库等。我留下了一些看起来有用的东西，比如一些与DBT和雪花相关的Dag以及可以从Google sheets中提取数据的Sheetload。</p><p id="4ff2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随着所有不必要的东西消失，我为<a class="ae iu" href="https://gitlab.com/snowboard-analysis/analytics/blob/master/extract/fitbit_load.py" rel="noopener ugc nofollow" target="_blank"> Fitbit </a>和<a class="ae iu" href="https://gitlab.com/snowboard-analysis/analytics/blob/master/extract/trace_load.py" rel="noopener ugc nofollow" target="_blank"> Trace </a>创建了我的提取，以及每个的<a class="ae iu" href="https://gitlab.com/snowboard-analysis/analytics/tree/master/dags/extract" rel="noopener ugc nofollow" target="_blank">Dag</a>。我还必须为<a class="ae iu" href="https://gitlab.com/snowboard-analysis/data-image/tree/master/data_image" rel="noopener ugc nofollow" target="_blank">数据映像</a>更新<code class="du lk ll lm ln b">requirements.txt</code>文件，以便添加Trace和Fitbit包。</p><h1 id="86fa" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">建立GKE</h1><p id="1ab3" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">如果您还没有Google Cloud帐户，请创建一个。如果你创建一个新的，你会得到300美元的信用，这是免费试用这个项目的完美选择。创建好之后，在<a class="ae iu" href="https://console.cloud.google.com/apis/api/container.googleapis.com/overview" rel="noopener ugc nofollow" target="_blank">控制台</a>中启用GKE API。</p><p id="a37e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，转到<a class="ae iu" href="https://console.cloud.google.com/iam-admin/serviceaccounts" rel="noopener ugc nofollow" target="_blank">创建一个角色为<code class="du lk ll lm ln b">Kubernetes Engine Developer</code>的服务帐户</a>。当询问密钥时，创建一个并下载它。</p><p id="17c5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦集群设置好了(见上面通过Gitlab设置)，就该在本地设置kubernetes来使用我们创建的集群了。一个简单的方法是:</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="bdee" class="lw ki hi ln b fi lx ly l lz ma">gcloud auth activate-service-account --key-file &lt;/path/to/downloaded/key.json&gt;<br/>gcloud container clusters get-credentials data-ops --region us-west1-a --project &lt;name-of-your-project&gt;</span></pre><p id="3d68" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，为了上传必要的秘密，创建一个<code class="du lk ll lm ln b">secrets.yaml</code>文件(填入您自己的值):</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="7f2c" class="lw ki hi ln b fi lx ly l lz ma">apiVersion: v1<br/>kind: Secret<br/>metadata:<br/>  name: airflow<br/>type: Opaque<br/>stringData:<br/>    SNOWFLAKE_ACCOUNT: ""<br/>    SNOWFLAKE_PASSWORD: ""<br/>    SNOWFLAKE_USER: ""<br/>    SNOWFLAKE_LOAD_DATABASE: "RAW"<br/>    SNOWFLAKE_LOAD_PASSWORD: ""<br/>    SNOWFLAKE_LOAD_ROLE: "LOADER"<br/>    SNOWFLAKE_LOAD_USER: ""<br/>    SNOWFLAKE_LOAD_WAREHOUSE: "LOADING"<br/>    SNOWFLAKE_TRANSFORM_DATABASE: "ANALYTICS"<br/>    SNOWFLAKE_TRANSFORM_SCHEMA: "ANALYTICS"<br/>    SNOWFLAKE_TRANSFORM_PASSWORD: ""<br/>    SNOWFLAKE_TRANSFORM_ROLE: "TRANSFORMER"<br/>    SNOWFLAKE_TRANSFORM_USER: ""<br/>    SNOWFLAKE_TRANSFORM_WAREHOUSE: "TRANSFORMING_S"<br/>    SNOWFLAKE_PERMISSION_USER: ""<br/>    SNOWFLAKE_PERMISSION_PASSWORD: ""<br/>    SNOWFLAKE_PERMISSION_ROLE: "PERMISSION_BOT"<br/>    SNOWFLAKE_PERMISSION_DATABASE: "SNOWFLAKE"<br/>    SNOWFLAKE_PERMISSION_WAREHOUSE: "ADMIN"<br/>    TRACE_CLIENT_KEY: ""<br/>    TRACE_CLIENT_SECRET: ""<br/>    TRACE_OAUTH_TOKEN: ""<br/>    TRACE_OAUTH_TOKEN_SECRET: ""<br/>    FITBIT_CLIENT_ID: ""<br/>    FITBIT_CLIENT_SECRET: ""<br/>    FITBIT_ACCESS_TOKEN: ""<br/>    FITBIT_REFRESH_TOKEN: ""<br/>    AIRFLOW__CORE__SQL_ALCHEMY_CONN: ""<br/>    AIRFLOW__CORE__FERNET_KEY: ""<br/>    NAMESPACE: "default"<br/>    cloudsql-credentials: |-<br/>      {<br/>      "type": "service_account",<br/>      "project_id": "",<br/>      "private_key_id": "",<br/>      "private_key": "",<br/>      "client_email": "",<br/>      "client_id": "",<br/>      "auth_uri": "",<br/>      "token_uri": "": "",<br/>      "client_x509_cert_url": ""<br/>      }</span></pre><p id="2608" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我把所有东西都放在<code class="du lk ll lm ln b">stringData</code>而不是<code class="du lk ll lm ln b">data</code>里，只是为了更容易查看和编辑。然而，对于prod环境，您可能希望使用<code class="du lk ll lm ln b">data</code>并自己进行base 64编码。<code class="du lk ll lm ln b">cloudsql-credentials</code>是您从Google下载的服务帐户凭证。像这样嵌入json时，缩进很重要！</p><p id="02a4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要上传秘密，只需运行<code class="du lk ll lm ln b">kubectl apply -f secrets.yaml</code>。当在本地运行airflow进行测试时，名称空间被设置为<code class="du lk ll lm ln b">testing</code>。为了创建名称空间，运行<code class="du lk ll lm ln b">kubectl create namespace testing</code>并上传机密，运行<code class="du lk ll lm ln b">kubectl apply --namespace testing -f ./secrets.yaml</code>。</p><h1 id="968c" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">设置雪花</h1><p id="759d" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">前往https://www.snowflake.com/开始30天的免费试用。当您创建一个帐户时，您也将创建一个具有<code class="du lk ll lm ln b">SYSADMIN</code>、<code class="du lk ll lm ln b">ACCOUNTADMIN</code>和<code class="du lk ll lm ln b">SECURITYADMIN</code>角色的主用户。尽管Gitlab数据团队使用各种用户、角色、数据库和仓库，但我简化了其中的大部分，只使用一个用户和几个不同的角色。关于权限，我还有相当多的东西要学习，但是在使用了一段时间之后，我能够让事情运行起来。</p><p id="660e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">repo有一个<code class="du lk ll lm ln b"><a class="ae iu" href="https://gitlab.com/gitlab-data/analytics/blob/master/load/snowflake/roles.yml" rel="noopener ugc nofollow" target="_blank">roles.yaml</a></code>文件，描述每个角色、用户、仓库、数据库和模式以及它们的权限。他们有一个DAG，使用<a class="ae iu" href="https://meltano.com/docs/command-line-interface.html#permissions" rel="noopener ugc nofollow" target="_blank"> Meltano permissions </a>命令来生成设置权限所需的SQL。目前，这只能在干模式下运行，但我想目标是Meltano应该能够处理所有权限的设置。我不得不运行它，复制粘贴SQL，并做一些修改，让事情顺利进行。因为DAG使用<code class="du lk ll lm ln b">PERMISSION_BOT</code>角色来运行，所以有一点先有鸡还是先有蛋的问题，所以我必须首先创建它并给它适当的权限。如果使用单个用户和角色，我可以将事情简化得多，设置起来也会容易得多，但我想更好地了解Gitlab是如何做到这一点的。</p><h1 id="0b24" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">设置气流</h1><p id="743f" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">大部分气流设置在<a class="ae iu" href="https://gitlab.com/snowboard-analysis/data-image/tree/master/airflow_image" rel="noopener ugc nofollow" target="_blank">气流图像</a>中完成。我对配置做了一些更改，主要是为了减少活动连接的数量，我还对图像做了一些更改，使它们变得更小。我还必须更改部署清单中的硬编码项目名称，以匹配我的项目。</p><p id="344e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为简单起见，我只使用了SaaS Postgres数据库的气流数据。我用了https://www.elephantsql.com/的<a class="ae iu" href="https://www.elephantsql.com/" rel="noopener ugc nofollow" target="_blank">但是任何公共Postgres db都可以。需要记住的一点是，在任何给定的时间点，您都有10到20个活动连接(来自web服务器、调度器和工作器)。在那之后，我只是如上所述在我的Kubernetes秘密中添加了<code class="du lk ll lm ln b">AIRFLOW__CORE__SQL_ALCHEMY_CONN</code> <br/>和<code class="du lk ll lm ln b">AIRFLOW__CORE__FERNET_KEY</code>。</a></p><p id="f1fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要部署，只需:</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="71fa" class="lw ki hi ln b fi lx ly l lz ma">cd airflow_image/manifests<br/>apply -f ./ingress.yml<br/>apply -f ./persistent_volume.yaml<br/>apply -f ./services.yml<br/>apply -f ./deployment.yaml</span></pre><p id="3c6c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这将创建并启动所有必要的Kubernetes组件来部署气流，并公开web服务器。要在本地访问它，运行<code class="du lk ll lm ln b">kubectl port-forward deployment/airflow-deployment 1234:8080</code>，然后你可以在<code class="du lk ll lm ln b">localhost:1234</code>打开它。</p><h1 id="8117" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">建立DBT</h1><p id="13a1" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">DBT主要作为DAG运行。有一个docker映像，它捆绑了运行DBT命令所需的所有软件包，该映像用于在DAG中作为任务运行不同的命令。DBT项目结构是分析报告的子目录。我在这里做的主要事情是创建我自己的Fitbit和Trace dbt <a class="ae iu" href="https://gitlab.com/snowboard-analysis/analytics/tree/master/transform/snowflake-dbt/models" rel="noopener ugc nofollow" target="_blank">模型</a>，它们执行以下操作:</p><ul class=""><li id="ab70" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">创建标准的snake case列名</li><li id="17d2" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">将单位从追踪制转换为英制</li><li id="743e" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">每天汇总Fitbit睡眠数据</li><li id="cd5f" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">加入每天的所有Fitbit测量</li></ul><p id="c3ba" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">DBT的一个非常酷的功能是能够测试来自你的模型的数据。这有助于确保没有重复条目、空值等。我仍然没有玩这个，但以后会得到它。</p><p id="3557" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在生产中，DBT <code class="du lk ll lm ln b">profiles.yml</code>文件是使用来自secrets的env变量生成的。然而，对于本地测试，您应该创建自己的<code class="du lk ll lm ln b">~/.dbt/profiles.yml</code>文件，看起来像这样:</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="dc31" class="lw ki hi ln b fi lx ly l lz ma">gitlab-snowflake:<br/>  target: dev<br/>  outputs:<br/>    dev:<br/>      type: snowflake<br/>      threads: 8<br/>      account: <br/>      user: <br/>      password: <br/>      role: TRANSFORMER<br/>      database: ANALYTICS<br/>      warehouse: TRANSFORMING_S<br/>      schema: ANALYTICS</span></pre><h1 id="5ea5" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">测试气流Dag和DBT</h1><p id="bd38" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">主回购有一个<code class="du lk ll lm ln b">docker-compose.yml</code>和一个<code class="du lk ll lm ln b">Makefile</code>，这使得在本地测试Dag和DBT模型变得容易。然而，这仍然需要Kubernetes集群来运行气流任务，并且还需要<code class="du lk ll lm ln b">testing</code>名称空间中的秘密(见上文)。</p><p id="39e0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">每当您运行Airflow DAGs时，DAG定义将从您的本地repo中提取，但是Kubernetes pod将使用指定的<code class="du lk ll lm ln b">GIT_BRANCH</code>从注册表中提取最新的图像，并从托管repo中提取最新的代码，因此您对提取代码的更改将需要首先被推送。</p><p id="daac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要使用提供的docker-compose和Makefile，您需要设置几个环境变量:</p><pre class="lo lp lq lr fd ls ln lt lu aw lv bi"><span id="9c46" class="lw ki hi ln b fi lx ly l lz ma">export DBT_PROFILE_PATH=~/.dbt/profiles.yml<br/>export GOOGLE_APPLICATION_CREDENTIALS=~/.config/gcloud/legacy_credentials/&lt;service_account_email&gt;/adc.json<br/>export KUBECONFIG=~/.kube/config<br/>export GIT_BRANCH=&lt;your_test_branch&gt;</span></pre><p id="d876" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可以在这里阅读一些关于如何在本地<a class="ae iu" href="https://about.gitlab.com/handbook/business-ops/data-team/data-infrastructure/#in-merge-requests" rel="noopener ugc nofollow" target="_blank">测试的内容，但它主要涉及到使用<code class="du lk ll lm ln b">make init-airflow</code>在本地postgres容器上设置Airflow db，使用<code class="du lk ll lm ln b">make airflow</code>进入Airflow容器并运行Airflow命令，和/或使用<code class="du lk ll lm ln b">make dbt-image</code>进入DBT容器并运行dbt命令。</a></p><h1 id="7999" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">结论和经验教训</h1><p id="9908" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">现在，你可能想知道“为什么”？这不是矫枉过正吗？嗯，是的，是的，但我这样做的原因是为了熟悉Gitlab管道。这是一个很好的方式来了解Kubernetes，DBT，雪花和使用KubernetesPodOperator on Airflow运行任务。虽然我对其中的每一个工具都只是略知皮毛，但是我现在已经有了一个更好的基础来继续学习这些不同的工具。在我看来，通过做来学习比仅仅通过阅读要容易得多。</p><p id="910d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我从这个练习中得到了一些教训/收获:</p><ul class=""><li id="1ef6" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">Kubernetes和它上面的所有机器都按UTC时间运行。尽管这对于大多数服务器来说很常见，但是在Kubernetes中调试某些问题有点困难。我遇到的一个问题是<code class="du lk ll lm ln b">datetime.today().timestamp()</code>会根据机器设定的时区给出不同的结果。当我在本地测试我的提取时，它工作得很好，但是当在Kubernetes上运行时，它不能像预期的那样工作。在添加了一堆调试日志之后，我意识到这是因为Trace必须使用每个度假胜地的时区作为<code class="du lk ll lm ln b">date</code>的时区，所以当我过滤时，我需要告诉它时区在<code class="du lk ll lm ln b">MST</code>。</li><li id="771f" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">尽管在Pandas中以编程方式编写转换可能更容易，但使用DBT使分析人员更容易理解。它还允许您通过组合各种其他已定义的模型并使用宏来构建非常复杂的模型。DBT非常强大，我有很多关于最佳实践的东西要学习。有许多不同的方法来配置和构建您的模型，有不同的权衡。我希望看到一个使用dbt的Emacs模式，但是看起来我可能必须自己创建它。</li><li id="d529" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">如果您需要在KubernetesPodOperator上运行的Airflow任务中更改密码，请参见下面的文章:<a class="ae iu" rel="noopener" href="/@aiguofer/updating-secrets-from-a-kubernetes-pod-f3c7df51770d">从Kubernetes Pod更新密码</a>。</li></ul></div></div>    
</body>
</html>