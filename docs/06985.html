<html>
<head>
<title>Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-46a0f3cdecef?source=collection_archive---------17-----------------------#2020-06-09">https://medium.com/analytics-vidhya/logistic-regression-46a0f3cdecef?source=collection_archive---------17-----------------------#2020-06-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bee8a9270b5d8f4b0c62d187c1bd4076.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9BpxW7X7-qlEpArC.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://s3.ap-south-1.amazonaws.com/s3.studytonight.com/curious/uploads/pictures/1544244178-1.jpg" rel="noopener ugc nofollow" target="_blank">https://S3 . AP-south-1 . Amazon AWS . com/S3 . study tonight . com/curious/uploads/pictures/1544244178-1 . jpg</a></figcaption></figure><p id="175b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归类似于多元线性回归，只是结果是二元的。与其名称相反，逻辑回归是一种分类方法，在基于文本的分类中非常强大。它通过首先对逻辑函数执行回归来实现这一点，因此得名。由于它的快速计算速度和它的模型输出有助于新数据的快速评分，它是一种流行的方法。</p><p id="6000" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">问题是，我们如何从一个二元结果变量得到一个可以用线性方式建模的结果变量，然后再回到二元结果？就像线性回归模型一样，逻辑回归模型计算输入要素的加权和(加上一个偏差项)，但它不像线性回归模型那样直接输出结果，而是输出该结果的逻辑。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><p id="6699" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基本上，它首先像线性回归一样计算权重和截距，然后将结果传递给一个称为逻辑函数或sigmoid函数的函数，该函数将输出限制在0和1之间。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ka"><img src="../Images/f85bf41bed25a9f2a5f087a66751196f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p3B6NfsIRMGvXkK9.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://d2o2utebsixu4k.cloudfront.net/media/images/9a57ce9a-b10c-4ed0-9729-50d979af0a6f.jpg" rel="noopener ugc nofollow" target="_blank">https://d2o 2 utebsixu 4k . cloudfront . net/media/images/9 a 57 ce 9 a-b10c-4ed 0-9729-50d 979 af 0a 6 f . jpg</a></figcaption></figure><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es kf"><img src="../Images/e400ddab03e89dcad365970395dcb324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*A5aJEuk5SX-L-b8_2Kw7Bg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://cdn-images-1.medium.com/max/1500/1*A5aJEuk5SX-L-b8_2Kw7Bg.png" rel="noopener">https://cdn-images-1 . medium . com/max/1500/1 * A5aJEuk5SX-L-b8 _ 2 kw 7 BG . png</a></figcaption></figure><p id="c559" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输出(p)的范围在0和1之间；而如果<strong class="ix hj">Y = 1；如果p ≥ 0.5且为0；如果p ≤ 0.5 </strong></p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="d8a4" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">培训和成本函数</h1><p id="40dd" class="pw-post-body-paragraph iv iw hi ix b iy le ja jb jc lf je jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">现在我们知道了逻辑回归模型是如何估计概率并做出预测的。但是它是怎么训练出来的呢？训练模型的目的是估计正面实例(y = 1)的高概率和负面实例(y = 0)的低概率</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/3b816586bd52c638fd653c0f5e5f47e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*2R1-fqwBBcGhMJvxSEmOyg.png"/></div></figure><p id="8190" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面的等式是单个训练实例的成本函数。这个等式是有意义的，因为我们想要对每一个错误的预测进行严厉的惩罚。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/d8e6173b8aa813c51e61af7271ccb471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*I42ao6htTLybOsJm3GYCMw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片</figcaption></figure><p id="3742" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">整个训练集的成本函数就是所有训练实例的平均成本。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/50ceb12c7a3931af0d908c7259f597df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*6vlm4djs8tScQ_c5VJtkag.png"/></div></figure><p id="a873" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们的目标是找到最小化该成本函数的θ，但坏消息是，没有已知的<strong class="ix hj">闭合形式方程</strong>来计算它。但好消息是这个代价函数是<strong class="ix hj">凸</strong>的，所以<strong class="ix hj">梯度下降</strong>(或者其他任何优化算法)保证能找到全局最小值。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/58867c0a56a31784f9534bc97cda7404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*inBDmmDrSGzOhn49Z_tPcw.png"/></div></figure><p id="ca56" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于每个实例，它计算预测误差并将其乘以第j个特征值，然后计算所有训练实例的平均值。一旦你有了包含所有偏导数的梯度向量，你就可以在批量梯度下降算法中使用它。就这样:你现在知道如何训练一个逻辑回归模型。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="7578" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">词汇提醒</h1><p id="893d" class="pw-post-body-paragraph iv iw hi ix b iy le ja jb jc lf je jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">二元的:只涉及两个选项之间的选择或条件的。</p><p id="f3d3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">封闭型方程</strong>:如果一个方程从一个给定的普遍接受的集合中通过函数和数学运算解决了一个给定的问题，那么这个方程就是一个封闭型解</p><p id="cb68" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">凸函数:</strong>考虑一个函数y=f(x)，假设它在区间[a，b]上连续。如果对于[a，b]中的任意两点x1和x2，以下不等式成立，则函数y=f(x)称为向下凸(或向上凹):</p><blockquote class="ln lo lp"><p id="91df" class="iv iw lq ix b iy iz ja jb jc jd je jf lr jh ji jj ls jl jm jn lt jp jq jr js hb bi translated"><strong class="ix hj">f((x1+x2)/2)≤(f(x1)+f(x2))/2</strong></p></blockquote><p id="928e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">代价函数:</strong>是<strong class="ix hj">针对给定数据，衡量机器学习模型</strong>性能的函数。成本函数量化了预测值和期望值之间的误差，并且<strong class="ix hj">以单个实数的形式呈现。</strong></p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><p id="959a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可以查看我的github链接，了解真实世界数据集上的逻辑回归实现——<a class="ae iu" href="https://github.com/akshayakn13/Logistic-Regression" rel="noopener ugc nofollow" target="_blank">https://github.com/akshayakn13/Logistic-Regression</a></p><p id="7b1b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">查看我关于机器学习的其他文章:</p><p id="d598" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://ak95ml.blogspot.com/2020/06/linear-regression.html" rel="noopener ugc nofollow" target="_blank">从零开始的线性回归。</a></p><p id="ac84" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.blogger.com/blog/post/edit/1004924421828631592/5881650886724527591#" rel="noopener ugc nofollow" target="_blank">美汤刮痧</a></p><p id="f37c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.blogger.com/blog/post/edit/1004924421828631592/5881650886724527591#" rel="noopener ugc nofollow" target="_blank">我如何开始我作为机器学习爱好者的旅程</a></p><h1 id="0d93" class="kg kh hi bd ki kj lu kl km kn lv kp kq kr lw kt ku kv lx kx ky kz ly lb lc ld bi translated">了解更多关于逻辑回归的外部资源</h1><blockquote class="ln lo lp"><p id="a624" class="iv iw lq ix b iy iz ja jb jc jd je jf lr jh ji jj ls jl jm jn lt jp jq jr js hb bi translated">使用Scikit-Learn和TensorFlow- Aurélien Géron进行机器学习实践。</p><p id="fbb9" class="iv iw lq ix b iy iz ja jb jc jd je jf lr jh ji jj ls jl jm jn lt jp jq jr js hb bi translated"><em class="hi">数据科学家实用统计学-Peter Bruce和Andrew Bruce </em></p><p id="1ade" class="iv iw lq ix b iy iz ja jb jc jd je jf lr jh ji jj ls jl jm jn lt jp jq jr js hb bi translated">用Python构建机器学习系统——威利·里歇特·路易斯·佩德罗·科埃略</p></blockquote></div></div>    
</body>
</html>