<html>
<head>
<title>Contextual word embeddings — Part1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语境词嵌入—第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/contextual-word-embeddings-part1-20d84787c65?source=collection_archive---------4-----------------------#2020-05-10">https://medium.com/analytics-vidhya/contextual-word-embeddings-part1-20d84787c65?source=collection_archive---------4-----------------------#2020-05-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f434" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这个故事包含三个部分:对文字表述的思考，前ELMO和ELMO，以及乌尔姆菲特和以后。这个故事是对<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">` Stanford cs 224n:NLP with Deep Learning，class 13` </a>的一个简短总结。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/05aa2941a33be9f5b15612c5f908d283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Koo4uwJVFp1o8PhzFkoYKw.png"/></div></div></figure><p id="b278" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这个故事包含三个部分:对文字表述的思考，前ELMO和ELMO，以及乌尔姆菲特和以后。这个故事是<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank"> `Stanford CS224N:深度学习的NLP，13班`</a>的总结。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="11af" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">1.单词的表示</h1><p id="71f8" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated">也许你已经知道，我们可以训练一个单词向量来获得单词的一种表示，比如word2vec、GloVe和FastText嵌入。我们可以从随机单词向量开始，在我们感兴趣的任务上训练它们。<strong class="jm hj">但在大多数情况下，使用预先训练的单词向量是有帮助的，因为你可以从更多的数据中训练它们，它们可以知道更多的词汇。他们用生僻字帮忙。</strong></p><p id="b394" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">至于未知单词的单词向量，历史上最常见的是你已经得到了四个监督训练数据，你定义了一个词汇，这个词汇可能是在你的监督训练数据中出现5次或更多次的单词。你把其他一切都当成UNK。你也可以通过UNK矢量进行训练。但这有一些问题，<em class="lk">你无法区分不同的UNK词，无论是身份还是意义</em>。<strong class="jm hj">对于问答系统来说，这往往是个问题，在问答系统中，匹配单词身份是很重要的，甚至对于你的单词向量词汇表之外的单词也是如此。<em class="lk">一个解决办法是“字由字构成”。</em> </strong>你可以<strong class="jm hj">使用字符表示来学习其他单词的单词向量，</strong>虽然这种方法增加了一些复杂性。有几个小技巧。第一个是，<em class="lk">当你在测试时遇到新单词，可能是你的无监督单词，预训练的单词嵌入比你的实际系统有更大的词汇量。</em> <strong class="jm hj">因此，任何时候你遇到一个不在你的词汇库中但在<em class="lk">预训练单词嵌入</em>中的单词，只需获取该单词的单词向量并开始使用它。</strong> <em class="lk"> </em>还有第二个可能的提示，如果你看到一个仍然是未知单词的东西，<strong class="jm hj">而不是把它当作UNK，你只是当场分配它，一个随机的单词向量，所以这有一个效果，每个单词都有一个唯一的身份</strong>，这意味着如果你在问题和一个潜在的答案中看到相同的单词，它们会以一种精确的方式完美地匹配在一起，这是你不能用UNK匹配得到的，这些可以是一种有用的想法。</p><p id="0ba9" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">回到单词表示，对于word2vec、GloVe和FastText嵌入，<strong class="jm hj">有两个问题。<em class="lk">首先，无论一个单词出现在什么样的上下文中，它总是有相同的表示方式</em> </strong>，我们可能想要非常细粒度的词义消歧<strong class="jm hj"> <em class="lk">。其次，我们对一个词只有一种表示，但词有不同的方面，包括语义、句法行为和语域/内涵</em> </strong>。例如，在某种意义上，` arrival '和` arrival '的语义几乎相同，但它们是不同的词类:` arrival '是动词，` arrival '是名词，因此它们可以出现在完全不同的地方。您可能希望在依赖解析器中对它们做不同的事情。所以我们可能也想在这个基础上区分单词。这就是我们想要用新的上下文单词嵌入来解决的问题。</p><p id="7dde" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我们一直都有解决这些问题的方法吗？在神经语言模型(NLM)中，我们立即通过LSTM层粘贴单词向量。<strong class="jm hj"> <em class="lk">那些LSTM层被训练来预测下一个单词。但是这些语言模型在每一个位置上都产生了上下文特定的单词表示！</em> </strong></p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ll"><img src="../Images/857838e24858d8f281e31b7d591a5331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*XwfUEjtHgSQDmlgAJ_okyA.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">NLM模型结构示例。<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">来源:课程幻灯片</a></figcaption></figure><p id="4bb0" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">左边显示了一个NLM的例子。在底部，你将单词向量(<code class="du lq lr ls lt b">my favorite season is spring</code>)输入模型，然后你穿过一个或多个循环层(类似于LSTM层)，它正在计算位于每个单词上方的这些表示。那些隐藏状态的角色有点矛盾。它们用于预测，也用于下一个隐藏状态和输出状态等等。但是在很多方面你可以认为，这些表征实际上是一个单词在上下文中的表征。如果你想一想问答系统发生了什么，那就是它们是如何被使用的。我们对LSTM的文章中的一个问题前后对照，然后这些单词很好地代表了一个单词的意思和上下文。所以看起来我们似乎已经发明了一种方法来表达特定语境的单词。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="921c" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">2.前ELMO和ELMO</h1><h2 id="d1f9" class="lu ko hi bd kp lv lw lx kt ly lz ma kx jt mb mc kz jx md me lb kb mf mg ld mh bi translated">2.1 ELMO之前</h2><p id="0308" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated">做上下文表示的第一件事是马特·皮特斯在2017年写的一篇<a class="ae ix" href="https://arxiv.org/pdf/1705.00108.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>，这是这些上下文敏感单词嵌入的现代版本的一种前身。他和其他作者一起提出了一篇名为<strong class="jm hj"> <em class="lk"> TagLM </em> </strong>的论文。</p><p id="4ba6" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我们希望在命名实体识别等任务上做得更好。我们想做的是了解一个词在上下文中的意思。但标准来说，如果我们正在进行命名实体识别，我们只是在50万字的监督数据上训练它。对于学习单词和上下文的意思来说，这并不是一个很好的信息来源。那么我们为什么不采用半监督的方法呢？</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mi"><img src="../Images/cdcb80a5cd38eac4ac9b5f1d6bf5aa21.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*uy6VNubU3ury-OP0h8y7ew.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">TagLM结构。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="4a01" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj">我们从大量未标记的数据开始，从这些未标记的数据中，我们可以训练传统的单词嵌入模型，如Word2Vec，但我们也可以同时训练神经语言模型，如双LSTM语言模型<em class="lk">。</em> </strong>到目前为止，当我们想要在顶部学习我们的监督词性标记器时，我们要做的是<em class="lk">对于输入单词</em> <code class="du lq lr ls lt b"><em class="lk">New York is located</em></code> <em class="lk">，我们不仅可以使用与上下文无关的单词嵌入，还可以使用我们训练的递归语言模型，并在这个导入中运行它，然后我们将在我们的双LSTM语言模型中生成隐藏状态，我们还可以将这些作为特征输入到我们的序列标记模型中，这些特征将使它更好地工作</em>。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mj"><img src="../Images/fbde864488e3dfe5a4a4de335f35d9b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lT_EZ6WNdqsrzvX4jclVug.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">序列标签。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="fb9e" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这是第二张图，更详细地展示了这一点。<strong class="jm hj">对于LM，应用双LSTM对大量无监督数据进行训练，以获得上下文相关的单词嵌入</strong>。右图显示了预训练的bi-LM模型。在右下方，句子<code class="du lq lr ls lt b">New York is located</code>被分别馈入两个LM模型:前向LM和后向LM。获得每个单词的<em class="lk">隐藏状态表示。来自后向LM和前向LM的隐藏状态表示被连接，这给出了一组连接的LM嵌入。这些上下文相关的嵌入作为特征被输入到命名实体识别器(NER)中。</em>在预训练模型的训练期间，该模型在给定当前输入单词的情况下预测下一个单词。<em class="lk">NER(左侧)在监督下接受训练。</em>左下方显示的是同一句话<code class="du lq lr ls lt b">New York is located</code>正在接受训练。这个句子可以用word2vec风格的令牌嵌入和带有字符级CNN和/或rnn的字符级表示来表示。然后，这两个表示被连接起来，并被送入双LSTM层<em class="lk">。该双LSTM层的输出与LM嵌入的每个输出(来自预训练的bi-LM)连接，因此这些事物中的每一个都成为一对状态</em>。这些线对然后被送入第二层双LSTM。在顶部应用Softmax分类，以生成像位置的开始/结束这样的标签。</p><p id="bd20" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj">在某种意义上说它是预训练的主要原因是这个(左预训练bi-LM)首先被训练。</strong> <em class="lk">但人们认为这是预训练的主要原因是在你训练完这个</em><strong class="jm hj"><em class="lk"/>之后，它就被冻结了</strong>。<em class="lk">这就是你可以用参数来运行的东西，它会给你一个向量，这是你每个位置的上下文单词表示，然后它将在这个系统中使用。</em> <strong class="jm hj"> <em class="lk">当你在训练系统时，没有梯度流回这个改变和更新它的神经语言模型；只是固定了</em> </strong>。这就是人们谈论预训练时的感觉。</p><p id="a4f7" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> TagLM命名为“Pre-ELMO”，</strong>在“十亿字基准”的8亿训练词上进行训练。<a class="ae ix" href="https://arxiv.org/pdf/1705.00108.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lk">本文</em> </a> <em class="lk">表明，对于语言模型(LM)观测，一个在监督数据上训练的LM并无帮助；有一个双向LM有一点帮助；拥有一个巨大的LM设计有助于一个较小的模型。</em> <a class="ae ix" href="https://arxiv.org/pdf/1705.00108.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lk">本文</em> </a> <em class="lk">还表明，对于特定任务的BiLSTM观测值，仅仅使用LM嵌入进行预测并没有多大的作用</em>。请查看这篇<a class="ae ix" href="https://arxiv.org/pdf/1705.00108.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中的一些细节。有些外卖是<strong class="jm hj"> 1)。有一个双向的语言模型是有用的，而不是单向的；</strong> 2) <strong class="jm hj">有一个大的语言模型是有用的，可以获得很多收益，</strong>你需要在更多的数据上训练这个语言模型。如果你只是在你监督的训练数据上训练它，那就没用了。</p><p id="9d66" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">那段时间出现了一篇类似的<a class="ae ix" href="https://arxiv.org/pdf/1708.00107.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>`<strong class="jm hj"><em class="lk">Cove</em></strong>`。<em class="lk">它还具有使用经过训练的序列模型来为其他NLP模型提供上下文的思想</em>。<em class="lk">据说机器翻译是为了保留意义，所以这也许是个好目标？</em> <strong class="jm hj">它使用2层双LSTM，即seq2seq(注意)NMT系统的编码器作为上下文提供者。</strong>结果表明，CoVe向量在各种任务上确实优于GloVe向量。但是结果没有更简单的NLM训练强，也许NMT只是比LM更难。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="1b4b" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">2.前ELMO和ELMO</h1><h2 id="004a" class="lu ko hi bd kp lv lw lx kt ly lz ma kx jt mb mc kz jx md me lb kb mf mg ld mh bi translated">2.2 ELMO</h2><p id="7396" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated">ELMO是来自语言模型的嵌入，是深度语境化的词汇表征。它是<strong class="jm hj">单词标记向量或上下文单词向量</strong>的突破版本，它使用长上下文而不是上下文窗口来学习单词标记向量。它学习深度双NLM，并在预测中使用它的所有层。</p><p id="3ce5" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">从某种意义上来说，【TagLM和Elmo之间的差异有点小，这是一种细节上的差异。大致来说，他们又做了完全一样的事情，但是稍微好一点。他们做双向语言模型有点不同，实际上他们的关注点之一是尝试并提出一个紧凑的语言模型，即使他们没有世界上最强大的计算机硬件，人们也可以很容易地在其他任务中使用。因此，他们决定完全放弃单词表示，只使用字符CNN来构建单词表示，因为这样可以减少必须存储的参数数量，减少必须使用的大型矩阵。他们将隐藏维度扩展到4096维，然后用一种前馈投影层将其投影到512维，这是一种相当常见的技术，可以再次降低模型的参数化。因此，它们有许多当前方向的参数，但你需要更小的矩阵来包含下一级的输入。<strong class="jm hj">在各层之间，他们现在使用剩余连接，并做一些参数绑定</strong>。因此，ELMO培养了一个双向LM，目标是高性能但不过大的LM。为此，它使用了两个BiLSTM层。它还使用字符CNN来构建初始表示，该表示包含2048个字符n-gram过滤器和2个公路层以及512个dim投影。它使用4096个dim隐藏/单元LSTM状态，512个dim投影到下一个输入。它还使用剩余连接。它将令牌输入和输出(softmax)的参数联系起来，并将这些参数在前向和后向LMs之间联系起来。ELMo倾向于任务特定的biLM表示组合。这是一项创新，改进了仅使用LSTM堆栈顶层的情况。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mk"><img src="../Images/14034ff584d15163c3dd95d5b3fc77b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N2J9Hlcjy29EejQqoI7Kiw.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">ELMO方程式。γ^{task}标度ELMo对任务的总体有用性，s^{task}是softmax归一化混合模型权重。本质上，我们在语言模型中的特定位置j计算单词k的隐藏状态，(h)，然后我们将学习该级别(s)的权重，并将它们相加(σ)。Gamma是特定任务的全局比例因子。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="614c" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj">因此，在TagLM中，从预训练的LM输入到主模型中的只是NLM堆栈的顶层</strong>，这在当时完全是标准的礼节，你可能有3层神经语言模型，你认为在顶层，你可以真正捕捉句子的意思，而在底层，你可以处理它。ELMo假设<strong class="jm hj"> <em class="lk">实际上使用神经语言模型的biLSTM的所有层而不仅仅是顶层是有用的。</em> </strong>原来左复方程是这么回事。这个等式是每个位置的隐藏层的加权平均值，这将作为我们的基本表示。他们发现不同的任务可能偏好不同的层。<em class="lk">学习特定任务的全局比例因子Gamma。这使得他们能够控制上下文单词嵌入对于某些任务可能真的有用，而对于其他任务，它们可能不那么有用。所以你只是在学习一种特定的，对整个任务有用的东西。</em> <strong class="jm hj"> ELMO认为，一个单词可能有更多的语法意义和语义意义，可能这些可以在你的神经语言模型的不同层次上表现出来，然后对于不同的任务，你可以有区别地加权它们</strong>。</p><p id="64e2" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">将ELMO应用到任务中怎么样？我们首先运行Bi-LMs来获得每个单词的表示。然后，我们让(无论什么)最终任务模型使用这些表示:1)为了监督模型的目的，我们冻结ELMO权重的权重；2)我们将ELMO权重连接到特定于任务的模型中，其中细节取决于任务。您可能希望将它连接到中间层，就像TagLM所做的那样，这可能很好。但是在产生输出时使用这些ELMo表示可能也是有用的。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ml"><img src="../Images/982c0c52fee265b46d6f3619ea4e7935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b4uz3ccv8x8CXnd4-kM71Q.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">序列标签中的ELMo。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="cab8" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">序列标记器中的ELMo行为与TagLM相似。您已经计算了每个位置的ELMo表示，作为加权平均值，然后您将它连接到受监控系统的隐藏状态，并生成您的输出。</p><p id="99fa" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">人们对ELMo感到兴奋的是，这些ELMo表示可以应用于几乎任何NLP任务，并且非常有用，可以获得很好的收益。 </p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mm"><img src="../Images/f7b044d1b755f3030c1a6128665268e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H5rZvOkkcO8EuxS0Ezw6fw.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">ELMo适用于不同的任务</figcaption></figure><p id="8ba8" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">左图显示了一大堆截然不同的任务:小组问答、自然语言推理、语义角色标注、共同参照、NER和情感分析。各种不同的NLP任务，他们有一个先进的系统。基线通常有点类似于先前的技术状态，但通常实际上比当前的技术状态差一点。<a class="ae ix" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank">结果</a>显示，<em class="lk">在每种情况下，将ELMo向量添加到基线模型中间的隐藏表示中有助于您获得更好的结果，大约有3%的增益。</em> <strong class="jm hj"> <em class="lk">值得注意的是，两个biLSTM层具有不同的用途/含义。较低层更适合于较低层的语法，</em> </strong>，这对于诸如词性标注、句法依赖和NER之类的任务很有用。<strong class="jm hj"> <em class="lk">虽然更高层对于更高标签的语义更好，但是</em> </strong>对于情感分类、语义角色标注、问题回答和SNLI等任务是有用的。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="0739" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">3.乌尔姆菲特及以后</h1><p id="810e" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated">大约在同一时间，在2018年，另一个作品问世，就是这个作品，<strong class="jm hj"><em class="lk"/></strong><a class="ae ix" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank"><strong class="jm hj"><em class="lk">通用语言模型文本分类微调，或ULMfit。</em> </strong> </a>本质上，这与从学习一个大的LM模型到将这个LM信息应用到一个特定的NLP任务中的迁移学习具有相同的一般思想。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mn"><img src="../Images/89d3a3a48d3f3a266edb2ca9993bfe58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpe7FrthddXIO9M4h9GNWg.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">ULMFit结构。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="b201" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">ULMFit首先使用biLM在大型通用领域语料库上训练LM，然后在目标任务数据上调整LM，最后在目标任务上作为分类器进行调整。更具体地说，<a class="ae ix" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">作者</a>使用具有3个隐藏层的更深NLM来训练一个大的无监督语料库。然后你在你感兴趣的实际领域微调你的神经语言模型，这是他们做的额外阶段(b)。最后，介绍分类目标。本文将分类器应用于文本分类。他们做了一些不同的事情，这在某种意义上预示了后来在《变形金刚》中的工作。<strong class="jm hj"> <em class="lk">因此，他们并不仅仅是将这些功能输入到一个完全不同的网络中，而是继续使用同一个网络，但他们在顶部引入了一个不同的目标</em> </strong>。因此，你可以用这个网络做的一件事是用它来预测下一个单词作为语言模型，所以在这一点上，他们冻结了顶部softmax的参数(这就是为什么它用黑色显示)。但相反，他们可以坚持不同的预测单元，在那里它为特定的任务预测东西。因此，它可能在文本分类任务或类似的任务中预测积极或消极的情绪。<strong class="jm hj"> <em class="lk">所以在他们的模型中，他们在某种程度上重用了同一个网络，但坚持在那个网络的顶部，一个不同的层，来完成新的分类任务</em> </strong>。</p><p id="de3d" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">ULMfit有一些重点。首先，它使用了一个合理大小的“1 GPU”语言模型，而不是真正的大模型。第二，在LM微调中有很多注意事项，例如不同的每层学习速率和倾斜三角形学习速率(STLR)时间表。在学习分类器时，它逐渐解冻层和STLR。对于分类，它使用串联(= [h_{T}，maxpool(h)，meanpool(h)]。请将<a class="ae ix" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">纸</a>取出。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mo"><img src="../Images/d60e456b2d1c88ef317f0febb87ee6d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FZO__5FyBVsd9YrPCfXYQw.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">ULMfit迁移学习。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="d712" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> <em class="lk">结果显示，如果你能在大量数据上训练这只NLM，这意味着你将能够在你监督的任务上做得很好，即使是在非常少的数据上训练</em>。</strong>这是错误率，低就是好。X轴显示了训练样本的数量，这是在对数尺度上完成的。所以蓝线是如果你只是在监督数据上从头开始训练一个文本分类器，所以你需要很多数据才能做得很好。但是如果你从一个预先训练好的语言模型中利用这种迁移学习，你可以用很少的训练例子就能做得很好。<strong class="jm hj"> <em class="lk">本质上，一个数量级，更少的训练例子会给你同样多的表现</em> </strong>。<em class="lk">这两条线(橙色和绿色线)之间的差异对应于它们中间的额外阶段，也就是说，无论您是否正在对目标领域进行这种额外的微调，这都是您流程的一部分，他们发现这非常有帮助</em>。</p><p id="d0e5" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这是第一部分，请查看第二部分以便进一步阅读。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="9e6a" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">参考:</h1><ol class=""><li id="f0d2" class="mp mq hi jm b jn lf jq lg jt mr jx ms kb mt kf mu mv mw mx bi translated"><a class="ae ix" href="https://www.youtube.com/watch?v=S-CspeZ8FHc&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=13" rel="noopener ugc nofollow" target="_blank">斯坦福CS224N: NLP与深度学习| Winter 2019 |讲座13 —上下文单词嵌入</a></li><li id="4c73" class="mp mq hi jm b jn my jq mz jt na jx nb kb nc kf mu mv mw mx bi translated"><a class="ae ix" href="https://arxiv.org/pdf/1705.00108.pdf" rel="noopener ugc nofollow" target="_blank">使用双向语言模型的半监督序列标记</a></li><li id="26e8" class="mp mq hi jm b jn my jq mz jt na jx nb kb nc kf mu mv mw mx bi translated"><a class="ae ix" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank">深度语境化的词汇表征</a></li><li id="3642" class="mp mq hi jm b jn my jq mz jt na jx nb kb nc kf mu mv mw mx bi translated"><a class="ae ix" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">文本分类通用语言模型微调</a></li></ol></div></div>    
</body>
</html>