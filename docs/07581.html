<html>
<head>
<title>The intuition behind Latent Dirichlet Allocation (LDA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潜在狄利克雷分配背后的直觉</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-intuition-behind-latent-dirichlet-allocation-lda-fb1e1fb01543?source=collection_archive---------10-----------------------#2020-06-30">https://medium.com/analytics-vidhya/the-intuition-behind-latent-dirichlet-allocation-lda-fb1e1fb01543?source=collection_archive---------10-----------------------#2020-06-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="2310" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">知道如何理解LDA的假设</h2></div><p id="76ea" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我知道LDA用于主题建模，也知道我必须如何接收文本数据以及我可以预期的结果。但我不明白的是，哪一个数学部分负责寻找潜在或未知的主题，并且非常有兴趣了解达成模型以拟合数据的逻辑，发现潜在主题涉及的其他逻辑步骤，以及用于优化解决方案的算法。</p><p id="ba77" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">关于这个话题，我已知的未知有很多，所以我决定写一篇文章来澄清其中的一些。在浏览了研究论文、视频和其他信息来源后，我对主题是如何被发现的有了理解，我愿意与像你一样的其他机器学习爱好者分享。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es jt"><img src="../Images/529be62b0192d704215c6ed496231d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/0*GNC9jUdqjjMf_rCb"/></div></figure><p id="a0ad" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们进行以下技术测试时，请耐心等待；会值得的！</p><p id="6349" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的问题是:</p><p id="ce49" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">什么是狄利克雷，为什么在LDA中使用狄利克雷？</p><p id="a2f9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型中的不同变量代表什么？</p><p id="62ea" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">什么是狄利克雷？</strong></p><p id="d9b0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">原来狄利克雷分布是贝塔分布扩展到多维的推广。什么是贝塔分布？它是随机变量的单变量分布，范围为0到1，参数为α和β。β也是概率为p的二项分布的共轭先验，那么p的后验分布也是β分布，参数为alpha_dash=alpha+成功次数，beta_dash= beta+失败次数。同样，当我们假设一个狄利克雷分布作为先验时，在LDA的情况下，后验分布也将是狄利克雷分布。</p><p id="ea30" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">为什么在LDA中使用Dirichlet？</strong></p><p id="8ab7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">多项式分布是二项式分布的推广，它模拟n次试验的结果，其中每次试验的结果都有一个分类分布。并且它适合于LDA为不同文档的结果建模的当前环境，其中每个文档的结果具有不同的主题。在LDA中，我们希望每个文档的主题混合比例从某种分布中提取，最好是从概率分布中提取，这样它的总和就是1。所以在目前的情况下，我们想要概率的概率。因此，我们希望对多项式进行先验分布。我们选择狄利克雷，因为它是多项式分布的共轭先验。如果我们的似然是一个具有狄利克雷先验的多项式，那么后验也是如上所述的狄利克雷。</p><p id="d607" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">算法问题</strong></p><p id="7777" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们主要对解决第一个问题感兴趣:</p><p id="78a1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1.生成隐藏的潜在主题结构的术语的分布或主题是什么？</p><p id="f31b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，我们还可能有兴趣回答以下问题:</p><p id="72f8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.对于每个主题:在这些假设下生成文档的主题是什么？</p><p id="74ea" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.对于每个文档:与该文档相关联的主题的分布情况如何？</p><p id="75cd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4.对于每个单词:哪个主题产生了每个单词？</p><p id="eb2c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">模型和假设</strong></p><p id="d980" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在回答我之前没有回答的第二个问题之前，理解模型和整个概念背后的假设是相当重要的:模型中的不同变量代表什么？</p><p id="d5a7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们先来看看这个模型:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kb"><img src="../Images/e84cda3dfebb18f1d9950552682ccc49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*Ri5Pf7R2k7vENohO.png"/></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kb"><img src="../Images/8e8fab94ae5f98ca5452cb9a894303d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*2ntLu511cEmfh_It.png"/></div></figure><p id="44b9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">LDA的概念很吸引人，因为我们开始计算后验概率，以找到隐藏的主题结构，但在一种反直觉的风格中，我们认为如果这些隐藏的变量被观察到，我们将如何建模和处理。我们假设一个生成概率模型来拟合数据，并尝试使用给定观察值的后验推断来推断隐藏变量的条件分布。然后，我们将会发现隐藏的主题结构的文件，我们没有获得。</p><p id="0a4a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们首先假设一个文档展示了多个主题，并且每个文档都是一个语料库范围内的主题的混合。我们还隐含地假设文档中的单词不遵循任何顺序。现实中并不是这样，因为文档没有意义。但是它对我们的目标起作用。我们可以这样想，比如说，如果我们有几个随机的词:摇摆，尾巴，忠诚，朋友，动物，家养的，那么我们就能算出这些词代表了狗的整体。</p><p id="61f3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，我们遵循一个生成过程，并理解它不必精确地描述文档是如何产生的，因为它只需要有意义来实现我们的目标，即推断出后验。</p><p id="4283" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其他值得注意的假设是:</p><ul class=""><li id="d3a9" class="kc kd hi iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">每个文档都是通过选择一些主题比例来实现的</li><li id="91ba" class="kc kd hi iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">每个单词都来自其中一个主题</li><li id="e8f0" class="kc kd hi iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">每一个题目都包含了每一个单词的概率。例如，单词tiger在野生动物和高尔夫话题中出现的概率都很高。同时，这个词在股票交易话题中出现的概率可能很低。</li></ul><p id="9752" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">模型中的不同变量代表什么？</strong></p><p id="cff0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了理解上一节中提到的不同变量和不同假设，让我们详细讨论观察到的和隐藏的随机变量的联合分布的以下公式:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kb"><img src="../Images/eb7198c07e5f9afc07a04d4bba4b711f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*MvwBRK7Hu50-MuBV.png"/></div></figure><p id="ffc5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用黄色标记的项目来自狄利克雷分布。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kb"><img src="../Images/896f413f7800c10198cf8a152224d3c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*euGUpt6IBQZKITqc.png"/></div></figure><p id="2f88" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> LDA的后验推断</strong></p><p id="5903" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它被用于许多应用中，如信息检索、协作过滤、文档相似性和可视化跨学科文档。在这篇文章中，我们将看到如何变分贝叶斯(将被称为VB)方法是用来推断棘手的后验分布。理解VB方法是相当重要的，因为它推断后验分布部分，也携带关于隐藏变量的信息增益。</p><p id="a56b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有两类推理方法:精确推理和近似推理算法。精确推理算法包括强力推理，近似推理算法包括循环信念传播。循环信念传播的关键思想是在循环图或非树上引入消息传递。变分推理就是这样一种近似推理算法，我们将看到用VB方法来实现变分推理。</p><p id="28f7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">VB的思想如下:</p><ul class=""><li id="c652" class="kc kd hi iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">我们在潜在变量上选择一个分布族，它有自己的一组参数</li><li id="ab1a" class="kc kd hi iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">然后选择一个最接近后验分布的近似分布(我们称之为近似分布q)</li><li id="dbb3" class="kc kd hi iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">接下来，找到q的参数设置</li><li id="4bab" class="kc kd hi iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">我们在q的密度上优化算法以找到最佳近似</li><li id="046b" class="kc kd hi iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">最后，我们可以用拟合参数的近似分布q代替后验分布</li></ul><p id="9430" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们已经打开了宝盒，我们可以使用q来研究隐藏变量的后验分布，这就是我们如何能够在主题建模用例中找到隐藏的主题。需要注意的一点是，该过程并不提供主题名称，而是将相关的单词和它们相关的概率值组合在一起。通过对相关术语进行分组，揭示了隐藏的主题结构。</p><p id="697a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">推理问题</strong></p><p id="92aa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">计算后验分布被称为推理问题，它有两个方面:</p><p id="2aaa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1.我们必须找出一种方法来计算边际可能性，因为它将进一步包括在计算我们的主要目标，后验分布。</p><p id="d9f3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.来找出棘手的后验分布。</p><p id="5895" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了用VB方法实现推理，了解什么是Kullback-Leibler散度(将称为KLD)是有意义的。为什么？</p><p id="6d36" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">好吧，回想一下，我们手头有两个分布:</p><ul class=""><li id="3a7a" class="kc kd hi iz b ja jb jd je jg ke jk kf jo kg js kh ki kj kk bi translated">通过将LDA模型拟合到我们的数据，我们假设的生成概率分布</li><li id="fadd" class="kc kd hi iz b ja kl jd km jg kn jk ko jo kp js kh ki kj kk bi translated">我们期望从LDA得到的真实后验分布的近似分布(q)</li></ul><p id="c5ae" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的目标是得到最接近真实后验分布的近似分布，这样我们就可以更多地了解后验。KLD是一种广泛用于计算两个分布接近程度的方法。还不要累，因为它会变得有趣！我们必须最小化KLD，以便更接近真实的后验。</p><p id="7c60" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是事实证明我们实际上不能最小化这个量，所以我们通过詹森不等式假设一个凹函数(我知道没错！)并添加一个常数。该函数被称为证据下限(简称ELBO)。回想一下,“证据”是一个术语，用于观察的边际可能性(或其对数)。假设X为观察变量，Z为隐藏变量，ELBO可以写成:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kb"><img src="../Images/471ccb1159a9f599783776aa3db36979.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*ynocF9bPeT9In-3x.png"/></div></figure><p id="fea6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">测验时间到了；准备好了吗？</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kq"><img src="../Images/8832c0b15d4c17af5177ee35b1bae5a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/0*bBzMOPLXtKVQwEPH"/></div></figure><p id="e5a3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">问题:你在上面的等式中看到的第二个期望项是什么？</p><p id="ffa6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">提示:该术语是一种信息度量，它只考虑观察到特定事件的概率，因此它封装的信息是关于潜在概率分布的信息。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><p id="6390" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">而答案是:<strong class="iz hj">熵</strong></p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><p id="556a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从近似推理算法中消息传递的思想来理解熵的包含是相当清楚的。</p><p id="2414" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以证明KLD等于负的ELBO加上一个常数，如下所示:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kb"><img src="../Images/f541cff8f58f20d13d05b2db117a2589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/0*mdTy3fJdTjydPTWd.png"/></div></figure><p id="f131" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">最后，优化</strong></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ky"><img src="../Images/52e9fb19b9c3961a95c68f84004f2539.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*9n2L1BdFN8Ph_8sZ"/></div></figure><p id="1d48" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们使用优化算法用它的参数迭代地更新q。Sklearn使用期望最大化(EM)算法。E-step通过最小化到p(α和β固定)的KL-散度(我们已经看到这是如何实现的)来估计q中的变分参数γ和φ。M步使γ和φ固定的对数似然的下限最大化(在α和β上)。ELBO收敛到局部最小值，从而允许我们使用得到的q作为真实后验概率的代理。</p><p id="236c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">参考</strong></p><p id="42ac" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae kz" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" rel="noopener ugc nofollow" target="_blank">http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf</a></p><p id="eca1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae kz" href="https://www.seas.upenn.edu/~cis520/lectures/LDA.pdf" rel="noopener ugc nofollow" target="_blank">https://www.seas.upenn.edu/~cis520/lectures/LDA.pdf</a></p><p id="fe97" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae kz" href="https://www.cs.cmu.edu/~epxing/Class/1070815/notes/10708_scribe_lecture13.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . CMU . edu/~ epxing/Class/1070815/notes/10708 _ scribe _ lecture 13 . pdf</a></p><p id="754d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae kz" href="http://users.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf" rel="noopener ugc nofollow" target="_blank">http://users . umi ACS . UMD . edu/~ xyang 35/files/understanding-variable-lower . pdf</a></p><p id="6484" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae kz" href="https://stats.stackexchange.com/questions/193465/what-is-the-gradient-log-normalizer" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/193465/what-the-gradient-log-normalizer</a></p><p id="8f0a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">​</p><p id="34e4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">​</p><p id="678f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">​</p><p id="f011" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">​</p><p id="b3f9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi">​</p></div></div>    
</body>
</html>