<html>
<head>
<title>Dimensionality Reduction using Principal Component Analysis (PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用主成分分析(PCA)的维数减少</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/dimensionality-reduction-using-principal-component-analysis-pca-41e364615766?source=collection_archive---------5-----------------------#2020-09-13">https://medium.com/analytics-vidhya/dimensionality-reduction-using-principal-component-analysis-pca-41e364615766?source=collection_archive---------5-----------------------#2020-09-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="eabd" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">处理具有更多特征/维度的数据集是一项重要任务。</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jh"><img src="../Images/32d60066ecc07f7538d3ce68c75ac372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l8A5yMWy5DYzox1F"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">参考:<a class="ae jx" href="https://towardsdatascience.com/a-complete-guide-to-principal-component-analysis-pca-in-machine-learning-664f34fc3e5a" rel="noopener" target="_blank">培养基</a></figcaption></figure><p id="0ac8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">数据每秒钟都在增加，从这些数据中解读见解以解决问题变得至关重要。而且，随着数据特征的增加，数据集的维度也会增加。最终，机器学习模型需要处理复杂的数据，导致更加复杂。另一方面，有许多特征对于模型是无用的或者与其他特征相关。主成分分析(PCA)是降低维数和从数据集中扣除相关特征的一种方法。</p><p id="7e3a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">文章分为以下几节:</strong></p><ol class=""><li id="5db0" class="kb kc hi il b im in iq ir jy kd jz ke ka kf jg kg kh ki kj bi translated">定义- PCA</li><li id="d6f1" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kg kh ki kj bi translated">PCA 的需求和优势</li><li id="4b26" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kg kh ki kj bi translated">实时使用/应用</li><li id="7159" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kg kh ki kj bi translated">执行 PCA 的步骤—</li></ol><ul class=""><li id="0bfa" class="kb kc hi il b im in iq ir jy kd jz ke ka kf jg kp kh ki kj bi translated">数据标准化</li><li id="a763" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">计算协方差矩阵</li><li id="af39" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">确定特征值和特征向量</li><li id="5332" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">计算 PCA 特征</li></ul><p id="aef1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">5.使用 Python 实现 MNIST 数据集的主成分分析</p><p id="83c4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">6.结论</p><h1 id="4b0d" class="kq kr hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ak">什么是 PCA？</strong></h1><p id="ffd3" class="pw-post-body-paragraph ii ij hi il b im lo io ip iq lp is it jy lq iw ix jz lr ja jb ka ls je jf jg hb bi translated">主成分分析(PCA)是一种降维技术，它使您能够识别数据集中的相关性和模式，以便可以将其转换为维度显著减少的数据集，而不会丢失任何重要信息。</p><h1 id="7b81" class="kq kr hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ak">对 PCA 的需求</strong></h1><p id="b6b6" class="pw-post-body-paragraph ii ij hi il b im lo io ip iq lp is it jy lq iw ix jz lr ja jb ka ls je jf jg hb bi translated">具有更多要素的数据集需要更多时间来训练模型，并使数据处理和探索性数据分析(EDA)更加复杂。</p><h1 id="291c" class="kq kr hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ak">PCA 的优势</strong></h1><ul class=""><li id="79a7" class="kb kc hi il b im lo iq lp jy lt jz lu ka lv jg kp kh ki kj bi translated">减少培训时间。</li><li id="6b69" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">移除相关特征(移除噪声)。</li><li id="f9ed" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">易于数据探索(EDA)。</li><li id="4b55" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">易于可视化数据(最大 3D 数据)。</li></ul><h1 id="624a" class="kq kr hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ak">实时应用</strong></h1><p id="181a" class="pw-post-body-paragraph ii ij hi il b im lo io ip iq lp is it jy lq iw ix jz lr ja jb ka ls je jf jg hb bi translated">主成分分析用于人脸识别、计算机视觉、图像压缩、图像检测、目标检测、图像分类等领域的降维。</p><h1 id="b859" class="kq kr hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ak">执行 PCA 的步骤:</strong></h1><h2 id="5408" class="lw kr hi bd ks lx ly lz kw ma mb mc la jy md me le jz mf mg li ka mh mi lm mj bi translated"><strong class="ak">数据标准化</strong></h2><ul class=""><li id="4451" class="kb kc hi il b im lo iq lp jy lt jz lu ka lv jg kp kh ki kj bi translated">标准化就是对数据进行缩放，使所有的值/变量都在一个相似的范围内。标准化意味着重新调整数据，使平均值为 0，标准差为 1(单位方差)。</li><li id="d355" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated"><strong class="il hj">为什么？</strong>它是关于确保数据内部一致；也就是说，每种数据类型都有相同的内容和格式。</li></ul><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mk"><img src="../Images/9553ac8664f994198fdce1f3a92431fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aFfa3-_zKdx8yqcD"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">参考:<a class="ae jx" href="https://365datascience.com/standardization/" rel="noopener ugc nofollow" target="_blank">365 数据科学</a></figcaption></figure><h2 id="ed53" class="lw kr hi bd ks lx ly lz kw ma mb mc la jy md me le jz mf mg li ka mh mi lm mj bi translated"><strong class="ak">计算协方差矩阵</strong></h2><ul class=""><li id="b5d1" class="kb kc hi il b im lo iq lp jy lt jz lu ka lv jg kp kh ki kj bi translated">协方差矩阵是在数据标准化后计算的，用于查找数据集中的相关要素。协方差矩阵的每个元素代表两个特征之间的关系。</li><li id="d668" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated"><strong class="il hj">为什么？</strong>用于确定特征之间的相关性。</li></ul><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ml"><img src="../Images/21cab4b6cc4facd3f92658868b6cc946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PY5blqYrCyCiziJT"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">参考- <a class="ae jx" href="https://www.theanalysisfactor.com/covariance-matrices/" rel="noopener ugc nofollow" target="_blank">分析因子</a></figcaption></figure><h2 id="f560" class="lw kr hi bd ks lx ly lz kw ma mb mc la jy md me le jz mf mg li ka mh mi lm mj bi translated"><strong class="ak">确定特征值和特征向量</strong></h2><ul class=""><li id="5080" class="kb kc hi il b im lo iq lp jy lt jz lu ka lv jg kp kh ki kj bi translated">特征值和特征向量是为了确定数据集的主成分而必须从协方差矩阵计算的数学构造。</li><li id="5e90" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">主成分是从初始特征集获得的新的变量/特征集。它们压缩并拥有分散在初始特征中的大部分有用信息。</li><li id="3461" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">特征向量是这样的向量，当对它们进行线性变换时，它们的方向不会改变。</li><li id="655d" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">特征值简化表示各个特征向量的标量。</li><li id="21e2" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated"><strong class="il hj">为什么？</strong>寻找特征最大扩散的方向。</li></ul><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es mm"><img src="../Images/ac9e98bc3f82edf696c07bbf5b0b2ea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/0*gYvhmwB3SIXSierm"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">参考- <a class="ae jx" href="https://math.hmc.edu/calculus/hmc-mathematics-calculus-online-tutorials/linear-algebra/eigenvalues-and-eigenvectors/" rel="noopener ugc nofollow" target="_blank"> HMC </a></figcaption></figure><h2 id="fc46" class="lw kr hi bd ks lx ly lz kw ma mb mc la jy md me le jz mf mg li ka mh mi lm mj bi translated"><strong class="ak">计算主成分</strong></h2><ul class=""><li id="a068" class="kb kc hi il b im lo iq lp jy lt jz lu ka lv jg kp kh ki kj bi translated">假设一个数据集中有 5 个特征，那么在计算特征向量和各自的特征值后，将有 5 个主要特征，每个特征将有其特征值和特征向量。</li><li id="ac4e" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">与其他特征相比，具有最高特征值的特征具有数据的大部分细节/分布。</li><li id="41d6" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">因此，具有高特征值的特征被认为是主特征，并且这些特征将是 PCA 的输出。</li><li id="56e5" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">现在，这些主要成分将被用作训练模型和数据可视化的输入。</li></ul><p id="173d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">使用 Python 对 MNIST 数据集实施 PCA</strong></p><ul class=""><li id="ebad" class="kb kc hi il b im in iq ir jy kd jz ke ka kf jg kp kh ki kj bi translated">问题陈述——对 MNIST 数据集进行逐步主成分分析，以降低维数。</li><li id="1570" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">MNIST 数据集包含 0 到 9 个数字的各种图像，它主要用于初学者识别图像/数字。每个图像是 28 * 28 像素，当转换为矢量形式时，它将是 28 * 28 = 784 个特征，并且很难在屏幕上可视化 784 个特征并训练模型。因此，在这里我将降低数据集的维度，并可视化 2D 的数据。</li><li id="99a8" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">数据集已经准备好并转换成矩阵形式(以 CSV 格式存储)，因此很容易将其用于进一步的任务(本文没有解释如何准备数据，因为本文的主题是 PCA)。要下载数据，请访问—<a class="ae jx" href="https://www.kaggle.com/c/digit-recognizer/data" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/digit-recognizer/data</a></li><li id="c857" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated">让我们深入编码。首先，使用 pandas 加载和读取数据；</li></ul><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mn"><img src="../Images/c5223323b5e4762ccc7a0c36b1f9b96d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0Lk1G7OKJbhryDGD"/></div></div></figure><p id="0bab" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">然后分离标签数据和特征数据；</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mo"><img src="../Images/c587d9afc3dce9f5ee29e57604ac0fd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xJYA0_NGtNHNl2Fk"/></div></div></figure><p id="d29e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">应用栏目标准化 sklearn 图书馆；</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mp"><img src="../Images/d19015c39f8ab2bf87d86a32fb78acd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wUP8ZBxF60AIhkuV"/></div></div></figure><p id="b33f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">现在，根据标准化数据计算协方差，如下所示:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mq"><img src="../Images/b96273274b61035a8dbbbb01c0a7a045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*b6m0oy21aq4sBeLi"/></div></div></figure><p id="0e89" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">接下来，找到特征向量和特征值，这里我已经计算了前两个特征向量；</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mr"><img src="../Images/707594a9720c28dab7d99a479cf76705.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4eVS-k_u5FFq-Xoi"/></div></div></figure><p id="c0b1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">计算主成分；</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ms"><img src="../Images/3b6893fa270c04654edf500f705cf479.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5EZSXiAHHDf7PFI_"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mt"><img src="../Images/86ee40a43416b3d2afd64f9d0e192fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HiEK11KXDqyaoGIv"/></div></div></figure><p id="da76" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">使用 seaborn 绘制具有两个主要特征的数据；</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mu"><img src="../Images/85a4185e5cd22499749e4786c2f49441.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SfgAbgCwqIJWr3BC"/></div></div></figure><p id="3a12" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">从 sklearn 库中实现 PCA 包；</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mv"><img src="../Images/8feec22aa94a6ec4260802562f01e94e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0ZWJCLBuTceEiTSg"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mw"><img src="../Images/ef92e2cf50caed6a9a9c4e9612eb81c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4ZhoKus-JfzkFGvH"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mx"><img src="../Images/6b9d847a82eae2a5beb956a6e730c553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KP4WmmSEpjohYcb2"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es my"><img src="../Images/efdbb523dd8f06c058e72d9ccda0af8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jQ4VGbilQyvmtbH1"/></div></div></figure><h1 id="63f2" class="kq kr hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ak">结论</strong></h1><ul class=""><li id="b55c" class="kb kc hi il b im lo iq lp jy lt jz lu ka lv jg kp kh ki kj bi translated">维数从 784 减少到 2，主要信息保存在这两个特征中，这两个特征可用于确定数字。</li></ul></div><div class="ab cl mz na gp nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="hb hc hd he hf"><p id="c1a4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">你可以从<a class="ae jx" href="https://github.com/rajviishah/Principal-Component-Analysis" rel="noopener ugc nofollow" target="_blank"> Github </a>找到源代码</p><p id="4e6f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">如果你对这个库的任何函数/类有任何疑惑，那么我请求你查看文档。</p><p id="c4bc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">如果有任何修正和改进的范围，或者如果您有任何疑问，请通过 rajvishah2309@gmail.com<a class="ae jx" href="mailto:rajvishah2309@gmail.com" rel="noopener ugc nofollow" target="_blank">告诉我</a></p></div><div class="ab cl mz na gp nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="hb hc hd he hf"><p id="7e14" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">有关更多信息，请查看以下文章:</p><ul class=""><li id="0b3b" class="kb kc hi il b im in iq ir jy kd jz ke ka kf jg kp kh ki kj bi translated"><a class="ae jx" href="http://colah.github.io/posts/2014-10-Visualizing-MNIST/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2014-10-Visualizing-MNIST/</a></li><li id="97b8" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated"><a class="ae jx" href="https://365datascience.com/standardization/" rel="noopener ugc nofollow" target="_blank">https://365datascience.com/standardization/</a></li><li id="4893" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated"><a class="ae jx" href="https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#covariance-matrix" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/Articles/2015 _ PCA _ in _ 3 _ steps . html #协方差-矩阵</a></li></ul><p id="ec81" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">参考链接:</p><ul class=""><li id="f383" class="kb kc hi il b im in iq ir jy kd jz ke ka kf jg kp kh ki kj bi translated"><a class="ae jx" href="https://www.youtube.com/watch?v=n7npKX5zIWI&amp;t=1217s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=n7npKX5zIWI&amp;t = 1217s</a></li><li id="7cf2" class="kb kc hi il b im kk iq kl jy km jz kn ka ko jg kp kh ki kj bi translated"><a class="ae jx" href="https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2893/visualize-mnist-dataset/2/module-2-data-science-exploratory-data-analysis-and-data-visualization" rel="noopener ugc nofollow" target="_blank">https://www . applied ai course . com/lecture/11/applied-machine-learning-online-course/2893/visualize-Mn ist-dataset/2/module-2-data-science-explorative-data-analysis-and-data-visualization</a></li></ul></div></div>    
</body>
</html>