# 使用 python 从图像中提取文本

> 原文：<https://medium.com/analytics-vidhya/extracting-text-from-images-using-python-315ed54d90e8?source=collection_archive---------10----------------------->

> “让我们通过让每个人都可以使用人工智能来共同推动人类进步！”

基于图像的序列识别一直是计算机视觉领域的一个长期研究课题。在本文中，研究人员研究了场景文本识别问题，这是基于图像的序列识别中最重要和最具挑战性的任务之一。提出了一种新的神经网络结构，它将特征提取、序列建模和转录集成到一个统一的框架中。与以前的场景文本识别系统相比，所提出的体系结构具有四个显著的特性:(1)它是端到端可训练的，这与大多数现有算法的组件是单独训练和调整的不同。(2)它自然地处理任意长度的序列，不涉及字符分割或水平标度归一化。(3)它不局限于任何预定义的词典，并且在无词典和基于词典的场景文本识别任务中都取得了显著的性能。(4)它生成有效但小得多的模型，这对于真实世界的应用场景更实用。在包括 IIIT-5K、街景文本和 ICDAR 数据集在内的标准基准上的实验表明，所提出的算法优于现有技术。此外，该算法在基于图像的乐谱识别任务中表现良好，明显验证了其通用性。

 [## 一种用于图像序列识别的端到端可训练神经网络及其在图像识别中的应用

### 基于图像的序列识别一直是计算机视觉领域的一个长期研究课题。在本文中，我们…

arxiv.org](https://arxiv.org/abs/1507.05717) 

# 实施

我已经在 Google Colab 中实现了这项工作，我选择了 GPU 作为运行时类型，因为我必须从预训练的模型中进行预测。

EasyOCR 是 Python 中的一个库，它从图像中提取文本。这个库是由[包光石](https://arxiv.org/search/cs?searchtype=author&query=Shi%2C+B)、[、](https://arxiv.org/search/cs?searchtype=author&query=Bai%2C+X)、[丛瑶](https://arxiv.org/search/cs?searchtype=author&query=Yao%2C+C)在 2019 年开发的。这是最好的和目前使用的从图像中提取文本的库。

为了运行 EasyOCR，我们必须通过运行下面给出的命令来安装库，然后它会要求您重新启动运行时。

pip 安装 **easyocr**

**导入** matplotlib.pyplot **作为**PLT #这些是必需的头文件

**导入** cv2

**导入** easyocr

**从** pylab **导入** rcParams

**从** IPython.display **导入**图像

rcParams['figure.figsize'] = 8，16

reader = easyocr。reader([' en '])#这是检测和识别的预训练模型，括号中是必须检测的文字书写的语言类型。en 代表英语。预训练模型被保存到一个名为 reader 的变量中。

image(" image . jpg ")#显示必须通过其检测文本的输入图像

![](img/0b15939d81b4217800deab5b4676033b.png)

输入图像

output = reader . read text(' image . jpg ')#已经将预先训练好的模型保存到一个名为 reader 的变量中。调用函数“readtext ”,该函数实际上获取输入图像并从该图像中读取文本。我将 readtext 的输出保存在一个名为 output 的变量中。

# 输出:

输出变量的输出#内容

[([[43，87]，[211，87]，[211，131]，[43，131]，#第一部分是边界框坐标

“晚安”，#第二部分是从图中生成的文本

0.6844154000282288)] #预测或生成文本的置信度。

# 致谢和参考:

这个项目基于几篇论文/开源库的研究/代码。

检测部分采用了本文中的 CRAFT 算法

 [## 用于文本检测的字符区域意识

### 基于神经网络的场景文本检测方法最近已经出现，并且已经显示出有希望的结果。上一个…

arxiv.org](https://arxiv.org/abs/1904.01941) 

识别模型是 CRNN:

 [## 一种用于图像序列识别的端到端可训练神经网络及其在图像识别中的应用

### 基于图像的序列识别一直是计算机视觉领域的一个长期研究课题。在本文中，我们…

arxiv.org](https://arxiv.org/abs/1507.05717)