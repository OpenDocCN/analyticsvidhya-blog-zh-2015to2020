<html>
<head>
<title>T5: a detailed explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">T5:详细说明</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51?source=collection_archive---------0-----------------------#2020-06-08">https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51?source=collection_archive---------0-----------------------#2020-06-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="548f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">鉴于当前NLP迁移学习的情况，<strong class="ak">T</strong>ext-<strong class="ak">T</strong>o-<strong class="ak">T</strong>ext<strong class="ak">T</strong>transfer<strong class="ak">T</strong>transformer(T5)旨在探索什么最有效，以及我们可以在多大程度上利用我们已经拥有的工具。</h2></div><p id="fe7e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">近年来，出现了许多用于自然语言处理的迁移学习技术。但是一些技术可能几乎是相同的——只是使用不同的数据集或优化器——但是它们获得不同的结果，那么我们能说结果更好的技术比另一种更好吗？鉴于当前NLP迁移学习的情况，<strong class="iz hj">T</strong>ext-<strong class="iz hj">T</strong>o-<strong class="iz hj">T</strong>ext<strong class="iz hj">T</strong>transfer<strong class="iz hj">T</strong>transformer(T5)旨在探索什么最有效，以及我们可以在多大程度上利用我们已经拥有的工具。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="fd24" class="ka kb hi bd kc kd ke kf kg kh ki kj kk io kl ip km ir kn is ko iu kp iv kq kr bi translated">基线模型</h1><h2 id="d921" class="ks kb hi bd kc kt ku kv kg kw kx ky kk jg kz la km jk lb lc ko jo ld le kq lf bi translated">T5框架</h2><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lg"><img src="../Images/4e563a91830d4e492e9af4ccd68c6a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QOVXAn0bx8HKGrBIXAgydw.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">T5框架的示意图。资料来源:T5 <a class="ae lw" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">纸</a>。</figcaption></figure><p id="54b2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">许多任务都被投射到这个框架中:机器翻译、分类任务、回归任务(例如，预测两个句子有多相似，相似性得分在1到5的范围内)，其他序列到序列任务，如文档摘要(例如，从CNN每日邮报语料库中总结文章)。</p><h2 id="92bd" class="ks kb hi bd kc kt ku kv kg kw kx ky kk jg kz la km jk lb lc ko jo ld le kq lf bi translated">T5模型结构</h2><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es lx"><img src="../Images/ac10d4342dd64c9208cb176e7212137e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iJcUH1F0TmCQE5p2wQt9og.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">模型结构。来自:<a class="ae lw" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">杰伊·阿拉玛的博客</a></figcaption></figure><p id="ea65" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模式结构只是一种标准的普通编码器-解码器转换器。</p><h2 id="a1d5" class="ks kb hi bd kc kt ku kv kg kw kx ky kk jg kz la km jk lb lc ko jo ld le kq lf bi translated">预训练数据集</h2><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es ly"><img src="../Images/f2e5b2143b266140624e25ff38f55f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/1*kTc83HCd6a9nptgLMX93CA.gif"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">T5清理数据集。来源:来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure><p id="1e62" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">T5使用<a class="ae lw" href="https://commoncrawl.org/" rel="noopener ugc nofollow" target="_blank">普通抓取网页摘录文本</a>。作者应用了一些非常简单的启发式过滤。T5删除所有不以标点符号结尾的行。它还删除了带有单词javascript的行和任何带有花括号的页面(因为它经常出现在代码中)。<strong class="iz hj"> <em class="lz">它通过取3个句子组块的滑动窗口对数据集进行重复数据删除，这样只有其中一个出现在数据集</em> </strong>中。例如，在3页以上，中间页的最后一段被删除，因为相同的内容出现在第一页。最终得到750千兆字节的干净的英文文本。数据集在<a class="ae lw" href="https://www.tensorflow.org/datasets/catalog/c4" rel="noopener ugc nofollow" target="_blank"> tensorlow.text.c4 </a>上公开。</p><h2 id="5550" class="ks kb hi bd kc kt ku kv kg kw kx ky kk jg kz la km jk lb lc ko jo ld le kq lf bi translated">无人监管的目标</h2><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es ma"><img src="../Images/f24150d31a4047e2e02f3eb2b46d9ebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*9yFICqDlfprn-I_VZ5RHgw.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">无人监管的目标。来源T5 <a class="ae lw" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><p id="8394" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有了框架、模型架构和未标记的数据集，下一步是寻找无监督的目标，该目标为模型提供了一些从未标记的数据中学习的方法。在原文中，一些单词用一个唯一的sentinel标记去掉了。单词被独立地均匀地随机丢弃。该模型被训练来预测基本上标记记号，以描绘被丢弃的文本。</p><h2 id="9cae" class="ks kb hi bd kc kt ku kv kg kw kx ky kk jg kz la km jk lb lc ko jo ld le kq lf bi translated">工作流程</h2><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es mb"><img src="../Images/2873bd7beb2d231be0d56810c2d18a21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-drd83Rs_JphZ7oAYlAP2Q.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">预训练、微调和评估步骤。来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure><p id="aae8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，该模型以去噪目标和C4数据集为基础，基于Bert基大小的编码器-解码器变换器进行预处理，在2个⁵或约348个令牌上训练2个⁹步骤，学习速率为平方根倒数。微调任务包括<a class="ae lw" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">胶水、</a> <a class="ae lw" href="https://arxiv.org/abs/1506.03340" rel="noopener ugc nofollow" target="_blank"> CNN/DM </a> (CNN /每日邮报)、<a class="ae lw" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">小队</a>、<a class="ae lw" href="https://super.gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank">强力胶</a>、<a class="ae lw" href="https://nlp.stanford.edu/projects/nmt/" rel="noopener ugc nofollow" target="_blank">翻译任务</a> : WMT14 EnDe、WMT14 EnFr、WMT14 EnRo。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es mc"><img src="../Images/73723312bb1def6d9add4120f43a82e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nbjijmtJJ-eKmkdaqnzg4w.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">表格结构。</figcaption></figure><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es md"><img src="../Images/4e0eefd8ac8d656a292502776cded025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RfzVjzpod7Da-pXwKEwJNg.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">基线模型性能。来源:T5 <a class="ae lw" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><p id="9d84" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，在不同的环境中尝试不同的策略来测试它们如何影响表现。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="4926" class="ka kb hi bd kc kd ke kf kg kh ki kj kk io kl ip km ir kn is ko iu kp iv kq kr bi translated">策略比较</h1><h2 id="8b62" class="ks kb hi bd kc kt ku kv kg kw kx ky kk jg kz la km jk lb lc ko jo ld le kq lf bi translated">结构</h2><div class="lh li lj lk fd ab cb"><figure class="me ll mf mg mh mi mj paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/dda459de77508545ed18c7b4d68555d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*l1jqv50E_rKOEkBCI2mHug.png"/></div></figure><figure class="me ll mk mg mh mi mj paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><img src="../Images/07128fb3c4bc23a656f9ef4b54467484.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*hlBep8nQfxkdUwSTvHBH7w.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx ml di mm mn translated">不同的注意掩蔽模式(左)及其对应的模型(右)。来源:T5 <a class="ae lw" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure></div><p id="352c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">完全可见的掩码，其中每个输出条目都可以看到每个输入条目。在中间，由于模型不允许预测未来，所以有一个适合预测序列的临时掩码。带前缀的因果掩码允许模型将输入序列的第一位视为完全可视，然后开始预测输入序列中随后出现的内容。结果表明<strong class="iz hj">给定模型输入上的双向上下文是有价值的。</strong></p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mo"><img src="../Images/4f33765316b26c6079afe1a5c861fb00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/1*quZpSZjw0pEK88KjXpngeA.gif"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">模拟不同任务的表现。来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure><h2 id="ed39" class="ks kb hi bd kc kt ku kv kg kw kx ky kk jg kz la km jk lb lc ko jo ld le kq lf bi translated">目标</h2><p id="712d" class="pw-post-body-paragraph ix iy hi iz b ja mp ij jc jd mq im jf jg mr ji jj jk ms jm jn jo mt jq jr js hb bi translated">涉及三个目标:<strong class="iz hj">语言建模(预测下一个单词)、BERT式目标(用随机不同的单词屏蔽/替换单词并预测原文)，以及去混洗(随机混洗输入并尝试预测原文)。</strong></p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mu"><img src="../Images/13ca810ffd52fb1a43b4e073c8325dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/1*8hCjg1hdicbrHGrFcEl4zw.gif"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">不同的目标及其表现。来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure><p id="50bf" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">腐败策略有3个选项:</strong> <strong class="iz hj">只屏蔽令牌，不交换令牌；屏蔽令牌并用单个标记令牌替换它们；和移除令牌。</strong>性能显示<strong class="iz hj">“替换损坏的跨度”策略效果最佳</strong>。此外，应用了不同的损坏率，结果显示<strong class="iz hj">除非采用较大的损坏率(s.t. 50%)，否则此设置对性能不敏感</strong>，因为损坏率为10%、15%和25%的模型性能相似。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es mo"><img src="../Images/4382d6b8719cd37891727e0ebf9c68ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/1*eS81GEgaCvEjnsLzjkOtSQ.gif"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">不同的腐败策略及其绩效。来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es mv"><img src="../Images/62f37c562a15cc050f354fe89297e191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/1*kcWGJIYo0rR8J2iIvnf41A.gif"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">不同的跨距长度及其性能。来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure><p id="51b0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">给定一个好的损坏率，如果整个区间被丢弃，而不是对每个令牌进行iid判决(IID判决:一致随机地决定我们是否应该损坏该字)，会发生什么？基本上，在输入中的每个位置，应该丢弃一个范围的标记吗？这个范围应该有多长？与损坏率设置类似，<strong class="iz hj">该设置不会对性能产生太大影响，除非丢弃大量令牌</strong>(如10个令牌)。</p><p id="2d13" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这一客观实验的亮点是<strong class="iz hj"> word腐败目标往往效果最佳。</strong>模型往往在不同的损坏率下非常相似地工作，鉴于这一事实<strong class="iz hj">，建议使用产生短目标序列的目标，因为较短的序列做预训练更便宜。</strong></p><h1 id="fd09" class="ka kb hi bd kc kd mw kf kg kh mx kj kk io my ip km ir mz is ko iu na iv kq kr bi translated">数据集</h1><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es nb"><img src="../Images/d07e71a46ba016eaa18cd8a92cc08d29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/1*z5F5uPeZU-FBSHo2cMoWfQ.gif"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">不同数据集的附属和他们的表现。来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure><p id="41e5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">除了干净的C4数据集，没有任何过滤的相同数据也进行了尝试。结果表明，<strong class="iz hj">滤波确实有助于模型更好地运行。</strong>其他具有较小数量级的数据集(这意味着在一些约束域上更多)也适用。结果表明<strong class="iz hj">对领域数据的预训练有助于下游任务的执行。</strong>例如，在RealNews-like数据集上预训练的模型在记录上表现得更好(新闻文章上的问答数据集)——在新闻上预训练的模型往往在新闻问答任务上表现得更好。在维基百科和TBC数据集(Tronoto图书语料库)上训练的模型在multiRC数据集(包括一些小说数据的问答数据集)上效果更好。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="er es nc"><img src="../Images/22facec916743f0e0307f09a651857ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SapsvvsDQuTmWUGBpFf_QA.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">不同的训练步骤及其表现。来源:<a class="ae lw" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank"> T5论文</a></figcaption></figure><p id="d517" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果<strong class="iz hj">训练数据集是有限的，并且在训练</strong>期间被重复，该怎么办？设置这些训练步骤怎么样？如果你不打算这么长时间，你可能不需要担心这个。<strong class="iz hj">但是，如果您开始多次重复您的数据集，模型会开始记忆您的相关数据集</strong>(因为较大的训练步长会导致较小的训练损失)<strong class="iz hj">，这会导致下游任务的性能显著下降</strong></p><h2 id="ae07" class="ks kb hi bd kc kt ku kv kg kw kx ky kk jg kz la km jk lb lc ko jo ld le kq lf bi translated">多任务</h2><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es nd"><img src="../Images/d1bb78abf004117ed3ceb48d9ba37668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/1*5MUuWyi6Y7566p-Yk9guVQ.gif"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">不同的混合策略及其性能。来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure><p id="1c63" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该模型不是进行无监督的预训练和微调，而是在多任务上进行训练。有几种混合策略<strong class="iz hj">。任务可以平均训练</strong>。下一个选项是<strong class="iz hj">根据数据集</strong>中的示例数量对每个数据集进行加权，因为我们不想让小数据集过拟合而让大数据集过拟合。因为无人监督的任务是如此之大，以至于<strong class="iz hj">我们需要设定一个人为的限制</strong>来限制我们在这项任务上的训练量。这就是多语言BERT所做的——从不同的语言中取样。我们也可以<strong class="iz hj">采用这些数量的示例比例，并对其应用温度，使其更加接近均匀</strong>。在多项任务上同等训练会导致更差的表现。正确设置阈值和温度与预调整和微调设置的性能有些相似。特别是在GLUR、SQUAD和superGLUE上，当试图一次训练一个模型完成所有这些任务时，性能会显著下降。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es ne"><img src="../Images/3b4ced9a5eb3c767a126ae5cec1d2026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/1*6AMAl7vZO7GZ9eBchynWXw.gif"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">不同的训练策略及其表现。来源:来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure><p id="da7e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">除了<strong class="iz hj">多任务训练</strong>，还有其他训练策略。最初的一个是<strong class="iz hj">对无监督的任务进行预训练，并对每个单独的下游任务进行微调。MT-DNN  <strong class="iz hj">提供了另一种选择:多任务混合训练，然后对每个单独的任务进行微调。</strong>该策略缩小了无监督附属和微调之间的差距。<strong class="iz hj">留一项多任务训练是关于多任务混合，然后对一项在训练中不使用的任务进行微调。</strong>结果表明，该策略仍能生成相当好的预训练模型。最后一个选项是<strong class="iz hj">，仅与监督任务相关，并对同一组监督任务进行微调</strong>，这与计算机视觉非常相似。然而，这种策略极大地损害了性能。</strong></p><h1 id="4a7d" class="ka kb hi bd kc kd mw kf kg kh mx kj kk io my ip km ir mz is ko iu na iv kq kr bi translated">缩放比例</h1><p id="2ece" class="pw-post-body-paragraph ix iy hi iz b ja mp ij jc jd mq im jf jg mr ji jj jk ms jm jn jo mt jq jr js hb bi translated">如何放大模型？如果给你四倍的计算能力，你会如何使用它？你应该训练模型更长时间，或者使用更大的批量，或者训练更大的模型，或者训练四个模型并集成它们？<strong class="iz hj">训练模型的时间越长，性能越好</strong>，这也是<a class="ae lw" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"> Roberta </a>所做的主要事情之一(与模型相关的时间越长)。通过使模型变得更深更宽来使它变得更大，并将它的长度训练为原来的两倍，这也会产生相当大的收益。 <strong class="iz hj">训练四个模型，组装起来也很有帮助。</strong></p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es nf"><img src="../Images/033e56a3e7a47f2452d2465a2f1d15d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*jriKYJn_BTDoouyMNMvUhg.gif"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">不同的扩展策略及其性能。来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="b260" class="ka kb hi bd kc kd ke kf kg kh ki kj kk io kl ip km ir kn is ko iu kp iv kq kr bi translated">把所有的放在一起</h1><p id="00e0" class="pw-post-body-paragraph ix iy hi iz b ja mp ij jc jd mq im jf jg mr ji jj jk ms jm jn jo mt jq jr js hb bi translated">在将所有这些想法结合在一起并按比例放大后，作者训练了5种变体:小模型、基本模型、大模型和具有30亿和110亿参数的模型(通过使前馈层变宽)。除了翻译，他们都取得了不错的成绩。<em class="lz">对于翻译来说，作为这些最先进成果的一个非常重要的组成部分，反向翻译似乎比仅提供英语的预培训效果更好</em>。值得注意的是，强力胶被设计成对伯特来说很难，但对人类来说很容易。T5的表现非常接近人类水平。</p><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es ng"><img src="../Images/9c3f7923c0da3dfae68826ff9315abd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*WKWaVDoZbPCbx3WtTScnCw.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">T5型号尺寸变化。来源:<a class="ae lw" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank"> T5纸。</a></figcaption></figure><figure class="lh li lj lk fd ll er es paragraph-image"><div class="er es nh"><img src="../Images/83f8568cb123cfab84d02de5ac6138e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*IZGxfqttzSaG1NI1tRz6CQ.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">T5车型性能。来源:<a class="ae lw" href="https://www.youtube.com/watch?v=eKqWC577WlI&amp;list=UUEqgmyWChwvt6MFGGlmUQCQ&amp;index=4" rel="noopener ugc nofollow" target="_blank">科林·拉弗尔视频</a></figcaption></figure></div></div>    
</body>
</html>