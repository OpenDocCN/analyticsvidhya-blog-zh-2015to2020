<html>
<head>
<title>NLP Series: Encoder-Decoder Model and Attention Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP系列:编码器-解码器模型和注意模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nlp-attention-model-8127639ce37d?source=collection_archive---------18-----------------------#2020-07-09">https://medium.com/analytics-vidhya/nlp-attention-model-8127639ce37d?source=collection_archive---------18-----------------------#2020-07-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bce8621b9447a60f01c5537cca439791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pnJgUVxNvlAbLJNaGBJP4g.png"/></div></div></figure><p id="b42e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇文章中，我将解释注意力模型。为了理解注意力模型，需要有RNN和LSTM的先验知识。对于RNN和LSTM，你可以参考克里斯·纳伊克的youtube <a class="ae jo" href="https://www.youtube.com/watch?v=rdkIOM78ZPk" rel="noopener ugc nofollow" target="_blank">视频</a>，克里斯托·奥拉<a class="ae jo" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">博客</a>，以及苏丹书<a class="ae jo" href="https://www.youtube.com/watch?v=yPpfxFTh0XQ&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">讲座</a>。</p><p id="1bd7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">CNN在文本分析中的问题:</strong></p><p id="a81f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">CNN模型用于解决与视觉相关的用例，但未能解决，因为它无法记住特定文本序列中提供的上下文。它不能记住数据的顺序结构，即每个单词都依赖于前一个单词或句子。RNN、LSTM、编码器-解码器和注意力模型有助于解决这个问题。RNN、LSTM和编码器-解码器仍然苦于<strong class="is hj">记住大句子的顺序结构</strong>的上下文，从而导致不良的准确性。</p><p id="72a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了理解注意力模型，需要理解编码器-解码器模型，这是最初的构建块。</p><p id="e110" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">编解码器型号</strong></p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jp"><img src="../Images/79e5affc5393188392d9a8f23153de50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*psB8Be-AP8Dw8n3EIxVvJw.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图片取自Sudhanshu关于编码器-解码器模型的讲座</figcaption></figure><p id="3fe6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码器-解码器模型在时间尺度上由输入层和输出层组成。</p><p id="21d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">编码器:</strong>输入被提供给编码器层，在每个单元上没有立即输出，当到达句子/段落的结尾时，将给出输出。每个单元有两个输入:来自前一个单元的输出和当前输入。编码器中单元可以是多对一神经序列模型的RNN、LSTM、GRU或双向LSTM网络。目前，我们已经采用了单变量类型，可以是RNN/LSTM/格鲁。在基本上是神经网络的编码器网络中，它将试图通过提供的输入和反向传播来学习权重。一旦学习了权重，隐藏层的组合嵌入向量/组合权重作为编码器的输出给出。</p><p id="9dba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">解码器:</strong>编码器的输出给解码器的输入(图中表示为E)，解码器中第一个单元的初始输入是编码器的隐藏状态输出(图中表示为So)。随后，解码器网络中每个单元的输出作为输入提供给下一个单元以及前一个单元的隐藏状态。解码器中的每个单元都产生输出，直到遇到句子的结尾。编码器中的单元可以是多对一神经序列模型的LSTM、GRU或双向LSTM网络。目前，我们已经采取了单变量类型，可以是RNN/LSTM/格鲁</p><p id="5ee5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">优势:</strong></p><ol class=""><li id="8611" class="jy jz hi is b it iu ix iy jb ka jf kb jj kc jn kd ke kf kg bi translated">不像在LSTM，在编码器-解码器模型能够消费整个句子或段落作为输入。</li></ol><p id="cb5c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">缺点:</strong></p><ol class=""><li id="c95b" class="jy jz hi is b it iu ix iy jb ka jf kb jj kc jn kd ke kf kg bi translated">大/复杂句子的问题:当我们在解码器网络中进行前向传播时，从编码器接收的组合嵌入向量的有效性逐渐消失。正如我们看到的，解码器单元的输出被传递到下一个单元。在长句的情况下，嵌入向量的有效性丧失，从而在输出中产生较低的准确性，尽管它比双向LSTM好。</li></ol><p id="1721" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">解决方案:</strong>针对编解码模型所面临问题的解决方案是<strong class="is hj">注意力模型</strong>。</p><p id="33d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意力模型:</strong></p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kh"><img src="../Images/ce00f13449ed76f223ec18f0b1f49b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p6JWvEjffWtCfv4AYo7IZg.png"/></div></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bce8621b9447a60f01c5537cca439791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pnJgUVxNvlAbLJNaGBJP4g.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图片取自Sudhanshu关于基于注意力模型的讲座</figcaption></figure><p id="daef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意力模型是来自深度学习NLP的构建块。高级模型基于相同的概念。</p><p id="d7d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于大句子，以前的模型不足以预测大句子。这就是为什么不考虑整个长句，而是考虑句子中被称为注意的部分，这样句子的上下文就不会丢失。</p><p id="ccec" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们如何实现这一点？如前面在编码器-解码器模型中提到的，隐藏层的组合嵌入向量/组合权重的整个输出被作为解码器的输入。对于基于注意的机制，考虑句子/段落中的位或部分来集中或集中句子的部分，这样可以提高准确率。</p><p id="e1ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码器中的单元可以是多对一神经序列模型的LSTM、GRU或双向LSTM网络。目前，我们已经采用了双变量类型，可以是RNN/LSTM/格鲁。双向LSTM将在两个方向上学习重量，向前和向后，这将提供更好的准确性。</p><p id="579b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">参考上图，基于注意力的模型包括3个模块:</p><ol class=""><li id="c202" class="jy jz hi is b it iu ix iy jb ka jf kb jj kc jn kd ke kf kg bi translated">编码器</li><li id="8786" class="jy jz hi is b it ki ix kj jb kk jf kl jj km jn kd ke kf kg bi translated">解码器</li><li id="0853" class="jy jz hi is b it ki ix kj jb kk jf kl jj km jn kd ke kf kg bi translated">注意力</li></ol><p id="53a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">编码器:</strong>e noder si双向LSTM中的所有单元。存在向前连接的LSTM序列和向后连接的LSTM层序列。LSTM中每个单元的输入在向前和向后方向上被馈入输入X1、X2 …..Xn。来自LSTM中每个小区的前向和后向输出被组合以产生某种输出h1、h2 …...网络中RNN/LSTM小区的数量是可配置的。如果网络的大小是1000，并且提供了100个字，那么在100个字之后，它将遇到行尾，并且剩余的900个单元将不会被使用。</p><p id="9b6c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意模式:</strong>编码器h1，h2…hn的输出通过<strong class="is hj">注意单元</strong>传递到解码器的第一输入端。有可能这个句子的长度是5，有时是10。让我们考虑在解码器的第一个单元输入中从编码器取得三个隐藏输入。</p><p id="1704" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">隐藏层的多个输出通过前馈神经网络来创建上下文向量Ct，并且该上下文向量Ci被馈送到解码器作为输入，而不是整个嵌入向量。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/ff21eae1011ecde75f90e5176ce991cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*OWuZe6vzLsjr_s-MRudyFw.png"/></div></figure><p id="0901" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">a11、a21、a31是具有来自编码器的输出和到解码器的输入的前馈网络的权重。</p><p id="dc03" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上图中，h1、h2…hn是神经网络的输入，a11，a21，a31是隐单元的权重，它们是可训练的参数。Ci上下文向量是注意力单元的输出。</p><p id="22df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">a11权重指的是编码器的第一个隐藏单元和解码器的第一个输入。类似地，a21权重指的是编码器的第二隐藏单元和解码器的第一输入。此外，使用具有大量输入和权重的前馈神经网络，我们可以发现哪一个在上下文向量创建中贡献更大。</p><p id="79ce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">窗口大小(称为T)取决于句子/段落的类型。这是超参数，随不同类型的句子/段落而变化。50的窗口大小给出了更好的蓝色比例。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/640a9a770e4082ac11759f311d6da725.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/0*sypNoMZBx2HP_kqR.png"/></div></figure><p id="a692" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> eij </strong>是由函数<strong class="is hj"> a </strong>描述的前馈神经网络的输出分数，其试图捕捉在<strong class="is hj"> j </strong>处的输入和在I处的输出之间的对准。这是主要的注意力函数。对于编码器网络，类似地，对于解码器，输入Si-1是0。hj是通过前馈神经网络学习的。表示它是一个前馈网络。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/802ea684397d83379f15e0c9f0d880df.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/0*FNVPIYvJ0uPp_J_o.png"/></div></figure><p id="ec16" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">aij:为aij定义了两个条件:</p><ol class=""><li id="4754" class="jy jz hi is b it iu ix iy jb ka jf kb jj kc jn kd ke kf kg bi translated">aij应该总是大于零，这表示aij应该总是具有正值。这是因为在反向传播中，我们应该能够通过乘法来学习权重。负权重将导致消失梯度问题。</li><li id="1f1f" class="jy jz hi is b it ki ix kj jb kk jf kl jj km jn kd ke kf kg bi translated">所有权重的总和应该是1，以具有更好的正则化。这就是Softmax函数。这是窗户的尺寸，这里是3。</li></ol><p id="bdae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">a11、a21、a31是具有来自编码器的输出和到解码器的输入的前馈网络的权重。将进入第一上下文向量Ci的输入是h1 * a11 + h2 * a21 + h3 * a31。类似地，第二上下文向量是h1 * a12 + h2 * a22 + h3 * a32。</p><p id="669c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在<strong class="is hj">注意</strong>单元中，我们引入了编码器-解码器模型中不存在的<strong class="is hj">前馈网络</strong>。隐藏输出将学习并产生上下文向量，而不依赖于双LSTM输出。</p><p id="488c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所有的矢量h1、h2..等。，在他们的工作中使用的基本上是编码器中的前向和后向隐藏状态的串联。简单来说，所有的向量h1，h2，h3…，hTx是输入句子中Tx个单词的表示。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/6750d36bc0a6c7bbb83aac5893e09850.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/0*8P9lgcKBGKM3t1eB.png"/></div></figure><p id="53d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">权重也由前馈神经网络学习，并且使用注释的加权和来生成输出单词yi的上下文向量ci:</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/01da15e2dd9a922e87e476cf079c6664.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/0*i_pbUdawQ04iC0dw.png"/></div></figure><p id="b288" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">解码器:</strong>每个解码器单元有一个输出y1，y2…yn，每个输出在此之前被传递给softmax函数。第一个单元的输出被传递到下一个输入单元，并且通过关注单元创建的相关/单独的上下文向量也被作为输入传递。注意:每个单元都有一个独立的上下文向量和独立的前馈神经网络。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/4ba7787e9e7b2f9984f0825c326a025c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rNvHQZrWY3ISvqGXuAGu5Q.png"/></div></div></figure><p id="1ce1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的图片中，模型将尝试学习它所关注的单词。“它”是两只相依的动物和街道。在后学习中，<strong class="is hj">街</strong>被赋予了很高的权重。</p><p id="a9cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注意力模型的例子:</strong></p><p id="114f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">语言翻译:</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kt"><img src="../Images/d79334dd7f736b68b5ec6e18b56ba544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fr9dtUREseDRy2PuNUOgOQ.png"/></div></div></figure><p id="af42" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">结论:神经网络在训练过程中减少和增加特征的权重，同样注意模型在训练过程中考虑输入词。</p><p id="4777" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我要感谢Sudhanshu揭示了注意力机制这个复杂的话题，我在文章中也提到了很多。</p></div></div>    
</body>
</html>