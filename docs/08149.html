<html>
<head>
<title>Few Shot Learning — A Case Study (3)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">少数镜头学习——案例研究(3)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/few-shot-learning-a-case-study-3-84f6ea3cb322?source=collection_archive---------17-----------------------#2020-07-19">https://medium.com/analytics-vidhya/few-shot-learning-a-case-study-3-84f6ea3cb322?source=collection_archive---------17-----------------------#2020-07-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3038" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae jd" rel="noopener" href="/analytics-vidhya/few-shot-learning-a-case-study-2-805f5642acaf">上一篇文章</a>中，我们通过关系网络深入研究了少数镜头分类。此外，我们分析了图像分类任务的关系网络。在这篇文章中，我将分析文本分类的关系网络。在这里，我将进行大量的实验来评估具有各种<strong class="ih hj">嵌入网络</strong>的关系网络的有效性。</p><p id="3175" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">本文的流程:</strong></p><ol class=""><li id="957c" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">关系网络的修正</li><li id="6c39" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">不同类型的可能嵌入网络</li><li id="61fc" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">结果和分析</li><li id="5e74" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">结论</li></ol><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/933fd967e4e56f22b67fc3adceca9a8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HgXA9v1EsqlrRDaC_iORhQ.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">来源:<a class="ae jd" href="https://medium.com/u/db27b1ee40a3?source=post_page-----3df4f4f9570b----------------------" rel="noopener"> Moosend </a></figcaption></figure><h1 id="5999" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">修订本</h1><p id="a7c6" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">在继续之前，让我们快速修改一下关系网络(更多细节请阅读<a class="ae jd" rel="noopener" href="/analytics-vidhya/few-shot-learning-a-case-study-2-805f5642acaf">之前的博客</a>):</p><ol class=""><li id="2046" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">关系网络包含两个子网络。</li><li id="ad06" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">第一个是嵌入网络，它将提取每个输入的底层表示，而不管它属于哪个类。</li><li id="8dfd" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">嵌入网络将用于从支持和查询数据集提取特征。</li><li id="c9e1" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">并且第二网络将来自支持集的每个嵌入与查询集进行比较，并且将基于该比较给出结果。</li></ol><h1 id="08f5" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">文本嵌入网络</h1><p id="8fb2" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">每当有人听说单词嵌入、word2vec、GLoVe、语言模型(即BERT、RoBERTa等。)是脑海中浮现的关键技术。因此，在本节中，我将使用最先进的方法，即BERT，作为迁移学习和预训练的基础模型，为一些镜头文本分类设计各种嵌入网络。此外，我将从<strong class="ih hj">准确性和计算复杂性</strong>方面分析这些可能的网络。[ <a class="ae jd" href="https://github.com/Maitreyapatel/Few-Shot-Learning-A-Case-Study" rel="noopener ugc nofollow" target="_blank"> GitHub </a> ]</p><p id="ecd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT是最容易获得和最流行的语言模型之一。在BERT的基础架构中，总共有12层，这些层的每一层输出都可以用作字嵌入。可以从BERT中提取的所有不同类型的可能单词嵌入如下图所示:</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/c32847af9a83885cd28b41dca8d9f431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RvEw7wWBWJfosGknuSdyiw.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">图(1):不同类型的可能单词嵌入。资料来源:http://jalammar.github.io/illustrated-bert/</figcaption></figure><p id="f7e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，为了试验不同设置的关系网络，我将使用BERT来设计几个不同的嵌入网络，以找出最适合少量文本分类的嵌入提取方法。</p><ol class=""><li id="fb8a" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated"><strong class="ih hj">仅预训练BERT: </strong>这里将从预训练BERT的最后一层取每个词表示的平均值和最大值池来提取句子嵌入。</li><li id="2ca5" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj">微调BERT本身:</strong>在这种方法中，我们将根据少拍设置和数据集来微调BERT，而不是依赖于方法1中预先训练的BERT。</li></ol><p id="7bd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然我们有用于提取单词嵌入的SOTA方法(BERT ),但是有两个主要问题与之相关。第一，BERT的高计算复杂度。第二，从核心层面修改单词表示的能力。为了避免这些问题，我们可以引入一个顶层，使用它我们可以根据需求修改单词嵌入。因此，我们将执行另外两个实验:</p><ol class=""><li id="972e" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated"><strong class="ih hj">预训练的BERT + BiLSTM: </strong>这里，我们将在预训练的BERT之上应用双向LSTM来修改单词嵌入，使得来自LSTM的单个输出可以从BERT捕获相关信息用于文本分类。</li><li id="ec56" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj">预训练BERT + BiGRU: </strong>这个实验和上面那个一样。然而，我们只是用BiGRU替换了BiLSTM。这里，在一个理想的情况下，GRU应该以较少的可训练参数比LSTM略胜一筹。</li></ol><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es lm"><img src="../Images/8e2368e1022f60d675e5f5d278f35eab.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*AliFkpB4yrciBHFVEcLLFA.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">图(2):基于实验的预训练伯特+ LSTM/GRU的架构设计。</figcaption></figure><h1 id="8658" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">结果和分析</h1><p id="f01b" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">为了执行上述实验，使用了Kaggle <a class="ae jd" href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank">新闻类别数据集</a>。在实验设置中，50%的新闻类别用于训练，20%和30%的类别用于验证和测试目的。此外，这种分析是用5路2镜头分类设置来执行的。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ln"><img src="../Images/938d4458cf7e7bf56d60d83e4b0a8ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lCMF2MfAm_iatoilJkOkkA.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">表(1):不同实验中关系网络的准确性和计算复杂性。</figcaption></figure><p id="69a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上表中，我们可以观察到微调后的BERT的性能远远优于预训练的BERT。此外，在BERT的基础上添加BiLSTM和BiGRU可以大大改善结果。此外，我们可以看到，BERT的计算复杂度最高，其性能不佳的原因是所使用的预训练策略。因此，BERT需要修改它的大量参数。然而，LSTM/GRU从单词嵌入中提取所需的信息，用于少量的文本分类。</p><h1 id="1288" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">结论</h1><p id="1a0f" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">以下是需要记住的几点结论:</p><blockquote class="lo lp lq"><p id="4e0c" class="if ig lr ih b ii ij ik il im in io ip ls ir is it lt iv iw ix lu iz ja jb jc hb bi translated">即使对于少量文本分类，嵌入网络也是关键的组成部分。</p><p id="1a27" class="if ig lr ih b ii ij ik il im in io ip ls ir is it lt iv iw ix lu iz ja jb jc hb bi translated">微调BERT以计算复杂性为代价改善了结果。</p><p id="b237" class="if ig lr ih b ii ij ik il im in io ip ls ir is it lt iv iw ix lu iz ja jb jc hb bi translated">此外，在预训练的BERT之上的基于LSTM/GRU的层显著优于唯一的基于BERT的嵌入网络，并且具有<strong class="ih hj">非常低的计算复杂度</strong>。</p></blockquote></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><p id="df69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae jd" href="https://github.com/Maitreyapatel/Few-Shot-Learning-A-Case-Study" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中提供了所有上述实验的实现以及不同的结果图。</p><p id="94cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我将为语音分类任务实现相同的关系网络。<strong class="ih hj">敬请关注！</strong>订阅<a class="ae jd" href="https://maitreyapatel.github.io/blog-home.html" rel="noopener ugc nofollow" target="_blank">此处</a>获取即将发布的新文章通知，跟上当前的研究趋势。</p></div><div class="ab cl lv lw gp lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="hb hc hd he hf"><h1 id="5e9b" class="ke kf hi bd kg kh mc kj kk kl md kn ko kp me kr ks kt mf kv kw kx mg kz la lb bi translated">参考资料:</h1><ol class=""><li id="a212" class="je jf hi ih b ii lc im ld iq mh iu mi iy mj jc jj jk jl jm bi translated">宋，洪水，杨永信，张丽，向涛，菲利普HS托尔，和蒂莫西m。"学习比较:少量学习的关系网络."在<em class="lr">IEEE计算机视觉和模式识别会议记录</em>中，第1199–1208页。2018.</li><li id="3909" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">伯特:用于语言理解的深度双向转换器的预训练。<em class="lr"> arXiv预印本arXiv:1810.04805 </em> (2018)。</li></ol></div></div>    
</body>
</html>