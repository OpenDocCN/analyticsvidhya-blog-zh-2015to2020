<html>
<head>
<title>Review of DeepGIN: Deep Generative Inpainting Network for Extreme Image Inpainting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepGIN综述:用于极端图像修复的深度生成修复网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/review-of-deepgin-deep-generative-inpainting-network-for-extreme-image-inpainting-de5b191562b0?source=collection_archive---------10-----------------------#2020-09-07">https://medium.com/analytics-vidhya/review-of-deepgin-deep-generative-inpainting-network-for-extreme-image-inpainting-de5b191562b0?source=collection_archive---------10-----------------------#2020-09-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2d53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大家好，ECCV 20 AIM工作坊今年举办了<a class="ae jd" href="https://data.vision.ee.ethz.ch/cvl/aim20/" rel="noopener ugc nofollow" target="_blank">极限图像修复挑战赛</a>(第一届图像修复挑战赛)。我想在这篇文章中分享一篇名为<a class="ae jd" href="https://arxiv.org/abs/2008.07173" rel="noopener ugc nofollow" target="_blank"> DeepGIN </a>的挑战论文。源代码和相关资料可以在他们的github项目页面上找到:【https://github.com/rlct1/DeepGIN T4】</p><h1 id="ad03" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">目标</h1><ul class=""><li id="399a" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc kj kk kl km bi translated">填充图像中缺少的部分，如下图所示。</li></ul><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/314ef184fc469dfb0b9d41f2ef7ec025.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p4Xi5pbpWoUgBWf4BXZnHQ.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">图一。第一行显示不同的遮罩图像。第二和第三行显示使用最新技术和所提出的方法完成的图像。最后一行是地面真实图像。图像修复的难度很大程度上取决于被遮盖区域的大小和形状</figcaption></figure><h1 id="14b6" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">动机</h1><ul class=""><li id="2a0d" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc kj kk kl km bi translated">现有的图像修补方法在野外完成缺失部分时通常会遇到困难，因为它们被训练用于处理一种特定类型的缺失图案(掩模)或者单方面假定被遮蔽区域的形状和/或大小(参见图1中的例子)。缺失的部分越大，任务越困难)</li></ul><h1 id="8f19" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">解决办法</h1><ul class=""><li id="69d2" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc kj kk kl km bi translated">所提出的模型是两阶段网络，即粗重构阶段和精细化阶段。粗略重建阶段负责丢失部分的粗略估计，而精细化阶段负责精细化粗略完成的图像</li></ul><h1 id="29c3" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">贡献</h1><ol class=""><li id="36ec" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc ld kk kl km bi translated">提出一种空间金字塔膨胀(SPD)残差块来处理具有各种形状和大小的不同类型的掩模</li><li id="3446" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc ld kk kl km bi translated">强调自相似性对图像修复的重要性，采用多尺度自关注策略显著改善修复效果(MSSA)</li><li id="3962" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc ld kk kl km bi translated">设计反向投影(BP)策略，用于获得具有生成的图案和参考地面真实图像的更好对准的修补结果</li></ol><h1 id="18b4" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">方法</h1><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lj"><img src="../Images/6b17190d8775d04ed33470f07625f1a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lWtqW9ICX8BS3uuEcOanLA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">图二。提议模型的概述。有两个发生器和两个鉴别器</figcaption></figure><ul class=""><li id="1e2a" class="kc kd hi ih b ii ij im in iq lk iu ll iy lm jc kj kk kl km bi translated">如图2所示，建议的模型包括两个阶段。</li><li id="9b41" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated"><strong class="ih hj">粗重建阶段</strong>:粗生成器<strong class="ih hj"><em class="ln">G1</em></strong>以 中的<strong class="ih hj"> M </strong>和<strong class="ih hj"> I_ <em class="ln">为输入，给出一个粗完整图像<strong class="ih hj"> I_ <em class="ln">粗</em> </strong></em></strong></li><li id="342a" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated"><strong class="ih hj">细化阶段</strong>:细化生成器<strong class="ih hj"><em class="ln">G2</em></strong>被训练用细节和纹理来修饰粗略完成的图像</li><li id="c92e" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated"><strong class="ih hj">有条件的多尺度鉴别器</strong>:两个鉴别器在两个不同的尺度上接受输入，以在两个尺度上促进局部重构图案的更好细节和纹理</li></ul><h1 id="9047" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">空间金字塔膨胀(SPD)残余块</h1><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lo"><img src="../Images/127c5d8558397ab80c5e86f25a2f9174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oy44Dfivb_0_vOftDYlIlQ.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">图3。剩余块的变化。(一)是一个<a class="ae jd" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">标准残块</a>；(b)是简单的扩张残余阻滞；(c)和(d)是具有多个膨胀率的建议的残差块</figcaption></figure><ul class=""><li id="d5fa" class="kc kd hi ih b ii ij im in iq lk iu ll iy lm jc kj kk kl km bi translated">由于掩蔽区域的尺度是随机确定的，作者建议使用多个扩张率来扩大每一层的感受野，从而可以将远距离空间位置给出的信息包括在重建中。图3图示了SPD残差块的设计</li></ul><h1 id="0610" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">多尺度自我关注(MSSA)</h1><ul class=""><li id="a406" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc kj kk kl km bi translated">本文中使用的自我关注区块与<a class="ae jd" href="https://arxiv.org/abs/1711.07971" rel="noopener ugc nofollow" target="_blank">非局部区块</a>完全相同</li><li id="8c65" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated">自注意的主要思想是计算图像本身的自相似性，这对于根据掩蔽图像中剩余的有效像素来修改生成的图案是有用的</li><li id="0809" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated">如图2所示，作者应用MSSA代替单个SA，通过在三个不同的尺度上考虑自相似性来增强完整图像的一致性。为了避免额外的参数，在连接到SA块之前，它们简单地使用标准卷积层来减小信道大小</li></ul><h1 id="acfc" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">反向投影</h1><ul class=""><li id="82e6" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc kj kk kl km bi translated">作者还重新设计了<a class="ae jd" href="https://arxiv.org/abs/1803.02735" rel="noopener ugc nofollow" target="_blank">反向投影策略</a>，如图2中阴影反向投影区域所示。他们学习加权BP残差并将其添加回以更新最终完成的图像，因此生成的模式与参考地面真实图像具有更好的对齐</li></ul><h1 id="d7c3" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">条件多尺度鉴别器</h1><ul class=""><li id="1638" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc kj kk kl km bi translated">两个输入尺度下的两个鉴别器与生成器一起被训练，以模拟填充区域的细节。鉴别器输出一组特征图，并且这些图上的每个值以两种不同的比例表示输入图像中的局部区域。这鼓励了外观和语义的相似性</li></ul><h1 id="4a0d" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">损失函数</h1><p id="c082" class="pw-post-body-paragraph if ig hi ih b ii ke ik il im kf io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">损失函数中有<strong class="ih hj"> <em class="ln">五大项</em> </strong>:</p><ol class=""><li id="1434" class="kc kd hi ih b ii ij im in iq lk iu ll iy lm jc ld kk kl km bi translated"><strong class="ih hj"> <em class="ln"> L1损失</em> </strong>保证逐像素重建精度</li><li id="0fb7" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc ld kk kl km bi translated"><strong class="ih hj"> <em class="ln">对抗性损失</em> </strong>促使完成图像的分布接近真实图像的分布</li><li id="bd8f" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc ld kk kl km bi translated"><a class="ae jd" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="ln">特征感知损失</em> </strong> </a>鼓励每个完成的图像及其参考地面真实图像具有由训练有素的网络计算的相似特征表示，具有良好的泛化能力，如VGG-19</li><li id="c9fb" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc ld kk kl km bi translated"><a class="ae jd" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="ln">风格损失</em> </strong> </a>强调完成的图像与真实图像之间的纹理、颜色等风格相似性</li><li id="48e1" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc ld kk kl km bi translated"><a class="ae jd" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank"/></li></ol><h1 id="8b73" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">实验</h1><ul class=""><li id="e5da" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc kj kk kl km bi translated"><strong class="ih hj">随机掩码生成</strong>:在训练中使用了三种不同类型的掩码(如图1所示。即矩形掩模、自由形式掩模和元胞自动机掩模)。作者将这三种类型的掩模应用于每个训练图像，以实现更稳定的训练</li><li id="b5fc" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated"><strong class="ih hj">两阶段训练</strong>:训练过程分为两个阶段，即热身阶段和主要阶段。他们首先使用<strong class="ih hj"> <em class="ln"> L1损耗</em> </strong>对两台发电机进行10个历元的训练。然后，他们用鉴别器交替训练发生器100个时期</li><li id="6622" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated"><strong class="ih hj">训练数据</strong>:他们在两个数据集上训练提出的模型，比如<a class="ae jd" href="https://github.com/switchablenorms/CelebAMask-HQ" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="ln">CelebA-HQ</em></strong></a><strong class="ih hj"><em class="ln">数据集</em> </strong>(仅针对人脸图像)和<a class="ae jd" href="https://groups.csail.mit.edu/vision/datasets/ADE20K/" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="ln">ade 20k</em></strong></a><strong class="ih hj"><em class="ln">数据集</em> </strong>(一个更通用的数据集，包含建筑物、人物、自然场景等。)</li><li id="dd90" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated"><strong class="ih hj">消融研究</strong>:作者首先提供证据证明他们建议的策略和构建模块的有效性，即SPD剩余模块、MSSA和BP</li></ul><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ls"><img src="../Images/b10c95b7d02e6422adb9f6bb3b586327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D_odvkUmfnOYM5hKEx-9Qg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">表1。CelebA-HQ数据集的消融研究。最好的结果是用粗体字</figcaption></figure><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lt"><img src="../Images/bcd892958309d6fad4252060f3a2a0a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-xNsZ5nccibl-mZA82ngmg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">图4。CelebA-HQ数据集上的建议模型变化的结果</figcaption></figure><ul class=""><li id="726d" class="kc kd hi ih b ii ij im in iq lk iu ll iy lm jc kj kk kl km bi translated">基线被表示为StdResBlk(仅粗略，仅第一阶段)和StdResBlk(用于修补的典型ResNet)，对于它们，所有SA块和BP分支被消除，并且所有SPD残余块被标准残余块代替，如图3所示</li><li id="f24a" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated">从表1的定量结果来看，与StdResBlk-SA(单一SA)相比，MSSA的就业为PSNR带来了1.06 dB的增长。这反映了MSSA对图像修复任务的重要性</li><li id="d1f1" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated">对于定性结果，图4显示了所提出的模型的变化的比较。没有第二个细化阶段，完成的图像缺少面部细节，正如你在第二列的第一个例子中看到的</li><li id="c8ab" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated"><strong class="ih hj">与之前工作的比较</strong>:为了测试提出的模型的泛化能力，作者在两个公开可用的数据集<a class="ae jd" href="https://github.com/NVlabs/ffhq-dataset" rel="noopener ugc nofollow" target="_blank"> FFHQ </a>和<a class="ae jd" href="https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/" rel="noopener ugc nofollow" target="_blank">牛津大楼</a>上将他们的模型与一些最先进的方法进行了比较</li></ul><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lu"><img src="../Images/98ebddf689dbcd1098420aa049319242.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YhzTwPhpnFUHnisvb-N71g.png"/></div></div></figure><ul class=""><li id="e2b8" class="kc kd hi ih b ii ij im in iq lk iu ll iy lm jc kj kk kl km bi translated">对于定量结果，表2显示，在所有实验中，他们提出的模型在逐像素重建精度方面优于其他两种方法(即PSNR、SSIM、L1误差。).在大多数情况下，它们还实现了更好的估计感知质量(即FID和LPIPS)</li></ul><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lv"><img src="../Images/620b3313748efe610cd82731a56f978f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cbj4IR2OU4xBrXa7dmDPCw.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">图5。FFHQ和牛津建筑数据集的定性结果</figcaption></figure><ul class=""><li id="155d" class="kc kd hi ih b ii ij im in iq lk iu ll iy lm jc kj kk kl km bi translated">对于定性结果，在图5中，可以看到深填充v1和v2未能在大矩形遮罩上实现令人满意的视觉质量，如第一列和第四列所示。请注意，作者试图在像素精度和视觉质量之间寻求平衡。为了表明这一点，他们还提供了预测的语义分割结果</li></ul><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lw"><img src="../Images/2fc717001270c19baa1c3dfaeba2e23b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QXXja0PeIWudmo_0djdwiA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">图6。预测的语义分割测试结果的可视化</figcaption></figure><ul class=""><li id="6ee8" class="kc kd hi ih b ii ij im in iq lk iu ll iy lm jc kj kk kl km bi translated">很明显，他们的测试结果在语义上比其他两种方法更接近事实，例如，见前两列的报纸和草坪的交叉点</li><li id="6471" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated">一些额外的测试结果也可以在他们的github项目页面上找到</li></ul><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lx"><img src="../Images/68776b6cc53dd51653a9186fbb8cbd2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h0yRSPxf8cu-wUR9bLMtnA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">图7。AIM极限图像修复挑战赛2020的测试结果</figcaption></figure><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ly"><img src="../Images/10c517152da9e1f998dd060a21b54f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dgA4GvcS1g7FrGLp3GyFjw.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">图8。AIM挑战测试集的更多测试结果</figcaption></figure><h1 id="718b" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">结论</h1><ul class=""><li id="6aa8" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc kj kk kl km bi translated">回想一下，作者在本文中提出了三种主要的图像修复策略，即空间金字塔膨胀(SPD)残差块、多尺度自注意(MSSA)以及反投影(BP)。他们还指出，需要在逐像素重建精度和视觉质量之间取得适当的平衡，以避免产生一些奇怪的图案</li></ul><p id="d955" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强烈建议感兴趣的读者阅读<a class="ae jd" href="https://arxiv.org/abs/2008.07173" rel="noopener ugc nofollow" target="_blank">论文</a>并访问他们的<a class="ae jd" href="https://github.com/rlct1/DeepGIN" rel="noopener ugc nofollow" target="_blank"> github项目页面</a>了解更多细节。</p><h1 id="be59" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">个人想法</h1><ol class=""><li id="283f" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc ld kk kl km bi translated">从他们的烧蚀研究来看，似乎用于图像修复的一级网络也是可能的，并且应该对此做进一步的研究</li><li id="96d2" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc ld kk kl km bi translated">高PSNR通常图像模糊，如何解决这个问题对赢得挑战至关重要</li></ol><h1 id="d520" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">参考</h1><ul class=""><li id="a811" class="kc kd hi ih b ii ke im kf iq kg iu kh iy ki jc kj kk kl km bi translated">https://arxiv.org/abs/2008.07173<a class="ae jd" href="https://arxiv.org/abs/2008.07173" rel="noopener ugc nofollow" target="_blank">迪普金</a></li><li id="4ba4" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated"><a class="ae jd" href="https://github.com/rlct1/DeepGIN" rel="noopener ugc nofollow" target="_blank">https://github.com/rlct1/DeepGIN</a></li><li id="e5f8" class="kc kd hi ih b ii le im lf iq lg iu lh iy li jc kj kk kl km bi translated"><a class="ae jd" href="https://data.vision.ee.ethz.ch/cvl/aim20/" rel="noopener ugc nofollow" target="_blank">https://data.vision.ee.ethz.ch/cvl/aim20/</a></li></ul></div></div>    
</body>
</html>