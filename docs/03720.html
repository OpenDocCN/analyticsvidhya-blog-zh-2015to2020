<html>
<head>
<title>Knowledge Distillation for Convolutional Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积网络的知识蒸馏</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/knowledge-distillation-for-convolutional-networks-e73682c611e8?source=collection_archive---------6-----------------------#2020-02-15">https://medium.com/analytics-vidhya/knowledge-distillation-for-convolutional-networks-e73682c611e8?source=collection_archive---------6-----------------------#2020-02-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/de45bbc1d3da66900783f692ce5a1c7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Dqfvqyp_YAiYNZte"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">帕特里克·托马索在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h2 id="7877" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">“知识蒸馏”到底是什么？</h2><p id="31fa" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">最近，神经网络被证明是学习各种任务的好方法。然而，这些神经网络越来越深，参数数量增加到数百万甚至数十亿，这将这些网络的使用限制在高计算设备上。随着智能手表、增强现实眼镜和各种其他设备等智能移动设备的兴起，当前的需求是拥有参数数量更少的网络。</p><p id="0d9d" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">知识蒸馏就是这样一种技术，它可以传递像雷斯内特、VGG等预先训练好的大型模型的知识。到更小的网络。正如<em class="kt">Geoffrey hint on 2015</em>【1】的论文<em class="kt">在神经网络</em>中提到的，将知识从教师模型转移到学生模型的一种“显而易见”的方式是通过对学生模型的训练过程使用“软目标”。</p><h2 id="72f5" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">好吧，我对它的用途深信不疑，但是它到底是怎么做的呢？</h2><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/17b91aa1372d02dca2cc0989c923e1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/0*d_zfSbWCKW8LHikh.jpg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">师生模式概述[2]</figcaption></figure><p id="3310" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">如上图所示，损失函数使用教师和学生的类别概率的KL散度以及实际标签的损失。</p><p id="48ad" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">现在让我们来看看用于知识提炼的损失函数。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/d1743a612a0a5645e05da43fda045505.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/1*HKe3RPu2VJrSzPI7Ynf3nw.gif"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">损失函数</figcaption></figure><p id="2b2d" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">我们来分析一下。<em class="kt"> m </em>是批量大小。<em class="kt"> Dₖₗ </em>是<em class="kt"> P </em>(来自教师网络的“软标签”)和<em class="kt"> Q </em>(来自学生网络的softmax分数)的输出之间的KL散度。<em class="kt"> T </em>这里是温度软化概率分布；α <em class="kt"> </em>是根据数据[1]训练w.r.t硬目标时，教师指导的相对重要性。</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h2 id="3be7" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">理论讲够了，让我们来看一些代码。</h2><p id="8c07" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">让我们从一些基本的东西开始。导入必要的库</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="8433" class="iv iw hi li b fi lm ln l lo lp">import time<br/>import copy<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import torch<br/>import torchvision<br/>from torchvision import datasets, transforms<br/>from torchsummary import summary<br/>from torch.optim import lr_scheduler<br/>import torch.nn.functional as F<br/>import torch.nn as nn<br/>import torchvision.models as models<br/>from torch import nn, optim</span></pre><p id="d4ef" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">现在让我们导入数据集。我使用的是CIFAR10数据集。您可以尝试使用任何数据集进行知识提炼。我将图像大小调整为(224，224)，因为预训练模型Resnet是在ImageNet上训练的，ImageNet的图像大小为(224，224)。</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="f06f" class="iv iw hi li b fi lm ln l lo lp">transform = transforms.Compose([transforms.Resize((224,224)),<br/>                                transforms.ToTensor(),<br/>                                transforms.Normalize([0.485,0.456,  <br/>                                0.406], [0.229, 0.224, 0.225])])</span><span id="40ef" class="iv iw hi li b fi lq ln l lo lp">trainset = datasets.CIFAR10(‘/content/train/’, download=True, train=True, transform=transform)<br/>valset = datasets.CIFAR10(‘/content/val/’, download=True, train=False, transform=transform)</span><span id="218c" class="iv iw hi li b fi lq ln l lo lp">trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)<br/>valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)</span><span id="2688" class="iv iw hi li b fi lq ln l lo lp">len_trainset = len(trainset)<br/>len_valset = len(valset)</span><span id="5a1a" class="iv iw hi li b fi lq ln l lo lp">classes = (‘plane’, ‘car’, ‘bird’, ‘cat’,‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’)</span><span id="9dc2" class="iv iw hi li b fi lq ln l lo lp">device = torch.device(“cuda:0” if torch.cuda.is_available() else “cpu”)</span></pre><p id="1911" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">作为健全检查的形状的图像和标签</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="37f1" class="iv iw hi li b fi lm ln l lo lp">dataiter = iter(trainloader)<br/>images, labels = dataiter.next()<br/>print(images.shape)<br/>print(labels.shape)</span></pre><p id="4a42" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">现在，让我们定义教师网络，即ResNet50，并冻结其内层。</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="af58" class="iv iw hi li b fi lm ln l lo lp">resnet = models.resnet50(pretrained=True)<br/>for param in resnet.parameters():<br/>   param.requires_grad = False</span><span id="c3f4" class="iv iw hi li b fi lq ln l lo lp">num_ftrs = resnet.fc.in_features<br/>resnet.fc = nn.Linear(num_ftrs, 10)<br/>resnet = resnet.to(device)<br/>criterion = nn.CrossEntropyLoss()<br/>optimizer = optim.Adam(resnet.fc.parameters())</span></pre><p id="41e5" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">太好了！让我们来训练这个预先训练好的模型。</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="7d4d" class="iv iw hi li b fi lm ln l lo lp">def train_and_evaluate(model, trainloader, valloader, criterion, optimizer, len_trainset, len_valset, num_epochs=25):<br/>   model.train()<br/>   best_model_wts = copy.deepcopy(model.state_dict())<br/>   best_acc = 0.0<br/>   for epoch in range(num_epochs):<br/>      model.train()<br/>      print(‘Epoch {}/{}’.format(epoch, num_epochs — 1))<br/>      print(‘-’ * 10)<br/>      running_loss = 0.0<br/>      running_corrects = 0<br/>      for inputs, labels in trainloader:<br/>         inputs = inputs.to(device)<br/>         labels = labels.to(device)<br/>         optimizer.zero_grad()<br/>         outputs = model(inputs)<br/>         loss = criterion(outputs, labels)<br/>         _, preds = torch.max(outputs, 1)<br/>         loss.backward() <br/>         optimizer.step()  <br/>         running_loss += loss.item() * inputs.size(0)<br/>         running_corrects += torch.sum(preds == labels.data)<br/>      epoch_loss = running_loss / len_trainset<br/>      epoch_acc = running_corrects.double() / len_trainset<br/>      print(‘ Train Loss: {:.4f} Acc: {:.4f}’.format(epoch_loss,<br/>             epoch_acc)) <br/>         <br/>      model.eval()<br/>      running_loss_val = 0.0 <br/>      running_corrects_val = 0<br/>      for inputs, labels in valloader:<br/>         inputs = inputs.to(device)<br/>         labels = labels.to(device)<br/>         outputs = model(inputs) <br/>         loss = criterion(outputs,labels)<br/>         _, preds = torch.max(outputs, 1)<br/>         running_loss_val += loss.item() * inputs.size(0)<br/>         running_corrects_val += torch.sum(preds == labels.data)<br/>      <br/>      epoch_loss_val = running_loss_val / len_valset<br/>      epoch_acc_val = running_corrects_val.double() / len_valset<br/>      <br/>      if epoch_acc_val &gt; best_acc:<br/>         best_acc = epoch_acc_val<br/>         best_model_wts = copy.deepcopy(model.state_dict())<br/>      <br/>      print(‘ Val Loss: {:.4f} Acc: {:.4f}’.format(epoch_loss_val,<br/>             epoch_acc_val))<br/>      <br/>      print()<br/>      print(‘Best val Acc: {:4f}’.format(best_acc))<br/>      model.load_state_dict(best_model_wts)</span><span id="d5b7" class="iv iw hi li b fi lq ln l lo lp">return model</span></pre><p id="6605" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">现在运行该函数来训练ResNet。</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="6808" class="iv iw hi li b fi lm ln l lo lp">resnet_teacher = train_and_evaluate(resnet,trainloader,<br/>                                   valloader,criterion,optimizer_ft,<br/>                                   len_trainset,len_valset,10)</span></pre><p id="66fb" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">太好了！我们的工作已经完成了一半。现在，让我们继续定义我们的学生网络，这将从我们刚刚培训的教师网络中学习。</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="659e" class="iv iw hi li b fi lm ln l lo lp">class Net(nn.Module):</span><span id="43bf" class="iv iw hi li b fi lq ln l lo lp">“””<br/>   This will be your student network that will learn from the <br/>   teacher network in our case resnet50.<br/>   “””<br/>   def __init__(self):<br/>      super(Net, self).__init__()<br/>      self.layer1 = nn.Sequential(<br/>         nn.Conv2d(3, 64, kernel_size = (3,3), stride = (1,1), <br/>         padding = (1,1)),<br/>         nn.ReLU(inplace=True),<br/>         nn.Conv2d(64, 64, kernel_size = (3,3), stride = (1,1), <br/>         padding = (1,1)),<br/>         nn.ReLU(inplace=True),<br/>         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, <br/>         dilation=1, ceil_mode=False)<br/>      )<br/>      self.layer2 = nn.Sequential(<br/>         nn.Conv2d(64, 128, kernel_size = (3,3), stride = (1,1), <br/>         padding = (1,1)),<br/>         nn.ReLU(inplace=True),<br/>         nn.Conv2d(128, 128, kernel_size = (3,3), stride = (1,1), <br/>         padding = (1,1)),<br/>         nn.ReLU(inplace=True),<br/>         nn.MaxPool2d(kernel_size=2, stride=2, padding=0, <br/>         dilation=1, ceil_mode=False)<br/>      )<br/>      self.pool1 = nn.AdaptiveAvgPool2d(output_size=(1,1))<br/>      self.fc1 = nn.Linear(128, 32)<br/>      self.fc2 = nn.Linear(32, 10)<br/>      self.dropout_rate = 0.5<br/>   <br/>   def forward(self, x):<br/>      x = self.layer1(x)<br/>      x = self.layer2(x)<br/>      x = self.pool1(x)<br/>      x = x.view(x.size(0), -1)<br/>      x = self.fc1(x)<br/>      x = self.fc2(x)<br/>   return x</span><span id="7085" class="iv iw hi li b fi lq ln l lo lp">net = Net().to(device)</span></pre><p id="15c2" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">再次对网络的输出进行健全性检查。</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="c8a8" class="iv iw hi li b fi lm ln l lo lp">dataiter = iter(trainloader)<br/>images, labels = dataiter.next()<br/>out = net(images.cuda())<br/>print(out.shape)</span></pre><p id="a058" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">好吧！让我们定义我在开始描述的损失函数和一个辅助函数。</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="6c76" class="iv iw hi li b fi lm ln l lo lp">def loss_kd(outputs, labels, teacher_outputs, temparature, alpha):<br/>   KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/temparature, <br/>             dim=1),F.softmax(teacher_outputs/temparature,dim=1)) * <br/>             (alpha * temparature * temparature) + <br/>             F.cross_entropy(outputs, labels) * (1. — alpha)<br/>   return KD_loss</span><span id="bde7" class="iv iw hi li b fi lq ln l lo lp">def get_outputs(model, dataloader):<br/>   '''<br/>   Used to get the output of the teacher network<br/>   '''<br/>   outputs = []<br/>   for inputs, labels in dataloader:<br/>      inputs_batch, labels_batch = inputs.cuda(), labels.cuda()<br/>      output_batch = model(inputs_batch).data.cpu().numpy()<br/>      outputs.append(output_batch)<br/>   return outputs</span></pre><p id="ea63" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">现在，进入整个事情的主要训练循环。</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="fa52" class="iv iw hi li b fi lm ln l lo lp">def train_kd(model,teacher_out, optimizer, loss_kd, dataloader, temparature, alpha):<br/>   model.train()<br/>   running_loss = 0.0<br/>   running_corrects = 0<br/>   for i,(images, labels) in enumerate(dataloader):<br/>      inputs = images.to(device)<br/>      labels = labels.to(device)<br/>      optimizer.zero_grad()<br/>      outputs = model(inputs)<br/>      outputs_teacher = torch.from_numpy(teacher_out[i]).to(device)<br/>      loss = loss_kd(outputs,labels,outputs_teacher,temparature, <br/>                     alpha)<br/>      _, preds = torch.max(outputs, 1)<br/>      loss.backward()<br/>      optimizer.step()<br/>      running_loss += loss.item() * inputs.size(0)<br/>      running_corrects += torch.sum(preds == labels.data)<br/>   <br/>   epoch_loss = running_loss / len(trainset)<br/>   epoch_acc = running_corrects.double() / len(trainset)<br/>   print(‘ Train Loss: {:.4f} Acc: {:.4f}’.format(epoch_loss, <br/>          epoch_acc))</span><span id="aec6" class="iv iw hi li b fi lq ln l lo lp">def eval_kd(model,teacher_out, optimizer, loss_kd, dataloader, temparature, alpha):<br/>   model.eval()<br/>   running_loss = 0.0<br/>   running_corrects = 0<br/>   for i,(images, labels) in enumerate(dataloader):<br/>      inputs = images.to(device)<br/>      labels = labels.to(device)<br/>      outputs = model(inputs)<br/>      outputs_teacher = torch.from_numpy(teacher_out[i]).cuda()<br/>      loss = loss_kd(outputs,labels,outputs_teacher,temparature, <br/>                     alpha)<br/>      _, preds = torch.max(outputs, 1)<br/>      running_loss += loss.item() * inputs.size(0)<br/>      running_corrects += torch.sum(preds == labels.data)<br/>   epoch_loss = running_loss / len(valset)<br/>   epoch_acc = running_corrects.double() / len(valset)<br/>   print(‘ Val Loss: {:.4f} Acc: {:.4f}’.format(epoch_loss,<br/>          epoch_acc))<br/>   return epoch_acc</span><span id="5f58" class="iv iw hi li b fi lq ln l lo lp">def train_and_evaluate_kd(model, teacher_model, optimizer, loss_kd, trainloader, valloader, temparature, alpha, num_epochs=25):<br/>   teacher_model.eval()<br/>   best_model_wts = copy.deepcopy(model.state_dict())<br/>   outputs_teacher_train = get_outputs(teacher_model, trainloader)<br/>   outputs_teacher_val = get_outputs(teacher_model, valloader)<br/>   print(“Teacher’s outputs are computed now starting the training <br/>         process-”)<br/>   best_acc = 0.0<br/>   for epoch in range(num_epochs):<br/>      print(‘Epoch {}/{}’.format(epoch, num_epochs — 1))<br/>      print(‘-’ * 10)<br/>      <br/>      # Training the student with the soft labes as the outputs <br/>      from the teacher and using the loss_kd function<br/>      <br/>      train_kd(model, outputs_teacher_train, <br/>               optim.Adam(net.parameters()),loss_kd,trainloader, <br/>               temparature, alpha)<br/>     <br/>      # Evaluating the student network</span><span id="cfb6" class="iv iw hi li b fi lq ln l lo lp">epoch_acc_val = eval_kd(model, outputs_teacher_val, <br/>                          optim.Adam(net.parameters()), loss_kd, <br/>                          valloader, temparature, alpha)<br/>      if epoch_acc_val &gt; best_acc:<br/>         best_acc = epoch_acc_val<br/>         best_model_wts = copy.deepcopy(model.state_dict())<br/>         print(‘Best val Acc: {:4f}’.format(best_acc))<br/>         model.load_state_dict(best_model_wts)<br/>   return model</span></pre><p id="6eba" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">瞧啊。！！！你完了。最后要做的就是运行函数来训练你的学生网络。:)</p><pre class="kv kw kx ky fd lh li lj lk aw ll bi"><span id="68e8" class="iv iw hi li b fi lm ln l lo lp">stud=train_and_evaluate_kd(net,resnet_teacher,<br/>optim.Adam(net.parameters()),loss_kd,trainloader,valloader,1,0.5,20)</span></pre><p id="e130" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">PS:我已经把温度设置为1，alpha设置为0.5。这些是你可以调整的超参数。</p><p id="7437" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">这篇关于卷积网络的知识蒸馏的文章到此结束。希望您喜欢您刚刚阅读的内容，感谢您的宝贵时间。</p><p id="ea1a" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi">✌️</p><h2 id="a874" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考</h2><p id="2982" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">辛顿、杰弗里、奥里奥尔·维尼亚尔斯和杰夫·迪恩。"从神经网络中提取知识."arXiv:1503.02531 (2015)。</p><p id="258b" class="pw-post-body-paragraph jt ju hi jv b jw ko jy jz ka kp kc kd jg kq kf kg jk kr ki kj jo ks kl km kn hb bi translated">[2] Cho，Jungchan和Lee，Minsik。“使用组稀疏性和知识蒸馏为嵌入式智能传感器系统构建紧凑的卷积神经网络”【https://doi.org/10.3390/s19194307<a class="ae iu" href="https://doi.org/10.3390/s19194307" rel="noopener ugc nofollow" target="_blank"/>(2019)</p></div></div>    
</body>
</html>