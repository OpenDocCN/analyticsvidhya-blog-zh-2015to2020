# 单词袋(蝴蝶结)

> 原文：<https://medium.com/analytics-vidhya/bag-of-words-bow-984fe7adc79d?source=collection_archive---------20----------------------->

![](img/f6553b2b31a8579e604bf099e54a357f.png)

由[胡萨姆·阿卜德](https://unsplash.com/@hussam3bd?utm_source=medium&utm_medium=referral)在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 拍摄的照片

# 鞠躬的步骤包括:

## 1.d 维字典的构造

*   这里，我们创建了文档语料库中所有唯一单词的数组。
*   让我们拥有独一无二的词汇。
*   每一个独特的单词都是一个维度。

> ***注 1:*** *一个文本，可以是一个单词，也可以是一个句子，在自然语言处理中称为文档。*
> 
> ***注 2:*** *这类文档的集合称为文档语料库。*

## 1.1 示例:

假设文档语料库中有两个文档，如下所示:

1.  这辆车开起来很好，但是很贵。
2.  这辆车不贵，开起来很好。

我们创建文档语料库中所有唯一单词的字典(或数组),如下所示:

`[This, car, drives, good, and, is, expensive, not]`

## 2.为每个文档创建向量

*   对于每个文档，我们创建一个 d 维向量。
*   向量的每个维度都对应一个唯一的单词。
*   每个维度的值等于在给定文档中对应于该维度的唯一单词的出现次数。

> ***注 3:*** *一般弓会产生稀疏矢量。在稀疏向量中，大多数维度的值为 0。*

## 2.1 示例:

假设向量 v1 和 v2 分别对应于文档 1 和文档 2。那么这些向量表示为:

v1 = `[1 1 1 1 1 1 1 0]`

v2 = `[1 1 1 1 1 1 1 1]`

## 3.计算向量之间的距离

*   欧几里得距离是在向量之间找到的。
*   所考虑的向量之间较小的欧几里德距离对应于它们相应的文档之间较大的相似性。
*   另一方面，所考虑的向量之间较大的欧几里德距离对应于它们对应的文档之间较小的相似性。

## 3.1 示例:

我们计算 v1 和 v2 之间的欧几里德距离为:

|v1-v2| = 1

# 二进制弓-弓的变体

*   这里，向量的维数的值是 0 或 1。
*   如果对应于该维度的唯一单词出现至少一次，则该值为 1，否则为 0。
*   二进制弓也被称为布尔弓。

# 通过 Sklearn 实现 BoW

使用示例通过 sklearn 实现单词包

在上面的输出中，我们可以看到每个唯一单词在每个文档中出现的次数。

# 结论:

*   单词包可以被认为是对向量之间的不同单词进行计数。
*   当单词之间有细微差别时，单词袋就不好用了。
*   那就是说 BoW 不考虑词的语义。例如，单词“美味”和“可口”是同义词，但是弓认为它们是不同的。
*   单词包包含许多停用词(这些词很琐碎)。

> [在 GitHub 上关注我](https://github.com/deveshSingh06)
> 
> [鞠广大笔记本](https://github.com/deveshSingh06/Natural_Language_Processing/blob/master/1.%20Bag%20Of%20Words.ipynb)