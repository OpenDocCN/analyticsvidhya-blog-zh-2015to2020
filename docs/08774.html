<html>
<head>
<title>Multi linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多元线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-linear-regression-6ea7a16c9fe7?source=collection_archive---------17-----------------------#2020-08-12">https://medium.com/analytics-vidhya/multi-linear-regression-6ea7a16c9fe7?source=collection_archive---------17-----------------------#2020-08-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="5eb4" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">解码:</h2><div class=""/><div class=""><h2 id="3f8b" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">手工计算</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es jg"><img src="../Images/d26a821d60a17fd0f2888f6b7e5ea00a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Y66EEkvkzT1cprXbc-lQrg.gif"/></div></figure><p id="5979" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">多元线性回归(MLR)，也简称为多元回归，是一种使用几个解释变量来预测响应变量结果的统计技术。这些变量本质上既可以是分类的，也可以是数字的。</p><p id="0d93" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hs">请注意:</strong>分类值应转换为序数尺度或名义尺度，为类别的每个组分配权重。该公式将考虑分配给每个类别的权重。</p><p id="58f2" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">多元回归是线性回归的延伸，只使用一个解释变量。结果也是一个线性方程，然而现在起作用的变量来自许多维度。多元线性回归也是使用2次、3次或更多次的多项式模型的基础模型。</p><p id="7583" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果你想了解线性回归的计算。点击这里查看这篇文章。</p><div class="kk kl ez fb km kn"><a rel="noopener follow" target="_blank" href="/@nishigandha.sharma.90/decoding-simple-linear-regression-b3f5528e54b7"><div class="ko ab dw"><div class="kp ab kq cl cj kr"><h2 class="bd hs fi z dy ks ea eb kt ed ef hr bi translated">解码:简单线性回归</h2><div class="ku l"><h3 class="bd b fi z dy ks ea eb kt ed ef dx translated">公式和计算</h3></div><div class="kv l"><p class="bd b fp z dy ks ea eb kt ed ef dx translated">medium.com</p></div></div><div class="kw l"><div class="kx l ky kz la kw lb jm kn"/></div></div></a></div><p id="c774" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc"> 𝑦 = b₀ + b₁X₁ + ⋯ + bᵣXᵣ + 𝜀.</em></p><p id="5928" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">b0 —常数/ y轴截距</p><p id="23c4" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">b1、b2-每个变量的系数</p><p id="4c04" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">X1，X2 —预测值</p><p id="6547" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">𝜀 —错误率—这是一个很小的可以忽略的值，也称为ε值。对于这种计算，我们将不考虑误差率</p><p id="9345" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们采用以下虚拟数据进行计算:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ld"><img src="../Images/58e3a4adea4138983f9a728d1af14af7.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*URT95Px6J5GgAseWVZ8HIA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">正在考虑的数据</figcaption></figure><p id="6443" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这里X1和X2是X预测值，y是因变量。根据上面给出的多直线公式，我们需要计算b0、b1和b2。让我们先看看b0的公式。</p><p id="91d5" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc">B0 =ȳ—B1 *</em><strong class="jq hs"><em class="lc"/></strong><em class="lc">x̄1—B2 *</em><strong class="jq hs"><em class="lc"/></strong><em class="lc">x̄2</em></p><p id="d9f5" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如你所见，要计算b0，我们需要先计算b1和b2。让我们看看公式:</p><p id="1212" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc">B1 =(σx2 _ sq)(σx1 y)—(σx1 x2)(σx2 y)/(σx1 _ sq)(σx2 _ sq)—(σx1 x2)* * 2</em></p><p id="2937" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc">B2 =(σx1 _ sq)(σx2 y)—(σx1 x2)(σx1 y)/(σx1 _ sq)(σx2 _ sq)—(σx1 x2)* * 2</em></p><p id="dfd4" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这看起来绝对是一个可怕的公式，但是如果你仔细观察，b1和b2的分母是一样的，分子是两个变量x1和x2以及y的叉积。</p><p id="7c26" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">值得注意的是，这里x1和x2的值与我们的预测值X1和X2不同，它是预测值的计算值。在我们找到b1和b2之前，我们将计算x1和x2的以下值，以便我们可以计算b1和b2，然后是b0:</p><p id="a5a1" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc">(σXi _ sq)</em></p><p id="34ae" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc">(σXi y)</em></p><p id="7e78" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc">(σx1 x2)</em></p><p id="fe5d" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这里“I”代表x的值，比如变量1或变量2，N是记录的数量，在这种情况下是10。现在我们可以看看计算系数所需的每个变量的公式。</p><p id="388a" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc">(σXi _ sq)=(σXi2)——(σXi)* * 2/N</em></p><p id="6c8c" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc">(σXi y)=(σXi y)——((σXi)(σy))/N</em></p><p id="2e12" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc">(σX1 x2)=(σX1 x2)——((σX1)(σX2))/N</em></p><p id="9c7b" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">看起来我们又有了3个固定的公式，但是不要担心，让我们一步一步来计算表中需要的值。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es li"><img src="../Images/7862922a56242aeb711352a79c1fdcdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*7ZrL2qkDMseRaQ6HhhH8pg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">使用额外的必需列计算的原始数据</figcaption></figure><p id="29ef" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">很好，现在我们有了所有需要的值，当在上述公式中进行估算时，将得到以下结果:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lj"><img src="../Images/a175e16f8d95e163f802367078117203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*33EofRVZ3t_dHcQEtg8gdg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">系数所需的计算</figcaption></figure><p id="8b12" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在是时候计算b1、b2和b0了:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lj"><img src="../Images/531801b6aae03f41ecc7e7d964a9ec05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Kb6_w_qmc7LX30ENoQHidQ.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">系数和y轴截距计算</figcaption></figure><p id="8bdf" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们现在有了多线性线的方程:</p><p id="ed45" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc"> y = b0 + b1*X1 +b2*X2 </em></p><p id="fafe" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="lc">y =(-0.72)+0.02(X1)+0.38(X2)</em></p><p id="4c7a" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们尝试计算一个新值，并使用Sklearn的库进行比较:</p><p id="4490" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">假设X1 = 5，X2 = 6:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es lk"><img src="../Images/ceebbb1fbd2cd1f64403120b8873df77.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*pEBhBe9hr6Pb7c3aLe94Pw.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">当X1 = 5且X2 = 6时的y值</figcaption></figure><p id="f6ed" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在和Sklearn的线性回归比较。</p><p id="0694" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hs">注意:</strong> Sklearn拥有计算简单和多元线性回归的相同库。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es ll"><img src="../Images/45b41dd2920b975a48bd1fced1061e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*cFMGuks_OSz67BkK5b2dyg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">Sklearn库中的线性回归</figcaption></figure><p id="b423" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">耶！！！对于内置的线性回归函数，我们也得到了完全相同的结果。</p><p id="208d" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，我们可以断定我们的计算是正确的。</p><p id="d2c4" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">希望你们都更清楚如何在后端计算多元线性回归模型。</p><p id="0ed8" class="pw-post-body-paragraph jo jp hi jq b jr js is jt ju jv iv jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">欢迎任何反馈。如果你今天学到了新东西，请鼓掌！</p></div></div>    
</body>
</html>