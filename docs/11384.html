<html>
<head>
<title>Self-attention and the Non-local Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自我关注与非局域网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/self-attention-and-non-local-network-559349fe0662?source=collection_archive---------5-----------------------#2020-12-01">https://medium.com/analytics-vidhya/self-attention-and-non-local-network-559349fe0662?source=collection_archive---------5-----------------------#2020-12-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ih ii ij ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es ig"><img src="../Images/c79aebc5e66af186d8a36b21f95837a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kZLuEKm7gfOJF3enMtSZA.jpeg"/></div></div><figcaption class="ir is et er es it iu bd b be z dx translated">图片来源:<a class="ae iv" href="https://pxhere.com/en/photo/1063830" rel="noopener ugc nofollow" target="_blank">https://pxhere.com/en/photo/1063830</a></figcaption></figure><p id="4c45" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">在<a class="ae iv" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" rel="noopener ugc nofollow" target="_blank">变形金刚</a>的背景下，自我关注这个概念可能已经被讨论了一百万次。一方面，Transformer的提出通过使用注意机制解决了建模长循环的问题。另一方面，变形金刚的大多数<a class="ae iv" rel="noopener" href="/lsc-psd/introduction-of-self-attention-layer-in-transformer-fc7bff63f3bc">解释都是基于时间域，而不是空间域。</a></p><p id="432b" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">论文<a class="ae iv" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html" rel="noopener ugc nofollow" target="_blank">非局部神经网络</a>将自我关注概念扩展到空间域，以模拟图像的非局部属性，并展示了如何将这一概念用于视频分类。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="d1d7" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">先来看自我关注。</p><p id="68b4" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这里的<a class="ae iv" rel="noopener" href="/lsc-psd/introduction-of-self-attention-layer-in-transformer-fc7bff63f3bc">非常清楚地解释了变压器的工作原理。对于句子中的每个单词，提取3个特征的元组:(K=key，Q=query，V=value)。然后，该元组被传递到softmax层，以提取相应的注意力:</a></p><figure class="kc kd ke kf fd ik er es paragraph-image"><div class="er es kb"><img src="../Images/497d2b1bd8dab425875837737d09fd38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*W802-MrKQkmbhevwj5M-yg.png"/></div><figcaption class="ir is et er es it iu bd b be z dx translated">图片取自变压器原纸</figcaption></figure><p id="22d8" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">由等式表示，它被写成如下:</p><figure class="kc kd ke kf fd ik er es paragraph-image"><div class="er es kg"><img src="../Images/3714cec4e5d394bb4524708dab5745aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*rPRpc6nN1i6mvZ9Jsh-FJA.png"/></div></figure><p id="b2c9" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这种表示在NLP领域非常简单。问题是，有没有办法从另一个角度看问题，例如，在图像/视频分析中？</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="7fd0" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">由于Q、K、V都是基于输入x的表示，因此可以将上述注意力矩阵重新表述如下:</p><figure class="kc kd ke kf fd ik er es paragraph-image"><div class="er es kh"><img src="../Images/e58e01badb4d84a7159a69e6bc758b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*5T5OE7NYS-sp0mPIPppIhg.png"/></div><figcaption class="ir is et er es it iu bd b be z dx translated">注意y是沿某个轴的softmax计算</figcaption></figure><p id="baf4" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">y可以很容易地解释为嵌入式高斯域中非线性函数的核表示。因此，我们可以将y写成如下:</p><figure class="kc kd ke kf fd ik er es paragraph-image"><div class="er es ki"><img src="../Images/8c5ba68b56817aca2624dd8fcf129cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*D2NXKHLMNLZY26hnjUH2iQ.png"/></div></figure><p id="5f2e" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">也就是论文中的等式(1)。使用这个公式，可以有多个内核表示(或者本文中所谓的实例化),如高斯、点积或者使用Relu的连接。</p><p id="c4be" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这个公式可以很容易地与著名的2005年CVPR论文中的<a class="ae iv" href="https://ieeexplore.ieee.org/abstract/document/1467423" rel="noopener ugc nofollow" target="_blank">离散非局部均值去噪滤波器联系起来，这就是为什么这个模型被称为非局部网络。</a></p><p id="1bc3" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">非本地网络基本遵循自关注设计，如下图:</p><figure class="kc kd ke kf fd ik er es paragraph-image"><div class="er es kj"><img src="../Images/833ad347540c56892aeac5e46c263a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*FePkcctz0n_-22UqJfFFVA.png"/></div><figcaption class="ir is et er es it iu bd b be z dx translated">图片来自原非本地网文</figcaption></figure><p id="9f11" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这是同一帧内以及不同帧之间的像素之间的顶部响应的图示:</p><figure class="kc kd ke kf fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es kk"><img src="../Images/068e5206a48aaa297399a7f46d63a4fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QNnRhsr0FnbLQyHPDQ0okA.png"/></div></div><figcaption class="ir is et er es it iu bd b be z dx translated">图片来自原非本地网文。请注意跨多个帧的足球对象之间的高度非局部响应。</figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="6d9c" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">总之，论文<a class="ae iv" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html" rel="noopener ugc nofollow" target="_blank">非局部神经网络</a>提供了一个有趣的视角，将最初的softmax注意力重新表述为非线性模型的核心表示，并建立了与传统非局部均值滤波器的联系。</p></div></div>    
</body>
</html>