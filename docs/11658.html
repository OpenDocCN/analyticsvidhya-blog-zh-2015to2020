<html>
<head>
<title>Weakly Supervised Learning for Object Localization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于目标定位的弱监督学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/weakly-supervised-learning-for-object-localization-4b73d4f4f4a6?source=collection_archive---------9-----------------------#2020-12-13">https://medium.com/analytics-vidhya/weakly-supervised-learning-for-object-localization-4b73d4f4f4a6?source=collection_archive---------9-----------------------#2020-12-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/c00a56e8ed5bd15f4182d6eef26c02f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*m9R4vdP4vUEUVcdf.png"/></div></figure><p id="58ed" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">监督学习技术需要大量高质量的标注作为标签。对于像分割这样的任务，创建注释的时间和成本通常高于分类这样的任务。为了克服这些限制，一个低成本的选择是利用大量收集的低质量注释，即使用弱监督学习。</p><p id="b78e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在本文中，我们将重点关注将 WSL 技术应用于基于图像的数据，确切地说，是用于对象定位的 WSL。</p><h1 id="37d5" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">目标定位</h1><p id="d26f" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">对象定位是指识别图像中一个或多个对象的位置，并围绕它们的范围绘制边界框。对象检测将图像的定位与图像中一个或多个对象的分类相结合。</p><p id="eccf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对象定位定位图像中对象的存在，并用边界框突出显示它们的位置。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kn"><img src="../Images/1d1e698a687d194db142ba1180148d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Yu0RFxQYDcF1htcn.png"/></div></div></figure><blockquote class="kw kx ky"><p id="cff3" class="im in kz io b ip iq ir is it iu iv iw la iy iz ja lb jc jd je lc jg jh ji jj hb bi translated"><strong class="io hj"> <em class="hi">输入:</em> </strong> <em class="hi">有一个或多个对象类的图像。</em></p><p id="62bf" class="im in kz io b ip iq ir is it iu iv iw la iy iz ja lb jc jd je lc jg jh ji jj hb bi translated"><strong class="io hj"> <em class="hi">输出:</em> </strong> <em class="hi">一个或多个边框高亮显示对象类。</em></p></blockquote><p id="94f8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">用于对象检测的一些性能最佳的 DL 模型:</strong></p><ul class=""><li id="198f" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj li lj lk ll bi translated">R-CNN</li><li id="1592" class="ld le hi io b ip lm it ln ix lo jb lp jf lq jj li lj lk ll bi translated">快速 R-CNN</li><li id="0312" class="ld le hi io b ip lm it ln ix lo jb lp jf lq jj li lj lk ll bi translated">更快的 R-CNN</li><li id="523d" class="ld le hi io b ip lm it ln ix lo jb lp jf lq jj li lj lk ll bi translated">YOLO</li><li id="b86e" class="ld le hi io b ip lm it ln ix lo jb lp jf lq jj li lj lk ll bi translated">YOLOv2 和 YOLOv3</li></ul><h1 id="1a7c" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">弱监督学习</h1><blockquote class="kw kx ky"><p id="9be9" class="im in kz io b ip iq ir is it iu iv iw la iy iz ja lb jc jd je lc jg jh ji jj hb bi translated">虽然传统的监督方法需要边界框注释来学习这样的任务，但是<strong class="io hj">弱监督对象定位(WSOL)方法可以仅利用图像级标签(例如图像中的对象列表)来学习。</strong></p></blockquote><p id="0df0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">标准监督对象检测与弱监督方法</strong></p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lr"><img src="../Images/61f26d3b303f53ea39561aad833d5ec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nyAPGg6ErkhU5n3diBlq-g.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">标准物体检测方法</figcaption></figure><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lw"><img src="../Images/81e4da676d6aef113ac4460ef023ff1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T91IetPjDtBrXzHtRjZ3eg.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">弱监督目标检测</figcaption></figure><blockquote class="kw kx ky"><p id="9114" class="im in kz io b ip iq ir is it iu iv iw la iy iz ja lb jc jd je lc jg jh ji jj hb bi translated">在弱监督方法中，<strong class="io hj">不是提供基本事实</strong>标签，而是在训练时做一个<strong class="io hj">比测试时所需输出更低程度的注释</strong>。</p></blockquote><ul class=""><li id="b963" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj li lj lk ll bi translated">当图像是正面的时，至少存在一个来自目标类别的对象实例。</li><li id="eb81" class="ld le hi io b ip lm it ln ix lo jb lp jf lq jj li lj lk ll bi translated">当图像为负时，不存在来自目标类别的对象实例。</li></ul><p id="18ef" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">解决这个问题的 DL 方法是找到感兴趣对象的<strong class="io hj">类激活图</strong> ( <strong class="io hj"> CAM </strong>)并用一个边界框突出显示它。<strong class="io hj">类激活图</strong>在论文中介绍了<em class="kz"> </em> <a class="ae lx" href="https://arxiv.org/abs/1512.04150" rel="noopener ugc nofollow" target="_blank"> <em class="kz">利用 CNN 中的全局平均池学习深度特征进行判别定位</em> </a>。一个<strong class="io hj"> CAM 表示 CNN 用来识别类别的区分区域</strong>。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es ly"><img src="../Images/303ecd1d7ccb6e8e4cac6275a586e4cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*u9WhnfgtJT7j8ua4.jpeg"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">CNN 的架构与 CAM | <a class="ae lx" href="https://miro.medium.com/max/1400/1*hXdqemUHnAlwKUIcl-4PDA.jpeg" rel="noopener">来源</a></figcaption></figure><blockquote class="kw kx ky"><p id="ace4" class="im in kz io b ip iq ir is it iu iv iw la iy iz ja lb jc jd je lc jg jh ji jj hb bi translated">该网络主要由众多卷积层<strong class="io hj"> </strong>组成，紧接着是最终输出层之前的<strong class="io hj">全局平均池</strong>。</p><p id="78c7" class="im in kz io b ip iq ir is it iu iv iw la iy iz ja lb jc jd je lc jg jh ji jj hb bi translated">简单地说，我们可以说<strong class="io hj"> CAM </strong>基本上确定了具有图像的主要成分的区域，该区域确定了该图像的类别。</p></blockquote><p id="504e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们可以使用一个简单的阈值方法来分割凸轮，并获得一个包围盒。首先，CAM 中值高于 CAM 最大值 20%的区域用于创建分割图。然后，绘制一个边界框以紧密包围分割图中的最大连通分量。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lz"><img src="../Images/0499fb8dafaf3e214632e0e7686aa593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-Qi8gnS6NyA2Bqay.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated"><em class="ma">周等人的汉化成果。</em> <a class="ae lx" href="https://lionbridge.ai/wp-content/uploads/2020/04/bharath-06.png" rel="noopener ugc nofollow" target="_blank"> <em class="ma">来源</em> </a></figcaption></figure><h2 id="8538" class="mb jl hi bd jm mc md me jq mf mg mh ju ix mi mj jy jb mk ml kc jf mm mn kg mo bi translated">对于实施 CAM(使用迁移学习):</h2><p id="39f5" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">跟随这篇文章:<a class="ae lx" href="https://towardsdatascience.com/demystifying-convolutional-neural-networks-using-gradcam-554a85dd4e48" rel="noopener" target="_blank"> <strong class="io hj">使用 GradCam </strong> </a>揭开卷积神经网络的神秘面纱</p><blockquote class="kw kx ky"><p id="8500" class="im in kz io b ip iq ir is it iu iv iw la iy iz ja lb jc jd je lc jg jh ji jj hb bi translated">请继续关注定制模型的实现。；)</p></blockquote><p id="fc97" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要阅读更多关于 WSOL 的内容，以下是基于它的一些论文:</p><ul class=""><li id="ff78" class="ld le hi io b ip iq it iu ix lf jb lg jf lh jj li lj lk ll bi translated"><a class="ae lx" href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640273.pdf" rel="noopener ugc nofollow" target="_blank">用于弱监督定位的图像间通信</a></li><li id="8747" class="ld le hi io b ip lm it ln ix lo jb lp jf lq jj li lj lk ll bi translated"><a class="ae lx" href="http://openaccess.thecvf.com/content_WACV_2020/papers/Yang_Combinational_Class_Activation_Maps_for_Weakly_Supervised_Object_Localization_WACV_2020_paper.pdf" rel="noopener ugc nofollow" target="_blank">用于弱监督目标定位的组合类激活图</a></li><li id="6541" class="ld le hi io b ip lm it ln ix lo jb lp jf lq jj li lj lk ll bi translated"><a class="ae lx" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Xue_DANet_Divergent_Activation_for_Weakly_Supervised_Object_Localization_ICCV_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank"> DANet:弱监督对象定位的发散激活</a></li><li id="8044" class="ld le hi io b ip lm it ln ix lo jb lp jf lq jj li lj lk ll bi translated"><a class="ae lx" href="https://arxiv.org/pdf/2002.11359.pdf" rel="noopener ugc nofollow" target="_blank">反思弱监督目标定位路线</a></li></ul></div><div class="ab cl mp mq gp mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="hb hc hd he hf"><p id="5bd2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">请联系我:</p><p id="24b3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">领英:<a class="ae lx" href="https://www.linkedin.com/in/manmohan-dogra-4a6655169/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/manmohan-dogra-4a6655169/</a></p><p id="9260" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">GitHub:<a class="ae lx" href="https://github.com/immohann" rel="noopener ugc nofollow" target="_blank">https://github.com/immohann</a></p><p id="0fa4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">推特:<a class="ae lx" href="https://twitter.com/immohann" rel="noopener ugc nofollow" target="_blank">https://twitter.com/immohann</a></p></div></div>    
</body>
</html>