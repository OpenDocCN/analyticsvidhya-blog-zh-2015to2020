<html>
<head>
<title>Semantic Segmentation: The easiest possible implementation in code!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语义分段:代码中最简单的实现！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/semantic-segmentation-the-easiest-possible-implementation-in-code-193bf27b86b8?source=collection_archive---------0-----------------------#2019-07-05">https://medium.com/analytics-vidhya/semantic-segmentation-the-easiest-possible-implementation-in-code-193bf27b86b8?source=collection_archive---------0-----------------------#2019-07-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="50c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分割对于图像解释任务是至关重要的。那就不要落后于潮流。让我们实施它，你很快就会成为专业人士！</p><p id="3794" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">什么是语义分割？</p><p id="0c69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它描述了将一幅图像的每个像素与一个类别标签(如<em class="jd">花</em>、<em class="jd">人</em>、<em class="jd">路</em>、<em class="jd">天</em>、<em class="jd">海</em>或<em class="jd">车</em>)相关联的过程，也就是说，我们要输入一幅图像，然后输出该图像中每个像素的一个类别的决定，因此对于该图像中的每个像素，例如，该输入图像是一只坐在床上的狗。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/83f195867efa2522dc0bda15b763fffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*ma9XpjFwPgkM078YSGY9iA.png"/></div></figure><p id="00c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在输出中，我们希望为每个像素定义一组类别，即狗、床、后面的桌子和橱柜。语义分割后，图像看起来会像这样:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jm"><img src="../Images/3e420adbf40c2f2b8dbae8517d9abc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_pldf5yn6Ty3jy6cFGz5mQ.png"/></div></div></figure><p id="b5bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于语义分割的一个有趣的事情是，它不区分实例，即，如果在这个图像中有两只狗，它们将被描述为只有一个标签，即，狗，而不是狗1和狗2。</p><p id="f0f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">语义分割通常用于:</p><ul class=""><li id="b051" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated">自动驾驶</li><li id="fca3" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">工业检查</li><li id="f61a" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">卫星图像中明显区域的分类</li><li id="62d3" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">医学影像评论</li></ul><p id="8c27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">语义分段实现:</p><ol class=""><li id="b177" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc kf jx jy jz bi translated">第一种方法是<strong class="ih hj">滑动窗口</strong>方法，我们将输入图像分解成许多小的局部图像，但我希望你已经猜到这在计算上会很昂贵。所以，我们实际上并不怎么用这个。</li><li id="eef4" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc kf jx jy jz bi translated">另一种方法是<strong class="ih hj">全卷积网络</strong>，其中网络具有整个巨大的卷积层堆栈，没有完全连接的层，这保持了输入的空间大小。这在计算上也非常昂贵。</li><li id="d302" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc kf jx jy jz bi translated">第三个也是最好的一个是<strong class="ih hj">上采样&amp;下采样</strong>图像。因此，我们不是对图像的全空间分辨率进行所有卷积，而是以原始分辨率遍历少量卷积层，然后对该特征图进行下采样，之后再进行上采样。这里，我们只是想在网络的后半部分提高我们预测的空间分辨率，这样我们的输出图像现在可以与输入图像具有相同的维度。它的计算效率更高。因为你可以把网络做得很深，以更便宜的空间分辨率来操作。</li></ol><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es kg"><img src="../Images/4da766c520918d11e130d64493dd157d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9SK1we-aOFztBy3_d18-w.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated"><a class="ae kl" href="https://youtu.be/nDPWywWRIRo" rel="noopener ugc nofollow" target="_blank">图片提供:斯坦福大学</a></figcaption></figure><p id="2479" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以让我们用代码来实现它:</p><ul class=""><li id="e221" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">导入处理</strong>所需的必要库，即<br/>py torch的重要功能，如数据加载器、变量、转换和优化器相关函数，<br/>导入VOC12和cityscapes的数据集类，从transform.py文件导入Relabel、<br/> ToLabel和Colorize类，<br/>从iouEval.py文件导入iouEval类。</li></ul><pre class="jf jg jh ji fd km kn ko kp aw kq bi"><span id="8b10" class="kr ks hi kn b fi kt ku l kv kw">#SSCV IIITH 2K19<br/>import random<br/>import time<br/>import numpy as np<br/>import torch<br/>print(torch.__version__)<br/>import math<br/>from PIL import Image, ImageOps<br/>from torch.optim import SGD, Adam, lr_scheduler<br/>from torch.autograd import Variable<br/>from torch.utils.data import DataLoader<br/>from torchvision.transforms import  Resize<br/>from torchvision.transforms import ToTensor, ToPILImage<br/>from dataset import cityscapes<br/>from dataset import idd_lite<br/>import sys<br/>print(sys.executable)<br/>from transform import Relabel, ToLabel, Colorize<br/>import matplotlib<br/>from matplotlib import pyplot as plt<br/>%matplotlib inline<br/>import importlib<br/>from iouEval import iouEval, getColorEntry #importing iouEval class from the iouEval.py file<br/>from shutil import copyfile</span></pre><ul class=""><li id="bcf3" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">定义几个全局参数:</strong></li></ul><pre class="jf jg jh ji fd km kn ko kp aw kq bi"><span id="f7a6" class="kr ks hi kn b fi kt ku l kv kw">NUM_CHANNELS = 3 #RGB Images<br/>NUM_CLASSES = 8 #IDD Lite has 8 labels or Level1 hierarchy of labels<br/>USE_CUDA = torch.cuda.is_available() <br/>IMAGE_HEIGHT = 160<br/>DATA_ROOT = ‘/tmp/school/6-segmentation/user/1/6-segmentation/idd1_lite’<br/>BATCH_SIZE = 2<br/>NUM_WORKERS = 4<br/>NUM_EPOCHS = 100<br/>ENCODER_ONLY = True<br/>device = torch.device(“cuda” )<br/>#device = ‘cuda’<br/>color_transform = Colorize(NUM_CLASSES)<br/>image_transform = ToPILImage()<br/>IOUTRAIN = False<br/>IOUVAL = True</span></pre><ul class=""><li id="f5d1" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">增强，即</strong>执行不同的功能，对图像和目标进行随机增强:</li></ul><pre class="jf jg jh ji fd km kn ko kp aw kq bi"><span id="0dab" class="kr ks hi kn b fi kt ku l kv kw">class MyCoTransform(object):<br/> def __init__(self, enc, augment=True, height=160):<br/> self.enc=enc<br/> self.augment = augment<br/> self.height = height<br/> pass<br/> def __call__(self, input, target):<br/> # Resizing data to required size<br/> input = Resize((self.height,320), Image.BILINEAR)(input)<br/> target = Resize((self.height,320), Image.NEAREST)(target)</span><span id="82ed" class="kr ks hi kn b fi kx ku l kv kw">if(self.augment):<br/> # Random horizontal flip<br/> hflip = random.random()<br/> if (hflip &lt; 0.5):<br/> input = input.transpose(Image.FLIP_LEFT_RIGHT)<br/> target = target.transpose(Image.FLIP_LEFT_RIGHT)<br/> <br/> #Random translation 0–2 pixels (fill rest with padding)<br/> transX = random.randint(0, 2) <br/> transY = random.randint(0, 2)</span><span id="2ce1" class="kr ks hi kn b fi kx ku l kv kw">input = ImageOps.expand(input, border=(transX,transY,0,0), fill=0)<br/> target = ImageOps.expand(target, border=(transX,transY,0,0), fill=7) #pad label filling with 7<br/> input = input.crop((0, 0, input.size[0]-transX, input.size[1]-transY))<br/> target = target.crop((0, 0, target.size[0]-transX, target.size[1]-transY))</span><span id="f45d" class="kr ks hi kn b fi kx ku l kv kw">input = ToTensor()(input)<br/> <br/> target = ToLabel()(target)<br/> <br/> target = Relabel(255,7)(target)<br/> return input, target</span></pre><ul class=""><li id="ae8e" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">加载数据</strong>:我们将遵循pytorch推荐的语义，并使用dataloader来加载数据。</li></ul><pre class="jf jg jh ji fd km kn ko kp aw kq bi"><span id="1313" class="kr ks hi kn b fi kt ku l kv kw">best_acc = 0</span><span id="1a38" class="kr ks hi kn b fi kx ku l kv kw">co_transform = MyCoTransform(ENCODER_ONLY, augment=True, height=IMAGE_HEIGHT)<br/>co_transform_val = MyCoTransform(ENCODER_ONLY, augment=False, height=IMAGE_HEIGHT)</span><span id="3e50" class="kr ks hi kn b fi kx ku l kv kw">#train data<br/>dataset_train = idd_lite(DATA_ROOT, co_transform, ‘train’)<br/>print(len(dataset_train))<br/>#test data<br/>dataset_val = idd_lite(DATA_ROOT, co_transform_val, ‘val’)<br/>print(len(dataset_val))</span><span id="ff6f" class="kr ks hi kn b fi kx ku l kv kw">loader_train = DataLoader(dataset_train, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)<br/>loader_val = DataLoader(dataset_val, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=False)</span></pre><ul class=""><li id="cd33" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">既然是分类问题，我们就用交叉熵损失，但是为什么呢？</strong></li></ul><p id="e46c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，答案是负对数似然在较小的值时变得不快乐，在那里它可以达到无限的不快乐(那太可悲了)，在较大的值时变得不那么不快乐。因为我们对所有正确类别的损失函数求和，所以实际发生的情况是，每当网络在正确类别分配高置信度时，不快乐就低，但是当网络在正确类别分配低置信度时，不快乐就高。</p><pre class="jf jg jh ji fd km kn ko kp aw kq bi"><span id="fe32" class="kr ks hi kn b fi kt ku l kv kw">criterion = torch.nn.CrossEntropyLoss()</span></pre><ul class=""><li id="95c3" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">现在让我们加载模型&amp;优化它！</strong></li></ul><pre class="jf jg jh ji fd km kn ko kp aw kq bi"><span id="79ce" class="kr ks hi kn b fi kt ku l kv kw">model_file = importlib.import_module(‘erfnet’)<br/>model = model_file.Net(NUM_CLASSES).to(device)<br/>optimizer = Adam(model.parameters(), 5e-4, (0.9, 0.999), eps=1e-08, weight_decay=1e-4) <br/>start_epoch = 1</span></pre><ul class=""><li id="461e" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">所以，编码的最终本质就在这里，也就是训练！</strong></li></ul><pre class="jf jg jh ji fd km kn ko kp aw kq bi"><span id="32fe" class="kr ks hi kn b fi kt ku l kv kw">import os<br/>steps_loss = 50<br/>my_start_time = time.time()<br/>for epoch in range(start_epoch, NUM_EPOCHS+1):<br/> print(“ — — — TRAINING — EPOCH”, epoch, “ — — -”)</span><span id="79da" class="kr ks hi kn b fi kx ku l kv kw">epoch_loss = []<br/> time_train = []</span><span id="d163" class="kr ks hi kn b fi kx ku l kv kw">doIouTrain = IOUTRAIN <br/> doIouVal = IOUVAL</span><span id="4730" class="kr ks hi kn b fi kx ku l kv kw">if (doIouTrain):<br/> iouEvalTrain = iouEval(NUM_CLASSES)</span><span id="f856" class="kr ks hi kn b fi kx ku l kv kw">model.train()<br/> for step, (images, labels) in enumerate(loader_train):</span><span id="9145" class="kr ks hi kn b fi kx ku l kv kw">start_time = time.time()<br/> inputs = images.to(device)<br/> targets = labels.to(device)<br/> <br/> outputs = model(inputs, only_encode=ENCODER_ONLY)</span><span id="2991" class="kr ks hi kn b fi kx ku l kv kw"># zero the parameter gradients<br/> optimizer.zero_grad()<br/> <br/> # forward + backward + optimize<br/> loss = criterion(outputs, targets[:, 0])<br/> loss.backward()<br/> optimizer.step()</span><span id="c286" class="kr ks hi kn b fi kx ku l kv kw">epoch_loss.append(loss.item())<br/> time_train.append(time.time() — start_time)</span><span id="14e6" class="kr ks hi kn b fi kx ku l kv kw">if (doIouTrain):<br/> #start_time_iou = time.time()<br/> iouEvalTrain.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)<br/> #print (“Time to add confusion matrix: “, time.time() — start_time_iou)</span><span id="d3e6" class="kr ks hi kn b fi kx ku l kv kw"># print statistics<br/> if steps_loss &gt; 0 and step % steps_loss == 0:<br/> average = sum(epoch_loss) / len(epoch_loss)<br/> print(‘loss: {average:0.4} (epoch: {epoch}, step: {step})’, “// Avg time/img: %.4f s” % (sum(time_train) / len(time_train) / BATCH_SIZE))</span><span id="93fa" class="kr ks hi kn b fi kx ku l kv kw">average_epoch_loss_train = sum(epoch_loss) / len(epoch_loss)</span><span id="ac98" class="kr ks hi kn b fi kx ku l kv kw">iouTrain = 0<br/> if (doIouTrain):<br/> iouTrain, iou_classes = iouEvalTrain.getIoU()<br/> iouStr = getColorEntry(iouTrain)+’{:0.2f}’.format(iouTrain*100) + ‘\033[0m’<br/> print (“EPOCH IoU on TRAIN set: “, iouStr, “%”) <br/>my_end_time = time.time()<br/>print(my_end_time — my_start_time)</span></pre><p id="b096" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">经过100个纪元的训练后，你会看到:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ky"><img src="../Images/584a803dfdf7ca8bb20ab8d0e012bfb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*BvVKePfByIzv5YxVb7XgyQ.png"/></div></figure><ul class=""><li id="0132" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">验证:</strong></li></ul><pre class="jf jg jh ji fd km kn ko kp aw kq bi"><span id="d229" class="kr ks hi kn b fi kt ku l kv kw">#Validate on val images after each epoch of training<br/>print(“ — — — VALIDATING — EPOCH”, epoch, “ — — -”)<br/>model.eval()<br/>epoch_loss_val = []<br/>time_val = []</span><span id="bf0b" class="kr ks hi kn b fi kx ku l kv kw">if (doIouVal):<br/> iouEvalVal = iouEval(NUM_CLASSES)</span><span id="a283" class="kr ks hi kn b fi kx ku l kv kw">for step, (images, labels) in enumerate(loader_val):<br/> start_time = time.time()</span><span id="b8a1" class="kr ks hi kn b fi kx ku l kv kw">inputs = images.to(device) <br/> targets = labels.to(device)<br/> <br/> with torch.no_grad():<br/> outputs = model(inputs, only_encode=ENCODER_ONLY) <br/> #outputs = model(inputs)<br/> loss = criterion(outputs, targets[:, 0])<br/> epoch_loss_val.append(loss.item())<br/> time_val.append(time.time() — start_time)</span><span id="7bf1" class="kr ks hi kn b fi kx ku l kv kw">#Add batch to calculate TP, FP and FN for iou estimation<br/> if (doIouVal):<br/> #start_time_iou = time.time()<br/> iouEvalVal.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)<br/> #print (“Time to add confusion matrix: “, time.time() — start_time_iou)<br/> <br/> if steps_loss &gt; 0 and step % steps_loss == 0:<br/> average = sum(epoch_loss_val) / len(epoch_loss_val)<br/> print(‘VAL loss: {average:0.4} (epoch: {epoch}, step: {step})’, <br/> “// Avg time/img: %.4f s” % (sum(time_val) / len(time_val) / BATCH_SIZE))</span><span id="0740" class="kr ks hi kn b fi kx ku l kv kw">average_epoch_loss_val = sum(epoch_loss_val) / len(epoch_loss_val)</span><span id="8415" class="kr ks hi kn b fi kx ku l kv kw">iouVal = 0<br/>if (doIouVal):</span><span id="22ee" class="kr ks hi kn b fi kx ku l kv kw">iouVal, iou_classes = iouEvalVal.getIoU()<br/> print(iou_classes)<br/> iouStr = getColorEntry(iouVal)+’{:0.2f}’.format(iouVal*100) + ‘\033[0m’<br/> print (“EPOCH IoU on VAL set: “, iouStr, “%”)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es kz"><img src="../Images/e2ff1fb7e7550ebe3436c111b448b20a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tyisdQJPMIL0uAbJdNlRdA.png"/></div></div></figure><ul class=""><li id="5529" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><strong class="ih hj">可视化输出:</strong></li></ul><pre class="jf jg jh ji fd km kn ko kp aw kq bi"><span id="4da1" class="kr ks hi kn b fi kt ku l kv kw"># Qualitative Analysis<br/>dataiter = iter(loader_val)<br/>images, labels = dataiter.next()</span><span id="607b" class="kr ks hi kn b fi kx ku l kv kw">if USE_CUDA:<br/> images = images.to(device)</span><span id="d7d6" class="kr ks hi kn b fi kx ku l kv kw">inputs = images.to(device)</span><span id="7085" class="kr ks hi kn b fi kx ku l kv kw">with torch.no_grad():<br/> outputs = model(inputs, only_encode=ENCODER_ONLY)</span><span id="efd3" class="kr ks hi kn b fi kx ku l kv kw">label = outputs[0].max(0)[1].byte().cpu().data</span><span id="09c1" class="kr ks hi kn b fi kx ku l kv kw">label_color = Colorize()(label.unsqueeze(0))</span><span id="b021" class="kr ks hi kn b fi kx ku l kv kw">label_save = ToPILImage()(label_color)<br/>plt.figure()<br/>plt.imshow(ToPILImage()(images[0].cpu()))<br/>plt.figure()<br/>plt.imshow(label_save)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es la"><img src="../Images/74fb9b14db12722da011dbf310e2c8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*N_gwYa5NhSO_0Va_1XrHTA.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">输出图像</figcaption></figure><p id="8b0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">很快你就可以准备好你的模型了！是不是很神奇？请随意玩你新设计的模型！尝试增加更多的纪元，看你的模型表现得更好！</p><p id="923b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，简而言之，现在您将能够轻松地将图像的每个像素与类别标签相关联。您现在可以调整超参数来检查出现的变化。这篇文章展示了语义分割的基础，为了对实例进行分类，你需要进行实例分割，这是语义分割的高级版本。</p><blockquote class="lb lc ld"><p id="5ad7" class="if ig jd ih b ii ij ik il im in io ip le ir is it lf iv iw ix lg iz ja jb jc hb bi translated"><strong class="ih hj">最后，要运行这个，你需要数据加载器文件，可以从这里下载:</strong> <a class="ae kl" href="https://github.com/Garima13a/Semantic-Segmentation" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">语义分段</strong> </a></p></blockquote></div></div>    
</body>
</html>