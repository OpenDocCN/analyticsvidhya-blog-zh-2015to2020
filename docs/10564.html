<html>
<head>
<title>Machine Learning — Multivariate Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习—多元线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-multivariate-linear-regression-8f9878c0f56f?source=collection_archive---------8-----------------------#2020-10-24">https://medium.com/analytics-vidhya/machine-learning-multivariate-linear-regression-8f9878c0f56f?source=collection_archive---------8-----------------------#2020-10-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="43b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归是最常用的机器学习算法之一。一元线性回归是更简单的形式，而多元线性回归则适用于更复杂的问题。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/4b7c55dc81ecab44e722e48038cb39b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*UtN6F3akfoF4HwvQGDgShg.png"/></div></figure><blockquote class="jl jm jn"><p id="ecef" class="if ig jo ih b ii ij ik il im in io ip jp ir is it jq iv iw ix jr iz ja jb jc hb bi translated">这是我关于线性回归的第二篇论文，第一篇是关于<a class="ae js" rel="noopener" href="/@aabiyev01/machine-learning-univariate-linear-regression-1acddb85aa0b">一元线性回归</a>。您可以通过它来获得线性回归的背景知识、机器学习算法的基础知识以及对本文的更好理解。</p></blockquote><h2 id="b8b8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated"><strong class="ak">本文包含以下主题:</strong></h2><ul class=""><li id="fe7b" class="ko kp hi ih b ii kq im kr iq ks iu kt iy ku jc kv kw kx ky bi translated">多元线性回归导论；</li><li id="2f57" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">算法的假设；</li><li id="1b36" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">数据集的操作和矩阵乘法；</li><li id="db2e" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">成本函数；</li><li id="1d68" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">梯度下降。</li></ul><h1 id="0f37" class="le ju hi bd jv lf lg lh jz li lj lk kd ll lm ln kg lo lp lq kj lr ls lt km lu bi translated">多元线性回归导论</h1><p id="39c0" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq lv is it iu lw iw ix iy lx ja jb jc hb bi translated">在 ML 问题中，有各种各样的数据集，它们在维度(行数和列数)上互不相同。众所周知，列表示特征，行表示样本，更多的特征意味着更复杂的模型。在线性回归中，如果列数为 1，则称为一元线性回归，如果列数超过 1，则称为多元线性回归(MLR)。单变量和多变量 LR 算法的关键概念是相似的，但在方程中存在一些差异，这是由数据集的不同维度造成的。</p><h1 id="8c43" class="le ju hi bd jv lf lg lh jz li lj lk kd ll lm ln kg lo lp lq kj lr ls lt km lu bi translated">算法的假设</h1><p id="fa5d" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq lv is it iu lw iw ix iy lx ja jb jc hb bi translated">在单变量 LR 中，直线的方程用于证明假设，多变量 LR 的假设与此类似，但它因参数的数量而不同。为了简单和更好的理解，让我们从数据集的例子开始，其中有三个特征(x₁，x₂，x₃).在这种情况下，假设如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/4d6a29198bec1cfff593c069d7eb978b.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*MVwtmNa5WraUFMmD9eioow.png"/></div></figure><p id="6cb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个单一特征(x)，都有一个<em class="jo"> θ </em>值和额外的<em class="jo"> θ₀ </em>。方程的一般形式:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/71de48295ec12e5ac83e5aa6dc92d189.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*g4pejWwEUjV7pUIGbVj7ZA.png"/></div></figure><p id="f10f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，</p><ul class=""><li id="2d49" class="ko kp hi ih b ii ij im in iq ma iu mb iy mc jc kv kw kx ky bi translated">n —特征的数量。</li></ul><p id="56eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个 ML 算法的目标都是找到最合适的假设。在 MLR 假设中包含<em class="jo"> x-s </em>和<em class="jo"> θ-s </em>。<em class="jo"> x-s </em>在数据集中给定，因此假设与它们的<em class="jo"> θ-s </em>互不相同。<strong class="ih hj"> <em class="jo"> θ-s </em>以随机值初始化，然后算法开始优化它们的值。</strong>我们的目标是为给定的数据集计算最合适的<em class="jo"> θ-s </em>。成本函数和梯度下降是这种方式的工具，这将在下一节中讨论。</p><h1 id="3956" class="le ju hi bd jv lf lg lh jz li lj lk kd ll lm ln kg lo lp lq kj lr ls lt km lu bi translated">矩阵乘法和数据集操作</h1><p id="f370" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq lv is it iu lw iw ix iy lx ja jb jc hb bi translated">如果特征的数量是一个很大的数字，如数百或数千，上面的等式将不适合使用，因为我们必须写出每个参数。这就是为什么，一个更合适的方法——矩阵乘法被应用。这听起来可能很复杂，但我们会一步一步地继续，你会发现这实际上比方程本身更简单。</p><ol class=""><li id="b087" class="ko kp hi ih b ii ij im in iq ma iu mb iy mc jc md kw kx ky bi translated">让我们复习一下矩阵乘法:</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es me"><img src="../Images/4080c41a5591653ba61e9037e7dad52d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4AmoOQI6dmTslePC2OeuaQ.png"/></div></div></figure><p id="c025" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是矩阵乘法的一个例子。关键是左矩阵的列数必须等于右矩阵的行数。</p><p id="353d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于矩阵乘法的进一步阅读。</p><p id="f7da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.现在我们将用这些矩阵来表示<em class="jo"> x-s </em>和<em class="jo"> θ-s </em>。让我们看看下面的数据集和 theta:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mj"><img src="../Images/53989bad907a6e66b5e58e773f3cfd57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*0DveOlNxLKhNV-wouQkTqA.png"/></div></figure><p id="b1cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jo"> θ-s </em>成为矢量(一列矩阵):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mk"><img src="../Images/f3158d53ff6111420fbf5aac4432b72a.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/format:webp/1*NhWq1dlgOH2u00rCX8ncXQ.png"/></div></figure><p id="426a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.第三步是这些矩阵的乘法运算:</p><p id="f65b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回头看矩阵，X 矩阵的列是 3，但是<em class="jo"> θ </em>矩阵的行是 4。然而我们不能将这些矩阵相乘为 3 ≠ 4。这是因为假设等式中的额外参数——θ₀。为了能够使用矩阵乘法，一个额外的列<strong class="ih hj"> <em class="jo">个</em> </strong>被添加到 x 矩阵。将一列相加后，x 的列数和<em class="jo"> θ </em>的行数相等，我们可以做矩阵乘法:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/0fc0db97e0cc7677f529a7b32aa72ea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*DOXXsBySolR3r9nprNnNuQ.png"/></div></figure><p id="a0de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于 x 矩阵的变化，我们改写假设如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/31eb6ee2946c329d0b1f521f6db2f3f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*XquFekRZQjIYnmlQnS0F1A.png"/></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">假设。</figcaption></figure><p id="acfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总而言之，我们对假设做了很小的改动——增加了 x <em class="jo"> ₀ </em>等于 1，并乘以<em class="jo"> θ₀ </em>。当<em class="jo"> θ₀ </em>乘以<strong class="ih hj"> 1 </strong>时，等式<strong class="ih hj">的结果不会改变</strong>。如上所述，添加它是为了使θ和 x-s 的数量相等，以便能够进行<strong class="ih hj">矩阵乘法</strong>。</p><h1 id="3a24" class="le ju hi bd jv lf lg lh jz li lj lk kd ll lm ln kg lo lp lq kj lr ls lt km lu bi translated">价值函数</h1><p id="1211" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq lv is it iu lw iw ix iy lx ja jb jc hb bi translated">在完成假设和数据集之后，我们可以开始算法的实现。成本函数用于确定假设与数据集的拟合程度。例如，我们有一对 x 矩阵和 y 值。x 和 y 在训练集中给定。我们先用 x 矩阵和<em class="jo"> θ </em>矩阵计算 h(x)，然后比较 h(x)和 y，在理想情况下，h(x)等于 y，但大多数情况下不可能对数据集(x 矩阵)的每个样本都使 h(x)等于 y。这就是为什么我们需要为整个数据集选择最优的解决方案。</p><p id="1880" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设的适合度通过成本函数计算，其值越小，假设越好。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mr"><img src="../Images/aff0d16dd0dc666951c65e06df33aac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*_4xKRK-gEA_3xYDIp8HVmA.png"/></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">成本函数。</figcaption></figure><p id="09c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如前所述，成本函数的值应该是最小的，因此从这一点来看，我们的目的是通过优化<em class="jo"> θ-s </em>来最小化其值。</p><h1 id="fa2a" class="le ju hi bd jv lf lg lh jz li lj lk kd ll lm ln kg lo lp lq kj lr ls lt km lu bi translated">梯度下降</h1><p id="7e74" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq lv is it iu lw iw ix iy lx ja jb jc hb bi translated">梯度下降算法的目的是优化<em class="jo"> θ-s </em>，使得代价函数值最小。正如微积分所知，函数的导数是它随时间的变化率。所以在梯度下降中，我们取成本函数对相关θ的导数，然后乘以α。获取的值由先前的<em class="jo"> θ </em>值替代:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ms"><img src="../Images/9994ace91fab638158ae636883157bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*D2PDA_dTqmnxmOu7RIGc4g.png"/></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">梯度下降。</figcaption></figure><p id="cb55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，</p><ul class=""><li id="78c9" class="ko kp hi ih b ii ij im in iq ma iu mb iy mc jc kv kw kx ky bi translated">α——学习率；</li><li id="6a53" class="ko kp hi ih b ii kz im la iq lb iu lc iy ld jc kv kw kx ky bi translated">m——样本数量。</li></ul><p id="26af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">应用梯度下降，直到成本函数值非常小。</p><p id="75df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一节将讨论线性回归在 ML 问题上的实现。</p><h1 id="5030" class="le ju hi bd jv lf lg lh jz li lj lk kd ll lm ln kg lo lp lq kj lr ls lt km lu bi translated">谢谢你。</h1></div></div>    
</body>
</html>