<html>
<head>
<title>RCNN Review[1311.2524]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RCNN审查[1311.2524]</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/rcnn-review-1311-2524-898c3148789a?source=collection_archive---------28-----------------------#2020-04-06">https://medium.com/analytics-vidhya/rcnn-review-1311-2524-898c3148789a?source=collection_archive---------28-----------------------#2020-04-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="13e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经计划阅读主要的物体探测论文(虽然我已经粗略地阅读了它们中的大部分，但我会详细地阅读它们，好到足以写一篇关于它们的博客)。这些论文与基于深度学习的对象检测相关。随时给建议或询问疑惑会尽我所能帮助大家。我将在下面写下每篇论文的arxiv代码，并在下面给出博客(我写的时候会不断更新)和他们论文的链接。任何从这个领域开始的人都可以跳过许多这样的论文。当我读完所有的论文后，我还会写下它们的优先级/重要性(根据理解主题的必要性)。我写这篇博客是考虑到和我相似的读者仍在学习。万一我犯了任何错误(我将通过从各种来源(包括博客、代码和视频)深入理解论文来尽量减少错误)，任何人都可以随意地在博客上强调它或添加评论。我已经提到了我将在博客结尾涉及的论文列表。</p><p id="17c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们开始吧:)</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><p id="40b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RCNN论文是引发基于深度学习的目标检测研究的主要论文之一。RCNN将现有技术的结果提高了30%,这是一个显著的进步。从理论上讲，这篇论文比上一篇博客中讨论的一些其他论文(如overfeat)更容易理解。</p><p id="b2e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RCNN目标检测系统基于三个模块。这三个模块包括区域提议、用于特征提取的CNN和基于第三SVM的分类器。图1总结了这个网络。现在，我将简要介绍每个模块</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es jk"><img src="../Images/ee663b34f5362f4cd76460889d62b8fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*iBKN5KPIJGCY0ML5WDZPgg.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图1 RCNN物体探测系统</figcaption></figure><h2 id="9a23" class="jw jx hi bd jy jz ka kb kc kd ke kf kg iq kh ki kj iu kk kl km iy kn ko kp kq bi translated">区域提议</h2><p id="730f" class="pw-post-body-paragraph if ig hi ih b ii kr ik il im ks io ip iq kt is it iu ku iw ix iy kv ja jb jc hb bi translated">RCNN的第一个模块提出可能包含对象的区域(候选边界框)。这些区域是使用选择性搜索提出的，作者建议的区域数量约为2k。这些区域可能包含一个对象，但我们目前还不确定，但选择性搜索可以近似确定该对象的位置，并从建议中删除不相关的背景。作者提出了其他可以使用的区域提议技术，但最终使用了选择性搜索。</p><p id="6900" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择性搜索根据颜色、纹理、大小和形状对相似区域进行分层分组。这些区域随后被分组为多个边界框(在这种情况下为2k)。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es kw"><img src="../Images/3a7bd219bc29ef74451e6059808dd99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B8eFNSi1E3AUUkJHDCZZfA.jpeg"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">选择性搜索分层分组。<a class="ae kx" href="https://www.koen.me/research/selectivesearch/" rel="noopener ugc nofollow" target="_blank">图片信用</a></figcaption></figure><h2 id="7d73" class="jw jx hi bd jy jz ka kb kc kd ke kf kg iq kh ki kj iu kk kl km iy kn ko kp kq bi translated">特征抽出</h2><p id="1f7a" class="pw-post-body-paragraph if ig hi ih b ii kr ik il im ks io ip iq kt is it iu ku iw ix iy kv ja jb jc hb bi translated">对于每个区域建议，使用alexnet模型提取了4096个维度特征。所有区域的大小都调整为227*227。由于这些建议的大小不同，它们会将周围紧密包围框中的所有像素扭曲到所需的大小。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ky"><img src="../Images/debbac03800d79e87d135d15a311453a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50V3wlDnSKt9qluIYSl_yQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">扭曲的图像区域</figcaption></figure><p id="b8ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，对于每个类别，我们使用为该类别训练的SVM对每个提取的特征向量进行评分。应用贪婪的非最大抑制，如果一个区域与大于学习阈值的较高得分的所选区域有交集-并集(IoU)重叠，则拒绝该区域。</p><h2 id="b29d" class="jw jx hi bd jy jz ka kb kc kd ke kf kg iq kh ki kj iu kk kl km iy kn ko kp kq bi translated">培养</h2><p id="8815" class="pw-post-body-paragraph if ig hi ih b ii kr ik il im ks io ip iq kt is it iu ku iw ix iy kv ja jb jc hb bi translated">由于缺少对象检测数据，首先使用图像级分类(迁移学习)在较大的辅助imagenet数据集上对所使用的模型进行预训练。该模型后来针对新的检测任务和新的领域(扭曲的提议窗口)进行了微调。只是最后一层从像网的1000类改成了pascal VOC的21类(20+1背景)。IOU≥0.5的所有区域与地面实况框重叠，作为该框类别的正极，其余区域作为负极。一旦模型被微调，一个线性SVM被训练用于每一个类。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><p id="f98d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献:</strong></p><ol class=""><li id="6768" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated">【https://arxiv.org/abs/1311.2524 T2】T3【RCNN论文】</li><li id="7053" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">【https://www.koen.me/research/selectivesearch T4】</li><li id="d921" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/" rel="noopener ugc nofollow" target="_blank">https://www . learnopencv . com/selective-search-for-object-detection-CPP-python/</a></li><li id="a69e" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e" rel="noopener" target="_blank">https://towards data science . com/r-CNN-fast-r-CNN-faster-r-CNN-yolo-object-detection-algorithms-36d 53571365 e</a></li></ol><h1 id="67a2" class="lr jx hi bd jy ls lt lu kc lv lw lx kg ly lz ma kj mb mc md km me mf mg kp mh bi translated">论文列表:</h1><ol class=""><li id="5e8e" class="ld le hi ih b ii kr im ks iq mi iu mj iy mk jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1312.6229.pdf" rel="noopener ugc nofollow" target="_blank"> OverFeat:使用卷积网络的综合识别、定位和检测</a>。[ <a class="ae kx" rel="noopener" href="/@sanchittanwar75/overfeat-review-1312-6229-4fd925f3739f">链接到博客</a></li><li id="f32e" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">丰富的特征层次，用于精确的对象检测和语义分割(RCNN)。</a> ←你完成了这篇博客。</li><li id="cc73" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池(SPPNet)。</a> [ <a class="ae kx" rel="noopener" href="/@sanchittanwar75/review-spatial-pyramid-pooling-1406-4729-bfc142988dd2">链接到博客</a> ]</li><li id="b4cf" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">快速R-CNN </li><li id="a64a" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">更快的R-CNN:使用区域提议网络实现实时目标检测。【博客链接】</li><li id="bb59" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测。</a>【博客链接】</li><li id="61bf" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank"> SSD:单次多盒探测器</a>。[博客链接]</li><li id="9ea7" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/abs/1605.06409" rel="noopener ugc nofollow" target="_blank"> R-FCN:通过基于区域的完全卷积网络的目标检测。</a>【博客链接】</li><li id="823e" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1612.03144.pdf" rel="noopener ugc nofollow" target="_blank">用于目标检测的特征金字塔网络。</a>【博客链接】</li><li id="d913" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1701.06659.pdf" rel="noopener ugc nofollow" target="_blank"> DSSD:解卷积单粒子探测器</a>。[博客链接]</li><li id="b4e6" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">密集物体检测的焦点丢失(视网膜网)。</a>【博客链接】</li><li id="63ef" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1804.02767.pdf" rel="noopener ugc nofollow" target="_blank"> YOLOv3:增量改进</a>。[博客链接]</li><li id="f90a" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1805.09300v3.pdf" rel="noopener ugc nofollow" target="_blank">狙击手:高效多尺度训练</a>。[博客链接]</li><li id="fae1" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1904.04514.pdf" rel="noopener ugc nofollow" target="_blank">标注像素和区域的高分辨率表示。</a>【博客链接】</li><li id="1b81" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1904.01355v5.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS:全卷积一级目标检测</a>。[博客链接]</li><li id="bf26" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1904.07850.pdf" rel="noopener ugc nofollow" target="_blank">以物为点</a>。[博客链接]</li><li id="c5c4" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">CornerNet-Lite:高效的基于关键点的对象检测。【博客链接】</li><li id="8758" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1904.08189v3.pdf" rel="noopener ugc nofollow" target="_blank"> CenterNet:用于对象检测的关键点三元组</a>。[博客链接]</li><li id="87dd" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">用于实时对象检测的训练时间友好网络。【博客链接】</li><li id="e0f2" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated">CBNet:一种用于目标检测的新型复合主干网络体系结构。【博客链接】</li><li id="1ba9" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><a class="ae kx" href="https://arxiv.org/pdf/1911.09070v2.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet:可扩展且高效的对象检测</a>。[博客链接]</li></ol><p id="7fc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">和平…</p></div></div>    
</body>
</html>