<html>
<head>
<title>Neural Machine Translations ( Implementing Encoder — Decoder )</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经机器翻译(实现编码器-解码器)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-machine-translations-implementing-encoder-decoder-658c3facd530?source=collection_archive---------12-----------------------#2020-11-26">https://medium.com/analytics-vidhya/neural-machine-translations-implementing-encoder-decoder-658c3facd530?source=collection_archive---------12-----------------------#2020-11-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0620dd32275afbaeaad8b61ab9d5f8aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4aXSpnmBA_1Nj6j7VgXVGA.png"/></div></div></figure><p id="0ecd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi jo translated">我们一生中都至少使用过一次翻译器，不管是为了真正的目的还是为了精彩的字幕，废话。但是有没有想过它是如何工作的或者如何创建一个？让我们潜入<strong class="is hj">神经机器翻译的世界。</strong></p><h1 id="be9e" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">表面上的编码器-解码器模型</h1><p id="639c" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated"><strong class="is hj">编码器:</strong>编码器的作用是吸收输入句子中存在的信息，并将这些信息编码成一个数组/一组数字，因此得名编码器。</p><p id="bcaa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">解码器:</strong>同样，对于解码器来说，它的工作是学习如何使用编码器提供给它的信息，将其转换成与提供给编码器的句子/文本具有相同含义的另一种语言句子。</p><h1 id="18c6" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">深入研究模型</h1><p id="8473" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">要理解下面的解释和代码，您必须熟悉一般的<strong class="is hj"><em class="la"/></strong><em class="la"/>和<a class="ae lb" href="https://www.tensorflow.org/tutorials" rel="noopener ugc nofollow" target="_blank"><strong class="is hj"><em class="la">tensor flow</em></strong></a>。</p><h2 id="cd76" class="lc jy hi bd jz ld le lf kd lg lh li kh jb lj lk kl jf ll lm kp jj ln lo kt lp bi translated"><strong class="ak">预处理</strong></h2><p id="0e19" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">在下面的函数中，参数是您的数据集拥有的句子的一个<strong class="is hj"> <em class="la">语言</em> </strong> ( <em class="la"> NumPy 数组</em>)，<strong class="is hj"><em class="la">vocab _ size</em></strong>(<em class="la">int</em>)这是您认为您的语言将拥有的唯一且有意义的完整单词的数量(这是一个可以调整的超参数，例如，您可以将其作为 10000)， <strong class="is hj"/></p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="lu lv l"/></div></figure><p id="7604" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，如果语言是我们模型的输出，我们希望它有<em class="la">‘序列开始’</em>和<em class="la">‘序列结束’</em>标记，“sos”将作为<em class="la">第一时间步</em>输入，这样我们可以让模型学习仅使用编码器隐藏状态(解码器的初始状态)来预测输出句子中的第一个单词。 然后使用<em class="la">‘Tokenizer’</em>类创建一个记号化器，因为我们使用的是文本，即句子，所以我们将使用<em class="la"> fit_on_text </em>方法，这样记号化器可以为文本尸体中的每个唯一单词分配一个记号，最后，为了将每个句子转换成一个记号向量，我们使用<em class="la"> texts_to_sequence </em>方法。</p><p id="5e93" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，由于成批的句子必须具有相同的长度，我们必须填充它们，输出填充<em class="la">、</em>和输入填充<em class="la">、【pre】、</em>，原因是我们将反转输入张量，因此翻转<em class="la">、【pre】、</em>后，填充将变成<em class="la">、【post】、</em>，这就是我们想要的。</p><p id="ad2c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果文本被输入，它通常在输入到编码器之前被反转。例如,“我踢足球”变成了“我踢足球”。这很有用，因为这里的“I”是解码器需要翻译的第一件事。</p><h1 id="93b8" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">模型</h1><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/1c7b5423889359d0405f5b4de760544c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lf4C7Z3TJk1V9gUPb3ao0w.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">机器翻译(oreilly.com)</figcaption></figure><p id="a024" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">编码器型号</strong></p><p id="e0af" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码器模型由两个主要层组成，<em class="la">嵌入层，</em>这一层创建每个记号映射到的高维空间，例如，让我们假设"&lt; sos &gt;"具有记号" 2 "，嵌入层的输出将是形状的向量(1，嵌入维度)，现在每个记号的嵌入在训练期间被学习，因此，像"好"、"棒极了"、"很棒"这样的词将被聚集在一起，而像"最差"、"悲伤"这样的词， “bad”将在距离前一组一定距离处形成一个簇，简而言之，具有相同含义或相似性的单词被聚集在一起，嵌入的另一个优点是它不是离散的而是矢量化的。因此，模型可能在训练期间将“great”和“est”学习为不同的单词，但是它将能够在预测期间理解“greest”。 <a class="ae lb" href="https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12" rel="noopener" target="_blank">了解有关嵌入的更多信息</a>。</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="lu lv l"/></div></figure><p id="f9f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">函数<em class="la"> init_hidden_state </em>用于返回形状(batch，num。神经元的)所以，它可以作为可以传递给编码器的第一个状态。模型的其余部分是琐碎的。</p><p id="48c1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">解码器型号</strong></p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="lu lv l"/></div></figure><p id="bad1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">解码器模型与编码器相同，只有一个完全连接层的不同，因此我们可以预测稀疏类，即目标语言的离散标记。</p><p id="52b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">自定义损失功能</strong></p><p id="ab7f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们需要一个定制的损失函数来计算损失，因此，我们可以使用它进行反向传播来更新权重。需要自定义损失，因为你们可能已经注意到，我们刚刚创建了两个类 Encoder 和 Decoder，但从未将它们绑定到 Keras 模型类，我们这样做是因为在训练期间，我们希望将一个时间步长传递给模型以及模型可能预测的任何内容，对于下一个输入，我们将传递真实标签作为输入，这样模型就可以为下一个时间步长进行自我训练。 这种方法叫做<a class="ae lb" href="https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c" rel="noopener" target="_blank"> <em class="la">老师逼迫</em> </a>，就像我们做错了事，老师纠正我们一样。 在推断过程中，我们只是将解码器的输出作为下一个时间步长的输入，这种类型的控制使用 Keras 模型类是不可行的。因此，我们必须使用自定义的训练循环和损耗。</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="lu lv l"/></div></figure><p id="39f5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们不希望我们的损失考虑填充令牌(一种令牌，使得特定语句中的每个句子可以具有相同的长度)，我们为此使用简单的<a class="ae lb" href="https://keras.io/api/layers/core_layers/masking/" rel="noopener ugc nofollow" target="_blank">掩码</a>，并使用‘SparseCategoricalCrossentropy’计算损失，并返回平均损失。</p><p id="d66b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">训练</strong></p><p id="bada" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在此函数中执行的所有张量操作都在一个由<a class="ae lb" href="https://www.tensorflow.org/api_docs/python/tf/GradientTape" rel="noopener ugc nofollow" target="_blank"> <em class="la">梯度带</em> </a> <em class="la">、</em>组成的块内，正如其名称所暗示的那样，它是一个“带”，记录所有张量操作，以便以后梯度可以沿网络向下流动。</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="lu lv l"/></div></figure><p id="d37c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输入和隐藏状态张量被传递给编码器，然后第一个时间步，即“sos”令牌加上编码器的最终隐藏状态被传递给解码器，解码器预测下一个时间步，即目标句子中的第一个单词，该时间步的真实标签和预测标签然后被传递给损失函数以计算损失，解码器的下一个输入是当前时间步的真实标签。这种类型的训练被称为<em class="la">教师强制</em>。最后，计算并应用梯度。</p><p id="2489" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">预测</strong></p><p id="be78" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在进行预测<em class="la">时，不执行教师强制</em>，并且先前的解码器输出充当当前时间戳的输入。</p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="lu lv l"/></div></figure><p id="a631" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“argmax”方法用于获取预测数组中最大值的索引，如果这是一个“eos”标记，则返回结果，否则将该单词附加到结果中。</p><h1 id="a1b4" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated"><strong class="ak">结果</strong></h1><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/43920a3d93f775c0637a555d4e7d5901.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WVBJTVIZS_YwQR1bBS1O0A.png"/></div></div></figure></div></div>    
</body>
</html>