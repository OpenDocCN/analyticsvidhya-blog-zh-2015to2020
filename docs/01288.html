<html>
<head>
<title>A Comparative Study of Clustering Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类算法的比较研究</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/comparative-study-of-the-clustering-algorithms-54d1ed9ea732?source=collection_archive---------2-----------------------#2019-10-13">https://medium.com/analytics-vidhya/comparative-study-of-the-clustering-algorithms-54d1ed9ea732?source=collection_archive---------2-----------------------#2019-10-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/aaa59e095adf7b705ef8b3c89dc46484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOPVU49imNwtZGT9dSP3JQ.jpeg"/></div></div></figure><p id="7e91" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">聚类基本上被定义为将数据分成相似对象的组。被称为簇的每个组由彼此相似的对象和与其他组相比不相似的对象组成。让我们比较不同类型的集群。讨论的算法有:<strong class="is hj"> <em class="jo"> k-means算法、层次聚类算法、自组织映射算法和期望最大化聚类算法。</em>T3】</strong></p><h1 id="3755" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">对比指标:</strong></h1><p id="8186" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">现在，我将决定讨论聚类算法之间比较的因素:</p><ol class=""><li id="8a6f" class="ks kt hi is b it iu ix iy jb ku jf kv jj kw jn kx ky kz la bi translated">数据集的大小</li><li id="aad5" class="ks kt hi is b it lb ix lc jb ld jf le jj lf jn kx ky kz la bi translated">聚类数</li><li id="17b5" class="ks kt hi is b it lb ix lc jb ld jf le jj lf jn kx ky kz la bi translated">数据集类型和使用的软件类型</li><li id="168e" class="ks kt hi is b it lb ix lc jb ld jf le jj lf jn kx ky kz la bi translated">算法的性能</li><li id="ba31" class="ks kt hi is b it lb ix lc jb ld jf le jj lf jn kx ky kz la bi translated">算法的准确性</li><li id="aad1" class="ks kt hi is b it lb ix lc jb ld jf le jj lf jn kx ky kz la bi translated">算法的质量</li></ol><h1 id="78b1" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">算法是如何实现的</strong>？</h1><p id="efe5" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">LNKnet软件:麻省理工学院林肯实验室提供的公共领域软件。</p><p id="a673" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">聚类和树形视图软件:</strong>聚类和树形视图软件是为分析从不同数据集发现的数据提供计算和图形环境的程序。</p><p id="2a56" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">选择该软件的原因是它们是实现几种数据聚类算法的最流行的软件。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/eb552572b01969bb8be213e48b05b076.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*duHMTsPwMG1UBkHoPlqFoA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">比较</figcaption></figure><p id="4288" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">现在</strong>我将开始单独详细讨论聚类算法，并展示如何实现<strong class="is hj"> </strong>的代码。</p><h1 id="9b46" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">1—K-表示:</strong></h1><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/b66878b0921af52c932116f91bf4378a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*Rs6-9EUJRCetxmL8.gif"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">k均值的工作原理</figcaption></figure><p id="f92c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">K-means是一种众所周知的划分方法。对象被分类为k个组中的一个，k被选作先验。通过计算每个组的质心并将每个对象分配给质心最近的组来确定簇成员。这种方法通过重复重新分配集群成员来最小化集群内的总体分散。</p><p id="d627" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在一般意义上，k划分算法将一组对象S和一个整数k作为输入，并将S划分成子集S1、S2、S3…Sk。它使用平方和作为优化准则，设xri是Si的第r个元素，|Si|是Si中元素的数量，d(xri，xsi)是xri和xsi之间的距离。</p><h2 id="2acf" class="lq jq hi bd jr lr ls lt jv lu lv lw jz jb lx ly kd jf lz ma kh jj mb mc kl md bi translated"><strong class="ak">算法:</strong></h2><p id="4e66" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">第一步:选择K作为聚类数。</p><p id="4456" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">步骤2:初始化K个簇的码本向量(例如，随机地)</p><p id="16d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">步骤3:对于每个新的样本向量:计算新向量和每个簇的码本向量之间的距离。使用在<strong class="is hj">时间内减少的学习率，用新矢量重新计算最近的码本矢量。</strong></p><h2 id="d023" class="lq jq hi bd jr lr ls lt jv lu lv lw jz jb lx ly kd jf lz ma kh jj mb mc kl md bi translated"><strong class="ak">时间复杂度和空间复杂度</strong>复杂度<strong class="ak"> : </strong></h2><p id="1412" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">其时间复杂度为O(nkl)，其中n为模式数，k为聚类数，l为算法收敛所需的迭代次数。其空间复杂度为O(k+n)。它需要额外的空间来存储数据矩阵。它是顺序独立的；对于给定的聚类中心初始种子集，它生成相同的数据分区，而不管模式呈现给算法的顺序。</p><h2 id="5ec8" class="lq jq hi bd jr lr ls lt jv lu lv lw jz jb lx ly kd jf lz ma kh jj mb mc kl md bi translated"><strong class="ak">如何实现K-means？</strong></h2><p id="9aab" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我们有sklearn来实现k-means。</p><p id="2378" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">K-Means的代码样本:</strong></p><pre class="lh li lj lk fd me mf mg mh aw mi bi"><span id="2b67" class="lq jq hi mf b fi mj mk l ml mm">from pandas import DataFrame<br/>import matplotlib.pyplot as plt<br/>from sklearn.cluster import KMeans<br/>#making a toy dataset<br/>Data = {'x': [25,34,22,27,33,33,31,22,35,34,67,54,57,43,50,57,59,52,65,47,49,48,35,33,44,45,38,43,51,46],<br/>        'y': [79,51,53,78,59,74,73,57,69,75,51,32,40,47,53,36,35,58,59,50,25,20,14,12,20,5,29,27,8,7]<br/>       }<br/>#converting into dataframe<br/>df = DataFrame(Data,columns=['x','y'])<br/>  <br/>kmeans = KMeans(n_clusters=3).fit(df)#applying kmeans on the dataset                      #with number of clusters=3<br/>centroids = kmeans.cluster_centers_<br/>print(centroids)<br/><br/>plt.scatter(df['x'], df['y'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)<br/>plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)</span></pre><p id="2489" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的代码片段中，我使用kmeans算法来解释如何在代码中使用kmeans。kmeans的应用非常简单。sklearn货源充足。在这里，我做了一个玩具数据集，并在上面应用了kmeans。</p><p id="9d64" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">输出:</strong></p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/69f730efa7bdcb2b558a1d23db376bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*swKt9fkhwH9NOpjCtbszlQ.png"/></div></div></figure><h1 id="acba" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">2 —分层聚类算法:</h1><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/c5b1108c5aef404ea1b0b50150167580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Mv9bXIuIfH9DAqyx.gif"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">分层聚类的工作原理</figcaption></figure><p id="ca93" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">分区算法基于指定组的初始数量，并迭代地在组之间重新分配对象以收敛。相比之下，分层算法组合或划分现有的组，创建反映组被合并或划分的顺序的分层结构。在凝聚方法中，通过合并建立层次结构，对象最初属于一个单例集合列表S1，…，S2，Sn。然后，使用成本函数从列表中找到要合并的“最便宜”的集合对{Si，Sj}。一旦合并，Si和Sj将从集合列表中删除，并替换为Si U Sj。这个过程重复进行，直到所有对象都在一个组中。凝聚层次聚类算法的不同变体可以使用不同的成本函数。完全连锁、平均连锁和单一连锁方法分别使用两个聚类成员之间的最大、平均和最小距离。</p><h2 id="c781" class="lq jq hi bd jr lr ls lt jv lu lv lw jz jb lx ly kd jf lz ma kh jj mb mc kl md bi translated"><strong class="ak">算法:</strong></h2><p id="50d2" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">步骤1-计算包含每对模式之间距离的邻近矩阵。将每个模式视为一个集群。</p><p id="247f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">步骤2-使用邻近度矩阵找到最相似的聚类对。将这两个集群合并为一个集群。</p><p id="ec34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">步骤3-如果所有模式都在一个集群中，停止。否则，转到步骤2</p><p id="9f68" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">优点:</strong></p><p id="3222" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">关于粒度级别的嵌入式灵活性。</p><p id="667c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">易于处理任何形式的相似性或距离。</p><p id="6377" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此适用于任何属性类型。</p><p id="4411" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">分层聚类算法更加通用。</p><h2 id="678d" class="lq jq hi bd jr lr ls lt jv lu lv lw jz jb lx ly kd jf lz ma kh jj mb mc kl md bi translated"><strong class="ak">时间复杂度和空间复杂度</strong>复杂度<strong class="ak"> : </strong></h2><p id="4647" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">时间复杂度= O(n)其中n是数据点的数量。</p><p id="5476" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">空间复杂度= O(n)其中n是数据点的数量。</p><h2 id="032d" class="lq jq hi bd jr lr ls lt jv lu lv lw jz jb lx ly kd jf lz ma kh jj mb mc kl md bi translated"><strong class="ak">如何实现层次聚类算法？</strong></h2><pre class="lh li lj lk fd me mf mg mh aw mi bi"><span id="cfbb" class="lq jq hi mf b fi mj mk l ml mm">from sklearn.cluster import AgglomerativeClustering<br/>cluster = AgglomerativeClustering(n_clusters=3, affinity=’euclidean’, linkage=’ward’) <br/>cluster.fit_predict(df)<br/>plt.figure(figsize=(10, 7)) <br/>plt.scatter(df[‘x’], df[‘y’], c=cluster.labels_)</span></pre><p id="dbf4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我在同一个玩具数据集上应用了层次聚类算法。有三个集群。</p><p id="2309" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">输出:</strong></p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/73f3353d1691b3f928d23548506efdfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ksrQuaeMu9iQarOIedVQMA.png"/></div></div></figure><h1 id="8c4f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">3 —自组织映射算法</h1><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/82b3ef6eb7c06b6325e114b3564ce7eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*eehec1ZZ_4vMSe69GTzYCg.png"/></div></figure><p id="7411" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">受大脑中神经网络的启发，自组织映射(SOM)使用竞争和合作机制来实现无监督学习。在经典的SOM中，一组节点以几何图案排列，通常是二维点阵。每个节点都与一个与输入空间维数相同的权重向量相关联。SOM的目的是找到从高维输入空间到节点的2-D表示的良好映射。使用SOM进行聚类的一种方式是将由相同节点表示的输入空间中的对象视为分组到一个聚类中。</p><p id="2e8a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">算法:</strong></p><p id="438b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面简单讨论一下算法。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/c4b3445a2aebb168b88a9765b375cb3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/0*156tjT5_orgbxJ6G.gif"/></div></figure><p id="a4e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">优势:</strong></p><ul class=""><li id="0951" class="ks kt hi is b it iu ix iy jb ku jf kv jj kw jn ms ky kz la bi translated">som简化了聚类，并允许用户直观地识别同质数据组。在Viscovery中，有几种聚类算法(SOM Single Linkage、Ward和SOM-Ward)可用于自动构建聚类。这很容易理解，而且效果很好。</li></ul><p id="8ccc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">时间复杂度:</strong></p><p id="92ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">T=O(NC)=O(S^2)</p><p id="f997" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">如何实现SOM </strong></p><p id="852d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我在同一个玩具数据集上使用SOM。</p><pre class="lh li lj lk fd me mf mg mh aw mi bi"><span id="f398" class="lq jq hi mf b fi mj mk l ml mm">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from mpl_toolkits.mplot3d import Axes3D<br/>import somoclu<br/>data = np.float32(df)<br/>n_rows, n_columns = 100, 160<br/>som = somoclu.Somoclu(n_columns, n_rows, data=data)<br/>som.train()<br/>som.view_component_planes()</span></pre><p id="d91e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">输出:</strong></p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/72798efe682d9f9a1f9ee003e3bf61e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*4Koc8BH2-Fofxsc2hgD58w.png"/></div></div></figure><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/d11f3cd05a0724224ca3a111c8a192d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*qpV1m9RhhMAecffi_fNZAQ.png"/></div></figure><h1 id="9046" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">4 —期望值最大化聚类算法</h1><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/b7f83ebd439d20bcc030f494f2506164.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*xXknjjRWhEFsx8AbPRixKA.png"/></div></figure><p id="0233" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们开始讨论这个算法。期望最大化(EM)是统计界公认的聚类算法。EM是一种基于距离的算法，它假设数据集可以建模为多元正态分布的线性组合，并且该算法可以找到最大化模型质量度量(称为对数似然)的分布参数。</p><p id="6956" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">算法:</strong></p><p id="43c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面开始讨论一下算法步骤。</p><p id="210b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">步骤1:给定一组不完整的数据，考虑一组起始参数。</p><p id="1d68" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">步骤2:期望步骤(E-step):使用数据集的观察到的可用数据，估计(猜测)缺失数据的值。</p><p id="0fcf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">步骤3:最大化步骤(M-步骤):使用期望(E)步骤之后生成的完整数据，以便更新参数。</p><p id="3cfe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第四步:重复第二步和第三步，直到收敛。</p><p id="d462" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">优点:</strong></p><ul class=""><li id="82e3" class="ks kt hi is b it iu ix iy jb ku jf kv jj kw jn ms ky kz la bi translated">它收敛缓慢。</li><li id="27f4" class="ks kt hi is b it lb ix lc jb ld jf le jj lf jn ms ky kz la bi translated">它只收敛到局部最优。</li><li id="05ec" class="ks kt hi is b it lb ix lc jb ld jf le jj lf jn ms ky kz la bi translated">它需要向前和向后的概率(数值优化只需要向前的概率)。</li></ul><p id="037b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">时间复杂度:</strong></p><p id="0b72" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">O(m.n)，其中m是迭代次数，n是参数个数。</p><p id="eb67" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">如何实现EM算法？</strong></p><p id="25b2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为期望最大化算法是高斯混合模型。</p><pre class="lh li lj lk fd me mf mg mh aw mi bi"><span id="72cb" class="lq jq hi mf b fi mj mk l ml mm">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.mixture import GaussianMixture<br/>gmm = GaussianMixture(n_components=3)<br/>gmm.fit(data)<br/>plt.scatter(df['x'], df['y'])</span></pre><p id="aabe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">我想就此结束对算法的比较研究。希望我已经对算法进行了清晰的对比研究。非常感谢你的阅读！</strong></p><h1 id="01de" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">参考文献:</strong></h1><div class="mw mx ez fb my mz"><a href="https://en.wikipedia.org/wiki/Mixture_model" rel="noopener  ugc nofollow" target="_blank"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hj fi z dy ne ea eb nf ed ef hh bi translated">混合模型</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">在统计学中，混合模型是一种概率模型，用于表示一个样本中存在的子群体</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">en.wikipedia.org</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn io mz"/></div></div></a></div><div class="mw mx ez fb my mz"><a href="https://en.wikipedia.org/wiki/Hierarchical_database_model" rel="noopener  ugc nofollow" target="_blank"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hj fi z dy ne ea eb nf ed ef hh bi translated">分层数据库模型</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">分层数据库模型是一种数据模型，其中数据被组织成树状结构。数据是…</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">en.wikipedia.org</p></div></div><div class="ni l"><div class="no l nk nl nm ni nn io mz"/></div></div></a></div><div class="mw mx ez fb my mz"><a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" rel="noopener  ugc nofollow" target="_blank"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hj fi z dy ne ea eb nf ed ef hh bi translated">分层聚类</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">在数据挖掘和统计中，层次聚类(也称为层次聚类分析或HCA)是一种方法…</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">en.wikipedia.org</p></div></div><div class="ni l"><div class="np l nk nl nm ni nn io mz"/></div></div></a></div></div></div>    
</body>
</html>