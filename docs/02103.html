<html>
<head>
<title>Introduction to Gradient Descent Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降算法简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-gradient-descent-algorithm-55e60f53680b?source=collection_archive---------10-----------------------#2019-11-30">https://medium.com/analytics-vidhya/introduction-to-gradient-descent-algorithm-55e60f53680b?source=collection_archive---------10-----------------------#2019-11-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/da09156148f1c4664e925a4edf41ae9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mBDF_9Zx2XSpoVyZleVPFA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">梯度下降</strong></figcaption></figure><p id="01c4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">想象你蒙着眼睛站在山顶上。你被要求下山并找到山谷。你会怎么做？因为你不确定你需要向哪里和哪个方向移动才能到达地面，你可能会在更高的倾斜方向上迈出小步，并试图找出这条路是否通向你的目的地。你会重复这个过程，直到你到达地面。这正是梯度下降算法的工作原理。</p><p id="e15c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度下降是一种优化算法，广泛用于机器学习问题，以最小化成本函数。所以等等！成本函数到底是什么？</p><h2 id="f296" class="jt ju hi bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">价值函数</h2><p id="d8cc" class="pw-post-body-paragraph iv iw hi ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js hb bi translated">成本函数是你想要最小化的东西。例如，在线性回归的情况下，当您试图用一条线来拟合您的数据点时，它可能不会精确地拟合数据集中的每一个点。成本函数帮助我们衡量预测值与相应的真实值有多接近。如果x是输入变量，y是输出变量，h(假设)是我们学习算法的预测输出。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/de469f005d0904ecb7c8df59042b38b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*RJo2uXud05SiRQs-hr3R1A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">实际与预测</figcaption></figure><p id="bd23" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">hθ(x) = θ0 + θ1x，其中θ0为截距，θ1为梯度或斜率。</p><p id="8b93" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的目标是使输出y和预测输出hθ(x)之间的误差最小。即最小化(hθ(x)-y)* * 2。这也叫<strong class="ix hj">误差平方和。</strong>从数学上讲，这个成本函数可以写成</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/70da5b26d3571c464595a2bd3e3d27c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*KC937GsfAzUcLIkU3b4Q2w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">线性回归的成本函数</figcaption></figure><p id="4b01" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的目的是确定θ的值，使假设尽可能准确。换句话说，尽可能使J(θ0，θ1)最小。</p><p id="85a4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们对成本函数有了一些了解，让我们试着理解梯度下降到底是如何帮助我们降低这个成本函数的。</p><h2 id="2012" class="jt ju hi bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated"><strong class="ak">梯度下降</strong></h2><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/acfdca122622f108c6f56a07e1ced431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t4aYsxpCqz2eymJ4zkUS9Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">梯度下降(图片来源:Coursera)</figcaption></figure><p id="1597" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们用一些初始权重(θ0和θ1)运行我们的算法，梯度下降不断更新这些权重，直到它找到这些权重的最佳值，这导致最小化成本函数。</p><p id="96b5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">简单来说，我们的目标是从红色区域移动到蓝色区域，如上图所示。最初，我们从θ0和θ1的任意随机值(比如0，0)开始。在每一步中，我们都保持小幅更新θ0和θ1的值，以尝试降低J(θ0，θ1)。我们需要不断更新这些值，直到达到局部最小值。</p><p id="a1d4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有趣的是，对于一个给定的问题，可以有多个局部最小值，如上图所示。我们的起点决定了我们最终到达哪个局部最小值。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/338fcf1fa0767a629a4951d26178dc6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*NcsmWFvdn3ww1HwNTFPr5A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">梯度下降目标和更新规则</figcaption></figure><p id="4c96" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们对每一个权重取成本函数的偏导数。这里<strong class="ix hj">α</strong>符号表示<strong class="ix hj">学习率。</strong></p><p id="3cd8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">学习率</strong></p><p id="6dc8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将学习速度设定在最佳水平是很重要的。如果学习速率太高，那么它会导致过大的步长，这又会导致超过最小值，并且梯度下降永远不会收敛到局部最小值。如果学习率太低，步长将非常小，梯度下降将花费大量时间收敛到局部最小值。</p><p id="40d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一个需要注意的重要事情是，我们不必在每一步都改变alpha的值。因为随着梯度下降接近全局最小值，导数项变得更小，所以即使更新也变得更小，并且算法在接近最小值时采取更小的步骤。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es la"><img src="../Images/de16976098a701cb2c706a97f4227f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*heQR61fa32f8OzjYSyE5tg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">学习率(图片来源:GitHub)</figcaption></figure><p id="a0bf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在线性回归的情况下，成本函数总是一个<strong class="ix hj">凸函数(碗形)</strong>，并且总是有一个最小值。所以梯度下降总是会收敛到全局最优。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/346a59a849bd8872b25393d903ea2749.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*YKfQ7B4-BzqmQLYSHu5JbA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">线性回归的成本函数</figcaption></figure><h2 id="64ee" class="jt ju hi bd iu jv jw jx jy jz ka kb kc jg kd ke kf jk kg kh ki jo kj kk kl km bi translated">结论</h2><p id="5d85" class="pw-post-body-paragraph iv iw hi ix b iy kn ja jb jc ko je jf jg kp ji jj jk kq jm jn jo kr jq jr js hb bi translated">梯度下降有助于我们降低成本函数，从而提高我们的机器学习模型的准确性。由于其减少误差的能力，并且由于其可以应用于具有许多变量的大型数据集，它被用于大多数机器学习算法中。</p><p id="b481" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lc">这是我在Medium上的第一篇帖子。喜欢的请一定要分享和鼓掌！！</em></p></div></div>    
</body>
</html>