<html>
<head>
<title>Semi-supervised learning (note of ML class of Hung-Yi Lee)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">半监督学习(洪-李易的ML类笔记)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/semi-supervised-learning-note-of-ml-class-of-hung-yi-lee-3aed0a7207f1?source=collection_archive---------12-----------------------#2020-02-05">https://medium.com/analytics-vidhya/semi-supervised-learning-note-of-ml-class-of-hung-yi-lee-3aed0a7207f1?source=collection_archive---------12-----------------------#2020-02-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0ec1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">讲座视频:<a class="ae jd" href="https://www.youtube.com/watch?v=fX_guE7JNnY&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=21" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=fX_guE7JNnY&amp;list = PLJV _ El 3 uvtspy 9 ocry 30 obpnlc 89 Yu 49&amp;index = 21</a></p><p id="4fe9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是半监督学习中大家很好的辅导讲座。如果你懂中文，这个讲座会是个不错的选择。<br/>本帖所有数字均来自本次讲座。</p><p id="6e11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">半监督学习的第一点是，我们必须知道'<em class="je">直推式</em>和'<em class="je">归纳式</em>'学习之间的区别。<br/> —直推式学习:使用测试集作为无标签数据，但不使用测试集的标签。<br/> —归纳学习:找到更多的数据作为未标记数据。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jf"><img src="../Images/13cc83c36e44a7c98f2cd10c7987793a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*9ac3pRVsA1D-XWG0lQ-Pwg.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">生成模型中的半监督学习</figcaption></figure><p id="e031" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">半监督学习在生成模型中是如何实现的？</p><ol class=""><li id="5a01" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated">通过随机初始化或通过预训练模型初始化模型的参数。</li><li id="67fa" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">步骤1:计算未标记数据的后验概率</li><li id="b3a3" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">步骤2:更新未标记数据的先验概率和均值</li><li id="38ae" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">使用新的先验和均值回到步骤1来更新后验概率</li></ol><p id="8c8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">整个过程看起来像EM(期望最大化)算法，第一步是‘E’，第二步是‘M’。</p><p id="7ee7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2半监督学习实现中经常用到的假设是低密度分离和光滑假设。</p><h1 id="0481" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">低密度分色(黑白世界)</strong></h1><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ld"><img src="../Images/713b411673c3180f3214396bd1180c95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*1tHBIL0mJsu8zXlUKi8Tiw.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">半监督学习中低密度分离最典型的例子是自训练</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es le"><img src="../Images/ab5a4a595fe099c3a6bf095439548ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*I8i8t8FB0L79l7wbh2wKTQ.png"/></div></figure><p id="45aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然自训练看起来很像生成模型中的半监督学习，但是我们经常在自训练中使用硬标签，在生成模型中使用软标签。为什么？如果我们在NN模型中使用软标签，例如，对新目标使用[0.7±0.3]，我们将无法更新模型，因为新目标与原始标签相同。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lf"><img src="../Images/8ab346e322ef90176a18a5c24e40cc40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*HbE1-gSH3z0lUD2O_hj95Q.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">未标记的数据如何能够更新模型</figcaption></figure><p id="b3ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如何让‘未标记数据→标记数据’影响模型训练？修改损失函数！我们计算计算输出(新标签)的熵，并将其作为正则项添加到损失函数中。我们都知道神经网络训练的重点是最小化损失，因此新标签的熵也在最小化过程中。</p><p id="bfbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么有效？看一下上图的左边部分。在进行分类时，我们总是希望输出结果是“简单的”。如果所有类别的概率非常接近，我们将认为模型仍然需要更多的时间来训练。这很糟糕。根据这个概念，我们知道输出熵越小，输出越好。此外，它满足低密度分离的假设。</p><h1 id="f3f4" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">平滑度假设</h1><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lg"><img src="../Images/8e94fa6b6fa186dfb40c1b25fd6bdeeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*w2myRf3-A6FvjkxCPYXxMw.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">平滑假设的概念</figcaption></figure><p id="5a62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">光滑性假设的概念是，如果点在同一个高密度簇中，那么成为标签的概率就高。换句话说，如果我们发现接近的点属于同一个标签，这个模型(函数)就是光滑的。</p><p id="7634" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将用两个例子来解释这个概念。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lh"><img src="../Images/8d64db9f127287be29872d396e6b8169.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*QAvRvzf_iOSPuDV30NHRAQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">示例1</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/38b5d73421dcdb2a72ec3af8cb304e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*3PFICGCf6LHia_S0rVPJnw.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">示例2</figcaption></figure><p id="18dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在例1中，如果NN一开始看到图片1、2和3，它可能认为图片2和3属于同一个标签。随着我们收集更多的数据(图4~7)，我们会发现图1、2和4~7可能“在同一高密度区域”，因此它们应该是同一类。</p><p id="8ba4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例2是文档分类。开始时，数据集中只有文档1、2、3和4，其中文档1是类1，文档4是类2，文档3是未标记的数据。模型很难对文档2和3进行正确的分类。在我们收集更多的文件之后，我们将能够做正确的分类。</p><p id="2d2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们也可以将集群扩展到图中。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/f354c72170ffdd8f75a2fb9dc2913c58.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*obzEx_G6nobhisiq2tsp-w.png"/></div></figure><p id="e62c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果节点在同一个连通图中，它们属于同一个类的概率就越高。麻烦的是数据集的数量。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lj"><img src="../Images/28da95c3b21357be919cd2a293055ceb.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*EVTJ6W4MJBsSZJu3fvlG7A.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">数据不足的例子</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lk"><img src="../Images/43c7f3b0e3df3774111bf6574552b907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*62j7XDVsv6Hbh9-x3iofbw.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">将点制成图形方法</figcaption></figure><p id="3cef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用deep autoencoder提取数据(标记和未标记数据)特征，并进行聚类。然后把邻居连起来形成一个图。我们也可以计算高斯RBF的倒数作为边的权重。</p><p id="7c57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图构造完成后，如何将图的概念和半监督学习结合起来？</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ll"><img src="../Images/ccf6ad0005ae44a0789c9c4354474e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*74tO2Y-BjOoR2YfEnzNb_w.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx">1</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lm"><img src="../Images/d289c1cc5e99e3510b36b98c8c704606.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*RhTR1NLJdt91Xk1J8wZkUw.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx">2</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ln"><img src="../Images/3f60cccb25632830c125ec0cc0483d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*0IV9QbZ4yoVVmNq6NkAaMg.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx">3</figcaption></figure><p id="4e62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(图1)我们定义了图的光滑度。图中图形实例的节点嵌入是标量，而现实世界中的节点嵌入通常是矩阵。</p><p id="e4e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(图2)矩阵形式的图的光滑度函数。</p><p id="85db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(图3)将平滑度作为正则项添加到损失函数中。然后更新模型参数。然后再次计算平滑度…</p></div></div>    
</body>
</html>