<html>
<head>
<title>Generic Model-Agnostic CNN(GMAN) For Single Image Dehazing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于单个图像去雾的通用模型不可知 CNN(GMAN)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gman-net-for-image-dehazing-65a2b3f679a5?source=collection_archive---------4-----------------------#2020-09-30">https://medium.com/analytics-vidhya/gman-net-for-image-dehazing-65a2b3f679a5?source=collection_archive---------4-----------------------#2020-09-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="aaad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">论文说明:刘铮的“用于单个图像去雾的通用模型不可知卷积神经网络”及其在<a class="jd je ge" href="https://medium.com/u/b1d410cb9700?source=post_page-----65a2b3f679a5--------------------------------" rel="noopener" target="_blank"> TensorFlow </a>(版本 2+)中的实现。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/e2070dc1063e03102f1967aefc2192c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1CpNH7V8Ru8cm5ROQDo9xg.jpeg"/></div></div></figure><h1 id="984f" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">介绍</h1><p id="c7a0" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">薄雾和烟雾是影响图像质量以及图像分析的最常见的环境因素。提出了一种端到端的图像去雾生成方法。它基于设计一个完全卷积的神经网络来识别输入图像中的霾结构，并恢复清晰、无霾的图像。</p><p id="ea5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通用数据模型是对传统数据模型的推测。它们描述了规范化的通用连接类型，以及可能通过这种连接类型连接的各种事物。</p><blockquote class="ku kv kw"><p id="c26c" class="if ig kx ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">为什么这个方法是“不可知论的”，我们来回答一下。</p><p id="e406" class="if ig kx ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">到目前为止，所有提出的最新(SOTA)方法都探索了大气散射(解释如下)模型。GMAN 网络是不可知论的，因为它没有使用大气散射模型，却比所有使用该模型的论文产生了更好的结果。</p></blockquote><p id="7f35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在这里获得<a class="ae lb" href="https://arxiv.org/abs/1810.02862" rel="noopener ugc nofollow" target="_blank">的研究论文。</a></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es lc"><img src="../Images/46529a03798727623c5f537d3ef65fd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0MFC3OkDGpLD7cKssUs37w.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated"><strong class="bd jt">左:</strong>朦胧图像，<strong class="bd jt">右:</strong>使用 GMAN 的去雾图像</figcaption></figure><h1 id="d3b3" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">大气散射模型</h1><p id="32b6" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">用于表示 ASM 的等式为:</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lh"><img src="../Images/d51e29bbd65554602549a571e1c48953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*kuizBQkGDryS91eOwUvxDQ.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated"><a class="ae lb" href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Li_AOD-Net_All-In-One_Dehazing_ICCV_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank">形象信用</a></figcaption></figure><p id="90ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，<br/> I(x):观测到的雾天图像<br/> J(x):原始图像<br/> A:全球大气光照<br/> t(x):传输矩阵</p><p id="15eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kx"> A </em>指整个场景中大气的自然光。<br/> <em class="kx"> t(x) </em>代表从物体到达相机的光量。<br/>计算如下:</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es li"><img src="../Images/951a257045eacc052c5100c608b88e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*itCwVWi4FN0ng3KPEJlMOw.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated"><a class="ae lb" href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Li_AOD-Net_All-In-One_Dehazing_ICCV_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank">形象信用</a></figcaption></figure><p id="660d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在 GMAN 之前，所有的方法都致力于获得参数 A 和 t(x ),以从模糊的图像中恢复清晰的图像。但是原始图像的有损重建问题不太可能等价地转换成参数 A 和 t(x)(或它们的变型)的估计问题，至少当这两个问题服从相同的评估度量时是如此。除此之外，原始图像和模糊图像之间的复杂关系不能仅仅通过大气散射模型(ASM)来捕捉。此外，使用 ASM 可以在合成模糊图像上给出好的结果，但是它不能在自然模糊图像上产生期望的结果。GMAN 网络提出了一个解决方案。</p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="f112" class="jr js hi bd jt ju lq jw jx jy lr ka kb kc ls ke kf kg lt ki kj kk lu km kn ko bi translated">GMAN +并行网络架构</h1><h2 id="3ecf" class="lv js hi bd jt lw lx ly jx lz ma mb kb iq mc md kf iu me mf kj iy mg mh kn mi bi translated">让我们从 GMAN 开始</h2><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mj"><img src="../Images/1bea0c0a4e77528ca2e8b80e52193ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nE7o6HsOrdUUff3_XJoXyQ.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated"><strong class="bd jt"> GMAN 建筑(没有感性损失)</strong></figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es mk"><img src="../Images/406737b195c9704da2626a5556d6dfae.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*WPqNf1ykPrSk4QnQMAvKQg.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">有快捷连接的 CNN。</figcaption></figure><p id="86c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从功能上来说，这种架构由端到端生成方法组成，该方法使用编码器-解码器方法来解决去雾问题。输入图像遇到的前两层是具有 64 个通道的卷积块。紧随其后的是跨距为 2 的 2 个下采样块(编码器)。然后，编码图像被馈送到具有 4 个残余块的层构建。每个块包含一个快捷连接(与 ResNets 相同)。这个残余层有助于理解霾结构。在这之后是上采样或去卷积(解码器)层，其重构剩余层的输出。最后两层是卷积块，用于将上采样的特征图转换为 3 通道 RGB 图像，最终添加到输入图像(全局残差层)，然后 ReLU 以给出去雾输出。这个全局剩余层有助于捕捉场景中具有不同深度的对象的边界细节。架构的编码器部分有助于降低图像的维度，然后将降采样后的图像馈送到残差层以提取图像特征，解码器部分预期学习并重新生成无霾图像的缺失数据。</p><p id="8a88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在原始论文中，没有并行网络，但为了提高性能和均方损耗，也使用了感知损耗，但同一作者后来添加了该并行网络，其性能优于具有感知损耗的原始架构。所以我不会在这里失去知觉。</p><h2 id="e0d4" class="lv js hi bd jt lw lx ly jx lz ma mb kb iq mc md kf iu me mf kj iy mg mh kn mi bi translated">现在让我们来处理并行网络</h2><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es ml"><img src="../Images/eaa86949d4d6561347bb5eda4adbcc66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*LKeCPxyPW6XSZkTRrzQMCQ.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">平行网络，这些方块的膨胀率是绿色:4，蓝色:2，紫色:1</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es mm"><img src="../Images/a02983cb9cfe6657e6edc0919793f2e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*rZzC8wZL8oVFREUxr2l4dg.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">核大小为 3，膨胀率为 2 的膨胀卷积。</figcaption></figure><p id="6957" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PN 架构比 GMAN 浅，但具有相同的编码器-解码器结构。但它的不同之处在于，它使用了<a class="ae lb" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md" rel="noopener ugc nofollow" target="_blank">膨胀卷积</a>(或带孔卷积)来代替传统卷积。具有较大接收场的卷积层不可学习以生成概化的特征地图。膨胀允许接收场的指数增长，而没有像素级的空间维度损失。简而言之，这个平行网络有助于捕捉被 GMAN 网络忽略的细节。前 3 层分别是膨胀率为 4、2、2 的编码器块。接下来的三层的膨胀率为 1。随后是膨胀率为 4 的去卷积块，以将特征图转换为它们的原始尺寸。之后是最终的卷积层，它将图像转换为 3 通道 RGB 图像。现在，除了最后一层，所有层都有 64 个通道。之后，与 GMAN 相同的输出与原始输入相加，并通过 ReLU 单元。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es mn"><img src="../Images/e6470bba55b11460a1e5dab62bceecf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*ISHeGzmrhhL9YVSZL7r4Sw.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">组合网络</figcaption></figure><p id="8448" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，在所有这两个网络的输出相加后，得到最终的去雾图像。参数α1 和α2 应该由网络自己学习。理论讲够了，现在我们从代码开始。</p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="8b9a" class="jr js hi bd jt ju lq jw jx jy lr ka kb kc ls ke kf kg lt ki kj kk lu km kn ko bi translated">代码在<a class="jd je ge" href="https://medium.com/u/b1d410cb9700?source=post_page-----65a2b3f679a5--------------------------------" rel="noopener" target="_blank">张量流</a> v2.3 中</h1><h2 id="3155" class="lv js hi bd jt lw lx ly jx lz ma mb kb iq mc md kf iu me mf kj iy mg mh kn mi bi translated">你可以从我的<a class="ae lb" href="https://github.com/sanchitvj/Image-Dehazing-using-GMAN-net" rel="noopener ugc nofollow" target="_blank"> GitHub </a>获得完整的代码。</h2><h2 id="c388" class="lv js hi bd jt lw lx ly jx lz ma mb kb iq mc md kf iu me mf kj iy mg mh kn mi bi translated">预处理和加载数据</h2><p id="f127" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">这个函数获取图像的路径，读取它并解码成一个 uint8 张量。然后，我们将其大小调整为 412x548，数据集中的图像大小为 413x550，但由于输入值为奇数，网络函数产生了问题。最后，我们将其归一化，并以归一化张量的形式返回图像。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="22b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此函数获取原始和模糊图像的路径，然后创建一个带有关键字的拆分字典:“train”、“val”，其中 90%为训练数据，10%为验证数据。如果你浏览数据集，那么原始图像具有类似“0011”的名称，并且对应的模糊图像具有类似“0011_0.85_0.2”的名称，因此每个原始图像都具有不止一个模糊图像，其中下划线之后的数字表示某种比率，其中模糊被添加到原始图像中。所以在函数下面，分组，原始图像，以及相应的朦胧图像并返回它们。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="baa2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">below 函数对训练集的原始图像路径和模糊图像路径应用 from_tensor_slices()函数，后跟加载图像的映射函数(load_image)。然后压缩两个训练数据集。对验证数据集进行类似的操作。最后返回两个数据集。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mo mp l"/></div></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mq"><img src="../Images/1905d58d66315ac3d614311371aff352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eLMKd_HyYV6kyz2LRp9BbQ.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">模糊、原始和去雾的图像。</figcaption></figure><p id="48dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用下面的函数来显示单个历元训练后验证数据的输出。它以一个典型的、朦胧的、原始的图像作为论据。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mo mp l"/></div></figure><h2 id="aad4" class="lv js hi bd jt lw lx ly jx lz ma mb kb iq mc md kf iu me mf kj iy mg mh kn mi bi translated">网络函数</h2><p id="7690" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">下面是网络功能。我们已经用 Conv2D，Conv2DTranspose 构造了这个函数。第一个是 GMAN 网络，其中除了编码层具有 128 个滤波器之外，所有层都具有几个滤波器，如 64 个，并且最终输出层具有 3 个通道(RGB)。在 GMAN 之后，并行网络(PN)已经扩展了卷积层，所有层都具有 64 个滤波器，除了最后一个具有 3 个通道(RGB)。我已经在上面详细解释了架构。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mo mp l"/></div></figure><h2 id="78cb" class="lv js hi bd jt lw lx ly jx lz ma mb kb iq mc md kf iu me mf kj iy mg mh kn mi bi translated">培养</h2><p id="8183" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">我从这个项目中学到的最好的东西是如何定制训练一个模型。通常 fit，predict 看起来很吸引人，但真正的训练方式是这样的。下面的函数训练模型。在每个时期，我们都有一个训练循环和一个验证循环。在训练循环中，我们获取训练数据并计算梯度，然后应用它们来计算训练损失。在验证循环中，我们采用在该历元中计算的梯度，并将其应用于验证数据，以检查输出(使用 display_img 函数)和验证损失。最后，我们保存该时期的模型(权重、变量等)并重置损失指标。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="4451" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，在我们训练模型之前，我们将定义一些超参数。我使用的批量大小为 8，因为超过这个数量，GPU 就会耗尽内存。我们没有将内核权重初始化为零，而是使用随机正态初始化来提供更好的结果。为了减少过拟合，使用了权重衰减为 1e-4 的 L2 正则化子。请注意，根据研究论文，每一层都没有相同的内核初始化。最后调用训练函数。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mo mp l"/></div></figure><h2 id="aa4b" class="lv js hi bd jt lw lx ly jx lz ma mb kb iq mc md kf iu me mf kj iy mg mh kn mi bi translated">评估(测试)</h2><p id="6711" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">现在我从谷歌上随机拍摄了一些模糊的(自然模糊的)图片，并对它们进行了测试，下面的函数就是用来测试的。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mo mp l"/></div></figure><p id="a6a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面显示了一个测试输出，下面显示了其他输出。其余的<a class="ae lb" href="https://github.com/sanchitvj/Image-Dehazing-using-GMAN-net/tree/master/results/test%20results" rel="noopener ugc nofollow" target="_blank">你可以查看这里的</a> <strong class="ih hj">。</strong>您可以在这里查看确认输出<a class="ae lb" href="https://github.com/sanchitvj/Image-Dehazing-using-GMAN-net/tree/master/results/validation%20results" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mr"><img src="../Images/e9026b85f9c1bab7d9c8dd5acf3719b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KQm319Ie8S0nhzG5_624MQ.png"/></div></div></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mr"><img src="../Images/69c1f4680cffeb1437706fea9ae9224c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TgOdTiDT8iTkjjDOVlrW1g.png"/></div></div></figure><blockquote class="ms"><p id="37a0" class="mt mu hi bd mv mw mx my mz na nb jc dx translated">V <!-- --> oila！！！我们完了。</p></blockquote><h1 id="4bee" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc nc ke kf kg nd ki kj kk ne km kn ko bi translated">结束思考</h1><p id="8126" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">我们已经成功地设计了一个算法去模糊(恢复)一个模糊的图像。我们在这里学到的要点是:</p><ul class=""><li id="ae59" class="nf ng hi ih b ii ij im in iq nh iu ni iy nj jc nk nl nm nn bi translated">去雾领域背后的 GMAN 建筑与研究。</li><li id="9b19" class="nf ng hi ih b ii no im np iq nq iu nr iy ns jc nk nl nm nn bi translated">如何在<a class="jd je ge" href="https://medium.com/u/b1d410cb9700?source=post_page-----65a2b3f679a5--------------------------------" rel="noopener" target="_blank"> TensorFlow </a>中定制训练一个模型？</li><li id="cf36" class="nf ng hi ih b ii no im np iq nq iu nr iy ns jc nk nl nm nn bi translated">如何处理要素和标注都是图像的数据集？</li></ul><h2 id="758f" class="lv js hi bd jt lw lx ly jx lz ma mb kb iq mc md kf iu me mf kj iy mg mh kn mi bi translated">参考资料:</h2><ul class=""><li id="2a74" class="nf ng hi ih b ii kp im kq iq nt iu nu iy nv jc nk nl nm nn bi translated"><a class="ae lb" href="https://github.com/Seanforfun/GMAN_Net_Haze_Removal" rel="noopener ugc nofollow" target="_blank">https://github.com/Seanforfun/GMAN_Net_Haze_Removal</a></li><li id="a76a" class="nf ng hi ih b ii no im np iq nq iu nr iy ns jc nk nl nm nn bi translated">你可以从这里得到数据集:【https://www.kaggle.com/wwwwwee/dehaze T2】</li><li id="a9b2" class="nf ng hi ih b ii no im np iq nq iu nr iy ns jc nk nl nm nn bi translated">感谢<a class="jd je ge" href="https://medium.com/u/b1d410cb9700?source=post_page-----65a2b3f679a5--------------------------------" rel="noopener" target="_blank">tensor flow</a>:<a class="ae lb" href="https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/guide/keras/writing _ a _ training _ loop _ from _ scratch</a></li></ul><p id="0ea3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以查看一下这个项目的<a class="ae lb" href="https://www.kaggle.com/sanchitvj/gman-net-for-image-dehazing-using-tf2" rel="noopener ugc nofollow" target="_blank"> Kaggle 笔记本</a>，输出最多 5 个历元。欢迎任何建议。在<a class="ae lb" href="http://github.com/sanchitvj" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上查看我的其他项目。</p><blockquote class="ms"><p id="43ce" class="mt mu hi bd mv mw nw nx ny nz oa jc dx translated">如果你觉得我的工作有用，请鼓掌。</p></blockquote><h2 id="4d8d" class="lv js hi bd jt lw ob ly jx lz oc mb kb iq od md kf iu oe mf kj iy of mh kn mi bi translated">享受学习！！！</h2></div></div>    
</body>
</html>