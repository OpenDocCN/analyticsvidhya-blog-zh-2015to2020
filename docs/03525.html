<html>
<head>
<title>Understanding Activation Functions and Hidden Layers in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经网络中的激活函数和隐藏层</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-activation-functions-and-hidden-layers-in-neural-networks-4fca2b980917?source=collection_archive---------4-----------------------#2020-02-06">https://medium.com/analytics-vidhya/understanding-activation-functions-and-hidden-layers-in-neural-networks-4fca2b980917?source=collection_archive---------4-----------------------#2020-02-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/2695a30e430822f0fafdb67729517262.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*oxypjX22FEfgQo4MQzWNbg.gif"/></div></figure><p id="cd43" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">人工神经网络是我们大脑的虚拟表示，它由神经元的一个输入层组成，在那里输入一些信息，它们逐渐通过一些隐藏层和公式以获得最大的信息，最终进入输出层，在那里我们获得我们想要的输出。神经网络是深度学习中分类和回归问题的支柱。</p><p id="b8dd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">假设，如果我们仔细观察上面的gif，它是神经网络的3D表示，以及用于分类猫和狗图像的隐藏层。隐藏层的数量完全是假设的，它们是根据每个问题的需要来使用的。显然，隐藏层数越多，输出的精度就越高。</p><p id="2714" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">好的，那么什么是激活函数呢？它是如何与神经元联系在一起的？</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es jk"><img src="../Images/cb3b4217c635dd9ad3b648087cd7b832.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*ShmSY0T071Sr0v_hrPVmYw.jpeg"/></div></figure><blockquote class="jp jq jr"><p id="5297" class="im in js io b ip iq ir is it iu iv iw jt iy iz ja ju jc jd je jv jg jh ji jj hb bi translated">激活函数是决定神经网络输出的数学方程。它决定条件<em class="hi">神经元是否应该被</em><strong class="io hj"><em class="hi"/></strong><em class="hi">【激发】。它们的值范围从-1到1或从0到1。</em></p></blockquote><p id="a34f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">假设，在输入层x1、x2中有两个神经元被施加了它们各自的强度，称为权重和偏置b。激活函数被应用于权重和神经元的输入值加上偏置的乘积的总和，从而产生输出。</p><p id="b739" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> Y =激活(wi*xi +b)，i=1，2，3… </strong></p><p id="29c1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">可以有两种类型的激活功能:- <strong class="io hj">线性和非线性</strong></p><p id="d8d0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">但是神经网络中激活函数的主要目的是将非线性引入网络。</p><p id="2d9b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">为什么是非线性？</strong></p><p id="c020" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要理解这一点，我们必须深入学习过程的反向传播方法。因此，在深度学习模型的训练期间，在前向传播之后，<strong class="io hj">我们得到一个损失函数，该损失函数应该通过任何手段最小化，并且通过梯度下降逐渐更新隐藏层的神经元的相应权重，以获得更好的输出。如果激活函数是线性的，那么就不可能回过头来理解将哪些权重分配给输入神经元以获得更好的预测。</strong></p><p id="5cf0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">非线性激活函数也允许反向传播，因为梯度下降项与输入相关。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es jw"><img src="../Images/ffe6c8c381ee8d24925441eaa057ca26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*uyP9hvjCemVlW79Lf7Ol-Q.gif"/></div></figure><p id="46e9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">大多数情况下，可以有<strong class="io hj"> 4 </strong>个激活功能:-</p><h2 id="89fb" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki kj kk jb kl km kn jf ko kp kq kr bi translated">1.乙状结肠或逻辑激活功能</h2><p id="1a7c" class="pw-post-body-paragraph im in hi io b ip ks ir is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj hb bi translated">Sigmoid函数曲线看起来像S形。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/fb4b8c5cf4d4a21422d42ed9ab844c32.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/0*X9UZbWYa1Gj6Xn93.png"/></div></figure><p id="1a70" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们使用sigmoid函数的主要原因是因为它存在于<strong class="io hj"> (0到1)之间。</strong>因此，它特别用于我们必须<strong class="io hj">预测概率</strong>作为输出的模型。由于任何事情的概率只存在于<strong class="io hj"> 0和1之间，</strong> sigmoid是正确的选择。</p><p id="4b02" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">函数是<strong class="io hj">可微的</strong>。这意味着，我们可以在任意两点找到s形曲线的斜率。</p><p id="144c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">函数是<strong class="io hj">单调的</strong>，但函数的导数不是。</p><p id="297c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">逻辑sigmoid函数会导致神经网络在训练时停滞不前。</p><h2 id="cd9d" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki kj kk jb kl km kn jf ko kp kq kr bi translated">2.双曲正切激活函数</h2><p id="f92d" class="pw-post-body-paragraph im in hi io b ip ks ir is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj hb bi translated">Tanh也像是乙状结肠的更好版本。tanh函数的范围是从(-1到1)。tanh也是s形的(s形)。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/3f6d6765b06872b8b42998f316b735c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/0*MCRWFtG7fX5vTDkl.jpeg"/></div></figure><p id="517c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">优点是负输入将被映射为强负输入，而零输入将被映射到双曲正切图中的零附近。</p><p id="87b5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">函数是<strong class="io hj">可微的</strong>。</p><p id="2bd7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">函数<strong class="io hj">单调</strong>，而其<strong class="io hj">导数不单调</strong>。</p><p id="f876" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">双曲正切函数主要用于两类之间的分类。</p><h2 id="c9d4" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ix ki kj kk jb kl km kn jf ko kp kq kr bi translated">3.ReLU(整流线性单位)激活功能</h2><p id="5d0d" class="pw-post-body-paragraph im in hi io b ip ks ir is it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj hb bi translated">ReLU是目前世界上使用最多的激活函数。因为它用于几乎所有的卷积神经网络或深度学习。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es kz"><img src="../Images/951ae3b2bcd67270c6aed07e076df6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yYQsLdEUCa5TD0rU.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated"><strong class="bd jz"> Relu v/s乙状结肠</strong></figcaption></figure><p id="d23b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如你所看到的，ReLU是半整流(从底部)。当z小于零时f(z)为零，当z大于或等于零时f(z)等于z。</p><p id="dfc5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">范围:</strong>【0到无穷大】</p><p id="ad82" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">函数及其导数<strong class="io hj">都是</strong> <strong class="io hj">单调</strong>。</p><p id="2c32" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">但问题是，所有的负值立即变为零，这降低了模型根据数据进行适当拟合或训练的能力。这意味着给予ReLU激活函数的任何负输入都会在图形中立即将值变成零，这反过来会通过不适当地映射负值来影响结果图形。</p><p id="0c8a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以，让我们现在移动到<strong class="io hj">隐藏层:- </strong></p><p id="c4e2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">隐藏层允许将神经网络的功能分解成特定的数据转换。每个隐藏层的功能都是专门产生一个定义的输出。例如，用于识别人的眼睛和耳朵的隐藏层功能可以与后续层结合使用，以识别图像中的面部。虽然单独识别眼睛的功能不足以独立识别物体，但它们可以在神经网络中共同发挥作用。</p><p id="a256" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">深度学习中有一个非常著名的术语超参数，它可以促进学习过程。隐藏层的数量是在处理之前已经知道的超参数之一。</p><p id="881e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了添加隐藏层，我们需要回答以下两个问题:</p><ol class=""><li id="6132" class="li lj hi io b ip iq it iu ix lk jb ll jf lm jj ln lo lp lq bi translated"><strong class="io hj">所需的隐藏层数是多少？</strong></li><li id="a61a" class="li lj hi io b ip lr it ls ix lt jb lu jf lv jj ln lo lp lq bi translated"><strong class="io hj">每一个隐藏层中隐藏神经元的数量是多少？</strong></li></ol><p id="ebbd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">首先，当两个或多个线性函数的组合变成线性时，如果我们使用线性激活函数，隐藏层是没有用的。</p><p id="9428" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">根据损失函数的最小化，我们需要反向传播并更新输入层和隐含层神经元的权重。因此，隐藏层和隐藏神经元的数量取决于梯度下降。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lw"><img src="../Images/c9bad73e1279bcba11fd2c1778bdc2ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*riA4dkmOTRgR3bzHlitqhA.png"/></div></div></figure><p id="bb26" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">也可以说，在某些时期之后，模型的训练和测试精度如何彼此越来越接近，也可以指示应该使用多少个隐藏层来获得更高的预测精度。</p></div></div>    
</body>
</html>