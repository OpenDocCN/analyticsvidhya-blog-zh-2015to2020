<html>
<head>
<title>Parallax Images</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视差图像</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/parallax-images-14e92ebb1bae?source=collection_archive---------8-----------------------#2020-11-22">https://medium.com/analytics-vidhya/parallax-images-14e92ebb1bae?source=collection_archive---------8-----------------------#2020-11-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f84c" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">看待无聊的2D图像的新方法</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/b68ea61990739e840f90fa76ac9ff5d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*T2UzD6ZjHrkmCaJzcSSd7g.gif"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">视差图像演示</figcaption></figure><p id="6c1c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们都看过3D电影，幻觉图像，以及它们看起来有多好的诀窍，这给了我一个想法，让我制作一些工具，当用户移动他的头部时，图像可以改变它们的视角。想象一下它看起来有多酷。</p><p id="eb40" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">会有什么影响？</strong> <br/>我们都很熟悉术语<a class="ae kf" href="https://en.wikipedia.org/wiki/Parallax#:~:text=Parallax%20(from%20Ancient%20Greek%20%CF%80%CE%B1%CF%81%CE%AC%CE%BB%CE%BB%CE%B1%CE%BE%CE%B9%CF%82,inclination%20between%20those%20two%20lines." rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj">视差</strong> </a>它只是物体表观位置的不同变化量，这取决于我们离它有多远。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><div class="er es kg"><img src="../Images/7e721769bb3ec5cd576812b0b51769d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BRiv1s96yDOQREyHXT72TA.jpeg"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">视差</figcaption></figure><p id="6250" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">因此，如果我们能在2D图像中获得相同的效果，即图像的不同层有不同的移动，那么我们就能在这些图像中获得深度感和我们想要的酷效果。</p><h1 id="fe98" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated"><strong class="ak">我们来分解一下流程</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ld"><img src="../Images/2a432f959c071cfe750e394c1377cc8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*YRE9NHt8zeeUpyj3KVztqA.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">深度图</figcaption></figure><p id="82ed" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">所以首先，我们需要把一张图像分成不同的层，为此我们需要一张2D图像的深度图。<a class="ae kf" href="https://en.wikipedia.org/wiki/Depth_map" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj">深度图</strong> </a>是简单的黑白图像，其中图像的白度表明对象离视点有多近。在我们得到基本图层后，我们需要<a class="ae kf" href="https://en.wikipedia.org/wiki/Inpainting#:~:text=Inpainting%20is%20a%20conservation%20process,to%20present%20a%20complete%20image." rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj">修补</strong> </a>每个图层中缺失的部分。最后，我们将一张图片分成不同的图层。现在我们可以显示不同的层，它们看起来和原始图像一样。现在，我们可以使用我们的相机进行<strong class="jl hj">面部检测</strong>并测量用户头部的移动，然后移动这些层以匹配新的视点。</p></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><h1 id="17b9" class="kl km hi bd kn ko ll kq kr ks lm ku kv io ln ip kx ir lo is kz iu lp iv lb lc bi translated"><strong class="ak">让我们看看如何编写这个工具的代码</strong></h1><p id="73d6" class="pw-post-body-paragraph jj jk hi jl b jm lq ij jo jp lr im jr js ls ju jv jw lt jy jz ka lu kc kd ke hb bi translated">首先，我们需要导入一些文件，将这段代码复制到您的文件中。<br/>我推荐使用4.1.0.25版本的OpenCV，因为以后的版本在使用face_cascade的时候bug很少。对于其他库，您可以使用任何版本，但尽量使用较新的版本。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="9294" class="ma km hi lw b fi mb mc l md me">import os, sys</span><span id="08a7" class="ma km hi lw b fi mf mc l md me">import numpy as np<br/>import pygame as pg<br/>import cv2</span></pre><p id="fc49" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在我们需要加载图像和<strong class="jl hj">深度图</strong>并调整它们的大小以匹配尺寸。现在，我们将为我们的代码提供一个深度图，但是你可以使用一个模型<a class="ae kf" href="https://github.com/intel-isl/MiDaS" rel="noopener ugc nofollow" target="_blank"><strong class="jl hj"/></a>来生成你自己的深度图，我已经在我的主工具中使用了它。可以看看我的<a class="ae kf" href="https://github.com/strikeraryu/Parallax_Image" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj"> GitHub回购</strong> </a>。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="5b67" class="ma km hi lw b fi mb mc l md me">img = cv2.imread('moon.jpg', flags=cv2.CV_8UC4)<br/>depth_map = cv2.imread('moon_depth_map.png')<br/>depth_map = cv2.cvtColor(depth_map,cv2.COLOR_RGB2GRAY)<br/>img = cv2.resize(img, depth_map.shape[:2])</span></pre><div class="iy iz ja jb fd ab cb"><figure class="mg jc mh mi mj mk ml paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><img src="../Images/e379ae83f4aa6e63084905e24402c0c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*41zyuBp44yolCGsm9szgmA.jpeg"/></div></figure><figure class="mg jc mh mi mj mk ml paragraph-image"><div role="button" tabindex="0" class="kh ki di kj bf kk"><img src="../Images/ec57e357aac07f25c8c21c4ea1b8f409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*-Zq9r6gFsPzSeogDv6W4hA.png"/></div></figure></div><p id="7be4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，在我们已经加载了深度图之后，我们可以通过在不同的阈值对深度图进行阈值处理来为不同的层创建遮罩。制作一个图层时，我们需要两个蒙版，一个是这个图层的蒙版，另一个是前一个图层的蒙版，用来修补缺失的部分。我们将采取循环外的最后一层，这样我们可以提取这一层的所有剩余部分。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="cdec" class="ma km hi lw b fi mb mc l md me">   <br/>layers = []     <br/>prev_thres = 255<br/>div=30<br/>     <br/>for thres in range(255 - div, 0, -div):        <br/>   ret, mask = cv2.threshold(depth_map, thres, 255,          cv2.THRESH_BINARY)<br/>        <br/>   ret, prev_mask = cv2.threshold(depth_map, prev_thres, 255, cv2.THRESH_BINARY)  <br/>       <br/>   prev_thres = thres        <br/>   inpaint_img = cv2.inpaint(img, prev_mask, 10, cv2.INPAINT_NS)<br/>   layer = cv2.bitwise_and(inpaint_img, inpaint_img, mask = mask) </span><span id="f9f7" class="ma km hi lw b fi mf mc l md me">   layers.append(conv_cv_alpha(layer, mask))  <br/>    <br/># adding last layer <br/>   <br/>mask = np.zeros(depth_map.shape, np.uint8)    <br/>mask[:,:] = 255   <br/> <br/>ret, prev_mask = cv2.threshold(depth_map, prev_thres, 255, cv2.THRESH_BINARY)<br/>     <br/>inpaint_img = cv2.inpaint(img, prev_mask, 10, cv2.INPAINT_NS)    layer = cv2.bitwise_and(inpaint_img, inpaint_img, mask = mask)</span><span id="5ecf" class="ma km hi lw b fi mf mc l md me">layers.append(conv_cv_alpha(layer, mask))<br/>     <br/>layers = layers[::-1]</span></pre><p id="d54f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们已经颠倒了这些层，所以我们可以按照从最后一层<strong class="jl hj">到第一层</strong>的顺序排列它们。当我们将图层添加到列表中时，我们使用了一个函数'<strong class="jl hj">conv _ cv _阿尔法</strong>'，这将添加阿尔法值<strong class="jl hj">(将RGB变为RGBA) </strong>，并使用蒙版使图层的一部分变得透明。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="1fde" class="ma km hi lw b fi mb mc l md me">def conv_cv_alpha(cv_image, mask):    <br/>    b, g, r = cv2.split(cv_image)    <br/>    rgba = [r, g, b, mask]    <br/>    cv_image = cv2.merge(rgba,4)    <br/>          <br/>    return cv_image</span></pre><p id="935a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在是人脸检测和显示图像的部分。对于人脸检测，我们将使用<a class="ae kf" href="http://www.willberger.org/cascade-haar-explained/#:~:text=Haar%20Cascade%20is%20a%20machine,of%20Simple%20Features%22%20in%202001." rel="noopener ugc nofollow" target="_blank"><strong class="jl hj">Haar scade</strong></a>。从他们的官方<a class="ae kf" href="https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml" rel="noopener ugc nofollow" target="_blank"> Github库</a>下载它们。</p><p id="3297" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">要下载它们，右键单击“Raw”= &gt;“将链接另存为”。确保它们在您的工作目录中。</p><p id="00a1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，我们将加载haar cascade进行人脸检测，并创建一个从图像中返回人脸矩形的函数。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="22d1" class="ma km hi lw b fi mb mc l md me">face_cascade = cv2.CascadeClassifier( 'haarcascade_frontalface_default.xml')   <br/><br/>def get_face_rect(img):    <br/>    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    <br/>    face_rects = face_cascade.detectMultiScale(gray_img, 1.3, 5)<br/>    if len(face_rects) == 0:         <br/>        return ()</span><span id="3d6e" class="ma km hi lw b fi mf mc l md me">    return face_rects[0]</span></pre><p id="fad5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在我们要展示的图像会根据用户的头部移动。我们将使用<strong class="jl hj"> OpenCV </strong>读取cam，然后使用<strong class="jl hj"> Pygame </strong>渲染每一帧。为了计算每一层的偏移，我们将计算头部从帧中心的偏移，然后将其缩小以获得一个小的偏移值。之后，我们将乘以每个层的索引值，以获得相应层的偏移值，您也可以乘以一些常量值，以获得更好的结果。</p><p id="04e8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将创建一个比原始图像略小的Pygame窗口，并加载相机。我们已经使用了<strong class="jl hj"> scale </strong>，所以你改变它的值使最终结果变大。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="3c4f" class="ma km hi lw b fi mb mc l md me">scale = 1<br/>off_set = 20</span><span id="6e80" class="ma km hi lw b fi mf mc l md me">width, height = layers[0].get_width(), layers[0].get_height()        win = pg.display.set_mode((int((width - off_set)*scale), int((height - off_set)*scale)))    <br/>pg.display.set_caption('Parallax_image')</span><span id="326f" class="ma km hi lw b fi mf mc l md me">scaled_layers = []    <br/>for layer in layers: <br/>             scaled_layers.append(pg.transform.scale(layer, (int(width*scale), int(height*scale))))</span><span id="d55e" class="ma km hi lw b fi mf mc l md me">cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)</span></pre><p id="26aa" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将设置一些常数。您可以使用这些常量来获得不同的结果。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="88f8" class="ma km hi lw b fi mb mc l md me">x_transform = True     # allow shift in x-axis<br/>y_transform = False    # allow shift in y-axis<br/>sens = 50              # the amount of scale down of shift value<br/>show_cam = False       # show your face cam<br/>shift_x = 0    <br/>shift_y = 0    <br/>run = True</span></pre><p id="c850" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">最后，主循环渲染所有层。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="1594" class="ma km hi lw b fi mb mc l md me">while run:<br/>    for event in pg.event.get():<br/>        if event.type==pg.QUIT:<br/>            run = False</span><span id="9e51" class="ma km hi lw b fi mf mc l md me">    ret, frame = cap.read()<br/>    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)<br/>    initial_pos = (frame.shape[0]/2, frame.shape[1]/2)<br/>    face_rect = get_face_rect(frame)</span><span id="1cbd" class="ma km hi lw b fi mf mc l md me">    if len(face_rect) != 0:<br/>        x,y,w,h, = face_rect<br/>        face_rect_frame = cv2.rectangle(frame, (x, y), (x + w, y + h), (255,255,0), 3)</span><span id="ae2d" class="ma km hi lw b fi mf mc l md me">        shift_x = (initial_pos[0] - (x + w/2))/(sens*scale)<br/>        shift_y = (initial_pos[1] - (y + h/2))/(sens*scale)</span><span id="db22" class="ma km hi lw b fi mf mc l md me">    win.fill((255, 255, 255))<br/>                 <br/>    for i, layer in enumerate(scaled_layers):<br/>        new_x = -off_set/2<br/>        new_y = -off_set/2<br/>        if x_transform:<br/>            new_x = 0 + shift_x*i<br/>        if y_transform:<br/>            new_y = 0 + shift_y*i<br/>        win.blit(layer, (new_x, new_y)) <br/>        <br/>   face_rect_frame = cv2.resize(face_rect_frame, (100, 100))<br/>   if show_cam:<br/>       win.blit(conv_cv_pygame(face_rect_frame), (0, 0))<br/>   pg.display.update()</span><span id="ac76" class="ma km hi lw b fi mf mc l md me">cap.release()<br/>cv2.destroyAllWindows()<br/>pg.quit()</span></pre><p id="6547" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这就是最终结果。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/6314651d04ac7d8110f1971a9f3d263a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*H6L0UFuTpWzD_IB0EFqSvg.gif"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">决赛成绩</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/c46d5fa3d26c255160df71d94e1dfc9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*24XN55FuLp0IKIa3PQefPw.gif"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">不同图像的演示</figcaption></figure><p id="028d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我已经创建了这个工具的一个更高级的版本，你可以选择图像，它会自动创建视差图像，深度图会自动生成。</p><p id="7299" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">你可以在我的<a class="ae kf" href="https://github.com/strikeraryu/Parallax_Image" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj"> GitHub </strong> </a>回购上查看更多。</p></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><p id="c139" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">请随时联系我🔥。<strong class="jl hj"/><a class="ae kf" href="https://www.linkedin.com/in/aryamaan-jain-9330a8190/" rel="noopener ugc nofollow" target="_blank"><strong class="jl hj">LinkedIn</strong></a><strong class="jl hj"/><a class="ae kf" href="https://twitter.com/striker_aryu" rel="noopener ugc nofollow" target="_blank"><strong class="jl hj">Twitter</strong></a><strong class="jl hj"/><a class="ae kf" href="https://www.instagram.com/striker_aryu/?hl=en" rel="noopener ugc nofollow" target="_blank"><strong class="jl hj">insta gram</strong></a></p><p id="6aa6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">你可以检查我的其他项目，并继续关注更多。👀</p></div></div>    
</body>
</html>