<html>
<head>
<title>Natural Language Processing: Learning by Doing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理:边做边学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/natural-language-processing-ed71ff6e41f2?source=collection_archive---------11-----------------------#2020-07-14">https://medium.com/analytics-vidhya/natural-language-processing-ed71ff6e41f2?source=collection_archive---------11-----------------------#2020-07-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3278c58094584e00e27289af249e1ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mjK_HxjAK91aNoE9jAU3bQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">ut sav sres tha click @ unsplash</strong></figcaption></figure><p id="c3b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">存在各种格式的数据，这些数据为了消费目的而被疯狂地构造。<br/>即<br/> <em class="jt"> —结构化格式。<br/> —非结构化格式<br/> —半结构化格式。</em></p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="7a3d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">幸运的是，在第一次写这篇文章的时候，Andrew NG和他的团队推出了迄今为止最好的专业化系列，它是关于自然语言处理的。</p><p id="2b2b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于我们的用例，我们将首先尝试根据文本的内容对其进行分类，无论它们是好还是坏？<br/>我们将探索如何为情感分析预处理推文。<br/>在这个练习中，我们将使用NLTK附带的Twitter数据集。这个数据集已经被手工注释，用于快速建立模型的基线。NLTK包含5000 +ve和5000 -ve的tweets。</p><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="cd84" class="kk kl hi kg b fi km kn l ko kp">all_positive_tweets=twitter_samples.strings('positive_tweets.json')<br/>all_negative_tweets=twitter_samples.strings('negative_tweets.json')</span><span id="8675" class="kk kl hi kg b fi kq kn l ko kp">print('Number of positive tweets: ', len(all_positive_tweets))<br/>print('Number of negative tweets: ', len(all_negative_tweets))</span><span id="08e2" class="kk kl hi kg b fi kq kn l ko kp">Number of positive tweets:  5000<br/>Number of negative tweets:  5000</span></pre><p id="3365" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">让我们看看数据传播的情况。</strong></p><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="7fe9" class="kk kl hi kg b fi km kn l ko kp">fig = plt.figure(figsize=(5, 5))<br/>labels = 'Positives', 'Negative'<br/>sizes = [len(all_positive_tweets), len(all_negative_tweets)] <br/>plt.pie(sizes, labels=labels, autopct='%1.1f%%',<br/>        shadow=True, startangle=90)<br/>plt.axis('equal')  <br/>plt.show()</span></pre><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="ab fe cl kr"><img src="../Images/cc65400b053379bfb29ebc0f833a5301.png" data-original-src="https://miro.medium.com/v2/format:webp/1*QMwZfN2y294_d3yjtGnJnA.png"/></div></figure><p id="dd82" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">一条推文在我们的数据集中的样子</strong></p><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="c0cb" class="kk kl hi kg b fi km kn l ko kp"># print positive in greeen<br/>print('\033[92m' + all_positive_tweets[random.randint(0,5000)])<br/><br/># print negative in red<br/>print('\033[91m' + all_negative_tweets[random.randint(0,5000)])</span><span id="7a39" class="kk kl hi kg b fi kq kn l ko kp">[92m@clarelea101 Deal. I'm great with kids :)<br/>[91mOMG selena tweets while i was busy out :(</span></pre><p id="5b3c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据是无模式的、非结构化的，包含有意义的语法和语义结构信息。</p><h2 id="b70d" class="kk kl hi bd iu ks kt ku kv kw kx ky kz jg la lb lc jk ld le lf jo lg lh li lj bi translated"><strong class="ak">预处理</strong></h2><p id="8e38" class="pw-post-body-paragraph iv iw hi ix b iy lk ja jb jc ll je jf jg lm ji jj jk ln jm jn jo lo jq jr js hb bi translated">我们可以看到在给定的文本数据中存在各种各样的垃圾，我们需要在进一步的步骤之前清除这些垃圾，例如:</p><ul class=""><li id="3291" class="lp lq hi ix b iy iz jc jd jg lr jk ls jo lt js lu lv lw lx bi translated">删除停用词、特殊字符。</li><li id="ae78" class="lp lq hi ix b iy ly jc lz jg ma jk mb jo mc js lu lv lw lx bi translated">将高语法单词根植于它们的词根。</li></ul><p id="189e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是我们可以用于数据预处理的便捷函数。</p><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="c715" class="kk kl hi kg b fi km kn l ko kp">def process_tweet(tweet):<br/>    stemmer = PorterStemmer()<br/>    stopwords_english = stopwords.words('english')<br/>    tweet = re.sub(r'\$\w*', '', tweet)<br/>    tweet = re.sub(r'^RT[\s]+', '', tweet)<br/>    tweet = re.sub(r'https?:\/\/.*[\r\n]*', '', tweet)<br/>    tweet = re.sub(r'#', '', tweet)<br/>    tokenizer = TweetTokenizer(preserve_case=False,        strip_handles=True,reduce_len=True)<br/>    tweet_tokens = tokenizer.tokenize(tweet)<br/><br/>    tweets_clean = []<br/>    for word in tweet_tokens:<br/>        if (word not in stopwords_english and  <br/>                word not in string.punctuation): <br/>            stem_word = stemmer.stem(word)  # stemming word<br/>            tweets_clean.append(stem_word)<br/><br/>    return tweets_clean</span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="fe72" class="kk kl hi bd iu ks kt ku kv kw kx ky kz jg la lb lc jk ld le lf jo lg lh li lj bi translated">特征抽出</h2><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="3661" class="kk kl hi kg b fi km kn l ko kp">def build_freqs(tweets, ys):<br/>    freqs = {}<br/>    for y, tweet in zip(yslist, tweets):<br/>        for word in process_tweet(tweet):<br/>            pair = (word, y)<br/>            if pair in freqs:<br/>                freqs[pair] += 1<br/>            else:<br/>                freqs[pair] = 1<br/>    return freqs</span><span id="564c" class="kk kl hi kg b fi kq kn l ko kp">freqs = build_freqs(tweets, ys)</span><span id="f072" class="kk kl hi kg b fi kq kn l ko kp">def extract_features(tweet, freqs,flag):<br/>       word_l = process_tweet(tweet)<br/>       x = np.zeros((1, 4)) <br/>       x[0,0] = 1<br/>       <br/>       if flag == 1:<br/>            x[0,3]=1<br/>       else:<br/>            x[0,3]=0<br/>        <br/>    for word in word_l:<br/>              x[0,1] += len([1 for key, value in freqs.items() if word==key[0]and key[1]==1])<br/>                x[0,2] += len([1 for key, value in freqs.items() if word==key[0]and key[1]==0])<br/>        <br/>    return x</span></pre><p id="336e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将我们的特征空间作为2个向量，一个带注释的标志作为标签。<br/>只需将特征矩阵与其对应的标签进行组合。训练模型并观察性能，我们将使用混淆矩阵。</p><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="525b" class="kk kl hi kg b fi km kn l ko kp">def dataMaker(all_positive_tweets,all_negative_tweets):<br/>    extracted_pos_feature = []<br/>    for tweet in all_positive_tweets:<br/>        feature = extract_features(tweet,freqs,1)<br/>        extracted_pos_feature.append(feature[0])<br/>        <br/>    extracted_neg_feature = []<br/>    for tweet in all_negative_tweets:<br/>        feature = extract_features(tweet,freqs,0)<br/>        extracted_neg_feature.append(feature[0])<br/>        <br/>    return extracted_pos_feature,extracted_neg_feature</span><span id="2615" class="kk kl hi kg b fi kq kn l ko kp">pos,neg = dataMaker(all_positive_tweets,all_negative_tweets)<br/>pos_df = pd.DataFrame(pos)<br/>neg_df = pd.DataFrame(neg)<br/>featured_tweet_data = pd.concat([pos_df,neg_df],axis=0)<br/>featured_tweet_data.head(n=10)</span></pre><p id="4708" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我们将决定分割并训练我们的模型，训练后，我们将尝试验证模型并绘制准确度矩阵。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="605e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">模型构建:</strong></p><pre class="kb kc kd ke fd kf kg kh ki aw kj bi"><span id="448b" class="kk kl hi kg b fi km kn l ko kp">featured_tweet_data.columns = ["Bias","Pos_Count","Neg_Count","Tag"]</span><span id="9d8c" class="kk kl hi kg b fi kq kn l ko kp">x = featured_tweet_data[["Pos_Count","Neg_Count"]]<br/>y = featured_tweet_data["Tag"]<br/><br/>X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.20, random_state=42)<br/>model = LogisticRegression()<br/>model.fit(X_train,y_train)<br/>y_pred = model.predict(X_test)<br/>confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)</span><span id="fdb7" class="kk kl hi kg b fi kq kn l ko kp">fig, ax = plt.subplots(figsize=(10, 10))<br/>ax.matshow(confmat, cmap=plt.cm.cividis, alpha=0.3)<br/>for i in range(confmat.shape[0]):<br/>    for j in range(confmat.shape[1]):<br/>        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')<br/><br/>plt.xlabel('Predicted label')<br/>plt.ylabel('True label')<br/><br/>plt.tight_layout()<br/>plt.show()</span></pre><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="ab fe cl kr"><img src="../Images/21d32f6e55f73488020535105166aec1.png" data-original-src="https://miro.medium.com/v2/format:webp/1*7iVLnYdG0EJZSCd-zBlcFg.png"/></div></figure><p id="ae7f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，目前看文本数据和它的处理有关信息提取，以及它如何导致分类后矢量化。专门的人可以看看这个NLP的端到端传播和相应领域的增长。</p><figure class="kb kc kd ke fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="255a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你喜欢它，别忘了在下面留下你的宝贵反馈。这将有助于勘探的改进。</p><p id="5926" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢您抽出时间。</p></div></div>    
</body>
</html>