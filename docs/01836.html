<html>
<head>
<title>Life is Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生活是强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/life-is-reinforcement-learning-7dece10190c5?source=collection_archive---------11-----------------------#2019-11-16">https://medium.com/analytics-vidhya/life-is-reinforcement-learning-7dece10190c5?source=collection_archive---------11-----------------------#2019-11-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a917e6390bbab7737d4014a3e484d0ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MUI1Nw-BkcveZ_LN"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">罗伯特·科埃略在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><blockquote class="iv iw ix"><p id="c402" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">从与环境的互动中学习的本质是获得所有直觉和智慧的基石。人类的思想和行为可以被解释为从其环境中习得的反应，由此发展了当前的学习和强化概念，这为更深入地理解现实世界的问题提供了有用的基础，并更成功地设计了实用的解决方案。</p></blockquote><p id="927d" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj"> <em class="ja">关键词:</em> </strong> <em class="ja">强化学习，马尔可夫决策过程，Q-学习，策略梯度算法，演员-评论家算法。</em></p><h2 id="6333" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jx kl km kn jy ko kp kq jz kr ks kt ku bi translated">什么是强化学习？</h2><p id="4877" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj jx kx jm jn jy ky jq jr jz kz ju jv jw hb bi translated">在我们的一生中，这种互动是知识的主要来源，是关于我们为了实现不同目标而采取的行动的结果的丰富信息。</p><p id="a232" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">有许多这样的情况，我们的行为直接受到激励或惩罚的影响。例如，我们在考试中因为成绩好而学习，跑马拉松而获得认可。我们所有的行为都受到一种动机的影响，那就是通过努力获得回报。受此启发，我们不断探索，试图发现哪种行为可能会带来更好的回报。这个过程在继续，没有任何明确的主管，我们从基于我们以前的行动获得的反馈中获得经验，我们随着时间的推移改进我们的政策，变得越来越强大，因此，我们离我们的目标越来越近。</p><p id="69c0" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">这就是我们机器学习术语中所说的<strong class="jb hj"><em class="ja">【RL】</em></strong><em class="ja">又名</em> <strong class="jb hj"> <em class="ja"> </em> </strong>半监督学习模型。</p><h2 id="d8e0" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jx kl km kn jy ko kp kq jz kr ks kt ku bi translated">家庭的</h2><p id="be71" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj jx kx jm jn jy ky jq jr jz kz ju jv jw hb bi translated">正如现代RL的创始人之一Richard Sutton所描述的，关键实体是<strong class="jb hj">主体、环境、政策、奖励信号、价值函数</strong>和<strong class="jb hj">环境模型。</strong></p><p id="213e" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">代理:</strong>在RL中起核心作用。从通过传感器感知环境开始，经历、决策任务，最后通过执行器对环境做出反应，这是智能体应该做的事情。所以任何拥有这些品质的人都是特工。例如，我们人类是一个代理人，因为我们有传感器眼睛，耳朵等，在与我们经历的宇宙中的不同环境进行交互时，做出决定，建立我们的策略，并最终通过手，腿等执行器来行动。</p><p id="8ff3" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">环境:</strong>这个词本身是自成体系的。重要的是代理人和环境之间的区别，即他们的边界线。一个有趣的事实是这里的边界不像一个代理人身体的物理边界；相反，它比那要近得多。例如，机器人的任何机械连接，像它的传感硬件、马达都是环境的一部分。任何不能被代理任意改变的东西都被认为是环境的一部分。对于动物肌肉来说，骨骼被认为是环境的一部分。<br/>因此，边界线代表了一个主体绝对控制的极限，而不是它对环境的了解。<strong class="jb hj"> <em class="ja">有趣的是在某些情况下，环境是完全可知的</em> </strong>。这是RL的一个重要话题，也是今天讨论的一部分。<br/>状态空间是环境的一部分，通常用<em class="ja"> S </em>表示。</p><p id="db03" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj"> □ </strong>在足球比赛中，环境是部分可观察的，因为球员不知道其他球员的策略、位置和球的速度。类似地，纸牌游戏、骰子、战舰也属于这一类，因为对手的状态对玩家(代理人)是部分隐藏的。<br/>尽管如此，带时钟的象棋游戏、纵横字谜都是完全可观察环境的完美例子。</p><blockquote class="iv iw ix"><p id="7809" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">马尔可夫决策过程(MDP) : </strong> <br/> —给定现在，未来独立于过去。</p></blockquote><p id="ed41" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">当环境完全已知时，我们称这种表象为MDP。正式定义，</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/680caf14da1cdc8643783f7f6d17eacf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wXAByUMnO4u-Znh5uOHrBg.png"/></div></div></figure><p id="ad4b" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj"> <em class="ja">带回家的信息</em>:<br/></strong>历史代表了我们不看数据流就能知道的关于过去的最多信息。瓶颈是历史也随着时间增长<em class="ja"> t </em>变得庞大而笨拙。</p><p id="a856" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">状态被视为历史的函数。</strong></p><p id="be80" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">如果状态是马尔可夫的，那么我们可以安全地清除缓存，并且只关心当前状态<em class="ja"> Sₜ </em>以便处理未来的<em class="ja"> Sₚ </em>其中<em class="ja"> p=t+1。</em></strong></p><p id="0372" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">T</strong>智能体和环境在一系列离散时间步骤的每一步相互作用，<em class="ja"> t =0，1，2，3，…。</em>在每个时间步<em class="ja"> t </em>代理接收环境状态的一些表示<em class="ja"> Sₜ ∈ S，</em>基于此选择动作<em class="ja"> Aₜ ∈ A，</em>作为结果接收奖励，时间递增<em class="ja"> t+1，</em>环境转换到新状态<em class="ja"> Sₚ (p=t+1) </em>并且过程继续<em class="ja">。</em></p><p id="0286" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">策略:</strong>基本上它是一个代理为完成任务而遵循的策略。可能是好的，也可能是坏的，但这是学习代理在给定时间的行为方式。选择哪种行动的决定是由政策决定的。策略可以是确定性的，也可以是随机的。<br/>设，<em class="ja"> A </em>为动作空间，则确定性策略<em class="ja"> π </em>是从状态空间<em class="ja"> S </em>到选择每个可能动作的概率空间<em class="ja"> P(A) </em>的映射，而随机策略<em class="ja"> π(a|s) </em>是在给定状态<em class="ja"> s ∈ S </em>的情况下选择任意动作<em class="ja"> a ∈ A </em>的概率，即<em class="ja"><br/>除非另有说明<em class="ja"> S，</em>A都是有限集。</em></p><p id="68f3" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">奖励:</strong>它是<strong class="jb hj"> <em class="ja">动机</em> </strong>也是训练一个算法的有力工具。它既可以是积极的，也可以是消极的，因此是代理人决定什么是好政策和坏政策的主要依据。如果遵循的政策导致低回报，那么它有可能被改变，但是，有一个权衡。<br/> <strong class="jb hj"> <em class="ja">“有时候为了成就一番伟业，你不得不失去一些小事”。</em> </strong> <br/>一个代理人的最终目标是累积报酬最大化。<br/>它是一个实数，有两种定义。可以定义为状态-动作对<em class="ja"> r : S × A ↦ ℝ </em>或者状态-动作-下一个状态三元组<em class="ja"> r : S × A × S ↦ ℝ </em>。前者对在给定状态下执行某个动作给予奖励，而后者则针对状态间的特定转换给予奖励。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/5c673fd42afce7de68e853e42fe0081d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPs5qmiGJEoiJovQ_LURxA.png"/></div></div></figure><p id="dd67" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">价值功能:</strong>在RL <strong class="jb hj">中也起核心作用的另一个实体。</strong>这种<strong class="jb hj"> </strong>成分决定了什么<strong class="jb hj"> <em class="ja">小事情要失去才能成就大事情。</em> </strong>换句话说，它指定了从长远来看什么更有利可图。坦率地说，我们不太关心回报，但这个功能在决策中至关重要。<strong class="jb hj"> <em class="ja">这是RL </em> </strong>的心脏。行动的选择是基于回报最高的状态，而不是最高的回报。一旦我们学会了有效地估计这个函数，我们的战斗就成功了一半。所以长话短说学习RL差不多就是学习价值函数了。<br/>价值函数是为特定策略定义的。定义了两个值函数:状态值函数</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/5036c477f76d213b0da5d8ddde6e7d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ifxd0UObhuzrRri5Cs2UtA.png"/></div></div></figure><p id="9b99" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">和动作值函数</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/95ad3440b1e23c3f91684dd7a504c787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6rNk5Fmmo8BxphvbOxDMg.png"/></div></div></figure><p id="9b45" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">这里<em class="ja">γ∈【0，1】</em>是<em class="ja">折扣因子(</em> <strong class="jb hj"> <em class="ja">诱惑度！</em>T7<em class="ja">)</em>。后者在文学上有一个‘特殊’的名字即，<em class="ja">Q</em>-值。就是强调<em class="ja">质量</em>。对于折扣因子和价值函数的详细讨论，下面给出了一些经典参考文献。<br/>价值函数是根据经验估算的。然而，确定价值要困难得多，但是市场上存在某些强大的方法，可以根据代理人随时间推移所做的一系列观察来估计价值。显然，价值函数逼近是智能的主要目的，也是RL算法中最重要的组成部分。计算价值函数的一种有效方式是通过<strong class="jb hj"> <em class="ja">贝尔曼方程</em> </strong>。这个等式的重要性在于它为计算每个状态和相应动作的值的迭代方法打开了大门，因为它表达了一个状态的值与其后续状态的值之间的关系。</strong></p><p id="5678" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">对于<em class="ja"> s∈ S，</em></p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/bc9d12695d732f1b98db5c6890c1ce9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DEjegOw1of_fcuQ2AipSPg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd kc">期望状态值和状态-动作值方程</strong></figcaption></figure><p id="c471" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">一旦我们在状态<em class="ja"> s </em>中选择了一个动作<em class="ja"> a </em>，那么后一个动作也可以被重写为:</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/c68452720df3ca4f858807b5ec00d864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*X_Y2JEHzJKj4n1dd6M4l9A.png"/></div></figure><p id="f911" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">有了这些方程，现在也有可能计算最优政策和最优价值函数。一个策略<em class="ja"> π₁ </em>大于(或优于)π₂当且仅当对应的状态值大于或等于每个状态<em class="ja"> s ∈ S. </em>优于任何策略<em class="ja"> </em>的策略称为最优策略(o.p)。求解一个给定的MDP意味着评估o.p</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/dd68525bf960951341e2f20959518411.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EOKEFw3TbTLN9steBxAtIg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd kc">政策改进定理</strong></figcaption></figure><p id="7674" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">现在的问题是如何提出一个更好的政策？给定任何政策，如何找到相对更好的政策？这是通过一个反馈过程完成的，并被命名为“<strong class="jb hj"> <em class="ja">贪婪算法</em> </strong>”。这项政策通过贪婪的行动逐步得到改进。重复这个过程，你最终会得到一个o.p</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/6c9525fef735be667d09e6222bcb1939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bxPAOydEQnuHa1y5DiZCXg.png"/></div></div></figure><p id="2cf9" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">第一步→第二步→第一步→第二步。。。。。递归地继续这个过程，在一段时间后，你将观察不到状态值函数的改进。恭喜你。你已经完成了获得o.p .的使命。</p><p id="f216" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj"/>这就是所谓的<strong class="jb hj"> <em class="ja">策略迭代</em> </strong>方法。</p><p id="4b9d" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj"/>概念“arg max”表示动作<em class="ja"> a </em>，其后面的表达式被最大化。</p><p id="c9e1" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">所有这些概念都可以很容易地扩展到随机政策。</p><p id="d786" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">然而，这种方法有一个缺点。该算法在每一步都涉及政策评估，这在大多数实际问题中本身就是一个麻烦的任务。可以使用近似法，一些高效的数值方法(如最小二乘法)也可用于编程解决此类问题。</p><p id="9e52" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">它可以在其他时间探讨，但现在我将在这里简要概述一种不同的方法。大概动态编程中最流行的一种机制叫做<strong class="jb hj"> <em class="ja">值迭代</em> </strong>方法。</p><p id="3e23" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj"> □ </strong>之前，我们选择了一个随机策略并评估其价值函数，然后获得先前策略的改进版本，并再次评估其价值函数，以此类推。我们的意图是提取一个o.p. <br/>但是在这个过程中，我们随机选择一个价值函数，我们的意图是实现最优价值函数。一旦获得，那么由此产生的策略也应该是最优的。这是肉。下面给出了基本的数学框架。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/11e457370fc502de5800a19c3bd95607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9eQC2tjDKHThPajuXLzNg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">西尔弗教授指出这是一步到位的前瞻</figcaption></figure><p id="823b" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">一个自然的问题是何时终止？实际上，一旦价值函数在一次扫描中仅发生少量变化，我们就可以停止。</p><blockquote class="iv iw ix"><p id="26aa" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">我们所讨论的一切，只有在给定一个完美的环境模型，如MDP时，才是适用的。但是生活并不总是那么容易。如果我们没有MDP的知识，也就是说，支配这一现象的方程不存在，那会怎么样呢？模型的缺乏产生了对MDP采样的需要，以便获得关于未知模型的统计知识。这些被称为<strong class="jb hj">无模型RL </strong>技术，蒙特卡罗学习、时间差分学习是文献中常用的两种主要算法。在不久的将来，我也会就这个话题写一篇独立的博客。</p></blockquote><p id="9cee" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">环境的模型:</strong>模型用于规划，例如，它有助于预测未来的状态和可能的回报。基于此，行动的过程可以在没有经历环境的情况下被预先决定。<br/>在基于模型的RL中，代理并不拥有环境的先验模型，而是在学习时对其进行估计。在学习过程中，它与环境的交互次数最少，并试图构建一个模型。在归纳出一个合理的环境模型后，代理应用类似动态编程的算法来计算策略。相比之下，在无模型RL中，代理通过试错过程学习模型。它直接从经验中学习，执行一个动作，收集奖励(积极的或消极的)，然后更新价值函数。</p><p id="4a7a" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">我们来看一个例子，玩家A和B在下一盘棋。A走了第一步，然后B走了一步，A又走了一步，过了一段时间后，A发现他的几步都不够好。他输了比赛，但他从错误中吸取了教训。然而，如果A在观察了对手的几个动作后，在他的大脑中模拟了他的动作，并准备了一个模型并相应地进行了比赛，那么情况就会不同了！！</p><h1 id="6170" class="ln kb hi bd kc lo lp lq kg lr ls lt kk lu lv lw kn lx ly lz kq ma mb mc kt md bi translated">欢迎任何批评、建议和提示</h1><p id="ceca" class="pw-post-body-paragraph iy iz hi jb b jc kv je jf jg kw ji jj jx kx jm jn jy ky jq jr jz kz ju jv jw hb bi translated">参考文献<br/> 1。强化学习:导论——理查德·萨顿和安德鲁·g·巴尔托</p><p id="43d7" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">2.强化学习:Youtube视频——大卫·西尔弗</p></div></div>    
</body>
</html>