<html>
<head>
<title>Multi-Armed Bandit Analysis of Softmax Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Softmax算法的多臂土匪分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-armed-bandit-analysis-of-softmax-algorithm-e1fa4cb0c422?source=collection_archive---------6-----------------------#2020-02-21">https://medium.com/analytics-vidhya/multi-armed-bandit-analysis-of-softmax-algorithm-e1fa4cb0c422?source=collection_archive---------6-----------------------#2020-02-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9257" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">超越了<a class="ae jd" rel="noopener" href="/@kfoofw/multi-armed-bandit-analysis-of-epsilon-greedy-algorithm-8057d7087423">ε贪婪算法</a>，Softmax算法提供了进一步的优化，以提高探索期间的奖励机会。</p><p id="6847" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了获得更好的直觉，考虑2个伯努利臂的以下两种情况:</p><ul class=""><li id="7c7a" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">第一只手平均奖励0.1，而另一只手平均奖励0.9。</li><li id="2f0d" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">第一只手平均奖励0.1，而另一只手平均奖励0.11。</li></ul><p id="c03e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在两种情况下都使用Greedy，对于指定的ε百分比的探索试验，算法将在两种情况下在两个分支之间随机选择，而不管两个分支之间的平均回报有多不同。</p><p id="7905" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是Softmax算法可以提供的优化解决方案的机会所在。对于任何给定的回合，不是统一地探索所有的武器，而是根据它们当前平均奖励的差异提供不同机会的武器。</p><p id="df45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，Softmax建议在每个给定回合选择每个手臂的概率分布如下:</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/c0d3f417723399eeb957bc4c580bf5a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/0*cME3JGTJf8uMD4oC.png"/></div></figure><p id="54d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<code class="du ka kb kc kd b">r_i</code>代表当前回合每只手臂的当前奖励回报平均值。</p><p id="f2e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du ka kb kc kd b">tau</code>参数是一个超参数，最终决定随机化的程度。</p><ul class=""><li id="3495" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">当<code class="du ka kb kc kd b">tau</code>很大时，所有分支的总指数元素接近合成值1，因此探索任何特定分支的机会是1。</li><li id="446d" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">当<code class="du ka kb kc kd b">tau</code>较小时，每个分支的总指数元素与它们的当前回报成指数比例，因此，具有较高平均回报的分支将有较高的机会在探索阶段被选中。</li></ul><p id="fc07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">值得注意的是，Softmax算法的结构化探索结合了利用和探索两个方面。Softmax不是将试验分为勘探和开采阶段，而是通过使用它来增加选择较高回报臂的机会，同时也使选择较低回报臂成为可能(这是某种形式的勘探)。</p><p id="b8e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的分析是基于约翰·迈尔斯·怀特的书<a class="ae jd" href="https://www.oreilly.com/library/view/bandit-algorithms-for/9781449341565/" rel="noopener ugc nofollow" target="_blank">《网站优化的强盗算法》</a>。为了进一步理解代码，我加入了一些注释以便于理解。</p><p id="767b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是创建SoftMax算法设置和逐步更新arm的计数和值的代码。</p><ul class=""><li id="1980" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">计数:代表手臂被拉动的记录时间。</li><li id="d413" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">值:代表已知的平均报酬。在伯努利臂的情况下，值代表奖励的概率，范围从0到1。</li></ul><pre class="jt ju jv jw fd ke kd kf kg aw kh bi"><span id="9e80" class="ki kj hi kd b fi kk kl l km kn">import math<br/>import random</span><span id="a835" class="ki kj hi kd b fi ko kl l km kn"># Arm selection based on Softmax probability<br/>def categorical_draw(probs):<br/>    z = random.random()<br/>    cum_prob = 0.0<br/>    <br/>    for i in range(len(probs)):<br/>        prob = probs[i]<br/>        cum_prob += prob<br/>        <br/>        if cum_prob &gt; z:<br/>            return i<br/>    return len(probs) - 1</span><span id="799d" class="ki kj hi kd b fi ko kl l km kn"># Softmax algorithm<br/>class Softmax:<br/>    def __init__(self, tau, counts, values):<br/>        self.tau = tau<br/>        self.counts = counts # Count represent counts of pulls for each arm. For multiple arms, this will be a list of counts.<br/>        self.values = values # Value represent average reward for specific arm. For multiple arms, this will be a list of values.<br/>        return<br/>    <br/>    # Initialise k number of arms<br/>    def initialize(self, n_arms):<br/>        self.counts = [0 for col in range(n_arms)]<br/>        self.values = [0.0 for col in range(n_arms)]<br/>        return<br/>    <br/>    def select_arm(self):<br/>        # Calculate Softmax probabilities based on each round<br/>        z = sum([math.exp(v / self.tau) for v in self.values])<br/>        probs = [math.exp(v / self.tau) / z for v in self.values]<br/>        <br/>        # Use categorical_draw to pick arm<br/>        return categorical_draw(probs)<br/>    <br/>    # Choose to update chosen arm and reward<br/>    def update(self, chosen_arm, reward):<br/>        # update counts pulled for chosen arm<br/>        self.counts[chosen_arm] = self.counts[chosen_arm] + 1<br/>        n = self.counts[chosen_arm]<br/>        <br/>        # Update average/mean value/reward for chosen arm<br/>        value = self.values[chosen_arm]<br/>        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward<br/>        self.values[chosen_arm] = new_value<br/>        return</span></pre><p id="7115" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据之前在Epsilon Greedy文章中的讨论，我们将使用伯努利分布来表示每条手臂的奖励函数。</p><pre class="jt ju jv jw fd ke kd kf kg aw kh bi"><span id="6bbd" class="ki kj hi kd b fi kk kl l km kn">class BernoulliArm():<br/>    def __init__(self, p):<br/>        self.p = p<br/>    <br/>    # Reward system based on Bernoulli<br/>    def draw(self):<br/>        if random.random() &gt; self.p:<br/>            return 0.0<br/>        else:<br/>            return 1.0</span></pre><p id="73c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了进行任何进一步的分析，需要一个操作脚本来处理模拟。以下代码有助于为epsilon值的特定运行创建模拟。</p><ul class=""><li id="e345" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">num_sims:表示独立模拟的数量，每个模拟的长度等于“地平线”。</li><li id="bd73" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">horizon:表示每轮模拟的时间步长/试验次数</li></ul><pre class="jt ju jv jw fd ke kd kf kg aw kh bi"><span id="115a" class="ki kj hi kd b fi kk kl l km kn">def test_algorithm(algo, arms, num_sims, horizon):<br/>    <br/>    # Initialise variables for duration of accumulated simulation (num_sims * horizon_per_simulation)<br/>    chosen_arms = [0.0 for i in range(num_sims * horizon)]<br/>    rewards = [0.0 for i in range(num_sims * horizon)]<br/>    cumulative_rewards = [0 for i in range(num_sims * horizon)]<br/>    sim_nums = [0.0 for i in range(num_sims *horizon)]<br/>    times = [0.0 for i in range (num_sims*horizon)]<br/>    <br/>    for sim in range(num_sims):<br/>        sim = sim + 1<br/>        algo.initialize(len(arms))<br/>        <br/>        for t in range(horizon):<br/>            t = t + 1<br/>            index = (sim -1) * horizon + t -1<br/>            sim_nums[index] = sim<br/>            times[index] = t<br/>            <br/>            # Selection of best arm and engaging it<br/>            chosen_arm = algo.select_arm()<br/>            chosen_arms[index] = chosen_arm<br/>            <br/>            # Engage chosen Bernoulli Arm and obtain reward info<br/>            reward = arms[chosen_arm].draw()<br/>            rewards[index] = reward<br/>            <br/>            if t ==1:<br/>                cumulative_rewards[index] = reward<br/>            else:<br/>                cumulative_rewards[index] = cumulative_rewards[index-1] + reward<br/>                <br/>            algo.update(chosen_arm, reward)<br/>    <br/>    return [sim_nums, times, chosen_arms, rewards, cumulative_rewards]</span></pre><h1 id="50e2" class="kp kj hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">模拟手段差异较大的兵种</h1><p id="43d3" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">类似于前面对ε-greedy的分析，模拟包括以下内容:</p><ul class=""><li id="223e" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">创造5个兵种，其中四个平均奖励0.1，最后一个/最好的平均奖励0.9。</li><li id="d350" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">将模拟输出保存到制表符分隔的文件中</li><li id="f083" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">为每个<code class="du ka kb kc kd b">tau</code>值创建5000个独立的模拟，总共5个ε值，范围从0.1到0.5。</li></ul><p id="f605" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择5000个独立模拟是因为我们想要确定平均性能。每个模拟可能受随机性质/运行的影响，并且性能可能由于随机机会而有偏差。因此，运行合理的大量模拟来评估平均均值/性能非常重要。</p><pre class="jt ju jv jw fd ke kd kf kg aw kh bi"><span id="45fb" class="ki kj hi kd b fi kk kl l km kn">import random</span><span id="5968" class="ki kj hi kd b fi ko kl l km kn">random.seed(1)<br/># out of 5 arms, 1 arm is clearly the best<br/>means = [0.1, 0.1, 0.1, 0.1, 0.9]<br/>n_arms = len(means)<br/># Shuffling arms<br/>random.shuffle(means)</span><span id="4f54" class="ki kj hi kd b fi ko kl l km kn"># Create list of Bernoulli Arms with Reward Information<br/>arms = list(map(lambda mu: BernoulliArm(mu), means))<br/>print("Best arm is " + str(np.argmax(means)))</span><span id="f4ee" class="ki kj hi kd b fi ko kl l km kn">f = open("standard_results_soft.tsv", "w+")</span><span id="9658" class="ki kj hi kd b fi ko kl l km kn"># Create simulations for each tau/temperature value<br/>for tau in [0.1, 0.2, 0.3, 0.4, 0.5]:<br/>    algo = SoftMax(tau, [], [])<br/>    algo.initialize(n_arms)<br/>    results = test_algorithm(algo, arms, 5000, 250)<br/>    <br/>    # Store data<br/>    for i in range(len(results[0])):<br/>        f.write(str(epsilon) + "\t")<br/>        f.write("\t".join([str(results[j][i]) for j in range(len(results))]) + "\n")<br/>f.close()</span></pre><p id="a849" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用一些数据预处理和基本的Altair可视化，我们可以为每个ε值绘制拉最佳臂的概率。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es lr"><img src="../Images/cde3acb963352db6510700fda4263764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/0*rMdMIcU2fS_m7QXG.png"/></div></figure><p id="8e11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于不同的<code class="du ka kb kc kd b">tau</code>值，选择最佳臂的最终速率有不同的渐近极限。如前所述，<code class="du ka kb kc kd b">tau</code>的值越高，挑选武器的随机化程度越高，这解释了渐近线。对于0.1的<code class="du ka kb kc kd b">tau</code>值，它似乎最终收敛到接近1。</p><p id="19fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于所有的<code class="du ka kb kc kd b">tau</code>值来说，收敛到它们相应渐近线的速度似乎是相似的，除了0.1似乎需要两倍于其他值的时间。从达到收敛的相反角度来看，似乎存在某种朝向渐近线的指数衰减方式，如开始时几乎相似的梯度所示(从0到10的时间步长)。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ls"><img src="../Images/27c6b6e9a44e89a567d0a01349437b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/0*JrhLSP_GGvHkPwpk.png"/></div></figure><p id="30ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看累积奖励系统，与Epsilon Greedy算法相比，Softmax算法有明显的不同。我们看到，对于Epsilon Greedy算法模拟，值为0.1的<code class="du ka kb kc kd b">epsilon</code>必须进行追赶，因为它在实验的早期阶段没有进行足够的探索来发现最佳arm。因此，在250个时间步长的大部分时间里，其累积回报低于其他<code class="du ka kb kc kd b">epsilon</code>值。</p><p id="929d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Softmax算法，我们看到其算法在利用中提供了足够的探索。因此，就累积奖励而言，<code class="du ka kb kc kd b">tau</code>的较低值一直保持较高。这很直观，因为<code class="du ka kb kc kd b">tau</code>的值越低，随机探索的程度越低。因此，选择较差返回臂的程度较低。</p><p id="e73a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还必须注意的是，对于Epsilon Greedy，累积奖励值在140到180之间。在这种情况下，0.1和0.2的Softmax <code class="du ka kb kc kd b">tau</code>值能够达到200到220的累积奖励范围，这意味着他们不断利用最好的手臂，并且在很早的时候就这样做了。</p><h1 id="5467" class="kp kj hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">平均差异相对较小的武器的模拟</h1><p id="e3df" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">之前的分析是对回报差异很大的武器的模拟练习。我们将分析扩展到两臂相对较近的情况。</p><p id="2048" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的例子中，我们模拟了5个分支，其中4个分支的平均值为0.8，而最后一个/最佳分支的平均值为0.9。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es lr"><img src="../Images/c6930897e2a3dce9cd3d5bce74370774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/0*Ibn-6nCzeCnQM7eV.png"/></div></figure><p id="d0ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着arm的回报越来越接近，我们看到Softmax算法的性能大幅下降。选择最佳手臂的比率现在在0.22到0.32之间。有趣的是，观察到0.2的<code class="du ka kb kc kd b">tau</code>值具有0.32的渐近线，而0.1的<code class="du ka kb kc kd b">tau</code>值具有0.25的渐近线(这与<code class="du ka kb kc kd b">tau</code>值0.4的渐近线相似)。</p><p id="a541" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这似乎意味着奖励函数的差异减小使得更难确定哪个是最好的手臂，特别是相对于<code class="du ka kb kc kd b">tau</code>值0.1。事实上，对于由更高的<code class="du ka kb kc kd b">tau</code>值0.2(或者甚至0.3)表示的更高程度的随机化，我们获得了选择最佳臂的更高比率。</p><p id="3a6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">应当注意，在这种情况下，对于ε贪婪算法，与Softmax算法的范围0.22至0.32相比，选择最佳arm的比率实际上更高，如范围0.5至0.7所示。这似乎意味着Epsilon Greedy可能更适合于均值差异小得多的基于多臂的情况。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ls"><img src="../Images/33ee58e02735b1a3444159724e7e879c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/0*IcqMqSSrKDvOzvKF.png"/></div></figure><p id="c8a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与我们在Epsilon Greedy分析中看到的类似，所有<code class="du ka kb kc kd b">tau</code>值的累积奖励在本质上非常接近，以至于无法区分。同样，这很可能是因为各兵种的奖励手段都相当接近。</p><p id="307e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于这两个分支的平均回报率很接近，因此验证总体累积遗憾会更有意思。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es lt"><img src="../Images/0a528e4765fead76dde84024dbfbd688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/0*6QH5lY3HKNF-iYzS.png"/></div></figure><p id="fea2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于累积遗憾图，我们看到0.2的<code class="du ka kb kc kd b">tau</code>值在15.5左右最好，而0.5的<code class="du ka kb kc kd b">tau</code>值在19.2最差。总的来说，与范围为12.3到14.8的Epsilon Greedy算法相比，Softmax算法最差。对于表现更好的<code class="du ka kb kc kd b">tau</code>值，在图中有一个逐渐变小的趋势，但作为一个整体，Softmax算法似乎比Epsilon Greedy算法更糟糕。</p><h1 id="47c2" class="kp kj hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">摘要</h1><p id="c4ce" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">在对Softmax算法的分析中，我们涵盖了算法直觉以及应用Softmax算法的实验结果。这些结果是根据之前提到的<a class="ae jd" rel="noopener" href="/@kfoofw/multi-armed-bandit-analysis-of-epsilon-greedy-algorithm-8057d7087423">ε贪婪算法</a>进行基准测试的。</p><p id="cdbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个学习收获是，对于具有更近均值的arm，Softmax算法在确定最佳arm方面似乎不那么稳健，而Epsilon Greedy更适合于此。</p><p id="d911" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于bandit模拟分析项目的参考，请参考此<a class="ae jd" href="https://github.com/kfoofw/bandit_simulations" rel="noopener ugc nofollow" target="_blank"> Github repo </a>。关于实际代码的快速参考，请参考本<a class="ae jd" href="https://github.com/kfoofw/bandit_simulations/blob/master/python/notebooks/analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>。</p></div></div>    
</body>
</html>