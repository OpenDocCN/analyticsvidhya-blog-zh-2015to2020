<html>
<head>
<title>Analyzing Data and Performance Tuning of Apache Spark Engine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark引擎的数据分析和性能调整</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/analyzing-data-and-performance-tuning-of-apache-spark-engine-1dfdf4e2c705?source=collection_archive---------11-----------------------#2019-11-18">https://medium.com/analytics-vidhya/analyzing-data-and-performance-tuning-of-apache-spark-engine-1dfdf4e2c705?source=collection_archive---------11-----------------------#2019-11-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="4f42" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi jk translated"><span class="l jl jm jn bm jo jp jq jr js di"> A </span> pache Spark是一种快速的内存处理框架，旨在支持和处理大数据。任何形式的数据都非常巨大(即GB、TB、PB ),并且无法用标准配置的个人计算机处理，这种数据称为大数据。Spark使用各种API来加载数据和执行分析。常用的两个库是dataframe API和Spark SQL。</p><p id="0fb5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Dataframe就像以命名列组织的数据集，相当于关系数据库中的表。Spark SQL是apache spark处理结构化数据的模块。spark与python编程的合作被称为PySpark。它是python在spark上的API，以python风格编写程序(类似于pandas库，但spark遵循一个懒惰的评估，我将在本文后面谈到)。</p><p id="42a0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇文章中，我将通过一个旧金山火警电话数据集来检索见解。此外，您将了解如何使用spark缓存表和RDD分区配置来提高查询效率。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/39f10f3e253ce23c9105c318e3933565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HfJ7nhjgrFaxE9ggn8rckw.jpeg"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">消防部门服务电话</figcaption></figure><h1 id="0955" class="kj kk hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated"><strong class="ak">关于数据集</strong></h1><p id="f12b" class="pw-post-body-paragraph im in hi io b ip lh ir is it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj hb bi translated"><strong class="io hj">消防呼叫服务</strong>包括所有消防单位对呼叫的响应。每个记录包括呼叫号码、事故号码、地址、单位标识符、呼叫类型和处置。还包括所有相关的时间间隔。因为该数据集基于响应，并且大多数呼叫涉及多个单元，所以每个呼叫号码都有多个记录。地址与街区号码、交叉路口或电话亭相关联，而不是特定的地址。</p><p id="a6e9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数据提取自<a class="ae lm" href="https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3" rel="noopener ugc nofollow" target="_blank">https://data.sfgov.org</a></p><h1 id="b8c8" class="kj kk hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated"><strong class="ak">数据分析</strong></h1><p id="7ab3" class="pw-post-body-paragraph im in hi io b ip lh ir is it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj hb bi translated"><em class="ln">从数据源加载数据</em> <br/>通过推断模式从csv文件创建数据帧，运行spark作业。执行时间为<strong class="io hj"> 16s </strong>。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="98ee" class="lt kk hi lp b fi lu lv l lw lx">firedatadf = spark.read.csv(‘/xxxx/sf_open_data/fire_dept_calls_for_service/Fire_Department_Calls_for_Service.csv’, header=True, inferSchema=True)</span><span id="796b" class="lt kk hi lp b fi ly lv l lw lx">firedatadf.printSchema()</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lz"><img src="../Images/cbaaeef586ad3c61ad1e6f39f915cf0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cvsy4YXVO-5oy4i-WQmv9A.png"/></div></div></figure><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="b74f" class="lt kk hi lp b fi lu lv l lw lx">firedatadf.count()<br/>len(firedatadf.columns)</span><span id="c95f" class="lt kk hi lp b fi ly lv l lw lx">4091248<br/>34</span><span id="e66a" class="lt kk hi lp b fi ly lv l lw lx">#The dataset contains 4 millions rows, 34 columns<br/>#While checking the count, it does a full scan of the table data #reading 1.6 GB of data from disk. <br/>#Time taken is <strong class="lp hj">20s</strong></span></pre><p id="ea18" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以看到列名中有空格，这可能是以后保存文件时的一个问题。因此，我们将创建一个用户定义的模式，并显式地将该模式推断到该数据集。请注意，从第一步加载的csv花费了<strong class="io hj"> 16s </strong>，因为它通过采样和读取来推断模式。这对于小规模的数据集来说很好，但是对于万亿字节的数据来说，最好使用定义好的模式来加快加载速度。为此，我将从pyspark.sql.types导入模块。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="ae82" class="lt kk hi lp b fi lu lv l lw lx">from pyspark.sql.types import StructType, StructField, IntegerType, StringType</span><span id="2a8f" class="lt kk hi lp b fi ly lv l lw lx">Schema_new = StructType([StructField(‘CallNumber’, IntegerType(), True),<br/> StructField(‘UnitID’, StringType(), True),<br/> StructField(‘IncidentNumber’, IntegerType(), True),<br/> StructField(‘CallType’, StringType(), True), <br/> StructField(‘CallDate’, StringType(), True), <br/> StructField(‘WatchDate’, StringType(), True), <br/> StructField(‘ReceivedDtTm’, StringType(), True), <br/> StructField(‘EntryDtTm’, StringType(), True), <br/> StructField(‘DispatchDtTm’, StringType(), True), <br/> StructField(‘ResponseDtTm’, StringType(), True), <br/> StructField(‘OnSceneDtTm’, StringType(), True), <br/> StructField(‘TransportDtTm’, StringType(), True), <br/> StructField(‘HospitalDtTm’, StringType(), True), <br/> StructField(‘CallFinalDisposition’, StringType(), True), <br/> StructField(‘AvailableDtTm’, StringType(), True), <br/> StructField(‘Address’, StringType(), True), <br/> StructField(‘City’, StringType(), True), <br/> StructField(‘ZipcodeofIncident’, IntegerType(), True), <br/> StructField(‘Battalion’, StringType(), True), <br/> StructField(‘StationArea’, StringType(), True), <br/> StructField(‘Box’, StringType(), True), <br/> StructField(‘OriginalPriority’, StringType(), True), <br/> StructField(‘Priority’, StringType(), True), <br/> StructField(‘FinalPriority’, IntegerType(), True), <br/> StructField(‘ALSUnit’, BooleanType(), True), <br/> StructField(‘CallTypeGroup’, StringType(), True),<br/> StructField(‘NumberofAlarms’, IntegerType(), True),<br/> StructField(‘UnitType’, StringType(), True),<br/> StructField(‘Unitsequenceincalldispatch’, IntegerType(), True),<br/> StructField(‘FirePreventionDistrict’, StringType(), True),<br/> StructField(‘SupervisorDistrict’, StringType(), True),<br/> StructField(‘NeighborhoodDistrict’, StringType(), True),<br/> StructField(‘Location’, StringType(), True),<br/> StructField(‘RowID’, StringType(), True)])</span></pre><p id="c91c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用用户定义的模式加载数据。这次需要<strong class="io hj">0.33秒</strong>T19】做一个快速计数并打印Schema来检查结构。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="56b3" class="lt kk hi lp b fi lu lv l lw lx">firedatanewDF = spark.read.csv(‘/xxx/sf_open_data/fire_dept_calls_for_service/Fire_Department_Calls_for_Service.csv’, header=True, schema=Schema_new)</span><span id="1e72" class="lt kk hi lp b fi ly lv l lw lx">firedatanewDF.count()</span><span id="88dc" class="lt kk hi lp b fi ly lv l lw lx">firedatanewDF.printSchema()</span></pre><p id="6e09" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们检查有多少不同的呼叫类型及其计数。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="2b9a" class="lt kk hi lp b fi lu lv l lw lx">firedatanewDF.groupBy(‘CallType’).count().orderBy(‘count’,ascending = False).show(100,False)</span><span id="c616" class="lt kk hi lp b fi ly lv l lw lx">#time: <strong class="lp hj">14.25s</strong></span></pre><p id="11f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该数据有34种不同的呼叫类型。看起来有250万个医疗急救电话，其次是结构火灾、警报和交通碰撞。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ma"><img src="../Images/eb27cc8b441683621368995360adcbfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vP3mBvUKcXQSQwMYo1Sa9A.png"/></div></div></figure><p id="b115" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们看看7月4日有多少服务电话，假设由于烟花爆竹有很高的火灾事故和医疗紧急情况的概率。为了处理时间戳数据，我们必须将日期和时间列转换成时间戳。默认情况下，推断模式得到字符串格式的日期列。我将使用unix_timestamp()函数将字符串转换为时间戳。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="6660" class="lt kk hi lp b fi lu lv l lw lx">from pyspark.sql.functions import *<br/>pattern1 = ‘MM/dd/yyyy’<br/>pattern2 = ‘MM/dd/yyyy hh:mm:ss aa’<br/></span><span id="a3b0" class="lt kk hi lp b fi ly lv l lw lx">firedatanewTsdf = fireServiceCallsDF \<br/> .withColumn(‘CallDateTS’, unix_timestamp(fireServiceCallsDF[‘CallDate’], pattern1).cast(“timestamp”)) \<br/> .drop(‘CallDate’) \<br/> .withColumn(‘WatchDateTS’, unix_timestamp(fireServiceCallsDF[‘WatchDate’], pattern1).cast(“timestamp”)) \<br/> .drop(‘WatchDate’) \<br/> .withColumn(‘ReceivedDtTmTS’, unix_timestamp(fireServiceCallsDF[‘ReceivedDtTm’], pattern2).cast(“timestamp”)) \<br/> .drop(‘ReceivedDtTm’) \<br/> .withColumn(‘EntryDtTmTS’, unix_timestamp(fireServiceCallsDF[‘EntryDtTm’], pattern2).cast(“timestamp”)) \<br/> .drop(‘EntryDtTm’) \<br/> .withColumn(‘DispatchDtTmTS’, unix_timestamp(fireServiceCallsDF[‘DispatchDtTm’], pattern2).cast(“timestamp”)) \<br/> .drop(‘DispatchDtTm’) \<br/> .withColumn(‘ResponseDtTmTS’, unix_timestamp(fireServiceCallsDF[‘ResponseDtTm’], pattern2).cast(“timestamp”)) \<br/> .drop(‘ResponseDtTm’) \<br/> .withColumn(‘OnSceneDtTmTS’, unix_timestamp(fireServiceCallsDF[‘OnSceneDtTm’], pattern2).cast(“timestamp”)) \<br/> .drop(‘OnSceneDtTm’) \<br/> .withColumn(‘TransportDtTmTS’, unix_timestamp(fireServiceCallsDF[‘TransportDtTm’], pattern2).cast(“timestamp”)) \<br/> .drop(‘TransportDtTm’) \<br/> .withColumn(‘HospitalDtTmTS’, unix_timestamp(fireServiceCallsDF[‘HospitalDtTm’], pattern2).cast(“timestamp”)) \<br/> .drop(‘HospitalDtTm’) \<br/> .withColumn(‘AvailableDtTmTS’, unix_timestamp(fireServiceCallsDF[‘AvailableDtTm’], pattern2).cast(“timestamp”)) \<br/> .drop(‘AvailableDtTm’)</span></pre><p id="75ef" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用printSchema检查结构和数据类型</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="3ae2" class="lt kk hi lp b fi lu lv l lw lx"># Checking the period of incidents recorded in dataset.</span><span id="bbb3" class="lt kk hi lp b fi ly lv l lw lx">firedatanewTSdf.select(year('CallDateTS')).distinct().orderBy('year(CallDateTS)').show()</span><span id="e995" class="lt kk hi lp b fi ly lv l lw lx"># The data available is from year 2000 until 2016. This is a big dataset.</span><span id="b98a" class="lt kk hi lp b fi ly lv l lw lx"># Just for ease of readability, i created new columns with year, day # of month, month using <em class="ln">withColumn </em>function</span><span id="579b" class="lt kk hi lp b fi ly lv l lw lx">firedatanewTSdf2 = firedatanewTSdf1.withColumn('Year',year('CallDateTS').cast('int')).withColumn('DayOfMonth',dayofmonth('CallDateTS').cast("int")).withColumn('Month',month('CallDateTS').cast('int'))</span></pre><p id="5a3a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">查询7月4日的消防服务电话。下面的计数显示了2016年和2010年的最高通话次数</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="eec6" class="lt kk hi lp b fi lu lv l lw lx">firedatanewTSdf2.filter(‘Month == 7’).filter(‘DayOfMonth == 4’).groupBy(‘Year’).count().orderBy(‘Year’, ascending=False).show()</span><span id="046d" class="lt kk hi lp b fi ly lv l lw lx">|Year|count|<br/>+-----+-----+<br/>|2016|  958| <br/>|2015|  850| <br/>|2014|  783| <br/>|2013|  870| <br/>|2012|  761| <br/>|2011|  791| <br/>|2010|  907| <br/>|2009|  798| <br/>|2008|  838| <br/>|2007|  703| <br/>|2006|  737| <br/>|2005|  651| <br/>|2004|  752| <br/>|2003|  734| <br/>|2002|  731| <br/>|2001|  691| <br/>|2000|  790|</span></pre><p id="969d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">进一步分析后发现，在2016年，医疗事故是来电的最主要原因，我们可以将这些原因与之前步骤中的来电类型联系起来。每年的这个时候都有超过10万次的通话。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="5515" class="lt kk hi lp b fi lu lv l lw lx">firedatanewTSdf2.filter(‘year = 2016’).groupBy(‘calltype’).count().orderBy(‘count’, ascending = False).show(10,False)</span><span id="b666" class="lt kk hi lp b fi ly lv l lw lx">|calltype                       |count |<br/>+-------------------------------+------+<br/>|Medical Incident               |105191| <br/>|Alarms                         |15460 | <br/>|Structure Fire                 |14509 | <br/>|Traffic Collision              |6721  | <br/>|Citizen Assist / Service Call  |1895  | <br/>|Other                          |1834  | <br/>|Outside Fire                   |1764  | <br/>|Water Rescue                   |1118  | <br/>|Gas Leak (Natural and LP Gases)|651   |</span></pre><h1 id="02b6" class="kj kk hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated"><strong class="ak">加入操作——查找邻近小区的呼叫数和主要情况</strong></h1><p id="a7cf" class="pw-post-body-paragraph im in hi io b ip lh ir is it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj hb bi translated"><strong class="io hj">哪个街区产生的呼叫数量最多</strong></p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="9a85" class="lt kk hi lp b fi lu lv l lw lx">firedatanewTSdf2.groupBy(‘NeighborhoodDistrict’).count().orderBy(‘count’, ascending = False ).show(100, False)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mb"><img src="../Images/d3eae795b52dd3cace55339ef7c7073c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IN3RRv5XoL2NaZX63f-MdA.png"/></div></div></figure><p id="9890" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">评论</strong><a class="ae lm" href="https://www.yelp.com/biz/the-tenderloin-san-francisco-5" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">Yelp</strong></a><strong class="io hj">:</strong>请参考链接。</p><p id="4aa8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">旧金山的田德隆区是我们国家最危险、最肮脏、最恶心的地区之一。在<strong class="io hj">里脊</strong>中，你很有可能会被谋杀。但是有一些漂亮的墙壁艺术。旧金山的这一部分是美国每个大城市所有错误的生动缩影。</p><p id="41ee" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">连接事故数据集中的数据帧，分析主要情况</strong></p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="2a8c" class="lt kk hi lp b fi lu lv l lw lx">## reading <a class="ae lm" href="https://data.sfgov.org/Public-Safety/Fire-Incidents/wr8u-xric" rel="noopener ugc nofollow" target="_blank"><strong class="lp hj">incident</strong></a> dataset from same source</span><span id="ff0b" class="lt kk hi lp b fi ly lv l lw lx">incidentsDF = spark.read.csv(‘/xxx/sf_open_data/fire_incidents/Fire_Incidents.csv’, header=True, inferSchema=True).withColumnRenamed(‘Incident Number’, ‘IncidentNumber’).cache()</span><span id="c856" class="lt kk hi lp b fi ly lv l lw lx">## Join dataframes</span><span id="4b70" class="lt kk hi lp b fi ly lv l lw lx">join_DF = firedatanewTSdf2.join(incidentsDF, firedatanewTSdf2.IncidentNumber == incidentsDF.IncidentNumber)</span></pre><p id="c9aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">结果</strong></p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="baf5" class="lt kk hi lp b fi lu lv l lw lx">join_DF.filter(col(‘NeighborhoodDistrict’).like(‘Tenderloin’)).groupBy(‘Primary Situation’).count().orderBy(‘count’, ascending = False).show(10,False)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mc"><img src="../Images/31986334143383501edbb8a53ee47a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eQ1dEE-YznWa5dLQlE-3sA.png"/></div></div></figure><p id="ddcf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">查一下旧金山富人区的电话。很好奇它们会发出什么样的叫声。参考最贵社区信息链接:<a class="ae lm" href="https://www.investopedia.com/articles/personal-finance/092515/most-expensive-neighborhoods-san-francisco.asp" rel="noopener ugc nofollow" target="_blank">点击此处</a></p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="1286" class="lt kk hi lp b fi lu lv l lw lx">## no. of calls from <strong class="lp hj">Pacific Heights and Financial District</strong></span><span id="c49c" class="lt kk hi lp b fi ly lv l lw lx">firedatanewTSdf2.filter(col(‘NeighborhoodDistrict’).like(‘Pacific Heights’)).groupBy(‘NeighborhoodDistrict’).count().show()</span><span id="33da" class="lt kk hi lp b fi ly lv l lw lx">firedatanewTSdf2.filter(col(‘NeighborhoodDistrict’).like(‘Financial District%’)).groupBy(‘NeighborhoodDistrict’).count().show(2,False)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es md"><img src="../Images/011a76c97c4908cf150d1844d778175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MGEGPh5fmBGiUQbjTavhwQ.png"/></div></div></figure><p id="6538" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">过滤邻居小区</strong></p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="073b" class="lt kk hi lp b fi lu lv l lw lx">## Filter <strong class="lp hj">Pacific Heights</strong></span><span id="55b3" class="lt kk hi lp b fi ly lv l lw lx">join_DF.filter(col('NeighborhoodDistrict').like('<strong class="lp hj">Pacific Heights</strong>')).groupBy('Primary Situation').count().orderBy('count', ascending = False).show(10,False)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es me"><img src="../Images/84a096f720271c1e96db3a8158e4f55d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sA6Py3xr9D4FwtaSVKLu6w.png"/></div></div></figure><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="4a33" class="lt kk hi lp b fi lu lv l lw lx">## Filter <strong class="lp hj">Financial District</strong></span><span id="d254" class="lt kk hi lp b fi ly lv l lw lx">join_DF.filter(col(‘NeighborhoodDistrict’).like(‘<strong class="lp hj">Financial</strong>%’)).groupBy(‘Primary Situation’).count().orderBy(‘count’, ascending = False).show(10,False)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mf"><img src="../Images/e5213ae6003a5b250e9117851f1403e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiMkGdcjBAEIN4igeeehdA.png"/></div></div></figure><p id="968c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从上面的分析来看，电话似乎大多是非生命威胁或虚假警报。来自这些地区的呼叫可被视为低-中优先级。</p><p id="e981" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们使用dataframe API完成了初始分析。虽然我们可以通过结合与旧金山医疗事故相关的其他数据集来深入挖掘数据集以找到相关性。</p><h1 id="859b" class="kj kk hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated"><strong class="ak">火花发动机性能分析。</strong></h1><p id="f3e0" class="pw-post-body-paragraph im in hi io b ip lh ir is it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj hb bi translated">请记住，在编写上述部分的所有输出时，我提到了spark job完成查询所花费的时间。现在，是时候使用缓存表和RDD分区来比较性能了。对于更复杂的查询，计算时间会增加。Spark使用惰性评估，直到在转换栈上触发一个动作时才返回结果。Spark是一个非常强大的内存计算工具，它通过在内存中缓存数据来节省时间，不像Hadoop文件系统那样在磁盘中写入中间排序(洗牌阶段)结果。因此，能够更快地提供结果。</p><p id="4990" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Spark围绕着<em class="ln">弹性分布式数据集</em> (RDD)的概念，这是一个可以并行操作的容错元素集合。创建rdd有两种方法:<em class="ln">并行化驱动程序中的现有集合</em>，或者引用外部存储系统中的数据集，如共享文件系统、HDFS、HBase或任何提供Hadoop InputFormat的数据源。在本例中，我将把我的数据帧转换成RDD，并读取分区数量。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="2ce2" class="lt kk hi lp b fi lu lv l lw lx">firedatanewTSdf2.rdd.getNumPartitions()</span><span id="e476" class="lt kk hi lp b fi ly lv l lw lx">13</span></pre><p id="1042" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在spark内部，数据帧由分区组成，这意味着在这种情况下，默认情况下，全部数据的13分之一驻留在每个分区中。然而，这可能不是最佳的。如果数据足够大，您可以检查机器上运行的内核数量，并以内核数量的2或3倍的倍数配置分区。这样，每个数据块几乎大小相等，可以在一个阶段中并行运行。在这种情况下，我将把我的RDD重新分区成8个块(8核机器)并缓存它。您可以在<em class="ln"> Spark UI </em>(执行者标签)上查看执行者的数量。在下面的查询中，我创建了我的表的视图，并缓存了我的视图。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="942a" class="lt kk hi lp b fi lu lv l lw lx">firedatanewTSdf2.repartition(8).createOrReplaceTempView(‘firedataV1’)<br/>spark.catalog.cacheTable(‘firedataV1’)<br/>spark.table(‘firedataV1’).count()</span></pre><p id="f14c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">缓存是懒惰的，它不缓存任何东西，直到一个动作被触发。因此，我触发了一个count函数调用，该函数从磁盘读取所有数据，在内存中创建一个临时数据帧，默认为13个分区(阶段65)，然后重新分区为8个分区(阶段66)，并将其缓存在内存中。这一步跑起来需要更长的时间<strong class="io hj"> 1.93分钟</strong>。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mg"><img src="../Images/bd2869f7a608e8d7f5bdea4be2f562a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TpUIVAqQgIGe8oThVtyvg.png"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mh"><img src="../Images/1b7dbcd8c5fb9195fd8ce611c877383c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AT5y39whmNuPtxMEA_c8Fg.png"/></div></div></figure><p id="d8a0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我有一台8核EC2机器，因此重新划分为8个。从下图中可以看到，缓存分区(RDD)为8，100%缓存，内存大小为645 MB。Spark使用列压缩方法来调整文件大小，这使得内存中的存储比磁盘上的序列化数据更有效。每个分区80 MB。这样，8个任务可以在一个阶段的8个槽中运行。您可以根据集群中的数据和节点(加上核心)的大小来决定分区因子。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mi"><img src="../Images/927248bfa1a187ec21380ae84d733894.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1j92VTG5hLZzQInuhIpiQ.png"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mj"><img src="../Images/1d8f09ceca5555f8ea1752ca3ccda955.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VfM_QB96JiHAPaqzW808HA.png"/></div></div></figure><p id="d098" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从缓存的表中读取数据帧中的数据。现在，如果我执行一个计数操作，花费的时间要少得多。Spark作业跳过第一阶段，因为它不再需要从磁盘中读取数据，然后进入第二阶段，从内存中有8个分区的数据帧中读取数据。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="3e5d" class="lt kk hi lp b fi lu lv l lw lx">firedataService_df = spark.table(‘firedataV1’)</span><span id="96ad" class="lt kk hi lp b fi ly lv l lw lx">firedataService_df.count()<br/># time : 0.40s</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mk"><img src="../Images/194872776241712466ebbc2ef8759ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*qloXJ_5HveOzX55MahK9Bw.png"/></div></figure><h1 id="ab1c" class="kj kk hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated"><strong class="ak">使用Spark SQL进行性能检查</strong></h1><p id="5c3d" class="pw-post-body-paragraph im in hi io b ip lh ir is it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj hb bi translated">让我们使用<strong class="io hj"> Spark SQL </strong>从缓存的表中查询类似的数据，并比较返回结果所需的时间。我可以看到性能的提高。该查询用了<strong class="io hj"> 1.81s </strong>返回相同的结果。<br/>使用缓存表中的新数据帧，saprk作业在<strong class="io hj"> 1.95s </strong>中完成并返回相同的结果，与<strong class="io hj"> 20s </strong>相比要好得多(参见顶部第二个块)。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="bbd2" class="lt kk hi lp b fi lu lv l lw lx">%sql select Calltype, count(Calltype) as count from firedataV1 group by Calltype order by count(calltype) desc</span><span id="fd12" class="lt kk hi lp b fi ly lv l lw lx">## %sql is a command available in databricks notebook instead of using <em class="ln">spark.sql</em> command</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ml"><img src="../Images/5b4b6c358b52fab704ca441ca08a2524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Edl0d6QPWYyImrEKrXBaZA.png"/></div></div></figure><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="5fe1" class="lt kk hi lp b fi lu lv l lw lx">firedataService_df.groupBy(‘CallType’).count().orderBy(‘count’,ascending = False).show(100,False)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mm"><img src="../Images/e73b42e622f0a11684e0a3e808ebac31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fjOP23TbVWl8Fmnkjwqxuw.png"/></div></div></figure><h1 id="d525" class="kj kk hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">从拼花格式读取文件:非常高效</h1><p id="b5c0" class="pw-post-body-paragraph im in hi io b ip lh ir is it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj hb bi translated">拼花地板是一种柱状格式，许多其他数据处理系统都支持这种格式。Spark SQL支持读取和写入自动保留原始数据模式的Parquet文件。写入拼花文件时，出于兼容性原因，所有列都自动转换为可空。[参考Spark文档]</p><p id="ae0f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一旦我们在Spark上完成了数据的ETL(从csv中提取、转换、将字符串转换成时间戳),我们就可以将文件保存为parquet格式，供以后分析使用。与CSV或JSON相比，它的效率非常高。它根据我们的分区将文件存储在8个不同的gunzip文件中。</p><pre class="ju jv jw jx fd lo lp lq lr aw ls bi"><span id="20f9" class="lt kk hi lp b fi lu lv l lw lx">firedataService_df.write.format(‘parquet’).save(‘/tmp/firedataService_parquet/files/’)</span><span id="f046" class="lt kk hi lp b fi ly lv l lw lx">#I am reading the parquet files from disk:<br/>newdataDF = spark.read.parquet(‘/tmp/firedataService_parquet/files/’)</span><span id="ce37" class="lt kk hi lp b fi ly lv l lw lx">#spark job took <strong class="lp hj">0.52s </strong>to read from disk</span><span id="838c" class="lt kk hi lp b fi ly lv l lw lx">newdataDF.count()<br/># time: <strong class="lp hj">0.57s</strong></span></pre></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="96cc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">本教程描述了使用<strong class="io hj"> dataframe API </strong>和<strong class="io hj"> Spark SQL </strong>在Spark中操作数据的简单方法。我尝试演示了数据帧上的连接、转换和操作，对查询、以parquet格式读写文件的性能改进的高级概述。Spark能够使用多节点集群处理数TB的数据。大部分信息可在Apache Spark网站的<a class="ae lm" href="https://spark.apache.org/docs/latest/index.html" rel="noopener ugc nofollow" target="_blank">文档</a>部分获得。</p><p id="8b53" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我使用了databricks笔记本和EC2实例来执行我的分析。你可以使用Jupiter笔记本，apache zeppelin，Azure HDinsight捆绑hadoop，spark，zeppelin或使用AWS EMR等。</p><p id="da5a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">谢谢大家！！</p></div></div>    
</body>
</html>