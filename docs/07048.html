<html>
<head>
<title>Linear Regression Types and Implementation!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的类型和实现！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-types-and-implementation-1797dbc76c08?source=collection_archive---------25-----------------------#2020-06-11">https://medium.com/analytics-vidhya/linear-regression-types-and-implementation-1797dbc76c08?source=collection_archive---------25-----------------------#2020-06-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5efd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将了解线性回归的类型以及如何使用<strong class="ih hj"> <em class="jd"> scikitlearn </em> </strong>库来实现它。我们还将了解被称为<strong class="ih hj">多项式回归的特殊类型的线性回归。</strong></p><p id="8f9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归是任何人在开始学习机器学习时都会实施的第一个模型。线性回归是一个非常有用的模型。它有很多好的特性，通过实现它可以实现很多事情。</p><h2 id="3219" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">什么时候用线性回归？线性回归有哪些不同的类型？</h2><p id="7d1b" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">线性回归是一种监督模型，用于回归(预测连续值)。它不能用于分类问题。线性回归是一个简单的模型，很容易理解和实现。线性回归是一种非常简单的方法，但已被证明在许多情况下非常有用。</p><p id="20ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不同类型的线性回归:</p><ul class=""><li id="9307" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kj kk kl km bi translated"><strong class="ih hj">简单线性回归</strong></li><li id="ac1e" class="ke kf hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated"><strong class="ih hj">多元线性回归</strong></li><li id="ac8d" class="ke kf hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated"><strong class="ih hj">多项式回归</strong></li></ul><p id="b3f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">简单线性回归:</strong></p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ks"><img src="../Images/1badfc8ecbd97f0bc1d75200fbbb7cef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zKOVzlTbxhez4aj2S-AIDw.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated"><strong class="bd jg">简单线性回归</strong> PC:维基百科</figcaption></figure><p id="e43d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在简单线性回归中，得到两个<strong class="ih hj">连续变量之间的关系。一个变量是因变量，另一个变量是自变量。因变量依赖于自变量。因变量用“y”表示，自变量用<em class="jd">“x”表示。</em>线性回归线是一条直线。</strong></p><p id="a3d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单线性回归表示为:</p><blockquote class="li lj lk"><p id="8106" class="if ig jd ih b ii ij ik il im in io ip ll ir is it lm iv iw ix ln iz ja jb jc hb bi translated"><strong class="ih hj"> y = b0 + b1*x </strong></p></blockquote><p id="1c07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，<strong class="ih hj"> y </strong>是因变量。<strong class="ih hj"> x </strong>为自变量。<strong class="ih hj"> b1 </strong>是自变量的系数。<strong class="ih hj"> b0 </strong>是截距</p><p id="c1ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果系数为负，那么自变量和因变量之间的关系是成反比或负相关的(即如果一个增加，另一个减少，反之亦然)。</p><p id="17ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单线性回归的实现:</p><pre class="kt ku kv kw fd lo lp lq lr aw ls bi"><span id="4fe0" class="je jf hi lp b fi lt lu l lv lw"><strong class="lp hj"><em class="jd">#Importing the Libraries</em></strong><em class="jd"><br/></em>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/></span><span id="7bfa" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Importing the Data</em></strong><em class="jd"><br/></em>data = pd.read_csv('Salary_Data.csv')<br/></span><span id="d1e0" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Separating the target and data values<br/></em></strong>X = data.iloc[:, :-1].values<br/>y = data.iloc[:, -1].values<br/></span><span id="86ef" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Splitting the Dataset<br/></em></strong>from sklearn.model_selection import train_test_split<br/>X_train , X_test , y_train , y_test = train_test_split(X, y, test_size = 0.2 , random_state = 3)<br/></span><span id="551a" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Training the data<br/></em></strong>from sklearn.linear_model import LinearRegression<br/>regressor = LinearRegression()<br/>regressor.fit(X_train,y_train)<br/></span><span id="f2d2" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Predicting the values<br/></em></strong>y_pred = regressor.predict(X_test)<br/></span><span id="df5a" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Checking the efficiency of the model<br/></em></strong>from sklearn import metrics<br/>print(metrics.mean_squared_error(y_test, y_pred))<br/>print(metrics.r2_score(y_test , y_pred))<br/>print(metrics.mean_absolute_error(y_test , y_pred))<br/>print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))</span><span id="686a" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Getting the Coefficient and intercept of the Regression Line<br/></em></strong>print(regressor.coef_)<br/>print(regressor.intercept_)</span></pre><p id="5fe0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">注:根据数据集</em>中的数据类型，实施步骤可能会有所不同</p><p id="188e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多元线性回归:</strong></p><p id="ee4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在多元线性回归中，自变量不止一个。多元线性回归表示为:</p><blockquote class="li lj lk"><p id="d915" class="if ig jd ih b ii ij ik il im in io ip ll ir is it lm iv iw ix ln iz ja jb jc hb bi translated"><strong class="ih hj"> y = b0 + b1*x1 + b2*x2 + …..+ bn*xn </strong></p></blockquote><ul class=""><li id="d219" class="ke kf hi ih b ii ij im in iq kg iu kh iy ki jc kj kk kl km bi translated">y-因变量</li><li id="1db0" class="ke kf hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated">x1，x2，…，xn —独立变量</li><li id="beb3" class="ke kf hi ih b ii kn im ko iq kp iu kq iy kr jc kj kk kl km bi translated">b0，b1，…..，bn —系数</li></ul><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ly"><img src="../Images/d6645d1b7e1c86a30cbf6e8b07d01ebc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QXoXptAtopUMxpXeb-xRMw.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">多元线性回归</figcaption></figure><p id="3e83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自变量和因变量之间的关系可以是正的，也可以是负的。<em class="jd"> For Ex </em> : x1，x2，x3可以是正相关，而其他自变量可以是负相关。</p><p id="3a1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多元线性回归的实现:</p><pre class="kt ku kv kw fd lo lp lq lr aw ls bi"><span id="e80c" class="je jf hi lp b fi lt lu l lv lw"><strong class="lp hj"><em class="jd">#Importing the Libraries</em></strong><em class="jd"><br/></em>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/></span><span id="c619" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Importing the Data</em></strong><em class="jd"><br/></em>data = pd.read_csv('Startups.csv')<br/></span><span id="3812" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Separating the target and data values<br/></em></strong>X = data.iloc[:, :-1].values<br/>y = data.iloc[:, -1].values<br/></span><span id="474d" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Encoding the Categorical Data</em></strong><br/>from sklearn.compose import ColumnTransformer<br/>from sklearn.preprocessing import OneHotEncoder<br/>ct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [3])], remainder='passthrough')<br/>X = np.array(ct.fit_transform(X))<br/></span><span id="173d" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Splitting the Dataset<br/></em></strong>from sklearn.model_selection import train_test_split<br/>X_train , X_test , y_train , y_test = train_test_split(X, y, test_size = 0.2 , random_state = 3)<br/></span><span id="948d" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Training the data<br/></em></strong>from sklearn.linear_model import LinearRegression<br/>regressor = LinearRegression()<br/>regressor.fit(X_train,y_train)<br/></span><span id="b43b" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Predicting the values<br/></em></strong>y_pred = regressor.predict(X_test)<br/></span><span id="e269" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Printing the predicted and actual </em></strong>valuesprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))<br/></span><span id="2a05" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Checking the efficiency of the model</em></strong><br/>from sklearn import metrics<br/>print(metrics.mean_squared_error(y_test, y_pred))<br/>print(metrics.r2_score(y_test , y_pred))<br/>print(metrics.mean_absolute_error(y_test , y_pred))<br/>print(np.sqrt(metrics.mean_squared_error(y_test, y_pred)))</span></pre><p id="6884" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实现部分，有一个额外的编码步骤。之所以这样做，是因为数据集中有一列包含使用<strong class="ih hj"> OneHotEncoding </strong>技术编码的分类变量。否则，简单线性回归和多元线性回归的实现是一样的！</p><p id="f440" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多项式回归:</strong></p><p id="3c00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多项式回归是一种特殊类型的线性回归。因变量和自变量之间的关系不是线性的。</p><p id="6b80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它表示为:</p><blockquote class="li lj lk"><p id="822a" class="if ig jd ih b ii ij ik il im in io ip ll ir is it lm iv iw ix ln iz ja jb jc hb bi translated"><strong class="ih hj">y = B0+B1 *<em class="hi">x1</em>+B2 * x1+⋯+b<em class="hi">n * xn</em>ⁿ</strong></p></blockquote><p id="2bb3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多项式回归的应用包括估计疫情或流行病期间的感染/死亡率，预测板球比赛的比分等..它也广泛用于化学领域。</p><h2 id="295d" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">为什么多项式回归被称为多项式‘线性’回归？</h2><p id="57a2" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">多项式回归被称为多项式线性回归，因为自变量的系数仍然是“线性的”。只有自变量不是线性的。通过建立一个模型，我们试图找到一个系数项的值，如b1，b2，…，bn，但我们并不试图找到自变量本身的值。这就是为什么多项式回归仍然被称为多项式线性回归。</p><p id="cfba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多项式回归的实现:</p><pre class="kt ku kv kw fd lo lp lq lr aw ls bi"><span id="94b3" class="je jf hi lp b fi lt lu l lv lw"><strong class="lp hj"><em class="jd">#Importing the Libraries<br/></em></strong>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/></span><span id="82e4" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Importing the Data<br/></em></strong>data = pd.read_csv('Position_Salaries.csv')<br/></span><span id="5c33" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Separating the target and data values<br/></em></strong>X = data.iloc[:, 1:-1].values<br/>y = data.iloc[:, -1].values<br/></span><span id="83e9" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Training the data</em></strong><br/>from sklearn.preprocessing import PolynomialFeatures<br/>from sklearn.model_selection import LinearRegression<br/>poly_reg = PolynomialFeatures(degree = 2)<br/>X_poly = poly_reg.fit_transform(X)<br/>lin_reg_2 = LinearRegression()<br/>lin_reg_2.fit(X_poly, y)<br/></span><span id="22fb" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Visualizing the Polynomial Regression Plot<br/></em></strong>plt.scatter(X, y, color = 'red')<br/>plt.plot(X, lin_reg_2.predict(poly_reg.fit_transform(X)), color = 'blue')<br/>plt.show()<br/></span><span id="4367" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd"># Predicting a new result with Polynomial Regression<br/></em></strong>lin_reg_2.predict(poly_reg.fit_transform([[6.5]]))<br/></span><span id="2bdd" class="je jf hi lp b fi lx lu l lv lw"><strong class="lp hj"><em class="jd">#Getting the Coefficient and intercept of the Regression Line<br/></em></strong>print(lin_reg_2.coef_)<br/>print(lin_reg_2.intercept_)</span></pre><p id="7055" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:将阶数更改为较高的数字可能会导致模型过度拟合。</p><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lz"><img src="../Images/78372afa752d4f0dc7ac10ca95964337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*21bfKv63yQgqxsrOqR0j7A.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">使用多项式回归的过度拟合示例</figcaption></figure><p id="a6fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">万岁…结束了！..在本文中，我们已经了解了不同类型的线性回归以及如何使用sklearn实现它！</p><blockquote class="li lj lk"><p id="574c" class="if ig jd ih b ii ij ik il im in io ip ll ir is it lm iv iw ix ln iz ja jb jc hb bi translated">感谢您阅读这篇文章。如果你喜欢这篇文章，请留下一些掌声以示感谢。关注我更多类似的文章<em class="hi">。</em>如果您对本文有任何疑问/疑问或反馈，欢迎在评论区联系我。祝您愉快:)！</p></blockquote></div></div>    
</body>
</html>