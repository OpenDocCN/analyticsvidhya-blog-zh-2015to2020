<html>
<head>
<title>Neural Networks for Word Embeddings: Introduction to Natural Language Processing Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入的神经网络:自然语言处理导论第3部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-networks-for-word-embeddings-4b49e0e9c955?source=collection_archive---------1-----------------------#2018-10-14">https://medium.com/analytics-vidhya/neural-networks-for-word-embeddings-4b49e0e9c955?source=collection_archive---------1-----------------------#2018-10-14</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><div class=""/><figure class="ev ex im in io ip er es paragraph-image"><div role="button" tabindex="0" class="iq ir di is bf it"><div class="er es il"><img src="../Images/e11609bba41a093aaabaca69eef7bd54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0WR-pvHqQCTZT257"/></div></div></figure><p id="e333" class="pw-post-body-paragraph iw ix ho iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hh bi translated">这是我的NLP介绍系列的第三部分。</p><p id="9df9" class="pw-post-body-paragraph iw ix ho iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hh bi translated">在<a class="ae jv" rel="noopener" href="/analytics-vidhya/introduction-to-natural-language-processing-part-1-777f972cc7b3">第一部分</a>中，我们谈到了单词袋模型，这是一种简单的语言表示，它根据一个术语在文档中出现的次数来创建向量。在<a class="ae jv" href="https://towardsdatascience.com/spam-or-ham-introduction-to-natural-language-processing-part-2-a0093185aebd" rel="noopener" target="_blank">第二部分</a>中，我们使用Tf-idf矢量器对文本消息进行分类，它包含了…</p></div></div>    
</body>
</html>