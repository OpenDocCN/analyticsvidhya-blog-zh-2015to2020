<html>
<head>
<title>Use Transfer learning in BERT model to predict correct descriptive answer for open-ended questions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BERT模型中的迁移学习来预测开放式问题的正确描述性答案</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/use-transfer-learning-in-bert-model-to-predict-correct-descriptive-answer-for-open-ended-questions-e2c4d94e6098?source=collection_archive---------7-----------------------#2019-11-01">https://medium.com/analytics-vidhya/use-transfer-learning-in-bert-model-to-predict-correct-descriptive-answer-for-open-ended-questions-e2c4d94e6098?source=collection_archive---------7-----------------------#2019-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es et"><img src="../Images/980f57fbfb50d2629302f7591e3b3127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xYMMOXcU3KHXJO7Bx2Bz_w.jpeg"/></div></div></figure><p id="9dfd" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj">开放式问题</strong>是一个不能用“是”或“否”或者静态回答来回答的问题。开放式问题是一种需要回答的陈述。可以将回答与提问者已知的信息进行比较。</p><p id="517b" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">开放式问题的示例:</p><ul class=""><li id="adfc" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">说说你和你主管的关系。</li><li id="77fa" class="jn jo hi ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">你如何看待你的未来？</li><li id="e299" class="jn jo hi ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">告诉我这张照片中的孩子。</li><li id="4e4f" class="jn jo hi ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">政府的目的是什么？</li><li id="003a" class="jn jo hi ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">你为什么选择那个答案？</li></ul><h1 id="e0df" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">问题</h1><p id="ee62" class="pw-post-body-paragraph ip iq hi ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm hb bi translated">这个问题是一个简单的问答问题，我们必须预测一个答案对于一个给定的问题是否正确，但是这里的缓存是答案是描述性的。对于给定的问题，有10个答案，其中9个是不正确的，1个是正确的。问答集的一个示例是:</p><blockquote class="le lf lg"><p id="0a5a" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj"> 1)空气污染暴露的症状？</strong></p><p id="e47b" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj">答案1: </strong>长期暴露在污染的空气中会产生永久性的健康影响比如:1肺部加速老化。2肺活量丧失，肺功能下降。3疾病的发展，如哮喘、支气管炎、肺气肿，可能还有癌症。4寿命缩短。</p><p id="97f2" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj"> 2)空气污染暴露的症状？</strong></p><p id="d2dc" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj">答案2: </strong>长期暴露在颗粒物污染中会导致重大健康问题包括:1呼吸道症状加重，如气道刺激、咳嗽或呼吸困难。2肺功能下降。3哮喘加重。4 .儿童慢性呼吸道疾病的发展。</p><p id="9282" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj"> 3)空气污染暴露的症状？</strong></p><p id="2f04" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj">答案3: </strong>空气污染对健康的短期影响。即使在空气污染水平较低的日子，非常敏感的人也可能会受到健康影响。使用每日空气质量指数了解空气污染水平，并了解推荐的行动和健康建议。这里的建议适用于任何有症状的人。短期影响。空气污染对健康有一系列的影响。然而，英国的日常空气污染预计不会上升到人们需要对其习惯进行重大改变以避免暴露的水平；没有人需要害怕去户外，但他们可能会经历一些明显的症状，这取决于他们属于以下哪个人群:</p><p id="ed30" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj"> 4)空气污染暴露症状？</strong></p><p id="0850" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj">回答4: </strong>长期暴露在颗粒污染中会导致严重的健康问题，包括:呼吸道症状增加，如呼吸道刺激、咳嗽或呼吸困难、肺功能下降</p><p id="487c" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj"> 5)空气污染暴露的症状？</strong></p><p id="b0ff" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj">答案5: </strong>长期暴露在污染的空气中会产生永久性的健康影响如:肺部加速老化；肺活量下降，肺功能下降；疾病的发展，如哮喘、支气管炎、肺气肿，可能还有癌症；寿命缩短</p><p id="efaa" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj"> 6)空气污染暴露的症状？</strong></p><p id="61c9" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj">答案6: </strong>短期接触颗粒物污染可:加重肺部疾病引起哮喘发作和急性支气管炎；增加呼吸道感染的易感性；导致心脏病患者心脏病发作和心律不齐；即使你是健康的，你可能会经历暂时的症状，如:眼睛，鼻子和喉咙的刺激；咳嗽</p><p id="8356" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj"> 7)空气污染暴露的症状？</strong></p><p id="0a67" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj">答案7: </strong>患有哮喘的儿童可能会注意到，在空气污染水平高于平均水平的日子里，他们需要增加使用缓解药物。患有心脏或肺部疾病的成人和儿童出现症状的风险更大。遵循医生关于锻炼和控制病情的常规建议。即使在空气污染水平较低的日子，非常敏感的人也可能会受到健康影响。任何有症状的人都应该遵循提供的指导。</p><p id="16ce" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj"> 8)空气污染暴露症状？</strong></p><p id="6c09" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj">答案8: </strong>长期暴露在污染的空气中会对健康产生永久性影响，如:肺部加速老化。肺活量下降，肺功能下降。哮喘、支气管炎、肺气肿等疾病的发展，可能还有癌症。</p><p id="bd44" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj"> 9)空气污染暴露的症状？</strong></p><p id="d834" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj">回答9: </strong>您的实际不良反应风险取决于您目前的健康状况、污染物类型和浓度以及您暴露在污染空气中的时间长短。高空气污染水平会导致直接的健康问题，包括:1加重心血管和呼吸系统疾病。2增加了心脏和肺部的压力，它们必须更加努力地向身体供氧。3呼吸系统细胞受损。</p><p id="7262" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj"> 10)空气污染暴露的症状？</strong></p><p id="a342" class="ip iq lh ir b is it iu iv iw ix iy iz li jb jc jd lj jf jg jh lk jj jk jl jm hb bi translated"><strong class="ir hj">回答10: </strong>暴露在这种颗粒中会影响你的肺部和心脏。长期暴露在颗粒污染中会导致严重的健康问题，包括:呼吸道症状增加，如呼吸道刺激、咳嗽或呼吸困难；肺功能下降；哮喘加重</p></blockquote><p id="a130" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">在所有这些答案中，第9个答案是正确的。对于所有正确的问题，预测的标签必须是1，对于所有错误的答案，预测的标签必须是1。因此这是一个<strong class="ir hj">二元分类</strong>问题。</p><h2 id="a81e" class="ll kc hi bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">挑战</h2><p id="f5af" class="pw-post-body-paragraph ip iq hi ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm hb bi translated">但是解决这些问题的一些挑战是:</p><ul class=""><li id="5a13" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">高度不平衡的数据。一个正确答案有9个错误答案。不平衡比例是9:1。</li></ul><figure class="ma mb mc md fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es lz"><img src="../Images/fb1aa44b44aaceed27e59f261c72e82e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XZ02sTW0jwYnysLdYjZtMA.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">显示数据集中正确答案类和错误答案类分布的图</figcaption></figure><ul class=""><li id="1398" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">没有功能工程的空间。由于每个问题都是独特的，所以没有任何功能工程可以完成</li></ul><p id="0fcf" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">在解决问题时，需要克服这些挑战。</p><h1 id="b702" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">方法</h1><h2 id="b6bb" class="ll kc hi bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">1)数据清理</h2><p id="fba0" class="pw-post-body-paragraph ip iq hi ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm hb bi translated">简单的数据处理技术已经完成，例如</p><ul class=""><li id="c503" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">HTML的删除</li></ul><p id="6621" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">移除一些html标签和实体，如<p class="translated">、T0、T1</p></p><pre class="ma mb mc md fd mi mj mk ml aw mm bi"><span id="9e40" class="ll kc hi mj b fi mn mo l mp mq">import re<br/>#function to clean the word of any html-tags and make it lower case<br/>def cleanhtml(sentence): <br/>    cleanr = re.compile('&lt;.*?&gt;')<br/>    cleanentity = re.compile('&amp;.*;')<br/>    cleantext = re.sub(cleanr, ' ', sentence)<br/>    cleantext = re.sub(cleanentity, ' ', cleantext)<br/>    return cleantext.lower()</span></pre><ul class=""><li id="f0f1" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">移除URL</li></ul><p id="22f5" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">由于URL也有特殊的结构，常规的数据处理会使其失去意义，而且URL没有任何重要的意义，因此删除它会使数据变得有效</p><pre class="ma mb mc md fd mi mj mk ml aw mm bi"><span id="ef9d" class="ll kc hi mj b fi mn mo l mp mq">url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+';</span><span id="e03f" class="ll kc hi mj b fi mr mo l mp mq">for i in range(preprocessed_questions.shape[0]):<br/>    preprocessed_questions[i] = re.sub(url_regex, '', preprocessed_questions[i]);</span></pre><ul class=""><li id="4567" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">删除标点符号等</li></ul><p id="4318" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">标点符号如；%&amp;应该删除，因为它们没有重要意义。还把不是、还没有等扩展为不是、还没有等</p><pre class="ma mb mc md fd mi mj mk ml aw mm bi"><span id="cd19" class="ll kc hi mj b fi mn mo l mp mq">#function to clean the word of any punctuation or special characters<br/>def cleanpunc(sentence): <br/>    cleaned = re.sub(r'[?|!|"|#|:|=|+|_|{|}|[|]|-|$|%|^|&amp;|]',r'',sentence)<br/>    cleaned = re.sub(r'[.|,|)|(|\|/|-|~|`|&gt;|&lt;|*|$|@|;|→]',r'',cleaned)<br/>    return  cleaned</span></pre><p id="eed9" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">这里，由于某些原因，我们特意省略了停用字词的删除过程，这将在后面的部分中揭示。</p><h2 id="f7f4" class="ll kc hi bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">2)使用的指标</h2><p id="40f3" class="pw-post-body-paragraph ip iq hi ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm hb bi translated">考虑到数据是高度不平衡的，度量标准也应该适应这一事实。因此，选择的指标是</p><ul class=""><li id="f1b1" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">混淆矩阵</li></ul><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/678728c7c7e39b6ba910644bed5114ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*fA8e2fMtCe1RG__lSjWWTQ.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">混淆矩阵</figcaption></figure><ul class=""><li id="3ff4" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">精确</li><li id="7994" class="jn jo hi ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">召回</li></ul><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/db15621bd046c13edf4006f9194eaa7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*9aXOI8S07MbQgkqAMG8Wyg.png"/></div></figure><ul class=""><li id="aecb" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">f1-分数</li></ul><figure class="ma mb mc md fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mu"><img src="../Images/426dbb312cfdcb3c1151eb26023eb240.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c5cm1z_gDRQnAACQNdvbBQ.png"/></div></div></figure><p id="b4ab" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">使用F1-score是因为只有当精确度和召回率高时它才会高。</p><p id="99d9" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">准确性不是这里的首选指标，因为数据中有很大的不平衡。这里还使用了混淆矩阵，我们必须知道每一类中正确和错误预测的数据点数</p><ul class=""><li id="cf2d" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">平均倒数等级</li></ul><p id="75c4" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">平均倒数排名计算如下:<br/> 1)每个问题有10个可能的答案，其中任何一个都是正确的。因此，我们认为这十个答案都是正确的。我们对这些概率进行排序，并给每个答案分配等级。概率最高的答案排名第一，排名第二的答案排名第二，依此类推。我们取其中正确答案的排名，并取排名的倒数。像这样，我们把所有正确答案的倒数加起来</p><p id="6783" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">这个想法是，我们希望所有的正确答案得到一个更高的排名，理想情况下是排名1。<br/>理想的MRR值是1，非常差的MRR将是0。</p><pre class="ma mb mc md fd mi mj mk ml aw mm bi"><span id="ed99" class="ll kc hi mj b fi mn mo l mp mq">#Calulates the rank of the correct answer for each question based on the sorted order of its probability</span><span id="4bfa" class="ll kc hi mj b fi mr mo l mp mq">#Takes the inverse of that rank and adds it together</span><span id="c352" class="ll kc hi mj b fi mr mo l mp mq">def mrr(y_pred, y_true):<br/>    count = 0<br/>    mean_reciprocal_rank = 0<br/>    while y_pred.shape[0]:<br/>        y_pred_chunk = y_pred[0:10]<br/>        y_pred = y_pred[10:]<br/>        y_true_chunk = y_true[0:10]<br/>        y_true = y_true[10:]<br/>        y_true_array = np.argmax(y_true_chunk, axis=1)<br/>        try:<br/>            index_of_correct_answer = y_true_array.tolist().index(1)<br/>        except Exception as e:<br/>            continue<br/>        prob_of_correct_answer = y_pred_chunk[index_of_correct_answer]<br/>        sorted_prob_pred = np.array(sorted(y_pred_chunk, key=lambda x:(-x[1])))  <br/>        sorted_prob_pred = np.array(sorted(sorted_prob_pred, key=lambda x:(x[0])))<br/>        for rank in range(10):<br/>            if (sorted_prob_pred[rank][0] == prob_of_correct_answer[0] and sorted_prob_pred[rank][1] == prob_of_correct_answer[1]):<br/>                break<br/>        rank = rank+1<br/>        mean_reciprocal_rank += (1/rank)<br/>        count = count+1<br/>    return mean_reciprocal_rank/count</span></pre><h2 id="ac51" class="ll kc hi bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated"><strong class="ak"> 3)使用的技术</strong></h2><p id="3502" class="pw-post-body-paragraph ip iq hi ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm hb bi translated">用来解决这个问题的机器学习技术就是深度学习。</p><h2 id="63d0" class="ll kc hi bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated"><strong class="ak">为什么要用深度学习？</strong></h2><p id="3739" class="pw-post-body-paragraph ip iq hi ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm hb bi translated">选择深度学习来解决这个问题的主要原因是:</p><ul class=""><li id="c2be" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm js jt ju jv bi translated">这个问题不需要可解释性和延迟作为约束。如果这些约束<strong class="ir hj"> </strong>存在，使用深度学习技术将不是正确的选择，因为深度学习模型需要将输入传递到其所有层，这需要大量时间来进行标签预测，并且深度学习中没有可解释性的概念，因为它们是复杂的模型</li><li id="c989" class="jn jo hi ir b is jw iw jx ja jy je jz ji ka jm js jt ju jv bi translated">对于这个问题，特征工程似乎是不可能的。因此，从数据中自动学习特征的深度学习技术将对该问题非常有帮助。</li></ul><h2 id="2cba" class="ll kc hi bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">为什么是伯特？</h2><p id="fca0" class="pw-post-body-paragraph ip iq hi ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm hb bi translated">尝试了如下几种神经网络结构:</p><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es mv"><img src="../Images/0b56c123125ad71611b08ca595912b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*S_XqQo-1OpIQK23rayXVmQ.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">具有5个隐含层的神经网络</figcaption></figure><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es mw"><img src="../Images/0e7fbfd801c33c7469b43562e3d2b3dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*vGH0dIof7OuwzpMG6wa8qw.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">具有四个一维卷积网络神经网络</figcaption></figure><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/9efe0febdf15068ec6017f2eb4a31f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*h_FFaZGwjmQU0roY69Qupw.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">具有一维卷积层和LSTM层神经网络</figcaption></figure><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es my"><img src="../Images/0a8519aa27957870a602221a7116df88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*PyF8-NxYuC772UViHaPw9g.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">具有LSTM层和时间分布密集层的神经网络</figcaption></figure><figure class="ma mb mc md fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mz"><img src="../Images/0f4e478b9aa83a5d0ef61a0c2dc927bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Km-iZpAG-9KSaD5gKDJ3VA.png"/></div></div></figure><p id="9386" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">前四个模型使用两种类型的数据嵌入进行训练:</p><ol class=""><li id="eaec" class="jn jo hi ir b is it iw ix ja jp je jq ji jr jm na jt ju jv bi translated">手套向量</li><li id="b3ed" class="jn jo hi ir b is jw iw jx ja jy je jz ji ka jm na jt ju jv bi translated">TF-IDF加权手套向量</li></ol><p id="91aa" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">第五层是数据嵌入层，它将数据嵌入作为学习的一部分。但是没有一个管用。即使超参数调整和增加数据大小也没有增加。类别1的预测结果仍然很差，并且所有训练的模型都是哑模型，因为问题很难通过这些简单的体系结构来解决。</p><figure class="ma mb mc md fd ii er es paragraph-image"><div class="er es nb"><img src="../Images/458fba35056db07de28a5d3064d5712e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*6wLRnDdqSf3resK00NJnxw.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">获得混淆矩阵</figcaption></figure><p id="792e" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">由于存在巨大的类别不平衡，通过分配类别权重来训练神经网络以平衡不平衡。</p><p id="3bb1" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">类1的预测结果稍好，但这是以类0的预测为代价的。我们可以观察到像发生在0班和1班之间的拔河比赛。虽然1类的预测很好，但这确实影响了0类的预测</p><p id="249c" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">每一个神经网络都与张量板相连，以深入研究问题。在研究梯度的张量板直方图时，我们可以观察到大多数、几乎所有的梯度都具有正确的值，即它们既不太小也不太大。因此，我们可以得出结论，网络没有遇到消失或爆炸梯度问题。可能网络不太强大，无法学习如此复杂的问题<br/> <br/>此外，在检查每个网络的MRR时，我们可以观察到，它给出了非常低的MRR(0.33)，这意味着许多错误答案数据集被错误地预测为正确答案标签，这是因为大多数错误答案被预测为正确，而一些正确答案的排名非常低(意味着与其他错误答案相比，预测的概率非常低<br/></p><p id="b529" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">我们可以继续尝试不同的架构来解决这个问题，但是计算量会很大。训练和实验这些问题需要大量的计算能力。</p><p id="5758" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">这就是迁移学习扭转局面的地方。不要尝试多种架构，因为这是计算密集型的，最好使用迁移学习和微调预训练的NLP模型来解决这个问题。BERT是google构想的最先进的NLP模型，因此BERT被选择用于迁移学习</p><h2 id="d5b0" class="ll kc hi bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">4)伯特</h2><p id="3a98" class="pw-post-body-paragraph ip iq hi ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm hb bi translated">BERT(来自变压器的双向编码器表示)是Google最近发表的论文，目前是所有NLP模型的最新技术。BERT的主要创新是它的注意力技术，它屏蔽了输入中的几个单词，并试图从上下文信息(即它周围的单词)中预测它。与以前从左到右处理数据的NLP模型不同，它从左到右和从右到左处理输入。</p><p id="32e9" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">Pytorch提供了一个预训练的BERT模型，该模型针对我们的数据进行了微调。该模型在google colab中使用其GPU进行训练。</p><p id="8685" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">要用google colab GPU训练任何深度学习模型，在google colab中打开你的jupyter笔记本，点击<strong class="ir hj">编辑→笔记本设置</strong>，然后在随后出现的对话框中，选择GPU作为硬件加速器，然后点击<strong class="ir hj">保存</strong>。</p><pre class="ma mb mc md fd mi mj mk ml aw mm bi"><span id="92b0" class="ll kc hi mj b fi mn mo l mp mq">model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)<br/>model.cuda()</span></pre><p id="d90b" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj">输入格式:</strong></p><p id="1a7e" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">输入以“问题”的形式输入。”对伯特的回答。在训练bert模型时，这种技术比简单的问题和答案的串联更有效，因此被使用。</p><p id="b2fa" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj">训练技巧:</strong></p><p id="a78a" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">由于google colab在没有会话超时的情况下只能工作12个小时，并且不可能一起训练4个纪元的BERT模型，因此使用了一个简单的技巧来实现这一点。<br/>BERT py torch模型针对5L训练数据进行训练，并使用10K验证数据进行一个时期的验证。<br/>之后，将其保存为文件，然后启动新的google colab会话，并再次使用另一个时期的5L数据训练先前训练的模型。</p><p id="c0de" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">按照这种方法，我们通过存储和训练前一个时期的BERT模型来训练4个时期的BERT模型</p><pre class="ma mb mc md fd mi mj mk ml aw mm bi"><span id="5fb8" class="ll kc hi mj b fi mn mo l mp mq">param_optimizer = list(model.named_parameters())<br/>no_decay = ['bias', 'gamma', 'beta']<br/>optimizer_grouped_parameters = [<br/>    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],<br/>     'weight_decay_rate': 0.01},<br/>    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],<br/>     'weight_decay_rate': 0.0}<br/>]</span><span id="d2bd" class="ll kc hi mj b fi mr mo l mp mq">optimizer = BertAdam(optimizer_grouped_parameters,<br/>                     lr=2e-5,<br/>                     warmup=.1)</span></pre><p id="b7f5" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">训练BERT模型</p><pre class="ma mb mc md fd mi mj mk ml aw mm bi"><span id="c44a" class="ll kc hi mj b fi mn mo l mp mq"># Store our loss and accuracy for plotting<br/>train_loss_set = []</span><span id="f929" class="ll kc hi mj b fi mr mo l mp mq"># Number of training epochs (authors recommend between 2 and 4)<br/>epochs = 4</span><span id="8f66" class="ll kc hi mj b fi mr mo l mp mq"># trange is a tqdm wrapper around the normal python range<br/>for _ in trange(epochs, desc="Epoch"):<br/>  <br/>  <br/>  # Training<br/>  <br/>  # Set our model to training mode (as opposed to evaluation mode)<br/>  model.train()<br/>  <br/>  # Tracking variables<br/>  tr_loss, tr_accuracy, tr_f1_score = 0, 0, 0<br/>  nb_tr_examples, nb_tr_steps = 0, 0</span><span id="81e2" class="ll kc hi mj b fi mr mo l mp mq"># Train the data for one epoch<br/>  for step, batch in enumerate(train_dataloader):<br/>    if nb_tr_steps%1000 == 0:<br/>      print("Step", nb_tr_steps)<br/>    # Add batch to GPU<br/>    batch = tuple(t.to(device) for t in batch)<br/>    # Unpack the inputs from our dataloader<br/>    b_input_ids, b_input_mask, b_labels = batch<br/>    # Clear out the gradients (by default they accumulate)<br/>    optimizer.zero_grad()<br/>    # Forward pass<br/>    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)<br/>    train_loss_set.append(loss.item())    <br/>    # Backward pass<br/>    loss.backward()<br/>    # Update parameters and take a step using the computed gradient<br/>    optimizer.step()<br/>    <br/>    with torch.no_grad():<br/>      # Forward pass, calculate logit predictions<br/>      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)<br/>    <br/>    # Move logits and labels to CPU<br/>    #print("logits", logits)<br/>    logits = logits.detach().cpu().numpy()<br/>    label_ids = b_labels.to('cpu').numpy()</span><span id="1aad" class="ll kc hi mj b fi mr mo l mp mq">tmp_tr_accuracy = flat_accuracy(logits, label_ids)<br/>    tmp_f1_score = f1(label_ids, logits)<br/>    <br/>    tr_accuracy += tmp_tr_accuracy<br/>    tr_f1_score += tmp_f1_score<br/>    <br/>    # Update tracking variables<br/>    tr_loss += loss.item()<br/>    nb_tr_examples += b_input_ids.size(0)<br/>    nb_tr_steps += 1</span><span id="69dd" class="ll kc hi mj b fi mr mo l mp mq">  print("Train loss: {}".format(tr_loss/nb_tr_steps))<br/>  print("Train Accuracy: {}".format(tr_accuracy/nb_tr_steps))<br/>  print("Train f1-score: {}".format(tr_f1_score/nb_tr_steps))<br/>    <br/>    <br/># Validation<br/># Put model in evaluation mode to evaluate loss on the validation set<br/>  model.eval()</span><span id="f848" class="ll kc hi mj b fi mr mo l mp mq"># Tracking variables <br/>  eval_loss, eval_accuracy, eval_f1_score = 0, 0, 0<br/>  nb_eval_steps, nb_eval_examples = 0, 0</span><span id="21e3" class="ll kc hi mj b fi mr mo l mp mq"># Evaluate data for one epoch<br/>  for batch in validation_dataloader:<br/>    # Add batch to GPU<br/>    batch = tuple(t.to(device) for t in batch)<br/>    # Unpack the inputs from our dataloader<br/>    b_input_ids, b_input_mask, b_labels = batch<br/>    # Telling the model not to compute or store gradients, saving memory and speeding up validation<br/>    with torch.no_grad():<br/>      # Forward pass, calculate logit predictions<br/>      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)<br/>    <br/>    # Move logits and labels to CPU<br/>    logits = logits.detach().cpu().numpy()<br/>    label_ids = b_labels.to('cpu').numpy()</span><span id="d2f9" class="ll kc hi mj b fi mr mo l mp mq">tmp_eval_accuracy = flat_accuracy(logits, label_ids)<br/>    tmp_f1_score = f1(label_ids, logits)<br/>    <br/>    eval_accuracy += tmp_eval_accuracy<br/>    eval_f1_score += tmp_f1_score<br/>    nb_eval_steps += 1</span><span id="19e9" class="ll kc hi mj b fi mr mo l mp mq">print("Validation Accuracy: {}".format(eval_accuracy/nb_eval_steps))<br/>  print("Validation f1-score: {}".format(eval_f1_score/nb_eval_steps))</span></pre><h2 id="5eae" class="ll kc hi bd kd lm ln lo kh lp lq lr kl ja ls lt kp je lu lv kt ji lw lx kx ly bi translated">4)结果</h2><p id="4c96" class="pw-post-body-paragraph ip iq hi ir b is kz iu iv iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm hb bi translated">结果真是不可思议。不同于我训练的典型的简单神经网络，它是一个完全愚蠢的模型，伯特工作起来非常有魅力。</p><figure class="ma mb mc md fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nc"><img src="../Images/87a4264b4dc84ba863c4b2a49a0def0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VrP3GzlICLezrjUONDyG8g.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">BERT模型的混淆矩阵、精度和召回矩阵</figcaption></figure><p id="cf6f" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">BERT模型对类别1给出了非常好的预测。类别0具有非常好的召回值0.93，而类别1给出相对较低的召回值0.644。获得的F1分数为0.9472，准确度为0.89，表明该模型表现非常好。</p><p id="d59e" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">另一个值得注意的点是，MRR非常好，这个BERT模型达到了0.0.8862530的MRR，这表明即使一些正确答案没有被正确预测，但与它们的共同错误答案数据点相比，它们的排名更高。</p><p id="c46b" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">但是，如果我们针对更大的数据集和更高数量的历元来训练BERT，结果将会得到很好的改善。由于BERT模型的内存和会话超时限制，很难为具有更大数据量的更高历元训练模型。但是随着更高的计算和存储能力，结果可能会得到改善。</p><p id="274b" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">上面的案例研究是一个很好的例子，表明当我们没有足够的基础设施和计算资源时，要解决深度学习问题，迁移学习是最有效的技术:)</p><h1 id="c8c4" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">参考资料:</h1><ol class=""><li id="6c6b" class="jn jo hi ir b is kz iw la ja nd je ne ji nf jm na jt ju jv bi translated"><a class="ae ng" href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" rel="noopener" target="_blank">https://towards data science . com/Bert-explained-state-of-art-state-language-model-for-NLP-F8 b 21 a9 b 6270</a></li><li id="7335" class="jn jo hi ir b is jw iw jx ja jy je jz ji ka jm na jt ju jv bi translated">https://mccormickml.com/2019/07/22/BERT-fine-tuning/<a class="ae ng" href="https://mccormickml.com/2019/07/22/BERT-fine-tuning/" rel="noopener ugc nofollow" target="_blank"/></li></ol><p id="5ca3" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">我的git hub全部作品链接:<a class="ae ng" href="https://github.com/gayathriabhi/BERT-model-to-predict-the-correct-descriptive-answer" rel="noopener ugc nofollow" target="_blank">https://github . com/gayathriabhi/BERT-model-to-predict-the-correct-descriptive-answer</a></p><p id="40fd" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">领英简介:<a class="ae ng" href="https://www.linkedin.com/in/gayathri-s-a90322126/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/gayathri-s-a90322126/</a></p></div></div>    
</body>
</html>