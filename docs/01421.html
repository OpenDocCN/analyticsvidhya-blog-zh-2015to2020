<html>
<head>
<title>Word2Vec(SkipGram) Explained!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec(SkipGram)讲解！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/word2vec-skipgram-explained-f7b3af90f02e?source=collection_archive---------9-----------------------#2019-10-21">https://medium.com/analytics-vidhya/word2vec-skipgram-explained-f7b3af90f02e?source=collection_archive---------9-----------------------#2019-10-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="2243" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">文字与人工智能相遇的地方</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/4e26f2add59523bbc15bb26ebf1b6a95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntQRZXNoESEnjsNR6TgpSA.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图片由<a class="ae jn" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2175285" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的Gerd Altmann 提供</figcaption></figure><h1 id="ace8" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">介绍</h1><p id="ac0a" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">在本文中，我们将探究Word2Vec中的跳格模型是什么，向量是如何创建的，以及如何使用它们。重点不在于介绍，而在于如何得出单词vectors。</p><p id="445c" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">Word2Vec是创建单词“矢量表示”的一种方式。这些通常被称为单词嵌入。这里的意图是将具有相似含义(或上下文)的单词放在一起，而将不相关的单词放在一起。</p><p id="e15e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">例如，让我们假设词汇表中的每个单词都用一个100维的向量来表示。然后，通过单词的适当嵌入，我们应该能够看到‘苹果’和‘桔子’的向量应该比‘苹果’和‘氢’的向量更接近。当我们说“更近”时，我们指的是这些矢量表示之间的“余弦”距离。</p><p id="0bcd" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">现在，让我们看看如何构建这些向量。本教程将只涵盖skip-gram模型(CBOW或Glove等主题留待以后的文章讨论)。我们将看到Skip-gram是神经网络的一个非常简单的用例(只有一个隐藏层),我们实现嵌入的方式简单得令人难以置信。</p><h1 id="b0b4" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">跳格模型</h1><p id="0bf3" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">在高层次上，给定上下文中的单词，skip-gram模型将尝试预测附近的单词(作为目标)。让我们详细阐述这一点。</p><p id="ed3c" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj">背景和目标</strong></p><p id="fab5" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">要理解这一点，我们就拿“人生如骑自行车”这句话来说吧。这个句子中的每个单词(或标记)可以被认为是一个“上下文”,有了这个上下文，我们可以预测附近的单词。显然，我们不能将一个相距5段的单词定义为“附近”的单词。因此，必须选择足够小的窗口大小来决定哪些单词同时出现。让我们以2个单词的窗口大小来理解。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lh"><img src="../Images/aeace4941b2ae26bc54ba014d610c1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9IzUS6H1VtsvWhKZmXfUQ.png"/></div></div></figure><p id="d260" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">让我们用“生活”作为上下文词来举例。因为我们将窗口大小设为2，所以单词“is”和“like”是目标。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lh"><img src="../Images/61182e90c66fdf513db1fee9f3d8ce35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZKXbR8MTf1aQBs-sfgJBHg.png"/></div></div></figure><p id="a11b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">下面是上下文及其相应的目标。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es li"><img src="../Images/a70e4d36d051d3af6192815f016c14e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*QZ-uUZPKh7T3V7d5Fv7bXQ.png"/></div></figure><p id="2b35" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">为了确保这个想法被理解，对另一个上下文单词“riding”重复相同的步骤。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lh"><img src="../Images/ff94b4892f4d4ce4013e25588c744a8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SUddtrg1kQcBsSfanvaAOw.png"/></div></div></figure><p id="c97c" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">下面是上下文及其相应的目标。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es li"><img src="../Images/4556c08dd2de9fdfd014ba673422b2f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*xDKLzPCqFHj8ps_8R691yA.png"/></div></figure><p id="91cd" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">有了这种理解，就可以为整个语料库建立上下文和目标词组合。</p><p id="fca9" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这里，神经网络用于使用上下文单词来预测目标单词。随着神经网络用足够大的语料库学习和更新隐藏层的权重，这些权重将神奇地成为我们代表上下文单词的单词向量。</p><h1 id="0608" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">网络</h1><p id="1fa7" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">现在上下文和目标是可用的，单层神经网络被用于使用上下文单词来预测目标单词。让我们更深入地了解一下神经网络的样子。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lj"><img src="../Images/040cad5110f53f9d8dd85abd013b157f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBaqjqpnBIXtXLzWjkuLQQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:http://mccormickml.com/</figcaption></figure><h2 id="8787" class="lk jp hi bd jq ll lm ln ju lo lp lq jy kp lr ls ka kt lt lu kc kx lv lw ke lx bi translated">输入层</h2><p id="ebce" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">如上所述，源和目标都是从语料库中挑选出来用于训练神经网络的单词。每个单词表示为输入的方式是使用一键编码方法。</p><p id="75d7" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">如果语料库中有10，000个唯一的单词，则输入是一个长度为10，000的数组，其中只有一个单词为“1 ”,其余的单词为零。</p><p id="5f4b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">让我们观察一下上面例子中的输入向量是什么样子的。下面是我们采用的单词的一键表示法(在现实生活中，单词在分配一键向量之前是按字母顺序排序的)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/0d74291995f4cddfa2c219062e2d5ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*WIQQt5El9ygsicAjLfnWsQ.png"/></div></figure><h2 id="d48e" class="lk jp hi bd jq ll lm ln ju lo lp lq jy kp lr ls ka kt lt lu kc kx lv lw ke lx bi translated">输出层</h2><p id="902b" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">神经网络的输出是特定单词在给定输入单词“附近”的概率。如果语料库中有10，000个单词，则输出层是长度为10，000的数组，并且每个元素是0到1之间的值，表示目标单词与上下文单词一起出现的概率。</p><h2 id="3b7a" class="lk jp hi bd jq ll lm ln ju lo lp lq jy kp lr ls ka kt lt lu kc kx lv lw ke lx bi translated">培养</h2><p id="32c0" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">给定输入单词，神经网络预测目标单词。神经网络将输入单词的一键表示作为输入，并预测每个目标单词的概率作为输出。为了训练模型并使损失最小化，我们需要将实际值与预测值进行比较并计算损失。为此，目标单词的一键表示将作为输出层的实际值。在神经网络的输出端使用Softmax层。</p><p id="75c5" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">让我们观察一下这个例句中的网络是什么样子的。在这里，“骑”被认为是中心词，而“自行车”是目标</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/9d97f6155ccaae90c595ac6ef6f92c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ZJi8nHP86lCU-kvENV8ww.png"/></div></div></figure><h2 id="fbcf" class="lk jp hi bd jq ll lm ln ju lo lp lq jy kp lr ls ka kt lt lu kc kx lv lw ke lx bi translated">隐蔽层</h2><p id="1674" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">这里是所有奇迹发生的地方！！</p><p id="7c44" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">隐藏层中神经元的数量将定义我们如何“嵌入”10，000长度的稀疏向量(以及嵌入到什么维度)。让我们把50作为隐藏层的维数。这意味着我们正在为通过网络的每个单词创建50个特征。</p><p id="01dd" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">现在让我们把注意力集中在重量上。在我们的例子中，输入层的大小为10，000，我们刚刚决定隐藏层的大小为50。因此，权重矩阵的大小为(50×10000)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ma"><img src="../Images/87082676e402ee730d26c6b63a2cc461.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*dzLZQncNnQi10lb7NdtGWQ.png"/></div></figure><p id="001a" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">首先，让我们从隐藏层的角度来看权重。对于隐藏层中的第一个神经元，有10，000个输入连接，每个连接来自输入数组的每个元素。但是，因为输入层是一个热码表示，所以只有一个输入是“1 ”,其余的输入都是零。因此，对于任何单词，10，000个权重中只有一个将被传递给激活函数。对于隐藏层的所有50个神经元都是如此。</p><p id="4058" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">现在，让我们从输入层的角度来看权重。对于任何一个字，由于一位热码表示，输入数组中只有一个元素是10，000中的1。对于该输入元素，隐藏层中的50个神经元中的每一个都有50个连接(权重)。当下一个单词作为输入出现时，另一个输入元素将具有值1，并且它将具有其自己的到隐藏层的50个连接(权重)。因此，在某种程度上，语料库中的每个单词都有自己的一组50个权重，当该单词出现在上下文中时，将使用这些权重。</p><p id="3797" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这50个权重是代表语料库中的单词的单词向量。</p><h2 id="3c9e" class="lk jp hi bd jq ll lm ln ju lo lp lq jy kp lr ls ka kt lt lu kc kx lv lw ke lx bi translated">直觉</h2><p id="1451" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">这是事情变得清晰的地方。让我们说两个词有相似的上下文。如果我们把出现在第一个单词周围的所有单词和出现在第二个单词周围的所有单词都拿来，可以说会有一些单词是相同的。从神经网络的角度来看，在这种情况下，两个不同的上下文单词给出相同的目标单词作为输出。因此，这些上下文单词的权重将被类似地更新。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mb"><img src="../Images/b64f3e561c1fe4d9a2a3780ec0e9b7f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*cdK9Q6oUj9c8PpRCFjDCRg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">词的空间表征</figcaption></figure><p id="15eb" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">由于大量目标单词对于两个上下文单词是相同的，所以它们的权重更新将彼此相似。因此，两个上下文单词的最终权重将彼此接近。有了大的语料库和足够好的隐藏层维度，这些权重将成为单词的良好空间表示。</p><p id="8f81" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">有了这些权重，我们可以执行多项任务，从寻找相似的单词、单词类比(例如国王对于男人就像女王对于女人)到更复杂的任务，如命名实体识别、词性标注等</p><p id="88a2" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">当谷歌在Word2Vec上发表论文时，它使用了300维向量表示，训练是在谷歌新闻文章上进行的。</p><h1 id="1c57" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">然后</h1><p id="7590" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">本文只讨论了使用Skip-gram模型的Word2Vec的初始实现的细节。但是从权重矩阵的大小可以看出，获得嵌入层是一个计算量很大的过程。Google如何处理这个问题将在下一篇文章中详细讨论。</p><p id="7dee" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">下一篇文章还将介绍使用Word2Vec检查两个或更多单词的相似性和相似性的python代码。</p></div></div>    
</body>
</html>