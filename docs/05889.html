<html>
<head>
<title>Using NLP and LDA to map the evolution of the Neural Information Processing Systems conference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用NLP和LDA绘制神经信息处理系统的进化会议</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/using-nlp-and-lda-to-map-the-evolution-of-the-neural-information-processing-systems-conference-9bf3d5927b4a?source=collection_archive---------36-----------------------#2020-05-04">https://medium.com/analytics-vidhya/using-nlp-and-lda-to-map-the-evolution-of-the-neural-information-processing-systems-conference-9bf3d5927b4a?source=collection_archive---------36-----------------------#2020-05-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4d46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NLP的有趣之处在于，你很少看到它与时间序列分析混合在一起。所以，在这篇文章中，我们研究了一段时间内话题的变化。我们在这里使用了本·哈默在神经信息处理系统会议上制作的数据集，在这里可以得到<a class="ae jd" href="https://www.kaggle.com/benhamner/nips-papers" rel="noopener ugc nofollow" target="_blank"/>。套用他的话来说，NIPS研讨会是世界上顶级的机器学习会议之一。它涵盖的主题从深度学习和计算机视觉到认知科学和强化学习。单独使用sci-kit，我们如何确定受试者的进展？</p><p id="e247" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集非常干净，我们只关注论文标题。这部电影始于1989年。完成于2017年，这是有完整记录的最近一年。我们在这里选择2010年和2017年来比较这些主题。<a class="ae jd" href="https://www.kaggle.com/danielroyblackdiam/kernel2805e7026d/edit" rel="noopener ugc nofollow" target="_blank">所有的代码都在我们Kaggle的内核</a>中。</p><p id="aea7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一个结论:论文爆炸！2017年为631人，而2010年为221人。其次，简单的一袋字数统计显示，2017年的一些关键词，2010年甚至都没有涵盖，比如‘深度’[‘学习’]。</p><p id="eee2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这一点上，我们必须明确指出，我们显然删除了通常的停用词，但它们的数据科学对等词，如“数据”、“大”、“学习”和“模型”，这些词要么出现得非常频繁，要么没有带来大量信息。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jf"><img src="../Images/b6e22a08f6c01359c5740ce2f81b082a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fugnYvwH8AaYTnhQLuZZtQ.png"/></div></figure><p id="8b15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这第一张图有被2017年最大量的出版物扭曲的问题。所以我们把每个科目标准化了，用它除以涵盖科目的总数。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jf"><img src="../Images/390cc91324efa148d5280f3eb72bb50a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H9MpghOYDPFYtmuxy8OMoQ.png"/></div></figure><p id="e477" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果现在非常清晰。2010年流行的东西在2017年就不那么受欢迎了，2017年接受治疗最多的受试者在2010年几乎没有接受治疗。“贝叶斯”、“分类”、“回归”等术语不再流行，而“神经”、“网络”、“深度”、“梯度”变得无处不在。</p><blockquote class="jn jo jp"><p id="c85a" class="if ig je ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><em class="hi">数据清楚地显示了机器学习领域如何从统计学转向纯算法的、基于样本的解决方案</em></p></blockquote><p id="96f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">说明这一点的另一种方式是通过在主体的语料库上运行潜在的狄利克雷分配。LDA总是提供稍微超现实主义的主题，但是却很好地展示了潜在的主题表面</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><pre class="jg jh ji jj fd ka kb kc kd aw ke bi"><span id="cd34" class="kf kg hi kb b fi kh ki l kj kk"><em class="je"># Load the LDA model from sk-learn</em><br/>from sklearn.decomposition import LatentDirichletAllocation as LDA<br/> <br/><em class="je"># Helper function</em><br/>def print_topics(model, count_vectorizer, n_top_words):<br/>    words = count_vectorizer.get_feature_names()<br/>    for topic_idx, topic <strong class="kb hj">in</strong> enumerate(model.components_):<br/>        print("<strong class="kb hj">\n</strong>Topic #<strong class="kb hj">%d</strong>:" % topic_idx)<br/>        print(" ".join([words[i]<br/>                        for i <strong class="kb hj">in</strong> topic.argsort()[:-n_top_words - 1:-1]]))<br/>        <br/><em class="je"># Tweak the two parameters below (use int values below 15)</em><br/>number_topics = 5<br/>number_words = 5<br/><br/><em class="je"># Create and fit the LDA model</em><br/>lda = LDA(n_components=number_topics)<br/>lda.fit(count_data)<br/><br/><em class="je"># Print the topics found by the LDA model</em><br/>print("Topics found via LDA:")<br/>print_topics(lda, count_vectorizer, number_words)</span></pre><p id="4db3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一结果也很好地展示了潜在的现实。机器学习现在更少概率性，更多神经、深度、网络驱动。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kl"><img src="../Images/61bc33c832a8f2304814f412e28840c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwXDx52_1v6jaslq2DVbyg.png"/></div></div></figure><p id="a4ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为一个领域，文本数据随时间变化的分析似乎是开放的。一边是时序库和工具(scikit，pandas)，另一边是纯NLP: gensim，nltk和excellent spaCy。</p><p id="bcd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">值得称赞的是，这篇作品是由@ larshulstaert在<a class="ae jd" href="https://www.datacamp.com?tap_a=5644-dce66f&amp;tap_s=880795-41d646&amp;utm_medium=affiliate&amp;utm_source=danielroy1" rel="noopener ugc nofollow" target="_blank">数据营</a>上从<em class="je">机器学习最热门的话题</em>中自由解读的——你也可以在<a class="ae jd" href="http://www.conjecto.co/" rel="noopener ugc nofollow" target="_blank">猜想</a>上找到它。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><p id="cdf9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je">原载于</em><a class="ae jd" href="https://www.linkedin.com/pulse/using-nlp-lda-map-evolution-neural-information-systems-roy-cfa/" rel="noopener ugc nofollow" target="_blank"><em class="je">https://www.linkedin.com</em></a><em class="je">。</em></p></div></div>    
</body>
</html>