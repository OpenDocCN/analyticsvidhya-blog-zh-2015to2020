<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-gradient-descent-without-the-math-bc31a4781c88?source=collection_archive---------24-----------------------#2020-05-24">https://medium.com/analytics-vidhya/understanding-gradient-descent-without-the-math-bc31a4781c88?source=collection_archive---------24-----------------------#2020-05-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><p id="c638" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">人工神经网络，第2部分—理解梯度下降(没有数学)</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es if"><img src="../Images/789c0b2e308c1f91c8ae6a8ede3631fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eEX_-2gDE-7NEkZt"/></div></div><figcaption class="ir is et er es it iu bd b be z dx translated">妮可·Y-C在<a class="ae iv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="a667" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">在本系列的<a class="ae iv" rel="noopener" href="/analytics-vidhya/artificial-neural-networks-part-1-d36fb7bce6bb">文章1 </a>中，我们回顾了人工神经网络的基础知识，相关的组件如节点、权重、偏差、激活函数等。这个帖子是关于，另一个主要话题。我将解释<strong class="hj iw">梯度下降</strong>的概念，不使用大量的数学。</p><p id="9813" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">梯度下降背后的主要动机是更新权重和偏差，以达到成本/损失函数值最小的点。这就产生了一个几乎能做出正确预测的模型。也就是说，我们试图优化损失函数。</p><p id="6462" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">首先，让我们回顾一些关键术语—</p><ul class=""><li id="0b61" class="ix iy hi hj b hk hl ho hp hs iz hw ja ia jb ie jc jd je jf bi translated"><strong class="hj iw">成本函数</strong> —也称为损失函数。这是我们想要最小化的误差。这取决于输入、权重、偏差和误差。</li><li id="db49" class="ix iy hi hj b hk jg ho jh hs ji hw jj ia jk ie jc jd je jf bi translated"><strong class="hj iw">导数</strong>—x点的斜率。它显示了基于另一个变量的函数的变化率。这表示为<strong class="hj iw"> <em class="jl"> dy/dx </em> </strong> <em class="jl">。</em>这些有助于了解如何改变x的值，以使y发生所需的变化。</li><li id="f6c3" class="ix iy hi hj b hk jg ho jh hs ji hw jj ia jk ie jc jd je jf bi translated"><strong class="hj iw">全局最小值</strong> —成本函数值几乎为零的点。总有一个全局最小值。</li></ul><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es jm"><img src="../Images/3f637c37d8ab26121e8fc669a052d046.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/0*mdIDwZdppAjozS-S.png"/></div><figcaption class="ir is et er es it iu bd b be z dx translated">最小值和最大值 —维基</figcaption></figure><ul class=""><li id="f163" class="ix iy hi hj b hk hl ho hp hs iz hw ja ia jb ie jc jd je jf bi translated"><strong class="hj iw">局部最小值</strong> —成本函数被假定为凸函数，但它很少是凸函数。会有很多点具有最小成本函数。可以有一个以上的本地最小值。</li><li id="2373" class="ix iy hi hj b hk jg ho jh hs ji hw jj ia jk ie jc jd je jf bi translated"><strong class="hj iw">正向传播</strong> —输入给出初始信息，然后通过网络的每一层向最终输出层移动。</li></ul><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es jn"><img src="../Images/4f46199f244ff847020aab3b78f181c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/0*cUazGnm_THQucqah.gif"/></div><figcaption class="ir is et er es it iu bd b be z dx translated"><a class="ae iv" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank">正向传播</a></figcaption></figure><ul class=""><li id="0d89" class="ix iy hi hj b hk hl ho hp hs iz hw ja ia jb ie jc jd je jf bi translated"><strong class="hj iw">反向传播</strong> —最后一层计算误差，误差通过各层返回，计算相对于其他变量的梯度。<em class="jl">梯度下降使用该梯度值执行学习。</em></li><li id="a481" class="ix iy hi hj b hk jg ho jh hs ji hw jj ia jk ie jc jd je jf bi translated"><strong class="hj iw">偏导数</strong> —在涉及多个输入的情况下，函数<em class="jl"> f </em>不仅仅是x的函数，还包含多个变量。它通过改变多个变量中的一个来计算<em class="jl"> f </em>的变化，同时保持其他所有变量不变。</li></ul><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es jo"><img src="../Images/ba73766c0ed40cc8f8d506805604220f.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*WIsMqmmqFm6ThfQMW2zARg.png"/></div></figure><p id="38e1" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">为了理解这个过程，请看左边的图片。让我们假设我们有一个函数f(x ),我们想最小化它。我们的过程从一个随机点(A)开始，目标是达到最低点(B)。为了达到B点，我们采取小步骤并计算导数，以了解如何改变输入来获得输出的相应变化。</p><p id="6032" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">但是运动的方向是如何决定的呢？运动的方向由x的值决定</p><ul class=""><li id="7bca" class="ix iy hi hj b hk hl ho hp hs iz hw ja ia jb ie jc jd je jf bi translated">x &lt;0, then the derivative of f(x) will be less than 0. The point will be on the negative side in the image above and we move in the right direction.</li><li id="5f20" class="ix iy hi hj b hk jg ho jh hs ji hw jj ia jk ie jc jd je jf bi translated">x&gt; 0，那么f(x)的导数将大于0。点将在上面图像的正侧，我们向左移动。</li></ul><p id="9db5" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">这是一个循序渐进的过程，朝着极小点前进的步长称为<strong class="hj iw">学习速率</strong>。你肯定可以有更高的学习率和更快的收敛速度，但是，很有可能超过最小值而完全跳过它。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es jp"><img src="../Images/aaf7d0b59babd9245ba13cc88a064656.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*tTrlz0LQIid7WpjPMJOtkQ.png"/></div><figcaption class="ir is et er es it iu bd b be z dx translated">大步长</figcaption></figure><p id="063e" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">学习率较低，可能需要很长时间才能收敛。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es jq"><img src="../Images/6c6469ec85976882e6e16f706f104900.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*P_iXTAXSV97_CMsyZ3Hn6w.png"/></div><figcaption class="ir is et er es it iu bd b be z dx translated">小步长</figcaption></figure><p id="e256" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">可以通过使用各种方法来选择学习率的值。一种方法是设置一个小的常量值，另一种方法是使用一组值，然后选择产生最佳输出的值。</p><p id="3a20" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">因此，<em class="jl">梯度下降涉及以迭代的方式进行小的移动，从而产生有助于达到最优解的最佳配置。</em></p><p id="0731" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">在我们有多个输入的情况下，这是真实问题中的情况，我们使用<strong class="hj iw"> <em class="jl">偏导数</em> </strong>。这里，函数不仅依赖于x的值，还依赖于其他相关变量。偏导数捕捉函数f(x)的值的变化，函数f(x)是包含多个变量的函数，相对于保持其他所有变量不变的那些变量之一的值的变化。</p><p id="0f6b" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">在损失函数是权重(w)、偏差(b)、误差(E)和输入(I)的函数的情况下，偏导数将通过改变其中一个来捕捉损失函数输出的变化，同时保持其他所有参数不变。例如，捕捉改变权重(w)对损失函数输出的影响。这将有助于获得导致最佳成本函数输出的权重值。</p><p id="ac59" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">然而，当数据集真的很大时，以迭代方式对整个数据集执行梯度下降会变得计算昂贵，因为采取单个步骤的时间变得很长。为了减轻这种情况，使用了<strong class="hj iw">随机梯度下降</strong>。它将梯度视为期望值，期望值可以使用实际数据的一小部分来计算。在每一步，从训练集中统一提取一个小批量数据，并计算梯度，用于更新更大数据集上的值。这在更快地向最小值收敛方面提供了优于梯度下降的好处，并且找到了有用但不一定是最好的成本函数值。</p><p id="2a68" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">让我们用流程图来回顾训练神经网络的步骤</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es jr"><img src="../Images/6350cbceef6a0562e45bf2d87541b8b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*BQqU2HVw4h1ypjca2binSQ.png"/></div></figure><p id="8e78" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">当使用像Keras和Tensorflow这样的库实现NN时，反向传播和优化的实现由库负责，您无需担心。您需要确定的是层数、选择正确的激活函数、损失函数和数据预处理。</p><p id="0d72" class="pw-post-body-paragraph hg hh hi hj b hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie hb bi translated">在下一部分，我们将使用Keras和Tensorflow来实现神经网络。</p></div></div>    
</body>
</html>