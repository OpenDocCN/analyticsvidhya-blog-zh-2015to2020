<html>
<head>
<title>Understanding the Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-gradient-descent-b2d6a2839e2b?source=collection_archive---------23-----------------------#2020-01-09">https://medium.com/analytics-vidhya/understanding-the-gradient-descent-b2d6a2839e2b?source=collection_archive---------23-----------------------#2020-01-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="9d85" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">深度学习的祈祷:第4部分</h2><div class=""/><div class=""><h2 id="bf07" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">梯度下降背后的数学简介</h2></div><p id="bd9a" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">在上一篇文章中，我们已经讨论了张量及其运算。现在，在这篇文章中，我们将处理一个称为梯度下降的优化算法。因此，如果你错过了我以前的文章，这里有我的个人资料的链接来阅读那些内容——IJAS·阿·H</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ke"><img src="../Images/d6b192b5faec50dd24c29fcb8371500d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wTXUYbdv9ryQt2ISqnAolw.jpeg"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">深度学习封面图片的祈祷</figcaption></figure><p id="f8c3" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">虽然<strong class="ji hs">这篇文章假设你已经掌握了线性回归的基本知识</strong>，但是<strong class="ji hs">T5，因为我们试图用线性回归来解释梯度下降的概念，所以让我们快速刷新一下<em class="ku">线性回归的概念。</em></strong></p><p id="dda1" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">假设我们有一些数据，我们将这些数据绘制在x，y图上，如图1所示。假设<em class="ku"> x </em>为身高<em class="ku"> y </em>为体重。在线性回归中，我们试图找到一条线(<strong class="ji hs">y =斜率* x+截距，线方程</strong>)，如果给定了<strong class="ji hs"><em class="ku"/></strong><em class="ku"/>，该线可以预测<strong class="ji hs"> <em class="ku"> y </em> </strong>的值(即，如果给定了身高，我们试图近似体重)。但真正的问题是我们可以找到许多这样的可能的线，如图2所示。<strong class="ji hs"> <em class="ku">那么我们如何才能选择最合适的线呢？</em> </strong></p><div class="kf kg kh ki fd ab cb"><figure class="kv kj kw kx ky kz la paragraph-image"><img src="../Images/4299a913e63e82131bed65b16565ab7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*3c4Dd18InXk9_37VZR1Enw.png"/></figure><figure class="kv kj lb kx ky kz la paragraph-image"><img src="../Images/2d5732ea0226c1184b293238bb89cd52.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*ty131lxuAuvQ4YexYotKtg.png"/><figcaption class="kq kr et er es ks kt bd b be z dx lc di ld le translated">图1:随机数据点</figcaption></figure></div><p id="74b3" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">通过计算近似值(使用所选线近似的值)与实际值的<em class="ku">误差</em>或<em class="ku">偏差(差异)</em>来选择最佳线。换句话说，我们计算误差平方和(近似值和真实值的平方差之和)。<strong class="ji hs">然后选择产生最小平方误差值的线作为最佳拟合。</strong></p><p id="6dad" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">现在让我们考虑真实的机器学习场景，这里我们给每个数据点作为机器学习算法的输入，结果，它预测一些输出。然后，我们通过取实际输出和预测输出之间的差值来评估每个输出数据的<em class="ku">误差</em>。我们使用术语<strong class="ji hs"> <em class="ku">损失函数</em> </strong>来计算单个训练示例的误差。现在我们考虑称为<strong class="ji hs">成本函数的<strong class="ji hs">损失函数</strong>(即误差平方和/数据点数)的平方平均值。</strong></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es lf"><img src="../Images/029c26595f6721a634dde7f022e4c835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3maeTYFUs9t-mnmzZ-gs3w.jpeg"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">图2</figcaption></figure><p id="a555" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><em class="ku">一旦我们计算了平方平均值的和(即成本函数)，那么我们的工作就是找到产生最小成本的线参数(即我们需要产生最小误差的斜率(m)和截距(b)值)。</em></p><h2 id="1e03" class="lg lh hi bd li lj lk ll lm ln lo lp lq jp lr ls lt jt lu lv lw jx lx ly lz ho bi translated">深入了解这个概念</h2><p id="af0a" class="pw-post-body-paragraph jg jh hi ji b jj ma is jl jm mb iv jo jp mc jr js jt md jv jw jx me jz ka kb hb bi translated">我们知道<strong class="ji hs"> y =斜率*x +截距</strong>(即y = mx + c)，我们也知道误差等于，</p><blockquote class="mf"><p id="1c93" class="mg mh hi bd mi mj mk ml mm mn mo kb dx translated">E = (1/n)*[(Y₁- Y₁') + (Y₂- Y₂') ……。(Yₙ- Yₙ') ]</p></blockquote><p id="ab3d" class="pw-post-body-paragraph jg jh hi ji b jj mp is jl jm mq iv jo jp mr jr js jt ms jv jw jx mt jz ka kb hb bi translated">重写我们得到的等式，</p><blockquote class="mf"><p id="f19c" class="mg mh hi bd mi mj mk ml mm mn mo kb dx translated">e =(1/n)*[(y₁-(mx₁+c))+(y₂-(mx₂+c))……。+ (Yₙ-(mxₙ+c)) ]</p></blockquote><p id="0515" class="pw-post-body-paragraph jg jh hi ji b jj mp is jl jm mq iv jo jp mr jr js jt ms jv jw jx mt jz ka kb hb bi translated">现在，该算法迭代地给<strong class="ji hs"> <em class="ku"> m </em> </strong>和<strong class="ji hs"> <em class="ku"> c </em> </strong>不同的值，并试图找出E最小的<strong class="ji hs"> <em class="ku"> m </em> </strong>和<strong class="ji hs"> <em class="ku"> c </em> </strong>的最佳拟合值。这种寻找a和b的最佳值的方法被称为“<em class="ku">最小二乘法</em></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es mu"><img src="../Images/4d3af9457e7fe62058717788244610f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fpy2yaYByM0g5Bs7f-kDLg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">图3:场景的3D可视化</figcaption></figure><p id="f978" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">通过这种方法，我们可以收敛到最小平方误差值，但我们需要为此计算大量的m和c值。即使这种计算是由计算机执行的，它也不是找到最小误差值的有效方法。称为梯度下降的优化技术的重要性来了。</p><h2 id="eeeb" class="lg lh hi bd li lj lk ll lm ln lo lp lq jp lr ls lt jt lu lv lw jx lx ly lz ho bi translated">梯度下降</h2><p id="3d1d" class="pw-post-body-paragraph jg jh hi ji b jj ma is jl jm mb iv jo jp mc jr js jt md jv jw jx me jz ka kb hb bi translated">梯度下降是一种优化算法，可以帮助我们在最小化平方误差的过程中减少大量计算。该算法在远离最小值时采取大步，在接近最小值时采取小步。这是通过利用所谓的<em class="ku">学习率</em>来实现的。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es mu"><img src="../Images/37239d2939194646fe5e3afd349a7e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*69kLFBGYeS66IxN2G1rFcA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">图4:梯度下降</figcaption></figure><blockquote class="mv mw mx"><p id="8871" class="jg jh ku ji b jj jk is jl jm jn iv jo my jq jr js mz ju jv jw na jy jz ka kb hb bi translated">注意:达到最小值或底部的步长称为<strong class="ji hs">学习率</strong>。</p></blockquote><h2 id="00ab" class="lg lh hi bd li lj lk ll lm ln lo lp lq jp lr ls lt jt lu lv lw jx lx ly lz ho bi translated">梯度下降背后的数学！</h2><p id="e712" class="pw-post-body-paragraph jg jh hi ji b jj ma is jl jm mb iv jo jp mc jr js jt md jv jw jx me jz ka kb hb bi translated">如前所述，误差平方和的等式等于:</p><blockquote class="mf"><p id="c9ca" class="mg mh hi bd mi mj mk ml mm mn mo kb dx translated">e =(1/n)*[(y₁-(mx₁+c))+(y₂-(mx₂+c))……。+ (Yₙ-(mxₙ+c)) ]</p></blockquote><p id="68ce" class="pw-post-body-paragraph jg jh hi ji b jj mp is jl jm mq iv jo jp mr jr js jt ms jv jw jx mt jz ka kb hb bi translated">我们想要得到给出最小残差平方和的<strong class="ji hs"> <em class="ku">截距</em> </strong>和<strong class="ji hs"> <em class="ku">斜率</em> </strong>的值。</p><blockquote class="mv mw mx"><p id="e7a9" class="jg jh ku ji b jj jk is jl jm jn iv jo my jq jr js mz ju jv jw na jy jz ka kb hb bi translated"><strong class="ji hs">注</strong></p><p id="4a97" class="jg jh ku ji b jj jk is jl jm jn iv jo my jq jr js mz ju jv jw na jy jz ka kb hb bi translated">微积分中的<strong class="ji hs">导数</strong>计算为图形在特定点的<strong class="ji hs">斜率</strong>。</p></blockquote><p id="bd4d" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">根据链式法则，我们对截距和斜率求导，</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nb"><img src="../Images/d81c817b9d703227b3c17a909610e554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*osTU69hjjryvnC_UhqOYtg.png"/></div></div></figure><p id="c3ee" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">现在让我们考虑关于截距的导数(<em class="ku">等式. 1 </em>)。给定我们的是<em class="ku"> x </em>值和<em class="ku"> y </em>值，所以我们直接代入方程。最初，我们假设截距为0，斜率为1，因此，利用这些值，我们可以计算该点的斜率</p><p id="19ed" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">现在，我们利用<strong class="ji hs"> <em class="ku">学习速率(学习速率是预先确定的)，</em> </strong>我们将结果(即导数w.r.t截距)与<strong class="ji hs"> <em class="ku">学习速率</em> </strong>相乘以获得<strong class="ji hs">步长。</strong></p><p id="775f" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">利用步长，我们计算新的截距</p><blockquote class="mf"><p id="8742" class="mg mh hi bd mi mj mk ml mm mn mo kb dx translated">新截距=旧截距-步长</p></blockquote><p id="d2f9" class="pw-post-body-paragraph jg jh hi ji b jj mp is jl jm mq iv jo jp mr jr js jt ms jv jw jx mt jz ka kb hb bi translated">然后，我们再次将<strong class="ji hs"> <em class="ku">新截距值</em> </strong>输入等式1(即截距的导数),并再次重新计算新截距。<strong class="ji hs">重复该过程，直到步长非常接近零</strong>。步长值的减小速率最初会更大(即，当远离最小值时，步长值会变大，当接近最小值时，步长值会变小)，但会逐渐减小。</p><p id="dbbf" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">类似地，对导数w.r.t斜率重复相同的过程(<em class="ku">等式2 </em>)。</p><p id="0ca5" class="pw-post-body-paragraph jg jh hi ji b jj jk is jl jm jn iv jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">这是梯度下降的基本概述。由于我们的主要关注点是Keras的深度学习，所以所有这些功能都存在于其中。但是如果你想知道更多关于梯度下降的知识，请访问参考资料中的链接。</p><h2 id="9aee" class="lg lh hi bd li lj lk ll lm ln lo lp lq jp lr ls lt jt lu lv lw jx lx ly lz ho bi translated">参考</h2><ol class=""><li id="51be" class="nc nd hi ji b jj ma jm mb jp ne jt nf jx ng kb nh ni nj nk bi translated"><a class="ae nl" href="https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e" rel="noopener" target="_blank">理解梯度下降背后的数学原理</a></li><li id="9a9c" class="nc nd hi ji b jj nm jm nn jp no jt np jx nq kb nh ni nj nk bi translated"><a class="ae nl" href="https://youtu.be/sDv4f4s2SB8" rel="noopener ugc nofollow" target="_blank">梯度下降，逐步下降</a></li><li id="aad3" class="nc nd hi ji b jj nm jm nn jp no jt np jx nq kb nh ni nj nk bi translated"><a class="ae nl" href="https://youtu.be/PaFPbb66DxQ" rel="noopener ugc nofollow" target="_blank">https://youtu.be/PaFPbb66DxQ</a></li></ol></div></div>    
</body>
</html>