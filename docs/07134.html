<html>
<head>
<title>How Machines ‘Learn’ — AI: Explainer and Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器如何“学习”——人工智能:解释者和例子</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-machines-learn-ai-explainer-and-examples-daa71a472716?source=collection_archive---------27-----------------------#2020-06-14">https://medium.com/analytics-vidhya/how-machines-learn-ai-explainer-and-examples-daa71a472716?source=collection_archive---------27-----------------------#2020-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="ec5b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">打开神经网络</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/27de563a96101056f2575c9b4b36e04d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Kiy9Z3GWb-DBbV9R"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:https://images7.alphacoders.com/925/thumb-1920-925903.jpg</figcaption></figure><p id="5812" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">沃利在哪？让我们扫描这些面孔:“不不不不不不不..是啊！”从你的眼睛接受视觉刺激，你可以联想到人海中的面孔是否属于你要找的人。</p><p id="6028" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">你刚刚执行了一个功能来识别一张特定的脸——<a class="ae jn" href="https://www.theguardian.com/science/2018/oct/10/how-many-faces-average-person-recognises-5000" rel="noopener ugc nofollow" target="_blank">，这是我们在</a>进化上所擅长的。你的智能手机可能具有面部识别解锁功能，该功能基于一个<strong class="jq hj">神经网络</strong>，该网络将你的面部图像映射到你的身份。<strong class="jq hj">神经网络</strong>是一种将给定输入映射到期望输出的数学函数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kk"><img src="../Images/2b1f7b3f354c082e2635bd54046019f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/0*SPtqRf99d3PTozAr"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://0a.io/chapter1/calculus-explained.html" rel="noopener ugc nofollow" target="_blank">微积分用pics和gif讲解</a></figcaption></figure><p id="e9c0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">那么神经网络是如何学会做到这一点的呢？就像试图让一只狗去捡东西一样，我们必须训练它！</p><p id="e631" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">用于比较目的；一个蹒跚学步的孩子在看到一两个例子后，可能有能力知道什么是小狗..<br/>然而，数据饥渴的<strong class="jq hj">神经网络</strong>在训练期间看到多达数万个样本后，需要花费大量时间和精力来学习特定类别的特征表示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kl"><img src="../Images/13130b463c4e2da3cbd093e3b6f5ba5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*bhFifratH9DjKqMBTeQG5A.gif"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://analyticsindiamag.com/how-to-create-your-first-artificial-neural-network-in-python/" rel="noopener ugc nofollow" target="_blank">https://analyticsindiamag . com/how-to-create-your-first-artificial-neural-network-in-python/</a></figcaption></figure><p id="ab15" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这太疯狂了。你所说的训练是什么？训练一个<strong class="jq hj">神经网络</strong>有两个主要组成部分。<br/>首先，我们有<strong class="jq hj">正向传播</strong>，在开始时，这基本上是对正确答案/输出可能是什么的猜测。在我的第一篇文章中，我也谈到了这是如何发生在计算机视觉中的。</p><p id="2506" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然后，我们将猜测的正确性(或错误)与实际答案进行比较，并在此基础上，递归地向后更新网络的部分，以便未来的猜测更加准确。这一步称为<strong class="jq hj">反向传播。</strong></p><p id="98cb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">经过多次迭代，我们的网络在猜测正确答案方面变得越来越好..有人甚至会说它在做预测！；o</p><p id="4df9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">请记住，神经网络背后最初的<a class="ae jn" href="http://aris.onl/intuitive-neural-nets/" rel="noopener ugc nofollow" target="_blank">直觉是松散地复制大脑中神经元的放电，加强特定序列神经元对给定刺激的放电。</a></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es km"><img src="../Images/090b671dd8d0c26d2f0adfc7184cec10.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/0*3l1bjopuv30APlym"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://i.pinimg.com/originals/23/b1/1b/23b11ba3f0760cd585e5692bca858ed5.gif" rel="noopener ugc nofollow" target="_blank">https://I . pinimg . com/originals/23/B1/1b/23 b 11 ba 3 f 0760 CD 585 e 5692 BCA 858 ed5 . gif</a></figcaption></figure><p id="7a98" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们介绍一下神经网络的组成部分:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kn"><img src="../Images/90c582561b5595e50885ebe5efb46352.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/0*nY0RhegndAVN8geF"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://missinglink.ai/guides/neural-network-concepts/neural-network-bias-bias-neuron-overfitting-underfitting/" rel="noopener ugc nofollow" target="_blank">https://missing link . ai/guides/neural-network-concepts/neural-network-bias-bias-neuron-over fitting-under fitting/</a></figcaption></figure><ul class=""><li id="5c92" class="ko kp hi jq b jr js ju jv jx kq kb kr kf ks kj kt ku kv kw bi translated">输入层，<strong class="jq hj"> x </strong></li><li id="8a44" class="ko kp hi jq b jr kx ju ky jx kz kb la kf lb kj kt ku kv kw bi translated">隐藏层</li><li id="18db" class="ko kp hi jq b jr kx ju ky jx kz kb la kf lb kj kt ku kv kw bi translated">输出层，<strong class="jq hj"> ŷ </strong></li><li id="b37c" class="ko kp hi jq b jr kx ju ky jx kz kb la kf lb kj kt ku kv kw bi translated">加权连接，<strong class="jq hj"> Wᵢ </strong>和偏置，<strong class="jq hj"> bᵢ </strong>各层之间</li></ul><p id="0200" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">你可以看到这些连接成一个<a class="ae jn" href="https://www.statisticshowto.com/directed-acyclic-graph/" rel="noopener ugc nofollow" target="_blank">非循环图</a>，其中每一层的神经元都有加权连接——突触<a class="ae jn" href="https://en.wikipedia.org/wiki/Synapse" rel="noopener ugc nofollow" target="_blank">到下一层。这些权重放大或抑制其输入的响应。</a></p><p id="2fea" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">此外，我们没有计算输入层，所以你在上面看到的图是一个两层的神经网络；我们有一个输出层和一个隐藏层。</p><p id="eac0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">至于单个神经元的输入，它的作用是:</p><ol class=""><li id="71b4" class="ko kp hi jq b jr js ju jv jx kq kb kr kf ks kj lc ku kv kw bi translated">将输入乘以权重</li><li id="cb17" class="ko kp hi jq b jr kx ju ky jx kz kb la kf lb kj lc ku kv kw bi translated">将这些值相加，然后加上偏差</li><li id="26a6" class="ko kp hi jq b jr kx ju ky jx kz kb la kf lb kj lc ku kv kw bi translated">通过激活函数传递整个值</li></ol><p id="1b26" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">那就是<strong class="jq hj">正向传播</strong>！我们所做的是通过我们的网络传递输入值，得到一个输出值，瞧。</p><p id="ca39" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于那些对这背后的微积分更感兴趣的人，你可以看看底部的附录。</p><p id="3560" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，我们确定模型输出ŷ与实际值y的接近程度。为此，我们使用<strong class="jq hj">损失函数</strong>。一个简单的<strong class="jq hj">损失函数</strong>是对我们的样本取误差平方和:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ld"><img src="../Images/5d02195f3213f87e4812288f057e0831.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*BsbKsFvCGPvZq3LlhqrS_g.png"/></div></figure><p id="357b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">回想一下，在训练中，我们希望优化权重和偏差，以<strong class="jq hj">最小化损失函数</strong>。为此，我们使用一种叫做<strong class="jq hj">梯度下降</strong>的流行方法将误差向后传播。对于感兴趣的人，这背后的计算将在附录中讨论。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es le"><img src="../Images/dcdc6ca572b0c4d87239fb8db7bd872c.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*VdhW7HmrFzMqQ3pR"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://media.giphy.com/media/O9rcZVmRcEGqI/giphy.gif" rel="noopener ugc nofollow" target="_blank">https://media.giphy.com/media/O9rcZVmRcEGqI/giphy.gif</a></figcaption></figure><p id="bd34" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">利用<strong class="jq hj">梯度下降</strong>，我们<strong class="jq hj">通过我们的模型<strong class="jq hj">神经网络</strong>反向传播</strong>误差，以更新权重和偏差，这递增地减少我们损失函数的误差，直到我们不能再这样做，达到局部最小值。</p><p id="b13f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">同时还有其他可以考虑的细微差别，例如学习率、过拟合、标准化等..还有更多的事情要做。我们刚刚经历了一个<strong class="jq hj">神经网络</strong>如何能够“学习”一个接受输入并将其映射到所需输出的函数！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lf"><img src="../Images/ba37a41b828b885d5549e1c98ad2da2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*jU3CtN5gCMnHgOq7"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" rel="noopener" href="/praemineo/hello-neural-network-the-beginners-guide-to-understanding-neural-networks-b9b2e9c18c83"> Hello神经网络——理解神经网络的初学者指南</a></figcaption></figure><p id="b453" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在是例子！在我们的日常生活中，我们的互动可能比我们想象的要多，而且这种互动在未来可能会增加，下面是一些例子:</p><p id="fd60" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">有针对性的广告:</strong>使用你的细节和信息作为输入参数，比如年龄、性别和位置，人们可以训练一个神经网络来确定哪些广告将与<a class="ae jn" href="https://www.thinkwithgoogle.com/marketing-resources/ai-personalized-marketing/" rel="noopener ugc nofollow" target="_blank">个性化营销</a>有最高的参与度。</p><p id="d202" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">自动化聊天机器人:</strong>在线聊天体验中更快的交互，以回答您的问题并升级用户体验。这属于自然语言处理领域，由微软提供服务。</p><p id="a2b7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">信用评级:</strong>甲骨文在他们的<a class="ae jn" href="https://blogs.oracle.com/datascience/using-the-artificial-neural-network-for-credit-risk-management" rel="noopener ugc nofollow" target="_blank">博客</a>中写道，确定个人客户的风险是可能的。</p><p id="0713" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">财务预测:</strong>愿意利用神经网络进行投资的投资者希望获得竞争优势，这样的公司之一是<a class="ae jn" href="https://www.twosigma.com/" rel="noopener ugc nofollow" target="_blank"> TwoSigma </a>。</p><p id="941e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">欺诈检测:</strong>许多处理各种交易的公司都可能不得不处理欺诈交易。为了领先一步，组织可以利用<a class="ae jn" href="https://iopscience.iop.org/article/10.1088/1757-899X/263/4/042039" rel="noopener ugc nofollow" target="_blank">神经网络来检测异常</a>。</p><p id="a2da" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">厉害！我们坚持到了最后！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es le"><img src="../Images/c92c31ceb9afdceefe6d19adbb96910b.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*v5mukx1DlpfTzYMs"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:https://media0.giphy.com/media/fxtEUQIqolzxSaxVSF/giphy.gif</figcaption></figure><p id="91ef" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在我之前的两篇文章中有更多关于计算机视觉的例子:</p><ul class=""><li id="e21a" class="ko kp hi jq b jr js ju jv jx kq kb kr kf ks kj kt ku kv kw bi translated"><a class="ae jn" rel="noopener" href="/analytics-vidhya/computer-vision-ai-explainer-and-examples-1666b089263a">计算机视觉人工智能:解释者和例子——分析Vidhya </a></li><li id="46d4" class="ko kp hi jq b jr kx ju ky jx kz kb la kf lb kj kt ku kv kw bi translated"><a class="ae jn" rel="noopener" href="/analytics-vidhya/deepfake-ai-explainer-and-examples-6d9e4bec55c0"> Deepfake AI:解释器和例子——分析Vidhya </a></li></ul><p id="acbc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">更多资源:</p><ul class=""><li id="b2b2" class="ko kp hi jq b jr js ju jv jx kq kb kr kf ks kj kt ku kv kw bi translated"><a class="ae jn" href="https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6" rel="noopener" target="_blank">用Python构建自己的神经网络！</a></li><li id="9dc4" class="ko kp hi jq b jr kx ju ky jx kz kb la kf lb kj kt ku kv kw bi translated"><a class="ae jn" href="https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;feature=youtu.be" rel="noopener ugc nofollow" target="_blank">更详细地了解反向传播</a></li></ul><p id="6fdc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">感谢阅读，希望你学到了一些东西！还有❤❤·卡尔文和维纳斯。</p><p id="8d64" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">数据科学家Steven Vuong</p><p id="1315" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">欢迎对下一篇文章的评论、反馈和建议。<br/><a class="ae jn" href="mailto:stevenvuong96@gmail.com" rel="noopener ugc nofollow" target="_blank">stevenvuong96@gmail.com</a><br/><a class="ae jn" href="https://www.linkedin.com/in/steven-vuong/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/steven-vuong/</a><br/>T17】https://github.com/StevenVuong/</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><p id="cddb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">附录:<br/>数学棉花糖的正向和反向传播背后的更多演算</strong></p><p id="b556" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，我们有<strong class="jq hj">正向传播</strong>。为了更好地解释这一点，我们可以先看看单个神经元如何接受输入并产生输出<strong class="jq hj"> : </strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ln"><img src="../Images/599fde488ba33499631e438c08710e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nILz2aAuFwMts1MX"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron.png.webp" rel="noopener ugc nofollow" target="_blank">https://python machine learning . pro/WP-content/uploads/2017/09/Single-perceptron . png . webp</a></figcaption></figure><p id="77b1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们在上面看到的是输入乘以权重并与偏差相加。然后，这些信号通过用sigma表示的<strong class="jq hj">激活函数</strong>，以产生输出。</p><p id="f6c1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一个常用的<strong class="jq hj">激活函数</strong>是sigmoid函数:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lo"><img src="../Images/9347b003e555293874099ba91a41dbe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/0*exr7jVfLY_D21zuO"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://miro.medium.com/max/970/1*Xu7B5y9gp0iL5ooBj7LtWw.png" rel="noopener">https://miro . medium . com/max/970/1 * Xu 7b 5y 9 gp 0 il 5 oobj 7 ltww . png</a></figcaption></figure><p id="678d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一个重要的特征是它在任何点都是可微的，这意味着我们可以确定沿z轴任意两点之间的斜率。</p><p id="6840" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">激活函数</strong>需要将<a class="ae jn" href="https://ai.stackexchange.com/questions/5493/what-is-the-purpose-of-an-activation-function-in-neural-networks" rel="noopener ugc nofollow" target="_blank">非线性</a>引入我们的模型。</p><p id="885c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，对于初始输入x₁，我们的第一层输出是第一层权重乘以输入，并与偏差相加，然后将其插入我们的激活函数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lp"><img src="../Images/bd777500dfd91a2b8f69223fbd841011.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*cl8hsey-M3qMRVUmmsjuvQ.png"/></div></figure><p id="8647" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">由于我们的第一层输出与第二层输入相同，我们可以说:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lq"><img src="../Images/ac875136af4ed9b932df18050c9e8bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:138/format:webp/1*5gHV6J45-m8QOI90UFmz2Q.png"/></div></figure><p id="8340" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">给我们第二层输出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lr"><img src="../Images/89085c657c3b27774b1eefa8a4b787b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*uObOZMRlEyW-7b7Ab-gYMQ.png"/></div></figure><p id="52cf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因为我们的第二层是最后一层，我们有:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ls"><img src="../Images/09804d3a7ba8c3b52c772e81653f55ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:150/format:webp/1*jE5BcjE1oTZzI2ha8elCaw.png"/></div></figure><p id="e6c0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">兵棒嘣！将这一切拼凑成我们的双层<strong class="jq hj">神经网络</strong>，我们有:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lt"><img src="../Images/bc311d5a46d34ce0671d8b439140338b.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*x_EfgzYeMckdSuptlDnHvw.png"/></div></div></figure><p id="9fbc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这给了我们模型输出ŷ，万岁！</p><p id="d271" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在我们继续之前，gif中断！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lu"><img src="../Images/596535afd3025c077d37b3ebfe3587aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*YCchrJbtY63uJZxL"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://media1.tenor.com/images/1e977363e546d3020a09062593852840/tenor.gif?itemid=5631613" rel="noopener ugc nofollow" target="_blank">https://media 1 . tenor . com/images/1e 977363 e 546d 3020 a 09062593852840/tenor . gif？itemid=5631613 </a>..五个人请赞助我</figcaption></figure><p id="b93c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">回想一下，我们使用误差平方和作为损失函数<strong class="jq hj">:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ld"><img src="../Images/5d02195f3213f87e4812288f057e0831.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*BsbKsFvCGPvZq3LlhqrS_g.png"/></div></figure><p id="27f7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于<strong class="jq hj">反向传播</strong>，我们使用<a class="ae jn" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review" rel="noopener ugc nofollow" target="_blank">链式法则</a>来计算相对于我们的权重和偏差的偏导数，以便相应地调整它们。这些允许我们在多个步骤中确定成本函数的局部最小值。为此，我们使用<strong class="jq hj">梯度下降</strong>。</p><p id="26cf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，作为确定W₂更新量的示例，我们可以计算:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/fdd7f3759e1224bb0c63edc8085b8f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*JdVgZwUPkOFCt8waEo1wyw.png"/></div></figure><p id="e721" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，我们可以更新参数W₂:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/d43ba3d9882ea5eaa93a3460b4dc2081.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*fGTBVT4FfqlLsGVpKP_FGQ.png"/></div></figure><p id="5cda" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">其中μ是我们想要调整W₂.的程度这就是所谓的<a class="ae jn" href="https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/#:~:text=Learning%20Rate%20and%20Gradient%20Descent,-Deep%20learning%20neural&amp;text=Specifically%2C%20the%20learning%20rate%20is,is%20adapted%20to%20the%20problem." rel="noopener ugc nofollow" target="_blank">学习率</a>。将同样的原理应用于b₂，然后是我们的第一层参数W₁和b₁，我们完成了一个完整的训练<a class="ae jn" href="https://deepai.org/machine-learning-glossary-and-terms/epoch" rel="noopener ugc nofollow" target="_blank">时期</a>！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lu"><img src="../Images/a74068d9db07ca13a38702920463eba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*h1NfQj_zrAxg9vjn"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://media1.tenor.com/images/e0a669626522df539e2c3cced9454700/tenor.gif?itemid=8102486" rel="noopener ugc nofollow" target="_blank">https://media 1 . tenor . com/images/e0a 669626522 df 539 e 2 C3 cced 9454700/tenor . gif？itemid=8102486 </a></figcaption></figure></div></div>    
</body>
</html>