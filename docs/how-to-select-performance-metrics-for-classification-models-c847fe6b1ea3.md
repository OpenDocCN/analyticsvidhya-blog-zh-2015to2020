# 如何为分类模型选择性能指标

> 原文：<https://medium.com/analytics-vidhya/how-to-select-performance-metrics-for-classification-models-c847fe6b1ea3?source=collection_archive---------1----------------------->

## 准确性、敏感性、特异性、精确度、F1 评分、概率阈值、AUC、ROC 曲线

![](img/c2dafd233398b9e74cf22286690f6e1b.png)

我们使用分类模型来预测给定输入数据的类别标签。为了评估这样的模型，我们可以选择任何可用的度量标准，如准确性、敏感性、特异性、精确度、F1 评分、概率阈值、AUC、ROC 曲线。**重要的是，这种选择是以** **分析推理**为后盾的。
通常，我们选择*模型*精度*来评估模型。这是一个受欢迎的选择，因为它非常容易理解和解释。**准确性与建立分类模型的总体目标非常吻合，即准确预测新观测值的类别。***

准确性可能不是每次都是最好的模型评估指标。只有当所有类在数据中具有相似的流行度时，它才能很好地传达模型的健康状况。

**假设我们预测一颗小行星是否会撞击地球？**

如果我们的模型每次都说不，它将是高度准确的，但对我们来说没有多大价值。撞击地球的小行星数量很少，但错过其中的一颗可能会代价高昂。当类的分布不平衡时，精确度不是一个好的模型评估度量。

对于本文，让我们关注二进制分类(两个输出类)。这些度量也可以扩展到多类分类问题。

# 混淆矩阵

混淆矩阵是实际类值和预测类值的非常**直观的交叉表**。它包含属于每个类别的观察计数。

构建模型→使用模型对测试数据进行分类预测→为每个模型创建混淆矩阵。使用以下比率之一来比较任意两个模型。

![](img/79a5885fd965bf6c274cd4188db84779.png)

让我们看看可以从混淆矩阵中得出的所有指标，以及何时使用它们:

1.**准确度** —正确预测与总预测的比率。
*重要时:*拥有对称数据集(FN & FP 计数接近)
*使用时:*假阴性&假阳性的代价也差不多。

**准确度= (TP+TN)/(TP+FP+FN+TN)**

2.**灵敏度/召回率** —数据中真阳性与总(实际)阳性的比率。
*重要的时候:*识别阳性是至关重要的。
*用于:*假阴性的发生是不可接受/不可容忍的。你宁愿有一些额外的假阳性(假警报)也不愿保存一些假阴性。例如，在预测金融违约或致命疾病时。

**敏感度或召回率= TP/(TP+FN)**

3.**精度** —真阳性与总预测阳性的比率。
*重要时刻:*你希望对自己的预测更加自信。
*用于:*假阳性的发生是不可接受/不可容忍的。比如垃圾邮件。你宁愿收件箱里有一些垃圾邮件，也不愿错过一些被错误发送到垃圾邮件箱的普通邮件。

**精度= TP/(TP+FP)**

4.**特异性** —数据中真阴性与总阴性的比率。
*重要时:*你想覆盖所有真底片。
*用在:*你不想发出假警报。例如，你正在进行一项药物测试，所有测试呈阳性的人将立即入狱。

**特异性= TN/(TN+FP)**

5.**F1-分数** —兼顾精度和召回率。这是精确度和召回率的调和平均值。
*重要时:*你的阶级分布不均。
*用在:*误报和漏报的代价不同。F1 分数传达了精确度和召回率之间的平衡。如果在精确度和召回率之间有一个平衡，它会更高。如果精确度或召回率这两个指标中的一个以牺牲另一个为代价得到提高，F1 的分数就不会那么高。

**F1 得分= 2*(召回率*精确度)/(召回率+精确度)**

请注意，其中每一个都是以这样一种方式定义的，它们捕获了模型性能的不同方面。当我们选择这些指标之一来改进我们的模型时，我们需要记住:

a)我们试图解决的问题

b)我们拥有的数据集(数据中每个类别的流行程度)

c)我们必须为任一类型的错误分类(假阳性和假阴性)支付的成本

# 最佳概率阈值— ROC 曲线

假设我们正在构建一个电子邮件分类模型，以检测恐怖分子之间通过电子邮件进行的可疑通信。

在这种情况下，恐怖分子电子邮件属于“是”类，非恐怖分子电子邮件属于“否”类

我们选择敏感度作为改进该模型的度量。

***为什么？***
因为模型正确识别恐怖分子的邮件是绝对必要的。对于被认为有用的模型，它的真实阳性率应该很高。在这个过程中，我们可能会有一些假阳性/假警报，但这可能是我们必须做出的妥协。

假设我们最终得到了两个非常好的模型，它们具有相同的敏感度分数。这是否意味着这两个模型具有同等的预测能力？编号

回想一下，大多数分类算法预测一个观察值属于“是”类的概率。我们需要为这些概率确定一个阈值，以便将观察结果分为两类。概率高于阈值的观察结果被分类为是。比方说，我们得到一封邮件是恐怖分子邮件的概率为 0.75。如果我们已经将我们系统的阈值设置为 0.8，那么我们会将此电子邮件归类为非恐怖电子邮件。如果我们将阈值设置为 0.7，我们会将该电子邮件归类为恐怖电子邮件。**我们的系统性能会随着阈值的变化而变化。**

可以调整该阈值，以针对特定问题调整模型的行为。一个例子是减少一种或另一种类型的误差(FP/FN)。

现在，这是我们刚刚讨论过的两个不同但相互关联的问题。具有相同灵敏度(TPR)的两个模型是不等价的。在这两种型号中，FPR 较低的型号显然是更好、更可靠的型号。我们不希望在被误归类为恐怖分子邮件的非恐怖分子邮件上浪费任何调查资源。)
我们设定的阈值，可以帮助我们提高或降低 TPR。如果我们选择一个低的阈值，更多的电子邮件将被归类为恐怖电子邮件，我们将能够捕捉更多的真阳性，但即使假阳性率也会增加。阈值的选择需要在假阳性和假阴性之间进行权衡。

在这方面，ROC 曲线是一个有用的资源。

# 受试者工作特性曲线

ROC 曲线是候选阈值在 0.0 和 1.0 之间的真阳性率/灵敏度(y 轴)对假阳性率/1 特异性(x 轴)的图。

![](img/9a4ea115a6c302764793d805c24c56b9.png)

橙色曲线上的点(灰色)是相应的阈值。

在所有可能阈值上绘制 ROC 曲线。

1.在上面的曲线中，如果你想要一个假阳性率非常低的模型，你可以选择 0.8 作为你的阈值。如果你喜欢低 FPR，但你不想要糟糕的 TPR，你可能会选择 0.5，这是曲线开始向右急转弯的点。

如果你喜欢低假阴性率/高灵敏度(例如，因为你不想错过潜在的恐怖分子)，那么你可能会决定在 0.2 和 0.1 之间的某个地方，你开始得到进一步提高灵敏度的严重递减回报。

2.请注意阈值为 0.5 和 0.4 时的图表。两个阈值的灵敏度都在 0.6 左右，但 FPR 在阈值 0.4 时更高。很明显，如果我们对敏感度= 0.6 感到满意，我们应该选择阈值= 0.5。

ROC 曲线对于选择阈值非常有用。它的形状包含了很多信息:

a)图的 x 轴上的较小值表示较低的假阳性和较高的真阴性。

b)图的 y 轴上较大的值表示较高的真阳性和较低的假阴性。

c)在低 x 值下具有高 y 值的模型是好模型。

ROC 曲线是一个非常有用的工具，还有几个原因:

a)不同模型的曲线通常可以直接比较，或者针对不同的阈值进行比较。

b)曲线下面积(AUC)可用作模型技巧的总结。

# 比较 ROC 曲线

![](img/c82491c26014ff6433a6821b24963864.png)

ROC 曲线下的面积也称为 AUC(曲线下面积)。
AUC 是我们可以用来改进模型的另一个性能指标。AUC 代表可分性的程度或度量。它告诉我们这个模型在多大程度上能够区分不同的类。AUC 越高，该模型就越能准确地预测类别“是”为“是”和“否”为“否”

AUC 忽略了阈值和患病率，并给出了模型可分性的度量。

**AUC 越大，分类器/模型越好。**

![](img/c50f60706ed0d9309bf079be240b4351.png)

模型 1 和模型 2 的 ROC 曲线具有相同的曲线下面积。但是当我们选择一个阈值时，我们想看看曲线最陡和最平坦的部分在哪里开始和停止。

尽管两种模型的 AUC 相同，但模型和阈值的选择取决于我们试图解决的问题以及我们能够为错误分类支付的成本。

# 最后

我们上面讨论的所有性能指标都来自混淆矩阵。它们是相互关联的。在我们开始构建模型时，确定最适合当前问题的指标非常重要。同时，认识到这些指标中没有一个能够单独传达一个模型的完整健康状况是非常重要的。因此，建议我们在改进机器学习模型时，使用这些指标的适当组合。

感谢阅读。期待您的回复:)