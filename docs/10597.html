<html>
<head>
<title>PyTorch for Deep Learning — LSTM for Sequence Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的PyTorch序列数据的LSTM</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pytorch-for-deep-learning-lstm-for-sequence-data-d0708fdf5717?source=collection_archive---------0-----------------------#2020-10-26">https://medium.com/analytics-vidhya/pytorch-for-deep-learning-lstm-for-sequence-data-d0708fdf5717?source=collection_archive---------0-----------------------#2020-10-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b83880faf79e7c5529c0c04d2bcdaac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JL4fZwwpWi0TLC8kFykHYw.jpeg"/></div></div></figure><h1 id="ef22" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">注意</h1><p id="e2ff" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">抱歉拼错了network，lol。所有的代码文件都可以在:<a class="ae km" href="https://github.com/ashwinhprasad/PyTorch-For-DeepLearning" rel="noopener ugc nofollow" target="_blank">https://github.com/ashwinhprasad/PyTorch-For-DeepLearning</a>获得</p><h1 id="e947" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是rnn和LSTMs？</h1><p id="fbc6" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">递归神经网络非常适合序列数据和时间序列数据。长短期记忆是一种用于深度学习领域的人工递归神经网络架构。LSTMs和rnn用于序列数据，可以更好地处理时间序列问题。</p><p id="f7b6" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">“LSTM”是“RNN”的高级版本，LSTM可以通过在普通的“RNN”上添加“门”来记住之前学到的东西。LSTM和RNN在PyTorch的工作是相似的。因此，一旦我们编写了Lstm部分，RNNs也将更容易理解。<br/>在这本笔记本中，我们将尝试用递归神经网络预测正弦波。</p><p id="062a" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">RNNs和LSTMs的理论不在本文讨论范围之内。这只适用于rnn和lstm的pytorch实现。</p><h1 id="7280" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">跳到代码:</h1><ol class=""><li id="fd7d" class="ks kt hi jq b jr js jv jw jz ku kd kv kh kw kl kx ky kz la bi translated"><strong class="jq hj">导入库</strong></li></ol><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="fd0a" class="lk ir hi lg b fi ll lm l ln lo"><em class="lp">#importing the libraries</em><br/>import numpy as np <br/>import torch<br/>import matplotlib.pyplot as plt</span></pre><p id="c199" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj"> 2。数据预处理</strong></p><p id="44f9" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">我正在创建一个正弦波，正如我已经说过的，lstm接受序列输入。因此，输入如下:<br/> <strong class="jq hj">输入:[点1，点2，点3…..，点n]预测:[点n+1]。</strong> <br/>我们需要许多这样的行来创建数据集。</p><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="8183" class="lk ir hi lg b fi ll lm l ln lo"><em class="lp">#creating the dataset</em><br/>x = np.arange(1,721,1)<br/>y = np.sin(x*np.pi/180)  + np.random.randn(720)*0.05<br/>plt.plot(y)</span></pre><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/18e45b1e1dd43f53b0e0067160860ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*04zpGUZwHA05K-Zv-E-LyQ.png"/></div></figure><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="03f8" class="lk ir hi lg b fi ll lm l ln lo"><em class="lp"># structuring the data</em> <br/>X = [] <br/>Y = [] <br/><strong class="lg hj">for</strong> i <strong class="lg hj">in</strong> range(0,710):<br/>     list1 = []<br/>     <strong class="lg hj">for</strong> j <strong class="lg hj">in</strong> range(i,i+10):<br/>         list1.append(y[j])<br/>     X.append(list1)<br/>     Y.append(y[j+1])</span></pre><p id="81b5" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">为了以这样一种方式安排数据，将它发送到一个递归神经网络，我们这样做，并以这种形式安排准备数据集。<br/> <strong class="jq hj">输入:【点1，点2，点3…..，点n]预测:[点n+1]。</strong></p><p id="0b50" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj"> 3。列车试运行</strong></p><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="3c45" class="lk ir hi lg b fi ll lm l ln lo"><em class="lp">#train test split</em><br/>X = np.array(X)<br/>Y = np.array(Y)<br/>x_train = X[:360]<br/>x_test = X[360:]<br/>y_train = Y[:360]<br/>y_test = Y[360:]</span></pre><p id="698f" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">这是不言自明的</p><p id="5e1e" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj"> 4。数据集和数据加载器</strong></p><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="ee58" class="lk ir hi lg b fi ll lm l ln lo"><em class="lp">#dataset</em><br/><strong class="lg hj">from</strong> <strong class="lg hj">torch.utils.data</strong> <strong class="lg hj">import</strong> Dataset<br/><br/><strong class="lg hj">class</strong> <strong class="lg hj">timeseries</strong>(Dataset):<br/>    <strong class="lg hj">def</strong> __init__(self,x,y):<br/>        self.x = torch.tensor(x,dtype=torch.float32)<br/>        self.y = torch.tensor(y,dtype=torch.float32)<br/>        self.len = x.shape[0]<br/><br/>    <strong class="lg hj">def</strong> __getitem__(self,idx):<br/>        <strong class="lg hj">return</strong> self.x[idx],self.y[idx]<br/>  <br/>    <strong class="lg hj">def</strong> __len__(self):<br/>        <strong class="lg hj">return</strong> self.len<br/><br/>dataset = timeseries(x_train,y_train)</span><span id="3665" class="lk ir hi lg b fi lr lm l ln lo">#dataloader<br/><strong class="lg hj">from</strong> <strong class="lg hj">torch.utils.data</strong> <strong class="lg hj">import</strong> DataLoader <br/>train_loader = DataLoader(dataset,shuffle=<strong class="lg hj">True</strong>,batch_size=256)</span></pre><p id="8d41" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">Pytorch的数据集和数据加载器类有助于简化数据访问和小批量梯度下降</p><p id="5162" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj"> 5。递归神经网络</strong></p><p id="0222" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">这类似于用pytorch创建的所有其他神经网络。但是，第一层是lstm层，它将接受作为序列的输入</p><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="8ea2" class="lk ir hi lg b fi ll lm l ln lo"><em class="lp">#neural network</em><br/><strong class="lg hj">from</strong> <strong class="lg hj">torch</strong> <strong class="lg hj">import</strong> nn<br/><br/><strong class="lg hj">class</strong> <strong class="lg hj">neural_network</strong>(nn.Module):<br/>    <strong class="lg hj">def</strong> __init__(self):<br/>        super(neural_network,self).__init__()<br/>        self.lstm = nn.LSTM(input_size=1,hidden_size=5,num_layers=1,batch_first=<strong class="lg hj">True</strong>)<br/>        self.fc1 = nn.Linear(in_features=5,out_features=1)<br/><br/>    <strong class="lg hj">def</strong> forward(self,x):<br/>        output,_status = self.lstm(x)<br/>        output = output[:,-1,:]<br/>        output = self.fc1(torch.relu(output))<br/>        <strong class="lg hj">return</strong> output<br/><br/>model = neural_network()</span></pre><p id="13e5" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">输入尺寸:</strong>输入到层的一个lstm单元的尺寸。在我们的例子中，这是一个，因为在每个时间步，我们都要输入一个标量数。</p><p id="22cd" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">隐藏大小:</strong>这是我们可以选择的，权重矩阵将根据这一点进行自我调整，这也将是每个时间步长的输出向量的维度。</p><p id="d043" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">num_layers : 如果它大于1，让我们将rnn层堆叠在彼此之上。在我们的例子中，一层就足够了。所以，我选择了1</p><p id="5d75" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">6。损失、优化器、时期</p><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="a991" class="lk ir hi lg b fi ll lm l ln lo"><em class="lp"># optimizer , loss</em><br/>criterion = torch.nn.MSELoss()<br/>optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)<br/>epochs = 1500</span></pre><p id="9169" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">7。训练循环(向前传球和向后传球)</p><p id="9a70" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">计算相对于所有参数的损耗梯度，并执行梯度下降</p><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="bdba" class="lk ir hi lg b fi ll lm l ln lo"><em class="lp">#training loop</em><br/><strong class="lg hj">for</strong> i <strong class="lg hj">in</strong> range(epochs):<br/>    <strong class="lg hj">for</strong> j,data <strong class="lg hj">in</strong> enumerate(train_loader):<br/>        y_pred = model(data[:][0].view(-1,10,1)).reshape(-1)<br/>        loss = criterion(y_pred,data[:][1])<br/>        loss.backward()<br/>        optimizer.step()<br/>    <strong class="lg hj">if</strong> i%50 == 0:<br/>        print(i,"th iteration : ",loss)</span></pre><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/017e7dae66d0f3ebd8942f6009229edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*7ZVV2E-RftQO7Ash_TaVeg.png"/></div></figure><p id="bcbe" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj"> 8。神经网络预测</strong></p><pre class="lb lc ld le fd lf lg lh li aw lj bi"><span id="211b" class="lk ir hi lg b fi ll lm l ln lo"><em class="lp">#test set actual vs predicted</em><br/>test_set = timeseries(x_test,y_test)<br/>test_pred = model(test_set[:][0].view(-1,10,1)).view(-1)<br/>plt.plot(test_pred.detach().numpy(),label='predicted')<br/>plt.plot(test_set[:][1].view(-1),label='original')<br/>plt.legend()</span></pre><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/c7ee421bf6976201e2eb66c499a4b4b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*FJdEiS2uuBVEHdJ6C5KuZw.png"/></div></figure><h1 id="bd98" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结论</h1><p id="6a3f" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">正如我们所见，LSTM非常强大，而处理序列数据是自然语言处理中非常重要的一部分。在上面的预测中，我们可以看到模型已经很好地预测了正弦波。</p><h2 id="c74f" class="lk ir hi bd is lu lv lw iw lx ly lz ja jz ma mb je kd mc md ji kh me mf jm mg bi translated">谢谢你</h2></div></div>    
</body>
</html>