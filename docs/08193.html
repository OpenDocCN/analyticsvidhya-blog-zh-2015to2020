<html>
<head>
<title>Text Generation Using Long Short Term Memory Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用长短期记忆网络的文本生成</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-generation-using-long-short-term-memory-network-2c288966f648?source=collection_archive---------24-----------------------#2020-07-20">https://medium.com/analytics-vidhya/text-generation-using-long-short-term-memory-network-2c288966f648?source=collection_archive---------24-----------------------#2020-07-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a3072ca57310660f0b7a68cfa3d02e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mH_2C4g3Cre5CPGw6311_Q.jpeg"/></div></div></figure><p id="1842" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将在文本数据上训练一个LSTM网络，它自己学习生成与训练材料形式相同的新文本。如果你在文本数据上训练你的LSTM，它会学习产生新的单词，类似于我们训练的单词。LSTM通常会从源数据中学习人类语法。当用户像聊天机器人一样输入文本时，你也可以使用类似的技术来完成句子。</p><p id="b573" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用 <em class="jo"> tensorflow 2.x </em>导入我们的依赖项— <em class="jo"/></p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="2afb" class="jy jz hi ju b fi ka kb l kc kd">import string<br/>import numpy as np<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.utils import to_categorical<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense,LSTM,Embedding<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span></pre><p id="2cdd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">读取数据</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="465a" class="jy jz hi ju b fi ka kb l kc kd">file=open('t8.shakespeare.txt','r+')<br/>data=file.read()</span></pre><h2 id="2f7c" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">文本清理</h2><p id="136c" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">获得文本数据后，清理文本数据的第一步是对你要达到的目标有一个清晰的认识，并在这种背景下回顾你的文本，看看到底什么会有帮助。</p><p id="662e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在数据中有许多标点符号和数字字符，以消除它</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="d79a" class="jy jz hi ju b fi ka kb l kc kd">data=data.split('\n') <br/>data=data[253:]<br/>data=' '.join(data)</span></pre><p id="ea24" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">cleaner函数有助于删除数据中的标点和数字，并将中的所有字符转换为小写</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="79fc" class="jy jz hi ju b fi ka kb l kc kd">def cleaner(data):<br/>    token=data.split()<br/>    table=str.maketrans('','',string.punctuation)<br/>    token=[w.translate(table) for w in token]<br/>    token=[word for word in token if word.isalpha()]<br/>    token=[word.lower() for word in token]<br/>    return token</span><span id="8c7e" class="jy jz hi ju b fi lc kb l kc kd">words=cleaner(data=data)</span></pre><h2 id="c4ab" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">创造一个单词序列</h2><p id="62c0" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">seed_length是50，这意味着前50个单词将是我的输入，下一个单词将是我的输出。它需要大量的计算能力和内存来处理所有数据。所以我只用前10万个单词来训练我的神经网络。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="b681" class="jy jz hi ju b fi ka kb l kc kd">seed_length=50+1<br/>sentence=list()<br/>for i in range(seed_length,len(words)):<br/>    sequence=words[i-seed_length:i]<br/>    line=' '.join(sequence)<br/>    sentence.append(line)<br/>    if i &gt;100000:<br/>        break</span></pre><p id="372d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">神经网络要求对输入数据进行整数编码，这样每个单词都由一个唯一的整数表示。编码后将整数转换成整数序列</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="5f0b" class="jy jz hi ju b fi ka kb l kc kd">tokenizer=Tokenizer()<br/>tokenizer.fit_on_texts(sentence)<br/>sequence=tokenizer.texts_to_sequences(sentence)<br/>sequence=np.array(sequence)</span></pre><p id="6344" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">分离自变量和目标变量</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="60c0" class="jy jz hi ju b fi ka kb l kc kd">X,y=sequence[:,:-1],sequence[:,-1]<br/>vocab_size=len(tokenizer.word_index)+1<br/>y=to_categorical(y,num_classes=vocab_size)</span></pre><h2 id="cec7" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">创建LSTM网络</h2><p id="b898" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">嵌入层被定义为网络的第一个隐藏层。它必须需要3个参数</p><ol class=""><li id="68af" class="ld le hi is b it iu ix iy jb lf jf lg jj lh jn li lj lk ll bi translated">vocab_size —文本数据中词汇的大小。</li><li id="d7b9" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">output_dim —单词将嵌入其中的向量的大小。</li><li id="ef14" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">输入长度—输入序列的长度。</li></ol><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="3330" class="jy jz hi ju b fi ka kb l kc kd">model=Sequential()<br/>model.add(Embedding(vocab_size,50,input_length=50))<br/>model.add(LSTM(100,return_sequences=True))<br/>model.add(LSTM(100))<br/>model.add(Dense(100,activation='relu'))<br/>model.add(Dense(vocab_size,activation='softmax'))<br/>model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])<br/>model.summary()</span></pre><h2 id="3ccd" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">训练我们的模型</h2><p id="fd5f" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">训练你的模型更多的时代，我们的网络将能够学习如何生成单词。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="cf41" class="jy jz hi ju b fi ka kb l kc kd">model.fit(X,y,batch_size=256,epochs=1000)</span></pre><p id="5ce6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">generate函数帮助我们生成50个单词之后的单词，作为模型的输入</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="7c99" class="jy jz hi ju b fi ka kb l kc kd">def generate(text,n_words):<br/>    text_q=[]<br/>    for _ in range(n_words):<br/>        encoded=tokenizer.texts_to_sequences(text)[0]<br/>        encoded=pad_sequences([encoded],maxlen=sequence_length,truncating='pre')<br/>        prediction=model.predict_classes(encoded)<br/>        for word , index in tokenizer.word_index.items():<br/>            if index==prediction:<br/>                predicted_word=word<br/>                break<br/>        text=text+' '+predicted_word<br/>        text_q.append(predicted_word)<br/>    return ' '.join(text_q)</span></pre><p id="75e4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用函数并生成接下来的100个单词</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="c104" class="jy jz hi ju b fi ka kb l kc kd">input = sentence[0]<br/>generate(input,100)</span></pre><p id="9b1c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">感谢阅读！我希望这篇文章是有帮助的。</p><p id="2401" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你们的评论和掌声让我有动力创作更多的材料。我很欣赏你！😊</p></div></div>    
</body>
</html>