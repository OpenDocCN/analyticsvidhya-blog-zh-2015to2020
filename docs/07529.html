<html>
<head>
<title>Getting Started With Super Resolution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超分辨率入门</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/starting-with-super-resolution-26573b9cfcb7?source=collection_archive---------31-----------------------#2020-06-28">https://medium.com/analytics-vidhya/starting-with-super-resolution-26573b9cfcb7?source=collection_archive---------31-----------------------#2020-06-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6afd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将基于PASCAL-VOC 2007数据集总结我使用<a class="ae jd" href="https://en.wikipedia.org/wiki/Super-resolution_imaging" rel="noopener ugc nofollow" target="_blank">超分辨率</a>任务的简短经验。在本次会议期间，我尝试了几种不同型号的超分辨率图像，从72*72*3到144*144*3，再到288*288*3。</p><p id="3377" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个任务首先需要的是获取数据集，我使用了<a class="ae jd" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank">ka ggle</a>T4】预装的数据集。<br/>因为我训练了不同的模型，并且有大量的数据，所以在整个工作中，我使用了数据生成器来加载图像并调整它们的大小。</p><h2 id="d92a" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">用于加载图像的数据生成器</h2><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="96a6" class="je jf hi ke b fi ki kj l kk kl">import cv2<br/>import os</span><span id="a4f0" class="je jf hi ke b fi km kj l kk kl">image_dir = '../input/pascal-voc-2007/voctrainval_06-nov-2007/VOCdevkit/VOC2007/JPEGImages'</span><span id="d1a6" class="je jf hi ke b fi km kj l kk kl">def load_images_iter(amount = 64,start_pos = 0, end_pos = 5011):<br/>  ans = []<br/>  all_im = os.listdir(image_dir)<br/>  all_im.sort()<br/>  print (len(all_im))<br/>  while (True):<br/>    for idx,img_path in enumerate(all_im[start_pos:end_pos]):<br/>      if (len(ans) !=0 and len(ans)%amount == 0):<br/>        ret = ans<br/>        ans = []<br/>        yield ret<br/>      ans.append(cv2.imread(image_dir+"/"+img_path))</span></pre><p id="61e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的排序函数不是强制性的。我使用它是因为我使用生成器不仅是为了训练和验证模型，也是为了查看模型结果。因此，我需要确保我一直在看相同的图片，所以sort函数保证了这一点。</p><p id="bf51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我为不同的用途创造了几种不同的发电机。这是一个训练集生成器的例子(数据被分割，前1000张图片用于验证，其余的用于训练)。为了调整图片的大小，我使用了CV2的调整大小功能。</p><h2 id="7631" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">列车数据发生器</h2><p id="f97a" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">生成器返回一个元组，第一个值是train，第二个值是output21的另一个元组，输出2个预期结果。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="3b9f" class="je jf hi ke b fi ki kj l kk kl">def train_generator_2(batch=64, tar1_size=144, tar2_size=288 , train_size=72):<br/>  collect_train = []<br/>  collect_target_1 = []<br/>  collect_target_2 = []<br/>  while True:<br/>      file_gen = load_images_iter(amount=batch, start_pos = <br/>                          1000)#first 1000 images are for validation<br/>      imgs = []<br/>      while (True):<br/>        try:<br/>          imgs = next(file_gen)<br/>        except:<br/>          break<br/>        for idx,img in enumerate(imgs): <br/>          if (len(collect_train)!=0 and<br/>                  len(collect_train)%batch == 0):<br/>            ans_train = np.asarray(collect_train,dtype=np.float)<br/>            ans_target_1 = <br/>              np.asarray(collect_target_1,dtype=np.float)<br/>            ans_target_2 = <br/>              np.asarray(collect_target_2,dtype=np.float)<br/>            collect_train = []<br/>            collect_target_1 = []<br/>            collect_target_2 = []<br/>            yield (ans_train, (ans_target_1,ans_target_2))<br/>          collect_train.append(cv2.resize(img,<br/>                              (train_size,train_size))/255.0)<br/>          collect_target_1.append(cv2.resize(img,<br/>                                 (tar1_size,tar1_size))/255.0)<br/>          collect_target_2.append(cv2.resize(img,<br/>                                 (tar2_size,tar2_size))/255.0)</span></pre><p id="d137" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与任何机器学习和深度学习工作一样，我从一些数据探索开始。我们有5011张不同尺寸的图片。这些照片之间没有共同的动机，也没有任何明显的相似之处。以下是一些不同尺寸的样本图片:</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ks"><img src="../Images/fafa91365cd918a6a4c5310613cd50de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HReI_EMbfhiWXbUAwtZslQ.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">不同尺寸的图像(72*72、144*144、288*288)</figcaption></figure><p id="4238" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个任务中的所有模型，我使用MSE作为损失函数，PSNR作为度量。<br/>模型的损耗由2个输出损耗平均组合而成。每个输出都有自己的<a class="ae jd" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio" rel="noopener ugc nofollow" target="_blank"> PSNR </a>度量。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="9d61" class="je jf hi ke b fi ki kj l kk kl">from keras import backend as K<br/>import math</span><span id="a971" class="je jf hi ke b fi km kj l kk kl">def PSNR(y_true, y_pred):<br/>    max_pixel = 1.0<br/>    return 10.0 * (1.0 / math.log(10)) * K.log((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true))))</span></pre><h1 id="f438" class="le jf hi bd jg lf lg lh jk li lj lk jo ll lm ln jr lo lp lq ju lr ls lt jx lu bi translated"><strong class="ak">第一款</strong></h1><p id="069e" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">因此，我要讨论的第一个模型是一个基本模型，它有两个输出，用于两个不同大小的图像(144*144*3，288*288*3)。</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lv"><img src="../Images/9ca6b915bc6089ac2bd2007c39b393ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Za5CTklcV7Z_R5Acp6WuBg.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">第一模型结构</figcaption></figure><p id="b5de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从这个结构中，我们可以看到，为了得到所需的尺寸，我们需要保持原始的输入高度和宽度尺寸。所以对于卷积层，我使用了填充。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="7b78" class="je jf hi ke b fi ki kj l kk kl">inp = Input((72,72,3))<br/>x = Conv2D(64,(3,3), activation = 'relu', padding = 'same')(inp)<br/>x = Conv2D(64,(3,3), activation = 'relu', padding = 'same')(x)<br/>x = UpSampling2D(size=(2,2))(x)<br/>x = Conv2D(3,(1,1), activation = 'relu', padding = 'same')(x)<br/><br/>model1 = Model(inputs = inp ,outputs = x)</span></pre><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lw"><img src="../Images/e877e1641e8acb0ef0acc16d0642a18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4fjBdPzvRuCD9b_PhD7nfA.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">第一模型损失</figcaption></figure><p id="4a7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型非常简单，因此从图中可以看出，收敛速度非常快。</p><p id="9e13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，乍一看，我们似乎从损失函数中得到了很好的结果，但如果我们仔细看看这些图像，就会发现它们讲述了一个不同的故事。</p><p id="995e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下图中:<br/>右边的两幅图像是尺寸缩小到144*144和288*288的图像。<br/>左边的两幅图像是具有各自尺寸的模型预测。</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lx"><img src="../Images/cf736bc1894702d6696e0531f63084e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lmzBpj-dV9RLc9Y3Voy7mA.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">缩小图像与第一个模型预测</figcaption></figure><p id="7ddf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在这里可以看到，虽然损失是好的，但我们实际上仍然得到相当糟糕的结果。</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ly"><img src="../Images/a38c8aee6573fc0f8696ba86c236e4f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*o0F19LMD-W027S0iy79CXw.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">第一模型PSNR图</figcaption></figure><p id="ebbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们看一下PSNR图，我们看到它在21附近收敛，这显然不是一个好结果。此外，我们需要记住，高PSNR并不能保证图像对人眼来说是好的，这只是一个辅助指标，帮助我们更好地了解模型的好坏。</p><h1 id="6818" class="le jf hi bd jg lf lg lh jk li lj lk jo ll lm ln jr lo lp lq ju lr ls lt jx lu bi translated">第二个模型</h1><p id="01bc" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">在这个模型中，我试图用残差块来代替卷积层。</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lz"><img src="../Images/2070a56f4e601e9fdf2dba4d1aba0a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YnXdB_RYOH3-neFpPk8p0w.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">模型结构</figcaption></figure><p id="dd30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种模型结构给我们的模型增加了更多的参数，所以它可能会帮助我们改进我们得到的结果。</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ma"><img src="../Images/8cade23dcfae668fb1ddf80410a36c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FipqoKgrX6yONpe6VLJPQ.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">第二模型损失&amp; PSNR图</figcaption></figure><p id="9c26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们可以看到，像以前一样，我们在2个时期后收敛，损失稍微好一点，PSNR也是如此。但这是否意味着我们也有更好的图像？</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mb"><img src="../Images/40def27e4a6707eda06fe9a900d53d10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OCge0GEVyEoJ2sogbWMcTQ.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">图像尺寸288*288的第一模型预测与第二模型预测</figcaption></figure><p id="e6fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我会说，对于144*144大小的图像，我们没有得到任何改善。</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mb"><img src="../Images/fffc2959ada53956ce667b3ec1ca577e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c3hHknpSTVm-z4BDkF9L8g.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">图像尺寸288*288的第一模型预测与第二模型预测</figcaption></figure><p id="cfcf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于288*288的图片，我们得到明显更好的结果。<br/>如果我们查看上一个时期的模型结果以获得更大的图像，我们可以看到损耗差为<strong class="ih hj"> <em class="mc"> 0.005 </em> </strong>，PSNR差为<strong class="ih hj"> <em class="mc"> 0.2998 </em> </strong>。我会认为这种差别太小，不会有任何影响，但显然，我错了。</p><h1 id="d1d4" class="le jf hi bd jg lf lg lh jk li lj lk jo ll lm ln jr lo lp lq ju lr ls lt jx lu bi translated">第三种模式</h1><p id="0a52" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">该模型使用<a class="ae jd" href="https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d" rel="noopener" target="_blank"> ATROUS </a>卷积代替上述残差块。这个模型有两倍多的参数..</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es md"><img src="../Images/a48516ed3bdd2a38fcf206fa3792ae22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0X0EHNbsl7eteDTBq82wPQ.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">第三模型结构</figcaption></figure><p id="3a92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个模型中，我尝试了一点损失权重来改善我的结果。起初，我认为给第一个输出(144*144图像)更多的权重将有助于改进模型，但经过一些调整后，我发现实际上相反是正确的，并将权重设置为<strong class="ih hj"><em class="mc">【0.3，0.7】</em></strong>。<br/>我最初的直觉是，如果我们改善得更快，较小的图像会更容易改善，但显然我错了。我假设给第二个输出更大的权重有助于改善两个输出的结果，因为模型的第一层属于两个输出，这有助于更快地为第二个图像设置它们，并且无意中也有助于第一个输出的结果。</p><p id="2141" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，当查看该模型的损失和PSNR时，我看不到结果有任何显著改善，实际上第二个输出的PSNR下降了一点点。<br/>这是否意味着照片会一样？</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mb"><img src="../Images/389cd0ea63920488925eb1c0e3473f58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oMaelEVsPr_PFBA7pCd2Zg.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">第二模型预测144*144对第三模型预测144*144</figcaption></figure><p id="f6f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到这个模型对144*144的图像做了一点改进。</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es me"><img src="../Images/4890c7fb6aed6f6a09334cf318ffb9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fJ_FfYtt_Z5qt1wE0rymnQ.png"/></div></div></figure><p id="6839" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">很难决定这个模型对288*288图像的预测。预测看起来几乎一样，除了一些小细节不同，而且不总是更好，那么谁做得更好呢？—让每个人自己决定。</p><h1 id="51a8" class="le jf hi bd jg lf lg lh jk li lj lk jo ll lm ln jr lo lp lq ju lr ls lt jx lu bi translated">第四种模式</h1><p id="e3be" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">在这个模型中，我使用了另一个预训练的模型作为我的模型的特征提取器。我决定用EfficiNetB4型号。</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mf"><img src="../Images/a2a26c7e181e6d893225b2573e7b2824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KwT9JCgj7gw-zIweZbPaLA.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">第四模型结构</figcaption></figure><p id="462e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个模型不在Keras中，所以我必须从另一个库中安装它。我只用了模型的第一块。由于连接层，我们需要保持两个连接层的大小相同。为了实现这一点，我使用了上采样来匹配卷积层的大小。我不想训练EfficiNet模型，所以我冻结了这个模型的权重..</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="6e70" class="je jf hi ke b fi ki kj l kk kl">import efficientnet.keras as efn <br/><br/>efn = efn.EfficientNetB4(include_top = False, input_shape = (None,None,3))</span><span id="f48a" class="je jf hi ke b fi km kj l kk kl">my_efn = Model(inputs = efn.input, outputs = efn.get_layer("block1a_activation").output)<br/>my_efn.trainable = False<br/>for layer in efn.layers:<br/>  my_efn.trainable = False</span></pre><p id="0093" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个模型实际上比以前的模型有更少的参数。这会影响模型结果吗？<strong class="ih hj">是</strong></p><p id="4847" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以在模型损耗和PSNR以及图像中看到</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mg"><img src="../Images/7e270cd97037325185fb3e750850740c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UVtZ2EzSNMMFiwZohzdZrA.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">第四模型损失&amp; PSNR图</figcaption></figure><p id="38eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">损失略高于以前的型号。<br/>这款车型PSNR减半。</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div class="er es mh"><img src="../Images/0637c941df01f6e8a2cc36427fec4320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*O5Bv0bq8jBFWvOXlIEjRyg.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">第四模型预测</figcaption></figure><p id="e136" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到这个模型并没有得到正确的结果。EfficiNetB4权重和少量参数的组合导致结果很糟糕。<br/>所以我一次又一次地尝试改进这个模型，直到我得到合理的结果。</p><p id="c39b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我做的第一个改变是在EfficiNet结果的上采样后添加一个卷积层，因为<strong class="ih hj">在没有卷积的情况下使用上采样是一个糟糕的做法</strong>，然而这并没有解决问题。</p><p id="29a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之后，我添加了更多的卷积，得到了一个类似这样的模型:</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mi"><img src="../Images/203cdcbcc1a97f14ea572b8b72fc97ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHoA9JXOGBREn5GlMj4_gA.png"/></div></div></figure><p id="257a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与第三个模型权重相比，在这里，我使用的损失权重是<strong class="ih hj"><em class="mc">【0.65，0.35】</em></strong>它们产生了更好的结果。我认为这是因为第二个输出有4个卷积层，而第一个输出只有2个。</p><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mb"><img src="../Images/25df33886db90eabbf2a3fe302c8da4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ndPdWn_XV7MlrpYod1l-A.png"/></div></div></figure><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mj"><img src="../Images/ab104b3a3b7ff1aedc50e5d11637d4ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JKKl2VjY-7H107LJ_Nri3g.png"/></div></div></figure><p id="e3d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，144*144和288*88图像的预测是相同的。这是通过使用<strong class="ih hj">参数数量的一半</strong>来实现的。</p><h2 id="d540" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">这里能学到什么？</h2><p id="93a8" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">特征提取器模型不仅仅是一个现成的解决方案，它需要重新思考模型架构。使用它可以显著减少模型所需的参数数量，从而减少训练时间。<br/>此外，这让我认为，如果我们匹配先前的参数数量，我们可以获得更好的结果。</p><h1 id="56a7" class="le jf hi bd jg lf lg lh jk li lj lk jo ll lm ln jr lo lp lq ju lr ls lt jx lu bi translated">第五种模式</h1><p id="349a" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">最后一个模型实际上应该与第四个模型相同，只是使用深度来分隔层，而不是上采样。我使用了之前模型的最终版本，只是用深度到空间层替换了上采样层。</p><h2 id="49a7" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">如何使用深度来分隔图层(代码示例)</h2><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="fa25" class="je jf hi ke b fi ki kj l kk kl">Lambda(lambda x:depth_to_space(x,2))</span></pre><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mk"><img src="../Images/4588a8b6a53d66e122240cd3ebf02e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TG3r-2lWEXFmmizDswNKRw.png"/></div></div></figure><p id="c7f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，对于144*144和288*288，第五种模型的预测更好。这让我得出结论，使用深度到空间的上采样提高了超分辨率结果。</p><h1 id="87cf" class="le jf hi bd jg lf lg lh jk li lj lk jo ll lm ln jr lo lp lq ju lr ls lt jx lu bi translated"><strong class="ak">结论</strong></h1><h2 id="a0ac" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">决赛成绩</h2><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ml"><img src="../Images/8712d6ea581a01af3ac123b723a465ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MtrLTzrFerRrDcwN7OV6vw.png"/></div></div></figure><figure class="jz ka kb kc fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ml"><img src="../Images/809e99e1819e5a85e2d9e4679ce06e88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UgYlF3fNYLKbPcaAGwTwjg.png"/></div></div></figure><p id="8a0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们可以看到模型的改进，但我还远远没有达到原始图像的效果。</p><p id="12e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个过程中，我学到了很多关于超分辨率的知识，并看到即使相对简单的模型也能产生一些合理的结果，但没有真正好的结果，我假设更大更深的模型可能会改善结果。<br/>我注意到从MSE或PSNR等不同的值来理解图像质量的难度。</p><p id="849d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经看到，使用预先训练的网络可以是一个强大的工具，但它并不总是现成的解决方案，需要一些调整。</p><p id="d57e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，我还看到了全卷积网络的威力，它如何帮助避免定义输入大小，并允许在同一网络中使用不同大小的输入。</p><p id="9497" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://www.kaggle.com/arielamsel/assignment3-super-resolution" rel="noopener ugc nofollow" target="_blank">笔记本在卡格尔</a></p></div></div>    
</body>
</html>