<html>
<head>
<title>Hidden Markov Model — Part 1 of the HMM series</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">隐马尔可夫模型HMM系列的第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hidden-markov-model-part-1-of-the-hmm-series-3f7fea28a08?source=collection_archive---------2-----------------------#2019-07-21">https://medium.com/analytics-vidhya/hidden-markov-model-part-1-of-the-hmm-series-3f7fea28a08?source=collection_archive---------2-----------------------#2019-07-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0759" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">它是如何构成的，如何处理？</em></p><p id="aa98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">注意:在这一系列的文章中，我将讲述我在过去几天中对HMM的了解。我试图让文章简单易懂。虽然这不是一本教科书能提供给你的系统方法，但我希望我的文章能帮助你克服旅途中的一些瓶颈。祝你好运！</em></p><p id="8eb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一部分:<a class="ae je" rel="noopener" href="/@rmwkwok/hidden-markov-model-part-1-of-the-hmm-series-3f7fea28a08">隐马尔可夫模型的体系结构</a> <br/>第二部分:<a class="ae je" rel="noopener" href="/@rmwkwok/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86">训练HMM的算法:Baum-Welch算法</a> <br/>第三部分:<a class="ae je" rel="noopener" href="/@rmwkwok/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6">用训练好的HMM进行预测的算法:Viterbi算法</a></p><h2 id="c756" class="jf jg hi bd jh ji jj jk jl jm jn jo jp iq jq jr js iu jt ju jv iy jw jx jy jz bi translated">隐马尔可夫模型简介</h2><p id="1f68" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kc is it iu kd iw ix iy ke ja jb jc hb bi translated">简单来说，HMM是一种概率模型，用于从观察到的数据中推断出未观察到的信息。以手机的屏幕键盘为例，你有时可能会打错你想输入的字符。在这里，你输入错误的字符是观察到的数据，而你打算在头脑中输入的是未观察到的数据。作为另一个例子，由于一些随机噪声源，您的GPS读数(观察到的)可能会在您的实际位置(未观察到的)附近跳动。</p><p id="815c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有意思？在许多情况下，观察到的并不是真相，而隐马尔可夫模型是一种可以把隐藏的真相还给我们的方法。然而，它不是万能的，要使用它，它必须满足一些假设，HMM就是建立在这些假设上的。我在这里的方法是用文字讨论HMM，并辅以一些数学，如果这次不是你喜欢的，可以跳过。开始吧！</p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><h1 id="e4fa" class="km jg hi bd jh kn ko kp jl kq kr ks jp kt ku kv js kw kx ky jv kz la lb jy lc bi translated">状态序列和可观察值</h1><p id="8dac" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kc is it iu kd iw ix iy ke ja jb jc hb bi translated">在我上面的两个例子中，我们实际上都在处理数据序列。例如，我们可能会将“杂项”误打成“杂项”<strong class="ih hj"> <em class="jd"> m </em> </strong> eous,“杂项”实际上都是字符序列。在GPS示例中，如果我们在该位置(22.3525 N 113.8475 E)静止不动，那么如果我们连续五秒钟每秒记录一次读数，则应该重复五次。然而，随着读数的跳跃，我们可能会看到(22.3535 N 113.8455 E)，(22.3585 N 113.8325 E)，(22.3123 N 113.8600 E)，(22.3212 N 113.8397 E)，(22.3421 N 113.7989 E)。</p><p id="b35f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，对于每个观察到的数据，我们有一个相关的隐藏真相。在正式的HMM讨论中，人们把每个“隐藏真相”称为“状态变量”，把每个“观察数据”称为“观察变量”。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ld"><img src="../Images/6cecd37d7ad0f14e34d71b697dd66395.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0tiYtorXruLN3wmGaKHLw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">嗯。上述橙色方框相互“链接”形成一个马尔可夫链。链条中的每个状态都“链接”到一个观察到的变量。</figcaption></figure><h1 id="4d68" class="km jg hi bd jh kn lt kp jl kq lu ks jp kt lv kv js kw lw ky jv kz lx lb jy lc bi translated">HMM结构中的状态和可观察值</h1><p id="98bd" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kc is it iu kd iw ix iy ke ja jb jc hb bi translated">我们来看上图。在这个HMM架构中，我们有一系列相互“链接”的状态(隐藏的事实)。箭头在这里有<em class="jd">依赖</em>的意思。例如，时间= 1时的状态取决于时间= 0时的状态。这是马尔可夫模型中非常重要的假设，使得它如此简单——<em class="jd">任何状态仅仅依赖于它的一些先前状态</em>。我在state后面加了一个(s ),因为它是可选的。如果一个状态依赖于它的第一个先前状态，我们称之为简单马尔可夫链，而如果一个状态依赖于它的第一个<em class="jd"> n个</em>先前状态，我们称之为n阶马尔可夫链。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ly"><img src="../Images/5bc49b86fca178b30ea4c689851b6c5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jx_2VY8KNuRE9VP0ZT0T3A.png"/></div></div></figure><p id="c3b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑国家之间的依赖性是很自然的，因为我们认为它们之间有某种联系。例如，一个单词中字符的组合不是随机的，它必须按照正确的顺序才能被正确理解。而最简单的依赖就是一个状态只依赖于一个之前的状态，这也是讨论将要重点讨论的。</p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><h1 id="6894" class="km jg hi bd jh kn ko kp jl kq kr ks jp kt ku kv js kw kx ky jv kz la lb jy lc bi translated">状态转换、发射和初始状态</h1><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lz"><img src="../Images/9068e2104eb90e32764e1776dc87d50a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2rvoDrZ_QlZMWSDPHlMTQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">嗯。橙色方框代表状态，绿色方框代表可观察状态</figcaption></figure><p id="19e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经将状态、观察到的变量以及它们的依赖关系放入HMM架构中。我们将进一步量化相关性，从状态转换开始，即从时间<em class="jd"> k-1 </em>到时间<em class="jd"> k </em>的状态转换，或者从一个橙色框到下一个橙色框的变化。</p><p id="d704" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设有<em class="jd"> M </em>种可能的状态可供选择，即状态可以是{1，2，3，…，<em class="jd"> m </em>中的任意一种。我们可以将从状态<em class="jd"> i </em>到状态<em class="jd"> j </em>的转移概率量化为<em class="jd"> aᵢⱼ.</em>由于从<em class="jd"> M </em>种可能状态中的任何一种状态到<em class="jd"> M </em>种可能状态中的另一种状态的转换都可能发生，所以总共有<em class="jd"> M </em> × <em class="jd"> M </em>种可能性。我们可以将它们排列成下面的矩阵表示，称为状态转移矩阵<strong class="ih hj"><em class="jd"/></strong>。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es ma"><img src="../Images/076e432ceaa660f877f99eccabb6c01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*b4zFCJtl_8_EmwhpkaLVeg.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated"><em class="mb">状态转移矩阵A. aᵢⱼ表示从状态I转移到状态j的概率</em></figcaption></figure><p id="3d0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为所有的<em class="jd"> aᵢⱼ </em>都是概率值，都被有界在0和1之间，每一行都要求和为1。一般来说，对于每个时间步<em class="jd"> t </em>，我们可以有一个这样的矩阵<strong class="ih hj"><em class="jd"/></strong>。然而，我们将考虑一个<em class="jd">平稳马尔可夫链</em>，意味着矩阵<strong class="ih hj"> <em class="jd"> A </em> </strong>在时间上是常数。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mc"><img src="../Images/2600776cb7c7cd7f3d8555d1dbcf7a45.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*GNtcNmEhIAwhgKcIsS-B8A.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated"><em class="mb"> aᵢⱼ表示给定状态I时状态j的概率</em></figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es md"><img src="../Images/8962513aa364634be31da4a82cd0c455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8j5BQPQ_QdMsmPm7gDvnSQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">具有转移的先前状态的矩阵乘法</figcaption></figure><p id="c3a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，我们可以构建发射矩阵<strong class="ih hj"> <em class="jd"> B </em> </strong>来存储状态<em class="jd"> i </em>的概率，从而得到观察值<em class="jd"> j </em>。这里，假设可观测值有<em class="jd"> K </em>个可能值，这意味着它可以取{1，2，…, <em class="jd"> k </em>中的任何一个。这里也假设了平稳性，因此我们有一个恒定的发射矩阵。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es me"><img src="../Images/3776b596215dd5fdcb4b05b66a90fe93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*9WAhQ55hQFWoBUG5LQhLLA.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated"><em class="mb">发射矩阵B. bᵢⱼ代表状态I发射可观测j的概率</em></figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mf"><img src="../Images/fc968f87dfa5529338b3a2b9d8fbee96.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*AL6fwTcIG-fJgrsX7muMJw.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated"><em class="mb"> bᵢⱼ代表观察j给定状态I的概率</em></figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mg"><img src="../Images/ddfd88960b573c0598c7cb2b285e4e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*HTMRTBdz-qDRROLZWZUZ3A.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">状态和发射的矩阵乘法</figcaption></figure><p id="2d44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经接近全图，现在有了架构，状态转移矩阵<strong class="ih hj"> <em class="jd"> A </em> </strong>，发射矩阵<strong class="ih hj"> <em class="jd"> B </em> </strong> <em class="jd">。我们需要的最后一件事是，初始状态的概率分布，π₀，₀.我们需要它来开始第一个状态，所以马尔可夫链中即将到来的状态可以通过状态转移矩阵来估计。</em></p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mh"><img src="../Images/f6d5aab3d53db2e8df49a8ac01bb30cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*l7YqS8127wmhX3YHbbz1ZQ.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">初始状态分布π₀.</figcaption></figure><p id="4de2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了给你一些实际的数字，让我们看看下面这个关于单词的例子。我从牛津高级学习词典中取出了<a class="ae je" href="https://www.oxfordlearnersdictionaries.com/wordlist/english/oxford3000/" rel="noopener ugc nofollow" target="_blank"> 3000个单词列表，并列出一个字符作为起始字符的概率，以及一个字符出现在另一个字符之后的概率。</a></p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mi"><img src="../Images/503364f3ee91c301ec74b0662af17b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*ejBf4GQ5_jaXmtXMrURYVQ.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">此处仅显示了部分表格，所有值均为百分比。第一行是初始状态分布，表示有一个字符(状态)作为起始字符的概率，比如有10.1%的几率看到以‘c’开头的单词。所有剩余的行形成状态转移矩阵。例如，在一个单词中，有39.2%的几率‘d’后面跟着‘e’。</figcaption></figure><p id="884d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">表格的第一行给出了初始状态分布<strong class="ih hj"> π₀ </strong>，其余的组成了状态转移矩阵<strong class="ih hj"><em class="jd"/></strong>。对于发射矩阵<strong class="ih hj">例如，如果我们打算键入“G”(状态)，那么有70%我们正确地键入“G”(观察到的)，有30%得到{'T '，' Y '，' F '，' H '，' C '，' V '，' B'}中的任何一个，或者每个的大约4.2%。通过这个论证，我们可以构造发射矩阵。</strong></p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mj"><img src="../Images/a49ce11804f063a4652bdb83254553fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*8PsImCVcSdX81IkQ5dsh7Q.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">键盘示例</figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mk"><img src="../Images/8a23bb5c9964378483994ede00293f7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*BoX6_Li3A4fX4DZyBISiwg.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">此处仅显示了部分表格，所有值均为百分比。该表构成了排放矩阵。例如，有5%的机会，故意键入' d '(状态)将导致' c '(观察)。</figcaption></figure></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><h1 id="74cb" class="km jg hi bd jh kn ko kp jl kq kr ks jp kt ku kv js kw kx ky jv kz la lb jy lc bi translated">摘要</h1><p id="8495" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kc is it iu kd iw ix iy ke ja jb jc hb bi translated">让我们在这里休息一下。到目前为止，我们已经看到，HMM模型是由状态变量和相关联的观察变量构成的，并且对于每个状态，它依赖于多少个先前的状态。然后通过状态转移矩阵<strong class="ih hj"> <em class="jd"> A </em> </strong>，发射矩阵<strong class="ih hj"> <em class="jd"> B </em> </strong>，初始状态分布<strong class="ih hj"> π₀ </strong>对模型进行参数化。请注意，我们一直在谈论的HMM是一个平稳、简单的隐马尔可夫模型，它采用离散的状态变量、离散的观察变量，并且这些变量在时间上是离散的，因此，它是HMM家族中的一个特例。</p><p id="3507" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，虽然我用了单词列表来做计数，并且用一些实际的数字来表示矩阵，但是我们实际上并没有这样做，我们也不能总是通过证明来构造发射矩阵。在接下来的文章中，我们将讨论用于训练(或调整参数矩阵)的Baum-Welch算法，以及用于从观察数据推断整个隐藏状态序列的Viterbi算法。</p></div></div>    
</body>
</html>