<html>
<head>
<title>Machine Learning: Support Vector Regression (SVR)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:支持向量回归(SVR)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-support-vector-regression-svr-854524391634?source=collection_archive---------13-----------------------#2020-06-15">https://medium.com/analytics-vidhya/machine-learning-support-vector-regression-svr-854524391634?source=collection_archive---------13-----------------------#2020-06-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f2b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客中，我们将介绍另一种有趣的机器学习算法，称为支持向量回归(SVR)。但是在学习支持向量机之前，我们先来学习一下支持向量机(SVM)，因为支持向量机是基于SVM的。</p><p id="f621" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SVM是一种监督学习算法，它通过分析数据和识别模式，尝试基于分类或回归来预测值。用于分类的算法称为SVC(支持向量分类器),用于回归的算法称为SVR(支持向量回归)。</p><p id="1617" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们了解一些基本概念</p><ul class=""><li id="2710" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">超平面</strong>:超平面是一个平面，用于根据类别的值来划分类别。超平面总是比用于绘制结果或分析的实际平面小1维。例如，在具有1个特征和1个结果的线性回归中，我们可以制作一个2-D平面来描述关系，并且拟合到该平面的回归线是1-D平面。因此，这个平面称为超平面。类似地，对于三维关系，我们得到一个二维超平面。</li></ul><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/25bd1007fcca6092a411ff7dcaeb6637.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FwrX8viaCLljRAAxiSAp8Q.png"/></div></div></figure><ul class=""><li id="c5e5" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">支持向量</strong>:支持向量是空间中距离超平面较近的点，也决定了超平面的方向。所画的线或平面称为支持向量线或支持向量平面。</li></ul><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es jy"><img src="../Images/96df41166a5b0f9cc2f4272d9cef15c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*MHtUN-DgYPZWPe2dU5G6Lg.png"/></div></figure><ul class=""><li id="5f7f" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">边距宽度</strong>:两条支持向量线或平面的垂直距离称为边距宽度。</li></ul><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es jz"><img src="../Images/ebe649e157dc1f3f0df085ebe05f4635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Fjj7EblDs2J88GgJmyKL8w.png"/></div></figure><h1 id="4ddf" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">核心诡计</h1><p id="a0c9" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">在上图中，我们看到我们的数据是线性可分的。但是考虑下面的情况。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es ld"><img src="../Images/fc023c6ae361e081f94b768fd06f7677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6wQ0-PpTAIB1tnVQ.png"/></div></div></figure><p id="d89b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，简单的线性划分是不可能的。所以，SVM内核给它增加了一个维度。添加另一个维度后，数据可以用一个平面来分离。可以得出以下直觉。</p><p id="8f01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SVM算法试图在支持向量和位于支持向量平面之上或之下的点之间画一个具有最高边界宽度的超平面，即负侧上的那些点保持在负超平面之下，而正侧上的点保持在正超平面之上。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es le"><img src="../Images/b9c191a4287e62c321b404c4da56ecfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SVIcw7hDcoH6WPfK.png"/></div></div></figure><h1 id="b243" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">支持向量回归</h1><p id="4547" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">假设有一个数据集，我们试图用一个雇员的年龄来预测他的工资。因此，我们可以创建一个线性回归模型，这将有助于我们。在线性回归中，我们试图拟合一条尽可能减小误差的直线。但是通过支持向量机，我们将拟合一条线(对于2-d)或一个超平面(对于n-d ),试图将误差限制在一定程度。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es lf"><img src="../Images/3fdd04c592447eb6f48cde55302c052b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*407C6bjGggCsN92U.png"/></div></div></figure><p id="b809" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SVR背后的主要直觉是，大多数点位于超平面上，其余点位于正负超平面内。但是在SVC中，我们试图画一个超平面，使得负的点位于负的超平面之下，正的点位于正的超平面之上。</p><p id="15bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在SVR中，我们试图找到这样的平面，使得位于正或负超平面之外的点被减少。</p><p id="101b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设“a”是支持向量线到超平面的距离。</p><p id="4d19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1 —设超平面的方程为:</p><h2 id="c9bf" class="lg kb hi bd kc lh li lj kg lk ll lm kk iq ln lo ko iu lp lq ks iy lr ls kw lt bi translated"><strong class="ak"> Y= wx + b </strong></h2><p id="3698" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">2 —正超平面的方程是:</p><h2 id="d153" class="lg kb hi bd kc lh li lj kg lk ll lm kk iq ln lo ko iu lp lq ks iy lr ls kw lt bi translated">+a = wx + b</h2><p id="d60b" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">3 —负超平面的方程是:</p><h2 id="1526" class="lg kb hi bd kc lh li lj kg lk ll lm kk iq ln lo ko iu lp lq ks iy lr ls kw lt bi translated">-a = wx + b</h2><p id="7837" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">4-因此我们的超平面应该满足以下条件:</p><h1 id="4eda" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">-a &lt; Y-wx+b &lt; +a</h1><p id="f5ab" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">因此，我们应该选择“a ”,使支持向量位于其中，误差最小。</p><p id="c293" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢你们阅读这篇文章。敬请关注更多博客。祝你有愉快的一天。</p></div></div>    
</body>
</html>