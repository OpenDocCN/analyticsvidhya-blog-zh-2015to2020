<html>
<head>
<title>A Complete Guide to RCNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RCNN完全指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-complete-guide-to-rcnn-2dc92c00243f?source=collection_archive---------14-----------------------#2020-03-27">https://medium.com/analytics-vidhya/a-complete-guide-to-rcnn-2dc92c00243f?source=collection_archive---------14-----------------------#2020-03-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="8ca2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">原RCNN: <br/> </strong>区域CNN的概念是Girshick在他的论文中给出的[12]。<br/>原始RCNN的算法工作如下:</p><ol class=""><li id="2f21" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated">输入图像。</li><li id="7b7b" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">使用选择性搜索算法提取潜在目标区域。</li><li id="8833" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">将学习特征提取转移到计算特征。</li><li id="6476" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">将它们分类。</li></ol><p id="842c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">原始CNN的问题是它非常慢，并且没有使用深度神经网络进行分类。</p><p id="498b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">选择性搜索:<br/> </strong>基本上有4个区域组成一个物体:变化的尺度、颜色、<br/>纹理和包围。选择性搜索识别图像中的这些模式，并在此基础上提出各种区域。流程工作如下:<br/> 1 .拍摄输入图像</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es jy"><img src="../Images/33a6f9958ebe203ec09424603bb25a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*JJD6r6fAk10Ha1gFfSm5xQ.png"/></div></figure><p id="3ca4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.生成初始子分割，以便我们在图像中有多个区域。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kg"><img src="../Images/7518152858d8687af44648387c35c912.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*ryTTUkuUyBwfDQ9VokXllQ.png"/></div></figure><p id="592e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.然后组合相似的区域以形成更大的区域(基于颜色<br/>相似性、纹理、大小和形状)。</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kh"><img src="../Images/29d1f04076a03c2cc377718f9370d4a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*gPfG5HLH3SWQWyJFvQEp-A.png"/></div></figure><p id="6341" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">4.把这个发给康文网。</p><p id="cd63" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">快速RCNN: </strong> <br/>使用RPN的概念来预测潜在区域的位置。<br/>流程如下:<br/> 1。区域建议(我们将在更快的RCNN部分简要讨论)<br/> 2。特征提取<br/> 3。计算包围盒坐标<br/> 4。提供类别标签</p><p id="e5a1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">更快的RCNN: </strong> <br/>是由提出上述两种方法的Girshick [8]提出的。<br/>其架构由以下组件组成:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es ki"><img src="../Images/de7b8293d4ba98a343075eb8bcb8c7ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*jP9WSscOPXoaMl7CGgLXxw.png"/></div></figure><ol class=""><li id="a13b" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated"><em class="kj">基本网络</em>:在ImageNet数据集上预训练的CNN。我们使用ResNetor MobileNet作为基础网络，而原始文件使用VGG网络。这里我们简单地通过使用全卷积深度神经网络来提取特征图。我们将完全连接的网络从<br/>架构中移除。这里我们的主要目的是获得特征图。然后，区域建议网络和ROI汇集模块使用特征<br/>图。</li><li id="7343" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated"><em class="kj">锚盒</em>:锚是物体<br/>检测中使用最多的技术之一。使用不同形状的盒子来检测同一网格中的不同对象。输出y的形状与我们输出中的所有锚盒一致。创建锚点始于每r个像素对图像坐标进行采样的过程[在最初的快速RCNN纸中r = 16。例如，我们可以创建以每个采样(x，y)坐标为中心的九个锚点。这里的九个锚点是64 x 64、128 x 128和256 x 256的组合，长宽比为1:1、2:1、1:2。这里，仅使用锚点的问题是，如果我们在600 x 800的图像上使用16像素的步幅，我们将获得总共1989个位置。在1989个位置的每个位置周围有9个锚，我们总共有1989 x 9 = 17901个边界框位置供CNN评估。</li><li id="f92f" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated"><em class="kj">区域提议网络</em>:如果生成锚点的目标是在对象的所有可能的尺度和尺寸上获得良好的<br/>覆盖，<br/>区域提议网络的目标是将边界框的数量削减到<br/>可管理的大小。接受锚点并预测其客观分数。<br/>涉及将ROI标记为前景或背景。前景锚定被提前发送，而背景锚定被丢弃。两条并行路径中的RPN输出。RPN的第一个输出是指示图像是背景还是前景的分数。对于k个锚，我们有2k个输出。第二个输出是我们的边界框回归器，用于调整锚点以更好地适应对象。这是通过1 x 1卷积实现的。对于k个滤波器，输出为4k体积，提供4个δx、δy、δh、δw值。此外，我们应用非最大抑制来减少要传递给ROI合并模块的位置数量。然后，我们选择前N名提案。Girshick使用N = 2000。所以总结一下RPN的训练:<br/> <br/> a .把我们的主播分为后台和前台。<br/> b .保持两个<br/> c .两个损失函数:一个测量前景<br/>相对于背景的准确度，第二个损失函数用于我们的边界框<br/>回归，其仅在我们的前景锚上操作。</li><li id="91f6" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated"><em class="kj"> ROI合并模块</em>:ROI合并模块的主要目标是取所有N个<br/>提案位置，然后从conv <br/>特征图中提取相应的ROI特征。ROI汇集模块然后将ROI的<br/>提取特征的尺寸调整为7 x 7 x D。该固定尺寸为两个即将到来的FC层准备<br/>特征。其功能如下:<br/> a .使用数组切片从特征图中提取相应的面片<br/>。<br/> b .将其大小调整为14 x 14 x D其中D是特征图的深度。<br/> c .应用2 x 2步幅的最大池，产生7 x 7 x D特征<br/>矢量。</li></ol><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kk"><img src="../Images/4d57cad0060521ab962088c0ef475bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*myCzEwB6_QaSDjQgy2P9Kw.png"/></div></div></figure><p id="5a7b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">最终的特征向量然后被馈送到基于区域的CNN，用于最终的<br/>边界框。</p><p id="c0fa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">5.<em class="kj">基于区域的卷积神经网络</em>:该模块有两个<br/>用途。一个是基于来自ROI合并模块的裁剪特征mp获得每个边界框位置<br/>的最终类别标签。进一步优化<br/>边界框预测(x，y)坐标，以获得更高的精确度。通过两个完全连接的网络实现<br/>。这里的额外节点是背景。第二个<br/> FC为每个类别提供了4个增量值。这两个输出意味着我们将采用<br/>两个损失函数，一个分类交叉熵用于分类，平滑<br/> L1损失用于回归。最后一步是对我们的边界框应用非最大抑制类别<br/>。我们可以依次训练RPN和RCNN，或者<br/>将所有四个损失函数组合起来，一起训练。一般来说，将他们一起训练效果会更好。</p><p id="12d7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">总结更快的RCNN: </strong> <br/> 1 .用于特征提取的基网络。<br/> 2。RPN接受锚点集合，并输出它认为对象在图像中的位置的建议。<br/> 3。然后使用ROI池从每个建议中提取特征图。<br/> 4。最后，基于区域的RCNN获得最终类别标签，并进一步细化<br/>建议位置以获得更好的准确性。</p><p id="747a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">参考文献</strong></p><p id="11de" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">[1]吴恩达。卷积神经网络，deeplearning.ai。<br/> [2]皮埃尔·塞尔马内，大卫·艾根。过吃:使用卷积网络的综合识别、定位和<br/>检测。<br/>【3】约瑟夫·雷德蒙，桑托什·迪夫瓦拉，罗斯·吉尔希克。你只看一次:<br/>统一的，实时的物体检测。<br/>【4】IOU-<br/><a class="ae kp" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-ob" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2016/11/07/intersection-over-union-IOU-for-ob</a><br/>object-detection/<br/>【5】Saptarshi Chakraborty，Dhrubajyoti Das。活性检测综述。<br/>【6】LBP-<br/><a class="ae kp" href="https://www.pyimagesearch.com/2015/12/07/local-binary-patterns-with-python-openc" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2015/12/07/local-binary-patterns-with-python-openc</a><br/>v/<br/>【7】RCNN-<br/><a class="ae kp" href="https://www.analyticsvidhya.com/blog/2018/11/implementation-faster-r-cnn-python-object-d" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/11/implementation-faster-r-CNN-python-object-d</a><br/>etection/。更快的R-CNN:走向实时- <br/>用区域建议网络进行实时目标检测。<br/>【9】阿德里安·罗斯布鲁克。用Python进行计算机视觉的深度学习<br/>【10】mogel mose A，Trivedi MM，Moeslund TB。智能驾驶辅助系统的基于视觉的交通标志检测和<br/>分析:前景和调查。<br/>【11】Lin，Tsung-Yi等.微软coco:上下文中的常见对象<br/>【12】Ross Girshick，Jeff Donahue，Trevor Darrell，Jitendra Malik用于精确对象检测和语义分割的丰富特征层次<br/></p></div></div>    
</body>
</html>