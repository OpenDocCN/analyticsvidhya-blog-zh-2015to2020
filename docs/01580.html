<html>
<head>
<title>A Simple &amp; Practical Introduction To Essential Techniques Of Feature Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单实用的特征约简基本技术介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-simple-practical-introduction-to-essential-techniques-of-feature-reduction-db4002fd16b5?source=collection_archive---------6-----------------------#2019-11-01">https://medium.com/analytics-vidhya/a-simple-practical-introduction-to-essential-techniques-of-feature-reduction-db4002fd16b5?source=collection_archive---------6-----------------------#2019-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/adfca9def73c873d1e679b37fa1f8f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dt7dHzsti6GOFmj73XX5Uw.jpeg"/></div></div></figure><p id="7805" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在机器学习模型中，基本目标是创建一个简单的模型，可能不太准确，但提供更快的结果。复杂模型倾向于过度拟合，导致更好的样本内误差，但高的样本外误差。特征工程、特征选择、特征减少和特征提取技术有助于创建一个简单而强大的模型，该模型根据最佳数量的特征进行训练，并能更快地进行预测。</p><h1 id="87b4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">为什么要减少特征？</h1><p id="0931" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated"><strong class="is hj">特征约简在机器学习流水线中起着至关重要的作用，并解决以下问题:</strong></p><ul class=""><li id="de7a" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated"><strong class="is hj">多重共线性</strong> :-在多重共线性中，自变量之间是相关的，预测因变量或目标变量的基本方法是具有与目标变量相关且不相关的特征。所有的自变量都应该带有一些定义目标变量的附加信息。</li><li id="78b7" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><strong class="is hj">维数灾难/过度拟合</strong> :-当我们有一个非常高的特征数时，目标函数将变得太复杂，同时试图从所有变量中捕捉信息，并且导出的函数可能不是平滑的曲线。这可能会导致模型过拟合。</li><li id="5fa1" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">计算量大 :-移除不太重要的特征不仅使模型简单，还减少了计算时间，从而节省了系统资源。</li><li id="5117" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">降低了机器学习模型的有效性。</li></ul><p id="001b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">在高维数据集中，特征约简技术可以帮助您:</strong></p><ul class=""><li id="8d25" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">移除信息量较少的特征。</li><li id="f870" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">它使计算更有效率。</li><li id="7764" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">使模型简单，不容易过度拟合。</li></ul><p id="ba33" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">在这篇博客中，我们将介绍以下降维技术的基本功能及其在python中的实现。</strong></p><ul class=""><li id="9523" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">主成分分析</li><li id="cb36" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">皱胃向左移</li><li id="9787" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">德拉贡诺夫狙击步枪（Snayperskaya Vinyovka Dragunov的缩写）</li></ul><h1 id="5985" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">主成分分析:</h1><p id="9cfc" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">方差越大，信息量越大的特征是PCA工作的基本原理。PCA是一种无监督的统计方法，基于协方差矩阵分析。协方差矩阵<strong class="is hj">定义了一个变量与另一个变量的关系。使用协方差矩阵，计算特征向量&amp;的特征值。PCA创建新的尺寸或特征，从而保留原始特征的最大变化。这个新维度是一条使特征之间的方差最大化的线，被称为<strong class="is hj">主成分</strong>。PC的等式可以定义如下:</strong></p><p id="27fa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PC1 =β1<strong class="is hj">*</strong>X1+β2<strong class="is hj">*</strong>X2其中β1&amp;β2称为PCA负荷。</p><p id="95de" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最大变化的方向由特征向量确定，特征向量中的数据点被称为<strong class="is hj"> PCA负荷</strong>，而<strong class="is hj">特征值</strong>代表原始特征内方差方面的信息。</p><ul class=""><li id="47fc" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">数据集主成分的最大数量将等于数据集中可用要素的数量。</li><li id="5057" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">加载数组的维数与原始特征的数目相同，即行数和列数等于原始特征的数目。</li></ul><p id="ad97" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如以上等式中所定义的，PC1，即第一分量得分，是实际缩放特征数据和第一特征向量的PCA负载的乘积之和。类似地，PC2分数是特征数据和第二特征向量的PCA负载的乘积的和。</p><p id="c844" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">对于每个分量，我们使用PCA得分计算方差，计算的方差也被称为主分量的特征值。</strong></p><p id="8bf7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Explained_variance_ratio </strong>是每个分量的特征值与所有分量的特征值之和的比值。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/50eb99500ee590e999a363a2ccc2b881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*LlZmsty64H8v3Q0CIZ4cSQ.jpeg"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">**图片来源:斯坦福大学Andrew NG教授的机器学习讲座* *</figcaption></figure><ul class=""><li id="19d4" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">如上图所示，2D图中的数据点被映射到1D轴上，从而以最小的误差获得最大的方差。</li><li id="4456" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">这里的<strong class="is hj">误差</strong>是点到轴的垂直距离的平方和。</li><li id="64c4" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">类似地，在3D到2D的情况下，点被映射在平面上。</li></ul><p id="a0a1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我推荐阅读法尔哈德·马利克的这篇好文章，以获得关于特征向量特征值的更详细的数学知识。</p><h1 id="4956" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">关于数据集:</h1><p id="c7d0" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我们将使用一个内置的虹膜数据集，它可以通过Sklearn API加载。<strong class="is hj">鸢尾数据集</strong>包含3类鸢尾植物的数据。</p><p id="ccf8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们导入所需的库并加载数据集。在给定的数据集中，我们有4个独立的特征。</p><p id="64eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将首先使用<strong class="is hj"> StandardScaler API </strong>缩放数据集，使所有变量处于相同的比例，然后我们将对缩放后的数据应用PCA。</p><pre class="lg lh li lj fd lp lq lr ls aw lt bi"><span id="5395" class="lu jp hi lq b fi lv lw l lx ly">#Importing libraries<br/>import pandas as pd # Pandas library to create dataframe<br/>import warnings # To ignore depreciation warning <br/>warnings.filterwarnings('ignore')<br/>import numpy as np <br/>import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import StandardScaler # Standard Scaling Library<br/>from sklearn.decomposition import PCA # From Sklearn.decompostion API import PCA <br/>from sklearn.datasets import load_iris # Loading dataset from sklearn.dataset API<br/># Magic Function to display plot in code cell<br/>%matplotlib inline</span><span id="c843" class="lu jp hi lq b fi lz lw l lx ly">#loading dataset<br/>iris = load_iris() # loading dataset in iris variable<br/>X = pd.DataFrame(iris.data) # Create DataFrame<br/>X.columns = ['sepal_len', 'sepal_wid', 'petal_len', 'petal_wid'] # Naming column headers<br/>y = iris.target # Loading Dependent or target variable <br/>from sklearn.model_selection import train_test_split # sklearn API to create random train test subsets<br/>X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0) # Test_size-0.2 will give create <br/>#subset of 80:20 ratio i.e 80% Training Set &amp; 20% Test set<br/>X_train.head(3)</span></pre><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/862028a5f5f9d3f7beb7ce9c0c93b655.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*tWybf6gnpgcj4z7CXma-9Q.png"/></div></figure><pre class="lg lh li lj fd lp lq lr ls aw lt bi"><span id="76fd" class="lu jp hi lq b fi lv lw l lx ly"># Scaling the dataset before applying PCA will ensure that the variation from the original components are captured equally.<br/>scaler = StandardScaler() # Creating standard scaler object<br/>X_train_scaled = scaler.fit_transform(X_train) # Scaling train set using fit_transform method<br/>X_test_scaled = scaler.transform(X_test) # Transform test set using the scaler object fitted on train set.<br/>pca = PCA(n_components=4) # Creating pca object with 4 Principal components<br/>pca.fit_transform(X_train_scaled)<br/>pd.DataFrame({'lab':['PC1','PC2','PC3','PC4'],'val':pca.explained_variance_ratio_}).plot.bar(x='lab',y='val',rot=0);</span></pre><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/38ccc8b7d1a04befb93c42db30454f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*c_GjuBMFDu4oWKvcAA9KVQ.png"/></div></figure><p id="ba82" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">要点记住:</strong></p><ul class=""><li id="6521" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated"><strong class="is hj">解释方差比率</strong>用于确定充分提供原始特征所含信息的主成分数量。</li><li id="842e" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">在上面的例子中，前两个主成分解释了&gt; 90%的特征变化，因此可以去掉其余的两个主成分。</li><li id="6382" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><strong class="is hj"> n_components: </strong>该参数可以用来限制主成分的个数。</li><li id="b843" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">例如:在1000个特征的情况下，使用n_components参数，我们可以通过使用<strong class="is hj">explained _ variance _ ratio _</strong>值将特征的数量限制为最佳值。</li><li id="7f99" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><strong class="is hj">此外，由于计算限制导致内存错误问题，PCA在高维稀疏矩阵上效果不佳。在这种情况下，可以使用TruncatedSVD。</strong></li></ul><p id="f289" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">PCA的另一种变体也是可用的，KPCA即KernelPCA，其中内核参数作为参数传递。使用最多的核是“RBF ”,它将原始特征映射到非线性函数，这与PCA是线性变换相反。</strong></p><h1 id="8406" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">线性判别分析:</h1><p id="61e6" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">顾名思义，LDA是一种有监督的线性变换技术。在将高维特征映射到低维时，它试图为因变量保留尽可能多的判别能力。PCA和LDA都是线性技术，但是与LDA相反，PCA不考虑类别标签，LDA基于最大化不同类别之间的分离的方法。</p><p id="caee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，让我们来看二进制类问题。</p><ul class=""><li id="0456" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">创建一个新轴，并将数据投影到轴上，以最大化两个类之间的间隔。</li><li id="87c9" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">使用新绘制的数据点计算两个类别的平均值和方差。</li><li id="de58" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">现在最大化两个类之间的分离:</li><li id="2360" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">两个等级的平均值应尽可能接近，即平均值之间的差值应最大。</li><li id="e895" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">两个类别的方差之和应该尽可能低，即方差之和的值越低，每个类别的数据点越紧凑，类别之间的间隔也越大。LDA将这种方差最小化称为分散。</li><li id="0835" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">新创建的轴称为线性判别式，它是最适合的线，可以最大化平均值之间的距离，最小化分散。</li></ul><p id="54c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">在多类问题的情况下，假设3类问题</strong></p><ul class=""><li id="5137" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">LDA将创建两个轴来分隔3个类。</li><li id="1b0f" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">这里，它将从选择每个类的质心点开始。</li><li id="036e" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">LDA将计算新平面或轴的特征值和特征向量。</li><li id="7f39" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">本征向量将定义轴的方向性，而本征值定义它所携带的关于原始数据分布的信息量。值越低，它携带的信息越少。</li><li id="6a0a" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">它试图优化线性方程，使平均值之间的距离最大化，分散最小化。</li></ul><p id="60eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">要点记住:</strong></p><ul class=""><li id="9245" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated"><strong class="is hj">由于LDA考虑了因变量，如果数据不是正态分布，可能会给出有偏差的结果。</strong></li><li id="bb4b" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><strong class="is hj">在LDA中，特征缩放不影响LDA的结果。因此特征缩放是可选的。</strong></li></ul><pre class="lg lh li lj fd lp lq lr ls aw lt bi"><span id="ce1f" class="lu jp hi lq b fi lv lw l lx ly"># lets start with importing the required libraries<br/>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA<br/>lda  = LDA(n_components=2)<br/>X_new = lda.fit_transform(X_train,y_train)<br/>pd.DataFrame({'lab':['LD1','LD2'],'val':lda.explained_variance_ratio_}).plot.bar(x='lab',y='val',rot=0);</span></pre><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/f8588424a7af5bdb2ee013c195afc51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*6LIx4bxOeUrmobMox7x98A.png"/></div></figure><p id="4b54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">修卡</strong></p><ul class=""><li id="1dfa" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">这里，即使是单一的线性判别式也能解释&gt; 95%的方差。</li></ul><h1 id="b847" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">让我们看看LDA Vs PCA</h1><pre class="lg lh li lj fd lp lq lr ls aw lt bi"><span id="7ed1" class="lu jp hi lq b fi lv lw l lx ly">#Plotting PCA Plot &amp; LDA Plot<br/>lda_plot = pd.DataFrame(X_new) <br/>lda_plot[2] = y_train # Adding label column in lda_plot dataframe<br/>plt.figure(figsize=(10,5)) # Defining figure size<br/>plt.subplot(1,2,1) # Initiating 1st Plot<br/>plt.scatter(lda_plot[0],lda_plot[1],c=lda_plot[2]) # Plotting scatter plot for LD1 vs LD2<br/>plt.title('Linear Discriminant Analysis')<br/>plt.xlabel('LD1')<br/>plt.ylabel('LD2')</span><span id="33b8" class="lu jp hi lq b fi lz lw l lx ly">pca_plot = pd.DataFrame(pca_mod[:,:2])<br/>pca_plot[2] = y_train<br/>plt.subplot(1,2,2)# Initiating 2nd Plot<br/>plt.scatter(pca_plot[0],pca_plot[1],c=pca_plot[2]) # Plotting scatter plot for PC1 vs PC2<br/>plt.title('Principal Component Analysis')<br/>plt.xlabel('PC1')<br/>plt.ylabel('PC2')<br/>plt.show()</span></pre><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/fd27079793d48c5ac94dbfd1f067e7f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*oFaKB5VCL7siqQkHjVLZCQ.png"/></div></figure><ul class=""><li id="087f" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated"><strong class="is hj">在PCA图中，主成分沿方差最大的轴定义。</strong></li><li id="acbb" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><strong class="is hj">在LDA图中，线性判别式沿轴定义，以说明最大的类间方差。LD1充分描述了最大类间方差。</strong></li></ul><h1 id="46ec" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">奇异值分解:</h1><p id="442a" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">维基百科将SVD定义为实数或复数矩阵的线性代数因式分解方法。对我们来说，这是另一种帮助减少或分解高维矩阵的方法。并且SVD的这一特性在信号处理、图像压缩、图像恢复、特征脸等各种应用中是有用的。</p><p id="954f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">让我们看看奇异值分解是如何实现的？</strong></p><ul class=""><li id="ac41" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">奇异值分解将高阶矩阵分解成3个矩阵:U，σ&amp; V，其中</li><li id="8d59" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">U &amp; V向量是正交向量。</li><li id="d7c8" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">σ向量是奇异值的对角矩阵。</li><li id="acb5" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">现介绍如下:</li></ul><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/7da09c4aa297864f76f01d0695b52e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TzD8gMaYgk5nQO1W15XE0g.jpeg"/></div></div></figure><h1 id="d57a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">计算奇异值分解</h1><ul class=""><li id="35b0" class="kr ks hi is b it km ix kn jb me jf mf jj mg jn kw kx ky kz bi translated">为了计算SVD，我们需要计算矩阵A与其转置的<strong class="is hj">乘积的<strong class="is hj">特征值&amp;特征向量</strong>以及转置矩阵与矩阵A </strong>的乘积，如下所示。</li><li id="ad79" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">如下推导:</li><li id="c2a6" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">向量U可以用矩阵A与其转置矩阵乘积的特征向量来表示。</li><li id="1a5b" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">向量V可以由转置矩阵与矩阵A的乘积的特征向量来表示</li></ul><p id="40a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae lo" href="https://youtu.be/daHVmoOrLrI" rel="noopener ugc nofollow" target="_blank">参考:南多·德弗雷塔斯的演讲</a></p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/73d5be9b525b6b0b774cb656af8656e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ujSdI8t9SSp2XGICppQJZQ.jpeg"/></div></div></figure><ul class=""><li id="3e11" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated"><strong class="is hj">计算出的A的非零奇异值为非零特征值的+ve平方根。</strong></li><li id="6dd0" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><strong class="is hj">σ的对角线值代表保留的方差，并且总是按降序排列。(与PCA中讨论的概念相似)。</strong></li><li id="eefd" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><strong class="is hj">回到降维，我们可以使用这些值来降维，只考虑解释最大方差的值，并截断其余的值。</strong></li></ul><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/c5f3b1a958e2b8b25770dbc847d5d9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V68LfTH_4by7tkLjv9GQnQ.jpeg"/></div></div></figure><p id="3b1a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们用Python来编码它。</p><p id="d139" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将在同一个IRIS数据集上使用TruncatedSVD API执行降维。</p><pre class="lg lh li lj fd lp lq lr ls aw lt bi"><span id="1bc6" class="lu jp hi lq b fi lv lw l lx ly">from sklearn.decomposition import TruncatedSVD # Importing TruncatedSVD API from sklearn library<br/>svd = TruncatedSVD(n_components=3) # Defining TruncatedSVD Object<br/>svd.fit_transform(X_train_scaled); # Calling fit_transform method on X_train_scaled Dataset<br/>pd.DataFrame({'Components':['C1','C2','C3'],'Variance':svd.explained_variance_ratio_}).plot.bar(x='Components',y='Variance',rot=0);</span></pre><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/e581a3ff12c6b5547541a772a23b61c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*MLyX_X5sSYN-WHib6GOeCw.png"/></div></figure><ul class=""><li id="8fef" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">在TruncatedSVD API中，我们可以传递<strong class="is hj"> n_components </strong>参数，该参数应该总是小于独立特征的数量。(在上述情况下，特征数量为4，因此n_components应小于或等于3)</li><li id="b996" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">为了确定<strong class="is hj"> n_components </strong>参数值，我们可以使用scree图并确定解释最大方差的组件数量。</li><li id="64b8" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">与PCA相反，TruncatedSVD对<strong class="is hj">稀疏矩阵</strong>有效。详细解释请参考sklearn文档。</li><li id="9b7f" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">除了TruncatedSVD，我们可以使用sklearn的<strong class="is hj"> randomized_svd </strong> API，它使用随机化来近似截断奇异值分解，以加速计算。</li></ul><p id="c79c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，在结束之前，我将提到另一种基于奇异值分解的降维技术，称为<strong class="is hj">【LSA】</strong>。</p><ul class=""><li id="2a61" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">TruncatedSVD在NLP中用于缩减TF-IDF(术语频率-逆文档频率)矩阵。</li><li id="0044" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">TF-IDF表示给定文档中单词的频率&amp;单词在语料库中的稀有程度。</li><li id="b012" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">潜在语义分析进一步对简化的矩阵应用余弦相似性，并且基于其值使用余弦相似性来查找相似的文档(值越接近1表示相似性越高)。</li></ul></div></div>    
</body>
</html>