<html>
<head>
<title>Introduction To Linear Regression — E-commerce Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归简介—电子商务数据集</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-linear-regression-e-commerce-dataset-cfa65b2c1213?source=collection_archive---------13-----------------------#2019-09-25">https://medium.com/analytics-vidhya/introduction-to-linear-regression-e-commerce-dataset-cfa65b2c1213?source=collection_archive---------13-----------------------#2019-09-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/bfbffa1c1db41cb6f16eeacf8dde7f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*WyeS_iEva6Yg5nvuGVrxlQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">线性回归模型</figcaption></figure></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><p id="0330" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">在本帖中，我们将了解什么是线性回归，它背后的一点数学知识，并尝试在电子商务数据集上拟合一个线性回归模型。</p><h1 id="b4da" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">线性回归</h1><p id="1470" class="pw-post-body-paragraph ix iy hi iz b ja kt jc jd je ku jg jh ji kv jk jl jm kw jo jp jq kx js jt ju hb bi translated">维基百科说..<strong class="iz hj">线性回归</strong>是一种对标量响应(或因变量)和一个或多个解释变量(或自变量)之间的关系进行建模的线性方法。一个解释变量的情况称为<strong class="iz hj">简单线性回归</strong>。对于一个以上的解释变量，这个过程叫做<strong class="iz hj">多元线性回归。</strong>'</p><p id="2ef1" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">更通俗地说，就是用线性回归模型来预测变量或因素之间的关系。被预测的因子称为<strong class="iz hj"> </strong>标量响应(或因变量)。用于预测因变量的值的因素称为解释变量(或自变量)。</p><p id="18df" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">线性回归模型在现实世界中有许多应用，例如预测公司的增长、预测产品销售、预测个人可能投票给谁、根据体重预测血糖水平等等。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><p id="bc2c" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">现在让我们来看看线性回归背后的数学。</p><p id="970e" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">如果<strong class="iz hj"> y </strong>表示我们要预测的因变量，<strong class="iz hj"> x </strong>表示用来预测<strong class="iz hj"> y，</strong>的自变量，那么它们之间的数学关系可以写成</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/cbe57178521bf8fd683307ca78df2910.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*4hcxmRRavMLslVMvb0xBew@2x.png"/></div></figure><p id="d258" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">注意，这个方程是一条直线的方程！。</p><p id="2383" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">当我们有n个独立变量时，方程可以写成</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/feb0d615e56b51a5d7fa2771d76000ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*b9IlisWgOg9nXwY_9EsNcQ@2x.png"/></div></figure><p id="713b" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">这里<strong class="iz hj"> c </strong>表示y轴截距(直线与y轴相交的点)<strong class="iz hj"> m </strong>表示自变量<strong class="iz hj"> x </strong>的斜率。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/681fc587f1b31bbe6ad8bd9a0b1f64f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eiyjaIDrF3f4DYovwc_KtA@2x.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">m = 2/3，c = 1。y = (2/3)x + 1</figcaption></figure><p id="b82f" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">上图显示了一条直线，其方程为y = (2/3)x + 1。我们通过计算斜率(2/3)和<strong class="iz hj"> c </strong>为1(它在1处切割y轴)得到值<strong class="iz hj"> m </strong>。</p><p id="f39e" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">所以基本上，如果我们有一个包含因变量和自变量的方程，我们可以通过替换自变量的值来预测因变量。</p><p id="ddca" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">我们的目标是找到使<strong class="iz hj"> yₐ </strong>(实际)和<strong class="iz hj"> yᵢ </strong>(预测)之间的差异最小化的<strong class="iz hj"> <em class="lj"> m </em> </strong>和<strong class="iz hj"> c </strong>的值。</p><p id="11e5" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">一旦我们得到这两个参数的最佳值，我们就有了最佳拟合的<strong class="iz hj">线</strong>，我们可以用它来预测y的值，给定x的值。</p><p id="3cf1" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">为了最小化<strong class="iz hj"> yₐ </strong>和<strong class="iz hj"> yᵢ </strong>之间的差异，我们使用了<strong class="iz hj">最小二乘法的方法。</strong></p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h2 id="c6ed" class="lk jw hi bd jx ll lm ln kb lo lp lq kf ji lr ls kj jm lt lu kn jq lv lw kr lx bi translated">最小二乘法</h2><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/85653d43b820d85ab6005ee196a75fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KyQ3Wb7hEi_EZjw8cZctcw@2x.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">显示实际数据和模型线之间距离的图表</figcaption></figure><p id="f843" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">最小二乘法帮助我们找到最佳拟合线。通过保持<strong class="iz hj"> yₐ </strong>(实际)和<strong class="iz hj"> yᵢ </strong> <em class="lj"> </em>(预测)之间的平方差之和最小化<em class="lj">，可以找到<strong class="iz hj"> m </strong>(斜率)和<strong class="iz hj"> c </strong>(截距)的值。</em></p><p id="0f39" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">我们将展示找到最佳拟合<strong class="iz hj">线的<strong class="iz hj"> m </strong>和<strong class="iz hj"> c </strong>的步骤。</strong></p><p id="cf6b" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">步骤1: </strong>计算<em class="lj"> x </em>值的平均值和<em class="lj"> y值</em>的平均值</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/d1857707f3b78b73937f52c6626431ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*hTw3Thmaz4Cvfhkx6f4g4w@2x.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">x和y值的平均值</figcaption></figure><p id="c86c" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">第二步:</strong>下面的公式给出了最佳拟合直线的<strong class="iz hj"> m </strong>(斜率)。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/bb262644877bc8060937654a9e10c891.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*H--IEOtN-qccGB8cBNl1Kw@2x.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">公式求<strong class="bd jx"> m </strong>(斜率)</figcaption></figure><p id="d31d" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">第三步:</strong>用公式计算直线的值<em class="lj"/><strong class="iz hj"><em class="lj">c</em></strong><em class="lj">(y</em>-截距):</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/635e6751b076056fb19506955707562b.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*fCGA8U5x-TXYRniJLzdr9g.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">找到<strong class="bd jx"> c </strong>的公式(y轴截距)</figcaption></figure><p id="8453" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj">第四步:</strong>用斜率<strong class="iz hj"> <em class="lj"> m </em> </strong> <em class="lj"> </em>和<em class="lj">y</em>-截距<strong class="iz hj"> c </strong>组成直线的方程。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><p id="2998" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">对于一个庞大的数据集来说，这需要大量的等式和计算。当所有这些计算都要完成的时候，我们真的想预测一些东西吗？？。别担心，python和它的库是来拯救世界的！</p><h1 id="5a9a" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">用<code class="du mb mc md me b">scikit-learn</code>进行线性回归</h1><p id="a041" class="pw-post-body-paragraph ix iy hi iz b ja kt jc jd je ku jg jh ji kv jk jl jm kw jo jp jq kx js jt ju hb bi translated"><code class="du mb mc md me b"><a class="ae mf" href="http://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank">scikit-learn</a></code>是一个开源的python模块，提供简单高效的数据挖掘和数据分析工具，构建在NumPy、SciPy和matplotlib之上。</p><p id="21eb" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">让我们对电子商务客户数据使用<code class="du mb mc md me b">scikit-learn</code>实现一个线性回归模型。</p><p id="eabc" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">我们希望预测客户在电子商务平台上的“年消费金额”,以便这些信息可以用于为特定客户提供个性化优惠或忠诚会员等。</p><p id="e463" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">因此,“每年花费的金额”就是这里的<strong class="iz hj">因变量</strong>。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="b84e" class="lk jw hi me b fi mk ml l mm mn"># Importing required libraries<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn import metrics<br/>import warnings<br/>warnings.filterwarnings('ignore')<br/>%matplotlib inline</span><span id="0c41" class="lk jw hi me b fi mo ml l mm mn">customers = pd.read_csv('Ecomm-Customers.csv')<br/>customers.info()</span></pre><p id="205a" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">customers.info()给出了下面的输出，基本上给出了数据集的概述。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="0b6d" class="lk jw hi me b fi mk ml l mm mn">&lt;class ‘pandas.core.frame.DataFrame’&gt;<br/>RangeIndex: 500 entries, 0 to 499<br/>Data columns (total 8 columns):<br/>Email 500 non-null object<br/>Address 500 non-null object<br/>Avatar 500 non-null object<br/>Avg. Session Length 500 non-null float64<br/>Time on App 500 non-null float64<br/>Time on Website 500 non-null float64<br/>Length of Membership 500 non-null float64<br/>Yearly Amount Spent 500 non-null float64<br/>dtypes: float64(5), object(3)<br/>memory usage: 31.4+ KB</span></pre><p id="ddab" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">接下来，我们将检查数据集中的一些行的外观。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="7cbf" class="lk jw hi me b fi mk ml l mm mn">customers.head()</span></pre><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es mp"><img src="../Images/2febc68ab2bf51a8589aab415844c37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jM-agKVVQVjr-eFTQu9Y7A.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae mf" href="https://github.com/fahadanwar10/LinearRegression/blob/master/Ecomm-Customers.csv" rel="noopener ugc nofollow" target="_blank"> Ecomm-Customers.csv </a></figcaption></figure><p id="9c7e" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">使用pairplot查看各列之间是否存在与“每年花费金额”相关的某种关联。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="6048" class="lk jw hi me b fi mk ml l mm mn">sns.pairplot(customers)</span></pre><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/2ef3b04fe10d759cc122911ae2fe7789.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*DmaINJ7RTroUZ-RgGHcMjg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">数据集的配对图</figcaption></figure><p id="8fa3" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">我们关注的是“每年花费的金额”，所以我用红色突出显示了最明显的变量(“会员时间”和“应用时间”)，它们与因变量呈正相关。</p><p id="7fef" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">现在让我们使用热图，看看是否有更多的变量需要考虑。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="0427" class="lk jw hi me b fi mk ml l mm mn">sns.heatmap(customers.corr(), linewidth=0.5, annot=True)</span></pre><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es mr"><img src="../Images/d8fade5d0199a643861670fea2d86b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AK5DSypOhGZLIAqG8JeIgQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">数据集的热图</figcaption></figure><p id="b17f" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">除了已知的变量，我们可以看到还有一个可能的变量(Avg。会话长度)，这有助于预测因变量。</p><p id="2c07" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">目前，让我们坚持使用与因变量相关程度更高的变量</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="4920" class="lk jw hi me b fi mk ml l mm mn">x = customers[['Time on App', 'Length of Membership']]<br/>y = customers['Yearly Amount Spent']</span></pre><p id="a9ff" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">我们希望稍后测试我们的模型，所以让我们将数据集分成训练和测试数据。我们将使用训练数据来拟合我们的模型，并使用测试数据来测试我们的模型。通常我们保留30%作为测试数据，70%作为训练数据。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="ae53" class="lk jw hi me b fi mk ml l mm mn">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 50)</span></pre><p id="533c" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">现在是线性回归模型。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="d833" class="lk jw hi me b fi mk ml l mm mn">lm = LinearRegression()<br/>lm.fit(x_train, y_train)</span></pre><p id="ab66" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">从库中使用线性回归就是这么简单。我们之前讨论的所有方程，都是在我们写<em class="lj"> lm.fit(x_train，y_train)时执行的。</em>很整洁吧！</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="939f" class="lk jw hi me b fi mk ml l mm mn">print("Coeffs are Time on App : {0} , Length of Membership: {1}".format(lm.coef_[0], lm.coef_[1]))<br/>print("Intercept : ",lm.intercept_)</span></pre><p id="370c" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">输出:</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="9197" class="lk jw hi me b fi mk ml l mm mn">Coeffs are Time on App : 37.24859675165942 , Length of Membership: 62.76419727475292<br/>Intercept :  -172.29634898449677</span></pre><p id="bc9e" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">系数就是我们想要计算的<strong class="iz hj"> m </strong>(斜率)值，截距就是<strong class="iz hj"> c </strong>值。现在我们已经拟合了我们的线性回归模型，让我们得到预测结果！。</p><p id="e999" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">我们对测试数据使用预测函数来获得因变量(每年花费的金额)的预测值。然后，我们将绘制一个散点图，显示“每年花费金额”的测试(实际)值和预测值。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="6ad1" class="lk jw hi me b fi mk ml l mm mn">result = lm.predict(x_test)<br/>plt.scatter(y_test, result)<br/>plt.xlabel("Actual values")<br/>plt.ylabel("Predicted values")</span></pre><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/bcc6e62cbfbfeae77054358c72e915ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*wnPyWWugNvN_HuUZlHl8YQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">预测值与实际值</figcaption></figure><p id="868d" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">现在是时候弄清楚我们的预测模型有多好了。我们有一些衡量标准来发现模型的效果如何。</p><p id="b04c" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">我们可能会在下一篇文章中详细介绍这些指标。目前，我已经粘贴了下面的链接供进一步阅读。</p><ul class=""><li id="8c3e" class="mt mu hi iz b ja jb je jf ji mv jm mw jq mx ju my mz na nb bi translated"><a class="ae mf" href="https://en.wikipedia.org/wiki/Coefficient_of_determination" rel="noopener ugc nofollow" target="_blank"> R2得分</a></li><li id="1331" class="mt mu hi iz b ja nc je nd ji ne jm nf jq ng ju my mz na nb bi translated"><a class="ae mf" href="https://www.investopedia.com/terms/v/variance.asp" rel="noopener ugc nofollow" target="_blank">差异</a></li><li id="8bc2" class="mt mu hi iz b ja nc je nd ji ne jm nf jq ng ju my mz na nb bi translated"><a class="ae mf" href="https://www.statisticshowto.datasciencecentral.com/mean-squared-error/" rel="noopener ugc nofollow" target="_blank">均方误差</a></li></ul><p id="bca2" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">还有更多的指标可以使用，回归指标的完整列表可以在<a class="ae mf" href="https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="eb1c" class="lk jw hi me b fi mk ml l mm mn">print(‘R2 score : ‘,metrics.r2_score(y_test, result))<br/>print(‘Variance: ‘,metrics.explained_variance_score(y_test,result))<br/>print(‘MSE: ‘, metrics.mean_squared_error(y_test,result))</span></pre><p id="292f" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">输出:</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="875d" class="lk jw hi me b fi mk ml l mm mn">R2 score :  0.8881506494029392<br/>Variance:  0.8895559640312203<br/>MSE:  711.9352710839121</span></pre><p id="19a2" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">R2分数越高越好，MSE越低越好。从价值观来看，似乎还有改进的余地。</p><p id="748b" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">还记得我们遗漏了一个正相关程度较低的变量吗？让我们添加那个变量(Avg。会话长度)并看看它是否改进了我们的模型。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="baef" class="lk jw hi me b fi mk ml l mm mn">x = customers[['Time on App', 'Length of Membership','Avg. Session Length']]</span></pre><p id="44b3" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">像前面一样分割数据集。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="48d4" class="lk jw hi me b fi mk ml l mm mn">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 50)</span></pre><p id="86f6" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">让我们来拟合模型</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="f396" class="lk jw hi me b fi mk ml l mm mn">lm.fit(x_train, y_train)</span></pre><p id="cea1" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">让我们检查一下<strong class="iz hj"> m </strong>和<strong class="iz hj"> c </strong>的值。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="a6ef" class="lk jw hi me b fi mk ml l mm mn">print("Coeffs are Time on App : {0} , Length of Membership: {1} , Avg. Session Length: {2}".format(lm.coef_[0], lm.coef_[1], lm.coef_[2]))<br/>print("Intercept : ",lm.intercept_)</span></pre><p id="169d" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">输出:</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="85aa" class="lk jw hi me b fi mk ml l mm mn">Coeffs are Time on App : 38.74012697347563 , Length of Membership: 61.779801807105294 , Avg. Session Length: 25.66375684798914<br/>Intercept :  -1034.1551554733614</span></pre><p id="6618" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">上述多元线性回归模型方程得出为</p><p id="302f" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">y = 38.74012697347563 *(<strong class="iz hj">App上的时间</strong> ) +61.779801807105294*( <strong class="iz hj">会员时长</strong> ) + 25.66375684798914*( <strong class="iz hj">平均。会话长度</strong>)-18960.688686888617</p><p id="5e81" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">现在我们已经拟合了模型，让我们来看看预测值和实际值之间的图表。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="a564" class="lk jw hi me b fi mk ml l mm mn">result = lm.predict(x_test)<br/>plt.scatter(y_test, result)<br/>plt.xlabel("Actual values")<br/>plt.ylabel("Predicted values")</span></pre><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/4aeda23d92bc732c9956e9101e21ef49.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*vTLRdD3FD799KJwdU4sIBw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">预测值与实际值</figcaption></figure><p id="a156" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">现在，这看起来比之前的图表更精简，这意味着预测值和实际值更加接近。我们已经知道R2分数会更高，MSE会更低。</p><p id="1d98" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">让我们看看我们的模型这次进展如何。</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="747c" class="lk jw hi me b fi mk ml l mm mn">print('R2 score : ',metrics.r2_score(y_test, result))<br/>print('Variance: ',metrics.explained_variance_score(y_test,result))<br/>print('MSE ', metrics.mean_squared_error(y_test,result))</span></pre><p id="d738" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">输出:</p><pre class="kz la lb lc fd mg me mh mi aw mj bi"><span id="b777" class="lk jw hi me b fi mk ml l mm mn">R2 score :  0.9813533752076671<br/>Variance:  0.9813536018865052<br/>MSE  118.68812653328345</span></pre><p id="7d03" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">这是R2分数(0.88 -&gt; 0.98)和MSE (711.93 -&gt; 118.68)的显著提高，增加了一个新的变量。</p><p id="671c" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">因此，添加列“平均”。“会话长度”极大地改进了我们的模型，尽管它与因变量几乎没有正相关性。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><p id="95a4" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">这是对Python中线性回归的一个快速介绍。我希望你喜欢这篇文章，并继续关注我的文章。</p><p id="cea9" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">完整的Jupyter笔记本以及数据集csv文件可以在我的<a class="ae mf" href="https://github.com/fahadanwar10/LinearRegression" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。请随时查看，并在回复中对模型提出改进建议。</p><p id="dd13" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">感谢您的阅读！</p><p id="b4ff" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj"> <em class="lj">(这是我在Medium上的第一篇帖子。请随意表达你的爱，当然也请给予反馈。)</em> </strong></p><p id="a14d" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">我的其他帖子:</p><div class="ni nj ez fb nk nl"><a rel="noopener follow" target="_blank" href="/@fahadanwar10/gradient-descent-intro-and-implementation-in-python-8b6ab0557b7c"><div class="nm ab dw"><div class="nn ab no cl cj np"><h2 class="bd hj fi z dy nq ea eb nr ed ef hh bi translated">梯度下降python中的介绍和实现</h2><div class="ns l"><h3 class="bd b fi z dy nq ea eb nr ed ef dx translated">梯度下降是机器学习中的一种优化算法，用于通过迭代移动…</h3></div><div class="nt l"><p class="bd b fp z dy nq ea eb nr ed ef dx translated">medium.com</p></div></div><div class="nu l"><div class="nv l nw nx ny nu nz ik nl"/></div></div></a></div><p id="b2f3" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">此外，检查我的博客，并订阅它，以获得内容之前，你看到它在这里。<a class="ae mf" href="https://machinelearningmind.com/" rel="noopener ugc nofollow" target="_blank">https://machinelearningmind.com/</a></p></div></div>    
</body>
</html>