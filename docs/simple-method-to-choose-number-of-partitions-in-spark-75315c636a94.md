# Spark 中选择分区数的简单方法

> 原文：<https://medium.com/analytics-vidhya/simple-method-to-choose-number-of-partitions-in-spark-75315c636a94?source=collection_archive---------1----------------------->

在本文结束时，您将能够分析您的 spark 作业，并确定您是否拥有适用于您的 Spark 环境的正确配置设置，以及您是否利用了所有资源。

每当你从事 spark 工作时，你应该考虑两件事。

*   避免溢出
*   通过利用所有内核最大限度地提高并行性。

这两者是相辅相成的，你应该能够充分利用它们。

**溢出:** 每当出现洗牌，数据必须四处移动，而执行器无法将数据保存在其内存中时，就会发生溢出。所以它必须使用存储器将数据保存在内存中一段时间。
如果分区大小不对，就会出现溢出。务必避免溅出。对于读取 50 GB，溢出可能高达 500 GB。

**分区:**

让我们从一些基本的默认和期望的火花配置参数开始。

*   默认火花混洗分区— 200
*   期望的分区大小(目标大小)= 100 或 200 MB
*   分区数量=输入阶段数据大小/目标大小

下面是如何选择分区计数的示例。

**案例 1** :

*   输入阶段数据 100GB
*   目标大小= 100MB
*   核心数= 1000
*   最佳分区数= 100，000 MB / 100 = 1000 个分区
*   spark . conf . set(" spark . SQL . shuffle . partitions "，1000)
*   **分区数量不应少于核心数量**

**案例二**:

*   输入大小数据— 100GB
*   目标大小= 100MB
*   核心数= 96
*   最佳分区数= 100，000 MB / 100 MB = 1000 个分区
*   spark . conf . set(" spark . SQL . shuffle . partitions "，960)
*   当分区数大于核心数时，分区数应该是核心数的一个因子。否则，我们在最后一次运行中就不会使用内核。

**输入:**

*   读取与您的核心数匹配的分区数的输入数据
*   spark . conf . set(" spark . SQL . files . maxpartitionbytes "，1024 * 1024 * 128)-将分区大小设置为 128 MB
*   应用此配置，然后读取源文件。它会将文件分割成 128MB 的倍数
*   验证 df.rdd.partitions.size
*   通过这样做，我们将能够非常快地读取文件

**输出:**

*   保存数据时，尽量利用所有内核。
*   如果分区的数量与内核数量相匹配，或者是内核数量的一个因素，我们将实现并行性，从而减少时间。

**外卖:**

*   分区太少会导致并发性降低。
*   分区太多会导致很多洗牌。
*   分区数量通常在 100 到 10，000 之间。
*   下限:群集中至少 2 倍数量的核心。
*   上限:确保任务至少需要 100ms。