<html>
<head>
<title>Logistic Regression with Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">有划痕的逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-with-scracth-c23b9afb04b2?source=collection_archive---------28-----------------------#2020-12-29">https://medium.com/analytics-vidhya/logistic-regression-with-scracth-c23b9afb04b2?source=collection_archive---------28-----------------------#2020-12-29</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="er es ie"><img src="../Images/ebf760f380252c9ab14986167a8bc4b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*PWA6A99Nb7qHkOapGFJkpA.png"/></div></figure><p id="f342" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">你好，欢迎来到我们的 Scratch 逻辑回归文章，这是我们的 Scratch 机器学习系列的第二篇文章。如果您需要检查我们将使用的数据，您可以从链接<a class="ae jj" href="https://github.com/capogluuu/ml_scratch/tree/main/Logistic%20Regression" rel="noopener ugc nofollow" target="_blank"> <strong class="in hi">这里的</strong> </a>访问数据。</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jo jp l"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">数据集的样本零件</figcaption></figure><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es ju"><img src="../Images/b39674d2b6821168e336a7cd394a2f0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*vihKmFd7I5fQV0Ec7YaD2Q.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">df.describe()</figcaption></figure><h1 id="d292" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">逻辑回归</h1><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es kt"><img src="../Images/c5b8365670b63da82ee78e7d06190616.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*laimBTHKZY3hhxS4rkwDAg.png"/></div></figure><p id="ebad" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们在逻辑回归算法中的主要目标是计算具有某些属性的元素属于哪个分类变量的比率。为了进行这种计算，我们将使用类似于线性模型的链接“b0 + w1 * x1 + w2 * x2”。我们的目标是找到最佳的 b0、w1 和 w2 变量。为了找到这些值，我们执行以下操作:</p><ol class=""><li id="3eb4" class="ku kv hh in b io ip is it iw kw ja kx je ky ji kz la lb lc bi translated">加载数据集</li><li id="cb40" class="ku kv hh in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">特征缩放</li><li id="dd64" class="ku kv hh in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">模型拟合</li><li id="9b54" class="ku kv hh in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">梯度下降</li><li id="5f10" class="ku kv hh in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">测定测试和控制数据</li><li id="6fd8" class="ku kv hh in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">预言；预测；预告</li><li id="02e7" class="ku kv hh in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">衡量预测结果</li></ol><h2 id="0298" class="li jw hh bd jx lj lk ll kb lm ln lo kf iw lp lq kj ja lr ls kn je lt lu kr lv bi translated">最小-最大缩放</h2><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/05d46fb2e834b32322ee9287445807ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*9aZrNPx1f5cBThwTDdri8g.png"/></div></figure><p id="62b5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">根据最小-最大缩放方法，数据在 0 和 1 之间缩放。这样，我们可以在进行精确计算(如梯度下降)的部分提高效率。</p><h2 id="ab98" class="li jw hh bd jx lj lk ll kb lm ln lo kf iw lp lq kj ja lr ls kn je lt lu kr lv bi translated">失去的功能</h2><p id="992f" class="pw-post-body-paragraph il im hh in b io lx iq ir is ly iu iv iw lz iy iz ja ma jc jd je mb jg jh ji ha bi translated">损失函数是衡量模型预测值与真实值的接近程度或距离的测量单位。在这段代码中，我们使用了下面的对数函数来最小化。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/acf57abe36a91c56919085a7ddc9c30c.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*dFv1jhgBQCho8_KvBlaGgA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">失去的功能</figcaption></figure><p id="83d4" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">h (X)是我们估计的值，而 y 是训练数据中的值。</p><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es md"><img src="../Images/f09778b7159763344d1373c5d8641333.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*B0tJIsOnqL4cANulGlAw3g.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><strong class="bd jx">矢量化丢失功能</strong></figcaption></figure><h2 id="a1e5" class="li jw hh bd jx lj lk ll kb lm ln lo kf iw lp lq kj ja lr ls kn je lt lu kr lv bi translated">梯度下降</h2><p id="d484" class="pw-post-body-paragraph il im hh in b io lx iq ir is ly iu iv iw lz iy iz ja ma jc jd je mb jg jh ji ha bi translated">梯度下降是一种优化算法，它将最小化我们的损失函数。梯度下降的主要目的是找到对我们最好的系数。因为我们不能像人一样手动确定，所以我们让计算机来完成这个过程。计算机也在你梯度的相反方向上不断地小步前进，最终达到一个局部最小值。这样，我们就可以得到系数。</p><p id="d5ff" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">梯度下降算法要做的事情</p><ol class=""><li id="37cc" class="ku kv hh in b io ip is it iw kw ja kx je ky ji kz la lb lc bi translated">确定起始重量</li><li id="8745" class="ku kv hh in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">对每个参数的成本函数求导</li><li id="813b" class="ku kv hh in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated">更新重量和偏差值</li></ol><p id="1f14" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们建议您观看吴恩达的视频，以便更好地理解梯度下降。</p><p id="9709" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated"><a class="ae jj" href="https://www.youtube.com/watch?v=uJryes5Vk1o" rel="noopener ugc nofollow" target="_blank"> <strong class="in hi">渐变下降(C1W2L04) — YouTube </strong> </a></p><h2 id="8b4a" class="li jw hh bd jx lj lk ll kb lm ln lo kf iw lp lq kj ja lr ls kn je lt lu kr lv bi translated">预言；预测；预告</h2><figure class="jk jl jm jn fd ii er es paragraph-image"><div class="er es me"><img src="../Images/892d7e1e632ba92fcce3ed133f5635fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*EaFzmiGE5Hpp9LO7GFh3sw.png"/></div></figure><p id="6ebb" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">对于我们的函数，我们使用通过梯度下降确定的最佳 b0 和 b1 值。下一步，用我们上面看到的 1/1 + e ^ -z 函数，我们决定哪个分类变量更接近我们正在处理的样本。作为这个过程的结果，我们根据概率(在 0-1 之间)决定它将属于哪个类别。我将在代码中介绍这些细节。继续阅读😊</p><h2 id="8154" class="li jw hh bd jx lj lk ll kb lm ln lo kf iw lp lq kj ja lr ls kn je lt lu kr lv bi translated">让我们编码</h2><ol class=""><li id="d67a" class="ku kv hh in b io lx is ly iw mf ja mg je mh ji kz la lb lc bi translated">首先，让我们导入必要的库</li></ol><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jo jp l"/></div></figure><p id="e3ec" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">2.我们上传我们的数据，并在预处理后将它们提供给模型😊</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jo jp l"/></div></figure><p id="45da" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在这段代码中，我们看到了选择将要处理的列的阶段，将它们保留在预处理阶段，并将它们呈现给模型。最后，我们通过比较模型的结果来衡量我们的成功率。</p><p id="20ba" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">3.让我们编码我们的类结构</p><figure class="jk jl jm jn fd ii"><div class="bz dy l di"><div class="jo jp l"/></div></figure><p id="32f7" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">我们通过在 __init__ 函数中进行预定义来开始我们的类结构。您可能不熟悉这里的 control_level 变量。Control_level 告诉我们，在计算了测试数据集中的样本属于哪个类别的概率之后，应该使用哪个波段来进行估计。为了用一个例子来解释这一点，如果我们的 control_level 值是 0.65，并且作为运算的结果，我们有一个概率值 0.60，我们可以说这个变量属于类别 0 和 1 中的 0，因为它是 0.65&gt; 0.60。</p><p id="a5d5" class="pw-post-body-paragraph il im hh in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji ha bi translated">在定义它们之后，我们试图用梯度下降步骤找到最佳参数，然后用我们找到的参数预测我们的结果。</p><h2 id="3b91" class="li jw hh bd jx lj lk ll kb lm ln lo kf iw lp lq kj ja lr ls kn je lt lu kr lv bi translated">这就是这篇文章的内容😊</h2><h2 id="ed6a" class="li jw hh bd jx lj lk ll kb lm ln lo kf iw lp lq kj ja lr ls kn je lt lu kr lv bi translated">请访问我的 Github 页面查看完整版代码:<a class="ae jj" href="https://github.com/capogluuu/ml_scratch" rel="noopener ugc nofollow" target="_blank">https://github.com/capogluuu/ml_scratch</a></h2><h1 id="9ad9" class="jv jw hh bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">谢谢你</h1><p id="9e61" class="pw-post-body-paragraph il im hh in b io lx iq ir is ly iu iv iw lz iy iz ja ma jc jd je mb jg jh ji ha bi translated">有用的资源</p><ol class=""><li id="0fa1" class="ku kv hh in b io ip is it iw kw ja kx je ky ji kz la lb lc bi translated"><a class="ae jj" href="https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io/en/latest/logistic _ regression . html</a></li><li id="6f51" class="ku kv hh in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated"><a class="ae jj" href="https://math.stackexchange.com/questions/3287412/updating-weights-in-logistic-regression-using-gradient-descent" rel="noopener ugc nofollow" target="_blank">https://math . stack exchange . com/questions/3287412/updating-weights-in-logistic-regression-using-gradient-descent</a></li><li id="ede6" class="ku kv hh in b io ld is le iw lf ja lg je lh ji kz la lb lc bi translated"><a class="ae jj" href="https://www.youtube.com/playlist?list=PLNeKWBMsAzboR8vvhnlanxCNr2V7ITuxy" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/playlist?list = plnekwbmsazbor 8 vvhnlanxcnr 2 V7 ituxy</a>(吴恩达逻辑回归)</li></ol></div></div>    
</body>
</html>