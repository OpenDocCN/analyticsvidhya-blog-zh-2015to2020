# NSFW åˆ†ç±»å™¨éåˆ¶å®¡æŸ¥å†…å®¹ã€‚

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/nsfw-classifier-to-curb-out-censored-content-d8e88dac165c?source=collection_archive---------5----------------------->

![](img/ef91184d077ff901ef9b2f686f2a908b.png)

[è¿ˆå¡Â·å¨å»‰å§†æ–¯](https://unsplash.com/@mr_williams_photography?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹ç…§

**ä¸ºäº†è®©ç½‘ç»œå¯¹å„¿ç«¥æ›´å®‰å…¨ï¼Œè®¸å¤šç¤¾äº¤åª’ä½“ï¼Œå¦‚ Tumblrã€è„¸ä¹¦ã€Instagramï¼Œéƒ½åœ¨é™åˆ¶ NSFW å†…å®¹ï¼Œæ‰“é€ ä¸€ä¸ªæ›´å®‰å…¨çš„ç¤¾åŒºã€‚**

æœ€è¿‘ï¼Œå°åº¦æ”¿åºœç¦æ­¢é¿å­•å¥—å¹¿å‘Šï¼Œå› ä¸ºå®ƒä»¬å¯¹å„¿ç«¥æ¥è¯´æ˜¯ä¸‹æµçš„ã€‚äºæ˜¯æˆ‘äº§ç”Ÿäº†ä¸€ä¸ªç–‘é—®ï¼Œè¿™äº›å¹¿å‘ŠçœŸçš„ä¸é›…å—ï¼Ÿè®©æˆ‘ä»¬åˆ©ç”¨æ·±åº¦å­¦ä¹ çš„åŠ›é‡æ‰¾åˆ°ç­”æ¡ˆã€‚æ‰€ä»¥è®©æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ **NSFW åˆ†ç±»å™¨æ¥æ‰¾å‡ºå¹¿å‘Šçš„æœ¬è´¨ã€‚**

# **å»ºç«‹æ¨¡å‹çš„æ­¥éª¤:**

1.  æ£€ç´¢æ•°æ®
2.  é¢„å¤„ç†æ•°æ®
3.  è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹
4.  ä½¿ç”¨ OpenCV æµ‹è¯•å¹¿å‘Š
5.  æ”¹è¿›æ¨¡å‹ä»¥å‘ç°å›¾åƒä¸­çš„ç»†èŠ‚

# 1.è·å–æ•°æ®:

è·å–æ•°æ®æ˜¯æœ€é‡è¦çš„ä¸€æ­¥ã€‚ä½†æ˜¯ 30%çš„äº’è”ç½‘å†…å®¹æ˜¯è‰²æƒ…çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ•°æ®ã€‚æˆ‘ä»¬åœ¨ reddit å’Œ 9gag ä¸Šä¹Ÿæœ‰å¾ˆå¤šä¸å®‰å…¨çš„å†…å®¹ã€‚æ‰€ä»¥æˆ‘ä»¬åªéœ€è¦æå–å›¾åƒæ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘åœ¨ Github ä¸Šå‘ç°äº† Alex Kim åˆ¶ä½œçš„è¿™ä¸ªå¾ˆæ£’çš„å›è´­ã€‚æˆ‘ç”¨å®ƒä¸‹è½½äº†ä¸‰ç§å›¾åƒæ•°æ®:è‰²æƒ…ã€ä¸­æ€§å’Œæ€§æ„Ÿã€‚

# 2.é¢„å¤„ç†æ•°æ®:

ç”±äºæˆ‘ä»¬ä»è„šæœ¬ä¸­ä¸‹è½½çš„æ•°æ®å°†è¢«åˆ†ç±»åˆ° 3 ä¸ªä¸åŒçš„æ–‡ä»¶å¤¹ä¸­ï¼Œå®ƒä»¬æœ‰å„è‡ªçš„å›¾åƒï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦åˆ†åˆ«æ ‡è®°å®ƒä»¬ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ Keras ImageDataGenerator çš„å¼ºå¤§åŠŸèƒ½æ¥ç”Ÿæˆæ›´å¤šçš„æ•°æ®ã€‚ç¨åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ flowfromdirectory()å‡½æ•°æ¥å¤„ç†è®­ç»ƒã€‚

```
# File : NSFW.ipynb
train_data_generation **=** ImageDataGenerator(rescale**=**1.**/**255, rotation_range**=**30, width_shift_range**=**0.2, height_shift_range**=**0.2, shear_range**=**0.2, zoom_range**=**0.2, channel_shift_range**=**20, horizontal_flip**=True**)
```

# 3.è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹:

æˆ‘ä»¬å°†åœ¨ Keras ä¸­ä½¿ç”¨è¿ç§»å­¦ä¹ ï¼Œå¹¶ä¸ºæ­¤ä½¿ç”¨ MobileNetV2ã€‚å› ä¸ºå®ƒæœ‰å¾ˆå°‘çš„å‚æ•°ï¼Œéå¸¸é€‚åˆåº”ç”¨ç¨‹åºã€‚ç„¶åæˆ‘åœ¨æœ€åä¸€å±‚å°è¯•äº†ä¸åŒçš„æ¶æ„ï¼Œæˆ‘åœ¨å¯†é›†å±‚ä½¿ç”¨äº† 256 ä¸ªå•ä½å’Œ 128 ä¸ªå•ä½ï¼Œä½†æ˜¯ä»–ä»¬å¼€å§‹è¿‡åº¦é€‚åº”ã€‚ç„¶åï¼Œæœ€åæˆ‘æƒ³å‡ºäº†ä¸€ä¸ª 32 ä¸ªå•ä½çš„å•ä¸€å¯†é›†å±‚ï¼Œæ‰¹é‡æ ‡å‡†åŒ–ï¼Œè¾å­¦ï¼Œå¹¶è·å¾—äº† 93.5%çš„æµ‹è¯•æ•°æ®çš„å‡†ç¡®æ€§ã€‚

```
# File : NSFW.ipynb
conv_m **=** MobileNetV2(weights**=**'imagenet', include_top**=False**, input_shape**=**(size, size, 3)) 
conv_m.trainable **= False** model.add(conv_m) model.add(AveragePooling2D(pool_size**=**(7, 7))) model.add(Flatten())
model.add(Dense(32, activation **=** 'relu')) model.add(BatchNormalization()) model.add(Dropout(0.5))
model.add(Dense(3, activation**=**'softmax'))
```

æˆ‘ä½¿ç”¨äº† ModelCheckpoint å’Œ ReduceLROnPlateau å›è°ƒï¼Œç„¶åä½¿ç”¨äº†å¸¦æœ‰åŠ¨é‡çš„ SGD åˆ†ç±»å™¨ã€‚

```
model.compile( loss**=**'categorical_crossentropy', optimizer**=**SGD(lr **=** 0.1, momentum **=** 0.9), metrics**=**['accuracy'])
```

ç„¶åï¼Œæˆ‘ä»¥ 10 çš„æ­¥é•¿å¯¹å®ƒè¿›è¡Œäº† 100 æ¬¡è®­ç»ƒï¼Œå› ä¸ºæˆ‘æ²¡æœ‰ä»»ä½• GPUï¼Œæ‰€ä»¥æˆ‘èŠ±äº†å¤§çº¦ 7-8 ä¸ªå°æ—¶æ¥è®­ç»ƒ 25GB å¤§å°çš„æ¨¡å‹ã€‚æˆ‘åœ¨ GCP ä¸Šç”¨ 60 GB å†…å­˜è¿è¡Œå®ƒã€‚ä½ å¯ä»¥ä»æˆ‘çš„ Github è·å¾—è®­ç»ƒè¿‡çš„æ¨¡å‹ï¼Œè§æ–‡ç« æœ«å°¾ã€‚

**æˆ‘ä»¬æ¨¡å‹çš„è¾“å‡º:**

```
from PIL import Image
import numpy as np
from skimage import transform
def load(filename):
    np_image = Image.open(filename)
    np_image = np.array(np_image).astype('float32')/255
    np_image = transform.resize(np_image, (224, 224, 3))
    np_image = np.expand_dims(np_image, axis=0)
    img=mpimg.imread(filename)
    plt.imshow(img)
    return np_imageimage = load("selena.jpg")
ans = model.predict(image)
maping = {0 : "Neutral", 1 : "Porn", 2 : "Sexy"}
new_ans = np.argmax(ans[0])print(maping[new_ans], np.round(ans,2))
print("With {} probability".format(ans[0][new_ans]))
```

è¾“å…¥:

![](img/c638c512cb93e77ce84af4d8326c5eca.png)

è¾“å‡º:
æ€§æ„Ÿ[[0.01 0ã€‚0.99]]
ä»¥ 0.98 çš„æ¦‚ç‡

**åœ¨ iPhone ä¸Šéƒ¨ç½²æ¨¡å‹ã€‚**

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ iOS å…¼å®¹å½¢å¼çš„æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ python åº“ coremltoolsã€‚

```
import coremltools
model.author **=** "Lakshay Chhabra"
model.short_description **=** "NSFW Image Classifier"
output_labels **=** ['Neutral', 'Porn', 'Sexy']ios = coremltools.converters.keras.convert(model, input_names=['image'], output_names = ['output'],                       class_labels = output_labels, image_input_names = 'image', image_scale=1/255.0)ios.save('NSFW.mlmodel')
```

æˆ‘ä»¬çš„æ¨¡å‹ä¿å­˜ä¸‹æ¥äº†ï¼Œè®©æˆ‘ä»¬å°†å®ƒåŠ è½½åˆ° swift åº”ç”¨ç¨‹åºä¸­ã€‚
**ç¬¬ä¸€æ­¥:** è®¾è®¡ app çš„å‰ç«¯ï¼Œæˆ‘ä»¬ä¸ºå®ƒä½¿ç”¨äº†æ•…äº‹æ¿ã€‚æˆ‘ä»¬é€‰æ‹©äº†å¸¦æœ‰æ¨¡ç³Šæ•ˆæœçš„å›¾åƒè§†å›¾ï¼Œå› ä¸ºå›¾åƒå°†æ˜¯ NSFWï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å®ƒä»¬éšè—åœ¨æ¨¡ç³Šå±‚ä¸‹ã€‚

**ç¬¬äºŒæ­¥:** æˆ‘ä»¬æ ¹æ®è‡ªå·±çš„éœ€è¦è°ƒæ•´äº†å›¾åƒçš„å¤§å°ã€‚

```
func resizeImage(image: UIImage) -> UIImage {                        var newSize: CGSize 
newSize = CGSize(width: 224, height: 224)
let rect = CGRect(x: 0, y: 0, width: 224, height: 224)                UIGraphicsBeginImageContextWithOptions(newSize, false, 1.0)     image.draw(in: rect)        
let newImage = UIGraphicsGetImageFromCurrentImageContext()        UIGraphicsEndImageContext()               
return newImage!   
 }
```

**ç¬¬ä¸‰æ­¥:** æ‰€ä»¥ç°åœ¨æˆ‘ä»¬å°†ä½¿ç”¨æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹æ¥é¢„æµ‹ç»“æœï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æ¥è‡ª[æ–‡ä»¶](https://github.com/lakshaychhabra/NSFW-ios-ML/blob/master/NSFW%20Detect/ViewController.swift)çš„ä¸€å°æ®µä»£ç ã€‚

```
let request = VNCoreMLRequest(model: model) { (request, error) in            print(request.results!)
guard let classification = request.results?.first as? VNClassificationObservation
else{ 
fatalError("cant find the image")
}
DispatchQueue.main.async {
let confidenceRate = (classification.confidence) * 100                self.output_label.text = "\(confidenceRate)% it's \(String(describing: classification.identifier))"
self.k = self.dict[classification.identifier]!                if(self.k == 0){
self.warning.text = "Safe Image"
}
else if(self.k == 1){
self.warning.text = "NSFW Image"
}else{ 
self.warning.text = "Not For Kids Image"
}                           
}
```

**è®©æˆ‘ä»¬çœ‹çœ‹è¾“å‡º:**

# 4.ä½¿ç”¨ OpenCV æµ‹è¯•å¹¿å‘Š:

å¯¹äºæµ‹è¯•è§†é¢‘æ¥è¯´ï¼ŒopenCV æ˜¯ä¸€ä¸ªå¾ˆæ£’çš„åŒ…ã€‚è§†é¢‘åªæ˜¯å›¾åƒåºåˆ—ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¯¹æ¯ä¸ªå›¾åƒè¿›è¡Œåˆ†ç±»ï¼Œå¦‚æœä»»ä½•å›¾åƒä¸å®‰å…¨ï¼Œæˆ‘ä»¬å¯ä»¥å°†è§†é¢‘åˆ—ä¸ºä¸å®‰å…¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ openCV åŠ è½½è§†é¢‘ï¼Œä¸è¦å¿˜è®°è¿™ä¸€æ­¥ï¼Œå› ä¸ºå®ƒéå¸¸é‡è¦ã€‚

```
# File : NSFW Video Detector
# We are dividing image by 255.0 as keeping image pixels in range of # 0-1 as it is easier to train. As we used 0-1 range in training so # we need our input as same as we provided while training #classifier.vs = cv2.VideoCapture(input_vid)
writer = None
(W, H) = (None, None)

# loop over frames from the video file stream
while True:
    # read the next frame from the file
    (grabbed, frame) = vs.read()

    # if the frame was not grabbed, then we have reached the end
    # of the stream
    if not grabbed:
        break

    # if the frame dimensions are empty, grab them
    if W is None or H is None:
        (H, W) = frame.shape[:2]

    output = frame.copy()
    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    frame = frame/255.0
    frame = cv2.resize(frame, (224, 224)).astype("float32")

#     frame -= mean

    # make predictions on the frame and then update the predictions
    # queue
    preds = model.predict(np.expand_dims(frame, axis=0))[0]
    print(preds)
    Q.append(preds)# perform prediction averaging over the current history of
    # previous predictionsresults = np.array(Q).mean(axis=0)
    i = np.argmax(preds)
    label = labels[i]
    # draw the activity on the output frame
    text = "activity: {}:".format(label)
    cv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (0, 255, 0), 5)# check if the video writer is None
    if writer is None:
        # initialize our video writer
        fourcc = cv2.VideoWriter_fourcc(*"MJPG")
        writer = cv2.VideoWriter(output_vid, fourcc, 30, (W, H), True)# write the output frame to disk
    writer.write(output)# show the output image
    cv2.imshow("Output", output)
    key = cv2.waitKey(1) & 0xFF# if the `q` key was pressed, break from the loop
    if key == ord("q"):
        break

# release the file pointers
print("[INFO] cleaning up...")
# writer.release()
vs.release()
```

å½“è¿è¡Œé¿å­•å¥—å¹¿å‘Šæ—¶ï¼Œä¸Šè¿°ä»£ç çš„è¾“å‡ºæ˜¯:

ä¸€äº›åœºæ™¯è¢«åˆ—ä¸ºæ€§æ„Ÿå’Œè‰²æƒ…ï¼Œæ‰€ä»¥æˆ‘æƒ³æ”¿åºœæ˜¯å¯¹çš„ğŸ˜œã€‚

# 5.æ”¹è¿›æ¨¡å‹ä»¥å‘ç°å›¾åƒä¸­çš„ç»†èŠ‚:

å¦‚æœæˆ‘ä»¬è¦åˆ†ç±»çš„å›¾åƒå¾ˆå¤§ï¼Œè€Œå›¾åƒä¸­åªæœ‰å¾ˆå°ä¸€éƒ¨åˆ†æ˜¯ NSFW å†…å®¹ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä½¿ç”¨äº†æ»‘åŠ¨çª—å£æŠ€æœ¯æ¥éå†å›¾åƒä¸­çš„å°å¸§ï¼Œå¹¶é€è¡Œå¯¹å®ƒä»¬è¿›è¡Œåˆ†ç±»ã€‚å½“æˆ‘æŠŠè¿™å¼ å›¾ç‰‡ä½œä¸ºä¸€ä¸ªæ•´ä½“è¾“å…¥åˆ†ç±»å™¨æ—¶ï¼Œå®ƒæŠŠå®ƒè¯†åˆ«ä¸ºè‰²æƒ…å›¾ç‰‡ã€‚ä½†æ˜¯å¯èƒ½å­˜åœ¨ NSFW å«é‡éå¸¸å°‘æƒ…å†µã€‚æ‰€ä»¥æˆ‘ä»¬æ¥è¯•è¯•å§ã€‚

PSã€‚æˆ‘åœ¨å¤„ç†å›¾åƒåè¾“å…¥äº†å®¡æŸ¥ï¼Œæ‰€ä»¥å¯¹è¯»è€…æ¥è¯´æ˜¯å®‰å…¨çš„ã€‚

```
def check(unsave = 0):
    image = cv2.imread("final.png")
    (winW, winH) = (224, 224)
    maping = {0 : "Neutral", 1 : "Porn", 2 : "Sexy"}
    writer = None
    for resized in pyramid(image, scale=5):
        # loop over the sliding window for each layer of the pyramid
        for (x, y, window) in sliding_window(resized, stepSize=48, windowSize=(winW, winH)):
            # if the window does not meet our desired window size, ignore itif window.shape[0] != winH or window.shape[1] != winW:
                continue# THIS IS WHERE YOU WOULD PROCESS YOUR WINDOW, SUCH AS APPLYING A
            # MACHINE LEARNING CLASSIFIER TO CLASSIFY THE CONTENTS OF THE
            # WINDOW
            output = resized.copy()
            frame = cv2.cvtColor(window, cv2.COLOR_BGR2RGB)
            frame = frame/255.0
            preds = model.predict(np.expand_dims(frame, axis=0))[0]
            i = np.argmax(preds)
            label = maping[i]
            print(preds, label)

            if unsave:
                if i == 1:
                    return "Porn Found"

            if not unsave:
                clone = resized.copy()
                image = cv2.rectangle(clone, (x, y), (x + winW, y + winH), (0, 255, 0), 2)
                cv2.putText(image, label, (x, y+50), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (0, 255, 0), 5)cv2.imshow("Window", clone)
                cv2.waitKey(1)
                time.sleep(0.09)if writer is None:
                # initialize our video writer
                    fourcc = cv2.VideoWriter_fourcc(*"MJPG")
                    writer = cv2.VideoWriter("1.avi", fourcc, 8, (1080, 720), True)# write the output frame to disk
                writer.write(clone)
    return "Save to View"# Frame by Frame ipynb
# Just added a very small change**def** isUnsave(): 
    ans **=** check(1)
    print(ans)
# This check function is similar to what we made during Video 
# Detection but sliding window concept is added. 
```

# æœªæ¥å·¥ä½œ:

1.  è¯¥åˆ†ç±»å™¨æœªèƒ½è¯†åˆ«ç”·æ€§ç”Ÿæ®–å™¨ï¼Œå› ä¸ºå®ƒæ²¡æœ‰åœ¨ç”·æ€§ç”Ÿæ®–å™¨ä¸Šè¿›è¡Œè®­ç»ƒã€‚å› æ­¤ï¼Œä»æœªæ¥çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ›´å¤šçš„æ•°æ®ä¸Šè®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚
2.  è¿™ä¸ªåˆ†ç±»å™¨æ— æ³•è¯†åˆ«æ¼«ç”»ã€åŠ¨æ¼«å’Œéå¸¸ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ reddit ä¸ŠåºŸå¼ƒæ›´å¤šæ•°æ®ï¼Œå¹¶è¿›ä¸€æ­¥è®­ç»ƒå®ƒã€‚

# æ€»ç»“:

æ„å»ºè¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ç§å¾ˆæ£’çš„ä½“éªŒï¼Œåœ¨å„ç§å›¾åƒä¸Šæµ‹è¯•å®ƒæ›´æœ‰è¶£ã€‚è¯¥æ¨¡å‹åœ¨æ—¥å¸¸ç”Ÿæ´»å›¾åƒä¸Šæ•ˆæœå¾ˆå¥½ï¼Œç²¾ç¡®åº¦å¯ä»¥è¿›ä¸€æ­¥æé«˜ã€‚å¯¹äºå¤±è´¥æ¡ˆä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åŠ¨æ¼«å’Œæ›´å¤šè‰²æƒ…èµ„æ–™ä¸Šè®­ç»ƒæ¨¡å‹ã€‚

# èµ„æº:

1.  æˆ‘çš„ NSFW åˆ†ç±»å™¨çš„ Github åº“:[https://github.com/lakshaychhabra/NSFW-Detection-DL](https://github.com/lakshaychhabra/NSFW-Detection-DL)
2.  iOS åº”ç”¨çš„ Github åº“:[https://github.com/lakshaychhabra/NSFW-ios-ML](https://github.com/lakshaychhabra/NSFW-ios-ML)
3.  æˆ‘çš„ä½œå“é›†:[lakshaychabra . github . io](https://lakshaychhabra.github.io)

# å‚è€ƒèµ„æ–™:

ç‰¹åˆ«æ„Ÿè°¢æ•°æ®ç§‘å­¦ç¤¾åŒºåœ¨ç½‘ç»œä¸Šæä¾›çš„ç²¾å½©å†…å®¹ã€‚

1.  æ•°æ®é›†:[https://github.com/alex000kim/nsfw_data_scraper](https://github.com/alex000kim/nsfw_data_scraper)
2.  åšå®¢å‚è€ƒ:[https://www . freecodecamp . org/news/how-to-set-up-nsfw-content-detection-with-machine-learning-229 a 9725829 c/](https://www.freecodecamp.org/news/how-to-set-up-nsfw-content-detection-with-machine-learning-229a9725829c/)
3.  OpenCV å‚è€ƒ:[https://www . pyimagesearch . com/2019/07/15/video-classification-with-keras-and-deep-learning/](https://www.pyimagesearch.com/2019/07/15/video-classification-with-keras-and-deep-learning/)ï¼Œ[https://www . pyimagesearch . com/2015/03/23/sliding-windows-for-object-detection-with-python-and-OpenCV/](https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/)
4.  å…³äº Ban çš„æ–°é—»:[https://www . the Hindu . com/news/national/govt-bans-é¿å­•å¥—-å¹¿å‘Š-ä»æ—©ä¸Š 6 ç‚¹åˆ°æ™šä¸Š 10 ç‚¹-å› ä¸ºä»–ä»¬æ˜¯ä¸é›…çš„/article21461765.ece](https://www.thehindu.com/news/national/govt-bans-condom-ads-from-6-am-to-10-pm-because-they-are-indecent/article21461765.ece)