<html>
<head>
<title>Exploring Other Face Recognition Approaches (Part 1) — CosFace</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索其他人脸识别方法(第一部分)——CosFace</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/exploring-other-face-recognition-approaches-part-1-cosface-4aed39afe7a8?source=collection_archive---------6-----------------------#2020-08-17">https://medium.com/analytics-vidhya/exploring-other-face-recognition-approaches-part-1-cosface-4aed39afe7a8?source=collection_archive---------6-----------------------#2020-08-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8e9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在探索了标准的MTCNN或人脸级联之外的各种人脸检测方法后，在系列4篇文章(<a class="ae jd" rel="noopener" href="/analytics-vidhya/exploring-other-face-detection-approaches-part-1-retinaface-9b00f453fd15"> 1 </a>、<a class="ae jd" rel="noopener" href="/analytics-vidhya/exploring-other-face-detection-approaches-part-2-ssh-7c85179cd98d"> 2 </a>、<a class="ae jd" rel="noopener" href="/analytics-vidhya/exploring-other-face-detection-approaches-part-3-pcn-395d3b07d62a"> 3 </a>、<a class="ae jd" rel="noopener" href="/analytics-vidhya/exploring-other-face-detection-approaches-part-4-tiny-face-684c8cba5b01"> 4 </a>)中，让我们讨论一下基于人脸识别系统的下一步，即从人脸中提取特征，与数据库中的其他人脸进行比较。在这里，我们也不会探讨常见的模型，即使用三重损失的<a class="ae jd" href="https://arxiv.org/abs/1503.03832" rel="noopener ugc nofollow" target="_blank"> facenet </a>和使用铰链损失的<a class="ae jd" href="http://dlib.net/dnn_face_recognition_ex.cpp.html" rel="noopener ugc nofollow" target="_blank"> dlib的基于resnet </a>的人脸识别模型。在这一系列文章中，我们将探索除标准方法之外的其他方法来获得人脸特征向量。在第1部分中，我们将讨论CosFace。</p><p id="6300" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将讨论三种不同类型的人脸识别方法。CosFace<br/>T15】2。<a class="ae jd" rel="noopener" href="/analytics-vidhya/exploring-other-face-recognition-approaches-part-2-arcface-88cda1fdfeb8"> ArcFace </a> <br/> 3。<a class="ae jd" rel="noopener" href="/analytics-vidhya/exploring-other-face-recognition-approaches-part-3-dream-a5627ced45be">梦:深度残差同变映射</a></p><h1 id="7eb1" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">介绍</h1><p id="d18a" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">大余量余弦损失(LMCL)被称为CosFace，通过L2归一化特征和权重向量将传统的softmax损失重新表达为余弦损失，以消除径向变化，在此基础上引入余弦余量项，以进一步最大化角度空间中的决策余量。作为结果，我们获得了用于精确人脸验证的最小类内余量和最大类间余量。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/d51e2bdc8ae42e416dd317e6aa704820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*FKtz4jvnhFDByoIhfkI6xw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">提议的CosFace框架概述。在训练阶段，在不同类别之间以较大的差距学习有区别的人脸特征。在测试阶段，测试数据被输入到CosFace以提取人脸特征，这些特征随后被用于计算余弦相似性分数以执行人脸验证和识别。</figcaption></figure><h1 id="daee" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">方法</h1><p id="569c" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">首先我们将讨论共面损失，然后讨论特征归一化对改善结果的影响，最后讨论余量对损失函数的影响。</p><h2 id="4a32" class="kt jf hi bd jg ku kv kw jk kx ky kz jo iq la lb js iu lc ld jw iy le lf ka lg bi translated">大幅度余弦损失</h2><p id="58a3" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">softmax损失通过最大化地面实况类的后验概率来区分不同类的要素。给定输入特征向量x及其标签y，softmax损失可表示为:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lh"><img src="../Images/99d7f7303e23dd6717fe267f715458bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*O6c_sjF9E2cAXGKkWFNmpA.png"/></div></figure><p id="7a5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中p表示x被正确分类的后验概率。n是训练样本的数量，C是类别的数量。f被表示为具有权重向量w的全连接层的激活。为了简单起见，将bias保持为0。因此，f由下式给出:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es li"><img src="../Images/929c7ac7e788ee7d8087eadb8bb5cccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*H__MCglPYJ0GVnm6gUsglQ.png"/></div></figure><p id="4fc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，θ是W和x之间的角度。该公式表明，向量的范数和角度都对后验概率有贡献。</p><p id="627d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了有效的特征学习，W的范数必须是不变的，因此我们通过L2归一化来固定范数(W) = 1。由于在测试阶段我们使用余弦相似度来比较两个人脸特征向量，我们可以说特征向量的范数对评分函数不起任何作用。因此，在训练时，我们可以固定norm(x)=s。因此，后验概率仅依赖于角度的余弦，因此，损失可以表示为:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lj"><img src="../Images/f75c1db3d575a2f3574438266a2820cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*bduug8N5bUMipDMNtCRVAA.png"/></div></figure><p id="a5a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们已经将范数(x)固定为s，所以得到的模型学习在角度空间中可分离的特征，该角度空间在这里被称为Softmax Loss (NSL)的<em class="lk">归一化版本。</em></p><p id="1c8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是NSL损失仅仅强调正确分类是不够的。为了解决这个问题，余弦余量被引入到损失函数中。<br/>考虑二元类的例子，让θi表示学习的特征向量和类Ci的权重向量之间的角度(i = 1，2)。对于C 1，NSL强制cos(θ1 ) &gt; cos(θ2 ),并且类似地对于C2，以便来自不同类的特征被正确分类。为了开发一个大幅度分类器，我们还需要cos(θ1)m&gt;cos(θ2)和cos(θ2)m&gt;cos(θ1)，其中m ≥ 0是一个固定参数，用于控制余弦幅度。由于cos(θi)m低于cos(θI)，因此分类的约束条件更加严格。因此，LMCL的公式为:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ll"><img src="../Images/f8eaf1cfa72cc9a380795c8019a5df64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*ZcbdUaPVnZa3U9PXa3KnBg.png"/></div></figure><p id="14ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">服从于，</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lm"><img src="../Images/cd9f38f8702a4f453ab32029d20e5829.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*IpnoQ7QeYYGuFaHYvZhg2Q.png"/></div></figure><p id="a0de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中N是训练样本的数量，xi特征向量具有相应的标签yi，Wj是权重向量，θj是Wj和xi之间的角度。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ln"><img src="../Images/358931920770ad302970f9ceb49e67a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*_Fj-DNCCgRmNCJO5DwjROw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">两类情形下不同损失函数的决策裕度比较。虚线代表决策边界，灰色区域是决策边界。</figcaption></figure><p id="fe5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Softmax </strong>损失通过<br/>范数(W1)cos(θ1) =范数(W2)cos(θ2)来定义判定边界，因此边界取决于权重向量的幅度和角度，因此判定余量在余弦空间中重叠。</p><p id="9c68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> NSL </strong>将权重向量归一化为幅度1，因此判决边界由下式给出:cos(θ1) = cos(θ2)。从上图可以看出，通过去除径向变异，可以很好的对margin=0的样本进行分类。但是它对噪声并不鲁棒。</p><p id="45f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> A-Softmax </strong>通过引入额外的余量来改善Softmax损失，使决策边界成为:<br/>C1:cos(mθ1)≥cos(θ2)<br/>C2:cos(mθ2)≥cos(θ1)<br/>上图中的第三个图描绘了决策区域，其中灰色区域是决策余量。然而，A-Softmax的裕量并不与所有θ值一致，随着θ减小，裕量变小，当θ=0时，裕量完全消失。</p><p id="c7fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> LMCL </strong>将余弦空间而不是角度空间中的决策余量定义为:<br/>C 1:cos(θ1)≥cos(θ2)+m<br/>C 2:cos(θ2)≥cos(θ1)+m<br/>cos(θ1)最大化，而cos(θ2)最小化，以便C1(类似于C2)执行大余量分类。上图的最后一个子图说明了余弦空间中LMCL的决策边界，其中我们可以<br/>在角度余弦的生成分布中看到一个清晰的余量(√2m)。这表明LMCL比NSL更稳健。</p></div><div class="ab cl lo lp gp lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hb hc hd he hf"><h2 id="f737" class="kt jf hi bd jg ku kv kw jk kx ky kz jo iq la lb js iu lc ld jw iy le lf ka lg bi translated"><strong class="ak">特征归一化</strong></h2><p id="13dc" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">为了导出余弦损失的公式并去除径向变化，权重向量和特征向量都被归一化。结果，特征向量分布在超球面上，其中缩放参数s[先前定义]控制半径的大小。</p><p id="5084" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么需要特征标准化？</strong> <br/>没有特征归一化的原始softmax loss隐式地学习特征向量的欧几里德范数(L2范数)和<br/>角度的余弦值。自适应地学习L2范数以最小化总损失，导致相对弱的余弦约束。相反，LMCL要求整个特征向量集具有相同的L2范数，使得学习仅依赖于余弦值来发展辨别能力。来自相同类别的特征向量被聚集在一起，而来自不同类别的特征向量在超球面上被分开。</p><p id="895e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参数‘s’的值应该是什么？<br/> </strong>给定归一化的学习特征向量x和单位权向量W，类别总数为c .假设学习特征向量分别位于超球面上，并以对应的权向量为中心。设Pw表示类中心的期望最小后验概率(即W)，s的下界:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lv"><img src="../Images/6256c29529879af4e15c76b4fbb07cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*-8bVp6wijH8c-o1NWqDUbQ.png"/></div></figure><p id="4b40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于这个界限，我们可以说，如果我们期望对于具有一定数量的类的分类的最优Pw，则s应该被一致地扩大。期望的s应该更大以处理更多的类别，因为类别数量的增加增加了分类的难度。因此，需要具有大半径s的超球面来嵌入具有小的类内距离和大的类间距离的特征。</p></div><div class="ab cl lo lp gp lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hb hc hd he hf"><h2 id="cb46" class="kt jf hi bd jg ku kv kw jk kx ky kz jo iq la lb js iu lc ld jw iy le lf ka lg bi translated">余弦边距' m '的效果</h2><p id="2fd3" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">m的最佳选择潜在地导致更有希望的高辨别人脸特征的学习。<br/>合理选择较大的m ∈ [0，C/(C1)){请参考参考文献中给出的论文，以理解m的这个范围}应该促进对高辨别特征的学习。因为所有的特征向量都根据相应类别的权重向量集中在一起。事实上，当m过大时，模型无法收敛，因为余弦约束<br/>(即两类的cosθ1m&gt;cosθ2或cosθ2m&gt;cosθ1)变得更严格，很难满足。此外，具有过大m的余弦约束迫使训练过程对噪声数据更加敏感。不断增加的m在某一点上开始降低整体性能，这是<br/>无法收敛的原因。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es lw"><img src="../Images/6cf833e065b041587fab23f87203112d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wQ5Ke2a_ql6yoCHi-9oNAg.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">具有2D特征的8个恒等式上不同损失函数的小实验。第一行将2D特征映射到欧几里得空间，而第二行将2D特征投影到角度空间。随着裕度项m的增加，该差距变得明显。</figcaption></figure></div><div class="ab cl lo lp gp lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hb hc hd he hf"><h1 id="023f" class="je jf hi bd jg jh mb jj jk jl mc jn jo jp md jr js jt me jv jw jx mf jz ka kb bi translated">结论</h1><p id="ed23" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">我们了解了一种新的人脸识别损失函数，它在余弦空间中工作，并帮助模型学习非常有区别的特征。</p></div><div class="ab cl lo lp gp lq" role="separator"><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt lu"/><span class="lr bw bk ls lt"/></div><div class="hb hc hd he hf"><h1 id="f90d" class="je jf hi bd jg jh mb jj jk jl mc jn jo jp md jr js jt me jv jw jx mf jz ka kb bi translated">参考</h1><p id="a565" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">论文:<a class="ae jd" href="https://arxiv.org/pdf/1801.09414.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1801.09414.pdf</a></p></div></div>    
</body>
</html>