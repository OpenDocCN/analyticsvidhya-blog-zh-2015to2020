<html>
<head>
<title>End to end Sentiment Text Classification pipeline entirely on GPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">完全基于GPU的端到端情感文本分类管道</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sentiment-text-classification-using-only-gpu-accelerated-libraries-35d7cbd6c55b?source=collection_archive---------6-----------------------#2019-09-11">https://medium.com/analytics-vidhya/sentiment-text-classification-using-only-gpu-accelerated-libraries-35d7cbd6c55b?source=collection_archive---------6-----------------------#2019-09-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/66871b7f6cb37bda72c419e5628dae14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Zrt_7HAElRsapDRG-WoEw.png"/></div></div></figure></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="3992" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">介绍</h1><p id="ee46" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">在这篇文章中，我将讨论我如何完全在GPU上实现情感文本分类管道。这之所以成为可能，仅仅是因为现有的开源GPU加速python库，即<a class="ae kt" href="https://rapids.ai/" rel="noopener ugc nofollow" target="_blank"> Rapids.ai </a>和<a class="ae kt" href="https://numba.pydata.org/" rel="noopener ugc nofollow" target="_blank"> numba </a>等。在这个例子中，我主要使用了<code class="du ku kv kw kx b">cuDF</code>、<code class="du ku kv kw kx b">nvStrings</code>、<code class="du ku kv kw kx b">numba</code>和<code class="du ku kv kw kx b">pytorch</code>。</p><p id="5193" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated"><strong class="jx hj"> cuDF: </strong> <code class="du ku kv kw kx b">cuDF</code>是一个用于数据操作和准备的GPU加速数据帧库。它是RAPIDs生态系统中的一个单独的包，提供了与<code class="du ku kv kw kx b">Pandas</code>非常相似的python APIs。</p><p id="5aba" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated"><strong class="jx hj">NV strings:</strong><code class="du ku kv kw kx b">nvStrings</code><a class="ae kt" href="https://github.com/rapidsai/custrings" rel="noopener ugc nofollow" target="_blank">cus trings</a>的Python绑定)，在GPU上启用字符串操作。是的，你没看错！在这篇文章的后面，我们会对此进行更多的讨论。</p><p id="0533" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated"><strong class="jx hj"> numba: </strong> <code class="du ku kv kw kx b">numba</code>使用行业标准的<code class="du ku kv kw kx b"><a class="ae kt" href="https://llvm.org/" rel="noopener ugc nofollow" target="_blank">LLVM</a></code> / <code class="du ku kv kw kx b"><a class="ae kt" href="https://docs.nvidia.com/cuda/nvvm-ir-spec/index.html" rel="noopener ugc nofollow" target="_blank">NVVM</a></code>编译器库，在运行时将Python函数翻译成优化的机器代码。Python中Numba编译的数值算法可以接近C或CUDA的速度。</p><p id="60c1" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated"><strong class="jx hj"> cuPy: </strong> <code class="du ku kv kw kx b">CuPy</code>是NumPy兼容的多维数组在CUDA上的实现。CuPy由核心的多维数组类<code class="du ku kv kw kx b">cupy.ndarray</code>和上面的很多函数组成。它支持<code class="du ku kv kw kx b">numpy.ndarray</code>接口的子集。</p><p id="777a" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated"><strong class="jx hj"> pyTorch: </strong> <code class="du ku kv kw kx b">pyTorch</code>是建立在<a class="ae kt" href="https://en.wikipedia.org/wiki/Torch_(machine_learning)" rel="noopener ugc nofollow" target="_blank"> torch </a>之上的机器学习库。它得到了脸书人工智能研究小组的支持。经过最近的发展，它已经获得了很大的普及，因为它的简单性，动态图形，因为它是本质上的pythonic。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h2 id="a7f0" class="ld iy hi bd iz le lf lg jd lh li lj jh kg lk ll jl kk lm ln jp ko lo lp jt lq bi translated">问题陈述:</h2><p id="f650" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">给定包含两列<em class="lr">评论</em>和<em class="lr">标签的CSV文件，评论</em>列包含电影的文本评论，标签包含<code class="du ku kv kw kx b">0</code>或<code class="du ku kv kw kx b">1</code>取决于评论的情绪。每一行包含一个训练示例。在这篇文章中，我将一步一步地描述建立LSTM模型来解决这类问题的过程。</p><h2 id="8a6c" class="ld iy hi bd iz le lf lg jd lh li lj jh kg lk ll jl kk lm ln jp ko lo lp jt lq bi translated">TL；速度三角形定位法(dead reckoning)</h2><ol class=""><li id="49d3" class="ls lt hi jx b jy jz kc kd kg lu kk lv ko lw ks lx ly lz ma bi translated">利用cuDF的<code class="du ku kv kw kx b">read_csv()</code>，将单词向量和输入数据直接载入GPU内存。</li><li id="fdad" class="ls lt hi jx b jy mb kc mc kg md kk me ko mf ks lx ly lz ma bi translated">利用cuStrings的<code class="du ku kv kw kx b">nvstrings</code>操作输入文本数据和<code class="du ku kv kw kx b">nvcategory</code>词汇生成和单词到序列生成。</li><li id="ce22" class="ls lt hi jx b jy mb kc mc kg md kk me ko mf ks lx ly lz ma bi translated">利用<code class="du ku kv kw kx b">Numba</code>代替<em class="lr"> numpy </em>存储输入n array和输出n array。</li><li id="b366" class="ls lt hi jx b jy mb kc mc kg md kk me ko mf ks lx ly lz ma bi translated">编写了一个自定义的实用程序方法来将numba cuda数组转换为torch cuda张量，因为这是pytorch github中的一个未解决的问题<a class="ae kt" href="https://github.com/pytorch/pytorch/issues/23067" rel="noopener ugc nofollow" target="_blank"> #23067 </a>。</li><li id="010c" class="ls lt hi jx b jy mb kc mc kg md kk me ko mf ks lx ly lz ma bi translated">利用<code class="du ku kv kw kx b">pytorch</code>创建LSTM模型并进行训练和预测。</li></ol><p id="6922" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">如果您观察到从数据加载开始直到训练/预测，<strong class="jx hj">主机和设备内存之间没有一次数据传输，那么整个时间内，所有数据都驻留在GPU内存中</strong>。这一点非常重要，因为我们知道主机和设备之间的数据传输非常耗时，会降低性能。</p><p id="deff" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">我已经包括了工作要点笔记本和colab笔记本。尝试一下…</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="8f05" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">1.加载单词向量</h1><p id="d58d" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">如果你不熟悉单词的矢量表示，我建议你去看看下面的博客。</p><div class="mg mh ez fb mi mj"><a href="https://www.tensorflow.org/tutorials/representation/word2vec" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab dw"><div class="ml ab mm cl cj mn"><h2 class="bd hj fi z dy mo ea eb mp ed ef hh bi translated">单词的向量表示|张量流核心|张量流</h2><div class="mq l"><h3 class="bd b fi z dy mo ea eb mp ed ef dx translated">在本教程中，我们来看看Mikolov等人的word2vec模型。</h3></div><div class="mr l"><p class="bd b fp z dy mo ea eb mp ed ef dx translated">www.tensorflow.org</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx io mj"/></div></div></a></div><p id="cc28" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">在下载了一些预先训练好的单词向量(对于这篇文章的范围，我们将考虑<a class="ae kt" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>)之后，你可以使用<code class="du ku kv kw kx b">cudf.read_csv()</code>读取它们，并直接加载到GPU内存中。</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="7cd4" class="ld iy hi kx b fi ng nh l ni nj">pre_df = cudf.read_csv("glove.6B.50d.txt",<br/>                       header=None,<br/>                       delim_whitespace=True,<br/>                       quoting=3)  #ignore quoting<br/>print(pre_df.head())</span></pre><p id="12a8" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">一旦加载了单词向量，我们就可以对这些潜在的表示进行各种检查。一个非常简单的方法是找到相对于每个单词最近的单词，就是使用余弦相似度。对于这个任务，你可以编写numba内核，它将在GPU上运行。</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="4f6f" class="ld iy hi kx b fi ng nh l ni nj">@cuda.jit(device=True)<br/>def dot(a, b, dim_size):<br/>  summ = 0<br/>  for i in range(dim_size):<br/>    summ += (a[i]*b[i])<br/>  return summ</span><span id="8429" class="ld iy hi kx b fi nk nh l ni nj">@cuda.jit(device=True)<br/>def cosine_sim(a, b, dim_size):<br/>  return dot(a,b, dim_size) / ( math.sqrt(dot(a, a, dim_size)) * math.sqrt(dot(b, b, dim_size)) )</span></pre><p id="48fd" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated"><code class="du ku kv kw kx b">@cuda.jit()</code>是一个numba注释器，指导解释器生成NVVM IR，然后在GPU设备上运行。</p><p id="3066" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">加载这些单词向量后，接下来我们将加载数据集并从每个训练示例中生成标记，然后将每个标记映射到一个唯一的整数id。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="92ce" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">2.正在加载电影评论数据集</h1><p id="8e95" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">对于这个例子，我从这篇<a class="ae kt" href="https://www.analyticsvidhya.com/blog/2019/01/guide-pytorch-neural-networks-case-studies/" rel="noopener ugc nofollow" target="_blank">文章</a>中借用了数据集。你可以从<a class="ae kt" href="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/01/train.csv" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据集。要读取输入数据集，可再次使用<code class="du ku kv kw kx b">cudf.read_csv()</code>…</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="66da" class="ld iy hi kx b fi ng nh l ni nj">sents = cudf.read_csv("train.csv",<br/>                      quoting=3,<br/>                      skiprows=1,<br/>                      names=['review', 'label'])<br/>print(sents.head())</span></pre><p id="7f37" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">这个<code class="du ku kv kw kx b">sents</code>数据帧包含两列:列<em class="lr">评论</em>包含电影的文本评论，列<em class="lr">标签</em>包含0或1，指示评论是正面还是负面。</p><p id="28a4" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">接下来，我们将对来自<code class="du ku kv kw kx b">sents</code>数据帧的每个训练句子进行预处理，标记化和填充，然后根据它们创建词汇。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="0fab" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">3.使用nvStrings进行数据预处理</h1><p id="adf0" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">在加载了<code class="du ku kv kw kx b">sents</code> df之后，我们需要将文本数据转换成我们的LSTM模型能够理解的东西。所以，我使用了手套词嵌入数据框架<code class="du ku kv kw kx b">pre_df</code>。在这里，nvStrings拯救了我…</p><p id="5422" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">为了对评论进行预处理，<em class="lr">的<em class="lr">评论</em>列发送了</em> df，需要转换成nvstrings对象。为简单起见，将nvstings对象视为存储在GPU内存中的字符串列表。</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="62f5" class="ld iy hi kx b fi ng nh l ni nj"># To get a nvstrings object from a cuDF column of "object" dtype<br/>gstr = sents['review'].data<br/>gstr.size()</span></pre><p id="4d31" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">文本格式化:文本数据包含许多特殊字符，为了格式化这些字符，我使用了nvstrings的<code class="du ku kv kw kx b">.replace()</code>方法。</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="4cf1" class="ld iy hi kx b fi ng nh l ni nj"># Utility method to clean the strings available in nvstring object <br/>def clean_sents(gstr):<br/>  gstr = gstr.replace(r"[^A-Za-z0-9(),!?\'\`]", " ")<br/>  gstr = gstr.replace(r"\'s", " \'s")<br/>  gstr = gstr.replace(r"\'ve", " \'ve")<br/>  gstr = gstr.replace(r"n\'t", " n\'t")<br/>  gstr = gstr.replace(r"\'re", " \'re")<br/>  gstr = gstr.replace(r"\'d", " \'d")<br/>  gstr = gstr.replace(r"\'ll", " \'ll")<br/>  gstr = gstr.replace(r",", " , ")<br/>  gstr = gstr.replace(r"!", " ! ")<br/>  gstr = gstr.replace(r"\(", " \( ")<br/>  gstr = gstr.replace(r"\)", " \) ")<br/>  gstr = gstr.replace(r"\?", " \? ")<br/>  gstr = gstr.replace(r"\s{2,}", " ")<br/>  return gstr.strip().lower()</span><span id="838c" class="ld iy hi kx b fi nk nh l ni nj">gstr = clean_sents(gstr)</span></pre><p id="33c1" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated"><strong class="jx hj">标记化和填充:</strong>在清理评论之后，我们需要从文本中生成标记，以从GloVe中获得它们的潜在表示。因为不是所有的评论都包含相同数量的单词，所以需要填充，以便可以将其传递到LSTM模型的嵌入层中。</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="ab53" class="ld iy hi kx b fi ng nh l ni nj"># setting the max length of each review to 20<br/>MAX_LEN = 20<br/>num_sents = gstr.size()</span><span id="e608" class="ld iy hi kx b fi nk nh l ni nj"># generate the tokens<br/>seq = gstr.split_record(' ')</span><span id="02a0" class="ld iy hi kx b fi nk nh l ni nj"># padding each strings if smaller or trim down if larger<br/>for i in range(len(seq)):<br/>  l = seq[i].size()<br/>  if l&lt;= MAX_LEN:<br/>    seq[i] = seq[i].add_strings(nvstrings.to_device((MAX_LEN-l)*['PAD']))<br/>  else:<br/>    seq[i] = seq[i].remove_strings(list(range(MAX_LEN,l)))</span><span id="6b5c" class="ld iy hi kx b fi nk nh l ni nj">print(seq)</span></pre><p id="2ae5" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated"><strong class="jx hj"> Vocab和word_to_index: </strong>接下来，我们需要用所有可用的令牌创建Vocab，并给它们分配一个整数id。对于这个任务，我再次使用了<code class="du ku kv kw kx b">nvcategory</code>，它是<em class="lr">库</em>的一部分。然后，我们需要将vocab中的这些标记映射到相应的预训练单词向量。为此，我使用了cuDF的<code class="du ku kv kw kx b">merge</code>方法。</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="3dd3" class="ld iy hi kx b fi ng nh l ni nj"># generating the indices corresponding each token <br/>c = nvcategory.from_strings_list(seq)</span><span id="c230" class="ld iy hi kx b fi nk nh l ni nj">c.keys_size()   # total number of unique tokens<br/>c.size()        # total number of tokens or vocabulary</span><span id="e009" class="ld iy hi kx b fi nk nh l ni nj"># creating gdf using unique tokens<br/>sent_df = cudf.DataFrame({'tokens':c.keys()})<br/>sent_df.head()</span><span id="896b" class="ld iy hi kx b fi nk nh l ni nj">all_token = vocab_df.shape[0]<br/>print(all_token)</span><span id="4f36" class="ld iy hi kx b fi nk nh l ni nj"># creating embedding matrix <br/>vocab_df = sent_df.merge(pre_df,<br/>                         left_on='tokens',<br/>                         right_on='0',<br/>                         how='left')<br/>vocab_df.drop_column('0')<br/>vocab_df.drop_column('tokens')</span><span id="0dbe" class="ld iy hi kx b fi nk nh l ni nj"># filling the not found tokens with random vector<br/>for c in vocab_df.columns:<br/>  vocab_df[c] = vocab_df[c].fillna(cupy.random.normal(size=all_token)).astype(np.float32)</span><span id="a886" class="ld iy hi kx b fi nk nh l ni nj"># embedding matrix<br/>vocab = vocab_df.as_gpu_matrix(order='C')</span></pre><p id="aaaf" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated"><strong class="jx hj">X _ train和y_train的准备:</strong>这里我用<em class="lr"> numba </em>来处理GPU中的ndarrays。Numba为数组操作提供了与<em class="lr"> numpy </em>非常相似的API。</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="662c" class="ld iy hi kx b fi ng nh l ni nj"># preparing the X_train <br/>X_train = cuda.device_array((num_sents, MAX_LEN), dtype=np.float32)<br/>c.values(X_train.device_ctypes_pointer.value)<br/>print(X_train.shape)</span><span id="bd06" class="ld iy hi kx b fi nk nh l ni nj"># preparation of y_train<br/>y_train = sents['label'].astype('float32').to_gpu_array()<br/>print(y_train.shape)</span></pre><p id="4af9" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">接下来，我们建立一个非常简单的LSTM模型，第一层作为嵌入层，接着是lstm单元，然后是线性单元，最后是输出单元。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="3b8b" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">4.LSTM建筑模型</h1><p id="7052" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">现在，我们已经准备好X_train，y_train和嵌入矩阵，所以我们可以创建我们的神经网络结构。在这个例子中，我创建了一个玩具LSTM模型。</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="f894" class="ld iy hi kx b fi ng nh l ni nj">def create_emb_layer(weights_matrix, non_trainable=False):<br/>  num_embeddings, embedding_dim = weights_matrix.shape<br/>  emb_layer = nn.Embedding(num_embeddings, embedding_dim)<br/>  emb_layer.weight = nn.Parameter(weights_matrix)<br/>  if non_trainable:<br/>    emb_layer.weight.requires_grad = False</span><span id="274c" class="ld iy hi kx b fi nk nh l ni nj">return emb_layer, num_embeddings, embedding_dim</span><span id="53c6" class="ld iy hi kx b fi nk nh l ni nj">class ToyLSTM(nn.Module):<br/>  def __init__(self, weights_matrix, hidden_size, output_size, num_layers):<br/>    super(ToyLSTM, self).__init__()<br/>    self.embedding, num_embeddings, embedding_dim =<br/>                    create_emb_layer(weights_matrix, True)</span><span id="f8dd" class="ld iy hi kx b fi nk nh l ni nj">    self.hidden_size = hidden_size<br/>    self.output_size = output_size<br/>    self.num_layers = num_layers</span><span id="7498" class="ld iy hi kx b fi nk nh l ni nj">self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,     batch_first=True)<br/>    self.linear = nn.Linear(hidden_size, hidden_size//2)<br/>    self.out = nn.Linear(hidden_size//2, output_size)<br/>    self.relu = nn.ReLU()</span><span id="cd4b" class="ld iy hi kx b fi nk nh l ni nj">def forward(self, inp):<br/>    h_embedding = self.embedding(inp)<br/>    h_lstm, _ = self.lstm(h_embedding)<br/>    max_pool, _ = torch.max(h_lstm, 1)<br/>    linear = self.relu(self.linear(max_pool))<br/>    out = self.out(linear)<br/>    return out</span></pre><p id="5f78" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">这是模型的摘要:</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="c8f3" class="ld iy hi kx b fi ng nh l ni nj">ToyLSTM(<br/>  (embedding): Embedding(2707, 50)<br/>  (lstm): LSTM(50, 10, num_layers=3, batch_first=True)<br/>  (linear): Linear(in_features=10, out_features=5, bias=True)<br/>  (out): Linear(in_features=5, out_features=1, bias=True)<br/>  (relu): ReLU()<br/>)</span></pre></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="38ef" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">5.培训和验证</h1><p id="3d1c" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">此时，我们已经准备好了<code class="du ku kv kw kx b">X_train</code>、<code class="du ku kv kw kx b">y_train</code>和嵌入矩阵<code class="du ku kv kw kx b">vocab</code>，现在我们需要将它们转换成torch张量，这不是一件简单的事情。因为，没有api可以直接把numba gpu数组转换成torch张量(GitHub issue <a class="ae kt" href="https://github.com/pytorch/pytorch/issues/23067" rel="noopener ugc nofollow" target="_blank"> #23067 </a>)。所以，我写了一个自定义方法来将cuda数组转换成torch cuda张量，这要感谢<code class="du ku kv kw kx b">__cuda_array_interface__</code>。</p><figure class="my mz na nb fd ij"><div class="bz dy l di"><div class="nl nm l"/></div></figure><p id="1907" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">使用这种方法，我将<code class="du ku kv kw kx b">X_train</code>、<code class="du ku kv kw kx b">y_train</code>和嵌入矩阵<code class="du ku kv kw kx b">vocab</code>转换为torch cuda张量，无需从设备向主机传输任何数据。如果你熟悉GPU范式，那么你一定知道，尽量减少主机和设备之间的数据传输有多重要。</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="f866" class="ld iy hi kx b fi ng nh l ni nj">#instantiate the model <br/>toy_lstm = ToyLSTM(weights_matrix=devndarray2tensor(vocab),<br/>                   hidden_size=10,<br/>                   output_size=1,<br/>                   num_layers=3).cuda()</span><span id="7e04" class="ld iy hi kx b fi nk nh l ni nj">#defining loss_function and optimizer<br/>loss_function = nn.BCEWithLogitsLoss(reduction='mean')<br/>optimizer = optim.Adam(toy_lstm.parameters())</span></pre><p id="2dc2" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">接下来，我用torch的<code class="du ku kv kw kx b">DataLoader()</code> api创建了一个“火车装载器”</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="4ab5" class="ld iy hi kx b fi ng nh l ni nj">train = TensorDataset(devndarray2tensor(X_train).to(torch.int64),<br/>                      devndarray2tensor(y_train))<br/>trainloader = DataLoader(train, batch_size=128)</span></pre><p id="9d02" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">现在，一切都准备好了，模型可以训练了…</p><pre class="my mz na nb fd nc kx nd ne aw nf bi"><span id="5e40" class="ld iy hi kx b fi ng nh l ni nj">for epoch in range(1, 25):<br/>  #training part<br/>  toy_lstm.train()<br/>  for data, target in trainloader:<br/>    optimizer.zero_grad()<br/>    output = toy_lstm(data)<br/>    loss = loss_function(output, target.view(-1,1))<br/>    loss.backward()<br/>    optimizer.step()</span></pre><h1 id="65e8" class="ix iy hi bd iz ja nn jc jd je no jg jh ji np jk jl jm nq jo jp jq nr js jt ju bi translated">结论</h1><p id="a28f" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">这篇文章的关键是“<em class="lr"> GPU不仅在ML模型训练和推理方面表现出色，而且可以有效地用于运行整个数据科学管道</em>”。只有随着Rapids.ai、numba、cuPy和其他gpu加速库的出现，这才有可能实现。因为所有这些库都是开源的，所以越来越多的新特性正在快速增加。</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h2 id="3e2f" class="ld iy hi bd iz le lf lg jd lh li lj jh kg lk ll jl kk lm ln jp ko lo lp jt lq bi translated">尝试一下:</h2><figure class="my mz na nb fd ij"><div class="bz dy l di"><div class="nl nm l"/></div><figcaption class="ns nt et er es nu nv bd b be z dx translated">e2e _文本_分类_gpu.ipynb</figcaption></figure><p id="9454" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated"><strong class="jx hj">谷歌colab</strong>:<a class="ae kt" href="https://colab.research.google.com/drive/19vvRdl-icydcIAVqBJ3VBP9s1HyAZ2KT" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/19 vvrdl-icydciavqbj 3 vbp 9 S1 hyaz 2 kt</a></p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><p id="bd32" class="pw-post-body-paragraph jv jw hi jx b jy ky ka kb kc kz ke kf kg la ki kj kk lb km kn ko lc kq kr ks hb bi translated">如果您遇到任何问题或想分享一些建议，欢迎评论，谢谢:)</p><h2 id="ff58" class="ld iy hi bd iz le lf lg jd lh li lj jh kg lk ll jl kk lm ln jp ko lo lp jt lq bi translated">参考资料:</h2><ol class=""><li id="0063" class="ls lt hi jx b jy jz kc kd kg lu kk lv ko lw ks lx ly lz ma bi translated"><a class="ae kt" href="https://www.analyticsvidhya.com/blog/2019/01/guide-pytorch-neural-networks-case-studies/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2019/01/guide-py torch-neural-networks-case-studies/</a></li><li id="c265" class="ls lt hi jx b jy mb kc mc kg md kk me ko mf ks lx ly lz ma bi translated"><a class="ae kt" href="https://medium.com/rapids-ai" rel="noopener">https://medium.com/rapids-ai</a></li><li id="da55" class="ls lt hi jx b jy mb kc mc kg md kk me ko mf ks lx ly lz ma bi translated"><a class="ae kt" rel="noopener" href="/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76">https://medium . com/@ martinpella/how-to-use-pre-trained-word-embedding-in-py torch-71ca 59249 f 76</a></li><li id="b36e" class="ls lt hi jx b jy mb kc mc kg md kk me ko mf ks lx ly lz ma bi translated"><a class="ae kt" href="http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/" rel="noopener ugc nofollow" target="_blank">http://www . wild ml . com/2015/12/implementing-a-CNN-for-text-class ification-in-tensor flow/</a></li><li id="4976" class="ls lt hi jx b jy mb kc mc kg md kk me ko mf ks lx ly lz ma bi translated"><a class="ae kt" href="https://discuss.pytorch.org/t/creating-variable-from-existing-numba-cuda-array/14474" rel="noopener ugc nofollow" target="_blank">https://discuse . py torch . org/t/creating-variable-from-existing-numba-cuda-array/14474</a></li></ol></div></div>    
</body>
</html>