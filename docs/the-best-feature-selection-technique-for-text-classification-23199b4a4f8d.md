# 文本分类的最佳特征选择技术

> 原文：<https://medium.com/analytics-vidhya/the-best-feature-selection-technique-for-text-classification-23199b4a4f8d?source=collection_archive---------12----------------------->

一个简单的特征选择代码！

![](img/cb05fda3a1fd0ac215d1d5fcd3233c1f.png)

在我们进入代码之前，让我们先了解一些关于特性选择的事情

1.  什么是特征选择？

**特征选择**是在开发预测模型时减少输入变量数量的过程。希望减少输入变量的数量，以降低建模的计算成本，并且在某些情况下，提高模型的性能。

2.特征选择是如何工作的？

**特征选择**是您自动或手动**选择那些对您感兴趣的预测**变量**或输出贡献最大的**特征**的过程。在您的数据中包含不相关的**特征**会降低模型的准确性，并使您的模型基于不相关的**特征**进行学习。**

3.特征选择在文本分类中的重要性。

特征选择是文本分类领域中最重要的步骤之一。因为文本数据大多存在高维问题。为了减少高维数的灾难，使用了特征选择技术。特征选择背后的基本思想是只保留重要的特征，去掉不太重要的特征。

与高维度相关的问题如下:

1.向模型中添加不必要的噪声

2.高空间和时间复杂度

3.过度拟合

我们今天要讨论的特征选择技术是卡方特征选择。

卡方检验在统计学中用于检验两个事件的独立性。更具体地说，在特征选择中，我们使用它来测试特定术语的出现和特定类别的出现是否独立。

事不宜迟，让我们直接进入代码

```
# Load librariesfrom sklearn.feature_selection import SelectKBestfrom sklearn.feature_selection import chi2# N features with highest chi-squared statistics are selectedchi2_features = SelectKBest(chi2, k = can be any number)X = chi2_features.fit_transform(X, y)
```

代码的前两行只是导入卡方特征选择所需的包。SelectKBest 函数用于基于卡方得分选择 K 个顶级特征。k 可以是任何数字，这取决于您正在处理的特征的数量。

例如，如果在创建单词包后，您有 50，000 个特征(列)。在这种情况下，您应该尝试将 K 值保持在 40，000 到 10，000 之间，并检查哪个值给出的结果最好。一旦你找到了给出最佳精度的最佳数字，你就可以最终将它设置为默认的 K 值。

这篇文章是写给那些刚开始使用 NLP，并且被使用哪种特性选择技术以及如何实现这个技术的问题所困扰的人的。在大多数情况下，文本清理的特征选择是一个令人头疼的问题。这段代码可以帮助你用最基本的特征选择技术进行文本清理，并且可以直接使用。

感谢您的阅读；我希望你学到了新的东西！

干杯。