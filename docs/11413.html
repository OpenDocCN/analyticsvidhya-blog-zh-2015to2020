<html>
<head>
<title>Extract-Transform-Load in Elasticsearch and Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Elasticsearch和Python中的提取-转换-加载</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/extract-transform-load-in-elasticsearch-and-python-11cd4acb225e?source=collection_archive---------4-----------------------#2020-12-02">https://medium.com/analytics-vidhya/extract-transform-load-in-elasticsearch-and-python-11cd4acb225e?source=collection_archive---------4-----------------------#2020-12-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="dd09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">连接和使用Elasticsearch-Python接口处理ETL过程中的高数据量的关键要点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/6d18de9e0eaa4ab6f0396741de31cf55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p9OzKm9z2KOLousn.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:<a class="ae jt" href="https://towardsdatascience.com/getting-started-with-elasticsearch-in-python-c3598e718380" rel="noopener" target="_blank">https://towards data science . com/getting-started-with-elastic search-in-python-c 3598 e 718380</a></figcaption></figure><p id="6b02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di"> W </span>当我们设计企业级解决方案时，我们会记下数据层。现在有大量的数据事务发生，并且有大量的数据层技术可用。像MySQL、Oracle和PostgreSQL这样的数据库被归类为“关系型”。其他数据库，比如MongoDB，被归类为“非关系型”,也就是说，持久化的数据在数据库引擎中没有维护它们之间的固有关系。还有其他一些我们称之为“图数据库”的数据库比如Neo4j，可以处理“高度连接的数据”。Elasticsearch属于第二类。</p><p id="f509" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Elasticsearch专门用于在数据之间没有太多修改(或“更新”)的情况下快速插入和快速检索的情况。像快速推拉动作这样的例子可以使用弹性搜索引擎作为中间存储系统来存储某些数据，比如日志。例如，如果我们考虑一个API Hub，它的主要工作是作为一个完整的系统提供用户需要的终端。本质上，它所做的是将用户的请求定向到apt端点，并为正确的用户提供正确的响应。因为它是一个<em class="kd">集成平台</em>，许多用户可以连接到它，产生大量的流量，使得几乎不可能持续监控系统的TPS(每秒事务数)的变化等指标。此外，当产生大量流量时，错误计数等洞察以及请求-响应趋势(随时间增加/减少)等智能洞察更难挖掘，因为跟踪流量很麻烦。Elasticsearch在这类场景中表现良好，具有流畅高效的事务日志插入和检索速度。例如，我们可以即时记录请求-响应周期的各个部分(根据使用屏蔽等技术的隐私场景；特别是在你受到GDPR约束的情况下)，将它们编入Elasticsearch，并使用为高效数据检索技术开发的Lucene查询来挖掘它们。</p><h1 id="14f9" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">弹性搜索连接器</h1><p id="d34c" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">在MySQL等数据库中，我们有用不同语言编写的独立驱动程序(如Java、Python等)。)与数据库进行交互。默认情况下，Elasticsearch有一个REST接口与之交互，它充当数据存储和业务逻辑之间的通用接口，与语言无关。本质上，您要做的就是用您喜欢的任何语言调用请求模块(HTTP request <em class="kd"> clients </em>),并使用带有正确消息体(和头)和路径参数的正确端点。然而，我们在此之上被提供了抽象层，并且有提供这些抽象的“驱动程序”(它们是库，用您的首选语言调用HTTP客户端)。使用这些库，您可以与Elasticsearch节点进行交互，而无需自己编写REST请求，库会将您的方法调用转换为Elasticsearch查询，这与普通的数据库驱动程序非常相似。</p><p id="a269" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于python，有两个主要的Elasticsearch库，您可以从中受益，而无需自己编写查询。然而，在某些复杂的场景中，这些库允许您沿着抽象的阶梯向下爬，自己编写Elasticsearch查询。</p><ol class=""><li id="d854" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated"><code class="du lq lr ls lt b">elasticsearch-py</code> : <a class="ae jt" href="https://elasticsearch-py.readthedocs.io/en/7.10.0/" rel="noopener ugc nofollow" target="_blank">这个库</a>用来初始化你的业务层(业务逻辑)和数据层(Elasticsearch)之间的连接。它非常类似于您用来启动与数据库的连接的连接池。它允许打开一个套接字，发送查询并检索请求查询的响应。</li><li id="19ad" class="lh li hi ih b ii lu im lv iq lw iu lx iy ly jc lm ln lo lp bi translated"><code class="du lq lr ls lt b">elasticsearch-dsl</code> : <a class="ae jt" href="https://elasticsearch-dsl.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">这个</a>是建立在第一个库<code class="du lq lr ls lt b">elasticsearch-py</code>之上的抽象，用来“<em class="kd">为Python中所有与Elasticsearch相关的代码</em>提供公共基础”。这有点类似于处理关系数据库时使用的ORM(但不完全相同)。它允许您使用方法参数和其他几个特定于Python的编程语用学来减轻来自Elasticsearch的查询。它使用<code class="du lq lr ls lt b">elasticsearch-py</code>来启动连接，并根据发送到Elasticsearch的<code class="du lq lr ls lt b">elasticsearch-py </code>来翻译您的查询。</li></ol><h1 id="f801" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">案例研究</h1><p id="9da1" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">让我们考虑一下上面讨论过的场景:将处理过的日志索引到Elasticsearch中。<a class="ae jt" href="https://netflixtechblog.com/lessons-from-building-observability-tools-at-netflix-7cfafed6ab17" rel="noopener ugc nofollow" target="_blank">网飞在他们的技术平台</a>上做了同样的事情来开发“观察工具”。谷歌有自己的服务来大规模处理日志。同样，这种现象已经变得普遍，并在大规模<a class="ae jt" href="https://digitalguardian.com/blog/what-log-analysis-use-cases-best-practices-and-more" rel="noopener ugc nofollow" target="_blank">中使用，坚持几种不同的做法</a>。</p><p id="96db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本质上，我们可以通过两种方式完成任务:</p><ol class=""><li id="e8f0" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">动态执行弹性搜索的处理部分和索引。</li><li id="fa60" class="lh li hi ih b ii lu im lv iq lw iu lx iy ly jc lm ln lo lp bi translated">稍后作为调度作业运行处理部分，而不会干扰请求-响应循环。</li></ol><p id="e81a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一个场景明显影响了我们的TPS(每秒事务数)指标，因为它动态地处理消息。也就是说，当数据量较高时，它会消耗一些CPU周期并需要大量内存，这会影响并降低我们每秒可以处理的事务数量。我们可以想出一些办法来解决这个问题。我们可以使用具有多个消息队列的分布式架构在几个其他机器之间分配处理工作，以便在每个机器之间进行通信，并最终将结果和索引归入Elasticsearch(例如Map-Reduce)。然而，这种解决方案需要几个额外的资源，这最终会导致要求更高的成本(计算成本，因此是金钱成本)。因此，一个消耗较少资源的替代方法是在分布中使用较少的资源，并在已经索引的原始日志序列上运行相同的处理。也就是说，我们避开了请求-响应循环中的处理步骤，只将原始日志索引到Elasticsearch中，稍后再进行处理。这些日志由一个预定的消费者作业取出(查询)，放入一个临时作业队列，进行处理，并索引回Elasticsearch。</p><p id="cd36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管这是一种对资源要求低得多的替代方法，但它会导致消耗已处理日志的系统预期并呈现低刷新率。与动态场景不同，除非计划的作业以非常高的频率运行(这最终会导致更高的成本)，否则索引不会频繁更新已处理的消息。然而，这种低刷新率可以适应某些场景，例如生成每日、每周或每月报告，以便挖掘诸如趋势和其他基于机器学习的信息的模式。它总是成本和用例之间的平衡，并且总是有机会成本被放弃。</p><h1 id="a26c" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">处理大量数据</h1><p id="246c" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">数据处理发生在业务逻辑层，它剥离、转换模式并将其转换成不同的格式，以便索引到Elasticsearch中。现在，在我上面提到的案例研究中，数据量可能非常大，特别是在给定时间段内有大量数据交易发生时。这使得ETL(<strong class="ih hj">Extract Transform Load</strong>)过程在所有极端情况下都更加困难:提取大量数据是困难的，因为它消耗大量带宽，转换会由于资源的内存和速度限制而变得困难，并且在转换过程可能扩展之后，加载会由于消息的大小而再次变得困难。因此，我们必须小心地从业务逻辑层处理ETL过程，使用首选语言:在本例中是Python。</p><p id="9bb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对我来说，选择python的唯一原因是<a class="ae jt" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank">熊猫</a>中<code class="du lq lr ls lt b">DataFrame</code>的概念。我已经习惯了它，并且我认为它是一个操作数据的简单而直观的工具。一个<code class="du lq lr ls lt b">DataFrame</code>本质上是一个二维矩阵(像一个表一样),包括一个标题，它给我们一个更简单的抽象来选择(一个接一个或者作为一个可迭代的),更新和删除数据点。我发现的另一个非常有用的功能是它可以序列化成不同的格式:比如<code class="du lq lr ls lt b">JSON</code>、<code class="du lq lr ls lt b">CSV</code>、<code class="du lq lr ls lt b">XLSX </code>等等。人们可以将<code class="du lq lr ls lt b">DataFrame</code>的概念想象成关系架构(如SQL)中<code class="du lq lr ls lt b">Table</code>的抽象。它允许您查询，从而读取和更新数据。利用<code class="du lq lr ls lt b">DataFrame</code>抽象的优势，我们可以将我们遇到的大量问题减少到一个非常小的集合中。当它与Python固有的工具如<code class="du lq lr ls lt b">Generators</code>一起使用时，似乎有可能避开我们原本会面临的一大堆问题。</p><h2 id="be94" class="lz kf hi bd kg ma mb mc kk md me mf ko iq mg mh ks iu mi mj kw iy mk ml la mm bi translated">解决萃取问题(E)</h2><p id="0907" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated"><code class="du lq lr ls lt b">elasticsearch-py</code>库在从Elasticsearch检索数据时使用Python的<em class="kd">生成器</em>的概念，这取决于您使用的搜索类型。如果您希望使用有限结果的搜索，也就是说，您希望指定一个限制并只检索有限数量的结果，那么您可以使用Elasticsearch提供的默认搜索API。然而，如果您想要检索特定查询的所有数据，您需要通过一个名为Scan或Scroll的API来浏览Elasticsearch提供的分页。现在，如果你看看这两个在<code class="du lq lr ls lt b">elasticsearch-py</code>中是如何实现的，你会发现一个显著的不同。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mn"><img src="../Images/a450345a664066b38eef94e1239bd056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNh8HWo5Nb58iZWtOKPa_w.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><code class="du lq lr ls lt b">elasticsearch-py</code>扫描API的实现。深入研究一下，您会注意到一个简洁的实现:在检索完成后，卷轴被清理。</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mo"><img src="../Images/2737d099a5c80f97670a1c6c9e02ea72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*DKFUzJrtZppr5Vj8mzgS-g.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><code class="du lq lr ls lt b">elasticsearch-py</code>搜索API的实现</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mp"><img src="../Images/afeb06f947c03e4591bd083de79cefa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*301rDt8RJaBgj9iVpH-Adg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">如何调用<strong class="bd kg"> scan( ) </strong></figcaption></figure><p id="a502" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du lq lr ls lt b">scan()</code>使用Python中的生成器，而<code class="du lq lr ls lt b">search()</code>没有。这在很大程度上是由于两个场景中预期的结果数量:扫描API比搜索API预期更多的结果，因此它使用Python的iterable技术来优化其执行。Python生成器有这个固有的属性，只有当你遍历它时，它才提供数据<em class="kd">。在连续的调用之间，它的执行状态会被专门记在内存中，这使得它成为遍历大量数据的理想选择。这在Python Wiki中通过使用两个简单的调用<code class="du lq lr ls lt b">sum(range(1000000))</code>和<code class="du lq lr ls lt b">sum(xrange(1000000))</code>来解释。<code class="du lq lr ls lt b">range()</code>先在内存中建立一个数字列表，并将其馈送给<code class="du lq lr ls lt b">sum()</code>，而<code class="du lq lr ls lt b">xrange()</code>逐个建立数字，并将其馈送给<code class="du lq lr ls lt b">sum()</code>，后者占用的内存较少。</em></p><blockquote class="mq mr ms"><p id="772c" class="if ig kd ih b ii ij ik il im in io ip mt ir is it mu iv iw ix mv iz ja jb jc hb bi translated">另一方面，当我们使用<code class="du lq lr ls lt b">xrange</code>时，我们不会产生在内存中构建1，000，000个元素列表的成本。由<code class="du lq lr ls lt b">xrange</code>创建的生成器将生成每个数字，该数字将消耗累加和。— Python Wiki</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/b01c47b6b1128164ef85844126146b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*fQxadyzRVCTy_WUhAjAC3Q.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">调用生成器产生的结果</figcaption></figure><p id="e366" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，Scan API的实现对<code class="du lq lr ls lt b">scan()</code>中的每个结果使用了一个<code class="du lq lr ls lt b">foreach</code>循环。这个<code class="du lq lr ls lt b">scan()</code>方法调用典型的<code class="du lq lr ls lt b">search()</code>方法，返回预定数量的消息。一个页面的默认大小是1000(即每次搜索检索1000)，但是您可以将它调整到您在Elasticsearch实例中配置的限制。所以，每次迭代会产生1000个结果(如果你设置<code class="du lq lr ls lt b">size</code>为1000)。将它调整到Elasticsearch提供的默认值(每次搜索10，000)会减少检索时间，但会增加转换的内存占用。</p><p id="1ac2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过遍历iterable调用生成器生成的响应。现在，我们必须注意，只有在遍历一次<em class="kd"> </em>即<em class="kd">时，才使用生成器。只有当我们不打算多次使用该组生成的值时，生成器才会提供性能优势。</em>”，如Python Wiki所述。因此，我们将进行迭代，进行必要的处理，并将它们附加到一个JSON(或一个等效物)中。在不需要这一步的地方，也就是说，一个简单的预处理步骤就足够了，不需要将它与其他结果合并，不需要持久化它们。您可以直接将iterable的结果用于转换步骤。</p><h2 id="d48a" class="lz kf hi bd kg ma mb mc kk md me mf ko iq mg mh ks iu mi mj kw iy mk ml la mm bi translated">用转型解决问题(T)</h2><p id="a119" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">在这一步中，我们面临的第一个问题是加载和处理大量数据。执行转换步骤的明显方法是在循环中遍历生成器响应，因为生成器的工作方式会占用较少的内存。</p><p id="9d87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，琐碎而直接的转换在复杂的场景中很少见，比如从弹性搜索消息(比如日志)中生成更复杂的见解。我们需要从多个索引中查询，合并结果并提供有用的结果。在这种情况下，你不能简单地使用迭代器而不被发现。您需要在迭代器中进行初步预处理，将预处理的结果保存在某个地方(例如文件系统)，并使用保存的结果将它们进一步转换成有价值的信息。</p><p id="db49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是熊猫的概念变得非常方便的地方。如果您已经指定了<code class="du lq lr ls lt b">chunksize</code>参数，Pandas <code class="du lq lr ls lt b">read_csv()</code>再次提供了一个生成器来进行迭代。<code class="du lq lr ls lt b">chunksize</code>指定了每次迭代需要检索多少数据点，与完全加载整个预处理文件相比，这大大减少了每次迭代的内存占用。</p><p id="8d05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，我们可以对熊猫数据帧使用<a class="ae jt" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html" rel="noopener ugc nofollow" target="_blank">琐碎操作。</a>我用过的最有用的是<a class="ae jt" href="https://cmdlinetips.com/2018/02/how-to-subset-pandas-dataframe-based-on-values-of-a-column/" rel="noopener ugc nofollow" target="_blank">过滤操作、</a> <a class="ae jt" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html" rel="noopener ugc nofollow" target="_blank">合并/连接操作</a>、<a class="ae jt" href="https://thispointer.com/pandas-find-duplicate-rows-in-a-dataframe-based-on-all-or-selected-columns-using-dataframe-duplicated-in-python/" rel="noopener ugc nofollow" target="_blank">重复相关操作</a>和<a class="ae jt" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html" rel="noopener ugc nofollow" target="_blank">应用操作</a>。</p><p id="4bad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Pandas针对大数据量下的使用进行了优化。但是，在某些操作(如合并操作)中，如果使用非常大的输入数据量，可能会发生内存溢出。例如，合并操作所需的空间受到两个输入数据集的大小和最终合并数据集的大小的限制。如果输入数据集非常大，则可以假设产生的输出也很大，这会导致更大的内存占用量(内部连接的情况除外，在这种情况下只有非常少量的公共元素)。必须小心处理这类场景，使用贪婪的方法，比如使用特定于您的业务案例的逻辑来分割输入数据空间。例如，如果您使用地理数据，您可以使用现有的层次结构(例如，在地理信息的情况下，您可以使用国家→城市等层次结构。)或创建逻辑层次结构，以便在执行合并/连接等内存密集型操作之前减少输入数据空间。此外，除了熊猫还有其他的库，比如<code class="du lq lr ls lt b"><a class="ae jt" href="https://dask.org/" rel="noopener ugc nofollow" target="_blank">Dask</a></code> <a class="ae jt" href="https://dask.org/" rel="noopener ugc nofollow" target="_blank">库</a>有非常好的基准。</p><p id="6eb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们必须考虑的另一个方面是<strong class="ih hj">资源使用与时间消耗</strong>。对于一个特定的转换作业，我们必须断言资源(磁盘、内存、CPU周期和网络)的使用量以及完成一个作业所需的时间。很多时候，为了满足另一个，你不得不放弃一个。例如，如果您正在一个行资源集群上运行，您必须放弃快速的处理速度。另一方面，如果您想要一个更快的处理管道，您将不得不拥有一个资源丰富的机器集群来为您完成这项工作。这总是你需要寻找的平衡，取决于你所处的环境。<a class="ae jt" href="https://www.elastic.co/guide/en/elasticsearch/plugins/current/mapper-size.html" rel="noopener ugc nofollow" target="_blank">有一些工具</a>可以用来断言，比如说， Elasticsearch中每个文档<em class="kd">的大小(在文档被索引之后)。此外，通过使用Python 中的活动监视器和<a class="ae jt" href="https://docs.python.org/3/library/resource.html" rel="noopener ugc nofollow" target="_blank">监视器，我们可以使用它们来对我们的资源消耗情况进行下降估计。为了监控日志记录方面的情况，我们可以使用<code class="du lq lr ls lt b">elasticsearch-py</code>库的默认日志记录器和一个附加的文件处理程序，如下所示。</a></em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mx"><img src="../Images/31ba276b2ac5f118c52c304720f85c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*r0z7LmtLUj7Qgm4tJFSjVQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">elasticsearch-py的默认记录器名为<strong class="bd kg"> elasticsearch </strong>，附加的文件在这里作为<strong class="bd kg"> log.log </strong>给出</figcaption></figure><h2 id="047f" class="lz kf hi bd kg ma mb mc kk md me mf ko iq mg mh ks iu mi mj kw iy mk ml la mm bi translated">解决装载问题(1)</h2><p id="27d9" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">使用<code class="du lq lr ls lt b">elasticsearch-py</code>提供的<a class="ae jt" href="https://elasticsearch-py.readthedocs.io/en/7.9.1/helpers.html" rel="noopener ugc nofollow" target="_blank">批量插入助手</a>可以将转换后的消息加载到Elasticsearch的另一个索引中。bulk helper需要另一个生成器来生成要插入的数据。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mp"><img src="../Images/7a56808147ef3eba1354c87ded3edb20.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*Q-oi5UPOavggVtqX4aG0qA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">批量生成器示例。</figcaption></figure><p id="dfbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我通常使用这个生成器向消息添加一个惟一的ID字段，并生成一个类似文档的结构(在Python中是一个字典),然后保存在Elasticsearch中。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es my"><img src="../Images/1ad98a8243eec40f9de4765f13d0976b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yb6czq8ZOvPbn-4eAYuy2g.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">Elasticsearch的插入示例，它返回成功插入的次数</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mz"><img src="../Images/01ca118fb621d4a8ee0b8a2671ce6cca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*KhLbpEcu-jyqCOaKXEZ3Lw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">向elasticsearch-py库中的Elasticsearch发送批量数据</figcaption></figure><p id="42a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<code class="du lq lr ls lt b">streaming_bulk()</code>函数中调用<code class="du lq lr ls lt b">_process_bulk_chunk()</code>函数，该函数向Elasticsearch提供的批量API发起批量请求(通过HTTP)。它处理错误和成功，你可以返回最终结果，就像我用变量<code class="du lq lr ls lt b">successes</code>做的那样。理想情况下，成功消息插入的数量应该等于变量<code class="du lq lr ls lt b">successes</code>的值。断言这一点，再加上使用<code class="du lq lr ls lt b">elasticsearch-py</code>日志，我们可以得出关于批量插入的良好结论。</p><p id="e1e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">插入一整批和插入多批有其不同之处。例如，插入一整批数据会耗费时间和带宽。另一方面，插入大量数据会重复查询Elasticsearch实例。这在某种程度上相当于一组重复的插入查询，而不是SQL中的批量插入。正如lukaseder在本文的<a class="ae jt" href="https://blog.jooq.org/2018/04/19/the-performance-difference-between-sql-row-by-row-updating-batch-updating-and-bulk-updating/" rel="noopener ugc nofollow" target="_blank">中指出的，批量/批量更新显然是这场SQL插入之战的赢家。在Elasticsearch中，对于每个插入，您需要建立一个HTTP连接(在应用层)，这转化为在传输层建立一个TCP套接字。这导致了传输层的高成本(想象一下用HTTP而不是WebSockets进行实时通信)。此外，Elasticsearch需要为您发送的每个插入请求调用解析器，如果您向Elasticsearch发送多个插入请求，而不是使用批量API，这将意味着它调用大量解析，并且特定请求的总往返时间(请求-响应时间)将显著增加，从而导致整个插入的效率和吞吐量下降。因此，在大容量数据插入中，应该首选批量API。</a></p><h1 id="7998" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">备注和结论</h1><p id="bcf2" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">我没有在上面的章节中留下关于Elasticsearch查询的注释，因为如果你使用的是<code class="du lq lr ls lt b">elasticsearch-dsl</code>，它们会非常直观。该库将每个查询转换成一个方法调用，将查询术语作为变量传递。例如，检查下面的查询。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es na"><img src="../Images/c6b717a83161c341521c7e5b8f7452e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*4VgoGi4eOCz7kQ8YjnMQlA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">搜索将<strong class="bd kg"> employeeName </strong>作为字段并且其值等于employeeName变量的消息，以及满足<strong class="bd kg"> search_type </strong>和<strong class="bd kg"> message_type </strong>值的字段<strong class="bd kg"> message </strong></figcaption></figure><p id="67eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该库使用<code class="du lq lr ls lt b">Search</code>对象启动搜索，并使用<code class="du lq lr ls lt b">execute()</code>调用搜索API，使用<code class="du lq lr ls lt b">scan()</code>调用扫描API。</p><p id="f739" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Elasticsearch的文档结构主要是JSON。因此，您需要在业务逻辑层中以类似的结构来表示您的对象。在像Java这样的语言中，这是使用映射结构来完成的，而在Python中，这是使用字典来完成的。字典可以通过使用Pandas和<code class="du lq lr ls lt b">json</code>库进行序列化和反序列化。在有用的情况下，我两者都用。更多的时候，我用熊猫的<code class="du lq lr ls lt b">read_json()</code>搭配<code class="du lq lr ls lt b">orient='records'</code>和<code class="du lq lr ls lt b">to_json()</code>来完成前面提到的任务。此外，当处理诸如epoch时间戳之类的情况时，我在通过<code class="du lq lr ls lt b">read_json()</code>反序列化JSON对象时使用<code class="du lq lr ls lt b">keep_default_dates=False</code>,因为这个函数似乎将时间戳对象转换为pandas-native形式，而这些形式使用<code class="du lq lr ls lt b">json</code>库的<code class="du lq lr ls lt b">dump()</code>方法是无法序列化的。如果你要一起使用它们，最好坚持这些实践。否则，最好在整个项目执行过程中使用一个库，为此，我更喜欢Pandas，因为它包含了许多<code class="du lq lr ls lt b">json</code>库不希望有的功能。<code class="du lq lr ls lt b">json</code>仅仅是一个从字典到json的序列化和反序列化库，反之亦然。</p><p id="0854" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当你在<code class="du lq lr ls lt b">elasticsearch-dsl</code>中使用<code class="du lq lr ls lt b">Search</code>对象时，另一种非常微妙的方式是使用<code class="du lq lr ls lt b">update_from_dict()</code>方法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nb"><img src="../Images/6dcd7b02561e3f86a3c7e89305a70972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*wCAfrbKcMtfNLup8BLpOug.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd kg"> update_from_dict( ) </strong>方法</figcaption></figure><p id="7e35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种方法提供了灵活性，在我们需要从Elasticsearch进行低级查询的情况下，<code class="du lq lr ls lt b">elasticsearch-py</code>提供了使用定制的类似字典的查询。</p><p id="8fb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，使用Elasticsearch和Python进行ETL过程的难易程度取决于您的用例。然而，当你使用Python时，编写内容的容易程度是非常高的，因为它包含了数据抽象，例如<code class="du lq lr ls lt b">DataFrames</code>。然而，必须始终记住，当我们评估流程的效率时，要注意资源消耗和时间之间的权衡。</p><blockquote class="mq mr ms"><p id="afb8" class="if ig kd ih b ii ij ik il im in io ip mt ir is it mu iv iw ix mv iz ja jb jc hb bi translated">注意:本文使用的几乎所有代码片段都来自于<code class="du lq lr ls lt b">elasticsearch-py</code>库，这是一个开源库，可以从Elastic的<a class="ae jt" href="https://github.com/elastic/elasticsearch-py" rel="noopener ugc nofollow" target="_blank">官方Github链接</a>到资源库中找到。其中一些是普通的代码片段，不是任何企业项目的一部分。</p></blockquote></div></div>    
</body>
</html>