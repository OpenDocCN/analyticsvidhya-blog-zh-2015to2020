<html>
<head>
<title>How does PySpark work? — step by step (with pictures)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark是如何工作的？—循序渐进(附图片)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-does-pyspark-work-step-by-step-with-pictures-c011402ccd57?source=collection_archive---------0-----------------------#2020-06-03">https://medium.com/analytics-vidhya/how-does-pyspark-work-step-by-step-with-pictures-c011402ccd57?source=collection_archive---------0-----------------------#2020-06-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7db6090e4212e93151774aab10ae9d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVUnsIEG_tMTycsG4xyeHQ.jpeg"/></div></div></figure><p id="5dc3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你是否发现自己在谈论Spark时并没有真正理解你所使用的所有词汇？对于Spark在Python中是如何使用的，是不是感觉没有一个清晰的心智模型？</p><p id="a133" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本指南旨在帮助您在几分钟内快速上手。</p><p id="cf68" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(如果你不知道Spark是什么，最好从这里开始<a class="ae jo" href="https://databricks.com/spark/about" rel="noopener ugc nofollow" target="_blank"/>，然后<a class="ae jo" href="https://realpython.com/pyspark-intro/#hello-world-in-pyspark" rel="noopener ugc nofollow" target="_blank">摆弄代码，</a>稍后再回到本文)。</p><h1 id="79f0" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">关键问题</h1><p id="bf7d" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">如果您曾经处理过数据，尤其是大数据，您可能知道Spark是一个非常棒的工具。如果你像我一样，使用Python做几乎所有的事情，你可能遇到过py Spark——也就是Python API for Spark。</p><blockquote class="ks"><p id="1285" class="kt ku hi bd kv kw kx ky kz la lb jn dx translated">但是PySpark到底是什么呢？</p><p id="92cb" class="kt ku hi bd kv kw kx ky kz la lb jn dx translated">Python不是有点慢吗？</p><p id="fc80" class="kt ku hi bd kv kw kx ky kz la lb jn dx translated">为什么会有人用这种语言做大数据处理？</p><p id="df87" class="kt ku hi bd kv kw kx ky kz la lb jn dx translated">当我使用PySpark时，我的数据实际上会发生什么变化？</p></blockquote><p id="3fb6" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">我们马上会回答这些问题。</p><p id="7f54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">不过，在我们深入本质之前，让我们先来看看一些核心的数据工程概念。我保证，会有帮助的。</p><h1 id="f1cc" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">关键心智模型</h1><p id="0a41" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">(如果对您来说这些都是老掉牙的内容，请随意跳过这一部分)</p><p id="542e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们简单看一下以下术语:</p><ul class=""><li id="3077" class="lh li hi is b it iu ix iy jb lj jf lk jj ll jn lm ln lo lp bi translated"><strong class="is hj"> <em class="lq">每个程序员都应该知道的数字</em> </strong></li><li id="0c61" class="lh li hi is b it lr ix ls jb lt jf lu jj lv jn lm ln lo lp bi translated"><strong class="is hj">大数据</strong></li><li id="7178" class="lh li hi is b it lr ix ls jb lt jf lu jj lv jn lm ln lo lp bi translated"><strong class="is hj">颠簸</strong></li><li id="e91f" class="lh li hi is b it lr ix ls jb lt jf lu jj lv jn lm ln lo lp bi translated"><strong class="is hj">并行计算</strong></li><li id="e6a7" class="lh li hi is b it lr ix ls jb lt jf lu jj lv jn lm ln lo lp bi translated"><strong class="is hj">进程间通信</strong></li><li id="70c9" class="lh li hi is b it lr ix ls jb lt jf lu jj lv jn lm ln lo lp bi translated">JVM </li></ul><p id="2f97" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在深入研究(Py)Spark之前，您需要理解这些概念。</p><blockquote class="ks"><p id="039b" class="kt ku hi bd kv kw kx ky kz la lb jn dx translated">为什么？</p></blockquote><p id="689f" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">你希望你的知识形成一棵坚实的树，而不是一堆树叶。所以我们从主干和粗枝开始——核心概念。这将使学习更精细的细节变得更加有趣和易于管理。</p><p id="e180" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">还有，你不希望这些核心知识在2周后蒸发掉。如果你想记住你在这里读过的东西，你需要使用间隔重复。Anki 是一个很好的工具，可以让你快速上手。</p><p id="0e11" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里有一个现成的Anki模板给你——这将帮助你巩固你从这篇文章中学到的东西</p><h2 id="bff8" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated"><strong class="ak"> <em class="mk">数字每个程序员都应该知道</em> </strong></h2><p id="f441" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><em class="lq">速度:处理器&gt;内存&gt;磁盘&gt;网络</em></p><p id="a787" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">几年前，一个名叫Peter Norvig(谷歌研究总监)的非常聪明的人在网上发布了一个数字表<a class="ae jo" href="http://norvig.com/21-days.html#answers" rel="noopener ugc nofollow" target="_blank">描述了计算机上发生的各种数据相关操作的速度。</a></p><p id="0a33" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些数字被称为每个程序员都应该知道的<em class="lq">数字</em>，它们对于理解数据工程非常重要。</p><p id="d9b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">不要说太多细节，要点如下</strong>:</p><blockquote class="ks"><p id="6c45" class="kt ku hi bd kv kw kx ky kz la lb jn dx translated">处理器速度极快。记忆力也挺快的。磁盘比较慢。访问网络中的其他计算机甚至更慢。</p><p id="aa5f" class="kt ku hi bd kv kw kx ky kz la lb jn dx translated">超负荷工作的计算机速度极慢。*</p></blockquote><p id="ce90" class="pw-post-body-paragraph iq ir hi is b it lc iv iw ix ld iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">*(我自己的话，不是彼得·诺维格的话)</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/8a3675c4e8b812441a52ec9f48c4d533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WVLybi00sRFYRld2X7Sfww.png"/></div></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">这是一个(有点过时的)类比，可以帮助你理解“快”和“慢”在计算机上的含义。假设你在美国西雅图的一个酒店房间里，你需要走到某个地方去获取数据。</figcaption></figure><h2 id="ea6e" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">大数据</h2><p id="4141" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><em class="lq">难以在笔记本电脑上处理</em></p><p id="a3b8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“大数据”是无法在您的计算机上轻松处理的大量数据。正是这<strong class="is hj">的数据量可能会让你的电脑开始工作得非常慢。</strong></p><p id="be17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当通过网络将大数据的一部分发送到另一台计算机，并在两台不同的计算机上的两个进程之间分配计算更快时，您知道您正在处理大数据。</p><h2 id="5c93" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">并行计算</h2><p id="8423" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><em class="lq">多台计算机一起处理数字</em></p><p id="c6ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是指两个进程(程序)同时处理同一个数据集的不同部分。这通常比一个进程试图做所有的工作要快。</p><p id="8b63" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">分布式计算是相似的，但是进程运行在不同的计算机上。Spark充分利用了分布式计算。</p><h2 id="e0fe" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">痛打</h2><p id="a57c" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><em class="lq">太多的数据会堵塞你的电脑</em></p><figure class="mm mn mo mp fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/b1db54f88f877c803a942cbd0ada5fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/1*Z-mcQcJYM9OBfKh8I-7kYA.gif"/></div></figure><p id="d05d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果某个程序占用了太多的内存，你的计算机会大大变慢。</p><p id="b52f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">进程需要随机存取内存(RAM)来快速运行。当你启动一个进程(程序)时，操作系统将开始给它分配内存。但是如果没有足够的内存来存放所有的数据呢？</p><p id="6b5d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你的程序试图处理的数据超过了它的内存容量，它将开始花费大量的时间从磁盘上存储和读取数据。这太慢了。</p><p id="15bf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">想象一下，你正在参加数学考试，你正试图解一个长方程。突然，你的试卷上的空间用完了，你不得不开始在泥板上划动你的笔记！这将花费很多时间，并减缓你的速度。同样，您的应用程序正在做更少的计算，因为它忙于做所有的家务！</p><p id="1d95" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">更糟糕的是，你的程序不再和处理器交流了(它正忙着在粘土板上划，也就是你的硬盘)。这使得处理器认为你不太需要它，它将开始接受其他任务，这给可用资源带来了更大的压力。</p><p id="0ab5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计算可能会在某个时候完成，但会花费很多时间。你要不惜一切代价避免这种情况。将一些数据发送到另一台计算机可能会更快，即使通过网络发送相对较慢(在计算机世界中)。</p><h2 id="8ca8" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">进程间通信</h2><p id="3458" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><em class="lq">当一个程序必须和另一个程序对话的时候</em></p><p id="da3b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">程序之间是如何对话的？不允许一个进程在内存中创建一个对象(例如Python字典)，然后将这部分内存直接传递给另一个进程。</p><p id="c354" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">两个程序不允许访问彼此分配的内存空间，所以它们通常必须在其他地方“写东西”，在一个两个进程都可以访问的地方。</p><p id="e1f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">简而言之——进程为彼此留下消息。它可以是一个文件。它可以是发送到网络套接字的数据(例如，想象一下当您通过互联网发送web表单时会发生什么)。</p><p id="6528" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，在Python中，两个不同的进程可以将彼此的数据作为一串字节保存在硬盘上(Python称之为“<a class="ae jo" href="https://wiki.python.org/moin/UsingPickle" rel="noopener ugc nofollow" target="_blank">pickle</a>”)。</p><p id="50d5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在计算机的世界里，IPC是相当慢的东西。</p><h2 id="3349" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated"><strong class="ak"> JVM — Java虚拟机</strong></h2><p id="6a72" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><em class="lq">这是Java进程在你的计算机上运行的方式</em></p><p id="0819" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Java有自己特殊的方式将代码(文本)翻译成计算机指令JVM。它使Java变得非常快速和可移植。出于这个原因，许多语言都被设计成在JVM中运行——Java本身、Scala、Clojure，甚至有趣的实验，如<a class="ae jo" href="https://en.wikipedia.org/wiki/Jython" rel="noopener ugc nofollow" target="_blank"> Jython </a>。</p><p id="9d36" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了简单起见，无论何时在本文中看到JVM这个词，只要想到“Java”就可以了。</p></div><div class="ab cl mv mw gp mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hb hc hd he hf"><p id="2533" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们已经介绍了相关的CS概念，让我们将它们与PySpark的工作原理以及它的优缺点联系起来。</p><h1 id="5803" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">PySpark循序渐进</h1><h2 id="1e97" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">当你在终端中输入“pyspark”时会发生什么？</h2><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/782edfc3e4b3089347b58c7e8f6e4fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4uH6Xje4JJXVArHEiXlF2g.png"/></div></div></figure><p id="ccd6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该命令启动一个Python程序，您可以与之交互。它还启动了…一个JVM程序！</p><p id="0a4d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用<code class="du nd ne nf ng b">ps aux</code>可以看到正在运行的Python和JVM进程:</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/e7decf0756af2556057da20f48a882d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IUm2Ua-T9oAAbfJZv51FEA.png"/></div></div></figure><p id="7e3a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PySpark所做的是，它允许你使用Python程序向名为Spark的JVM程序发送命令！</p><p id="3920" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">迷茫？</p><p id="44d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里的关键点是<strong class="is hj"> Spark是用Java和Scala编写的，而不是用Python </strong>编写的。所有的计算、所有的查询优化、所有酷的火花都发生在Python程序之外。</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ni"><img src="../Images/ed6d42fe1e2c27116de1b4463fcb24c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iFON_HLetRXovNWv2m8lJw.png"/></div></div></figure><p id="cd00" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么，为什么首先要有Python应用程序呢？不会让事情变得更复杂吗？</p><p id="52a1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嗯，是的。但是Spark对于数据科学家、数据分析师和其他帮助你的企业蓬勃发展，但通常不喜欢使用Java/Scala的人来说非常有用。</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nj"><img src="../Images/69dd59cd6f536ea15d2b7f1b094c9572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FbPER287J_QwTibo6GCG7Q.jpeg"/></div></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">数据科学家太忙了，没有时间研究Java</figcaption></figure><p id="f72a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，有人想出了一种通过Python控制Spark的方法，而不是让人们学习Java或Scala。</p><h2 id="9482" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">PySpark中的“火花”对象</h2><p id="d8e4" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">您的PySpark shell带有一个名为<code class="du nd ne nf ng b">spark</code>的变量。就变量而言，这个很酷。</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nk"><img src="../Images/904cef757567281647426422cc04a9ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yo2olCJrrxqwBgXjufxnnw.png"/></div></div></figure><p id="6bc9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，您可以启动<code class="du nd ne nf ng b">pyspark</code> shell并输入<code class="du nd ne nf ng b">spark.sql(.....)</code>，然后立即对来自某个数据库的数据进行一些转换。只用一行代码！—那太好了。</p><p id="4dff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是因为<code class="du nd ne nf ng b">spark</code>变量让您可以访问一个叫做<code class="du nd ne nf ng b">sparkContext</code>的东西。</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nl"><img src="../Images/893914d1792c7a6d86b8959b9b4be0d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KUeP13gL-dz9lFopXJ5Isw.png"/></div></div></figure><p id="7b4b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您与之交互的这个<code class="du nd ne nf ng b">sparkContext</code>是来自<code class="du nd ne nf ng b">pyspark</code>库的Python对象。如你所见，它似乎做了很多重要的事情:<code class="du nd ne nf ng b">runJob</code>、<code class="du nd ne nf ng b">stop</code>、<code class="du nd ne nf ng b">setLogLevel</code>、<code class="du nd ne nf ng b">cancelAllJobs</code> …:</p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="e165" class="lw jq hi ng b fi nq nr l ns nt">&gt;&gt;&gt; dir(spark.sparkContext)</span><span id="a88f" class="lw jq hi ng b fi nu nr l ns nt">[... 'accumulator', 'addFile', 'addPyFile', 'appName', 'applicationId', 'binaryFiles', 'binaryRecords', 'broadcast', 'cancelAllJobs', 'cancelJobGroup', 'defaultMinPartitions', 'defaultParallelism', 'dump_profiles', 'emptyRDD', 'environment', 'getConf', 'getLocalProperty', 'getOrCreate', 'hadoopFile', 'hadoopRDD', 'master', 'newAPIHadoopFile', 'newAPIHadoopRDD', 'parallelize', 'pickleFile', 'profiler_collector', 'pythonExec', 'pythonVer', 'range', 'runJob', 'sequenceFile', 'serializer', 'setCheckpointDir', 'setJobDescription', 'setJobGroup', 'setLocalProperty', 'setLogLevel', 'setSystemProperty', 'show_profiles', 'sparkHome', 'sparkUser', 'startTime', 'statusTracker', 'stop', 'textFile', 'uiWebUrl', 'union', 'version', 'wholeTextFiles']</span></pre><p id="475f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么，让我们看看这个神秘的火花背景到底是什么。</p><h2 id="30a5" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">pyspark是什么？SparkContext真的有吗？</h2><p id="fabc" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">来看看它的<a class="ae jo" href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext" rel="noopener ugc nofollow" target="_blank">官方文档</a>:</p><blockquote class="nv nw nx"><p id="77ef" class="iq ir lq is b it iu iv iw ix iy iz ja ny jc jd je nz jg jh ji oa jk jl jm jn hb bi translated">Spark功能的主要入口点。SparkContext表示到Spark集群的连接，可用于在该集群上创建RDD和广播变量。</p></blockquote><p id="ac3d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嗯，这有点误导。Python 中的SparkContext <em class="lq">真正做的唯一一件事是，它连接到你计算机上的一个网络端口，通过那个端口，它到达JVM程序(Spark)中的一个SparkContext对象。然后<strong class="is hj">你的Python SparkContext告诉Java SparkContext你想要做什么。</strong></em></p><p id="ea5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">想象一下，你在你的私人飞机上，你告诉你的助手约翰·史密斯，你想去肯尼亚的内罗毕。约翰·史密斯走进驾驶舱和飞行员(也是约翰·史密斯)交谈。然后飞行员把飞机带到内罗毕。它们的名字是一样的，但是只有飞行员约翰·史密斯真正驾驶飞机。</p><p id="42b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">类似地，Java中的SparkContext实际上完成了所有很酷的、繁重的Spark任务，比如启动作业和接收结果。Python对象本质上是一个传声筒，您可以通过它与Java对象对话。</p><p id="eab8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">类似地，整个PySpark库本质上是位于重型Java机器旁边的一层薄薄的Python。</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div class="er es ob"><img src="../Images/85411131942fbdb2539f593422003ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*7J8YTnvtcITef0m_rsj3hw.jpeg"/></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">Java SparkContext — Pilot(飞行)</figcaption></figure><figure class="mm mn mo mp fd ij er es paragraph-image"><div class="er es oc"><img src="../Images/294857f4101077d8b73cd0ac33b273ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*LB69-0WcUUVn-JCJG3eeww.jpeg"/></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">python spark context——不是一个飞行员(只是说话)</figcaption></figure><h2 id="bba9" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">等等，PySpark是怎么和Java进程对话的？这是什么鬼把戏？</h2><p id="5773" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">PySpark之所以能够在JVM进程中完成任务，要感谢一个名为Py4J的Python库(如“Python for Java”)。Py4J允许Python程序:</p><ul class=""><li id="c197" class="lh li hi is b it iu ix iy jb lj jf lk jj ll jn lm ln lo lp bi translated">打开一个监听端口(25334)</li><li id="ad9a" class="lh li hi is b it lr ix ls jb lt jf lu jj lv jn lm ln lo lp bi translated">启动一个JVM程序</li><li id="bd9f" class="lh li hi is b it lr ix ls jb lt jf lu jj lv jn lm ln lo lp bi translated">让JVM程序监听不同的网络端口(25333)</li><li id="ded1" class="lh li hi is b it lr ix ls jb lt jf lu jj lv jn lm ln lo lp bi translated">向Java进程发送命令并监听响应</li></ul><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es od"><img src="../Images/2904c74ad7aa57500c36a0917b814604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sj9XLmq2EpR0QCvVVZdf6w.png"/></div></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">作者:皮奥特·茹萨科夫斯基，<a class="ae jo" href="https://deepsense.ai/cooperative-data-exploration/" rel="noopener ugc nofollow" target="_blank">https://deepsense.ai/cooperative-data-exploration/</a></figcaption></figure><p id="7875" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Py4J可以在Python app和JVM app之间搭建桥梁！</p><p id="f2c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">难怪PySpark的作者决定使用该库来创建Python shell和Spark之间的桥梁。</p><h2 id="9ecd" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">呃，为什么PySpark通过端口与Spark通信？不是说网络连接慢吗？</h2><p id="2de3" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">是的。使用PySpark会使基于Spark的应用程序变慢。您在这里处理的是进程间通信！</p><p id="bfeb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果希望Python进程和JVM进程相互通信，它们不能只访问内存中的相同数据——操作系统不允许这样做。</p><p id="7ba6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，这两个进程既可以将消息写入(序列化)到一个文件中，也可以通过网络套接字相互通信。这也需要数据的序列化和反序列化。</p><p id="a6ed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为什么这是个问题？</p><p id="c155" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">想象一下，用Python加载一个4GB的pandas数据帧并对其进行处理，然后将其发送到Spark。这个4GB的数据帧必须被序列化(<a class="ae jo" href="https://github.com/apache/spark/blob/master/python/pyspark/context.py#L86" rel="noopener ugc nofollow" target="_blank">腌制</a>)成一个字节流，通过<a class="ae jo" href="https://github.com/apache/spark/blob/master/python/pyspark/context.py#L567" rel="noopener ugc nofollow" target="_blank">临时文件</a>或<a class="ae jo" href="https://github.com/apache/spark/blob/master/python/pyspark/context.py#L555" rel="noopener ugc nofollow" target="_blank">特殊服务器</a>传递给JVM进程，然后JVM进程将对其进行反序列化……然后Spark必须再次序列化结果，将其返回给Python。</p><p id="6ef4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">毫无疑问，使用PySpark会让基于Spark的应用程序做很多额外的事情。</p><h2 id="38d5" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">那么使用PySpark有什么好处呢？</h2><figure class="mm mn mo mp fd ij er es paragraph-image"><div class="er es oe"><img src="../Images/305b836b7cca4ff65bb0dfb9efbd0fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*yTVVp1pwCKxGUyyaCeZ9zg.png"/></div></figure><p id="177a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们前面所说的，很多从事数据工作的人喜欢PySpark，因为它允许他们使用Python。Python对于任何需要快速构建原型、探索数据或思考问题的时候都非常有用。</p><p id="d9f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它的学习曲线也很低，这意味着拥有领域知识的人(分析师)可以轻松地将他们的知识转化为代码，而不必是经验丰富的程序员。这对于你的业务来说是非常好的，因为知识可以很快转化为解决方案(事实上，这些解决方案有时会让开发人员想哭，这是另一个讨论:)。</p><h2 id="8fda" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">我能看到我的数据逐步发生变化的例子吗？</h2><p id="e57d" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">当然可以。让我们看一个从PySpark执行作业的最小例子。</p><p id="50e5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们启动Python shell和JVM:</p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="00ff" class="lw jq hi ng b fi nq nr l ns nt">pyspark</span></pre><p id="b531" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以看到Python和Java正在运行，这两个进程之间有一点点网络通信:</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es of"><img src="../Images/36463c84ad9bf763946e9e6f977a2b84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y4yXuLELi3m7ppSzBX_esA.png"/></div></div></figure><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es og"><img src="../Images/dbfb4a2e959dcff0a7dc43789e4542fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vFGBDjZAFOyKKWdAWAi2xQ.png"/></div></div></figure><p id="7bfa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们用Python创建一些数据:</p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="ef45" class="lw jq hi ng b fi nq nr l ns nt">&gt;&gt;&gt; some_string = spark.sparkContext.parallelize("hello hello hello")</span></pre><p id="b0db" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">什么都没发生。如果我们转到<code class="du nd ne nf ng b">localhost:4040</code>，我们可以看到Spark UI还没有显示任何工作。</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oh"><img src="../Images/4176d5d6f7cf5b4da341957028199733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3J91qhJIctcpmNX1y_Jzg.png"/></div></div></figure><p id="4d21" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是意料之中的。<code class="du nd ne nf ng b">parallelize</code>被懒洋洋地执行。这是因为<code class="du nd ne nf ng b">some_string</code>代表了一个<a class="ae jo" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations" rel="noopener ugc nofollow" target="_blank">火花RDD </a>(一个用于处理分布式数据集的接口):</p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="3a38" class="lw jq hi ng b fi nq nr l ns nt">&gt;&gt;&gt; type(some_string)<br/>&lt;class 'pyspark.rdd.RDD'&gt;</span></pre><p id="808c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">RDD的很多操作都很懒惰——这是Spark设计的一部分。除非你在RDD上调用一个<em class="lq">动作</em>操作，否则它们不会执行(也就是说，你要求Spark给你一个结果)。</p><p id="56d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们对数据采取行动:</p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="2c03" class="lw jq hi ng b fi nq nr l ns nt">&gt;&gt;&gt; some_string.take(5)<br/>['h', 'e', 'l', 'l', 'o']</span></pre><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oi"><img src="../Images/9bccde79c8c151ecb9d507eed3123a0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_1cjWdUZws3xQPFJcSzaA.png"/></div></div></figure><p id="8058" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">很好，我们可以看到Spark执行了一些东西。您可以看到Python向JVM发送了一个请求，JVM做出了响应(并执行了一些额外的网络活动):</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oj"><img src="../Images/cc54be81176510e8f9c1f1c01eb3dfa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x-U9wTJbHC3JNqvEuGR3qw.png"/></div></div></figure><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oj"><img src="../Images/b88defbe8006c2001d1782e713c9ff0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nJfOVT6eaRK9h9P3D9poDg.png"/></div></div></figure><p id="3cbf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么，在这个简单的例子中，您的数据发生了什么变化呢？</p><p id="b808" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Python </strong>:</p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="323a" class="lw jq hi ng b fi nq nr l ns nt">Serialize "hello hello hello" -&gt; temporary file <br/>Tell JVM (via Py4J) to pick up the file and create a Java RDD ("parallelize" the data)<br/>Create a Python variable to store information about the Java RDD</span></pre><p id="d2a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> JVM </strong></p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="1691" class="lw jq hi ng b fi nq nr l ns nt">Read the temporary file into a collection of (byte) arrays<br/> <br/>Create a Java RDD object from this collection (partitions and distributes the data in memory - this is the point of the parallelize operation)</span><span id="9296" class="lw jq hi ng b fi nu nr l ns nt">Tell Python where to find the object in the JVM </span></pre><p id="78f7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Python </strong></p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="3061" class="lw jq hi ng b fi nq nr l ns nt">Ask JVM for the first 5 records from the data stored in Spark's memory</span></pre><p id="8fd8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">JVM </p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="e8ff" class="lw jq hi ng b fi nq nr l ns nt">Return results via Py4J</span></pre><p id="1845" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Python </strong></p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="1f88" class="lw jq hi ng b fi nq nr l ns nt">Unpickle incoming data into a string<br/>Display result<br/>['h', 'e', 'l', 'l', 'o']</span></pre><p id="800d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个简单的例子中，Spark没有做多少有用的事情——它只是从Python中取出序列化数据，将其分割成分区，并存储在自己的(分布式)内存中。</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ok"><img src="../Images/8d9d159b97bbd21a25ec0defc3491948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oBanHltBljQEdMqF8Tg4VQ.png"/></div></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">在处理大数据时，将数据存储在多台计算机的内存中非常有用</figcaption></figure><p id="f9b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们想进行某种转换，比如将文本数据转换成大写，会怎么样呢？JVM如何知道如何在字符串上执行Python代码，比如<code class="du nd ne nf ng b">.upper()</code>？Spark是否将Python语言中的所有函数都映射到Scala/Java中的等价函数？</p><p id="443d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">并没有。<a class="ae jo" href="https://github.com/apache/spark/blob/7dff3b125de23a4d6ce834217ee08973b259414c/core/src/main/scala/org/apache/spark/SparkEnv.scala#L75" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Spark创建Python工作进程来执行Python函数。</strong> </a></p><p id="e792" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它将序列化的数据、<em class="lq">和</em>一些序列化的Python函数交给它们来执行。</p><p id="ba05" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们创建一个更大的输入来说明这一点:</p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="6007" class="lw jq hi ng b fi nq nr l ns nt">&gt;&gt;&gt; something = 'hello ' * 1000000<br/>&gt;&gt;&gt; another_string = spark.sparkContext.parallelize(something)</span></pre><p id="7f25" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们对它运行一个地图函数并请求一个结果:</p><pre class="mm mn mo mp fd nm ng nn no aw np bi"><span id="4ca7" class="lw jq hi ng b fi nq nr l ns nt">&gt;&gt;&gt; another_string.map(lambda a: a.upper()).take(100)<br/>20/06/01 16:39:24 WARN TaskSetManager: Stage 2 contains a task of very large size (1507 KB). The maximum recommended task size is 100 KB.</span><span id="f244" class="lw jq hi ng b fi nu nr l ns nt">['H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L', 'O', ' ', 'H', 'E', 'L', 'L']</span></pre><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ol"><img src="../Images/094b44e2c282a735181c56a7cd9dccc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-qOxItTm55NmBZFE5AB1A.png"/></div></div></figure><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es om"><img src="../Images/e1c124c5e8920d19281175d25701f279.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EdrlDmGx_9xZ-wrUJoezjA.png"/></div></div></figure><p id="653d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以看到JVM发出了一个大文件。这是发送到Spark集群进行并行化的输入数据(即，保存在Spark workers的内存中)。</p><p id="1af0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(注意:在这里,“集群”并不十分引人注目——它只是本地计算机上的一个进程；但通常它是云中一组计算机上的一组进程)。</p><p id="e975" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有趣的是，您可以看到新的Python进程正在被创建(这些是我们之前提到的Python工作者):</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es on"><img src="../Images/7351d19f3d9d24d104b77e64c805b92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35m5Ce-ikTPC1T2zDKeLaQ.png"/></div></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">s</figcaption></figure><p id="e3bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每个PySpark进程都从Spark接收数据和代码。他们对数据执行<code class="du nd ne nf ng b">.upper()</code>函数，然后将结果再次序列化为Pickle格式。序列化的数据被传递回Spark并存储在Spark workers的内存中。</p><p id="8488" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，Spark将结果(1kb大)发送回我们最初的PySpark进程。</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oo"><img src="../Images/b097ee5b45e42fe0852745746cfb9ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s4rO3dHJtaC4SmAWBk6RQQ.png"/></div></div></figure><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ol"><img src="../Images/094b44e2c282a735181c56a7cd9dccc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-qOxItTm55NmBZFE5AB1A.png"/></div></div></figure><p id="c274" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PySpark然后反序列化结果，并为您打印出一个Python数组。</p><p id="1bef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">咻，在各种Python和JVM进程之间有很多来回。</p><p id="66d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">幸运的是，你很少会简单地手工创建一百万行文本并在一张空白的RDD上工作。你通常会从一些SQL数据库或pandas中读取数据。您的数据也将更加结构化，例如，将有带名称的列。</p><p id="fad0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这允许PySpark使用更优化的解决方案，如DataFrame类或Apache Arrow序列化格式，并让Spark完成大部分繁重的计算(数据连接、过滤等)。</p><p id="def1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据您的代码，Python进程要么需要做很少的工作，要么需要做很多工作。一个很好的例子是决定是在pandas中还是在Spark中进行表连接——当然，您希望是后者。这可不是白叫Py <strong class="is hj"> Spark </strong>的。</p></div><div class="ab cl mv mw gp mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hb hc hd he hf"><h1 id="114b" class="jp jq hi bd jr js op ju jv jw oq jy jz ka or kc kd ke os kg kh ki ot kk kl km bi translated">摘要</h1><h2 id="4dc3" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated"><strong class="ak">py spark到底是什么？</strong></h2><p id="4d1a" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">它是实际Spark应用程序的Python包装器，用Java和Scala编写。</p><h2 id="eb80" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated"><strong class="ak">Python是不是有点慢？</strong></h2><p id="b429" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">是的，使用PySpark会使事情变得更慢——它必须通过网络连接与实际的Spark对话。除此之外，在整个作业执行过程中，数据会被序列化和反序列化。</p><h2 id="708b" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated"><strong class="ak">为什么会有人用PySpark做大数据处理？</strong></h2><p id="2abc" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">因为PySpark非常方便，可以让人们提高工作效率。</p><h2 id="d4ef" class="lw jq hi bd jr lx ly lz jv ma mb mc jz jb md me kd jf mf mg kh jj mh mi kl mj bi translated">使用PySpark时，我的数据会发生什么变化？</h2><p id="3092" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">数据被序列化到一个文件中，并被Spark JVM进程获取。Spark在其工作人员的内存中分发数据。</p><p id="4750" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后Spark可以运行内置的Spark操作，比如对数据进行连接、过滤和聚合——如果它能够读取数据的话。</p><p id="10a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">否则，Spark可以启动一组新的Python进程，向它们传递一些序列化的Python代码和序列化的数据，并要求它们对数据执行代码。</p><p id="c814" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">无论哪种情况，结果都会作为序列化数据再次存储在Spark的内存中。</p><p id="89f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，您的数据从Spark发送回来，并从一堆字节反序列化为Python对象。你现在可以打印这些对象，将它们传递给其他函数等等。</p></div><div class="ab cl mv mw gp mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hb hc hd he hf"><p id="0e34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注</strong>:</p><p id="1e0f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我简化了一些实现细节，以便帮助您关注关键的心智模型。我将在未来写一篇更高级的文章来涵盖更令人讨厌的细节。</p></div><div class="ab cl mv mw gp mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hb hc hd he hf"><p id="168d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">属性</strong></p><p id="f986" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">头图—<a class="ae jo" href="https://www.reddit.com/r/woahdude/comments/55klru/lightning_snake/" rel="noopener ugc nofollow" target="_blank">https://www . Reddit . com/r/woahdude/comments/55 klru/lightning _ snake/？UTM _ source = share&amp;UTM _ medium = web2x</a></p><p id="2747" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“颠簸”——<a class="ae jo" href="https://upload.wikimedia.org/wikipedia/commons/6/67/Thrashing.GIF" rel="noopener ugc nofollow" target="_blank">https://upload . wikimedia . org/Wikipedia/commons/6/67/颠簸。GIF </a></p><p id="47ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“数据科学家”——https://www.pxfuel.com/en/free-photo-jmtew<a class="ae jo" href="https://www.pxfuel.com/en/free-photo-jmtew" rel="noopener ugc nofollow" target="_blank"/></p><p id="4662" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“存储延迟”——<a class="ae jo" href="https://blog.codinghorror.com/content/images/2014/May/storage-latency-how-far-away-is-the-data.png" rel="noopener ugc nofollow" target="_blank">https://blog . coding horor . com/content/images/2014/May/storage-latency-how-far-away-the-data . png</a></p><h1 id="865c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">关于作者</h1><p id="69bf" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">Obi是一名具有软件测试背景的数据工程师。他喜欢健身操，抱怨，和对生产力的极客。</p><p id="8069" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在<a class="ae jo" href="https://www.linkedin.com/in/obiorciuch/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上和他联系。</p></div></div>    
</body>
</html>