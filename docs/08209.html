<html>
<head>
<title>Building ML Model to predict whether the cancer is benign or malignant on Breast Cancer Wisconsin Data Set !! Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在乳腺癌Wisconsin数据集上建立ML模型预测癌症是良性还是恶性！！第二部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-ml-model-to-predict-whether-the-cancer-is-benign-or-malignant-on-breast-cancer-wisconsin-b8249b55fc62?source=collection_archive---------5-----------------------#2020-07-21">https://medium.com/analytics-vidhya/building-ml-model-to-predict-whether-the-cancer-is-benign-or-malignant-on-breast-cancer-wisconsin-b8249b55fc62?source=collection_archive---------5-----------------------#2020-07-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f04f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你跳过了<a class="ae jd" rel="noopener" href="/@shahid_dhn/building-ml-model-to-predict-whether-the-cancer-is-benign-or-malignant-on-breast-cancer-wisconsin-a09b6c32e7b8"> <strong class="ih hj">第一部分</strong></a><strong class="ih hj"/>，我请求你先看完它。</p><p id="86b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">做机器学习模型预测癌症是良性还是恶性的步骤包括:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="5ab9" class="jn jo hi jj b fi jp jq l jr js">Step 1: Define Problem Statement<br/>Step 2: Data Source<br/>Step 3: Cleaning the Data<br/>Step 4: Data Analysis and Exploration<br/><strong class="jj hj">Step 5: Feature Selection</strong><br/>Step 6: Data Modelling<br/>Step 7: Model Validation<br/>Step 8: Hyperparameter Tuning<br/>Step 9: Deployment</span><span id="df22" class="jn jo hi jj b fi jt jq l jr js">In this part 2, I will cover <strong class="jj hj">step 5</strong> </span><span id="d029" class="jn jo hi jj b fi jt jq l jr js">We are using the Breast Cancer Wisconsin <a class="ae jd" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29" rel="noopener ugc nofollow" target="_blank"><strong class="jj hj">dataset</strong></a> available on UCI Machine Learning Repository.</span></pre><h2 id="4b07" class="jn jo hi bd ju jv jw jx jy jz ka kb kc iq kd ke kf iu kg kh ki iy kj kk kl km bi translated">定义问题陈述</h2><p id="d718" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们的目标是确定哪些特征最有助于预测恶性或良性癌症，并对乳腺癌进行良性或恶性分类。</p><h2 id="5664" class="jn jo hi bd ju jv jw jx jy jz ka kb kc iq kd ke kf iu kg kh ki iy kj kk kl km bi translated">数据源</h2><p id="a734" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们已经使用了公开可用的数据集乳腺癌威斯康星州，并已从UCI机器学习库下载。资源库:<a class="ae jd" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/datasets/Breast+Cancer+Wisconsin+% 28 diagnostic % 29</a>。</p><p id="01d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤5:预测建模中的特征选择技术</strong></p><p id="8f93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">基于投票的减少数据集中特征数量的方法</strong></p><p id="82e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的单元格中，我们将选择一组变量，最具预测性的变量，来构建我们的机器学习模型。特征选择是自动或手动选择对您感兴趣的预测变量或输出贡献最大的那些特征的过程。数据中包含不相关的要素会降低模型的准确性，并使模型基于不相关的要素进行学习。相关特征的选择也可以受益于正确的领域知识。</p><p id="ca21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么我们需要选择变量？</strong></p><ol class=""><li id="8118" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc kx ky kz la bi translated">对于生产:更少的变量意味着更少的客户端输入需求(例如，客户在网站或移动应用程序上填写表格)，因此用于错误处理的代码更少。这样就减少了出现bug的几率。</li><li id="003b" class="ks kt hi ih b ii lb im lc iq ld iu le iy lf jc kx ky kz la bi translated">对于模型性能:更少的变量意味着更简单、更易解释、更少过度拟合的模型</li></ol><p id="cb84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本<strong class="ih hj">第2部分中，</strong>我们将选择具有不同方法的特性</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="bd5e" class="jn jo hi jj b fi jp jq l jr js">1. Feature selection with <strong class="jj hj">correlation<br/></strong>2. Univariate feature selection <strong class="jj hj">(Chi-square)<br/></strong>3. Recursive feature elimination<strong class="jj hj"> (RFE) </strong>with random forest<br/>4. Recursive feature elimination with cross validation<strong class="jj hj">(RFECV)</strong> with random forest<br/>5. Tree based feature selection with random forest classification<br/>6. <strong class="jj hj">L1</strong>-based feature selection <strong class="jj hj">(LinearSVC)<br/></strong>7. Tree-based feature selection<strong class="jj hj"> (ExtraTrees)<br/></strong>8.<strong class="jj hj"> Vote based</strong> feature selection</span></pre><p id="1736" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">导入库</strong></p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="7cd4" class="jn jo hi jj b fi jp jq l jr js"><strong class="jj hj"># importing the libraries</strong><br/>import numpy as np <br/><strong class="jj hj"># data processing, CSV file I/O</strong><br/>import pandas as pd <br/><strong class="jj hj"># data visualization library</strong><br/>import seaborn as sns  <br/>import matplotlib.pyplot as plt<br/>import time</span></pre><p id="dcef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">导入数据集</strong></p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="e8f3" class="jn jo hi jj b fi jp jq l jr js">dataset = pd.read_csv('<a class="ae jd" href="https://raw.githubusercontent.com/Muhd-Shahid/Breast-Cancer-Wisconsin/master/data_breast-cancer-wiscons.csv'" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/Muhd-Shahid/Breast-Cancer-Wisconsin/master/data_breast-cancer-wiscons.csv'</a>)</span></pre><h2 id="d0fb" class="jn jo hi bd ju jv jw jx jy jz ka kb kc iq kd ke kf iu kg kh ki iy kj kk kl km bi translated">删除数据集中不必要的列</h2><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="d8d5" class="jn jo hi jj b fi jp jq l jr js"><strong class="jj hj"># y includes our labels and x includes our features</strong><br/>y = dataset.diagnosis # M or B <br/>list_drp = [‘Unnamed: 32’,’id’,’diagnosis’]<br/>x = dataset.drop(list_drp,axis = 1 )</span></pre><ol class=""><li id="8a3c" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc kx ky kz la bi translated"><strong class="ih hj">相关性特征选择</strong></li></ol><p id="8795" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从<a class="ae jd" href="https://seaborn.pydata.org/generated/seaborn.heatmap.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">热图</strong> </a>图中可以看出，来自<strong class="ih hj"> </strong> <a class="ae jd" rel="noopener" href="/@shahid_dhn/building-ml-model-to-predict-whether-the-cancer-is-benign-or-malignant-on-breast-cancer-wisconsin-a09b6c32e7b8"> <strong class="ih hj">第1部分</strong> </a>的radius_mean、perimeter_mean和area_mean相互关联，因此我们将仅使用area_mean。如果你问我如何选择area_mean作为要使用的特征，那么实际上没有正确的答案，我只是看着群体图，area_mean对我来说看起来很清楚，但我们不能在不尝试的情况下在其他相关特征中进行精确的分离。因此，让我们寻找其他相关的特征，并期待与随机森林分类器的准确性。</p><p id="fcff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">紧密度均值、凹度均值和凹点均值是相互关联的。所以我只选择凹度_均值。除此之外，半径se、周长se和面积se是相关的，我只使用面积se。半径_最差、周长_最差和面积_最差是相关的，所以我使用面积_最差。紧致_最差，凹度_最差和凹点_最差所以我用凹度_最差。紧致性_se，凹度_se和凹点_se所以我用凹度_se。texture_mean和texture_worst是相关的，我使用texture_mean。area_worst和area_mean是相关的，我用area_mean。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="294c" class="jn jo hi jj b fi jp jq l jr js">drop_list_cor = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']<br/>x_1 = x.drop(drop_list_cor,axis = 1 )        # do not modify x, we will use it later <br/>x_1.head()<br/><strong class="jj hj">selected_feature_corr</strong>=x_1.columns</span><span id="57c4" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>Index(['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean','symmetry_mean','fractal_dimension_mean', 'texture_se', 'area_se','smoothness_se', 'concavity_se', 'symmetry_se', 'fractal_dimension_se','smoothness_worst', 'concavity_worst', 'symmetry_worst',        'fractal_dimension_worst'],dtype='object')</strong></span></pre><p id="441d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。单变量特征选择(卡方)</strong></p><p id="2b84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在单变量特征选择中，我们将使用SelectKBest来移除除k个最高得分特征之外的所有特征。</p><div class="lg lh ez fb li lj"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" rel="noopener  ugc nofollow" target="_blank"><div class="lk ab dw"><div class="ll ab lm cl cj ln"><h2 class="bd hj fi z dy lo ea eb lp ed ef hh bi translated">sklearn.feature_selection。选择最佳-sci kit-学习0.23.1文档</h2><div class="lq l"><h3 class="bd b fi z dy lo ea eb lp ed ef dx translated">sci kit-learn:Python中的机器学习</h3></div><div class="lr l"><p class="bd b fp z dy lo ea eb lp ed ef dx translated">scikit-learn.org</p></div></div><div class="ls l"><div class="lt l lu lv lw ls lx ly lj"/></div></div></a></div><p id="a9ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种方法中，我们需要选择要使用多少功能。比如k(特征数)会是5还是10还是15？答案只有尝试或者直觉。我没有尝试所有的组合，但我只选择k = 10，并找到最好的10个特征。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="3784" class="jn jo hi jj b fi jp jq l jr js"><strong class="jj hj"># split data train 70 % and test 30 %</strong></span><span id="fd57" class="jn jo hi jj b fi jt jq l jr js">from sklearn.model_selection import train_test_split<br/>x_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)</span></pre><p id="47ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们对特征选择应用卡方检验</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="0047" class="jn jo hi jj b fi jp jq l jr js">from sklearn.feature_selection import SelectKBest<br/>from sklearn.feature_selection import chi2<br/># find best scored 10 features<br/>select_feature = SelectKBest(chi2, k=10).fit(x_train, y_train)<br/># let's print the number of total and selected features</span><span id="68f5" class="jn jo hi jj b fi jt jq l jr js"># this is how we can make a list of the selected features<br/><strong class="jj hj">selected_feature_chi2</strong> = x_train.columns[select_feature.get_support()]</span><span id="2588" class="jn jo hi jj b fi jt jq l jr js"># let's print some stats<br/>print('total features: {}'.format((x_train.shape[1])))<br/>print('selected features: {}'.format(len(selected_feature_chi2)))<br/>print('Chosen best 10 feature by Chi2:',selected_feature_chi2)</span><span id="2384" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:</strong><br/><strong class="jj hj">total features: 16 <br/>selected features: 10 <br/>Chosen best 10 feature by Chi2: Index(['texture_mean', 'area_mean', 'concavity_mean', 'symmetry_mean','area_se', 'concavity_se', 'smoothness_worst', 'concavity_worst','symmetry_worst','fractal_dimension_worst'],       dtype='object')</strong></span></pre><p id="0a80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。使用随机森林的递归特征消除(RFE)</strong></p><div class="lg lh ez fb li lj"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener  ugc nofollow" target="_blank"><div class="lk ab dw"><div class="ll ab lm cl cj ln"><h2 class="bd hj fi z dy lo ea eb lp ed ef hh bi translated">sklearn.feature_selection。RFE-sci kit-学习0.23.1文档</h2><div class="lq l"><h3 class="bd b fi z dy lo ea eb lp ed ef dx translated">递归特征消除的特征排序。给定一个为特征分配权重的外部估计器(例如…</h3></div><div class="lr l"><p class="bd b fp z dy lo ea eb lp ed ef dx translated">scikit-learn.org</p></div></div><div class="ls l"><div class="lz l lu lv lw ls lx ly lj"/></div></div></a></div><p id="2ccf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本上，它使用一种分类方法(在我们的例子中是随机森林)，为每个特征分配权重。其绝对权重最小的特征从当前集合特征中被剪除。该过程在删减集上递归重复，直到期望数量的特征</p><p id="aad0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像前面的方法一样，我们将使用10个特征。但是，我们会使用哪10个特性呢？我们将用RFE方法来选择它们。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="19db" class="jn jo hi jj b fi jp jq l jr js">from sklearn.feature_selection import RFE<br/># Create the RFE object and rank each pixel<br/>clf_rf_3 = RandomForestClassifier()      <br/>rfe = RFE(estimator=clf_rf_3, n_features_to_select=10, step=1)<br/>rfe = rfe.fit(x_train, y_train)<br/># let's print the number of total and selected features</span><span id="87ad" class="jn jo hi jj b fi jt jq l jr js"># this is how we can make a list of the selected features<br/><strong class="jj hj">selected_feature_rfe</strong> = x_train.columns[rfe.support_]</span><span id="cd30" class="jn jo hi jj b fi jt jq l jr js"># let's print some stats<br/>print('total features: {}'.format((x_train.shape[1])))<br/>print('selected features: {}'.format(len(selected_feature_rfe)))<br/>print('Chosen best 10 feature by rfe:',x_train.columns[rfe.support_])</span><span id="836b" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>total features: 16 <br/>selected features: 10 <br/>Chosen best 10 feature by rfe: Index(['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean','area_se','concavity_se', 'fractal_dimension_se', 'concavity_worst','symmetry_worst', 'fractal_dimension_worst'], dtype='object')</strong></span></pre><p id="1542" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">rfe选出的10个最佳特征是纹理均值、面积均值、平滑度均值、凹度均值、面积se、凹度se、分形维数se、凹度最差、对称性最差、分形维数最差。它们与前面的(selectkBest)方法相似。因此，我们不需要再次计算精度。简而言之，我们可以说我们用rfe和selectkBest方法做了很好的特性选择。然而，正如你所看到的，有一个问题，好吧，我除了我们用两种不同的方法找到最好的10个特征，这些特征几乎是相同的，但为什么是10。也许如果我们使用最佳2或最佳15特征，我们将具有更好的准确性。因此，让我们看看rfecv方法需要使用多少特性。</p><p id="3064" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。随机森林的交叉验证递归特征消除(rfe cv)</strong></p><div class="lg lh ez fb li lj"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" rel="noopener  ugc nofollow" target="_blank"><div class="lk ab dw"><div class="ll ab lm cl cj ln"><h2 class="bd hj fi z dy lo ea eb lp ed ef hh bi translated">sklearn.feature_selection。rfe cv-sci kit-学习0.23.1文档</h2><div class="lq l"><h3 class="bd b fi z dy lo ea eb lp ed ef dx translated">具有递归特征消除和最佳特征数量交叉验证选择的特征排序。看…</h3></div><div class="lr l"><p class="bd b fp z dy lo ea eb lp ed ef dx translated">scikit-learn.org</p></div></div><div class="ls l"><div class="ma l lu lv lw ls lx ly lj"/></div></div></a></div><p id="833b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们不仅要找到最佳特征，还要找到需要多少特征才能达到最佳精度。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="9d94" class="jn jo hi jj b fi jp jq l jr js">from sklearn.feature_selection import RFECV<br/># The "accuracy" scoring is proportional to the number of correct classifications<br/>clf_rf_4 = RandomForestClassifier() <br/>rfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation<br/>rfecv = rfecv.fit(x_train, y_train)</span><span id="3e83" class="jn jo hi jj b fi jt jq l jr js"># let's print the number of total and selected features</span><span id="3343" class="jn jo hi jj b fi jt jq l jr js"># this is how we can make a list of the selected features<br/><strong class="jj hj">selected_feature_rfecv</strong> = x_train.columns[rfecv.support_]</span><span id="952f" class="jn jo hi jj b fi jt jq l jr js"># let's print some stats<br/>print('total features: {}'.format((x_train.shape[1])))<br/>print('selected features: {}'.format(len(selected_feature_rfecv)))<br/>print('Optimal number of features :', rfecv.n_features_)<br/>print('Best features by rfecv:',x_train.columns[rfecv.support_])</span><span id="da4c" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>total features: 16 <br/>selected features: 9 <br/>Optimal number of features : 9 Best features by rfecv: Index(['texture_mean','area_mean','concavity_mean', 'fractal_dimension_mean','area_se','concavity_se', 'fractal_dimension_se','concavity_worst','symmetry_worst'],       dtype='object')</strong></span></pre><p id="93b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。基于树的特征选择和随机森林分类</strong></p><div class="lg lh ez fb li lj"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="lk ab dw"><div class="ll ab lm cl cj ln"><h2 class="bd hj fi z dy lo ea eb lp ed ef hh bi translated">3.2.4.3.1.sk learn . ensemble . randomforestclassifier-sci kit-learn 0 . 23 . 1文档</h2><div class="lq l"><h3 class="bd b fi z dy lo ea eb lp ed ef dx translated">class sk learn . ensemble . RandomForestClassifier(n _ estimators = 100，*，criterion='gini '，max_depth=None…</h3></div><div class="lr l"><p class="bd b fp z dy lo ea eb lp ed ef dx translated">scikit-learn.org</p></div></div><div class="ls l"><div class="mb l lu lv lw ls lx ly lj"/></div></div></a></div><p id="7f06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在随机森林分类方法中有一个特征重要性属性，即特征重要性(越高的特征越重要)。！！！使用feature_importance方法，在训练数据中不应该有相关的特征。随机森林在每次迭代中随机选择，因此特征重要性列表的顺序可以改变。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="8df6" class="jn jo hi jj b fi jp jq l jr js">clf_rf_5 = RandomForestClassifier()      <br/>clr_rf_5 = clf_rf_5.fit(x_train,y_train)<br/>importances = clr_rf_5.feature_importances_<br/>std = np.std([tree.feature_importances_ for tree in clf_rf.estimators_],<br/>             axis=0)<br/>indices = np.argsort(importances)[::-1]</span><span id="8be9" class="jn jo hi jj b fi jt jq l jr js"># Print the feature ranking<br/>print("Feature ranking:")</span><span id="17f4" class="jn jo hi jj b fi jt jq l jr js">for f in range(x_train.shape[1]):<br/>    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))</span><span id="087e" class="jn jo hi jj b fi jt jq l jr js"># Plot the feature importances of the forest</span><span id="60dd" class="jn jo hi jj b fi jt jq l jr js">plt.figure(1, figsize=(14, 13))<br/>plt.title("Feature importances")<br/>plt.bar(range(x_train.shape[1]), importances[indices],<br/>       color="g", yerr=std[indices], align="center")<br/>plt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)<br/>plt.xlim([-1, x_train.shape[1]])<br/>plt.show()</span><span id="a678" class="jn jo hi jj b fi jt jq l jr js"># let's print the number of total and selected features</span><span id="4c67" class="jn jo hi jj b fi jt jq l jr js"># this is how we can make a list of the selected features<br/><strong class="jj hj">selected_feature_rf</strong> = x_train.columns[indices]</span><span id="6172" class="jn jo hi jj b fi jt jq l jr js"># let's print some stats<br/>print('total features: {}'.format((x_train.shape[1])))<br/>print('Chosen optimal features by rf:',selected_feature_rf[1:10])</span><span id="5d7f" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>total features: 16 <br/>Chosen optimal features by rf: <br/>Index(['concavity_worst', 'area_se','concavity_mean', 'symmetry_worst', 'concavity_se','texture_mean','smoothness_mean', 'symmetry_mean','fractal_dimension_mean'],dtype='object')</strong></span></pre><figure class="je jf jg jh fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es mc"><img src="../Images/6c380b1402aba521b2a65c4890521cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iYakkpP85mU7bgNfcfWIdA.png"/></div></div></figure><p id="1f9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你在上面的图中看到的，在7个最佳特性之后，特性的重要性降低了。</p><h2 id="c309" class="jn jo hi bd ju jv jw jx jy jz ka kb kc iq kd ke kf iu kg kh ki iy kj kk kl km bi translated">6.基于L1的特征选择</h2><div class="lg lh ez fb li lj"><a href="https://scikit-learn.org/stable/modules/feature_selection.html?highlight=correlation%20matrix#feature-selection-using-selectfrommodel" rel="noopener  ugc nofollow" target="_blank"><div class="lk ab dw"><div class="ll ab lm cl cj ln"><h2 class="bd hj fi z dy lo ea eb lp ed ef hh bi translated">1.13.功能选择-sci kit-了解0.23.1文档</h2><div class="lq l"><h3 class="bd b fi z dy lo ea eb lp ed ef dx translated">sklearn.feature_selection模块中的类可用于…上的特征选择/维度缩减</h3></div><div class="lr l"><p class="bd b fp z dy lo ea eb lp ed ef dx translated">scikit-learn.org</p></div></div><div class="ls l"><div class="mj l lu lv lw ls lx ly lj"/></div></div></a></div><p id="02f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://scikit-learn.org/stable/modules/linear_model.html#linear-model" rel="noopener ugc nofollow" target="_blank">用L1范数惩罚的线性模型</a>具有稀疏解:它们的许多估计系数为零。当目标是减少数据的维数以用于另一个分类器时，它们可以与<code class="du mk ml mm jj b"><a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">feature_selection.SelectFromModel</strong></a></code>一起使用来选择非零系数。特别是，用于此目的的稀疏估计器是用于回归的<code class="du mk ml mm jj b"><a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">linear_model.Lasso</strong></a></code>，以及用于分类的<code class="du mk ml mm jj b"><a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">linear_model.LogisticRegression</strong></a></code>和<code class="du mk ml mm jj b"><a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">svm.LinearSVC</strong></a></code>:</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="accc" class="jn jo hi jj b fi jp jq l jr js">from sklearn.svm import LinearSVC<br/>from sklearn.feature_selection import SelectFromModel<br/>lsvc = LinearSVC(C=0.01, penalty="l1", dual=False,max_iter=2000).fit(x_train, y_train) <br/>model = SelectFromModel(lsvc, prefit=True) <br/>x_new = model.transform(x_train) <br/>print(x_train.columns[model.get_support()])</span><span id="23f4" class="jn jo hi jj b fi jt jq l jr js"># let's print the number of total and selected features</span><span id="85dc" class="jn jo hi jj b fi jt jq l jr js"># this is how we can make a list of the selected features<br/><strong class="jj hj">selected_feature_lsvc</strong> = x_train.columns[model.get_support()]</span><span id="9831" class="jn jo hi jj b fi jt jq l jr js"># let's print some stats<br/>print('total features: {}'.format((x_train.shape[1])))<br/>print('selected features: {}'.format(len(selected_feature_lsvc)))<br/>print('Best features by lsvc:',x_train.columns[model.get_support()])</span><span id="39ec" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj"><em class="mn">Outcome:<br/>total features: 16 <br/>selected features: 3 <br/>Best features by lsvc: Index(['texture_mean', 'area_mean', 'area_se'], dtype='object')</em></strong></span></pre><p id="45b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7。基于树的特征选择</strong></p><div class="lg lh ez fb li lj"><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py" rel="noopener  ugc nofollow" target="_blank"><div class="lk ab dw"><div class="ll ab lm cl cj ln"><h2 class="bd hj fi z dy lo ea eb lp ed ef hh bi translated">森林的重要性-sci kit-了解0.23.1文档</h2><div class="lq l"><h3 class="bd b fi z dy lo ea eb lp ed ef dx translated">这个例子显示了使用森林的树木来评估特征对人工分类的重要性…</h3></div><div class="lr l"><p class="bd b fp z dy lo ea eb lp ed ef dx translated">scikit-learn.org</p></div></div><div class="ls l"><div class="mo l lu lv lw ls lx ly lj"/></div></div></a></div><p id="824e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用森林或树木来评估特征在人工分类任务中的重要性。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="c87e" class="jn jo hi jj b fi jp jq l jr js">from sklearn.ensemble import ExtraTreesClassifier<br/>from sklearn.feature_selection import SelectFromModel<br/># Build a forest and compute the impurity-based feature importances<br/>clf = ExtraTreesClassifier(n_estimators=32,random_state=0)<br/>clf.fit(x_train, y_train)<br/>clf.feature_importances_ <br/>importances = clf.feature_importances_<br/>std = np.std([tree.feature_importances_ for tree in clf.estimators_],<br/>             axis=0)<br/>indices = np.argsort(importances)[::-1]</span><span id="6c83" class="jn jo hi jj b fi jt jq l jr js"># Print the feature ranking<br/>print("Feature ranking:")</span><span id="6091" class="jn jo hi jj b fi jt jq l jr js">for f in range(x_train.shape[1]):<br/>    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))</span><span id="fdf7" class="jn jo hi jj b fi jt jq l jr js"># Plot the impurity-based feature importances of the forest<br/>plt.figure(1, figsize=(14, 13))<br/>plt.title("Feature importances")<br/>plt.bar(range(x_train.shape[1]), importances[indices],<br/>        color="r", yerr=std[indices], align="center")<br/>plt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)<br/>plt.xlim([-1, x_train.shape[1]])<br/>plt.show()</span><span id="7c41" class="jn jo hi jj b fi jt jq l jr js"># let's print the number of total and selected features</span><span id="0a47" class="jn jo hi jj b fi jt jq l jr js"># this is how we can make a list of the selected features<br/><strong class="jj hj">selected_feature_extraTrees</strong> = x_train.columns[model.get_support()]</span><span id="eb8d" class="jn jo hi jj b fi jt jq l jr js"># let's print some stats<br/>print('total features: {}'.format((x_train.shape[1])))<br/>print('selected features: {}'.format(len(selected_feature_extraTrees)))<br/>print('Best features by ExtraTrees:',x_train.columns[model.get_support()])</span><span id="d53c" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>total features: 16 selected features: 3 Best features by ExtraTrees: Index(['texture_mean', 'area_mean', 'area_se'], dtype='object')</strong></span></pre><figure class="je jf jg jh fd md er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es mc"><img src="../Images/d949eba190448257bf4e9828aa1e192f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nG_O5IBBhR_b1Gu-plmCIQ.png"/></div></div></figure><p id="7dce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 8。基于投票的功能选择</strong></p><p id="abbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">将所有结合在一起</strong></p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="1c54" class="jn jo hi jj b fi jp jq l jr js"><strong class="jj hj">from</strong> <strong class="jj hj">functools</strong> <strong class="jj hj">import</strong> reduce<br/>dfs = [fs_corr, fs_chi2, fs_rfe, fs_rfecv, fs_rf, fs_l1, fs_extratrees]<br/>final_results = reduce(<strong class="jj hj">lambda</strong> left,right: pd.merge(left,right,on='index'), dfs)</span><span id="921c" class="jn jo hi jj b fi jt jq l jr js">final_results.head()</span></pre><figure class="je jf jg jh fd md er es paragraph-image"><div class="er es mp"><img src="../Images/35ff9149806f4da7bedc1709e8c14c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*EvvjqSWcgD5hoU2XJB7mTw.png"/></div></figure><p id="3657" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">可变分数</strong></p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="768c" class="jn jo hi jj b fi jp jq l jr js">columns = ['Chi_Square', 'RF', 'Extratrees']<br/>score_table = pd.DataFrame({},[])<br/>score_table['index'] = final_results['index']<br/><strong class="jj hj">for</strong> i <strong class="jj hj">in</strong> columns:<br/>    score_table[i] = final_results['index'].isin(list(final_results.nlargest(10,i)['index'])).astype(int)<br/><br/><em class="mn">#score_table['Corr'] = final_results['Corr'].astype(int) # Excluding</em><br/>score_table['RFE'] = final_results['RFE'].astype(int)<br/>score_table['RFECV'] = final_results['RFECV'].astype(int)<br/>score_table['L1'] = final_results['L1'].astype(int)</span><span id="112f" class="jn jo hi jj b fi jt jq l jr js">score_table['final_score'] = score_table.sum(axis=1)<br/>score_table.sort_values('final_score',ascending=0)</span></pre><figure class="je jf jg jh fd md er es paragraph-image"><div class="er es mq"><img src="../Images/c134bd86b07960fbfcc5c1e67dc6cba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*JFlitnyhFnIplA4oURSEoA.png"/></div></figure><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="7a8a" class="jn jo hi jj b fi jp jq l jr js"><strong class="jj hj"><em class="mn"># 1. Correlation features</em><br/>Outcome:<br/>Index(['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean','symmetry_mean','fractal_dimension_mean', 'texture_se', 'area_se','smoothness_se', 'concavity_se', 'symmetry_se', 'fractal_dimension_se','smoothness_worst', 'concavity_worst', 'symmetry_worst',        'fractal_dimension_worst'],dtype='object')</strong></span><span id="ce74" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj"><em class="mn"># 2. Chi-Square features</em></strong><br/>list(score_table['index'][score_table['Chi_Square']==1])</span><span id="6515" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>['texture_mean', 'area_mean', 'concavity_mean', 'symmetry_mean',<br/> 'area_se', 'concavity_se', 'smoothness_worst', 'concavity_worst',<br/> 'symmetry_worst', 'fractal_dimension_worst']</strong></span><span id="a101" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj"><em class="mn"># 3. RFE features</em></strong><br/>list(score_table['index'][score_table['RFE']==1])</span><span id="86a3" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean',<br/> 'area_se', 'smoothness_se', 'concavity_se', 'smoothness_worst',<br/> 'concavity_worst', 'symmetry_worst']</strong></span><span id="743a" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj"># 4. RFECV Features</strong><br/>list(score_table['index'][score_table['RFECV']==1])</span><span id="d1c5" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean',<br/> 'fractal_dimension_mean', 'area_se', 'concavity_se', 'concavity_worst', 'symmetry_worst']</strong></span><span id="642a" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj"><em class="mn"># 5. RF features</em></strong><br/>list(score_table['index'][score_table['RF']==1])</span><span id="61b8" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>['texture_mean', 'area_mean', 'concavity_mean', 'area_se', 'concavity_se', 'fractal_dimension_se', 'smoothness_worst',<br/> 'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst']</strong></span><span id="20dc" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj"><em class="mn"># 6. L1 features</em></strong><br/>list(score_table['index'][score_table['L1']==1])</span><span id="a4f8" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>['texture_mean', 'area_mean', 'area_se']</strong></span><span id="a6ac" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj"><em class="mn"># 7. ExtraTrees features</em></strong><br/>list(score_table['index'][score_table['Extratrees']==1])</span><span id="cfc4" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:<br/>['texture_mean', 'area_mean', 'concavity_mean', 'fractal_dimension_mean', 'area_se', 'concavity_se',<br/> 'smoothness_worst', 'concavity_worst', 'symmetry_worst',<br/> 'fractal_dimension_worst']</strong></span><span id="44af" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj"><em class="mn"># 8. Voted features</em></strong><br/>list(score_table['index'][score_table['final_score']&gt;=2])</span><span id="8e14" class="jn jo hi jj b fi jt jq l jr js"><strong class="jj hj">Outcome:</strong><br/><strong class="jj hj"><em class="mn">['texture_mean',  'area_mean',  'smoothness_mean',  'concavity_mean',  'fractal_dimension_mean',  'area_se',  'concavity_se',  'smoothness_worst',  'concavity_worst',  'symmetry_worst',  'fractal_dimension_worst']</em></strong></span></pre><p id="53be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae jd" rel="noopener" href="/@shahid_dhn/building-ml-model-to-predict-whether-the-cancer-is-benign-or-malignant-on-breast-cancer-wisconsin-d6cf8b47f49a"> <strong class="ih hj">第3部分</strong> </a> <strong class="ih hj">，</strong>中，我们将使用这些来自特征选择技术的不同特征集，并将看到模型的性能。</p><p id="c707" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结尾注释</strong></p><p id="7468" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们关于特征选择的第二部分到此结束。本<strong class="ih hj">第2部分</strong>的目的是提供一个深入和逐步的指南来使用不同种类的特征选择方法。</p><p id="ed1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就我个人而言，我很喜欢写这篇文章，也很想从你的反馈中学习。你觉得这个<strong class="ih hj"> Part 2 </strong>有用吗？我将感谢你的建议/反馈。请随时通过下面的评论提出你的问题。</p><h2 id="d081" class="jn jo hi bd ju jv jw jx jy jz ka kb kc iq kd ke kf iu kg kh ki iy kj kk kl km bi translated">我们将在<a class="ae jd" rel="noopener" href="/@shahid_dhn/building-ml-model-to-predict-whether-the-cancer-is-benign-or-malignant-on-breast-cancer-wisconsin-d6cf8b47f49a">第3部分</a>探讨第6 &amp; 7步:模型构建和验证</h2><p id="687d" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">敬请期待！</p><p id="8ecc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文使用的所有代码和数据集都可以从我的<a class="ae jd" href="https://github.com/Muhd-Shahid/Breast-Cancer-Wisconsin" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> GitHub </strong> </a>中访问。</p><p id="e6d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该代码也可作为<a class="ae jd" href="https://github.com/Muhd-Shahid/Breast-Cancer-Wisconsin/blob/master/BCW_Feature_Selection_Part%202.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> Jupyter笔记本</strong> </a>。</p></div></div>    
</body>
</html>