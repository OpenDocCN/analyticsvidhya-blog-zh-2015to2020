<html>
<head>
<title>Twitter Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">推特情感分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/twitter-sentiment-analysis-134553698978?source=collection_archive---------8-----------------------#2019-12-24">https://medium.com/analytics-vidhya/twitter-sentiment-analysis-134553698978?source=collection_archive---------8-----------------------#2019-12-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="fb71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用文本挖掘技术和自然语言处理技术将推文分为正面或负面。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/0d22e011563ae0e3681bb4310df17f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VT7AxioAGXplMe7RAEYfSA.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jq"><img src="../Images/9a62fb0166f6390e195da7aa476b0672.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*Df-gJ2MOIZmoDKPVnva39A.png"/></div></figure><p id="83a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现实世界中的大多数数据都是非结构化文本格式，因此，对于数据科学爱好者来说，学习文本挖掘和自然语言技术以使用这些文本获得有用的见解是必不可少的。</p><p id="4a38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过这篇博客，我将解释如何对给定的监督数据集进行情感分析。这个问题是从维迪亚的一个分析竞赛中得到的。下面是问题的链接—<a class="ae jr" href="https://datahack.analyticsvidhya.com/contest/linguipedia-codefest-natural-language-processing-1/" rel="noopener ugc nofollow" target="_blank">https://data hack . analyticsvidhya . com/contest/linguipedia-code fest-natural-language-processing-1/</a></p><h1 id="0dd4" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">问题介绍</h1><p id="3741" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">情感分析是文本的上下文挖掘，它识别和提取源材料中的主观信息，并帮助企业在监控在线对话的同时了解其品牌、产品或服务的社会情感。品牌可以使用这些数据来客观地衡量他们产品的成功。在这次挑战中，我们获得了推特数据来预测网民对电子产品的情绪。给定客户关于制造和销售手机、电脑、笔记本电脑等各种技术公司的推文，任务是识别推文是否对这些公司或产品有负面情绪。</p><h1 id="a87d" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">方法</h1><p id="a53a" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">首先，我们将导入我们将在分析过程中使用的所有必要的库。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="e8b7" class="la jt hi kw b fi lb lc l ld le">import numpy as np</span><span id="e789" class="la jt hi kw b fi lf lc l ld le">import pandas as pd</span><span id="4511" class="la jt hi kw b fi lf lc l ld le">import re</span><span id="e8a7" class="la jt hi kw b fi lf lc l ld le">import matplotlib.pyplot as plt</span><span id="9e75" class="la jt hi kw b fi lf lc l ld le">import seaborn as sns</span><span id="4851" class="la jt hi kw b fi lf lc l ld le">from sklearn.linear_model import LogisticRegression</span><span id="9dc0" class="la jt hi kw b fi lf lc l ld le">from sklearn.svm import SVC</span><span id="5454" class="la jt hi kw b fi lf lc l ld le">from sklearn.model_selection import train_test_split</span><span id="c485" class="la jt hi kw b fi lf lc l ld le">from sklearn.metrics import f1_score</span><span id="1182" class="la jt hi kw b fi lf lc l ld le">from sklearn.model_selection import GridSearchCV</span><span id="3d65" class="la jt hi kw b fi lf lc l ld le">from sklearn.model_selection import cross_val_score</span><span id="4eba" class="la jt hi kw b fi lf lc l ld le">from sklearn.model_selection import KFold</span><span id="c7de" class="la jt hi kw b fi lf lc l ld le">from sklearn.metrics import roc_curve</span><span id="1c1f" class="la jt hi kw b fi lf lc l ld le">from sklearn.metrics import precision_recall_curve</span><span id="1efd" class="la jt hi kw b fi lf lc l ld le">from sklearn.metrics import roc_auc_score</span><span id="741f" class="la jt hi kw b fi lf lc l ld le">from sklearn.linear_model import LogisticRegression</span><span id="18a5" class="la jt hi kw b fi lf lc l ld le">from sklearn.metrics import classification_report</span><span id="f3fe" class="la jt hi kw b fi lf lc l ld le">from sklearn.metrics import confusion_matrix</span><span id="40a1" class="la jt hi kw b fi lf lc l ld le">from sklearn.ensemble import RandomForestClassifier</span><span id="9d67" class="la jt hi kw b fi lf lc l ld le">from sklearn.ensemble import GradientBoostingClassifier</span><span id="0d04" class="la jt hi kw b fi lf lc l ld le">from sklearn.model_selection import RandomizedSearchCV</span><span id="1d51" class="la jt hi kw b fi lf lc l ld le">from sklearn.metrics import f1_score, make_scorer</span></pre><p id="f02e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们导入给定的训练和测试数据，这些数据包含客户的不同推文。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="5a48" class="la jt hi kw b fi lb lc l ld le">train = pd.read_csv(‘train_2kmZucJ.csv’).drop(columns = [‘id’])</span><span id="7258" class="la jt hi kw b fi lf lc l ld le">test = pd.read_csv(‘test_oJQbWVk.csv’).drop(columns = [‘id’])</span><span id="51ce" class="la jt hi kw b fi lf lc l ld le">train.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lg"><img src="../Images/8ce6357929d31801e2f2d8f44287731b.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*dKGL8aLZYwmqqc_21YES2Q.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">培训数据(表头)</figcaption></figure><h1 id="32b4" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">文本预处理和可视化</strong></h1><p id="6f17" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">显然，这些推文包含大量嘈杂的数据，需要在我们继续分析之前将其删除。<strong class="ih hj">标签‘0’</strong>对应于具有<strong class="ih hj">正面</strong>情绪的推文，而<strong class="ih hj">标签‘1’</strong>对应于具有<strong class="ih hj">负面</strong>情绪的推文。现在，让我们检查训练数据中正面和负面推文的百分比。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="353c" class="la jt hi kw b fi lb lc l ld le">#Printing percentage of tweets with +ve and -ve sentiments</span><span id="d4ae" class="la jt hi kw b fi lf lc l ld le">print(‘Percentage of tweets labeled as a negative sentiment ‘, end = ‘’)</span><span id="f6d9" class="la jt hi kw b fi lf lc l ld le">print(sum(train[‘label’]==1)*100/train.shape[0], end =’%\n’)</span><span id="3f1d" class="la jt hi kw b fi lf lc l ld le">print(‘Percentage of tweets labeled as a positive sentiment ‘, end = ‘’)</span><span id="2a77" class="la jt hi kw b fi lf lc l ld le">print(sum(train[‘label’]==0)*100/train.shape[0], end =’%\n’)</span><span id="d1de" class="la jt hi kw b fi lf lc l ld le">ax = train[‘label’].value_counts().plot(kind=’bar’,</span><span id="2d0c" class="la jt hi kw b fi lf lc l ld le">figsize=(10,6),</span><span id="fa75" class="la jt hi kw b fi lf lc l ld le">title=”Distribution of positive and negative sentiments in the data”)</span><span id="7414" class="la jt hi kw b fi lf lc l ld le">ax.set_xlabel(“Sentiment ( 0 == positive, 1 == negative)”)</span><span id="b8aa" class="la jt hi kw b fi lf lc l ld le">ax.set_ylabel(“Count”)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ll"><img src="../Images/68833435bf4e5d0ea002690ed0c68f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*yaTnn28C8T-LvKbleR2VDg.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lm"><img src="../Images/210fcb11c2120ee6326b01870226e420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*llWAWp_rYqNvayqaGgsPeA.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">正面和负面标签的推文</figcaption></figure><p id="6fd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的条形图和百分比显示，大约75%的推文是正面的，而25%是负面的。因此，我们可以推断数据是<strong class="ih hj">不平衡的</strong>。我们将使用一个<strong class="ih hj"> <em class="jd">加权F1分数</em> </strong>来分析我们的模型。</p><p id="b191" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，第一步是去除像标点符号、标签、@和其他非字母数字的噪音数据。只有字母数字数据才是有意义的数据，可以帮助我们识别情感。为了去除有噪声的数据，我们将导入<strong class="ih hj"> RegexpTokenizer </strong>，它将基于正则表达式将字符串分割成子字符串。我们将使用的正则表达式是' \w+,它将标记所有字母数字数据，并从tweets中删除所有其他噪音。可以通过这个链接了解各种记号化器—<a class="ae jr" rel="noopener" href="/@makcedward/nlp-pipeline-word-tokenization-part-1-4b2b547e6a3">https://medium . com/@ makcedward/NLP-pipeline-word-tokenization-part-1-4b 2 b 547 E6 a 3</a>。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="4ee6" class="la jt hi kw b fi lb lc l ld le">from nltk.tokenize import RegexpTokenizer</span><span id="731c" class="la jt hi kw b fi lf lc l ld le">regexp = RegexpTokenizer(r”\w+”)</span><span id="69cd" class="la jt hi kw b fi lf lc l ld le">#applying regexptokenize to both training and test sets</span><span id="d2bf" class="la jt hi kw b fi lf lc l ld le">train[‘tweet’]=train[‘tweet’].apply(regexp.tokenize)</span><span id="b9a3" class="la jt hi kw b fi lf lc l ld le">test[‘tweet’]=test[‘tweet’].apply(regexp.tokenize)</span><span id="1edc" class="la jt hi kw b fi lf lc l ld le">train.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ln"><img src="../Images/5023f2dcd089ef75c2907a3f506c810b.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*PH8Og57bLR1w_IlMvC08aQ.png"/></div></figure><p id="7781" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练集和测试集的处理是相同的。</p><p id="836f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们有了字母数字数据的符号化版本，我们的下一步将是删除所有对情感分析无用的常用词。像about、above、other标点符号、连词等词在任何文本数据中都被大量使用，但对我们的目的来说并不特别有用。这些词被称为<strong class="ih hj">停用词</strong>。我们现在将删除停用词，使我们的推文更干净，便于分析。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="3b7d" class="la jt hi kw b fi lb lc l ld le">import nltk</span><span id="70c2" class="la jt hi kw b fi lf lc l ld le">from nltk.corpus import stopwords</span><span id="8563" class="la jt hi kw b fi lf lc l ld le">nltk.download(‘stopwords’)</span><span id="4b31" class="la jt hi kw b fi lf lc l ld le">#remove stopwords from both training and test set</span><span id="58a9" class="la jt hi kw b fi lf lc l ld le">train[‘tweet’] = train[‘tweet’].apply(lambda x: [item for item in x if item not in list_stop_words])</span><span id="c229" class="la jt hi kw b fi lf lc l ld le">test[‘tweet’] = test[‘tweet’].apply(lambda x: [item for item in x if item not in list_stop_words])</span></pre><p id="d800" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">移除停用词后，我们将移除所有长度为&lt;=2. In general, <strong class="ih hj">的词。小词(长度&lt; =2)对情感分析没有用</strong>，因为它们没有意义。在我们的分析中，这些很可能是噪音。除了去掉小单词，我们将把所有的记号都转换成小写。这是因为像“苹果”或“苹果”这样的词在感伤的上下文中有相同的意思。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="af27" class="la jt hi kw b fi lb lc l ld le">train[‘tweet’] = train[‘tweet’].apply(lambda x: ‘ ‘.join([w for w in x if len(w)&gt;2]))</span><span id="7d55" class="la jt hi kw b fi lf lc l ld le">test[‘tweet’] = test[‘tweet’].apply(lambda x: ‘ ‘.join([w for w in x if len(w)&gt;2]))</span><span id="d7e7" class="la jt hi kw b fi lf lc l ld le">train[‘tweet’] = train[‘tweet’].str.lower()</span><span id="8c72" class="la jt hi kw b fi lf lc l ld le">test[‘tweet’] = test[‘tweet’].str.lower()</span></pre><p id="b253" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，是时候建立一个单词云，并获得一些关于最常见单词的见解了。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="82cf" class="la jt hi kw b fi lb lc l ld le">from wordcloud import WordCloud</span><span id="a4f5" class="la jt hi kw b fi lf lc l ld le">all_words = ‘’.join([word for word in train[‘tweet’]])</span><span id="2362" class="la jt hi kw b fi lf lc l ld le">#building a wordcloud on the data from all tweets</span><span id="1143" class="la jt hi kw b fi lf lc l ld le">wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)</span><span id="87fd" class="la jt hi kw b fi lf lc l ld le">plt.figure(figsize=(10, 7))</span><span id="9410" class="la jt hi kw b fi lf lc l ld le">plt.imshow(wordcloud, interpolation=”bilinear”)</span><span id="7f55" class="la jt hi kw b fi lf lc l ld le">plt.axis(‘off’)</span><span id="5fe1" class="la jt hi kw b fi lf lc l ld le">plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lo"><img src="../Images/e897ae042bc51c6a186663f7fc5cc92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*hF8PLYCErN2Or_t_pu6g4w.png"/></div></figure><p id="8500" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步是使用词干化或词汇化方法，这对任何文本挖掘问题都非常重要。<strong class="ih hj">词干化和词汇化</strong>是自然语言处理领域中的文本规范化(或有时称为单词规范化)技术，用于为进一步处理准备文本、单词和文档。这样做是为了为具有相似词根和上下文的单词构建通用单词，这使得使用分类算法建模更容易。例如，玩耍、玩耍和被玩耍的意思相同，但却是不同的词。如果我们不对一个共同的单词“play”进行标准化，每个分类模型都会认为这三个单词是不同的，用这些数据构建模型会导致过度拟合。</p><p id="68a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">词干化和词元化的主要区别在于词干化通过切断单词的结尾或开头来工作，而词元化通过将单词改变为有意义的词根来工作，这是通过使用<strong class="ih hj"> WordNet </strong>对单词进行词法分析来实现的。</p><p id="a8d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">出于我们的目的，我们将使用词汇化，因为它引入了有意义的常用词，从而更好地进行情感分析。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="9714" class="la jt hi kw b fi lb lc l ld le">nltk.download(‘wordnet’)</span><span id="2603" class="la jt hi kw b fi lf lc l ld le">from nltk.stem import WordNetLemmatizer</span><span id="fad9" class="la jt hi kw b fi lf lc l ld le">wordnet_tokenizer = WordNetLemmatizer()</span><span id="8a5d" class="la jt hi kw b fi lf lc l ld le">train[‘tweet’] = train[‘tweet’].apply(wordnet_tokenizer.lemmatize)</span><span id="dcab" class="la jt hi kw b fi lf lc l ld le">test[‘tweet’] = test[‘tweet’].apply(wordnet_tokenizer.lemmatize)</span></pre><p id="d34d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，是时候看看正面和负面推文中出现频率最高的20个词了。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="2ba8" class="la jt hi kw b fi lb lc l ld le">pos = train[train[‘label’] == 0]</span><span id="9fc6" class="la jt hi kw b fi lf lc l ld le">neg = train[train[‘label’] == 1]</span><span id="0765" class="la jt hi kw b fi lf lc l ld le">pos_sentiment_words = ‘’.join([word for word in pos[‘tweet’]]) #words from the tweets that are positive</span><span id="b852" class="la jt hi kw b fi lf lc l ld le">neg_sentiment_words = ‘’.join([word for word in neg[‘tweet’]]) ##words from the tweets that are negative</span><span id="faf4" class="la jt hi kw b fi lf lc l ld le">#top 20 words on positive tweets</span><span id="86f4" class="la jt hi kw b fi lf lc l ld le">list_pos_words = [ x for x in pos_sentiment_words.split()] #list of positive sentiment words</span><span id="4e86" class="la jt hi kw b fi lf lc l ld le">freq_dis_pos = nltk.FreqDist(list_pos_words) #number of occurances of each word</span><span id="f371" class="la jt hi kw b fi lf lc l ld le">freq_dataframe = pd.DataFrame({‘Words’: list(freq_dis_pos.keys()), ‘Count’: list(freq_dis_pos.values())}) #data frame of words and count</span><span id="9440" class="la jt hi kw b fi lf lc l ld le"># selecting top 20 most frequent hashtags</span><span id="1da2" class="la jt hi kw b fi lf lc l ld le">freq_dataframe = freq_dataframe.nlargest(columns=”Count”, n = 20)</span><span id="04bc" class="la jt hi kw b fi lf lc l ld le">plt.figure(figsize=(16,5))</span><span id="94ed" class="la jt hi kw b fi lf lc l ld le">ax = sns.barplot(data=freq_dataframe, x= “Words”, y = “Count”)</span><span id="65c0" class="la jt hi kw b fi lf lc l ld le">ax.set(ylabel = ‘Count’)</span><span id="12d4" class="la jt hi kw b fi lf lc l ld le">ax.set(xlabel = ‘Top 20 words used in positive context’)</span><span id="2997" class="la jt hi kw b fi lf lc l ld le">plt.title(“Top 20 words in the tweets labeled as POSITIVE SENTIMENT”)</span><span id="8d76" class="la jt hi kw b fi lf lc l ld le">plt.show()</span><span id="5cf5" class="la jt hi kw b fi lf lc l ld le">#top 20 words on negative tweets</span><span id="dde9" class="la jt hi kw b fi lf lc l ld le">list_neg_words = [ x for x in neg_sentiment_words.split()]   #list of positive sentiment words</span><span id="93d8" class="la jt hi kw b fi lf lc l ld le">freq_dis_pos = nltk.FreqDist(list_neg_words)   #number of occurances of each word</span><span id="1d78" class="la jt hi kw b fi lf lc l ld le">freq_dataframe = pd.DataFrame({'Words': list(freq_dis_pos.keys()), 'Count': list(freq_dis_pos.values())})  #data frame of words and count</span><span id="6760" class="la jt hi kw b fi lf lc l ld le"># selecting top 20 most frequent hashtags</span><span id="1942" class="la jt hi kw b fi lf lc l ld le">freq_dataframe = freq_dataframe.nlargest(columns="Count", n = 20)</span><span id="3a85" class="la jt hi kw b fi lf lc l ld le">plt.figure(figsize=(16,5))</span><span id="a8f1" class="la jt hi kw b fi lf lc l ld le">ax = sns.barplot(data=freq_dataframe, x= "Words", y = "Count")</span><span id="fcac" class="la jt hi kw b fi lf lc l ld le">ax.set(ylabel = 'Count')</span><span id="e74e" class="la jt hi kw b fi lf lc l ld le">ax.set(xlabel = 'Top 20 words used in negative context')</span><span id="e389" class="la jt hi kw b fi lf lc l ld le">plt.title("Top 20 words in the tweets labeled as NEGATIVE SENTIMENT")</span><span id="1333" class="la jt hi kw b fi lf lc l ld le">plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lp"><img src="../Images/fc3e2e49a739243c29f4e9ca3779764c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*stcGMK91miLRvICOlGoRLg.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated">正面推文中的前20个词</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lq"><img src="../Images/8fa2660a81a2749666724a2bb26e1bc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FEfsdJi8eTfm97g_8GGt6Q.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated">负面推文中的前20个词</figcaption></figure><p id="5ce8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们也看看正面和负面推文的词云。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="4530" class="la jt hi kw b fi lb lc l ld le">#WordCloud for positive tweets</span><span id="5708" class="la jt hi kw b fi lf lc l ld le">wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(pos_sentiment_words)</span><span id="da04" class="la jt hi kw b fi lf lc l ld le">plt.figure(figsize=(10, 7))</span><span id="6abb" class="la jt hi kw b fi lf lc l ld le">plt.imshow(wordcloud, interpolation=”bilinear”)</span><span id="05d2" class="la jt hi kw b fi lf lc l ld le">plt.axis(‘off’)</span><span id="11af" class="la jt hi kw b fi lf lc l ld le">plt.title(‘Wordcloud for positive tweets’)</span><span id="9f0e" class="la jt hi kw b fi lf lc l ld le">plt.show()</span><span id="5e0f" class="la jt hi kw b fi lf lc l ld le">#wordcloud for negative tweets</span><span id="78eb" class="la jt hi kw b fi lf lc l ld le">wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(neg_sentiment_words)</span><span id="ee84" class="la jt hi kw b fi lf lc l ld le">plt.figure(figsize=(10, 7))</span><span id="d52a" class="la jt hi kw b fi lf lc l ld le">plt.imshow(wordcloud, interpolation=”bilinear”)</span><span id="2b91" class="la jt hi kw b fi lf lc l ld le">plt.axis(‘off’)</span><span id="2a14" class="la jt hi kw b fi lf lc l ld le">plt.title(‘Wordcloud for negative tweets’)</span><span id="3d5a" class="la jt hi kw b fi lf lc l ld le">plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lo"><img src="../Images/732f55e9680f5cc54bceac7a85613477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*eTdhEviTwsYzT3jqRpt5Dw.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">正面推文的Wordcloud</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lo"><img src="../Images/22b56ec6c0c37e23ee418dcddd0093fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*CwOHMxYJbSepg4CJddSAsw.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">负面推文的Wordcloud</figcaption></figure><p id="79b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的柱状图和词云，我们可以看到负面推文中的一些词是fuck，fucking，ipod，time等，而正面推文中的一些词是life，day，photography，sony，instagram等。</p><h1 id="6421" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">建模</strong></h1><p id="2e24" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">现在，我们已经清理了我们的数据，并通过WordClouds获得了一个简要的视图，我们将为建模准备数据。文本数据的准备包括转换成机器学习模型可以理解的某种数字格式。计算机只理解数字数据，因此这是必要的。</p><p id="eb45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常，有多种方法可以将文本数据转换成数字形式，如CountVectorizer、TfIdf等。<strong class="ih hj">计数矢量器</strong>基于单词袋模型。它的工作原理是计算单词在每个文档中的出现频率(在本例中是每个tweet)。</p><p id="fa52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用<strong class="ih hj"> TfIdf( </strong>术语频率-逆文档频率)，数值随着单词计数的增加而增加，但会被相同单词在不同语料库中的出现所抵消。例如，如果有一个单词“apple”在一个文档中出现多次，并且在大约80%的推文中出现，count vectorizer将为apple提供一个高值，但<strong class="ih hj"> TfIdf </strong>将具有一个可以忽略的值，因为它是在许多文档中出现的一个常见单词，因此不是一个用于分类文档(或这里的推文)的有用单词。</p><p id="1de8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，我们将使用<strong class="ih hj"> TfIdf。</strong></p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="71fd" class="la jt hi kw b fi lb lc l ld le">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="accd" class="la jt hi kw b fi lf lc l ld le">tfidf_vectorizer = TfidfVectorizer(min_df = 2, max_df = .9, max_features = 1000, ngram_range = (1, 1))</span><span id="aa53" class="la jt hi kw b fi lf lc l ld le">tfidf_fit = tfidf_vectorizer.fit(train[‘tweet’])</span><span id="3307" class="la jt hi kw b fi lf lc l ld le">tfidf = tfidf_fit.transform(train[‘tweet’])</span><span id="c20f" class="la jt hi kw b fi lf lc l ld le">tfidf_test = tfidf_fit.transform(test[‘tweet’])</span></pre><p id="c364" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将使用train_test_split来创建新的训练和测试集，以获得最佳模型。然后，我们可以使用原始测试集上的最佳模型来获得F1分数。</p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="50ed" class="la jt hi kw b fi lb lc l ld le">X_train, X_test, y_train, y_test = train_test_split(tfidf, train[‘label’], random_state = 4, test_size = .3)</span><span id="3218" class="la jt hi kw b fi lf lc l ld le">#converting sparse matrices to np.array</span><span id="3506" class="la jt hi kw b fi lf lc l ld le">X_train = X_train.toarray()</span><span id="18c7" class="la jt hi kw b fi lf lc l ld le">X_test = X_test.toarray()</span><span id="1c45" class="la jt hi kw b fi lf lc l ld le">y_train = np.array(y_train).reshape(-1,1)</span><span id="336a" class="la jt hi kw b fi lf lc l ld le">y_test = np.array(y_test).reshape(-1,1)</span></pre><p id="019d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将尝试三个模型— <strong class="ih hj">逻辑回归</strong>(具有超参数调整)、支持向量机(SVM)和朴素贝叶斯分类器的<strong class="ih hj">多项式B </strong>，并寻找可用于我们测试集的最佳模型。</p><h1 id="6613" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">逻辑回归</strong></h1><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="f9ae" class="la jt hi kw b fi lb lc l ld le">#Logistic Regression with GridSearch</span><span id="c4dc" class="la jt hi kw b fi lf lc l ld le">clf = LogisticRegression()</span><span id="a60c" class="la jt hi kw b fi lf lc l ld le"># use a full grid over all parameters</span><span id="6add" class="la jt hi kw b fi lf lc l ld le">param_grid = {“C”:np.logspace(-3,3,7), “penalty”:[“l1”,”l2"]}</span><span id="a2d0" class="la jt hi kw b fi lf lc l ld le">f1 = make_scorer(f1_score , average=’weighted’)</span><span id="91b9" class="la jt hi kw b fi lf lc l ld le"># run grid search</span><span id="9871" class="la jt hi kw b fi lf lc l ld le">grid = GridSearchCV(clf, cv=5,scoring=f1, param_grid=param_grid)</span><span id="9d7e" class="la jt hi kw b fi lf lc l ld le">grid.fit(X_train, y_train)</span><span id="62b2" class="la jt hi kw b fi lf lc l ld le">print(“Grid-Search with roc_auc”)</span><span id="e0f9" class="la jt hi kw b fi lf lc l ld le">print(“Best parameters:”, grid.best_params_)</span><span id="ad52" class="la jt hi kw b fi lf lc l ld le">print(“Best cross-validation score (f1)): {:.3f}”.format(grid.best_score_))</span><span id="3d9c" class="la jt hi kw b fi lf lc l ld le">y_predict = grid.predict(X_test)</span><span id="5c74" class="la jt hi kw b fi lf lc l ld le">print(‘The weighted F1 score with the best hyperparameters is ‘, end = ‘’)</span><span id="bd9e" class="la jt hi kw b fi lf lc l ld le">print(f1_score(y_test, y_predict, average=’weighted’))</span><span id="1edd" class="la jt hi kw b fi lf lc l ld le">print (“Classification Report: “)</span><span id="016c" class="la jt hi kw b fi lf lc l ld le">print (classification_report(y_test, y_predict))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ll"><img src="../Images/d1d57fa32392b959358af46cc24395b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*N_Q8zAD6PrHRkIRuBcioXA.png"/></div></figure><p id="cbe8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">具有最佳超参数的逻辑回归的加权F1分数是0.8900，这看起来非常好。</p><p id="bd43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">F1分数:-</p><div class="lr ls ez fb lt lu"><a href="https://en.wikipedia.org/wiki/F1_score" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">F1分数</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">在二进制分类的统计分析中，F值(也称为F值或F度量)是测试准确性的度量。它…</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">en.wikipedia.org</p></div></div><div class="md l"><div class="me l mf mg mh md mi jo lu"/></div></div></a></div><p id="43fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">加权F1分数是标签“0”和标签“1”的F1分数的加权平均分数，我们可以从上面的分类报告中看到。现在，让我们检查SVM和朴素贝叶斯模型。</p><h1 id="b398" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">支持向量机</strong></h1><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="a840" class="la jt hi kw b fi lb lc l ld le"># Classifier — Algorithm — Support Vector Classifier</span><span id="218d" class="la jt hi kw b fi lf lc l ld le">SVM = SVC(C=1.0, kernel=’linear’, degree=3, gamma=’auto’)</span><span id="b3b6" class="la jt hi kw b fi lf lc l ld le">SVM.fit(X_train,y_train)</span><span id="a324" class="la jt hi kw b fi lf lc l ld le"># predict the labels on validation dataset</span><span id="b362" class="la jt hi kw b fi lf lc l ld le">y_predict = SVM.predict(X_test)</span><span id="7e89" class="la jt hi kw b fi lf lc l ld le"># Use accuracy_score function to get the accuracy</span><span id="3d36" class="la jt hi kw b fi lf lc l ld le">print(‘The weighted F1 score ‘, end = ‘’)</span><span id="37d5" class="la jt hi kw b fi lf lc l ld le">print(f1_score(y_test, y_predict,average=’weighted’))</span><span id="cdd2" class="la jt hi kw b fi lf lc l ld le">print (“Classification Report: “)</span><span id="2071" class="la jt hi kw b fi lf lc l ld le">print (classification_report(y_test, y_predict))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mj"><img src="../Images/b94df223277662b17cb70910c0d2fea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*C2zqCfh5Y9W-AzK3gEVRNg.png"/></div></figure><p id="ea46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SVC给我们的加权F1值是0.8916，比逻辑回归好一点。</p><h1 id="705c" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">朴素贝叶斯分类器</strong></h1><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="a871" class="la jt hi kw b fi lb lc l ld le">clf = MultinomialNB()</span><span id="3d52" class="la jt hi kw b fi lf lc l ld le"># use a full grid over all parameters</span><span id="7c14" class="la jt hi kw b fi lf lc l ld le">param_grid = {‘alpha’:[0,1] }</span><span id="dde8" class="la jt hi kw b fi lf lc l ld le">f1 = make_scorer(f1_score , average=’weighted’)</span><span id="d9c5" class="la jt hi kw b fi lf lc l ld le"># run grid search</span><span id="42b1" class="la jt hi kw b fi lf lc l ld le">grid = GridSearchCV(clf, cv=5,scoring=f1, param_grid=param_grid)</span><span id="d332" class="la jt hi kw b fi lf lc l ld le">grid.fit(X_train, y_train)</span><span id="d5b3" class="la jt hi kw b fi lf lc l ld le">print(“Grid-Search with roc_auc”)</span><span id="6b0c" class="la jt hi kw b fi lf lc l ld le">print(“Best parameters:”, grid.best_params_)</span><span id="0e14" class="la jt hi kw b fi lf lc l ld le">print(“Best cross-validation score (f1)): {:.3f}”.format(grid.best_score_))</span><span id="ce20" class="la jt hi kw b fi lf lc l ld le">y_predict = grid.predict(X_test)</span><span id="eed8" class="la jt hi kw b fi lf lc l ld le">print('The weighted F1 score with the best hyperparameters is ', end = '')</span><span id="618e" class="la jt hi kw b fi lf lc l ld le">print(f1_score(y_test, y_predict, average='weighted'))</span><span id="2532" class="la jt hi kw b fi lf lc l ld le">print ("Classification Report: ")</span><span id="d964" class="la jt hi kw b fi lf lc l ld le">print (classification_report(y_test, y_predict))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mk"><img src="../Images/709780fe43c6c081a70e1eef0eac2f63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*7vgDt1pIAebmCykFK3hqvQ.png"/></div></figure><p id="c4dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Naive-Bayes模型为我们提供了三个模型中最好的加权F1分数。分数是0.8918。一般来说，除了最佳得分之外，朴素贝叶斯分类器说话非常快，因此是计算高效的算法。</p><p id="ea65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经获得了最佳模型，我们可以在预处理的测试数据上使用<strong class="ih hj">模型来预测他们的情绪，该数据存储为tfidf_test。</strong></p><pre class="jf jg jh ji fd kv kw kx ky aw kz bi"><span id="c25a" class="la jt hi kw b fi lb lc l ld le">tfidf_test</span><span id="532d" class="la jt hi kw b fi lf lc l ld le">sparse_test = tfidf_test</span><span id="9776" class="la jt hi kw b fi lf lc l ld le">#storing preprocessed test data into a array</span><span id="c323" class="la jt hi kw b fi lf lc l ld le">test_data = sparse_test.toarray()</span><span id="4276" class="la jt hi kw b fi lf lc l ld le">print(test_data.shape)</span><span id="7ea7" class="la jt hi kw b fi lf lc l ld le">#doing predictions using the best algorithm whicch was Naive Bayes in this case</span><span id="5b3a" class="la jt hi kw b fi lf lc l ld le">y_predict = grid.predict(test_data)</span></pre><p id="86b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您的阅读，让您的数据科学之旅更进一步。坚持学习，多看博客。</p><p id="0e59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请在模型或博客上发布任何反馈或改进。</p></div></div>    
</body>
</html>