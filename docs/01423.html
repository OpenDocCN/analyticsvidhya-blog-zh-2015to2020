<html>
<head>
<title>A Deep Dive into Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对决策树的深入探究</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-deep-dive-to-decision-trees-6575e016d656?source=collection_archive---------11-----------------------#2019-10-21">https://medium.com/analytics-vidhya/a-deep-dive-to-decision-trees-6575e016d656?source=collection_archive---------11-----------------------#2019-10-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7416" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是一种将数据分成不同类别的技术。我们可以从我们的数据中导出相关信息，并相应地提出问题，将我们的数据分类到几个节点，即几组问题。</p><p id="4985" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在使用决策树对我们的数据执行任何预测分析时，首先我们分析我们的数据，然后根据各种条件，我们根据某些类别对数据进行分类。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/3cd238a693e57d49d971d4bea97348b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tMZziUWja6IpembeqWAd6g.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">决策树的结构</figcaption></figure><p id="0646" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树之所以如此命名，是因为它从一个根节点开始，像树一样分支成多个决策(即上图中提到的内部节点和叶节点)。根节点随着决策和条件的数量不断增长，例如<strong class="ih hj">当我们呼叫任何客户服务支持时，他们在电话拨号盘上为我们提供了许多可供按压的选项，以便我们可以查询我们喜欢的信息</strong>。因此，客户服务支持部门正在构建一个决策树，以帮助我们找到正确的方案。</p><h2 id="c15c" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated">决策树制作中使用的重要术语→</h2><ol class=""><li id="cf38" class="ko kp hi ih b ii kq im kr iq ks iu kt iy ku jc kv kw kx ky bi translated"><strong class="ih hj"> <em class="kz">根节点</em> </strong> →是整个树开始的基节点。这是表示树的整个群体的第一个节点。</li><li id="2ecf" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc kv kw kx ky bi translated"><strong class="ih hj"> <em class="kz">叶节点</em> </strong> →出现在树的末端或最后位置的节点。</li><li id="0c28" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc kv kw kx ky bi translated"><strong class="ih hj"> <em class="kz">分裂</em> </strong> →将根节点分成几个不同子部分的过程，即内部节点和叶节点。</li><li id="72e8" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc kv kw kx ky bi translated"><strong class="ih hj"> <em class="kz">分支树</em> </strong> →当我们分割我们的树时，即当我们分离出我们的决策树的任何特定部分时，生成的节点。</li><li id="e2f0" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc kv kw kx ky bi translated"><strong class="ih hj"> <em class="kz">修剪</em> </strong> →是劈的反义词。这是从树上去掉不需要的树枝的过程。</li><li id="dab9" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc kv kw kx ky bi translated"><strong class="ih hj"> <em class="kz">父节点&amp;子节点</em> </strong> →根节点始终是父节点，与其相连的其他节点都是子节点。由此，我们可以推断所有的顶部节点都是父节点，并且从父节点导出的所有底部节点都是子节点。</li></ol><p id="1045" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们继续了解如何构建决策树之前，有一些在构建决策树时经常出现的术语。这些是:→</p><ul class=""><li id="b0af" class="ko kp hi ih b ii ij im in iq lf iu lg iy lh jc li kw kx ky bi translated"><strong class="ih hj"> <em class="kz">基尼指数</em> </strong> →是基于分类回归树(CART)算法构建决策树的杂质度量。</li><li id="be2e" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc li kw kx ky bi translated"><strong class="ih hj">构建决策树就是要找到能获得最高信息增益的属性。</strong></li><li id="bb71" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc li kw kx ky bi translated"><strong class="ih hj"> <em class="kz">方差减少</em> </strong> →这是一种用于<strong class="ih hj">连续目标变量</strong>(用于回归问题)的算法。选择具有较低<strong class="ih hj">方差</strong>(我们的数据变化的程度)的分割作为分割总体的标准。</li><li id="e7d2" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc li kw kx ky bi translated"><strong class="ih hj"> <em class="kz">卡方</em> </strong> →这是一种找出子节点和父节点统计显著性差异的算法。</li><li id="1cb5" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc li kw kx ky bi translated"><strong class="ih hj"> <em class="kz">熵</em> </strong> →它有助于决定开始决策的最佳属性。也有助于说出具有最高信息增益的属性。它是杂质的存在(随机程度)。</li></ul><p id="67f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">|熵</strong> = -P( <strong class="ih hj">是</strong> ) * <strong class="ih hj"> log2 </strong> P( <strong class="ih hj">是</strong> ) -P( <strong class="ih hj">否</strong> ) * <strong class="ih hj"> log2 </strong> P( <strong class="ih hj">否</strong> ) <strong class="ih hj"> | </strong></p><p id="94d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里，</p><p id="b040" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ P( <strong class="ih hj">是</strong>或<strong class="ih hj">否</strong>)是所有回答中“是”的概率<strong class="ih hj">。</strong></p><p id="99cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> → log2 </strong> P( <strong class="ih hj">是</strong>或，<strong class="ih hj">否</strong>)是总响应中“是”响应的<strong class="ih hj">对数概率</strong>。</p><h2 id="61dc" class="jt ju hi bd jv jw jx jy jz ka kb kc kd iq ke kf kg iu kh ki kj iy kk kl km kn bi translated">购物车算法:→</h2><p id="415a" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">现在，当我们开始构建决策树时，我们总是以连续的方式计算这些东西——</p><ol class=""><li id="a427" class="ko kp hi ih b ii ij im in iq lf iu lg iy lh jc kv kw kx ky bi translated">熵</li><li id="b5ca" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc kv kw kx ky bi translated">来自数据集特定属性的信息，</li><li id="1c0c" class="ko kp hi ih b ii la im lb iq lc iu ld iy le jc kv kw kx ky bi translated">从Outlook功能中获取信息。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/8fb1f33275a996e5da7fb6630fd1ceb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*xTz_RXjaFDEfH1XQj-Jpyg.png"/></div></figure><p id="5f66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将构建一个决策树，以这个例子作为我们的数据集。</p><p id="f32e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，<strong class="ih hj">首先我们计算从属列</strong>的熵，即'<em class="kz"> play' </em>其中，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/d9f4d2f22cb5e238a5a9b1c167291534.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*_OtgYE7_7OkK58AvKGRspQ.png"/></div></figure><p id="9465" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，<strong class="ih hj">我们将计算数据集</strong>的每个独立列的信息增益。当我们看我们找到的专栏时-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/c089165dafd5da658939869a42645d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*cLlCqGrBPGoZf231nbWWeg.png"/></div></figure><p id="dfe0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">则展望列的晴天、阴天和雨天列的总<strong class="ih hj">熵</strong>、来自该属性的信息和来自该属性的信息增益为:-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/3056ec1c0ac6a1cb9039852bd1d5df94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*9mDqkW25JfI9L45xY2qMow.png"/></div></figure><p id="f501" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，当我们计算其他列的信息增益时，信息增益最高的列将被选为生成决策树的根节点。因此，当我们计算信息增益时，我们将得到:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/d6616e57094a2119fc9910a4199fdcb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*g-omrKgMPTi06hwcfQ5YfQ.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/66f9a2f468090c71c6cb525943e33fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*phF1l8ppGKZql3EFgp477Q.png"/></div></figure><p id="07fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们将开始构建我们的决策树→</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/c74338233bbb9fcfaa52c998d9d9fae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*3wOcndmk1VitGAGZfL4rIA.png"/></div></figure><p id="9477" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，对于下一步，出于计算信息增益的目的，具有<strong class="ih hj"> outlook名称的列将被认为是根节点</strong>，而<strong class="ih hj">其余列被认为是子节点</strong>。<strong class="ih hj">注意</strong> —这里阴节点已经有了它的叶节点，所以不应该考虑它。<strong class="ih hj">我们继续执行相同的步骤，直到到达叶节点，并在我们最终拥有叶节点之前，使列父节点在每个迭代步骤中具有最高的信息增益</strong>。然后当我们到达叶节点后，我们将得到我们的决策树——</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lt"><img src="../Images/b65ce7f8cba1db2707bc8128d9b3b22b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOAEpTuwmfk8Osedu73wew.png"/></div></div></figure><p id="af6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是决策树是如何从零开始构建的。我们必须始终牢记<strong class="ih hj">这一关键事项</strong>，即<strong class="ih hj">如果因变量和自变量之间存在高度非线性和复杂关系，那么决策树最适合这种情况</strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/e89f02d9f951dbadd1b7842f2c50c989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JKnh1gvr-f0LUPE3.jpg"/></div></div></figure><p id="25e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望你一定已经对机器学习中如何为各种问题构造决策树有了一些直觉。所以，如果你对此有任何疑问，请在评论区告诉我。在那之前，学习，吃饭，睡觉，重复。</p><p id="e702" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kz">演员表→ Eudreka，你管决策树上的视频</em> </strong></p></div></div>    
</body>
</html>