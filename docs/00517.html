<html>
<head>
<title>PyTorch-Transformers: An Incredible Library for State-of-the-Art NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch-Transformers:最先进的NLP的不可思议的图书馆</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-pytorch-transformers-an-incredible-library-for-state-of-the-art-nlp-ddecd75884ff?source=collection_archive---------1-----------------------#2019-07-18">https://medium.com/analytics-vidhya/introduction-to-pytorch-transformers-an-incredible-library-for-state-of-the-art-nlp-ddecd75884ff?source=collection_archive---------1-----------------------#2019-07-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b7a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">想象一下，有能力构建支持谷歌翻译的<a class="ae jd" href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&amp;utm_medium=pytorch-transformers-nlp-python" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP) </a>模型。嗯——我们现在可以坐在自己的机器前做这件事了！最新的NLP版本被HuggingFace的人称为PyTorch-Transformers。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/ddb582c399fffecc621e3efb617389e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Z1KfmQl2FRkEYXNB.jpg"/></div></div></figure><h1 id="ca13" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">PyTorch-Transformers是什么？</h1><blockquote class="ko kp kq"><p id="5e80" class="if ig kr ih b ii ij ik il im in io ip ks ir is it kt iv iw ix ku iz ja jb jc hb bi translated"><em class="hi"> PyTorch-Transformers是一个用于自然语言处理(NLP)的最先进的预训练模型库。</em></p></blockquote><p id="6977" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该库目前包含PyTorch实现、预训练的模型权重、使用脚本和以下模型的转换实用程序:</p><ol class=""><li id="0ab1" class="kv kw hi ih b ii ij im in iq kx iu ky iy kz jc la lb lc ld bi translated"><strong class="ih hj">伯特(来自谷歌)</strong></li><li id="98ae" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><strong class="ih hj"> GPT(来自OpenAI) </strong></li><li id="3e41" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><strong class="ih hj"> GPT-2(来自OpenAI) </strong></li><li id="1070" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><strong class="ih hj"> Transformer-XL(来自谷歌/CMU) </strong></li><li id="85da" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><strong class="ih hj"> XLNet(来自谷歌/CMU) </strong></li><li id="c8b6" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><strong class="ih hj"> XLM(脸书)</strong></li></ol><p id="c1a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于各种NLP任务，上述所有模型都是同类中最好的。其中一些型号还是上个月的产品！</p><p id="4686" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大多数最先进的模型需要大量的训练数据和在昂贵的GPU硬件上进行数天的训练，这是只有大型技术公司和研究实验室才能负担得起的。但是随着PyTorch变形金刚的推出，现在任何人都可以利用最先进的模型的力量！</p><h1 id="d5f7" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">在机器上安装PyTorch-Transformers</h1><p id="79f5" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">在Python中安装Pytorch-Transformers相当简单。您可以只使用pip安装:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="a53e" class="lt jr hi lp b fi lu lv l lw lx">pip install pytorch-transformers</span></pre><p id="83b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">或者，如果您正在使用Colab:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="6551" class="lt jr hi lp b fi lu lv l lw lx">!pip install pytorch-transformers</span></pre><p id="f3d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于这些模型中的大多数都是GPU密集型的，我建议本文使用<a class="ae jd" href="https://colab.research.google.com/notebooks/welcome.ipynb#recent=true" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>。</p><p id="e752" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kr">注:</em> </strong> <em class="kr">本文中的代码是使用</em><strong class="ih hj"><em class="kr">py torch</em></strong><em class="kr">框架编写的。</em></p><h1 id="5164" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">使用GPT-2预测下一个单词</h1><p id="5ca1" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">因为PyTorch-Transformers支持许多为语言建模而训练的NLP模型，所以它很容易允许像句子完成这样的自然语言生成任务。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ly"><img src="../Images/eb0ec4f465c514745f206a4c244e725d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/0*gqMhR3zkX8svM6UC.png"/></div></figure><p id="cf1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2019年2月，OpenAI通过发布一个名为<strong class="ih hj"> GPT-2的新的基于变压器的语言模型，引起了轩然大波。</strong> GPT-2是一个基于转换器的生成语言模型，在来自互联网的40GB精选文本上进行训练。</p><p id="b6fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们使用GPT-2建立我们自己的句子完成模型。我们将试着预测句子中的下一个单词:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="60ff" class="lt jr hi lp b fi lu lv l lw lx">what is the fastest car in the _________</span></pre><p id="58d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我选择这个例子是因为这是Google的文本补全给出的第一个建议。下面是执行相同操作的代码:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="3c7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代码很简单。我们将文本标记和索引为一个数字序列，并将其传递给<em class="kr"> GPT2LMHeadModel </em>。这只不过是顶部带有语言建模头的GPT2模型转换器(线性层，权重与输入嵌入相关)。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mb"><img src="../Images/f71068ff88a1e501a299cc4c879fb6c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vc5-3FR37y9G8sA6.png"/></div></div></figure><p id="f9e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">厉害！该模型成功预测下一个单词为<strong class="ih hj">【世界】</strong>。这是相当惊人的，因为这是谷歌的建议。我建议您用不同的输入句子来尝试这个模型，看看它在预测句子中的下一个单词时表现如何。</p><h1 id="26c2" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">使用GPT-2、Transformer-XL和XLNet的自然语言生成</h1><p id="7342" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">现在让我们将文本生成提升到一个新的层次。<strong class="ih hj">我们不是只预测下一个单词，而是根据给定的输入生成一段文本。</strong>让我们看看我们的模型对以下输入文本给出了什么样的输出:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="9e88" class="lt jr hi lp b fi lu lv l lw lx">In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.</span></pre><p id="8cc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用PyTorch-Transformers为此任务提供的现成脚本。让我们首先克隆他们的存储库:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="eecd" class="lt jr hi lp b fi lu lv l lw lx">!git clone <a class="ae jd" href="https://github.com/huggingface/pytorch-transformers.git" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/pytorch-transformers.git</a></span></pre><h1 id="c621" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">GPT-2</h1><p id="a787" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">现在，您只需要一个命令来启动模型！</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="7fab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看我们的GPT-2模型对于输入文本给出了什么输出:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="6cff" class="lt jr hi lp b fi lu lv l lw lx">The unicorns had seemed to know each other almost as well as they did common humans. The study was published in Science Translational Medicine on May 6. What's more, researchers found that five percent of the unicorns recognized each other well. The study team thinks this might translate into a future where humans would be able to communicate more clearly with those known as super Unicorns. And if we're going to move ahead with that future, we've got to do it at least a</span></pre><p id="e9da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这不是很疯狂吗？模型生成的文本非常有凝聚力，实际上可能会被误认为是一篇真实的新闻文章。</p><h1 id="6934" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">XLNet</h1><p id="c0cb" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">XLNet将最先进的自动回归模型Transformer-XL的思想整合到预训练中。</p><p id="a2a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以使用以下代码来实现相同的目的:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="e2db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是XLNet给出的输出:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="9210" class="lt jr hi lp b fi lu lv l lw lx">St. Nicholas was located in the valley in Chile. And, they were familiar with the southern part of Spain. Since 1988, people had lived in the valley, for many years. Even without a natural shelter, people were getting a temporary shelter. Some of the unicorns were acquainted with the Spanish language, but the rest were completely unfamiliar with English. But, they were also finding relief in the valley.&lt;eop&gt; Bioinfo &lt; The Bioinfo website has an open, live community about the</span></pre><p id="98bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有意思。虽然GPT-2模型直接关注独角兽新闻的科学角度，但XLNet实际上很好地构建了上下文并巧妙地引入了独角兽的话题。看看Transformer-XL表现如何！</p><h1 id="bea2" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">变压器-XL</h1><p id="30cc" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">Google为语言建模提出了一种称为Transformer-XL(意为超长)的新方法，这使得Transformer架构能够学习更长期的依赖性。</p><p id="6e62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以使用下面的代码来运行Transformer-XL:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="138d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是生成的文本:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="d70d" class="lt jr hi lp b fi lu lv l lw lx">both never spoke in their native language ( a natural language ). If they are speaking in their native language they will have no communication with the original speakers. The encounter with a dingo brought between two and four unicorns to a head at once, thus crossing the border into Peru to avoid internecine warfare, as they did with the Aztecs. On September 11, 1930, three armed robbers killed a donkey for helping their fellow soldiers fight alongside a group of Argentines. During the same year</span></pre><p id="2d4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这太棒了。有趣的是看到不同的模型如何关注输入文本的不同方面以进一步生成。这种差异是由许多因素造成的，但主要是由于不同的训练数据和模型架构。</p><h1 id="37eb" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">为BERT训练一个屏蔽语言模型</h1><p id="dd86" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">谷歌人工智能的新语言表示模型BERT framework使用预训练和微调来创建最先进的NLP模型，用于广泛的任务。</p><p id="e702" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用以下两个无监督预测任务对BERT进行预训练:</p><ol class=""><li id="57f6" class="kv kw hi ih b ii ij im in iq kx iu ky iy kz jc la lb lc ld bi translated">掩蔽语言建模(MLM)</li><li id="6e3a" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">下一句预测</li></ol><p id="2132" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以用PyTorch-Transformers来实现这两个。那么，让我们看看如何为BERT实现屏蔽语言模型。</p><h2 id="3272" class="lt jr hi bd js mc md me jw mf mg mh ka iq mi mj ke iu mk ml ki iy mm mn km mo bi translated">问题定义</h2><p id="1eca" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">让我们正式定义我们的问题陈述:</p><blockquote class="ko kp kq"><p id="cff0" class="if ig kr ih b ii ij ik il im in io ip ks ir is it kt iv iw ix ku iz ja jb jc hb bi translated">给定一个输入序列，我们将随机屏蔽一些单词。然后，该模型应该根据序列中其他非屏蔽单词提供的上下文，预测屏蔽单词的原始值。</p></blockquote><p id="d6a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么我们为什么要这样做呢？模型在训练过程中学习语言的规则。我们很快就会看到这个过程有多有效。</p><p id="7fdc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们使用<code class="du mp mq mr lp b">BertTokenizer </code>从文本字符串准备一个标记化的输入:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="222a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我们的文本在标记化后的样子:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ms"><img src="../Images/f4784ca98d9f973d362a043ea75be984.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/0*jclyxytBE9nm1pnS.png"/></div></figure><p id="6e16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步是将它转换成一个整数序列，并创建它们的PyTorch张量，以便我们可以直接使用它们进行计算:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="3ad7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意<strong class="ih hj">我们在句子的第8个索引处设置了【掩码】，这个索引是单词‘Hensen’。</strong>这就是我们的模型试图预测的。</p><p id="b3eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们的数据已经为BERT正确地进行了预处理，我们将创建一个屏蔽语言模型。现在让我们使用<code class="du mp mq mr lp b">BertForMaskedLM</code>来预测一个屏蔽令牌:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="21a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看模型的输出是什么:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="b7c3" class="lt jr hi lp b fi lu lv l lw lx">Predicted token is: henson</span></pre><p id="bf58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">真是令人印象深刻。</p><p id="7ab6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个在单一输入序列上训练掩蔽语言模型的小演示。然而，对于许多基于变压器的架构来说，这是培训过程中非常重要的一部分。这是因为它允许模型中的双向训练——这在以前是不可能的。</p><h1 id="6e28" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">维迪亚对PyTorch变形金刚的分析</h1><p id="f9d4" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">在本文中，我们使用PyTorch-Transformers实现并探索了各种最新的NLP模型，如BERT、GPT-2、Transformer-XL和XLNet。这更像是一个第一印象实验，我做这个实验是为了给你一个好的直觉，告诉你如何使用这个神奇的库。</p><p id="bf90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是我认为你会喜欢这个图书馆的6个令人信服的理由:</p><ol class=""><li id="ed44" class="kv kw hi ih b ii ij im in iq kx iu ky iy kz jc la lb lc ld bi translated"><strong class="ih hj">预训练模型</strong>:为6种最先进的NLP架构提供预训练模型，为这些模型的27种变体提供预训练权重</li><li id="38f5" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><strong class="ih hj">预处理和微调API: </strong> PyTorch-Transformers不会在预先训练好的权重处停止。它还提供了一个简单的API来完成这些模型所需的所有预处理和微调步骤。</li><li id="8392" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><strong class="ih hj">使用脚本:</strong>它还附带了针对SQUAD 2.0(斯坦福问答数据集)和GLUE(通用语言理解评估)等基准NLP数据集运行这些模型的脚本。</li><li id="d306" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><strong class="ih hj">多语言:</strong> PyTorch-Transformers支持多语言。</li><li id="bd5e" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><strong class="ih hj"> TensorFlow兼容性:</strong>您可以在PyTorch中将TensorFlow检查点作为模型导入</li><li id="d1a2" class="kv kw hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated"><strong class="ih hj">伯特学:</strong>越来越多的研究领域涉及调查像伯特这样的大型变压器的内部工作原理(有人称之为“伯特学”)</li></ol><p id="2816" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你曾经实现过像伯特和GPT-2这样的最先进的模型吗？你对PyTorch变形金刚的第一印象是什么？下面在评论区讨论吧。</p></div><div class="ab cl mt mu gp mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="hb hc hd he hf"><p id="2238" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kr">原载于2019年7月18日</em><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/07/pytorch-transformers-nlp-python/" rel="noopener ugc nofollow" target="_blank"><em class="kr">【https://www.analyticsvidhya.com】</em></a><em class="kr">。</em></p></div></div>    
</body>
</html>