<html>
<head>
<title>REINFORCE Algorithm: Taking baby steps in reinforcement learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化算法:在强化学习中迈出小步</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforce-algorithm-taking-baby-steps-in-reinforcement-learning-ebb1048419e9?source=collection_archive---------8-----------------------#2020-11-24">https://medium.com/analytics-vidhya/reinforce-algorithm-taking-baby-steps-in-reinforcement-learning-ebb1048419e9?source=collection_archive---------8-----------------------#2020-11-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="691c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用增强算法求解 OpenAI 的 Cartpole、Lunar Lander 和 Pong 环境。</p><p id="da3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强化学习可以说是人工智能中最酷的分支。它已经证明了自己的实力:震惊世界，击败了国际象棋、围棋甚至 DotA 2 的世界冠军。</p><p id="3c8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我将介绍一个相当基本的算法，并展示它如何在某些游戏中达到超人的性能水平。</p><p id="d4b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强化学习处理设计与<em class="jd">【环境】</em>互动的<em class="jd">【代理】</em>，并通过系统的试错法自学如何<em class="jd">【解决】</em>环境。一个环境可以是一个游戏，比如国际象棋或赛车，或者甚至可以是一个任务，比如解决一个迷宫或实现一个目标。代理是执行活动的机器人。</p><p id="0390" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理人通过与环境互动获得“回报”。代理人学习执行所需的“行动”,以最大化它从环境中获得的回报。如果代理累积了某个预定义的奖励阈值，则认为环境已解决。这种书呆子式的谈话是我们如何教机器人玩超人象棋或双足机器人走路的。</p><h1 id="8e22" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">强化算法</h1><p id="efb9" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">强化属于一种特殊的强化学习算法，称为策略梯度算法。该算法的一个简单实现包括创建一个<strong class="ih hj">策略</strong>:一个将状态作为输入并生成采取行动的概率作为输出的模型。策略本质上是代理的指南或备忘单，告诉它在每个状态下采取什么行动。然后，在每一步，该策略都被反复执行和微调，直到我们得到一个解决环境问题的策略。</p><p id="1c3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">策略通常是一个神经网络，它将状态作为输入，并生成跨动作空间的概率分布作为输出。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/d43ff0042b7cd25ac1b83c700d7c68e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/0*Qu8sYAfRTIaweTK9.png"/></div></figure><p id="a4c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">策略:示例</p><p id="7e41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该政策的目标是最大化<em class="jd">“预期报酬”。</em></p><p id="c454" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个策略生成在环境的每个站中采取行动的概率。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es kp"><img src="../Images/45c3bcac872bf3e1e4d09e70f0f092cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LzejPjuEIoGmPho1.png"/></div></div></figure><p id="ee09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">政策 1 与政策 2——不同的轨迹</p><p id="3334" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理从这些概率中进行采样，并选择要在环境中执行的操作。在一集的结尾，我们知道代理人如果遵循这个政策可以得到的总回报。我们通过代理人采取的路径反向传播奖励，以估计给定政策在每个州的“预期奖励”。</p><p id="8523" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，折扣奖励是代理人在未来收到的所有奖励的总和乘以系数γ。</p><p id="be11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">任何一个阶段的折扣奖励都是它在下一步得到的奖励+该代理在未来得到的所有奖励的折扣总和。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es ku"><img src="../Images/72bafc1f2d3081638cac806cc18ff6c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CUZEA4W6ENu5D2w9.png"/></div></div></figure><p id="3ca4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过反向传播奖励来计算每个州的折扣因子。</p><p id="88f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于上面的等式，我们是这样计算预期回报的:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kv"><img src="../Images/b25a2220fe1d7bfb150c9d5d34b363df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/0*VEGDnGWab3hNMNBZ.png"/></div></figure><p id="30f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">按照增强算法的最初实现，预期奖励是概率对数和折扣奖励的乘积之和。</p><h1 id="b418" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">算法步骤</h1><p id="f8c6" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">实施加强的步骤如下:</p><ol class=""><li id="4c54" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated"><em class="jd">初始化随机策略(将状态作为输入并返回动作概率的神经网络)</em></li><li id="b89c" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><em class="jd">使用策略玩游戏的 N 个步骤——记录行动概率——来自策略、奖励——来自环境、行动——由代理取样</em></li><li id="aa4c" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><em class="jd">通过反向传播</em>计算每一步的折扣奖励</li><li id="706b" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><em class="jd">计算预期报酬 G </em></li><li id="4617" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><em class="jd">调整策略的权重(NN 中的反向传播误差)以增加 G </em></li><li id="d7d2" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><em class="jd">从 2 </em>开始重复</li></ol><p id="c073" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的<a class="ae lk" href="https://github.com/kvsnoufal/reinforce" rel="noopener ugc nofollow" target="_blank"> Github </a>上使用 Pytorch 检查实现。</p><h1 id="93b8" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">民众</h1><p id="4347" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">我已经在 Pong，CartPole 和月球着陆器上测试了这个算法。在 Pong 和月球着陆器上进行训练需要花费很长时间——在云 GPU 上分别进行超过 96 小时的训练。这个算法有几个更新可以让它收敛得更快，我在这里没有讨论或者实现。如果有兴趣进一步学习，请查看行动者-批评家模型和最近的政策优化。</p><p id="5e40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">翻筋斗</strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ll"><img src="../Images/08972d559c05232e3ee4a781901d095c.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/1*UHhys2ZNYswayeOiQzWfYg.gif"/></div></figure><p id="2112" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">状态:状态:</strong></p><p id="608c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">水平位置，水平速度，极点角度，角速度</p><p id="808a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">动作:</em> </strong></p><p id="68bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">向左推车，向右推车</p><p id="0fa0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机政策玩法:</strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lm"><img src="../Images/bc842a944cbbfd03b08e4f17e33e52fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*5Izgc2GW3jUuMfS7yQuXCg.gif"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">随机游戏-总奖励 18</figcaption></figure><p id="3a31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">经过强化训练的代理策略:</strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ll"><img src="../Images/78c11da32135ebbd163dffb774a6d792.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/1*QqMCAktCSBGlnaGIQGzq3w.gif"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">训练有素的代理-总奖励 434</figcaption></figure><h1 id="0ffc" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">月球着陆器</h1><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ll"><img src="../Images/d53c2eb6aabbf7a97ec9b8bd41c1c0e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/1*4HOexmwYLH1U80S5jBGMsA.gif"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">随机播放代理</figcaption></figure><p id="09e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">状态:</em> </strong></p><p id="706e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">状态是 8 个向量的数组。我不确定它们代表什么。</p><p id="ed35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">动作:</em> </strong></p><p id="a0d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">0:不执行任何操作</p><p id="0ae7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1:点燃左侧发动机</p><p id="68d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2:关闭发动机</p><p id="5beb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3:点燃右侧发动机</p><p id="a558" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">经过强化训练的代理策略:</strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ll"><img src="../Images/3f28c3755580a5e56d145b9f3596bb12.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/1*B3_tembtVdhshUMtCtyrPg.gif"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">经过强化训练的月球着陆器</figcaption></figure><h1 id="f891" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">恶臭</h1><p id="7f4c" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">这很难训练。在 GPU 云服务器上训练了几天。</p><p id="ce4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">状态:状态:</strong>图像</p><p id="c9c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">动作:</em> </strong>向左移动桨，向右移动桨</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ll"><img src="../Images/768429753e8178cea01ecf6f6e761d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/1*1CrhN52-wLfIIoUk0Y4Kmw.gif"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">受过训练的代理人</figcaption></figure><p id="0d65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强化学习取得了超越强化的飞跃性进展。我在这篇文章中的目标是 1。学习强化学习和 2 的基础知识。展示如此简单的方法在解决复杂问题时是多么的强大。我很想在一些赚钱的“游戏”上试试这些，比如股票交易……我猜这是数据科学家的圣杯。</p><p id="e3bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lk" href="https://github.com/kvsnoufal/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> Github 回购</strong></a>:【https://github.com/kvsnoufal/reinforce】T4</p><p id="0686" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">巨人的肩膀:</strong></p><ol class=""><li id="30c6" class="kw kx hi ih b ii ij im in iq ky iu kz iy la jc lb lc ld le bi translated">策略梯度算法(<a class="ae lk" href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2018/04/08/policy-Gradient-Algorithms . html</a>)</li><li id="4175" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">派生增强(<a class="ae lk" rel="noopener" href="/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63">https://medium . com/@ thechrisyoon/derivating-policy-gradients-and-implementing-REINFORCE-f 887949 BD 63</a>)</li><li id="14ec" class="kw kx hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">Udacity 的强化学习课程(<a class="ae lk" href="https://github.com/udacity/deep-reinforcement-learning" rel="noopener ugc nofollow" target="_blank">https://github.com/udacity/deep-reinforcement-learning</a>)</li></ol><p id="cfdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">关于作者</strong></p><blockquote class="lr ls lt"><p id="35f4" class="if ig jd ih b ii ij ik il im in io ip lu ir is it lv iv iw ix lw iz ja jb jc hb bi translated">我在阿联酋迪拜控股公司工作，是一名数据科学家。你可以通过<a class="ae lk" href="https://www.analyticsvidhya.com/cdn-cgi/l/email-protection" rel="noopener ugc nofollow" target="_blank">【邮件保护】</a>或<a class="ae lk" href="https://www.linkedin.com/in/kvsnoufal/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/kvsnoufal/</a>联系我</p></blockquote><p id="be85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您也可以在我们的移动应用程序上阅读这篇文章</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es lx"><img src="../Images/12475f2705236f6db046db3953586098.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/0*qwNHHaoYzU2_SXEn.png"/></div></div></figure><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ly"><img src="../Images/7437aea3c371fe5e458df276660513e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/0*jncj2yKstv_W7c8I.png"/></div></figure></div><div class="ab cl lz ma gp mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hb hc hd he hf"><p id="54e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">原载于 2020 年 11 月 24 日 https://www.analyticsvidhya.com</em><em class="jd">的</em> <a class="ae lk" href="https://www.analyticsvidhya.com/blog/2020/11/reinforce-algorithm-taking-baby-steps-in-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank"> <em class="jd">。</em></a></p></div></div>    
</body>
</html>