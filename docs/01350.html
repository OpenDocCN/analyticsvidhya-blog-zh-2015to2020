<html>
<head>
<title>Creating Video Art via TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过TensorFlow创作视频艺术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/video-art-via-tensorflow-and-transfer-learning-1b2b0344b1ee?source=collection_archive---------12-----------------------#2019-10-16">https://medium.com/analytics-vidhya/video-art-via-tensorflow-and-transfer-learning-1b2b0344b1ee?source=collection_archive---------12-----------------------#2019-10-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5a5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你今早醒来时有没有渴望把你的YouTube视频变成动画古典绘画？还是变得更狂野？如果是这样，你来对地方了。在这个简短的教程中，我将演示如何将一个视频转换成你选择的动画艺术。</p><p id="4ddf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，这里有一些现代艺术:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/5ba0c705c1b7b3506c4fdd1fd5e66c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qWfoZxl2OACzMFo0.jpg"/></div></div></figure><p id="3e45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用它作为我们想要的基础风格，让我们来改造一个公园瀑布。这个视频的前半部分是原创，后半部分是风格化版本。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jp jq l"/></div></figure><p id="d036" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是另一个例子，这次是一个更加城市化的场景。我用莫奈的画和埃舍尔的版画设计了这个视频。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jr jq l"/></div></figure><p id="7211" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些渲染背后的一般技术是<em class="js">迁移学习</em>。迁移学习允许我们利用预先训练的神经网络来完成完全不同的任务。这里，我们重新调整了vgg19网络的用途。这个网络被训练来将数百万张图像分类到1000个类别中的一个。我们对这些分类不感兴趣，但我们感兴趣的是网络在隐藏层中学习了什么。这些隐藏层在不断增加的抽象层次上对图像知识(边缘、颜色、风格等的感知)进行编码。样式算法的任务是在生成新图像时重用该信息。</p><p id="3a09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">阅读<a class="ae jt" href="https://www.tensorflow.org/tutorials/generative/style_transfer" rel="noopener ugc nofollow" target="_blank">这篇</a>来了解这个想法的介绍。</p><p id="1c0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文中的实现需要相当多的工作。然而，我们在Google的朋友为我们简化了事情，创建了一个Tensorflow Hub模块来封装这种逻辑。你可以在这里找到这个模块<a class="ae jt" href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_arbitrary_image_stylization.ipynb" rel="noopener ugc nofollow" target="_blank"/>。给定这个样式模块，我们需要做的就是加载我们的原始图像和目标样式图像，然后通过hub运行它们。结果是由给定样式转换的图像。</p><p id="7acb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的例子中，我们感兴趣的是视频而不是图像。因此，我们需要首先将我们的视频转换成图像序列，通过中枢样式化模型对这些图像进行样式化，然后从得到的样式化帧中重建视频。</p><p id="e168" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看一下代码。</p><p id="12bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主要功能如下。流程是这样的:我们将原始视频和想要用作样式的图像(如莫奈或埃舍尔jpeg)作为输入。然后，我们从输入的视频中提取音轨，然后将视频解压缩为一系列图像。在进行过程中，我们还对每张图片运行样式模块。结果是一个样式化图像的目录。最后，我们将这些图像重新组合成一个视频文件，并重新连接音频轨道。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="c05a" class="jz ka hi jv b fi kb kc l kd ke">if len(sys.argv) != 3:<br/>    print('usage: video.py video style')<br/>    exit()</span><span id="be2c" class="jz ka hi jv b fi kf kc l kd ke">name_original = sys.argv[1]<br/>name_style = sys.argv[2]</span><span id="5ddf" class="jz ka hi jv b fi kf kc l kd ke"># load and cache the styling dnn<br/>hub_module = hub.load(HUB_URL)</span><span id="f205" class="jz ka hi jv b fi kf kc l kd ke"># extract audio from the video<br/>extract_mp3(PATH_VIDEOS + name_original)</span><span id="a6ec" class="jz ka hi jv b fi kf kc l kd ke"># extract all frames from the video, style them<br/># and put results into tmp<br/>generate_frames(PATH_VIDEOS + name_original, <br/>    PATH_STYLES + name_style)</span><span id="45d3" class="jz ka hi jv b fi kf kc l kd ke"># regenerate the video from the styled frames<br/>output_name = os.path.splitext(name_original)[0] <br/>    + '.' + os.path.splitext(name_style)[0] + '.mp4'<br/>generate_video(PATH_OUTPUTS + output_name)</span><span id="3821" class="jz ka hi jv b fi kf kc l kd ke"># recombine the extracted audio into the newly-styled video<br/>input_name = output_name<br/>output_name = os.path.splitext(name_original)[0] <br/>    + '.' + os.path.splitext(name_style)[0] + '.audio.mp4'</span><span id="6c92" class="jz ka hi jv b fi kf kc l kd ke">add_mp3(PATH_OUTPUTS + input_name, PATH_OUTPUTS + output_name)</span></pre><p id="f71d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们来看看底层函数。</p><p id="4eaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，这里是提取mp3的方法。我们通过子进程直接使用ffmpeg来完成这项工作，因为没有好的python绑定。结果就是一个mp3，我们储存起来以后用。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="8f14" class="jz ka hi jv b fi kb kc l kd ke">def extract_mp3(path_video):</span><span id="7a95" class="jz ka hi jv b fi kf kc l kd ke">    print('Extracting audio: ', path_video, PATH_TMP_MP3)</span><span id="07c3" class="jz ka hi jv b fi kf kc l kd ke">    command = 'ffmpeg -i {0} -f mp3 -ab 192000 <br/>        -vn {1}'.format(path_video, PATH_TMP_MP3)</span><span id="46f4" class="jz ka hi jv b fi kf kc l kd ke">    subprocess.call(command, shell=True)</span></pre><p id="60a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我使用OpenCV包从视频中提取所有图像。我们捕获视频，然后将每个图像解压到工作目录中。当我们这样做的时候，我们将样式中枢应用到每张图片上。结果是一个包含视频的原始图像帧以及样式帧的目录。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="30f9" class="jz ka hi jv b fi kb kc l kd ke">def generate_frames(path_input_video, path_image_style):</span><span id="f46c" class="jz ka hi jv b fi kf kc l kd ke">    video_capture = cv2.VideoCapture(path_input_video)<br/>    image_style = load_image(path_image_style);</span><span id="c420" class="jz ka hi jv b fi kf kc l kd ke">    for count in range(MAX_FRAMES):</span><span id="8baa" class="jz ka hi jv b fi kf kc l kd ke">        success, image = video_capture.read()<br/>        if success == False: break</span><span id="8281" class="jz ka hi jv b fi kf kc l kd ke">        path_frame = PATH_TMP + (str(count).zfill(5)) + '.jpg'<br/>        path_converted_frame = PATH_TMP + <br/>            'x' + (str(count).zfill(5)) + '.jpg'</span><span id="d71c" class="jz ka hi jv b fi kf kc l kd ke">        cv2.imwrite(path_frame, image)</span><span id="482e" class="jz ka hi jv b fi kf kc l kd ke">        image = load_image(path_frame)<br/>        results = hub_module(tf.constant(image), <br/>                tf.constant(image_style))<br/>        image = tf.squeeze(results[0], axis=0)<br/>        mpl.image.imsave(path_converted_frame, image)<br/>        print(count, path_frame, path_converted_frame)</span></pre><p id="1f2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了视频的所有样式的图像。下一步是遍历这些样式化的图像，并将它们转换回视频。这是按如下方式完成的:</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="bc2a" class="jz ka hi jv b fi kb kc l kd ke">def generate_video(path_output_video):</span><span id="18f2" class="jz ka hi jv b fi kf kc l kd ke">    image_list = []<br/>    count = 0<br/>    path_converted_frame = PATH_TMP + 'x' + <br/>        (str(count).zfill(5)) + '.jpg'</span><span id="c8a7" class="jz ka hi jv b fi kf kc l kd ke">    image = cv2.imread(path_converted_frame)<br/>    height, width, layers = image.shape<br/>    size = (width,height)<br/>    print('size: ', size)</span><span id="6f3c" class="jz ka hi jv b fi kf kc l kd ke">    converted_files = [file_name for file_name in <br/>        os.listdir(PATH_TMP) if 'x' in file_name]<br/>    converted_files.sort()</span><span id="7282" class="jz ka hi jv b fi kf kc l kd ke">    for file_name in converted_files:</span><span id="9068" class="jz ka hi jv b fi kf kc l kd ke">        path_converted_frame = PATH_TMP + file_name<br/>        image = cv2.imread(path_converted_frame)<br/>        print(path_converted_frame)<br/>        image_list.append(image)</span><span id="8b7e" class="jz ka hi jv b fi kf kc l kd ke">    video_writer = cv2.VideoWriter(path_output_video, <br/>        cv2.VideoWriter_fourcc(*'mp4v'), VIDEO_FPS, size)</span><span id="5637" class="jz ka hi jv b fi kf kc l kd ke">    for i in range(len(image_list)):<br/>        video_writer.write(image_list[i])</span><span id="f075" class="jz ka hi jv b fi kf kc l kd ke">    video_writer.release()<br/>    print('video generated: ', path_output_video)</span></pre><p id="cd6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了一个风格化的视频。最后一步是重新连接音轨。我们再次将ffmpeg应用于此任务:</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="3247" class="jz ka hi jv b fi kb kc l kd ke">def add_mp3(path_input_video, path_output_video):</span><span id="de2e" class="jz ka hi jv b fi kf kc l kd ke">    print('Adding audio: ', PATH_TMP_MP3, <br/>        path_input_video, <br/>        path_output_video)</span><span id="a0a8" class="jz ka hi jv b fi kf kc l kd ke">    command = 'ffmpeg -i {0} -i {1} -c:v copy -c:a <br/>        aac -strict experimental {2} '.<br/>        format(path_input_video, <br/>            PATH_TMP_MP3, <br/>            path_output_video)</span><span id="d57f" class="jz ka hi jv b fi kf kc l kd ke">    subprocess.call(command, shell=True)</span></pre><p id="9623" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就是这样！你可以在这里看到完整的代码清单。如果你想看看其他的人工智能项目，可以看看我的<a class="ae jt" href="https://www.christopherminson.com/" rel="noopener ugc nofollow" target="_blank">网站</a>。</p><p id="4e6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与大多数ML项目相比，这段代码相对较快。即使在运行缓慢的机器上，你也可以做有用的事情。例如，我在低端市场t2.large ec2实例上生成了这些视频。没有GPU，没有TPU，什么都没有。在那种环境下，一个15秒的视频通常不到一个小时就可以渲染出来。考虑到所涉及的处理量，令人印象深刻。更快的机器当然会超过这个基准。</p><p id="9ac3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我发现有些风格比其他风格更好。颜色、笔触风格和阴影转移得相当好，而较大规模的结构被最小化或丢失。因此，像莫奈的画这样的东西会翻译得相当好。相比之下，毕加索或埃舍尔的作品会因其不同寻常的几何形状而失去个性。那对你来说可能重要也可能不重要。此外，将样式图像保持在合理的分辨率也很重要。对于我的测试用例，我发现512x512左右的任何东西都足够好。您的里程可能会有所不同。</p><p id="8edc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，值得停下来欣赏一下迁移学习的总体酷劲。这是一种强大的技术，应用广泛，有时并不明显。考虑一下:我们只是采用了一个经过训练的深度神经网络来对图像进行分类，然后使用该网络来创建风格化的视频。这是一个很大的飞跃。我希望随着时间的推移，这种技术会变得更加强大和普及。</p></div></div>    
</body>
</html>