<html>
<head>
<title>Support Vector Machine (SVM) and Kernels Trick</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(SVM)和核技巧</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-svm-and-kernel-trick-part-1-theory-d990e2872ace?source=collection_archive---------2-----------------------#2020-08-27">https://medium.com/analytics-vidhya/introduction-to-svm-and-kernel-trick-part-1-theory-d990e2872ace?source=collection_archive---------2-----------------------#2020-08-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/9d476b16e055c1e9ed5e87a37962979c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*z9E86MWCf0hy-Al4-c-b2A.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">我自己的SVM设计使用<a class="ae iq" href="https://www.canva.com/" rel="noopener ugc nofollow" target="_blank"> Canva </a></figcaption></figure><h1 id="bc19" class="ir is hi bd it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated"><strong class="ak">什么是SVM？</strong></h1><p id="cefd" class="pw-post-body-paragraph jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hb bi translated">支持向量机(SVM)是机器学习中包含的监督学习中的一种分类和回归算法，也称为支持向量网络。SVM更常用于分类问题，而不是回归。</p><p id="2df1" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">SVM算法是由Vapnik和他的同事Bernhard Bose和Isabelle Guyon在1992年首先提出的，作为模式识别中的一系列高级概念。</p><p id="313e" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">SVM通过使用结构风险最小化(SRM)原理来工作，该原理旨在获得在输入空间中将数据分成两类的最佳超平面线。</p><p id="f651" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">起初，SVM线性工作，但后来SVM再次开发，以便它可以通过寻找用于计算数据类之间距离(边距)的超平面来非线性工作。在SVM应用可以应用于线性和非线性分类。</p><p id="72c6" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">SVM方法根据其特点分为两种，即线性SVM和非线性SVM。线性SVM是使用软边界将可以线性分成两类的数据进行分类。线性分类通常适用于维数较低的数据集，也就是说，数据集只有很少的要素需要分类。同时，非线性SVM在高维工作空间中使用核概念。核概念是通过修改SVM算法来解决非线性问题所使用的函数。</p><p id="c676" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">SVM概念被称为寻找最佳超平面的尝试，该超平面将在输入空间中将数据分成两类。关于SVM概念的训练过程的主要目标是找到超平面的位置。SVM方法使用点积函数。超平面是用来分隔数据集的线。超平面可以是二维中的一条线，也可以是多个平面中的一个平面。SVM算法中确定最佳超平面的例子。下面是SVM最佳超平面的插图。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ks"><img src="../Images/d62c75029b8413dad201fd45ca988696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r_UGNlW6praP-eaO2kjeLw.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">SVM上最佳超平面确定的图解</figcaption></figure><p id="3cb2" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">超平面可以通过测量超平面边缘来获得，超平面边缘是超平面与每个数据类的最近点之间的距离。分隔超平面的最近点称为支持向量。</p><p id="f860" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">在上图中，有一个黄色圆圈数据，它是+1类中的数据，红色方框数据是-1类中的数据。黄色圆圈数据是class +1的成员，而红色方框数据是class -1的成员。红线中可以看到的最佳超平面在正超平面和负超平面的中间。同时，支持向量是黄色圆圈和红色圆圈。现在我将描述SVM的一部分类型。看看吧！</p><h2 id="1060" class="lb is hi bd it lc ld le ix lf lg lh jb ka li lj jf ke lk ll jj ki lm ln jn lo bi translated"><strong class="ak">林尼尔·SVM</strong></h2><p id="13ac" class="pw-post-body-paragraph jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hb bi translated">线性分类通常用于低维数据集。数据集的维度越低，意味着需要分类的要素就越少。两幅图像中的超平面可以通过测量超平面和每一类中最近点之间的距离(余量)来获得。</p><p id="4c93" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">属于线性分类的例子是确定年龄和饮食因素是否影响人类健康。在这种情况下，只有两个特征是影响人类健康的因素，即年龄因素作为特征x，食物因素作为特征y。以下是线性SVM情况的可视化。</p><p id="9eea" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">线性SVM是SVM的工作原理之一，用于可线性分离的数据，如下图所示。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/cdb927da6d3efd20f85d157d871933fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*UDOVT0TpDCnzkZeNt31nPQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">林尼尔·SVM的形象化</figcaption></figure><p id="9389" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">SVM中可用的数据由符号(xi) ∈ R^d和每个类的标签来符号化，即类+1和类-1，它们被假设由d维超平面完美地分开，给定符号yi ∈ {-1，+ 1} <em class="lq"> </em>，其中<em class="lq"> </em> i = 1，2，…，<em class="lq">l</em>；其中l是大量数据。从而得到超平面方程的定义如下:<br/> f (x) = w ^ T.x + b或w.x + b = 0</p><p id="9979" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">从而根据一个超平面方程得到线性SVM中的正类:<br/> w. (xi) + b ≤ + 1</p><p id="3f65" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">而对于线性SVM中的负类超平面方程是:<br/> w. (xi) + b ≥ - 1</p><p id="2a16" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">信息:<br/> <em class="lq"> w </em> =权重(权重向量)<br/> <em class="lq"> x </em> =矩阵输入值(特征)<br/> <em class="lq"> b </em> =偏差</p><p id="1b7c" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">为了计算最大的余量值，通过优化超平面和每个类中最近点之间的距离值来完成。用二次规划(QP)作为公式求带方程约束的方程的极小点:<br/> τ (w) = 1/2 ‖w‖ ^ 2</p><p id="f9a1" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">易()。w + b)-1≥0</p><p id="5ea3" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">上述问题可以用各种计算技巧来解决，其中一种是使用Langrange乘子方程如下:<br/> L (w，b，α)= 1/2‖w‖^ 2-∑_(I = 1)<em class="lq">l</em>αI(yi((Xi)。w + b) -1)</p><p id="37e9" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">用，<br/> i = 1，2，…，<em class="lq"> l </em></p><p id="8904" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">语言乘数= <em class="lq"> αi </em>其值为零或正值(αi≥0)，其中i = 1，2，…，<em class="lq"> l </em>。从而将朗奇乘子方程修改为只包含αi如下:<br/>∑_(I = 1)^<em class="lq">l</em>αI-1/2∑_(I，j = 1) ^ <em class="lq"> l </em> αi αj yi yj (xi)，xj</p><p id="771d" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">用，<br/>αI≥0；(i = 1，2，…，<em class="lq">l</em>)；∑_ (i = 1) ^ <em class="lq"> l </em> αi yi = 0</p><p id="24d2" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">上面的大多数计算获得了正的αi，其中与正的αi相关的数据被称为支持向量。以便使用下面的等式来确定新数据分类的结果:<br/> <em class="lq"> Class = sign f (x) </em></p><h2 id="93e7" class="lb is hi bd it lc ld le ix lf lg lh jb ka li lj jf ke lk ll jj ki lm ln jn lo bi translated"><strong class="ak">非林尼尔·SVM</strong></h2><p id="44e4" class="pw-post-body-paragraph jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hb bi translated">非线性SVM是SVM的另一个工作原理，用于因其维数高而无法线性分离的数据。使用核概念进行非线性分类。非线性情况下的核心概念在确定用作模型的分类极限中起作用。</p><p id="c146" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">非线性SVM将核心概念的功能应用于具有高维度的空间。高维的含义是数据集有两个以上的特征需要分类。例如，非线性分类案例，即影响人类健康的因素，由年龄因素、饮食因素、运动因素、遗传、疾病史和压力水平组成。</p><p id="952f" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">在这个例子中，核心概念用于确定用作模型的分类边界。下图显示了非线性SVM的情况。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/7c1dd99ec60d231b5312115823c20220.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*sJanLHLUrHfh1ZWzEWX6xQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">非线性SVM的可视化</figcaption></figure><p id="fde8" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">由SVM算法中的过程生成的模型的准确性非常依赖于所使用的参数和核函数。在非线性SVM中使用核函数是需要考虑的事情，因为SVM的性能取决于核函数的选择。</p><p id="b744" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">非林尼尔SVM在实践中是用内核实现的，所以它可以用被称为内核技巧的内核函数来分离数据。</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><h1 id="d867" class="ir is hi bd it iu lz iw ix iy ma ja jb jc mb je jf jg mc ji jj jk md jm jn jo bi translated"><strong class="ak">内核绝招</strong></h1><p id="73da" class="pw-post-body-paragraph jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hb bi translated">使用内核技巧，SVM可以在非线性数据情况下工作得很好。内核技巧的功能是将低维输入空间和变换映射到高维空间。</p><ul class=""><li id="ae89" class="me mf hi jr b js kn jw ko ka mg ke mh ki mi km mj mk ml mm bi translated"><strong class="jr hj">径向基函数核(RBF) </strong></li></ul><p id="dbae" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">RBF核是最广泛使用的核概念，用于解决无法线性分离的数据集的分类问题。已知该核在某些参数下具有良好的性能，并且与其他核相比，训练的结果具有较小的误差值。RBF核函数的公式为:</p><p id="276c" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><code class="du mn mo mp mq b"><strong class="jr hj">K(x,xi) = exp(-gamma * sum((x – xi^2))</strong></code></p><p id="c587" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">高斯核RBF有两个参数，即γ和σ。伽马参数有一个默认值，即γ = 1 / (2σ) ^ 2。当伽马值较高时，数据周围的点可能会在计算中被考虑。sigma参数用于查找每个数据集的最佳值。</p><p id="f7bc" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">在RBF核函数方程中，xi-x是两个不同特征空间中x1和x2之间的欧几里德距离，σ (sigma)是确定核权重的RBF核参数。在SVM，西格玛参数需要调整，以提供准确的分类结果。σ参数的默认值为σ = 1。</p><ul class=""><li id="8407" class="me mf hi jr b js kn jw ko ka mg ke mh ki mi km mj mk ml mm bi translated"><strong class="jr hj">多项式内核</strong></li></ul><p id="b135" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">多项式核是线性核的更一般化形式。在机器学习中，多项式核是适用于支持向量机(SVM)和其他核化的核函数，其中核表示特征空间中训练样本向量的相似性。多项式核也适用于解决标准化训练数据集的分类问题。多项式核函数的公式为:</p><p id="75d1" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><code class="du mn mo mp mq b"><strong class="jr hj">K(x,xi) = 1 + sum(x * xi)^d</strong></code></p><p id="e15a" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">当数据不能线性分离时，使用这个内核。<br/>多项式核有一个度参数(d ),用于在每个数据集中查找最优值。d参数是多项式核函数的次数，默认值为d = 2。d值越大，产生的系统精度将会波动且不太稳定。这是因为d参数值越高，生成的超平面线越弯曲。</p><ul class=""><li id="3b54" class="me mf hi jr b js kn jw ko ka mg ke mh ki mi km mj mk ml mm bi translated"><strong class="jr hj">s形内核</strong></li></ul><p id="917f" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">sigmoid核的概念是人工神经网络(ANN)的发展，其核函数的方程为:</p><p id="2848" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><code class="du mn mo mp mq b"><strong class="jr hj">K(x,xi) = tanh</strong></code><strong class="jr hj"><em class="lq">(α</em></strong><code class="du mn mo mp mq b"><strong class="jr hj">xi.xj +</strong></code><strong class="jr hj">【β】</strong></p><p id="b319" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">Sigmoid核已经在理论上被提出用于支持向量机(SVM ),因为它源于神经网络，但是直到现在它还没有在实践中被广泛使用。</p><p id="0c11" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">sigmoid核广泛应用于分类过程的神经网络中。具有sigmoid核的SVM分类具有复杂的结构，并且人类很难解释和理解sigmoid核如何做出分类决策。对这些核的兴趣源于它们在用神经网络和逻辑回归分类、特定性质、线性和累积分布方面的成功。</p><p id="1761" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">sigmoid内核通常是有问题的或无效的，因为它很难有正参数。sigmoid函数现在没有在研究中广泛使用，因为它有一个主要缺点，即sigmoid函数的输出值范围不是以零为中心。这导致反向传播过程的发生，这是不理想的，因此ANN的权重不是均匀地分布在正值和负值之间，而是趋向于接近极值0和1。</p><ul class=""><li id="8521" class="me mf hi jr b js kn jw ko ka mg ke mh ki mi km mj mk ml mm bi translated"><strong class="jr hj">线性内核</strong></li></ul><p id="7340" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">线性核可以用作任意两个给定观测值的正常点积。核函数的等式是:</p><p id="f11b" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><code class="du mn mo mp mq b"><strong class="jr hj">K(x, xi) = sum(x * xi)</strong></code></p><p id="39aa" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">最后，就是这样。希望这一节有助于你们理解SVM和内核技巧的概念。您可以在下面给出一些评论、想法、反馈或建议。请继续学习，并关注更多内容！</p></div></div>    
</body>
</html>