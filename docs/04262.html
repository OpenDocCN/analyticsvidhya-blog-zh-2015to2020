<html>
<head>
<title>Gradient Descent Problems and Solutions in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的梯度下降问题及其解决方案</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-problems-and-solutions-in-deep-learning-8002bbac09d5?source=collection_archive---------3-----------------------#2020-03-12">https://medium.com/analytics-vidhya/gradient-descent-problems-and-solutions-in-deep-learning-8002bbac09d5?source=collection_archive---------3-----------------------#2020-03-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ae703555fe8a9a6eb844a4140d207cd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EnUl17eBywv1EwAGi-vQRg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:https://deeplizard.com/learn/video/qO_NLVjD6zE</figcaption></figure><h1 id="df8f" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">里程碑</h1><ul class=""><li id="5a40" class="jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated"><strong class="jv hj">简介</strong></li><li id="6b3d" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated"><strong class="jv hj">神经网络故事简而言之</strong> <br/>反向传播的症结</li><li id="56ed" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg kh ki kj kk bi translated"><strong class="jv hj">梯度下降</strong>T9】直觉<br/>为什么在NN中求导？<br/>渐变下降是如何工作的？<br/>梯度下降问题<br/>梯度问题的原因<br/>避免梯度问题的解决方案</li></ul></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="f173" class="iv iw hi bd ix iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js bi translated">介绍</h1><p id="bd11" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">梯度问题是神经网络训练的障碍。通常你可以在涉及基于梯度的方法和反向传播的人工神经网络中找到这一点。但是今天，在深度学习时代，各种替代解决方案被引入，根除了网络学习的缺陷。这个博客将让你对各种梯度问题有一个深入的了解，详细描述其因果情况和解决方案。此外，博客将推断一个关于神经网络架构和学习过程以及关键计算的想法。</p><h1 id="8ea7" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">神经网络故事简而言之</h1><p id="717c" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">神经网络是具有权重、偏置和激活功能的相互连接的神经元的网络。学习从输入的线性(仿射)变换到非线性变换开始，使用激活函数通过正向传播和反向传播阶段。现在让我们把它带到核心。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/2cda1ae869c328d6619e1446acdf225f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*xkyyL6J1_Q6ll_K9t6rS3g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图1</figcaption></figure><p id="88ec" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">整个学习过程分为递归的前向和后向传播。</p><p id="7c96" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated"><strong class="jv hj">正向传播:</strong></p><p id="5829" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">输入被传递给隐藏层的神经元，带有一些随机初始化的权重和偏差，如下面所示的<strong class="jv hj">线性变换</strong>。</p><blockquote class="mb mc md"><p id="b714" class="lc ld me jv b jw lw le lf jy lx lg lh mf ly lj lk mg lz lm ln mh ma lp lq kg hb bi translated">z =(输入*重量)+偏差</p></blockquote><p id="6613" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">为了解决复杂问题，<strong class="jv hj">引入非线性变换</strong>，通过<strong class="jv hj">激活函数</strong>实现。线性变换(z)的输出，即输入的加权和，被提供给下面的激活函数。</p><blockquote class="mb mc md"><p id="77b9" class="lc ld me jv b jw lw le lf jy lx lg lh mf ly lj lk mg lz lm ln mh ma lp lq kg hb bi translated">A = f(z)</p></blockquote><p id="5578" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">假设对于Sigmoid激活函数，</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/e2b66b371f5b8de0b4603a9802318161.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*bTAu8YGWd8WPzpgs1wSysQ.png"/></div></figure><p id="6bb4" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">每一层都应用这些线性方程，然后是非线性方程。因此，每一层的最终输出将是激活函数的输出。</p><p id="fd2b" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">您将在输出图层(y^)中获得预测输出，在每个图层中执行相同的过程。</p><p id="0fb7" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated"><strong class="jv hj">反向传播:</strong></p><p id="022d" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">预测output(y^)可能与实际产量(y)不同，因此，使用<strong class="jv hj">损失(成本)函数(J) </strong>计算损失。这说明我们的预测与实际情况有多大的偏差。让我们用<strong class="jv hj">损失平方和均值</strong>函数来表示损失。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/d5e35da01076d64ee1e0ce62f983bdda.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*o2rq6xQmmJfHBXF7WcT8Rw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">版权</figcaption></figure><p id="89ab" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">这种损失被传播回初始层，同时更新每层中每个神经元的权重。用最佳权重反向传播误差的过程被称为反向传播。同时计算<strong class="jv hj">每个权重如何影响“误差”</strong>。想法是最小化在成本函数(误差)中贡献更多的神经元的权重。</p><p id="b76f" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">现在，<strong class="jv hj">降低了成本函数(没有损失)，权重应该调整</strong>。这可以通过跨训练迭代的反复试验来完成，这非常麻烦。因此，需要优化器来调整权重，以便最小化成本函数(损失)。为了理解重量是如何影响输入的，计算成本函数的导数，即相对于重量的损失率(<strong class="jv hj"> dJ / dw </strong>)。于是，<em class="me">来了</em> <strong class="jv hj"> <em class="me">渐变下降</em> </strong>。</p><p id="ad23" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">如上所述，反向传播用于计算成本函数J(w)的偏导数，其值将用于梯度下降算法。最终结果将是优化的权重，该权重将按照下面的等式进行更新。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="ab fe cl mk"><img src="../Images/b242019ba1bbc44c9ed9482c85f52dad.png" data-original-src="https://miro.medium.com/v2/format:webp/1*UdfwyEx7_S2fTtlihSjNCg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">版权</figcaption></figure><p id="58f9" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">术语“向后”是指梯度计算从网络向后开始。首先计算最后一层的权重梯度，而第一层在最后。</p><p id="43f1" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">再次前馈激活输出，获得损耗，并重复直到获得满意的结果。</p><h2 id="f977" class="ml iw hi bd ix mm mn mo jb mp mq mr jf ka ms mt jj kc mu mv jn ke mw mx jr my bi translated">反向传播的症结</h2><p id="25c9" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">具有输入(x)的一组输入神经元与具有特定权重(w)的下一层神经元相连接，该组输入神经元被相乘并被传递给给出特定输出的激活函数。计算误差时要记住实际输出，实际输出是使用成本函数的某些导数反向传播的。让我们讨论一下。</p><p id="11ae" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">考虑下面的网络。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/01e5b0bd5f5c931401894f2c97240d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*Il8J3K0K9ZpIBzi3sLMVNg.png"/></div></figure><p id="3ac4" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">基本上，反向传播是为了在下一次迭代中更新权重以减少损失。将重量公式更新为:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="ab fe cl mk"><img src="../Images/b242019ba1bbc44c9ed9482c85f52dad.png" data-original-src="https://miro.medium.com/v2/format:webp/1*UdfwyEx7_S2fTtlihSjNCg.png"/></div></figure><p id="d00c" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">因此，我们需要有导数(dJ/dw ),于是，链式法则的微分概念就出现了，即复合函数的导数。</p><p id="699a" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">为了计算相对于第一重量的误差导数，通过链式法则反向传播(如图所示)。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es na"><img src="../Images/fe53381757a978d73fac15bf6ca08ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*Nl6oZVBzZizVstcH5tVEcg.png"/></div></figure><p id="359b" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">上面显示的是由于重量引起的误差函数变化是由于激活引起的误差函数变化(最终)输出乘以由于重量引起的激活变化。整个网络的输出是Hidden2神经元(a2)的激活，因此是Hidden2层的Sigmoid函数的导数。</p><p id="ae37" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">汇总所有导数将放入图中的权重更新方程，给出新的权重。</p><h1 id="ab73" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">梯度下降</h1><h2 id="e41a" class="ml iw hi bd ix mm mn mo jb mp mq mr jf ka ms mt jj kc mu mv jn ke mw mx jr my bi translated">直觉</h2><p id="0fec" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">在一项研究中有一个美丽的解释:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nb"><img src="../Images/c2124ad2e6f000b36a88d4fe832bc4c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*seK-ipd1vML_Gselsvbhpw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="http://cs231n.stanford.edu/slides/winter1516_lecture3.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="cd04" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">假设你被蒙上眼睛，必须到达山的最低点的一个湖。由于能见度为零，你只能通过触摸地面和获得坡度的想法来达到。无论地面下降到哪里，我们都向下走一步，以便更快地到达湖边。这个向斜坡下降的过程充当梯度下降算法，这是一个<strong class="jv hj">迭代方法</strong>。</p><blockquote class="nc"><p id="3308" class="nd ne hi bd nf ng nh ni nj nk nl kg dx translated">当我们说梯度时，它是指损失函数相对于网络中的权重的梯度。</p></blockquote><p id="431d" class="pw-post-body-paragraph lc ld hi jv b jw nm le lf jy nn lg lh ka no lj lk kc np lm ln ke nq lp lq kg hb bi translated"><strong class="jv hj">梯度</strong>是一个有方向和大小的矢量。梯度是凸曲线的斜率(<strong class="jv hj">导数</strong> w.r.t权值)。它是在反向传播期间计算的，之后参数(权重)得到更新。</p><h2 id="c041" class="ml iw hi bd ix mm mn mo jb mp mq mr jf ka ms mt jj kc mu mv jn ke mw mx jr my bi translated">为什么要用NN中的导数？</h2><p id="d289" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">导数一般<strong class="jv hj">用于</strong>优化问题，如梯度下降到<strong class="jv hj">优化权重</strong>(增加/减少)以达到最小成本函数值。<br/>在NN中，在反向传播期间计算成本函数相对于权重(w)的导数。这是指计算梯度下降时权重参数变化的影响。</p><h2 id="e1ed" class="ml iw hi bd ix mm mn mo jb mp mq mr jf ka ms mt jj kc mu mv jn ke mw mx jr my bi translated"><strong class="ak">梯度下降的工作原理</strong></h2><p id="6477" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">在NN中，通过梯度下降算法<strong class="jv hj">计算应该向后传播的最佳权重，该算法又通过偏导数</strong>计算<strong class="jv hj">，如图3所示。</strong></p><p id="13a6" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">下面说说图吧。<br/>成本函数与权重之间的图表显示了它如何实现其达到全局成本最小点的目标。朝向极小点的每个<strong class="jv hj">步长</strong>由梯度(斜率)即<strong class="jv hj">导数</strong>决定<strong class="jv hj">，而步长取决于<strong class="jv hj"> learn_rate </strong>。选择不适当的learn_rate和激活函数会导致各种梯度问题，这些问题将在后面的章节中讨论。</strong></p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nr"><img src="../Images/7ed7af0c3682fe6b42df2e1eb5879524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sSZdvsDSa8Zqyd87VGsPBQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.youtube.com/watch?v=b4Vyma9wPHo" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="1914" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated"><strong class="jv hj">梯度下降算法</strong></p><ol class=""><li id="23d4" class="jt ju hi jv b jw lw jy lx ka ns kc nt ke nu kg nv ki kj kk bi translated">随机初始化权重w</li><li id="0695" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg nv ki kj kk bi translated">使用成本函数的导数wrt权重J(w)计算梯度G</li></ol><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nw"><img src="../Images/7c471f6bd649b233386e0c9455dad5d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*p9DpbTYVgQd49sjZaZHpOg.png"/></div></figure><p id="01ef" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">3.权重更新等式:<em class="me"> w = w-ηG </em> <br/> <em class="me">这里，η是一个learn_rate，它不应该太高或太低而跳过或根本不收敛到最小点。</em></p><p id="e984" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">4.重复步骤2到3，直到它变成一个常数变化。</p><h1 id="4475" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">梯度问题</h1><h2 id="907a" class="ml iw hi bd ix mm mn mo jb mp mq mr jf ka ms mt jj kc mu mv jn ke mw mx jr my bi translated">一.消失梯度</h2><p id="1d66" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">消失梯度是神经网络学习过程中的一个场景，其中<strong class="jv hj">模型根本不学习</strong>。这是因为当梯度变得太小，几乎为零，导致权重卡住，永远不会达到最小损失的最优值(全局最小值)。从而网络无法学习和收敛。尤其是在链式规则微分期间，从最后一层到初始层的反向传播可能根本不会导致权重的更新。</p><h2 id="9698" class="ml iw hi bd ix mm mn mo jb mp mq mr jf ka ms mt jj kc mu mv jn ke mw mx jr my bi translated">二。爆炸梯度</h2><p id="5baa" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">与消失梯度完全相反，当模型不断学习时，权重不断更新，但模型永远不会收敛。计算相对于权重的<strong class="jv hj">梯度</strong>(损失)，该权重在早期层中变得非常大，以至于<strong class="jv hj">爆炸</strong>。继续振荡，采取大的步长，如上面第三个图所示，并在远离收敛点时偏离收敛点。</p><h2 id="c2b2" class="ml iw hi bd ix mm mn mo jb mp mq mr jf ka ms mt jj kc mu mv jn ke mw mx jr my bi translated">三。鞍点(极小极大点)</h2><p id="c583" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">损失函数曲面上的鞍点是这样的外交点，从一个维度看，临界点似乎是最小值，而从其他维度看，它似乎是最大值点。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nx"><img src="../Images/826c42ed0b5ed3b57d95d25fbd00b024.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*aA4jibK_AyqON54LWehjTg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://srdas.github.io/DLBook/GradientDescentTechniques.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="339e" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">鞍点是围绕学习大惊小怪，因为它导致了混乱。当模型学习在达到“最小值”时停止，因此斜率=0，这实际上是来自其他维度的最大成本值。这导致了非最佳点。</p><p id="5552" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">当梯度下降在多维中运行时，鞍点进入视野。</p><p id="3b3d" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">这是临界点的场景。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ny"><img src="../Images/12b8611aeb9ea7d2353a17c821b52941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-keAwGe2TOfMhCogs_3g3g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://math.libretexts.org/Bookshelves/Calculus/Map%3A_University_Calculus_(Hass_et_al.)/13%3A_Partial_Derivatives/13.7%3A_Extreme_Values_and_Saddle_Points" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="74d0" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">梯度问题的原因</h1><p id="3258" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">在进入梯度问题的原因之前，让我们看看其他参数是如何导致我们的神经网络模型不能收敛的。</p><ol class=""><li id="74a6" class="jt ju hi jv b jw lw jy lx ka ns kc nt ke nu kg nv ki kj kk bi translated">学习率</li><li id="9b90" class="jt ju hi jv b jw kl jy km ka kn kc ko ke kp kg nv ki kj kk bi translated">梯度下降</li></ol><h2 id="3b7b" class="ml iw hi bd ix mm mn mo jb mp mq mr jf ka ms mt jj kc mu mv jn ke mw mx jr my bi translated">学习率</h2><p id="5840" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">学习率是指权重减少/增加的速率。低学习率导致如此多的更新，并且模型将永远无法达到实际上是低成本函数(损失)值的全局最小点。高学习率将随着太大的权重更新而爆炸，并且可能跳过模型收敛点。因此，设置最佳值将使我们的模型达到最小点(低成本值)。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nz"><img src="../Images/98a0975b88fcf01d6a7840ec4a8be046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*An4tZEyQAYgPAZl396JzWg.png"/></div></div></figure><p id="aed5" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">如果梯度项(<strong class="jv hj"> dJ/dw </strong>)太小或太大，模型也不会收敛，该梯度项是权重更新方程(梯度下降公式)中误差函数的导数。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="ab fe cl mk"><img src="../Images/b242019ba1bbc44c9ed9482c85f52dad.png" data-original-src="https://miro.medium.com/v2/format:webp/1*UdfwyEx7_S2fTtlihSjNCg.png"/></div></figure><p id="6658" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">当与学习率相乘时，计算的梯度(<strong class="jv hj"> dJ/dw </strong>)太小，导致值变小。如果学习率也很低，效果会更小。用重量减去这个较小的值几乎不会导致重量的任何变化。导致模型不收敛。<br/>或者相反，较高梯度与学习率的乘积导致较高的值，当从权重中减去时，导致每个时期中巨大的权重更新，因此可能反弹最优值。这两种情况都不允许模型收敛。</p><p id="815a" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated"><strong class="jv hj">但问题是，</strong></p><h1 id="ce93" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">为什么梯度会太低或太高？</strong></h1><p id="830a" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">梯度下降法是一种权重优化算法，它包括代价函数和激活函数。怎么会？好吧，让我们看看反向传播过程中梯度下降的链式法则。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es na"><img src="../Images/fe53381757a978d73fac15bf6ca08ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*Nl6oZVBzZizVstcH5tVEcg.png"/></div></figure><p id="3890" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">这里，J指的是成本函数，其中项(dJ/dw1)是成本函数w.r.t权重的<strong class="jv hj">导数。通俗地说，我们看看w1对误差函数有什么影响。项(dy^/da2)是输出层</strong>的激活函数的<strong class="jv hj">导数。另一项(da2/da1)是隐藏层激活函数</strong>、<em class="me">的<strong class="jv hj">导数，假设输出和隐藏层都是Sigmoid激活函数。</strong></em></p><p id="28c1" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated"><strong class="jv hj">Sigmoid导数的范围</strong>为<strong class="jv hj"> (0，1/4)</strong>。当sigmoid函数导数链乘以最大值的1/4时，会产生更多更小的值。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es oa"><img src="../Images/1e380a481ffd460d9eaf1f3d8445a6aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*YM9LE3ujzPxUf9g2L94_AA.png"/></div></figure><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es oa"><img src="../Images/d259c54a6a0d086b531bc0dbba70e89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*St5qLrGDdnYW6KqYfZh_pg.png"/></div></figure><p id="8202" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">这两个术语的乘积:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es ob"><img src="../Images/6719d34c0d9ed97a5a324ff586ab3458.png" data-original-src="https://miro.medium.com/v2/resize:fit:192/format:webp/1*bgW124niAL93aZPvlK_1eg.png"/></div></figure><p id="ba07" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">Sigmoid导数范围(例如，1/4)乘以范围的权重(-1，1)会得到更小的值。简单的数学表明两个较小的数会导致更多的较小的数。因此，这种导数链推断(dJ/dw)为微小值。这就是为什么梯度会太低以至于几乎消失。同样，对于激活函数双曲正切(<strong class="jv hj"> tanh </strong>)，其<strong class="jv hj">导数范围为【0，1】</strong>，也是更小的有限值，结果与上述相同。不管怎样，Sigmoid和tanh的最大输出范围分别是(0，1)和(-1，1)。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es oc"><img src="../Images/0fdca45b4b965a88b10dbc702dd86991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*kbfvtXvlOFMjfPpaffAnNA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d" rel="noopener" target="_blank">来源</a></figcaption></figure><blockquote class="nc"><p id="eeeb" class="nd ne hi bd nf ng od oe of og oh kg dx translated">在Sigmoid和Tanh激活函数的情况下，当从输出层传播到初始层时，梯度呈指数下降。因此，学习非常缓慢或根本不学。</p></blockquote><p id="3406" class="pw-post-body-paragraph lc ld hi jv b jw nm le lf jy nn lg lh ka no lj lk kc np lm ln ke nq lp lq kg hb bi translated"><strong class="jv hj">当Sigmoid激活函数或替代的双曲正切函数由于其导数的范围而出现时，梯度问题就出现了。更明确地说，sigmoid导数的范围(0，1/4)和tanh [0，1]是问题的根本原因。</strong></p><h1 id="5ce7" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">避免梯度问题的解决方案</h1><p id="d805" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">为了避免渐变问题，最好为隐藏层选择一个合适的激活函数。对于隐藏层，可以使用除sigmoid和tanh之外的任何激活函数。例如，ReLU、LeakyReLU等。但是他们将如何解决梯度问题呢？</p><h2 id="158d" class="ml iw hi bd ix mm mn mo jb mp mq mr jf ka ms mt jj kc mu mv jn ke mw mx jr my bi translated">ReLU:</h2><p id="2adc" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated">在Sigmoid和tanh函数的缺点之后，ReLU变得相当流行。这在隐藏层中非常有用。</p><p id="462f" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">输入范围从(-infinite，infinite)分别产生范围为(0，Input)的输出。小于0的输入的ReLU函数的导数为0，而等于或大于1的为1。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es oi"><img src="../Images/cb8a67d4830442520321762aebb8321d.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*edsoHDBeamlrdvenLiYA7A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="5a48" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">由于ReLU函数不像sigmoid和tanh那样在(0，1)的范围内，所以梯度不会很小，从而解决了消失梯度问题。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es oj"><img src="../Images/3ffb8a8887308ec573920a01d74d561e.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*yN4orYTrJqNa3fZ46Bl8KA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://analyticsindiamag.com/most-common-activation-functions-in-neural-networks-and-rationale-behind-it/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="e6e8" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">然而，ReLU有一个导致<strong class="jv hj">死亡神经元</strong>的缺点，这不在本博客的讨论范围之内。为了克服它的缺点，eLU被引进了。</p></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><h1 id="4747" class="iv iw hi bd ix iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js bi translated"><strong class="ak">一些很棒的参考是:</strong></h1><p id="7957" class="pw-post-body-paragraph lc ld hi jv b jw jx le lf jy jz lg lh ka li lj lk kc ll lm ln ke lo lp lq kg hb bi translated"><a class="ae iu" href="https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b" rel="noopener ugc nofollow" target="_blank">https://ayearofai . com/rohan-4-the vanishing-gradient-problem-ec68f 76 FFB 9b</a></p><p id="9a12" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated"><a class="ae iu" href="https://brilliant.org/wiki/backpropagation/" rel="noopener ugc nofollow" target="_blank">https://brilliant.org/wiki/backpropagation/</a></p><p id="85e9" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated"><a class="ae iu" href="https://www.jeremyjordan.me/nn-learning-rate/" rel="noopener ugc nofollow" target="_blank">https://www.jeremyjordan.me/nn-learning-rate/</a></p></div><div class="ab cl kq kr gp ks" role="separator"><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv kw"/><span class="kt bw bk ku kv"/></div><div class="hb hc hd he hf"><p id="ca83" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">快乐阅读！</p><p id="f329" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated"><strong class="jv hj"> <em class="me">可以通过</em></strong><a class="ae iu" href="https://www.linkedin.com/in/kaul-shachi" rel="noopener ugc nofollow" target="_blank"><strong class="jv hj"><em class="me">LinkedIn</em></strong></a><strong class="jv hj"><em class="me">与我取得联系。</em> </strong></p><p id="61a9" class="pw-post-body-paragraph lc ld hi jv b jw lw le lf jy lx lg lh ka ly lj lk kc lz lm ln ke ma lp lq kg hb bi translated">欢迎在评论区分享你的观点或任何误导性的信息。:)</p></div></div>    
</body>
</html>