<html>
<head>
<title>Using Tensorflow 2.0 to Build a CNN for Image Classification 😎</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Tensorflow 2.0 构建用于图像分类的 CNN😎</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/using-tensorflow-2-0-to-build-a-cnn-for-image-classification-4e50848ce1c0?source=collection_archive---------6-----------------------#2020-08-07">https://medium.com/analytics-vidhya/using-tensorflow-2-0-to-build-a-cnn-for-image-classification-4e50848ce1c0?source=collection_archive---------6-----------------------#2020-08-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c42c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">目标受众:熟悉 Python，对神经网络有基本了解的 ML 爱好者。我绝不是专家，想分享我学到的东西！</strong></p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><p id="1900" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TensorFlow 版本于 2019 年 9 月(差不多 1 年前)发布，促进了机器学习模型的创建和使用。即使有一些使用 TensorFlow 版的经验，我发现在 MNIST 基准数据集上实现一个准确率约为 98.5%的工作模型有一个陡峭的学习曲线。在这里，我想分享我如何在 Python 3 中用 TensorFlow 2.0 实现了一个卷积神经网络，以便帮助那些可能正在学习曲线中挣扎的人。</p><h1 id="9ac6" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">安装方法:</h1><p id="36c4" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">下面是我使用的硬件和软件规格，它们导致了一个工作安装和一个工作模型:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/43c90b68370a1517e7df9227f93ed53f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EfA4sIUXwL0CUs7N"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">由<a class="ae ld" href="https://unsplash.com/@maxcodes?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">麦斯威尔·尼尔森</a>在<a class="ae ld" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><ul class=""><li id="9205" class="le lf hi ih b ii ij im in iq lg iu lh iy li jc lj lk ll lm bi translated">操作系统:Windows 10</li><li id="1e2c" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">IDE: PyCharm 2019.3.5</li><li id="ef1e" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">Python 3.6.7</li><li id="b6cc" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">张量流 2.3.0</li><li id="ece1" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">NVIDIA GeForce GTX1050</li><li id="c4a5" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">NVIDIA 驱动程序版本 451.67</li><li id="4eca" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">CUDA 10.1(更新 2)</li><li id="0eaf" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">CUPTI 10.1(自带 CUDA)</li><li id="1eeb" class="le lf hi ih b ii ln im lo iq lp iu lq iy lr jc lj lk ll lm bi translated">cuDNN v7.6.5</li></ul><p id="e99a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之前在 Tensorflow v1.0 (TF1)中，有一个单独的 GPU tensorflow 包，而在 TF2，有一个针对 CPU、GPU 和多 GPU tensorflow 版本的全局包。用你的 Python 包管理器安装 TF2 (我在 PyCharm 中使用了 pip)。👍</p><p id="300b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TF2 应该默认使用你的 CPU，但是如果你有一个<a class="ae ld" href="https://developer.nvidia.com/cuda-gpus" rel="noopener ugc nofollow" target="_blank">有效的 NVIDIA GPU </a>并且想像我一样使用它，那么你就需要再遵循几个<a class="ae ld" href="https://www.tensorflow.org/install/gpu" rel="noopener ugc nofollow" target="_blank">安装步骤</a>。你必须安装各自的<a class="ae ld" href="https://developer.nvidia.com/cuda-toolkit-archive" rel="noopener ugc nofollow" target="_blank"> CUDA </a>、<a class="ae ld" href="https://developer.nvidia.com/cuda-toolkit-archive" rel="noopener ugc nofollow" target="_blank"> CUPTI </a>和<a class="ae ld" href="https://developer.nvidia.com/rdp/cudnn-archive" rel="noopener ugc nofollow" target="_blank"> cuDNN </a>库。TF 网站保存了这些库的兼容版本列表<a class="ae ld" href="https://www.tensorflow.org/install/source#linux" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="b12a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检查 TF2 是否已成功安装在您的系统上:</p><pre class="ko kp kq kr fd ls lt lu lv aw lw bi"><span id="26da" class="lx jl hi lt b fi ly lz l ma mb">import tensorflow as tf<br/>print(tf.constant('Hello from TensorFlow ' + tf.__version__))</span></pre><p id="432a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于 GPU 构建，请检查您是否有一个 GPU，并为其安装了 TF2:</p><pre class="ko kp kq kr fd ls lt lu lv aw lw bi"><span id="ed51" class="lx jl hi lt b fi ly lz l ma mb">import tensorflow as tf<br/>print(tf.config.list_physical_devices('GPU'))<br/>print(tf.test.is_built_with_cuda())</span></pre><p id="f59c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果这些行如预期运行，那么恭喜你🎉，您就可以在 TensorFlow 2.0 中建立模型了。如果你在安装过程中有问题(我肯定有)，请在下面评论，我会尽力提供反馈。</p><h1 id="c273" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">构建模型框架:</h1><p id="cf77" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">现在让我们来看看我们将用来在 TF2 构建定制机器学习(ML)模型的主干。注意，这个框架使用了 Keras 包，这是一个高级 TensorFlow 包装器，使得定制 ML 模型(尤其是 CNN 的)更加容易。这里我用 32c3p 2–32c3p 2–32c5s 2-D128-D10 架构做了一个 CNN 图像分类器:</p><pre class="ko kp kq kr fd ls lt lu lv aw lw bi"><span id="e1c7" class="lx jl hi lt b fi ly lz l ma mb">import os<br/># Turn off TensorFlow warning messages in program output<br/>os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'<br/>import tensorflow as tf<br/>from tensorflow.keras import Model<br/>from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, BatchNormalization, Dropout<br/><br/>class Image_CNN(Model):<br/>  def __init__(self):<br/>    super().__init__()<br/>    self.conv1 = Conv2D(32, 3, input_shape=(..., 3), strides=1, activation='relu')<br/>    self.conv2 = Conv2D(32, 3, strides=1, activation='relu')<br/>    self.conv3 = Conv2D(32, 5, strides=2, activation='relu')<br/><br/>    self.pool1 = MaxPool2D(pool_size=(2,2))<br/>    self.batchnorm = BatchNormalization()<br/>    self.dropout40 = Dropout(rate=0.4)<br/><br/>    self.flatten = Flatten()<br/>    self.d128 = Dense(128, activation='relu')<br/>    self.d10softmax = Dense(10, activation='softmax')<br/><br/>  def call(self, x, training=False):<br/>    x = self.conv1(x)<br/>    x = self.pool1(x)<br/>    x = self.conv2(x)<br/>    x = self.pool1(x)<br/>    x = self.conv3(x)<br/>    x = self.batchnorm(x)<br/>    if training:<br/>        x = self.dropout40(x, training=training)<br/><br/>    x = self.flatten(x)<br/>    x = self.d128(x)<br/>    x = self.d10softmax(x)<br/><br/>    return x</span></pre><p id="9c85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，我们导入了 CNN 的构建块，并在我们的“Image_CNN”类中实现了它们。自定义模型继承自 Keras 模型，必须包含 2 个方法(函数):“__init__”和“call”。“__init__”方法定义了层,“call”方法定义了在急切执行下使用的层的顺序和数量，这是 TF2 最大的新特性之一。</p><p id="9878" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意#1 </strong>:为了使你的 ML 模型完全定制化，你应该定义你的模型的每一个构建块(层),而不是使用预先打包的 Keras 层。</p><p id="c467" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意#2 </strong>:在这个 CNN 中，我在卷积步骤后添加了一个漏层。这不仅有助于训练神经网络，它应该只用于训练(我们不想删除我们的任何测试结果)。</p><h1 id="624c" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">定义训练和测试步骤:</h1><p id="52a3" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">我们定义了将为每个数据点和每个时期执行的训练函数和测试函数。</p><pre class="ko kp kq kr fd ls lt lu lv aw lw bi"><span id="61b3" class="lx jl hi lt b fi ly lz l ma mb">@tf.function<br/>def train_step(images, labels):<br/>  with tf.GradientTape() as tape:<br/>    # training=True is only needed if there are layers with different<br/>    # behavior during training versus inference (e.g. Dropout).<br/>    predictions = model(images, training=True)<br/>    loss = loss_object(labels, predictions)<br/>  gradients = tape.gradient(loss, model.trainable_variables)<br/>  optimizer.apply_gradients(zip(gradients, model.trainable_variables))<br/><br/>  train_loss(loss)<br/>  train_accuracy(labels, predictions)<br/><br/>@tf.function<br/>def test_step(images, labels):<br/>  # training=False is only needed if there are layers with different<br/>  # behavior during training versus inference (e.g. Dropout).<br/>  predictions = model(images, training=False)<br/>  t_loss = loss_object(labels, predictions)<br/><br/>  test_loss(t_loss)<br/>  test_accuracy(labels, predictions)</span></pre><p id="dd21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个函数都由一个 tensorflow 装饰函数包装，该函数将 Python 函数转换为静态 tensorflow 图。因为 TF2 使用急切执行，这些函数的逐行求值可能会很慢，所以我们将函数转换成一个静态图来加速代码执行。👍</p><p id="a929" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您还会注意到，在“train_step”方法中，我们使用了 tf。TF2 的另一个新特色。这也是向 TensorFlow 的急切执行方法转变的结果。因为我们的模型急切地执行(而不是作为静态图形)，我们需要在它们运行时跟踪每一层的梯度，我们使用 GradientTape 来完成这项工作。这些梯度然后被馈送到所选择的优化器中，以通过最小化损失函数来继续学习过程。🧠</p><h1 id="b112" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">准备数据集并运行模型:</h1><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mc"><img src="../Images/a44dad8c82db9612dbefb062830eb91f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IrhPOPxI70n8zzJk"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">由<a class="ae ld" href="https://unsplash.com/@swimstaralex?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">拍摄的亚历山大·辛恩</a>在<a class="ae ld" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="aa8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在准备训练和测试我们的神经网络！在这里，我们加载了经典的手写数字 MNIST 数据集，它可以直接从 tf.keras.datasets 导入。有 60，000 个训练图像和 10，000 个测试图像，每个图像的维数为 28x28。然后使用 tf.data.Dataset 将这些图像以 32 个一批的方式输入到我们的模型中。</p><pre class="ko kp kq kr fd ls lt lu lv aw lw bi"><span id="d7c4" class="lx jl hi lt b fi ly lz l ma mb">if __name__ == '__main__':<br/>    import time<br/>    start_time = time.time()<br/><br/>    # Load MNIST images and normalize pixel range to 0-1.<br/>    mnist = tf.keras.datasets.mnist<br/>    (x_train, y_train), (x_test, y_test) = mnist.load_data()<br/>    x_train, x_test = x_train / 255.0, x_test / 255.0<br/><br/>    # Add a channels dimension.<br/>    x_train = x_train[..., tf.newaxis].astype("float32")<br/>    x_test = x_test[..., tf.newaxis].astype("float32")<br/><br/>    # Set up a data pipeline for feeding the training and testing data into the model.<br/>    shuff_size = int(0.25 * len(y_train))<br/>    batch_size = 32<br/>    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(shuff_size).batch(batch_size)<br/>    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)<br/><br/>    # Instantiate our neural network model from the predefined class. Also define the loss function and optimizer.<br/>    model = Image_CNN()<br/>    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)<br/>    optimizer = tf.keras.optimizers.Adam()<br/><br/>    # Define the metrics for loss and accuracy.<br/>    train_loss = tf.keras.metrics.Mean(name='train_loss')<br/>    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')<br/>    test_loss = tf.keras.metrics.Mean(name='test_loss')<br/>    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')<br/><br/>    # Run and iterate model over epochs<br/>    EPOCHS = 5<br/>    for epoch in range(EPOCHS):<br/><br/>      # Reset the metrics at the start of the next epoch<br/>      train_loss.reset_states()<br/>      train_accuracy.reset_states()<br/>      test_loss.reset_states()<br/>      test_accuracy.reset_states()<br/><br/>      # Train then test the model<br/>      for images, labels in train_ds:<br/>        train_step(images, labels)<br/>      for test_images, test_labels in test_ds:<br/>        test_step(test_images, test_labels)<br/><br/>      # Print results<br/>      template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'<br/>      print(template.format(epoch + 1,<br/>                            train_loss.result(),<br/>                            train_accuracy.result() * 100,<br/>                            test_loss.result(),<br/>                            test_accuracy.result() * 100))<br/>    print("time elapsed: {:.2f}s".format(time.time() - start_time))</span></pre><p id="b667" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们正在执行多标签图像分类，所以我们的精度度量将是分类交叉熵，并且我们将在 5 个时期内训练我们的 CNN。让我们看看我们做得怎么样:</p><pre class="ko kp kq kr fd ls lt lu lv aw lw bi"><span id="d3cc" class="lx jl hi lt b fi ly lz l ma mb">Epoch 1, Loss: 1.5532550811767578, Accuracy: 92.288330078125, Test Loss: 1.4825093746185303, Test Accuracy: 98.0</span><span id="f903" class="lx jl hi lt b fi md lz l ma mb">Epoch 2, Loss: 1.4951773881912231, Accuracy: 96.80332946777344, Test Loss: 1.4800351858139038, Test Accuracy: 98.12999725341797</span><span id="1d65" class="lx jl hi lt b fi md lz l ma mb">Epoch 3, Loss: 1.488990306854248, Accuracy: 97.30999755859375, Test Loss: 1.4788316488265991, Test Accuracy: 98.2699966430664</span><span id="96d9" class="lx jl hi lt b fi md lz l ma mb">Epoch 4, Loss: 1.4862409830093384, Accuracy: 97.55833435058594, Test Loss: 1.4759035110473633, Test Accuracy: 98.58000183105469</span><span id="f26e" class="lx jl hi lt b fi md lz l ma mb">Epoch 5, Loss: 1.484948992729187, Accuracy: 97.66667175292969, Test Loss: 1.475019931793213, Test Accuracy: 98.63999938964844</span><span id="c0f3" class="lx jl hi lt b fi md lz l ma mb">time elapsed: 26.20s</span></pre><p id="5991" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经成功安装了 TF2，创建了 CNN 图像分类器，并使用 GPU 在 30 秒内实现了良好的测试准确性！</p><p id="18a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你对我们在✌的过程有任何问题和评论，欢迎在下面发帖</p><pre class="ko kp kq kr fd ls lt lu lv aw lw bi"><span id="5c3f" class="lx jl hi lt b fi ly lz l ma mb">import os<br/># Turn off TensorFlow warning messages in program output<br/>os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'<br/>import tensorflow as tf<br/>from tensorflow.keras import Model<br/>from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, BatchNormalization, Dropout<br/><br/>class Image_CNN(Model):<br/>  def __init__(self):<br/>    super().__init__()<br/>    self.conv1 = Conv2D(32, 3, input_shape=(..., 3), strides=1, activation='relu')<br/>    self.conv2 = Conv2D(32, 3, strides=1, activation='relu')<br/>    self.conv3 = Conv2D(32, 5, strides=2, activation='relu')<br/><br/>    self.pool1 = MaxPool2D(pool_size=(2,2))<br/>    self.batchnorm = BatchNormalization()<br/>    self.dropout40 = Dropout(rate=0.4)<br/><br/>    self.flatten = Flatten()<br/>    self.d128 = Dense(128, activation='relu')<br/>    self.d10softmax = Dense(10, activation='softmax')<br/><br/>  def call(self, x, training=False):<br/>    x = self.conv1(x)<br/>    x = self.pool1(x)<br/>    x = self.conv2(x)<br/>    x = self.pool1(x)<br/>    x = self.conv3(x)<br/>    x = self.batchnorm(x)<br/>    if training:<br/>        x = self.dropout40(x, training=training)<br/><br/>    x = self.flatten(x)<br/>    x = self.d128(x)<br/>    x = self.d10softmax(x)<br/><br/>    return x<br/><br/>@tf.function<br/>def train_step(images, labels):<br/>  with tf.GradientTape() as tape:<br/>    # training=True is only needed if there are layers with different<br/>    # behavior during training versus inference (e.g. Dropout).<br/>    predictions = model(images, training=True)<br/>    loss = loss_object(labels, predictions)<br/>  gradients = tape.gradient(loss, model.trainable_variables)<br/>  optimizer.apply_gradients(zip(gradients, model.trainable_variables))<br/><br/>  train_loss(loss)<br/>  train_accuracy(labels, predictions)<br/><br/>@tf.function<br/>def test_step(images, labels):<br/>  # training=False is only needed if there are layers with different<br/>  # behavior during training versus inference (e.g. Dropout).<br/>  predictions = model(images, training=False)<br/>  t_loss = loss_object(labels, predictions)<br/><br/>  test_loss(t_loss)<br/>  test_accuracy(labels, predictions)<br/><br/><br/>if __name__ == '__main__':<br/>    import time<br/>    start_time = time.time()<br/><br/>    # Load MNIST images and normalize pixel range to 0-1.<br/>    mnist = tf.keras.datasets.mnist<br/>    (x_train, y_train), (x_test, y_test) = mnist.load_data()<br/>    x_train, x_test = x_train / 255.0, x_test / 255.0<br/><br/>    # Add a channels dimension.<br/>    x_train = x_train[..., tf.newaxis].astype("float32")<br/>    x_test = x_test[..., tf.newaxis].astype("float32")<br/><br/>    # Set up a data pipeline for feeding the training and testing data into the model.<br/>    shuff_size = int(0.25 * len(y_train))<br/>    batch_size = 32<br/>    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(shuff_size).batch(batch_size)<br/>    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)<br/><br/>    # Instantiate our neural network model from the predefined class. Also define the loss function and optimizer.<br/>    model = Image_CNN()<br/>    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)<br/>    optimizer = tf.keras.optimizers.Adam()<br/><br/>    # Define the metrics for loss and accuracy.<br/>    train_loss = tf.keras.metrics.Mean(name='train_loss')<br/>    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')<br/>    test_loss = tf.keras.metrics.Mean(name='test_loss')<br/>    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')<br/><br/>    # Run and iterate model over epochs<br/>    EPOCHS = 5<br/>    for epoch in range(EPOCHS):<br/><br/>      # Reset the metrics at the start of the next epoch<br/>      train_loss.reset_states()<br/>      train_accuracy.reset_states()<br/>      test_loss.reset_states()<br/>      test_accuracy.reset_states()<br/><br/>      # Train then test the model<br/>      for images, labels in train_ds:<br/>        train_step(images, labels)<br/>      for test_images, test_labels in test_ds:<br/>        test_step(test_images, test_labels)<br/><br/>      # Print results<br/>      template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'<br/>      print(template.format(epoch + 1,<br/>                            train_loss.result(),<br/>                            train_accuracy.result() * 100,<br/>                            test_loss.result(),<br/>                            test_accuracy.result() * 100))<br/>    print("time elapsed: {:.2f}s".format(time.time() - start_time))</span></pre></div></div>    
</body>
</html>