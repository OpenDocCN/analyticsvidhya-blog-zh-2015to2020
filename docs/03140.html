<html>
<head>
<title>Optimizations — An essential mathematical toolkit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最优化——一个基本的数学工具包</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimizations-an-essential-mathematical-toolkit-b36e77b0e03a?source=collection_archive---------19-----------------------#2020-01-17">https://medium.com/analytics-vidhya/optimizations-an-essential-mathematical-toolkit-b36e77b0e03a?source=collection_archive---------19-----------------------#2020-01-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/e40bfa1dac6dd39a066cac438fcd9f71.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*q2HY2Yn-hJwGuFK8dBE62Q.png"/></div></figure><h1 id="9ef1" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">介绍</h1><p id="3272" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">有意或无意的优化是我们日常生活的一部分。我们优化的一个非常珍贵的东西是时间，我们以这样一种方式优化它，使时间可用于我们所有的家务和爱好。</p><p id="3d21" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">从个人一生的预算开始，到一个国家的预算，都是关于优化的。所有这些优化都是在保持特定标准或约束的情况下完成的。</p><p id="5478" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">机器学习使用优化技术来寻找与所使用的算法相对应的损失或目标函数的最优解。根据损失函数的性质，有不同类型的优化。</p><p id="b6fc" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">解决最优化问题有解析方法和数值方法。当问题有精确解时，就使用解析解。当没有直接解时，使用数值解，这些大多是获得最优解的迭代过程。</p><p id="994a" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">优化算法还应具备以下特性:</p><p id="c8a1" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">准确性</strong>:算法应该能够精确地识别解决方案，并且不应该对系统或数据中的错误或舍入问题敏感。</p><p id="ac16" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">健壮性</strong>:算法应该在各种问题上表现良好。</p><p id="8878" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">效率</strong>:算法不应该使用过多的存储或时间来收敛。</p><p id="cbba" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">然而，获得所有上述属性是不可能的，因此，优化算法是这些属性之间的折衷。</p><p id="331e" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">在这篇文章中，我想简单介绍几种在机器学习算法中广泛使用的优化方法。在此之前，我们需要了解一些优化中使用的术语。</p><h1 id="4f02" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">优化术语</h1><h1 id="920a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">凸函数</h1><p id="5084" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">凸函数是一个连续函数，它在定义域中每个区间中点的值不超过它在区间两端的值的算术平均值。数学上，函数f(x)在区间[a，b]上是凸的，如果对于[a，b]中的任意两点x1和x2以及任意λ，其中0 </p><p id="8173" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">f[λx1+(1-λ)x2] &lt;= λf(x1)+(1-λ)f(x2).</p><p id="111a" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">An example for a convex function would be f(x) = x2</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kn"><img src="../Images/17cd65c54a923254c5e05a0af4386a66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0oNQd_PA8dQjsg5FwE869w.jpeg"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">Convex function</figcaption></figure><h1 id="984a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">Concave Function</h1><p id="2ab2" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">A function is a concave function if on an interval [a,b] if for any two points x1 and x2 in [a,b] and any λ, where 0</p><p id="1da1" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">f[λx1+(1-λ)x2] &gt; = λf(x1)+(1-λ)f(x2)。</p><p id="8998" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">凹函数=-凸函数。一个例子是f(x) = -x2</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es la"><img src="../Images/14ed6e7ddf9ece6879990cdcfce3043d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yF7muw-omx2824yq4HgfTg.jpeg"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">凹函数</figcaption></figure><h1 id="50a4" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">局部最优</h1><p id="5c72" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">对于h的所有值，如果f(x*) &lt;= f(x*+h) for all values of h.</p><p id="dafd" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">A function f(x) is said to have a local maximum at x=x*, if f(x*) &gt; = f(x*+h ),则称函数f(x)在x=x*处有局部最小值。</p><h1 id="0c84" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">全局最优</h1><p id="576f" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">如果f(x*) &lt;= f(x) for all values of x.</p><p id="4e9c" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">A function f(x) is said to have a local maximum at x=x*, if f(x*) &gt; = f(x)对于x的所有值，则称函数f(x)在x=x*处有局部最小值。</p><h1 id="5f0f" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">拐点</h1><p id="8b72" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">如果函数在点x*的任一侧增加或减少，那么x*就是拐点。</p><h1 id="9360" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">鞍点</h1><p id="ea97" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">鞍点是图上的x**点，这里既没有最大值也没有最小值</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lb"><img src="../Images/4c566f9465ef1f078d175e837cc74a49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZuxFG_He-5jzmL4cIvaX3w.jpeg"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">局部和全局极值</figcaption></figure><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lc"><img src="../Images/50f7d74a082d3d48d2dd947dc7610e06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgSGRxg1nIsKtepHHb9BxQ.jpeg"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">鞍点和拐点</figcaption></figure><h1 id="1ad8" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">函数的梯度</h1><p id="15ad" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">函数∇f(x的梯度)，x=(x1，x2，..xn)是由x的偏导数构成的向量，它与f(x)的切线正交。</p><h1 id="8ed4" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">约束和无约束优化</h1><h1 id="7a96" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">线性优化—受约束</h1><p id="5e0f" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">线性优化采取以下形式:</p><p id="5a23" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">最大化:w1x1 + w2x2</p><p id="86f7" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">受约束:x1 + x2 &lt;= Z, x1&gt; =0，x2&gt;=0</p><p id="cb5e" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">这里，最大化的目标函数以及约束都是线性的。约束可以是线性等式或不等式。例如，预算问题，根据制造业生产的产品数量最大化收入(最大化:收入，约束:产品数量)，根据成本降低最大化利润。</p><p id="5d35" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">线性优化问题或线性规划问题从1827年就存在了，然而乔治·B·丹齐格在1947年提出了一种称为单纯形法的分析方法来有效地解决线性规划问题。单纯形法甚至是现在最广泛使用的线性规划问题的解决方案。</p><h1 id="afd7" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">凸非线性优化-无约束</h1><p id="db75" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">凸非线性优化采取以下形式</p><p id="fa0c" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">最大化:f(X)，其中f(X)是凸非线性函数，X=(x1，x2，x3…..xn)在真实空间中。</p><p id="c5cc" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">凸函数的一个重要性质是它的局部极小值与全局极小值相同。在非线性优化中，与线性优化不同，没有解析方法来解决问题，因此我们依赖于数值方法，如梯度下降和牛顿法来解决凸非线性优化。</p><h1 id="1850" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">梯度下降法。</h1><p id="d8af" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">梯度下降法是一种基于梯度计算的迭代算法。它从一个初始化点开始，并根据函数的梯度到达一个最小值或最大值。这个最小值或最大值可以是局部的或全局的最小值或最大值。因为对于凸函数，局部和全局最小值或最大值是相同的，所以该算法在获得全局最小值或最大值方面对凸非线性函数工作良好。</p><p id="3405" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">第一步</strong>:初始化xi点</p><p id="33db" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">第二步</strong>:求函数∇f(x的梯度)——梯度给我们达到极值的方向。</p><p id="6049" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">对于最大化问题，选择∇f(x)，和</p><p id="cfa5" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">对于最小化问题，选择-∇f(x)</p><p id="a49c" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">第三步</strong>:找到合适的步长或学习率λi可以通过解方程得到</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/3c285f407a7203fac22f18d29d9e895f.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*JBNEJqQAJoiygs4_wreiGg.png"/></div></figure><p id="fff2" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">步骤4 </strong>:找到新的近似xi+1，如下</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es le"><img src="../Images/3d2430c3e8e31c6c4d4540a8a8f6e6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*b5v9EfogDExcy2d4tEjXXg.png"/></div></figure><p id="b82e" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">第五步:迭代直到我们得到最优。</p><p id="23c7" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">用途</strong>:线性回归的均方误差、逻辑回归的对数损失函数等回归损失函数可以用梯度法求解。在深度学习中使用梯度下降方法的变体，例如随机梯度下降和具有动量的随机梯度下降。</p><h1 id="d823" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">牛顿方法</h1><p id="8b95" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">牛顿法寻找函数的根而不是梯度，这种算法比梯度下降算法更快地收敛到最优最小值/最大值，然而它是内存密集型的并且也是计算密集型的，因此限制了它的使用。这种算法只能用于连续可微的函数。在拉夫逊描述了一般三次方程的解之后，这也被称为牛顿-拉夫逊法</p><p id="d609" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">以下是使用牛顿-拉夫森方法为连续可微函数f(x)找到最优解的步骤:</p><p id="2f01" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">第一步</strong>:初始化点xi</p><p id="567c" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">第二步</strong>:求f(x)的导数，其中f(x)的导数≠ 0</p><p id="5ba1" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">第三步</strong>:求出新值xi+1为</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lf"><img src="../Images/6746dd562c1686e80f81003cf7072345.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*tc14zXdSOVDMPZudo9XpBg.png"/></div></div></figure><p id="57ce" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">步骤4 </strong>:重复，直到xi+1≈ xi时达到最优解</p><h1 id="bc57" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">非凸非线性优化-无约束</h1><p id="3613" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">非凸优化采取这样的形式，</p><p id="db72" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">Min {f(x) = 1/n(∑fi(x)}，i= 1到n</p><p id="63a4" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">非凸函数可能有许多局部极值和鞍点，因此寻找最优解成为一项繁琐的任务。这类问题存在于深度神经网络和一些机器学习概念中，如K-means聚类。</p><p id="646a" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">为什么梯度下降在这里不起作用？</p><p id="5243" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">1.梯度下降算法有收敛于鞍点或任何其他局部最小值的趋势。或者需要大量迭代来收敛，这是计算密集型的。</p><p id="d2bf" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">2.当存在大量训练数据时，梯度下降也是不可行的。</p><p id="c5eb" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">随机梯度下降(SGD)试图最小化上述限制，因此在该领域获得了普及。</p><h1 id="656e" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">随机梯度法</h1><p id="0b9c" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">随机梯度下降法是梯度下降算法的简化。与梯度下降算法的唯一不同之处在于，在每次迭代中，SGD随机选取“n”个替换样本进行计算，并用作f(x)梯度的无偏估计量，而不是计算f(x)的梯度。</p><p id="a93c" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">使用随机采样以较少的迭代次数降低了收敛于鞍点的风险。</p><p id="2343" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">SGD及其修改广泛用于深度学习和几种机器学习算法，如SVM和k-Means。</p></div></div>    
</body>
</html>