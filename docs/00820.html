<html>
<head>
<title>Understanding loss functions : Hinge loss</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解损失函数:铰链损失</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-loss-functions-hinge-loss-a0ff112b40a1?source=collection_archive---------0-----------------------#2019-09-11">https://medium.com/analytics-vidhya/understanding-loss-functions-hinge-loss-a0ff112b40a1?source=collection_archive---------0-----------------------#2019-09-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/76d603dad7c1b5916385ad3a142dcc15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wi3evUWuxDodLD6CmMFUNw.jpeg"/></div></div></figure><div class=""/><p id="cfe3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在机器学习中，我们经常会遇到损失函数。对于像我这样没有计算机科学背景的人来说，探索损失函数背后的数学概念并在我的模型中实现是很困难的。因此，在这里，我将尝试用最简单的术语解释什么是损失函数，以及它如何帮助优化我们的模型。我将只考虑分类示例，因为它更容易理解，但是这些概念可以应用于所有技术。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="6b75" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们需要理解任何分类模型的基本目标是正确地分类尽可能多的点。尽管有时会发生错误分类(考虑到我们没有过度拟合模型，这是好事)。现在，我们需要测量我们错误分类了多少点。这在两个方面帮助了我们。</p><ol class=""><li id="a52c" class="jv jw ht is b it iu ix iy jb jx jf jy jj jz jn ka kb kc kd bi translated">预测模型的准确性</li><li id="73a5" class="jv jw ht is b it ke ix kf jb kg jf kh jj ki jn ka kb kc kd bi translated">优化成本函数，以便我们从正确分类的点中获得比错误分类的点更多的价值</li></ol><p id="5b5c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，用最简单的术语来说，损失函数可以表示如下。</p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kj"><img src="../Images/725481957a4a25509b81802a2dc13a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8QcUE0_u3U7EBBBQV3hUNg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图1:错误分类点的分数</figcaption></figure><p id="8c40" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，数学上很难优化上述问题。我们需要一些具体的数学方程式来理解这个分数。</p><p id="2f60" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们直观地理解一个决策边界。</p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ks"><img src="../Images/46005f52ec3b498976bdb99b6ba01914.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N0FCu9Rfm4gwalsdAyK37g.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图2:区分正负点的决策边界</figcaption></figure><p id="c640" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">左侧的点被正确分类为正，而右侧的点被分类为负。错误分类的点用红色<strong class="is hu">标记</strong>。</p><p id="592e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们可以尝试将所有错误分类的点放在决策边界的一边。姑且称此为'<strong class="is hu"> <em class="kt">犹太区</em> </strong>'。</p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ku"><img src="../Images/68ca2d9dbbbda146d4d19055520f795c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cBSvOXkfEIdPdEvu49EvDw.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图3:错误分类的点对于yf(x)总是有一个负号</figcaption></figure><p id="f48a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从我们的基本线性代数中，我们知道如果(<strong class="is hu"> 𝑦,𝑦̂ </strong>)的符号不匹配，<strong class="is hu"> yf(x) </strong>将始终为&gt; 0，其中“<strong class="is hu"> 𝑦 </strong>将表示我们的模型的输出，而“<strong class="is hu"> 𝑦̂ </strong>将表示实际的类标签。</p><p id="4b0d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，如果我们绘制<strong class="is hu"> yf(x) </strong>对损失函数，我们得到下图。</p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kv"><img src="../Images/a999128872516914ff1aa4c8c9384ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v_EnxplC8Cejwc4bIcY5Fg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图4:YF(x)与各种算法的损失函数的曲线图</figcaption></figure><p id="0f01" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们考虑图3中的错误分类图。我们可以看到，对于<strong class="is hu"> yf(x) &gt; 0 </strong>，我们正在指定“0”损失。这些点已被正确分类，因此我们不想对总分数做出更多贡献(参见图1)。然而，对于那些yf(x) &lt; 0 的点，我们指定损失为‘1 ’,也就是说这些点要为被错误分类付出更多的代价，就像下面这样。</p><figure class="kk kl km kn fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kw"><img src="../Images/e106348260985cc78925a764fdca37f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*akYEBiRUZdJ-BcK-clyWBg.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">图5:损失函数直觉</figcaption></figure><p id="0308" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我希望，现在损失函数背后的直觉，以及它如何影响一个模型的总体数学成本，已经很清楚了。</p><p id="f3a8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">几乎所有的分类模型都是基于某种模型的。例如逻辑回归有逻辑损失(图4:指数)，SVM有铰链损失(图4:支持向量)等。</p><h1 id="b694" class="kx ky ht bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><span class="l lv lw lx bm ly lz ma mb mc di"> H </span> <strong class="ak">英格损失支持向量机</strong></h1><p id="966e" class="pw-post-body-paragraph iq ir ht is b it md iv iw ix me iz ja jb mf jd je jf mg jh ji jj mh jl jm jn hb bi translated">从我们的SVM模型中，我们知道铰链损耗= [ <strong class="is hu"> 0，1- yf(x) </strong>。</p><p id="5af3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">查看图4中SVM的曲线图，我们可以看到，对于<strong class="is hu"> yf(x) ≥ 1 </strong>，铰链损耗为“<strong class="is hu"> 0 </strong>”。然而，当<strong class="is hu"> yf(x) &lt; 1 </strong>时，铰链损耗会大幅增加。由于<strong class="is hu"> yf(x) </strong>随着每个错误分类的点(图5中非常错误的点)而增加，铰链损耗的上界{ <strong class="is hu"> 1- yf(x) </strong> }也呈指数增加。</p><p id="fd09" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，远离决策余量的点具有更大的损失值，从而对这些点不利。</p><p id="6861" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="kt">结论</em>:这只是对什么是损失函数，铰链损失如何工作的一个基本理解。我将很快发表对“铰链损耗”有更深理解的其他文章。</p><p id="57f5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="kt">参考文献</em>:</p><p id="b103" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">机器学习原理:<a class="ae mi" href="https://www.youtube.com/watch?v=r-vYJqcFxBI" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=r-vYJqcFxBI</a></p><p id="79e2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">普林斯顿大学:最优化与凸性讲座:<a class="ae mi" href="https://www.cs.princeton.edu/courses/archive/fall16/cos402/lectures/402-lec5.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . Princeton . edu/courses/archive/fall 16/cos 402/lectures/402-le C5 . pdf</a></p></div></div>    
</body>
</html>