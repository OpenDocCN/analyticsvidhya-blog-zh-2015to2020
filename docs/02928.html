<html>
<head>
<title>Part 1: Sentiment Analysis in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第1部分:PyTorch中的情感分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8?source=collection_archive---------5-----------------------#2020-01-08">https://medium.com/analytics-vidhya/part-1-sentiment-analysis-in-pytorch-82b35edb40b8?source=collection_archive---------5-----------------------#2020-01-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ced163294a9baeacee00a17ff0937ecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nnaPkBrBvkcg2SBt"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae iu" href="https://unsplash.com/@tengyart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Tengy Art </a>拍摄的照片</figcaption></figure><p id="9094" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上一篇<a class="ae iu" rel="noopener" href="/analytics-vidhya/numpy-vs-pytorch-linear-regression-from-scratch-452a121fb0e8">文章</a>中，我们探讨了如何创建基本的神经网络。现在我们将探索更复杂的神经网络。</p><p id="0650" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将了解如何创建用于训练的批处理，以及如何在PyTorch中创建深度神经网络。</p><p id="66c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">dataset:<a class="ae iu" href="https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/nica potato/women-ecommerce-clothing-reviews</a></p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="6cb8" class="kc kd hi jy b fi ke kf l kg kh">dataset.head()</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es ki"><img src="../Images/2048f7dbe80901214ae40ef3e713f0b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*aNGEBq1VT8cdcLLwwvSjNQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">资料组</figcaption></figure><p id="855c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有多种方法可以将文本转换成数字/向量，我们将坚持基础知识，稍后再探索单词嵌入。因为这个练习的目的是检查PyTorch的深度学习模型构建。这里是<a class="ae iu" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" rel="noopener ugc nofollow" target="_blank">链接</a>到基本的单词嵌入技术。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="922a" class="kc kd hi jy b fi ke kf l kg kh">from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split</span><span id="833f" class="kc kd hi jy b fi kj kf l kg kh"># Shuffle the data and then split it, keeping 20% aside for testing</span><span id="6d45" class="kc kd hi jy b fi kj kf l kg kh">X_train, X_test, y_train, y_test = train_test_split(dataset['review'], dataset['recommended'], test_size=0.2)</span><span id="3029" class="kc kd hi jy b fi kj kf l kg kh">vectorizer = CountVectorizer(lowercase=True)<br/>vectorizer.fit(dataset['review'])</span><span id="a283" class="kc kd hi jy b fi kj kf l kg kh">X_train_vec = vectorizer.transform(X_train)<br/>X_test_vec = vectorizer.transform(X_test)</span><span id="3d4c" class="kc kd hi jy b fi kj kf l kg kh">print(X_train_vec.shape)<br/>print(X_test_vec.shape)</span><span id="86b8" class="kc kd hi jy b fi kj kf l kg kh">'''Output<br/>(18112, 14145)<br/> (4529, 14145)'''</span></pre><p id="7b9e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们现在知道词汇量是14145。现在让我们把数据集转换成张量。CountVectorizer给出了稀疏矩阵，首先我们要把它转换成稠密矩阵。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="095c" class="kc kd hi jy b fi ke kf l kg kh">X_train_tensor = torch.from_numpy(X_train_vec.todense()).float()<br/>X_test_tensor = torch.from_numpy(X_test_vec.todense()).float()<br/>Y_train_tensor = torch.from_numpy(np.array(y_train))<br/>Y_test_tensor = torch.from_numpy(np.array(y_test))</span></pre><p id="4903" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们建立基本网络，并尝试在没有训练模型的情况下进行预测</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="221e" class="kc kd hi jy b fi ke kf l kg kh">class Network(torch.nn.Module):</span><span id="7612" class="kc kd hi jy b fi kj kf l kg kh">     def __init__(self,vocab_size,out_classes):<br/>        super().__init__()<br/>        self.linear = torch.nn.Linear(vocab_size,out_classes)</span><span id="3dc3" class="kc kd hi jy b fi kj kf l kg kh">    def forward(self,x):<br/>        return self.linear(x)</span><span id="be51" class="kc kd hi jy b fi kj kf l kg kh">#Declare dimensions<br/>VOCAB_SIZE = 14145<br/>OUT_CLASSES = 1</span><span id="ad4b" class="kc kd hi jy b fi kj kf l kg kh">#Initialize model<br/>model = Network(VOCAB_SIZE,OUT_CLASSES)</span><span id="e47b" class="kc kd hi jy b fi kj kf l kg kh">#Prediction without training<br/>pred = model(X_train_tensor[1])<br/>print(pred)</span><span id="a48e" class="kc kd hi jy b fi kj kf l kg kh">'''output<br/>  tensor([0.0790], grad_fn=&lt;AddBackward0&gt;)'''</span></pre><p id="d59b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们开始训练，但在训练之前，我们必须创建一批训练数据。我们将使用DataLoader类进行批处理，但它需要TensorDataset</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="ea62" class="kc kd hi jy b fi ke kf l kg kh">from torch.utils.data import Dataset, TensorDataset</span><span id="555d" class="kc kd hi jy b fi kj kf l kg kh">train_data = TensorDataset(X_train_tensor, Y_train_tensor)</span><span id="e8b9" class="kc kd hi jy b fi kj kf l kg kh">'''TensorDataset creates list of tuples with each record containing feature_tuple and lable_tuple.'''</span><span id="ee1c" class="kc kd hi jy b fi kj kf l kg kh">print(train_data[0])</span><span id="33a3" class="kc kd hi jy b fi kj kf l kg kh">'''Output<br/>  (tensor([0., 0., 0.,  ..., 0., 0., 0.]), tensor(1))<br/>'''</span></pre><p id="9883" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们有了Tensordataset，让我们创建批处理</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="3daa" class="kc kd hi jy b fi ke kf l kg kh">from torch.utils.data import DataLoader</span><span id="164d" class="kc kd hi jy b fi kj kf l kg kh">train_loader = DataLoader(train_data,batch_size=16, shuffle=True)</span></pre><p id="5f1a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此对象/train_loader是可迭代的</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a328" class="kc kd hi jy b fi ke kf l kg kh">print(next(iter(train_loader)))</span><span id="9440" class="kc kd hi jy b fi kj kf l kg kh">'''output'''<br/>[tensor([[0., 0., 0.,  ..., 0., 0., 0.],          [0., 0., 0.,  ..., 0., 0., 0.],          [0., 0., 0.,  ..., 0., 0., 0.],          ...,          [0., 0., 0.,  ..., 0., 0., 0.],          [0., 0., 0.,  ..., 0., 0., 0.],          [0., 0., 0.,  ..., 0., 0., 0.]]),  tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1])]</span></pre><p id="87fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们开始训练。'</p><p id="7fb0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但在此之前，我们必须更新我们的网络，因为早些时候我们已经创建了基本的班轮网络。但这里我们需要网络可以输出0或1的值，所以让我们添加sigmoid层和几个隐藏节点</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="95d0" class="kc kd hi jy b fi ke kf l kg kh">class Network(torch.nn.Module):</span><span id="bca3" class="kc kd hi jy b fi kj kf l kg kh">    def __init__(self,vocab_size,hidden_units,num_classes): <br/>      super().__init__()<br/>      #First fully connected layer<br/>      self.fc1 = torch.nn.Linear(vocab_size,hidden_units)<br/>      #Second fully connected layer<br/>      self.fc2 = torch.nn.Linear(hidden_units,num_classes)<br/>      #Final output of sigmoid function      <br/>      self.output = torch.nn.Sigmoid()</span><span id="92d2" class="kc kd hi jy b fi kj kf l kg kh">def forward(self,x):<br/>      fc1 = self.fc1(x)<br/>      fc2 = self.fc2(fc1)<br/>      output = self.output(fc2)<br/>      return output[:, -1]</span></pre><p id="93f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们开始训练。有关培训步骤的详细信息，请查看<a class="ae iu" rel="noopener" href="/analytics-vidhya/numpy-vs-pytorch-linear-regression-from-scratch-452a121fb0e8">文章</a>。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="cb07" class="kc kd hi jy b fi ke kf l kg kh">NUM_EPOCHS = 5<br/>VOCAB_SIZE = 14145<br/>HIDDEN_UNITS = 3<br/>OUT_CLASSES = 1<br/>LEARNING_RATE = 0.001</span><span id="e6b7" class="kc kd hi jy b fi kj kf l kg kh">#Initialize model<br/>model = Network(VOCAB_SIZE,HIDDEN_UNITS,OUT_CLASSES)<br/>print(model)</span><span id="82f0" class="kc kd hi jy b fi kj kf l kg kh">#Initialize optimizer<br/>optimizer =torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)</span><span id="afe6" class="kc kd hi jy b fi kj kf l kg kh">#Initialize loss function<br/>loss_fun = torch.nn.BCELoss()</span><span id="4930" class="kc kd hi jy b fi kj kf l kg kh">for i in range(NUM_EPOCHS):</span><span id="4fe6" class="kc kd hi jy b fi kj kf l kg kh">   for x_batch,y_batch in train_loader:</span><span id="4a0f" class="kc kd hi jy b fi kj kf l kg kh">       model.train()<br/>       y_pred = model(x_batch)<br/>       loss = loss_fun(y_pred,y_batch.float())<br/>       loss.backward()<br/>       optimizer.step()<br/>       optimizer.zero_grad()<br/></span><span id="7b71" class="kc kd hi jy b fi kj kf l kg kh"> print('After {} epoch training loss is {}'.format(i,loss.item()))</span><span id="c915" class="kc kd hi jy b fi kj kf l kg kh">'''Output</span><span id="acd0" class="kc kd hi jy b fi kj kf l kg kh">Network(   (fc1): Linear(in_features=14145, out_features=3, bias=True)   (fc2): Linear(in_features=3, out_features=1, bias=True)   (output): Sigmoid() ) </span><span id="70ce" class="kc kd hi jy b fi kj kf l kg kh">After 0 epoch training loss is 0.4326445162296295<br/>After 1 epoch training loss is 0.40662047266960144<br/>After 2 epoch training loss is 0.3978360891342163<br/>After 3 epoch training loss is 0.5115123987197876<br/>After 4 epoch training loss is 0.2605650722980499 '''</span></pre><p id="0819" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">总结:</strong></p><p id="5fff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">完整的笔记本可在<a class="ae iu" href="https://github.com/sarang0909/Explore-PyTorch/blob/master/Part1_Pytorch_Sentiment_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> git-repo </a>获得。</p><p id="58ab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们已经了解了如何创建批处理以及如何训练神经网络。但是我们还没有看到很多与深度学习模型训练相关的重要东西。在以后的文章中，我们会看到更多关于优化器、损失函数、超参数调整等的细节。</p><p id="f86d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下一篇文章中，我们将通过拥抱脸来探索变形金刚。</p><p id="9bc0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你喜欢这篇文章或有任何建议/意见，请在下面分享！</p><p id="8352" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.linkedin.com/in/sarang-mete-6797065a/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p></div></div>    
</body>
</html>