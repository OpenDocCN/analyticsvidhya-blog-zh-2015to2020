<html>
<head>
<title>PyTorch For Deep Learning — Binary Classification ( Logistic Regression )</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于深度学习的 PyTorch 二元分类(逻辑回归)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pytorch-for-deep-learning-binary-classification-logistic-regression-382abd97fb43?source=collection_archive---------0-----------------------#2020-09-13">https://medium.com/analytics-vidhya/pytorch-for-deep-learning-binary-classification-logistic-regression-382abd97fb43?source=collection_archive---------0-----------------------#2020-09-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/39d442eac4afef410e1e34e0143f1df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KdDcAi4Klsi3u7ZlSSNySg.jpeg"/></div></div></figure><p id="aa53" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这篇博文是关于如何用 PyTorch 创建一个分类神经网络。<br/> <em class="jo">注:本帖中的神经网络包含 2 层，神经元很多。但是，如果输出特征的数量和层数减少到 1，这将只是一个普通的逻辑回归</em></p><p id="2054" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">说到这里，让我们进入代码</p><ol class=""><li id="1438" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn ju jv jw jx bi translated"><strong class="is hj">导入库</strong></li></ol><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="4cca" class="kh ki hi kd b fi kj kk l kl km">#importing the libraries</span><span id="e766" class="kh ki hi kd b fi kn kk l kl km">import torch<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span></pre><p id="43f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 2。数据集</strong></p><p id="5dfb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇文章中，我们将使用 sklearn 著名的乳腺癌数据集</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="1acd" class="kh ki hi kd b fi kj kk l kl km">#importing the dataset</span><span id="2dba" class="kh ki hi kd b fi kn kk l kl km">from sklearn.datasets import load_breast_cancer<br/>data = load_breast_cancer()<br/>x = data['data']<br/>y = data['target']<br/>print("shape of x: {}\nshape of y: {}".format(x.shape,y.shape))</span><span id="a90f" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">output:</strong><br/>shape of x: (569, 30) <br/>shape of y: (569,)</span></pre><p id="2ee2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3。特征缩放</strong></p><p id="e8f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在将特征发送到神经网络之前，将特征缩放到标准法线是很重要的。</p><p id="6a8a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基本上，这将减去列的平均值，并除以列中每个值(自变量)的标准偏差</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="bc6b" class="kh ki hi kd b fi kj kk l kl km">#feature scaling</span><span id="6ff8" class="kh ki hi kd b fi kn kk l kl km">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>x = sc.fit_transform(x)</span></pre><p id="c311" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 4。数据集和数据加载器</strong></p><p id="fe5e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">pytorch 中的 Dataset 类基本上覆盖了一个元组中的数据，并使我们能够访问每个数据的索引。这对于创建可用于混洗、应用小批量梯度下降等的 dataloader 类是必要的。</p><p id="7423" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">查看之前的帖子，了解更多关于这种工作方式的例子</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="1b37" class="kh ki hi kd b fi kj kk l kl km">#defining dataset class</span><span id="39c5" class="kh ki hi kd b fi kn kk l kl km">from torch.utils.data import Dataset, DataLoader</span><span id="9f3d" class="kh ki hi kd b fi kn kk l kl km">class dataset(Dataset):<br/>  def __init__(self,x,y):<br/>    self.x = torch.tensor(x,dtype=torch.float32)<br/>    self.y = torch.tensor(y,dtype=torch.float32)<br/>    self.length = self.x.shape[0]<br/> <br/>  def __getitem__(self,idx):<br/>    return self.x[idx],self.y[idx]</span><span id="24d3" class="kh ki hi kd b fi kn kk l kl km">  def __len__(self):<br/>    return self.length</span><span id="2328" class="kh ki hi kd b fi kn kk l kl km">trainset = dataset(x,y)</span><span id="cf61" class="kh ki hi kd b fi kn kk l kl km">#DataLoader<br/>trainloader = DataLoader(trainset,batch_size=64,shuffle=False)</span></pre><p id="2310" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 5。用于分类的神经网络</strong></p><p id="b8c0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在 Pytorch 中，神经网络是使用面向对象编程创建的。这些层是在 init 函数中定义的，向前传递是在 forward 函数中定义的，当调用该类时会自动调用该函数。</p><p id="2d57" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于 nn 类，这些函数是可能的。从 torch 继承的模块。</p><p id="0136" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">神经网络的输出在 0 和 1 之间，因为 sigmoid 函数被应用于输出，这使得网络适合于二进制分类。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="084e" class="kh ki hi kd b fi kj kk l kl km">#defining the network</span><span id="850b" class="kh ki hi kd b fi kn kk l kl km">from torch import nn<br/>from torch.nn import functional as F</span><span id="5329" class="kh ki hi kd b fi kn kk l kl km">class Net(nn.Module):<br/>  def __init__(self,input_shape):<br/>    super(Net,self).__init__()<br/>    self.fc1 = nn.Linear(input_shape,32)<br/>    self.fc2 = nn.Linear(32,64)<br/>    self.fc3 = nn.Linear(64,1)</span><span id="1e32" class="kh ki hi kd b fi kn kk l kl km">  def forward(self,x):<br/>    x = torch.relu(self.fc1(x))<br/>    x = torch.relu(self.fc2(x))<br/>    x = torch.sigmoid(self.fc3(x))<br/>    return x</span></pre><p id="68a9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">如果简单的逻辑回归就足够了，fc2 和 fc3 层可以去掉。</em></p><p id="cfef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 6。更多参数</strong></p><p id="d507" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">选择各种参数，如时期数、损失函数、学习率等</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="7766" class="kh ki hi kd b fi kj kk l kl km">#hyper parameters</span><span id="cdfb" class="kh ki hi kd b fi kn kk l kl km">learning_rate = 0.01<br/>epochs = 700</span><span id="17f9" class="kh ki hi kd b fi kn kk l kl km"># Model , Optimizer, Loss</span><span id="d86e" class="kh ki hi kd b fi kn kk l kl km">model = Net(input_shape=x.shape[1])<br/>optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)<br/>loss_fn = nn.BCELoss()</span></pre><p id="5901" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">BCELoss 是用于二进制交叉熵损失的 pytorch 类，它是用于二进制分类的标准损失函数。</p><figure class="jy jz ka kb fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/636094ba25f11a3f29eb0e68f8a17f68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4eap2fiZ_HSbzesyUoocvw.png"/></div></div></figure><h1 id="077b" class="kp ki hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">培养</h1><p id="70a6" class="pw-post-body-paragraph iq ir hi is b it lm iv iw ix ln iz ja jb lo jd je jf lp jh ji jj lq jl jm jn hb bi translated">从损失函数中找到的梯度被用于改变权重值，并且该过程被重复几次。</p><p id="3336" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这样做是为了最小化损失函数并提高精度</p><p id="4e58" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，数据集没有划分为训练集和测试集，因为数据量已经很低</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="fe53" class="kh ki hi kd b fi kj kk l kl km">#forward loop</span><span id="43fe" class="kh ki hi kd b fi kn kk l kl km">losses = []<br/>accur = []</span><span id="5bff" class="kh ki hi kd b fi kn kk l kl km">for i in range(epochs):<br/>  for j,(x_train,y_train) in enumerate(trainloader):<br/>    <br/>    #calculate output<br/>    output = model(x_train)<br/> <br/>    #calculate loss<br/>    loss = loss_fn(output,y_train.reshape(-1,1))<br/> <br/>    #accuracy<br/>    predicted = model(torch.tensor(x,dtype=torch.float32))<br/>    acc = (predicted.reshape(-1).detach().numpy().round() == y).mean()</span><span id="b07c" class="kh ki hi kd b fi kn kk l kl km">    #backprop<br/>    optimizer.zero_grad()<br/>    loss.backward()<br/>    optimizer.step()</span><span id="9a40" class="kh ki hi kd b fi kn kk l kl km"><br/>  if i%50 == 0:<br/>    losses.append(loss)<br/>    accur.append(acc)<br/>    print("epoch {}\tloss : {}\t accuracy : {}".format(i,loss,acc))</span><span id="f97c" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">output:<br/></strong><em class="jo">epoch 0 loss : 0.6731628775596619  accuracy : 0.6274165202108963 epoch 50 loss : 0.2154722362756729  accuracy : 0.9507908611599297 epoch 100 loss : 0.1049698144197464  accuracy : 0.9718804920913884 epoch 150 loss : 0.08159173280000687  accuracy : 0.9824253075571178 epoch 200 loss : 0.06935682147741318  accuracy : 0.984182776801406 epoch 250 loss : 0.0607854388654232  accuracy : 0.984182776801406 epoch 300 loss : 0.053541962057352066  accuracy : 0.9859402460456942 epoch 350 loss : 0.047878947108983994  accuracy : 0.9876977152899824 epoch 400 loss : 0.043598778545856476  accuracy : 0.9894551845342706 epoch 450 loss : 0.039949361234903336  accuracy : 0.9912126537785588 epoch 500 loss : 0.036903925240039825  accuracy : 0.9929701230228472 epoch 550 loss : 0.03445163369178772  accuracy : 0.9929701230228472 epoch 600 loss : 0.032331496477127075  accuracy : 0.9929701230228472 epoch 650 loss : 0.030418962240219116  accuracy : 0.9929701230228472</em></span></pre><p id="cd9f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">detach()函数从张量中删除 requires_grad，以便可以将其转换为 numpy，accuracy 是一个存储每个历元精度的列表。除此之外，如果所有以前的帖子都已阅读，这里的一切都是不言自明的。</p><p id="7e30" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">模型分析</strong></p><p id="34d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">绘制各个时期的损失和准确性，看它如何随着训练而变化</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="a291" class="kh ki hi kd b fi kj kk l kl km">#plotting the loss</span><span id="f91f" class="kh ki hi kd b fi kn kk l kl km">plt.plot(losses)<br/>plt.title('Loss vs Epochs')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('loss')</span></pre><figure class="jy jz ka kb fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/4f82af90a96c0fe0e64259928517bc89.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*-IqPJCgNT3IZ_8FHuCG4XQ.png"/></div></figure><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="c11b" class="kh ki hi kd b fi kj kk l kl km">#printing the accuracy</span><span id="8754" class="kh ki hi kd b fi kn kk l kl km">plt.plot(accur)<br/>plt.title('Accuracy vs Epochs')<br/>plt.xlabel('Accuracy')<br/>plt.ylabel('loss')</span></pre><figure class="jy jz ka kb fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/7b241a98283108683fa6b09d52e16bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*aZqg4J3_0RNnI6WwCsRsnQ.png"/></div></figure><p id="cef0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个损失和准确度图证明了我们的模型学习得很好。</p><h1 id="8990" class="kp ki hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">谢谢你</h1></div></div>    
</body>
</html>