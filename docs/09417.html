<html>
<head>
<title>Logistic Regression — A Geometric Perspective</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归——几何视角</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-a-geometric-perspective-c2997fdb54cc?source=collection_archive---------15-----------------------#2020-09-05">https://medium.com/analytics-vidhya/logistic-regression-a-geometric-perspective-c2997fdb54cc?source=collection_archive---------15-----------------------#2020-09-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b85089be3305d706cf442f83f57890ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O_mAIbssRsxWTBxZY9a0Aw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Jossuha Théophile 在<a class="ae iu" href="https://unsplash.com/s/photos/math?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="3052" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di"> L </span>逻辑回归是最流行的监督机器学习技术之一，广泛用于解决分类问题。在这篇博文中，我们将逐步理解逻辑回归，我们也将到达逻辑回归内部解决的最优化问题。</p><p id="10d8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">尽管“逻辑回归”的名字中有<strong class="ix hj">回归</strong>，但它实际上解决了分类问题。逻辑回归只能解决两类分类问题，但我们也可以在<strong class="ix hj">一对一</strong>设置中使用它来解决多类分类问题。</p><p id="fccd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有多种方法可以得出逻辑回归的优化方程，我们可以遵循概率方法、损失最小化方法或几何方法。在本讨论中，我们将遵循<strong class="ix hj">几何方法</strong>得出优化方程。</p><p id="b4c9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是在我们开始推导逻辑回归之前，我们先来谈谈直线的方程。</p><h2 id="4e2e" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated">一条线的方程是什么？</h2><p id="fa55" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">众所周知，一条线的方程是，</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/98c4c69f5b65b35daf6e95a40aeb07b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4qa_gzMRpNzC94CDOFp0VA.png"/></div></div></figure><p id="0dd0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是，还有另一种形式可以表达直线方程，称为<strong class="ix hj">一般形式</strong>。直线方程的一般形式是，</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/1fe632e72a32268c2d3e0d3a627ccb1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KqCLAbz27LJ6nApdnanG3w.png"/></div></div></figure><p id="803e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是这两个方程是如何联系在一起的呢？让我们把这两个等式联系起来。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/ebbd82f4421b9f562cfd895f8ebd23ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFFVIYI13kdUyqldJ1DLxw.png"/></div></div></figure><p id="7d85" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">就是这样！很简单，对吧？</p><p id="d1f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了推导逻辑回归，我们将使用直线方程的一般形式。</p><h2 id="d091" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated">逻辑回归是如何工作的？</h2><p id="b01d" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">假设:逻辑回归的大假设是<em class="lj">我们的数据点在它们所在的空间(<em class="lj"> 2D，3D或nD </em>)中是线性可分的</em>。</p><p id="9324" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">步骤1 </strong>:由于逻辑回归只能解决两个分类任务，所以我们将得到两个唯一的输出或目标变量(<em class="lj"> Y </em>)。在逻辑回归的几何公式中，我们将一个目标变量标记为<em class="lj"> -1 </em>，将另一个标记为<em class="lj"> +1。</em></p><p id="0812" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设，<em class="lj"> D是我们的数据集，我们必须解决两类分类问题。</em></p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/5dbfd7195a052e8a9f9f610b60213893.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*e38CEN-dXbunGuq5sLZUiw.png"/></div></figure><p id="0cde" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归背后的核心思想:<strong class="ix hj"> </strong>在逻辑回归中，我们试图找出(在<em class="lj"> 2D </em>中的)直线或(在<em class="lj"> 3D </em>中的)平面或(当维数大于<em class="lj"> 3 </em>时的)超平面，它们最好地分离了我们所有的输入变量(<em class="lj"> X </em>)。</p><p id="b24e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，对于高维空间，逻辑回归的决策边界是线(在2D)、平面(在3D)和超平面。</p><p id="6c1c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">视觉上，</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/d30dc108e38c6f511ecd6d4cd0cbdef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zl2bdcH_Cy1ddlAPqjRK5A.png"/></div></div></figure><blockquote class="lm ln lo"><p id="ee97" class="iv iw lj ix b iy iz ja jb jc jd je jf lp jh ji jj lq jl jm jn lr jp jq jr js hb bi translated">为了得到最好地分离我们需要的所有数据点的线，<em class="hi"> w和w_0，</em>为了降低数学复杂性，我们将丢弃w_0的截距项，并且对于这个讨论，我们将把自己限制在仅仅二维空间中，但是这个想法也可以容易地扩展到多维设置。</p></blockquote><p id="58fa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">第二步</strong>:假设，<em class="lj"> w </em>是垂直于最佳分隔我们所有输入变量(<em class="lj"> X)的直线的单位向量。</em>某一特定点(假设x_i)到直线的距离为，</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/dee9af7cb37cb29712cd9dd3b5e4daf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4BC0O59tTtFvW1AQfFyaZw.png"/></div></div></figure><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/829fd15a959cf7e3d1283fad76adb246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i21MCqWLy7FFge1Gr_lvFQ.png"/></div></div></figure><p id="9f16" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们所讨论的，我们的输出变量有两个标签，+1代表一组数据点，而-1代表另一组数据点。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/0cdcd0f48956ad26a20a83cf9206d78d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7q-dn-evZv2bI2PILBfKEA.jpeg"/></div></div></figure><p id="a621" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">第三步</strong>:现在我们将x_i到对应类标号(Y_i)的直线的距离相乘，也称为x_i的有符号距离。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/f9118b83097060069520b05d672db0ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jtaZ4WMRrcGoHz8kr-ipoQ.png"/></div></div></figure><p id="cf77" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="lj">注意:当且仅当x_i </em>的符号距离为<em class="lj"> +ve且w </em>时，x _ I才会被正确分类。我们希望所有的<em class="lj"> x的</em>都被正确分类。</strong></p><p id="64f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，将最大化符号距离的<em class="lj"> w单位向量</em>将与垂直于最佳分离我们所有数据点的平面的<em class="lj"> w </em>相同。我们希望找到最佳的<em class="lj"> w </em>，它最大化我们数据集中所有<em class="lj"> x_i </em>的符号距离。所以，最优的<em class="lj"> w </em>可以写成:</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/cf9d5ae2443ea9b363bb15947d6607b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qdC6dKmOm1a4RSk9oxfqiw.png"/></div></div></figure><p id="5bd3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是在现实世界中，我们的数据集包含大量异常值，上面的寻找最优值的公式不容易出现异常值。一个简单的异常值可以扭曲整个设置。</p><p id="54ef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">第四步</strong>:为了使其易于离群，研究人员引入了一个名为<strong class="ix hj"> <em class="lj">的函数，Sigmoid function </em> </strong>将弱模型转化为一个健壮的易于离群的模型。实际上，我们采用的不是原始的带符号距离，而是Sigmoid函数。但是这个sigmoid函数是什么呢？</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/1885da14fab24984b13af4ad480ba775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yNaiE2b63wZ0fFcKsi8tbg.png"/></div></div></figure><p id="eb38" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">视觉上，</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/ea9581f6c13d9a8d44a2b253fa067dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*0G-0uMOUeYE9YSQEG89W7w.gif"/></div></div></figure><p id="d0b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lj"> sigmoid函数</em>在0附近几乎是线性的，但随着x变大或变小，它会逐渐消失。在取我们的符号距离的sigmoid之后，优化方程变成，</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/dbd5bf978ca979599da9871a35dfd28c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CfVhhe419vY4nNqzDTMUJg.png"/></div></div></figure><p id="b131" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">步骤5 </strong>:现在，我们想使我们的函数单调，所以我们只取它的对数。众所周知，对数是一个单调函数。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/3d0765fd48fb16da89799d23648dd786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pvXFnVt7nQXwOAIbu23ZjQ.png"/></div></div></figure><p id="6017" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">步骤6 </strong>:现在，我们的目标是将最大化问题转化为最小化问题。这很简单，我们只要接受它的负面影响，我们就完成了。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/22199b8337126df1171d50e19521e7f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQ9dzMvlTUy9i8a1gujTiA.png"/></div></div></figure><p id="8092" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以上是逻辑回归内部求解的优化方程。让我们更清楚地了解上面的等式。等式说，</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/c0421edca43e42c46b66e6a831a4835a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H1zHhGm_uCoB7d4wK6HzVw.png"/></div></div></figure><h2 id="9879" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated">引入正规化:</h2><p id="cd9d" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">真实世界的数据集包含大量的离群值。如果我们使模型过度适应训练数据，那么会增加模型的泛化误差，这是我们不希望的！但是一个模型什么时候会过度拟合呢？只有当损失变为0或几乎为0时。</p><p id="eb46" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们来看看我们已经推导出的损失函数。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/3aa758cb9f1c908d07f8cc683a14cc46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AbI1PvFdD-YXQEQJaiMwJQ.png"/></div></div></figure><p id="bc28" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们知道，log(1) = 0。因此，当且仅当exp(-z_i)趋于0时，loss将变为0或接近0，这意味着我们剩下log(1)为0。但是什么时候项变成0呢？我们来画一下。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/cade00461454b9a74844614080a29258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvbklxe-qqpIkDiYLOY7hg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">x轴代表z_i，y轴代表exp(-z_i)。</figcaption></figure><p id="8822" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">可以看到，只有当z_i趋于无穷大(或者大于5)时，exp(-z_i)才变成0，我们不要忘记z_i只有一个变量<em class="lj"> w. </em></p><p id="1153" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作为一个正则表达式，我们将使用λ乘以L2范数<em class="lj"> w，</em>，其中λ是一个超参数。随着exp(-z_i)的减少，正则项将自动增加，这将避免训练数据中的过度拟合。我们通过超参数调谐找到lamda，在正确的lamda处我们将有很好的拟合！</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/48c772584017e9026b539c9e994e2010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lRIPr-Ol1DHLeJWq1da0-Q.png"/></div></div></figure><p id="c4fd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有其他可用的正则化技术。命名，L1正则，创造sparcity，弹性网正则等。</p><h2 id="361c" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated"><strong class="ak">但是我们如何解这个复杂的方程并得到最优的w呢？</strong></h2><p id="22cf" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">没什么特别的。我们使用梯度下降或梯度下降的任何其他变体(如小批量SGD、SGD等)来解决它，其中我们多次更新w以最小化损失，最终我们得到最小化损失的最佳w。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/a468e616762ff0bc55d256cc55fa1fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*laN3aseisIU3T9QTIlob4Q.gif"/></div></figure><h2 id="2397" class="kc kd hi bd ke kf kg kh ki kj kk kl km jg kn ko kp jk kq kr ks jo kt ku kv kw bi translated">参考资料:</h2><ol class=""><li id="d9db" class="mg mh hi ix b iy kx jc ky jg mi jk mj jo mk js ml mm mn mo bi translated"><a class="ae iu" href="https://youtu.be/wzwZtZAJGq8" rel="noopener ugc nofollow" target="_blank">AAIC的逻辑回归。</a></li><li id="5833" class="mg mh hi ix b iy mp jc mq jg mr jk ms jo mt js ml mm mn mo bi translated"><a class="ae iu" href="https://youtu.be/IHZwWFHWa-w" rel="noopener ugc nofollow" target="_blank">梯度下降，神经网络如何学习。</a></li><li id="6297" class="mg mh hi ix b iy mp jc mq jg mr jk ms jo mt js ml mm mn mo bi translated">W <a class="ae iu" href="https://youtu.be/FiSy6zWDfiA" rel="noopener ugc nofollow" target="_blank">机器学习中的Norm是什么？</a></li></ol></div></div>    
</body>
</html>