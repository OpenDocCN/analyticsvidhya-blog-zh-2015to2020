<html>
<head>
<title>Sentiment analysis for text with Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度学习的文本情感分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sentiment-analysis-for-text-with-deep-learning-2f0a0c6472b5?source=collection_archive---------0-----------------------#2019-04-02">https://medium.com/analytics-vidhya/sentiment-analysis-for-text-with-deep-learning-2f0a0c6472b5?source=collection_archive---------0-----------------------#2019-04-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/d95e0a835f48c7587ec53493b68c1b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*33q1bTIm6jOzZXf_.png"/></div></figure><p id="30a9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我开始用twitter数据做一个NLP相关的项目，项目目标之一是对每条推文进行情感分类。然而，当我探索可用的资源，如NLTK情感分类器和python中的其他可用资源时，我对这些模型的性能感到失望。在二元分类(即只有正面或负面类别)任务中，我最多只能获得60%到70%的准确率。</p><p id="4709" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，我开始研究如何提高我的模型性能。一个显而易见的选择是建立一个基于深度学习的情感分类模型。</p><p id="b1d3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我写这篇博客是为了分享我关于建立情感分类深度学习模型的步骤的经验，希望你会觉得有用。代码库的链接可以在<a class="ae jk" href="https://gitlab.com/praj88/deepsentiment" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="b14d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我设计的模型提供了一个0到1之间的情绪得分，0表示非常消极，1表示非常积极。这是通过建立一个多类分类模型来完成的，即10个类，每个十分位数一个类。</p><p id="59f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">建立情感分类的深度学习模型有<strong class="io hj"> 5个主要步骤</strong>:</p><p id="3812" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第一步:</strong>获取数据。</p><p id="f542" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第二步:</strong>生成嵌入</p><p id="3a1f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第三步</strong>:架构建模</p><p id="4840" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第四步</strong>:模型参数</p><p id="9c24" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第五步:</strong>训练并测试模型</p><p id="f21d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">步骤6: </strong>运行模型</p><p id="2ab2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我将在下面详细介绍上述每个步骤。</p><h2 id="24d3" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">第一步:获取数据</h2><p id="7c23" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">获取用于训练深度学习模型的标记数据是建立模型最困难的部分之一。幸运的是，我们可以使用斯坦福情感树库数据来达到我们的目的。</p><p id="9141" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数据集“dictionary.txt”由239，233行句子组成，每行有一个索引。该索引用于将每个句子与文件“labels.txt”中的情感分数进行匹配。分数范围从0到1，0表示非常负面，1表示非常正面。</p><p id="5a09" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下面的代码读取dictionary.txt和labels.txt文件，将分数组合到每个句子中。该代码位于<strong class="io hj"><em class="kl">train/utility _ function . py</em></strong>中</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="b233" class="jl jm hi kr b fi kv kw l kx ky">def read_data(path):</span><span id="ab56" class="jl jm hi kr b fi kz kw l kx ky"># read dictionary into df</span><span id="cb7e" class="jl jm hi kr b fi kz kw l kx ky">df_data_sentence = pd.read_table(path + ‘dictionary.txt’)</span><span id="1d13" class="jl jm hi kr b fi kz kw l kx ky">df_data_sentence_processed = df_data_sentence[‘Phrase|Index’].str.split(‘|’, expand=True)</span><span id="6613" class="jl jm hi kr b fi kz kw l kx ky">df_data_sentence_processed = df_data_sentence_processed.rename(columns={0: ‘Phrase’, 1: ‘phrase_ids’})</span><span id="e410" class="jl jm hi kr b fi kz kw l kx ky"># read sentiment labels into df</span><span id="5ae3" class="jl jm hi kr b fi kz kw l kx ky">df_data_sentiment = pd.read_table(path + ‘sentiment_labels.txt’)</span><span id="de54" class="jl jm hi kr b fi kz kw l kx ky">df_data_sentiment_processed = df_data_sentiment[‘phrase ids|sentiment values’].str.split(‘|’, expand=True)</span><span id="1333" class="jl jm hi kr b fi kz kw l kx ky">df_data_sentiment_processed = df_data_sentiment_processed.rename(columns={0: ‘phrase_ids’, 1: ‘sentiment_values’})</span><span id="505d" class="jl jm hi kr b fi kz kw l kx ky">#combine data frames containing sentence and sentiment</span><span id="a999" class="jl jm hi kr b fi kz kw l kx ky">df_processed_all = df_data_sentence_processed.merge(df_data_sentiment_processed, how=’inner’, on=’phrase_ids’</span><span id="6e98" class="jl jm hi kr b fi kz kw l kx ky">return df_processed_all</span></pre><p id="39e1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数据分为3部分:</p><ul class=""><li id="2e5d" class="la lb hi io b ip iq it iu ix lc jb ld jf le jj lf lg lh li bi translated">train.csv:这是用于训练模型的主要数据。这是整体数据的50%。</li><li id="3c8d" class="la lb hi io b ip lj it lk ix ll jb lm jf ln jj lf lg lh li bi translated">val.csv:这是一个验证数据集，用于确保模型不会过度拟合。这是整体数据的25%。</li><li id="805d" class="la lb hi io b ip lj it lk ix ll jb lm jf ln jj lf lg lh li bi translated">test.csv:用于测试模型后期训练的准确性。这是整体数据的25%。</li></ul><h2 id="99fa" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">步骤2:生成嵌入</h2><p id="1c3d" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">在训练这个模型之前，我们将把每个单词转换成单词嵌入。您可以将单词嵌入视为单词的数字表示，以使我们的模型能够学习。关于单词嵌入的更多细节，请阅读这个<a class="ae jk" href="https://www.datascience.com/resources/notebooks/word-embeddings-in-python" rel="noopener ugc nofollow" target="_blank">博客</a>。</p><p id="22b6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kl">什么是单词嵌入？</em></p><p id="b5dd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">它们是向量表示，捕捉句子中潜在单词相对于其他单词的上下文。这种变换导致具有相似含义的单词在超平面中更紧密地聚集在一起，而不同的单词在超平面中位于更远处。</p><p id="f35a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kl">我们如何将每个单词转换成单词嵌入体？</em></p><p id="ed24" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将使用一个预先训练好的单词嵌入模型，称为<a class="ae jk" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a>。对于我们的模型，我们将使用100维嵌入来表示每个单词。用于将数据转换为单词嵌入的详细代码在<strong class="io hj"><em class="kl">train/utility _ function . py .</em></strong>中。该函数通过从手套预训练向量中执行查找，基本上将每个单词替换为其各自的嵌入。下图显示了这个过程，其中每个单词都被转换成一个嵌入，并输入到一个神经网络中。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lo"><img src="../Images/2caf7c221c6e45fb1c6b4344ac14cdfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lNsgjlmv5FFVUxm7.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">将句子转换为神经网络的嵌入(<a class="ae jk" href="https://www.datascience.com/resources/notebooks/word-embeddings-in-python" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="e6ec" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">以下代码用于将数据分为训练集、val集和测试集。数据的相应嵌入也存储在weight_matrix变量中。</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="bd43" class="jl jm hi kr b fi kv kw l kx ky">def load_data_all(data_dir, all_data_path,pred_path, gloveFile, first_run, load_all):</span><span id="0a64" class="jl jm hi kr b fi kz kw l kx ky">   numClasses = 10</span><span id="29ae" class="jl jm hi kr b fi kz kw l kx ky"># Load embeddings for the filtered glove list<br/>    if load_all == True:<br/>        weight_matrix, word_idx = uf.load_embeddings(gloveFile)<br/>    else:<br/>        weight_matrix, word_idx = uf.load_embeddings(filtered_glove_path)</span><span id="a530" class="jl jm hi kr b fi kz kw l kx ky">    # create test, validation and trainng data<br/>    all_data = uf.read_data(all_data_path)<br/>    train_data, test_data, dev_data = uf.training_data_split(all_data, 0.8, data_dir)</span><span id="8a99" class="jl jm hi kr b fi kz kw l kx ky">train_data = train_data.reset_index()<br/>    dev_data = dev_data.reset_index()<br/>    test_data = test_data.reset_index()</span><span id="a1ae" class="jl jm hi kr b fi kz kw l kx ky">    maxSeqLength, avg_words, sequence_length = uf.maxSeqLen(all_data)<br/>  </span><span id="2955" class="jl jm hi kr b fi kz kw l kx ky"># load Training data matrix<br/>    train_x = uf.tf_data_pipeline_nltk(train_data, word_idx, weight_matrix, maxSeqLength)<br/>    test_x = uf.tf_data_pipeline_nltk(test_data, word_idx, weight_matrix, maxSeqLength)<br/>    val_x = uf.tf_data_pipeline_nltk(dev_data, word_idx, weight_matrix, maxSeqLength)</span><span id="1101" class="jl jm hi kr b fi kz kw l kx ky">    # load labels data matrix<br/>    train_y = uf.labels_matrix(train_data)<br/>    val_y = uf.labels_matrix(dev_data)<br/>    test_y = uf.labels_matrix(test_data)</span><span id="a5c1" class="jl jm hi kr b fi kz kw l kx ky">return train_x, train_y, test_x, test_y, val_x, val_y, weight_matrix</span></pre><h2 id="bd3f" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">步骤3:模型架构</h2><p id="35df" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">为了训练模型，我们将使用一种称为LSTM(长短期记忆)的递归神经网络。这个网络的主要优点是，它能够记住过去的数据序列，即在我们的情况下的单词，以便对单词的情感做出决定。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lx"><img src="../Images/d364e9ab1cac7dd0fc6bda672fad3b32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hkpZ3Ezkn7KINHKP.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">一个RNN网络(<a class="ae jk" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="0d5f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如上图所示，它基本上是细胞的一系列拷贝，每个细胞的输出都作为输入传递给下一个细胞。LSTM网络本质上是相同的，但是每个小区的架构稍微复杂一些。如下图所示，这种复杂性允许每个细胞决定哪些过去的信息要记住，哪些要忘记，如果你想了解更多关于LSTM内部运作的信息，请访问这个令人惊叹的<a class="ae jk" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">博客</a> <em class="kl">(插图来源于这个博客)。</em></p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lx"><img src="../Images/02d8ecc0ed61dbeb541e04752d4bf844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5_DsbLOQt9pYuqTy.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">一个LSTM细胞(<a class="ae jk" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="442c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将使用Keras创建网络。Keras基于tensorflow构建，可用于构建大多数类型的深度学习模型。我们将指定模型的层如下。为了估计诸如漏失、单元数量等参数，我使用不同的参数值进行了网格搜索，并选择了具有最佳性能的参数。</p><p id="a156" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">层数:</strong></p><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/1d2fcdc533145c62d9d1f006efce6bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*iu04m9kXfzAoBNJRmy_xnw.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">模型架构</figcaption></figure><p id="7f90" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">层1: </strong>矢量大小为100并且每个句子的最大长度被设置为56的嵌入层。</p><p id="46ac" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第2层:</strong> 128个单元双向LSTM层，其中嵌入数据被馈送到网络。我们增加了一个0.2的压差，这是用来防止过度拟合。</p><p id="b155" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第3层:</strong>一个512层的密集网络，接收来自LSTM层的输入。这里增加了0.5的漏失。</p><p id="3396" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第4层:</strong>激活softmax的10层稠密网络，每个类用于表示一个情感类别，类1表示0.0到0.1之间的情感得分，类10表示0.9到1之间的情感得分。</p><p id="e77c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在Keras中创建LSTM模型的代码:</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="72d9" class="jl jm hi kr b fi kv kw l kx ky">import os<br/>import numpy as np<br/>import keras<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import Flatten<br/>from keras.layers import LSTM<br/>from keras.layers.embeddings import Embedding<br/>from keras.layers import Bidirectional<br/>from keras.preprocessing import sequence<br/>from keras.layers import Dropout<br/>from keras.models import model_from_json<br/>from keras.models import load_model</span><span id="78c6" class="jl jm hi kr b fi kz kw l kx ky"><br/>def create_model_rnn(weight_matrix, max_words, EMBEDDING_DIM):</span><span id="4902" class="jl jm hi kr b fi kz kw l kx ky"># create the model</span><span id="d927" class="jl jm hi kr b fi kz kw l kx ky">model = Sequential()<br/>model.add(Embedding(len(weight_matrix), EMBEDDING_DIM, weights=[weight_matrix], input_length=max_words, trainable=False))<br/>model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))<br/>model.add(Dense(512, activation='relu'))<br/>model.add(Dropout(0.50))<br/>model.add(Dense(10, activation='softmax'))</span><span id="5670" class="jl jm hi kr b fi kz kw l kx ky"># Adam Optimiser<br/>model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])</span><span id="dfab" class="jl jm hi kr b fi kz kw l kx ky">return model</span></pre><h2 id="fd8f" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated"><strong class="ak">第四步:模型参数:</strong></h2><p id="4b35" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated"><strong class="io hj">激活功能:</strong>我已经用ReLU作为激活功能了。ReLU是一个非线性激活函数，有助于模型捕获数据中的复杂关系。</p><p id="8113" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">优化器:</strong>我们使用adam优化器，它是一个自适应学习率优化器。</p><p id="ec9c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">损失函数:</strong>我们将使用<strong class="io hj">交叉熵损失</strong>，也称为Softmax损失，训练网络输出10个类别的概率。对于多类分类非常有用。</p><h2 id="d235" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">步骤5:训练和测试模型</h2><p id="d87a" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">我们通过将训练、验证和测试数据集传递到下面的函数中来开始模型的训练:</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="0aff" class="jl jm hi kr b fi kv kw l kx ky">def train_model(model,train_x, train_y, test_x, test_y, val_x, val_y, batch_size):</span><span id="7deb" class="jl jm hi kr b fi kz kw l kx ky"># save the best model and early stopping<br/>saveBestModel = keras.callbacks.ModelCheckpoint('../best_weight_glove_bi_100d.hdf5', monitor='val_acc', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)</span><span id="c5e6" class="jl jm hi kr b fi kz kw l kx ky">earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')</span><span id="70db" class="jl jm hi kr b fi kz kw l kx ky"># Fit the model<br/>model.fit(train_x, train_y, batch_size=batch_size, epochs=25,validation_data=(val_x, val_y), callbacks=[saveBestModel, earlyStopping])</span><span id="b51b" class="jl jm hi kr b fi kz kw l kx ky"># Final evaluation of the model<br/>score, acc = model.evaluate(test_x, test_y, batch_size=batch_size)<br/>return model</span></pre><p id="94ec" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我已经对一次 <strong class="io hj"> 500 </strong>个项目的<strong class="io hj">批次进行了培训。随着批量的增加，训练的时间会减少，但需要额外的计算能力。因此，这是计算能力和训练时间之间的折衷。</strong></p><p id="6341" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">培训将持续25个时期。一个时期意味着网络已经看到了整个训练数据一次。随着时代数量的增加，模型可能会过度适应训练数据。因此，为了防止模型过度拟合，我启用了早期停止。</p><p id="9c47" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">早期停止是一种方法，它允许我们指定任意大量的训练时期，并且一旦模型性能在支持/验证数据集上停止改善，就停止训练。</p><p id="4327" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该模型在10类情感分类测试集上提供了48.6%的准确率。在2类二进制(正或负)数据集上，精度会高得多。</p><h2 id="a4ad" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">步骤6:运行模型</h2><p id="cf84" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">一旦模型被训练，你可以使用下面的代码在keras中保存模型。</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="118c" class="jl jm hi kr b fi kv kw l kx ky">model.save_weights("/model/best_model.h5")</span></pre><p id="531c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下一步是实时使用训练好的模型对新数据进行预测。为了做到这一点，您需要将输入数据转换为嵌入数据，类似于我们处理训练数据的方式。下面的函数<em class="kl"> live_test </em>执行所需的数据预处理，并返回训练模型的结果。</p><p id="9e83" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里，为了确保模型结果的稳健性，我从模型中选取了平均前3个情绪带。这为模型结果提供了更好的校准。</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="f4b2" class="jl jm hi kr b fi kv kw l kx ky">def live_test(trained_model, data, word_idx):</span><span id="5825" class="jl jm hi kr b fi kz kw l kx ky">live_list = []<br/>live_list_np = np.zeros((56,1))</span><span id="9ff6" class="jl jm hi kr b fi kz kw l kx ky"># split the sentence into its words and remove any punctuations.</span><span id="18fb" class="jl jm hi kr b fi kz kw l kx ky">tokenizer = RegexpTokenizer(r'\w+')<br/>data_sample_list = tokenizer.tokenize(data)<br/>labels = np.array(['1','2','3','4','5','6','7','8','9','10'], dtype = "int")</span><span id="85af" class="jl jm hi kr b fi kz kw l kx ky"># get index for the live stage<br/>data_index = np.array([word_idx[word.lower()] if word.lower() in word_idx else 0 for word in data_sample_list])<br/>data_index_np = np.array(data_index)</span><span id="300d" class="jl jm hi kr b fi kz kw l kx ky"># padded with zeros of length 56 i.e maximum length<br/>padded_array = np.zeros(56)<br/>padded_array[:data_index_np.shape[0]] = data_index_np<br/>data_index_np_pad = padded_array.astype(int)<br/>live_list.append(data_index_np_pad)<br/>live_list_np = np.asarray(live_list)</span><span id="3193" class="jl jm hi kr b fi kz kw l kx ky"># get score from the model<br/>score = trained_model.predict(live_list_np, batch_size=1, verbose=0)<br/>single_score = np.round(np.argmax(score)/10, decimals=2) # maximum of the array i.e single band</span><span id="061c" class="jl jm hi kr b fi kz kw l kx ky"># weighted score of top 3 bands<br/>top_3_index = np.argsort(score)[0][-3:]<br/>top_3_scores = score[0][top_3_index]<br/>top_3_weights = top_3_scores/np.sum(top_3_scores)<br/>single_score_dot = np.round(np.dot(top_3_index, top_3_weights)/10, decimals = 2)</span><span id="40fe" class="jl jm hi kr b fi kz kw l kx ky">return single_score_dot, single_score</span></pre><p id="4d86" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如下面的代码所示，您可以指定模型路径、样本数据和live_test函数的相应嵌入。它将返回样本数据的情感。</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="14ed" class="jl jm hi kr b fi kv kw l kx ky"># Load the best model that is saved in previous step<br/>weight_path = '/model/best_model.hdf5'<br/>loaded_model = load_model(weight_path)</span><span id="bd4a" class="jl jm hi kr b fi kz kw l kx ky"># sample sentence<br/>data_sample = "This blog is really interesting."<br/>result = live_test(loaded_model,data_sample, word_idx)</span></pre><h2 id="0204" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">结果</h2><p id="f80b" class="pw-post-body-paragraph im in hi io b ip kg ir is it kh iv iw ix ki iz ja jb kj jd je jf kk jh ji jj hb bi translated">让我们通过取样来比较我们的深度学习模型和NLTK模型的结果。</p><p id="d6c7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">LSTM模式:这句话<em class="kl">“太棒了！！今天下雨了！!"</em>包含负面内容，我们的模型能够预测这一点，如下所示。它的得分是0.34。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lz"><img src="../Images/4c11175edb1b9a74f9ce94294d099482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9KZJVTt5-Hl2QR4Hib5DmA.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">LSTM模型</figcaption></figure><p id="50b1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">NLTK模型:当用双词NLTK模型分析同一个句子时，该句子的得分为0.74，是肯定的。</p><figure class="km kn ko kp fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/4c31a3dba5844c13d42b73085ac17f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*kh7NUpqJpnerkgXZen05Ew.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">NLTK模型</figcaption></figure><p id="e79a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">万岁！！为文本数据创建深度学习情感分类模型的教程到此结束。</p><p id="c23b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您可以从<a class="ae jk" href="https://gitlab.com/praj88/deepsentiment" rel="noopener ugc nofollow" target="_blank"> gitlab </a>下载源代码，并根据您自己的数据对网络进行训练。我将在另一篇博客中讨论如何使用dockers和api服务大规模部署这个模型。</p></div></div>    
</body>
</html>