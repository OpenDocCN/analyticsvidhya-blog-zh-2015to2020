<html>
<head>
<title>Autonomous Drone using ML , RL &amp; AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用ML、RL和AI的自主无人机</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/self-flying-drone-using-reinforcement-learning-4a1ab5a37932?source=collection_archive---------18-----------------------#2020-07-26">https://medium.com/analytics-vidhya/self-flying-drone-using-reinforcement-learning-4a1ab5a37932?source=collection_archive---------18-----------------------#2020-07-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/95251acd48a08cf18adcf4d754310e44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*54UPsG5LRmOr7wiahc1vEw.jpeg"/></div></div></figure><p id="7404" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">四轴飞行器或<strong class="is hj">四旋翼直升机</strong>正成为越来越受欢迎的个人和专业用途飞机。它的机动性使其适用于许多领域，从最后一英里的运输到电影摄影，从杂技到搜救。</p><p id="bebf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这一发展的下一步是使四轴飞行器能够自主实现所需的控制行为，如起飞和着陆。您可以用经典的方法设计这些控制(比如，通过实现PID控制器)。或者，您可以使用强化学习来构建能够自己学习这些行为的代理。这就是我们在这个项目中要做的！</p><p id="c8b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">安装依赖:</strong></p><p id="2842" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Matplotlib==2.0.0</p><p id="fe08" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Numpy==1.14.1</p><p id="8eb9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">熊猫==0.19.2</p><p id="dc48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Python 2.7</p><p id="95f5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Keras 1.1.0</p><p id="19ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">张量流r0.10</p><p id="1240" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">重要文件:</strong></p><p id="7ff8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Github链接:<a class="ae jo" href="https://github.com/yash143/Self-Flying-drone/tree/master" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">https://github.com/yash143/Self-Flying-drone/tree/master</strong></a><strong class="is hj">，</strong>跟着我一起看上面链接提供的Jupyter笔记本(<a class="ae jo" href="https://github.com/yash143/Self-Flying-drone/blob/master/Self_flying_drone.ipynb" rel="noopener ugc nofollow" target="_blank">Self _ flying _ drone . ipynb</a>)。</p><p id="7842" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">动机:</strong></p><p id="7ee6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从童年起，我就对遥控汽车、机器人、无人机和一切使用人工智能的东西情有独钟，但我永远也不能拥有这些小玩意。虽然我们距离建立超级智能的人工智能系统还很远，但计算机视觉、强化学习和深度学习的最新发展已经为我创造了一个令人兴奋的时代，至少可以实现我现在创造人工智能系统的小小梦想。</p><p id="9457" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">背景:</strong></p><p id="6e49" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们所知，最近Q-Learning的进步导致深度Q-Learning在许多方面使生活变得更简单。然而，深度Q网络的一个很大的局限性是，如果你考虑汽车的例子，输出/动作是离散的，而像转向这样的动作是连续的。使DQN适应连续域的一个显而易见的方法是简单地离散动作空间。然而，我们遇到了“维数灾难问题”。例如，如果您将方向盘从-90度离散化到+90度，每个离散化5度，将加速度从0公里离散化到300公里，每个离散化5公里，您的输出组合将是36个转向状态乘以60个速度状态，等于2160个可能的组合。当你想制造机器人来执行一些非常专业的事情时，情况会更糟，例如需要精细控制动作的脑外科手术，而天真的离散化将无法达到操作所需的精度。我希望这个例子足以说明为什么DQN不适合所有的任务。</p><p id="c942" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">谷歌Deepmind设计了一种新算法，通过结合3种技术来解决连续行动空间问题1) <a class="ae jo" href="http://jmlr.org/proceedings/papers/v32/silver14.pdf" rel="noopener ugc nofollow" target="_blank">确定性策略梯度算法</a> 2) <a class="ae jo" href="https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node66.html" rel="noopener ugc nofollow" target="_blank">行动者-批评家方法</a> 3) <a class="ae jo" href="https://arxiv.org/abs/1312.5602" rel="noopener ugc nofollow" target="_blank">深度Q网络</a>称为<a class="ae jo" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank">深度确定性策略梯度(DDPG) </a></p><p id="7fee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">无人机模拟器(物理)(可选):</strong></p><p id="c65e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">文件:Drone_physics.py </strong></p><p id="1352" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">链接:</strong><a class="ae jo" href="https://github.com/yash143/Self-Flying-drone/blob/master/Drone_physics.py" rel="noopener ugc nofollow" target="_blank">https://github . com/yash 143/Self-Flying-Drone/blob/master/Drone _ physics . py</a></p><p id="eec6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那些对四轴飞行器背后的物理学感兴趣的人，让我们开始吧。</p><p id="56dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">简单来说，当能量从电池流向转子马达时，螺旋桨开始旋转，正是每个螺旋桨相对于其他螺旋桨的旋转改变了高度和方向。加速转子，它们将产生足够的升力来克服重力，使无人机越来越高。但其背后的物理原理远比看起来复杂。<strong class="is hj">所以我会尽量简化。</strong></p><p id="21e8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们需要移动坐标系，因为惯性坐标系的轴是地球固定的，机身坐标系的轴与无人机对齐，如下图所示。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jp"><img src="../Images/d1bc7a7d39f15541e1bea36fd7c8a505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pECz4MHqUQd6YoUFJAruEQ.png"/></div></div></figure><p id="31e5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以使用下面的函数/矩阵来移动帧。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="59eb" class="jz ka hi jv b fi kb kc l kd ke">def earth_to_body_frame(ii, jj, kk):<br/>    <br/>    R = [[Cos(kk)*Cos(jj), Cos(kk)*Sin(jj)*Sin(ii)-Sin(kk)*Cos(ii), <br/>             Cos(kk) * Sin(jj) * Cos(ii) + Sin(kk) * Sin(ii)],<br/>         [Sin(kk)*Cos(jj), Sin(kk)*Sin(jj)*Sin(ii)+Cos(kk)*Cos(ii), <br/>             Sin(kk)*Sin(jj)*Cos(ii)-Cos(kk)*Sin(ii)],<br/>         [-Sin(jj), Cos(jj)*Sin(ii), Cos(jj)*Cos(ii)]]<br/>    return np.array(R)</span></pre><p id="9058" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<strong class="is hj"> ii=phi，jj=theta，kk=psi为欧拉角</strong>。R的转置给出了体坐标系到地球坐标系的转换矩阵。所以当你把惯性系[x，y，z]中的一个向量乘以R，你就得到这个向量相对于物体坐标系的表示。</p><p id="1b44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们的项目中，无人机由4个旋翼组成。代理通过设置四个旋翼的每秒转数来控制无人机。因此我们的<strong class="is hj">动作空间大小为4。</strong></p><p id="304f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">无人机的位置</strong>由<strong class="is hj"> (x，y，z)和欧拉角φ，θ，psi</strong>(【x，y，z，φ，θ，psi】)定义在任意时间步t，我们的最终目的是改变无人机在每个时间步之后的位置。</p><p id="40c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了改变x，y，z分量，我们需要计算无人机的加速度，因为<strong class="is hj">位置[x，y，z] =初始位置[x，y，z] +速度[x，y，z] *dt + 0.5 *加速度[x，y，z]* dt*dt。</strong></p><p id="af76" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们知道<strong class="is hj">线性_加速度=线性_力/无人机质量</strong>。所以linear_forces可以从下面的函数计算出来。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="ae21" class="jz ka hi jv b fi kb kc l kd ke">def find_body_velocity():<br/>        body_velocity = <br/>        np.matmul(earth_to_body_frame(*list([x ,y ,z ])), <br/>        velocity[x,y,z])<br/>        return body_velocity<br/>def get_linear_drag():<br/>        (areas = np.array([length * height, width * height, width <br/>        *length]) , #rho=density of air , #C_d=drag coefficient)<br/>        <br/>        linear_drag = 0.5 * rho * find_body_velocity()**2 * <br/>        areas_of_drone * C_d<br/>        <br/>        return linear_drag<br/>def get_linear_forces(self, thrusts):<br/>        # Gravity<br/>        gravity_force = mass * gravity * np.array([0, 0, 1])<br/>        # Thrust<br/>        thrust_body_force = np.array([0, 0, sum(thrusts)])<br/>        # Drag<br/>        drag_body_force = -get_linear_drag()<br/>        body_forces = thrust_body_force + drag_body_force<br/><br/>        linear_forces = <br/>        np.matmul(body_to_earth_frame(*list([x,y,z])), <br/>        body_forces)<br/>        linear_forces += gravity_force<br/>        return linear_forces</span></pre><p id="5623" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们终于可以在计算linear_forces后更新[x，y，z]的值了。</p><p id="e058" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Angles[phi，theta，psi] = Intial_Angles[phi，theta，psi]+angular _ velocity * self . dt+0.5 * angle _ accels * self . angular _ accels * self . dt * * 2</strong></p><p id="a3b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在为了更新欧拉角，我们同样需要角加速度。</p><p id="51a9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">角加速度=力矩/惯性矩</strong>。所以力矩可以从下面的函数中计算出来。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="a0a8" class="jz ka hi jv b fi kb kc l kd ke">I_x = 1 / 12. * mass * (height**2 + width**2)<br/>I_y = 1 / 12. * mass * (height**2 + length**2) <br/>I_z = 1 / 12. * mass * (width**2 + length**2)<br/>moments_of_inertia = np.array([I_x, I_y, I_z])  # moments of inertia</span><span id="1425" class="jz ka hi jv b fi kf kc l kd ke">def get_moments(thrusts):<br/>        thrust_moment = np.array([(thrusts[3] - <br/>        thrusts[2])*self.l_to_rotor,(thrusts[1] - thrusts[0]) * <br/>        self.l_to_rotor,0])<br/><br/>        drag_moment =  C_d * 0.5 * rho * angular_v * <br/>        np.absolute(angular_v) *areas *dims *dims<br/>        <br/>        moments = thrust_moment - drag_moment # + motor_inertia_moment<br/>        return moments</span></pre><p id="8e9c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们终于可以更新φ，θ和psi了。</p><p id="47e5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们的整个状态数组<strong class="is hj">【x，y，z，phi，theta，psi】</strong>将在每个时间步长之后更新，从而产生一个新的状态。以上模拟的完整代码在Drone_physics.py中提供</p><p id="13a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">重要提示:</strong></p><p id="f12f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">状态空间= [x，y，z，phi，theta，psi] </strong></p><p id="ceb1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">动作空间= [ v1，v2，v3，v4](4个转子的速度)</strong></p><p id="a19e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">政策网络:</strong></p><p id="d0f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们将定义一个<strong class="is hj">策略网络</strong>来实现我们的AI驱动程序。这个网络会取无人机的状态(<strong class="is hj">【x，y，z，phi，theta，psi】</strong>)并决定动作(<strong class="is hj">4个旋翼的速度</strong>)。它被称为基于策略的强化学习，因为我们将直接参数化策略</p><p id="8ef8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="kg">π</em>t22】θ(<em class="kg">s</em>，<em class="kg">a</em>)=<em class="kg">p</em>[<em class="kg">a</em>∣<em class="kg">s</em>，<em class="kg"> θ </em></p><p id="fcb1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，s是状态，a是动作并且<em class="kg"> θ </em>是策略网络的模型参数。我们可以认为政策是主体的行为，即从状态到行动的映射函数。</p><p id="5852" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">确定性与随机性策略:</strong></p><p id="7e3a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请注意，有两种类型的策略:</p><p id="32e9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">确定性策略:<em class="kg">a</em>=<em class="kg">μ</em>(<em class="kg">s</em>)</p><p id="7700" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随机政策:<em class="kg">π</em>(<em class="kg">a</em>∣<em class="kg">s</em>)=<em class="kg">p</em>[<em class="kg">a</em>∣<em class="kg">s</em>]</p><p id="ad4b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为什么除了确定性策略之外，我们还需要随机策略？很容易理解确定性策略。我看到一个特定的状态输入，然后我采取特定的行动。但有时确定性策略不起作用，比如在GO的例子中，您的第一个状态是如下所示的空棋盘:</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es kh"><img src="../Images/b7ac583a032274279d9257148c27e43c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*DxuulQ0z6j_vzD2j59bRgw.png"/></div></figure><p id="a3d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你使用相同的确定性策略，你的网络将总是把石头放在一个“特定”的位置，这是一个非常不可取的行为，因为它使你可以被你的对手预测到。在这种情况下，随机策略比确定性策略更合适。</p><p id="518e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">政策目标函数:</strong></p><p id="a2d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么怎么才能找到<em class="kg">π</em>T2】θ(<em class="kg">s</em>，<em class="kg"> a </em>)？其实可以用强化技术来解决。例如，假设无人机正试图到达我们指定的位置。开始时，无人机可能只是没有到达目标位置，可能会超出界限并收到负奖励，因此神经网络将调整模型参数<em class="kg"> θ </em>，以便下次它将尝试避免超出界限。经过多次尝试后，它会发现“啊，如果我不越过指定的范围，我可能会到达目的地”。在数学语言中，我们称这些政策为目标函数。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/c9d4e18e3cab4816a0dd0bcc77df06db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qsTY469AMY-IMUlqeJhltg.png"/></div></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kj"><img src="../Images/5b85f80192bb32db7ba023d0ef904f78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xi-h1ED-moZTDw_oxyMaAA.png"/></div></div></figure><p id="c40c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">演员-评论家算法:</strong></p><p id="5d0d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Actor-Critic算法本质上是一种将策略梯度法和价值函数法结合在一起的混合方法。政策功能被称为<strong class="is hj">参与者</strong>，而价值功能被称为<strong class="is hj">评论家</strong>。本质上，演员在给定环境的当前状态下产生动作，而批评家产生一个信号来批评演员所做的动作。我认为在人类世界中，初级员工(演员)做实际工作，而你的老板(评论家)批评你的工作，这是很自然的，希望初级员工下次能做得更好。在我们的项目中，我们使用连续Q学习(SARSA)作为我们的评论模型，使用策略梯度方法作为我们的行动者模型。下图解释了价值函数/策略函数和参与者-批评家算法之间的关系。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kk"><img src="../Images/f878802d7af64949bd3614c7285dbbbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fum-XtVFOl7bzek28zjQgA.png"/></div></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/81bdd7dc4ff68093439a56d15c418b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kck5at5bBKhIcrxodd14bQ.jpeg"/></div></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/365d6586e2cb4f137c44c521150a1fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wkiO9nsMlYA6ny4qthJBCA.jpeg"/></div></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es km"><img src="../Images/379da61d23bcae8442539da50365bc7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*4RM1b0zOW9jJ9a1_bs2oLw.png"/></div></figure><p id="be42" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们需要制作两个文件</p><p id="1f21" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 1)无人机_任务。PY:我们将在这个文件中定义我们的任务(环境)。</strong></p><p id="33bd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">链接:<a class="ae jo" href="https://github.com/yash143/Self-Flying-drone/blob/master/Drone_task.py" rel="noopener ugc nofollow" target="_blank">https://github . com/yash 143/Self-Flying-Drone/blob/master/Drone _ task . py</a></p><p id="78bf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 2)无人机_智能体。开发我们的强化学习代理</strong></p><p id="0303" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">链接:<a class="ae jo" href="https://github.com/yash143/Self-Flying-drone/blob/master/Drone_agent.py" rel="noopener ugc nofollow" target="_blank">https://github . com/yash 143/Self-Flying-Drone/blob/master/Drone _ agent . py</a></p><p id="b67c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">无人机任务:</strong></p><p id="d1a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码和解释如下。我们无人机的尝试任务是垂直起飞，然后在30高度悬停。我给特工提供了6D的目标姿势。奖励功能旨在鼓励无人机垂直起飞并停留在垂直轴上目标位置的范围内。然后，我将奖励限制在[-1，1]范围内，以便于通过神经网络进行学习，并避免爆炸梯度。</p><p id="5e41" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">奖励=-. 03<em class="kg">(ABS(self . sim . pose[2]—self . target _ pos[2])+. 005</em>self . sim . v[2]奖励= np.clip(奖励，-1，1) </strong></p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="4f5a" class="jz ka hi jv b fi kb kc l kd ke">import numpy as np<br/>from physics_sim import PhysicsSim<br/><br/>class TakeOff():<br/>    """Task (environment) that defines the goal and provides    feedback <br/>       to the agent."""<br/>    def __init__(self, init_pose=None, init_velocities=None, <br/>        init_angle_velocities=None, runtime=5., target_pos=None):<br/>        """Initialize a Task object.<br/>        Params<br/>        ======<br/>            init_pose: initial position of the quadcopter in (x,y,z) <br/>            dimensions and the Euler angles<br/>            init_velocities: initial velocity of the quadcopter in <br/>            (x,y,z) dimensions<br/>            init_angle_velocities: initial radians/second for each of <br/>            the three Euler angles<br/>            runtime: time limit for each episode<br/>            target_pos: target/goal (x,y,z) position for the agent<br/>        """<br/>        # Simulation<br/>        self.sim = PhysicsSim(init_pose, init_velocities, <br/>        init_angle_velocities, runtime) <br/>        self.action_repeat = 3<br/><br/>        self.state_size = self.action_repeat * 6<br/>        self.action_low = 0<br/>        self.action_high = 900<br/>        self.action_size = 4<br/><br/>        # Goal<br/>        self.target_pos = target_pos if target_pos is not None else <br/>           np.array([0., 0., 10.]) <br/><br/>    def get_reward(self):<br/>        reward=-.03*(abs(self.sim.pose[2] -self.target_pos[2]))             +.005*self.sim.v[2] <br/>        reward=np.clip(reward, -1, 1)<br/>        return reward<br/><br/>    def step(self, rotor_speeds):<br/>        """Uses action to obtain next state, reward, done."""<br/>        reward = 0<br/>        pose_all = []<br/>        for _ in range(self.action_repeat):<br/>            done = self.sim.next_timestep(rotor_speeds) # update the <br/>            sim pose and velocities<br/>            reward += self.get_reward() <br/>            pose_all.append(self.sim.pose)<br/>        next_state = np.concatenate(pose_all)<br/>        return next_state, reward, done<br/><br/>    def reset(self):<br/>        """Reset the sim to start a new episode."""<br/>        self.sim.reset()<br/>        state = np.concatenate([self.sim.pose] * self.action_repeat) <br/>        return state</span></pre><p id="3b77" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">__init__()方法用于初始化指定任务所需的几个变量。</p><ul class=""><li id="3d3c" class="kn ko hi is b it iu ix iy jb kp jf kq jj kr jn ks kt ku kv bi translated">模拟器被初始化为PhysicsSim类的一个实例(来自Drone_physics.py)。</li><li id="fd13" class="kn ko hi is b it kw ix kx jb ky jf kz jj la jn ks kt ku kv bi translated">受最初DDPG论文中方法论的启发，我们使用了动作重复。对于代理的每个时间步，我们执行模拟动作_重复时间步。如果你不熟悉动作重复，请阅读DDPG论文中的<strong class="is hj">结果</strong>部分。</li><li id="2a89" class="kn ko hi is b it kw ix kx jb ky jf kz jj la jn ks kt ku kv bi translated">我们设置状态向量中元素的数量。对于示例任务，我们只使用6维姿势信息。要设置状态的大小(state_size)，我们必须将动作重复次数考虑在内。</li><li id="0520" class="kn ko hi is b it kw ix kx jb ky jf kz jj la jn ks kt ku kv bi translated">该环境将总是具有4维动作空间，每个转子有一个条目(action_size=4)。您可以在此设置每个条目的最小值(action_low)和最大值(action_high)。</li><li id="2b8c" class="kn ko hi is b it kw ix kx jb ky jf kz jj la jn ks kt ku kv bi translated">这个提供的文件中的示例任务是让代理到达目标位置。我们将目标位置指定为变量。</li></ul><p id="cb8b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">reset()方法重置模拟器。代理应该在每集结束时调用此方法。你可以在下面的代码单元中看到这样的例子。</p><p id="2c7d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">step()方法可能是最重要的。它接受代理对action rotor_speeds的选择，该选择用于准备传递给代理的下一个状态。然后，根据get_reward()计算奖励。如果超过了时间限制，或者四轴飞行器超出了模拟的范围，则该集被认为已经完成。</p><h1 id="3110" class="lb ka hi bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">Keras代码解释:</h1><p id="147a" class="pw-post-body-paragraph iq ir hi is b it ly iv iw ix lz iz ja jb ma jd je jf mb jh ji jj mc jl jm jn hb bi translated">我们的代理将包括以下功能。</p><p id="dbeb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1)重放缓冲器</p><p id="fce5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2)演员网络</p><p id="fc6f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3)评论家网络</p><p id="f7f8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4)DDPG代理</p><p id="976a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5)噪音</p><p id="d9a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">文件:<strong class="is hj"> Drone_agent.py </strong></p><p id="a40f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">链接:<strong class="is hj"/><a class="ae jo" href="https://github.com/yash143/Self-Flying-drone/blob/master/Drone_agent.py" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">https://github . com/yash 143/自飞-Drone/blob/master/Drone _ agent . py</strong></a></p><p id="f4d5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们了解每个功能</p><p id="8fdc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">重放缓冲:</strong></p><p id="6e9e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">大多数现代强化学习算法受益于使用重放存储器或缓冲器来存储和调用经验元组。</p><p id="9b1d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下是您可以使用的重放缓冲区的示例实现:</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="5458" class="jz ka hi jv b fi kb kc l kd ke">import random<br/>from collections import namedtuple, deque<br/><br/>class ReplayBuffer:<br/>    """Fixed-size buffer to store experience tuples."""<br/><br/>    def __init__(self, buffer_size, batch_size):<br/>        """Initialize a ReplayBuffer object.<br/>        Params<br/>        ======<br/>            buffer_size: maximum size of buffer<br/>            batch_size: size of each training batch<br/>        """<br/>        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)<br/>        self.batch_size = batch_size<br/>        self.experience = namedtuple("Experience", field_names= <br/>        ["state", "action", "reward", "next_state", "done"])<br/><br/>    def add(self, state, action, reward, next_state, done):<br/>        """Add a new experience to memory."""<br/>        e = self.experience(state, action, reward, next_state, done)<br/>        self.memory.append(e)<br/><br/>    def sample(self, batch_size=64):<br/>        """Randomly sample a batch of experiences from memory."""<br/>        return random.sample(self.memory, k=self.batch_size)<br/><br/>    def __len__(self):<br/>        """Return the current size of internal memory."""<br/>        return len(self.memory)</span></pre><p id="2b4c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">演员(政策)网络:</strong></p><p id="1398" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">先说一下如何在Keras建立演员网络。这里我们使用了3个隐藏层，分别有32、64和32个隐藏单元，并带有relu激活功能。输出由4个连续动作(转子速度)和sigmoid激活函数组成。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="9713" class="jz ka hi jv b fi kb kc l kd ke">from keras import layers, models, optimizers<br/>from keras import backend as K<br/><br/>class Actor:<br/>    """Actor (Policy) Model."""<br/><br/>    def __init__(self, state_size, action_size, action_low, <br/>         action_high):<br/>         Params<br/>        ======<br/>            state_size (int): Dimension of each state<br/>            action_size (int): Dimension of each action<br/>            action_low (array): Min value of each action dimension<br/>            action_high (array): Max value of each action dimension<br/>        """<br/>        self.state_size = state_size<br/>        self.action_size = action_size<br/>        self.action_low = action_low<br/>        self.action_high = action_high<br/>        self.action_range = self.action_high - self.action_low<br/>        self.build_model()<br/><br/>    def build_model(self):<br/>        """Build an actor (policy) network that maps states -&gt; <br/>         actions."""<br/>        # Define input layer (states)<br/>        states = layers.Input(shape=(self.state_size,), name='states')<br/><br/>        # Add hidden layers<br/>        net = layers.Dense(units=32, activation='relu')(states)<br/>        net = layers.Dense(units=64, activation='relu')(net)<br/>        net = layers.Dense(units=32, activation='relu')(net)<br/><br/>        # Try different layer sizes, activations, add batch <br/>         normalization, regularizers, etc.<br/><br/>        # Add final output layer with sigmoid activation<br/>        raw_actions = layers.Dense(units=self.action_size, <br/>           activation='sigmoid' , name='raw_actions')(net)<br/><br/>        # Scale [0, 1] output for each action dimension to proper range<br/>        actions = layers.Lambda(lambda x: (x * self.action_range) + <br/>           self.action_low, name='actions')(raw_actions)<br/><br/>        # Create Keras model<br/>        self.model = models.Model(inputs=states, outputs=actions)<br/><br/>        # Define loss function using action value (Q value) gradients<br/>        action_gradients = layers.Input(shape=(self.action_size,))<br/>        loss = K.mean(-action_gradients * actions)<br/><br/>        # Incorporate any additional losses here (e.g. from <br/>           regularizers)<br/><br/>        # Define optimizer and training function<br/>        optimizer = optimizers.Adam()<br/>        updates_op = <br/>        optimizer.get_updates(params=self.model.trainable_weights, <br/>         loss=loss)<br/>        self.train_fn = K.function(<br/>            inputs=[self.model.input, action_gradients, <br/>            K.learning_phase()],<br/>            outputs=[],<br/>            updates=updates_op)</span></pre><p id="893b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请注意，输出层产生的原始动作在[0.0，1.0]范围内(使用sigmoid激活函数)。因此，我们添加了另一个层，它将每个输出缩放到每个操作维度的期望范围。这为任何给定的状态向量产生了确定性的动作。稍后将在这个动作中添加一个噪声，以产生一些探索性的行为。</p><p id="771b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一件要注意的事情是如何使用动作值(Q值)梯度来定义损失函数</p><p id="c3fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些梯度需要使用critic模型进行计算，并在训练时输入。因此，它被指定为训练函数中使用的“输入”的一部分</p><p id="4d38" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">评论家(价值)模型:</strong></p><p id="807c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">批评家网络的构建与Deep-Q网络非常相似。批评家网络将状态和动作都作为输入。根据DDPG的论文，直到Q网络的第二个隐藏层才包括这些动作。这里我们使用了Keras函数<a class="ae jo" href="https://keras.io/layers/core/" rel="noopener ugc nofollow" target="_blank">合并</a>将动作和隐藏层合并在一起。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="22e2" class="jz ka hi jv b fi kb kc l kd ke">class Critic:<br/>    """Critic (Value) Model."""<br/><br/>    def __init__(self, state_size, action_size):<br/>        Params<br/>        ======<br/>            state_size (int): Dimension of each state<br/>            action_size (int): Dimension of each action<br/>        """<br/>        self.state_size = state_size<br/>        self.action_size = action_size<br/><br/>        self.build_model()<br/><br/>    def build_model(self):<br/>        """Build a critic (value) network that maps (state, action) <br/>         pairs -&gt; Q-values."""<br/>        # Define input layers<br/>        states = layers.Input(shape=(self.state_size,), name='states')<br/>        actions = layers.Input(shape=(self.action_size,), <br/>        name='actions')<br/><br/>        # Add hidden layer(s) for state pathway<br/>        net_states = layers.Dense(units=32, activation='relu')(states)<br/>        net_states = layers.Dense(units=64, activation='relu')(net_states)<br/><br/>        # Add hidden layer(s) for action pathway<br/>        net_actions = layers.Dense(units=32, activation='relu')(actions)<br/>        net_actions = layers.Dense(units=64, activation='relu')(net_actions)<br/><br/>        # Try different layer sizes, activations, add batch <br/>        normalization, regularizers, etc.<br/><br/>        # Combine state and action pathways<br/>        net = layers.Add()([net_states, net_actions])<br/>        net = layers.Activation('relu')(net)<br/><br/>        # Add more layers to the combined network if needed<br/><br/>        # Add final output layer to prduce action values (Q values)<br/>        Q_values = layers.Dense(units=1, name='q_values')(net)<br/><br/>        # Create Keras model<br/>        self.model = models.Model(inputs=[states, actions], <br/>        outputs=Q_values)<br/><br/>        # Define optimizer and compile model for training with built-in <br/>        loss function<br/>        optimizer = optimizers.Adam()<br/>        self.model.compile(optimizer=optimizer, loss='mse')<br/><br/>        # Compute action gradients (derivative of Q values w.r.t. to <br/>        actions)<br/>        action_gradients = K.gradients(Q_values, actions)<br/><br/>        # Define an additional function to fetch action gradients (to <br/>        be used by actor model)<br/>        self.get_action_gradients = K.function(<br/>            inputs=[*self.model.input, K.learning_phase()],<br/>            outputs=action_gradients)</span></pre><p id="b3fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在某些方面，它比actor模型简单，但是有一些事情值得注意。首先，actor模型意味着将状态映射到动作，而critic模型需要将(状态，动作)对映射到它们的Q值。这反映在输入层中。</p><p id="c11c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这两层可以首先通过单独的“路径”(迷你子网)进行处理，但最终需要合并。这可以通过使用Keras中的添加图层类型来实现(见<a class="ae jo" href="https://keras.io/layers/merge/" rel="noopener ugc nofollow" target="_blank">合并图层</a>)</p><p id="cd44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该模型的最终输出是任何给定(状态、动作)对的Q值。然而，我们还需要计算这个Q值相对于相应动作向量的梯度，这是训练演员模型所需要的。这个步骤需要明确地执行，并且需要定义一个单独的函数来提供对这些梯度的访问</p><p id="9dd0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> DDPG代理:</strong></p><p id="015e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们现在准备将参与者和策略模型放在一起，以构建我们的DDPG代理。请注意，我们将需要每个模型的两个副本——一个本地，一个目标。这是深度Q学习的“固定Q目标”技术的扩展，用于将正在更新的参数与产生目标值的参数分离。下面是代理类的概要:</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="445e" class="jz ka hi jv b fi kb kc l kd ke">class DDPG():<br/>    """Reinforcement Learning agent that learns using DDPG."""<br/>    def __init__(self, task):<br/>        self.task = task<br/>        self.state_size = task.state_size<br/>        self.action_size = task.action_size<br/>        self.action_low = task.action_low<br/>        self.action_high = task.action_high<br/><br/>        # Actor (Policy) Model<br/>        self.actor_local = Actor(self.state_size, self.action_size, <br/>        self.action_low, self.action_high)<br/>        self.actor_target = Actor(self.state_size, self.action_size, <br/>        self.action_low, self.action_high)<br/><br/>        # Critic (Value) Model<br/>        self.critic_local = Critic(self.state_size, self.action_size)<br/>        self.critic_target = Critic(self.state_size, self.action_size)<br/><br/>        # Initialize target model parameters with local model <br/>        parameters<br/>        self.critic_target.model.set_weights(self.critic_local.model.get_weights())<br/>        self.actor_target.model.set_weights(self.actor_local.model.get_weights())<br/><br/>        # Noise process<br/>        self.exploration_mu = 0<br/>        self.exploration_theta = 0.15<br/>        self.exploration_sigma = 0.2<br/>        self.noise = OUNoise(self.action_size, self.exploration_mu, self.exploration_theta, self.exploration_sigma)<br/><br/>        # Replay memory<br/>        self.buffer_size = 100000<br/>        self.batch_size = 64<br/>        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)<br/><br/>        # Algorithm parameters<br/>        self.gamma = 0.99  # discount factor<br/>        self.tau = 0.01  # for soft update of target parameters<br/><br/>    def reset_episode(self):<br/>        self.noise.reset()<br/>        state = self.task.reset()<br/>        self.last_state = state<br/>        return state<br/><br/>    def step(self, action, reward, next_state, done):<br/>         # Save experience / reward<br/>        self.memory.add(self.last_state, action, reward, next_state, done)<br/><br/>        # Learn, if enough samples are available in memory<br/>        if len(self.memory) &gt; self.batch_size:<br/>            experiences = self.memory.sample()<br/>            self.learn(experiences)<br/><br/>        # Roll over last state and action<br/>        self.last_state = next_state<br/><br/>    def act(self, state):<br/>        """Returns actions for given state(s) as per current policy."""<br/>        state = np.reshape(state, [-1, self.state_size])<br/>        action = self.actor_local.model.predict(state)[0]<br/>        return list(action + self.noise.sample())  # add some noise for exploration<br/><br/>    def learn(self, experiences):<br/>        """Update policy and value parameters using given batch of experience tuples."""<br/>        # Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)<br/>        states = np.vstack([e.state for e in experiences if e is not None])<br/>        actions = np.array([e.action for e in experiences if e is not None]).astype(np.float32).reshape(-1, self.action_size)<br/>        rewards = np.array([e.reward for e in experiences if e is not None]).astype(np.float32).reshape(-1, 1)<br/>        dones = np.array([e.done for e in experiences if e is not None]).astype(np.uint8).reshape(-1, 1)<br/>        next_states = np.vstack([e.next_state for e in experiences if e is not None])<br/><br/>        # Get predicted next-state actions and Q values from target models<br/>        #     Q_targets_next = critic_target(next_state, actor_target(next_state))<br/>        actions_next = self.actor_target.model.predict_on_batch(next_states)<br/>        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])<br/><br/>        # Compute Q targets for current states and train critic model (local)<br/>        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)<br/>        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)<br/><br/>        # Train actor model (local)<br/>        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]), (-1, self.action_size))<br/>        self.actor_local.train_fn([states, action_gradients, 1])  # <br/>        custom training function<br/><br/>        # Soft-update target models<br/>        self.soft_update(self.critic_local.model, <br/>        self.critic_target.model)<br/>        self.soft_update(self.actor_local.model, <br/>        self.actor_target.model)   <br/><br/>    def soft_update(self, local_model, target_model):<br/>        """Soft update model parameters."""<br/>        local_weights = np.array(local_model.get_weights())<br/>        target_weights = np.array(target_model.get_weights())<br/><br/>        assert len(local_weights) == len(target_weights), "Local and <br/>        target model parameters must have the same size"<br/><br/>        new_weights = self.tau * local_weights + (1 - self.tau) * <br/>        target_weights<br/>        target_model.set_weights(new_weights)</span></pre><p id="0fcc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意，在经过一批经验的训练之后，我们可以把我们新学到的权重(从本地模型)复制到目标模型。然而，单个批次可能会在过程中引入大量差异，因此最好执行软更新，由参数tau控制。</p><p id="8aa1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要使这一切正常工作，你还需要一个合适的噪声模型，这将在下面介绍。</p><p id="4dc0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">奥恩斯坦-乌伦贝克噪声:</strong></p><p id="44a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">什么是奥恩斯坦-乌伦贝克过程？用简单的英语来说，它就是一个具有均值回复特性的随机过程。</p><p id="be44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"><em class="kg">dx</em><em class="kg">t</em>=<em class="kg">θ</em>(<em class="kg">μ</em>—<em class="kg">x</em><em class="kg">t</em>)<em class="kg">dt</em>+<em class="kg">σdW</em><em class="kg">t</em></strong></p><p id="8300" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，<em class="kg"> θ </em>表示变量回复到均值的“速度”。<em class="kg"> μ </em>代表均衡或平均值。<em class="kg"> σ </em>是过程的波动程度。</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="87ba" class="jz ka hi jv b fi kb kc l kd ke">import numpy as np<br/>import copy<br/><br/>class OUNoise:<br/>    """Ornstein-Uhlenbeck process."""<br/><br/>    def __init__(self, size, mu, theta, sigma):<br/>        """Initialize parameters and noise process."""<br/>        self.mu = mu * np.ones(size)<br/>        self.theta = theta<br/>        self.sigma = sigma<br/>        self.reset()<br/><br/>    def reset(self):<br/>        """Reset the internal state (= noise) to mean (mu)."""<br/>        self.state = copy.copy(self.mu)<br/><br/>    def sample(self):<br/>        """Update internal state and return it as a noise sample."""<br/>        x = self.state<br/>        dx = self.theta * (self.mu - x) + self.sigma * <br/>        np.random.randn(len(x))<br/>        self.state = x + dx<br/>        return self.state</span></pre><p id="1e02" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">训练:</strong></p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/74bd68c1cf70c9e9da8592c012bf344a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zt79msIpVCn4gzdIhVN9Og.png"/></div></div></figure><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="7fb7" class="jz ka hi jv b fi kb kc l kd ke"><br/>from task import Task<br/>from agents.agent import DDPG<br/>## The task I chose to focus on is to perform a TAKE-OFF operation to height 20<br/>from task import TakeOff<br/><br/># Quadcopter starting position parameters<br/>runtime = 5.                                     # time limit <br/>init_pose = np.array([0., 0., 10., 0., 0., 0.])   # initial pose<br/>init_velocities = np.array([0., 0., 0.])         # initial velocities<br/>init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities<br/>done = False # initialise done bool variable<br/><br/>num_episodes = 1000 # simulation will run for 500 episodes<br/>target_pos = np.array([0., 0., 30., 0., 0., 0.])             # target height is 20<br/><br/># select task of interest<br/>task = TakeOff(init_pose = init_pose,<br/>               init_velocities = init_velocities,<br/>               init_angle_velocities = init_angle_velocities,<br/>               target_pos = target_pos, <br/>               runtime = runtime)<br/># select agent to perform the task of interest<br/>agent = DDPG(task)<br/><br/>file_output1 = 'TakeOff-data.txt' # file name for saved results<br/><br/>labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',<br/> 'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',<br/> 'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4', <br/> 'episode', 'total_reward']<br/>results = {x : [] for x in labels}<br/><br/># Run the simulation, and save the results.<br/># In this while loop, we get both a summary text output from this cell, <br/># and results saved in a txt file<br/>with open(file_output, 'w') as csvfile:<br/>    writer = csv.writer(csvfile)<br/>    writer.writerow(labels)<br/> <br/> for i_episode in range(1, num_episodes+1):<br/>        state = agent.reset_episode() # start a new episode<br/> while True:<br/>            action = agent.act(state) <br/>            next_state, reward, done = task.step(action)<br/>            agent.step(action, reward, next_state, done)<br/>            state = next_state<br/> if done:<br/> # get summary text output<br/> print("\r Total Number of Episode = {:4d}, Total Score = {:7.3f}, Best Recorded Score = {:7.3f}".format(<br/>                    i_episode, agent.total_reward, agent.best_total_reward), end="")  # [debug]<br/> # and save results in text file<br/>                to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(action) + [i_episode] + [agent.total_reward]<br/> for ii in range(len(labels)):<br/>                    results[labels[ii]].append(to_write[ii])<br/>                writer.writerow(to_write)<br/> #print(task.sim.pose[:3])<br/> break<br/>        sys.stdout.flush()</span></pre><p id="fcbb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">验证和性能:</strong></p><p id="1262" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们开发代理时，关注它的表现是很重要的。使用上面的代码作为灵感，建立一个机制来记录/保存每集获得的总奖励。如果剧集奖励逐渐增加，这表明您的代理正在学习。</p><p id="212b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是我得到的！</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es me"><img src="../Images/b61a7be1bb5a2edc620efd8ef06a0ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*UzZWHq2vwaL6uwX1nK0Fbg.png"/></div></figure><p id="2e01" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该代理在学习垂直起飞和悬停到射程内的目标位置方面做得相当不错。取得的最好成绩是-1.78，过去10集的平均成绩是-72.6，最差成绩是-171.1。没有特别的“啊哈”时刻，但很明显，在最后十几集里，代理人在决定一项政策之前，正在尝试不同的方法，这表明代理人实际上经历了一个学习曲线。</p><p id="455f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">概要:</strong></p><p id="f2f5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">恭喜，你终于来了！在这个项目中，我们设法教四轴飞行器如何使用Keras和DDPG算法飞行。虽然DDPG可以学习合理的政策，但我仍然认为它与人类学习驾驶的方式截然不同。例如，我们使用奥恩斯坦-乌伦贝克过程来进行探索。然而，当动作的数量增加时，组合的数量增加，并且进行探索是非常具有挑战性的。</p><p id="5dc8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，说了这么多，算法还是相当强大的，因为我们有一个连续控制的无模型算法，这在机器人学中非常重要。</p><p id="b2a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我的下一篇博客中，我将展示无人机的模拟视频输出。</p><p id="ccc0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">经过一番深思熟虑和复杂的事情后，这里有一个笑话给你:如果你能以某种方式将无人机的速度设置为11.2公里/秒，它甚至可以到达太空:)</strong></p></div></div>    
</body>
</html>