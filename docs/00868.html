<html>
<head>
<title>Gradient Descent through the Mathematical Prism</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过数学棱镜的梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-through-the-mathematical-prism-418cb3bca473?source=collection_archive---------11-----------------------#2019-09-14">https://medium.com/analytics-vidhya/gradient-descent-through-the-mathematical-prism-418cb3bca473?source=collection_archive---------11-----------------------#2019-09-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="c13d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">一个最广泛使用的最优化问题之一，梯度下降的简单数学观点。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/693a2eeba4ee8a3dfbbe7da0dde9d062.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h_T_VOx5o1PTfV4Z"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">迈克尔·泽兹奇在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="287a" class="jv jw hi bd jx jy jz ka kb kc kd ke kf io kg ip kh ir ki is kj iu kk iv kl km bi translated">介绍</h1><p id="8ee7" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv kw kx ky kz la lb lc ld le lf lg lh li hb bi translated">优化问题是任何一种数据科学问题的核心，它本身是数学中一个非常大的领域。由于数学和数据科学密切相关，任何模型的建立都需要一定程度的数学优化。真正意义上的优化意味着“获得最优或最佳解决方案”，在某些情况下可以是最大值，在某些情况下可以是最小值。在我们的例子中，<em class="lj"> n优化问题意味着在给定的一定范围内最大化或最小化一个实值函数。</em>在众多被广泛应用的优化问题中，今天我们来看看它们的母体——梯度下降的真实形式——香草问题。将给出数学背景，在下一篇文章中，我们也将用python从头实现一个。听起来很有趣，那我们走吧！！</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h2 id="c4d4" class="lk jw hi bd jx ll lm ln kb lo lp lq kf kw lr ls kh la lt lu kj le lv lw kl lx bi translated">首先是一些术语:</h2><p id="1a28" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv kw kx ky kz la lb lc ld le lf lg lh li hb bi translated">所以，在我们理解什么是梯度下降以及如何下降之前，让我们先弄清楚一些我们将要用到的基本原理和术语。</p><p id="8838" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated"><strong class="kp hj">权重和偏差:</strong></p><p id="9650" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">因此，对于从线性回归到深度神经网络的每一个模型构建，有一点是共同的，我们试图获得一个假设函数。这实质上绑定了乘以一些权重的输入变量，以便根据需要减少或增加每个变量以及一些偏差或额外常数的重要性。因此，权重是提供给每个输入变量的权重，而偏差是我们引入来控制建模行为的额外常数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es md"><img src="../Images/46897350e2cacadf64c8232ccc43b456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*HsvORLf6R5ewwc8zGjhbMw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">具有n个输入和n个权重的线性假设函数的一般方程。</figcaption></figure><p id="7f23" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated"><strong class="kp hj">损失函数:</strong></p><p id="b7ed" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">变量之间的函数或关系，定义在一行数据或数据点上。它量化了预测某个值所承受的惩罚。基本上，对于给定的数据点和预测，它会告诉你预测的误差。根据算法和问题的类型，损失函数是不同的。例如回归的平方损失、SVM的铰链损失、分类问题的二元损失等。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es me"><img src="../Images/d83c3d9994c05ed9ed9c3d360c4494b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*1nBJNJJsf7hl1m69ltZbaA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">损失函数类似于RMSE</figcaption></figure><p id="8cac" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated"><strong class="kp hj">成本函数:</strong></p><p id="2de2" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">这是上述的一般化形式，考虑了批次中的所有数据点。通常，它是训练数据点上所有损失函数的总和。在某些情况下，一些额外的惩罚也被称为正则化。例如，对于RMSE(均方根误差)、逻辑回归成本函数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mf"><img src="../Images/0f9fe12fd8db8d4d7b4edba05b99bf76.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*IYABSzKRLU8qyRBeo62XSg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">成本函数示例</figcaption></figure><p id="55c8" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated"><strong class="kp hj">目标函数:</strong></p><p id="4432" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">这是成本函数的一般情况，该术语来源于数学优化。它代表一个你想要最大化或最小化的方程或函数。因此，我们可以说成本函数是目标函数的一个特例。</p><p id="431b" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated"><strong class="kp hj">渐变:</strong></p><p id="ca3c" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">梯度是“坡度”的另一种说法。图形中某一点的梯度越高，该点的曲线就越陡。负梯度意味着曲线向下倾斜。从数学上讲，它是曲线在某个给定点的导数。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h2 id="e7ec" class="lk jw hi bd jx ll lm ln kb lo lp lq kf kw lr ls kh la lt lu kj le lv lw kl lx bi translated">示例案例</h2><p id="15ff" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv kw kx ky kz la lb lc ld le lf lg lh li hb bi translated">为了使直觉更简单，我们将采用可以表示线性回归的线性函数或更简单的具有线性激活的感知器模型。因此，我们将假设函数定义如下，该函数可视为一条直线的方程，其中<em class="lj"> y = mx + c。此外，该方程可视为一个多变量输入矩阵乘以权重矩阵并添加偏差矩阵的方程。这里h(x)是当单个输入x被传递给模型时的输出。</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/fcbbe51513de5b211031efcad82b9bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*J62UmS6IcqUzgHm4w1cE1A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">假设函数</figcaption></figure><p id="8c93" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">考虑到这是一个回归问题，损失函数和成本函数如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es me"><img src="../Images/d83c3d9994c05ed9ed9c3d360c4494b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*1nBJNJJsf7hl1m69ltZbaA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">损失函数误差用平方来防止负误差</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mh"><img src="../Images/6017609215269c00be704ccbf286c245.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*3ale7GwKx3OFM6MhF4cvgA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">上述损失函数的成本函数</figcaption></figure><p id="1344" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">如果你仔细观察，成本函数也有1/2乘以常数，这只不过是在对成本函数求导时，从额外的常数归一化方程。</p><p id="6854" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">因此，给定上述等式，我们需要优化成本函数以获得模型的最终对应权重。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h2 id="fc6f" class="lk jw hi bd jx ll lm ln kb lo lp lq kf kw lr ls kh la lt lu kj le lv lw kl lx bi translated">下降开始了…</h2><p id="c84e" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv kw kx ky kz la lb lc ld le lf lg lh li hb bi translated">现在，典型梯度下降的步骤如下所示:</p><ul class=""><li id="eee8" class="mi mj hi kp b kq ly kt lz kw mk la ml le mm li mn mo mp mq bi translated">为上述方程或目标函数曲线上的起点选择一些随机权重和偏差。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mr"><img src="../Images/5d2a3988e1876e1885346c7cb942cc6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*u8I7X5nCtwDkvYLx60rkUg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/crash-course/reducing-loss/gradient-descent</a></figcaption></figure><ul class=""><li id="1b62" class="mi mj hi kp b kq ly kt lz kw mk la ml le mm li mn mo mp mq bi translated">计算曲线上该点的斜率，它只不过是曲线上该点的导数。</li><li id="de36" class="mi mj hi kp b kq ms kt mt kw mu la mv le mw li mn mo mp mq bi translated">通过对函数求偏导数，计算移动到下一步所需的变化。</li><li id="acab" class="mi mj hi kp b kq ms kt mt kw mu la mv le mw li mn mo mp mq bi translated">将这种变化乘以模型的学习率，通常是超参数(alpha)。</li><li id="b5cb" class="mi mj hi kp b kq ms kt mt kw mu la mv le mw li mn mo mp mq bi translated">更新权重和偏差并继续。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mx"><img src="../Images/ebd4a02f3b06dc00433e24958aafe6aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*58Kr0lW-yCNF62tsidq3GQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/crash-course/reducing-loss/gradient-descent</a></figcaption></figure><p id="b61c" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">从数学等式的角度来看，上述步骤意味着什么，这就是我们要研究的内容。简单来说，我们需要将w.r.t以上的成本函数分别部分微分到W和b，计算更新后的权重并继续前进。要在方程式中书写:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es my"><img src="../Images/e84f21bd686f7e4d8a4319087cba2d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*vmau2I8FzekTNQUUxMGvWA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">重量更新</figcaption></figure><p id="139d" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">对于上面偏导数的计算，我们需要简单的导数法则和乘积法则。在求偏导数的时候，只要把其他变量当作常数来求导就行了。下面给出了方法:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mz"><img src="../Images/2141301510babe65f6165a84596a77c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*zbtmAneeRsxL6RItLtYiaw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">成本函数的偏导数</figcaption></figure><p id="b39f" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">类似地，对于其他变量b，我们计算偏导数，给出以下值:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es na"><img src="../Images/787773a0c2b97b7a7bb82e62d63a89f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*qQxYQPYJqfBEoxNjElttew.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">导数输出</figcaption></figure><p id="4f04" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">最后，更新后的权重和偏差如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/b70cbd19a9df71536d4ded4e05b8dc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*K5HHR-L8f7jHJHhTr2i6hw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">重量和偏差更新</figcaption></figure><p id="5005" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">这个更新权重和偏差的过程一直进行到“模型收敛”发生。什么是真正的模型收敛，我们已经找到了我们的成本函数的全局最小值，这实际上告诉我们，给定的权重范围，这是误差最小的状态，因此，给定数据点的精度最高。</p><p id="ca73" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">不仅如此，我们还需要调整步长和学习速率等因素。想要更好的动画例子，我推荐向大师们学习，谷歌:</p><p id="5530" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated"><a class="ae jn" href="https://developers.google.com/machine-learning/crash-course/fitter/graph" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/crash-course/reducing-loss/gradient-descent</a></p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="c891" class="jv jw hi bd jx jy jz ka kb kc kd ke kf io kg ip kh ir ki is kj iu kk iv kl km bi translated">优化的类型</h1><p id="6e98" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv kw kx ky kz la lb lc ld le lf lg lh li hb bi translated">有各种类型的优化算法，也在行业中大量使用。梯度下降优化有自己的多种口味。因此，在我们讨论原始版本之前，让我们先看看其他版本:</p><ul class=""><li id="7457" class="mi mj hi kp b kq ly kt lz kw mk la ml le mm li mn mo mp mq bi translated"><strong class="kp hj">批量梯度下降</strong></li></ul><p id="7b43" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">顾名思义，它会立即对批次进行重量校正，也就是说，只有在整批产品通过后，才会进行重量更新。这是最常用的，在Keras等包中是默认的。</p><ul class=""><li id="c5dc" class="mi mj hi kp b kq ly kt lz kw mk la ml le mm li mn mo mp mq bi translated"><strong class="kp hj">随机梯度下降</strong></li></ul><p id="8fc8" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">这里一次一个数据点，因此它是侵略性的并且计算量也很大。正如预期的那样，会有很多振荡，有可能达不到全局最小值。</p><ul class=""><li id="4d2c" class="mi mj hi kp b kq ly kt lz kw mk la ml le mm li mn mo mp mq bi translated"><strong class="kp hj">小批量梯度下降</strong></li></ul><p id="5ca5" class="pw-post-body-paragraph kn ko hi kp b kq ly ij ks kt lz im kv kw ma ky kz la mb lc ld le mc lg lh li hb bi translated">它把以上两者结合起来，因此在学习第一批漏掉的内容时既不太激进也不太自满。</p><h1 id="e282" class="jv jw hi bd jx jy nc ka kb kc nd ke kf io ne ip kh ir nf is kj iu ng iv kl km bi translated">结论:</h1><p id="dbb3" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv kw kx ky kz la lb lc ld le lf lg lh li hb bi translated">我们研究了普通梯度下降优化及其背后的复杂数学，以计算更新的权重，以及如何使模型收敛以优化权重和偏差。如果您有任何进一步的问题或疑问，请随时评论。<em class="lj">再见！！</em></p></div></div>    
</body>
</html>