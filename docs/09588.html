<html>
<head>
<title>What is Gradient Descent?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是梯度下降？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-is-gradient-descent-e59d981d5cdb?source=collection_archive---------5-----------------------#2020-09-12">https://medium.com/analytics-vidhya/what-is-gradient-descent-e59d981d5cdb?source=collection_archive---------5-----------------------#2020-09-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="6c05" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">概观</h1><p id="167e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">本教程是关于梯度下降的基础。这也是机器学习入门文章“什么是机器学习？”，可以在这里找到<a class="ae kb" rel="noopener" href="/swlh/what-is-machine-learning-ff27b518909b">。</a></p><h1 id="515b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">那么什么是梯度下降呢？</h1><p id="8fb2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">梯度下降是一种为模型寻找最佳权重的方法。我们用梯度下降算法寻找最佳的机器学习模型，误差最低，准确率最高。梯度下降的一个常见解释是，站在一个不平坦的棒球场上，蒙上眼睛，你想找到场地的最低点。很自然地，你会用你的脚一点一点地走到球场的最低点。寻找任何向下的斜坡。从概念上讲，这是我们正在做的事情，以最小化我们的错误，并找到我们最好的机器学习模型。</p><h1 id="a471" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">这和我们第一个教程里的y = mx + b方程有什么关系？</h1><p id="7377" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们可以计算导数、我们的误差，并更新我们的权重(也称为<code class="du kc kd ke kf b">m</code>和<code class="du kc kd ke kf b">b</code>)。</p><h1 id="8bf4" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">例子</h1><p id="3c51" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们开始吧。我们将使用的两个库是numpy和matplotlib。Numpy是一个很棒的数学计算库，而matplotlib用于可视化和绘图。</p><h1 id="36e2" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">导入我们将使用的库</h1><pre class="kg kh ki kj fd kk kf kl km aw kn bi"><span id="0226" class="ko ig hi kf b fi kp kq l kr ks"><em class="kt"># Numpy is a powerful library in Python to do mathemetical computations</em><br/><strong class="kf hj">import</strong> <strong class="kf hj">numpy</strong> <strong class="kf hj">as</strong> <strong class="kf hj">np</strong><br/><br/><em class="kt"># Importing matplotlib for visualizations</em><br/><strong class="kf hj">from</strong> <strong class="kf hj">matplotlib</strong> <strong class="kf hj">import</strong> pyplot <strong class="kf hj">as</strong> plt</span></pre><h1 id="62c9" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">我们可以创建一些符合这个等式的数据:<code class="du kc kd ke kf b">y = 4x + 2</code>其中<code class="du kc kd ke kf b">m</code> = 4，<code class="du kc kd ke kf b">b</code> = 2</h1><p id="c9b5" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">既然我们知道了基本事实，我们就可以根据这个等式创建一些数据。我们还可以将使用梯度下降计算的权重与地面真实值进行比较，其中m = 4，b = 2。</p><pre class="kg kh ki kj fd kk kf kl km aw kn bi"><span id="cd42" class="ko ig hi kf b fi kp kq l kr ks">X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])<br/>y = np.array([6, 10, 14, 18, 22, 26, 30, 34, 38, 42])<br/><br/>plt.plot(X, y)<br/>plt.title('Made up data following the equation y = 4x + 2')<br/>plt.ylabel('y')<br/>plt.xlabel('X')<br/>plt.show()</span></pre><figure class="kg kh ki kj fd kv er es paragraph-image"><div class="er es ku"><img src="../Images/7c175b635396bb562bc9a97bc5985f14.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*_sKTX9Mp5H7n6AkOL1Gyvg.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">(图1)</figcaption></figure><p id="fb71" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated"><strong class="jf hj">现在让我们用Python创建一个梯度下降函数</strong></p><pre class="kg kh ki kj fd kk kf kl km aw kn bi"><span id="6983" class="ko ig hi kf b fi kp kq l kr ks"><strong class="kf hj">def</strong> gradient_descent(x, y, learning_rate, steps):<br/>    <em class="kt"># Randomly initialize m and b. Here we set them to 0.</em><br/>    m = b = 0<br/>    <br/>    <em class="kt"># N is just the number of observations</em><br/>    N = len(x)<br/>    <br/>    <em class="kt"># Creating an empty list to plot how the error changes</em><br/>    <em class="kt"># over time later on with matplotlib.</em><br/>    error_history = list()<br/>    <br/>    <em class="kt"># Loop through the number of iterations specified to get closer to the optimal model</em><br/>    <strong class="kf hj">for</strong> i <strong class="kf hj">in</strong> range(steps):<br/>        <em class="kt"># Since y = mx + b, we predict y is going to be m * x + b</em><br/>        y_predicted = (m * x) + b<br/>        <br/>        <em class="kt"># We calculate the error for each model we try, attempting to get the least amount of error possible</em><br/>        <em class="kt"># In this case we calculate the mean squared error (MSE)</em><br/>        error = (1 / N) * sum([value ** 2 <strong class="kf hj">for</strong> value <strong class="kf hj">in</strong> (y - y_predicted)])<br/>        <em class="kt"># Append to the error history, so we can visualize later</em><br/>        error_history.append(error)<br/>        <br/>        <em class="kt"># Calculate the partial derivatives for m and b</em><br/>        dm = (-2 / N) * sum(x * (y - y_predicted))<br/>        db = (-2 / N) * sum((y - y_predicted))<br/>        <br/>        <em class="kt"># Update m and b based on the partial derivatives and the specified learning rate</em><br/>        m = m - learning_rate * dm<br/>        b = b - learning_rate * db<br/>        <br/>        <em class="kt"># Print the step number, error, and weights</em><br/>        print(f"Step {i + 1} <strong class="kf hj">\n</strong> Error = <strong class="kf hj">{error}</strong> <strong class="kf hj">\n</strong> m = <strong class="kf hj">{m}</strong> <strong class="kf hj">\n</strong> b = <strong class="kf hj">{b}</strong> <strong class="kf hj">\n</strong>")<br/>        <br/>    <strong class="kf hj">return</strong> m, b, error_history</span></pre><h1 id="412c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">让我们来分解这个函数</h1><ol class=""><li id="170e" class="lh li hi jf b jg jh jk jl jo lj js lk jw ll ka lm ln lo lp bi translated">首先我们设置m和b等于0。这是为了随机初始化这些变量，您可以将它们设置为您想要的任何值。</li></ol><p id="72b5" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated">2.然后我们找到我们有多少数据点，并把它作为变量<code class="du kc kd ke kf b">N</code></p><p id="b81a" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated">3.我们创建一个空数组来保存错误的历史(记住，错误是我们预测的值和实际值之间的差)</p><p id="b73e" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated">4.最后，我们创建梯度下降循环:</p><ul class=""><li id="4dbf" class="lh li hi jf b jg lc jk ld jo lq js lr jw ls ka lt ln lo lp bi translated">首先，在循环中，我们计算我们的预测点，也就是mx + b中的y。因此，我们以<code class="du kc kd ke kf b">y_predicted = mx + b</code>结束</li><li id="37d4" class="lh li hi jf b jg lu jk lv jo lw js lx jw ly ka lt ln lo lp bi translated">我们计算每个预测点的误差。在这种情况下，我们使用均方误差(MSE)。计算方法是找出误差，然后求平方，再取所有平方误差的平均值。</li><li id="e4bb" class="lh li hi jf b jg lu jk lv jo lw js lx jw ly ka lt ln lo lp bi translated">然后，我们将MSE添加到历史数组中，以便以后可视化。</li><li id="413a" class="lh li hi jf b jg lu jk lv jo lw js lx jw ly ka lt ln lo lp bi translated">现在，我们为梯度下降算法做一些非常重要和基本的事情——我们计算权重的偏导数。偏导数只是保持其他变量不变，而你要弄清楚你所观察的变量在这个过程中是如何表现的。</li><li id="9c3b" class="lh li hi jf b jg lu jk lv jo lw js lx jw ly ka lt ln lo lp bi translated">然后我们根据偏导数和我们指定的学习率更新我们的权重<code class="du kc kd ke kf b">m</code>和<code class="du kc kd ke kf b">b</code>。(学习速度是你迈出的一大步)</li><li id="07e7" class="lh li hi jf b jg lu jk lv jo lw js lx jw ly ka lt ln lo lp bi translated">最后我们打印出值并返回变量。</li></ul><p id="260f" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated"><em class="kt">公式:</em></p><figure class="kg kh ki kj fd kv er es paragraph-image"><div class="er es lz"><img src="../Images/2cc79d11dcbde24af7ee49b116371516.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*Oo9UqEguXmTwGeyQs8Y73A.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">(公式)</figcaption></figure><p id="a526" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated">我们可以指定梯度下降算法要运行多少个“步骤”。基本上，你被蒙住眼睛时可以在棒球场上走多少步。</p><pre class="kg kh ki kj fd kk kf kl km aw kn bi"><span id="542e" class="ko ig hi kf b fi kp kq l kr ks">steps = 5</span></pre><p id="3d5d" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated">我们还可以指定“学习率”，这将告诉我们这些步骤有多大。从概念上讲，想想每次你在棒球场上迈出一步时，你被允许迈出多大的一步。太大的一步可能会导致你在场地的最低点拍摄过度。但是，过小的一步会让你花更长的时间找到最低点。这个变量在深度学习中被广泛讨论。现在我们将它设置为0.01。</p><pre class="kg kh ki kj fd kk kf kl km aw kn bi"><span id="1f68" class="ko ig hi kf b fi kp kq l kr ks">learning_rate = 0.01</span></pre><p id="0497" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated">现在让我们通过梯度下降运行我们的虚构数据，看看会弹出什么样的<code class="du kc kd ke kf b">m</code>和<code class="du kc kd ke kf b">b</code>值。</p><pre class="kg kh ki kj fd kk kf kl km aw kn bi"><span id="710f" class="ko ig hi kf b fi kp kq l kr ks">m, b, error_history = gradient_descent(X, y, learning_rate, steps)</span></pre><blockquote class="ma mb mc"><p id="ca76" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi translated">[输出]</p><p id="8b39" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi translated">第一步</p><p id="44ea" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi translated">误差= 708.0，m = 3.3000000000000003，b = 0.48</p><p id="7ec2" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi">…</p><p id="3ede" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi translated">第五步</p><p id="8f48" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi translated">误差= 0.40731798550902065，m = 4.194540273，b = 0.6321866478</p></blockquote><p id="a720" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated">注意，误差继续减小，并且<code class="du kc kd ke kf b">m</code>接近4，而<code class="du kc kd ke kf b">b</code>接近2。如果我们增加算法允许采取的步骤数，它将更接近真实值。</p><pre class="kg kh ki kj fd kk kf kl km aw kn bi"><span id="c7d8" class="ko ig hi kf b fi kp kq l kr ks">steps = 10<br/>m, b, error_history = gradient_descent(X, y, learning_rate, steps)</span></pre><blockquote class="ma mb mc"><p id="e4e7" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi translated">[输出]</p><p id="553a" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi translated">第一步</p><p id="53c6" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi translated">误差= 708.0，m = 3.3，b = 0.48</p><p id="4fa3" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi">…</p><p id="cd04" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi translated">第十步</p><p id="36e8" class="jd je kt jf b jg lc ji jj jk ld jm jn md le jq jr me lf ju jv mf lg jy jz ka hb bi translated">误差= 0.3875，m = 4.19234，b = 0.66093</p></blockquote><p id="d7a0" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated">您可以看到，随着更多的步骤，我们越来越接近4和2的真实<code class="du kc kd ke kf b">m</code>和<code class="du kc kd ke kf b">b</code>值！</p><h1 id="fec4" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">让我们看看在每一步之后错误是如何变化的</h1><pre class="kg kh ki kj fd kk kf kl km aw kn bi"><span id="14fc" class="ko ig hi kf b fi kp kq l kr ks">plt.plot(range(steps), error_history)<br/>plt.title('Error over time')<br/>plt.ylabel('Error')<br/>plt.xlabel('Number of Steps')<br/>plt.show()</span></pre><figure class="kg kh ki kj fd kv er es paragraph-image"><div class="er es mg"><img src="../Images/7b26a52b57f3842a28ba9e7dfc3d6faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*n1CbYtZqG12sNYNpuf7K4Q.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">(图2)</figcaption></figure><p id="69b0" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated">我们可以看到，它急剧下降。然后慢慢变低。在训练深度学习或其他机器学习模型时，你会经常看到这种类型的情节。</p><h1 id="2f77" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">现在让我们看看梯度下降算法得出的线，用实际的线绘制。</h1><pre class="kg kh ki kj fd kk kf kl km aw kn bi"><span id="a9b1" class="ko ig hi kf b fi kp kq l kr ks">y_predicted = (m * X) + b<br/><br/>plt.plot(X, y)<br/>plt.plot(X, y_predicted)<br/>plt.title('Estimation Using Gradient Descent')<br/>plt.ylabel('y')<br/>plt.xlabel('X')<br/>plt.legend(["Actual", "Estimated using Gradient Descent"])<br/>plt.show()</span></pre><figure class="kg kh ki kj fd kv er es paragraph-image"><div class="er es ku"><img src="../Images/d9cab8f170701aabeb54fbafb2c4d88e.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*CgXLltI41vJeKDZ_yu0iAw.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">(图3)</figcaption></figure><p id="3e6e" class="pw-post-body-paragraph jd je hi jf b jg lc ji jj jk ld jm jn jo le jq jr js lf ju jv jw lg jy jz ka hb bi translated">我们可以看到梯度下降算法做得非常好！如果我们允许梯度下降运行更长时间，它将能够更接近这个图中的实际函数。</p><h1 id="b581" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">摘要</h1><p id="c2cc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">总之，梯度下降是一种优化机器学习模型的算法。它试图找到模型的最佳权重，以逼近在数据中找到的函数。随着时间的推移，误差将(有希望)减少，准确性将提高。为此，梯度下降算法计算权重的偏导数。这些偏导数告诉算法在哪个方向更新权重。而学习率告诉我们，在偏导数给定的方向上，权重允许采取多大的步长。</p><h1 id="4eac" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">个人简历</h1><p id="f29e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Frankie Cancino是Target的高级人工智能科学家，住在旧金山湾区，是数据科学明尼阿波利斯集团的创始人。</p><h1 id="cca6" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">链接</h1><ul class=""><li id="fb7d" class="lh li hi jf b jg jh jk jl jo lj js lk jw ll ka lt ln lo lp bi translated"><a class="ae kb" href="https://github.com/frankiecancino/ML_Tutorials/blob/master/gradient_descent.ipynb" rel="noopener ugc nofollow" target="_blank">原Jupyter笔记本</a></li><li id="863a" class="lh li hi jf b jg lu jk lv jo lw js lx jw ly ka lt ln lo lp bi translated"><a class="ae kb" rel="noopener" href="/swlh/what-is-machine-learning-ff27b518909b">什么是机器学习？</a></li><li id="96be" class="lh li hi jf b jg lu jk lv jo lw js lx jw ly ka lt ln lo lp bi translated"><a class="ae kb" href="https://www.linkedin.com/in/frankie-cancino/" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="fee2" class="lh li hi jf b jg lu jk lv jo lw js lx jw ly ka lt ln lo lp bi translated"><a class="ae kb" href="https://twitter.com/frankiecancino" rel="noopener ugc nofollow" target="_blank">推特</a></li></ul></div></div>    
</body>
</html>