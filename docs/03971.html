<html>
<head>
<title>Ensemble Modeling : Unwrapping the Basic Exact Greedy Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成建模:展开基本的精确贪婪算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/boosting-models-unwrapping-the-basic-exact-greedy-algorithm-b67d88c7189a?source=collection_archive---------7-----------------------#2020-02-28">https://medium.com/analytics-vidhya/boosting-models-unwrapping-the-basic-exact-greedy-algorithm-b67d88c7189a?source=collection_archive---------7-----------------------#2020-02-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/5b63e79ded400d0d4d5744397a697161.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8hM_mVddBytACjwl"/></div></div></figure><div class=""/><p id="f849" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> <em class="jo"> ~ AI和机器学习将提高一切事物的标准。现在不关注的会掉队~一个小组成员</em>T3】</strong></p><p id="32af" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi jp translated">当一个人正在学习如何使用一种新的设备对一个静止的目标发射任何类型的弹药时，他/她会错过击中精确的内部或他/她瞄准的地方。在进行第一次尝试后，这个人走向目标，观察子弹击中的位置，以便用其他因素校准角度，并调整他的第一次尝试。将进行几次尝试，直到射手最终击中目标。进行多次试验的想法是从每一步中学习，并调整前一步产生的误差，直到尽可能精确地击中目标。</p><p id="95e1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种想法可以用来理解boosting背后的数学，boosting是集成建模技术之一。然而，更重要的是一步一步地深入研究基本的精确贪婪算法，该算法用于在树学习中寻找最佳分裂，例如boosting模型。</p><h1 id="f5f7" class="jy jz ht bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">介绍集合模型</h1><p id="c2a4" class="pw-post-body-paragraph iq ir ht is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">集成模型是聚合的简单决策树或基础学习器，有助于控制偏差或方差。三个公认的但不限于合奏模特的家族是:</p><ol class=""><li id="88e7" class="lb lc ht is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated">Bagging:通过并行生成独立的决策树来控制方差。它在训练基础学习者时用替换法采样数据。因此，树重复地适合观察值的自举子集。最终预测是基础学习者的平均输出(回归)或多数投票(分类)。例如，RandomForests。</li><li id="6b0d" class="lb lc ht is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">Boosting:通过构建顺序树来减少前一个树的错误，从而控制偏差。在boosting中，特定树的增长会考虑已经增长的其他树，这些树是拟合到前一个模型的残差的可加模型。加法模型在它们之前被添加到原始模型，并且该过程继续进行，直到模型收敛(达到最优解或最低分数，例如均方误差)为止。例如，梯度增强和极限梯度增强(XBoost)。</li><li id="0025" class="lb lc ht is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">堆叠:使用不同的模型预测作为新模型中的新特征。</li></ol><p id="1dc0" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本文中，我将讨论一个非常简单的框架来理解boosting如何使用精确的贪婪算法真正工作。然而，梯度下降(上升)的引入使得具有n维数据的推进模型非常复杂。因此，在使用基于任何技术的高级算法之前，理解这些技术的基础是至关重要的。如果从业者不知道模型背后的基础知识，就不应该使用模型。因此，在跳到梯度推进模型或可扩展学习系统(如著名的XBoost)之前，理解贪婪算法的工作原理是至关重要的。</p><blockquote class="lp"><p id="6ff0" class="lq lr ht bd ls lt lu lv lw lx ly jn dx translated">平均是第一个和基线模型！</p></blockquote><h1 id="2449" class="jy jz ht bd ka kb kc kd ke kf kg kh ki kj lz kl km kn ma kp kq kr mb kt ku kv bi translated">精确贪婪算法</h1><h2 id="386d" class="mc jz ht bd ka md me mf ke mg mh mi ki jb mj mk km jf ml mm kq jj mn mo ku mp bi translated">第一个模型</h2><p id="d316" class="pw-post-body-paragraph iq ir ht is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">在解释精确贪婪算法背后的机制之前，让我们先来看看一个非常重要的模型，叫做平均！</p><figure class="mr ms mt mu fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/27e01edc7ad9853c8d32869ff21599dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*pjNdrl1CjiuQuD2mIyo8RA.png"/></div><figcaption class="mv mw et er es mx my bd b be z dx translated">最小化误差目标函数</figcaption></figure><p id="d1e4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">设α是预测变量y的期望值，目标是使α与y的实际值的平方和最小。</p><p id="1384" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对SE目标函数求微分，目的是使其最小化，这将使我们得出结论，Y(α)的期望值实际上是所有真实Y的平均值。因此，平均值是第一个已知的模型，直到优化被引入计算算法，模型变得复杂。</p><p id="85c8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在讨论了第一个模型——平均之后，现在我们将深入讨论精确的贪婪算法。在贪婪算法中，将不断使用平均来寻找最佳分割，这将最小化每次迭代的误差平方和。我将使用excel演示该算法，目的是理解它是如何工作的，因为scikit-learn boosting模型已经支持该算法，并且用python构建boosting模型相当简单。</p><h2 id="0b4f" class="mc jz ht bd ka md me mf ke mg mh mi ki jb mj mk km jf ml mm kq jj mn mo ku mp bi translated">该算法</h2><p id="0873" class="pw-post-body-paragraph iq ir ht is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">算法不叫贪得无厌！贪婪算法是在树学习中寻找变量之间和每个变量内部的最佳分割的许多技术之一。这个过程是通过枚举每个变量中所有可能的分裂来完成的。根据相对于最优解使用的损失函数(梯度下降、平方误差、绝对误差……)选择最佳分割。最优解将分别是梯度损失函数的最优梯度和平方误差成本函数的最小平方误差和。首先，该算法按升序对连续特征(变量)进行排序，然后查找该变量的每两行之间的分割统计。如果变量有n行，则算法枚举n-1个拆分。如果数据有m个特征，那么我们有(n-1)*m个分裂。</p><p id="483a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从这个计算要求很高的过程中截取一小部分，我们观察它在boosting模型中是如何工作的。升压模型的步骤是:</p><ol class=""><li id="79c6" class="lb lc ht is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated">如果MSE是损失函数，则通过取Y变量的平均值来初始化基本模型。如果使用MAE(平均绝对误差),那么初始模型将是Y列的中间值。</li><li id="8b98" class="lb lc ht is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">计算残差(Y-初始模型预测)</li><li id="3914" class="lb lc ht is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">贪婪算法通过寻找给出最低SSE的变量的最佳分割来计算第一个加法模型。X要素中的特定分割用于计算分割前后残差的平均值。</li><li id="7ee5" class="lb lc ht is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">加法模型被添加(拟合)到每一行的残差，以给出新的函数，该函数具有比初始函数更低的MSE。</li><li id="3db8" class="lb lc ht is b it lk ix ll jb lm jf ln jj lo jn lg lh li lj bi translated">该过程继续进行，其中几个其他加性模型被拟合以给出更新的模型，直到损失函数收敛，其中MSE不再显著变化。</li></ol><p id="b1c1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看一个例子，其中Y是一些假设的变量，我们试图使用一个假设的特征X来预测Y。</p><h2 id="098c" class="mc jz ht bd ka md me mf ke mg mh mi ki jb mj mk km jf ml mm kq jj mn mo ku mp bi translated">贪婪算法助推应用</h2><figure class="mr ms mt mu fd hk er es paragraph-image"><div class="er es mz"><img src="../Images/ce60e8f513d1eae23812e5828b6f1f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*wxqo3gl3ShL-X5eVc-2QzA.png"/></div><figcaption class="mv mw et er es mx my bd b be z dx translated">初始数据集被呈现给boosting模型</figcaption></figure><figure class="mr ms mt mu fd hk er es paragraph-image"><div class="er es na"><img src="../Images/10599af55f01ff598709c270c04f3fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*rHUlQ1jXtoObJejdlIeG-Q.png"/></div><figcaption class="mv mw et er es mx my bd b be z dx translated">x变量被排序。F0 = avg(Y ),因为损失函数是MSE。计算残差。</figcaption></figure><figure class="mr ms mt mu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nb"><img src="../Images/f4e874a9f01c3c9779951cac0279b2f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABdrKQWGOjgQb0XzfEK7BQ.png"/></div></div><figcaption class="mv mw et er es mx my bd b be z dx translated">贪婪算法在x的行之间计算加法函数h1。选择具有最低SSE的分割来拟合F0上的h1。</figcaption></figure><p id="5b60" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计算F1的残差(Y-F1)。一个新的附加模型被安装在它们上面。</p><figure class="mr ms mt mu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nc"><img src="../Images/d9335326e58fdc0fddd59520eea19030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OD_EBXcxCg_yu3LL-9IceA.png"/></div></div><figcaption class="mv mw et er es mx my bd b be z dx translated">通过将新的加性模型h2添加到先前的F1模型，第二次提升函数F2。贪婪算法在迭代8时检测到此处的分裂(即第8行和第9行之间)。</figcaption></figure><figure class="mr ms mt mu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nd"><img src="../Images/b78d24c95d28610498421f9b08d749a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gTRPae7D9azmqZgWN22cag.png"/></div></div><figcaption class="mv mw et er es mx my bd b be z dx translated">假设这是最后一步，最好的模型F3具有最低的MSE。流程和之前一样。</figcaption></figure><p id="5f61" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该过程可以继续回归新的加法模型，并在推导新的模型之前将它们拟合到函数的残差上，直到MSE不再变化。</p><figure class="mr ms mt mu fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ne"><img src="../Images/8c91ff2e8106522f66e0910098e8eb39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bFbCKeKsoXi8Q3ngfyd5Uw.png"/></div></div></figure><p id="5977" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是我们在后台使用精确贪婪算法的boosting过程的总结。注意每个新模型的MSE是如何低于前一个模型的。只要MSE不断减小，就可以得到更多的函数，但要小心过度拟合！因此，一些boosting引入了正则化来控制拟合，而其他boosting则添加了并行学习来加快计算速度，更不用说稀疏感知分裂查找算法了。</p><p id="4003" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于梯度增强模型，上面讨论的思想可以被推广，其中为每个分裂计算损失函数的梯度，并且在贪婪算法迭代的每一步以指定的学习速率将加性函数拟合到梯度。学习速率可以是自适应的，以便不会错过损失梯度函数的最佳点或最小点。</p><figure class="mr ms mt mu fd hk er es paragraph-image"><div class="er es nf"><img src="../Images/e9f2e2782f79b1da62da497f60294222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*HwpFPnJ1_Oi4fDDsLuuEvA.png"/></div><figcaption class="mv mw et er es mx my bd b be z dx translated">XBoost文档:梯度提升中的贪婪算法。g和H分别是损失函数的一阶和二阶梯度统计量。</figcaption></figure><p id="dc55" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">至于如果Y是类别变量，损失函数将是二元交叉熵或二项式偏差损失函数，其中假设模型预测每个类别的概率分布。</p><h1 id="8cba" class="jy jz ht bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论</h1><p id="fec2" class="pw-post-body-paragraph iq ir ht is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">在为模型提供变量并致力于获得良好的性能之前，理解模型如何工作背后的基础知识是非常重要的。统计模型变得越来越复杂，新的工具被用来处理高维数据集。建模开始使用平均，直到优化被引入，难以可视化的理论目标函数可以通过计算解决，而不是看起来像一个黑箱。因此，在跳到boosting之前，人们应该知道树学习是如何构造的，如何找到数据的最佳分割，以及几个算法背后的逻辑，这些算法使模型吐出预测。</p><p id="7228" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你想要excel表格，请随时在LinkedIn上联系我！</p></div></div>    
</body>
</html>