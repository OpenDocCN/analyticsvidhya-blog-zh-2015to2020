<html>
<head>
<title>Computer Vision Part 6: Semantic Segmentation, classification on the pixel level.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机视觉第6部分:语义分割，像素级分类。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/computer-vision-part-6-semantic-segmentation-classification-on-the-pixel-level-ee9f5d59c1c8?source=collection_archive---------4-----------------------#2019-12-29">https://medium.com/analytics-vidhya/computer-vision-part-6-semantic-segmentation-classification-on-the-pixel-level-ee9f5d59c1c8?source=collection_archive---------4-----------------------#2019-12-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/480d65f292b10f8f4f49e4430824d46f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fN-N_iekKkLGMTL6.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用Deeplab进行语义分割</figcaption></figure><p id="b596" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在前两章中，我们广泛讨论了如何构建神经网络架构，以及这些架构背后的基本原理，以对图像进行分类或检测图像中的对象，并在这些检测到的对象周围绘制边界框。通过查看每个像素并确定它属于哪个对象或类，可以在粒度上变得更细。如这里简要概述的<a class="ae js" rel="noopener" href="/overture-ai/part-2-overview-of-computer-vision-methods-69c56843c567"/>和下面看到的，<strong class="iw hj">语义分割</strong>将把<strong class="iw hj">每个像素分配到一个类别</strong>，但是不会区分同一类别内的多个出现<strong class="iw hj">，而<strong class="iw hj">实例分割</strong>进行这种区分，并且<strong class="iw hj">识别类别</strong>内的唯一出现。</strong></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/4e6514048555d82ad44ee36a190a52f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*NTxgIukL1EJ86Uw_X0Xt_w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">语义分割与实例分割。</figcaption></figure><h1 id="46dc" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">1.概念</h1><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/1a00683ef16a049ccdc39b9c9b566de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wWOTphs4Qt8P2xPqvucnUg.png"/></div></div></figure><p id="c3b2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们可以简单地堆叠一组<strong class="iw hj">卷积</strong>层，如我们所知，图像中的局部特征被捕获，创建一个层次集来提取更广泛的结构。通过连续的卷积层捕捉图像中日益复杂的特征，CNN可以将图像编码为其内容的紧凑表示。该架构然后通过这种分层表示来学习输入图像与其相应分割输出之间的直接<strong class="iw hj">映射</strong> <strong class="iw hj">。为了实现conv层的这种连接，必须在所有层上使用相同的填充，从而保持输入图像的分辨率，因此大大增加了计算成本。</strong></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kx"><img src="../Images/c249172b835d36a5a565ad319d8d2eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0U0d1L2AEWHGv5YZl_mBfw.png"/></div></div></figure><p id="4789" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从之前的系列中，我们知道通过<strong class="iw hj">连续的卷积层</strong>，CNN可以通过增加特征地图的数量来捕捉图像中日益复杂的特征<strong class="iw hj">。此外，通过使用汇集和/或步长卷积来压缩空间分辨率导致了较低的计算负荷。这种<strong class="iw hj">编码器配置</strong>非常适合图像分类任务，因为它只关心图像的<strong class="iw hj">内容，而不关心位置</strong>。然而，对于<strong class="iw hj">分割</strong>任务，有必要使<strong class="iw hj">具有像素方式预测</strong>的全分辨率掩模。因此，具有如上所示的<strong class="iw hj">编码器-解码器</strong>结构的架构并不少见。</strong></p><p id="8810" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">卷积解码器获取卷积编码器的低分辨率输出，并将对其进行上采样</strong>。该解码器的最后一步是生成一个存储图像的逐像素标记的数组。上采样是如何发生的？直观地说，我们可以进行池化(取消池化)的反向操作，即获取单个像素值，并将其值分布到更高的分辨率上。如果架构能够学习如何最好地对像素进行上采样会怎么样？这就是转置卷积发挥作用的地方。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/4c2663ba52f3661f99e90701de5a1a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Uwuqf7dNVbrEpgeT.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">卷积与转置卷积</figcaption></figure><p id="52b6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如上所示，卷积采用输入的点积，转置卷积采用输入并乘以所有权重。转置卷积可以容易地实现，因为卷积的向前和向后传递简单地颠倒。</p><h1 id="33fa" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">2.结构</h1><h1 id="7dad" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">2.1.全卷积网络</h1><p id="eb9c" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated">在<a class="ae js" href="https://arxiv.org/pdf/1411.4038.pdf" rel="noopener ugc nofollow" target="_blank"> 2015中，龙等人</a>介绍了第一种通过调整分类网络 (AlexNet、GoogleNet和)并针对<strong class="iw hj">分割</strong>任务对其进行微调来训练<strong class="iw hj">全卷积网络的方法。</strong></p><p id="88fb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">通过<strong class="iw hj">将全连接层转换成跨越整个输入区域的卷积核</strong>，使分类器架构适应全卷积版本。这种转换有一个重要的方面，因为这些转换的卷积中的每一个都将<strong class="iw hj">输出标签的粗略热图</strong>，这使得向后和向前传递的训练都非常简单。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/13fe2302535f8a7b7950312620c72b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y1SzQyD6YFAipOrZc7-lcg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">FCN建筑概述。省略卷积层和转换的全连接层。</figcaption></figure><p id="c67f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">上面我们可以观察到FCN的建筑。我们看到<strong class="iw hj">编码器会将</strong>图像压缩成分辨率更低的图像。代表汇集图层的格网也显示了相对的空间粗糙度。在第5个池层之后，分辨率降低了32倍，这导致了上采样操作之后的粗略分割。本文讨论了“是什么”和“是什么”之间的平衡问题。为了理解图像中存在什么，全局(粗略)信息是必要的，而为了精确定位图像中存在什么，局部(精细)信息是必要的。因此，<strong class="iw hj">跳过连接</strong>(与resnets相似)和<strong class="iw hj">慢速上采样</strong>的组合。</p><h1 id="8830" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">2.2.优信网</h1><p id="da10" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated">在FCN的基础上，<a class="ae js" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank"> O. Ronneberger et al. (2015) </a>提出了一个网络，<em class="lf">“由一个捕获上下文的收缩路径和一个</em> <strong class="iw hj"> <em class="lf">对称</em> </strong> <em class="lf">扩展路径组成，能够实现精确定位。”</em>和训练策略“<em class="lf">，它依赖于对数据扩充的大量使用，以便更有效地使用可用的带注释的样本</em></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/672d24589f0cae487d2c8efc2ec73ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N8rxON9YJxSiffAMXajcog.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">U-net架构，其中蓝框对应于多通道特性图，通道数量在这些框的顶部标出。方框的左下边缘表示x-y尺寸，白色方框表示复制的特征地图。</figcaption></figure><p id="4024" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上图可以看出，第一部分是卷积、ReLU和max池操作的集合。步长为2的2x2最大池操作将导致下采样步骤，其中特征通道的数量加倍，大小减半。第二部分包括对特征图进行上采样、连接相应的裁剪后的特征图的序列，这是必要的，因为在每次卷积、卷积和应用ReLU之后会丢失边界像素。</p><p id="594f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">与FCN类似，<strong class="iw hj">来自压缩路径的高分辨率特征与上采样输出</strong>相结合，然后被馈送到一系列卷积层。与FCN的主要区别在于，在上采样部分，存在大量特征通道，这允许网络<strong class="iw hj">将上下文信息传播到更高分辨率的层</strong>。此外，网络不包含完全连接的层，而仅使用卷积输出。</p><h1 id="2861" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">2.3.FC-DenseNet</h1><p id="64fa" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated">FC DenseNet 或100层提拉米苏是一种基于DenseNet架构的图像分类分割技术。DenseNet基于从早期层到后来层的快捷连接的范例。DenseNet的特别之处在于<strong class="iw hj">所有层都相互连接</strong>。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/82430890e18b7350d0413aa8d68e919e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MTurWHmQki01AE16WuStAw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">增长率为k = 4的5层密集块，其中k是指后续层的通道数。每一层都将所有前面的特征图作为输入。</figcaption></figure><p id="8ea1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">每个层都将其自己的特征映射传递给所有后续层。ResNet使用元素相加来组合特性，DenseNets使用<strong class="iw hj">串联</strong>。因此，每一层都从所有前面的层接收一组知识。也许与直觉相反，这比传统方法需要更少的参数，因为不需要重新学习冗余的特征图。</p><p id="12e7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面，我们将看到该架构的概述。每一层产生一个k输出特征图，它与前面提到的增长率相一致。然后，这被馈送到下一层的密集块的瓶颈中，以减少输入特征映射的数量，从而提高其各自的复合块的计算效率。在传递到下一层之前，要素地图通过过渡层进行压缩。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/fd6534567ebba8d3b417c61d98d5f152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-yXFvOF7tu6tcuqjVoeJxA.png"/></div></div></figure><p id="c604" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面，我们可以观察DenseNet如何用于创建FC-DenseNet进行图像分割。我们可以在以前的FCN和U-Net中发现类似的编码器-解码器结构。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/e00caec6cf3faf969b70a84658758443.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5p9hwBW8dk_4nb5cyQg_VA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">FC-DenseNet的体系结构</figcaption></figure><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/00a0524dc897d59acf7690d529a3e948.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*FsYSwf4X9hfQQpr98-G0kg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">4层密集块示意图</figcaption></figure><p id="6fcd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于分割，密集块仅包含一系列复合块，其中应用了0.2的丢失。必须注意不要出现特征地图的<strong class="iw hj">爆炸。</strong>这样，每一层仅连接先前的连接。最后一层执行所有层的输出的连接，因此包含4∫k个特征图。在密集块之后发生的过渡上/下卷积仅应用于由最后一个密集块获得的特征图，这是因为特征数量的线性增长对存储器的要求太高。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/905d267aac7c0c7f9beb636f9f6a9615.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Ns7oIBrg03r7b2Jnk3T5Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">构建向下/向上转换卷积的模块</figcaption></figure><p id="da8b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">由于共用层，来自早期密集块的一些<strong class="iw hj">信息丢失。因此，跳过连接有助于我们重用特征地图，其中上采样路径可以从下采样路径恢复一些空间详细信息。</strong></p><h1 id="99c2" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">2.4.DeepLab</h1><p id="09f2" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated">没有一篇声誉好的帖子敢不提及谷歌对这一主题的挑战就不完整。因此，按照谷歌的工作方式，我们将看看2015年首次亮相的主要概念的一系列有趣改进。</p><h2 id="a7c7" class="lm jz hi bd ka ln lo lp ke lq lr ls ki jf lt lu km jj lv lw kq jn lx ly ku lz bi translated"><strong class="ak"> 2.4.1。DeepLab v1 </strong></h2><p id="20d0" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated">DeepLab v1 <a class="ae js" href="https://arxiv.org/pdf/1606.00915.pdf" rel="noopener ugc nofollow" target="_blank">介绍</a> 2主要思想:<strong class="iw hj">阿特鲁卷积和全连通条件随机场(FC CRF) </strong>。下面，我们找到了架构:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/d68f41ca1b621d9ca437104bce83d28a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IfnkKJ49mX9N71zH7ypjPA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">深度卷积神经网络(DCNN)中枢(VGG或ResNet)与atrous卷积一起用于对信号进行下采样。然后，将特征地图放大到原始尺寸。最后，使用模糊条件随机场来改进分割结果。</figcaption></figure><p id="0130" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">阿特鲁卷积来自法语“à trous ”,意思是洞。阿特鲁卷积也被称为<strong class="iw hj">扩张卷积</strong>。下面，我们看到扩张卷积是一个标准卷积，其中您<strong class="iw hj">在两个维度上均匀地跳过一些像素</strong>。更准确地说，下面我们观察到速率为2的稀释卷积，它从输入到卷积核每2个像素采样一次。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/0d0aff29af590bcec49089ca1fb6f222.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*5KHoU-pZBa4HCM-v.gif"/></div></figure><p id="b9cc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们看到，与标准等效物相比，扩张卷积的<strong class="iw hj">感受野更大。你可能想知道为什么这是相关的？</strong></p><p id="5a63" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">所有先前讨论的架构都使用依赖于空间池的多尺度CNN来创建编码器-解码器类型的结构。这样做是为了结合和平衡:</p><ul class=""><li id="4a3f" class="mc md hi iw b ix iy jb jc jf me jj mf jn mg jr mh mi mj mk bi translated">像素级精度</li><li id="aef7" class="mc md hi iw b ix ml jb mm jf mn jj mo jn mp jr mh mi mj mk bi translated">图像的全局知识</li></ul><p id="e003" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">DeepLab使用扩展层来解决这个平衡问题，而不是典型的池化。通过<strong class="iw hj">控制复杂卷积中的视场</strong>，我们可以在精确定位(小视场)和<strong class="iw hj">上下文同化</strong>(大视场)之间找到最佳的<strong class="iw hj">折衷，而不会过多增加参数的数量。这可以从下面看出:</strong></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/933757018c6d4f321620cefb120f3ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MU5HTNYvs1W2TQBPoW8Idw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">多层扩张的回旋，其中扩张因子在每层之后呈指数增加，导致也呈指数增加的有效感受野。感受野以比参数数量(线性)更快的速度(指数)增长。</figcaption></figure><p id="2b71" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一旦创建和升级了特征地图，就应用FC CRF。CRF是一种<strong class="iw hj">统计学习方法</strong>，它将上下文考虑在内。这种上下文可以理解为预测之间的依赖关系。在自然语言处理中，CRF是预测的顺序相关性，而在计算机视觉中，邻近像素被定义为相关性。顾名思义，FC CRF使用所有像素来创建一个远程模型，该模型可用于<strong class="iw hj">平滑噪声分割图。</strong>这里的主要挑战是由于模型的全连接性质而导致的计算爆炸。2012年，一篇<a class="ae js" href="https://arxiv.org/pdf/1210.5644.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>介绍了FC CRF模型的高效推理算法。该模型最终用于DeepLab v1和DeepLab v2，导致这两种架构都不能用作端到端的学习框架。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/aa1cd2f8e9e0f5c0f24728f543cc3e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDD2mDQ5fzTA1ibMo0ndGQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">第一行是得分/特征图，第二行代表softmax函数的结果。</figcaption></figure><h2 id="7461" class="lm jz hi bd ka ln lo lp ke lq lr ls ki jf lt lu km jj lv lw kq jn lx ly ku lz bi translated">2.4.2.<strong class="ak"> DeepLab v2 </strong></h2><p id="271c" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated">DeepLab v1使用VGGNet作为主干，DeepLab v2使用<strong class="iw hj"> ResNet </strong>并引入<strong class="iw hj">阿特鲁空间金字塔池(ASPP) </strong>。这是两种架构之间的主要区别。因此，让我们检查ASPP离开深实验室v1作为基地。</p><p id="806a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">顾名思义，这仅仅是SPP 的一个<strong class="iw hj">版本。在<a class="ae js" href="https://arxiv.org/abs/1406.4729" rel="noopener ugc nofollow" target="_blank"> 2014 </a>中，引入了空间金字塔池，解决了<strong class="iw hj">关于CNN </strong>中需要固定大小的输入图像的担忧，因为完全连接的层在设计上具有固定大小的输入。因此，正是在从卷积层到FCN的过渡处施加了这种大小限制。SPP是卷积层和FCN之间的新层，用于将输入大小映射到固定大小的输出。下面，可以看到SPP的图解。通过获取最后一层的特征图，并将其分成与图像尺寸成比例的多个<strong class="iw hj">空间箱</strong>。这意味着不管图像大小，箱的数量是固定的。如我们所见，在不同的粒度级别上创建箱，最终层由整个图像组成。最后，使用最大池合并每个过滤器的每个空间箱。因为仓的数量是已知的，所以我们可以连接不同的输出，以给出F*B的固定长度表示，其中F是滤波器数量，B是仓的数量，这是随后被馈送到FCN的维度向量。</strong></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/a8309e9b234df5fb8d0a014783034552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBujawg2yBOme8oYyJETrQ.png"/></div></div></figure><p id="f282" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">阿特鲁SPP类似于SPP，但是它不使用仓，而是使用具有不同采样率的多个并行滤波器。然后融合提取的特征以生成最终结果。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/9e8721f1f5cebaa4c32279d11a5409e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OOnZhMJm6akm3USDbso5YQ.png"/></div></div></figure><p id="fd2b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">ASPP提出了这样一个事实，即同一类的对象在一幅图像中可以有不同的比例，从而提高了精确度。</p><h2 id="060a" class="lm jz hi bd ka ln lo lp ke lq lr ls ki jf lt lu km jj lv lw kq jn lx ly ku lz bi translated">2.4.3.<strong class="ak"> DeepLab v3 </strong></h2><p id="a4f1" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated">在<a class="ae js" href="https://arxiv.org/abs/1706.05587" rel="noopener ugc nofollow" target="_blank">重新思考用于语义图像分割的阿特鲁卷积</a>中，来自DeepLab v2的主要概念经过了修改和改进，产生了一种新的架构，其性能明显优于以前的架构。我们将讨论的主要改进包括:</p><ul class=""><li id="73d5" class="mc md hi iw b ix iy jb jc jf me jj mf jn mg jr mh mi mj mk bi translated">级联或并行阿特鲁卷积，采用多种速率捕捉多尺度背景</li><li id="d41b" class="mc md hi iw b ix ml jb mm jf mn jj mo jn mp jr mh mi mj mk bi translated">ASPP的改进</li></ul><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mu"><img src="../Images/2be092293cd68befe4f9d1353059fd3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*skVTCndOSjAmvFRFoDfwCw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">当输出步距= 16时，在模块3之后应用速率&gt; 1的atrous卷积时，级联模块(无atrous卷积和有atrous卷积)</figcaption></figure><p id="1324" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，我们可以从上图中看出，块4被复制了几次，其中在级联上放置了大量卷积。与没有atrous卷积的架构相比，我们可以保持恒定的步幅，同时保持更大的视野，同时具有最少数量的参数和更大的特征图，从而易于在更深的块中捕获长距离信息。</p><p id="fa01" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">第二，通过引入<strong class="iw hj"> 4个并行的atrous卷积</strong>重新访问ASPP，在特征图的顶部应用不同的atrous率以捕获多尺度信息。但是发现<em class="lf"/><strong class="iw hj"><em class="lf">随着采样率变大，有效滤波器权值的个数变小</em></strong><em class="lf"/>。为了解决这个问题，在最后一个要素地图上应用了全局平均池。然后将结果送入1x1卷积，批量归一化，最后双线性上采样至所需尺寸。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/3b4812c2b2b75fd3045eb2cc810d2c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ufX2yEa-fMLj4n15jPADdw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">具有atrous卷积(ASPP)的并行模块，增加了图像级功能</figcaption></figure><h2 id="14ac" class="lm jz hi bd ka ln lo lp ke lq lr ls ki jf lt lu km jj lv lw kq jn lx ly ku lz bi translated">2.4.4.<strong class="ak"> DeepLab v3+ </strong></h2><p id="9cde" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated">下面，我们可以看到编码器部分已经是DeepLab v3提供的了。通过<strong class="iw hj">扩展带有解码器模块的DeepLabv3，v3+ </strong>诞生了。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/47b6280796103357e8e203392924cc4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7F6h8yl1ow0rThO9wMjrxA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">SPP(左)和编码器-解码器结构(中)的组合产生了DeepLab v3+(右)，其中编码器部分提供了丰富的语义信息，而解码器提供了更详细的对象边界信息。</figcaption></figure><p id="7319" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae js" href="https://arxiv.org/pdf/1802.02611.pdf" rel="noopener ugc nofollow" target="_blank"> DeepLab v3+ </a>也基于深度方向可分离卷积修改atrous卷积。正常卷积可以分解成深度卷积和点态卷积。通过使用深度方向卷积，可以大大减少所需的计算量。通过应用相同的基本原理来分解深度方向上的卷积，表明它也降低了计算复杂度。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/7fd68a62f21a39577da43023764c4476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQtLVPH9ehrK91l9Zzjhaw.png"/></div></div></figure><p id="5d2b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面，我们可以观察一下DeepLab v3+的架构。从这个图中可以清楚地看到，DeepLab v3确实代表了编码器。</p><p id="19da" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">来自编码器的特征首先通过因子4进行双线性上采样，然后与来自主干网络的相应低级特征连接。这些低级特征首先通过1x1卷积来减少通道数量，通道数量会扭曲更丰富的编码器特征的重要性。之后，在再次以因子4进行上采样之前，出现另一组卷积。这些最后的3×3卷积有助于在放大图像之前细化特征。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/33c1a2d81f3bf5de63db81df6e3ebb6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pIi_zR73RM3ymMosmulBcg.png"/></div></div></figure><p id="f0b3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">作为主干，对ResNet-101和Xception进行了研究，结果表明SOTA是在PASCAL VOC 2012数据集上使用DeepLab v3+ Xception架构实现的。</p><h1 id="9aa6" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">2.5.快速FCN</h1><p id="5142" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated"><a class="ae js" href="http://wuhuikai.me/FastFCNProject/fast_fcn.pdf" rel="noopener ugc nofollow" target="_blank">2019</a>用<strong class="iw hj">联合金字塔上采样(JPU) </strong>代替了扩张卷积的原理。作者讨论了旨在用JPU解决这个问题的膨胀卷积所引入的繁重的计算复杂度和内存。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/bc6b0752099eeb3266a1ea10e319fd1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qIjtbbsVYqRwpTs6-7_i8Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用与扩张FCN相同的主干，JPU模块将最后3个特征图作为输入图，并生成高分辨率特征图。</figcaption></figure><p id="fc79" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Fast FCN主干和DilatedFCN的区别在于最后两个卷积阶段。通常，输入特征图首先由常规卷积层处理，随后是一系列扩展卷积。快速FCN在概念上用步进卷积处理输入特征图，然后使用几个常规卷积来生成输出。与DilatedFCN相比，这减轻了计算负担。从概念上讲，因为这是这个想法的主要要点，但发现在梯度下降过程中需要较长的收敛时间。因此，JPU被创建来近似这个优化过程。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/0ba237c3f4a6ff49f400adfdf50b5feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YrT7w4KaIPPUGsgt.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">JPU街区</figcaption></figure><p id="3d11" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如我们在上面看到的，每个特征映射都通过你的常规卷积块。然后，特征图被上采样和连接，然后通过具有不同膨胀率的四个卷积。最后，卷积的结果被再次连接并通过最终的卷积层。ASPP只利用最后一个特征图中的信息，<strong class="iw hj"> JPU从多级特征图中提取多尺度上下文信息</strong>，这导致了更好的性能。</p><h1 id="9983" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">2.6.门控SCNN</h1><p id="0dd5" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated">2019年夏，<a class="ae js" href="https://arxiv.org/pdf/1907.05740.pdf" rel="noopener ugc nofollow" target="_blank">用于语义切分的门控形状CNNs】推出。GSCNN引入了一个新的<strong class="iw hj"> CNN架构，有两个流</strong>:之前架构中讨论过的经典流和一个形状流。基本原理是，在前述架构设计中存在固有的低效，因为颜色、形状和纹理信息都在一个深度CNN内一起处理，而它们可能包含非常不同数量和类型的信息。</a></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/433e35f6dbf18d6d93713f6f200a5f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7BpbxCKiMieMmdmxXgRrZg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">GSCNN架构。常规流可以是任何主干架构。形状流侧重于通过一组残差块和选通卷积层进行形状处理。最后，这两个流通过阿特鲁空间金字塔池进行融合，以获得精确的语义分割输出。</figcaption></figure><p id="dd0f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">虽然语义分割的任务和语义边界密切相关，但是形状流不采用来自常规流的特征。相反，<strong class="iw hj">门控卷积层</strong> (GCL)通过过滤掉其余信息来帮助shape流只处理相关信息。常规流形成对场景的高级理解，使用GCL，我们可以确保形状流只关注相关信息，然后传递到形状流中的下一层进行进一步处理。直观上，形状流可以被视为生成注意力地图的一系列过程，其中具有重要的<strong class="iw hj">边界信息</strong>的区域在权重方面越来越重。最后，在形状流中，由于可以从规则流的语义分割掩码中获得边缘的基本事实，因此可以在输出边界上使用受监督的二进制交叉熵损失来监督形状流。</p><p id="8c5f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最后一步是使用ASPP融合规则流和形状流，以确保多尺度上下文信息得以保留。这种改进产生了一种架构，它可以围绕对象边界产生更清晰的预测，并显著提高更薄和更小对象的性能。</p></div><div class="ab cl nc nd gp ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="hb hc hd he hf"><h1 id="19a7" class="jy jz hi bd ka kb nj kd ke kf nk kh ki kj nl kl km kn nm kp kq kr nn kt ku kv bi translated">结论</h1><p id="94db" class="pw-post-body-paragraph iu iv hi iw b ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn ld jp jq jr hb bi translated">我们已经讨论了语义分割的不同关键架构。对于每一个，我们可以观察一个编码器-解码器结构，其目标是提取和组合细粒度的位置信息和粗粒度的内容信息。分割在自动驾驶、医疗保健、机器人导航、定位和场景理解等应用中至关重要。</p><p id="3da0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在下一章中，我们将讨论实例分割，它为我们提供了一种获取图像中所有类别的单个<strong class="iw hj">实例</strong>的方法。</p></div></div>    
</body>
</html>