<html>
<head>
<title>How to Solve a Machine Learning Problem: Example with Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何解决机器学习问题:代码示例</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-solve-a-machine-learning-problem-example-with-code-695e623102c8?source=collection_archive---------7-----------------------#2019-10-02">https://medium.com/analytics-vidhya/how-to-solve-a-machine-learning-problem-example-with-code-695e623102c8?source=collection_archive---------7-----------------------#2019-10-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/ab2cd03e50c8bc2ec97d9d681c83b40e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*9vbfjCmpSMWHN3IwCKp9pQ.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated">克里斯·利维拉尼在<a class="ae iq" href="https://unsplash.com/s/photos/data-science?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="20c1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="jp">先决条件:机器学习基础</em></p><p id="fa34" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在过去的几年里，机器学习和人工智能已经成为几乎所有技术相关博客中的热门词汇。通过这篇文章，我想用Kaggle的一个例子简要说明如何应用机器学习技术来解决一个问题(对于那些不知道的人来说，Kaggle是一个举办机器学习竞赛的平台)。</p><h1 id="569e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">理解问题</h1><p id="a0f4" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated">第一步是理解手头的问题。我们需要分析我们试图解决的问题，然后提出理想的解决方案。我们可以从主题专家那里获得帮助，以获得关于该问题的领域知识。例如，如果我们致力于癌症预测模型，我们可以从医学专家那里获得帮助，以了解手头的数据集。</p><p id="34c7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我举的例子是桑坦德银行来自Kaggle的客户满意度预测问题。西班牙银行Santander提供了与其客户相关的数据，他们需要一个机器学习模型来预测客户是否满意。如果银行能够准确地确定其客户满意度，它就可以大幅改善其业务。问题描述可以从这个<a class="ae iq" href="https://www.kaggle.com/c/santander-customer-satisfaction/overview" rel="noopener ugc nofollow" target="_blank">链接</a>获得。</p><h1 id="3022" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">数据清理、探索性数据分析(EDA)和功能工程</h1><p id="42a0" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated">数据集由370个特征和我们需要预测的目标值组成。提供的数据是原始的。在应用机器学习模型之前，我们需要对其进行预处理和清理。在EDA中，我们分析数据结构及其分布。散点图、箱线图、直方图等各种图用于分析每个特征的分布，每个特征如何与目标变量相关，了解数据中的异常等，以便帮助我们进行特征工程，这是机器学习中最重要的步骤之一。在特征工程中，我们修改实际特征以获得更好的模型性能。进行正确的特征工程可以极大地提高模型的准确性。</p><p id="2dba" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在桑坦德银行提供的数据集中，所有的特征名称都是匿名的。因此，由于我们不知道实际的功能名称，很难进行更多的功能工程。但是让我们尝试一些基本的特征工程技术。</p><p id="0f67" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">数据集可以从这个<a class="ae iq" href="https://www.kaggle.com/c/santander-customer-satisfaction/data" rel="noopener ugc nofollow" target="_blank">链接</a>下载。</p><p id="0e3c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们读取数据。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="851d" class="lc jr hi ky b fi ld le l lf lg">data=pd.read_csv("data.csv")<br/>data.head(5)</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/d261a2729ba0d64ab38256a1cc9be31c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bcaZCGMrjIc43iICQ5Y06Q.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">数据集样本</figcaption></figure><p id="9eae" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">现在让我们清理数据，做一些功能工程。</p><p id="5ac1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从数据中删除ID列，因为它不包含任何有用的信息。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="ddfe" class="lc jr hi ky b fi ld le l lf lg">#Dropping the ID column from data file<br/>data.drop(columns=["ID"],inplace=True)</span></pre><p id="5d06" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">检查并删除空值(如果有)。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="2d3b" class="lc jr hi ky b fi ld le l lf lg">i=0<br/>for col in data.columns:<br/>    if(data[col].isnull().any()):<br/>        i+=1<br/>print("No. of features that have null values:",i)</span></pre><p id="906a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">输出:<em class="jp">具有空值的特征数量:0 </em></p><p id="7fe2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">数据集中没有任何空值。</p><p id="fc18" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">所有数据点都具有相同值的特征不包含任何有用的信息。所以我们可以去掉这样的特征。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="1919" class="lc jr hi ky b fi ld le l lf lg">i=0<br/>for col in data.columns:<br/>    if len(data[col].unique()) == 1:<br/>        data.drop(columns=[col],inplace=True)<br/>        i+=1<br/>print(" No. of features that have constant values and removed:",i)</span></pre><p id="8f01" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">Out: <em class="jp">具有恒定值并被删除的特征数:34 </em></p><p id="014f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">现在，让我们获得一些关于数据集的基本信息。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="5f70" class="lc jr hi ky b fi ld le l lf lg">data.describe()</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lm"><img src="../Images/a8d7a6673d450eab92e4ccefd4c203b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BairRijbwBDXQK6nI_atDg.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">数据的基本统计</figcaption></figure><p id="51fc" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从上面我们可以看到，与其他特性不同，var3特性有一个最小值，它是一个很大的负值。这看起来像是异常现象。许多Kagglers也在比赛的<a class="ae iq" href="https://www.kaggle.com/cast42/debugging-var3-999999" rel="noopener ugc nofollow" target="_blank">讨论区</a>指出了这一点。所以让我们仔细检查一下。</p><p id="1082" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们检查var3特性中的唯一值</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="eda9" class="lc jr hi ky b fi ld le l lf lg">print("Unique values in var3")<br/>print(np.sort(data['var3'].unique()))</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/75b2d70b15524ef5d76a84457825b0bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*TAYTjAPJyJiEuDyU492jkQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">var3功能中的唯一值</figcaption></figure><p id="f777" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从上面的分布中，我们可以看到值的范围是从0到238，并且-999999可能是一个异常值。</p><p id="1d27" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">还让我们检查这些唯一值的计数。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="a2ce" class="lc jr hi ky b fi ld le l lf lg">print("Value count for var3:")<br/>data['var3'].value_counts().to_frame().T</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lo"><img src="../Images/6f4f296f055b35ca9de843e26aeff289.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gC5sdTT52nQiT5TgBE1cMQ.png"/></div></div></figure><p id="d174" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们绘制var3特征值的分布图。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/7a4114d6443a755657f097ec5fd5ff9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*gmzEGf_9o6zk_gz5ZHLVHw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">var3特征值的分布</figcaption></figure><p id="22b8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从上面的分布，我们可以看到考虑-999999作为一个离群值。所以让我们替换这个值-999999。我使用了一个KNN模型来预测可以替代-999999的值。方法如下:</p><ul class=""><li id="c6aa" class="lq lr hi it b iu iv iy iz jc ls jg lt jk lu jo lv lw lx ly bi translated">取var3！=-999999(无var3功能)作为输入列车数据(Xtrain)。</li><li id="5751" class="lq lr hi it b iu lz iy ma jc mb jg mc jk md jo lv lw lx ly bi translated">取所有的var3值，其中var3！=-999999作为列车数据标签(Ytrain)。</li><li id="d531" class="lq lr hi it b iu lz iy ma jc mb jg mc jk md jo lv lw lx ly bi translated">取var3=-999999(不含var3特征)的所有数据点作为输入测试数据(Xtest)。</li><li id="8dde" class="lq lr hi it b iu lz iy ma jc mb jg mc jk md jo lv lw lx ly bi translated">使用KNN模型，预测y_test，这是所需的var3值</li></ul><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="0353" class="lc jr hi ky b fi ld le l lf lg">data_cols=data.columns<br/>#Reference: <a class="ae iq" href="https://www.kaggle.com/cast42/debugging-var3-999999" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/cast42/debugging-var3-999999</a><br/>X_train = data.loc[data['var3'] != -999999, data.columns[1:]] #Taking all the datapoints where var3!=-999999 (without var3 feature) as the Xtrain<br/>y_train = data.loc[data['var3'] != -999999, 'var3']#Taking all the var3 values where var3!=-999999 as the Ytrain<br/>X_test = data.loc[data['var3'] == -999999, data.columns[1:]]#Taking all the datapoints where var3!=-999999 (without var3 feature) as the Xtest</span><span id="2ba8" class="lc jr hi ky b fi me le l lf lg">#Defining KNN for predicitng the values to replace for -999999<br/>clf = KNeighborsClassifier(n_neighbors=20,n_jobs=-1)<br/>clf.fit(X_train, y_train)</span><span id="3195" class="lc jr hi ky b fi me le l lf lg">y_test = clf.predict(X_test)</span></pre><p id="bb34" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">由于KNN预测所有的值都是2，我们可以用2替换var3特性中的-999999。我用不同的k值试过KNN，结果都是2。<br/>var 3特征中的多数值也是2。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="225e" class="lc jr hi ky b fi ld le l lf lg">#Replacing -999999 with 2<br/>data['var3'].replace(to_replace=-999999,value=2,inplace=True)</span></pre><p id="1f68" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">现在让我们检查是否有重复的行。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="45d2" class="lc jr hi ky b fi ld le l lf lg">dup=data.duplicated(keep='first')#keep='first' to retain one instance of all the duplicate points<br/>dup_count=data["TARGET"][dup].value_counts()<br/>print("There are {} duplicate entries for TARGET=0(happy customers) and {} duplicate entries for TARGET=1(unhappy customers)".format(dup_count[0],dup_count[1]))<br/>print("Total duplicate rows={}".format(dup_count[0]+dup_count[1]))</span></pre><p id="1fbf" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">out:<em class="jp">TARGET = 0(满意的客户)有4648个重复条目，TARGET=1(不满意的客户)有193个重复条目<br/>重复行总数=4841 </em></p><p id="0f39" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">删除重复的行。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="0260" class="lc jr hi ky b fi ld le l lf lg">#Removing duplicate rows by keeping only one instance of all<br/>dup_ind=data.index[dup]<br/>data.drop(index=dup_ind,inplace=True)</span></pre><p id="aa97" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们检查是否存在具有不同目标标签的相同特征值。即同一个顾客被标记为快乐和不快乐。这是无效数据。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="9595" class="lc jr hi ky b fi ld le l lf lg">#Checking if any rows have same features but different Target labels which is invalid<br/>dup_list=data[data_cols[:-1]].duplicated(keep=False) <br/>dup_count=data["TARGET"][dup_list].value_counts()<br/>print("There are {} data points that have same feature values, but with different target labels".format(dup_count[0]+dup_count[1]))</span></pre><p id="61da" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">Out: <em class="jp">有266个数据点具有相同的特征值，但目标标签不同</em></p><p id="1d6a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从上面，我们有266个数据点，它们具有相同的特征，但是具有不同的目标标签。即133个数据点具有目标=0，另外133个数据点具有目标=1。这是无效的。因此，我们删除一组133个重复点，然后使用KNN模型的预测为剩余的133个点找到合适的标签。<br/>KNN的列车数据是没有重复数据点的全部数据。<br/>测试数据是一组133个重复的数据点。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="c7d6" class="lc jr hi ky b fi ld le l lf lg">dup_total=data[data_cols[:-1]].duplicated(keep=False)#getting boolean representation of all duplicates<br/>dup_first=data[data_cols[:-1]].duplicated(keep='first')#getting boolean representation of first set of 133 duplicates<br/>dup_last=data[data_cols[:-1]].duplicated(keep='last')#getting boolean representation of second set of 133 duplicates</span><span id="360a" class="lc jr hi ky b fi me le l lf lg">dup_total_index=data.index[dup_total]#indeces of all duplicates <br/>temp_x_train=data.drop(index=dup_total_index)#creating train data without any duplicates<br/>temp_y_train=temp_x_train["TARGET"]<br/>temp_x_train.drop(columns=["TARGET"],inplace=True)# dropping target variable from train data</span><span id="54cd" class="lc jr hi ky b fi me le l lf lg">dup_first_index=data.index[dup_first]#indices of one set of 133 duplicates<br/>temp_xtest=data.loc[dup_first_index]#test data with one set of 133 duplicates<br/>temp_xtest.drop(columns=["TARGET"],inplace=True)# dropping target variable from test data</span><span id="4756" class="lc jr hi ky b fi me le l lf lg">#Defining KNN for predicitng the values <br/>clf = KNeighborsClassifier(n_neighbors=10,n_jobs=-1)<br/>clf.fit(temp_x_train, temp_y_train)<br/>pred=clf.predict(temp_xtest)</span></pre><p id="32d9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们有一个高度不平衡的数据集。0(满意)是多数目标值。我通过改变k值来尝试这个模型。但是每次，模型都预测输出为0。这可能是因为偏向多数阶级。但是我们取的是预测值，因为这比随机赋值要好。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="b242" class="lc jr hi ky b fi ld le l lf lg">#Replacing the target values of one set of 133 duplicate rows with predicted value <br/>for ind in dup_first_index:<br/>    data.loc[ind,'TARGET']=0</span><span id="d19e" class="lc jr hi ky b fi me le l lf lg">#Dropping the other set of 133 duplicates<br/>dup_last_index=data.index[dup_last]#indices of one set of 133 duplicates<br/>data.drop(index=dup_last_index,inplace=True)</span></pre><p id="9a83" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">现在让我们检查数据是否平衡。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="e50d" class="lc jr hi ky b fi ld le l lf lg">count=data["TARGET"].value_counts()<br/>happy_percent=np.round(100*count[0]/(count[0]+count[1]),2)<br/>unhappy_percent=np.round(100*count[1]/(count[0]+count[1]),2)</span><span id="1c78" class="lc jr hi ky b fi me le l lf lg">print("Number of happy customers={} and their percentage={}%".format(count[0],happy_percent))<br/>print("Number of unhappy customers={} and their percentage={}%".format(count[1],unhappy_percent))</span></pre><p id="759e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">输出:<em class="jp">满意客户数=68364，百分比=96.22% <br/>不满意客户数=2682，百分比=3.78% </em></p><p id="b44a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们可以看到数据集是高度不平衡的。与任何组织一样，这是意料之中的，大多数客户都会感到满意。</p><p id="24e2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们画出目标变量来了解一下阶级的不平衡。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="3024" class="lc jr hi ky b fi ld le l lf lg">plt.bar([0,1],[happy_percent,unhappy_percent])<br/>plt.xticks([0,1],['Happy','Unhappy'])<br/>plt.xlabel("Customer Category")<br/>plt.ylabel("Percentage")<br/>plt.title("Target distribution")<br/>plt.show()</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/097f29303a34a1fd2375637b5fd2db28.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*rpSDUow_3S2C-HF60dKNDg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">阶级不平衡</figcaption></figure><p id="58b1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">不平衡的数据可能会导致模型将大多数数据点预测为多数类。因此，我们需要在将数据集输入机器学习模型之前对其进行平衡。在此之前，让我们将数据分为训练和测试数据集，以防止数据泄漏问题。</p><p id="a24f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">分割成列车测试数据</strong></p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="71b7" class="lc jr hi ky b fi ld le l lf lg">X=data.drop(columns=["TARGET"])<br/>Y=data["TARGET"]</span><span id="311f" class="lc jr hi ky b fi me le l lf lg">X_train1,X_test,y_train1,y_test=train_test_split(X, Y, test_size=0.30, random_state=0,stratify=Y) #Splitting data into test and train datasets</span><span id="c6a1" class="lc jr hi ky b fi me le l lf lg">count=y_train1.value_counts()<br/>print("No. of data points before upsampling=",y_train1.shape[0])<br/>print("Percentage of unhappy customers in train data before upsampling={}%".format(round(count[1]*100/(count[0]+count[1]),2)))<br/>print("Percentage of happy customers in train data before upsampling={}%".format(round(count[0]*100/(count[0]+count[1]),2)))</span></pre><p id="4138" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">Out: <em class="jp">上采样前数据点数= 49732 <br/>上采样前列车数据中不满意客户的百分比=3.77% <br/>上采样前列车数据中满意客户的百分比=96.23% </em></p><p id="a7ca" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">使用SMOTE平衡数据</strong></p><p id="d580" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">有许多方法可以处理类不平衡，如复制少数类点、使用类权重、使用SMOTE等算法。这里我使用了SMOTE或合成少数过采样技术来对少数类进行上采样。SMOTE使用最近邻算法来生成新的合成数据，我们可以使用这些数据来训练我们的模型。你可以从这个<a class="ae iq" href="https://arxiv.org/pdf/1106.1813.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>阅读更多关于SMOTE算法的内容。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="3923" class="lc jr hi ky b fi ld le l lf lg">from imblearn.over_sampling import SMOTE<br/>smt = SMOTE()<br/>X_train, y_train = smt.fit_sample(X_train1, y_train1)# upsampling using SMOTE<br/>#SMOTE return numpy array. Converting it to dataframe<br/>X_train=pd.DataFrame(X_train,columns=list(data_cols)[:-1])<br/>y_train=pd.DataFrame(y_train,columns=["TARGET"])<br/>count=y_train["TARGET"].value_counts()<br/>print("No. of data points after upsampling=",y_train.shape[0])<br/>print("Percentage of unhappy customers in train data before upsampling={}%".format(round(count[1]*100/(count[0]+count[1]),2)))<br/>print("Percentage of happy customers in train data before upsampling={}%".format(round(count[0]*100/(count[0]+count[1]),2)))</span></pre><p id="6626" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">Out: <em class="jp">上采样后的数据点数= 95710 <br/>上采样前列车数据中不满意客户的百分比=50.0% <br/>上采样前列车数据中满意客户的百分比=50.0% </em></p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="c4af" class="lc jr hi ky b fi ld le l lf lg">happy_percent=np.round(100*count[0]/(count[0]+count[1]),2)<br/>unhappy_percent=np.round(100*count[1]/(count[0]+count[1]),2)<br/>plt.bar([0,1],[happy_percent,unhappy_percent])<br/>plt.xticks([0,1],['Happy','Unhappy'])<br/>plt.xlabel("Customer Category")<br/>plt.ylabel("Percentage")<br/>plt.title("Target distribution after upsampling")<br/>plt.show()</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/ab98f1f382e9830b83b425cbc82ea9f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*XML5bx3eELv35yVdCFoi_A.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">平衡后的数据分布</figcaption></figure><p id="fd03" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">数据标准化</strong></p><p id="7bb0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们需要将数据标准化，以避免规模效应。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="e321" class="lc jr hi ky b fi ld le l lf lg">#Train data standardization<br/>std=StandardScaler()<br/>X_train = std.fit_transform(X_train)</span><span id="743e" class="lc jr hi ky b fi me le l lf lg">#Test data standardization<br/>X_test = std.transform(X_test)</span></pre><h2 id="ca25" class="lc jr hi bd js mh mi mj jw mk ml mm ka jc mn mo ke jg mp mq ki jk mr ms km mt bi translated">使用主成分分析进行降维</h2><p id="ddff" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated">让我们检查功能的数量:</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="ccc0" class="lc jr hi ky b fi ld le l lf lg">print("Number of features in data=",X_train.shape[1])</span></pre><p id="63ff" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">Out: <em class="jp">数据中的特征数= 335 </em></p><p id="53b2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">由于特征的数量很大，让我们通过考虑方差来使用PCA降低维数。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="199b" class="lc jr hi ky b fi ld le l lf lg">pca = decomposition.PCA()<br/>pca.n_components = X_train.shape[1]<br/>pca_data = pca.fit_transform(X_train)</span><span id="bf78" class="lc jr hi ky b fi me le l lf lg">percentage_var_explained = pca.explained_variance_ / np.sum(pca.explained_variance_)<br/>cum_var_explained = np.cumsum(percentage_var_explained)</span><span id="fe5a" class="lc jr hi ky b fi me le l lf lg"># Plot the PCA spectrum<br/>plt.figure(1, figsize=(8, 6))</span><span id="6907" class="lc jr hi ky b fi me le l lf lg">plt.plot(cum_var_explained, linewidth=2)<br/>plt.title("Explained_variance VS n_components")<br/>plt.grid()<br/>plt.xlabel('n_components')<br/>plt.ylabel('Cumulative_explained_variance')<br/>plt.show()</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/61f3055c27460d3d680bee61760fc7bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*iFM9PkmvTyLfWkUsLJBVjg.png"/></div></figure><p id="ae4a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们可以从上面的图中看到，即使我们使用150个特征而不是335个，我们也可以保持大约100%的方差。因此，让我们将维度减少到150，以便我们可以使用较低的计算资源配置来训练模型。下图显示了150个特征的解释差异。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="0860" class="lc jr hi ky b fi ld le l lf lg">#Plotting variance of 150 features<br/>plt.figure(1, figsize=(8, 6))</span><span id="0114" class="lc jr hi ky b fi me le l lf lg">plt.plot(cum_var_explained[0:150], linewidth=2) <br/>plt.title("Explained_variance VS n_components")<br/>plt.grid()<br/>plt.xlabel('n_components')<br/>plt.ylabel('Cumulative_explained_variance')<br/>plt.show()</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/05f0b0a4b1d90602489e1903cad1314e.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*M3aVkmwLCtwtDzxnrzwLOA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">解释了150个特征的差异</figcaption></figure><p id="1602" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们把维数降低到150。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="b8ef" class="lc jr hi ky b fi ld le l lf lg">pca.n_components = 150<br/>X_train=pca.fit_transform(X_train) #PCA on train data<br/>X_test=pca.transform(X_test) #PCA on test data<br/>print("Size of train data after dimensionality reduction(PCA)=",X_train.shape)<br/>print("Size of test data after dimensionality reduction(PCA)=",X_test.shape)</span></pre><p id="b6d9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">Out: <em class="jp">降维后的训练数据量(PCA)= (95710，150) <br/>降维后的测试数据量(PCA)= (21314，150) </em></p><h1 id="b1e8" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">定义机器学习模型</h1><p id="9ce8" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated">我们需要根据我们的问题选择一个模型。如果我们有一个分类任务，我们需要一个分类器模型。如果我们需要预测一个真实值，我们需要使用回归模型。因为我们的任务是将客户分类为快乐或不快乐，所以我们需要一个分类器模型。</p><p id="bc76" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们需要检查不同模型对数据的性能，并选择性能最佳的模型。在本文中，我测试了以下模型:</p><ul class=""><li id="c5a8" class="lq lr hi it b iu iv iy iz jc ls jg lt jk lu jo lv lw lx ly bi translated">XgBoost</li><li id="36f2" class="lq lr hi it b iu lz iy ma jc mb jg mc jk md jo lv lw lx ly bi translated">带校准的逻辑回归</li><li id="57e5" class="lq lr hi it b iu lz iy ma jc mb jg mc jk md jo lv lw lx ly bi translated">RFDT</li></ul><p id="d4a1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> XgBoost型号</strong></p><p id="bf06" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">XgBoost是一种高级的boosting算法，在大多数情况下，与其他模型相比，它可以提供可观的性能。</p><p id="60c5" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">该模型在训练数据上被训练，并且超参数基于交叉验证(CV)数据上的模型性能被调整。XgBoost中有很多hyper参数。我在这里考虑了“n估计量”和“学习率”作为例子。你可以从这个<a class="ae iq" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html" rel="noopener ugc nofollow" target="_blank">链接</a>了解更多关于XgBoost算法的内容。</p><p id="a905" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">GridsearchCV是sklearn提供的一个函数，用于遍历hyper参数的所有组合，并选择具有最佳性能的模型。你可以通过这个<a class="ae iq" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">链接</a>了解更多。</p><p id="abec" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我已经使用AUC分数来评估模型性能。因此，给出最佳AUC分数的超参数将被GridSearchCV视为最佳模型。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="93f2" class="lc jr hi ky b fi ld le l lf lg">tuned_param={'n_estimators':range(10,20),'learning_rate':[.05,.06,.07,.08,.09,.1,0.15,0.2,0.25,0.3]}<br/>xgb_model=GridSearchCV(XGBClassifier(n_jobs=-1,random_state=0),param_grid=tuned_param,scoring='roc_auc',return_train_score=True)<br/>xgb_model.fit(X_train, y_train)</span><span id="0c8c" class="lc jr hi ky b fi me le l lf lg">print("Best hyperparamters-",xgb_model.best_params_)<br/>print("Best AUC value: ",xgb_model.best_score_ )</span></pre><p id="6daa" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">Out: <em class="jp">最佳超参数- {'learning_rate': 0.3，' n_estimators': 19} <br/>最佳AUC值:0.917746861944092</em></p><p id="4c8a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">现在，让我们绘制所有超参数组合的CV数据的AUC分数的热图，以了解模型性能如何随超参数而变化。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="4e84" class="lc jr hi ky b fi ld le l lf lg">auc_df=pd.DataFrame(xgb_model.cv_results_['params'])<br/>auc_df["auc"]=xgb_model.cv_results_['mean_test_score'] #Creating a data frame with hyperparameters and AUC</span><span id="231b" class="lc jr hi ky b fi me le l lf lg">auc_df=auc_df.pivot(index='learning_rate',columns='n_estimators',values='auc') #Pivoting the dataframe for plotting heat map</span><span id="0b1d" class="lc jr hi ky b fi me le l lf lg">plt.figure(figsize=(20,10))<br/>sns.heatmap(data=auc_df,annot=True)<br/>plt.title("AUC plot for CV data")<br/>plt.show()</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mw"><img src="../Images/c903df96d4c3080541e1b361568dca46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1JmgAqCBRVBi5emkBjBh1g.png"/></div></div></figure><p id="7fe7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">下面是我们获得的最佳模型的ROC曲线。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/c36688a4d8af695ae1ba4595792e7c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*l0Fu8IrEgOQ2zglu2DpJwQ.png"/></div></figure><p id="3104" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">此外，下面还有几个图来评估模型性能。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="e3e9" class="lc jr hi ky b fi ld le l lf lg">pred_train_prob=xgb_model.predict_proba(X_train)<br/>pred_test_prob=xgb_model.predict_proba(X_test)<br/>print("Train AUC=",roc_auc_score(y_train,pred_train_prob[:,1]))<br/>print("Test AUC=",roc_auc_score(y_test,pred_test_prob[:,1]))</span><span id="d76a" class="lc jr hi ky b fi me le l lf lg">pred_train=xgb_model.predict(X_train)<br/>pred_test=xgb_model.predict(X_test)<br/>#Plotting confusion, precision and recall matrices of train data<br/>plot_confusion_matrix(y_train, pred_train)</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es my"><img src="../Images/be3eb872555cc0581bc8449770b5a9e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*avZ3I8NPXDEfRnH6WuZ20A.png"/></div></div></figure><p id="3626" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">带校准的逻辑回归</strong></p><p id="be5f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">逻辑回归是最简单的分类器模型之一。它依靠对数损失来进行标签预测。我使用了校准和逻辑回归模型。校准有助于获得更可靠的预测概率。你可以从这个<a class="ae iq" href="https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/" rel="noopener ugc nofollow" target="_blank">博客</a>中了解更多关于校准的信息。</p><p id="87ba" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">逻辑回归的超参数是正则化项alpha(代码中的C)。现在，让我们针对超参数α的各种值，在我们的训练数据上拟合校准的逻辑回归模型。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="6889" class="lc jr hi ky b fi ld le l lf lg">alpha_list=[10**a for a in range(-5,3)]<br/>train_auc_list=[]<br/>test_auc_list=[]<br/>for i in alpha_list:<br/>    log_clf=LogisticRegression(penalty='l2',C=i, max_iter=500,n_jobs=-1)<br/>    calib_clf=CalibratedClassifierCV(log_clf, method="sigmoid")<br/>    calib_clf.fit(X_train, y_train)<br/>    pred_train=calib_clf.predict_proba(X_train)<br/>    pred_test=calib_clf.predict_proba(X_test)<br/>    train_auc=roc_auc_score(y_train,pred_train[:,1])<br/>    test_auc=roc_auc_score(y_test,pred_test[:,1])<br/>    train_auc_list.append(train_auc)<br/>    test_auc_list.append(test_auc)</span></pre><p id="8dbd" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">出局:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/f8fae2d2a871607b6c3216a387c91eb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*_nLOrX3gzatokWT0o8AXxA.png"/></div></figure><p id="9788" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们绘制AUC w.r.t超参数的变化。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="7458" class="lc jr hi ky b fi ld le l lf lg">for i in range(0,len(alpha_list)):<br/>    print("For alpha=",alpha_list[i]," train AUC=",train_auc_list[i]," and test AUC=",test_auc_list[i])</span><span id="dd3c" class="lc jr hi ky b fi me le l lf lg">plt.title("alpha VS AUC")<br/>plt.plot(alpha_list,train_auc_list,label="Train")<br/>plt.plot(alpha_list,test_auc_list,label="Test")<br/>plt.xlabel("alpha")<br/>plt.ylabel("AUC")<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es na"><img src="../Images/57067ab43e441a8d95189fcd60d2eae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*aBbtcAvC55XEI18vIp286Q.png"/></div></figure><p id="18d8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们找出给出最佳AUC值的超参数。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="a5ed" class="lc jr hi ky b fi ld le l lf lg">best_alpha_ind=np.argmax(test_auc_list)<br/>print("Best alpha=",alpha_list[best_alpha_ind])<br/>print("Best train AUC=",train_auc_list[best_alpha_ind])<br/>print("Best test AUC=",test_auc_list[best_alpha_ind])</span></pre><p id="dd72" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">Out: <em class="jp">最佳alpha= 0.001 <br/>最佳列车AUC= 0.8205906942467746 <br/>最佳测试AUC = 0.7925951460788</em></p><p id="ceec" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">逻辑回归模型的ROC曲线绘制如下。代码与我们用于XgBoost模型的代码相同。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es nb"><img src="../Images/746fe89b98d407f568128dc1aaff354f.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*Y3gMam1pDdrGRMgQ-dI4vQ.png"/></div></figure><p id="e2fe" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们检查混淆矩阵、精确矩阵和召回矩阵。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nc"><img src="../Images/1e5d27f433aebff7fce4fd1a71ef3905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M04T2T-itDzU_etJEPfVXw.png"/></div></div></figure><p id="1753" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">随机森林决策树(RFDT)模型</strong></p><p id="b206" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">RFDT是一个集合模型，使用决策树作为它的基础模型。决策树基本上是嵌套的if-else语句。RFDT的超级参数是基础决策树的数量(n_estimators)和它的最大深度(max_depth)。你可以从我的关于RFDT 的博客<a class="ae iq" rel="noopener" href="/@jijogeorgeab/a-walk-through-random-forest-decision-tree-rfdt-algorithm-with-code-932271af4ec7">中读到更多关于RFDT及其工作的内容。</a></p><p id="37a2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们只需修改一行代码，就可以将XgBoost模型的代码转换成RFDT模型的代码。这就是sklearn的妙处！！！</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="a443" class="lc jr hi ky b fi ld le l lf lg">tuned_param={'max_depth':range(1,10),'n_estimators':range(1,6)}<br/>rfdt_model=GridSearchCV(RandomForestClassifier(n_jobs=-1),param_grid=tuned_param,scoring='roc_auc',return_train_score=True)<br/>rfdt_model.fit(X_train, y_train)</span></pre><p id="0968" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们检查给出最佳AUC值的超参数值。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="d17a" class="lc jr hi ky b fi ld le l lf lg">print("Best hyperparamters-",rfdt_model.best_params_)<br/>print("Best AUC value: ",rfdt_model.best_score_ )</span></pre><p id="3e97" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">Out: <em class="jp">最佳超参数- {'max_depth': 9，' n_estimators': 5} <br/>最佳AUC值:0.942976539431358</em></p><p id="7c4e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们绘制各种超参数值的AUC值的热图</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nd"><img src="../Images/22a9e41fa6c5143178f98c95da39dd96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tEKffVvsQgGQpMyJ1kq5Hw.png"/></div></div></figure><p id="c710" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">RFDT的ROC曲线</strong></p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/d56f5f160651325a95428d61f509290d.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*CSRaWXLiue9ZvLXTcBkzjw.png"/></div></figure><p id="9b30" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们检查混淆矩阵、精确矩阵和召回矩阵。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ne"><img src="../Images/49f421675ec1de188c964aa018cf887b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*inpjO6ni7vmP3pwiQYUiIA.png"/></div></div></figure><h1 id="15ea" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">结论</h1><p id="fe74" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated">我们已经简要讨论了设计和实现机器学习模型的方法。首先我们必须彻底理解手头的问题。学科专家可以帮助我们获取领域知识。下一步是预处理数据，并执行EDA和功能工程。这一步至关重要，因为在此获得的见解将贯穿整个设计过程。</p><p id="55aa" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">一旦特征工程完成，我们可以将数据分成训练、交叉验证(CV)和测试数据。在训练数据上训练模型，在CV数据上进行超参数调整，并且在测试数据上进行实际测试。生成不同的模型，并选择给出最佳性能的模型。</p><p id="2dab" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们从各种模型中获得的结果总结如下。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/a310e0f81d707be1ab77997c0b708bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*FrszHCT88FdpqgKdaGPoHQ.png"/></div></figure><p id="e0dc" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从表中我们可以看出，XgBoost模型在测试数据上给出了最好的结果。所以我们选择XgBoost模型作为这个问题的首选模型。</p><h1 id="ee7f" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">未来范围</h1><p id="9235" class="pw-post-body-paragraph ir is hi it b iu ko iw ix iy kp ja jb jc kq je jf jg kr ji jj jk ks jm jn jo hb bi translated">作为未来的工作，您可以尝试更多的功能工程技术(例如:从现有功能引入新功能)，并检查您是否可以获得更好的性能。</p><p id="c47a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">从结果表中，我们可以观察到训练AUC相对高于测试AUC。这意味着我们的模型有点过度拟合。您可以尝试一些正则化技术，看看是否可以消除过度拟合效果。</p><p id="a00d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们使用的模型也是经典的机器学习模型。你可以尝试使用深度学习模型，以获得更好的性能。</p><h1 id="5e26" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">参考</h1><ul class=""><li id="a545" class="lq lr hi it b iu ko iy kp jc ng jg nh jk ni jo lv lw lx ly bi translated"><a class="ae iq" href="https://www.kaggle.com/c/santander-customer-satisfaction/overview" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/Santander-customer-satisfaction/overview</a></li><li id="b958" class="lq lr hi it b iu lz iy ma jc mb jg mc jk md jo lv lw lx ly bi translated">【https://arxiv.org/pdf/1106.1813.pdf T2】号</li><li id="2881" class="lq lr hi it b iu lz iy ma jc mb jg mc jk md jo lv lw lx ly bi translated"><a class="ae iq" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html" rel="noopener ugc nofollow" target="_blank">https://xgboost . readthedocs . io/en/latest/python/python _ API . html</a></li><li id="a789" class="lq lr hi it b iu lz iy ma jc mb jg mc jk md jo lv lw lx ly bi translated"><a class="ae iq" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . model _ selection。GridSearchCV.html</a></li><li id="4da3" class="lq lr hi it b iu lz iy ma jc mb jg mc jk md jo lv lw lx ly bi translated"><a class="ae iq" href="https://www.kaggle.com/cast42/debugging-var3-999999" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/cast42/debugging-var3-999999</a></li></ul></div></div>    
</body>
</html>