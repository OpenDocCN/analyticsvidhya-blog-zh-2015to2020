<html>
<head>
<title>Probabilistic Model Selection with AIC/BIC in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中AIC/BIC的概率模型选择</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/probabilistic-model-selection-with-aic-bic-in-python-f8471d6add32?source=collection_archive---------3-----------------------#2020-05-31">https://medium.com/analytics-vidhya/probabilistic-model-selection-with-aic-bic-in-python-f8471d6add32?source=collection_archive---------3-----------------------#2020-05-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="840f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">博客里程碑</h1><ul class=""><li id="1902" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><a class="ae jv" href="#47a2" rel="noopener ugc nofollow"> <strong class="jf hj">简述型号选择</strong> </a></li><li id="d4b4" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated"><a class="ae jv" href="#f913" rel="noopener ugc nofollow"> <strong class="jf hj">概率模型选择</strong> </a> <br/> - <a class="ae jv" href="#ee12" rel="noopener ugc nofollow">什么是AIC/BIC准则</a> <br/> - <a class="ae jv" href="#2ccc" rel="noopener ugc nofollow">快速类比</a>-<br/>-<a class="ae jv" href="#a343" rel="noopener ugc nofollow">应用</a> <br/> - <a class="ae jv" href="#4e28" rel="noopener ugc nofollow">实现</a></li><li id="6459" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated"><a class="ae jv" href="#0bd0" rel="noopener ugc nofollow"> <strong class="jf hj">参考文献</strong> </a></li></ul><p id="0376" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">亲爱的学习灵魂们..以舒适的姿势坐下，设定你的焦点，让我们开始选择你的最佳机器学习模型的困境。</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="7ee2" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi la translated">对如何选择最佳模型的秘密耿耿于怀。模型选择在建立机器学习模型中起着非常重要的作用。可以有多个合格的算法模型被视为候选模型，但是只有一个具有优化参数的算法模型可以被选为性能最佳的稳健模型。从候选者中选择最好的就是我们所说的模型选择。</p><h1 id="47a2" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">选型简介</strong></h1><p id="63be" class="pw-post-body-paragraph kb kc hi jf b jg jh ke kf ji jj kh ki jk lj kk kl jm lk kn ko jo ll kq kr jq hb bi translated">模型选择就像选择一个具有不同超参数的模型或不同候选模型中的最佳模型。通常情况下，任何模型的选择不应该仅仅依赖于它的性能，相反，还应该依赖于它的复杂性。</p><p id="1a0a" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">通常，我们将模型选择的技术分类如下:</p><ul class=""><li id="7eda" class="jd je hi jf b jg kd ji kg jk lm jm ln jo lo jq jr js jt ju bi translated">随机训练/测试分割</li><li id="6f5a" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated">重采样技术</li><li id="9c3a" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated">概率模型选择技术</li></ul><p id="cf9e" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated"><strong class="jf hj"> <em class="ks">本博客将只深入讨论概率模型选择，因为</em></strong><a class="ae jv" rel="noopener" href="/analytics-vidhya/model-selection-techniques-in-ml-ai-with-python-fdf308d9fa10"><strong class="jf hj"><em class="ks">ML/AI中的随机_训练/测试和重采样技术在用Python </em> </strong> </a> <strong class="jf hj"> <em class="ks">和</em> </strong> <a class="ae jv" rel="noopener" href="/analytics-vidhya/deeply-explained-cross-validation-in-ml-ai-2e846a83f6ed?source=friends_link&amp;sk=438258d3578674a242be66716ec2a0ed"> <strong class="jf hj"> <em class="ks">进行ML/AI中的模型选择时深入解释了交叉验证</em> </strong> </a> <strong class="jf hj"> <em class="ks">。</em>T53】</strong></p><blockquote class="lp lq lr"><p id="44d7" class="kb kc ks jf b jg kd ke kf ji kg kh ki ls kj kk kl lt km kn ko lu kp kq kr jq hb bi translated">根据AIC的理论，最适合的模型是用最少的独立变量解释最大变化的模型。</p></blockquote><h1 id="f913" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">概率模型选择</strong></h1><ul class=""><li id="a9d4" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">在最大似然估计下，借助<strong class="jf hj">对数似然概率框架选择最佳模型。</strong></li><li id="ef0b" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated">统计方法的好坏可以用<strong class="jf hj">信息准则</strong> (IC)来衡量，有一定的分值。因此，它指的是基于似然函数的模型选择方法<strong class="jf hj">。分数越低，模型越好。这来自统计学的信息论。</strong></li><li id="cf68" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated">考虑到<strong class="jf hj">模型性能和复杂性</strong>，而另一种重采样的模型选择技术仅检查模型性能。</li><li id="c3e1" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated">通过评分方法选择模型，评分基于:<br/> - <strong class="jf hj">使用<strong class="jf hj">对数似然法</strong>评估训练数据的性能</strong>，对数似然法源于MLE的概念，以优化模型参数。它显示了你的模型与你的数据的吻合程度。它提供了总误差的指示。<br/> - <strong class="jf hj">模型复杂性</strong>使用模型中的参数数量(或自由度)进行评估。</li></ul><blockquote class="lp lq lr"><p id="f99a" class="kb kc ks jf b jg kd ke kf ji kg kh ki ls kj kk kl lt km kn ko lu kp kq kr jq hb bi translated">对达到高拟合优度的模型进行评分奖励，如果模型变得过于复杂则进行惩罚</p></blockquote><ul class=""><li id="2580" class="jd je hi jf b jg kd ji kg jk lm jm ln jo lo jq jr js jt ju bi translated">常见的概率方法有:<br/> ~来自<em class="ks">的AIC(阿凯克信息准则)</em>~来自<br/>的BIC(贝叶斯信息准则)<em class="ks">的贝叶斯概率</em></li></ul><p id="f699" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">让我们了解更多关于AIC和BIC的技术。</p><h2 id="ee12" class="lv ig hi bd ih lw lx ly il lz ma mb ip jk mc md it jm me mf ix jo mg mh jb mi bi translated">什么是AIC/BIC标准</h2><ul class=""><li id="8818" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">这些IC方法来自<strong class="jf hj">频率主义者和贝叶斯概率</strong>领域。得分最低的任何选择方法意味着丢失的信息较少，因此是最好的模型。这是信息论的一个症结。</li><li id="59e1" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated"><strong class="jf hj">使用<em class="ks"> Log-likelihood计算:</em> </strong>包括<em class="ks">均方误差(回归)</em>和log_loss如cross_entropy(分类)。</li><li id="98b3" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated"><strong class="jf hj">惩罚参数以对抗过度拟合:</strong>建议通过添加更多参数来最大化可能性，这可能会导致模型更加复杂和过度拟合。因此，AIC/BIC增加了对附加参数的惩罚。这就是它保持平衡的方式。<br/> <strong class="jf hj"> <em class="ks">总结为在拟合数据(对数似然)和模型复杂度(对样本估计模型参数的惩罚)之间保持平衡。</em> </strong></li></ul><figure class="mk ml mm mn fd mo er es paragraph-image"><div class="er es mj"><img src="../Images/0eb3ae36a8601e3f7ffef243b0c41b09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*AdCs7L2gcMHsc_RzkplO0A.png"/></div></figure><ul class=""><li id="64c6" class="jd je hi jf b jg kd ji kg jk lm jm ln jo lo jq jr js jt ju bi translated">评估候选模型中模型的质量。</li></ul><blockquote class="lp lq lr"><p id="1d6e" class="kb kc ks jf b jg kd ke kf ji kg kh ki ls kj kk kl lt km kn ko lu kp kq kr jq hb bi translated">在不过度拟合的情况下，模型与数据的拟合程度如何</p></blockquote><ul class=""><li id="3e4a" class="jd je hi jf b jg kd ji kg jk lm jm ln jo lo jq jr js jt ju bi translated">公式是惩罚似然(惩罚项+负似然)的一种形式</li></ul><figure class="mk ml mm mn fd mo er es paragraph-image"><div class="er es mr"><img src="../Images/e982916248e43c3ced9aee5ecbc9e08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*oRNdu-do4lM_f2putrDKog.png"/></div></figure><p id="0f43" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">另外，让我们来澄清一个常见的困惑。<br/> <strong class="jf hj">适用于线性和非线性模型？</strong> <br/> <em class="ks">答:</em>是的，因为AIC/BIC是基于对数似然函数的一种模型，你可以有线性和非线性模型。</p><h2 id="d039" class="lv ig hi bd ih lw lx ly il lz ma mb ip jk mc md it jm me mf ix jo mg mh jb mi bi translated">AIC(又名赤池信息标准)</h2><ul class=""><li id="74d9" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><strong class="jf hj">AIC的诞生</strong></li></ul><p id="a5ec" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">简单回答。统计学中的信息论。</p><p id="5a3c" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">任何模型(比如线性回归)都不能展示研究的全部真相，它只是一个近似值。我们承认总会有一些信息丢失。现在怎么办？我们必须选择最接近真实情况的最佳模型，或者应该说最小化信息损失。Kullback和Leibler创造了KL信息，这是一种衡量信息损失的方法。后来，日本统计学家Hirotugu Akaike提出了最大似然和KL信息之间的关系。他开发了IC来估计KL信息，被称为Akaike的信息准则(AIC)。因此，K-L距离是两个模型之间的距离或差异方面的信息损失的度量。</p><ul class=""><li id="233e" class="jd je hi jf b jg kd ji kg jk lm jm ln jo lo jq jr js jt ju bi translated"><em class="ks">公式:</em></li></ul><figure class="mk ml mm mn fd mo er es paragraph-image"><div class="er es ms"><img src="../Images/6c19bd2e10b68a8c2a32af6e0e25a6c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*9Ig81d1cZ5oKdgrm6s9M8g.png"/></div></figure><p id="54f4" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">k=建立模型的独立变量的数量<br/> L=模型的最大似然估计</p><p id="db33" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">为了最大化似然对数(L ),更多的变量应该被添加到模型中，导致过度拟合。因此，引入了“2k”惩罚项，它并没有完全消除过拟合。因为惩罚力度不够。此外，公式不考虑观察值，而只考虑模型参数。</p><ul class=""><li id="527b" class="jd je hi jf b jg kd ji kg jk lm jm ln jo lo jq jr js jt ju bi translated">在<strong class="jf hj">小样本</strong>的情况下，大多数像AIC一样过度拟合的机会将最终选择许多参数。因此，<strong class="jf hj"> AIC修正</strong>被引入来解决这个问题。对于小样本量，</li></ul><figure class="mk ml mm mn fd mo er es paragraph-image"><div class="er es mt"><img src="../Images/e128c2c6c5423a4fe2cf1cdf9a30dc98.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*kZ0rzzcrl3Xbtk93P4SlIQ.png"/></div></figure><p id="338e" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">值越小，丢失的信息越少，模型越适合。</p><ul class=""><li id="dc44" class="jd je hi jf b jg kd ji kg jk lm jm ln jo lo jq jr js jt ju bi translated"><strong class="jf hj">当使用拟合模型(gi)而不是最佳模型(gmin)时，可以测量信息损失</strong>(δI):</li></ul><figure class="mk ml mm mn fd mo er es paragraph-image"><div class="er es mu"><img src="../Images/0c3823383db6daaa677da33bb2f6ed93.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*Tw5HYh9S9W5CSc0MppbGUg.png"/></div></figure><h2 id="9b4d" class="lv ig hi bd ih lw lx ly il lz ma mb ip jk mc md it jm me mf ix jo mg mh jb mi bi translated">BIC(又名施瓦兹信息标准)</h2><p id="2493" class="pw-post-body-paragraph kb kc hi jf b jg jh ke kf ji jj kh ki jk lj kk kl jm lk kn ko jo ll kq kr jq hb bi translated">在接受这个概念之前，我脑海中浮现出一个明显的问题。<br/><strong class="jf hj">“BIC为什么叫贝叶斯？”</strong></p><p id="5f92" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">下面引用的大部分参考文献。<br/>虽然“贝叶斯”包含在其名称中，但不需要先验信息。BIC计算不需要贝叶斯知识。只有在贝叶斯理论的框架下才能推导出最大化模型的后验概率。由于它忽略了先验分布，新的IC方法被称为基于先验的BIC (PBIC)。更多详情请参考<a class="ae jv" href="http://eprints.gla.ac.uk/179725/" rel="noopener ugc nofollow" target="_blank">本文</a>。</p><ul class=""><li id="7b01" class="jd je hi jf b jg kd ji kg jk lm jm ln jo lo jq jr js jt ju bi translated">BIC属于<strong class="jf hj">简约模型</strong>之下，即使参数数量最少也能解释得足够好。</li><li id="21ac" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated">公式:</li></ul><figure class="mk ml mm mn fd mo er es paragraph-image"><div class="er es mv"><img src="../Images/bbe2d216179e8e9cf1090c80ab08e729.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*BeojQFxFolvu_TPbR7qeVw.png"/></div></figure><p id="f7b2" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">k=建立模型的独立变量的数量<br/> L=模型的最大似然估计值<br/> n =样本大小(#观察值)<br/> log-base = e(自然对数)</p><p id="c637" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">为了最大化似然对数(L ),更多的变量应该被添加到模型中，导致过度拟合。BIC通过包含一个“log(n)k”的强惩罚来解决这个问题，这反而会使你陷入不适合的模型，即过于简单的模型。简单的模型无法捕捉数据的变化。</p><ul class=""><li id="2693" class="jd je hi jf b jg kd ji kg jk lm jm ln jo lo jq jr js jt ju bi translated">现在有趣的事情… <br/>从<a class="ae jv" href="http://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/BIC.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中，在模型选择过程中，我们可以看到悬浮的候选模型与最佳选择的模型相差多少。<br/><em class="ks">∈= BIC(M1 | D)——BIC(M2 | D)</em><br/>如果∏为正，那么M2比M1好但是好多少呢？</li></ul><figure class="mk ml mm mn fd mo er es paragraph-image"><div class="er es mw"><img src="../Images/661a170308096fbbaf6593880e0af6c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*C_Hac8uhLyEZyTNEzjai9A.png"/></div><figcaption class="mx my et er es mz na bd b be z dx translated"><a class="ae jv" href="http://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/BIC.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="2ccc" class="lv ig hi bd ih lw lx ly il lz ma mb ip jk mc md it jm me mf ix jo mg mh jb mi bi translated"><strong class="ak">快速类比</strong></h2><p id="f4cc" class="pw-post-body-paragraph kb kc hi jf b jg jh ke kf ji jj kh ki jk lj kk kl jm lk kn ko jo ll kq kr jq hb bi translated">这里是AIC/BIC的快速浏览学习，而不是懒惰的阅读内容。</p><figure class="mk ml mm mn fd mo er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es nb"><img src="../Images/fbec572cd6a5692071f79b166dfcb3f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GnIuX3eXEXlVBMhWdkDS2A.png"/></div></div><figcaption class="mx my et er es mz na bd b be z dx translated">沙池</figcaption></figure><h2 id="a343" class="lv ig hi bd ih lw lx ly il lz ma mb ip jk mc md it jm me mf ix jo mg mh jb mi bi translated">应用程序</h2><ul class=""><li id="d7fb" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><strong class="jf hj">特征选择</strong>:将添加/删除特征的模型与分数进行比较</li><li id="bebb" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated"><strong class="jf hj">正则化参数</strong> : AIC/BIC在山脊/套索模型中选择该参数</li></ul><h2 id="4e28" class="lv ig hi bd ih lw lx ly il lz ma mb ip jk mc md it jm me mf ix jo mg mh jb mi bi translated">履行</h2><p id="f6d8" class="pw-post-body-paragraph kb kc hi jf b jg jh ke kf ji jj kh ki jk lj kk kl jm lk kn ko jo ll kq kr jq hb bi translated">AIC和BIC技术可以通过以下方式实现:</p><ul class=""><li id="4276" class="jd je hi jf b jg kd ji kg jk lm jm ln jo lo jq jr js jt ju bi translated"><strong class="jf hj"> statsmodel库</strong>:在Python这个统计库里面，<strong class="jf hj">stats models . formula . API</strong>提供了一个直接计算aic/bic的方法。</li><li id="95f9" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated"><strong class="jf hj"> scikit-learn </strong> : Sklearn库还提供了AIC/BIC分数的LassoLarsIC估计量，该估计量仅限制线性模型。因此，当涉及到非线性模型时，它没有多大用处。</li><li id="6c2c" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated"><strong class="jf hj">手动计算</strong>:最好通过直接执行公式来计算这些分数。</li></ul><p id="8783" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">让我们切换到用Python 实现AIC/BIC的<a class="ae jv" href="https://medium.com/p/47b336b6ca58/edit" rel="noopener">代码来动手操作。</a></p><h1 id="0bd0" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">参考</h1><ul class=""><li id="cf56" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><br/>http://www.cef-cfr.ca/uploads/reference/mazerolle_2006.pdf<a class="ae jv" href="http://www.cef-cfr.ca/uploads/reference/mazerolle_2006.pdf" rel="noopener ugc nofollow" target="_blank">AIC相关解释:</a></li><li id="ce75" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated"><br/>信息论:<a class="ae jv" href="https://arxiv.org/pdf/1511.00860.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1511.00860.pdf</a></li><li id="0020" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated">强烈推荐<br/><a class="ae jv" href="http://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/BIC.pdf" rel="noopener ugc nofollow" target="_blank">http://www . cs . Toronto . edu/~ mbru bake/teaching/C11/讲义/BIC.pd </a></li><li id="6e60" class="jd je hi jf b jg jw ji jx jk jy jm jz jo ka jq jr js jt ju bi translated"><a class="ae jv" href="https://peerj.com/preprints/1103.pdf" rel="noopener ugc nofollow" target="_blank">https://peerj.com/preprints/1103.pdf</a></li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="ce9a" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated">如果你喜欢这个博客，你可以自由关注这个作者，因为这个作者保证会带来更多有趣的人工智能技术。<br/> 感谢，<br/>阅读愉快！:)</p><p id="4c8a" class="pw-post-body-paragraph kb kc hi jf b jg kd ke kf ji kg kh ki jk kj kk kl jm km kn ko jo kp kq kr jq hb bi translated"><strong class="jf hj"> <em class="ks">可以通过</em></strong><a class="ae jv" href="https://www.linkedin.com/in/kaul-shachi" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj"><em class="ks">LinkedIn</em></strong></a><strong class="jf hj"><em class="ks">取得联系。</em>T53】</strong></p></div></div>    
</body>
</html>