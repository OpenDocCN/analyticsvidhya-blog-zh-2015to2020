<html>
<head>
<title>Machine Learning &amp; Deep Learning Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和深度学习指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-deep-learning-guide-part-1-4ba7ce8cf7eb?source=collection_archive---------4-----------------------#2019-11-09">https://medium.com/analytics-vidhya/machine-learning-deep-learning-guide-part-1-4ba7ce8cf7eb?source=collection_archive---------4-----------------------#2019-11-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="d2c7" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">第1部分:关键术语、定义，以及从监督学习(线性回归)开始</h2></div><blockquote class="ix iy iz"><p id="e5d7" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">第1部分:关键术语，定义，从监督学习开始(线性回归)。</p><p id="e9fd" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><a class="ae jx" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-db520c4797da"> <em class="hi">第二部分:监督学习:回归(SGD)和分类(SVM、朴素贝叶斯、KNN和决策树)。</em> </a></p><p id="ee06" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><a class="ae jx" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-11ad26e0854c"> <em class="hi">第三部分:无监督学习(KMeans，PCA)，欠拟合vs过拟合，交叉验证</em> </a> <em class="hi">。</em></p><p id="95a1" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><a class="ae jx" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-da303a71b8e0"> <em class="hi">第四部分:深度学习:定义、层次、度量和损失、优化器和正则化</em> </a></p></blockquote><p id="8be2" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi kb translated"><span class="l kc kd ke bm kf kg kh ki kj di"> A </span> <em class="jc">几乎每个想要开始学习或从事机器学习和深度学习的人都会被淹没在:理论概念、大量数学规则或大量不必要的细节中。在本教程中，我们将能够理解几乎所有与机器学习和深度学习相关的概念。此外，我们将执行动手的例子，以建立一些与计算机视觉，语音识别和人工智能游戏代理相关的酷模型。之后，你将能够参加一些机器学习挑战，也许你可以获得高排名。</em></p><h1 id="313f" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">介绍</h1><p id="0ca3" class="pw-post-body-paragraph ja jb hi jd b je lc ij jg jh ld im jj jy le jm jn jz lf jq jr ka lg ju jv jw hb bi translated">在教程的这一部分，我们将首先澄清人工智能、机器学习和深度学习之间的混淆。然后，我们将继续了解不同类型的机器学习，并开始为每种类型创建示例。我们将在不深入定义和概念的情况下做到这一点。</p></div><div class="ab cl lh li gp lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="hb hc hd he hf"><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es lo"><img src="../Images/5f3632eff079b769a931021c1bd5cb4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vdHyRQfjt9__LKGgJsC2Kg.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">来源:<a class="ae jx" href="https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/" rel="noopener ugc nofollow" target="_blank">英伟达</a></figcaption></figure><h1 id="476b" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">什么是人工智能(AI)、机器学习(ML)、深度学习(DL)？</h1><p id="2c06" class="pw-post-body-paragraph ja jb hi jd b je lc ij jg jh ld im jj jy le jm jn jz lf jq jr ka lg ju jv jw hb bi translated"><a class="ae jx" href="https://www.tutorialspoint.com/artificial_intelligence_with_python/index.htm" rel="noopener ugc nofollow" target="_blank">人工智能</a> (AI)是机器展示的智能，与人类展示的智能形成对比。</p><p id="3ae5" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><a class="ae jx" href="https://www.tutorialspoint.com/artificial_intelligence_with_python/artificial_intelligence_with_python_machine_learning.htm" rel="noopener ugc nofollow" target="_blank">机器学习</a> (ML)可以定义为计算机科学领域，更具体地说是人工智能的一种应用，它为计算机系统提供了利用数据学习和根据经验改进的能力，而无需显式编程。</p><p id="1dd4" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><a class="ae jx" rel="noopener" href="/datadriveninvestor/difference-between-ml-ai-dl-23db64f7aa2">深度学习</a>(机器学习的子集)的工作方式与我们的大脑类似，使用网状网络，技术上称为<em class="jc">深度神经网络</em>。<br/>就像我们的大脑识别模式来分类事物，并从错误中学习一样——<em class="jc">深度学习也是如此</em>。它将未知数据与已知数据进行比较，从而对其进行相应的分类。</p><p id="8594" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">现在我们知道了区别，我们将开始讨论机器学习，之后我们将开始讨论深度学习。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es me"><img src="../Images/b48b9772461cd474b030b78bf570feac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R2P2e4gQXIDowbz87YXBaw.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">机器学习的类型及其用法</figcaption></figure><h1 id="7928" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">机器学习算法</h1><p id="60cd" class="pw-post-body-paragraph ja jb hi jd b je lc ij jg jh ld im jj jy le jm jn jz lf jq jr ka lg ju jv jw hb bi translated">机器学习可以分为以下几类:</p><ol class=""><li id="f781" class="mf mg hi jd b je jf jh ji jy mh jz mi ka mj jw mk ml mm mn bi translated"><em class="jc">监督学习</em></li><li id="0c35" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><em class="jc">无监督学习</em></li><li id="5dc2" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><em class="jc">强化学习</em></li></ol><p id="e5cb" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> I)监督学习:</strong></p><p id="0998" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">数据被标记为，对于每个输入x，我们有相应的输出y</p><p id="c3f1" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">它主要分为两个概念:</p><ol class=""><li id="1af5" class="mf mg hi jd b je jf jh ji jy mh jz mi ka mj jw mk ml mm mn bi translated"><strong class="jd hj">回归</strong>:预测连续数值。那栋房子会卖多少钱？</li><li id="dc4a" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">分类</strong>:估计离散值，并分配一个标签。它是一只猫还是一只狗？</li></ol><p id="220b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">这里有一些在监督学习中使用的流行算法。</p><p id="b894" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> A. </strong> <strong class="jd hj">回归:</strong></p><ol class=""><li id="54a4" class="mf mg hi jd b je jf jh ji jy mh jz mi ka mj jw mk ml mm mn bi translated">线性回归:我们试图找到输入和输出之间的“线性”关系。假设我们在二维空间上工作，我们在X轴上绘制输入，在Y轴上绘制输出。那么线性回归将是最佳拟合数据点的直线。</li><li id="4f7b" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" href="https://scikit-learn.org/stable/modules/sgd.html#regression" rel="noopener ugc nofollow" target="_blank"> <strong class="jd hj">随机梯度下降</strong></a><strong class="jd hj">【SGD】回归器:</strong>它实现了一个简单的随机梯度下降学习例程，支持不同的损失函数和惩罚来拟合线性回归模型。SGD回归器非常适合处理大量训练样本的回归问题(&gt; 10.000)。</li></ol><p id="9d00" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj">b .</strong>T33】分类:</p><ol class=""><li id="55e4" class="mf mg hi jd b je jf jh ji jy mh jz mi ka mj jw mk ml mm mn bi translated"><strong class="jd hj">支持向量机(SVM): </strong>主要用于分类问题。它使用一个多维超平面将数据分类。</li><li id="5554" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" href="https://www.tutorialspoint.com/artificial_intelligence_with_python/artificial_intelligence_with_python_machine_learning.htm" rel="noopener ugc nofollow" target="_blank"><strong class="jd hj"/></a><strong class="jd hj">:</strong>是一种分类算法。它假设一个类中某个特定特性的存在与任何其他特性的存在无关。</li><li id="ec5b" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" rel="noopener" href="/machine-learning-for-humans/supervised-learning-3-b1551b9c4930"><strong class="jd hj">k-最近邻</strong> </a> <strong class="jd hj"> : </strong>它被广泛用于解决分类问题。其思想是通过寻找<em class="jc"> k </em>个最近数据点标签的<strong class="jd hj">平均值</strong>来标记测试数据点<em class="jc"> x </em>(如果变量是连续的(或者<strong class="jd hj">模式</strong>如果它们是分类的)。</li><li id="6c05" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">决策树:</strong>多用于分类。顾名思义，我们在每个分支(决策节点)上画一棵树，我们根据独立变量分割数据。还有基于决策树构建的高级算法，比如基本上是决策树集合的随机森林。</li></ol><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es mt"><img src="../Images/8bf7014e4d8184c254f8a98440854b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*IHKhPBe4bJN_PRGJrCmYFw.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated"><a class="ae jx" rel="noopener" href="/deep-math-machine-learning-ai/different-types-of-machine-learning-and-their-types-34760b9128a2">监督学习的算法</a></figcaption></figure><p id="2d3d" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> II)无监督学习:</strong></p><p id="d1ea" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">到目前为止，我们考虑了数据被标记的情况。有时候，数据是没有标注的。这里我们将训练模型来预测相应的标签。</p><p id="bb8c" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">它主要分为两个概念:</p><ol class=""><li id="5538" class="mf mg hi jd b je jf jh ji jy mh jz mi ka mj jw mk ml mm mn bi translated"><strong class="jd hj">聚类</strong>:根据相似性将数据分组。</li><li id="7e15" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">降维</strong>:降维压缩数据，同时保持其结构和有用性。</li></ol><p id="1c36" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">这里有一些在无监督学习中使用的流行算法。</p><p id="0692" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj">a .</strong>T26】聚类:</p><p id="0247" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><a class="ae jx" href="https://scikit-learn.org/stable/modules/clustering.html#k-means" rel="noopener ugc nofollow" target="_blank"> <strong class="jd hj"> K-means </strong> </a>:通过尝试将样本分成n组等方差，最小化称为<em class="jc">惯性</em>或类内平方和的标准，对数据进行聚类。该算法要求指定聚类数。</p><p id="8459" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> B. </strong> <strong class="jd hj">降维:</strong></p><p id="0877" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj">主成分分析:</strong>是一种技术，用于强调数据集中的变化和突出强模式。它需要两种技术:</p><ol class=""><li id="2f21" class="mf mg hi jd b je jf jh ji jy mh jz mi ka mj jw mk ml mm mn bi translated"><strong class="jd hj">特征消除:</strong>顾名思义:我们通过消除特征来减少特征空间。</li><li id="ffb2" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c" rel="noopener" target="_blank"> <strong class="jd hj">特征提取</strong> </a> <strong class="jd hj"> : </strong>说我们有十个自变量。在特征提取中，我们创建十个“新”独立变量，其中每个“新”独立变量是十个“旧”独立变量的组合。然而，我们以特定的方式创建这些新的自变量，并根据它们预测因变量的程度对这些新变量进行排序。</li></ol><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es mu"><img src="../Images/8dcea4cd012e8444b7ca8567ad5ee143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*iyAB7zND9onZfRw2ZYVAUg.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">无监督学习:<a class="ae jx" href="http://Clustering" rel="noopener ugc nofollow" target="_blank">聚类</a></figcaption></figure></div><div class="ab cl lh li gp lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="hb hc hd he hf"><h1 id="f737" class="kk kl hi bd km kn mv kp kq kr mw kt ku io mx ip kw ir my is ky iu mz iv la lb bi translated">最后，一些编码</h1><p id="7d0b" class="pw-post-body-paragraph ja jb hi jd b je lc ij jg jh ld im jj jy le jm jn jz lf jq jr ka lg ju jv jw hb bi translated">在进入强化学习之前，是时候做一些编码了。</p><p id="bfd9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">请记住，我们的主要目标是解决机器学习和深度学习的问题和挑战。为此，我们将遵循以下步骤:</p><ol class=""><li id="3ce1" class="mf mg hi jd b je jf jh ji jy mh jz mi ka mj jw mk ml mm mn bi translated"><strong class="jd hj">数据定义</strong>:我们有一些数据。该数据由用变量X表示的输入或特征和<em class="jc">有时是</em>输出组成，后者是要从模型中提取的由变量Y表示的标签或预测。</li><li id="ddb2" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">训练/测试分割</strong>:我们将数据分割成训练和测试(稍后我们将介绍开发测试，但让我们坚持基础)</li><li id="e601" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">预处理:</strong>我们对每个数据集进行一些预处理、清除和校正。</li><li id="2bb7" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">算法选择:</strong>我们将根据问题的类型以及算法的好处和用途，使用一种或多种算法。该型号有一些<em class="jc">参数</em>可以设置。</li><li id="8939" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">训练:</strong>我们’<em class="jc">在训练集上训练’</em>我们的算法。也就是学习从特征(x)到标签(y)的映射。在这个阶段，模型的权重被更新。</li><li id="16ea" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">预测</strong>:我们用<em class="jc">“更新权重”</em>在测试集上测试我们的模型</li><li id="ff61" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">评估模型的性能</strong>:我们得到一个特定的度量标准(例如准确性)来评估我们的模型。</li><li id="8f12" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">微调</strong>:根据性能结果，我们开始<em class="jc">调整</em>并更新我们的参数，以提高精度并降低损耗。然后，我们重复步骤4-6进行一些迭代，或者直到精度和损失变得一致。</li></ol><p id="cb1b" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">我们将使用python和一个<a class="ae jx" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"><strong class="jd hj"><em class="jc">sci kit-Learn</em></strong></a>，它是一个开源的Python库，使用统一的接口实现了一系列机器学习、预处理、交叉验证和可视化算法。</p></div><div class="ab cl lh li gp lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="hb hc hd he hf"><h1 id="2797" class="kk kl hi bd km kn mv kp kq kr mw kt ku io mx ip kw ir my is ky iu mz iv la lb bi translated"><strong class="ak">监督学习-线性回归:</strong></h1><blockquote class="ix iy iz"><p id="2f16" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">你可以从<a class="ae jx" href="https://www.kaggle.com/mohammadhatoum/linearregression-example?scriptVersionId=23239900" rel="noopener ugc nofollow" target="_blank">这里</a>下载完整的Kaggle笔记本</p></blockquote><ol class=""><li id="8cf5" class="mf mg hi jd b je jf jh ji jy mh jz mi ka mj jw mk ml mm mn bi translated"><strong class="jd hj">数据定义</strong>:我们将使用<a class="ae jx" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data" rel="noopener ugc nofollow" target="_blank"> <strong class="jd hj"> <em class="jc">波士顿房屋数据</em> </strong> </a>。我们希望根据一些<a class="ae jx" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names" rel="noopener ugc nofollow" target="_blank"> <strong class="jd hj"> <em class="jc">属性</em></strong></a><strong class="jd hj"><em class="jc"/></strong>来预测房价，如城镇人均犯罪率、25，000平方米以上地段的住宅用地比例。英尺，每个住宅的平均房间数，等等。我下载了该文件，将其重命名为boston.csv，并添加了以下行作为文件的标题:<br/> "CRIM "、" ZN "、" INDUS "、" CHAS "、" NOX "、" RM "、" AGE "、" DIS "、" RAD "、" TAX "、" PTRATIO "、" B "、" LSTAT "、" MEDV "</li></ol><pre class="lp lq lr ls fd na nb nc nd aw ne bi"><span id="0f88" class="nf kl hi nb b fi ng nh l ni nj"># Load the diabetes dataset<br/>import pandas as pd<br/>boston = pd.read_csv('data/boston.csv')</span><span id="aedf" class="nf kl hi nb b fi nk nh l ni nj">y = boston['MEDV']<br/>X = boston.drop('MEDV',axis=1)</span><span id="31ff" class="nf kl hi nb b fi nk nh l ni nj">#View the top 5 rows<br/>boston.head())</span></pre><figure class="lp lq lr ls fd lt er es paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="er es nl"><img src="../Images/59392c95fd6b6cd90179c5b425e9c09c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QwzA5genUp5zN7xQHOOs6Q.png"/></div></div><figcaption class="ma mb et er es mc md bd b be z dx translated">显示波士顿数据集中的前五条记录</figcaption></figure><p id="88e5" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> 2。训练/测试分割:</strong>由于数据集非常小，我们将把它分成10%用于测试，90%用于训练</p><pre class="lp lq lr ls fd na nb nc nd aw ne bi"><span id="6407" class="nf kl hi nb b fi ng nh l ni nj"># Split train and test set</span><span id="3eef" class="nf kl hi nb b fi nk nh l ni nj">from sklearn.model_selection import train_test_split<br/>X_train,X_test,y_train,y_test = train_test_split(X,y,test_size= 0.1, random_state=0)</span><span id="5a24" class="nf kl hi nb b fi nk nh l ni nj"># View the shape (structure) of the data<br/>print(f"Training features shape: {X_train.shape}")<br/>print(f"Testing features shape: {X_test.shape}")<br/>print(f"Training label shape: {y_train.shape}")<br/>print(f"Testing label  shape: {y_test.shape}")</span></pre><blockquote class="nm"><p id="b9d5" class="nn no hi bd np nq nr ns nt nu nv jw dx translated">结果:<br/>训练特征形状:(455，13) <br/>测试特征形状:(51，13) <br/>训练标签形状:(455，)<br/>测试标签形状:(51，)</p></blockquote><p id="6664" class="pw-post-body-paragraph ja jb hi jd b je nw ij jg jh nx im jj jy ny jm jn jz nz jq jr ka oa ju jv jw hb bi translated"><strong class="jd hj"> 3。</strong> <strong class="jd hj">预处理:</strong>我们<strong class="jd hj"> </strong>不需要做任何数据预处理<strong class="jd hj">。</strong>但是在后面的例子里会用到<strong class="jd hj">。</strong></p><p id="0f82" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> 4。算法选择:</strong>我们将使用线性回归</p><pre class="lp lq lr ls fd na nb nc nd aw ne bi"><span id="de2d" class="nf kl hi nb b fi ng nh l ni nj">#Linear Regression<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error<br/>lr = LinearRegression(normalize=True)</span></pre><p id="cd70" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> 5。</strong> <strong class="jd hj">训练:</strong>简单来说我们调用函数fit，给它X_train和y_train作为参数。</p><pre class="lp lq lr ls fd na nb nc nd aw ne bi"><span id="b707" class="nf kl hi nb b fi ng nh l ni nj"><em class="jc">lr.fit(X_train, y_train) # Fit the model to the data</em></span></pre><p id="94f4" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> 6。预测:</strong>我们简单地称之为预测</p><pre class="lp lq lr ls fd na nb nc nd aw ne bi"><span id="3630" class="nf kl hi nb b fi ng nh l ni nj"><em class="jc">y_pred_lr = lr.predict(X_test)  # Predict labels</em></span></pre><p id="2fee" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> 7。</strong> <strong class="jd hj">评估模型的性能:</strong></p><pre class="lp lq lr ls fd na nb nc nd aw ne bi"><span id="4057" class="nf kl hi nb b fi ng nh l ni nj"># The mean squared error<br/>print(f"Mean squared error: { mean_squared_error(y_test, y_pred_lr)}")</span><span id="0ce2" class="nf kl hi nb b fi nk nh l ni nj"># Explained variance score: 1 is perfect prediction<br/>print(f"Variance score: {r2_score(y_test, y_pred_lr)}")</span><span id="150c" class="nf kl hi nb b fi nk nh l ni nj"># Mean Absolute Error<br/>print(f"Mean squared error: { mean_absolute_error(y_test, y_pred_lr)}")</span></pre><blockquote class="nm"><p id="544f" class="nn no hi bd np nq nr ns nt nu nv jw dx translated">结果:<br/>均方误差:41.72457625585755 <br/>方差得分:0.5149666667</p></blockquote><p id="71c0" class="pw-post-body-paragraph ja jb hi jd b je nw ij jg jh nx im jj jy ny jm jn jz nz jq jr ka oa ju jv jw hb bi translated"><strong class="jd hj"> 8。微调:</strong>在这个阶段我们不会做任何微调。让事情变得简单顺利。</p></div><div class="ab cl lh li gp lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="hb hc hd he hf"><h1 id="677d" class="kk kl hi bd km kn mv kp kq kr mw kt ku io mx ip kw ir my is ky iu mz iv la lb bi translated">错误和指标:</h1><p id="a4f6" class="pw-post-body-paragraph ja jb hi jd b je lc ij jg jh ld im jj jy le jm jn jz lf jq jr ka lg ju jv jw hb bi translated">从上面的第7步可以看出，我们使用了3个函数:<em class="jc"> mean_squared_error、r2_score、</em>和<em class="jc"> mean_absolute_error。</em>但是各自是什么意思呢？我们将再次使用<a class="ae jx" href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>中的定义</p><ol class=""><li id="f607" class="mf mg hi jd b je jf jh ji jy mh jz mi ka mj jw mk ml mm mn bi translated"><strong class="jd hj">均方误差:</strong>是与平方(二次)误差或损失的期望值相对应的风险度量。</li></ol><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es ob"><img src="../Images/56fede2842485f88694f8938f597b73c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*Uvp9oarTxIzXi7C9DzK1QQ.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">来源:https://scikit-learn.org</figcaption></figure><p id="da3a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> 2。r得分，决定系数:</strong>表示模型中已被自变量解释的方差(占y的)比例。它提供了一个拟合优度的指标，从而通过解释方差的比例来衡量模型预测未知样本的可能性。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es oc"><img src="../Images/7e1a37dc4945998131f0c405e7ebde9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*EoFD3CO3nzbY4vjyirdhPQ.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">来源:https://scikit-learn.org<a class="ae jx" href="https://scikit-learn.org" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="40b3" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated"><strong class="jd hj"> 3。平均绝对误差:</strong>是与绝对误差损失或L1-范数损失的期望值相对应的风险度量。</p><figure class="lp lq lr ls fd lt er es paragraph-image"><div class="er es od"><img src="../Images/187a1bb8726a976fd6a2df121beb92b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*92nLwjVih_Nm0VMpfaHhyA.png"/></div><figcaption class="ma mb et er es mc md bd b be z dx translated">来源:<a class="ae jx" href="https://scikit-learn.org" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org</a></figcaption></figure></div><div class="ab cl lh li gp lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="hb hc hd he hf"><h1 id="9a2c" class="kk kl hi bd km kn mv kp kq kr mw kt ku io mx ip kw ir my is ky iu mz iv la lb bi translated">概述</h1><p id="af39" class="pw-post-body-paragraph ja jb hi jd b je lc ij jg jh ld im jj jy le jm jn jz lf jq jr ka lg ju jv jw hb bi translated">我们已经到了本系列第1部分的末尾。在这一部分，我们能够了解到:</p><ol class=""><li id="1e04" class="mf mg hi jd b je jf jh ji jy mh jz mi ka mj jw mk ml mm mn bi translated"><strong class="jd hj">人工智能</strong> ( <strong class="jd hj"> AI </strong>)、<strong class="jd hj">机器学习</strong> ( <strong class="jd hj"> ML </strong>)和<strong class="jd hj">深度学习</strong> ( <strong class="jd hj"> DL </strong>)的区别。</li><li id="9e48" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated">最流行的机器学习类型:<strong class="jd hj">有监督的</strong>、<strong class="jd hj">无监督的</strong>和<strong class="jd hj">强化的</strong>。</li><li id="7b6e" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated">监督学习的不同概念:<strong class="jd hj">回归</strong>和<strong class="jd hj">分类。</strong>以及一些流行的回归算法的定义:<strong class="jd hj">线性回归</strong>和<strong class="jd hj">随机梯度下降</strong> <strong class="jd hj"> (SGD)回归器。</strong>用于分类:<strong class="jd hj">支持向量机、</strong> <strong class="jd hj">朴素贝叶斯、</strong><strong class="jd hj">【k-最近邻】、</strong>和<strong class="jd hj">决策树。</strong></li><li id="d3af" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated">无监督学习的不同概念:<strong class="jd hj">聚类</strong>和<strong class="jd hj">降维。</strong>以及一些流行的聚类算法(<strong class="jd hj"> K均值)</strong>和<strong class="jd hj"> </strong>降维算法<strong class="jd hj"> ( </strong> <strong class="jd hj">主成分分析)</strong>的定义</li><li id="d6d3" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated">解决机器学习和深度学习问题与挑战的步骤:<br/> <em class="jc"> 1。数据定义<br/> 2。训练/测试分割<br/> 3。预处理<br/> 4。算法选择<br/> 5。训练<br/> 6。预测<br/> 7。评估模型的性能<br/> 8。微调</em></li><li id="3722" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><strong class="jd hj">回归中常用的<strong class="jd hj">误差和度量</strong>:均方差</strong>、<strong class="jd hj"> R得分</strong>(决定系数)、平均绝对误差<strong class="jd hj">。</strong></li></ol><p id="bf51" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">最后，我们有一个如何使用<a class="ae jx" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"><strong class="jd hj"><em class="jc">Scikit-Learn</em></strong></a><strong class="jd hj"><em class="jc"/></strong>的实际操作示例，并且我们执行了一个完整的<strong class="jd hj">线性回归示例。</strong></p><p id="d371" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">在第2部分中，我们将继续讨论剩余的监督学习算法的例子，以及用于分类的相应误差和度量。我们还将对我们的数据进行一些预处理，我们将学习一些新概念，如交叉验证、过拟合和欠拟合。</p><p id="1588" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jy jl jm jn jz jp jq jr ka jt ju jv jw hb bi translated">感谢阅读！</p></div><div class="ab cl lh li gp lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="hb hc hd he hf"><h1 id="957d" class="kk kl hi bd km kn mv kp kq kr mw kt ku io mx ip kw ir my is ky iu mz iv la lb bi translated">参考链接:</h1><ol class=""><li id="9568" class="mf mg hi jd b je lc jh ld jy oe jz of ka og jw mk ml mm mn bi translated"><a class="ae jx" href="https://www.tutorialspoint.com/artificial_intelligence_with_python/index.htm" rel="noopener ugc nofollow" target="_blank">https://www . tutorialspoint . com/artificial _ intelligence _ with _ python/index . htm</a></li><li id="1c2a" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" href="https://www.tutorialspoint.com/artificial_intelligence_with_python/artificial_intelligence_with_python_machine_learning.htm" rel="noopener ugc nofollow" target="_blank">https://www . tutorialspoint . com/artificial _ intelligence _ with _ python/artificial _ intelligence _ with _ python _ machine _ learning . htm</a></li><li id="f310" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" rel="noopener" href="/datadriveninvestor/difference-between-ml-ai-dl-23db64f7aa2">https://medium . com/datadriveninvestor/difference-between-ml-ai-dl-23 db 64 f 7 aa 2</a></li><li id="8a8b" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" href="https://scikit-learn.org/stable/modules/sgd.html#regression" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/sgd.html#regression</a></li><li id="7112" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" rel="noopener" href="/machine-learning-for-humans/supervised-learning-3-b1551b9c4930">https://medium . com/machine-learning-for-humans/supervised-learning-3-b 1551 b9c 4930</a></li><li id="0f4e" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" href="https://scikit-learn.org/stable/modules/clustering.html#k-means" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/clustering . html # k-means</a></li><li id="edf0" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c" rel="noopener" target="_blank">https://towards data science . com/a-一站式主成分分析-5582fb7e0a9c </a></li><li id="1e56" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/machine-learning-databases/housing/housing . data</a></li><li id="2e41" class="mf mg hi jd b je mo jh mp jy mq jz mr ka ms jw mk ml mm mn bi translated"><a class="ae jx" href="https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.names" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/machine-learning-database/housing/housing . names</a></li></ol></div></div>    
</body>
</html>