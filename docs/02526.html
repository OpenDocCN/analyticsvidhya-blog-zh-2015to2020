<html>
<head>
<title>Attention Mechanism: A Quick Intuition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意机制:一种快速直觉</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/attention-mechanism-a-quick-intuition-26e154cdb49a?source=collection_archive---------5-----------------------#2019-12-21">https://medium.com/analytics-vidhya/attention-mechanism-a-quick-intuition-26e154cdb49a?source=collection_archive---------5-----------------------#2019-12-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7f8584f1bd2232ce352afac3c33be528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6W5EVy3jMlRh9XJr"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">斯蒂芬·科斯马在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><h1 id="ac2d" class="jc jd hi bd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz bi translated"><strong class="ak">内容:</strong></h1><ol class=""><li id="cc23" class="ka kb hi kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">介绍</li><li id="d72f" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn ko kp kq kr bi translated">序列间模型</li><li id="e7ef" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn ko kp kq kr bi translated">Seq2Seq型号的问题</li><li id="2a66" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn ko kp kq kr bi translated">需要关注</li><li id="d899" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn ko kp kq kr bi translated">自定义Keras关注层-代码示例</li><li id="1f82" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn ko kp kq kr bi translated">使用TensorFlow 2.0更新</li><li id="95ce" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn ko kp kq kr bi translated">结论</li><li id="df64" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn ko kp kq kr bi translated">参考</li></ol><h2 id="ae32" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated"><strong class="ak"> 1。简介</strong></h2><p id="f5a3" class="pw-post-body-paragraph ll lm hi kc b kd ke ln lo kf kg lp lq kh lr ls lt kj lu lv lw kl lx ly lz kn hb bi translated">在这篇文章中，我们将试图理解注意机制的基本直觉以及它为什么会出现。我们的目标是理解编码器-解码器模型的工作原理，以及注意力如何帮助获得更好的结果。</p><p id="5fde" class="pw-post-body-paragraph ll lm hi kc b kd ma ln lo kf mb lp lq kh mc ls lt kj md lv lw kl me ly lz kn hb bi translated">我们将看到如何使用Keras和TensorFlow 2.0提供的默认关注层来构建自定义关注层。</p><h2 id="c101" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated"><strong class="ak"> 2。序列间模型</strong></h2><p id="0ece" class="pw-post-body-paragraph ll lm hi kc b kd ke ln lo kf kg lp lq kh lr ls lt kj lu lv lw kl lx ly lz kn hb bi translated">序列到序列模型旨在将固定长度的输入映射到固定长度的输出，其中输入和输出的长度可能不同。<br/>示例:<em class="mf">“您正在阅读这篇文章”</em>在英语中是<em class="mf">“Vous lisez cet article”</em>在法语中<br/>输入长度= 5个单词<br/>输出长度= 4个单词<br/>在这里，LSTM/GRU无法将英语的每个单词映射到法语，因此我们使用一个序列到序列模型来解决类似这样的问题。</p><p id="48a3" class="pw-post-body-paragraph ll lm hi kc b kd ma ln lo kf mb lp lq kh mc ls lt kj md lv lw kl me ly lz kn hb bi translated">递归神经网络(RNN)的编码器-解码器模型是一种强大的序列到序列模型。这些模型在自然语言处理领域有很多使用案例，如机器翻译、图像字幕和文本摘要。</p><figure class="mh mi mj mk fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/36e1d9bd2e9459e7ae1447b1f5a0fddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*M6LYyQu9MbffqqFJJyjXBQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">谷歌翻译</figcaption></figure><p id="3f54" class="pw-post-body-paragraph ll lm hi kc b kd ma ln lo kf mb lp lq kh mc ls lt kj md lv lw kl me ly lz kn hb bi translated">序列到序列模型有两个组件，一个<strong class="kc hj">编码器</strong>和一个<strong class="kc hj">解码器</strong>。编码器将源句子编码成一个简洁的向量(称为<strong class="kc hj">上下文向量</strong>)，解码器将上下文向量作为输入，并使用编码的表示计算翻译。</p><h2 id="13fb" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated">3.Seq2Seq型号的问题</h2><p id="52ff" class="pw-post-body-paragraph ll lm hi kc b kd ke ln lo kf kg lp lq kh lr ls lt kj lu lv lw kl lx ly lz kn hb bi translated">这些模型的一个问题是，性能随着输入句子长度的增加而衰减。原因是—</p><ul class=""><li id="75f2" class="ka kb hi kc b kd ma kf mb kh ml kj mm kl mn kn mo kp kq kr bi translated">要预测的单词取决于输入句子的上下文，而不是单个单词。所以，基本上要预测一个法语单词，我们可以利用英语句子中的2-3个单词。这就是人类如何将一种语言翻译成另一种语言。</li><li id="8645" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn mo kp kq kr bi translated">另一个限制是，对于较长的句子，我们必须将输入句子的所有信息压缩到一个固定长度的向量中。并非句子中的所有单词对于预测正确的单词都很重要。</li></ul><p id="eac7" class="pw-post-body-paragraph ll lm hi kc b kd ma ln lo kf mb lp lq kh mc ls lt kj md lv lw kl me ly lz kn hb bi translated">现在，随着输入句子长度的增加，随着时间的推移，我们的LSTM/GRU失去了长句的上下文，从而失去了整个句子的意义，并最终导致较差的性能。</p><h2 id="a724" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated">4.需要关注</h2><p id="a085" class="pw-post-body-paragraph ll lm hi kc b kd ke ln lo kf kg lp lq kh lr ls lt kj lu lv lw kl lx ly lz kn hb bi translated">为了解决上述局限性，引入了注意机制。</p><p id="9b5c" class="pw-post-body-paragraph ll lm hi kc b kd ma ln lo kf mb lp lq kh mc ls lt kj md lv lw kl me ly lz kn hb bi translated">注意力的整体思想是，解码器可以访问编码器的过去状态，而不是仅仅依赖于上下文向量。在每个解码步骤中，解码器都会查看编码器的任何特定状态。</p><p id="8f53" class="pw-post-body-paragraph ll lm hi kc b kd ma ln lo kf mb lp lq kh mc ls lt kj md lv lw kl me ly lz kn hb bi translated">注意机制试图识别输入序列的哪些部分与输出中的每个单词相关，并使用<strong class="kc hj">相关信息</strong>来选择适当的输出。</p><h2 id="2546" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated">工作</h2><p id="8c59" class="pw-post-body-paragraph ll lm hi kc b kd ke ln lo kf kg lp lq kh lr ls lt kj lu lv lw kl lx ly lz kn hb bi translated">这里，我们利用双向GRU单元，其中输入序列向前和向后传递。输出然后被连接并传递给解码器。</p><figure class="mh mi mj mk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/e76c8735f866ee56a10f06b729f4cbb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8zI6BcIK_51m_m7egEkT7Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">双向GRU细胞</figcaption></figure><p id="6e0b" class="pw-post-body-paragraph ll lm hi kc b kd ma ln lo kf mb lp lq kh mc ls lt kj md lv lw kl me ly lz kn hb bi translated">为了解决这一限制，我们使用选定数量的过去编码状态的加权和。我们有两个约束:- <br/> 1。必要的过去状态数<br/> 2。所选过去状态的权重<br/>由于这些约束可以通过反向支持来学习，我们可以假设这是适合编码器和解码器之间的层</p><figure class="mh mi mj mk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/403756a680854919128baab7bc0556fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*YSIpAN5ulkV3z4ahxFxDnw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">编解码模型中的注意机制</figcaption></figure><h2 id="2d4f" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated">5.自定义Keras关注层-代码示例</h2><p id="d3cb" class="pw-post-body-paragraph ll lm hi kc b kd ke ln lo kf kg lp lq kh lr ls lt kj lu lv lw kl lx ly lz kn hb bi translated">这是一个代码片段，用于为其中一个问题创建关注层。</p><pre class="mh mi mj mk fd mr ms mt mu aw mv bi"><span id="30d3" class="kx jd hi ms b fi mw mx l my mz">from keras.layers import Layer<br/>import keras.backend as K</span><span id="46b7" class="kx jd hi ms b fi na mx l my mz">class Attention(Layer):<br/>    def __init__(self,**kwargs):<br/>        super(attention,self).__init__(**kwargs)<br/><br/>    def build(self,input_shape): <br/>        <strong class="ms hj"><em class="mf">"""<br/>        Matrices for creating the context vector.<br/>        """</em></strong></span><span id="81d9" class="kx jd hi ms b fi na mx l my mz">        self.W=self.add_weight(name="att_weight",shape=(input_shape[-1],1),initializer="normal")<br/>        self.b=self.add_weight(name="att_bias",shape=(input_shape[1],1),initializer="zeros")        <br/>        super(attention, self).build(input_shape)<br/><br/>    def call(self,x):<br/>        <strong class="ms hj"><em class="mf">"""<br/>        Function which does the computation and is passed through a softmax layer to calculate the attention probabilities and context vector. <br/>        """</em></strong></span><span id="d40d" class="kx jd hi ms b fi na mx l my mz">et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)<br/>        at=K.softmax(et)<br/>        at=K.expand_dims(at,axis=-1)<br/>        output=x*at<br/>        return K.sum(output,axis=1)<br/><br/>    def compute_output_shape(self,input_shape):<br/>        <strong class="ms hj"><em class="mf">"""<br/>        For Keras internal compatibility checking.<br/>        """<br/>        </em></strong>return (input_shape[0],input_shape[-1])<br/><br/>    def get_config(self):<br/>        <strong class="ms hj"><em class="mf">"""<br/>        The get_config() method collects the input shape and other information about the model.<br/>        """<br/>        </em></strong>return super(attention,self).get_config()</span></pre><figure class="mh mi mj mk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/98909202b46b4b6921aa9fe69b5e0a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CFm2Li4XZidbghK9Wk9wFQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://blog.floydhub.com/attention-mechanism/" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h2 id="11d9" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated">模型结构</h2><pre class="mh mi mj mk fd mr ms mt mu aw mv bi"><span id="fc88" class="kx jd hi ms b fi mw mx l my mz">input_text_bgru = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='float32')<br/>embedding_layer_bgru = Embedding(len(tokenizer.word_index) + 1,<br/>                                    300,<br/>                                    weights=[embedding_matrix],<br/>                                    input_length=MAX_SEQUENCE_LENGTH,<br/>                                    trainable=<strong class="ms hj">False</strong>)<br/>g = embedding_layer_bgru(input_text_bgru)<br/>g = SpatialDropout1D(0.4)(g)<br/>g = Bidirectional(GRU(64, return_sequences=<strong class="ms hj">True</strong>))(g)<br/>att = Attention(MAX_SEQUENCE_LENGTH)(g)<br/>g = Conv1D(64, kernel_size = 3, padding = "valid", kernel_initializer = "he_uniform")(g)<br/>avg_pool1 = GlobalAveragePooling1D()(g)<br/>max_pool1 = GlobalMaxPooling1D()(g)<br/>g = concatenate([att,avg_pool1, max_pool1])<br/>g = Dense(128, activation='relu')(g)<br/>bgru_output = Dense(2, activation='softmax')(g)</span></pre><h2 id="01d3" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated">6.使用TensorFlow 2.0更新</h2><p id="168e" class="pw-post-body-paragraph ll lm hi kc b kd ke ln lo kf kg lp lq kh lr ls lt kj lu lv lw kl lx ly lz kn hb bi translated">在TensorFlow 2.0中，注意力层被添加为其中一个层，现在可以直接实现，而无需显式定义。</p><pre class="mh mi mj mk fd mr ms mt mu aw mv bi"><span id="d856" class="kx jd hi ms b fi mw mx l my mz">query_value_attention_seq = <strong class="ms hj">tf.keras.layers.Attention()</strong>(<br/>    [query_seq_encoding, value_seq_encoding])</span></pre><p id="42d6" class="pw-post-body-paragraph ll lm hi kc b kd ma ln lo kf mb lp lq kh mc ls lt kj md lv lw kl me ly lz kn hb bi translated">这使得它更容易实现，并且在设计复杂的架构时，对于机器学习开发者来说变得不那么麻烦。</p><h2 id="c2c5" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated">7.结论</h2><p id="04cf" class="pw-post-body-paragraph ll lm hi kc b kd ke ln lo kf kg lp lq kh lr ls lt kj lu lv lw kl lx ly lz kn hb bi translated">注意力机制并不局限于机器翻译。它用于图像字幕，在CNN的帮助下，我们利用视觉注意力来获得特征地图。</p><p id="d671" class="pw-post-body-paragraph ll lm hi kc b kd ma ln lo kf mb lp lq kh mc ls lt kj md lv lw kl me ly lz kn hb bi translated">注意力机制的用途超出了我们在本文中提到的范围。希望你能对注意力机制试图解决的问题有一个大致的了解。本文用RNNs实现了Seq2Seq模型中的基本注意机制。然而，像<strong class="kc hj">谷歌的BERT </strong>和<strong class="kc hj"> XLNet </strong>这样的变形金刚模型是利用自我关注机制的重大进步，目前是NLP领域的最新技术。</p><h2 id="391d" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated">8.参考</h2><ul class=""><li id="ee1a" class="ka kb hi kc b kd ke kf kg kh ki kj kk kl km kn mo kp kq kr bi translated"><a class="ae iu" href="https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39" rel="noopener" target="_blank">https://towards data science . com/light-on-math-ml-attention-with-keras-DC 8 db C1 fad 39</a></li><li id="3a30" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn mo kp kq kr bi translated"><a class="ae iu" href="https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/</a></li><li id="2e74" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn mo kp kq kr bi translated"><a class="ae iu" href="https://blog.floydhub.com/attention-mechanism/" rel="noopener ugc nofollow" target="_blank">https://blog.floydhub.com/attention-mechanism/</a></li><li id="9c94" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn mo kp kq kr bi translated">【https://arxiv.org/abs/1409.0473 T2】号</li><li id="386c" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn mo kp kq kr bi translated">【https://arxiv.org/abs/1502.03044 T4】</li><li id="db8e" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn mo kp kq kr bi translated"><a class="ae iu" href="https://towardsdatascience.com/transformers-141e32e69591" rel="noopener" target="_blank">https://towardsdatascience.com/transformers-141e32e69591</a></li><li id="d997" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn mo kp kq kr bi translated"><a class="ae iu" href="https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad" rel="noopener" target="_blank">https://towards data science . com/understanding-Bert-is-it-a-game-changer-in-NLP-7 CCA 943 cf 3 ad</a></li></ul><h2 id="fb96" class="kx jd hi bd je ky kz la ji lb lc ld jm kh le lf jq kj lg lh ju kl li lj jy lk bi translated">与我联系</h2><ul class=""><li id="74ab" class="ka kb hi kc b kd ke kf kg kh ki kj kk kl km kn mo kp kq kr bi translated"><a class="ae iu" href="https://www.linkedin.com/in/nihal-das/" rel="noopener ugc nofollow" target="_blank"> <strong class="kc hj"> LinkedIn </strong> </a></li><li id="6189" class="ka kb hi kc b kd ks kf kt kh ku kj kv kl kw kn mo kp kq kr bi translated"><a class="ae iu" href="https://github.com/Nihal2409/" rel="noopener ugc nofollow" target="_blank"> <strong class="kc hj"> GitHub </strong> </a></li></ul><p id="f63e" class="pw-post-body-paragraph ll lm hi kc b kd ma ln lo kf mb lp lq kh mc ls lt kj md lv lw kl me ly lz kn hb bi translated"><strong class="kc hj"> ***感谢大家阅读本文。非常感谢您的建议！*** </strong></p></div></div>    
</body>
</html>