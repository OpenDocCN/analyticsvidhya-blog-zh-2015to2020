<html>
<head>
<title>Linear Regression With Gradient Descent Derivation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降导数线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-with-gradient-descent-derivation-c10685ddf0f4?source=collection_archive---------0-----------------------#2020-09-16">https://medium.com/analytics-vidhya/linear-regression-with-gradient-descent-derivation-c10685ddf0f4?source=collection_archive---------0-----------------------#2020-09-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ac4f168eaf786f3326d554f2dbabae61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5fYeUhdoQ8eDQL_PVL7FA.jpeg"/></div></div></figure><h1 id="45af" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">先决条件</h1><p id="cec1" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">唯一的先决条件是微分和矩阵乘法。</p><h1 id="8a4c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是线性回归？</h1><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es km"><img src="../Images/2fdb30fa6c443abd752cd1719d982ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*KsEqnI9pITu20-sRyUOZJw.png"/></div></figure><p id="cd37" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">简单的线性回归基本上是线性相关变量之间的线性关系的建模，该线性相关变量随后可用于预测新的独立变量的相关变量值<br/>为此，我们使用以下直线的方程:<code class="du kw kx ky kz b">y = m * x + c.</code></p><p id="b0e4" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">其中 y 是因变量，x 是自变量</p><p id="afcd" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated"><strong class="jq hj">例如</strong>:我们可以根据一个人的工作经验来预测他的工资。<br/>这里，薪水是因变量，经验是自变量，因为我们是在经验的帮助下预测薪水的。</p><p id="dee3" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated"><strong class="jq hj">何时使用线性回归</strong>:可以对因变量和自变量之间存在良好线性关系的数据进行线性回归。<br/>线性关系的程度可以借助相关性来发现。</p><h1 id="410f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">梯度下降</h1><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es la"><img src="../Images/ffe0e94229d3fe83ab6e1f51d2922db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*lYpF8xJ3TiDoq461I0AcOQ.jpeg"/></div></figure><p id="2687" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">梯度下降是一种优化算法，可用于寻找可微函数的全局或局部最小值。</p><h2 id="15c8" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated">各种各样的假设和定义。</h2><p id="0479" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">假设我们有一个自变量 x 和一个因变量 y. <br/>为了形成这 2 个变量之间的关系，我们有方程:<br/> <code class="du kw kx ky kz b"><strong class="jq hj">y = x * w + b</strong></code> <br/>其中<strong class="jq hj"> w </strong>是权重(或斜率)<br/> <strong class="jq hj"> b </strong>是偏差(或截距)<br/> <strong class="jq hj"> x </strong>是自变量列向量(例题)， <br/> <strong class="jq hj"> y </strong>是因变量列向量(示例)<br/>我们的主要目标是找到正确定义变量<strong class="jq hj"> x </strong>和<strong class="jq hj"> y </strong>之间关系的<strong class="jq hj"> w </strong>和<strong class="jq hj"> b </strong>。 我们借助一种叫做<strong class="jq hj">损失函数的东西来实现这一点。</strong></p><p id="6e3c" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated"><strong class="jq hj">损失函数:</strong>损失函数是表示我们的预测值与因变量的实际值偏离多少的函数。</p><p id="3eff" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">重要提示:<em class="lp">我们正试图找到</em> <strong class="jq hj"> <em class="lp"> w </em> </strong> <em class="lp">和</em> <strong class="jq hj"> <em class="lp"> b </em> </strong> <em class="lp">的值，以使我们的损失函数最小化。</em></p><h2 id="b278" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated">实施梯度下降的线性回归所涉及的步骤</h2><ol class=""><li id="800f" class="lq lr hi jq b jr js jv jw jz ls kd lt kh lu kl lv lw lx ly bi translated">随机初始化权重和偏差，或者用 0 初始化(两者都可以)。</li><li id="4d24" class="lq lr hi jq b jr lz jv ma jz mb kd mc kh md kl lv lw lx ly bi translated">用这个初始权重和偏差进行预测。</li><li id="91a1" class="lq lr hi jq b jr lz jv ma jz mb kd mc kh md kl lv lw lx ly bi translated">将这些预测值与实际值进行比较，并使用这些预测值和实际值定义损失函数。</li><li id="fd1f" class="lq lr hi jq b jr lz jv ma jz mb kd mc kh md kl lv lw lx ly bi translated">借助微分，计算损失函数如何随权重和偏差项而变化。</li><li id="2bda" class="lq lr hi jq b jr lz jv ma jz mb kd mc kh md kl lv lw lx ly bi translated">更新权重和偏差项，以便最小化损失函数。</li></ol><h1 id="0a68" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">用数学实现</h1><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/a74ebd41af8fcc5bfaa1b034c955ac7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10JTA2S8Q8H6_mVsaqJDCQ.jpeg"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">自变量和因变量的例子分别为</figcaption></figure><h2 id="aabf" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated">1 .假定</h2><p id="5062" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">假设我们有一个<strong class="jq hj"> x </strong>和<strong class="jq hj"> y </strong>向量，如上图所示(<em class="lp">上图只是一个例子</em>)。</p><h2 id="2df9" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated">2.将 w 和 b 初始化为 0</h2><p id="a2f2" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj"> w </strong> = 0，<strong class="jq hj"> b </strong> = 0</p><h2 id="3a2e" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated">3.用现在的 w 和 b 做一些预测，当然会错。</h2><p id="d0eb" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><code class="du kw kx ky kz b"><strong class="jq hj">y_pred = x*w + b</strong></code>，其中 y_pred 代表预测的 y 值。<br/>这个 y_pred 也会像 y 一样是一个向量。</p><h2 id="7b00" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated">4 .定义损失函数</h2><p id="1e15" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><code class="du kw kx ky kz b"><strong class="jq hj">loss = (y_pred — y)²/n</strong></code> <br/>其中 n 是数据集中实例的数量。很明显，这个损失函数代表了预测值与实际值的偏差。<br/>这个损失函数也将是一个向量。但是，我们将对向量中的所有元素求和，将其转换为标量。</p><h2 id="a980" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated">5.计算(<strong class="ak"> ∂ </strong>(损失)/ <strong class="ak"> ∂w) </strong></h2><blockquote class="mj mk ml"><p id="96ea" class="jo jp lp jq b jr kr jt ju jv ks jx jy mm kt kb kc mn ku kf kg mo kv kj kk kl hb bi translated">实变量函数的导数衡量的是函数值相对于自变量变化的敏感度。</p></blockquote><p id="079d" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">我们可以用微积分来找出损耗相对于<strong class="jq hj"> <em class="lp"> w </em> </strong>如何变化。</p><pre class="kn ko kp kq fd mp kz mq mr aw ms bi"><span id="f41c" class="lb ir hi kz b fi mt mu l mv mw"><strong class="kz hj">loss = (y_pred — y)²/n</strong></span><span id="5e75" class="lb ir hi kz b fi mx mu l mv mw"><strong class="kz hj">loss = (y_pred² + y² — 2y*y_pred)/n </strong><em class="lp">(expanding the whole square)</em></span><span id="ea51" class="lb ir hi kz b fi mx mu l mv mw"><strong class="kz hj">=&gt;( (x*w+b)² + y² — 2y*(x*w+b))/n </strong><em class="lp">(substitute y_pred)</em></span><span id="1f13" class="lb ir hi kz b fi mx mu l mv mw"><strong class="kz hj">=&gt; ((x*w+b)²/n ) + (y²/n) + ((-2y(x*w+b))/n) </strong><em class="lp">(splitting the terms)</em></span><span id="f27d" class="lb ir hi kz b fi mx mu l mv mw"><strong class="kz hj">Let A = ((x*w+b)²/n )<br/>Let B = (y²/n), <br/>Let C = ((-2y(x*w+b))/n)</strong></span><span id="be54" class="lb ir hi kz b fi mx mu l mv mw"><strong class="kz hj">A = ( x²w² + b² + 2xwb )/n </strong><em class="lp">(expanding)</em><strong class="kz hj"><br/>∂A/∂w = ( 2x²w + 2xb )/n </strong><em class="lp">(differentiating)</em></span><span id="8b8b" class="lb ir hi kz b fi mx mu l mv mw"><strong class="kz hj">∂B/∂w = 0 </strong><em class="lp">(differentiating)</em></span><span id="ae43" class="lb ir hi kz b fi mx mu l mv mw"><strong class="kz hj">C = (-2yxw — 2yb)/n<br/>∂C/∂w = (-2yx)/n </strong><em class="lp">(differentiating)</em></span></pre><p id="c9cb" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">所以，∂loss/∂w 将所有这些术语相加:</p><pre class="kn ko kp kq fd mp kz mq mr aw ms bi"><span id="a9b8" class="lb ir hi kz b fi mt mu l mv mw"><strong class="kz hj">∂loss/∂w = (2x²w + wxb — 2yx)/n<br/>=&gt; (2x(x*w + b — y))/n</strong></span></pre><p id="64af" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">于是，损失相对于<strong class="jq hj"> <em class="lp"> w </em> </strong>的导数被发现是:<br/> <code class="du kw kx ky kz b"><strong class="jq hj">(2/n)*(y_pred — y)*x</strong></code> <strong class="jq hj">。我们姑且称之为 dw。</strong></p><p id="7a27" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">如果我们对<strong class="jq hj"> <em class="lp"> b </em> </strong>进行相同的损失微分，我们将得到:<br/> <code class="du kw kx ky kz b"><strong class="jq hj">(2/n)*(y_pred — y)</strong></code> <strong class="jq hj">。我们姑且称之为 db 吧。</strong></p><p id="28c1" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">这个 dw 和 db 就是我们所说的<em class="lp">“渐变”</em></p><h2 id="f187" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated">6.更新 w 和 b</h2><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/a373ab7cbfb79b87635435bf2e27252d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*16uR2JPzCtu2qSCGPe7kbw.jpeg"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 6.1</figcaption></figure><p id="290f" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">正如我们在图 6.1 中看到的，如果我们随机初始化权重，它可能不会导致损失函数的全局最小值。<br/>我们有责任将权重更新到损失最小的点。我们已经在上面计算了<strong class="jq hj"> dw </strong>。</p><p id="4d47" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated"><strong class="jq hj"> dw </strong>无非是考虑到<strong class="jq hj"> w </strong>的初始位置，损失函数在点 w<br/>的切线的斜率。</p><h2 id="e5c4" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated"><strong class="ak">需要理解的重要一点:</strong></h2><p id="3a2a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj">在上面的图表中，当 w 的初始值较大时，损失正切的斜率将为正，需要减小它以达到全局最小值。<br/>如果 w 的值很低，并且我们想要增加它以达到全局最小值，那么在 w 点的损耗正切的斜率将是负的</strong></p><p id="b930" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">我们希望 w 的值稍微低一点，以便达到损失函数的全局最小值，如图 6.1 所示。</p><p id="8aaf" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">我们知道在上图中 dw 是正的，我们需要减少 w。<br/>这可以通过:</p><pre class="kn ko kp kq fd mp kz mq mr aw ms bi"><span id="1138" class="lb ir hi kz b fi mt mu l mv mw"><strong class="kz hj">w = w — alpha*dw<br/>b = b — alpha*bw</strong></span></pre><p id="9fa6" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">其中α是一个介于 0.1 到 0.0000001(近似值)之间的小数值，这个α称为学习率。<br/> <em class="lp">这样做，我们可以减少</em> <strong class="jq hj"> <em class="lp"> w </em> </strong> <em class="lp">如果斜率为负</em><strong class="jq hj"><em class="lp"/></strong><em class="lp">w</em><strong class="jq hj"><em class="lp">w</em></strong><em class="lp"/></p><h2 id="b7ed" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated"><strong class="ak"> 7。学习率</strong></h2><p id="0df9" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">学习率α是我们必须手动选择的，也是我们事先不知道的。选择它是一个试错的问题。<br/>我们不直接从<strong class="jq hj"> w </strong>中减去<strong class="jq hj"> dw </strong>的原因是因为，这可能会导致 w 的值发生太大的变化，并且可能不会以全局最小值结束，而是离它更远。</p><h2 id="f4cd" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated">8.训练循环</h2><p id="1260" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">计算梯度和更新权重和偏差的过程被重复几次，这导致权重和偏差的优化值。</p><h2 id="b3ad" class="lb ir hi bd is lc ld le iw lf lg lh ja jz li lj je kd lk ll ji kh lm ln jm lo bi translated">9.预言；预测；预告</h2><p id="9c91" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在训练循环之后，权重和偏差的值现在被优化，这可以用于在给定新 x 值的情况下预测新值。</p><p id="8900" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated"><code class="du kw kx ky kz b"><strong class="jq hj">y = x*w + b</strong></code></p><h1 id="652b" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">结论</strong></h1><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/76b72cc7f3cd30bcda37876197b5ecb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*PgY8467ODyWbhY-4Z6VzIw.jpeg"/></div></figure><p id="4d5d" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">梯度下降线性回归就是这样。<br/>来自“机器学习”的学习表示学习了<strong class="jq hj"> w </strong>和<strong class="jq hj"> b </strong>的梯度，然后更新 w 和 b 的部分。</p><h1 id="1f2d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">谢谢你</h1></div></div>    
</body>
</html>