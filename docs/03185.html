<html>
<head>
<title>My favorite Machine Learning Papers in 2019 Part 2 — NLP, ML in Science, Analysis of DL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2019年我最喜欢的机器学习论文第二部分— NLP，科学中的ML，DL的分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/my-favorite-machine-learning-papers-in-2019-part-2-nlp-ml-in-science-analysis-of-dl-b914e150eaf9?source=collection_archive---------20-----------------------#2020-01-19">https://medium.com/analytics-vidhya/my-favorite-machine-learning-papers-in-2019-part-2-nlp-ml-in-science-analysis-of-dl-b914e150eaf9?source=collection_archive---------20-----------------------#2020-01-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="336d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">关于这篇文章</h1><p id="0bc3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这篇文章中，我提供了2019年发表的机器学习论文中个人感兴趣的论文的概述。由于是大量的论文，把它放在一起，我把它分成三个职位。这篇文章是第二部分。</p><ul class=""><li id="b377" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">↓ <a class="ae km" rel="noopener" href="/analytics-vidhya/my-favorite-machine-learning-papers-in-2019-a9424c2f4f00">第一部分:图像视频与学习技巧</a>(1月10日贴)</li></ul><div class="kn ko ez fb kp kq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/my-favorite-machine-learning-papers-in-2019-a9424c2f4f00"><div class="kr ab dw"><div class="ks ab kt cl cj ku"><h2 class="bd hj fi z dy kv ea eb kw ed ef hh bi translated">2019年我最喜欢的机器学习论文</h2><div class="kx l"><h3 class="bd b fi z dy kv ea eb kw ed ef dx translated">关于这篇文章</h3></div><div class="ky l"><p class="bd b fp z dy kv ea eb kw ed ef dx translated">medium.com</p></div></div><div class="kz l"><div class="la l lb lc ld kz le lf kq"/></div></div></a></div><ul class=""><li id="7750" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">第2部分:自然科学的NLP，ML，DL的分析(现在在这里)</li><li id="4353" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka ki kj kk kl bi translated">第3部分:GAN、实际应用和其他领域(将于1月25日发布)</li></ul><p id="c4b9" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">第2部分提供了以下4个领域共24篇论文的概述。请注意，字段中有一些重复项，因为它们只是为了方便而设置的。</p><h2 id="fc81" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1.神经语言过程</h2><ol class=""><li id="a7d2" class="kb kc hi jf b jg jh jk jl jo mc js md jw me ka mf kj kk kl bi translated"><em class="mg">使用单语对齐更新预训练的单词向量和文本分类器</em></li><li id="a9c8" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg"> MIXOUT:有效的正则化来微调大规模预训练语言模型</em></li><li id="7c89" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">智能:通过有原则的正则化优化，对预先训练的自然语言模型进行稳健高效的微调</em></li><li id="fe8c" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg"> ERNIE:带有信息实体的增强语言表示</em></li><li id="cdb5" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg"> XLNet:用于语言理解的广义自回归预训练</em></li><li id="9e6c" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg"> RoBERTa:一种稳健优化的BERT预训练方法</em></li><li id="f0e9" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg"> ALBERT:一个用于语言表达自我监督学习的LITE BERT</em></li><li id="0b7e" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">好消息，各位！新闻图像的上下文驱动实体感知字幕</em></li><li id="ce75" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg"> PaperRobot:科学思想的增量式草稿生成</em></li><li id="3ce1" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">抵御神经假新闻</em></li></ol><h2 id="fbbc" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">2.变压器的改进</h2><ol class=""><li id="bf82" class="kb kc hi jf b jg jh jk jl jo mc js md jw me ka mf kj kk kl bi translated"><em class="mg"> TRANSFORMER-XL:固定长度上下文之外的注意力语言模型</em></li><li id="51ce" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">关于变压器架构中的层规范化</em></li><li id="92d0" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">用于长程序列建模的压缩变压器</em></li><li id="3fd3" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated">单头注意力RNN:停止用你的头脑思考</li></ol><h2 id="8ac5" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">3.与物理和数学相关的ML</h2><ol class=""><li id="10a7" class="kb kc hi jf b jg jh jk jl jo mc js md jw me ka mf kj kk kl bi translated"><em class="mg">深度神经网络的多电子薛定谔方程从头算解</em></li><li id="4872" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">牛顿vs机器:利用深度神经网络解决混沌三体</em></li><li id="6181" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">符号数学的深度学习</em></li><li id="875a" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">带ODE积分器的哈密顿图网络</em></li><li id="6350" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated">AI Feynman:一种受物理学启发的符号回归方法</li></ol><h2 id="f3e2" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">4.数字图书馆分析</h2><ol class=""><li id="e53a" class="kb kc hi jf b jg jh jk jl jo mc js md jw me ka mf kj kk kl bi translated"><em class="mg">一票赢天下:跨数据集和优化器推广彩票初始化</em></li><li id="95fa" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">操纵彩票:让所有彩票中奖</em></li><li id="2975" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">标签平滑什么时候有帮助？</em></li><li id="23c0" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">熵罚:走向超越IID假设的一般化</em></li><li id="5cf0" class="kb kc hi jf b jg lg jk lh jo li js lj jw lk ka mf kj kk kl bi translated"><em class="mg">基准测试神经网络对常见讹误和干扰的鲁棒性</em></li></ol></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><h1 id="f501" class="if ig hi bd ih ii mo ik il im mp io ip iq mq is it iu mr iw ix iy ms ja jb jc bi translated">1.神经语言过程</h1><p id="e599" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在自然语言处理方面，2018年底发表的关于改进的BERTs的论文还是很多的。有许多改进的BERT系统，但ALBERT和XLNet似乎是其中重要的。BERT可以作为各种任务的微调模型，对微调的研究也很突出。OpenAI不愿意公布GPT-2的完整模型，因为担心它在假新闻中被滥用，但也有关于假新闻措施的研究。</p><h2 id="59a6" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1–1.使用单语对齐更新预训练的词向量和文本分类器</h2><p id="9c65" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【https://arxiv.org/abs/1910.06241 T2】号</p><p id="dfd3" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">提出了一种进一步细化通过NLP微调获得的词向量的方法。使用在大规模语料库中获得的向量X和基于任务数据微调的向量Y，通过线性回归使用矩阵Q重新排列X，从而获得新的表达向量Z。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es mt"><img src="../Images/fe0489a4601f2f19b4fe70ce1014c7a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C18INGuPgLBLWHSv_Y4JBA.png"/></div></div></figure><h2 id="daef" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1–2.混合:大规模预训练语言模型微调的有效正则化</h2><p id="c022" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【https://arxiv.org/abs/1909.11299 T4】</p><p id="5023" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">他们提出了一种叫做MIXOUT的迁移学习方法，像Dropout一样随机丢弃神经元，而是给出迁移源网络的权重。防止破坏性遗忘，并允许Finetune在接近传输源重量的情况下使用。NLP成绩不错。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es ne"><img src="../Images/50586541fca41d1e433c5d2b7f3cf8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvyfx09k6u2a0JTRsZu9-g.png"/></div></div></figure><h2 id="b366" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1–3.SMART:通过有原则的正则化优化，对预训练的自然语言模型进行稳健而高效的微调</h2><p id="2cc3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1911.03437" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1911.03437</a></p><p id="7531" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">提出NLP迁移学习方法SMART防止破坏性遗忘，无需启发式学习速率调整。第一种正则化是相对于原始正则化具有小的参数变化，第二种是相对于输入扰动的鲁棒性</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nf"><img src="../Images/289ee4a9c4bba1616a36d74c47d3e0a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JHVe4W04v7VzdoODsMxJZw.png"/></div></div></figure><h2 id="52bb" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1–4.ERNIE:用信息实体增强语言表示</h2><p id="d612" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1905.07129" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.07129</a></p><p id="17a0" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">结合知识图改进语言模型的研究。句子中实体对应的部分是从KG取的。此外，通过随机屏蔽实体并学习从KG中获取合适的实体，促进了文档和KG的融合。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es ng"><img src="../Images/2e343955584cee6086034a97b184b991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uabGMoggzNozlrw5rTUR1g.png"/></div></div></figure><h2 id="58a7" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1–5.XLNet:用于语言理解的广义自回归预训练</h2><p id="85c3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1906.08237</a></p><p id="15ea" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">BERT通过预测屏蔽词来执行预训练，但这并不合适，因为在应用任务时缺少这样的机制(微调)。通过改变单词的预测顺序(保留原始顺序信息)，可以使用自回归模型获得双向语义依赖。由于顺序发生了变化，作者增加了查询流注意，用于获取顺序信息，此外还有正常的自我注意。伯特超过了20多项任务。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nh"><img src="../Images/4e8e0a31913c08793696b337b83d0f6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RGD1bJHZ5lG0C8xb87SxZw.png"/></div></div></figure><h2 id="f974" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1–6岁。RoBERTa:一种稳健优化的BERT预训练方法</h2><p id="dba4" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">https://arxiv.org/abs/1907.11692<a class="ae km" href="https://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank"/></p><p id="d2ab" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">BIn BERT，语言模型是通过解决填空题和句子对问题而创建的。但是前者一旦创建，就在学习过程中被重用。后者在其他研究中也没有取得多少成果。所以在学习前者的同时动态改变了面具的位置，后者被废除。添加更多数据可以提高性能。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es ni"><img src="../Images/53f2e07199f128de07ef1d318a56104f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73JvfnusOS6HNLK2j5VcXA.png"/></div></div></figure><h2 id="4c10" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1–7.ALBERT:一个用于语言表达自我监督学习的LITE BERT</h2><p id="4ae6" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【https://arxiv.org/abs/1909.11942 T4】</p><p id="0216" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">作者使用三种策略:1、分解一个矩阵，在保持高表达性的同时提高参数的效率。第二，通过共享参数来提高效率。介绍一个文件订购任务。比BERT大型模型具有更少参数和更高速度的高性能。</p><p id="50bf" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">最后一种策略将最初在BERT中引入的NSP任务(文档主题预测和文档一致性预测)改为SOP(句序预测)。他们认为话题预测太简单了，没有效果，所以我们只关注连贯预测。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nj"><img src="../Images/8f132546ed81b4f0a9228b93e9a0057f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oY6gy0bcEUaTeiVFaJC5og.png"/></div></div></figure><h2 id="32f0" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1–8.好消息，各位！用于新闻图像的上下文驱动的实体感知字幕</h2><p id="e9d0" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1904.01475" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1904.01475</a></p><p id="c5b8" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">传统的图像字幕只能用一般的词来描述，但是结合新闻文章，图像可以被更详细地描述。通过用特殊字符替换专有名词，可以处理不在数据中的单词。此外，还提供了数据集GoodNews。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nk"><img src="../Images/7b1498f411277c2ab8b2abbdf402afef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LrM11XcIfGtTdlVaI-vrig.png"/></div></div></figure><h2 id="8e93" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1–9.PaperRobot:科学思想的增量式草稿生成</h2><p id="53fd" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1905.07870" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.07870</a></p><p id="e7f8" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">使用从过去的论文制作的知识图(KG ),从标题自动生成“摘要、结论和下一篇工作，科学论文的下一个标题”的研究。通过链接预测来增加KG中元素之间的链接，以增强KG。从标题和KG中获得的重要元素是使用记忆和注意力生成的。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nl"><img src="../Images/7a6f6b8f2bca5b185539c13ddd912c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HHusAQ4Lq9FO0gG2T2xHiQ.png"/></div></div></figure><h2 id="dbd2" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">1–10.防御神经假新闻</h2><p id="2d07" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1905.12616" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.12616</a></p><p id="62a7" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi">MLで生成されたFake Newsに対応するために、GPT-2と似た機構で脅威モデルGROVERを作ったという研究。Fake Newsの内容だけでなく、著者・日付・タイトルも順次生成していくようなモデルになっている。言語モデルの潜在変数に分類器をつけてFake/Real判定をさせたところ、BERTやGPT2よりGROVER自身を使う方が判定精度はよかった（そりゃ当然という気がせんでもない）</p><p id="d39a" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">创建假新闻生成威胁模型GROVER的研究，该模型具有类似于GPT-2的机制，以解决ML生成假新闻的问题。这是一个模型，不仅生成假新闻的内容，而且还依次生成作者、日期和标题。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nm"><img src="../Images/e61712bb0f07399969dbc5142d70b102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0B_YbZ8eXAqPBh4eT-2WQ.png"/></div></div></figure></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><h1 id="d800" class="if ig hi bd ih ii mo ik il im mp io ip iq mq is it iu mr iw ix iy ms ja jb jc bi translated">2.变压器的改进</h1><p id="487b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Transformer模型已经成为自然语言处理中压倒性的存在，但是存在一些问题，例如沉重、棘手的学习以及只能处理短的固定长度。有很多研究可以缓解。此外，BERTs和其他模型需要的计算资源甚至连公司都无法准备，所以我个人喜欢处理它的研究，如单头注意力RNN。</p><h2 id="9e0a" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">2–1.TRANSFORMER-XL:超越固定长度上下文的注意力语言模型</h2><p id="2f73" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【https://arxiv.org/abs/1901.02860 T2】号</p><p id="f65c" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">使用通常只能处理较短固定长度的转换器编码器来引用整个文档的研究。Transformer-XL可以通过仅参考旧句子参数的参数而不进行梯度计算，用整句(长于固定长度)计算预测值。他们可以学习比原始变压器长450%的长期依赖性和比RNN长80%的长期依赖性。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nn"><img src="../Images/c1e3e7ae61e64481b0c8ff7bcb75eed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWPGLf5HxQU__7hSx6zxYg.png"/></div></div></figure><h2 id="a96b" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">2–2.变压器体系结构中的层规范化</h2><p id="dc90" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【https://openreview.net/forum?id=B1x8anVFPr T4】</p><p id="3b9e" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">通过将变压器编码器的层归一化位置从多头注意(或前馈网络)的前面改为跳过连接的后面，学习开始时的梯度不会爆炸，预热变得不必要。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es no"><img src="../Images/eb2f5d3bfa811d5c9f2ca7a642ad23a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NjgeU6WIumH8Hc3lnX529A.png"/></div></div></figure><h2 id="060e" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">2–3.用于长程序列建模的压缩变换器</h2><p id="c508" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://openreview.net/forum?id=SylKikSYDH" rel="noopener ugc nofollow" target="_blank">https://openreview.net/forum?id=SylKikSYDH</a></p><p id="31de" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">压缩转换器，通过压缩过去的序列，使注意力得以再现，超过记忆容量的时间序列长度可以被学习。基本型号是Transformer-XL。压缩方法有很多种，但最好的方法是用Conv1D来重现注意力。事实上，过去压缩的信息的关注权重较大，可以看出信息正在被有效利用。</p><p id="7668" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">此外，据说降低学习速率的方法不太好，而降低优化频率(增加批量大小)的方法对Compressive Transformer和Transformer-XL都有效。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nh"><img src="../Images/daa1e5d1dd5ba8f5a09cd897b91a08e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yrf2hPeTiJBhlJ-wa_aa1Q.png"/></div></div></figure><h2 id="d086" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">2–4.单头注意力RNN:停止用你的头脑思考</h2><p id="e485" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1911.11423" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1911.11423</a></p><p id="2719" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">一项将单头注意力与LSTM相结合的研究，以1 GPU /天产生与Transformer-XL相当的分数。有很多段子，它更接近于一篇博文，而不是一篇学术论文，但我个人很喜欢作者试图以1 GPU /天击败BERT这样的大型模型的意图。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es np"><img src="../Images/f51f4e6c7081ab6db21aae320a60636e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0zvRzXIGcVwsRbke3WhaFw.png"/></div></div></figure></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><h1 id="c811" class="if ig hi bd ih ii mo ik il im mp io ip iq mq is it iu mr iw ix iy ms ja jb jc bi translated">3.与物理和数学相关的ML</h1><p id="5654" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">似乎物理、数学等自然科学与机器学习的融合有所推进。我认为，有许多研究侧重于将物理约束放在模型或数据上，而不是以直接的方式将数据放入模型。针对符号数学的深度学习，可以解数学公式，可以发现物理规律的AI Feynman，个人觉得相当震撼。</p><h2 id="fa75" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">3–1.用深度神经网络从头算求解多电子薛定谔方程</h2><p id="05b8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【https://arxiv.org/abs/1909.02487 T2】号</p><p id="f9d5" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">用神经网络进行量子化学计算的费米网建议。在通常的量子化学计算中，波函数通过能量最小化来优化。在费米子中，波函数用NN来近似。它结合了相当多的物理约束，如HF近似、Slater行列式和反对称，能量计算也是通过物理计算来计算的。费米网在任何系统下都能产生好的结果。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nq"><img src="../Images/6c9417e6690410f4282ec5b2ebd692e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o2Q4f3f2B1-s4w7SDyoCpg.png"/></div></div></figure><h2 id="a9f0" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">3–2.牛顿vs机器:使用深度神经网络解决混沌三体</h2><p id="c4e2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【https://arxiv.org/abs/1910.07291 T4】</p><p id="3688" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">研究用神经网络来近似无法解析解决的三体的物理模拟效果很好。虽然仿真环境有限(在平面环境下，三个初始位置中只有一个位置发生实质性任意变化)，但我感受到了神经网络对物理仿真的适用性。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nr"><img src="../Images/46a6b99eaee340904354a30ba8682ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ps-wW201IDwMksGwkr7QpA.png"/></div></div></figure><p id="e88d" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated"><a class="ae km" rel="noopener" href="/analytics-vidhya/the-fusion-of-physics-simulation-and-machine-learning-a5f8a382c436">在我的博客里解释</a> g↓</p><div class="kn ko ez fb kp kq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/the-fusion-of-physics-simulation-and-machine-learning-a5f8a382c436"><div class="kr ab dw"><div class="ks ab kt cl cj ku"><h2 class="bd hj fi z dy kv ea eb kw ed ef hh bi translated">物理仿真与机器学习的融合</h2><div class="kx l"><h3 class="bd b fi z dy kv ea eb kw ed ef dx translated">关于这篇文章</h3></div><div class="ky l"><p class="bd b fp z dy kv ea eb kw ed ef dx translated">medium.com</p></div></div><div class="kz l"><div class="ns l lb lc ld kz le lf kq"/></div></div></a></div><h2 id="bf26" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">3–3.符号数学的深度学习</h2><p id="d142" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">https://arxiv.org/abs/1912.01412<a class="ae km" href="https://arxiv.org/abs/1912.01412" rel="noopener ugc nofollow" target="_blank"/></p><p id="02d4" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">用符号形式计算积分的研究。将公式分解成树形结构，用seq2seq作为计算每个符号出现概率的语言模型求解。它解决了相当复杂的积分问题，比Mathmatica或Matlab更精确。由于数据集必须自己准备，随机创建一个依赖于常数c1和c2的x的函数f，输出二阶导数f”和f(或x)的公式，如图。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nt"><img src="../Images/28c7a6832a97ba8a1cdcf84e50d728ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*azIddY0PW5DEIo85ETX8FQ.png"/></div></div></figure><h2 id="ae96" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">3–4.具有ODE积分器的哈密顿图网络</h2><p id="17bb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【https://arxiv.org/abs/1909.12790 T4】</p><p id="b9ec" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">他们建议HOGN通过哈密顿计算来计算物体的动量和运动，而不是直接用NN来预测这些。HOGN可以解释为通过哈密顿量的约束来学习物理系统，提高了轨道预测的精度。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nt"><img src="../Images/13777a91057a14b459b6fc3b27a57e70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-aOCU9aQrWtUb3zevh1g0A.png"/></div></div></figure><p id="389d" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated"><a class="ae km" rel="noopener" href="/analytics-vidhya/the-fusion-of-physics-simulation-and-machine-learning-a5f8a382c436">在我的博客里解释</a> g↓</p><div class="kn ko ez fb kp kq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/the-fusion-of-physics-simulation-and-machine-learning-a5f8a382c436"><div class="kr ab dw"><div class="ks ab kt cl cj ku"><h2 class="bd hj fi z dy kv ea eb kw ed ef hh bi translated">物理仿真与机器学习的融合</h2><div class="kx l"><h3 class="bd b fi z dy kv ea eb kw ed ef dx translated">关于这篇文章</h3></div><div class="ky l"><p class="bd b fp z dy kv ea eb kw ed ef dx translated">medium.com</p></div></div><div class="kz l"><div class="ns l lb lc ld kz le lf kq"/></div></div></a></div><h2 id="39ed" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated"><strong class="ak">3–5。AI Feynman:一种受物理学启发的符号回归方法</strong></h2><p id="bd3a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1905.11481" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.11481</a></p><p id="bf65" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated"><strong class="jf hj">能从数据中发现物理规律的研究。重点是通过把问题分解成无量纲的量来简化问题。首先，在执行量纲分析或具有无量纲量的多项式拟合之后，使用DL确认是否存在平移对称性等。</strong></p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nu"><img src="../Images/836fa66560c40788b14f0242828e9a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6x2AcRI_qinr4uaDCueprw.png"/></div></div></figure><p id="7d5e" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">我博客里的解释↓</p><div class="kn ko ez fb kp kq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/ai-feynman-a-machine-learning-model-that-can-discover-physical-laws-222239d2c4ea"><div class="kr ab dw"><div class="ks ab kt cl cj ku"><h2 class="bd hj fi z dy kv ea eb kw ed ef hh bi translated">能“发现”物理规律的机器学习模型AI Feynman</h2><div class="kx l"><h3 class="bd b fi z dy kv ea eb kw ed ef dx translated">关于这篇文章</h3></div><div class="ky l"><p class="bd b fp z dy kv ea eb kw ed ef dx translated">medium.com</p></div></div><div class="kz l"><div class="nv l lb lc ld kz le lf kq"/></div></div></a></div></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><h1 id="a451" class="if ig hi bd ih ii mo ik il im mp io ip iq mq is it iu mr iw ix iy ms ja jb jc bi translated">4.数字图书馆分析</h1><p id="daed" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我觉得对彩票假说的后续研究是相当有趣的研究。有一些关于未知数据中性能下降的研究。泛化的问题在现实世界的使用中很重要。</p><h2 id="2133" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">4–1.一张票赢所有人:跨数据集和优化器推广彩票初始化</h2><p id="a633" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1906.02773" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1906.02773</a></p><p id="05d8" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">在“彩票假说”中，只有好的初始值影响模型性能，好的初始值可以从一个数据集转移到另一个数据集。他们试验了不同的模型、数据集和优化器，但它是可以转移的。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es no"><img src="../Images/ec8e9fc47038179e18f32e2892bae96b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wt-R58dgkngfrWsZRmq2SA.png"/></div></div></figure><h2 id="1a81" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">4–2.操纵彩票:让所有彩票中奖</h2><p id="6650" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1911.11134" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1911.11134</a></p><p id="4df9" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">在彩票假设中，只有一部分初始值有助于准确性，并且仅使用那些初始值的学习给出了稀疏网络，其最初可以达到与密集网络相同的准确性水平。在彩票假设中，只有一部分初始值有助于准确性，并且仅使用那些初始值的学习给出了稀疏网络，其最初可以达到与密集网络相同的准确性水平。他们提出了被称为作弊彩票(RigL)的训练方法，这种方法可以用任意初始值创建稀疏且高度精确的网络。通过重复“用稀疏NN学习→删除具有小参数的节点→连接具有大梯度的节点”的操作来执行学习。尽管学习时间不是很长(大约1.2到2倍)，但由于稀疏性，推理速度大大提高，精度增加而不是降低。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nw"><img src="../Images/1a9960a3c1753f22190b81d6e7883925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t5IHfivocJgfcHjv-sZwPg.png"/></div></div></figure><h2 id="008d" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">4–3.标注平滑何时有帮助？</h2><p id="30a0" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【https://arxiv.org/abs/1906.02629 T2】号</p><p id="4cdb" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">使用软标签(如[0.9，0.1])而不是硬目标(如[0，1])进行标签平滑的效果研究。对于语言模型/分类问题是有效的，因为它具有减少具有相同标签的数据的分布范围的效果。然而，相似类的相似性信息因此消失，在提取时精度降低。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nx"><img src="../Images/bd22483cb3ab0d7ad431cd9e5265a7b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*io_1Z3E-JJWbYuQr3HvOJw.png"/></div></div></figure><h2 id="e968" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">4–4.熵罚:走向超越IID假设的一般化</h2><p id="0e62" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【https://arxiv.org/abs/1910.00164 T4】</p><p id="6d5b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">一项研究表明，由于模型在训练和测试之间学习到不寻常的共同特征，SOTA方法在真实数据集中被降级。使用信息瓶颈框架，他们提出了一个熵惩罚，该熵惩罚添加了一个正则化项，该正则化项惩罚第一层中每个通道和每个标签的平均值的偏差。在C-MNIST上有显著的改进，在训练和测试中颜色是不同的。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es nh"><img src="../Images/fe1b08732a628032775a84674df05482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Sl2Oi5ff2-y3e2vNw_TUw.png"/></div></div></figure><h2 id="315a" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">4–5.基准神经网络对常见讹误和干扰的鲁棒性</h2><p id="4b6c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae km" href="https://arxiv.org/abs/1903.12261" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1903.12261</a></p><p id="1772" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ll jq jr js lm ju jv jw ln jy jz ka hb bi translated">ICLR 2019最佳论文之一。提出了基于AlexNet的图像污染和扰动评价指标和数据集。分数是基于与Alex Net的干净数据相比时的准确度下降来计算的。作者说直方图平坦化，多尺度图像方法如MSDNetsw，多特征捕获方法如DenseNet，ResNet等。)更健壮。</p><figure class="mu mv mw mx fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es ny"><img src="../Images/87c4a99967b6d22ae9739a5cc9ae0eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sdIsFmgGNG1YCJPYL6IoFA.png"/></div></div></figure></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><h1 id="ddf8" class="if ig hi bd ih ii mo ik il im mp io ip iq mq is it iu mr iw ix iy ms ja jb jc bi translated">结论</h1><p id="1d42" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这篇博客中，我主要介绍了NLP，自然科学，以及与DL相关的分析。下周，我将发布2019年关于以下主题的有趣论文列表，如果你喜欢，请回来查看。</p><ul class=""><li id="1a9e" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated"><a class="ae km" rel="noopener" href="/analytics-vidhya/my-favorite-machine-learning-papers-in-2019-a9424c2f4f00">第一部分:图像和视频以及学习技巧</a> ↓</li></ul><div class="kn ko ez fb kp kq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/my-favorite-machine-learning-papers-in-2019-a9424c2f4f00"><div class="kr ab dw"><div class="ks ab kt cl cj ku"><h2 class="bd hj fi z dy kv ea eb kw ed ef hh bi translated">2019年我最喜欢的机器学习论文</h2><div class="kx l"><h3 class="bd b fi z dy kv ea eb kw ed ef dx translated">关于这篇文章</h3></div><div class="ky l"><p class="bd b fp z dy kv ea eb kw ed ef dx translated">medium.com</p></div></div><div class="kz l"><div class="la l lb lc ld kz le lf kq"/></div></div></a></div><ul class=""><li id="1dd7" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">第3部分:GAN、实际应用和其他领域(将于1月25日发布)</li></ul><h2 id="0d03" class="lo ig hi bd ih lp lq lr il ls lt lu ip jo lv lw it js lx ly ix jw lz ma jb mb bi translated">推特，一句话的论文解释。</h2><div class="kn ko ez fb kp kq"><a href="https://twitter.com/AkiraTOSEI" rel="noopener  ugc nofollow" target="_blank"><div class="kr ab dw"><div class="ks ab kt cl cj ku"><h2 class="bd hj fi z dy kv ea eb kw ed ef hh bi translated">阿基拉</h2><div class="kx l"><h3 class="bd b fi z dy kv ea eb kw ed ef dx translated">akira的最新推文(@AkiraTOSEI)。机器学习工程师/数据科学家/物理学硕士/…</h3></div><div class="ky l"><p class="bd b fp z dy kv ea eb kw ed ef dx translated">twitter.com</p></div></div><div class="kz l"><div class="nz l lb lc ld kz le lf kq"/></div></div></a></div></div></div>    
</body>
</html>