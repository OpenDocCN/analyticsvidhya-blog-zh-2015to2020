<html>
<head>
<title>Understanding RNN implementation in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解PyTorch中的RNN实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-rnn-implementation-in-pytorch-eefdfdb4afdb?source=collection_archive---------0-----------------------#2020-03-20">https://medium.com/analytics-vidhya/understanding-rnn-implementation-in-pytorch-eefdfdb4afdb?source=collection_archive---------0-----------------------#2020-03-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6d73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RNNs和其他类似GRU、LSTMs的循环变体是最常用的PyTorch模块之一。在这篇文章中，我介绍了<code class="du jd je jf jg b">RNN</code>模块的不同参数，以及它如何影响计算和结果输出。</p><p id="1ba6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章的完整笔记本可从以下网址获得:<a class="ae jh" href="https://github.com/rsk2327/DL-Experiments/blob/master/Understanding_RNNs.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/rsk 2327/DL-Experiments/blob/master/Understanding _ rnns . ipynb</a></p></div><div class="ab cl ji jj gp jk" role="separator"><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn"/></div><div class="hb hc hd he hf"><h1 id="12bd" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">基本示例</h1><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="ks kt l"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">RNN:基本例子</figcaption></figure><h2 id="0ce7" class="ky jq hi bd jr kz la lb jv lc ld le jz iq lf lg kd iu lh li kh iy lj lk kl ll bi translated"><strong class="ak"> RNN超参数</strong></h2><p id="b009" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">RNN细胞块的关键参数是:</p><ul class=""><li id="1994" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated"><code class="du jd je jf jg b">input_size</code> -定义定义输入序列每个元素(时间戳)的特征数量</li><li id="45c4" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><code class="du jd je jf jg b">hidden_size</code> -定义隐藏状态的大小。因此，如果<code class="du jd je jf jg b">hidden_size</code>设置为4，那么每个时间步的隐藏状态就是一个长度为4的向量</li><li id="bc18" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><code class="du jd je jf jg b">num_layers</code> -允许用户构建堆叠的rnn。堆叠式rnn的概念及其工作原理将在后面解释</li><li id="f1fc" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><code class="du jd je jf jg b">bias</code> -是否在RNN单元中包含偏差项</li><li id="312e" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><code class="du jd je jf jg b">bidirectional</code>-RNN层是否是双向的</li><li id="bd56" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated"><code class="du jd je jf jg b">batch_first</code> -定义输入格式。如果为真，则输入序列的格式为(批处理、序列、特征)</li></ul><p id="850c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了简单起见，对于基本示例，我们将<code class="du jd je jf jg b">input_size</code>、<code class="du jd je jf jg b">hidden_size</code>和<code class="du jd je jf jg b">num_layers</code>设置为1，并将<code class="du jd je jf jg b">bidirectional</code>设置为False。</p><p id="8bb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> RNN输出</strong></p><p id="6f29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">PyTorch中的RNN模块总是返回2个输出</p><ul class=""><li id="cb41" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated">总输出-包含与输入序列中所有元素(时间戳)相关的隐藏状态</li><li id="00c9" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">最终输出-包含输入序列最后一个元素的隐藏状态。</li></ul><p id="5973" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，最终输出不会提供总输出没有提供的任何新信息。在大多数情况下，最终输出可以从总输出中构造出来。然而，在少数情况下这是不可能的。</p><p id="bd9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于我们的例子，总产出的大小为[1，3，1]。这可以分解为</p><ul class=""><li id="06f0" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated">1:序列数</li><li id="6519" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">3:序列中元素的数量</li><li id="ed36" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">1:定义隐藏状态的特征数量。由<code class="du jd je jf jg b">hidden_size</code>参数直接控制</li></ul><p id="5130" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以理解，最终输出的大小为[1，1，1]，因为它只包含序列中最后一个元素的隐藏状态。</p><p id="a891" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> RNN参数</strong></p><p id="5dff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RNN模块有两种参数，<code class="du jd je jf jg b">weights</code>和<code class="du jd je jf jg b">biases</code>。参数的实际数量随着用于定义RNN层的不同超参数而变化。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es mg"><img src="../Images/64f7e5a1930c9a6cd669182a8943e512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*WCuT5Wvdr3USrfBY9FBSPg.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">RNN公式</figcaption></figure><p id="c32c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个例子中，我们只有两个参数，带的<em class="mf">和带</em>的<em class="mf">。</em></p><p id="22d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">手动计算</strong></p><p id="cf7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定RNN公式和RNN层权重，我们手动计算RNN输出。这让我们更好地理解隐藏状态是如何由<code class="du jd je jf jg b">RNN</code>模块在内部计算的。</p><p id="16c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于前面没有隐藏状态的第一个元素，我们将隐藏状态设置为0。</p><p id="b5f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以通过这个基本例子，我们可以观察到:</p><ol class=""><li id="3d62" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc mj lx ly lz bi translated">RNN对给定序列的所有特征重复进行非常基本的计算</li><li id="957b" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc mj lx ly lz bi translated">特定时间戳的输出取决于前一个时间戳的输出。</li></ol></div><div class="ab cl ji jj gp jk" role="separator"><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn"/></div><div class="hb hc hd he hf"><h1 id="a4eb" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">添加更多功能</strong></h1><p id="19ff" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">在下一次迭代中，我们向输入序列元素添加更多的特征。现在，每个元素都由一个三元素向量来表示，而不是1。输入序列现在具有[1，4，3]的形状</p><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="ks kt l"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">RNN:增加更多功能</figcaption></figure><p id="8658" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">鉴于特征的数量已经改变，我们通过将<code class="du jd je jf jg b">input_size</code>设置为3来对RNN层定义进行必要的修改。</p><p id="2c13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了上面的变化，我还设置了<code class="du jd je jf jg b">bias</code>为真。这将有助于演示隐藏状态计算中如何包含偏差。</p><p id="baf1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算输出</strong></p><p id="6729" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总输出和最终输出的形状是[1，4，1]和[1，1，1]。与前面的例子相比，这里唯一的变化是因为序列的长度不同。因此，特征向量(<code class="du jd je jf jg b">hidden_size</code>)的长度对输出的大小没有影响。</p><p id="2b5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像以前一样，手动计算合成的RNN隐藏状态值有助于我们确认RNN模块执行的内部计算。</p></div><div class="ab cl ji jj gp jk" role="separator"><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn"/></div><div class="hb hc hd he hf"><h1 id="109b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">增加隐藏尺寸</strong></h1><p id="4a83" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">在下一次迭代中，我们在前面的examepl的基础上，将<code class="du jd je jf jg b">hidden_size</code>参数增加到2，并研究它对计算和最终输出的影响。</p><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="ks kt l"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">RNN:增加隐藏大小</figcaption></figure><p id="b7eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">增加RNN层的隐藏状态大小有助于增加RNN模型的复杂性，并允许其潜在地捕捉更复杂的决策边界。它还为隐藏状态提供了更多的表达能力。由长度为10的向量表示的隐藏状态可以捕获比长度为1的向量多得多的信息。</p><p id="8b16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算输出</strong></p><p id="740d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以观察到的第一个变化是输出变量形状的变化。总输出和最终输出现在分别具有[1，4，2]和[1，1，2]的形状。这主要是由于每个元素的隐藏状态现在由长度为2的向量表示。</p><p id="f368" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于隐藏状态的手动计算，计算基本保持不变。代码中唯一的不同是使用了Torch的<code class="du jd je jf jg b">matmul</code>操作符，而不是我们之前使用的<code class="du jd je jf jg b">dot</code>操作符。</p></div><div class="ab cl ji jj gp jk" role="separator"><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn"/></div><div class="hb hc hd he hf"><h1 id="ef90" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">使用双向RNN </strong></h1><p id="62e7" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">双向rnn标志着我们迄今为止所看到的例子的一个重大变化。虽然基本的RNN公式保持不变，但在分析手动计算代码时，计算中的一些变化变得更加清晰。</p><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="ks kt l"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">RNN:双向变体</figcaption></figure><p id="96ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">顾名思义，双向RNN包括在两个方向上对输入序列应用RNN。有很多帖子详细介绍了双向RNNs背后的概念以及它们为什么有用，所以我就不在这里讨论了。</p><p id="c26a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要记住的关键点是双向RNN计算涉及序列的2次运行。为了便于理解，我将它们称为向前和向后运行。</p><p id="9ac3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算输出</strong></p><p id="cdd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要注意的第一个显著差异是输出形状的变化。总输出和最终输出现在分别具有[1，4，4]和[2，1，2]的形状。</p><p id="b72d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于总产出，它的形状可以分解为</p><ul class=""><li id="91ad" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated">1:序列数</li><li id="f9cd" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">4:序列中元素的数量</li><li id="2a1d" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">4:每个元素隐藏状态的大小。</li></ul><p id="be47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后一个shape元素表示隐藏状态的大小，它是4，因为RNN层是双向的。<em class="mf">在双向RNN中，通过向前和向后运行计算的隐藏状态被连接，以产生每个元素的最终隐藏状态。</em>因此，如果<code class="du jd je jf jg b">hidden_size</code>参数是3，那么最终隐藏状态的长度将是6。</p><p id="21ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于最终输出，其形状可以分解为</p><ul class=""><li id="e23e" class="lr ls hi ih b ii ij im in iq lt iu lu iy lv jc lw lx ly lz bi translated">2:向前/向后运行的总次数。或者是序列数量的两倍</li><li id="0afa" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">1:这是1，因为最终输出只取序列的最后一个元素</li><li id="687c" class="lr ls hi ih b ii ma im mb iq mc iu md iy me jc lw lx ly lz bi translated">2:单次运行的隐藏状态的大小。这等于<code class="du jd je jf jg b">hidden_size</code></li></ul><p id="5163" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在最终输出中，<code class="du jd je jf jg b">RNN</code>模块输出每次运行结束时计算的隐藏状态。因此，由于我们有一个双向层，有2个运行，因此有2个最终隐藏状态。这些隐藏状态中的每一个都将具有等于<code class="du jd je jf jg b">hidden_size</code>参数的长度。</p><p id="1759" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型参数</strong></p><p id="893d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当<code class="du jd je jf jg b">bidirectional</code>设置为真时，<code class="du jd je jf jg b">RNN</code>模块也获得新的参数来区分向前和向后运行。<code class="du jd je jf jg b">weights</code>和<code class="du jd je jf jg b">biases</code>的主要命名保持不变。但是，系统中会添加一组新的参数，这些参数的名称与以前的参数相同，但带有一个附加的“_reverse”后缀。这实质上使RNN层中的参数数量加倍。</p><p id="bdee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">手动计算</strong></p><p id="834f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们从正向计算开始，基本上使用我们到目前为止使用的相同程序。此运行的输出与总输出的前半部分(每行的前2个元素)完全匹配。</p><p id="e298" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于反向运行，程序与之前相同。唯一的区别是，我们现在从最后一个元素开始，并向序列的第一个元素移动。</p><p id="f275" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们得到了两次运行的结果，我们可以简单地连接两次输出，得到一个与总输出精确匹配的结果输出。</p></div><div class="ab cl ji jj gp jk" role="separator"><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn jo"/><span class="jl bw bk jm jn"/></div><div class="hb hc hd he hf"><h1 id="32f6" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">堆叠rnn</strong></h1><p id="504d" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">对于堆叠式rnn，我们探索了<code class="du jd je jf jg b">RNN</code>模块的<code class="du jd je jf jg b">num_layers</code>参数。堆叠的RNN可以被认为是堆叠在一起的单个RNN模块，一个模块的输出作为下一个RNN模块的输入。</p><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="ks kt l"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">RNN:堆叠层</figcaption></figure><p id="e72f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个例子，为了更好地解释与堆叠层相关的计算，我将<code class="du jd je jf jg b">bidirectional</code>设置为False。其他参数包括<code class="du jd je jf jg b">input_size</code> = 3、<code class="du jd je jf jg b">hidden_size</code> = 3、<code class="du jd je jf jg b">num_layers</code> = 2。</p><p id="3bcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型参数</strong></p><p id="7bdc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于堆叠的rnn可以被视为堆叠在一起的单个模块，因此堆叠的<code class="du jd je jf jg b">RNN</code>模块由每层的<code class="du jd je jf jg b">weights</code>和<code class="du jd je jf jg b">biases</code>组成，后缀代表每个权重对应的层。由于<code class="du jd je jf jg b">num_layers</code>已经设置为2，堆叠的<code class="du jd je jf jg b">RNN</code>模块共有8个参数- 4个<code class="du jd je jf jg b">weight</code>和4个<code class="du jd je jf jg b">bias</code>参数。</p><p id="ebce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算输出</strong></p><p id="b100" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总产出具有[1，4，3]的形状。这类似于单个RNN模块的输出。需要注意的一点是，在堆叠的<code class="du jd je jf jg b">RNN</code>模块中，总输出对应于由最后一个RNN层计算的隐藏状态。</p><p id="011a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终输出的形状为[2，1，3]。最终输出包含序列最后一个元素的隐藏状态，由<code class="du jd je jf jg b">RNN</code>模块中的每一层计算。因此，由于我们有1个序列和2层，最终输出的第一维长度为2。如果批处理中有2个序列，并且<code class="du jd je jf jg b">RNN</code>模块有3层，那么这个维度的长度将有6。</p><p id="8e51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">手动计算</strong></p><p id="84c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于第一层，使用相应的层参数，我们可以很容易地计算每个元素的隐藏状态，使用的过程与我们到目前为止一直使用的过程相同。</p><p id="c52e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于第二层和所有后续层，输入向量<code class="du jd je jf jg b">x</code>被前一层计算的隐藏状态所代替。</p><p id="fa25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于第1层，</p><pre class="kn ko kp kq fd mk jg ml mm aw mn bi"><span id="06d9" class="ky jq hi jg b fi mo mp l mq mr">h_current = torch.tanh(Tensor(matmul(x,wih_10.T) + bih_10  + matmul(h_previous,whh_10.T) + bhh_10))</span></pre><p id="60b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于第2层，</p><pre class="kn ko kp kq fd mk jg ml mm aw mn bi"><span id="b9e3" class="ky jq hi jg b fi mo mp l mq mr">h_current = torch.tanh(Tensor(matmul(output_1[i],wih_11.T) + bih_11  + matmul(h_previous,whh_11.T) + bhh_11))</span></pre><p id="1914" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，<code class="du jd je jf jg b">output_1</code>表示在层1中计算的隐藏状态。</p><p id="5a8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过比较手动计算的输出，我们可以确认总输出包含由第2层计算的隐藏状态，而最终输出包含由第1层和第2层计算的最后一个元素的隐藏状态。</p></div></div>    
</body>
</html>