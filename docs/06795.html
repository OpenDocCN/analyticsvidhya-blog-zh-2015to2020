<html>
<head>
<title>Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-dce0b8e3506e?source=collection_archive---------25-----------------------#2020-06-02">https://medium.com/analytics-vidhya/linear-regression-dce0b8e3506e?source=collection_archive---------25-----------------------#2020-06-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/2fcf2d03d54cd6d610bda24ae7c45252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d6yc2hc9i2PJvR-cb-ypnQ.png"/></div></div></figure><div class=""/><p id="6330" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归是最简单的机器学习算法之一，属于监督学习技术，用于解决回归问题。</p><p id="8a20" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它用于在自变量的帮助下预测连续的因变量。线性回归的目标是找到能够准确预测连续因变量输出的最佳拟合线。</p><p id="0e18" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过寻找最佳拟合线，算法建立了因变量和自变量之间的关系。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es jp"><img src="../Images/4572460bdfc0782c1d26e9db2634c1c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R9C3aJIiveoZJf1Sb4_R2Q.png"/></div></div></figure><p id="a903" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归的输出应该只是连续值，如价格、年龄、工资等。</p><p id="35ca" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">线性回归一般分为两种:</strong></p><ul class=""><li id="2db2" class="ju jv ht is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated">简单的<strong class="is hu">线性回归</strong>。</li><li id="4218" class="ju jv ht is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">多元线性回归。</li></ul><h1 id="7cda" class="ki kj ht bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">简单线性回归</h1><p id="c750" class="pw-post-body-paragraph iq ir ht is b it lg iv iw ix lh iz ja jb li jd je jf lj jh ji jj lk jl jm jn hb bi translated">在简单线性回归中，基本上有一个因变量和一个自变量，由以下公式定义:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ll"><img src="../Images/fc60ff79d66665d7f5c861fa7e4821ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ciZkakBkHEDxEBuTDDK6Cg.png"/></div></div></figure><p id="c9ac" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> b0 =代表截距</em></p><p id="42c6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> b1 =代表系数</em></p><p id="fa32" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> x =代表自变量</em></p><p id="a99d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> y =代表输出或因变量</em></p><p id="b516" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一个只有一个解释变量的回归模型。</p><p id="68b8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">y- <strong class="is hu">截距</strong>的部分估算过程是基于从<strong class="is hu">回归</strong>模型中排除相关变量。</p><h1 id="5051" class="ki kj ht bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated"><strong class="ak">多元线性回归</strong></h1><p id="f919" class="pw-post-body-paragraph iq ir ht is b it lg iv iw ix lh iz ja jb li jd je jf lj jh ji jj lk jl jm jn hb bi translated">对于一个<strong class="is hu">多元线性回归</strong>当我们有<strong class="is hu">多个输入(x) </strong>时，这条线被称为<strong class="is hu"> a平面</strong>或超平面。因此，这种表示就是方程的形式。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lm"><img src="../Images/e3a6919e20ff509ba74623a3ccb14e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*Jfx203VYFtcM958gbFsiXA.png"/></div></figure><h1 id="be5d" class="ki kj ht bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">最小平方回归线</h1><p id="7bfa" class="pw-post-body-paragraph iq ir ht is b it lg iv iw ix lh iz ja jb li jd je jf lj jh ji jj lk jl jm jn hb bi translated">最小二乘回归是一种为一组数据找到最佳拟合线的方法。它通过创建最小化垂直距离平方和的模型(<a class="ae ln" href="https://www.statisticshowto.com/residual/" rel="noopener ugc nofollow" target="_blank">残差</a>)来做到这一点。</p><p id="77ba" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">距离是平方的，以避免负号距离的问题。那么问题就变成了计算出你应该把线放在哪里，使得从点到线的距离最小化。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lo"><img src="../Images/0991807bd94438016353741b2f2df2db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GSi6R7mLgFMk1iEltaN_Pw.jpeg"/></div></div></figure><h1 id="3bfd" class="ki kj ht bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">梯度下降</h1><p id="db5b" class="pw-post-body-paragraph iq ir ht is b it lg iv iw ix lh iz ja jb li jd je jf lj jh ji jj lk jl jm jn hb bi translated">梯度下降是通过遵循成本函数的梯度来最小化函数的过程。为了使用梯度下降找到函数的局部最小值，我们采取与函数在当前点的梯度的负值成比例的步骤。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lp"><img src="../Images/06b7c314eb0604f710947e5956510a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*S-AV7aX00K1oMTQHidgw_g.png"/></div></figure><p id="b9e4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这包括知道成本和导数的形式，这样从一个给定点开始，你就知道了梯度，并且可以向那个方向移动。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es lq"><img src="../Images/daa6de7cb3be6bffb9c3c2ba85e0cb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*G3evFxIAlDchOx5Wl7bV5g.png"/></div></figure><h2 id="dc38" class="lr kj ht bd kk ls lt lu ko lv lw lx ks jb ly lz kw jf ma mb la jj mc md le me bi translated">Python实现</h2><p id="a2f2" class="pw-post-body-paragraph iq ir ht is b it lg iv iw ix lh iz ja jb li jd je jf lj jh ji jj lk jl jm jn hb bi translated"><strong class="is hu"> <em class="jo">数据集:薪资</em> </strong></p><p id="cfd4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">以下是薪资数据集线性回归的python实现链接:</em></p><p id="3f4f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae ln" href="https://github.com/InternityFoundation/MachineLearning_Navu4/tree/master/Day%206%20:%20Linear%20Regression" rel="noopener ugc nofollow" target="_blank">https://github . com/internity foundation/machine learning _ navu 4/tree/master/Day % 206% 20:% 20 linear % 20 regression</a></p><p id="72cc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面是<a class="ae ln" href="https://github.com/bhartendudubey/Supervised-Learning-Algorithms/blob/master/Linear_regression.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="jo"> Jupyter笔记本</em> </a>为python实现的线性回归数学方式。</p><p id="d146" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">链接:<a class="ae ln" href="https://github.com/InternityFoundation/MachineLearning_Navu4/blob/master/Day%206%20:%20Linear%20Regression/Linear_regression(mathematical_way).ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/internity foundation/machine learning _ navu 4/blob/master/Day % 206% 20:% 20 Linear % 20 regression/Linear _ regression(mathematical _ way)。ipynb </a></p></div></div>    
</body>
</html>