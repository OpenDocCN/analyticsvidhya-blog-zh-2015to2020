<html>
<head>
<title>Compressing 4 variables into 3 using Autoencoders — Data_compression[0]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自动编码器将4个变量压缩为3个— Data_compression[0]</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/compressing-4-variables-into-3-aec88be7c7a3?source=collection_archive---------33-----------------------#2020-05-11">https://medium.com/analytics-vidhya/compressing-4-variables-into-3-aec88be7c7a3?source=collection_archive---------33-----------------------#2020-05-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="52c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将学习如何训练一个模型，该模型可以将4个变量的信息编码为3个变量，并将它们存储在磁盘上，然后在需要时将4个变量解码回来。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/0841c203069425b1b4c80ff859afbe68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mBkdMoUDBhG_-qQ5jXJVaA.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:<a class="ae jt" href="https://pixabay.com/illustrations/analytics-information-innovation-3088958/" rel="noopener ugc nofollow" target="_blank">https://pix abay . com/插图/分析-信息-创新-3088958/ </a></figcaption></figure><h1 id="e499" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">介绍</h1><p id="5aa5" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">为了优化数据传输和存储，压缩数据一直是一个大问题，特别是由于数据爆炸式增长的速度明显加快。在本文中，我将尝试解释我的方法，将4个变量的数据压缩(编码)成3个，将它们存储在磁盘上，然后解压缩(解码)编码的变量，并在需要时恢复原始数据。我将分享我做的一些小调整，让编码器表现得更好。这可以被认为是图像压缩的较小版本，因为图像只是由有意义的顺序的数字组成的矩阵。实际上，这样做的主要动机是在将来尝试将类似的方法应用于图像压缩。</p><p id="c65d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">剧透:我很快也会写关于图像压缩的文章。😜</p><h1 id="d194" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">子问题</h1><ol class=""><li id="6f83" class="kx ky hi ih b ii ks im kt iq kz iu la iy lb jc lc ld le lf bi translated"><strong class="ih hj">预处理</strong>:从给定的数据中提取有用的信息，使模型更好地学习。</li><li id="6092" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated"><strong class="ih hj">模型定义</strong>:为任务决定和定义完美的神经网络架构。</li><li id="37ed" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated"><strong class="ih hj">训练</strong>:用梯度下降法训练模型。</li></ol><h2 id="13ef" class="ll jv hi bd jw lm ln lo ka lp lq lr ke iq ls lt ki iu lu lv km iy lw lx kq ly bi translated">1.预处理</h2><p id="742b" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">这一步是指操纵输入数据并提取有用的信息供神经网络使用，以使神经网络能够更好地学习。我们将分两步进行预处理。</p><p id="8040" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> A .正常化</strong></p><p id="5c79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(对于有深厚学习背景的人来说，这很常见。如果你已经知道什么是规范化以及我们为什么使用它，请直接跳到下一步。)</p><p id="ae7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是正常化？</strong></p><p id="0c43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lz">使输入的平均值为0，标准差为1。这使得数据被缩放，修复了数据中的异常值，并使得计算机的学习过程更容易、更平滑、更快。这可以通过下面的代码片段来实现:</em></p><pre class="je jf jg jh fd ma mb mc md aw me bi"><span id="8009" class="ll jv hi mb b fi mf mg l mh mi"># train contains the raw(initial) data</span><span id="7361" class="ll jv hi mb b fi mj mg l mh mi">mean = train.mean()<br/>std = train.std()</span><span id="7079" class="ll jv hi mb b fi mj mg l mh mi">train_data = (train - mean) / std<br/>test_data = (test - mean) / std</span></pre><p id="8f2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">标准化数据并不会真正影响原始数据和预测。在这种情况下，当您的模型预测来自3个编码变量的原始数据时，您只需将模型的结果乘以标准差，然后加上平均值，即可获得原始比例的数据。</p><p id="92a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> B .添加奇异值</strong></p><p id="f723" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">奇异值分解(SVD)是一个数学过程(基本上是矩阵运算),将矩阵分解成我们选择的低维。SVD基本上将我们的矩阵分解成3个更小的矩阵。其中一个包含原始矩阵的奇异值。我还尝试将这3个矩阵的不同组合与原始数据连接起来，但只有奇异值比其他组合更好。如果您对SVD一无所知，那么它确实值得放在您的工具箱中。你可以在这个<a class="ae jt" href="https://github.com/fastai/numerical-linear-algebra/blob/master/nbs/2.%20Topic%20Modeling%20with%20NMF%20and%20SVD.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>上了解更多关于奇异值的知识，你也可以在这个<a class="ae jt" href="https://colab.research.google.com/drive/1bHwfyiklHl88FxvZ4Bk5dU2cewxpjOK0?usp=sharing" rel="noopener ugc nofollow" target="_blank"> google colab </a>上运行同样的笔记本。</p><p id="bd6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我们要做的是首先从这4个变量中构造一个(4 x 4)对角线矩阵(这4个数字将位于矩阵的对角线位置)。我们将输入这个矩阵，用3个分量表示这个矩阵(3是因为我们想把变量压缩成3)。这将返回给我们3个矩阵。所以我试着连接这些矩阵的不同组合。但是只使用奇异值(只来自一个矩阵的数据，只有3个数字，矩阵的对角线元素)对我来说是最好的。</p><blockquote class="mk ml mm"><p id="f3e0" class="if ig lz ih b ii ij ik il im in io ip mn ir is it mo iv iw ix mp iz ja jb jc hb bi translated">"奇异值分解并不像它应该的那样出名."—吉尔伯特·斯特朗</p></blockquote><p id="2c6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lz">注意:最初我使用主成分分析(PCA)而不是奇异值分解，因为我认为它应该比奇异值分解表现得更好，但令人惊讶的是奇异值分解在比较中表现得稍微好一些。甚至在实践中使用SVD来实现PCA，所以使用SVD代替PCA也将节省一些计算。你可以在这个笔记本</em>  <em class="lz">中了解关于PCA </em> <a class="ae jt" href="https://github.com/fastai/numerical-linear-algebra/blob/master/nbs/3.%20Background%20Removal%20with%20Robust%20PCA.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="lz">的更多信息，你也可以在这个</em></a><a class="ae jt" href="https://colab.research.google.com/drive/15iQ3gLMXbnU1w-B9-y4BI4M2nUKm-rl-?usp=sharing" rel="noopener ugc nofollow" target="_blank"><em class="lz">Google colab</em></a><em class="lz">上自己运行同一个笔记本。</em></p><p id="7005" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么我认为PCA或SVD可能有帮助？</strong></p><p id="fe42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为SVD和PCA背后的整个概念是压缩数据的维度，尽可能多地保留信息。而且我们的任务也差不多。但是使用SVD或PCA不能将4个变量压缩成3个，因为它们的输出是三个不同的矩阵，这无论如何都不能导致压缩到3个变量。<em class="lz">(因此，根据我的观点，使用SVD/PCA的压缩对较大的矩阵更有效。)</em>因此，我想为什么不首先使矩阵2D(通过创建一个对角矩阵，如我前面所述)使用SVD或PCA压缩该矩阵，并将一些压缩数据与原始数据连接起来，看看是否有助于进一步压缩它。奇异值(SVD的三个输出矩阵之一)证明是有帮助的。</p><h2 id="679d" class="ll jv hi bd jw lm ln lo ka lp lq lr ke iq ls lt ki iu lu lv km iy lw lx kq ly bi translated">2.模型定义</h2><p id="798f" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">这一步指的是试图找到最佳的神经网络结构，以使结果最佳。</p><p id="3677" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以正如标题所说，我们将使用自动编码器。自动编码器由称为编码器和解码器的两个内部神经网络结构组成。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mq"><img src="../Images/665ad2f814e509349cfa8851f0425d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*wKE69-fX180Q_gkzYzGbwg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:<a class="ae jt" href="https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_structure.png" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/auto encoder #/media/File:auto encoder _ structure . png</a></figcaption></figure><ol class=""><li id="c938" class="kx ky hi ih b ii ij im in iq mr iu ms iy mt jc lc ld le lf bi translated"><strong class="ih hj">编码器:</strong>对数据进行编码的神经网络的一组顺序层。</li><li id="973d" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated"><strong class="ih hj">解码器:</strong>从代码中重建原始数据(解码/解压缩数据)的层。</li></ol><p id="2aa1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Pytorch中任务的简单自动编码可以定义为:</p><pre class="je jf jg jh fd ma mb mc md aw me bi"><span id="7352" class="ll jv hi mb b fi mf mg l mh mi"><strong class="mb hj">class</strong> <strong class="mb hj">Autoencoder</strong>(nn.Module):</span><span id="7588" class="ll jv hi mb b fi mj mg l mh mi"><em class="lz">    # here the parameter'in_features' is set<br/>    # to 7 because we have also concatenated the<br/>   # 3 singular values with the 4 variables.</em></span><span id="b252" class="ll jv hi mb b fi mj mg l mh mi"><strong class="mb hj">    def</strong> __init__(self, in_features=7):<br/>        super().__init__()<br/>        self.encoder = nn.Sequential(<br/>            nn.Linear(in_features, 128),<br/>            nn.BatchNorm1d(128),<br/>            nn.Tanh(),<br/>            nn.Linear(128, 3),<br/>            nn.Tanh()<br/>        )</span><span id="0b0e" class="ll jv hi mb b fi mj mg l mh mi">        self.decoder = nn.Sequential(<br/>            nn.Linear(3, 128),<br/>            nn.BatchNorm1d(128),<br/>            nn.Tanh(),<br/>            nn.Linear(128, 4),<br/>            nn.Tanh()<br/>        )</span><span id="9628" class="ll jv hi mb b fi mj mg l mh mi"><em class="lz">    # In Pytorch, The </em><strong class="mb hj"><em class="lz">forward</em></strong><em class="lz">() </em><strong class="mb hj"><em class="lz">method</em></strong><em class="lz"> is the actual network <br/>    #</em><strong class="mb hj"><em class="lz"> </em></strong><em class="lz">transformation. The </em><strong class="mb hj"><em class="lz">forward method</em></strong><em class="lz"> is the mapping that <br/>    # maps an input tensor to a prediction output tensor.</em></span><span id="4fa5" class="ll jv hi mb b fi mj mg l mh mi"><strong class="mb hj">    def</strong> forward(self, x):<br/>        encoded = self.encoder(x)<br/>       <strong class="mb hj"><em class="lz"> # Time to save the encoded data</em></strong><br/>        decoded = self.decoder(encoded)<br/>        return decoded</span></pre><p id="9720" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lz">注意:为了简单起见，这里我们只添加了几层，但最初可能会有更多层，比如一堆nn。线性，nn。BatchNorm1d和nn。Tanh层。nn。Tanh的作用是增加网络的非线性。</em></p><p id="e7d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么非线性？</strong></p><p id="fad2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为如果没有非线性，模型将变成一堆线性函数(这里是层),而一个以上线性函数的组合又是一个线性函数。</p><p id="f962" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还有其他非线性函数，如nn。ReLU也是但是nn。当我们测试时，Tanh在这种情况下工作得更好。</p><p id="4997" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么是Batchnorm？</strong></p><p id="b2c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们规范化输入数据以使模型的学习过程变得容易时，Batchnorm对隐藏层做了同样的事情。它使激活正常化，使学习过程更好。因为在训练过程中，一些激活可能会爆炸，所以Batchnorm会放大隐藏层中的所有内容。</p><h2 id="8ca8" class="ll jv hi bd jw lm ln lo ka lp lq lr ke iq ls lt ki iu lu lv km iy lw lx kq ly bi translated">3.培养</h2><p id="6cd8" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">这一步指的是最终训练模型并微调参数以充分利用模型。</p><p id="6047" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我使用了循环学习率(CLR)方法。这种方法让学习率在合理的边界之间循环变化，以优化训练。你可以在这里了解更多关于CLR <a class="ae jt" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mu"><img src="../Images/128d847cc3d3667302cca6c165a0cc25.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/0*0PBFr16GSwC6dzRs"/></div></figure><p id="341b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我用均方差作为损失函数。在对学习速度、时期、体重下降等参数做了一些小的调整后，我开始得到相当好的结果。</p><h1 id="22c4" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结果</h1><p id="32f4" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated"><strong class="ih hj"> A .不同方法的绩效</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mv"><img src="../Images/f1dd0c0fdd7b00cd817d5cc1f694e031.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*9ILtJrCIBoJGsdC2t04PSA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">截图来自我的白皮书</figcaption></figure><p id="6a35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">b .</strong>T6】验证损失</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/fe7de08097bbd6dd4f59c04c223b57c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/0*--axqdZmBHhl4vvf"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">jupyter笔记本截图</figcaption></figure><p id="f4b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">测试集中每个数据项的行平均均方误差损失图。平均亏损0.0073。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/c61d5efb758c94fbc693986de0b8a78d.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/0*UrwetH_54D70BmGD"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">jupyter笔记本截图</figcaption></figure><p id="6910" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为测试集中的每个数据条目分别绘制行[(预期结果)/预期]值的图形。</p><p id="219b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lz">注意:在最后一个图表中，由于将[预期结果]除以较小的数字，范围最大为10。</em></p><h1 id="c666" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">结论</h1><p id="2c72" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">我们可以对我们的数据和架构进行这种小调整，以提高模型质量。</p><ul class=""><li id="6b00" class="kx ky hi ih b ii ij im in iq mr iu ms iy mt jc mx ld le lf bi translated">你也可以看看我写的白皮书。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="my mz l"/></div></figure><ul class=""><li id="2c45" class="kx ky hi ih b ii ij im in iq mr iu ms iy mt jc mx ld le lf bi translated">在下面的GitHub库中找到代码。</li></ul><div class="na nb ez fb nc nd"><a href="https://github.com/rushabh-v/Autoencoder-4-to-3-variables" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab dw"><div class="nf ab ng cl cj nh"><h2 class="bd hj fi z dy ni ea eb nj ed ef hh bi translated">rush abh-v/自动编码器-4到3变量</h2><div class="nk l"><h3 class="bd b fi z dy ni ea eb nj ed ef dx translated">将4个变量压缩成3个变量的自动编码器。— rushabh-v/Autoencoder-4到3变量</h3></div><div class="nl l"><p class="bd b fp z dy ni ea eb nj ed ef dx translated">github.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr jn nd"/></div></div></a></div><p id="b026" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae jt" href="https://colab.research.google.com/github/rushabh-v/Autoencoder-4-to-3-variables/blob/master/main.ipynb" rel="noopener ugc nofollow" target="_blank"> google colab这里</a>玩转代码。</p><h2 id="05ec" class="ll jv hi bd jw lm ln lo ka lp lq lr ke iq ls lt ki iu lu lv km iy lw lx kq ly bi translated">参考</h2><ol class=""><li id="47cf" class="kx ky hi ih b ii ks im kt iq kz iu la iy lb jc lc ld le lf bi translated"><a class="ae jt" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank"> L. N. Smith关于循环学习率的研究论文【2017年4月】</a></li><li id="b31b" class="kx ky hi ih b ii lg im lh iq li iu lj iy lk jc lc ld le lf bi translated"><a class="ae jt" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Singular_value_decomposition</a></li></ol><p id="ed0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">P.S. <em class="lz">带图像压缩的数据压缩[1] </em>即将出版🤓</p></div><div class="ab cl ns nt gp nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="hb hc hd he hf"><blockquote class="nz"><p id="9fa6" class="oa ob hi bd oc od oe of og oh oi jc dx translated">“拷问数据，它什么都会坦白。”</p></blockquote></div></div>    
</body>
</html>