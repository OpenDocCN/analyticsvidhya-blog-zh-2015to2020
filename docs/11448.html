<html>
<head>
<title>A Beginner’s Guide to Natural Language Processing — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理入门指南—第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-beginners-guide-to-natural-language-processing-part-2-fdf73667df13?source=collection_archive---------14-----------------------#2020-12-03">https://medium.com/analytics-vidhya/a-beginners-guide-to-natural-language-processing-part-2-fdf73667df13?source=collection_archive---------14-----------------------#2020-12-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/a95597cef233c671a1d93b1ac9248912.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DB37nlSAKztMzDKyd25daQ.jpeg"/></div></div></figure><div class=""/><p id="1a87" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">预处理和文本清理在向人们介绍NLP的本系列文章的第1部分中有所介绍。在这篇文章中，你会发现:</p><ol class=""><li id="73cb" class="jp jq ht is b it iu ix iy jb jr jf js jj jt jn ju jv jw jx bi translated">代币。</li><li id="9c4d" class="jp jq ht is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">N-grams。</li><li id="9979" class="jp jq ht is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">共生矩阵。</li><li id="d422" class="jp jq ht is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">PMI——逐点互信息。</li></ol><h1 id="caa0" class="kd ke ht bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">代币</h1><p id="7210" class="pw-post-body-paragraph iq ir ht is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">标记是构成句子的单位。它可以是一系列字符、数字、标点符号或它们的组合，通常用空格隔开。</p><p id="7b42" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据所使用的包的不同，令牌的识别方式也不同。对于一些包，像<strong class="is hu"> NLTK </strong>，标点符号也被视为单独的标记。所以单词<strong class="is hu">T5不能T7】会拆分成2个令牌<strong class="is hu">T9】caT11】和<strong class="is hu">T13】n不能T15】。但是有了像Spacy这样的库，它将仍然是一个单词。然而，它通过将标点符号附加到它所伴随的单词上，产生了另一个问题。如下例所示，<strong class="is hu"> <em class="lg">叹息，</em> </strong>由NLTK制作了2个令牌，而由Spacy制作了一个令牌。</strong></strong></strong></p><figure class="lh li lj lk fd hk"><div class="bz dy l di"><div class="ll lm l"/></div></figure><p id="c1ca" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">NLTK中的过程似乎更容易，因为它给出了一个列表作为回报。然而，在扩展到更大的数据集和更下游的过程时，尽管有开销，Spacy做得更快。</p><p id="af43" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">令牌作为单独的实体通常没有什么意义。虽然它们确实向机器传达了字典的意思，但它们没有给出任何上下文，也没有给出句子的方向。为了解决这个问题，我们使用N元语法。</p><h1 id="0943" class="kd ke ht bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">N-Grams</h1><p id="b508" class="pw-post-body-paragraph iq ir ht is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">N-gram是通常同时出现的N个标记的组合。比如<strong class="is hu"><em class="lg">new</em></strong><em class="lg"/>这个词在很多语境中出现，但是<strong class="is hu"> <em class="lg"> york </em> </strong>这个词却和<strong class="is hu"> <em class="lg"> new频繁出现。</em> </strong>所以我们把两者结合起来，得到<strong class="is hu"> <em class="lg">纽约</em> </strong>给出更好的信息。</p><p id="dfb9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">组合两个代币(<strong class="is hu">一</strong>克)给我们一个<strong class="is hu">比</strong>克。高阶<strong class="is hu">n</strong>-gram由2 (n-1)个gram组成。2个二元组给出一个<strong class="is hu">三元组</strong>克，2个三元组形成一个<strong class="is hu">四元组</strong>克，以此类推。</p><p id="db45" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在<a class="ae jo" href="https://www.kaggle.com/coder98/twitter-data/version/1" rel="noopener ugc nofollow" target="_blank"> Twitter数据集</a>中发现的几个常见二元模型及其频率如下所示。</p><figure class="lh li lj lk fd hk er es paragraph-image"><div class="er es ln"><img src="../Images/8bbcd15b7fba21f72d4f9407355f9b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*jrXIeLfcS186nOpL4teSng.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">从推特收集的10个大人物。</figcaption></figure><p id="dbaa" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如你所看到的，像<em class="lg">got</em>、<em class="lg">got</em>和<em class="lg"> wanna </em>这样的词经常与撇号一起使用，并被分成两个词。名单上的其他人，对我们来说似乎很容易预测。但是当数据集不熟悉时，或者当它比我们所能理解的大得多时，我们可能无法从数据中获得很多意义来验证我们自己的二元模型。</p><p id="94b7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，当有许多二元模型具有相同的第一个标记或相同的最后一个标记时，统计度量在更准确地识别二元模型和定义它们的优先级方面是有用的。</p><h1 id="8f85" class="kd ke ht bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">共现</h1><p id="b1ab" class="pw-post-body-paragraph iq ir ht is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">我们能想到的最简单的方法之一，就是计算它们一起出现的次数，来找出记号之间的紧密联系。用专业术语来说，这叫做共现。它简单地计算一个标记跟随另一个标记的次数，并形成评分标准。通常，这被表示为n*n阶的矩阵，其中n是一组字符串/文档中唯一标记的总数。</p><p id="1cee" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于集合串:<br/>'猫坐'，'猫坐在帽子里'，'戴帽子的猫'，共现矩阵看起来是这样的。</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ls"><img src="../Images/a10df13754bb3e998cfed32af3d7d0ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QwjMtByigHIeYmSDLpaMtA.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">共现矩阵来查找二元模型，其中行名是第一个单词，列名是第二个单词。</figcaption></figure><p id="2252" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，这是一种相当幼稚的方法，因为它只提供计数。我们需要一种更健壮的方法，同时考虑其他参数，比如一个标记在整个语料库中出现了多少次，以及它作为二元模型的一部分出现了多少次。</p><h2 id="c2d6" class="lt ke ht bd kf lu lv lw kj lx ly lz kn jb ma mb kr jf mc md kv jj me mf kz mg bi translated">逐点互信息(PMI)</h2><p id="90a5" class="pw-post-body-paragraph iq ir ht is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">如果令牌<em class="lg"> play </em>在语料库中总共出现100次，并且紧接在令牌<em class="lg"> cricket </em>之前出现5次，则用<em class="lg"> play cricket </em>替换令牌<em class="lg"> play </em>是不合逻辑的。但是，如果它紧接在令牌<em class="lg">足球</em>之前出现95次，那么将令牌<em class="lg">玩</em>替换为<em class="lg">玩足球</em>似乎在直觉上是正确的。</p><p id="039c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑到形成N元语法的目的是减少独特标记的数量以形成健壮的语料库，这个过程似乎是恰当的。PMI是以相对方式形成二元模型的可用指标之一。</p><p id="89e9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PMI是使用条件概率计算的。</p><figure class="lh li lj lk fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/43be4dce31a43b35c068598db9971b2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ix_s6qBbj9b-LRirt9-giw.jpeg"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">PMI计算公式。</figcaption></figure><p id="cacf" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">p(x|y) =在记号y之后找到记号x的概率<br/> p(y|x) =在记号x之后找到记号y的概率<br/> p(y) =记号y在整个语料库中的概率。p(x) =标记x在整个语料库中的概率。</p><p id="3b1e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面给出了使用NLTK查找PMI分数和二元模型的示例代码段。</p><figure class="lh li lj lk fd hk"><div class="bz dy l di"><div class="ll lm l"/></div></figure><p id="62ad" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本系列的<a class="ae jo" href="https://chraviraj.medium.com/a-beginners-guide-to-natural-language-processing-part-3-edad0799397a" rel="noopener">下一部分</a>中，您将会发现——词性标注、词干提取和词汇化。</p></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><p id="12f7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我真的希望你喜欢这篇文章。如果这篇文章有帮助，请为它鼓掌。欢迎任何建议或批评。感谢阅读！</p></div></div>    
</body>
</html>