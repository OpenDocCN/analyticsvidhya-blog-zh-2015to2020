<html>
<head>
<title>Step-by-Step Deep Learning Tutorial to Build your own Video Classification Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一步一步的深度学习教程，构建自己的视频分类模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/step-by-step-deep-learning-tutorial-to-build-your-own-video-classification-model-2a7b3a85610b?source=collection_archive---------6-----------------------#2019-09-03">https://medium.com/analytics-vidhya/step-by-step-deep-learning-tutorial-to-build-your-own-video-classification-model-2a7b3a85610b?source=collection_archive---------6-----------------------#2019-09-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a6ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经写了大量关于如何使用图像数据构建计算机视觉模型的文章和指南。检测图像中的对象，对这些对象进行分类，从电影海报中生成标签——使用计算机视觉和深度学习，我们可以做很多事情。</p><p id="01d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一次，我决定将我的注意力转向计算机视觉不太为人所知的方面——视频！我们正以前所未有的速度消费视频内容。我觉得计算机视觉的这个领域对数据科学家来说有很大的潜力。</p><p id="2d6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我对将同样的计算机视觉算法应用于视频数据感到好奇。我用来建立图像分类模型的方法——可推广吗？</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/e0a90b17499a74c591dbae991cb9f488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*84wRJVfZMPvzdxWk.jpg"/></div></div></figure><p id="ddab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">机器处理视频可能会很棘手。与图像的静态性质相反，它们的动态性质会使数据科学家构建这些模型变得复杂。</p><p id="da18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是不要担心，这与处理图像数据没有什么不同。在本文中，我们将使用Python构建我们自己的视频分类模型。这是一个非常实用的教程，所以启动你的Jupyter笔记本吧——这将是一次非常有趣的旅程。</p><p id="a417" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jq">如果你是深度学习和计算机视觉领域的新手，我们有完美的课程供你开始你的旅程:</em></p><h1 id="7b98" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">视频分类概述</h1><p id="2d05" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">当你真正分解它时，你会如何定义视频？</p><blockquote class="ku kv kw"><p id="e61c" class="if ig jq ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated"><em class="hi">我们可以说视频是按照特定顺序排列的一组图像的集合。这些图像集也称为帧。</em></p></blockquote><p id="bd3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是为什么视频分类问题与图像分类问题没有什么不同。对于图像分类任务，我们获取图像，使用特征提取器(如卷积神经网络或CNN)从图像中提取特征，然后基于这些提取的特征对图像进行分类。<em class="jq">视频分类只需要一个额外的步骤。</em></p><p id="e741" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们首先从给定的视频中提取帧。然后，我们可以遵循与图像分类任务相同的步骤。这是处理视频数据最简单的方法。</p><p id="4114" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实际上，还有多种其他方法来处理视频，甚至还有一个视频分析的利基领域。我强烈推荐阅读下面的文章，以了解如何在Python中处理视频和提取帧:</p><p id="073b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，我们将使用CNN从视频帧中提取特征。如果您需要快速复习CNN是什么以及它们是如何工作的，您应该从这里开始:</p><h1 id="0304" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">建立视频分类模型的步骤</h1><p id="9ac1" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">为建立一个能够将视频分类到各自类别的模型而兴奋吗？我们将致力于<a class="ae jd" href="https://www.crcv.ucf.edu/data/UCF101.php" rel="noopener ugc nofollow" target="_blank">ucf 101——动作识别数据集</a>，它由属于101个不同类别的13，320个不同的视频剪辑组成。</p><p id="216b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我总结一下构建视频分类模型的步骤:</p><ol class=""><li id="4be0" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lf lg lh li bi translated">浏览数据集并创建定型和验证集。我们将使用训练集来训练模型，使用验证集来评估训练好的模型</li><li id="8963" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">从训练和验证集中的所有视频中提取帧</li><li id="65db" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">预处理这些帧，然后使用训练集中的帧训练模型。使用验证集中的框架评估模型</li><li id="75cb" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">一旦我们对验证集的性能感到满意，就可以使用训练好的模型对新视频进行分类</li></ol><p id="00c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们现在开始探索数据吧！</p><h1 id="9fe5" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">探索视频分类数据集</h1><p id="6593" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">你可以从<a class="ae jd" href="https://www.crcv.ucf.edu/data/UCF101.php" rel="noopener ugc nofollow" target="_blank">官方UCF101网站</a>下载数据集。数据集在一个<em class="jq">中。rar </em>格式，所以我们首先要从中提取视频。创建一个新文件夹，假设是“Videos”(您也可以选择任何其他名称)，然后使用以下命令提取所有下载的视频:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="b590" class="lt js hi lp b fi lu lv l lw lx">unrar e UCF101.rar Videos/</span></pre><p id="d260" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">UCF101的官方文档声明:</p><blockquote class="ku kv kw"><p id="29c5" class="if ig jq ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated"><strong class="ih hj"> " </strong>在训练和测试中，将属于同一组的视频分开是非常重要的。因为组中的视频是从单个长视频中获得的，所以在训练和测试集中共享来自同一组的视频将会提供高性能。”</p></blockquote><p id="0579" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们将按照官方文档中的建议，将数据集分为训练集和测试集。你可以从这里下载训练/测试分割<a class="ae jd" href="https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip" rel="noopener ugc nofollow" target="_blank">。<em class="jq">请记住，由于我们正在处理一个大型数据集，您可能需要很高的计算能力。</em></a></p><p id="f329" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将视频放在一个文件夹中，将训练/测试分割文件放在另一个文件夹中。接下来，我们将创建数据集。打开你的Jupyter笔记本，按照下面的代码块。我们将首先导入所需的库:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="8b60" class="lt js hi lp b fi lu lv l lw lx">import cv2     # for capturing videos<br/>import math   # for mathematical operations<br/>import matplotlib.pyplot as plt    # for plotting the images<br/>%matplotlib inline<br/>import pandas as pd<br/>from keras.preprocessing import image   # for preprocessing the images<br/>import numpy as np    # for mathematical operations<br/>from keras.utils import np_utils<br/>from skimage.transform import resize   # for resizing images<br/>from sklearn.model_selection import train_test_split<br/>from glob import glob<br/>from tqdm import tqdm</span></pre><p id="2e8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将视频名称存储在数据帧中:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="30d5" class="lt js hi lp b fi lu lv l lw lx"># open the .txt file which have names of training videos<br/>f = open("trainlist01.txt", "r")<br/>temp = f.read()<br/>videos = temp.split('\n')<br/><br/># creating a dataframe having video names<br/>train = pd.DataFrame()<br/>train['video_name'] = videos<br/>train = train[:-1]<br/>train.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ly"><img src="../Images/118baf6f23ae8399738eb4d178afa928.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/0*RBikFY7CZS92DQU9.png"/></div></figure><p id="daaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是<em class="jq">中给出的视频名称。txt </em>文件。它没有正确对齐，我们需要对它进行预处理。在此之前，我们也为测试视频创建一个类似的数据框架:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="c899" class="lt js hi lp b fi lu lv l lw lx"># open the .txt file which have names of test videos<br/>f = open("testlist01.txt", "r")<br/>temp = f.read()<br/>videos = temp.split('\n')<br/><br/># creating a dataframe having video names<br/>test = pd.DataFrame()<br/>test['video_name'] = videos<br/>test = test[:-1]<br/>test.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lz"><img src="../Images/26cd211b5f9241e37393d4fcfdb8fb93.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/0*OI4Y13tCk_GdCaEg.png"/></div></figure><p id="28dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将添加每个视频的标签(针对训练集和测试集)。有没有注意到视频名称中“/”之前的整个部分都代表了视频的标签？因此，我们将在“/”上拆分整个字符串，并为所有视频选择标签:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="10a2" class="lt js hi lp b fi lu lv l lw lx"># creating tags for training videos<br/>train_video_tag = []<br/>for i in range(train.shape[0]):<br/>    train_video_tag.append(train['video_name'][i].split('/')[0])<br/>    <br/>train['tag'] = train_video_tag<br/><br/># creating tags for test videos<br/>test_video_tag = []<br/>for i in range(test.shape[0]):<br/>    test_video_tag.append(test['video_name'][i].split('/')[0])<br/>    <br/>test['tag'] = test_video_tag</span></pre><p id="db5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么下一步是什么？现在，我们将从用于训练模型的训练视频中提取帧。我会将所有帧存储在名为train_1的文件夹中。</p><p id="ddbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，<strong class="ih hj"> <em class="jq">首先新建一个文件夹，重命名为‘train _ 1’</em></strong>然后按照下面给出的代码提取帧:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="df53" class="lt js hi lp b fi lu lv l lw lx"># storing the frames from training videos<br/>for i in tqdm(range(train.shape[0])):<br/>    count = 0<br/>    videoFile = train['video_name'][i]<br/>    cap = cv2.VideoCapture('UCF/'+videoFile.split(' ')[0].split('/')[1])   # capturing the video from the given path<br/>    frameRate = cap.get(5) #frame rate<br/>    x=1<br/>    while(cap.isOpened()):<br/>        frameId = cap.get(1) #current frame number<br/>        ret, frame = cap.read()<br/>        if (ret != True):<br/>            break<br/>        if (frameId % math.floor(frameRate) == 0):<br/>            # storing the frames in a new folder named train_1<br/>            filename ='train_1/' + videoFile.split('/')[1].split(' ')[0] +"_frame%d.jpg" % count;count+=1<br/>            cv2.imwrite(filename, frame)<br/>    cap.release()</span></pre><p id="ec62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将需要一些时间，因为训练集中有超过9，500个视频。一旦帧被提取出来，我们将把这些帧的名称和它们相应的标签保存在一个<em class="jq">中。csv </em>文件。创建此文件将帮助我们读取我们将在下一节看到的帧:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="a6d4" class="lt js hi lp b fi lu lv l lw lx"># getting the names of all the images<br/>images = glob("train_1/*.jpg")<br/>train_image = []<br/>train_class = []<br/>for i in tqdm(range(len(images))):<br/>    # creating the image name<br/>    train_image.append(images[i].split('/')[1])<br/>    # creating the class of image<br/>    train_class.append(images[i].split('/')[1].split('_')[1])<br/>    <br/># storing the images and their class in a dataframe<br/>train_data = pd.DataFrame()<br/>train_data['image'] = train_image<br/>train_data['class'] = train_class<br/><br/># converting the dataframe into csv file <br/>train_data.to_csv('UCF/train_new.csv',header=True, index=False)</span></pre><p id="b326" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们已经从所有的训练视频中提取了帧并保存在一个<em class="jq">中。csv </em>文件及其相应的标签。现在是时候训练我们的模型了，我们将使用它来预测测试集中视频的标签。</p><h1 id="3f56" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">训练视频分类模型</h1><p id="c4f8" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">终于到了训练我们视频分类模型的时候了！我相信这是教程中最令人期待的部分。为了便于理解，我将这个步骤分成了几个子步骤:</p><ol class=""><li id="f15b" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lf lg lh li bi translated">阅读我们之前为训练图像提取的所有帧</li><li id="9bd8" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">创建一个验证集，这将有助于我们检查我们的模型在看不见的数据上的表现</li><li id="71b8" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">定义我们模型的架构</li><li id="c158" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">最后，训练模型并保存其权重</li></ol><h1 id="0023" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">读取所有视频帧</h1><p id="6645" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">所以，让我们开始第一步，我们将提取帧。我们将首先导入库:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="a770" class="lt js hi lp b fi lu lv l lw lx">import keras<br/>from keras.models import Sequential<br/>from keras.applications.vgg16 import VGG16<br/>from keras.layers import Dense, InputLayer, Dropout, Flatten<br/>from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D<br/>from keras.preprocessing import image<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from tqdm import tqdm<br/>from sklearn.model_selection import train_test_split</span></pre><p id="007e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">记住，我们创造了一个<em class="jq">。csv </em>文件，包含每个帧的名称和它们对应的标签？让我们也来读一读:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="9962" class="lt js hi lp b fi lu lv l lw lx">train = pd.read_csv('UCF/train_new.csv')<br/>train.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ma"><img src="../Images/9cd686f3f76f4151ed84150de255d53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/0*OUXezOb9J1wIvKS0.png"/></div></figure><p id="6cf8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是前五行的样子。我们对每一帧都有相应的类或标签。现在，用这个<em class="jq">。csv </em>文件，我们将读取之前提取的帧，然后将这些帧存储为NumPy数组:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="4257" class="lt js hi lp b fi lu lv l lw lx"># creating an empty list<br/>train_image = []<br/><br/># for loop to read and store frames<br/>for i in tqdm(range(train.shape[0])):<br/>    # loading the image and keeping the target size as (224,224,3)<br/>    img = image.load_img('train_1/'+train['image'][i], target_size=(224,224,3))<br/>    # converting it to array<br/>    img = image.img_to_array(img)<br/>    # normalizing the pixel value<br/>    img = img/255<br/>    # appending the image to the train_image list<br/>    train_image.append(img)<br/>    <br/># converting the list to numpy array<br/>X = np.array(train_image)<br/><br/># shape of the array<br/>X.shape</span></pre><p id="0ff7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输出:(73844，224，224，3) </strong></p><p id="d070" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有73，844张大小为(224，224，3)的图片。接下来，我们将创建验证集。</p><h1 id="a2bd" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">创建验证集</h1><p id="0e4c" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">为了创建验证集，我们需要确保每个类在训练集和验证集中的分布是相似的。我们可以使用分层参数来做到这一点:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="21dc" class="lt js hi lp b fi lu lv l lw lx"># separating the target<br/>y = train['class']<br/><br/># creating the training and validation set<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)</span></pre><p id="a4e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，<em class="jq">分层= y </em>(即每帧的类别或标签)在训练和验证集中保持了类似的类别分布。</p><p id="bdc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">记住——一个视频可以分为101个类别。因此，我们必须在目标中创建101个不同的列，每个类别一个。为此，我们将使用<em class="jq"> get_dummies() </em>函数:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="ce09" class="lt js hi lp b fi lu lv l lw lx"># creating dummies of target variable for train and validation set<br/>y_train = pd.get_dummies(y_train)<br/>y_test = pd.get_dummies(y_test)</span></pre><p id="7508" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步—定义我们的视频分类模型的架构。</p><h1 id="54a7" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">定义视频分类模型的架构</h1><p id="d659" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">由于我们没有非常大的数据集，从头开始创建模型可能效果不好。因此，我们将使用一个预先训练好的模型，并利用它的学习来解决我们的问题。</p><p id="47a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个特定的数据集，我们将使用VGG-16预训练模型。让我们创建一个预训练模型的基础模型:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="9e78" class="lt js hi lp b fi lu lv l lw lx"># creating the base model of pre-trained VGG16 model<br/>base_model = VGG16(weights='imagenet', include_top=False)</span></pre><p id="198b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型在具有1000个类的数据集上进行训练。我们将按照我们的要求微调这个模型。<em class="jq"> include_top = False </em>将移除该模型的最后一层，以便我们可以根据需要对其进行调整。</p><p id="1d8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将从这个预训练模型中提取特征，用于我们的训练和验证图像:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="c2ad" class="lt js hi lp b fi lu lv l lw lx"># extracting features for training frames<br/>X_train = base_model.predict(X_train)<br/>X_train.shape</span></pre><p id="5de9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输出:(59075，7，7，512) </strong></p><p id="97e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在训练集中有59，075个图像，形状已经更改为(7，7，512)，因为我们已经通过VGG16体系结构传递了这些图像。类似地，我们将提取验证框架的特征:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="1469" class="lt js hi lp b fi lu lv l lw lx"># extracting features for validation frames<br/>X_test = base_model.predict(X_test)<br/>X_test.shape</span></pre><p id="3df3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输出:(14769，7，7，512) </strong></p><p id="0105" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">验证集中有14，769幅图像，这些图像的形状也变成了(7，7，512)。我们现在将使用完全连接的网络来微调模型。这个完全连接的网络接受一维输入。因此，我们将重塑图像到一个单一的维度:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="85ea" class="lt js hi lp b fi lu lv l lw lx"># reshaping the training as well as validation frames in single dimension<br/>X_train = X_train.reshape(59075, 7*7*512)<br/>X_test = X_test.reshape(14769, 7*7*512)</span></pre><p id="b128" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">建议始终将像素值标准化，即保持像素值在0和1之间。这有助于模型更快地收敛。</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="670e" class="lt js hi lp b fi lu lv l lw lx"># normalizing the pixel values<br/>max = X_train.max()<br/>X_train = X_train/max<br/>X_test = X_test/max</span></pre><p id="dd03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将创建模型的架构。我们必须为此定义输入形状。所以，让我们检查一下图像的形状:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="f5e3" class="lt js hi lp b fi lu lv l lw lx"># shape of images<br/>X_train.shape</span></pre><p id="23ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输出:(59075，25088) </strong></p><p id="9fe4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入形状将是25，088。现在让我们创建架构:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="8b0f" class="lt js hi lp b fi lu lv l lw lx">#defining the model architecture<br/>model = Sequential()<br/>model.add(Dense(1024, activation='relu', input_shape=(25088,)))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(512, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(256, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(101, activation='softmax'))</span></pre><blockquote class="ku kv kw"><p id="723c" class="if ig jq ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated"><em class="hi">我们有多个完全连接的密集层。我也添加了脱落层，这样模型就不会过拟合。最后一层的神经元数量等于我们拥有的类的数量，因此这里的神经元数量是101。</em></p></blockquote><h1 id="06ee" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">训练视频分类模型</h1><p id="719e" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">现在，我们将使用训练框架来训练我们的模型，并使用验证框架来验证模型。我们将保存模型的权重，这样我们就不必一次又一次地重新训练模型。</p><p id="f37a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，让我们定义一个函数来保存模型的权重:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="758d" class="lt js hi lp b fi lu lv l lw lx"># defining a function to save the weights of best model<br/>from keras.callbacks import ModelCheckpoint<br/>mcp_save = ModelCheckpoint('weight.hdf5', save_best_only=True, monitor='val_loss', mode='min')</span></pre><p id="83ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将根据验证损失来决定最佳模型。请注意，权重将保存为<em class="jq"> weights.hdf5 </em>。如果愿意，您可以重命名该文件。在训练模型之前，我们必须编译它:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="175b" class="lt js hi lp b fi lu lv l lw lx"># compiling the model<br/>model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])</span></pre><p id="5a40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用<em class="jq">分类交叉熵</em>作为<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/" rel="noopener ugc nofollow" target="_blank">损失函数</a>，优化器是Adam。让我们训练模型:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="1583" class="lt js hi lp b fi lu lv l lw lx"># training the model<br/>model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=128)</span></pre><p id="5a47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经训练这个模型200个纪元了。要下载我训练模型后得到的权重，可以使用<a class="ae jd" href="https://drive.google.com/file/d/1vo2TBWFlA-yish_h8CgxIrSHnqCHxUUM/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jq">这个链接</em> </strong> </a>。</p><p id="84de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在有了权重，我们将用它来预测新的视频。因此，在下一节中，我们将看到这个模型在视频分类任务中的表现如何！</p><h1 id="e5d9" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">评估我们的视频分类模型</h1><p id="14b3" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">让我们打开一个新的Jupyter笔记本来评估模型。评估部分也可以分成多个步骤，以便更清楚地理解流程:</p><ol class=""><li id="1e72" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lf lg lh li bi translated">定义模型架构并加载权重</li><li id="2de5" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">创建测试数据</li><li id="6027" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">对测试视频进行预测</li><li id="fea6" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">最后，评估模型</li></ol><h1 id="dd22" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">定义模型架构和加载权重</h1><p id="b077" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">您将熟悉第一步—导入所需的库:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="72c3" class="lt js hi lp b fi lu lv l lw lx">from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Flatten<br/>from keras.layers import Conv2D, MaxPooling2D<br/>from keras.preprocessing import image<br/>import numpy as np<br/>import pandas as pd<br/>from tqdm import tqdm<br/>from keras.applications.vgg16 import VGG16<br/>import cv2<br/>import math<br/>import os<br/>from glob import glob<br/>from scipy import stats as s</span></pre><p id="3c8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将定义模型架构，该架构将类似于我们在训练模型时的架构:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="22ce" class="lt js hi lp b fi lu lv l lw lx">base_model = VGG16(weights='imagenet', include_top=False)</span></pre><p id="38ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是预训练模型，我们接下来将对其进行微调:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="4434" class="lt js hi lp b fi lu lv l lw lx">#defining the model architecture<br/>model = Sequential()<br/>model.add(Dense(1024, activation='relu', input_shape=(25088,)))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(512, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(256, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(101, activation='softmax'))</span></pre><p id="706f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，由于我们已经定义了架构，我们现在将加载我们存储为<em class="jq"> weights.hdf5 </em>的训练权重:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="f29a" class="lt js hi lp b fi lu lv l lw lx"># loading the trained weights<br/>model.load_weights("weights.hdf5")</span></pre><p id="ab65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">也编译模型:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="ee92" class="lt js hi lp b fi lu lv l lw lx"># compiling the model<br/>model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])</span></pre><p id="02b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">确保损失函数、优化器和指标与我们在训练模型时使用的相同。</p><h1 id="8da8" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">创建测试数据</h1><p id="9f65" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">您应该已经按照UCF101数据集的官方文档下载了训练/测试分割文件。如果没有，从这里下载<a class="ae jd" href="https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip" rel="noopener ugc nofollow" target="_blank">。在下载的文件夹中，有一个名为“<em class="jq"> testlist01.txt </em>的文件，其中包含了测试视频的列表。我们将利用它来创建测试数据:</a></p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="d10b" class="lt js hi lp b fi lu lv l lw lx"># getting the test list<br/>f = open("testlist01.txt", "r")<br/>temp = f.read()<br/>videos = temp.split('\n')</span><span id="6140" class="lt js hi lp b fi mb lv l lw lx"># creating the dataframe<br/>test = pd.DataFrame()<br/>test['video_name'] = videos<br/>test = test[:-1]<br/>test_videos = test['video_name']<br/>test.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mc"><img src="../Images/740b209e9fbc98750e1afcd1c47d4997.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/0*jqR-ISPS7Yev1X1c.png"/></div></div></figure><p id="90c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在有了存储在数据帧中的所有视频的列表。为了将预测类别映射到实际类别，我们将使用<em class="jq"> train_new.csv </em>文件:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="8093" class="lt js hi lp b fi lu lv l lw lx"># creating the tags<br/>train = pd.read_csv('UCF/train_new.csv')<br/>y = train['class']<br/>y = pd.get_dummies(y)</span></pre><p id="8e8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将对测试集中的视频进行预测。</p><h1 id="5cf8" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">为测试视频生成预测</h1><p id="d6de" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在查看代码之前，让我总结一下我们将在这一步做什么。以下步骤将帮助您理解预测部分:</p><ol class=""><li id="6c36" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lf lg lh li bi translated">首先，我们将创建两个空列表——一个存储预测，另一个存储实际的标签</li><li id="3e5d" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">然后，我们将从测试集中取出每个视频，提取该视频的帧并将其存储在一个文件夹中(在当前目录中创建一个名为<strong class="ih hj"> <em class="jq"> temp </em> </strong>的文件夹来存储帧)。我们将在每次迭代中从这个文件夹中移除所有其他文件</li><li id="ecea" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">接下来，我们将从<strong class="ih hj"> <em class="jq"> temp </em> </strong>文件夹中读取所有帧，使用预先训练的模型提取这些帧的特征，预测标签，然后采用该模式为该特定视频分配标签并将其附加到列表中</li><li id="d4b1" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">我们将在第二个列表中为每个视频添加实际的标签</li></ol><p id="4de8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们对这些步骤进行编码并生成预测:</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="5440" class="lt js hi lp b fi lu lv l lw lx"># creating two lists to store predicted and actual tags<br/>predict = []<br/>actual = []</span><span id="c6e0" class="lt js hi lp b fi mb lv l lw lx"># for loop to extract frames from each test video<br/>for i in tqdm(range(test_videos.shape[0])):<br/>    count = 0<br/>    videoFile = test_videos[i]<br/>    cap = cv2.VideoCapture('UCF/'+videoFile.split(' ')[0].split('/')[1])   # capturing the video from the given path<br/>    frameRate = cap.get(5) #frame rate<br/>    x=1<br/>    # removing all other files from the temp folder<br/>    files = glob('temp/*')<br/>    for f in files:<br/>        os.remove(f)<br/>    while(cap.isOpened()):<br/>        frameId = cap.get(1) #current frame number<br/>        ret, frame = cap.read()<br/>        if (ret != True):<br/>            break<br/>        if (frameId % math.floor(frameRate) == 0):<br/>            # storing the frames of this particular video in temp folder<br/>            filename ='temp/' + "_frame%d.jpg" % count;count+=1<br/>            cv2.imwrite(filename, frame)<br/>    cap.release()<br/>    <br/>    # reading all the frames from temp folder<br/>    images = glob("temp/*.jpg")<br/>    <br/>    prediction_images = []<br/>    for i in range(len(images)):<br/>        img = image.load_img(images[i], target_size=(224,224,3))<br/>        img = image.img_to_array(img)<br/>        img = img/255<br/>        prediction_images.append(img)<br/>        <br/>    # converting all the frames for a test video into numpy array<br/>    prediction_images = np.array(prediction_images)<br/>    # extracting features using pre-trained model<br/>    prediction_images = base_model.predict(prediction_images)<br/>    # converting features in one dimensional array<br/>    prediction_images = prediction_images.reshape(prediction_images.shape[0], 7*7*512)<br/>    # predicting tags for each array<br/>    prediction = model.predict_classes(prediction_images)<br/>    # appending the mode of predictions in predict list to assign the tag to the video<br/>    predict.append(y.columns.values[s.mode(prediction)[0][0]])<br/>    # appending the actual tag of the video<br/>    actual.append(videoFile.split('/')[1].split('_')[1])</span></pre><p id="32a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">这一步需要一些时间，因为测试集中大约有3，800个视频。</strong>一旦我们有了预测，我们将计算模型的性能。</p><h1 id="30e1" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">评估模型</h1><p id="f724" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">是时候评估我们的模型了，看看到底是怎么回事。</p><p id="9627" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有实际的标签，也有模型预测的标签。我们将利用这些来获得准确度分数。<em class="jq">在UCF101的官方文档页面上，目前准确率为43.90%。</em>我们的模型能打败它吗？让我们检查一下！</p><pre class="jf jg jh ji fd lo lp lq lr aw ls bi"><span id="7c5f" class="lt js hi lp b fi lu lv l lw lx"># checking the accuracy of the predicted tags<br/>from sklearn.metrics import accuracy_score<br/>accuracy_score(predict, actual)*100</span></pre><p id="19a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">产量:44.8416337</strong></p><p id="568b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">太好了！我们的模型的44.8%的准确性与官方文档所陈述的(43.9%)相当。</p><p id="1f9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可能想知道为什么我们对低于50%的准确率感到满意。这种低准确性背后的原因主要是由于缺乏数据。我们只有大约13，000个视频，即使这些视频的持续时间也很短。</p><h1 id="1992" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">结束注释</h1><p id="fb28" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在本文中，我们讨论了计算机视觉最有趣的应用之一——视频分类。我们首先了解如何处理视频，然后我们提取帧，训练视频分类模型，最终在测试视频上获得了44.8%的可比准确率。</p><p id="cf65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在可以尝试不同的方法，旨在提高模型的性能。我能想到的一些方法是使用3D卷积，它可以直接处理视频。</p><p id="cc8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于视频是一系列的帧，我们也可以把它作为一个序列问题来解决。因此，可能有更多的解决方案，我建议你去探索它们。请随时与社区分享您的发现。</p><p id="7e98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一如既往，如果你有任何与本文相关的建议或疑问，请在下面的评论区发表，我很乐意回答。正如我之前提到的，如果你是这个领域的新手，一定要去看看<a class="ae jd" href="https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning-version2/?utm_source=blog&amp;utm_medium=step-by-step-deep-learning-tutorial-video-classification-python" rel="noopener ugc nofollow" target="_blank">的计算机视觉课程</a>。</p></div><div class="ab cl md me gp mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="hb hc hd he hf"><p id="1484" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jq">原载于2019年9月3日</em><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/09/step-by-step-deep-learning-tutorial-video-classification-python/" rel="noopener ugc nofollow" target="_blank"><em class="jq">https://www.analyticsvidhya.com</em></a><em class="jq">。</em></p></div></div>    
</body>
</html>