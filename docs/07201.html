<html>
<head>
<title>Random Forest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/random-forest-e205561c3489?source=collection_archive---------31-----------------------#2020-06-16">https://medium.com/analytics-vidhya/random-forest-e205561c3489?source=collection_archive---------31-----------------------#2020-06-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/ba499c46d133335f2cd517ab53066a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*Cu3z2HrWf2AhnHMm3Kv-Xg.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://www.frontiersin.org/files/MyHome%20Article%20Library/284242/284242_Thumb_400.jpg" rel="noopener ugc nofollow" target="_blank">随机森林</a></figcaption></figure><blockquote class="ir is it"><p id="6543" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随机森林是一种监督学习算法。就像你已经从它的名字中看到的，它创造了一个森林，使它变得随机。它建造的森林，是<a class="ae iq" rel="noopener" href="/swlh/decision-tree-regression-c977b732eb51">决策树</a>的<strong class="ix hj">合奏</strong>，大部分时间用<strong class="ix hj">套袋法</strong>训练。</p></blockquote><h2 id="27a0" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">集成学习</h2><p id="02e7" class="pw-post-body-paragraph iu iv hi ix b iy kr ja jb jc ks je jf ke kt ji jj ki ku jm jn km kv jq jr js hb bi translated">在<strong class="ix hj">集成学习</strong>中，我们多次采用多个算法或同一个算法，我们将它们放在一起或合并在一起，以制作比原始算法更强大的东西。</p><h2 id="b5b3" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">装袋技术</h2><p id="0d81" class="pw-post-body-paragraph iu iv hi ix b iy kr ja jb jc ks je jf ke kt ji jj ki ku jm jn km kv jq jr js hb bi translated"><strong class="ix hj">引导</strong>数据并使用<strong class="ix hj">集合</strong>做出决定被称为<strong class="ix hj">打包</strong>。它提高了机器学习算法的稳定性和准确性。bagging方法的一般思想是学习模型的组合增加了整体结果。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/c80f392e368578eb15929581d6723390.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*mogiJKKDZEezrpEUXpJQDg.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://www.kdnuggets.com/wp-content/uploads/Budzik-fig1-ensemble-learning.jpg" rel="noopener ugc nofollow" target="_blank">装袋技术</a></figcaption></figure><p id="bd76" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf ke jh ji jj ki jl jm jn km jp jq jr js hb bi translated">随机森林是一种<strong class="ix hj">集成学习</strong>技术。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/2095fed3c8ecb21178af839ea54d74ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*8lI2kFCsMF8AzHT0OQnG1g.png"/></div></figure><blockquote class="lc"><p id="9e98" class="ld le hi bd lf lg lh li lj lk ll js dx translated">随机森林建立多个决策树，并将它们合并在一起，以获得更准确和稳定的预测。</p></blockquote><p id="6c2a" class="pw-post-body-paragraph iu iv hi ix b iy lm ja jb jc ln je jf ke lo ji jj ki lp jm jn km lq jq jr js hb bi translated">让我们看看随机森林是如何工作的:</p><p id="714d" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf ke jh ji jj ki jl jm jn km jp jq jr js hb bi translated"><strong class="ix hj">步骤1: </strong>从训练集中随机选取K个数据点或样本，并创建一个“<strong class="ix hj">自举</strong>数据集。为了创建与原始数据集大小相同的引导数据集，我们只需从原始数据集中随机选择样本，我们还可以多次选择相同的数据或样本。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lr"><img src="../Images/5e7fa4416d27f6c85acf224220a95afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IGdxYTQNxqcmGinUJZK9Ww.jpeg"/></div></div></figure><p id="5009" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf ke jh ji jj ki jl jm jn km jp jq jr js hb bi translated"><strong class="ix hj">步骤2 </strong>:建立与这K个数据点或样本相关的决策树。现在，我们使用自举数据集创建一个决策树，但在每一步只考虑变量(或列)的随机子集。</p><p id="671b" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf ke jh ji jj ki jl jm jn km jp jq jr js hb bi translated"><strong class="ix hj">第三步</strong>:选择你想要建造的N棵树的数量，然后重复第一步&amp;第二步，你会得到各种各样的树。树的多样性使得随机森林比个体决策树更有效。</p><p id="1356" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf ke jh ji jj ki jl jm jn km jp jq jr js hb bi translated"><strong class="ix hj">步骤4 </strong>:对于一个新的数据点，让N棵树中的每一棵树预测该数据点的Y值。将数据运行到随机森林中的所有树后，我们将判断哪个选项获得了更多投票，该选项是最终输出，或者可以说是y的值。</p><h1 id="80e3" class="lw ju hi bd jv lx ly lz jz ma mb mc kd md me mf kh mg mh mi kl mj mk ml kp mm bi translated">为什么使用随机森林算法？</h1><blockquote class="lc"><p id="8a50" class="ld le hi bd lf lg mn mo mp mq mr js dx translated">随机森林给出高水平的精确和更稳定的预测。它还降低了过度拟合的风险，并可以有效地运行大型数据库，以产生高度准确的预测。</p></blockquote><p id="d090" class="pw-post-body-paragraph iu iv hi ix b iy lm ja jb jc ln je jf ke lo ji jj ki lp jm jn km lq jq jr js hb bi translated">随机森林算法到此为止。敬请关注更多博客。</p><p id="0acd" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf ke jh ji jj ki jl jm jn km jp jq jr js hb bi translated"><em class="iw">谢谢</em></p></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><p id="1280" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf ke jh ji jj ki jl jm jn km jp jq jr js hb bi translated">海量数据上随机森林的实现</p><blockquote class="ir is it"><p id="edec" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据集:<a class="ae iq" href="https://github.com/InternityFoundation/MachineLearning_Navu4/blob/master/15th%20June%20:%20Implementation%20of%20Decision%20Tree%20and%20Random%20Forest/titanic.csv" rel="noopener ugc nofollow" target="_blank">泰坦尼克号</a>数据集</p></blockquote><p id="83a6" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf ke jh ji jj ki jl jm jn km jp jq jr js hb bi translated">链接:<a class="ae iq" href="https://github.com/InternityFoundation/MachineLearning_Navu4/blob/master/15th%20June%20:%20Implementation%20of%20Decision%20Tree%20and%20Random%20Forest/Random_forest_on_titanic_dataset.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/internity foundation/machine learning _ navu 4/blob/master/15日%20 June % 20:% 20实现% 20 of % 20决策% 20树% 20和% 20 Random % 20 forest/Random _ forest _ on _ titanic _ dataset . ipynb</a></p></div></div>    
</body>
</html>