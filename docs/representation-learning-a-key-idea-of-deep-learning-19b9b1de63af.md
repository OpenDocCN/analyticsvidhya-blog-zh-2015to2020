# 表征学习:深度学习的核心理念

> 原文：<https://medium.com/analytics-vidhya/representation-learning-a-key-idea-of-deep-learning-19b9b1de63af?source=collection_archive---------9----------------------->

![](img/e8edbc24321e971f820c94bab5b8b6a8.png)

有用的表示已经在我们的日常生活和计算机系统中使用了很长时间。表征决定了许多信息处理任务的表现。正确的表达使问题易于理解、形象化并在更少的时间、更少的努力和更少的空间内解决问题。例如，从存储在数组中的数字中访问一个数字需要 O(1)时间，从存储在堆栈中的数字中访问一个数字需要 O(n)时间。在机器学习和深度学习中，有用的表示使学习任务变得容易。有用表示的选择主要取决于手头的问题，即学习任务。

在深度学习中，当以监督方式训练时，前馈神经网络可以被视为执行表示学习。输入层与所有隐藏层的结合应该将输入转换成有用的表示。这种表示充当最后一层(通常是分类层)的输入。该分类器通常是 softmax 回归分类器、线性回归或逻辑回归分类器，其利用从早期层接收的表示来成功解决目标问题。例如，在模型被训练来检测图像中的猫的情况下，对最后一层的表示是这样的，使得它可以分离包含猫和非猫图像的图像。

一个好的表现形式可以使后续的学习变得更容易。表征的选择通常取决于后续学习任务的选择。好的表示的目标是尽可能多地保存输入的信息，同时获得良好的属性，如独立性。表征学习的核心思想是，相同的表征可能在类似任务的所有环境中都有用。目的是利用第一项任务的数据提取有用的学习信息，或者有时直接预测第二项任务。

在这篇文章中，我讨论了表征学习的两个主要观点，迁移学习和领域适应。这两种想法都被证明在实践中非常有效，并开辟了扩展深度学习模型能力的可能性。

# 迁移学习

![](img/6faf2051dd3e186500d7899eb3277efb.png)

深度神经网络模型的中间层给出了有用的表示。

在迁移学习中，学习者必须完成两个或更多不同的任务，但是我们假设解释 P1 变化的许多因素与学习 P2 需要捕捉的变化相关。例如，考虑跟随监督学习任务

P1:猫狗图像的分类

P2:狮子和大象图像的分类

如果我们有大量的数据集来为 P1 训练，那么这可能有助于学习一些表示法，这些表示法有助于从为 P2 绘制的很少几个例子中快速概括。许多这样的视觉任务涉及共同的基本构造，如识别边缘、形状、对比度变化等。这些基本结构可以在一个任务的训练中学会，比如我们例子中的 P1，同样的识别这些共同特征的内部表征可以用在类似的任务中，比如我们例子中的 P2。这种能力使得转移已经获得的知识来解决未来类似的任务成为可能。

我对猫和狗的图像进行了图像分类实验。我使用的数据集有 8000 个训练图像和 2000 个测试图像。我使用一个已经训练好的 VGG16 模型来获得这些图像的有用表示。然后，输出表示被馈送到简单的神经网络，以正确地对图像进行分类。要了解更多细节，请阅读我之前关于 DNN、CNN 和迁移学习方法比较的文章:

[https://medium . com/analytics-vid hya/image-class ification-a-comparison-of-dnn-CNN-and-transfer-learning-approach-704535 beca 25](/analytics-vidhya/image-classification-a-comparison-of-dnn-cnn-and-transfer-learning-approach-704535beca25)

VGG16 模型的输出如下图所示:

![](img/25142c8bf6fceb61da1a6a01d7d137dd.png)

猫和狗的训练数据的代表性分布

![](img/3f6897857746913ab8cb94530efb2dfa.png)

猫和狗试验数据的代表性分布

从上面的图表中可以清楚地看到，我们得到的数据可以很容易地分为两类，即。猫和狗。我可以训练一个简单的神经网络来作为这种表示的分类器，这给了我大约 92%的准确率，而且花费的精力和时间要少得多。

还有一个类似于迁移学习的研究领域，叫做“概念漂移”。在概念漂移中，输入数据会随时间发生轻微变化。

# 领域适应

领域适应适用于目标相同但输入分布可能略有不同的任务。例如，在情感分析的情况下，目标可能是将评论标记为正面或负面。现在考虑以下任务:

P1:电影评论的情感分析

P2:对手机等电子产品用户评论的情感分析

涉及 P1 和 P2 的领域适应场景是，根据 P1 的数据训练的模型然后被应用于分析 P2 的用户评论。由于此类评论中词汇和句子结构的差异，使用 P1 的训练有素的代表可能很难很好地概括 P2。我们可能需要使用一些特殊的技术，如去噪，以增强领域适应性。

作为一个实验，我安装了 Flair(【https://github.com/flairNLP/flair】)并使用了它已经训练好的情绪分析模型。

```
# Sentiment analysis experiment using Flairimport flair
pos_sentence = “I am positive about it”
neg_sentence = “This is negative”
flair_sentiment = flair.models.TextClassifier.load(‘en-sentiment’)sent_pos = flair.data.Sentence(pos_sentence)
sent_neg = flair.data.Sentence(neg_sentence)print(sent_pos,sent_neg)flair_sentiment.predict(sent_pos)
flair_sentiment.predict(sent_neg)pos_sentiment = sent_pos.labels
pos_sentimentneg_sentiment = sent_neg.labels
print(pos_sentiment,neg_sentiment)
```

*输出:*

*加载文件/。flair/models/imdb-v0.4.pt
句子:“我对它持肯定态度”——5 个令牌句子:“这是否定的”——3 个令牌
【肯定(0.9862375259399414)】【肯定(0.8970600962638855)】*

所使用的模型是在 imdb 电影评论数据上训练的。显然，模型预测不是那么准确，需要一些调整或重新训练，以使其适用于目标领域。

像这样的表征学习思想构成了多任务学习的一部分，当直接应用或通过提供有用的中间表征来解决新问题时，为解决一个问题而训练的模型可以用于解决另一个问题。在这个领域有很多工作正在进行，它是创建一个通用模型来解决众多任务的重要工具之一，就像我们“人类”一样。