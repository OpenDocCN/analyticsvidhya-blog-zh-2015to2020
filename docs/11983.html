<html>
<head>
<title>YOLO Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">YOLO解释道</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/yolo-explained-5b6f4564f31?source=collection_archive---------0-----------------------#2020-12-27">https://medium.com/analytics-vidhya/yolo-explained-5b6f4564f31?source=collection_archive---------0-----------------------#2020-12-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/7e9d6e7f1007cbe745fc9667ac293fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xfXdebLeaMXt3Vct"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" rel="noopener" href="/analytics-vidhya/yolo-object-detection-made-easy-7b17cc3e782f">信用</a></figcaption></figure></div><div class="ab cl ir is gp it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hb hc hd he hf"><h1 id="a03d" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">什么是YOLO？</h1><p id="9de2" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">YOLO或你只看一次，是一个流行的实时对象检测算法。YOLO结合了曾经的多步骤过程，使用单个神经网络来执行检测对象的分类和边界框预测。因此，它针对检测性能进行了大量优化，并且可以比运行两个独立的神经网络来分别检测和分类对象快得多。它通过将传统的图像分类器重新用于识别对象的边界框的回归任务来做到这一点。本文将只关注YOLOv1，这是该架构经历的多次迭代中的第一次。尽管后续的迭代有许多改进，但架构背后的基本思想保持不变。YOLOv1简称为YOLO，可以以每秒45帧的速度执行比实时对象检测更快的速度，这使它成为需要实时检测的应用程序的绝佳选择。它一次查看整个图像，并且只查看一次——因此得名“只查看一次”——这使它能够捕捉到检测到的对象的上下文。这使得R-CNN在分别观察图像的不同部分时产生的假阳性检测数量减半。此外，YOLO可以概括各种对象的表示，使其更适用于各种新环境。现在我们已经对YOLO有了一个总体的了解，让我们来看看它到底是如何工作的。</p></div><div class="ab cl ir is gp it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hb hc hd he hf"><h1 id="148d" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">YOLO是如何工作的？</h1><p id="d091" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">YOLO基于将图像分割成更小图像的想法。图像被分割成尺寸为S×S的正方形网格，如下所示:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ku"><img src="../Images/dd4e9f1f4290870e51367a6fa99805d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/0*hYdtgqequinVZd6y"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">致谢:<a class="ae iq" href="https://pjreddie.com/publications/" rel="noopener ugc nofollow" target="_blank">研究论文</a>。</figcaption></figure><p id="8a4e" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">物体的中心，例如狗的中心所在的细胞，是负责检测该物体的细胞。每个单元将预测最佳边界框和每个框的置信度得分。这种架构的默认设置是模型预测两个边界框。分类分数从“0.0”到“1.0”，其中“0.0”为最低置信度，“1.0”为最高置信度；如果在该单元中不存在对象，置信度得分应该是“0.0”，并且如果模型完全确定其预测，得分应该是“1.0”。这些置信水平捕捉模型确定在该单元中存在对象并且边界框是准确的。每个边界框都由5个数字组成:x位置、y位置、宽度、高度和置信度。坐标“( x，y)”表示预测边界框的中心位置，宽度和高度是相对于整个图像尺寸的分数。置信度表示预测边界框和实际边界框之间的IOU，实际边界框被称为基本真实框。IOU代表交集超过并集，是预测和基础真值框的交集面积除以相同预测和基础真值框的并集面积。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ku"><img src="../Images/cfcacaaffb7b3c7c3bead4098898eb4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/0*aIHq0P9eySmRPVxb"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">下面是一个借据的例子:绿色的实际和预测框的相交面积除以紫色的两个框的结合面积。这将介于0和1之间，如果它们完全不重叠，则为0，如果它们是同一个盒子，则为1。因此，IOU越高越好，因为这是一个更准确的预测。鸣谢:图片来自<a class="ae iq" href="https://pjreddie.com/publications/" rel="noopener ugc nofollow" target="_blank">研究论文</a>本人修改。</figcaption></figure><p id="e316" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">除了输出边界框和置信度得分，每个单元还预测对象的类别。这种类预测由独热向量长度C表示，即数据集中的类的数量。然而，重要的是要注意，虽然每个单元可以预测任意数量的边界框和这些框的置信度得分，但是它只预测一个类。这是YOLO算法本身的局限性，如果在一个网格单元中有多个不同类别的对象，该算法将不能正确地对两者进行分类。因此，来自网格单元的每个预测将具有形状<strong class="jy hj"> <em class="li"> C + B * 5 </em> </strong>，其中<strong class="jy hj"> <em class="li"> C </em> </strong>是类的数量，而<strong class="jy hj"> <em class="li"> B </em> </strong>是预测的边界框的数量。<strong class="jy hj"> <em class="li"> B </em> </strong>这里乘以5是因为它包含了每个盒子的<strong class="jy hj"> <em class="li"> (x，y，w，h，置信度)</em> </strong>。因为每幅图像中都有<strong class="jy hj"> <em class="li"> S × S </em> </strong>网格单元，所以模型的整体预测是一个形状为<strong class="jy hj"><em class="li">S×S×(C+B</em>∫5<em class="li">)</em></strong>的张量。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ku"><img src="../Images/77f2fd3c044b3db65581791bcebd139c.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/0*1kbmx7BmEUfCM0m-"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">鸣谢:图片来自本人修改的<a class="ae iq" href="https://pjreddie.com/publications/" rel="noopener ugc nofollow" target="_blank">研究论文</a>。</figcaption></figure><p id="aef7" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">以下是仅预测每个像元的单个边界框时模型输出的示例。在这个图像中，狗的真实中心由标有“物体中心”的青色圆圈表示；因此，负责检测和包围盒子的网格单元是包含青色点的单元，以深蓝色突出显示。单元格预测的边界框由4个元素组成。红点代表边界框的中心，<strong class="jy hj">、<em class="li"> (x，y) </em>、</strong>，宽度和高度分别用橙色和黄色标记表示。值得注意的是，模型预测的是边界框的宽度和高度，而不是左上角和右下角的位置。分类由一个hot表示，在这个简单的例子中，有7个不同的类。第五类是预测，我们可以看到该模型对其预测相当确定。请记住，这只是一个示例，展示了可能的输出类型，因此这些值可能不会精确到任何实际值。下面是所有边界框和类预测的另一个图像，它们实际上会被制作出来，并最终得到结果。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lj"><img src="../Images/3bf233cae41da20a2d7c1847c436ffb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ejRHYHPQ9FNMzG1z"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">鸣谢:图片来自<a class="ae iq" href="https://pjreddie.com/publications/" rel="noopener ugc nofollow" target="_blank">研究论文</a>。</figcaption></figure></div><div class="ab cl ir is gp it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hb hc hd he hf"><h1 id="5976" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">YOLO建筑</h1><p id="d160" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">YOLO模型由三个关键部分组成:头部、颈部和脊柱。主干是网络的一部分，由卷积层组成，用于检测图像的关键特征并进行处理。主干首先在分类数据集(如ImageNet)上训练，并且通常以比最终检测模型更低的分辨率训练，因为检测需要比分类更精细的细节。颈部使用主干中具有完全连接层的卷积层的特征来预测概率和边界框坐标。头部是网络的最终输出层，它可以与具有相同输入形状的其他层互换，用于迁移学习。如前所述，头部是一个<strong class="jy hj">×S×(C+B∫5<em class="li">)</em></strong>张量，是原YOLO研究论文中的<strong class="jy hj">×7×30</strong>，分裂大小为7的<strong class="jy hj"> <em class="li"> S </em> </strong>，20个类<strong class="jy hj"> <em class="li"> C </em> </strong>，2个预测边界框<strong class="jy hj"> <em class="li"> B </em> </strong>。模型的这三个部分协同工作，首先从图像中提取关键视觉特征，然后对它们进行分类和绑定。</p></div><div class="ab cl ir is gp it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hb hc hd he hf"><h1 id="1a07" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">YOLO培训</h1><p id="e3d4" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">如前所述，模型的主干是在影像分类数据集上预先训练的。原始论文使用ImageNet 1000级竞争数据集，并预先训练24个卷积层中的20个，然后是平均池和完全连接层。然后，他们在模型中添加了4个卷积和2个完全连接的层，因为已经证明添加抽搐和完全连接的层可以提高性能。他们还将分辨率从<strong class="jy hj"> 244 <em class="li"> × </em> 244 </strong>提高到<strong class="jy hj"> <em class="li"> 448 × 448 </em> </strong>像素，因为检测需要更精细的细节。预测类别概率和边界框坐标的最后一层使用线性激活函数，而其他层使用泄漏ReLU函数。原始论文使用64的批量大小在Pascal VOC 2007和2012数据集上训练了135个时期。数据增加和丢失用于防止过拟合，丢失层的比率为0.5，用于第一个和第二个完全连接的层之间，以鼓励它们学习不同的东西(防止共同适应)。在<a class="ae iq" href="https://pjreddie.com/publications/" rel="noopener ugc nofollow" target="_blank">原始文件</a>中有更多关于学习率计划和其他训练超参数的详细信息。</p><p id="39b6" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">损失函数是简单的平方和，但必须进行修改。在没有修改的情况下，该模型将对定位误差、预测的和真实的边界框坐标之间的差异以及类预测误差进行加权。此外，当网格单元不包含对象时，其置信度得分趋向于0，这可以压倒来自包含对象的其他单元的渐变。这两个问题都是通过使用两个系数<strong class="jy hj"> <em class="li"> λcoord </em> </strong>和<strong class="jy hj"> <em class="li"> λnoobj </em> </strong>来解决的，这两个系数分别乘以坐标的损失和物体的损失。这些设置为<strong class="jy hj"> <em class="li"> λcoord = 5 </em> </strong>和<strong class="jy hj"> <em class="li"> λnoobj = 0.5 </em> </strong>，增加检测的权重，降低无物体丢失的重要性。最后，为了使小包围盒的权重与大包围盒的权重相等，宽度和高度差是平方根，而不是直接使用。这确保了错误与大和小盒子中的错误一样被处理，否则会阻碍模型预测大盒子。例如，如果边界框的预测宽度是10，而实际宽度是8，我们使用这个等式</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lk"><img src="../Images/85c5a3d2c7869dfd722c4f0a43ad3538.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/0*UzQYPo_mIrVY-A1_"/></div></div></figure><p id="18bc" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">我们发现损失为4。当我们放大到预测的宽度100和实际的98时，损失又是4。然而，与8分之2的差异相比，真实98分之2的差异可以忽略不计。所以10和8 <em class="li">之间的损耗应该</em>远大于100和98之间的损耗。所以我们用这个等式来代替:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/eb6024ebb644291a460b18323c4253e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/0*1ErIfIb-r2oYFAL5"/></div></figure><p id="629e" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">使用这个新公式，10和8的损耗是0.111，而100和98的损耗是0.010。请记住，将亏损本身视为一个数字是没有意义的，但价值之间的差异是有意义的。因此，0.111远小于4这一事实并不重要，但重要的是，大宽度和小宽度的损耗之差对于平方差为0%，而对于平方根差则为90.99%。这个例子说明了为什么平方根是重要的:我们想要同样对待大的和小的边界框。</p><p id="214d" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">每个网格单元预测多个边界框，但只有一个边界框负责检测对象。通过选择具有最高IOU的预测边界框来确定负责的边界框。其效果是，某些边界框将在预测边界框的某些形状和大小方面有所改进，而其他边界框将专注于其他形状。这是由于以下原因而发生的:如果在多个边界框预测边界时存在大的对象，则最佳的一个被奖励，并继续改进大框的预测。当一个小对象出现时，先前的预测器无法预测到一个好的拟合，因为它的边界框太大了。然而，另一个预测器具有更好的预测，并且它因很好地包围小对象而得到奖励。随着训练的进行，各种边界框的预测会发散，以专注于它们在训练早期擅长的任务。</p><p id="5687" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">让我们来看看损失函数:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lm"><img src="../Images/741b81ca32b9a9dfefe9ad81efab0f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0Z70hVRgYGAtn1LG"/></div></div></figure><p id="21fb" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">让我们来分解一下数学。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/86d8d8f704cdabdc65addb845f9fbd0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/0*LNA8nrHPn6ek-vAB"/></div></figure><p id="7466" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">双重求和仅仅意味着对所有网格单元(an <strong class="jy hj"> <em class="li"> S × S </em> </strong>正方形)和所有边界框<strong class="jy hj"> <em class="li"> B </em> </strong>求和。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/02a1f918a094e7c8d580325d59ae265e.png" data-original-src="https://miro.medium.com/v2/resize:fit:106/0*LP_dJ1KOXqjWH-qc"/></div></figure><p id="b8b1" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">这是一个标识函数，如果单元格<strong class="jy hj"> <em class="li"> i、</em> </strong> <em class="li">和</em>边界框<strong class="jy hj"> <em class="li"> j </em> </strong>中有一个对象，则设置为1，负责预测(如果它有最高的IOU，则负责预测，前面讨论过)。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lk"><img src="../Images/a08000a19453d5d1f320d7d52eb060e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/0*thncQ4CXPdi5DSQV"/></div></div></figure><p id="bb53" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">这表示实际的x坐标和单元格<strong class="jy hj"> <em class="li"> i </em> </strong>中的预测坐标之间的平方差。</p><p id="ab50" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">对x和y坐标重复这一过程，找到总中点之间的平方差。最后，当没有对象或当前边界框不是负责的对象时，标识函数为‘0’。换句话说，我们只计算最佳包围盒的损失。因此，等式的第一行是所有网格单元中对象的预测中点和实际中点之间的平方差之和，这些网格单元中有一个对象，并且是负责的边界框。</p><p id="5600" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">第二行是所有包含对象的网格单元中预测和实际宽度和高度的平方根之差的平方和。由于前面解释的原因，这些是平方根。</p><p id="2527" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">第三行是包含一个对象的所有单元格中的预测类概率和实际类概率之间的平方差。</p><p id="3973" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">第四行是相同的，但是是针对所有没有对象的单元格。这两条线在所有边界框中相加，因为每个边界框除了坐标之外还预测置信度得分。将这两条分开的原因是，我们可以将第四条线乘以noobj系数，以便在没有物体存在的情况下，如果模型分类错误，则惩罚不那么严厉。不过有一个奇怪的地方，在第4行中我们没有看到对象的标识函数，但是不清楚哪个边界框负责使标识函数为1。研究论文称，负责的盒子是IOU最高的那个，但是如果没有对象，就没有地面真实盒子，因此也就没有IOU值。人们可以训练所有的边界框，或者只训练最差的边界框，或者任何其他组合，但原始论文没有说明它们训练了哪一个。</p><p id="2a3b" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">最后一行有点棘手:第一次求和会遍历每个包含对象的网格单元。然后，对于该单个网格单元，找到预测的类别向量和实际向量之间的平方差。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lp"><img src="../Images/d39fd91c4bfc31e820b541565d67f433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ItTV4z-NK97XJdR69iVvuQ@2x.png"/></div></div></figure><p id="bf4e" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">例如，如果这个YOLO模型在5个类上训练，它将预测每个单元中类似于<strong class="jy hj"><em class="li">【pred】</em></strong>的向量。如果地面真值是<strong class="jy hj"> <em class="li">真值</em> </strong>向量，则该单元的损失将如图所示。</p><p id="7e37" class="pw-post-body-paragraph jw jx hi jy b jz ld kb kc kd le kf kg kh lf kj kk kl lg kn ko kp lh kr ks kt hb bi translated">最后，最后一行是所有包含对象的像元的预测类和实际类之间的平方差，这基本上只是检查分类有多远。这仅跨格网像元进行计算，而不是跨每个边界框进行计算，因为每个像元仅预测一个分类，而不管它还预测了多少个边界框。最后，将所有这些相加在一起，前两行乘以一个坐标系数以加重它们的权重，第四行乘以一个较小的无对象系数以减轻其权重。</p></div><div class="ab cl ir is gp it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hb hc hd he hf"><h1 id="5be9" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">YOLO的局限性</h1><p id="fb35" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">在最初的研究论文中，YOLO只能预测每个网格单元的有限数量的边界框，2个。虽然该数量可以增加，但是每个单元只能进行一个类别预测，这限制了当多个对象出现在单个网格单元中时的检测。因此，它与小对象的边界组进行斗争，例如一群鸟，或者不同类别的多个小对象。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/cfd53d0071bd913bb51692c933b20bb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*0OvMff3WSPbhffj1"/></div><figcaption class="im in et er es io ip bd b be z dx translated">这里你可以看到左下角只有5个人被YOLO检测到，而左下角至少有8个人。鸣谢:图片<a class="ae iq" rel="noopener" href="/@venkatakrishna.jonnalagadda/object-detection-yolo-v1-v2-v3-c3d5eca2312a">来源</a></figcaption></figure><h1 id="468f" class="iy iz hi bd ja jb lr jd je jf ls jh ji jj lt jl jm jn lu jp jq jr lv jt ju jv bi translated">结论</h1><p id="0ce2" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">YOLO是一个令人难以置信的用于物体检测和分类的计算机视觉模型。希望这篇文章能帮助你理解YOLO是如何在高层次上工作的。如果您想了解Python实现的本质细节，请耐心等待:稍后我将发布一篇关于YOLO的PyTorch实现的后续博客，跟随代码将是真正测试您理解的好方法。YOLO只是一个更大项目的第一步，这个项目是一个循环的YOLO模型，将进一步改善跨多帧的对象检测和跟踪，被称为ROLO。给我一个后续看到的实施，这将使用循环网络结合YOLO。感谢阅读，编码快乐！</p></div><div class="ab cl ir is gp it" role="separator"><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw ix"/><span class="iu bw bk iv iw"/></div><div class="hb hc hd he hf"><h1 id="93b6" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">链接</h1><ul class=""><li id="697d" class="lw lx hi jy b jz ka kd ke kh ly kl lz kp ma kt mb mc md me bi translated">原始的研究论文可以在这里找到</li><li id="e3d2" class="lw lx hi jy b jz mf kd mg kh mh kl mi kp mj kt mb mc md me bi translated">更多关于研究论文和他们其他出版物的信息可以在他们的网站<a class="ae iq" href="https://pjreddie.com/publications/" rel="noopener ugc nofollow" target="_blank">这里</a></li><li id="f5f0" class="lw lx hi jy b jz mf kd mg kh mh kl mi kp mj kt mb mc md me bi translated">所有其他来源在使用时都是链接的。</li></ul></div></div>    
</body>
</html>