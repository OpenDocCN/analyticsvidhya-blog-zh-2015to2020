<html>
<head>
<title>Journey of Apache Kafka &amp; Zookeeper Administrator ( Part 14 )</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阿帕奇·卡夫卡与动物园管理员之旅(14)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/journey-of-apache-kafka-zookeeper-administrator-part-14-e4648ad33232?source=collection_archive---------21-----------------------#2020-09-10">https://medium.com/analytics-vidhya/journey-of-apache-kafka-zookeeper-administrator-part-14-e4648ad33232?source=collection_archive---------21-----------------------#2020-09-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="57d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2020年7月(卡夫卡消费群监测)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/b73d9bfcddcacf2cd39ba0b0d75bbb67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sY_Y1XKQ5AoEXxFPIxx_zA.png"/></div></div></figure><p id="e70f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jp translated">消费者群体监控非常重要，因为它提供了关于消费者应用程序的统计数据，以及应用程序与实际数据流的差距。</p><p id="35f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第一:</strong> <a class="ae jy" href="https://github.com/yahoo/CMAK" rel="noopener ugc nofollow" target="_blank">雅虎卡夫卡经理</a>又名<a class="ae jy" href="https://github.com/yahoo/CMAK" rel="noopener ugc nofollow" target="_blank"> CMAK </a></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jz"><img src="../Images/2b5e1f2265600312c336dd74e28205cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HHBFu2VClwEBXzTCKWusvQ.png"/></div></div></figure><blockquote class="ka kb kc"><p id="7b5e" class="if ig kd ih b ii ij ik il im in io ip ke ir is it kf iv iw ix kg iz ja jb jc hb bi translated">CMAK有一个小问题，它只会显示最新的统计数据，你永远无法比较历史结果。如果你只想知道关于消费者的最新统计数据，那么这是Apache Kafka消费者群体监控的最佳工具之一。</p></blockquote><p id="cba5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第二:</strong>自定义监控脚本(基于Python 3)</p><p id="39ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有了这个工具，我的监控架构变得非常一致，<strong class="ih hj"> Python脚本用于指标提取+ Splunk用于索引&amp;仪表盘</strong>。</p><p id="e85b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要一个名为<a class="ae jy" href="https://pypi.org/project/kafka-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> kafka-python </strong> </a>的python包，因为当我们使用上面的库创建消费者时，它也允许消费者获取其统计数据，但目前<a class="ae jy" href="https://github.com/confluentinc/confluent-kafka-python" rel="noopener ugc nofollow" target="_blank"> Confluent-Kafka-python </a>包不支持同样的东西。</p><h1 id="f191" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">现在让我们检查代码。</h1><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="2ac6" class="lk ki hi lg b fi ll lm l ln lo">from kafka import KafkaConsumer, TopicPartition, KafkaAdminClient<br/>import json<br/>import sys<br/>from datetime import datetime<br/>import threading</span><span id="a2e1" class="lk ki hi lg b fi lp lm l ln lo">class KCMetric:<br/>    def __init__(self,topic,group_id,logDir,env):</span><span id="1458" class="lk ki hi lg b fi lp lm l ln lo">self.TOPIC = topic<br/>        self.GROUP = group_id<br/>        self.BOOTSTRAP_SERVERS = ['localhost']<br/>        self.ENV = env<br/>        self.LOGDIR = logDir<br/>        self.CTIMENOW = datetime.now()</span><span id="fa2e" class="lk ki hi lg b fi lp lm l ln lo">self.metricJsonDict = {<br/>            "topic": self.TOPIC,<br/>            "group_id": self.GROUP,<br/>            "env": self.ENV,<br/>            "<a class="ae jy" href="http://twitter.com/timestamp" rel="noopener ugc nofollow" target="_blank">@timestamp</a>": str(self.CTIMENOW),<br/>            "partition": {}<br/>        }</span><span id="4b7e" class="lk ki hi lg b fi lp lm l ln lo">def checkConsumerGroupName(self):<br/>        __kc = KafkaAdminClient(bootstrap_servers=self.BOOTSTRAP_SERVERS)<br/>        cgnTuple = (self.GROUP, 'consumer')<br/>        for i in __kc.list_consumer_groups():<br/>            if cgnTuple == i:<br/>                return True<br/>        return False</span><span id="ac29" class="lk ki hi lg b fi lp lm l ln lo">def getMetric(self):<br/>        consumer = KafkaConsumer(<br/>            bootstrap_servers=self.BOOTSTRAP_SERVERS,<br/>            group_id=self.GROUP,<br/>            enable_auto_commit=False<br/>        )</span><span id="baf3" class="lk ki hi lg b fi lp lm l ln lo">for p in consumer.partitions_for_topic(self.TOPIC):<br/>            tp = TopicPartition(self.TOPIC, p)<br/>            consumer.assign([tp])<br/>            committed_offset = consumer.committed(tp)<br/>            if committed_offset is None:<br/>                committed_offset = 0<br/>            for _, v in consumer.end_offsets([tp]).items():<br/>                latest_offset = v<br/>            self.metricJsonDict["partition"][p] = {}<br/>            self.metricJsonDict["partition"][p]["committed_offset"] = committed_offset<br/>            self.metricJsonDict["partition"][p]["latest_offset"] = latest_offset<br/>            self.metricJsonDict["partition"][p]["lag"] = latest_offset - committed_offset</span><span id="d383" class="lk ki hi lg b fi lp lm l ln lo">with open(self.LOGDIR + "kafka-consumer-group-metrics.log", 'a+') as logFile:<br/>            logFile.write("\n")<br/>            logFile.write(json.dumps(self.metricJsonDict))<br/>            logFile.write("\n")<br/>        consumer.close(autocommit=False)</span><span id="f8bd" class="lk ki hi lg b fi lp lm l ln lo">def main():<br/>    inputFile = sys.argv[1]<br/>    logDir = sys.argv[2]<br/>    env = sys.argv[3]</span><span id="7a81" class="lk ki hi lg b fi lp lm l ln lo"># clean up log file before writing new data<br/>    open(logDir + "/kafka-consumer-group-metrics.log", 'w').close()</span><span id="52e4" class="lk ki hi lg b fi lp lm l ln lo">for line in open(inputFile):<br/>        line = line.strip()<br/>        if not line.startswith("#") and len(line) &gt; 0:<br/>            topic = line.split()[0] + "." + env.split("-kafka")[0]<br/>            group_id = line.split()[1]<br/>            try:<br/>                kc = KCMetric(topic.strip(), group_id.strip(), logDir, env)<br/>                if kc.checkConsumerGroupName():<br/>                    _t = threading.Thread(<br/>                        target=kc.getMetric<br/>                    ).start()<br/>            except:<br/>                print("something failed")</span><span id="ff6f" class="lk ki hi lg b fi lp lm l ln lo">main()</span></pre><p id="fa95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最重要的功能是</strong></p><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="266d" class="lk ki hi lg b fi ll lm l ln lo">def getMetric(self):<br/>        consumer = KafkaConsumer(<br/>            bootstrap_servers=self.BOOTSTRAP_SERVERS,<br/>            group_id=self.GROUP,<br/>            enable_auto_commit=False<br/>        )</span><span id="44a6" class="lk ki hi lg b fi lp lm l ln lo">for p in consumer.partitions_for_topic(self.TOPIC):<br/>            tp = TopicPartition(self.TOPIC, p)<br/>            consumer.assign([tp])<br/>            committed_offset = consumer.committed(tp)<br/>            if committed_offset is None:<br/>                committed_offset = 0<br/>            for _, v in consumer.end_offsets([tp]).items():<br/>                latest_offset = v<br/>            self.metricJsonDict["partition"][p] = {}<br/>            self.metricJsonDict["partition"][p]["committed_offset"] = committed_offset<br/>            self.metricJsonDict["partition"][p]["latest_offset"] = latest_offset<br/>            self.metricJsonDict["partition"][p]["lag"] = latest_offset - committed_offset<br/>        consumer.close(autocommit=False)</span></pre><p id="f785" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你仔细检查我的代码，我正在做的是</p><ol class=""><li id="ff54" class="lq lr hi ih b ii ij im in iq ls iu lt iy lu jc lv lw lx ly bi translated">创造消费者。</li><li id="c7ba" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">列出给定主题的分区总数。</li><li id="8b2d" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">创建主题分区对象。</li><li id="f347" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">将使用者分配给TopicPartition对象。</li><li id="0bac" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">读取TopicPartition对象的两个度量(consumer . committed &amp; consumer . end _ offsets)。</li><li id="7a8b" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">将指标附加到Python Dict / JSON对象。</li><li id="a9fc" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">对所有分区重复步骤3–6。</li><li id="0630" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">关闭消费者而不提交任何东西&amp;另存为JSON。</li></ol><p id="2505" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个需要两个重要的输入</p><ol class=""><li id="9a83" class="lq lr hi ih b ii ij im in iq ls iu lt iy lu jc lv lw lx ly bi translated">主题名称</li><li id="8607" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">消费者组名称</li></ol><p id="7e32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我可以为所有消费者群体这样做，但我只需要监控和提醒<strong class="ih hj">在线交易主题&amp;消费者</strong>。</p><p id="b984" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其余的主题可以由CMAK监控，我已经在我的设置。</p><p id="67a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jy" href="https://github.com/116davinder/kafka-cluster-ansible/blob/master/roles/jmxMonitor/files/kafka-consumer-group-metric-input.txt" rel="noopener ugc nofollow" target="_blank">卡夫卡-消费者-群体-度量-输入. txt </a></p><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="983e" class="lk ki hi lg b fi ll lm l ln lo">#######################################################################<br/># This file is input file for consumer group metric collection script.<br/># Usage: &lt;topic name without environment&gt;[space]&lt;group_id&gt;<br/>#<br/># Note:<br/># 1. if you same consumer reading from mulitple topics then<br/>#    add multiple lines with each topicName and same group_id.<br/># 2. group_id is case sensitive.<br/># 3. if you have a typo in topicName or group_id, metric won't be collected.<br/># 4. group_id should be same across environments.<br/># 5. all group_id/topic stats can't be collected as those will be too much<br/>#    so only In Line Transaction Topic/group_id data should be collected.<br/>#######################################################################<br/>topic1 consumer_group_id1<br/>.....</span></pre><p id="65fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">样本输出</strong></p><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="f0e3" class="lk ki hi lg b fi ll lm l ln lo">{<br/>	"topic": "topic1",<br/>	"group_id": "group_id1",<br/>	"env": "dev-env-kafka",<br/>	"@timestamp": "2020-05-18 15:36:01.191032",<br/>	"partition": {<br/>		"0": {<br/>			"committed_offset": 13626733,<br/>			"latest_offset": 13626733,<br/>			"lag": 0<br/>		},<br/>		"1": {<br/>			"committed_offset": 13623397,<br/>			"latest_offset": 13623397,<br/>			"lag": 0<br/>		},<br/>		"2": {<br/>			"committed_offset": 13615804,<br/>			"latest_offset": 13615804,<br/>			"lag": 0<br/>		}<br/>	}<br/>}<br/>....</span></pre><p id="4142" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将在cron中运行这个脚本，所以我在ansible中添加了一个任务来覆盖这个脚本。</p><pre class="je jf jg jh fd lf lg lh li aw lj bi"><span id="e810" class="lk ki hi lg b fi ll lm l ln lo">- name: creating folder for jmx monitor<br/>  file:<br/>    path: "{{ kafkaInstallDir }}/jmxMonitor"<br/>    state: directory</span><span id="9691" class="lk ki hi lg b fi lp lm l ln lo">- name: copying script and input files<br/>  copy:<br/>    src: "{{ item }}"<br/>    dest: "{{ kafkaInstallDir }}/jmxMonitor/{{ item }}"<br/>  loop:<br/>    - kafka-consumer-group-metric.py<br/>    - kafka-consumer-group-metric-input.txt</span><span id="19d0" class="lk ki hi lg b fi lp lm l ln lo">- name: creating kafka consumer group metric collector cron<br/>  cron:<br/>    name: "kafka consumer group metric collector cron task"<br/>    minute: "*"<br/>    hour: "*"<br/>    weekday: "*"<br/>    user: root<br/>    job: 'python3 {{ kafkaInstallDir }}/jmxMonitor/kafka-consumer-group-metric.py {{ kafkaInstallDir }}/jmxMonitor/kafka-consumer-group-metric-input.txt {{ kafkaLogDir }}/ {{ kafkaClusterName }}'</span></pre><p id="448b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Ansible剧本:<a class="ae jy" href="https://github.com/116davinder/kafka-cluster-ansible/blob/master/clusterConsumerMetricSetup.yml" rel="noopener ugc nofollow" target="_blank">116 dav inder/Kafka-cluster-ansi ble/clusterconsumermetricsetup . yml</a></p><p id="db5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们可以用它来制作Splunk仪表盘。</p><p id="c060" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Splunk代码:</strong><a class="ae jy" href="https://github.com/116davinder/kafka-cluster-ansible/blob/master/files/splunk-dashboards/apache-kafka-consumer-group-metrics.xml" rel="noopener ugc nofollow" target="_blank">116 davinder/Kafka-cluster-ansi ble/Splunk-dashboards/Apache-Kafka-consumer-group-metrics . XML</a></p><p id="33a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Splunk示例仪表板:</strong><a class="ae jy" href="https://github.com/116davinder/kafka-cluster-ansible/blob/master/files/splunk-dashboards/Kafka-Consumer-Splunk-Metric-Dashboard.png" rel="noopener ugc nofollow" target="_blank">116 davinder/Kafka-cluster-ansi ble/Splunk-dashboards/Kafka-Consumer-Splunk-Metric-Dashboard . png</a></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es me"><img src="../Images/c2ebe4d776526d1b0394e754c6b29144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nEtQE9Y-Mk1mkfIK0frUcA.png"/></div></div></figure><p id="ba37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么要定制监控脚本？</strong></p><ol class=""><li id="f538" class="lq lr hi ih b ii ij im in iq ls iu lt iy lu jc lv lw lx ly bi translated">作为一个宽容的消费者，我需要了解过去六个月的历史趋势。</li><li id="8e06" class="lq lr hi ih b ii lz im ma iq mb iu mc iy md jc lv lw lx ly bi translated">它应该是可扩展的和易于理解的。</li></ol></div><div class="ab cl mf mg gp mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="hb hc hd he hf"><p id="0ae3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LinkedIn还有一个很棒的工具叫做<a class="ae jy" href="https://github.com/linkedin/Burrow" rel="noopener ugc nofollow" target="_blank"> Burrow </a>，但是我从来没有机会去探索它。</p></div><div class="ab cl mf mg gp mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="hb hc hd he hf"><p id="09eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个旅程将在下一篇文章中继续(对容器的初始支持)</p></div></div>    
</body>
</html>