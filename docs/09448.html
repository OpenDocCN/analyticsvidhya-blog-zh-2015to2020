<html>
<head>
<title>OPTIMIZERS IN DEEP LEARNING</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习中的优化器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimizers-in-deep-learning-36549ea9c59f?source=collection_archive---------24-----------------------#2020-09-06">https://medium.com/analytics-vidhya/optimizers-in-deep-learning-36549ea9c59f?source=collection_archive---------24-----------------------#2020-09-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="196a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">什么是优化器？</em>T3】</strong></p><p id="2639" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优化器是用来改变神经网络属性的算法或方法，如权重和学习速率，以减少损失。</p><p id="01c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">基础优化:</strong></p><h2 id="5f7d" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak"> <em class="jz">批量渐变下降(BGD): </em> </strong></h2><p id="6bfb" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kc is it iu kd iw ix iy ke ja jb jc hb bi translated">在 BGD，它将获取所有训练数据集，然后存储每个记录的所有损失值。然后将所有损失函数值的总和发送给优化器。</p><p id="fca3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单地说，在批量梯度下降中，所有的训练数据都被考虑在内以采取一个单独的步骤。</p><p id="00fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤:</p><ol class=""><li id="02e9" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc kk kl km kn bi translated">获取全部训练数据。</li><li id="963b" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">输入神经网络。</li><li id="d8a1" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">求成本函数值。</li><li id="267f" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">计算它的梯度。</li><li id="b19b" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">使用上述梯度更新权重。</li></ol><blockquote class="kt ku kv"><p id="474c" class="if ig jd ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated"><strong class="ih hj">缺点<em class="hi">缺点</em>缺点</strong></p></blockquote><ol class=""><li id="0908" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc kk kl km kn bi translated">因为这个方法在一次更新中计算整个数据集的梯度。计算非常慢</li><li id="9e11" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">遇到大量数据集会非常棘手，你无法投入新的数据来实时更新模型。</li><li id="eed8" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">我们必须预先定义迭代次数历元。</li><li id="6378" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">这个模型需要更多的内存空间来训练数据。</li></ol><h2 id="3a13" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">随机梯度下降(SGD):</h2><p id="bac5" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kc is it iu kd iw ix iy ke ja jb jc hb bi translated">在 SGD 中，我们不是取整个记录，而是一次取一个记录来输入神经网络并更新权重。</p><p id="1b9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于大型数据集，可能有相似的样本，所以 BGD 计算梯度。<strong class="ih hj">会有冗余，而且 SGD 只更新一次，没有冗余，速度更快，可以添加新的样本。</strong></p><p id="aa9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤:</p><ol class=""><li id="4b31" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc kk kl km kn bi translated">从数据中提取单个记录。</li><li id="20d5" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">把它输入神经网络。</li><li id="df5d" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">寻找损失函数值</li><li id="2388" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">使用上面的值找到它的梯度。</li><li id="93e2" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">使用梯度更新权重。</li></ol><figure class="la lb lc ld fd le er es paragraph-image"><div class="er es kz"><img src="../Images/7586df9510455fc10aeb2dd43dfdebbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*ivApO1d1GFpV3FHRbKpFQg.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">新加坡元的波动</figcaption></figure><blockquote class="kt ku kv"><p id="b5ae" class="if ig jd ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated"><strong class="ih hj">缺点:</strong></p></blockquote><p id="6151" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，因为 SGD 更新更频繁，所以成本函数会有剧烈的振荡。BGD 可以收敛到一个局部极小值，当然，SGD 的振荡可能会跳到一个更好的局部极小值。</p><h2 id="02e2" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><strong class="ak">小批量梯度下降:</strong></h2><p id="8cff" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq kc is it iu kd iw ix iy ke ja jb jc hb bi translated">MBGD 采用小批量样本，即每次 n 个样本来计算。这样可以减少参数更新时的方差，收敛更稳定。它可以充分利用深度学习库中高度优化的矩阵运算，进行更高效的梯度计算。</p><p id="e94f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤:</p><ol class=""><li id="f040" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc kk kl km kn bi translated">从训练集中取 n 个样本数据。</li><li id="6498" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">把它输入神经网络。</li><li id="c0d3" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">求成本函数值。</li><li id="30fb" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">通过使用上面的值找到它的梯度。</li><li id="c707" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">使用上述值更新重量。</li></ol><blockquote class="kt ku kv"><p id="7631" class="if ig jd ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated"><strong class="ih hj">缺点:</strong></p></blockquote><ol class=""><li id="1c94" class="kf kg hi ih b ii ij im in iq kh iu ki iy kj jc kk kl km kn bi translated">小批量梯度下降不能保证良好的收敛性。</li><li id="eda2" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">如果学习率太小，收敛速度会很慢。如果是的话。也是。大，损失函数会在最小值处振荡甚至偏离。</li><li id="dbd5" class="kf kg hi ih b ii ko im kp iq kq iu kr iy ks jc kk kl km kn bi translated">如果批量大小等于训练数据集。那么 MBGD 等于 BGD。</li></ol><p id="8a7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="0be1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上优化者都是很老的优化者了。还有非常先进的优化工具，比上面的要好得多。高级优化器有 Adagrade、Adadelta、RMSprop 和 Adam。这些优化器基于动量工作。</p></div></div>    
</body>
</html>