<html>
<head>
<title>NLP - Preprocessing for Deep Models, something you need to know about Padding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP——深度模型的预处理，一些你需要知道的关于填充的东西</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nlp-preprocessing-for-deep-models-something-you-need-to-know-79cd9f03d315?source=collection_archive---------13-----------------------#2020-02-04">https://medium.com/analytics-vidhya/nlp-preprocessing-for-deep-models-something-you-need-to-know-79cd9f03d315?source=collection_archive---------13-----------------------#2020-02-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1ded" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自然语言处理在机器学习领域，特别是序列到序列学习，目前非常流行。互联网上有很多资源。然而，我写这篇文章是因为我很难理解文本是如何作为输入进入深度神经网络模式的。为了解决这个问题，我有很多解决方案，但是我寻找的解决方案没有提供太多信息。</p><p id="e364" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我将主要展示如何准备递归/卷积/前馈机器学习模型的输入。例如，我正在使用名为“<a class="ae jd" href="https://www.kaggle.com/c/nlp-getting-started" rel="noopener ugc nofollow" target="_blank">的Kaggle数据集，这是真的还是假的？带有灾难推文的NLP</a></p><p id="7876" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，我们来看看资料<script src="”&lt;a" class="ae jd" href="https://gist.github.com/gdmanandamohon/0825d6fc2e4cce08fcb7ddbcc74a0955.js" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/gdmanandamohon/0825d 6 fc 2 e 4c e08 fcb 7 ddbcc 74 a 0955 . js&lt;/a&gt;"&amp;gt;&amp;lt;/script&amp;gt;&lt;/root&gt;</script></p><p id="3d56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所看到的，有许多行和列带有文本。如果我们随机选择一列，我们可以看到</p><p id="8b82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">```'.https://t.co/gFJfgTodad'``<a class="ae jd" href="http://twitter.com/NorwayMFA" rel="noopener ugc nofollow" target="_blank">@挪威外交部</a>#巴林警察此前死于一场交通事故，他们并非死于爆炸<a class="ae jd" href="https://t.co/gFJfgTodad'" rel="noopener ugc nofollow" target="_blank"/></p><p id="4aa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以在这里我们可以看到？我们看到它包括标签，句子标点，和一些不会处理的网络链接。因此，首先我们将删除使用此代码的标点符号</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="4615" class="jn jo hi jj b fi jp jq l jr js">import nltk<br/>bOfWords = set(nltk.corpus.words.words())<br/>def removeNonEng(s):<br/>    return " ".join(w for w in nltk.wordpunct_tokenize(s) if w.lower() in bOfWords or not w.isalpha())</span><span id="2601" class="jn jo hi jj b fi jt jq l jr js"><br/>def removepunctuation(reviews):<br/>    all_reviews=list()<br/>    for text in reviews:      <br/>      text = "".join([ch for ch in text if ch not in punctuation])<br/>      text = text.lower()<br/>      all_reviews.append(removeNonEng(text).split())<br/>    return all_reviews</span></pre><p id="8a45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以后我们会用这种格式的句子。提到我在这里导入了一个自然语言处理语料库工具<a class="ae jd" href="https://www.nltk.org/data.html" rel="noopener ugc nofollow" target="_blank"> NLTK </a>，它包含大约23万个英语单词。所以，如果你句子中的任何一个单词超出了英语单词的范围，我们就跳过这个单词。这是我们执行后的结果</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="fd81" class="jn jo hi jj b fi jp jq l jr js">['police', 'had', 'previously', 'in', 'a', 'road', 'accident', 'they', 'were', 'not', 'by', 'explosion']</span></pre><p id="c29e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们再说一句</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="17ba" class="jn jo hi jj b fi jp jq l jr js">['i', 'still', 'have', 'not', 'church', 'of', 'coming', 'forward',<br/> 'to', 'comment', 'on', 'the', 'accident', 'issue', 'and', 'disciplinary']</span></pre><p id="0fff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我有两个句子作为演示，但在数据集中，我将有一堆句子，所有的标记都将从单词包中分离并丢弃。然而，我们仍然有深度模型不接受的文本。所以我们必须把这些文本转换成数字。那么，我们如何将文本转换成数字呢？</p><p id="e83c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有3个最受欢迎策略</p><ol class=""><li id="d05f" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated"><a class="ae jd" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a></li><li id="f48e" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">嵌入层</li><li id="4f48" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jd" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a></li></ol><p id="5304" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我将展示如何使用Word2Vec和嵌入层来实现这一点，因为我尝试了这两种方法。所以，让我们从Word2vec开始</p><p id="5da7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Word2Vec </strong></p><p id="3f62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Word2Vec是一个预先训练好的模型，根据这个模型，如果你输入一个单词，它会生成这个单词相对于数十亿个单词和句子的向量值。你可以简单地从他们的网站上把它安装成一个包。我用的是<a class="ae jd" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>，因为它是最流行的一款。让我们看看这个预训练模型的参数是什么</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="78ce" class="jn jo hi jj b fi jp jq l jr js">model = Word2Vec(xClean, min_count=1, size= 50, workers=5, window =5, sg = 1)</span></pre><p id="3873" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们可以看到，它包含6个参数，所以参数是</p><ol class=""><li id="4506" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">xClean:您的标点符号删除数据集。</li><li id="66fb" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">min_count:一个单词出现的最少次数。</li><li id="e871" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">大小:你的单词对相应的数字向量会有多大</li><li id="532b" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">工人:流程</li><li id="5e70" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">窗口:考虑有多少前一个和后一个单词将被视为序列</li><li id="1ba2" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">sg: CBoW /跳跃图</li></ol><p id="6b55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你对参数很不了解，请随意访问这个<a class="ae jd" href="https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92" rel="noopener" target="_blank">网站</a>，希望你会有好的想法。</p><p id="a860" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，现在我们可以根据我们的数据训练word2Vec，保存模型。然而，到目前为止，我们已经训练了预训练模型，但是我们现在应该做什么呢？好了，现在让我们回到句子。</p><p id="2d00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">要点</strong>来了——如果我们看到Word2Vec size参数，它将是一个单词的dimension ```[1*size]` `矢量化输出。例如，如果我们认为第一句话的第一个词是“警察”。因此，对于警察，只有word2Vec将生成[1*50]向量。然后，如果我们考虑整个句子:让我们计算单词的数量，12个单词，这意味着对于整个句子，word2vec将产生[12*50]矩阵，或者你可以考虑作为列表的列表。</p><p id="1e99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于数据集中的每一个句子，你可能都要做同样的事情。那么我们如何做到这一点呢？</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="34ff" class="jn jo hi jj b fi jp jq l jr js">encoded_docs = [[model.wv[word] for word in post] for post in xClean]</span></pre><p id="5901" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有一个快照，它将如何寻找一个单词。但是，如果任何单词超出了自然英语数据，它将调用和例外，如' noi '。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="fa6f" class="jn jo hi jj b fi jp jq l jr js">In: model[‘noise’]</span><span id="9f41" class="jn jo hi jj b fi jt jq l jr js">Out: <br/>array([ 0.24653986,  0.02332969,  0.11663471, -0.08650146,  0.03783561,<br/>       -0.06231214,  0.01733457, -0.08161656, -0.03913275, -0.1288227 ,<br/>        0.10245258, -0.07542043,  0.1505393 ,  0.03581995,  0.02998094,<br/>        0.07354113, -0.08015608,  0.06226369,  0.03420312, -0.03641276,<br/>        0.01748629, -0.09113364, -0.2961975 , -0.04497311,  0.05542335,<br/>       -0.10479202, -0.07443704, -0.04653041, -0.14211805, -0.09488385,<br/>        0.06362087,  0.00408071, -0.05070934, -0.03291701,  0.00774795,<br/>       -0.2704018 , -0.0356291 , -0.10112489, -0.06312449,  0.13630798,<br/>       -0.10666654,  0.06010102, -0.05819688, -0.19708745, -0.13682345,<br/>       -0.12157825, -0.26685408, -0.1362774 , -0.13189617, -0.08612245],<br/>      dtype=float32)</span><span id="c9e8" class="jn jo hi jj b fi jt jq l jr js">In: model['noise'].shape<br/>Out: (50,)</span></pre><p id="6c5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将得到as List[List[List <int> ]。现在，如果我们思考这些句子，我们会发现字数并不相等。S100有12个字，S101有16个字。因此，对应的矩阵将是[12*50]和[16*50]，这两个矩阵在维数上是不相等的。现在想想，如果我们想把这个矩阵输入到LSTM模型中，我们该如何解决这个问题？因为你不能根据你的数据定义一个变分模型。因此你必须对数据进行填充。</int></p><p id="d336" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">应用Word2Vec数据的填充对我来说是完全陌生的，我在努力如何使向量相等。后来我发现这篇<a class="ae jd" href="https://towardsdatascience.com/padding-sequences-with-simple-min-max-mean-document-encodings-aa27e1b1c781" rel="noopener" target="_blank">文章</a>真的很有帮助。因为每篇文章/教程都在说如何将单词转换成向量<strong class="ih hj">，但是没有人说在将单词转换成向量之后，在输入之前，我们如何让它们等于输入模型。</strong></p><p id="ae24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，现在我们必须使不等长的向量变成相等的向量长度，并且对于每个句子都是一致的。为此，我们可以定义一个这样的函数</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="b40c" class="jn jo hi jj b fi jp jq l jr js">import math<br/>MAX_LENGTH =100</span><span id="dd7d" class="jn jo hi jj b fi jt jq l jr js">padded_posts = []</span><span id="d34f" class="jn jo hi jj b fi jt jq l jr js">for post in encoded_docs:<br/>    # Pad short posts with alternating min/max<br/>    if len(post) &lt; MAX_LENGTH:<br/>        pointwise_avg = np.mean(post)<br/>        padding = [pointwise_avg]<br/>        post += padding * math.ceil((MAX_LENGTH - len(post) / 2.0))<br/>        <br/>    # Shorten long posts or those odd number length posts we padded to 100<br/>    if len(post) &gt; MAX_LENGTH:<br/>        post = post[:MAX_LENGTH]    <br/>    # Add the post to our new list of padded posts<br/>    padded_posts.append(post)</span></pre><p id="4531" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我将100定义为最大的固定长度。因此，如果一个句子只有12或17个单词，它也将转换成100个，另一方面，如果任何一个句子超过100个单词，它将被截断并合并成100个。如果任何句子不能满足100个单词的平均值，它将填充空值</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="4bce" class="jn jo hi jj b fi jp jq l jr js">S101 = ['i', 'still', 'have', 'not', 'church', 'of', 'coming', 'forward',<br/> 'to', 'comment', 'on', 'the', 'accident', 'issue', 'and', 'disciplinary'] = &gt; Word2Vec =&gt; [17*50] </span><span id="d295" class="jn jo hi jj b fi jt jq l jr js">and</span><span id="cfb4" class="jn jo hi jj b fi jt jq l jr js">S100 = ['police', 'had', 'previously', 'in', 'a', 'road', 'accident', 'they', 'were', 'not', 'by', 'explosion'] = &gt; Word2Vec =&gt; [12*50]</span><span id="abaf" class="jn jo hi jj b fi jt jq l jr js">After padding <br/>S101 = [100*50]<br/>S100 = [100*50]</span></pre><p id="f0a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，所有维度都相同了，我们可以开始了，现在我们可以定义100个LSTM单元格，并通过LSTM单元格推送Word2Vec数据。</p><p id="4cb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">嵌入层</strong></p><p id="bb3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嵌入层来自深度学习库，如PyTorch或Tensorflow。然而，仍然嵌入层不会接受直接的文本，我们必须以不同的方式处理。所以我们必须玩弄文字。因此，同样的方式，我们将删除所有的标点符号，并有干净的数据。我们必须应用一个热编码。那么，这是什么OHE？如果你不知道，请仔细阅读这篇文章，它会让你的想法变得清晰。让我们试一试相同句子的热门代码-</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="d978" class="jn jo hi jj b fi jp jq l jr js">from keras.preprocessing.text import one_hot<br/>def oneHotEncode(docs):<br/>    vocab_size = 50<br/>    encoded_docs = [one_hot(d, vocab_size) for d in docs]<br/>    return encoded_docs<br/>xOneHot = oneHotEncode(xClean)<br/></span><span id="6a9d" class="jn jo hi jj b fi jt jq l jr js">print(xOneHot[100])<br/>'[3, 1, 8, 48, 2, 16, 4, 41, 40, 25, 17, 3, 9, 22, 19, 45, 48, 1, 16, 42]'</span><span id="e4dd" class="jn jo hi jj b fi jt jq l jr js">print(xOneHot[101])<br/>[48, 30, 9, 9, 35, 9, 15, 11, 48, 24, 48, 6, 22, 49, 6, 25, 49, 23, 30, 38, 19]</span></pre><p id="2f7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们在OHE之后看到的，它创造了一个单词序列，单词对应一个整数值。所以，对于两个不同的句子，它产生两个不同的向量。所以，现在我们也做填充。</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="0d01" class="jn jo hi jj b fi jp jq l jr js">from keras.preprocessing.sequence import pad_sequences<br/>max_seq_length =25<br/>def makePadded(xOneHot): <br/>    padded_docs = pad_sequences(xOneHot, maxlen=max_seq_length, padding='post')<br/>    return padded_docs</span></pre><p id="d7a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以这里每一个和每一个句子的列表长度都是相等的。所以填充后的列表将是类似的东西</p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="646c" class="jn jo hi jj b fi jp jq l jr js">S100 = [ 3,  1,  8, 48,  2, 16,  4, 41, 40, 25, 17,  3,  0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0], dtype=int32)</span></pre><p id="8351" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如您所见，在列表末尾添加了值为0的填充，因为我添加了post填充作为参数。前填充它就像[1*12]向量，后填充它是[1*25]向量。类似地，整个数据集将是List[List[List <int> ]，每个列表包含25个整数序列。现在你已经准备好给深度模型添加一个嵌入层了。</int></p><pre class="je jf jg jh fd ji jj jk jl aw jm bi"><span id="e51e" class="jn jo hi jj b fi jp jq l jr js">model = Sequential()<br/>model.add(Embedding(vocab_size, emb_out_len, input_length=max_seq_length))   #vocabsize, emb_out, inp len<br/>model.add(LSTM(emb_out_len))<br/>model.add(Flatten())<br/>model.add(Dense(1, activation='sigmoid'))<br/>model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])<br/>print(model.summary())</span><span id="7b7f" class="jn jo hi jj b fi jt jq l jr js">model.fit(paddedMat, Y, epochs=EPOC, verbose=0)<br/>loss, accuracy = model.evaluate(paddedMat, Y, verbose=0)<br/>print('Accuracy: %f' % (accuracy*100))</span></pre><p id="dc43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们已经回顾了如何使用Word2Vec和嵌入模型这两种不同的策略为自然语言处理准备数据。我希望你喜欢，虽然它很长。</p></div></div>    
</body>
</html>