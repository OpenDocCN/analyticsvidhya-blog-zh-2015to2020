<html>
<head>
<title>NLP Representation Techniques Part-3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP表示技术第三部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nlp-representation-techniques-part-3-18383b037e15?source=collection_archive---------30-----------------------#2020-06-28">https://medium.com/analytics-vidhya/nlp-representation-techniques-part-3-18383b037e15?source=collection_archive---------30-----------------------#2020-06-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d81b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经在我的第一篇博客中解释了单词袋和TFIDF。你可以在这里找到<a class="ae jd" rel="noopener" href="/analytics-vidhya/nlp-representations-techniques-e6d69096d4ad"><strong class="ih hj"/></a><strong class="ih hj">。</strong>然后我们讨论了TFIDF的一些问题，并在第2部分学习了单词嵌入，以及如何使用Word2Vec生成它们。如果你想了解这些，你可以在这里找到<a class="ae jd" rel="noopener" href="/analytics-vidhya/nlp-representations-techniques-part2-86b2fd4e04b9"><strong class="ih hj"/></a>。</p><p id="addc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将向你展示一些先进的方法和嵌入去当你在一个工业规模的项目工作。这些包括手套、BERT单词嵌入、BERT句子嵌入和多语言嵌入。</p><h1 id="751c" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">手套</h1><p id="ceb1" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">该手套是斯坦福大学在2014年提出的模型。目标是非常相同的，即学习单词嵌入和单词之间的关系。你可以在这里  <strong class="ih hj">阅读更多关于Glove <a class="ae jd" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">的内容。</strong></a></strong></p><p id="5b58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">幸运的是，我们没有从头开始训练这个模型，所以我们可以在用例中使用预先训练好的嵌入。你所要做的就是点击<a class="ae jd" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这里</strong> </a>然后去<strong class="ih hj">下载预先训练好的词向量</strong>部分。</p><p id="5c57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你会发现这些选项，如维基百科，推特，这是他们用来训练模型的训练数据类型，所以单词嵌入的概念将取决于语料库。您还会发现400，000，190，000或220，000，000个词汇，这些是该型号词汇中的单词数。</p><p id="f4cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经下载了400K Vocab的维基百科版本。您会发现这四个文件的名称类似于glove50d、glove100d或glove300d。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/c1f1340fcc90edcb3ba8be7351a86d9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CpcAmj2kR1U-xHOkn4Ax8g.png"/></div></div></figure><p id="e612" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果是50维或100维等等，这些50d、100d或200d指示每个单词的向量的大小。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kt"><img src="../Images/32106ed0ae637d742e3601eb8012a0da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8UNsM41jA94qiMf5pE40wg.png"/></div></div></figure><p id="0f2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦你打开这个文件，你会发现每一行的第一个值是单词，其余的行是单词的单词嵌入。现在我们只需要阅读这个文件，并提取单词embeddings。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ku"><img src="../Images/427abd281e2b42fd49c9afa04f716195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8o8fScLcOxo0DhvsuEhVKQ.png"/></div></div></figure><p id="f803" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我使用这段代码将文本文件转换成一个字典，字典中的单词作为键，单词嵌入作为相应的值。</p><p id="e082" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以看到我们只是循环遍历文件中的每一行，并在拆分为word并保留为数组后设置第一项。我在这里使用Numpy，因为它高效快捷。</p><p id="80aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了语料库中每个单词的单词嵌入。你可以应用我们在<a class="ae jd" rel="noopener" href="/analytics-vidhya/nlp-representations-techniques-part2-86b2fd4e04b9"> <strong class="ih hj">第二部分</strong> </a>中讨论的所有技术，并根据你的需要使用它们。</p><p id="a0a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">单词嵌入的大小有50维或100维的变化。一般来说，这个想法是，如果我们使用向量大小更大的单词嵌入，我们就有更多关于单词的信息，所以我们应该在准确性方面得到提高，因为模型有更多的信息可以学习。</p><p id="4bce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我在用LSTM神经网络做一个简单的情感分析。我试着从50维提升到100维，准确率提高了5-7 %,但是当我提升到300维时，提升幅度就没那么大了，只提高了0.03 %。你可以在你的项目上试试，看看有没有帮助。</p><h2 id="e110" class="kv jf hi bd jg kw kx ky jk kz la lb jo iq lc ld js iu le lf jw iy lg lh ka li bi translated">手套vs Word2Vec</h2><ol class=""><li id="d9a8" class="lj lk hi ih b ii kc im kd iq ll iu lm iy ln jc lo lp lq lr bi translated">如果你想用英语单词的知识建立一个通用的产品，并且你不想训练单词嵌入，那么就去找Glove吧</li><li id="772f" class="lj lk hi ih b ii ls im lt iq lu iu lv iy lw jc lo lp lq lr bi translated">如果你想为你的特定领域训练模型，可以使用Word2Vec或者像Bert这样更好的工具，并使用迁移学习。</li></ol><p id="4602" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.下载glove嵌入时，请始终查看vocab的大小，因为如果明天的新单词不在您的Vocab中，您将根据您的实现获得0或null。</p><h1 id="e4d8" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">伯特</h1><p id="cb4d" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi lx translated"><span class="l ly lz ma bm mb mc md me mf di">B</span><strong class="ih hj">I directional Encoder re presentations from Transformers(BERT)</strong>是Google开发的用于NLP(自然语言处理)任务的模型。</p><h2 id="cd86" class="kv jf hi bd jg kw kx ky jk kz la lb jo iq lc ld js iu le lf jw iy lg lh ka li bi translated">培训:</h2><p id="3a64" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">正如我们在word2Vec中看到的，我们试图根据上下文预测单词，如最后4个单词，或者我们有一个单词，我们试图预测上下文单词。</p><p id="7fab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在伯特的训练中。他们寻求一种新的方法，在语料库中随机屏蔽15%的单词，模型的目标是预测被屏蔽的单词。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mg"><img src="../Images/50fb6533cdc0c9e85a9fa3321f0510d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*N78Lm_Fn5LkF-G9Rb3PzMA.jpeg"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">这是一张双向LSTM的照片，但是伯特是在一个变形金刚模型上训练的。这个想法几乎是一样的。</figcaption></figure><p id="b8d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用具有注意机制的转换器从左到右和从右到左处理序列，以生成更好的嵌入</p><blockquote class="ml mm mn"><p id="1a8e" class="if ig mo ih b ii ij ik il im in io ip mp ir is it mq iv iw ix mr iz ja jb jc hb bi translated">例如:</p><p id="4a5a" class="if ig mo ih b ii ij ik il im in io ip mp ir is it mq iv iw ix mr iz ja jb jc hb bi translated">我想看电影</p><p id="ffe3" class="if ig mo ih b ii ij ik il im in io ip mp ir is it mq iv iw ix mr iz ja jb jc hb bi translated">会被这样处理:</p><p id="c02d" class="if ig mo ih b ii ij ik il im in io ip mp ir is it mq iv iw ix mr iz ja jb jc hb bi translated">我→想→想→看→a→电影</p><p id="54e1" class="if ig mo ih b ii ij ik il im in io ip mp ir is it mq iv iw ix mr iz ja jb jc hb bi translated">电影→a→观看→到→想要→I</p></blockquote><p id="4788" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么两个结果将被连接并向前传递。</p><p id="873d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以想象通过双向LSTM来处理它，你有一个单词序列x0，x1，x2，x3，每个单词被表示为一个独热向量，</p><p id="edd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每个时间戳，我们都有一个新单词和一个来自上一个时间戳的隐藏状态。这两者被连接起来，并馈入LSTM单元，这个过程被重复，直到句子结束或最大序列长度。现在想象对同一个句子或单词序列做同样的过程，从x3倒排到x2，x1，x0。在处理的最后，我们只是合并两个输出，并将其向前传递。</p><p id="b643" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们使用了带有注意力机制的变形金刚，但想法几乎是一样的。你拿起一个神经网络，给它一串文本，它试图预测一些单词，通过这样做，模型学习单词之间的关系。</p><p id="7679" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经有了训练好的模型，我们可以使用这个模型来生成单词嵌入。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ms"><img src="../Images/c8432e56ecf1a31d0ce4378117a8e8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0TUtOl9cFJGXpD78oOJ8_Q.png"/></div></div></figure><p id="a466" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是具有输入层、隐藏层和输出层的神经网络的简单图像。想象一下，用整个BERT架构移除该输入层，然后将该架构的输出馈入密集或隐藏层，该层进一步连接到输出层。</p><p id="10d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出层只是单词，但我们想要单词嵌入，而不是单词，因此我们移除该输出层，并获取网络中最后一个隐藏层的输出，并将其视为单词嵌入。</p><p id="2c3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以尝试这样做，但是我发现了一个很好的单词嵌入的Bert实现<a class="ae jd" href="https://github.com/imgarylai/bert-embedding" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这里</strong> </a>。你可以安装它，然后嘣！！它已经准备好了</p><p id="d33e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你只需传递你的句子列表，它会返回你文本中每个单词的单词嵌入列表。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mt"><img src="../Images/70d1b94e3ec6bd468db696961c7f33f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SNauJble7gYX5ji75PoB4g.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated"><strong class="bd jg">使用BERT进行单词嵌入</strong></figcaption></figure><p id="df2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以看到我有一些文本，我已经把这些文本分成句子，现在我把这些句子传递给模块，它返回给我一个句子列表以及每个单词的单词嵌入。确保传递给它的是一个句子列表，而不是一个句子，因为这样它会为每个字符返回一串数字。该模块还在内部将句子分成单词，因此您也不必这样做。</p><p id="e4f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模块将返回一个句子列表，对于每个句子，您将获得一个单词列表。</p><p id="715c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些模块返回给我一个长度为18的列表，它对应于上面截图所示的句子中的18个单词。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mu"><img src="../Images/c4e85970100550457f06167918e474c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5EQc9TNB7qaV7yrrQnI2mQ.png"/></div></div></figure><p id="1bd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于句子中的每个单词，你将得到一个大小为786的向量。你会得到一个向量列表，每个向量对应于句子中的每个单词。</p><p id="c113" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在这里阅读更多关于他们的<a class="ae jd" href="https://pypi.org/project/bert-embedding/" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"/></a><strong class="ih hj">。你还会发现更多类似BERT-1024的型号。这不仅仅是在一个有更多参数和隐藏状态的更大的模型上训练的同样的想法。因此，您可以选择786或1024等嵌入式系统。</strong></p><blockquote class="ml mm mn"><p id="7256" class="if ig mo ih b ii ij ik il im in io ip mp ir is it mq iv iw ix mr iz ja jb jc hb bi translated"><strong class="ih hj">由于Bert是一个基于transformer的模型，你也可以用它进行迁移学习。它基本上采用了整个BERT模型，并针对您的特定任务(如文档分类或问题回答)对其进行了微调。但那只是以后的话题。</strong></p></blockquote><p id="6cdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在你已经有了单词embeddings，你可以使用LSTM或者一些机器学习模型来完成你的NLP任务。</p><h2 id="ff74" class="kv jf hi bd jg kw kx ky jk kz la lb jo iq lc ld js iu le lf jw iy lg lh ka li bi translated">伯特的问题</h2><p id="e955" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">虽然BERT非常适合NLP任务，但是它也有一些问题。</p><p id="e0c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1)我已经讨论了如何将单词嵌入转换成句子嵌入，例如对单词嵌入取平均值，以及其他在<a class="ae jd" rel="noopener" href="/analytics-vidhya/nlp-representations-techniques-part2-86b2fd4e04b9"> <strong class="ih hj">部分</strong> </a> <strong class="ih hj">中详细讨论的技术。</strong>如果你将这些技术应用于BERT嵌入，它们不会给你想要的结果。这是一篇相关的<a class="ae jd" href="https://arxiv.org/abs/1908.10084" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">论文</strong> </a>在这里他们讨论了这个问题。</p><p id="446a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们还解决了这个问题，他们使用一个暹罗网络对架构进行了一些改变，然后针对句子级别的任务而不是单词级别的任务训练了整个模型。你可以免费使用他们的模型。在这里 可以找到所有<a class="ae jd" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">的信息。</strong></a></p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mv"><img src="../Images/1f1eb138a01e349eb4dddc2ce9e8b8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RSd8EAZUNJS694b4oENKlw.png"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">伯特句子嵌入</figcaption></figure><p id="a904" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你只需要安装程序库，模型就可以免费使用了。你只需要实例化这个模型，并把句子列表传递给它，它就会返回每个句子的句子嵌入列表。</p><p id="70da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我只给出了3个句子，每个句子我得到了1个向量。它具有与BERT原始模型相同的长度，即768，但是这次它具有整个句子的信息，而不仅仅是一个单词。</p><p id="e973" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以使用这个句子嵌入来做所有的机器学习工作，比如文档聚类或者文本分类。</p><p id="41bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2) BERT是一个非常大的模型，因此它需要GPU一次处理多个批次。你可以在上面讨论的同一个库中找到BERT的精华版本。</p><blockquote class="ml mm mn"><p id="6042" class="if ig mo ih b ii ij ik il im in io ip mp ir is it mq iv iw ix mr iz ja jb jc hb bi translated">蒸馏是一个过程，他们试图尽可能多地从神经网络中删除层，而实际上不会失去模型的准确性。这个过程导致神经网络具有更少的层、更少的参数，因此模型的大小缩小了。</p><p id="dc46" class="if ig mo ih b ii ij ik il im in io ip mp ir is it mq iv iw ix mr iz ja jb jc hb bi translated">总是搜索提炼的版本并尝试使用它，你不总是需要那个大模型来获得更好的准确性。</p></blockquote><p id="c005" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然您已经看到了如何从word2vec、Glove和Bert生成嵌入。是时候转向一些有趣的东西了，比如多语言嵌入。</p><h1 id="290c" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">多语言嵌入</h1><p id="d87e" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi lx translated">多语种嵌入不仅仅是一个非常酷的概念，你可以为不同的语言进行嵌入，并将它们映射到同一个空间。</p><p id="942f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">举个例子:如果你有一个英语句子，现在你把它转换成了一个向量。法语中有同样的句子，现在你把它转换成了一个向量。这两个向量在该特征空间中彼此非常接近。</p><p id="8823" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这也有助于我们为不同的语言建立系统，当你有一个NLP问题，但是你没有足够的语言训练数据。该模型可以在多语言嵌入而不是普通嵌入上训练，现在该模型可以只接受多种语言的输入。你不需要改变管道中的任何东西。</p><p id="33cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在这里  <strong class="ih hj">找到我对垃圾邮件分类器<a class="ae jd" href="https://github.com/shivambatra76/Multilingual-Spam-classifier" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">的多语言嵌入的完整实现。</strong>我仅使用英语数据进行训练，并尝试了不同的语言来测试该模型，虽然我建议您应该有一些针对您的目标语言的训练数据。你可以点击</a></strong> 查看更多关于多语言嵌入的信息。</p><p id="984f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您在这方面需要任何帮助，或者只想进行与数据科学相关的讨论，请联系<a class="ae jd" href="https://www.linkedin.com/in/shivam-batra-34b63a17a" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> me </strong> </a>。</p></div></div>    
</body>
</html>