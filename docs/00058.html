<html>
<head>
<title>Document Vectorization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文档矢量化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/document-vectorization-301b06a041?source=collection_archive---------2-----------------------#2018-08-19">https://medium.com/analytics-vidhya/document-vectorization-301b06a041?source=collection_archive---------2-----------------------#2018-08-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/e757e60f34a74d458ff164e4106bf3ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*DI-tgIXpo_3ZnkOBKN8f0A.jpeg"/></div></figure><div class=""/><p id="5ee3" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在我以前的博客文章中，我写了关于单词矢量化的实现和用例。你可以在这里阅读<a class="ae jk" href="http://techscouter.blogspot.com/2017/10/word-vectorization.html" rel="noopener ugc nofollow" target="_blank">。但是很多时候我们需要挖掘短语之间的关系，而不是句子之间的关系。举个例子</a></p><ul class=""><li id="5718" class="jl jm hp io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated">约翰今年休了很多次假</li><li id="1e2c" class="jl jm hp io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">树叶正在从树上落下</li></ul><p id="e026" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这两个句子中，一个常见的单词“leaves”根据其所在的句子有不同的含义。这个意思只有在我们理解完整短语的上下文时才能理解。或者我们想测量短语的相似性，并将它们归入一个名称下。</p><p id="bf13" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这更多的是关于python中doc2vec的实现，而不是算法的细节。该算法使用分级softmax或负采样；参见<a class="ae jk" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="io hq">托马斯·米科洛夫、程凯、格雷格·科拉多和杰弗里·迪恩:“向量空间中单词表示的有效估计，2013年ICLR研讨会论文集”</strong> </a>和<a class="ae jk" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="io hq">托马斯·米科洛夫、伊利亚·苏茨基弗、程凯、格雷格·科拉多和杰弗里·迪恩:“单词和短语的分布式表示及其组成性。在NIPS会议录，2013" </strong> </a>。</p><h1 id="37ae" class="jz ka hp bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">要求:</h1><p id="7a29" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">我将使用python包gensim在一组新闻上实现doc2vec，然后使用Kmeans集群将类似的文档绑定在一起。此实施需要以下包</p><ul class=""><li id="277a" class="jl jm hp io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated">gensim( <a class="ae jk" href="https://radimrehurek.com/gensim/install.html" rel="noopener ugc nofollow" target="_blank">安装细节</a>)</li><li id="c3d8" class="jl jm hp io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">sklearn( <a class="ae jk" href="http://scikit-learn.org/stable/install.html" rel="noopener ugc nofollow" target="_blank">安装细节</a>)</li><li id="fccc" class="jl jm hp io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">nltk( <a class="ae jk" href="https://www.nltk.org/install.html" rel="noopener ugc nofollow" target="_blank">安装细节</a></li><li id="26ec" class="jl jm hp io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">熊猫</li></ul><h1 id="bacf" class="jz ka hp bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">准备数据</h1><p id="0dc9" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">我在这个博客中使用了60个新闻文章标题，数据集可以在这里找到<a class="ae jk" href="https://drive.google.com/open?id=1IUSbrzcaeyC-nvKwopKux2BgtY5t_C96" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="29ad" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">以原始格式使用的数据很少能产生好的结果。记住<strong class="io hq">的原理<em class="lc">“垃圾进垃圾出</em> </strong>”</p><p id="3a24" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将执行一些清理步骤来规范和清除新闻标题。为此，首先让我们加载文件到内存中。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="a5b2" class="lm ka hp li b fi ln lo l lp lq">#import the libraries<br/>import pandas as pd<br/>df=pd.read_csv('path_to_dir/s_news.csv')<br/>#drop the Nan rows<br/>df.dropna(inplace=True)</span></pre><p id="40e7" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们已经将文件加载到内存中，让我们执行清理工作</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="fd9e" class="lm ka hp li b fi ln lo l lp lq">#import the libraries<br/>import re<br/>from nltk.corpus import stopwords<br/>from nltk.stem.wordnet import WordNetLemmatizer<br/>import gensim<br/>lemma = WordNetLemmatizer()<br/>stopword_set = set(stopwords.words('english')+['a','of','at','s','for','share','stock'])</span><span id="7d01" class="lm ka hp li b fi lr lo l lp lq">def process(string):<br/>    string=' '+string+' '<br/>    string=' '.join([word if word not in stopword_set else '' for word in string.split()])<br/>    string=re.sub('\@\w*',' ',string)<br/>    string=re.sub('\.',' ',string)<br/>    string=re.sub("[,#'-\(\):$;\?%]",' ',string)<br/>    string=re.sub("\d",' ',string)<br/>    string=string.lower()<br/>    string=re.sub("nyse",' ',string)<br/>    string=re.sub("inc",' ',string)<br/>    string=re.sub(r'[^\x00-\x7F]+',' ', string)<br/>    string=re.sub(' for ',' ', string)<br/>    string=re.sub(' s ',' ', string)<br/>    string=re.sub(' the ',' ', string)<br/>    string=re.sub(' a ',' ', string)<br/>    string=re.sub(' with ',' ', string)<br/>    string=re.sub(' is ',' ', string)<br/>    string=re.sub(' at ',' ', string)<br/>    string=re.sub(' to ',' ', string)<br/>    string=re.sub(' by ',' ', string)<br/>    string=re.sub(' &amp; ',' ', string)<br/>    string=re.sub(' of ',' ', string)<br/>    string=re.sub(' are ',' ', string)<br/>    string=re.sub(' co ',' ', string)<br/>    string=re.sub(' stock ',' ', string)<br/>    string=re.sub(' share ',' ', string)<br/>    string=re.sub(' stake ',' ', string)<br/>    string=re.sub(' corporation ',' ', string)<br/>    string=" ".join(lemma.lemmatize(word) for word in string.split())<br/>    string=re.sub('( [\w]{1,2} )',' ', string)<br/>    string=re.sub("\s+",' ',string)<br/>    <br/>    return string.split()</span><span id="323d" class="lm ka hp li b fi lr lo l lp lq">#drop the duplicate values of news <br/>df.drop_duplicates(subset=['raw.title'],keep='last',inplace=True)</span><span id="2057" class="lm ka hp li b fi lr lo l lp lq">#reindex the data frame<br/>df.index=range(0,len(df))</span><span id="d388" class="lm ka hp li b fi lr lo l lp lq">#apply the process function to the news titles<br/>df['title_l']=df['raw.title'].apply(process)<br/>df_new=df</span></pre><figure class="ld le lf lg fd hk er es paragraph-image"><div class="er es ls"><img src="../Images/7db390e761a0ef6b601b7d2a866a3fa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*fcXWT-W1tT_w1lIwURMSmg.png"/></div></figure><h1 id="cecf" class="jz ka hp bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">创建doc2vec模型</h1><p id="36aa" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">现在，当我们有了干净的数据，让我们把这些数据转换成向量。Gensim对doc2vec的实现需要gensim的TaggedDocuments类的对象。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="4de8" class="lm ka hp li b fi ln lo l lp lq">#import the modules<br/>from gensim.models.doc2vec import Doc2Vec, TaggedDocument</span><span id="f0cf" class="lm ka hp li b fi lr lo l lp lq">documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(list(df_new['title_l']))]</span></pre><p id="efc0" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们已经成功地清理了文档，让我们创建模型</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="9165" class="lm ka hp li b fi ln lo l lp lq">model = Doc2Vec(documents, size=25, window=2, min_count=1, workers=4)</span></pre><p id="4c23" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">到目前为止，我们已经有了一个完全加载的doc2vec模型，它包含了数据框架中的所有文档向量</p><p id="b6fc" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">打印所有的向量</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="8925" class="lm ka hp li b fi ln lo l lp lq">#appending all the vectors in a list for training<br/>X=[]<br/>for i in range(40):<br/>    X.append(model.docvecs[i])<br/>    print mdoel.docvecs[i]</span></pre><p id="2e3e" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这些向量现在包含文档的嵌入和文档的语义。我们可以使用模型中的方法来查找相似的新闻文章。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="91aa" class="lm ka hp li b fi ln lo l lp lq">#to create a new vector<br/>vector = model.infer_vector(process("Merger news with verizon"))</span><span id="36af" class="lm ka hp li b fi lr lo l lp lq"># to find the siilarity with vector<br/>model.similar_by_vector(vector)</span><span id="8da8" class="lm ka hp li b fi lr lo l lp lq"># to find the most similar word to words in 2 document<br/>model.wv.most_similar(documents[1][0])</span><span id="001c" class="lm ka hp li b fi lr lo l lp lq">#find similar documents to document 1<br/>model.docvecs.most_similar(1)</span></pre><h1 id="3471" class="jz ka hp bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">聚类文档</h1><p id="f6d3" class="pw-post-body-paragraph im in hp io b ip kx ir is it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj hb bi translated">我们将使用上一节中创建的向量，通过K-means聚类算法来生成聚类。sklearn中提供了K-means的实现，所以我将使用该实现。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="3f53" class="lm ka hp li b fi ln lo l lp lq">#import the modules<br/>from sklearn.cluster import KMeans<br/>import numpy as np<br/>#create the kmeans object withe vectors created previously<br/>kmeans = KMeans(n_clusters=4, random_state=0).fit(X)</span><span id="fae1" class="lm ka hp li b fi lr lo l lp lq">#print all the labels<br/>print kmeans.labels_</span><span id="9f7e" class="lm ka hp li b fi lr lo l lp lq">#craete a dictionary to get cluster data<br/>clusters={0:[],1:[],2:[],3:[]}<br/>for i in range(40):<br/>    clusters[kmeans.labels_[i]].append(' '.join(df_new.ix[i,'title_l']))<br/>print clusters</span></pre><p id="2e03" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这给了我们4个聚类，其中一个聚类中的所有新闻条目都在字典<strong class="io hq">“cluster”</strong>的相应关键字中。</p><p id="7bd5" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里我取了4个集群，你可以根据你的数据集取不同的数字。</p><p id="3214" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我希望这个博客是有帮助的。随意评论。</p></div></div>    
</body>
</html>