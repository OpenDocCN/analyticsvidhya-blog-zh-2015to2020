<html>
<head>
<title>Machine Learning &amp; Deep Learning Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和深度学习指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-deep-learning-guide-db520c4797da?source=collection_archive---------7-----------------------#2019-11-17">https://medium.com/analytics-vidhya/machine-learning-deep-learning-guide-db520c4797da?source=collection_archive---------7-----------------------#2019-11-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2ba9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated"><span class="l je jf jg bm jh ji jj jk jl di"> W </span>欢迎来到机器学习&amp;深度学习指南的第2部分，在这里我们学习和实践机器学习和深度学习，而不会被概念和数学规则所淹没。</p><blockquote class="jm jn jo"><p id="d86f" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-part-1-4ba7ce8cf7eb"> <em class="hi">第1部分:关键术语、定义和从监督学习(线性回归)开始。</em> </a></p><p id="e9fd" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated">第二部分:监督学习:回归(SGD)和分类(SVM、朴素贝叶斯、KNN和决策树)。</p><p id="ee06" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-11ad26e0854c"> <em class="hi">第三部分:无监督学习(KMeans，PCA)，欠拟合vs过拟合和交叉验证</em> </a> <em class="hi">。</em></p><p id="95a1" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-da303a71b8e0"> <em class="hi">第四部分:深度学习:定义、层次、度量和损失、优化器和正则化</em> </a></p></blockquote></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="c51a" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">学习目标</strong></h1><p id="02a2" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">在这一部分中，我们将继续讨论剩余的监督学习算法的例子，以及用于分类的相应误差和度量。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es le"><img src="../Images/b48b9772461cd474b030b78bf570feac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R2P2e4gQXIDowbz87YXBaw.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">机器学习的类型及其用法</figcaption></figure><h1 id="65d5" class="kb kc hi bd kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku ly kw kx ky bi translated"><strong class="ak">监督学习</strong>——<strong class="ak">随机梯度下降(SGD)回归器:</strong></h1><p id="35f9" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">在第1部分中，我们举例说明了如何创建我们的第一个回归模型——线性回归模型。现在我们将检查SGD回归器。</p><blockquote class="jm jn jo"><p id="33e7" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated">你可以从<a class="ae jt" href="https://www.kaggle.com/mohammadhatoum/stochastic-gradient-descent-regressor" rel="noopener ugc nofollow" target="_blank">这里</a>下载完整的Kaggle笔记本</p></blockquote><p id="67c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还将按照我们提到的步骤来解决机器学习问题:</p><ol class=""><li id="e909" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated">数据定义</li><li id="1b11" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">训练/测试分割</li><li id="2347" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">预处理</li><li id="109c" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">算法选择</li><li id="6082" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">培养</li><li id="9c16" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">预言；预测；预告</li><li id="145c" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">评估模型的性能</li><li id="8b9d" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">微调</li></ol></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><ol class=""><li id="1534" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated"><strong class="ih hj">数据定义:</strong>我们将使用“<a class="ae jt" href="https://www.kaggle.com/nsrose7224/crowdedness-at-the-campus-gym" rel="noopener ugc nofollow" target="_blank">拥挤度</a>在校园体育馆”数据集。给定一天中的某个时间(可能还有一些其他特征，包括天气)，预测体育馆会有多拥挤。我们将下载数据并将其保存在文件夹data中，并将其命名为crowdedness.csv</li></ol><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="e87a" class="ms kc hi mo b fi mt mu l mv mw">import numpy as np # linear algebra<br/>import pandas as pd # data processing<br/>df = pd.read_csv("data/crowdedness.csv")</span></pre><p id="691d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">打印各列以了解数据概况</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="2a1d" class="ms kc hi mo b fi mt mu l mv mw">print(df.columns.values)</span></pre><blockquote class="mx"><p id="7a50" class="my mz hi bd na nb nc nd ne nf ng jc dx translated">结果:<br/> ['人数' '日期' '时间戳' '星期几' '是周末'<br/>'是假日' '温度' '是学期开始' '是学期期间'<br/>'月' '小时]]</p></blockquote><p id="601a" class="pw-post-body-paragraph if ig hi ih b ii nh ik il im ni io ip iq nj is it iu nk iw ix iy nl ja jb jc hb bi translated">打印数据的信息以查看每一列的类型</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="ebd4" class="ms kc hi mo b fi mt mu l mv mw">print(df.info())</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nm"><img src="../Images/2bad01d8030fb494109d027e654939a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*aqq65weF9g2fa0rbaYtJEA.png"/></div></figure><p id="8e17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到有一个浮动列是温度，更有趣的是有一个对象列是日期。</p><p id="abeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们打印前几行，看看我们能找到什么。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="f736" class="ms kc hi mo b fi mt mu l mv mw"><em class="jp">print(df.head())</em></span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es nn"><img src="../Images/7e969592be95e9ad6e89d8eba42e46c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mt5sPXdcHbDicPddlhOWdA.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">显示拥挤度数据集中的前五条记录</figcaption></figure><p id="798d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以注意到，日期列是我们收集数据的日期和时间，时间戳也是如此。因此，为了确保我们将打印两列中的唯一值。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="5f42" class="ms kc hi mo b fi mt mu l mv mw"><em class="jp">print(df[‘date’].unique)</em></span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es no"><img src="../Images/d84762da4b8068f2d96c43165d32be06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CnKOkqScOpEDr-GMapEjGw.png"/></div></div></figure><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="c5e4" class="ms kc hi mo b fi mt mu l mv mw"><em class="jp">print(df['timestamp'].unique)</em></span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es np"><img src="../Images/37c37eaf989cba2cadac92bce5481bf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*P5EHJ8v2qkQyD8dENJ_43w.png"/></div></figure><p id="2a8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么我们是对的，这意味着我们可以删除这两列</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="34c6" class="ms kc hi mo b fi mt mu l mv mw"><em class="jp">df = df.drop(['date','timestamp'],axis=1)</em></span></pre><p id="f370" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">训练/测试分割:</strong></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="5cfa" class="ms kc hi mo b fi mt mu l mv mw"># Extract the training and test data<br/>data = df.values<br/>X = data[:, 1:]  # all rows, no label<br/>y = data[:, 0]  # all rows, label only</span><span id="3324" class="ms kc hi mo b fi nq mu l mv mw"># Extract the training and test data<br/>data = df.values<br/>X = data[:, 1:]  # all rows, no label<br/>y = data[:, 0]  # all rows, label only</span><span id="f72f" class="ms kc hi mo b fi nq mu l mv mw">from sklearn.model_selection import train_test_split<br/>X_train_original, X_test_original, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)</span><span id="80f3" class="ms kc hi mo b fi nq mu l mv mw"># View the shape (structure) of the data<br/>print(f"Training features shape: {X_train_original.shape}")<br/>print(f"Testing features shape: {X_test_original.shape}")<br/>print(f"Training label shape: {y_train.shape}")<br/>print(f"Testing label shape: {y_test.shape}")</span></pre><blockquote class="mx"><p id="1aee" class="my mz hi bd na nb nc nd ne nf ng jc dx translated">结果:<br/>训练特征形状:(46638，9) <br/>测试特征形状:(15546，9) <br/>训练标签形状:(46638，)<br/>测试标签形状:(15546，)</p></blockquote><p id="c118" class="pw-post-body-paragraph if ig hi ih b ii nh ik il im ni io ip iq nj is it iu nk iw ix iy nl ja jb jc hb bi translated"><strong class="ih hj"> 3。预处理:</strong>我们将使用由<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>定义的StandardScaler:通过<strong class="ih hj">移除平均值</strong>和<strong class="ih hj">缩放到单位方差</strong>来标准化特征。换句话说，分布将以0为中心，标准差为1。我们使用这种方法使模型工作得更快，并将我们所有的特征缩放到同一个集合。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="488f" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.preprocessing import StandardScaler<br/># Scale the data to be between -1 and 1<br/>scaler = StandardScaler()<br/>scaler.fit(X_train_original)<br/>X_train = scaler.transform(X_train_original)<br/>X_test = scaler.transform(X_test_original)</span></pre><p id="d5aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地理解所发生的事情，考虑下面的代码来显示我们的训练集中前60条记录的温度。我将使用<a class="ae jt" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"> Matplotlib </a></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="2b19" class="ms kc hi mo b fi mt mu l mv mw"># Import library<br/>import matplotlib<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>matplotlib.style.use('ggplot')</span><span id="9162" class="ms kc hi mo b fi nq mu l mv mw"># Specify number of plot to be displayed and the figure size<br/>fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 10))</span><span id="7ea1" class="ms kc hi mo b fi nq mu l mv mw"># Set a title and plot the data<br/>ax1.set_title('Before Scaling')<br/>ax1.plot(X_train_original[:60,3])</span><span id="794d" class="ms kc hi mo b fi nq mu l mv mw">ax2.set_title('After Standard Scaler')<br/>ax2.plot(X_train[:60,3])</span><span id="30d7" class="ms kc hi mo b fi nq mu l mv mw"># Display the graph<br/>plt.show()</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nr"><img src="../Images/bcc591f353c72af550852f1977b93e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*RLXoBbPxNWKoh0-G5aewiw.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">原始数据和缩放数据之间的比较</figcaption></figure><p id="6992" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，这两个图形具有相同的形状，但唯一的区别是Y轴上的值。在原始数据中，我们看到温度在46.26和67.91之间，而在缩放数据中，温度在-1.9505和1.47793之间。</p><p id="91b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们说这使得后台的计算更容易，导致模型运行更快。</p><h1 id="7c5e" class="kb kc hi bd kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku ly kw kx ky bi translated">重要注意事项:</h1><p id="7120" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">我想请你注意下面的笔记:</p><ol class=""><li id="e997" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated">在将数据分成训练集和测试集之后，我们进行了预处理</li><li id="8da8" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">我们只在训练集而不是测试集上应用了<em class="jp"> fit </em>。这意味着我们计算了训练集中数据的平均值和标准偏差，然后将其应用于训练集和测试集。</li></ol><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="741b" class="ms kc hi mo b fi mt mu l mv mw">scaled_train =  (train - train_mean) / train_std_deviation<br/>scaled_test = (test - train_mean) / train_std_deviation</span></pre><p id="bbf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是机器学习中使用的标准程序，其背后的原因是我们希望将测试集视为新的和未知的数据。通过这种方式，我们将测试我们的模型在使用新的未知数据的实际应用中的表现。</p><p id="07ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。</strong> <strong class="ih hj">算法选择:</strong>我们将SGDRegressor加上一些参数。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="2967" class="ms kc hi mo b fi mt mu l mv mw"># Establish a model<br/>from sklearn.linear_model import SGDRegressor<br/>sgd_huber=SGDRegressor(alpha=0.01, learning_rate='optimal', loss='huber',penalty='elasticnet')</span></pre><p id="0df9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。培训:</strong></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="c6eb" class="ms kc hi mo b fi mt mu l mv mw">sgd_huber.fit(X_train, y_train)</span></pre><p id="7ccc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 6。预测:</strong></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="bb69" class="ms kc hi mo b fi mt mu l mv mw">y_pred_lr = sgd_huber.predict(X_test)  # Predict labels</span></pre><p id="e34f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7。评估模型的性能:</strong></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="61e1" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error</span><span id="bc4b" class="ms kc hi mo b fi nq mu l mv mw"># The mean squared error<br/>print(f"Mean squared error: {round( mean_squared_error(y_test, y_pred_lr),3)}")</span><span id="c638" class="ms kc hi mo b fi nq mu l mv mw"># Explained variance score: 1 is perfect prediction<br/>print(f"Variance score: {round(r2_score(y_test, y_pred_lr),3)}")</span><span id="ec59" class="ms kc hi mo b fi nq mu l mv mw"># Mean Absolute Error<br/>print(f"Mean squared error: { round(mean_absolute_error(y_test, y_pred_lr),3)}")</span></pre><blockquote class="mx"><p id="ff6d" class="my mz hi bd na nb nc nd ne nf ng jc dx translated">结果:<br/>均方误差:348.267 <br/>方差得分:0.324 <br/>均方误差:14.617</p></blockquote><p id="29b4" class="pw-post-body-paragraph if ig hi ih b ii nh ik il im ni io ip iq nj is it iu nk iw ix iy nl ja jb jc hb bi translated"><strong class="ih hj"> 8。微调:</strong>如你所见，方差较低。我们能做些什么来增加它吗？是的。记得在步骤4中，我们设置了一些参数(alpha、learning_rate、loss和penalty)让我们使用不同的值。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="b729" class="ms kc hi mo b fi mt mu l mv mw"># Try different parameters<br/># Try different parameters<br/>sgd_l2 = SGDRegressor(alpha=0.01,learning_rate='optimal', loss='squared_loss',<br/>             penalty='l2')</span><span id="64f8" class="ms kc hi mo b fi nq mu l mv mw">sgd_l2.fit(X_train, y_train)<br/>print(f"Score on training set {round(sgd_l2.score(X_train, y_train),3)}")</span><span id="d9fc" class="ms kc hi mo b fi nq mu l mv mw">y_pred_lr = sgd_l2.predict(X_test)  # Predict labels</span><span id="d5b3" class="ms kc hi mo b fi nq mu l mv mw">from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error</span><span id="7c0f" class="ms kc hi mo b fi nq mu l mv mw"># The mean squared error<br/>print(f"Mean squared error: {round( mean_squared_error(y_test, y_pred_lr),3)}")</span><span id="ea30" class="ms kc hi mo b fi nq mu l mv mw"># Explained variance score: 1 is perfect prediction<br/>print(f"Variance score: {round(r2_score(y_test, y_pred_lr),3)}")</span><span id="885e" class="ms kc hi mo b fi nq mu l mv mw"># Mean Absolute Error<br/>print(f"Mean squared error: { round(mean_absolute_error(y_test, y_pred_lr),3)}")</span></pre><blockquote class="mx"><p id="7a2c" class="my mz hi bd na nb nc nd ne nf ng jc dx translated">结果:<br/>在训练集上的得分0.506 <br/>均方差:249.126 <br/>方差得分:0.517 <br/>均方差:12.064</p></blockquote><p id="d3be" class="pw-post-body-paragraph if ig hi ih b ii nh ik il im ni io ip iq nj is it iu nk iw ix iy nl ja jb jc hb bi translated">酷，误差减小，d，方差增大到0.5以上。但是这里最重要的问题是，我们如何知道要改变什么？答案是通过使用<strong class="ih hj">超参数调整</strong>。想法很简单，我们准备了几个<a class="ae jt" href="https://scikit-learn.org/stable/modules/grid_search.html#grid-search" rel="noopener ugc nofollow" target="_blank"> <em class="jp">超参数</em> </a>的组合，并应用它们。然后我们看到最好的结果和参数。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="a257" class="ms kc hi mo b fi mt mu l mv mw"># Establish a model<br/>model = SGDRegressor(learning_rate='optimal',penalty='l2')<br/>from sklearn.model_selection import GridSearchCV<br/># Grid search - this will take about 1 minute.<br/>param_grid = {<br/>    'alpha': 10.0 ** -np.arange(1, 7),<br/>    'loss': ['squared_loss', 'huber', 'epsilon_insensitive'],<br/>}<br/>clf = GridSearchCV(model, param_grid)<br/>clf.fit(X_train, y_train)<br/>print(f"Best Score: {round(clf.best_score_,3)}" )<br/>print(f"Best Estimator: {clf.best_estimator_}" )<br/>print(f"Best Params: {clf.best_params_}" )</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es ns"><img src="../Images/8b04f8632e2079359a6a5387ad1773a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-xGWAFWt-7ozkAFraRc6ZQ.png"/></div></div></figure><p id="c764" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">那么接下来是什么</strong>，到目前为止，我们看到了我们的SGDRegressor模型在数据集上的表现，它不是很好。怎样才能表现的更好？<br/>一种方法是使用不同的参数范围，但我认为不会增加太多。<br/>另一种方法是切换到不同的回归模型。RandomForestRegressor是一个不错的选择。</p><p id="044a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请记住，我们的目标不是解决一个问题本身，而是拥有解决这些问题和挑战的知识、直觉、程序和工具。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="8d15" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">监督学习—分类:</h1><p id="795b" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">为了更好地了解这个主题，我们将考虑一个分类问题，然后使用和比较多种分类算法。</p><blockquote class="jm jn jo"><p id="4a4e" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated">你可以从<a class="ae jt" href="https://www.kaggle.com/mohammadhatoum/supervised-learning-classification" rel="noopener ugc nofollow" target="_blank">这里</a>下载完整的Kaggle笔记本</p></blockquote><ol class=""><li id="e439" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated"><strong class="ih hj">数据定义:</strong>我们将使用<a class="ae jt" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank">泰坦尼克</a>数据。在这个挑战中，我们被要求建立一个预测模型来回答这个问题:“什么样的人更有可能生存？”使用乘客数据(如姓名、年龄、性别、社会经济阶层等)</li></ol><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="4630" class="ms kc hi mo b fi mt mu l mv mw">import warnings<br/>warnings.filterwarnings("ignore")</span><span id="3ad1" class="ms kc hi mo b fi nq mu l mv mw"># Load the diabetes dataset<br/>import pandas as pd<br/>train_df = pd.read_csv("../input/titanic/train.csv")<br/>test_df = pd.read_csv("../input/titanic/test.csv")</span></pre><p id="092e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">打印各列以了解数据概况</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="0322" class="ms kc hi mo b fi mt mu l mv mw">print(train_df.columns.values)</span></pre><blockquote class="mx"><p id="8385" class="my mz hi bd na nb nc nd ne nf ng jc dx translated">结果:<br/> ['PassengerId ' '幸存' ' Pclass ' '姓名' '性别' '年龄' ' SibSp' 'Parch' <br/>'车票' '票价' '舱位' '上船']</p></blockquote><pre class="nt nu nv nw nx mn mo mp mq aw mr bi"><span id="45a8" class="ms kc hi mo b fi mt mu l mv mw">print(test_df.columns.values)</span></pre><blockquote class="mx"><p id="be50" class="my mz hi bd na nb nc nd ne nf ng jc dx translated">结果:<br/> ['PassengerId' 'Pclass ' '姓名' '性别' '年龄' ' SibSp' 'Parch ' '车票' '票价'<br/>'客舱' '上船']</p></blockquote><p id="a93d" class="pw-post-body-paragraph if ig hi ih b ii nh ik il im ni io ip iq nj is it iu nk iw ix iy nl ja jb jc hb bi translated">让我们打印前几行，看看我们能找到什么。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="abec" class="ms kc hi mo b fi mt mu l mv mw">train_df.head()</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es ny"><img src="../Images/58e12c110447eb9893771d0b2181be74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cO0by0AhDosHPPQDt2azJA.png"/></div></div></figure><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="1502" class="ms kc hi mo b fi mt mu l mv mw">test_df.head()</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es nz"><img src="../Images/23915bba93d876e8bcbe2461f7f79f12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWNVIKxuM0dz2MSPqOYnPA.png"/></div></div></figure><p id="33f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在给出结果的反馈之前，让我们打印数据的信息以查看每一列的类型，然后我们将为type对象的列调用describe函数。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="4e77" class="ms kc hi mo b fi mt mu l mv mw">train_df.info()</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es oa"><img src="../Images/95c0b72b65ba83dff784eeb34915e5a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*4aGW-N8DdqyUNwFiYN9e2g.png"/></div></figure><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="b23e" class="ms kc hi mo b fi mt mu l mv mw">test_df.info()</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es oa"><img src="../Images/95c0b72b65ba83dff784eeb34915e5a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*4aGW-N8DdqyUNwFiYN9e2g.png"/></div></figure><p id="6612" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看一些与object类型的字段相关的统计数据</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="a839" class="ms kc hi mo b fi mt mu l mv mw">train_df.describe(include=['O'])</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es ob"><img src="../Images/0578a29fce9f88ef331d67c57d6cbe90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*9eaqpkyDS7zpOU7NM0ac7w.png"/></div></figure><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="bf32" class="ms kc hi mo b fi mt mu l mv mw">test_df.describe(include=['O'])</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es oc"><img src="../Images/d2e809502c688087a05e7f0fdcff7448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*4x8yOZXvVWWkjkKUtvx-sg.png"/></div></figure><p id="b413" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将检查哪些要素具有空值</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="e6e1" class="ms kc hi mo b fi mt mu l mv mw">train_df.columns[train_df.isnull().any()]</span></pre><blockquote class="mx"><p id="64eb" class="my mz hi bd na nb nc nd ne nf ng jc dx translated">结果:<br/> ['年龄'，'船舱'，'上船']</p></blockquote><p id="eb2e" class="pw-post-body-paragraph if ig hi ih b ii nh ik il im ni io ip iq nj is it iu nk iw ix iy nl ja jb jc hb bi translated">有趣的是，从上面，我们可以对数据有一些基本的了解。我们可以注意到:</p><ol class=""><li id="adc0" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated">我们有5列类型对象:姓名、性别、机票、舱位和登机。</li><li id="8f9f" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">PassengerId是乘客的唯一Id，因此应该删除它。</li><li id="cf2d" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">这个名字似乎有独特的价值，它可能对生存有很低的影响，所以我们可能需要放弃它。我还注意到它包括一些头衔(先生、夫人、小姐……)，这些头衔可用于生成新的特征。但是现在，我们将忽略它。</li><li id="527e" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">票是字母数字的。可能具有唯一的值，因此也可能被删除。</li><li id="6869" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">我们有以下数据类型:<br/> <em class="jp"> A .序数:幸存和Pclass <br/> B .分类:性别和已登机<br/> C .离散:SibSp和Parch <br/> D .连续:票价和年龄<br/> E .字母数字:机票和舱位</em></li></ol><p id="af21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们从特性名称中提取标题特性，这样我们就可以删除Name。我会用Kaggle建议的<a class="ae jt" href="https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/" rel="noopener ugc nofollow" target="_blank">特征工程</a>方法。我也会删除PassengerId。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="668f" class="ms kc hi mo b fi mt mu l mv mw">import string<br/>def substrings_in_string(big_string, substrings):<br/>    for substring in substrings:<br/>        if str.find(big_string, substring) != -1:<br/>            return substring<br/>    return np.nan</span><span id="86ad" class="ms kc hi mo b fi nq mu l mv mw">title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',<br/>                'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',<br/>                'Don', 'Jonkheer']<br/>train_df['Title']=train_df['Name'].map(lambda x: substrings_in_string(x, title_list))<br/>test_df['Title']=test_df['Name'].map(lambda x: substrings_in_string(x, title_list))</span><span id="10e4" class="ms kc hi mo b fi nq mu l mv mw">#replacing all titles with mr, mrs, miss, master<br/>def replace_titles(x):<br/>    title=x['Title']<br/>    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:<br/>        return 'Mr'<br/>    elif title in ['Countess', 'Mme']:<br/>        return 'Mrs'<br/>    elif title in ['Mlle', 'Ms']:<br/>        return 'Miss'<br/>    elif title =='Dr':<br/>        if x['Sex']=='Male':<br/>            return 'Mr'<br/>        else:<br/>            return 'Mrs'<br/>    else:<br/>        return title<br/>    <br/>train_df['Title']=train_df.apply(replace_titles, axis=1)<br/>test_df['Title']=test_df.apply(replace_titles, axis=1)</span><span id="e728" class="ms kc hi mo b fi nq mu l mv mw">#Drop the columns 'Name', 'PassengerId' and 'Ticket'<br/>train_df = train_df.drop(['Name','PassengerId','Ticket'],axis=1)<br/>test_df = test_df.drop(['Name','PassengerId','Ticket'],axis=1)</span></pre><p id="08c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，从Kaggle提供的特征定义中，我们可以看到<br/>“SibSp是泰坦尼克号上兄弟姐妹/配偶的数量”和“SibSp是泰坦尼克号上父母/子女的数量”<br/>因此，让我们创建一个名为Family_Size的新特征，它将包含SibSp和SibSp的总和。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="4089" class="ms kc hi mo b fi mt mu l mv mw">train_df['Family_Size']=train_df['SibSp']+train_df['Parch']<br/>test_df['Family_Size']=test_df['SibSp']+test_df['Parch']</span></pre><p id="b73c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于缺失的值，我有以下计划来填补:<br/> 1。年龄:我将使用年龄的数据平均值<br/> 2。小屋:我会用‘N’来填充它们。apollowed:只有两个缺少的值，所以我将使用mode</p><p id="56b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，我将把年龄和票价从连续变量转换成分类变量。这可以使用熊猫库中的<em class="jp"/><a class="ae jt" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html" rel="noopener ugc nofollow" target="_blank"><em class="jp">剪切</em> </a> <em class="jp"> </em>功能来完成。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="ef12" class="ms kc hi mo b fi mt mu l mv mw">import numpy as np<br/>from scipy.stats import mode</span><span id="096b" class="ms kc hi mo b fi nq mu l mv mw">for df in [train_df, test_df]:<br/>    <br/>    meanAge=np.mean(df.Age)<br/>    df.Age=df.Age.fillna(meanAge)<br/>    bins = (-1, 0,  50, 100)<br/>    group_names = ['Unknown', 'Under_50', 'More_Than_50']<br/>    categories = pd.cut(df.Age, bins, labels=group_names)<br/>    df.Age = categories<br/>    <br/>    df.Cabin = df.Cabin.fillna('N')<br/>    df.Cabin = df.Cabin.apply(lambda x: x[0])<br/>    <br/>    modeEmbarked = mode(df.Embarked)[0][0]<br/>    df.Embarked = df.Embarked.fillna(modeEmbarked)<br/>    <br/>    df.Fare = df.Fare.fillna(-0.5)<br/>    bins = (-1, 0, 8, 15, 31, 1000)<br/>    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']<br/>    categories = pd.cut(df.Fare, bins, labels=group_names)<br/>    df.Fare = categories</span></pre><p id="639b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们所做的是基本的特征工程。我们可以继续创建新的功能，或者检查功能之间的相关性和依赖性，但我们现在就离开它。</p><p id="34e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">训练/测试分割:</strong></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="9102" class="ms kc hi mo b fi mt mu l mv mw"># Extract the training and test data<br/>y = train_df['Survived']<br/>X = train_df.drop('Survived',axis=1)</span><span id="656b" class="ms kc hi mo b fi nq mu l mv mw">from sklearn.model_selection import train_test_split<br/>X_train,X_val,y_train,y_val = train_test_split(X,y,test_size= 0.2, random_state=0)</span><span id="98f5" class="ms kc hi mo b fi nq mu l mv mw"># View the shape (structure) of the data<br/>print(f"Training features shape: {X_train.shape}")<br/>print(f"Testing features shape: {X_val.shape}")<br/>print(f"Training label shape: {y_train.shape}")<br/>print(f"Testing label shape: {y_val.shape}")</span></pre><blockquote class="mx"><p id="f1cd" class="my mz hi bd na nb nc nd ne nf ng jc dx translated">结果:<br/>训练特征形状:(712，10) <br/>测试特征形状:(179，10) <br/>训练标签形状:(712，)<br/>测试标签形状:(179，)</p></blockquote><p id="a420" class="pw-post-body-paragraph if ig hi ih b ii nh ik il im ni io ip iq nj is it iu nk iw ix iy nl ja jb jc hb bi translated">3.<strong class="ih hj">预处理:</strong>现在是时候将type对象的字段转换成numerics了。这些是作为对象的字段。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="74e4" class="ms kc hi mo b fi mt mu l mv mw">train_df.describe(include=['O'])</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es ob"><img src="../Images/0578a29fce9f88ef331d67c57d6cbe90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*9eaqpkyDS7zpOU7NM0ac7w.png"/></div></figure><p id="714b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">特征:<em class="jp">性别，着手和头衔</em>很少有明显的价值。所以我们将使用<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html" rel="noopener ugc nofollow" target="_blank">普通编码器。<br/> </a>这种方式得到的值将是整数如下:<br/> 1。性别:女性会被0代替，男性会被1 <br/> 2代替。上船:C会被0代替，Q会被1代替，S会被2 <br/> 3代替。头衔:主人将被0取代，小姐将被1取代，先生将被2取代，夫人将被3取代</p><p id="9d31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于特征<em class="jp">年龄</em>和<em class="jp">票价</em>，我将使用<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank">标签编码器</a></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="3b44" class="ms kc hi mo b fi mt mu l mv mw"># Print top 10 records before transformation<br/>X_train[0:10]</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es od"><img src="../Images/bf307817e89a97cd81512b2741f9810b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a_FAUQdXtkUVjU9ROHzlNQ.png"/></div></div></figure><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="498f" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.preprocessing import OrdinalEncoder<br/>encoder_sex = OrdinalEncoder()<br/>X_train['Sex'] = encoder_sex.fit_transform(X_train['Sex'].values.reshape(-1, 1))<br/>X_val['Sex'] = encoder_sex.transform(X_val['Sex'].values.reshape(-1, 1))</span><span id="026e" class="ms kc hi mo b fi nq mu l mv mw">encoder_cabin = OrdinalEncoder()<br/>X_train['Cabin'] = encoder_cabin.fit_transform(X_train['Cabin'].values.reshape(-1, 1))<br/>X_val['Cabin'] = encoder_cabin.transform(X_val['Cabin'].values.reshape(-1, 1))</span><span id="decf" class="ms kc hi mo b fi nq mu l mv mw">encoder_embarked = OrdinalEncoder()<br/>X_train['Embarked'] = encoder_embarked.fit_transform(X_train['Embarked'].values.reshape(-1, 1))<br/>X_val['Embarked'] = encoder_embarked.transform(X_val['Embarked'].values.reshape(-1, 1))</span><span id="ff22" class="ms kc hi mo b fi nq mu l mv mw">encoder_title = OrdinalEncoder()<br/>X_train['Title'] = encoder_title.fit_transform(X_train['Title'].values.reshape(-1, 1))<br/>X_val['Title'] = encoder_title.transform(X_val['Title'].values.reshape(-1, 1))</span><span id="7c1f" class="ms kc hi mo b fi nq mu l mv mw">from sklearn.preprocessing import LabelEncoder<br/>features = ['Fare',  'Age']</span><span id="942c" class="ms kc hi mo b fi nq mu l mv mw">for feature in features:<br/>        le = LabelEncoder()<br/>        le = le.fit(X_train[feature])<br/>        X_train[feature] = le.transform(X_train[feature])<br/>        X_val[feature] = le.transform(X_val[feature])</span></pre><p id="6598" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将打印转换后的结果</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="ebda" class="ms kc hi mo b fi mt mu l mv mw"># Print top 10 records after transformation<br/>X_train[0:10]</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es oe"><img src="../Images/01e5179a5497e6fdce724e111a9d393c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*TGSLShaxJXSSelVPuQYnvA.png"/></div></figure><p id="e3c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.<strong class="ih hj">算法选择:</strong>我们将使用支持向量机(<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank">核SVM </a>)、朴素贝叶斯(<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html" rel="noopener ugc nofollow" target="_blank">高斯贝叶斯</a>)、k近邻(<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank">KNeighborsClassifier</a>)<br/>和决策树(<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">决策树分类器</a>)然后比较结果。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="91a8" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.svm import SVC<br/>from sklearn.naive_bayes import GaussianNB<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.tree import DecisionTreeClassifier</span><span id="12a5" class="ms kc hi mo b fi nq mu l mv mw">names = ["Kernel SVM", "Naive Bayes", "K Nearest Neighbor",<br/>         "Decision Tree"]</span><span id="74cd" class="ms kc hi mo b fi nq mu l mv mw">classifiers = [<br/>    SVC(kernel = 'rbf',gamma='scale'),<br/>    GaussianNB(),<br/>    KNeighborsClassifier(3),<br/>    DecisionTreeClassifier(max_depth=5)]</span></pre><p id="a4e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.<strong class="ih hj">训练和预测:</strong></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="1190" class="ms kc hi mo b fi mt mu l mv mw"># iterate over classifiers<br/>for name, clf in zip(names, classifiers):<br/>    clf.fit(X_train, y_train) <br/>    y_pred = clf.predict(X_val)<br/># Here we will add the error and evaluation metrics</span></pre><p id="3eff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.<strong class="ih hj">评估每个模型的性能:</strong>这里我们将检查分类的错误和评估指标:</p><p id="f2e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用3个函数:准确度_得分<em class="jp">、</em>分类_报告、<em class="jp">、混淆_矩阵<em class="jp">。</em>这些定义来自<a class="ae jt" href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a></em></p><ol class=""><li id="9147" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score" rel="noopener ugc nofollow" target="_blank">准确度分数</a>:计算准确度，正确预测的分数(默认)或计数(正常化=假)。</li></ol><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es of"><img src="../Images/c2d87d36a71e2b984c25ffa99efe74b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*ZlmLGrFoUGDr8OXaqY9z3w.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:<a class="ae jt" href="https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a></figcaption></figure><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="2182" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.metrics import accuracy_score</span><span id="ec58" class="ms kc hi mo b fi nq mu l mv mw">data = []<br/># iterate over classifiers<br/>for name, clf in zip(names, classifiers):<br/>    clf.fit(X_train, y_train)<br/>    <br/>    y_pred = clf.predict(X_val)<br/>    print(f"Accuracy for {name} : {accuracy_score(y_val, y_pred)*100.0}")<br/>    data.append(accuracy_score(y_val, y_pred)*100.0)</span><span id="4d89" class="ms kc hi mo b fi nq mu l mv mw">models = pd.DataFrame({<br/>    'Model': names,<br/>    'Score': data})<br/>models.sort_values(by='Score', ascending=False)</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es og"><img src="../Images/ee21f9d2a941764df9506f3d3c331a9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*l6Y7FAkmH1IvX1hjtCAlCg.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">准确度结果</figcaption></figure><p id="19be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如您所见，K近邻具有最高的准确性。</p><p id="ce71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report" rel="noopener ugc nofollow" target="_blank">分类报告</a>:构建一个显示主要分类指标的文本报告(<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" rel="noopener ugc nofollow" target="_blank">精度、召回率、f1值和支持度</a>)</p><p id="bebf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a.精度(也称为<a class="ae jt" href="https://en.wikipedia.org/wiki/Positive_predictive_value" rel="noopener ugc nofollow" target="_blank">阳性预测值</a>)是比率<code class="du oh oi oj mo b">tp / (tp + fp)</code>，其中<code class="du oh oi oj mo b">tp</code>是真阳性的数量，<code class="du oh oi oj mo b">fp</code>是假阳性的数量。精确度直观上是分类器不将阴性样品标记为阳性的能力。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es ok"><img src="../Images/6f089cd789289ed6ac08e5ce436a0154.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*Uy4Kf9_TfLKYRnuQUu65qw.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:<a class="ae jt" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">维基百科</a></figcaption></figure><p id="f9e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b.召回(也称为<a class="ae jt" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" rel="noopener ugc nofollow" target="_blank">灵敏度</a>)是比率<code class="du oh oi oj mo b">tp / (tp + fn)</code>，其中<code class="du oh oi oj mo b">tp</code>是真阳性的数量<code class="du oh oi oj mo b">fn</code>是假阴性的数量。召回直观上是分类器找到所有肯定样本的能力。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es ol"><img src="../Images/22eeb5a7c0b9b14459e179963b17ac38.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*iw0W5CGBDWLDhnLFTQ909g.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:<a class="ae jt" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">维基百科</a></figcaption></figure><p id="3129" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">c.<a class="ae jt" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank"> F1得分</a>是<a class="ae jt" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">精度和召回率</a>的<a class="ae jt" href="https://en.wikipedia.org/wiki/Harmonic_mean" rel="noopener ugc nofollow" target="_blank">调和平均值</a>，其中F1得分在1时达到最佳值(完美的精度和召回率)，在0时最差。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es om"><img src="../Images/8dc9efcc5d8b7b0cd8a4ff7471016878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*b1CElpYffWv35awTWeKPEQ.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:<a class="ae jt" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank">维基百科</a></figcaption></figure><p id="d24d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">d.支持度是<code class="du oh oi oj mo b">y_true</code>中每个类的出现次数</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="8b8a" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.metrics import classification_report</span><span id="c1ba" class="ms kc hi mo b fi nq mu l mv mw"># iterate over classifiers<br/>for name, clf in zip(names, classifiers):<br/>    clf.fit(X_train, y_train)<br/>    <br/>    y_pred = clf.predict(X_val)<br/>    <br/>    print(f"Classification Report for {name}")<br/>    print(classification_report(y_val, y_pred))<br/>    print('_'*60)</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es on"><img src="../Images/67a8a407d80e63edd8cb839e82148576.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*rfLHRgJRN7QGra0aV78RhA.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">内核SVM的分类报告</figcaption></figure><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es oo"><img src="../Images/04b62dc6bbd179dc6c3a4da72e59b6f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*qIYleDdBdTy0BuZpVDa1vw.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">朴素贝叶斯分类报告</figcaption></figure><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es op"><img src="../Images/05b47a608f1ff90daceba09a8d2d0881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*HvfI8jUxCXuXnhXi5qzd8A.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">K近邻的分类报告</figcaption></figure><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es oq"><img src="../Images/6afd363980664fec4b47247d39a3a8fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*pReMAKEsoaOyTMGc_ENOWA.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">决策树的分类报告</figcaption></figure><p id="d08b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a>(也称为<a class="ae jt" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">误差矩阵</a> )sc:是一种特定的表格布局，允许算法性能的可视化。它报告了<em class="jp">假阳性</em>、<em class="jp">假阴性</em>、<em class="jp">真阳性</em>和<em class="jp">真阴性的数量。</em></p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es or"><img src="../Images/4ffa0286bff166a902e0ed568276c9ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*w_sijeruuBey5YmDvcFFaQ.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated"><a class="ae jt" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">混淆矩阵</a></figcaption></figure><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="b84a" class="ms kc hi mo b fi mt mu l mv mw"># I will use the code from : <a class="ae jt" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html</a></span><span id="04be" class="ms kc hi mo b fi nq mu l mv mw">import matplotlib.pyplot as plt<br/>from sklearn.utils.multiclass import unique_labels<br/>from sklearn.metrics import confusion_matrix</span><span id="fefe" class="ms kc hi mo b fi nq mu l mv mw">def plot_confusion_matrix(y_true, y_pred, classes,<br/>                          normalize=False,<br/>                          title=None,<br/>                          cmap=plt.cm.Blues):<br/>    """<br/>    This function prints and plots the confusion matrix.<br/>    Normalization can be applied by setting `normalize=True`.<br/>    """<br/>    if not title:<br/>        if normalize:<br/>            title = 'Normalized confusion matrix'<br/>        else:<br/>            title = 'Confusion matrix, without normalization'</span><span id="ebc8" class="ms kc hi mo b fi nq mu l mv mw"># Compute confusion matrix<br/>    cm = confusion_matrix(y_true, y_pred)<br/>    # Only use the labels that appear in the data<br/>    #classes = classes[unique_labels(y_true, y_pred)]<br/>    if normalize:<br/>        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]<br/>        print("Normalized confusion matrix")<br/>    else:<br/>        print('Confusion matrix, without normalization')</span><span id="a1f7" class="ms kc hi mo b fi nq mu l mv mw">print(cm)</span><span id="fc72" class="ms kc hi mo b fi nq mu l mv mw">fig, ax = plt.subplots()<br/>    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)<br/>    ax.figure.colorbar(im, ax=ax)<br/>    # We want to show all ticks...<br/>    ax.set(xticks=np.arange(cm.shape[1]),<br/>           yticks=np.arange(cm.shape[0]),<br/>           # ... and label them with the respective list entries<br/>           xticklabels=classes, yticklabels=classes,<br/>           title=title,<br/>           ylabel='True label',<br/>           xlabel='Predicted label')</span><span id="5543" class="ms kc hi mo b fi nq mu l mv mw"># Rotate the tick labels and set their alignment.<br/>    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",<br/>             rotation_mode="anchor")</span><span id="d27c" class="ms kc hi mo b fi nq mu l mv mw"># Loop over data dimensions and create text annotations.<br/>    fmt = '.2f' if normalize else 'd'<br/>    thresh = cm.max() / 2.<br/>    for i in range(cm.shape[0]):<br/>        for j in range(cm.shape[1]):<br/>            ax.text(j, i, format(cm[i, j], fmt),<br/>                    ha="center", va="center",<br/>                    color="white" if cm[i, j] &gt; thresh else "black")<br/>    fig.tight_layout()<br/>    return ax</span><span id="2d98" class="ms kc hi mo b fi nq mu l mv mw">class_names = np.array([0,1])</span><span id="eca0" class="ms kc hi mo b fi nq mu l mv mw">np.set_printoptions(precision=2)</span><span id="d3c2" class="ms kc hi mo b fi nq mu l mv mw"># iterate over classifiers<br/>for name, clf in zip(names, classifiers):<br/>    clf.fit(X_train, y_train)<br/>    <br/>    y_pred = clf.predict(X_val)<br/>    <br/>    print(f"Confusion Matrix for {name}")<br/>    # Plot non-normalized confusion matrix<br/>    plot_confusion_matrix(y_val, y_pred, classes=class_names,<br/>                          title='Confusion matrix, without normalization')<br/>    # Plot normalized confusion matrix<br/>    plot_confusion_matrix(y_val, y_pred, classes=class_names, normalize=True,<br/>                          title='Normalized confusion matrix')<br/>    plt.show()<br/>    print('_'*60)</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es os"><img src="../Images/527bcb675bd728c5d9c7fe7eee8cd6c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*nnBVnebkQPfL4OyKM0_RLw.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">K近邻的混淆矩阵</figcaption></figure><p id="3bfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">那么下一步是什么</strong>，如果我们想提高性能，我们可以改变每个算法使用的参数(例如在 KNeighborsClassifier中使用不同的<em class="jp"> n_neighbors)或者我们可以使用不同的算法，例如<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank">随机森林</a>或<a class="ae jt" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>。</em></p><h1 id="ee84" class="kb kc hi bd kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku ly kw kx ky bi translated"><strong class="ak">重述</strong></h1><p id="cc89" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">我们已经到了本系列第2部分的结尾。在这一部分，我们能够了解到:</p><ol class=""><li id="f2bb" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated">第二个回归模型:<strong class="ih hj">随机梯度下降(SGD)回归器</strong></li><li id="5fd2" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">我们数据集的预处理:StandardScaler，OrdinalEncoder和LabelEncoder。</li><li id="231f" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><strong class="ih hj">数据分析</strong>和<strong class="ih hj">特征工程。</strong></li><li id="659a" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><strong class="ih hj">超参数调谐。</strong></li><li id="5017" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">用于监督学习的基本算法——分类:支持向量机、朴素贝叶斯、k-最近邻和决策树。</li><li id="2df0" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">用于分类的不同误差和矩阵:<strong class="ih hj"/><em class="jp"/><strong class="ih hj">分类矩阵</strong><em class="jp"/><strong class="ih hj">混淆矩阵。</strong></li></ol><p id="19f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们教程的第3部分，我们将讨论<strong class="ih hj">非监督学习</strong>以及如何将其与<strong class="ih hj">监督学习一起使用。</strong>我们还将学习如何进行交叉验证，以及过度拟合和欠拟合之间的区别。之后，我们会做一个关于强化学习的简报。然后就可以从<strong class="ih hj">深度学习开始了。</strong></p><p id="7fd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读！</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="5e33" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">参考链接:</h1><ol class=""><li id="fa6c" class="lz ma hi ih b ii kz im la iq ot iu ou iy ov jc me mf mg mh bi translated"><a class="ae jt" href="https://www.kaggle.com/nsrose7224/crowdedness-at-the-campus-gym" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/nsrose 7224/crowded ness-at-the-campus-gym</a></li><li id="3042" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/grid_search.html#grid-search" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/grid _ search . html # grid-search</a></li><li id="74e2" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . ensemble . randomforestregressor . html # sk learn . ensemble . randomforestregressor</a></li><li id="5357" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">【https://www.kaggle.com/c/titanic/data T4】</li><li id="0ac0" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/" rel="noopener ugc nofollow" target="_blank">https://triangle inequality . WordPress . com/2013/09/08/basic-feature-engineering-with-the-titanic-data/</a></li><li id="15be" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://www.kaggle.com/startupsci/titanic-data-science-solutions" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/startup sci/titanic-data-science-solutions</a></li><li id="bb31" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://www.kaggle.com/jeffd23/scikit-learn-ml-from-start-to-finish" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Jeff d23/sci kit-learn-ml-from-start-to-finish</a></li><li id="f8a4" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html" rel="noopener ugc nofollow" target="_blank">https://pandas . pydata . org/pandas-docs/stable/reference/API/pandas . cut . html</a></li><li id="3018" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . ordinalencoder . html</a></li><li id="400d" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . label encoder . html</a></li><li id="e2ce" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . SVM . SVC . html</a></li><li id="b257" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . naive _ Bayes。GaussianNB.html</a></li><li id="0c6a" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . neighbors . kneighborsclassifier . html</a></li><li id="75e8" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . tree . decision tree classifier . html</a></li><li id="1e8b" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/model _ evaluation . html</a></li><li id="e25b" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/model _ evaluation . html # accuracy-score</a></li><li id="73ed" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . metrics . classification _ report . html # sk learn . metrics . classification _ report</a></li><li id="50f0" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . metrics . precision _ recall _ fs core _ support . html # sk learn . metrics . precision _ recall _ fs core _ support</a></li><li id="98cf" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Positive _ and _ negative _ predictive _ values</a></li><li id="635c" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Precision_and_recall</a></li><li id="ad79" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Sensitivity_and_specificity</a></li><li id="ecab" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/F1_score</a></li><li id="210e" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://en.wikipedia.org/wiki/Harmonic_mean" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Harmonic_mean</a></li><li id="76fb" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . metrics .混淆_matrix.html#sklearn.metrics .混淆_matrix </a></li><li id="275d" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Confusion_matrix</a></li><li id="3e7c" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/model _ selection/plot _ confusion _ matrix . html</a></li></ol></div></div>    
</body>
</html>