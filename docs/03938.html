<html>
<head>
<title>Sentiment Analysis on Customer Review: Understanding NLP models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">顾客评论的情感分析:理解自然语言处理模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sentiment-analysis-on-cusomter-review-understanding-nlp-models-4d7e1f0524a?source=collection_archive---------11-----------------------#2020-02-26">https://medium.com/analytics-vidhya/sentiment-analysis-on-cusomter-review-understanding-nlp-models-4d7e1f0524a?source=collection_archive---------11-----------------------#2020-02-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/586d860a253214f5b4a793940bcd401f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-mRBgun9Bt7lajYG"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用智能手机的女性，<a class="ae iu" href="https://unsplash.com/@youxventures" rel="noopener ugc nofollow" target="_blank"> @youxventures </a></figcaption></figure><p id="14e2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">情感分析有很多行业用例，尤其是数字经济。它可以用来监测趋势，分析产品和市场研究等。用于分析文本的技术被称为自然语言处理(NLP)，它有许多迷人的方面。</p><p id="3ab4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该项目旨在:</p><ul class=""><li id="3667" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">尽可能准确的对客户的情绪进行分类；</li><li id="9f18" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">探索文本挖掘和处理的多种方式，包括TF-IDF和用Word2Vec进行文本嵌入；</li><li id="8321" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">了解每个流程的利弊。</li></ul><h2 id="b6ad" class="kh ki hi bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated"><strong class="ak">数据+ EDA </strong></h2><p id="41b3" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">我们的数据集由来自电子商务服装网站的23，486条客户评论组成。除了文本之外，还有其他特征，例如评论者的年龄、项目的类别等。我们选择忽略所有的特性，因为本文只关注NLP。因此，审查是我们关注的唯一特征。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/5b2fd2db728c521532c3ffa3f348c532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EgrxC_7JyZMkyAT6Si3f8Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">数据示例</figcaption></figure><p id="eed4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">情绪由“评级”决定，评级从1到5，5是最有利的。我们已将目标变量转换为二进制类别，即评分为4和5的评论为“积极”，评分为1至3的评论为“不积极”。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/273dc81d2adeb91b5c5479f854ac822d.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*AO7oIYg_RfToUsnfjI-E6Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">二元变换后的评分分布</figcaption></figure><p id="669b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">观察图表，正面评价比非正面评价多，正面82%，非正面18%。换句话说，无为模型通过给出正类的常数预测，可以达到82%的准确率，这比随机猜测模型(随机猜测是正还是不正)要好得多。记住这一点有助于我们确定模型的基线性能。</p><p id="ec8c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下面的文章中，我将把非正类称为负类，尽管它实际上是中性类和负类的组合。</p><p id="c1f9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们还想了解客户经常使用的词是什么。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/9740e594f937970fd9b3d7722d19ff1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*YUDJukLx06wOuke1sV2t3A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">正面评价最常用的15个词</figcaption></figure><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/0b4f8e9bb0fbf7f2ea725eb809253f93.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*M_HMZ9SG7ERMbehGTi_N6A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">负面评论最常用的15个词</figcaption></figure><p id="672d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上图显示了正面评论中最常用的词，下图显示了负面评论中最常用的词。这两个图表都是通过删除过于频繁且对文本有干扰作用的单词绘制的，例如“that”、“the”、代词等。</p><p id="cb73" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正面评论和负面评论提取的词汇(惊人地)相似。大多数评论谈论的是尺寸、合身程度、材料、面料、颜色等，这些都与服装项目的继承属性有关。也有代表顾客情感的词，如爱和喜欢，并且(令人惊讶地)在这两个类别中都有。请注意，在我们提取单词时，像“no”和“not”这样的单词被删除了。这或许可以解释为什么喜欢和喜欢会出现在负面评价中。我们将在接下来的会议中更好地理解这一现象。</p><h1 id="a233" class="lo ki hi bd kj lp lq lr kn ls lt lu kr lv lw lx ku ly lz ma kx mb mc md la me bi translated"><strong class="ak">方法论概述</strong></h1><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/876262f359aa077bdcf159d27465f0ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kEbVjLmRQ7CoZRyBsrsIIA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">NLP处理的步骤</figcaption></figure><p id="e80d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">像所有数据科学项目一样，也有针对文本数据的特征选择过程。唯一的区别是，我们还需要“提取”特征，因为没有提取，文本就不适合机器学习过程。</p><p id="4237" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将重点讨论不同的文本抽取模型的差异，包括词袋、词频—逆文档频率(TF-IDF)和文本嵌入。</p><p id="272c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于特征选择，我们将考察三种方法:词汇化、停用词去除和潜在语义分析(LSA)。</p><p id="be07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归，多项式朴素贝叶斯(MNB)，线性支持向量机(线性SVC)和随机森林模型被用于这个项目。</p><h2 id="c300" class="kh ki hi bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated"><strong class="ak">一袋话</strong></h2><p id="8a37" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">单词袋模型建立在计数的基础上:它返回一个单词计数的矩阵，每一列映射到语料库中出现的一个唯一的单词，每一行映射到每个数据点，在我们的例子中是一个评论。</p><p id="75f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于NLP初学者来说，这是一个简单易懂的模型。我们使用逻辑回归建立了我们的基线模型和单词袋模型。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/e63a828c9dc326cf1ebc4367e57a611d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Ep4uCKooYqPZ8N9ZJZgFg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">预测标签与真实标签:BOW模型</figcaption></figure><p id="157c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用我们的基线模型转换和预测了五个简短的评论。该模型得到了5个正确中3个，根据我们的输入给出了60%的准确度。它不是很理想，因为它比什么都不做的模型表现得更差。</p><p id="6010" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，单词袋模型有一些固有的缺点。特征的数量众多且难以减少。在我们的例子中，我们有超过11，000个来自单词包转换的特征。特别是在英语中，同一个词有多种形式。例如“is”“am”“are”本质上是同一个词。这就是为什么我们需要研究词汇化和停用词。</p><p id="e078" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">词汇化是一种将单词还原成原始形式的技术。在前面的例子中，“is”“am”“are”可以简化为“be”。停用字词删除旨在删除文本中出现频率过高的字词。您可以将它们视为文本数据的“噪音”。</p><p id="96e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意，处理文本的两种方式都会影响模型的性能。因此，不要把词汇化和停用词作为预处理流水线的一部分，而应该把它们作为调整步骤中的超参数。</p><p id="2e76" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们的模型中应用了词汇化和停用词去除之后，它并没有显著地提高性能。</p><h2 id="c97f" class="kh ki hi bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated"><strong class="ak"> TF-IDF </strong></h2><p id="b342" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">词频逆文档频率(TF-IDF)模型是我们探索的下一个模型。它建立在单词袋模型的基础上，但有所改进。</p><p id="ac7e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">词频是指一个词在一个文档中出现的频率，而文档频率是指该词在文档总数中出现的次数。转换后的矩阵包含每个单词的术语频率乘以逆对数文档频率的统计。</p><p id="0032" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">换句话说，如果一个单词在不同的文档中出现得太频繁，那么TF-IDF统计值往往很低。而该模型对出现在不太多文档中但对某些文档仍然重要的适度“罕见”的单词给予更多权重。</p><p id="c236" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们最好的TF-IDF模型是二元模型。我不打算在这里详细说明n-grams，但你可以谷歌一下感兴趣的。对于那些不熟悉bigram的人，你可以把bigram理解为两个单词的片段。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/57d9207166ff2073ce85399c5f0d1e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OWsUg3Y1BczT3NuWL0E6Pw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">预测标签与真实标签:TF-IDF二元模型</figcaption></figure><p id="69fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与我们的第一个模型相比，新模型的精确度提高了20%。与我们的单词袋模型相比，它可以更有效地检测负面评论。然而，我们意识到对于一个双元TF-IDF模型，特征的数量是众多的。在我们的例子中使用了50，000个特征。</p><p id="16b3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是潜在语义分析(LSA)来拯救。LSA技术背后的思想很简单:它以一些信息损失(“噪声”)为代价，将TF-IDF矩阵的有用信息压缩到较低的维度。</p><p id="ed3b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，我们将LSA应用于TF-IDF二元模型，等待奇迹发生:</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/4c62870315c5d355172fd7da3397e5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NO8EZOdME3aV7waNt4x9kA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">预测标签对真实标签:TF-IDF二元模型+ LSA模型</figcaption></figure><p id="26e9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">瞧啊。现在，通过简单的逻辑回归，准确率为100%,特征从超过50，000个减少到只有300个，不到原始特征数量的1%。</p><p id="afa3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">TF-IDF结合LSA给出了一个相当不错的预测，但我们觉得贪婪，以探索另一个特征提取模型。</p><h2 id="ce87" class="kh ki hi bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated"><strong class="ak">文本嵌入</strong></h2><p id="2399" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">如上所述，LSA技术保留了重要的信息，同时消除了模型中的“噪声”。另一方面，文本嵌入的方式不同。这项技术将每个单词转换成一个向量。使用SpaCy的文本嵌入，它返回文档向量，这些向量是文档中所有单词的平均值。</p><p id="c390" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看文本嵌入是否可以改进我们的模型:</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/09d26683840574ef6ae14a6599152e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FCK9XHfEHMSH4sEoUfz2hQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">预测标签与真实标签:文本嵌入模型</figcaption></figure><p id="9c97" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">新模型对负面评论更敏感，预测精度不如TF-IDF模型理想。这一发现有点令人惊讶，因为文本嵌入模型的效果不如TF-IDF模型。在阅读了一些参考资料后，我们明白文本嵌入对于像我们这样的简单数据集来说可能太复杂了。对于大数据应用来说，它会工作得更好。</p><h2 id="deba" class="kh ki hi bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated"><strong class="ak">结论</strong></h2><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/e428be21732551a23f649dea34f6057c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AZmJnkL-5TqDHWngEI6N2w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用我们的测试示例建模结果</figcaption></figure><p id="5957" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">应用了LSA的TF-IDF似乎是我们的赢家模型，我们的测试验证分数也证实了这一点。这对我们的任务是有效的，但结果很难解释。文本嵌入也有同样的问题，这使得模型难以直观地解释。单词袋模型是所有模型中最容易理解的，但在我们的案例中不是一个有效的模型。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/451377eb5d136fe0bea2e24de0496ada.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V_cfJAg2meSHXzTJX58ilQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用客户评论数据测试集模拟性能</figcaption></figure><p id="221c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用TF-IDF，我们在该项目中取得的最佳F1成绩为0.931。F1分数是反映模型性能的一种度量。理想模型的F1值为1，最差模型为0。</p><p id="805d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我还制作了一个应用程序，演示了这个模型使用Flask和JavaScript的能力。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/0c887e87f344901dea151118ffe97cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/1*iy3G316aXZLE9B7kOBCuww.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">烧瓶应用程序演示</figcaption></figure><p id="5ff5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最终选择的模型相当准确地检测了消费者情绪，但仍带有一些噪声。它可以通过进一步的特征处理来改善，或者通过更好的机器学习模型如深度学习模型来改善。</p><h2 id="b823" class="kh ki hi bd kj kk kl km kn ko kp kq kr jg ks kt ku jk kv kw kx jo ky kz la lb bi translated"><strong class="ak"> #讨论</strong></h2><p id="350f" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">现在我们已经教会了计算机阅读人类评论，接下来呢？</p><p id="c6f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">希望你喜欢这篇文章。</p></div></div>    
</body>
</html>