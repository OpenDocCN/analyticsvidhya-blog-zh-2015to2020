<html>
<head>
<title>BoW to BERT: Classify This!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">向伯特鞠躬:把这个分类！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bow-vs-bert-classification-1f24ba87240f?source=collection_archive---------4-----------------------#2019-10-10">https://medium.com/analytics-vidhya/bow-vs-bert-classification-1f24ba87240f?source=collection_archive---------4-----------------------#2019-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d0c612c89e3692ac658340bb12fe141a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3sTDoTYiVUzZVOwf.jpg"/></div></div></figure><p id="8d4b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">BERT在代表二元、多类和多标签/类情况的三个不同存储库上产生了最好的F1分数。然而，使用TF-IDF加权的一个热词向量使用SVM进行分类对于使用BERT来说是一个不错的选择，因为它很便宜。</p><p id="5c8a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">机器人在阅读，聊天机器人在聊天，有些甚至显然在写文章。在NLP的世界里，现在有很多的嗡嗡声和兴奋。也有很好的理由。计算机正在学习像人一样处理文本和语音。2001年的Hal可能终于来了，虽然晚了几年。开玩笑的是，这些机器人掌握的核心技能之一是对文本/语音进行动态分类，以便它们可以进一步处理。</p><p id="9509" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是这些年来，给定向量集的分类技术并没有改变多少。1995年<a class="ae jp" href="https://link.springer.com/article/10.1007/BF00994018" rel="noopener ugc nofollow" target="_blank">科尔特斯和瓦普尼克</a>的支持向量机(SVM)可能是最后一个重大进步。显然，嗡嗡声来自上游——文本被转换成矢量。通过相同的分类算法，更高质量的输入向量会带来更高的分类精度。如果您的文档向量更忠实地嵌入和反映了文档的含义，这对您有好处！您将从旧的分类器中获得更多的里程。但是谁在上游制造所有这些好的媒介呢？当然不是塞普蒂马教授！</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es jq"><img src="../Images/09ffb919668194b5421ff49903bfcd50.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/0*iD8wDjtopKV64Q6s.jpg"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated"><a class="ae jp" href="http://jakespot.co.za/septima-vector/" rel="noopener ugc nofollow" target="_blank">塞普蒂玛教授矢量</a></figcaption></figure><p id="a316" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">原来是伯特和他的朋友ELMO一伙。在早先的帖子<a class="ae jp" href="http://xplordat.com/2019/09/23/bow-to-bert/" rel="noopener ugc nofollow" target="_blank"> BoW to BERT </a>中，我们已经看到了单词vectors是如何进化来适应它们所处的环境的。我们观察了同一个单词的两个向量之间的相似性或缺乏相似性，但是在不同的上下文中。</p><p id="84da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这篇文章的目的是要看看文字向量的灵活性给实际的下游任务——文档分类——带来了什么不同。这里有一个简单的大纲。</p><ol class=""><li id="12df" class="jz ka hi is b it iu ix iy jb kb jf kc jj kd jn ke kf kg kh bi translated">以三个不同的文档库为例。来自斯坦福的<a class="ae jp" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank">电影评论</a>(用于二元情感分类)、通过scikit-learn的<a class="ae jp" href="https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html#" rel="noopener ugc nofollow" target="_blank"> 20新闻语料库</a>(用于多类别分类)以及通过NLTK的<a class="ae jp" href="https://www.nltk.org/book/ch02.html" rel="noopener ugc nofollow" target="_blank">路透社语料库</a>(用于多类别&amp;多标签分类)。对每个存储库重复2到4。</li><li id="fd62" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">使用1-hot &amp; fastText单词向量构建BoW文档向量。用逻辑回归和SVM分类。</li><li id="adbe" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">微调BERT几个时期(这里是5个),同时对顶层的分类标记[CLS]中的矢量进行分类</li><li id="7ff5" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn ke kf kg kh bi translated">比较在2和3中获得的(加权)F1分数。</li></ol><p id="2497" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">用scikit-learn实现逻辑回归和SVM。使用huggingface的<a class="ae jp" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> transformers </a>模块将BERT实现为Tensorflow 2.0层。让我们开始吧。我们将在这里查看一些代码片段，但是完整的代码可以从<a class="ae jp" href="https://github.com/ashokc/BoW-vs-BERT-Classification" rel="noopener ugc nofollow" target="_blank"> github </a>获得。</p><h2 id="aceb" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jb ky kz la jf lb lc ld jj le lf lg lh bi translated">1.用于分类的文档向量</h2><p id="277e" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">单词包(BoW)是一种从文档中的单词(它们的数字向量是特定的，1-hot，fastText等)构建文档向量的方法。我们已经在之前的文章中讨论过这个问题。这是我们得到的等式。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ln"><img src="../Images/7dfd1fba88a3b68381bcc9df507f7c07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*liQ6P3xFOTNFSSy_.jpg"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">等式1:当w_i是one-hot时，文档的BoW向量是单词向量的加权和，那么p = N。当从fastText、Glove等获得w_i时，p &lt;&lt; N</figcaption></figure><p id="a3a7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">BERT can be used to generate word vectors and Equation 1 above <em class="jo">可以用于获得文档向量。但是当分类是下游目的时，BERT不需要从单词向量构建文档向量。从顶层的“[CLS]”标记射出的向量用作针对特定分类目标微调的代表</em>文档向量的<em class="jo">。这是一个更小的BERT模型的示意图(来自<a class="ae jp" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank"> Jay Alammar </a>),它采用了12层，1.1亿个参数，最多510个单词序列。这里，用于分类目的的单词嵌入和CLS令牌向量的长度为768。</em></p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/eb5cb50b439039bf558da5546c95f796.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/0*9goAaqZwzlw0c5qW.jpg"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">图一。<a class="ae jp" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="2b8e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">用BERT转移学习和微调</strong></p><p id="9964" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">诸如来自fastText的那些公布的词向量在大量文本上被训练。我们可以在文档中使用它们。也就是说，这些词向量是可转移的。但是他们不会从我们的文件中吸收任何特定的知识。他们也不知道我们会用它们做什么。也就是说，它们既是静态的，也是任务不可知的。我们可以为我们的文档语料库构建定制的fastText单词向量。自定义向量嵌入语料库特定知识，但<em class="jo">不能转移</em>到不同的语料库。它们也是任务不可知的。</p><blockquote class="lp lq lr"><p id="1c19" class="iq ir jo is b it iu iv iw ix iy iz ja ls jc jd je lt jg jh ji lu jk jl jm jn hb bi translated">针对特定的文档语料库和特定的下游目标，微调通用的、可转移的单词向量是最新一批语言模型(如BERT)的一个特征。</p></blockquote><p id="2c03" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">BERT可以为句子中的任何单词生成数字向量(当然不会超过510个单词),无需额外的训练。但是在可能的情况下，用<em class="jo">我们的</em>文档对照<em class="jo">我们的</em>目标进一步培训BERT <em class="jo">一点</em>是有利的。这样获得的单词(和CLS令牌)向量将会学习一些新的技巧来更好地完成我们的任务和文档。请注意，我们说当可能的时候。即使是更小的BERT也是一头拥有1.1亿个参数的野兽。对于大型文档存储库，微调BERT的成本会非常高。幸运的是，我们的回购并不多。</p><h2 id="e7f4" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jb ky kz la jf lb lc ld jj le lf lg lh bi translated">2.文档和标签向量</h2><p id="fd46" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">文档用一个简单的正则表达式清理，如下所示。用一个空格字符将这些标记连接起来将会产生一个干净的文档。</p><pre class="jr js jt ju fd lv lw lx ly aw lz bi"><span id="fd95" class="kn ko hi lw b fi ma mb l mc md">def tokenize (text): # no punctuation &amp; starts with a letter &amp; between 2-15 characters in length<br/>    tokens = [word.strip(string.punctuation) for word in RegexpTokenizer(r'\b[a-zA-Z][a-zA-Z0-9]{2,14}\b').tokenize(text)]<br/>    return  [f.lower() for f in tokens if f]</span></pre><p id="30af" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">文档的字数对我们来说很重要，因为BERT每个文档的字数限制在510个字以内。实际上是512，但是另外两个被特殊的开始([CLS])和结束([SEP])标记占用。图2显示了这些回复的要点，包括字数的分布。请注意，大量的文件都在510字以下，这意味着伯特应该很高兴。但是BERT是资源密集型的，在我的电脑上，在遇到OOM问题之前，它只能处理每个文档约175个单词(批量大小为32)。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/eb2248ea438e2cc57975d5f151f9dee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*V4RXZ2PoKOMz5G0k.jpg"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">图二。文献库的命脉。所有文档都用于分类，但较长的文档会被截断为前X个单词。逻辑回归和SVM可以处理所有的单词，但是我们需要确保使用相同处理的文档在BERT和非BERT对应物之间进行双向比较。</figcaption></figure><p id="536b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图3显示了回购的分类标签分布。知道是相当重要的。在解释偏斜数据集上的分类器的性能时必须小心。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/08c663b154388dc112a2919735f21c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*baPKtjrVn569N9KL.jpg"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">图3。阶级分布。路透社的数据集是有偏差的，一些类别只有2个文档，而另一些类别有4000个文档。另外两个数据集相当平衡。</figcaption></figure><p id="e6dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在所有情况下，文档的标签向量与类的数量一样长。它将在对应于它所属的类的索引处具有1，而在其他地方具有0。因此，reuters repo中的一个文档的标签向量将有90个长度，值1的数量与它所属的类的数量一样多。另外两个repos中的标签向量是one-hot，因为它们的文档只能属于一个类。</p><h2 id="0424" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jb ky kz la jf lb lc ld jj le lf lg lh bi translated">3.用弓分类</h2><p id="e8fa" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">对于逻辑回归和SVM，我们按照等式1构建BoW向量。Tf-idf砝码用于<em class="jo"> W^j_i </em>。为<em class="jo"> w_i </em>尝试One-hot和fastText字向量。对于快速文本，我们使用300维向量，即等式1中的p = 300。下面是用一个热门词向量构建tf-idf向量的一段代码。</p><pre class="jr js jt ju fd lv lw lx ly aw lz bi"><span id="f09f" class="kn ko hi lw b fi ma mb l mc md">X = docs['train'] + docs['test'] # All docs<br/>X=np.array([np.array(xi) for xi in X]) # rows: Docs. columns: words<br/>vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=1).fit(X)<br/>word_index = vectorizer.vocabulary_ # Vocabulary has all words<br/>train_x = vectorizer.transform(np.array([np.array(xi) for xi in docs['train']])) # sparse tf-idf vectors using 1-hot word vectors<br/>test_x = vectorizer.transform(np.array([np.array(xi) for xi in docs['test']])) # sparse tf-idf vectors using 1-hot word vectors</span></pre><p id="592c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当对<em class="jo"> w_i </em>使用fastText单词向量时，我们从公布的单词向量中得到嵌入矩阵<em class="jo"> W </em>(每行代表长度为<em class="jo"> p </em>的单词向量)并将其与上面的tf-idf稀疏矩阵相乘。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/058619c657426cbe70d0f4810b5f6430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9QAiWFqbiT3wbhFx.jpg"/></div></div></figure><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/8a6c0dcfebbe18b1b1dbcfdd6735874b.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*pMwRVLM3GKK8lXNR.jpg"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">等式2。使用快速文本向量从稀疏X构建密集向量Z</figcaption></figure><pre class="jr js jt ju fd lv lw lx ly aw lz bi"><span id="5ecd" class="kn ko hi lw b fi ma mb l mc md">train_x = sparseMultiply (train_x, embedding_matrix)<br/><br/>def sparseMultiply (sparseX, embedding_matrix):<br/>    denseZ = []<br/>    for row in sparseX:<br/>        newRow = np.zeros(wordVectorLength)<br/>        for nonzeroLocation, value in list(zip(row.indices, row.data)):<br/>            newRow = newRow + value * embedding_matrix[nonzeroLocation]<br/>        denseZ.append(newRow)<br/>    denseZ = np.array([np.array(xi) for xi in denseZ])<br/>    return denseZ</span></pre><p id="93ad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用这些向量通过逻辑回归或SVM进行分类对于scikit-learn来说很简单。reuters语料库是多类和多标签的，因此我们需要将模型包装在OneVsRestClassifier中。由此得出的多标签混淆矩阵在此处汇总，得到一个加权混淆矩阵。</p><pre class="jr js jt ju fd lv lw lx ly aw lz bi"><span id="d341" class="kn ko hi lw b fi ma mb l mc md">if (clf == 'svm'):<br/>    model = LinearSVC(tol=1.0e-6,max_iter=20000)<br/>elif (clf == 'lr'):<br/>    model = LogisticRegression(tol=1.0e-6,max_iter=20000)<br/>if (docrepo == 'reuters'):<br/>    classifier = OneVsRestClassifier(model)<br/>    classifier.fit(train_x, y['train'])<br/>    predicted = classifier.predict(test_x)<br/>    mcm = multilabel_confusion_matrix(y['test'], predicted)<br/>    tcm = np.sum(mcm,axis=0) # weighted confusion matrix<br/>else:<br/>    train_y = [np.argmax(label) for label in y['train']]<br/>    test_y = [np.argmax(label) for label in y['test']]<br/>    model.fit(train_x, train_y)<br/>    predicted = model.predict(test_x)<br/>    cm = confusion_matrix(test_y, predicted)</span></pre><h2 id="895f" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jb ky kz la jf lb lc ld jj le lf lg lh bi translated">4.用BERT分类</h2><p id="fd80" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">正如我们之前所说的，BERT不需要BoW向量来进行分类。它将它们构建为针对特定分类目标的微调的一部分。BERT有自己的记号赋予器和词汇。我们使用它的标记器，并以BERT期望的方式准备文档。</p><p id="e54f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的代码片段获取一个文档列表，对它们进行标记，生成BERT用作输入的<em class="jo">id、掩码、</em>和<em class="jo">段</em>。每个文档产生3个列表，每个列表的长度与<em class="jo"> max_seq_length </em>一样，所有文档都一样。长于<em class="jo"> max_seq_length </em>标记的文档将被截断。短于<em class="jo"> max_seq_length </em>标记的文档用0进行后置填充，直到它们具有<em class="jo"> max_seq_length </em>标记。<em class="jo"> max_seq_length </em>本身最大限制为510。</p><pre class="jr js jt ju fd lv lw lx ly aw lz bi"><span id="8cdd" class="kn ko hi lw b fi ma mb l mc md">import tensorflow as tf # 2.0<br/>from transformers import *<br/><br/>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #  BertTokenizer from the transformers module<br/>def prepareBertInput(tokenizer, docs):<br/>    all_ids, all_masks, all_segments= [], [], []<br/>    for doc in tqdm(docs, desc="Converting docs to features"):<br/>        tokens = tokenizer.tokenize(doc)<br/>        if len(tokens) &gt; max_seq_length:<br/>            tokens = tokens[0 : max_seq_length]<br/>        tokens = ['[CLS]'] + tokens + ['[SEP]']<br/>        ids = tokenizer.convert_tokens_to_ids(tokens)<br/>        masks = [1] * len(ids)<br/>        # Zero-pad up to the sequence length.<br/>        while len(ids) &lt; max_seq_length:<br/>            ids.append(0)<br/>            masks.append(0)<br/>        segments = [0] * max_seq_length<br/>        all_ids.append(ids)<br/>        all_masks.append(masks)<br/>        all_segments.append(segments)<br/>    encoded = [all_ids, all_masks, all_segments]<br/>    return encoded</span></pre><p id="34cb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的图4运行了上面的几个句子。文档的每个实际标记列表前都有一个特殊的标记“[CLS]”，后附加“[SEP]”。id只是来自BERT词汇表的整数映射。掩码0表示将被忽略的填充令牌。对于我们的单个文档分类问题，片段只是零向量。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/2a1fef2c74a374cf979864458b01ade8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ioLzMn84MedkZh67.jpg"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">图4。为伯特准备文件。绿色的掩码表示活动令牌。</figcaption></figure><p id="09d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jp" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> transformers </a>模块可以加载预先训练好的BERT模型作为tensor flow 2.0<a class="ae jp" href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Model" rel="noopener ugc nofollow" target="_blank">TF . keras . model</a>子类对象。这使得在围绕BERT构建定制模型时，可以无缝地与其他Keras层集成。在我们定义完整的模型之前，我们应该考虑reuters repo的多标签情况。</p><pre class="jr js jt ju fd lv lw lx ly aw lz bi"><span id="4ab6" class="kn ko hi lw b fi ma mb l mc md">if (docrepo == 'reuters'):<br/>  multiLabel = True<br/>  tfMetric = tf.keras.metrics.BinaryAccuracy()<br/>  tfLoss = tf.losses.BinaryCrossentropy()<br/>  activation = 'sigmoid'<br/>  allMetrics = [tfMetric, tf.metrics.FalsePositives(), tf.metrics.FalseNegatives(), tf.metrics.TrueNegatives(), tf.metrics.TruePositives(), tf.metrics.Precision(), tf.metrics.Recall()]<br/>else:<br/>  multiLabel = False<br/>  tfMetric = tf.keras.metrics.CategoricalAccuracy()<br/>  tfLoss = tf.losses.CategoricalCrossentropy()<br/>  activation = 'softmax'<br/>  allMetrics = [tfMetric]</span></pre><ul class=""><li id="f544" class="jz ka hi is b it iu ix iy jb kb jf kc jj kd jn mj kf kg kh bi translated">在多标签的情况下，一个标签的存在不应该影响另一个标签的存在/不存在。所以最后的致密层需要一个sigmoid激活。如果任何标签的预测分数大于0.5，则该文档被分配该标签。</li><li id="28bd" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn mj kf kg kh bi translated">Softmax激活适用于单标签情况。它迫使所有概率的总和为1，从而在它们之间建立一种依赖关系。这对于单个标签的情况很好，因为标签是互斥的，并且我们选择具有最高预测概率的标签作为预测标签。</li><li id="99dd" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn mj kf kg kh bi translated">二进制度量查看每个标签的预测概率，如果它大于0.5，则它记录命中(比如1)或未命中(比如0)。因此，单个预测向量产生多个1和0，有助于所有文档和标签的整体预测能力。</li><li id="fdf8" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn mj kf kg kh bi translated">另一方面，分类度量寻找具有最大预测概率的标签，并产生单个1(对于命中)或单个0(对于未命中)。这适用于单标签的情况。</li><li id="5b7e" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn mj kf kg kh bi translated">其他TF提供的度量，例如<em class="jo">TF . metrics . false positives()</em>采用默认阈值0.5作为概率，因此适合于对多标签情况进行跟踪。</li></ul><p id="a1ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有了对方式讨论，我们就可以定义模型了。</p><pre class="jr js jt ju fd lv lw lx ly aw lz bi"><span id="2916" class="kn ko hi lw b fi ma mb l mc md">def getModel():<br/>    in_id = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32', name="bert_input_ids")<br/>    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32', name="bert_input_masks")<br/>    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), dtype='int32', name="bert_segment_ids")<br/>    inputs = [in_id, in_mask, in_segment]<br/>    top_layer_vectors, top_cls_token_vector = TFBertModel.from_pretrained(BERT_PATH, from_pt=from_pt)(inputs)<br/>    predictions = tf.keras.layers.Dense(len(labelNames), activation=activation,use_bias=False)(top_cls_token_vector)<br/>    model = tf.keras.Model(inputs=inputs, outputs=predictions)<br/>    model.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08, clipnorm=1.0), loss=tfLoss, metrics=allMetrics)<br/>    return model</span></pre><p id="436c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上述代码片段中的第6行将“未封装的L-12_H-768_A-12”模型作为一个层加载，并接受准备好的输入。它采用了12层(图1所示的变压器模块)，12个注意力头和1.1亿个参数。每个令牌在每层中都有一个768长的数字向量表示。“top_cls_token_vector”是从顶层的“[CLS]”令牌发出的768长的向量。</p><p id="ace6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下是上述用于电影二进制分类的Keras模型的示意图，任何文档最多使用175个单词。图像显示177是因为我们前面提到的两个特殊标记。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/ddcd2f863ba8046a43e549c87012b2a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZaN_gvSR9Q8CNi5K.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">图5。用BERT分类电影评论的Keras模型</figcaption></figure><h2 id="8cde" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jb ky kz la jf lb lc ld jj le lf lg lh bi translated">5.结果</h2><p id="de76" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">为了让所有运行都使用完全相同的文档，以及培训/测试拆分，我们提前做了准备。shell脚本运行存储库和分类器的各种组合，保存结果以供分析。弓法有36种组合。</p><ul class=""><li id="c17f" class="jz ka hi is b it iu ix iy jb kb jf kc jj kd jn mj kf kg kh bi translated">3篇报道(电影、20篇新闻、路透社)</li><li id="0044" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn mj kf kg kh bi translated">2个分类器(逻辑回归，SVM)</li><li id="52b0" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn mj kf kg kh bi translated">两种类型的单词向量(One-hot，fastText)，以及</li><li id="92a0" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn mj kf kg kh bi translated">每个文档考虑的最大字数有3个不同的值(175，510，全部)。考虑这些的原因是为了能够比较head-2-head与BERT，后者最多只能处理510个令牌，由于OOM问题，我的桌面上更像是175个。</li></ul><p id="e6ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于BERT，只有3次运行—每次回购运行一次。尽管BERT被限制为510个令牌，但在我的桌面上的实际限制是在32的批量中只允许175个令牌。从s3加载基本“无外壳_L-12_H-768_A-12”模型，并针对5个时期进行微调。对学习速度有一些影响，将单独探讨。</p><pre class="jr js jt ju fd lv lw lx ly aw lz bi"><span id="576c" class="kn ko hi lw b fi ma mb l mc md">history = model.fit(encoded['train'], y['train'], verbose=2, shuffle=True, epochs=5, batch_size=32)<br/>evalInfo = model.evaluate(encoded['test'], y['test'], verbose=2)</span></pre><p id="db27" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的图6是我们要找的东西，整个博客都在找它。我们比较的是所有标签的加权F1分数，因为各标签的支持度差异很大，特别是在路透社的情况下，如图3所示。这里有一些简单的结论。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8bc3f91ef4e5935e7c50775f7b9e3cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*i9URrrJ_-9nXb1CA.jpg"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">图6。伯特在所有情况下都是群体中的领导者，即使在某些情况下不是很突出。</figcaption></figure><ul class=""><li id="37a3" class="jz ka hi is b it iu ix iy jb kb jf kc jj kd jn mj kf kg kh bi translated">在所有情况下，伯特都能获得最好的F1成绩。</li><li id="20c3" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn mj kf kg kh bi translated">使用更多的令牌，逻辑回归和SVM稍微提高他们的分数。但拥有175枚代币的伯特依然是领头羊。正如我们在图2中看到的那样，大多数文档都少于175个单词。</li><li id="59d0" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn mj kf kg kh bi translated">相对于快速文本，一键单词向量可能更受欢迎。事实上，在热门词汇向量上打个蝴蝶结，用SVM作为分类器会产生相当不错的F1分数。</li><li id="6fbd" class="jz ka hi is b it ki ix kj jb kk jf kl jj km jn mj kf kg kh bi translated">也许这是一个简单、廉价的方法，但是BERT的优异表现也伴随着比BoW高得多的资源利用率。</li></ul><p id="14b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们之前顺便提到过，微调期间的学习速率对使用BERT获得的结果有一些影响。下面的图7说明了这一点。使用BERT时优化超参数也是一个计算挑战。</p><figure class="jr js jt ju fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/5e858e9d200ac39983b9e0c048a71ba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/0*TIKw9cDlhNSyD89F.jpg"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">图7。学习率必须足够小，以便对BERT进行微调。稍微调整一下学习速度可以提高F1成绩。</figcaption></figure><h2 id="48ff" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jb ky kz la jf lb lc ld jj le lf lg lh bi translated">6.结论</h2><p id="583e" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">当<em class="jo">分类</em>是目标时，使用tf-idf加权一个热词向量的BoW和传统方法(如SVM)应该是首先尝试的。它将建立一个基线，我们的目标是用更新的方法，如BERT来击败它。BERT以一定的代价产生高质量的结果。但是更快、更轻的BERT版本正在不断开发中，而且云计算选项也越来越便宜。此外，BERT嵌入并不局限于生成用于分类的句子向量，对于其他用例，one-hot和fastText嵌入对BERT没有任何影响。</p><p id="0502" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们以此结束这篇文章。在接下来的文章中，我们将看看如何使用BoW和BERT进行集群。</p></div></div>    
</body>
</html>