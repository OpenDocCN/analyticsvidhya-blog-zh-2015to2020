<html>
<head>
<title>Understanding the GPT-2 Source Code Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解新GPT协议源代码第3部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-gpt-2-source-code-part-3-9796a5a5cc7c?source=collection_archive---------0-----------------------#2019-05-21">https://medium.com/analytics-vidhya/understanding-the-gpt-2-source-code-part-3-9796a5a5cc7c?source=collection_archive---------0-----------------------#2019-05-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e65a421434f0bd70afc8c5a9e1754d1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ceUS920Pza9WHJaWd-iebA.jpeg"/></div></div></figure><p id="8e79" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嗨！这是研究GPT-2的源代码的继续。你可以在这里找到第一部分和第二部分<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-1-4481328ee10b">，在这里</a>找到<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-2-4a980c36c68b">。</a></p><p id="c77e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，我将在研究sample.py和model.py的同时，尝试介绍GPT-2的模型是如何工作的</p><h1 id="4c35" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">样本序列</h1><p id="919d" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">sample.py的主要功能是在给定条件/输入的情况下生成文本输出。这是由sample.py中的sample_sequence完成的。</p><h1 id="d576" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">输入</h1><p id="a7da" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">样本序列的输入如下所示</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1448" class="lb jq hi kx b fi lc ld l le lf">def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):</span></pre><p id="f18a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">*部分只是强制函数的用户直接指定参数。例如，给定函数</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ef51" class="lb jq hi kx b fi lc ld l le lf">def a(c):<br/>    print(c)<br/>def b(*,c):<br/>    print(c)</span></pre><p id="6a01" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">而第一个函数可以被称为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ff73" class="lb jq hi kx b fi lc ld l le lf">a(“hi”)</span></pre><p id="a6af" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">并且只输出hi，第二个函数必须被调用为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="0f90" class="lb jq hi kx b fi lc ld l le lf">b(c=”hi”)</span></pre><p id="f492" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">才能得到同样的结果！</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="c523" class="lb jq hi kx b fi lc ld l le lf">if start_token is None:<br/>        assert context is not None, 'Specify exactly one of start_token and context!'<br/>    else:<br/>        assert context is None, 'Specify exactly one of start_token and context!'<br/>        context = tf.fill([batch_size, 1], start_token)</span></pre><p id="9f88" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们在第1部分中看到的，这部分是针对generate _ unconditional _ samples . py和interactive _ conditional _ samples . py的，如果我们要生成无条件样本(没有输入的样本)，输入文本，上下文将被设置为一个由start标记初始化的张量。然而，否则，传入的编码文本将作为输入给出！在这里，在重新检查interactive _ conditional _ samples . py的代码后，我发现OpenAI没有决定在作为输入给出的传入文本的开头添加一个开始标记，这很有趣。相反，他们只是将其编码为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="0838" class="lb jq hi kx b fi lc ld l le lf">raw_text = input("Model prompt &gt;&gt;&gt; ")<br/>            while not raw_text:<br/>                print('Prompt should not be empty!')<br/>                raw_text = input("Model prompt &gt;&gt;&gt; ")<br/>            context_tokens = enc.encode(raw_text)</span></pre><p id="1d47" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我发现这很有趣，因为我认为你总是需要一个启动令牌！</p><h1 id="8806" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">为什么要有启动令牌？(知道的可以跳过！)</h1><p id="d60d" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我认为在阅读第1部分为什么我们必须有开始标记时可能会有点困惑。所以，我在这里试着解释一下！开始标记，顾名思义，表示文本的开始。例如，对于文本“我很高兴”，开始标记在前面，结束标记在后面，并以“<start_token>我很高兴<end_token>”结束。我们这样做的原因是，当我们加载一个文本时，每个文本都有不同的长度！</end_token></start_token></p><p id="ddc1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以，在我们把字符串编码成数字之后，这里通常的方法是在末尾加0。例如，如果I是1，am是2，happy是3，那么它将被编码为1230000…然而，这种方法的一个问题是，机器本身在学习序列的过程中，会对文本的开始和结束位置感到困惑。</p><p id="f50e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，如果大多数文本都很短，并且有一个长的传入文本，如1111111111113452000..，那么很有可能由于机器仅在11113452给出的位置经历了0，它将开始忽略这些数字，并且总体上导致训练的相当糟糕的结果！这就是开始标记被引入的原因。它们表示字符串的开始，因此机器学习算法知道文本开始的位置和结束标记，在文本的结尾表示文本结束的位置。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="daea" class="lb jq hi kx b fi lc ld l le lf">def step(hparams, tokens, past=None):<br/>        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)</span><span id="3f68" class="lb jq hi kx b fi lg ld l le lf">logits = lm_output['logits'][:, :, :hparams.n_vocab]<br/>        presents = lm_output['present']<br/>        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))<br/>        return {<br/>            'logits': logits,<br/>            'presents': presents,<br/>        }</span></pre><p id="e3d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，在sample_sequence函数中，定义了这个阶跃函数。</p><p id="c9db" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该函数的第一行调用模型文件中的模型函数，然后返回一个tensors lm_outputs字典，其中包含两个键“logits”和“present”。我还不知道它们是什么，但是当我们深入研究代码时，我确信我们会发现它们是什么。</p><h1 id="3a68" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">什么是张量？(知道的可以跳过！)</h1><p id="41ac" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">对于不那么熟悉Tensorflow的人来说，我觉得“张量”这个词有点神秘。它就像是图形的一个组成部分。在Tensorflow中，当您编写以下代码时，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="9b85" class="lb jq hi kx b fi lc ld l le lf">a = tf.constant(1)<br/>b = tf.constant(1)<br/>c = a + b</span></pre><p id="14a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">c的值不会是2。(TLDR；至少在Tensorflow 2.0更新之前没有，但由于这段代码是在此之前编写的，所以我忽略了它)</p><p id="505a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">事实上，它的值直到运行时才会被正确设置。它唯一知道的是，c是a和b相加后的值。</p><p id="2417" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以做的是设置a和b的值，以查看c的结果。为此，我们启动一个会话。张量流中的会话允许我们实际执行张量中的操作。所以，在这种情况下，我们可以通过做一个session来评估c的值。是这样做的。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="3ffb" class="lb jq hi kx b fi lc ld l le lf">with tf.Session() as sess:<br/>    print(sess.run([c]))</span></pre><p id="33dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">和[2]应该打印出来。如果我们想给a和b输入新的值，我们可以这样做</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="7660" class="lb jq hi kx b fi lc ld l le lf">with tf.Session() as sess:<br/>    sess.run([c],{a:2,b:3})</span></pre><p id="4487" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">和[5]将被输出。</p><h1 id="656e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">查看model.py内部</h1><p id="efb0" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">现在我们知道了model.py的模型函数输出张量，让我们看看model.py是如何设置的，并可能看到一些正在使用的算法！</p><h1 id="aaa8" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">张量流观测仪</h1><p id="8816" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">当我们看模型函数的顶部时，我们看到</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="317f" class="lb jq hi kx b fi lc ld l le lf">def model(hparams, X, past=None, scope=’model’, reuse=False):<br/> with tf.variable_scope(scope, reuse=reuse):</span></pre><p id="240a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Tensorflow在这里定义的变量作用域主要是为了便于调试。当作用域中一个名为x的张量出错时，如果作用域名设置为hello，那么错误会在错误消息中将该张量称为hello/x。</p><p id="010f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，虽然我还没有在这个用例中使用它，但它也可以用于<a class="ae jo" href="https://www.tensorflow.org/api_docs/python/tf/variable_scope" rel="noopener ugc nofollow" target="_blank">共享变量</a>！这就是重用参数的用武之地。举一个文档中的例子，它可以以如下方式使用！</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="b88b" class="lb jq hi kx b fi lc ld l le lf">def foo():<br/>  with tf.variable_scope("foo", reuse=tf.AUTO_REUSE):<br/>    v = tf.get_variable("v", [1])<br/>  return v<br/><br/>v1 = foo()  # Creates v.<br/>v2 = foo()  # Gets the same, existing v.<br/>assert v1 == v2</span></pre><h1 id="8fdf" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">提取批量和序列长度</h1><p id="9bb4" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">模型函数中的下一行是</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="96d2" class="lb jq hi kx b fi lc ld l le lf">batch, sequence = shape_list(X)</span></pre><p id="bdc8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，当我们回头看sample.py时，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="c8ed" class="lb jq hi kx b fi lc ld l le lf">def step(hparams, tokens, past=None):<br/>        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)</span></pre><p id="a5b8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们看到X被设置为称为令牌的步骤的输入。现在，让我们研究一下shape_list函数，看看它到底是做什么的。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f0a5" class="lb jq hi kx b fi lc ld l le lf">def shape_list(x):<br/>    """Deal with dynamic shape in tensorflow cleanly."""<br/>    static = x.shape.as_list()<br/>    dynamic = tf.shape(x)<br/>    return [dynamic[i] if s is None else s for i, s in enumerate(static)]</span></pre><p id="6a3f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于我不太清楚它是如何工作的，我进入了idle(如果你安装了python，只需在开始菜单中输入Idle就可以打开)并尝试了一些东西。</p><p id="7dd5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">开始之前，我看了看这条线</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="9bca" class="lb jq hi kx b fi lc ld l le lf">return [dynamic[i] if s is None else s for i, s in enumerate(static)]</span></pre><p id="ef80" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从中我们可以看出，如果静态的维度都不是none，那么函数只是返回x.shape.as_list()。因此，我做了一个张量如下</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="cc5e" class="lb jq hi kx b fi lc ld l le lf">example = tf.placeholder("float", [None, 5])</span></pre><p id="e51e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这用一个非5次方的张量初始化示例。这里的“无”表示在会话开始之前，它可以接受任何维作为第一维。</p><p id="0006" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我首先输出example.shape.as_list()，它输出</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="042e" class="lb jq hi kx b fi lc ld l le lf">[None, 5]</span></pre><p id="8fc3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于tf.shape(示例)，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="2951" class="lb jq hi kx b fi lc ld l le lf">&lt;tf.Tensor ‘Shape_5:0’ shape=(2,) dtype=int32&gt;</span></pre><p id="e3fb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">被退回。返回的第一维</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="3770" class="lb jq hi kx b fi lc ld l le lf">&lt;tf.Tensor 'strided_slice_2:0' shape=() dtype=int32&gt;</span></pre><p id="d807" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第二维度又回来了</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="b801" class="lb jq hi kx b fi lc ld l le lf">&lt;tf.Tensor ‘strided_slice_3:0’ shape=() dtype=int32&gt;</span></pre><p id="a0b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我把它传递给函数时，</p><p id="cf6d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[<tf.tensor shape="()" dtype="int32">，5]</tf.tensor></p><p id="1652" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">被输出。起初，我不太明白这样做的用处，但我注意到的一件事是，这允许批量大小和序列长度，形状列表函数的输出可以在运行时设置，因为它是一个张量！</p><p id="73a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，我认为未来的ML学生需要注意的一件事是，我们仍然需要意识到，我们不能用未定义大小的权重和偏差来训练网络。</p><h1 id="6c90" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">神经网络中最基本的概念(知道的可以跳过！)</h1><p id="cc67" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">神经网络中最基本的概念是矩阵乘法和矩阵加法。假设我们有一堆维数为(无，10)的数字作为输入。(TLDR，无一是批量大小)</p><p id="32eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设我们想知道这十个数字是好是坏，用0还是1来表示。</p><p id="28c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然可能有许多方法可以做到这一点，但ML工程师应该想到的第一个模型是直接将10维缩减为1维。这是通过将输入乘以维度(10，1)的权重矩阵并添加维度(1)的偏差来完成的。(我可能会在另一篇文章中讨论线性代数，但我不认为我会在这里讨论)。输出是(无，1)</p><p id="70a6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这允许维度按比例缩小，并且使用诸如梯度下降等技术来训练网络。然而，需要注意的一个重要方面是权重矩阵的维数和偏差是恒定的。这就是为什么他们是可训练的。因此，我们不能使用OpenAI的技术来秘密地将可变大小的维度设置为权重矩阵或偏差的维度之一。我决定写这个，因为，嗯，我犯了这些错误！</p><p id="6776" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">模型函数的下一行是</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="5895" class="lb jq hi kx b fi lc ld l le lf">wpe = tf.get_variable(‘wpe’, [hparams.n_ctx, hparams.n_embd],<br/> initializer=tf.random_normal_initializer(stddev=0.01))<br/>wte = tf.get_variable(‘wte’, [hparams.n_vocab, hparams.n_embd],<br/> initializer=tf.random_normal_initializer(stddev=0.02))</span></pre><p id="fb79" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">tf.get_variable是可以训练的变量，初始值设定项是这些变量被设置的初始值。在这种情况下，他们将其设置为均值为0，标准差为0.01或0.02的正态分布，我发现这很有趣，因为我倾向于将其设置为0或1。现在我想起来，直觉上这是有道理的，但除了经过测试，它提高了性能之外，我不认为有什么合理的解释。反正我自己去试试。</p><p id="e153" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然我不确定wpe和wte代表什么，但是我们可以研究hparams有什么值。</p><p id="a88e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">默认参数如下所示</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="e7da" class="lb jq hi kx b fi lc ld l le lf">def default_hparams():<br/>    return HParams(<br/>        n_vocab=0,<br/>        n_ctx=1024,<br/>        n_embd=768,<br/>        n_head=12,<br/>        n_layer=12,<br/>    )</span></pre><p id="8595" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">n_embd表示嵌入大小。</p><h1 id="8bc3" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">什么是嵌入(知道的可以跳过！)</h1><p id="2ea0" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">嵌入基本上是一种将每个数字表示为向量的方法。这使得机器学习算法能够理解单词之间的异同。比如让我们看看猫狗vs车房子之类的词。由于猫和狗在单词方面非常相似，我们希望代表它们的向量比像house和cat这样的单词更接近。</p><p id="f886" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嵌入大小给出了每个向量的大小，是768，这是相当大的！</p><h1 id="0d8f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">一个奇怪的属性</h1><p id="edb3" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">然而，默认参数中一个奇怪的属性是n_vocab是0。当我们看wte是如何定义的，我们发现n _ vocabs在这个维度中</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ca0e" class="lb jq hi kx b fi lc ld l le lf">wte = tf.get_variable(‘wte’, [hparams.n_vocab, hparams.n_embd],<br/> initializer=tf.random_normal_initializer(stddev=0.02))</span></pre><p id="a53e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，这将永远是一个0维的张量吗？当我查看与模型一起保存的hparams.json文件时，这个问题很快得到解决，正如我们在下面看到的，参数是不同的</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="52d5" class="lb jq hi kx b fi lc ld l le lf">{<br/>  "n_vocab": 50257,<br/>  "n_ctx": 1024,<br/>  "n_embd": 768,<br/>  "n_head": 12,<br/>  "n_layer": 12<br/>}</span></pre><h1 id="5b64" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">设置输入</h1><p id="aa81" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">因此，我们现在可以合理地假设n_vocab是词汇量。虽然我不会进入n_head和n_layer，但我认为假设n_ctx是上下文的最大长度是合理的，但是我们还不能确定！</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ce31" class="lb jq hi kx b fi lc ld l le lf">wpe = tf.get_variable(‘wpe’, [hparams.n_ctx, hparams.n_embd],<br/> initializer=tf.random_normal_initializer(stddev=0.01))<br/>wte = tf.get_variable(‘wte’, [hparams.n_vocab, hparams.n_embd],<br/> initializer=tf.random_normal_initializer(stddev=0.02))</span></pre><p id="9896" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们可以合理地确定wte是一个查找表，它保存了所有对应于令牌值的向量！</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="be7f" class="lb jq hi kx b fi lc ld l le lf">past_length = 0 if past is None else tf.shape(past)[-2]<br/>h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))</span></pre><p id="953b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，虽然这主要是一个猜测，我怀疑过去是模型到目前为止的输出。这主要是因为模型函数是从samples.py中的step函数调用的，所以我怀疑step函数是在每次模型输出新令牌并将该令牌作为输入添加到模型并再次调用时被调用的！这一点以后会得到证实。</p><p id="d3f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然我不能说过去的形状，但从名称判断，past_length应该包含到目前为止输出的文本的长度。</p><p id="b725" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，让我们看看h. tf.gather是一个函数，它返回第一个参数的索引，这个索引是由第二个参数给出的。例如，如果a是一个张量</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="c4d6" class="lb jq hi kx b fi lc ld l le lf">a = tf.constant([1,2,4])</span></pre><p id="b124" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要得到值2，我们只需要调用</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="bc3a" class="lb jq hi kx b fi lc ld l le lf">tf.gather(a, tf.constant([1]))</span></pre><p id="13a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，正如我们在加法的第一部分看到的，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="0857" class="lb jq hi kx b fi lc ld l le lf">tf.gather(wte, X)</span></pre><p id="549a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为x是记号，wte是将记号连接到向量的查找表，所以我们可以说这是到目前为止收集的记号的向量表示。让我们看看wpe部分，并试着弄清楚它是做什么的</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f0a3" class="lb jq hi kx b fi lc ld l le lf">tf.gather(wpe, positions_for(X, past_length))</span></pre><p id="9ff5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为此，让我们看看positions_for函数。函数的位置给定为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="6a31" class="lb jq hi kx b fi lc ld l le lf">def positions_for(tokens, past_length):<br/>    batch_size = tf.shape(tokens)[0]<br/>    nsteps = tf.shape(tokens)[1]<br/>    return expand_tile(past_length + tf.range(nsteps), batch_size)</span></pre><p id="48fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先要注意的一件有趣的事情是，与模型函数不同，batch_size的值和步骤是这样直接获取的</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="33bc" class="lb jq hi kx b fi lc ld l le lf">batch_size = tf.shape(tokens)[0]<br/>nsteps = tf.shape(tokens)[1]</span></pre><p id="8799" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我不知道他们为什么这样做，所以如果有人知道，请告诉我！</p><p id="3fda" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在这之后，expand_tile函数被这样调用，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="276c" class="lb jq hi kx b fi lc ld l le lf">expand_tile(past_length + tf.range(nsteps), batch_size)</span></pre><p id="aebe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">扩展图块函数给定为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="eebd" class="lb jq hi kx b fi lc ld l le lf">def expand_tile(value, size):<br/>    """Add a new axis of given size."""<br/>    value = tf.convert_to_tensor(value, name='value')<br/>    ndims = value.shape.ndims<br/>    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)</span></pre><p id="d467" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输入的参数是从past_length到past _ length+序列长度或x的范围，而大小是批处理大小。</p><p id="301b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ndims是张量的轴数。例如，如果它是一个二维张量，它是2，如果它是一个三维张量，它是3。</p><p id="d7ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">tf.tile基本上将第一个参数扩展了第二个参数倍。你可以点击查看文档<a class="ae jo" href="https://www.tensorflow.org/api_docs/python/tf/tile" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="756b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个平铺函数现在所做的是为所有批次堆叠一个范围从past_length到past_length+sequence length的批次！如果需要解释，请在评论里告诉我！</p><p id="3e85" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在当我们回到</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ef93" class="lb jq hi kx b fi lc ld l le lf">tf.gather(wpe, positions_for(X, past_length))</span></pre><p id="3c40" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们看到，从past_length到past _ length+x的序列长度的wpe的索引被采用。由于我首先不确定wpe是什么，我不能完全确定，所以我决定检查一下！我写了这个令人惊叹的博客。wpe基本上做的就是告诉模型某个特定的单词在哪里！所以，如果它像第5个词，这个wpe会添加签名说这个词是第5个词，并添加它，这是相当耐人寻味的。这叫位置编码！</p><p id="5318" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">h最终通过将表示记号的向量和位置编码相加而获得。</p><p id="5c74" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于这篇文章比预期的要长一些，我想我会把下一篇文章的观点留到下一篇，因为我将进入变形金刚，这对于专家和初学者来说都是一个相当复杂的话题！</p><h1 id="00c0" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">然后</h1><p id="b66a" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">如果你有兴趣，请查看下一篇文章<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-4-a5fbb89e5038">这里</a>！</p></div></div>    
</body>
</html>