<html>
<head>
<title>Understanding Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-neural-networks-776fa70d055c?source=collection_archive---------27-----------------------#2020-01-19">https://medium.com/analytics-vidhya/understanding-neural-networks-776fa70d055c?source=collection_archive---------27-----------------------#2020-01-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/0adf34c45d100924312fdbbd6a202893.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghwCPKjjWKDv9HTsMTBwDA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">资料来源:cognex.com</figcaption></figure><div class=""/><p id="423a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">语音识别、图像处理、面部识别是由基于神经网络工作的深度学习驱动的人工智能应用的一些例子。</p><p id="a13a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">神经网络是由芝加哥大学的两位研究人员沃伦·麦卡洛和沃尔特·皮茨于1944年首次提出的</p><h1 id="e809" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">什么是神经网络？</strong></h1><p id="5a59" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">神经网络是设计用来模仿人脑运作的计算机程序。神经网络(也称为神经元)中的每个程序只能执行基本计算。但是通过将众多的神经元连接在一起，整个网络的计算能力变得比每个单独的部分都强。在神经网络内的神经元之间建立连接的过程被称为训练并使用该数据变得更聪明——类似于人类学习信息的方式。</p><p id="3e1a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在该结构的上下文中，模式通过<em class="kv">输入层</em>被引入网络，该输入层具有输入数据中每个分量的输入，该输入数据随后被传递到<em class="kv">隐藏层</em>。在隐藏层中，所有的处理实际上都是通过以权重和偏好为特征的连接系统进行的。在输入层接收输入，神经元计算一个加权和加上偏置，并根据结果和预设的激活函数，决定是否应该触发或激活它。在这个过程的最后，最后一个隐藏层被链接到<em class="kv">输出层</em>，它有一个神经元用于每个可能的期望输出。</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es kw"><img src="../Images/fe30caef049d81e898b7c32ac2bb0a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*w5zWF_9zotB8uxqYzDYXQQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">神经网络的基本结构</figcaption></figure><h1 id="8726" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">神经网络是如何工作的？</strong></h1><p id="8a02" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">为了理解神经网络如何工作，我们需要理解可以包含在我们的网络中的不同类型的神经元</p><p id="dae5" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">第一种类型的神经元是<em class="kv">感知器</em>，即使有更多的现代作品，理解感知器也是有益的。第二种重要的神经元类型是<em class="kv">s形神经元</em></p><h1 id="43e7" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">感知器</strong></h1><p id="d192" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">感知器接受几个二进制输入x1、x2…并产生一个二进制输出，即0或1。感知器遵循的三个主要步骤是:</p><ol class=""><li id="f409" class="lb lc hx iw b ix iy jb jc jf ld jj le jn lf jr lg lh li lj bi translated">输入x1，x2，x3…被引入感知器。它可以接受更少或更多输入</li><li id="2d68" class="lb lc hx iw b ix lk jb ll jf lm jj ln jn lo jr lg lh li lj bi translated">重量w1、w2、w3…它们是表示各个输入对输出的重要性的实数</li><li id="1fb3" class="lb lc hx iw b ix lk jb ll jf lm jj ln jn lo jr lg lh li lj bi translated">神经元输出0或1通过加权和∑wjxj是小于还是大于阈值来计算。</li></ol><p id="746e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">用数学术语来说:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lp"><img src="../Images/c2fa3fe8647626b6ea3d4fdee77ccfb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lP7BEgFFR_rUlzNieePTPA.png"/></div></div></figure><p id="81de" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了简化上述等式，我们可以进行以下更改:</p><ol class=""><li id="bb12" class="lb lc hx iw b ix iy jb jc jf ld jj le jn lf jr lg lh li lj bi translated">将wjxj的总和替换为点积w.x，其中w和x是向量，其分量分别是权重和输入</li><li id="9182" class="lb lc hx iw b ix lk jb ll jf lm jj ln jn lo jr lg lh li lj bi translated">将阈值移动到不等式的另一边，并替换为bias -b</li></ol><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lq"><img src="../Images/0c22ae28929be1e909074c0ace9880c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cGpXcJ-Zq3zIo1dmCP8v_w.png"/></div></div></figure><p id="34a7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">偏差可以描述为感知器输出1的难易程度。如果偏差为负，感知器会发现很难输出1，如果偏差为正，则很容易输出1。</p><p id="ff84" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">感知器的优势之一是我们可以改变权重和偏差来获得决策模型。我们可以给这些输入分配更多的权重，这样如果它们将输出一个正输出。如果我们注意公式，我们可以观察到一个大的正偏差会使输出1变得非常容易；然而，非常负的偏差将使输出1的任务变得非常不可能。然而，感知器的一个缺点是，即使在单个神经元中，权重或偏差的微小变化也会使输出从0到1发生剧烈变化，反之亦然。这就是一种更现代的神经元派上用场的地方:s <strong class="iw hy"> <em class="kv"> igmoid神经元。</em></strong>sigmoid神经元和感知器的主要区别在于，输入和输出可以是0到1之间的任何连续值</p><h1 id="92e7" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">乙状结肠</strong></h1><p id="dada" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">Sigmoid类似于感知器，但权重和偏差的微小变化只会导致其输出的微小变化。</p><ol class=""><li id="cece" class="lb lc hx iw b ix iy jb jc jf ld jj le jn lf jr lg lh li lj bi translated">就像感知器一样，sigmoid接受输入x1，x2，x3…但是这些值可以是0到1之间的任何连续值，而不仅仅是0或1</li><li id="807b" class="lb lc hx iw b ix lk jb ll jf lm jj ln jn lo jr lg lh li lj bi translated">权重w1、w2、w3和偏差b被引入网络。</li><li id="7d35" class="lb lc hx iw b ix lk jb ll jf lm jj ln jn lo jr lg lh li lj bi translated">但输出不是0或1而是σ(w⋅x+b)，其中σ称为sigmoid函数，定义为:</li></ol><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lr"><img src="../Images/af8113f364f68a3a95a4ca054c0789ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X9SpjH82pjIfDiWurKLQbg.png"/></div></div></figure><p id="d38f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">更明确地说:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ls"><img src="../Images/65210e89f2e0444c65906ec98bbfa11c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0VyfaTzibCAuOX9kYRbBRw.png"/></div></div></figure><p id="55f0" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">sigmoid函数的形状是阶跃函数的平滑版本</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lt"><img src="../Images/78e86be77542248e0fa6203981b8c2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0QxYfp39pO5f-sKzqxc2MQ.png"/></div></div></figure><p id="4af0" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">事实上，如果σ是一个阶跃函数，那么sigmoid神经元将是一个感知器，因为输出将是0或1。</p><p id="98d0" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">σ的平滑性意味着权重的小变化δwj和偏置的小变化δb将产生神经元输出的小变化δoutput。事实上，微积分告诉我们，δ输出很好地近似为:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lu"><img src="../Images/b9e113cb3e059fdc0d30153fb2390a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-A9pifOuDRev-8wX0iyT2w.png"/></div></div></figure><h1 id="f536" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">神经网络如何学习？</strong></h1><p id="558f" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">机器学习的主要优势是它们每次在预测输出时学习和改进的能力。就神经网络而言，它是指一种算法，可以帮助我们找到权重和偏差，从而使网络的输出接近所有训练输入x的y(x)。为此，我们定义了一个成本函数:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lv"><img src="../Images/2016baaa66afbc62f39f64023f087363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WFlpYB56ZtHs3F7zbwJfPw.png"/></div></div></figure><p id="d926" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">其中w是网络中所有权重的集合，b是所有偏差，n是训练输入的总数，a是当x是输入时网络输出的向量，而∑是所有训练输入的总和，C是二次成本函数，也称为均方误差或MSE。</p><p id="327e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该算法的目的是找到使成本尽可能小的权重和偏差，为此，我们将使用梯度下降。</p><p id="808f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们假设我们试图最小化某个函数，C(v)。这可以是任何多变量的实值函数，v=v1，v2…为了最小化C(v ),可以把C想象成两个变量的函数，我们称之为v1和v2</p><p id="dea0" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">识别这一点的一种方法是使用微积分来试图找到全局最小值。我们可以计算导数，并用它们来寻找C达到最小值或最大值的地方。当C是一个或几个变量的函数时，这可能行得通。但当我们有更多的变量时，这将变成一场噩梦，对于神经网络，我们通常需要更多的变量。用微积分来最小化是行不通的。</p><p id="d609" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们可以计算导数来计算最小值的位置，或者我们可以从一个随机点开始，尝试做一个小的移动，从数学上来说，在v1方向移动δv1，在v2方向移动δv2，并计算函数δc的变化，我们可以将函数的变化表示为:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lw"><img src="../Images/bcbb087e3263bc1bdeb3f0244e006ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X3QwxKc_izw647GISzTYyA.png"/></div></div></figure><p id="2188" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们需要选择δv1和δv2，以便使δC为负。为此，将δv定义为v的变化向量，δv≦(δv1，δv2)会有所帮助。我们也将C的梯度定义为偏导数的向量</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lx"><img src="../Images/6eb4b644804b82e679dc70497182c96a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9-rKdV8d4fXt2q_92XjjZw.png"/></div></div></figure><p id="e91c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">根据这些定义，我们可以将δc写成δv和梯度∇C，如下所示:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es ly"><img src="../Images/11d181a2b4d431b498a7147a56670c27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*SkS2nSx4JGZPZttlWr_sJQ.png"/></div></figure><p id="dc21" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">上面的等式表明我们如何选择δv以使δC为负。</p><p id="e538" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">特别是，假设我们选择δv=−η∇c</p><p id="8fb2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">其中η是一个小的正参数(称为<em class="kv">学习率</em>)。那么上面的等式告诉我们</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lz"><img src="../Images/defa78cc0e6a510a9668cf9a6223a784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*meVSqbg0aGiIvNOz-Yob_w.png"/></div></div></figure><p id="ad41" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因为∥∇C∥2≥0，这保证了δc≤0，也就是说，c会一直减少，永远不会增加。</p><p id="b5f4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">总而言之，梯度下降算法的工作方式是重复计算梯度∇C，然后向相反的方向移动。</p><p id="5669" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们选择向任何方向移动的量称为学习率，它定义了我们多快达到全局最小值。为了使梯度下降正确工作，我们需要选择足够小的学习率，使得上面的方程是一个很好的近似。如果我们不这样做，我们可能会以δC &gt; 0结束，这显然是不好的！同时，我们不希望学习率太小，因为这将使变化δv很小，因此梯度下降算法将非常慢。在实际实现中，学习速率经常变化，因此上述方程仍然是一个很好的近似，但算法不会太慢。</p><p id="0b86" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">应用梯度下降规则有许多挑战。一个主要问题是训练大量训练样本所需的时间。一种叫做<em class="kv">随机梯度下降</em>的算法可以用来加速学习。其思想是通过计算随机选择的训练输入的小样本的梯度来估计梯度∇C。通过对这个小样本求平均，我们可以很快得到真实梯度∇C的良好估计，这有助于加速梯度下降，从而加快学习。</p><p id="88e2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">随机梯度下降通过随机挑选出少量m个随机选择的训练输入来工作，我们称之为小批量。我们计算这个小批量的梯度。然后，我们挑选出另一个随机选择的小批量和训练。这个过程一直重复，直到我们用尽了训练输入，这就是所谓的训练时期。在这一点上，我们重新开始一个新的训练时期。</p><h1 id="361a" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">神经网络中的反向传播</h1><p id="90f3" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">如上所述，神经网络可以使用梯度下降算法学习权重和偏差。但是为了计算成本函数的梯度，我们需要另一种叫做<em class="kv">反向传播</em>的算法。反向传播的目标是计算成本函数c相对于网络中任何权重w或偏差b的偏导数∂C/∂w和∂C/∂b。</p><h1 id="5b62" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">关于反向传播工作成本函数的假设</h1><p id="92d3" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">在做出假设之前，让我们定义成本函数:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ma"><img src="../Images/a816e365ac2c026a64ff86ec1e37ebe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ex20I5kixJG6dd63Wg4G8Q.png"/></div></div></figure><p id="0ff2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">其中:n为训练样本总数；总和超过单个训练示例x；y=y(x)是相应的期望输出；l表示网络的层数；和a^L=a^L(x)是当输入x时从网络输出的激活向量。</p><p id="4f01" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们需要的第一个假设是成本函数可以写成平均值:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mb"><img src="../Images/82e05c1451a1216cab77219f0fcfb359.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*4ddFKW9rGQjHq_JEfCJCXQ.png"/></div></figure><p id="8b0f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">个别训练例子的成本函数Cx。</p><p id="1b1d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这种假设下，反向传播让我们计算单个训练样本的偏导数∂Cx/∂w和∂Cx/∂b。然后，我们通过对训练样本进行平均来计算∂C/∂w和∂C/∂b。</p><p id="c49e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们对成本所做的第二个假设是，它可以写成神经网络输出的函数，如下所示:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mc"><img src="../Images/ce43b05b6fe21cd80c65f314fcc415aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yBjEYHoSS90FV9YwlzHs-Q.png"/></div></div></figure><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es md"><img src="../Images/6656f5fccf723bd480dbe8510914b7cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rGrrlb-BWIQ5VdjF1AU_w.png"/></div></div></figure><h1 id="f2bc" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">反向传播背后的四个基本方程</h1><p id="b78c" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">反向传播计算成本函数相对于权重和偏差的偏导数。但要做到这一点，我们必须引入一个中间量δ，我们称之为第1层的第j个神经元中的<em class="kv">误差</em>。反向传播基于四个基本方程。这些等式共同为我们提供了一种计算第1层中的误差δ和成本函数的梯度的方法。这四个等式描述如下:</p><p id="aef5" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">输出层误差的一个等式，</strong>δL<strong class="iw hy">:</strong>δL的分量由下式给出</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es me"><img src="../Images/c57c3a47a95e2677ed750beaa4ad55be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*vzwRLUG6M8gUmZaqjP-DxA.png"/></div></figure><p id="596b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">上述等式可以用基于矩阵的形式写成:</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/325eabdb6a5dd3fb42dcdb2d47451d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*3R3p-WDAXpsS4wg7pZwd-w.png"/></div></figure><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mg"><img src="../Images/402c781015e4cd33e829969f162a55e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nVpfUPClbl8LUzSw0tXbJg.png"/></div></div></figure><p id="4d4e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">误差方程</strong> δ^l <strong class="iw hy">关于下一层的误差，</strong> δ^l+1 <strong class="iw hy"> : </strong></p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/b9bc4b5b66f7021e1404ecd77a719a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSZH6v0Iv2nyHmF3ZTxiCw.png"/></div></div></figure><p id="2613" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">关于网络中任何偏差的成本变化率的等式:</strong></p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/9857fe76ac5d9a88e11d4d25d257bdc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*wISEU6Zlg-THyQH6p-3k7g.png"/></div></figure><p id="87e8" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy">关于网络中任意权重的成本变化率的等式:</strong></p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es ly"><img src="../Images/e21badb4f116185b6a994f977d7d26c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*1qIEn2Wy4bLdvt3qNelbYw.png"/></div></figure><h1 id="d0c6" class="js jt hx bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">反向传播算法</h1><p id="d277" class="pw-post-body-paragraph iu iv hx iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">反向传播方程为我们提供了一种计算成本函数梯度的方法。</p><ol class=""><li id="7f70" class="lb lc hx iw b ix iy jb jc jf ld jj le jn lf jr lg lh li lj bi translated"><strong class="iw hy">输入</strong> x <strong class="iw hy"> : </strong>设置输入层对应的激活a。</li><li id="891e" class="lb lc hx iw b ix lk jb ll jf lm jj ln jn lo jr lg lh li lj bi translated"><strong class="iw hy">前馈:</strong>对于每个l=2，3，…L计算</li></ol><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mj"><img src="../Images/f05d0865e6d8e2d43e28fde473ce083b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W3Z8J9Fnl-VwO2JGbK2I8w.png"/></div></div></figure><p id="91bb" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.<strong class="iw hy">输出误差</strong> δL <strong class="iw hy"> : </strong>计算矢量</p><figure class="kx ky kz la fd hk er es paragraph-image"><div class="er es mk"><img src="../Images/428f14a76c5d789a77f26aba9f254592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*SAnMfsNNXUT0eXrzrYNwfA.png"/></div></figure><p id="4a8c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">4.<strong class="iw hy">反向传播误差:</strong>对于每个l = L，L2，…，2计算</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/78b59336b2319172ffc52914a31725ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RJCCmhN762PRrvgv6ZEEZw.png"/></div></div></figure><p id="04ec" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> 5。输出:</strong>成本函数的梯度由下式给出</p><figure class="kx ky kz la fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/dcbe38da3da001f485374affe53a264d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hRnLEf5jmmTjGru6OvLb3w.png"/></div></div></figure><p id="ade9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因为我们从最后一层开始向后计算误差向量δ^l，所以该算法被称为反向传播。向后移动是成本是网络输出的函数这一事实的结果。为了理解成本如何随着先前的权重和偏差而变化，我们需要重复应用链式法则，通过各层反向工作以获得可用的表达式。</p><p id="3511" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">参考资料:</p><p id="e427" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae mm" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank">神经网络和深度学习</a> —迈克尔·尼尔森</p><p id="c19d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae mm" href="https://towardsdatascience.com/understanding-neural-networks-what-how-and-why-18ec703ebd31" rel="noopener" target="_blank">神经网络——什么、如何以及为什么</a>——<a class="ae mm" href="https://towardsdatascience.com/@meinzaugarat?source=follow_footer--------------------------follow_footer-" rel="noopener" target="_blank">Euge Inzaugarat</a></p></div></div>    
</body>
</html>