<html>
<head>
<title>Random Forest and Ensembles Learning with Amazon Food Reviews</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林和集合学习与亚马逊食品评论</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/random-forest-and-ensembles-learning-with-amazon-food-reviews-e58c72c189be?source=collection_archive---------6-----------------------#2020-11-21">https://medium.com/analytics-vidhya/random-forest-and-ensembles-learning-with-amazon-food-reviews-e58c72c189be?source=collection_archive---------6-----------------------#2020-11-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e8a5d7a8c0eeb44572189881e79b3290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qKfYwZGpJtQsjXKS.jpeg"/></div></div></figure><p id="2285" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇博客中，我们将尝试理解机器学习中最重要的算法之一，即随机森林算法。我们将尝试研究使随机森林如此特殊的东西，并尝试在现实世界的数据集上实现它。</p><h1 id="8ca6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">内容</h1><ol class=""><li id="5a69" class="km kn hi is b it ko ix kp jb kq jf kr jj ks jn kt ku kv kw bi translated">什么是合奏？</li><li id="d4ff" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">集成学习的类型。</li><li id="1d81" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">装袋。</li><li id="8d81" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">随机森林和建筑。</li><li id="d264" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">随机森林的最好和最坏情况。</li><li id="08e2" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">助推。</li><li id="c3ea" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">增压的类型。</li><li id="5c20" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">梯度推进。</li><li id="ecac" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">AdaBoost(自适应升压)。</li><li id="1dd4" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">XGBoost。</li><li id="3462" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">堆叠分类器。</li><li id="f22b" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">级联分类器。</li><li id="0b4c" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">随机森林和XGBOOST与亚马逊美食评论。</li></ol><h1 id="a0c2" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">什么是合奏？</h1><p id="084b" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">通常，单个模型会有偏差或方差，这就是为什么我们需要集成学习。</p><p id="40e0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">集成方法是一种将来自多个机器学习算法的预测结合在一起的技术，以做出比任何单个模型更准确的预测。由许多模型组成的模型被称为<strong class="is hj">集合模型</strong>。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/306e912f307f08c4fa31526679729b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/0*KGL8PW4vM4R1QuVe.png"/></div></figure><p id="8d53" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一般来说，在机器学习中，当我们有多个模型时，然后将它们组合起来，并有可能生成强大的模型，这被称为集成。</p><h2 id="58db" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">集成学习的类型</h2><ul class=""><li id="4870" class="km kn hi is b it ko ix kp jb kq jf kr jj ks jn ly ku kv kw bi translated">打包(引导汇总)</li><li id="242c" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn ly ku kv kw bi translated">助推</li><li id="4439" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn ly ku kv kw bi translated">堆垛</li><li id="3e19" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn ly ku kv kw bi translated">级联</li></ul><p id="67bf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当模型比我们更不同时，我们得到更好的表现。</p><p id="7f2a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">套袋(Bootstrap aggregating): </strong>随机森林是流行的套袋技术。</p><p id="c077" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Bootstrap是指带有替换的随机抽样。Bootstrap允许我们更好地理解数据集的偏差和方差。Bootstrap涉及从数据集中随机抽取一小部分数据。</p><p id="1124" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一个通用过程，可用于减少那些具有高方差的算法(通常是决策树)的方差。Bagging使每个模型独立运行，然后在最后聚合输出，而不偏向任何模型。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/fc11d81a09007cfcad038fc0b6c4da60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gEtsUJk2ZxB3jj5RHkDq0w.png"/></div></div></figure><p id="a3e9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">技术:</strong>使用Bootstrap样本训练模型，即每个模型都有不同的数据子集，并使用聚合将所有模型结合起来。</p><p id="c6fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">分类汇总的典型方法采用多数投票，如果是回归，则采用平均值(或)中值。</p><p id="bcb2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">当我们执行Bootstrap采样时？</strong></p><p id="6476" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当模型随着训练数据的变化而发生很大变化(称为高方差)时，当我们通过改变数据集将自举采样应用于该模型时，自举采样不会影响模型，因为我们使用了聚集技术。</p><p id="3006" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Bagging: </strong>这是一种可以减少模型中的方差而不影响偏倚的技术。</p><p id="4f69" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基本模型是低偏差和高方差模型。在这个模型上使用Bagging，我们可以将高方差减少到合理的方差，而不会因为bootstrap抽样和聚集而改变偏差。</p><p id="07d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">示例</strong>:深度决策树大(高方差低偏差)</p><h2 id="b7ff" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">决策树的问题？</h2><p id="07a5" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">决策树对训练它们的特定数据很敏感。如果训练数据被改变，则得到的决策树可能会非常不同，并且反过来预测也可能非常不同。</p><p id="ac00" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此外，训练决策树的计算成本很高，具有过度拟合的巨大风险，并且容易找到局部最优解，因为它们在进行分割后无法返回。</p><p id="9d6d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了解决这些弱点，我们转向随机森林，它展示了将许多决策树组合到一个模型中的能力。</p><h2 id="d6e3" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">随机森林与建筑</h2><p id="e731" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">随机森林是一种监督学习算法，使用集成学习方法进行分类和回归。</p><p id="e938" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随机森林是一种装袋技术。随机森林中的树是并行运行的。在构建树时，这些树之间没有交互。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/18863ec9e3d7f2e9dbe6d96eb94d8d29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FRJ5iZdAvPnrlcgRlkjGBA.png"/></div></div></figure><p id="50cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它通过在训练时构建大量决策树并输出类来运行，该类是各个树的类(分类)或均值预测(回归)的模式。</p><p id="0a9c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随机森林是一个元估计器(即它结合了多个预测的结果)，它聚合了许多决策树，并做了一些有益的修改。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/930ac9287fc1857ccd18626a828bbb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*aJJvzT_ePWcWfnYobaISUA.png"/></div></figure><p id="a6e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每个节点上可分割的特征数量被限制为总数的某个百分比(称为超参数)。这确保了集成模型不会过于依赖任何单个特征，并且公平地使用所有潜在的预测特征。</p><p id="d496" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每棵树在生成分裂时都会从原始数据集中抽取一个随机样本，进一步增加了随机性元素，以防止过度拟合。</p><p id="2478" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">自付(OOB)积分</strong></p><p id="a354" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在随机森林中，不需要交叉验证或单独的测试集来获得测试集误差的无偏估计。内部估计如下。</p><p id="26dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每个树都是使用来自原始数据的不同引导样本构建的。大约三分之一的情况被排除在引导样本之外，并且不用于第k棵树的构造。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/9a61445e509ac30f04eadbdbe0726ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rBApvoHGhFY0OeeI6CY84A.png"/></div></div></figure><p id="90ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以在交叉验证中使用这些点。</p><h2 id="0ce5" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">随机森林的最佳和最差情况</h2><ol class=""><li id="8522" class="km kn hi is b it ko ix kp jb kq jf kr jj ks jn kt ku kv kw bi translated">对于分类问题中的应用，随机森林算法将避免过拟合问题。</li><li id="f632" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">对于分类和回归任务，可以使用相同的随机森林算法。</li><li id="4f35" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">随机森林算法可用于从训练数据集中识别最重要的特征，换句话说，特征工程。</li><li id="c4ea" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">随着森林构建的进行，它会生成概化误差的内部无偏估计。</li><li id="aa0e" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">它有一个有效的方法来估计缺失数据，并在大部分数据缺失时保持准确性。</li><li id="ca3a" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">已经观察到随机森林对于一些具有噪声分类(或)回归任务的数据集来说过度拟合。</li><li id="c419" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">对于包含具有不同级别数量的分类变量的数据，随机森林偏向于那些具有更多级别的属性。因此，来自随机森林的可变重要性分数对于这种类型的数据是不可靠的。</li><li id="9669" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">决策树失败的地方，随机森林也会失败。</li></ol><h2 id="6d3d" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">助推</h2><p id="26a9" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">术语“增强”指的是将弱学习者转换为强学习者的一系列算法。</p><p id="e83f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">boosting不是创建一个强大的模型，而是将多个简单的模型组合成一个复合模型。这个想法是，随着我们引入越来越多的简单模型，整体模型变得越来越强大。在boosting术语中，简单模型被称为弱模型(或弱学习器)。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es md"><img src="../Images/12931a251f31b387e5708d1030cb438f.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*LpMBEdCEyfZ0ge2Ncu-6Wg.png"/></div></figure><p id="0ff3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后建立第二个模型，试图纠正第一个模型中存在的错误。继续这个过程并添加模型，直到正确预测了完整的训练数据集或者添加了最大数量的模型。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/b9e58807b814ca5716a7b314135c0e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DF2MO2szwOVa7zs1lhPtvA.png"/></div></div></figure><p id="f773" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">核心思想 : Boosting减少偏差，我们在Boosting中使用低方差高偏差模型作为基础学习器。</p><h2 id="90a6" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">顺序集成方法</h2><p id="eda7" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">boosting的主要思想是给系综<strong class="is hj"> <em class="mf">依次</em> </strong>添加新的模型。本质上，提升通过从<em class="mf">弱</em>模型(例如，只有几个分裂的决策树)开始，然后<em class="mf">通过继续构建新的树来提升</em>的性能，其中序列中的每个新树试图修复前一个树犯最大错误的地方(即，序列中的每个新树将关注前一个树具有最大预测误差的训练行)。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/9c05c2374fb81021ba96e400914cda51.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/0*vb7Xh6aTrKyhtVGw.png"/></div></figure><p id="db19" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">基础学习者</strong> : Boosting是一个框架，它迭代改进<em class="mf">任何</em>弱学习模型。许多梯度提升应用程序允许你随意“插入”不同类别的弱学习者。然而，在实践中，boosted算法总是使用决策树作为基础学习器。</p><p id="ac42" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">训练弱模型</strong>:弱模型是错误率仅略好于随机猜测的模型。本质上，通过关注先前树具有最大误差或残差的训练数据行。关于决策树，浅树即具有相对较少分裂的树表示弱学习者。在boosting中，1-6分裂的树是最常见的。</p><p id="fc6f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">关于错误的顺序训练</strong>:被增强的树顺序生长。每棵树都是使用来自先前生长的树的信息来生长的，以提高性能。通过将序列中的每棵树与前一棵树的残差进行拟合，我们允许序列中的每棵新树关注前一棵树的错误。其步骤如下...</p><ol class=""><li id="9cbf" class="km kn hi is b it iu ix iy jb mh jf mi jj mj jn kt ku kv kw bi translated">我们在开始时给出了列车数据Dtrain(，易)。为数据拟合一个浅层决策树，即F1(x)=Y。</li><li id="f924" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">然后，我们将下一个决策树拟合到残差，即h1(x)= Yi F1(x)。</li><li id="3ee4" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">将这个新树添加到我们的算法中，即F2(x)=F1(x)+h1(x)。</li><li id="95ff" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">将下一个决策树拟合到F2(x)的残差，即H2(x)= Yi F2(x)。</li><li id="83f9" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">将这个新树添加到我们的算法中，即F3(x)=F2(x)+h2(x)。</li><li id="0b82" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">继续这个过程，直到低残差，然后训练误差减少意味着偏差减少。</li></ol><p id="c13f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里的最终模型是j棵个体树的逐阶段相加模型，</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/bad0f2faa216e6df868fda5b34bd407b.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*x-eznKnd4aw6tL5wVrfLeA.png"/></div></figure><h2 id="f78d" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">增压的类型</h2><ol class=""><li id="2ef2" class="km kn hi is b it ko ix kp jb kq jf kr jj ks jn kt ku kv kw bi translated">梯度推进</li><li id="6c0e" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">自适应增强</li><li id="57a2" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">XGBoost</li></ol><h2 id="7bfa" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated"><strong class="ak">梯度推进</strong></h2><p id="dc1a" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">梯度推进机这个名字来源于这样一个事实，即这个过程可以推广到损失函数。</p><p id="df65" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">梯度推进被认为是梯度下降算法。梯度下降是一种非常通用的优化算法，能够找到各种问题的最优解。梯度下降的一般思想是迭代地调整参数以最小化成本函数。</p><p id="e162" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">梯度推进树解决了上面提到的部分限制。不是训练单个树，而是依次训练多个树。为了降低树的方差，它们受到限制。通过限制树的深度，他们变成了弱学习者。决策树的深度通常在3到6层之间选择。我们允许有一点深度，这样我们可以比较共同出现的变量。</p><h2 id="7189" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">梯度增强中梯度出现在哪里？</h2><p id="e9d1" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">残差不仅包含大小信息，还包含方向，因此残差是向量。剩余向量的符号也是向量。大多数文章将残差视为近似值和真实目标值之间的误差，但是我们一般将这些称为方向向量，以强调它们是向量，而不仅仅是大小。将残差视为误差向量也使得谈论优化绝对误差而不是平方误差的梯度提升机器变得尴尬。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/cae05602f97467bdc57da587ee5774cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*uoAbV8yhbps_qMjVVZH8pg.png"/></div></figure><p id="dd35" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过将近似值添加到残差中，梯度增强机器正在追踪梯度，因此，术语<em class="mf">梯度</em>增强。</p><p id="9e43" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">梯度推进提供了最小化任何损失函数，只要它是可微分的。</p><h2 id="78ce" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">剩余函数和损失函数</h2><p id="cce8" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">考虑下面显示的我们的升压最终方程。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/bad0f2faa216e6df868fda5b34bd407b.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*x-eznKnd4aw6tL5wVrfLeA.png"/></div></figure><p id="ef1f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">残余不过是，</p><p id="b115" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">误差= Yi F1(x)在模型k的末尾。</p><p id="59b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们将误差视为平方误差，那么损失函数为，</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/ff6a76dbb98e744e14c7611c7b01249f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*6FvAwUUsIP2_8AsE9Z-JrQ.png"/></div></div></figure><p id="e289" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后我们对w . r . t . Fk(x)求导，</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/21a4666b0f4619a1bb33172e37b24bb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*EWcGRhWkjdV0x3E33XfgWw.png"/></div></figure><p id="9fd0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个负梯度被称为<strong class="is hj">伪残差</strong>，它有助于任何损失函数达到可微。</p><p id="6d71" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">关键思想:</strong>我们可以用伪残差(负梯度)代替这个误差，因为我们可以使用任何损失函数。</p><p id="083d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">关于损失函数和梯度的更多信息，请访问<a class="ae mo" href="https://www.ritchievink.com/blog/2018/11/19/algorithm-breakdown-why-do-we-call-it-gradient-boosting/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">此处</strong> </a>。</p><p id="9ece" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">损失函数可以是逻辑损失、平方损失、铰链损失(或)任何可微的损失。</p><p id="673f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">梯度增强算法的步骤如下:</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/f0ed420ff527f43bc862a2a5028b7bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTxGJ-i32KBs4wc8-52jTg.png"/></div></div></figure><p id="2df6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">更多信息请访问<a class="ae mo" href="https://en.wikipedia.org/wiki/Gradient_boosting#Algorithm" rel="noopener ugc nofollow" target="_blank">T5这里T7】。</a></p><h2 id="0390" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated"><strong class="ak">正规化</strong></h2><p id="918b" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">将训练集拟合得太接近会导致模型的泛化能力下降。几种所谓的正则化技术通过约束拟合过程来减少这种过拟合效应。</p><p id="bace" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一个自然正则化参数是梯度增强迭代的数量<em class="mf"> M </em>，即当基础学习者是决策树时模型中树的数量。增加<em class="mf"> M </em>会减少训练集的误差，但将其设置得太高可能会导致过度拟合。<em class="mf"> M </em>的最佳值通常通过监控独立验证数据集的预测误差来选择。除了控制<em class="mf"> M </em>之外，还使用了其他几种正则化技术。</p><p id="817e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一个正则化参数是树的深度。该值越高，模型越有可能过度拟合训练数据。</p><h2 id="0a48" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">收缩</h2><p id="de61" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">梯度推进方法的一个重要部分是收缩正则化，它包括如下修改更新规则:</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/8e1160664f7f3f5bdbcf5d4748494684.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*whYab-G3blPYqi6M-WdR1Q.png"/></div></figure><p id="ab58" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中参数<strong class="is hj"> ν </strong>称为“学习率”。</p><p id="0e42" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">经验上已经发现，使用小的学习率，例如<strong class="is hj"> ν </strong> &lt; 0.1，在没有收缩(<strong class="is hj"> ν </strong> =1)的情况下，在模型的泛化能力上产生显著的改进。然而，这是以增加训练和查询期间的计算时间为代价的。较低的学习速率需要更多的迭代。</p><h2 id="160b" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">自适应增强</h2><p id="df2e" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">Adaboost将多个弱学习器组合成一个强学习器。AdaBoost中的弱学习器是具有单一分裂的决策树，称为决策树桩。</p><p id="28c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当AdaBoost创建它的第一个决策树桩时，所有的观察值被同等地加权。为了纠正之前的错误，错误分类的观测值现在比正确分类的观测值具有更大的权重。AdaBoost算法可用于分类和回归问题。</p><p id="529b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">AdaBoost广泛用于人脸检测中的图像处理。AdaBoost算法的步骤如下:</p><ol class=""><li id="8493" class="km kn hi is b it iu ix iy jb mh jf mi jj mj jn kt ku kv kw bi translated">初始化数据集，并为每个数据点分配相等的权重。</li><li id="ebe6" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">将其作为模型的输入，并识别错误分类的数据点。</li><li id="3ddf" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">增加错误分类的数据点的权重。</li><li id="3993" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">如果(得到所需结果)<br/>转到步骤5 <br/>否则<br/>转到步骤2</li><li id="5c5a" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">结束</li></ol><p id="f1c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑2D的这个玩具数据集，并假设我们的弱分类器是决策树桩(垂直或水平半平面):</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/2153a14a3e6fc31629b7a730a22eab11.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*YQ6ye4M3GFslco3qr5Mf9g.png"/></div></figure><p id="ad47" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第一轮:</strong>计算完权重后，我们将在进入下一步之前增加误分类点的权重。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/c7dbc41e650c979412959bc9877d531c.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*9PTszNcJfOzmON6bDWh8oA.png"/></div></figure><p id="9ab1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第二轮:</strong>这一步也是找到权值，增加误分类点的权值再进行下一步。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/cc465e31edaf4da0c0b66618a7e62fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*UGDP0YTOdxZABBbEq8wvVA.png"/></div></figure><p id="ea6b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第三轮</strong>:求权重，增加误分类点的权重，再进行下一步。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/5eea908136386df7c6cec215df44e2ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*KuboCHYf0f3KKv78V1OkRw.png"/></div></figure><p id="d584" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">最终分类器:</strong>像<strong class="is hj">α1 h1(x)+α2 H2(x)+α3 H3(x)</strong>一样把最后一步的所有模型组合起来我们得到F(x)模型。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/9289d534710276f370445a80117ef48a.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*dPlSLIC5Bp1FAF1s3eiEAg.png"/></div></figure><p id="d38d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有关AdaBoost的更多信息，请访问<a class="ae mo" href="https://alliance.seas.upenn.edu/~cis520/wiki/index.php?n=lectures.boosting" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">此处</strong> </a>。</p><h2 id="1246" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">XGBoost</h2><p id="a6af" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">XGBoost代表<strong class="is hj">极限梯度提升</strong>。XGBoost是梯度提升决策树的实现，旨在提高速度和性能。由于顺序模型训练，梯度推进机器在实施中通常非常慢。因此，它们的可伸缩性不是很好。因此，XGBoost侧重于计算速度和模型性能。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/df203b61d15f4c828c0cefca27fa3f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7RTGK8rbtepRxho4vGyNgg.png"/></div></div></figure><p id="c73a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该库提供了一个用于各种计算环境的系统，尤其是:</p><ul class=""><li id="a359" class="km kn hi is b it iu ix iy jb mh jf mi jj mj jn ly ku kv kw bi translated"><strong class="is hj">树构造的并行化</strong>在训练期间使用你所有的CPU核心。</li><li id="995f" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn ly ku kv kw bi translated"><strong class="is hj">分布式计算</strong>使用机器集群训练非常大的模型。</li><li id="5a27" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn ly ku kv kw bi translated"><strong class="is hj">核外计算</strong>适用于不适合内存的超大型数据集。</li><li id="0a23" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn ly ku kv kw bi translated"><strong class="is hj">数据结构和算法的缓存优化</strong>充分利用硬件。</li></ul><h2 id="e499" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">堆积分类器</h2><p id="1b94" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">堆叠是一种集成学习技术，通过元分类器来组合多个分类模型。基于完整的训练集训练个体分类模型；然后，基于集成中各个分类模型的输出(元特征)来拟合元分类器。元分类器可以根据预测的类别标签或来自集成的概率来训练。该算法可以总结如下，</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/90a64d25402446064d2899f35a920652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*fQvOKEd6r5UjSsY3v2KJlA.png"/></div></figure><p id="cb4d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种技术经常在现实世界中使用，但为了更好的表现，也广泛应用于Kaggle比赛。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es my"><img src="../Images/7a5fe5a319e4e27c6937d5fb6183ae17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*Eu9vtRG439yD-ZQM25EQEw.png"/></div></figure><p id="9f87" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在标准堆叠过程中，第一级分类器适合用于为第二级分类器准备输入的相同训练集，这可能导致过度适合。然而，堆叠分类器使用交叉验证的概念，数据集被分成<em class="mf"> k </em>个折叠，并且在<em class="mf"> k </em>个连续回合中，<em class="mf"> k-1 </em>个折叠被用于拟合第一级分类器；在每一轮中，然后将第一级分类器应用于在每次迭代中没有用于模型拟合的剩余1个子集。得到的预测然后被堆叠并作为输入数据提供给第二级分类器。</p><p id="d061" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在堆叠分类器的训练之后，第一级分类器适合整个数据集，如下图所示。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/dc2d5d0ba3b823e66a8168e07eb53e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*834g_L3CWR6cZ6DvqiawUQ.png"/></div></figure><p id="8052" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于时间复杂性，这种技术经常在现实世界中使用。关于堆叠分类器的更多信息，请访问<a class="ae mo" href="http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">此处</strong> </a>。</p><h2 id="02cd" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">级联分类器</h2><p id="cb6e" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">这类模型非常非常精确。级联最常用于不能犯错误的情况。例如，级联技术主要用于检测欺诈性信用卡交易，或者当您想要确定您没有癌症时。</p><p id="67aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当犯错误的代价非常非常高时，通常使用级联模型。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/b311aafa6ba9e037c7e7be2625d177ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eteRYq_AwmCjNAPDRqRwQQ.png"/></div></div></figure><p id="1ed4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设我们有一个事务查询点Xq，我们将把它提供给模型1。模型1可以是任何模型随机森林(或)逻辑回归。它可以是任何东西！基本上，模型1所做的是预测类概率，以确定给定查询点属于哪个类的可能性更高。</p><p id="8e0f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设类标签1表示交易是欺诈性的，类标签0表示交易不是欺诈性的。通常，预测概率由这个P(Yq=0)和P(Yq=1)给出，其中Yq是我们实际的类标签。现在让我们假设P(Yq=0)，即交易不是欺诈性的概率非常高。所以即使在我们稍微不确定的时候，我们也会训练另一个模型2。它做同样的事情，它接收查询点并预测P(Yq=0)。这个过程一直持续到我们确认。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es nb"><img src="../Images/7d1936f774ab9a3c92a7fa874a1f278f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*BCvKw2bl-woDpkMdMaBuyQ.png"/></div></figure><p id="f66c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上图中，我们依次有四个模型。我们在整个训练数据集上训练模型1，并在测试数据集上评估其性能。只要我们不确定某些数据点，即它们是否是欺诈性的，我们就将其传递到下一阶段。</p><p id="d7e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">直观上，级联是以这样一种方式设计的，即序列中的下一个模型只在模型不确定类别标签是什么的数据点上训练。我们必须根据模型在生产环境中运行时会看到什么类型的数据来训练我们的模型。</p><h2 id="3fa2" class="lk jp hi bd jq ll lm ln ju lo lp lq jy jb lr ls kc jf lt lu kg jj lv lw kk lx bi translated">Random Forest和XGBOOST提供亚马逊美食评论</h2><p id="24fd" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">让我们对来自Kaggle的真实数据集Amazon Fine Food Review分析应用随机森林和XGBOOST算法。</p><p id="131c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">首先我们想知道什么是亚马逊美食点评分析？</strong></p><p id="66ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个数据集由亚马逊的美食评论组成。这些数据跨越了10多年的时间，包括截至2012年10月的所有约500，000篇评论。评论包括产品和用户信息、评级和明文评论。我们也有来自所有其他亚马逊类别的评论。</p><p id="ba82" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">亚马逊评论通常是最公开可见的消费品评论。作为一个经常使用亚马逊的用户，我对检查亚马逊评论的大型数据库的结构和可视化这些信息很感兴趣，以便成为一个更聪明的消费者和评论者。</p><p id="0a15" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">来源:</strong><a class="ae mo" href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener ugc nofollow" target="_blank"><strong class="is hj"/>https://www.kaggle.com/snap/amazon-fine-food-reviews</a></p><p id="6204" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">亚马逊美食点评数据集由来自亚马逊的美食点评组成。</p><p id="9a1f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">评论数:568，454 <br/>用户数:256，059 <br/>产品数:74，258 <br/>时间跨度:1999年10月—2012年10月<br/>数据中的属性/列数:10</p><p id="2669" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">属性信息:</p><ol class=""><li id="e42a" class="km kn hi is b it iu ix iy jb mh jf mi jj mj jn kt ku kv kw bi translated">身份</li><li id="ebe0" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">产品Id —产品的唯一标识符</li><li id="bf23" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">UserId —用户的唯一标识符</li><li id="91b2" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">ProfileName</li><li id="92e8" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">帮助度分子—认为评论有帮助的用户数量</li><li id="905a" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">帮助度分母——表示他们认为评论是否有帮助的用户数量</li><li id="8781" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">分数—介于1和5之间的等级</li><li id="ce3f" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">时间—审核的时间戳</li><li id="04a4" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">摘要—审核的简要摘要</li><li id="48f9" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">文本—审阅的文本</li></ol><h1 id="26ab" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">目标</h1><p id="b627" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">给出一个评价，确定该评价是正面的(评分为4或5)还是负面的(评分为1或2)。</p><h1 id="1899" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数据预处理</h1><p id="6d62" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">数据预处理是一种用于将原始数据转换成干净数据集的技术。换句话说，无论何时从不同来源收集数据，都是以原始格式收集的，这对于分析是不可行的。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/0a897f67aa3286321e3ca16c37943d9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/0*hfWErRf-11parpCN.png"/></div></figure><p id="91ad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要了解亚马逊食品评论数据集和特征的完整概述，请访问我以前的博客链接<a class="ae mo" rel="noopener" href="/analytics-vidhya/amazon-fine-food-reviews-featurization-with-natural-language-processing-a386b0317f56"> <strong class="is hj">这里</strong> </a> <strong class="is hj">。</strong></p><h1 id="3398" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">列车测试分离</h1><p id="d0ed" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">当机器学习算法用于对不用于训练模型的数据进行预测时，训练-测试分离过程用于估计机器学习算法的性能。</p><p id="88a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您有一个数据集，您需要首先使用Sklearn <code class="du nd ne nf ng b">train_test_split</code>函数来分割它。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/91c0cdbcd106ed3b5751201b0d8ead71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*njTvShv18nNUEkKW.png"/></div></div></figure><h1 id="0110" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">使用单词包的文本特征化</h1><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/2a143ec9b7bcf0b06dbed16a5223e6e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MAN7-yWQ2hti3enr.png"/></div></div></figure><h1 id="6a1a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">超参数调谐</h1><p id="d434" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">我们要选择最佳深度和最佳n_estimators意味着更好的模型性能的树的数量，通过使用网格搜索交叉验证来选择这些。</p><p id="094e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们已经定义了一个Grid_search函数，当我们调用它时，它会给出结果。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ni"><img src="../Images/ad6504ab6eafbe69b4f610222b0ef7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FtfIxHYvB9HFqCfiyhzTYg.png"/></div></div></figure><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/51441a99020a0950b2060cade6ed90d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*BL2WyrcMOaG02xXkZWFpHA.png"/></div></figure><p id="0d14" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们使用网格搜索CV找到最佳参数后，我们希望使用测试数据检查性能，在本例研究中，我们使用AUC作为性能度量。</p><p id="95a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们已经定义了一个测试数据的函数，当我们调用它时，它会给出结果。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/c19f3649d28cd08ff9d60975766e8509.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*gKdeoBjbV9GiJw1CkjB75g.png"/></div></figure><h1 id="d5ea" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">性能指标</h1><p id="57d0" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">绩效指标用于衡量企业的行为、活动和绩效。这应该是在一个范围内测量所需数据的数据形式，允许形成支持总体业务目标实现的基础。</p><p id="65d5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要了解机器学习中使用的性能指标的详细信息，请访问我以前的博客链接<strong class="is hj"> </strong> <a class="ae mo" rel="noopener" href="/@sachin.s1dn/performance-metrics-for-machine-learning-models-80d7666b432e"> <strong class="is hj">这里</strong> </a> <strong class="is hj">。</strong></p><p id="f52b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们已经为性能指标定义了一个函数，当我们调用它时，它会给出结果。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/96b0bc054af4a9a54049140e075fac6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*GCfox0c_cuXzN8UQ_JPlLQ.png"/></div></figure><h1 id="7f77" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">弓上的特征重要性</h1><p id="5bed" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">Wordcloud中的20大重要特性。要了解Wordcloud，请访问<a class="ae mo" href="https://pypi.org/project/wordcloud/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">这里</strong> </a>。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/9550a9d8e0668d9c0dcd0f64991fcd89.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*dlMr6xXcHlHMY-84qZVLDg.png"/></div></figure><p id="12db" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">类似地，我们构建了一个具有TFIDF、AvgWord2Vec、TFIDF_AvgWord2Vec特性的随机森林和XGBOOST模型。要了解完整代码，请访问我的<a class="ae mo" href="https://github.com/Sachin-D-N/Amazon_Food_Reviews/blob/main/08.Random_Forest_Amazon_Food_Reviews/Random_Forest_Amazon_Food_Reviews_Assignment_.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> GitHub </strong> </a>链接。</p><h1 id="253d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="dd50" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">为了在表格中写入震荡，我们使用了python库PrettyTable。</p><p id="0c72" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">pretty table是一个简单的Python库，旨在使在视觉上吸引人的表格中表示表格数据变得快速而简单。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es nm"><img src="../Images/aae66827c534a76bc14cf9727b34ec22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*J1iY59_aVC75CFl7tekbZg.png"/></div></figure><h1 id="933f" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">观察</h1><ol class=""><li id="927c" class="km kn hi is b it ko ix kp jb kq jf kr jj ks jn kt ku kv kw bi translated">从上表中，我们得出结论，最佳深度为60且最佳估计值为500的随机森林中的TFIDF具有最高的AUC得分，即93.40 %。</li><li id="48ad" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">具有最佳深度10和最佳估计量500的xgboost和TFIDF具有最高的AUC分数，即93.96 %。</li><li id="a136" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn kt ku kv kw bi translated">随机森林和xgboost模型在测试数据上都表现得相当好。</li></ol><p id="3a3c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要了解亚马逊食品评论数据集和特征的完整概述，请访问我以前的博客链接<a class="ae mo" rel="noopener" href="/analytics-vidhya/amazon-fine-food-reviews-featurization-with-natural-language-processing-a386b0317f56"> <strong class="is hj">这里</strong> </a> <strong class="is hj">。</strong></p><p id="3486" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要了解机器学习中使用的性能指标的详细信息，请访问我以前的博客链接<strong class="is hj"> </strong> <a class="ae mo" rel="noopener" href="/@sachin.s1dn/performance-metrics-for-machine-learning-models-80d7666b432e"> <strong class="is hj">这里</strong> </a> <strong class="is hj">。</strong></p><p id="aa8c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要了解完整代码请访问我的<a class="ae mo" href="https://github.com/Sachin-D-N/Amazon_Food_Reviews/blob/main/08.Random_Forest_Amazon_Food_Reviews/Random_Forest_Amazon_Food_Reviews_Assignment_.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> GitHub </strong> </a>链接。</p><h1 id="3947" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">参考</h1><ul class=""><li id="1479" class="km kn hi is b it ko ix kp jb kq jf kr jj ks jn ly ku kv kw bi translated">应用人工智能</li><li id="fbee" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn ly ku kv kw bi translated">维基百科(一个基于wiki技术的多语言的百科全书协作计划ˌ也是一部用不同语言写成的网络百科全书ˌ 其目标及宗旨是为全人类提供自由的百科全书)ˌ开放性的百科全书</li><li id="e5fc" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn ly ku kv kw bi translated">Coursera</li><li id="87a2" class="km kn hi is b it kx ix ky jb kz jf la jj lb jn ly ku kv kw bi translated">数据营</li></ul><p id="bdf9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">感谢您的阅读和耐心。我希望你喜欢这个帖子，如果我的帖子有错误，请告诉我。如果你发现帖子中有什么错误或者有什么要补充的，就在评论中讨论吧…</p><p id="860a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">快乐学习！！</p></div></div>    
</body>
</html>