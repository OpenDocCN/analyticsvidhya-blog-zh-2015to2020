<html>
<head>
<title>Feature Selection Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-techniques-2614b3b7efcd?source=collection_archive---------0-----------------------#2019-09-27">https://medium.com/analytics-vidhya/feature-selection-techniques-2614b3b7efcd?source=collection_archive---------0-----------------------#2019-09-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a01fc28696281c45077fba3802a7c9a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hpkGnhZXOmxKMBoH"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">卢卡斯·布拉塞克在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="ff6e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">特征选择技术</strong></p><p id="c621" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">特征选择是机器学习的核心概念之一，它会极大地影响模型的性能。您用来训练机器学习模型的数据特征对您可以实现的性能有着巨大的影响。</p><p id="1a16" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">识别相关特征的问题</strong></p><p id="0c0b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们都可能面临这样的问题:从一组数据中识别相关特征，并删除不相关或不太重要的特征，而这些特征对我们的目标变量没有太大贡献，以便实现我们模型的更高精度。</p><p id="690a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">更少的属性是可取的，因为它降低了模型的复杂性，并且更简单的模型更容易理解和解释。</p><p id="2276" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">特征选择可以通过多种方式完成，但大致有3类:</strong> <br/> 1。过滤方法<br/> 2。包装方法<br/> 3。嵌入式方法</p><ol class=""><li id="227d" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated"><strong class="ix hj">过滤方法</strong></li></ol><p id="3bad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这种方法中，您只需过滤并提取相关要素的子集。模型是在选择特征之后构建的。这里的过滤是使用相关矩阵完成的，最常用的是使用<a class="ae iu" href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">皮尔逊相关</a>和VIF</p><p id="2886" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">答】皮尔逊相关</strong></p><p id="0431" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">皮尔逊相关是一个介于-1和1之间的数字，表示两个变量线性相关的程度。皮尔逊相关也被称为“乘积矩相关系数”(PMCC)或简称为“相关性”</p><p id="a70c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">皮尔逊相关性仅适用于<a class="ae iu" href="https://www.spss-tutorials.com/measurement-levels/#metric-variable" rel="noopener ugc nofollow" target="_blank">度量变量</a></p><p id="e3fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">相关系数的值在-1到1之间</p><ul class=""><li id="07aa" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js kc jz ka kb bi translated">越接近0的值意味着相关性越弱(精确的0意味着没有相关性)</li><li id="4b28" class="jt ju hi ix b iy kd jc ke jg kf jk kg jo kh js kc jz ka kb bi translated">越接近1的值意味着越强的正相关性</li><li id="6541" class="jt ju hi ix b iy kd jc ke jg kf jk kg jo kh js kc jz ka kb bi translated">越接近-1的值意味着越强的负相关性</li></ul><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/51c42aa146b0e6e957c6c8d6ceede711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_X2U_tqOa1DhLJLC_Q9fg.png"/></div></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kn"><img src="../Images/0d0400c435b6a93584a1be9edcfec3a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_6JGku-qyR57lzQuU5nuw.png"/></div></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/f44a9cfd07917a3c66bf3d929cb9a06d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*ZnSc5SBkrmB8vaU0-9JA5g.png"/></div></figure><p id="5544" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，我们的目标(因变量)是mpg，从上图中，我们找出了与自变量的强相关性和弱相关性，并设置了阈值</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/9ed479331e986bc29044ef15352ae0bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*BJAtYhWbDK2ecuPKuF3Eqg.png"/></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/95dd1e7279b70b89a41366737033a96c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*UQbz-Gh-INJgQVw4I3VTnA.png"/></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/8d159ad706b3a782197ef3927c3937c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*bNkhxE77JU2az5VgErzu3A.png"/></div></figure><p id="00dc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的代码可以看出，变量cyl和disp彼此高度相关(0.902033)。因此，我们与目标变量进行比较，其中目标变量mpg与cyl高度相关，因此会保留和删除另一个变量。然后我们检查其他变量，直到最后一个变量，我们剩下四个特征wt，qsec，gear，carb。这些是皮尔逊相关给出的最终特征。</p><p id="af0b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> B】方差通货膨胀系数(VIF) </strong></p><p id="d324" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">共线性是指两个变量高度相关，并且包含给定数据集中有关方差的相似信息的状态。要检测变量之间的共线性，只需创建一个相关矩阵，并找到绝对值较大的变量。</p><p id="08d0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">实施VIF的步骤</strong></p><ul class=""><li id="aa6f" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js kc jz ka kb bi translated">计算VIF因子。</li><li id="54ca" class="jt ju hi ix b iy kd jc ke jg kf jk kg jo kh js kc jz ka kb bi translated">检查每个预测变量的因子，如果VIF在5-10之间，则可能存在多重共线性，您应该考虑删除该变量。</li></ul><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/30d8e8cbfc3bcb129256c3eb55c85d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1rsBYeXdxLlBxEZOCXeMMw.png"/></div></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kt"><img src="../Images/02e47ffdf85f4020d47e0df48eb3f0e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2nHHnd84qCn6OkAWN2iq8w.png"/></div></div></figure><p id="181e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">VIF方法选择了3个特征disp，vs，am。这些是VIF给出的最终特征。</p><p id="d244" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。包装方法</strong></p><ul class=""><li id="6071" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js kc jz ka kb bi translated">包装器方法需要一个机器学习算法，并使用其性能作为评估标准。</li><li id="fabd" class="jt ju hi ix b iy kd jc ke jg kf jk kg jo kh js kc jz ka kb bi translated">将特征馈送到所选的机器学习算法，并基于模型性能添加/移除特征。</li><li id="5908" class="jt ju hi ix b iy kd jc ke jg kf jk kg jo kh js kc jz ka kb bi translated">这是一个迭代且计算量大的过程，但比滤波方法更精确。</li></ul><p id="524e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">向前选择一步</strong></p><p id="54b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">前向选择是一种迭代方法，我们从模型中没有特征开始。在每一次迭代中，我们不断地添加最能改进我们模型的特性，直到添加一个新变量不能改进模型的性能。</p><figure class="kj kk kl km fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/3fe8c762729a58887bb1518908720d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*wx32Z6HR447u_Z694k374g.png"/></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kv"><img src="../Images/8a3c642a981a4a3a02f3ebc26cf500f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S88GUzkGhmNpv5n5DojSPQ.png"/></div></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/6e3f62135a1ce6ceba828685c4e50cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i2rywXm8NGSkJrZnP17Zkg.png"/></div></div></figure><p id="4f15" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> B】落后淘汰</strong></p><p id="c5f3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在向后消除中，我们从所有特征开始，并在每次迭代中移除最不重要的特征，这提高了模型的性能。我们重复这一过程，直到在删除特征时没有观察到改进。</p><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kx"><img src="../Images/e417b3c16cf3463c61fd980d58924b35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C4JKg6XUq9LhiS2BQ5dr-Q.png"/></div></div></figure><p id="4717" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">后向淘汰法选取了3个特征wt，qsec，am。这些是反向消去法给出的最终特征。</p><p id="a1b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> C】递归特征消除</strong></p><p id="8d7d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是一种贪婪的优化算法，旨在找到性能最佳的特征子集。它重复地创建模型，并在每次迭代中保留性能最好或最差的特性。它用剩下的特征构造下一个模型，直到所有的特征都用完。然后，它根据要素被消除的顺序对其进行排序。</p><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/46fbf61d8ae39931bc9c42219c111308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*08d4RdKTIS_MdBlfm3NhKA.png"/></div></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/141eaff476159836993febbc5a52c0ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NlIYsYXld-FWK5qua6PaSQ.png"/></div></div></figure><p id="4d0f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">递归特征消除法选取了3个特征wt，qsec，am。这些是递归特征消除给出的最终特征。</p><h1 id="e440" class="la lb hi bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">3.嵌入式方法</h1><p id="f50b" class="pw-post-body-paragraph iv iw hi ix b iy ly ja jb jc lz je jf jg ma ji jj jk mb jm jn jo mc jq jr js hb bi translated">嵌入式方法在某种意义上是迭代的，它负责模型训练过程的每次迭代，并仔细提取那些对特定迭代的训练贡献最大的特征。正则化方法是最常用的嵌入式方法，其在给定系数阈值的情况下惩罚特征。这里我们将使用套索正则化进行特征选择。如果特征是不相关的，lasso惩罚它的系数并使其为0。因此，系数= 0的特征被移除，其余的被采用。</p><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/80d26f6f440a754cdf930f51721bc7ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xiXQDfjCl9EU8o1BSs1IPQ.png"/></div></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/b351c9ac24f3ccf772a435e0a25684d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rLOxhrEqsdImqDfdpQ4nrA.png"/></div></div></figure><figure class="kj kk kl km fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/10e8288d3d8eaf831a7ce0bc06483593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5q6e2VUUhXyXYWkHs7HnQ.png"/></div></div></figure><h1 id="b8a7" class="la lb hi bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">结论:</h1><p id="7a62" class="pw-post-body-paragraph iv iw hi ix b iy ly ja jb jc lz je jf jg ma ji jj jk mb jm jn jo mc jq jr js hb bi translated">过滤器方法不结合机器学习模型来确定特征是好是坏，而包装器方法使用机器学习模型并训练它的特征来决定它是否重要。</p><p id="f251" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与包装器方法相比，过滤器方法要快得多，因为它们不涉及训练模型。另一方面，包装器方法在计算上是昂贵的，并且在大规模数据集的情况下，包装器方法不是要考虑的最有效的特征选择方法。</p></div></div>    
</body>
</html>