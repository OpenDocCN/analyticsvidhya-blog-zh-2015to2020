<html>
<head>
<title>Handwritten Digit Recognition using Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于逻辑回归的手写数字识别</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/handwritten-digit-recognition-using-logistic-regression-8d3b3f7e31c0?source=collection_archive---------8-----------------------#2019-11-26">https://medium.com/analytics-vidhya/handwritten-digit-recognition-using-logistic-regression-8d3b3f7e31c0?source=collection_archive---------8-----------------------#2019-11-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/df281a2c61fdc0248bcac0108e311714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O35lwO5O4sK0_9GuE5aG0A.png"/></div></div></figure><p id="6e0b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">逻辑回归是一种监督学习算法，用于解决分类问题，如将电子邮件分类为垃圾邮件或非垃圾邮件。这可以用来识别从0到9的手写数字。手写数字的给定图像属于十个数字(0到9)中的一个。这就是所谓的一个对所有的分类。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jo"><img src="../Images/c1f220558ac99b13e4c467000cd90b55.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/1*2uHYectXVaQrvYCpOp6xKQ.gif"/></div></div></figure><p id="b912" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们使用MNIST手写数字图像的子集作为我们的数据。有10个标签/类别(0到9)。正则化参数是对拟合参数的控制。高阶多项式表达式会产生过拟合问题，因此我们可以添加正则项来惩罚高阶多项式的参数，以获得更好的拟合。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="dcad" class="jy jz hi ju b fi ka kb l kc kd"># Read the data from mat file<br/>datafile = 'example_data.mat'<br/>mat = scipy.io.loadmat(datafile)<br/>X = mat['X']<br/>y = mat['y']<br/><br/>num_labels = 10 # There are 10 labels/classes (0 to 9)<br/><br/>reg_param = 0.1 # Regularization parameter</span></pre><p id="d8f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有5000个大小为(20，20)的图像，并且有5000个相应的标签。为了考虑截距项，我们将在X(偏差项)上增加一个额外的第一列，并将其全部设置为1。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="e92b" class="jy jz hi ju b fi ka kb l kc kd"># Size of training set<br/>[m, n] = numpy.shape(X) # 5000, 400<br/><br/>X = numpy.insert(X, 0, 1, axis = 1) # Adding the bias term</span></pre><p id="09c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们实现计算成本函数和梯度下降的函数。逻辑回归的成本函数如下所示，其中，m -样本数，λ-正则化参数，h(x) -假设函数，θ-拟合参数。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ke"><img src="../Images/3a9568d15ff67b9a642d469f0c8c32d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lSXD5tQ-AIrx2yJTqzxOCA.png"/></div></div></figure><p id="ca27" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将使用Python内置的优化求解函数fmin_cg来查找最小化成本函数的最佳参数。所有10个标签的参数都进行了优化，所以我们将为每个类创建y_boolean。比如说y = [0，0，0，1，1，1，2，2，2，3，3，3，4，4，5，5，6，6，6，7，7，8，8，9，9]。对于label = 4，y_boolean = [0，0，0，0，0，0，0，0，0，0，1，1，1，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0]。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="c250" class="jy jz hi ju b fi ka kb l kc kd">for i in range(num_labels):<br/>  print("Optimizing parameters for digit" ,i)<br/><br/>  y_bool = np.matrix([1 if label == i else 0 for label in y])<br/><br/>  updated_theta = scipy.optimize.fmin_cg(costFunction, fprime = gradientDescent, x0 = initial_theta, args = (X, y, reg_param), maxiter = 50)</span></pre><p id="92c2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们来预测一下。我们会用X和更新后的θ找到假设函数。对于每个例子，我们会得到10个不同的概率，分别属于这10个类别。我们将预测概率最高的类。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="8319" class="jy jz hi ju b fi ka kb l kc kd">h_max = sigmoid(X.dot(updated_theta.T))<br/><br/>for i in range(1, m):<br/>  max_prob = numpy.argmax(h_max[i-1, :])<br/>  p[i-1] = max_prob</span></pre><p id="e8ee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后将预测与给定的标签进行比较，以计算模型的准确性。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="eb9c" class="jy jz hi ju b fi ka kb l kc kd">for j in range(m):<br/>  if(p[i] == y[i]):<br/>    num_correct = num_correct + 1<br/><br/>accuracy = (num_correct / m) * 100</span></pre><p id="15cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请在下面的GitHub链接中找到完整的项目。</p><div class="kf kg ez fb kh ki"><a href="https://github.com/rgeetha2010/LogisticRegression" rel="noopener  ugc nofollow" target="_blank"><div class="kj ab dw"><div class="kk ab kl cl cj km"><h2 class="bd hj fi z dy kn ea eb ko ed ef hh bi translated">2010年目标/物流回归</h2><div class="kp l"><h3 class="bd b fi z dy kn ea eb ko ed ef dx translated">监督学习算法的Python库，逻辑回归。-rgeetha 2010/logistic regression</h3></div><div class="kq l"><p class="bd b fp z dy kn ea eb ko ed ef dx translated">github.com</p></div></div><div class="kr l"><div class="ks l kt ku kv kr kw io ki"/></div></div></a></div><p id="feed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">参考:<a class="ae kx" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/learn/machine-learning</a>？</p></div></div>    
</body>
</html>