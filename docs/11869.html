<html>
<head>
<title>Installing and using PySpark on Windows machine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Windows机器上安装和使用PySpark</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/installing-and-using-pyspark-on-windows-machine-59c2d64af76e?source=collection_archive---------0-----------------------#2020-12-22">https://medium.com/analytics-vidhya/installing-and-using-pyspark-on-windows-machine-59c2d64af76e?source=collection_archive---------0-----------------------#2020-12-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="c169" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">简化了安装步骤(并且在一定程度上实现了自动化…)</h2></div><p id="750c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下步骤已经在两种不同的Windows 10笔记本电脑上进行了尝试，两种不同的Spark版本(2.x.x)和Spark <strong class="iz hj"> 3.1.2 </strong>。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/b12946d9ae4a5f55053111f518d32e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tP-dw4Oj_42BYbkdtYbjMA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">图片来源:<a class="ae kj" href="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/Apache_Spark_logo.svg/1280px-Apache_Spark_logo.svg.png" rel="noopener ugc nofollow" target="_blank">https://upload . wikimedia . org/Wikipedia/commons/thumb/f/F3/Apache _ Spark _ logo . SVG/1280 px-Apache _ Spark _ logo . SVG . png</a></figcaption></figure><h1 id="4906" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">安装先决条件</h1><blockquote class="lc ld le"><p id="9bd8" class="ix iy lf iz b ja jb ij jc jd je im jf lg jh ji jj lh jl jm jn li jp jq jr js hb bi translated">PySpark需要Java版本7或更高版本和Python版本2.6或更高版本。</p></blockquote><ol class=""><li id="37f9" class="lj lk hi iz b ja jb jd je jg ll jk lm jo ln js lo lp lq lr bi translated"><strong class="iz hj"> Java </strong></li></ol><p id="9b84" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要检查Java是否已经可用并找到它的版本，请打开命令提示符并键入以下命令。</p><pre class="ju jv jw jx fd ls lt lu lv aw lw bi"><span id="9ea9" class="lx kl hi lt b fi ly lz l ma mb">java -version</span></pre><p id="1a06" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果上面的命令输出如下，那么您已经有了Java，因此可以跳过下面的步骤。</p><pre class="ju jv jw jx fd ls lt lu lv aw lw bi"><span id="854f" class="lx kl hi lt b fi ly lz l ma mb">java version "1.8.0_271"<br/>Java(TM) SE Runtime Environment (build 1.8.0_271-b09)<br/>Java HotSpot(TM) 64-Bit Server VM (build 25.271-b09, mixed mode)</span></pre><p id="55c2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Java有两个包:<strong class="iz hj"> JRE </strong>和<strong class="iz hj"> JDK </strong>。在两者之间使用哪一个取决于你是想只使用Java还是想为Java开发应用程序。</p><p id="2855" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以从甲骨文网站<a class="ae kj" href="https://www.oracle.com/java/technologies/javase-jre8-downloads.html" rel="noopener ugc nofollow" target="_blank">下载其中任何一个。</a></p><blockquote class="lc ld le"><p id="33f0" class="ix iy lf iz b ja jb ij jc jd je im jf lg jh ji jj lh jl jm jn li jp jq jr js hb bi translated">你是想运行Java程序，还是想开发Java程序？如果您想运行Java程序，但不想开发它们，请下载Java运行时环境，或JRE</p><p id="4a34" class="ix iy lf iz b ja jb ij jc jd je im jf lg jh ji jj lh jl jm jn li jp jq jr js hb bi translated">如果你想为Java开发应用程序，下载Java开发工具包，或JDK。JDK包含JRE，因此您不必分别下载两者。</p></blockquote><p id="2cc1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">对于我们的例子，我们只想使用Java，因此我们将下载JRE文件</strong>。<br/> 1。根据您的Windows是32位还是64位，下载Windows x86(如<a class="ae kj" href="https://www.oracle.com/java/technologies/javase-jre8-downloads.html#license-lightbox" rel="noopener ugc nofollow" target="_blank">jre-8u271-windows-i586.exe</a>或Windows x64(<a class="ae kj" href="https://www.oracle.com/java/technologies/javase-jre8-downloads.html#license-lightbox" rel="noopener ugc nofollow" target="_blank">jre-8u271-windows-x64.exe</a>)版本2。该网站可能会要求注册，在这种情况下，你可以使用你的电子邮件id <br/> 3注册。下载后运行安装程序。</p><p id="eadc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">注</strong>:以上两个<code class="du mc md me lt b">.exe</code>文件需要管理员权限才能安装。</p><p id="f988" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您没有机器的管理员权限，请下载<code class="du mc md me lt b">.tar.gz</code>版本(例如<a class="ae kj" href="https://download.oracle.com/otn/java/jdk/8u271-b09/61ae65e088624f5aaa0b1d2d801acb16/jre-8u271-windows-x64.tar.gz" rel="noopener ugc nofollow" target="_blank">jre-8u271-windows-x64.tar.gz</a>)。然后，解压缩下载的文件，你就有了一个Java JRE或JDK安装。<br/>你可以使用<a class="ae kj" href="https://www.7-zip.org/download.html" rel="noopener ugc nofollow" target="_blank"> 7zip </a>解压文件。解压<code class="du mc md me lt b">.tar.gz</code>文件将得到一个<code class="du mc md me lt b">.tar</code>文件——使用7zip再解压一次。<br/>或者您可以在cmd中运行以下命令来提取下载的文件:</p><pre class="ju jv jw jx fd ls lt lu lv aw lw bi"><span id="d518" class="lx kl hi lt b fi ly lz l ma mb">tar -xvzf jre-8u271-windows-x64.tar.gz</span></pre><p id="1705" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">记下Java的安装位置，因为我们稍后将需要该路径。</p><p id="8c3d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.<strong class="iz hj"> Python </strong></p><p id="bc0d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用蟒蛇安装-<a class="ae kj" href="https://www.anaconda.com/products/individual" rel="noopener ugc nofollow" target="_blank">https://www.anaconda.com/products/individual</a></p><p id="3463" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用以下命令检查Python的版本。</p><pre class="ju jv jw jx fd ls lt lu lv aw lw bi"><span id="4097" class="lx kl hi lt b fi ly lz l ma mb">python --version</span></pre><p id="4476" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您已经使用Anaconda安装了它，请在Anaconda提示符下运行上面的命令。它应该给出如下所示的输出。</p><pre class="ju jv jw jx fd ls lt lu lv aw lw bi"><span id="0c9f" class="lx kl hi lt b fi ly lz l ma mb">Python 3.7.9</span></pre><p id="2c87" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">注意</strong> : Spark 2.x.x不支持Python 3.8。请安装python 3.7.x，更多信息请参考<a class="ae kj" href="https://stackoverflow.com/questions/62208730/pyspark-2-4-5-is-not-compatible-with-python-3-8-3-how-do-i-solve-this" rel="noopener ugc nofollow" target="_blank">本stackoverflow问题</a>。Spark 3.x.x支持Python 3.8。</p><h1 id="9de2" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">脚本安装</h1><p id="be4d" class="pw-post-body-paragraph ix iy hi iz b ja mf ij jc jd mg im jf jg mh ji jj jk mi jm jn jo mj jq jr js hb bi translated">以下步骤可以作为批处理文件编写脚本并一次性运行。脚本已在下面的演练后提供。</p><h1 id="2d2d" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">获取火花文件</h1><p id="e055" class="pw-post-body-paragraph ix iy hi iz b ja mf ij jc jd mg im jf jg mh ji jj jk mi jm jn jo mj jq jr js hb bi translated">从<a class="ae kj" href="https://archive.apache.org/dist/spark/" rel="noopener ugc nofollow" target="_blank"> Apache spark下载</a>网站下载所需的Spark版本文件。获取' spark-x.x.x-bin-hadoop2.7.tgz '文件，例如<a class="ae kj" href="https://archive.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">spark-2 . 4 . 3-bin-Hadoop 2.7 . tgz</a>。</p><p id="b9f5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Spark 3.x.x也附带了Hadoop 3.2，但是这个Hadoop版本在编写Parquet文件时会导致错误，所以建议使用Hadoop 2.7。</p><blockquote class="lc ld le"><p id="e715" class="ix iy lf iz b ja jb ij jc jd je im jf lg jh ji jj lh jl jm jn li jp jq jr js hb bi translated">对所选spark版本的剩余步骤进行相应的更改。</p></blockquote><p id="78e9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可以使用7zip解压文件。解压<code class="du mc md me lt b">.tgz</code>文件将得到一个<code class="du mc md me lt b">.tar</code>文件——使用7zip再解压一次。或者，您可以在cmd中对下载的文件运行以下命令来提取它:</p><pre class="ju jv jw jx fd ls lt lu lv aw lw bi"><span id="783c" class="lx kl hi lt b fi ly lz l ma mb">tar -xvzf spark-2.4.3-bin-hadoop2.7.tgz</span></pre><h1 id="0c08" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">把所有东西放在一起</h1><h2 id="0af1" class="lx kl hi bd km mk ml mm kq mn mo mp ku jg mq mr kw jk ms mt ky jo mu mv la mw bi translated">设置文件夹</h2><p id="8944" class="pw-post-body-paragraph ix iy hi iz b ja mf ij jc jd mg im jf jg mh ji jj jk mi jm jn jo mj jq jr js hb bi translated">在您选择的位置为spark安装创建一个文件夹。如<code class="du mc md me lt b">C:\spark_setup</code>。</p><p id="8139" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">提取spark文件并将文件夹粘贴到选择的文件夹:<code class="du mc md me lt b">C:\spark_setup\spark-2.4.3-bin-hadoop2.7</code></p><h2 id="98cc" class="lx kl hi bd km mk ml mm kq mn mo mp ku jg mq mr kw jk ms mt ky jo mu mv la mw bi translated">添加winutils.exe</h2><p id="b90e" class="pw-post-body-paragraph ix iy hi iz b ja mf ij jc jd mg im jf jg mh ji jj jk mi jm jn jo mj jq jr js hb bi translated">从这个<a class="ae kj" href="https://github.com/steveloughran/winutils" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>下载对应Spark和Hadoop版本的winutils.exe文件。</p><p id="1432" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们正在使用Hadoop 2.7，因此从<code class="du mc md me lt b">hadoop-2.7.1/bin/</code>下载winutils.exe。</p><p id="d071" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下面的路径中复制并替换这个文件(创建<code class="du mc md me lt b">\hadoop\bin</code>目录)</p><ul class=""><li id="2682" class="lj lk hi iz b ja jb jd je jg ll jk lm jo ln js mx lp lq lr bi translated"><code class="du mc md me lt b">C:\spark_setup\spark-2.4.3-bin-hadoop2.7\bin</code></li><li id="45c5" class="lj lk hi iz b ja my jd mz jg na jk nb jo nc js mx lp lq lr bi translated"><code class="du mc md me lt b">C:\spark_setup\spark-2.4.3-bin-hadoop2.7\hadoop\bin</code></li></ul><h2 id="2363" class="lx kl hi bd km mk ml mm kq mn mo mp ku jg mq mr kw jk ms mt ky jo mu mv la mw bi translated">设置环境变量</h2><p id="638b" class="pw-post-body-paragraph ix iy hi iz b ja mf ij jc jd mg im jf jg mh ji jj jk mi jm jn jo mj jq jr js hb bi translated">我们必须设置下面的环境变量，让spark知道所需的文件在哪里。</p><p id="02d3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在开始菜单中输入“环境变量”并选择“<strong class="iz hj">编辑系统环境变量</strong></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nd"><img src="../Images/341735d1b5aa50c8ffd012f2cff8f2cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*tMnrH8OgUbWPI5A1Om438A.png"/></div></figure><p id="ada6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">点击“<strong class="iz hj">环境变量… </strong>”</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ne"><img src="../Images/f073138e68f0d2f99ca64398f0236950.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*fzjEePV9w-V7nqPPWP-dWQ.png"/></div></figure><p id="29db" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">添加'<strong class="iz hj">新… </strong>'变量</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nf"><img src="../Images/04c6006066fb742640436cb56ad1e143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*kkxhwn919bYUVrCRD06Frg.png"/></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ng"><img src="../Images/a3ef22833dce50e2b39e2691627cc97d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*yf4vdg5Q8-o2oCNRYRVcig.png"/></div></figure><ol class=""><li id="7057" class="lj lk hi iz b ja jb jd je jg ll jk lm jo ln js lo lp lq lr bi translated">变量名:<code class="du mc md me lt b">SPARK_HOME</code> <br/>变量值:<code class="du mc md me lt b">C:\spark_setup\spark-2.4.3-bin-hadoop2.7</code>(设置文件夹路径)</li><li id="d64f" class="lj lk hi iz b ja my jd mz jg na jk nb jo nc js lo lp lq lr bi translated">变量名:<code class="du mc md me lt b">HADOOP_HOME</code> <br/>变量值:<code class="du mc md me lt b">C:\spark_setup\spark-2.4.3-bin-hadoop2.7\hadoop</code> <br/>或<br/>变量值:<code class="du mc md me lt b">%SPARK_HOME%\hadoop</code></li><li id="1137" class="lj lk hi iz b ja my jd mz jg na jk nb jo nc js lo lp lq lr bi translated">变量名:<code class="du mc md me lt b">JAVA_HOME</code> <br/>变量值:设置为Java安装文件夹，如<code class="du mc md me lt b">C:\Program Files\Java\jre1.8.0_271</code> <br/>根据上面安装的版本在“程序文件”或“程序文件(x86)”中查找。如果您使用的是<code class="du mc md me lt b">.tar.gz</code>版本，请将路径设置为解压缩的位置。</li><li id="fb87" class="lj lk hi iz b ja my jd mz jg na jk nb jo nc js lo lp lq lr bi translated">变量名:<code class="du mc md me lt b">PYSPARK_PYTHON</code> <br/>变量值:<code class="du mc md me lt b">python</code> <br/>该环境变量是确保涉及python工作器的任务(如UDF)正常工作所必需的。参见<a class="ae kj" href="https://stackoverflow.com/questions/56213955/python-worker-failed-to-connect-back-in-pyspark-or-spark-version-2-3-1" rel="noopener ugc nofollow" target="_blank">本栈溢出岗位</a>。</li><li id="8f61" class="lj lk hi iz b ja my jd mz jg na jk nb jo nc js lo lp lq lr bi translated">选择'<strong class="iz hj">路径</strong>变量，点击'<strong class="iz hj">编辑…【T38]'</strong></li></ol><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es nh"><img src="../Images/bc08d0cbbabed718ab074a21bee5c2a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*-emMdQkX4UUeTiSqBXivRA.png"/></div></figure><p id="63f2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">点击'<strong class="iz hj">新建</strong> ' <strong class="iz hj"> </strong>，添加火花仓路径，如<code class="du mc md me lt b">C:\spark_setup\spark-2.4.3-bin-hadoop2.7\bin</code>或<code class="du mc md me lt b">%SPARK_HOME%\bin</code></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ni"><img src="../Images/3a9c8561f6feb52ae4500042c4193788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*Df_R1Gi1kAz4VE64C8PDWQ.png"/></div></figure><p id="9feb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">已经设置了所有必需的环境变量。</p><p id="b2c9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">可选变量</strong>:如果您想在Jupyter笔记本上使用PySpark，请设置以下变量。如果没有设置，PySpark会话将在控制台上启动。</p><ol class=""><li id="12d2" class="lj lk hi iz b ja jb jd je jg ll jk lm jo ln js lo lp lq lr bi translated">变量名:<code class="du mc md me lt b">PYSPARK_DRIVER_PYTHON</code> <br/>变量值:<code class="du mc md me lt b">jupyter</code></li><li id="52f6" class="lj lk hi iz b ja my jd mz jg na jk nb jo nc js lo lp lq lr bi translated">变量名:<code class="du mc md me lt b">PYSPARK_DRIVER_PYTHON_OPTS</code> <br/>变量值:<code class="du mc md me lt b">notebook</code></li></ol><h1 id="251a" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">脚本安装</h1><p id="bf33" class="pw-post-body-paragraph ix iy hi iz b ja mf ij jc jd mg im jf jg mh ji jj jk mi jm jn jo mj jq jr js hb bi translated">编辑并使用下面的脚本来(几乎)自动化PySpark设置过程。</p><blockquote class="lc ld le"><p id="ba29" class="ix iy lf iz b ja jb ij jc jd je im jf lg jh ji jj lh jl jm jn li jp jq jr js hb bi translated">没有自动化的步骤:<strong class="iz hj"> Java &amp; Python安装</strong>，以及<strong class="iz hj">更新“路径”变量</strong>。</p><p id="ecbc" class="ix iy lf iz b ja jb ij jc jd je im jf lg jh ji jj lh jl jm jn li jp jq jr js hb bi translated">在运行脚本之前安装Java和Python，并在运行脚本之后编辑“Path”变量，如上面的演练中所述。</p></blockquote><p id="2bac" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">针对所需的spark版本和安装路径对脚本进行更改。保存为<code class="du mc md me lt b">.bat</code>文件，双击运行。</p><figure class="ju jv jw jx fd jy"><div class="bz dy l di"><div class="nj nk l"/></div></figure></div><div class="ab cl nl nm gp nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="hb hc hd he hf"><h1 id="a972" class="kk kl hi bd km kn ns kp kq kr nt kt ku io nu ip kw ir nv is ky iu nw iv la lb bi translated">在Windows上以独立模式使用PySpark</h1><p id="68da" class="pw-post-body-paragraph ix iy hi iz b ja mf ij jc jd mg im jf jg mh ji jj jk mi jm jn jo mj jq jr js hb bi translated">如果下面的命令不起作用，你可能需要在上述步骤后重启你的机器。</p><h2 id="4d26" class="lx kl hi bd km mk ml mm kq mn mo mp ku jg mq mr kw jk ms mt ky jo mu mv la mw bi translated">命令</h2><p id="8b4c" class="pw-post-body-paragraph ix iy hi iz b ja mf ij jc jd mg im jf jg mh ji jj jk mi jm jn jo mj jq jr js hb bi translated">每个命令都在单独的Anaconda提示符下运行</p><ol class=""><li id="bf39" class="lj lk hi iz b ja jb jd je jg ll jk lm jo ln js lo lp lq lr bi translated"><strong class="iz hj">部署Master <br/> </strong> <code class="du mc md me lt b">spark-class.cmd org.apache.spark.deploy.master.Master -h 127.0.0.1<br/></code>打开浏览器，导航到:<a class="ae kj" href="http://localhost:8080/." rel="noopener ugc nofollow" target="_blank"> http://localhost:8080/。这是SparkUI。</a></li><li id="f9d0" class="lj lk hi iz b ja my jd mz jg na jk nb jo nc js lo lp lq lr bi translated"><strong class="iz hj">正在部署工人<br/> </strong> <code class="du mc md me lt b">spark-class.cmd org.apache.spark.deploy.worker.Worker spark://127.0.0.1:7077<br/></code> SparkUI将显示工人状态。</li></ol><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es nx"><img src="../Images/7142a4294b582f1ec3a2399167fb201e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x3dvmgZ3AkOTkOuzABIiRA.png"/></div></div></figure><p id="874a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.<strong class="iz hj"> PySpark外壳<br/> </strong> <code class="du mc md me lt b">pyspark --master spark://127.0.0.1:7077 --num-executors 1 --executor-cores 1 --executor-memory 4g --driver-memory 2g --conf spark.dynamicAllocation.enabled=false</code></p><p id="7830" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据机器配置调整<code class="du mc md me lt b">num-executors</code>、<code class="du mc md me lt b">executor-cores</code>、<code class="du mc md me lt b">executor-memory</code>和<code class="du mc md me lt b">driver-memory</code>。SparkUI将显示PySparkShell会话列表。</p><p id="e25c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在运行上面的pyspark命令之前，在第三个anaconda提示符中激活所需的python环境。</p><p id="3791" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您已经设置了<code class="du mc md me lt b">PYSPARK_DRIVER_PYTHON</code>和<code class="du mc md me lt b">PYSPARK_DRIVER_PYTHON_OPTS</code>环境变量，上面的命令将打开Jupyter Notebook而不是pyspark shell。</p><h1 id="d22a" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">供选择的</h1><p id="13e0" class="pw-post-body-paragraph ix iy hi iz b ja mf ij jc jd mg im jf jg mh ji jj jk mi jm jn jo mj jq jr js hb bi translated">运行以下命令，使用计算机上的所有可用资源启动pyspark (shell或jupyter)会话。在运行pyspark命令之前，激活所需的python环境。</p><p id="9c88" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du mc md me lt b">pyspark --master local[*]</code></p></div><div class="ab cl nl nm gp nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="hb hc hd he hf"><p id="14bb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果任何步骤出错或者你面临任何问题，请在评论中告诉我。</p></div></div>    
</body>
</html>