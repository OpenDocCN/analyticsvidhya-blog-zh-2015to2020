<html>
<head>
<title>Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本挖掘101:使用潜在语义分析的主题建模逐步介绍(使用Python)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-mining-101-a-stepwise-introduction-to-topic-modeling-using-latent-semantic-analysis-using-add9c905efd9?source=collection_archive---------0-----------------------#2018-10-01">https://medium.com/analytics-vidhya/text-mining-101-a-stepwise-introduction-to-topic-modeling-using-latent-semantic-analysis-using-add9c905efd9?source=collection_archive---------0-----------------------#2018-10-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/ad73d4406ed0b79c71a7bffdda2c9db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lnEcfAgjdqEUtdFnkF3SFQ.jpeg"/></div></figure><p id="9546" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你去过维护良好的图书馆吗？我总是对图书馆员按名称、内容和其他主题组织一切的方式印象深刻。但是，如果你给这些图书管理员几千本书，并要求他们根据他们的流派来安排每本书，他们将很难在一天内完成这项任务，更不用说一个小时了！</p><p id="2dc2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">然而，如果这些书是数字格式的，这就不会发生在你身上，对吗？所有的安排似乎都在几秒钟内完成，不需要任何人工操作。所有人都欢呼自然语言处理(NLP)。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es jk"><img src="../Images/63427ef7c5562fc848fcb4c3cff54947.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/0*nTmWTGqs5h8E4Q6I.gif"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><em class="jt">资料来源:confessionsofabookgeek.com</em></figcaption></figure><p id="0ec7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">看看下面的文字片段:</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es ju"><img src="../Images/3fe50979309cd6ae3a4d1088ad40885f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/0*Gaclb1F9PgWMN8nT.png"/></div></figure><p id="89ee" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从突出显示的文本中可以看出，有三个主题(或概念)——主题1、主题2和主题3。一个好的主题模型会识别相似的单词，并将它们放在一个组或主题下。上面例子中最主要的主题是主题2，这表明这段文字主要是关于假视频的。</p><p id="76aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">好奇了吗？很好！在本文中，我们将学习一种叫做主题建模的文本挖掘方法。这是一种非常有用的抽取主题的技术，当你面临NLP挑战时，你会经常用到它。</p><p id="5cc3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jv">注:我强烈推荐阅读本文</em><a class="ae jw" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/" rel="noopener ugc nofollow" target="_blank"><em class="jv"/></a><em class="jv">来理解SVD和UMAP这样的术语。本文利用了它们，因此对它们有一个基本的了解将有助于巩固这些概念。</em></p><h1 id="eeb3" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">目录</h1><ol class=""><li id="0c78" class="kv kw hi io b ip kx it ky ix kz jb la jf lb jj lc ld le lf bi translated">什么是主题模型？</li><li id="1931" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated">什么时候使用主题建模？</li><li id="a394" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated">潜在语义分析概述(LSA)</li><li id="1512" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated">LSA在Python中的实现</li></ol><ul class=""><li id="b023" class="kv kw hi io b ip iq it iu ix ll jb lm jf ln jj lo ld le lf bi translated">数据读取和检查</li><li id="c753" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lo ld le lf bi translated">数据预处理</li><li id="0ef9" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lo ld le lf bi translated">文档术语矩阵</li><li id="4093" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lo ld le lf bi translated">主题建模</li><li id="aa83" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lo ld le lf bi translated">主题可视化</li></ul><p id="7771" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">5.LSA的利与弊</p><p id="00a8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">6.主题建模的其他技术</p><h1 id="3151" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">什么是主题模型？</h1><p id="a96d" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">主题模型可以定义为一种无监督的技术，用于发现各种文本文档中的主题。这些主题本质上是抽象的，即彼此相关的单词形成一个主题。同样，一个文档中可以有多个主题。现在，让我们将主题模型理解为一个黑盒，如下图所示:</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/e693b71506d6d5b96567947f775872c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*o4QTTXD7nHjsYQsK.png"/></div></figure><p id="181e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个黑盒(主题模型)形成了被称为主题的相似和相关单词的集群。这些主题在一个文档中有一定的分布，每个主题都是由它所包含的不同词的比例来定义的。</p><h1 id="6481" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">什么时候使用主题建模？</h1><p id="ab1d" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">回想一下我们之前看到的将相似的书排列在一起的例子。现在假设您必须用几个数字文本文档执行类似的任务。只要文档的数量是可管理的(也就是不要太多)，您就可以手动完成这个任务。但是当这些数字文本文档的数量多到无法想象的时候，会发生什么呢？</p><p id="58d0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这就是NLP技术脱颖而出的地方。对于这个特殊的任务，主题建模是我们将要用到的技术。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/20c41065ec1c7547cf5e67f8d1010cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*E_P2Vkt9ZxrgG1gw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><em class="jt">资料来源:topix.io/tutorial/tutorial.html</em></figcaption></figure><p id="10aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">主题建模有助于探索大量的文本数据、发现单词簇、文档间的相似性以及发现抽象主题。似乎这些理由还不够令人信服，主题建模也被用在搜索引擎中，其中搜索字符串与结果相匹配。越来越有趣了，不是吗？好吧，那就接着读吧！</p><h1 id="d0b5" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">潜在语义分析概述(LSA)</h1><p id="aa32" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">所有语言都有自己的错综复杂和细微差别，机器很难捕捉到(有时它们甚至会被我们人类误解！).这可以包括意思相同的不同单词，也可以包括拼写相同但意思不同的单词。</p><p id="f3dc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">例如，考虑下面两句话:</p><ol class=""><li id="65f3" class="kv kw hi io b ip iq it iu ix ll jb lm jf ln jj lc ld le lf bi translated">我非常喜欢他的上一部<strong class="io hj">小说</strong>。</li><li id="f9a5" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lc ld le lf bi translated">我们想做一个<strong class="io hj">小说</strong>营销活动。</li></ol><p id="7a60" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在第一句中，单词“novel”指的是一本书，在第二句中它指的是新的或新鲜的。</p><p id="438f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以很容易地区分这些单词，因为我们能够理解这些单词背后的上下文。然而，机器将不能捕捉这一概念，因为它不能理解单词被使用的上下文。这就是潜在语义分析(LSA)发挥作用的地方，因为它试图利用单词周围的上下文来捕捉隐藏的概念，也称为主题。</p><p id="c020" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，简单地将单词映射到文档并不会真正有所帮助。我们真正需要的是搞清楚文字背后隐藏的概念或话题。LSA就是这样一种技术，可以发现这些隐藏的主题。现在让我们深入了解LSA的内部运作。</p><h1 id="7cc0" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">执行LSA所涉及的步骤</h1><p id="1152" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">假设我们有<strong class="io hj"> m </strong>个文本文档，总共有<strong class="io hj"> n </strong>个唯一术语(单词)。我们希望从文档的所有文本数据中提取出k个主题。主题的数量k必须由用户指定。</p><ul class=""><li id="7408" class="kv kw hi io b ip iq it iu ix ll jb lm jf ln jj lo ld le lf bi translated">生成具有TF-IDF分数<strong class="io hj">的形状<strong class="io hj"> m x n </strong>的文档术语矩阵。</strong></li></ul><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/a1d531ed6215fde86a437539ea951c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/0*Gq8AgWwkpQ-pvfaD.png"/></div></figure><ul class=""><li id="ccdf" class="kv kw hi io b ip iq it iu ix ll jb lm jf ln jj lo ld le lf bi translated">然后，我们将使用奇异值分解(SVD)将上述矩阵的维数减少到<strong class="io hj"> k </strong>(所需主题的数量)维。</li><li id="d54f" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lo ld le lf bi translated">SVD将一个矩阵分解成另外三个矩阵。假设我们想用SVD分解一个矩阵A。它将被分解成矩阵U、矩阵S和VT(矩阵V的转置)。</li></ul><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/b0336fe36cb3435318f3439e2c08de3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/0*ZRAsqV_YN0OOl-iC.png"/></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="er es lw"><img src="../Images/a93f29e5769d274c8e6849de2a9b816e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RQ7tGRBA6wiMeGFD.png"/></div></div></figure><p id="fefc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">矩阵<strong class="io hj"> Uk(文档-术语矩阵)</strong>中的每一行都是相应文档的向量表示。这些向量的长度是k，这是期望主题的数量。我们数据中术语的向量表示可以在矩阵<strong class="io hj"> Vk(术语-主题矩阵)</strong>中找到。</p><ul class=""><li id="cc0c" class="kv kw hi io b ip iq it iu ix ll jb lm jf ln jj lo ld le lf bi translated">因此，SVD为我们的数据中的每个文档和术语提供了向量。每个向量的长度将是<strong class="io hj"> k </strong>。然后，我们可以使用余弦相似度方法，使用这些向量来查找相似的单词和相似的文档。</li></ul><h1 id="aae6" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">LSA在Python中的实现</h1><p id="d981" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">是时候启动Python并了解如何在主题建模问题中实现LSA了。一旦您的Python环境打开，请遵循我在下面提到的步骤。</p><h2 id="5e7e" class="mb jy hi bd jz mc md me kd mf mg mh kh ix mi mj kl jb mk ml kp jf mm mn kt mo bi translated">数据读取和检查</h2><p id="b726" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">让我们在继续做任何事情之前加载所需的库。</p><pre class="jl jm jn jo fd mp mq mr ms aw mt bi"><span id="3b4a" class="mb jy hi mq b fi mu mv l mw mx">import numpy as np <br/>import pandas as pd <br/>import matplotlib.pyplot as plt <br/>pd.set_option("display.max_colwidth", 200)</span></pre><p id="06f5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">在本文中，我们将使用来自<em class="jv"> sklearn </em>的“20个新闻组”数据集。你可以在这里下载数据集</strong><a class="ae jw" href="https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups" rel="noopener ugc nofollow" target="_blank"><strong class="io hj"/></a><strong class="io hj"/>并跟随代码。</p><pre class="jl jm jn jo fd mp mq mr ms aw mt bi"><span id="9049" class="mb jy hi mq b fi mu mv l mw mx">from sklearn.datasets import fetch_20newsgroups </span><span id="3250" class="mb jy hi mq b fi my mv l mw mx">dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes')) </span><span id="e486" class="mb jy hi mq b fi my mv l mw mx">documents = dataset.data </span><span id="b319" class="mb jy hi mq b fi my mv l mw mx">len(documents)</span></pre><p id="ae87" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">输出:</strong>11314</p><pre class="jl jm jn jo fd mp mq mr ms aw mt bi"><span id="9241" class="mb jy hi mq b fi mu mv l mw mx">dataset.target_names</span></pre><p id="2da7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">输出:</strong> ['alt .无神论'，' comp.graphics '，' comp.os.ms-windows.misc '，' comp.sys.ibm.pc.hardware '，' comp.windows.x '，' misc.forsale '，' rec.autos '，' rec.motorcycles '，' rec.sport.hockey '，' sci.crypt '，' sci.electronics '，' sci.med '，' sci.space '，' soc.religion.christian '，' talk.politics.guns '，' talk.politics.mideast '，' talk . politi</p><p id="9227" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该数据集有11，314个文本文档，分布在20个不同的新闻组中。</p><h2 id="dcde" class="mb jy hi bd jz mc md me kd mf mg mh kh ix mi mj kl jb mk ml kp jf mm mn kt mo bi translated">数据预处理</h2><p id="8914" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">首先，我们将尽可能地清理我们的文本数据。这个想法是使用regex <em class="jv"> replace("[^a-zA-Z#]"，" ")</em>一次性删除标点符号、数字和特殊字符，这将替换除了带空格的字母之外的所有内容。然后我们会删除较短的单词，因为它们通常不包含有用的信息。最后，我们将使所有的文本小写，以消除区分大小写。</p><pre class="jl jm jn jo fd mp mq mr ms aw mt bi"><span id="6464" class="mb jy hi mq b fi mu mv l mw mx">news_df = pd.DataFrame({'document':documents}) </span><span id="02bb" class="mb jy hi mq b fi my mv l mw mx"># remove everything except alphabets` <br/>news_df['clean_doc'] = news_df['document'].str.replace("[^a-zA-Z]", " ") </span><span id="3cd3" class="mb jy hi mq b fi my mv l mw mx"># remove short words <br/>news_df['clean_doc']=news_df['clean_doc'].apply(lambda x:' '.join([w for w in x.split() if len(w)&gt;3])) </span><span id="2122" class="mb jy hi mq b fi my mv l mw mx"># make all text lowercase <br/>news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())</span></pre><p id="f7fd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从文本数据中删除停用词是一个很好的做法，因为它们大多杂乱无章，几乎没有任何信息。停用词是像“它”、“他们”、“am”、“been”、“about”、“因为”、“while”等这样的术语。</p><p id="e407" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了从文档中移除停用词，我们必须对文本进行标记化，即将文本串分割成单独的标记或词。一旦我们移除了停用词，我们将把这些标记重新缝合在一起。</p><pre class="jl jm jn jo fd mp mq mr ms aw mt bi"><span id="aaa2" class="mb jy hi mq b fi mu mv l mw mx">from nltk.corpus import stopwords <br/>stop_words = stopwords.words('english') </span><span id="9090" class="mb jy hi mq b fi my mv l mw mx"># tokenization <br/>tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split()) </span><span id="3a0c" class="mb jy hi mq b fi my mv l mw mx"># remove stop-words <br/>tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) </span><span id="52ca" class="mb jy hi mq b fi my mv l mw mx"># de-tokenization <br/>detokenized_doc = [] <br/>for i in range(len(news_df)): <br/>    t = ' '.join(tokenized_doc[i]) <br/>    detokenized_doc.append(t) </span><span id="e20f" class="mb jy hi mq b fi my mv l mw mx">news_df['clean_doc'] = detokenized_doc</span></pre><h1 id="4eee" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">文档术语矩阵</h1><p id="c6e4" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">这是主题建模的第一步。我们将使用sklearn的<em class="jv">tfidf矢量器</em>创建一个包含1000个术语的文档术语矩阵。</p><pre class="jl jm jn jo fd mp mq mr ms aw mt bi"><span id="5f25" class="mb jy hi mq b fi mu mv l mw mx">from sklearn.feature_extraction.text import TfidfVectorizer </span><span id="65d9" class="mb jy hi mq b fi my mv l mw mx">vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True) </span><span id="a2dd" class="mb jy hi mq b fi my mv l mw mx">X = vectorizer.fit_transform(news_df['clean_doc']) </span><span id="2405" class="mb jy hi mq b fi my mv l mw mx">X.shape</span></pre><p id="c0a3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">输出:</strong> (11314，1000)</p><p id="e1fb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以使用所有的项来创建这个矩阵，但是这将需要相当多的计算时间和资源。因此，我们将功能的数量限制为1，000。如果你有计算能力，我建议尝试所有的条款。</p><h2 id="19a7" class="mb jy hi bd jz mc md me kd mf mg mh kh ix mi mj kl jb mk ml kp jf mm mn kt mo bi translated">主题建模</h2><p id="2011" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">下一步是将每个术语和文档表示为一个向量。我们将使用文档术语矩阵，并将其分解成多个矩阵。我们将使用sklearn的<em class="jv"> TruncatedSVD </em>来执行矩阵分解的任务。</p><p id="87fb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于数据来自20个不同的新闻组，让我们试着为文本数据设置20个主题。主题的数量可以通过使用<em class="jv"> n_components </em>参数来指定。</p><pre class="jl jm jn jo fd mp mq mr ms aw mt bi"><span id="aa81" class="mb jy hi mq b fi mu mv l mw mx">from sklearn.decomposition import TruncatedSVD </span><span id="85ea" class="mb jy hi mq b fi my mv l mw mx"># SVD represent documents and terms in vectors <br/>svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122) </span><span id="d6de" class="mb jy hi mq b fi my mv l mw mx">svd_model.fit(X) </span><span id="43c3" class="mb jy hi mq b fi my mv l mw mx">len(svd_model.components_)</span></pre><p id="46c0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">输出:</strong> 20</p><p id="3576" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jv"> svd_model </em>的组件是我们的主题，我们可以使用<em class="jv"> svd_model.components_ </em>来访问它们。最后，让我们在20个主题的每一个中打印几个最重要的单词，看看我们的模型做得如何。</p><pre class="jl jm jn jo fd mp mq mr ms aw mt bi"><span id="8d82" class="mb jy hi mq b fi mu mv l mw mx">terms = vectorizer.get_feature_names() <br/>for i, comp in enumerate(svd_model.components_): <br/>    terms_comp = zip(terms, comp) <br/>    sorted_terms = sorted(terms_comp, key= lambda x:x[1],        <br/>    reverse=True)[:7] <br/>    print("Topic "+str(i)+": ") <br/>    for t in sorted_terms: <br/>        print(t[0]) <br/>        print(" ")</span></pre><p id="e2ad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">输出:</strong></p><p id="0b94" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">话题0:喜欢知道的人想好时间感谢<br/>话题1:感谢windows卡驱动邮件文件推进<br/>话题2:游戏团队年游戏季玩家好<br/>话题3:驱动scsi硬盘的硬盘问题<br/>话题4: windows文件windows文件程序使用问题<br/>话题5:政府芯片邮件空间信息加密数据<br/>话题6:喜欢自行车知道芯片声音看起来看起来<br/>话题7:卡出售视频报价显示器价格耶稣<br/>话题 8:知卡芯片视频政务民剪<br/>话题9:好知时间自行车耶稣问题工作<br/>话题10:想芯片好谢剪需要加密<br/>话题11:谢对问题好自行车时间窗口<br/>话题12:好民视窗知文件售卖文件<br/>话题13:太空想知道nasa问题年份以色列<br/>话题14:太空好卡民时间nasa感谢<br/>话题15:民问题窗口时间游戏想要自行车<br/>话题16: 时间自行车对windows文件需要真的<br/>话题17:时间问题文件想以色列长邮件<br/>话题18:文件需要卡文件问题对好<br/>话题19:问题文件感谢二手空间芯片出售</p><h2 id="6444" class="mb jy hi bd jz mc md me kd mf mg mh kh ix mi mj kl jb mk ml kp jf mm mn kt mo bi translated">主题可视化</h2><p id="3498" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">为了找出我们的主题有多独特，我们应该把它们形象化。当然，我们不能可视化超过3维，但有像主成分分析和t-SNE这样的技术可以帮助我们将高维数据可视化为低维。在这里，我们将使用一个相对较新的技术称为UMAP(统一流形近似和投影)。</p><pre class="jl jm jn jo fd mp mq mr ms aw mt bi"><span id="6829" class="mb jy hi mq b fi mu mv l mw mx">import umap </span><span id="eb85" class="mb jy hi mq b fi my mv l mw mx">X_topics = svd_model.fit_transform(X) </span><span id="c55e" class="mb jy hi mq b fi my mv l mw mx">embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics) </span><span id="73ea" class="mb jy hi mq b fi my mv l mw mx">plt.figure(figsize=(7,5)) <br/>plt.scatter(embedding[:, 0], embedding[:, 1], <br/>c = dataset.target, <br/>s = 10, # size <br/>edgecolor='none' ) <br/>plt.show()</span></pre><p id="90c7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">输出:</strong></p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/2f226036d97606edb8bdb84f2a962edf.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/0*_dpY85gxLztt3G8f.png"/></div></figure><p id="0879" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如你在上面看到的，结果相当漂亮。每个点代表一个文档，颜色代表20个新闻组。我们的LSA模式似乎做得很好。随意摆弄UMAP的参数，看看情节如何改变它的形状。</p><p id="cae3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">本文的完整代码可以在这个</strong> <a class="ae jw" href="https://github.com/prateekjoshi565/latent_semantic_analysis" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj"> GitHub资源库</strong> </a> <strong class="io hj">中找到。</strong></p><h1 id="548c" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">LSA的利与弊</h1><p id="ec59" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">正如我们在上面看到的，潜在语义分析非常有用，但是它也有其局限性。了解LSA的两面很重要，这样你就知道什么时候利用它，什么时候尝试其他东西。</p><p id="d82e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">优点:</strong></p><ul class=""><li id="652f" class="kv kw hi io b ip iq it iu ix ll jb lm jf ln jj lo ld le lf bi translated">LSA快速且易于实现。</li><li id="880a" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lo ld le lf bi translated">它给出了不错的结果，比简单的向量空间模型好得多。</li></ul><p id="3161" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">缺点:</strong></p><ul class=""><li id="83cb" class="kv kw hi io b ip iq it iu ix ll jb lm jf ln jj lo ld le lf bi translated">由于它是一个线性模型，它可能无法很好地处理具有非线性依赖关系的数据集。</li><li id="3983" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lo ld le lf bi translated">LSA假设文档中的术语呈高斯分布，但这并不适用于所有问题。</li><li id="fd02" class="kv kw hi io b ip lg it lh ix li jb lj jf lk jj lo ld le lf bi translated">LSA涉及SVD，这是计算密集型的，并且很难随着新数据的出现而更新。</li></ul><h1 id="f93a" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">主题建模的其他技术</h1><p id="1514" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">除了LSA，还有其他先进高效的主题建模技术，如<strong class="io hj">潜在狄利克雷分配(LDA)和lda2Vec </strong>。<strong class="io hj">我们有一篇关于LDA的精彩文章，你可以点击这里</strong><a class="ae jw" href="https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/" rel="noopener ugc nofollow" target="_blank"><strong class="io hj"/></a>。lda2vec是一个更高级的主题建模，它基于word2vec单词嵌入。如果你想了解更多，请在下面的评论区告诉我，我很乐意回答你的问题。</p><h1 id="f13d" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">结束注释</h1><p id="869a" class="pw-post-body-paragraph im in hi io b ip kx ir is it ky iv iw ix lp iz ja jb lq jd je jf lr jh ji jj hb bi translated">这篇文章试图与大家分享我的学习。主题建模是一个非常有趣的主题，它为您提供了处理许多文本数据集的技能和技术。因此，我强烈建议大家使用本文中给出的代码，并将其应用于不同的数据集。如果您对本文有任何问题或反馈，请告诉我。快乐文采！</p></div><div class="ab cl na nb gp nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="hb hc hd he hf"><p id="a174" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jv">原载于2018年10月1日www.analyticsvidhya.com</em><em class="jv">的</em> <a class="ae jw" href="https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/" rel="noopener ugc nofollow" target="_blank"> <em class="jv">。</em></a></p></div></div>    
</body>
</html>