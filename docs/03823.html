<html>
<head>
<title>Multi-Armed Bandit Analysis of Upper Confidence Bound Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">置信上限算法的多臂Bandit分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-armed-bandit-analysis-of-upper-confidence-bound-algorithm-4b84be516047?source=collection_archive---------2-----------------------#2020-02-21">https://medium.com/analytics-vidhya/multi-armed-bandit-analysis-of-upper-confidence-bound-algorithm-4b84be516047?source=collection_archive---------2-----------------------#2020-02-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="46ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">置信上限(UCB)算法通常被称为“面对不确定性的乐观主义”。为了理解为什么，考虑在给定的回合中，每个手臂的奖励函数可以被理解为基于观察到的平均奖励率的点估计。从置信区间得出直觉，对于每个点估计，我们也可以在点估计周围加入某种形式的不确定性边界。从这个意义上说，我们对每条手臂都有上边界和下边界。</p><p id="89bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">UCB算法之所以被恰当地命名，是因为我们只关心上界，因为我们试图找到报酬率最高的手臂。</p><p id="ebed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">UCB算法有不同的变体，但在本文中，我们将研究UCB1算法。在每一轮给定的<code class="du jd je jf jg b">n</code>试验中，所有兵种的UCB奖励如下所示:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es jh"><img src="../Images/afa23484ecc615603516c2c464b868e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/0*tmIlZiXrDaaOTjFM.png"/></div></figure><p id="5d13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<code class="du jd je jf jg b">mu_i</code>代表当前回合中手臂<em class="jp"> i </em>的当前奖励回报平均值，<code class="du jd je jf jg b">n</code>代表试玩通过的次数，<code class="du jd je jf jg b">n_i</code>代表试玩历史中手臂<em class="jp"> i </em>的拉动次数。</p><p id="ea23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的公式很简单，但是有几个有趣的含义，如下所述:</p><ul class=""><li id="dde1" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">上限与<code class="du jd je jf jg b">ln(n)</code>的平方根成正比，这意味着当实验进行时，所有手臂的上限都增加了<code class="du jd je jf jg b">ln(n)</code>的平方根倍。</li><li id="d625" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">这个上界和<code class="du jd je jf jg b">n_i</code>的平方根成反比。特定手臂在过去使用的次数越多，置信边界向点估计值减小得越大。</li></ul><p id="8a7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此后，UCB算法总是挑选具有最高奖励UCB的手臂，如上面的等式所示。</p><p id="5433" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了公式解释，这里有一个简单的思想实验，收集一些关于UCB算法如何结合探索和开发的直觉。</p><p id="3bc0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分子和分母之间的时间复杂性造成了探索和利用之间的紧张关系。对于<code class="du jd je jf jg b">n</code>的任何增加，UCB仅增加对数时间，而对于<code class="du jd je jf jg b">n_i</code>的任何增加，UCB减少<code class="du jd je jf jg b">n_i</code>。因此，一个没有像其他臂那样经常被探索的臂将具有更大的UCB分量。取决于其当前平均，该特定臂的总体UCB函数表示可能大于具有较高回报但较小分量的其他臂，并因此使得该臂能够被挑选。</p><p id="ead7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下分析基于约翰·迈尔斯·怀特的《网站优化的强盗算法》一书。为了进一步理解代码，我加入了一些注释以便于理解。</p><p id="d8c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是创建UCB1算法设置和逐步更新arm的计数和值的代码。</p><ul class=""><li id="3587" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">计数:代表手臂被拉动的记录时间。</li><li id="2d93" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">值:代表已知的平均报酬。在伯努利臂的情况下，值代表奖励的概率，范围从0到1。</li></ul><pre class="ji jj jk jl fd kf jg kg kh aw ki bi"><span id="58c4" class="kj kk hi jg b fi kl km l kn ko">class UCB1():<br/>    def __init__(self, counts, values):<br/>        self.counts = counts # Count represent counts of pulls for each arm. For multiple arms, this will be a list of counts.<br/>        self.values = values # Value represent average reward for specific arm. For multiple arms, this will be a list of values.<br/>        return</span><span id="35c4" class="kj kk hi jg b fi kp km l kn ko">    # Initialise k number of arms<br/>    def initialize(self, n_arms):<br/>        self.counts = [0 for col in range(n_arms)]<br/>        self.values = [0.0 for col in range(n_arms)]<br/>        return<br/>    <br/>    # UCB arm selection based on max of UCB reward of each arm<br/>    def select_arm(self):<br/>        n_arms = len(self.counts)<br/>        for arm in range(n_arms):<br/>            if self.counts[arm] == 0:<br/>                return arm<br/>    <br/>        ucb_values = [0.0 for arm in range(n_arms)]<br/>        total_counts = sum(self.counts)<br/>        <br/>        for arm in range(n_arms):<br/>            bonus = math.sqrt((2 * math.log(total_counts)) / float(self.counts[arm]))<br/>            ucb_values[arm] = self.values[arm] + bonus<br/>        return ucb_values.index(max(ucb_values))<br/>    <br/>    # Choose to update chosen arm and reward<br/>    def update(self, chosen_arm, reward):<br/>        self.counts[chosen_arm] = self.counts[chosen_arm] + 1<br/>        n = self.counts[chosen_arm]<br/>        <br/>        # Update average/mean value/reward for chosen arm<br/>        value = self.values[chosen_arm]<br/>        new_value = ((n - 1) / float(n)) * value + (1 / float(n)) * reward<br/>        self.values[chosen_arm] = new_value<br/>        return</span></pre><p id="ea19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据之前文章的讨论，我们将使用伯努利分布来表示每条手臂的奖励函数。</p><pre class="ji jj jk jl fd kf jg kg kh aw ki bi"><span id="f5e8" class="kj kk hi jg b fi kl km l kn ko">class BernoulliArm():<br/>    def __init__(self, p):<br/>        self.p = p<br/>    <br/>    # Reward system based on Bernoulli<br/>    def draw(self):<br/>        if random.random() &gt; self.p:<br/>            return 0.0<br/>        else:<br/>            return 1.0</span></pre><p id="c43b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了进行任何进一步的分析，需要一个操作脚本来处理模拟，其中:</p><ul class=""><li id="fcb7" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">num_sims:表示独立模拟的数量，每个模拟的长度等于“地平线”。</li><li id="8c0e" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">horizon:表示每轮模拟的时间步长/试验次数</li></ul><pre class="ji jj jk jl fd kf jg kg kh aw ki bi"><span id="1793" class="kj kk hi jg b fi kl km l kn ko">def test_algorithm(algo, arms, num_sims, horizon):<br/>    <br/>    # Initialise variables for duration of accumulated simulation (num_sims * horizon_per_simulation)<br/>    chosen_arms = [0.0 for i in range(num_sims * horizon)]<br/>    rewards = [0.0 for i in range(num_sims * horizon)]<br/>    cumulative_rewards = [0 for i in range(num_sims * horizon)]<br/>    sim_nums = [0.0 for i in range(num_sims *horizon)]<br/>    times = [0.0 for i in range (num_sims*horizon)]<br/>    <br/>    for sim in range(num_sims):<br/>        sim = sim + 1<br/>        algo.initialize(len(arms))<br/>        <br/>        for t in range(horizon):<br/>            t = t + 1<br/>            index = (sim -1) * horizon + t -1<br/>            sim_nums[index] = sim<br/>            times[index] = t<br/>            <br/>            # Selection of best arm and engaging it<br/>            chosen_arm = algo.select_arm()<br/>            chosen_arms[index] = chosen_arm<br/>            <br/>            # Engage chosen Bernoulli Arm and obtain reward info<br/>            reward = arms[chosen_arm].draw()<br/>            rewards[index] = reward<br/>            <br/>            if t ==1:<br/>                cumulative_rewards[index] = reward<br/>            else:<br/>                cumulative_rewards[index] = cumulative_rewards[index-1] + reward<br/>                <br/>            algo.update(chosen_arm, reward)<br/>    <br/>    return [sim_nums, times, chosen_arms, rewards, cumulative_rewards]</span></pre><h1 id="ad63" class="kq kk hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">模拟手段差异较大的兵种</h1><p id="adc4" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">类似于之前对ε-greedy所做的分析，模拟包括以下内容:</p><ul class=""><li id="34a7" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">创造5个兵种，其中四个平均奖励0.1，最后一个/最好的平均奖励0.9。</li><li id="183f" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">将模拟输出保存到制表符分隔的文件中</li><li id="0194" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">创建5000个独立的模拟</li></ul><p id="1785" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本例中，由于UCB算法没有任何超参数，我们创建了一组5000次模拟。</p><p id="6083" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择5000个独立模拟是因为我们想要确定平均性能。每个模拟可能受随机性质/运行的影响，并且性能可能由于随机机会而有偏差。因此，运行合理的大量模拟来评估平均均值/性能非常重要。</p><pre class="ji jj jk jl fd kf jg kg kh aw ki bi"><span id="f56f" class="kj kk hi jg b fi kl km l kn ko">import random</span><span id="fa16" class="kj kk hi jg b fi kp km l kn ko">random.seed(1)<br/># out of 5 arms, 1 arm is clearly the best<br/>means = [0.1, 0.1, 0.1, 0.1, 0.9]<br/>n_arms = len(means)<br/># Shuffling arms<br/>random.shuffle(means)</span><span id="4cc9" class="kj kk hi jg b fi kp km l kn ko"># Create list of Bernoulli Arms with Reward Information<br/>arms = list(map(lambda mu: BernoulliArm(mu), means))<br/>print("Best arm is " + str(np.argmax(means)))</span><span id="802c" class="kj kk hi jg b fi kp km l kn ko">f = open("standard_ucb_results.tsv", "w+")</span><span id="1b01" class="kj kk hi jg b fi kp km l kn ko"># Create 1 round of 5000 simulations<br/>algo = UCB1([], [])<br/>algo.initialize(n_arms)<br/>results = test_algorithm(algo, arms, 5000, 250)<br/>    <br/># Store data<br/>for i in range(len(results[0])):<br/>    f.write("\t".join([str(results[j][i]) for j in range(len(results))]) + "\n")<br/>f.close()</span></pre><p id="6db8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用一些数据预处理和基本的Altair可视化，我们可以绘制出拉最佳手臂的概率。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es ls"><img src="../Images/4f5afcbbb42f0d373adf7483cdfd2d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/0*du4Ol1Rc0hRMw38z.png"/></div></figure><p id="f6fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实验的早期阶段，UCB算法在选择最佳臂的速度上有极大的波动，如0到60之间的时间步长所示。这可以通过在所有兵种中强调探索来解释，因为所有兵种的UCB分量在开始时要大得多。</p><p id="cef9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着试验的进行，所有组的UCB分量变得更小，并且每个组的UCB函数表示向每个组的平均回报均值收敛。因此，随着试验的进行，具有较高平均值的臂变得更容易被算法区分，并且变得更频繁地被挑选。因此，我们观察到选择最佳臂的速率似乎没有硬渐近线，而是向1收敛。向1收敛的速度随着接近1而减慢，实验时间范围太短，我们无法观察到任何进一步的收敛。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es lt"><img src="../Images/4a445912440677d60b68d9cbeb59f6dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/0*rd06Cqr4QVyMh_JJ.png"/></div></figure><p id="1df6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">UCB算法的累积奖励图与其他算法相当。虽然它的表现不如Softmax ( <code class="du jd je jf jg b">tau</code> = 0.1或0.2)的最佳表现(累积奖励超过200)，但UCB累积奖励范围接近该范围(约190)。</p><p id="191b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还在试验的早期阶段观察到某种形式的弯曲，这可以通过我们在选择最佳武器的比率中看到的极端波动得到证实。同样，当实验进行时，该算法可以区分最佳手臂，并以更高的频率挑选它，累积奖励图具有直线梯度(基于一致选择最佳手臂，其应该接近0.9的值)。</p><h1 id="34e4" class="kq kk hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">平均差异相对较小的武器的模拟</h1><p id="9a09" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">之前的分析是对回报差异很大的武器的模拟练习。我们将分析扩展到两臂相对较近的情况。</p><p id="24e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的例子中，我们模拟了5个分支，其中4个分支的平均值为0.8，而最后一个/最佳分支的平均值为0.9。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es ls"><img src="../Images/a258a8dd81200772ad6abd269e5e5d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/0*BNFmSRqizbDN2Wu5.png"/></div></figure><p id="e263" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于所有分支的报酬回报之间的差异减少，我们观察到UCB算法的性能大幅下降。选择最佳臂的比率现在接近0.32，这与我们在Softmax算法中看到的类似。</p><p id="b36c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与Softmax算法进行比较，这似乎意味着奖励函数中的差异减少使得更难确定哪个是最好的臂。请注意，在这种情况下，选择最佳手臂的随机几率是1/5或0.20。</p><p id="0659" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">应当注意，在这种情况下，对于ε贪婪算法，选择最佳arm的比率实际上更高，如范围0.5至0.7所示。这似乎也意味着ε贪婪可能更适合于基于多臂的情况，其中与UCB或Softmax算法相比，均值的差异小得多。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es lt"><img src="../Images/7c869bfa1d91fb82bce76479304d55ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/0*r-QhKRgINlT5EqQv.png"/></div></figure><p id="19b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于两臂的平均回报接近，最终的UCB累积回报值约为210。与选择将返回0.9 * 250 = 225的最佳臂相比，我们看到15的遗憾。在这种情况下，它可能看起来很小，但作为一个百分比，它可以被认为是重要的(6.67%)，这取决于应用程序的重点。</p><p id="a585" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看一看总的累积遗憾可以提供性能的更好的视角，特别是相对于其他算法。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es lu"><img src="../Images/5258f46ffac850aaba5a9a5137be1291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/0*Lf7kyh9aSv5b1qp2.png"/></div></figure><p id="2a42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于累积遗憾图，我们看到UCB1的累积遗憾为18，这类似于Softmax算法。与范围为12.3到14.8的Epsilon Greedy算法相比，它也是最差的。累积后悔线也比较直，也就是说算法会随着时间跨度的变长，继续累积更多的后悔。</p><h1 id="6d24" class="kq kk hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">摘要</h1><p id="47e0" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">在对UCB算法的分析中，我们分解了算法的公式，并对不同的arm进行了仿真实验，以说明其鲁棒性(或缺乏鲁棒性)。</p><p id="867d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与<a class="ae ke" rel="noopener" href="/@kfoofw/multi-armed-bandit-analysis-of-softmax-algorithm-e1fa4cb0c422"> Softmax算法</a>类似，一个学习收获是，对于具有更近均值的臂，UCB算法在确定最佳臂方面似乎不那么稳健，而<a class="ae ke" rel="noopener" href="/@kfoofw/multi-armed-bandit-analysis-of-epsilon-greedy-algorithm-8057d7087423">εGreedy</a>更适合于此。</p><p id="b491" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于土匪模拟分析的这个项目的参考，请参考这个<a class="ae ke" href="https://github.com/kfoofw/bandit_simulations" rel="noopener ugc nofollow" target="_blank"> Github repo </a>。关于实际代码的快速参考，请参考本<a class="ae ke" href="https://github.com/kfoofw/bandit_simulations/blob/master/python/notebooks/analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter笔记本</a>。</p></div></div>    
</body>
</html>