<html>
<head>
<title>Latent Dirichlet Allocation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">潜在狄利克雷分配</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/latent-dirichelt-allocation-1ec8729589d4?source=collection_archive---------4-----------------------#2020-03-03">https://medium.com/analytics-vidhya/latent-dirichelt-allocation-1ec8729589d4?source=collection_archive---------4-----------------------#2020-03-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8ca3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">潜在狄利克雷分配</strong> (LDA)被用作主题建模技术，可以将文档中的文本分类到特定主题。它使用狄利克雷分布为每个文档模型查找主题，为每个主题模型查找单词。</p><p id="5682" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">约翰·彼得·古斯塔夫·勒热纳·狄利克雷</strong>是19世纪的德国数学家，对现代数学领域做出了广泛贡献。有一种以他的名字命名的概率分布<strong class="ih hj">狄利克雷分布</strong>，它是LDA的基础。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/bbd0ecd35c7ef06f14aeb3cb5b5ebbce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*oWPvDrrsqGrkR_36.jpg"/></div></figure><h1 id="5626" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">LDA是如何工作的？</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kj"><img src="../Images/d4af03d2114c81df29bfd8f61ff413df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/0*J1oMupf58psVRVCH.png"/></div></figure><p id="e12b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">潜在狄利克雷分配是一种将句子映射到主题的技术。LDA根据我们输入的主题提取特定的主题集。在生成这些主题之前，有许多由LDA执行的过程。在应用这个过程之前，我们已经考虑了一定数量的规则和事实。</p><p id="9b3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用于主题建模的LDA假设:</p><ul class=""><li id="2082" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">具有相似主题的文档使用相似的词组</li><li id="4c98" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">然后可以通过搜索在整个语料库的文档中频繁出现的单词组来找到潜在主题</li><li id="b659" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">文档是潜在主题的概率分布，这意味着某个文档将包含特定主题的更多单词。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ky"><img src="../Images/f03b06fdce7d4b1e3702070504c37d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/0*4eA0oBk8xGLFgOsX.PNG"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kz"><img src="../Images/3846dfb39cd285aa6db06d6f8c633591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/0*rq0YIA1A8Jkuxc7T.PNG"/></div></figure><ul class=""><li id="0f43" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">主题本身是单词的概率分布</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es la"><img src="../Images/b70464f8a0a59ff1c1bd0b1b0f02e915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hZq6E5SKhSAUQMwA.PNG"/></div></div></figure><p id="dda1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些是用户在应用LDA之前必须了解的假设。</p><p id="0fed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">举例说明:</p><p id="c6ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有以下语句:</p><ul class=""><li id="c2b7" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">克里斯蒂亚诺罗纳尔多和莱昂内尔梅西都是伟大的足球运动员</li><li id="d0a5" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">人们也钦佩内马尔和拉莫斯的足球技术</li><li id="b27c" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">美国和中国都是强大的国家</li><li id="6bdd" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">中国正在建造最大的空气净化器</li><li id="b316" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">通过在全球范围内推广足球，印度也正在成为最发展中的国家之一</li></ul><p id="2edb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在LDA的帮助下，我们可以生成关于句子的主题集。如果我们考虑2个主题集，那么:</p><ul class=""><li id="4282" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">句子1和句子2都属于话题1</li><li id="ea76" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">句子3和句子4都属于话题2</li><li id="9f8d" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">句子5描述了70%的主题1和30%的主题2</li></ul><p id="7eb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LDA声明每个文档包含与各种主题相关的各种类型的上下文。因此，文档可以表示为各种类型主题的集合。每一个主题都有大量的单词，并有一定的概率范围。根据LDA，每个文档都有自己的属性。因此，LDA在制作文档之前会假定一定的规则和规定。就像应该有字数限制一样，一个文档应该有一定数量的用户设置的字数。文件内容也应该多样化。文档应该参考各种上下文，比如60%商业，20%政治，10%食物。包含在与主题相关的文档中的每个关键词以及该关系可以使用多项式分布来导出。我们之前讨论过的例子中，与商业领域相关的词的概率为3/5，与政治相关的词的概率为1/5。</p><p id="aaf0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设该模型应用于文档集合，LDA然后尝试从文档回溯以找到与文档上下文相关的主题集。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lf"><img src="../Images/13785db6840c261be94658697f9699fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*DIinuRdAfGxBa3YZ.png"/></div></figure><p id="2fd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们试图理解它的全部工作原理</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lg"><img src="../Images/9cb7fef7191b54aca37ca1677a8f898b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/0*8ZljTrglPzhXbDCp.jpg"/></div></figure><p id="d090" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们有来自某个数据集或随机来源的文档集。我们将决定要发现的固定数量的K个主题，并将使用LDA来学习每个文档的主题表示以及与每个主题相关联的单词。</p><p id="188f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LDA算法循环遍历每个文档，并将文档中的每个单词随机分配给K个主题中的一个。这种随机分配已经给出了所有文档的主题表示和所有文档的单词分布以及所有主题的单词分布。LDA将迭代每个文档中的每个单词来改进这些主题。但是这些主题表示并不好。所以我们必须改善这个限制。为此，创建了一个公式，其中提取了主要工作LDA。</p><p id="577f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代表LDA模型的板符号:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es lh"><img src="../Images/b59580f8744eaedd4d7ecc28ba4a140c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0JrdjODR99E7Ijm1.png"/></div></div></figure><p id="c4ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">m表示文档的数量<br/> N是给定文档中的字数(文档I具有{\displaystyle N_{i}}N_{i}个单词)<br/> α是每个文档主题分布的狄利克雷先验参数<br/> β是每个主题单词分布的狄利克雷先验参数<br/>θ是文档I的主题分布<br/> varphi是主题k的单词分布<br/> z是文档I中第j个单词的主题<br/> w是特定单词。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/2e2329a01d46f6104ed7b2265181dd87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/0*BQ-2xVEOpy0UwE4D.png"/></div></figure><p id="08fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用简单的术语解释:</p><p id="9484" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个文档中的每个单词和每个主题T，我们计算:<br/> <strong class="ih hj"> P(主题T |文档D) </strong> =文档D中当前分配给主题T的单词的比例</p><p id="22bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> P(单词W |主题T) </strong> =主题T的赋值在来自单词W的所有文档中的比例</p><p id="4260" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将W重新分配给一个新主题，我们以概率<strong class="ih hj"> P(主题T |文档D) * P(单词W |主题T) </strong>选择主题T。这实质上是主题测试生成单词w</p><p id="31d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在重复前面的步骤很多次之后，我们最终会达到一个大致稳定的状态，在这个状态下，分配是可以接受的。最后，我们将每个文档分配给一个主题。我们可以搜索最有可能被分配到某个主题的单词。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/a3bbada2192a009d7e66cb7ecdc7070b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/0*cZOX05PRH_a_QYU0.png"/></div></figure><p id="208a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们最终输出如下内容</p><ul class=""><li id="3e99" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">分配给主题4的文档</li><li id="363e" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">主题4最常见的单词(最有可能)(猫、兽医、鸟、狗……)</li><li id="80c7" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">由用户来解释这些主题。</li></ul><p id="ea29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">两个重要注意事项:</p><ul class=""><li id="8bad" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">用户必须决定文档中出现的主题数量</li><li id="3720" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">用户必须解释主题是什么</li></ul><p id="fc0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，一般来说，如果我们有一组文档，我们希望为文档的主题表示生成一组主题，我们可以使用LDA来执行。因为LDA将通过浏览每个文档来训练，并给主题分配单词。但这不是一个循环的过程。在第一个循环中，LDA随机地将单词分配给主题。这里有一个学习过程。它将检查每个文档中的每个单词，并应用上面讨论的公式。在重复各种迭代后，它会生成一组主题。</p><h1 id="9055" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">履行</h1><p id="b065" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">我们将尝试通过在数据集上应用LDA来更简单地理解它。</p><p id="d5d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用的数据集包含来自www.npr.org的信息或新闻。数据集包含全球最新的。我们将在新闻栏上实现LDA，并尝试找出世界上最常见主题，还将为未来的新闻指定一个主题。</p><h1 id="da43" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">数据预处理:</h1><pre class="je jf jg jh fd lp lq lr ls aw lt bi"><span id="10e7" class="lu jm hi lq b fi lv lw l lx ly">import pandas as pd<br/>df = pd.read_csv('npr.csv')<br/>df.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/51b1e646407ce6e8bcdfa78504504b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/0*LAZTXrLWJeUxMEg3.PNG"/></div></figure><p id="a93b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意我们没有文章的主题！让我们使用LDA来尝试找出文章的聚类。</p><pre class="je jf jg jh fd lp lq lr ls aw lt bi"><span id="083d" class="lu jm hi lq b fi lv lw l lx ly">from sklearn.feature_extraction.text import CountVectorizer<br/>cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')<br/>dtm = cv.fit_transform(df['Article'])</span></pre><p id="9824" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计数向量器</strong>:计数向量器是自然语言处理的另一部分，也被认为是TFIDF的long部分。因此，我们使用tfidf代替countvectorizer。它计算一个标记在文档中出现的次数，并使用这个值作为它的权重。我们应用计数矢量器将文本数据转换成计算机可读的形式。</p><ul class=""><li id="b5f0" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated"><strong class="ih hj"> max_df </strong>:在[0.0，1.0]或int范围内浮动，默认=1.0 <br/>用于去除出现过于频繁的单词。如果max_df = 0.50表示“忽略出现在50%以上文档中的术语”。如果max_df = 25表示“忽略出现在超过25个文档中的术语”。默认的max_df是1.0，这意味着“忽略出现在超过100%的文档中的术语”。因此，默认设置不会忽略任何术语。</li><li id="531e" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated"><strong class="ih hj"> min_df </strong>:在范围[0.0，1.0]或int内浮动，默认=1 <br/>用于去除很少出现的单词。如果min_df = 0.01表示“忽略出现在少于1%的文档中的术语”。如果min_df = 5表示“忽略出现在少于5个文档中的术语”。默认的min_df是1，这意味着“忽略出现在少于1个文档中的术语”。因此，默认设置不会忽略任何术语。</li></ul><h1 id="dbf7" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">LDA模型:</h1><pre class="je jf jg jh fd lp lq lr ls aw lt bi"><span id="d3dc" class="lu jm hi lq b fi lv lw l lx ly">from sklearn.decomposition import LatentDirichletAllocation<br/>LDA = LatentDirichletAllocation(n_components=7,random_state=42)<br/>LDA.fit(dtm)</span></pre><h1 id="24cd" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">显示储存的单词:</h1><pre class="je jf jg jh fd lp lq lr ls aw lt bi"><span id="7460" class="lu jm hi lq b fi lv lw l lx ly">len(cv.get_feature_names())<br/>&gt;&gt;&gt;54777<!-- --> </span><span id="6206" class="lu jm hi lq b fi ma lw l lx ly">for i in range(10):<br/>    random_word_id = random.randint(0,54776)<br/>    print(cv.get_feature_names()[random_word_id])</span><span id="1216" class="lu jm hi lq b fi ma lw l lx ly">&gt;&gt;&gt;cred<br/>fairly<br/>occupational<br/>temer<br/>tamil<br/>closest<br/>condone<br/>breathes<br/>tendrils<br/>pivot</span><span id="1d19" class="lu jm hi lq b fi ma lw l lx ly">for i in range(10):<br/>    random_word_id = random.randint(0,54776)<br/>    print(cv.get_feature_names()[random_word_id])</span><span id="06ce" class="lu jm hi lq b fi ma lw l lx ly">&gt;&gt;&gt;foremothers<br/>mocoa<br/>ellroy<br/>liron<br/>ally<br/>discouraged<br/>utterance<br/>provo<br/>videgaray<br/>archivist</span></pre><h1 id="a2ff" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">显示每个主题的热门词汇</h1><pre class="je jf jg jh fd lp lq lr ls aw lt bi"><span id="eddb" class="lu jm hi lq b fi lv lw l lx ly">len(LDA.components_)<br/>&gt;&gt;&gt;7<br/>len(LDA.components_[0])<br/>&gt;&gt;&gt;54777</span><span id="8072" class="lu jm hi lq b fi ma lw l lx ly">single_topic = LDA.components_[0]</span><span id="3687" class="lu jm hi lq b fi ma lw l lx ly"># Returns the indices that would sort this array.<br/>single_topic.argsort()</span><span id="0006" class="lu jm hi lq b fi ma lw l lx ly"># Word least representative of this topic<br/>single_topic[18302]</span><span id="fd1d" class="lu jm hi lq b fi ma lw l lx ly"># Word most representative of this topic<br/>single_topic[42993]</span><span id="954f" class="lu jm hi lq b fi ma lw l lx ly"># Top 10 words for this topic:<br/>single_topic.argsort()[-10:]<br/>&gt;&gt;&gt;array([33390, 36310, 21228, 10425, 31464,  8149, 36283, 22673, 42561,<br/>       42993], dtype=int64)<br/>       <br/>top_word_indices = single_topic.argsort()[-10:]<br/>for index in top_word_indices:<br/>    print(cv.get_feature_names()[index])</span></pre><p id="a25b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些看起来可能像商业文章。我们会表演的。在我们的矢量化文章上转换()以附加标签号。但在此之前，我们查看所有找到的主题。</p><pre class="je jf jg jh fd lp lq lr ls aw lt bi"><span id="9c9d" class="lu jm hi lq b fi lv lw l lx ly">for index,topic in enumerate(LDA.components_):<br/>    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')<br/>    print([cv.get_feature_names()[i] for i in topic.argsort()[-15:]])<br/>    print('\n')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es mb"><img src="../Images/eeb37b7f096f941e74d9fb64a03930f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uojkQlJpaBb4cvhw.PNG"/></div></div></figure><h1 id="cac8" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">将发现的主题标签附加到原始文章</h1><pre class="je jf jg jh fd lp lq lr ls aw lt bi"><span id="9e3c" class="lu jm hi lq b fi lv lw l lx ly">topic_results = LDA.transform(dtm)<br/>npr['Topic'] = topic_results.argmax(axis=1)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/a98dbf23ffa5e7f13d99098062312e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/0*bFuQGJYhj-tWFPC6.PNG"/></div></figure><h1 id="47ec" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">限制</h1><ul class=""><li id="e8ef" class="kk kl hi ih b ii lj im lk iq md iu me iy mf jc kp kq kr ks bi translated">我们可以生成的主题数量是有限的</li><li id="52ba" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">LDA无法描述导致不相关主题出现的相关性</li><li id="a29f" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">随着时间的推移，主题没有发展</li><li id="c772" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">LDA假设单词是可交换的，句子结构不是模型化的</li><li id="4411" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">无监督(有时弱监督是可取的，例如在情感分析中)</li></ul><p id="a089" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这样，您就有了潜在狄利克雷分配(LDA)的完整概念。享受吧。</p></div></div>    
</body>
</html>