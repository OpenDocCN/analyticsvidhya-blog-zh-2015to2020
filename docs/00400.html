<html>
<head>
<title>Multiple Linear Regression &amp; Factor Analysis in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多元线性回归和因子分析在R</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multiple-linear-regression-factor-analysis-in-r-35a26a2575cc?source=collection_archive---------0-----------------------#2019-05-28">https://medium.com/analytics-vidhya/multiple-linear-regression-factor-analysis-in-r-35a26a2575cc?source=collection_archive---------0-----------------------#2019-05-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b2c1" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">“用因子分析对变量进行分组，然后对其进行多元线性回归”</h2></div><p id="95c3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">什么是多重共线性，它如何影响回归模型？当回归模型的独立变量相关时会出现多重共线性，如果独立变量之间的共线性程度很高，则很难估计每个独立变量和因变量之间的关系以及估计系数的总体精度。即使具有高度多重共线性的回归模型可以给出高R平方，但几乎没有任何显著变量。</em></p><figure class="ju jv jw jx fd jy"><div class="bz dy l di"><div class="jz ka l"/></div></figure><p id="2991" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">目的是使用数据集</em><a class="ae kb" href="https://www.kaggle.com/ipravin/hair-customer-survey-data-for-100-customers" rel="noopener ugc nofollow" target="_blank"><em class="jt">Factor-Hair-revised . CSV</em></a><em class="jt">建立回归模型预测满意度。</em></p><p id="3993" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">数据探索:</em> </strong></p><p id="2261" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">我们导入数据，查看基本的描述性统计。</em> </strong></p><p id="5cd5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据头(数据)<br/> dim(数据)<br/> str(数据)<br/>人名(数据)<br/>形容(数据)</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es kc"><img src="../Images/bf8daf88695fba5736455535a8fd8b55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WH7x03jv_XQdUGhxkRVlVw.png"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es kj"><img src="../Images/bdb58de170e2af9579e7a941252bc468.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-Ppb-RUwa4Dt7qeEi5-0Q.png"/></div></div></figure><p id="534c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">变量ID是唯一的数字/ID，并且在回归方程中也不具有解释满意度的任何解释能力。所以我们可以安全地从数据集中删除ID。</em></p><p id="91c6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">#删除ID变量<br/> </em> data1 &lt; -子集(data，select = -c(1))</p><p id="ba70" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">项间相关性分析:</em> </strong> <em class="jt"> <br/>现在我们来绘制数据集的相关性矩阵图。</em></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es kk"><img src="../Images/0e725648766f431a0c2f806df3a9e25e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*axuGaypMKvbFwwXcWkarCg.png"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es kl"><img src="../Images/5c98cc5e9e43e3af84b50ecfcdb3d7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vfwSGdAyurG1GrxakXMi5g.png"/></div></div></figure><p id="9ecf" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">从上面的相关矩阵可以看出:<br/> 1。CompRes和DelSpeed高度相关<br/> 2。订单和价格高度相关<br/> 3。WartyClaim和TechSupport高度相关<br/> 4。CompRes和OrdBilling高度相关。OrdBilling和DelSpeed高度相关<br/> 6。Ecom和SalesFImage高度相关</em></p><p id="a12f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">为了检查多重共线性的模式，需要对相关系数进行t检验。让我们使用ppcor软件包来计算独立变量的偏相关系数以及t统计量和相应的p值。</em></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es km"><img src="../Images/e8f2d2c93cb4cf4ca7b1b41ba677f1f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9EJ5-OOYztN3pio3veWuGQ.png"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es kn"><img src="../Images/45463b399f610043e1975b52bf85d9e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4EItSVOZGj83n6mFq7cvgA.png"/></div></div></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ko"><img src="../Images/7656fc869ff6637e5b58561b540ce366.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*imdjJlrvcmDNMuLibJsq_A.png"/></div></div></figure><p id="8501" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">不出所料，销售人员形象和电子商务之间的相关性非常显著。交付速度和订单计费与投诉解决之间的相关性也是如此。还有，订单&amp;计费和交货速度的相关性。我们可以有把握地假设自变量之间存在高度的共线性。</em></p><p id="bae8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">多元线性回归模型按原样使用data1。</em> </strong> <br/> <em class="jt">作为预测分析，多元线性回归用于解释一个连续因变量和两个或多个自变量之间的关系。<br/>多元线性回归的公式为:</em></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es kp"><img src="../Images/b220dca5bd133b7e9577691261ce450e.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*RXMb9oqMpqaJkdsQBNFImg.png"/></div></figure><p id="5737" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">回归模型的假设:</em></strong><em class="jt"><br/></em><strong class="iz hj"><em class="jt">线性</em> </strong> <em class="jt">:因变量和自变量之间的关系应该是线性的。<br/> </em> <strong class="iz hj"> <em class="jt">同方差</em> </strong> <em class="jt">:误差的方差应保持不变。</em> <strong class="iz hj"> <em class="jt">多元正态:</em> </strong> <em class="jt">多元回归假设残差呈正态分布。<br/> </em> <strong class="iz hj"> <em class="jt">缺乏多重共线性:</em> </strong> <em class="jt">假设数据中很少或没有多重共线性。</em></p><p id="6b13" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">决定系数(R平方)是一种统计指标，用于衡量独立变量的变化可以解释多少结果变化。随着更多的预测因子被添加到回归模型</em> <strong class="iz hj"> <em class="jt"> </em> </strong> <em class="jt">模型中，R2 (R平方)总是增加，即使预测因子可能与结果变量无关。因此，R2本身不能用来确定哪些预测因子应该包含在模型中，哪些应该排除在外。R2只能在0和1之间，其中0表示结果不能被任何独立变量预测，1表示结果可以被独立变量准确预测</em></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="12cd" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">#Regression Model</em><br/>model0 = lm(Satisfaction~., data1)<br/>summary(model0)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lb"><img src="../Images/b3d4390b98658e2dfd10aa38c2bf78f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*6uFRAP8L0YXdy7KTdQI4ow.png"/></div></figure><p id="57d5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">在我们的模型中，调整后的R平方为:0.7774，这意味着自变量解释了因变量方差的78%，11个自变量中只有3个变量是显著的。F统计的p值小于0.05(显著性水平)，这意味着我们的模型是显著的。这意味着，至少有一个预测变量与结果变量显著相关。<br/>我们的模型方程可以写成:<br/>满意度=-0.66+0.37 * prod qual-0.44 * Ecom+0.034 * TechSup+0.16 * CompRes-0.02 *广告+0.14 prodline+0.80 * sales fimage-0.038 * comp pricing-0.10 * WartyClaim+0.14 * OrdBilling+0.16 * DelSpeed</em></p><p id="4ea6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">【变量通货膨胀因子(VIF) </em> </strong> <br/> <em class="jt">回归的假设:变量相互独立——多重共线性不应该存在。<br/>高可变通货膨胀系数(VIF)是多重共线性的标志。没有正式的VIF值来确定多重共线性的存在；然而，在较弱的模型中，大于2.5的VIF值可能会引起关注。</em></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lc"><img src="../Images/f887705dc1bcdd602b08868d8d9c1c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tN6oyYDeO5DVEzlw6xFiFQ.png"/></div></div></figure><p id="3591" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">从VIF值中，我们可以推断出变量DelSpeed和CompRes值得关注。</em></p><p id="077e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">补救措施:</em> </strong> <br/> <em class="jt">处理模型中多重共线性最常用的两种方法如下。<br/> *使用VIF或逐步算法去除一些高度相关的变量。<br/> *对相关变量进行主成分分析(PCA)/因子分析等分析设计。</em></p><p id="a510" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">因子分析:</em> </strong> <em class="jt"> <br/>现在我们来检查数据集中变量的可因子性。首先，让我们从数据中抽取所有独立变量的子集，创建一个新的数据集，并执行凯泽-迈耶-奥尔金(KMO)测试。</em></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ld"><img src="../Images/fbd59fdc9b4e9af39c3d3db4f932827c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XLoKfE_gJ-6TbsEiZcCfjw.png"/></div></div></figure><p id="ffb9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于MSA &gt; 0.5，我们可以对该数据进行因子分析。</p><p id="2f8c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">巴特利特球度检验应显著</em> </strong></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es le"><img src="../Images/e797099ec9de4b1f88638a5310e52e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H_JTUChL8irN4hbJxA0uvw.png"/></div></div></figure><p id="87d6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">Kaiser-Meyer Olkin(KMO)和Bartlett对抽样充分性的检验被用来检验因素分析的适当性。55个自由度的卡方近似为619.27，显著性水平为0.05。KMO统计值0.65也很大(大于0.50)。因此，因子分析被认为是进一步分析数据的适当技术。</em></p><p id="aa13" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">使用基图的scree图&amp;gg图</em> </strong> <em class="jt"> <br/>确定数据矩阵或相关矩阵中因子或分量数量的一种方法是检查连续特征值的“Scree”图。图中的突变表明提取了适当数量的成分或因子。<br/>scree图绘制了每个因子的特征值。从图中我们可以看出，在系数4之后，碎石曲线的曲率有一个急剧的变化。这表明在因子4之后，总方差占较小的数量。<br/>从碎石图中选择因子可基于:<br/> 1。Kaiser-Guttman归一化规则说，我们应该选择特征值大于1的所有因子。<br/> 2。弯管法则</em></p><p id="767c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">现在让我们使用Psych软件包的fa.parallel函数来执行并行分析，以找到可接受的因子数量并生成scree图。</em></p><p id="d571" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">parallel </p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lf"><img src="../Images/17e3a990e0e8c4e30dc7b8d1ab8ad235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IEsJb26SZipLwT77ipyTjg.png"/></div></div></figure><p id="9a0a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">蓝线显示实际数据的特征值，两条红线(上下重叠)显示模拟数据和重采样数据。在这里，我们观察实际数据中的大幅度下降，并找出它向右变平的点。<br/>着眼于情节3或4的因素会是一个不错的选择。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lg"><img src="../Images/e4a6cc1cb4354c2863095249285e2f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b9Mc6zX2fIJSuW2Oc0_6fQ.png"/></div></div></figure><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="10ef" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">#Plot a Scree plot using base plot:</em><br/>Factor = c(1,2,3,4,5,6,7,8,9,10,11)<br/>Eigen_Values &lt;-ev$values<br/>Scree &lt;- data.frame(Factor, Eigen_Values)<br/>plot(Scree, main = "Scree Plot", col= "Blue",ylim=c(0,4))<br/>lines(Scree,col='Red')<br/>abline(h = 1, col="Green")</span><span id="129f" class="kv kw hi kr b fi lh ky l kz la"><em class="jt">#Plotting Scree plot using ggplot</em><br/><strong class="kr hj">library</strong>(ggplot2)<br/>ggplot(data = Scree,mapping = aes(x=Factor,y=Eigen_Values))+<br/>  geom_point()+<br/>  geom_line()+<br/>  scale_y_continuous(name="Eigen Values",limits = c(0,4))+<br/>  theme(panel.background = element_blank())+<br/>  theme(plot.background = element_blank())+<br/>  theme(panel.grid.major.y = element_line(colour = "skyblue"))+<br/>  ggtitle("Scree Plot")</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es li"><img src="../Images/ea3a14450f3fb4f4160467a3d3765b5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TrRfBkWHQRSxB63KMssVMw.png"/></div></div></figure><p id="3c64" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">因此，根据elbow或Kaiser-Guttman归一化规则，我们可以继续使用4个因素。</em></p><p id="981f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">让我们用4个因素来进行因素分析。此外，让我们使用正交旋转(varimax ),因为在正交旋转中，旋转因子将保持不相关，而在倾斜旋转中，结果因子将相关。<br/>有不同方法来计算因子，其中一些是:<br/> </em> 1。<em class="jt">因子分析采用</em>因子分析法:</p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="ff5f" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">#Using </em>factanal<em class="jt"> command:<br/></em>nfactors &lt;- <!-- -->4<br/>fit &lt;- factanal(data2, nfactors, scores = c(<!-- -->"regression"<!-- -->),rotation = <!-- -->"none"<!-- -->)<br/>print(fit)</span><span id="768b" class="kv kw hi kr b fi lh ky l kz la"><em class="jt">#Varimax Rotation<br/></em>fit1 &lt;-factanal(data2,nfactors,scores = c(<!-- -->"regression"<!-- -->),rotation = <!-- -->"varimax"<!-- -->)<br/>print(fit1)</span></pre><p id="8d3b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt"> 2。使用fa方法的因子分析:</em></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="0720" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">#If fm=pa, factor analysis using principal axis method:<br/></em>fanone &lt;-  fa(r=data2, nfactors = <!-- -->4<!-- -->, rotate=<!-- -->"none"<!-- -->,fm=<!-- -->"pa"<!-- -->)<br/>print(fanone)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lj"><img src="../Images/c077f5244a08a870a54fc775a211c4b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQOsF6OJWi53_gnivGI5sg.png"/></div></div></figure><p id="9356" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">图形因子加载矩阵</em> </strong></p><p id="2fba" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">因子分析结果通常根据每个因子的主要载荷进行解释。这些结构可表示为载荷表或图形，其中所有具有绝对值的载荷&gt;一些切割点表示为一条边(路径)。</em></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="12b0" class="kv kw hi kr b fi kx ky l kz la">fa.diagram(fanone)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lk"><img src="../Images/80ba157908509e9e6cf97e0c13d3f530.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*YXoVjAIFQ5OqPiYlZ3yZpA.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated"><code class="du lp lq lr kr b">rotate=</code>“无”因子图</figcaption></figure><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="e078" class="kv kw hi kr b fi kx ky l kz la">fanone$loadings</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ls"><img src="../Images/9ef520b4d6c24808cf9a29f6a4751ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A60sxhywoNFwdPeHj3FkLg.png"/></div></div></figure><p id="377b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">前4个因子有一个特征值&gt; 1，它解释了几乎69%的方差。我们可以有效地将维数从11减少到4，同时只损失大约31%的方差。</em></p><p id="eb64" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">因子1解释了方差的29.20%；因子2占方差的20.20%；因子3解释了方差的13.60%；因素4占差异的6%。所有4个因素一起解释了69%的性能差异。</em></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="3f9e" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">#Rotated:<br/></em>fa1&lt;- fa(r=data2, nfactors = <!-- -->4<!-- -->, rotate=<!-- -->"varimax"<!-- -->,fm=<!-- -->"pa"<!-- -->)<br/>print(fa1)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lt"><img src="../Images/d2902a56c9bf48517c107d2213f24764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G9BJ7g7RJWoorMbFHZeF0w.png"/></div></div></figure><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="dd0b" class="kv kw hi kr b fi kx ky l kz la">fa1$loadings</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lu"><img src="../Images/fb677d7e2085a4cd905035f71d1d81ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SPw783czLWs1ZdVZKKJ7Jw.png"/></div></div></figure><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="f66b" class="kv kw hi kr b fi kx ky l kz la">#Print the factors diagram<br/>fa.diagram(fa1)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lv"><img src="../Images/0316ad8b0b8609aae39ab5620c96a7e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*mp656Q_MmT8YLFZ4yYizDg.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated"><code class="du lp lq lr kr b">Rotate=</code>“varimax”FA图</figcaption></figure><p id="af0f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">红色虚线表示竞争性定价略微低于PA4桶，且负载为负。</em></p><p id="ffe9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">标注和解释的因素</em> </strong></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lw"><img src="../Images/7ddde2762647c52f0fcc87678e56c87e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1D7DjRsozAjvhzwNoIDzGQ.png"/></div></div></figure><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="bf2b" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">#Scores for all the rows:<br/></em>head(fa1$scores)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lx"><img src="../Images/cec58f6b3a877353f2c46fb13eef33ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VI1fqCK7rpZTQ5y9Nm9z7A.png"/></div></div></figure><p id="e7a9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">以因子得分为自变量的回归分析:</em> </strong> <em class="jt"> <br/>我们把因变量和因子得分组合成一个数据集，并标注。</em></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="82df" class="kv kw hi kr b fi kx ky l kz la">regdata &lt;- cbind(data1[12], fa1$scores)<br/><em class="jt">#Labeling the data</em><br/><br/>names(regdata) &lt;- c("Satisfaction", "Purchase", "Marketing",<br/>                    "Post_purchase", "Prod_positioning")<br/>head(regdata)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ly"><img src="../Images/4be76491332c2ac4301da349a97e2239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BqouIEz4j9_Nyuy9ATOR8w.png"/></div></div></figure><p id="a415" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">让我们将数据集分成训练数据集和测试数据集(70:30) </em></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="658f" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">#Splitting the data 70:30</em><br/><em class="jt">#Random number generator, every time I run this coomand I come </em><br/><em class="jt">#up with different random numbers.</em></span><span id="f4d5" class="kv kw hi kr b fi lh ky l kz la">set.seed(100)<br/>indices= sample(1:nrow(regdata), 0.7*nrow(regdata))<br/>train=regdata[indices,]<br/>test = regdata[-indices,]</span></pre><p id="cfd9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">我们来训练回归模型。</em>T41】</strong></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="7bc2" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">#Regression Model using train data</em><br/>model1 = lm(Satisfaction~., train)<br/>summary(model1)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es lz"><img src="../Images/783aa974cbe1b9a802456da9290eea9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ot_QjvMpo6H0yeWivHjh3w.png"/></div></div></figure><p id="dfb6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">在模型中，购买、营销、产品定位因素非常显著，而购后因素不显著。让我们来看看VIF的分数。</em></p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ls"><img src="../Images/df17a4fd94a7f0f1941a3ec8059655c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D5TXB0n4gO7r6SAx4fxnYw.png"/></div></div></figure><p id="48c4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据VIF值，我们在模型1中没有多重共线性。</p><p id="01d1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">现在让我们检查测试数据集中模型的预测。</em></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="5271" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">#Model Performance metrics:</em><br/><strong class="kr hj">library</strong>(Metrics)<br/><em class="jt">#Model 1:</em><br/>pred_test1 &lt;- predict(model1, newdata = test, type = "response")<br/>pred_test1</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ma"><img src="../Images/89d911733bbc18d7fe3c669fc8c5e6a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3JKeHnpgOBYRi5zsdxz9Lw.png"/></div></div></figure><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="28df" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">#Find MSE and MAPE scores:</em><br/><em class="jt">#MSE/ MAPE of Model1</em><br/>test$Satisfaction_Predicted &lt;- pred_test1<br/>head(test[c(1,6)], 10)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mb"><img src="../Images/8546218a9d96fda88f85728bb2242783.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*3Ukh0jVBERJCf5dA74nS4Q.png"/></div></figure><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="845c" class="kv kw hi kr b fi kx ky l kz la">test_r2 &lt;- cor(test$Satisfaction, test$Satisfaction_Predicted) ^2<br/><br/>mse_test1 &lt;- mse(test$Satisfaction, pred_test1)<br/>rmse_test1 &lt;- sqrt(mse(test$Satisfaction, pred_test1))<br/>mape_test1 &lt;- mape(test$Satisfaction, pred_test1)</span><span id="ae23" class="kv kw hi kr b fi lh ky l kz la">model1_metrics &lt;- cbind(mse_test1,rmse_test1,mape_test1,test_r2)<br/>print(model1_metrics, 3)</span><span id="c0cd" class="kv kw hi kr b fi lh ky l kz la">##      mse_test1 rmse_test1 mape_test1 test_r2<br/>## [1,]     0.438      0.662     0.0864    0.64</span></pre><p id="08ea" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">由于功能“Post_purchase”并不重要，因此我们将删除该功能，然后让我们再次运行回归模型。</em></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="b7c4" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">##Regression model without post_purchase:</em><br/>model2 &lt;- lm(Satisfaction ~ Purchase+ Marketing+ <br/>                Prod_positioning, data= train)<br/>summary(model2)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ma"><img src="../Images/94d93dd8e813c63b912299f44403146c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pJY0FXxQkIGT1fCcbCAGcA.png"/></div></div></figure><p id="897f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">使用模型2预测测试数据集。</em></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="1e4e" class="kv kw hi kr b fi kx ky l kz la">pred_test2 &lt;- predict(model2, newdata = test, type = "response")<br/>pred_test2</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ld"><img src="../Images/38d72292b88170829c66a1ef5c625d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j1MlhGLzxqOKLeux7E0wQQ.png"/></div></div></figure><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="2117" class="kv kw hi kr b fi kx ky l kz la">test$Satisfaction_Predicted2 &lt;- pred_test2<br/>head(test[c(1,7)], 10)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es mc"><img src="../Images/4487b798bf0a8564268e089a7651e689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWT8BS96lWOuBquf6lEg5Q.png"/></div></div></figure><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="efc4" class="kv kw hi kr b fi kx ky l kz la">test_r22 &lt;- cor(test$Satisfaction, test$Satisfaction_Predicted2) ^2<br/>mse_test2 &lt;- mse(test$Satisfaction, pred_test2)<br/>rmse_test2 &lt;- sqrt(mse(test$Satisfaction, pred_test2))<br/>mape_test2 &lt;- mape(test$Satisfaction, pred_test2)<br/><br/>model2_metrics &lt;- cbind(mse_test2,rmse_test2,mape_test2,test_r22)<br/>model2_metrics</span><span id="c946" class="kv kw hi kr b fi lh ky l kz la">##      mse_test2 rmse_test2 mape_test2  test_r22<br/>## [1,] 0.4224132  0.6499332 0.08447357 0.6515163</span><span id="5b88" class="kv kw hi kr b fi lh ky l kz la">Overall &lt;- rbind(model1_metrics,model2_metrics)<br/>row.names(Overall) &lt;- c("Test1", "Test2")<br/>colnames(Overall) &lt;- c("MSE", "RMSE", "MAPE", "R-squared")<br/>print(Overall,3)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es md"><img src="../Images/dac15fff83ceaa111676f4a6835c450a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b_iqm6Uim6weMK3E_m5-sg.png"/></div></div></figure><p id="2fc8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt"> Test1模型矩阵包含所有4个分解特征。<br/> Test2模型矩阵没有分解特征“购买后”。</em></p><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="05f8" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">##Model with Interaction:</em><br/>names(regdata)<br/>## [1] "Satisfaction"     "Purchase"         "Marketing"       <br/>## [4] "Post_purchase"    "Prod_positioning"</span><span id="8e18" class="kv kw hi kr b fi lh ky l kz la">model3 &lt;- lm(lm(Satisfaction ~ Purchase+ Marketing+ Post_purchase+<br/>                 Prod_positioning+ Purchase*Post_purchase+<br/>                  Marketing*Prod_positioning+ Purchase* Marketing+<br/>                  Purchase* Prod_positioning*Marketing,data=train ))<br/>summary(model3)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es me"><img src="../Images/f6ce7b5790c60c53409d5c2f98bda8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ei4vSw8tvCdwngC5z7pww.png"/></div></div></figure><pre class="ju jv jw jx fd kq kr ks kt aw ku bi"><span id="ef4d" class="kv kw hi kr b fi kx ky l kz la"><em class="jt">##Predict with Interactions:</em><br/>pred_int_train = predict(model3, data = train, type = "response")<br/>pred_int_test = predict(model3, newdata = test, type = "response")<br/><br/>mse_train_int &lt;- mse(train$Satisfaction, pred_int_train)<br/>mse_test_int &lt;- mse(test$Satisfaction, pred_int_test)<br/><br/>rmse_train_int &lt;- sqrt(mse(train$Satisfaction, pred_int_train))<br/>rmse_test_int &lt;- sqrt(mse(test$Satisfaction, pred_int_test))<br/><br/>mape_train_int &lt;- mape(train$Satisfaction, pred_int_train)<br/>mape_test_int &lt;- mape(test$Satisfaction, pred_int_test)<br/><br/>r2_train &lt;- cor(train$Satisfaction, pred_int_train) ^2<br/>r2_test &lt;- cor(test$Satisfaction, pred_int_test) ^2<br/><br/>model3_metrics_train &lt;- cbind(mse_train_int,rmse_train_int,mape_train_int,r2_train)<br/>model3_metrics_test &lt;- cbind(mse_test_int,rmse_test_int,mape_test_int,r2_test)<br/><br/>interact_train_test &lt;- rbind(model3_metrics_train,model3_metrics_test)<br/><br/>row.names(interact_train_test) &lt;- c("Train","Test")<br/>colnames(interact_train_test) &lt;- c("MSE","RMSE","MAPE","R-squared")<br/><br/>print(interact_train_test,digits = 3)</span></pre><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es mf"><img src="../Images/a03bec26e3097528438d036e5aad6e6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F1OULExDwUGkfLGkNtrY_A.png"/></div></div></figure><p id="c866" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">包括相互作用模型，我们能够做出更好的预测。尽管相互作用并没有给个体变量带来显著的增加。但是有了相互作用模型，我们能够做出更接近的预测。在某些情况下，当我包含交互模式时，我能够增加模型性能度量。因此，我们可以推断，总的来说，模型是有效的，也没有过度拟合。</em></p><p id="6c10" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="jt">结论</em> </strong></p><p id="4be9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">在本文中，我们看到了如何使用因子分析来降低数据集的维度，然后我们对维度降低的列/特征使用多元线性回归进行进一步的分析/预测。本文涉及的主题有:<br/> 1。已检查多重共线性<br/> 2。运行因素分析<br/> 3。命名因素<br/> 4。用Y(因变量)和X(自变量)变量进行多元线性回归。</em></p><p id="3059" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我希望你们喜欢阅读这篇文章。如果您有任何反馈/建议，请告诉我。</p><p id="16d9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">你也可以在</em><a class="ae kb" href="http://www.linkedin.com/in/jay-narayan-das-43795a79" rel="noopener ugc nofollow" target="_blank"><strong class="iz hj"><em class="jt">LinkedIn</em></strong></a><em class="jt">联系我。</em></p><p id="1aa9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你觉得这篇文章有用，请鼓掌并与他人分享。</p><p id="b337" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">下次见！</em></p><p id="04cf" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">谢谢！！</em></p></div></div>    
</body>
</html>