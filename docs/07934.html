<html>
<head>
<title>Automatic Impression Generation from Medical Imaging Report</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从医学成像报告中自动生成印象</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/automatic-impression-generation-from-medical-imaging-report-3077d1d77d20?source=collection_archive---------12-----------------------#2020-07-12">https://medium.com/analytics-vidhya/automatic-impression-generation-from-medical-imaging-report-3077d1d77d20?source=collection_archive---------12-----------------------#2020-07-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="abd7" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">从医疗报告生成文本描述的过程——端到端深度学习模型</h2></div><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/465c797132dcb5be66113a9663efcb6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uj4WaNgdAk-F21rXl9yQfQ.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated"><a class="ae jv" href="https://www.boldbusiness.com/health/artificial-intelligence-radiology-evolving/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="c428" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">目录</h1><ol class=""><li id="b48d" class="ko kp hi kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">商业问题</li><li id="e0e2" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">数据集简介</li><li id="b59d" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">先决条件</li><li id="ff96" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">现有研究论文/解决方案</li><li id="7f9a" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">我的方法—解决方案</li><li id="e1da" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">XML解析创建数据点</li><li id="d06c" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">数据预处理</li><li id="e0f3" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">探索性数据分析</li><li id="c72f" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">数据点构造</li><li id="1e73" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">训练测试和验证分离</li><li id="44b5" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">标记化和数据集准备</li><li id="55dd" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">基本模型[CNN]</li><li id="d4f5" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">主模式[CNN-BiLSTM]</li><li id="14cf" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">结论</li><li id="ed5b" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">误差分析</li><li id="c69e" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">未来的工作</li><li id="06b7" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb lc ld le lf bi translated">参考</li></ol><h1 id="c270" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">1.商业问题</h1><p id="7f37" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">这里的问题陈述是从给定的胸部x光图像中找到印象。这些图像是胸部的两种类型的正面和侧面视图。使用这两种类型的图像作为输入，我们需要找到给定X射线的印象。</p><p id="ae1b" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">为了解决这个问题陈述，我们将建立一个预测模型，该模型涉及图像和文本处理，以建立一个深度学习模型。自动描述给定图像的内容是最近连接计算机视觉和自然语言处理的人工智能模型之一。</p><h1 id="c253" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">2.数据集简介</h1><p id="e33b" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated"><strong class="kq hj">印第安纳大学Open-i胸部x光片采集</strong></p><p id="a0d9" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">该数据集是来自印第安纳大学医院网络的大约7，470张胸部x射线和3，955份胸部x射线图像的放射学报告。—图像以png格式下载—报告以xml格式下载。</p><p id="eb2c" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">每个xml都是相应患者的报告。为了识别与报告相关联的图像，我们需要检查id中的xml tag <parentimages id="”image-id”"> id属性，我们有对应于png图像的图像名称。一个报告或xml可以关联多个图像。</parentimages></p><p id="8303" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">原始数据来源:<a class="ae jv" href="https://openi.nlm.nih.gov/" rel="noopener ugc nofollow" target="_blank">https://openi.nlm.nih.gov/</a></p><p id="0924" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">其他资源:<a class="ae jv" href="https://www.kaggle.com/raddar/chest-xrays-indiana-university" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/rad Dar/chest-x rays-Indiana-university</a></p><p id="7a4d" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">样本数据点:</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es md"><img src="../Images/fb41b17c61a53a24a7cb3e1af4e4c707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*5YBODJunUawimKtDSy5dDA.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">openi nlm数据点示例</figcaption></figure><h1 id="fc15" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">3.先决条件</h1><p id="5024" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">在我们深入研究这项工作之前，我假设您熟悉以下深度学习概念和python库。</p><p id="8e12" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">卷积神经网络，递归神经网络，LSTM，转移学习，激活函数，优化技术，如SGD，Adam。损失函数如分类交叉熵、稀疏分类交叉熵。最后，TensorBoard用于性能可视化和调试</p><p id="364e" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">Python，tensorflow，Keras，tokenizer，Pandas，numpy，Matplotlib。理解顺序Api、函数Api和模型子类keras模型实现的概念。我选择子类模型的原因是，它是<strong class="kq hj">完全可定制的</strong>，并且使你能够<strong class="kq hj">实现你自己定制的模型的前向传递</strong>。此外，我们可以控制网络和培训过程的每一个细节。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es me"><img src="../Images/ca6ad8385b153e7092054cf379892d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*kX4sdw53n4kJp3Ep4l9_NA.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated"><a class="ae jv" href="https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="9c9f" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">下面我已经提到了开始导入博客和教程。</p><p id="b797" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">1.<a class="ae jv" href="https://www.tensorflow.org/tutorials/text/nmt_with_attention" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/tutorials/text/NMT _ with _ attention</a>—tensor flow教程</p><p id="cee6" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">2.<a class="ae jv" href="https://www.tensorflow.org/tutorials/text/image_captioning" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/text/image_captioning</a>—张量流教程</p><p id="1d5d" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">3.<a class="ae jv" href="https://becominghuman.ai/transfer-learning-retraining-inception-v3-for-custom-image-classification-2820f653c557" rel="noopener ugc nofollow" target="_blank">https://becoming human . ai/Transfer-Learning-re training-inception-v3-for-custom-image-class ification-2820 f 653 c 557</a>—转学教程</p><p id="35f3" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">4.<a class="ae jv" href="https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202" rel="noopener" target="_blank">https://towards data science . com/a-simple-guide-to-the-versions-of-the-inception-network-7fc 52 b 863202</a>—inception v3模型教程</p><p id="30c1" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">5.<a class="ae jv" href="https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2017/03/20/ImageNet-vggnet-resnet-inception-xception-keras/</a>—为什么是ImageNet —为什么是InceptionV3</p><p id="9007" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">6.<a class="ae jv" href="https://www.pyimagesearch.com/2019/10/28/3-ways-to-create-a-keras-model-with-tensorflow-2-0-sequential-functional-and-model-subclassing/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2019/10/28/3-ways-to-create-a-keras-model-with-tensor flow-2-0-sequential-functional-and-model-subclass化/ </a> — 3种Keras模型实现方式</p><p id="f0ab" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">7.<a class="ae jv" href="https://www.tensorflow.org/tensorboard/get_started" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tensorboard/get_started</a>—冲浪板教程</p><h1 id="858e" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">4.现有研究论文/解决方案</h1><p id="3c03" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">这项工作的灵感来自以下研究和博客:</p><p id="a9a0" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><a class="ae jv" href="https://arxiv.org/abs/1502.03044" rel="noopener ugc nofollow" target="_blank">展示、参与和讲述:具有视觉注意力的神经图像字幕生成</a></p><p id="0c1f" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">在提到的论文中，他们使用了具有注意机制的编码器和解码器模型。在编码器部分，他们使用CNN从图像中提取特征。在解码器中，他们使用长短期记忆(LSTM)网络，该网络根据上下文向量、先前的隐藏状态和先前生成的单词，通过在每个时间步长生成一个单词来产生字幕。他们使用BLEU评分来衡量模型的性能。</p><p id="b44f" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我引用的其他博客很少。</p><p id="3c20" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">1.<a class="ae jv" href="https://towardsdatascience.com/image-captioning-in-deep-learning-9cd23fb4d8d2" rel="noopener" target="_blank">https://towards data science . com/image-captioning-in-deep-learning-9cd 23 FB 4d 8d 2</a></p><p id="711e" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">2.<a class="ae jv" href="https://www.analyticsvidhya.com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/04/solving-an-image-captioning-task-using-deep-learning/</a></p><h1 id="c930" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">5.我的方法—解决方案</h1><p id="453b" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">最初，我将使用EDA进行探索性数据分析第一部分，包括图像输入和文本输出。我可以发现数据不平衡、每个患者的图像可用性、每个患者的相关图像类型。在EDA之后，我将使用两种不同的方法来实现深度学习模型，以找到彼此的改进之处。</p><h2 id="b6f7" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">1.基本模型:</h2><p id="100c" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">一个简单的编码器和解码器架构。在编码器部分，它将具有CNN单个全连接层，以从预训练的InceptionV3模型中获取图像的特征向量。解码器部分将有LSTM层，它需要两个输入，一个是图像特征向量和每个时间步的文本到单词的序列。</p><h2 id="2eb4" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">2.主型号:</h2><p id="26ad" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">我将使用编码器-解码器架构来生成来自胸部x光的印象。编码器将输出图像特征向量。然后将特征向量传递给具有注意机制的解码器，这将为图像内容生成下一个单词。使用与基本模型相同的模型方法，我将创建一个新的架构，使用研究论文<a class="ae jv" href="https://www.aclweb.org/anthology/P16-2034.pdf" rel="noopener ugc nofollow" target="_blank">基于注意力的双向长短期记忆网络进行关系分类</a></p><p id="1029" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">作为第一步，我将使用InceptionV3模型对该数据集进行图像分类<a class="ae jv" href="https://www.kaggle.com/yash612/covidnet-mini-and-gan-enerated-chest-xray" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/yash 612/covid net-mini-and-gan-generated-chest-Xray</a>。利用该分类模型，我将保存该训练的权重，并通过将保存的权重加载到InceptionV3来在编码器特征提取中使用该权重。</p><h2 id="c878" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated"><strong class="ak">编码器:</strong></h2><p id="52dc" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">编码器是一个单一的全连接线性模型。输入图像被提供给InceptionV3以提取特征。这两个图像的提取特征被相加并输入到FC层以获得输出向量。编码器的最后一个隐藏状态连接到解码器。</p><h2 id="99cb" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated"><strong class="ak">解码器:</strong></h2><p id="5368" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">解码器是一个具有双向LSTM层的解码器，它在单词级别进行语言建模。第一次步骤接收来自编码器的编码输出和<start>向量。该输入传递到具有注意机制的两级双向LSTM层。输出向量是两个向量，一个是预测标签，另一个是解码器的先前隐藏状态，这在每个时间步长再次反馈给解码器。详细的架构在下面提到。</start></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es mt"><img src="../Images/0295d21ce51424fcdee4544dbdbb4314.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*vp8-LiTp-RlAzZbaS9ZyAQ.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">高级模型架构</figcaption></figure><h1 id="ea23" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">6.XML解析创建数据点</h1><p id="b2f5" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">在这一节中，我们将看到如何将原始xml数据解析和结构化为数据点，然后将数据点存储在csv文件中以满足未来的模型需求。</p><p id="85f0" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">原始XML树视图:</strong></p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="5724" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我们将从xml文件中提取抽象节点和ParentImage节点。在这我们有印象和图像文件名如下。</p><p id="0b04" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">印象等级:</strong></p><p id="8848" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我们将检索抽象文本值</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="5772" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">图像文件名:</strong></p><p id="c66a" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">id属性中可用的图像文件名。我们可以忽略其他细节，因为这些数据与我们的报告无关。正如我们所看到的，有两个parentImage节点，我们为这个报告准备了两个图像。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="b822" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">XML解析器代码来检索上面提到的细节。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">xml解析</figcaption></figure><p id="2514" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">提取后，我们有3955行，数据帧视图中的数据，</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mw"><img src="../Images/2c2985caf0933950f8f16117b3226d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZdV6a-EY6DPuCMrS12Yq7A.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">解析的数据</figcaption></figure><h1 id="ff77" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">7.数据预处理</h1><p id="6cd2" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">在这个阶段，文本数据被预处理以去除不想要的标签、文本、标点和数字。我们还将检查空单元格或NaN值。</p><ul class=""><li id="e378" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">如果图像名称列中有任何空单元格，我们将删除这些单元格。</li><li id="67ee" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">如果文本数据中有任何空值或NaN值，我们将用“No <column name="">”(例如:No Impression)替换它</column></li><li id="5830" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">计算每个文本列的字数，并将其添加到dataframe列。</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es mw"><img src="../Images/2c2985caf0933950f8f16117b3226d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZdV6a-EY6DPuCMrS12Yq7A.png"/></div></div></figure><p id="cf5b" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">在数据预处理之后，缺失值处理下面是dataframe视图，我们在最终数据点中总共有3851行。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nb"><img src="../Images/2f045c68ee91c7d0e2e0cd89a5c3416f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v9ZlJYJsVjxTjPhaMoUI-w.png"/></div></div></figure><ul class=""><li id="a148" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">唯一图像总数3851</li><li id="922a" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">唯一标题402的总数</li><li id="d0ef" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">唯一比较总数281</li><li id="eea3" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">独特标志总数2098</li><li id="b969" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">独特调查结果总数2545</li><li id="56ec" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">唯一印象总数1692</li></ul><h1 id="a70d" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">8.探索性数据分析</h1><p id="4236" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">在本节中，我们将看到通过总结和可视化数据集的主要特征来分析数据集的不同方法。</p><h2 id="ce32" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">8.1文本数据上的EDA</h2><p id="3c99" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">在文本分析中，我们将采用印象列目标变量。通过下面的可视化，我们可以看到出现频率最高的100个句子。</p><p id="4ca2" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">印象的句子出现次数</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nc"><img src="../Images/fb00f70e2cc1c47bed7e83e5cbc044b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C37E9X7KJa_giNdpvNhl5g.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">印象的句子出现</figcaption></figure><ul class=""><li id="e176" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">从上面的可视化我们可以看到，“没有急性心肺异常”发生了近600次。</li><li id="64eb" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">大多数较长的句子出现的次数少于10次</li></ul><p id="b408" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">印象词出现次数</strong></p><p id="23f0" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我们将使用印象栏的单词云来查看单词智能出现</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nc"><img src="../Images/e4427c057488eaf88a01a7081a5eab55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lfNYJ2_Ns_w1Ya8tvMhRNQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">词云</figcaption></figure><ul class=""><li id="71f0" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">上面的词云是在前1000个最大出现词上生成的。</li><li id="7e88" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">急性、心肺、异常、疾病、胸膜、积液、活动性这些是上面可视化中突出显示的词。</li></ul><p id="59b2" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">字数分布</strong></p><p id="7b52" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">让我们看看impression列中的字数分布，因为我们已经计算了impression_count列中的字数，我们看到的分布如下所示。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nd"><img src="../Images/81949f245ce6120ff036aa31e4be0953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xdu0foBIKcXABLeRUTKyuA.png"/></div></div></figure><p id="2128" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><em class="ne">最小字数为1 —最大字数为122 —平均字数为5.0 </em></p><ul class=""><li id="10eb" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">我们可以从这个分布中看出最大和最小字数。</li><li id="2318" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">最大出现次数通常为5</li><li id="2178" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">大多数情况下，字数在5到10之间。</li></ul><h2 id="3ed9" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">8.2图像数据的EDA</h2><p id="40da" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">让我们分析每个数据点或报告呈现的总图像。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nf"><img src="../Images/a63e90b4d1a466e198dbde9b9a4f7d08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQcr8N8vzUkAK47KUw7mLw.png"/></div></div></figure><p id="56b4" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><em class="ne">最小图像数为1 —最大图像数为5 —中值图像数为2.0 </em></p><ul class=""><li id="257c" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">每条记录最常见的图像数是2。</li><li id="2d8b" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">其次是单一图像。</li><li id="72d4" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">我们每张唱片也有5张图片。</li></ul><p id="77d1" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">显示随机25个病人的x光片</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ng"><img src="../Images/b352e5172e45e4ca16c3ca335083cbf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SCNhpkdqgd-xN9Ls5InZGw.png"/></div></div></figure><p id="ad90" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">正如我们所看到的，这些图像是正面和侧面的。并且每个患者具有与其相关联的一个或多于两个图像。让我们看看一些随机数据点的图像。</p><p id="4c98" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">样本数据点</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nh"><img src="../Images/ef10e91563ab37882917c50e237422a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q8Xvvm5UpFD0_pr4CXzeGg.png"/></div></div></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ni"><img src="../Images/3307998514de688e0e19c6d2ba36618a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7UndeFrhjAVS1MWeljDA3w.png"/></div></div></figure><h2 id="f183" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">8.3 EDA结果</h2><ul class=""><li id="22dc" class="ko kp hi kq b kr ks kt ku kv kw kx ky kz la lb na ld le lf bi translated">来自xml文件的所有原始文本被解析并创建数据集。</li><li id="5d78" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">每个病人都有多张x光片。</li><li id="90dc" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">主要发现是图像是如何按顺序排列的，或者图像的数量与每个记录有关。</li><li id="21c2" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">我们每次记录主要有2张正面和侧面的图像。此外，我们有1、3、4、5张图像与每条记录相关联。</li><li id="790a" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">没有丢失文件。我们总共有3955条记录和3个额外的特征(比较、指示和发现)，我们不会在这个模型和1个印象目标变量中使用。</li><li id="af8d" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">出现最多的词:印象-急性心肺</li><li id="cb78" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">图像有不同的形状。</li><li id="b9b7" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">所有的x光图像都是人体上半身，尤其是胸部。</li><li id="17d3" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">在文本特征中有一些未知的值，如XXXX XXXXX，它们被替换为空字符串。</li></ul><h2 id="8a37" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">8.4数据冲突</h2><p id="0073" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">只有正面和侧面两种图像类型，但是我们为每个数据点关联了1、3、4和5个图像。我们在这里遇到了一个冲突，如何为我们建立的模型提供数据点。由于这种冲突，我们需要想出一个办法来处理如何将数据输入到模型中。在构建我们的模型之前，我们看到一些数据点结构化方法可以帮助我们处理这种情况。</p><h1 id="f19e" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">9.数据点构造</h1><p id="7f18" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">因为我们有多于2个图像或者在某些情况下少于2个图像与每个数据点相关联。如果我们没有图像，我们就丢弃那些数据点。</p><p id="7b7a" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">让我们处理有1，3，4，5个图像的数据点。以下是图像集数量的数据点计数。</p><p id="7542" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">具有两个图像的数据点是3208</p><p id="c5a4" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">具有1个图像的数据点是446</p><p id="5ac3" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">具有3个图像的数据点是181</p><p id="8048" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">具有4个图像的数据点是15</p><p id="7307" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">具有1个图像的数据点是1</p><p id="6dd4" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">总数据点是3851个数据点</p><p id="5c96" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">方法，</p><p id="e2a6" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">将数据点限制为每个数据点2个图像，如果我们有5个图像，则它是4+1(所有图像+最后一个图像),因此使它成为4个数据点，如下所示。</p><p id="0df3" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">如果剩下的图像是正面的，那么最后一张图像应该是侧面的。</p><p id="a711" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">如果我有5个图像，这里第5个图像是侧面其他或正面，</strong></p><p id="617f" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">第一张图像+第五张图像= &gt;正面+侧面</p><p id="2e12" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">第二张图像+第五张图像= &gt;正面+侧面</p><p id="13da" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">第三幅图像+第五幅图像= &gt;正面+侧面</p><p id="8821" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">第四幅图像+第五幅图像= &gt;正面+侧面</p><p id="5f0f" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">从单个数据点增加到4个数据点</p><p id="bf05" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">同样，对于其他数据点，</p><p id="1d28" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">如果我有4个图像，那么，</strong></p><p id="e01b" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">第一(正面)+第四(侧面)</p><p id="dbf2" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">第二(正面)+第四(侧面)</p><p id="085f" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">第三(正面)+第四(侧面)</p><p id="093e" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">从1个数据点增加到3个数据点</p><p id="a837" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">如果我有3个图像，那么，</strong></p><p id="6bdd" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">第一(正面)+第三(侧面)</p><p id="bd97" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">第二(正面)+第三(侧面)</p><p id="8007" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">从单个数据点增加到2个数据点</p><p id="b412" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">如果我们只有一个图像，那么</strong></p><p id="12bd" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">第一张图像(正面或侧面)+复制第一张图像</p><p id="756c" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">相同的数据点计数。我们需要确保这个重复的数据点应该在训练测试和验证集中平均分配。如果我们没有侧面图像，那么保持正面作为最后的图像数据点。</p><p id="c2d3" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">因此，使用这种数据构造方法，我们还可以增加数据点，并得到精细的输入数据点。用于上述数据结构的代码。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="a0be" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">构建完数据点后，我们将把<start>和<end>标记添加到文本数据中。</end></start></p><p id="cf88" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">最终数据点，</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nj"><img src="../Images/859b26cc99f7161cbeeabaed29443b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dgRjILPo5R_kH-kwi0xOVw.jpeg"/></div></div></figure><h1 id="f120" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">10.训练测试和验证分离</h1><p id="e867" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">我们有一个单独的数据，一个没有重复的数据点，另一个有重复的数据点。我们需要拆分数据点，因为重复的数据点在所有三个拆分中是同等可用的。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nk"><img src="../Images/d33a980a163233dcbcad69caa8200f78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mYHUmmecys9mKgeP8knAxg.png"/></div></div></figure><p id="604b" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">在获取两个不同的数据集后，我们需要同等地连接数据集。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nl"><img src="../Images/cb4122e53d6b0caa66964d158da07154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B7ggP6pyNSGk4j3gcduxPg.jpeg"/></div></div></figure><p id="5e7e" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我们得到如上的最终数据点形状。</p><h1 id="c2b6" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">11.标记化和数据集准备</h1><h2 id="5833" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">11.1.标记化</h2><p id="3e61" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">我们不能向我们的深度学习模型输入原始文本。文本数据需要编码为数字，然后在机器学习和深度学习模型中使用。Keras深度学习库提供了一些基本的工具来执行这个操作。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="faf5" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">总词汇量(vocab_size)为1339，输出句子的最大长度为60。</p><h2 id="c373" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">11.2.数据集准备</h2><p id="7ff6" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">对于数据集准备，我们将使用迁移学习方法进行图像到特征向量的转换和文本数据标记化。</p><p id="1800" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">关于我为什么选择《盗梦空间》模型而非其他模型，请参考这篇博客<a class="ae jv" href="https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/</a></p><p id="226b" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我将使用在ImageNet数据集上训练的InceptionV3模型。最初，我将使用下面提到的数据集做一个x光分类任务。<a class="ae jv" href="https://www.kaggle.com/yash612/covidnet-mini-and-gan-enerated-chest-xray" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/yash 612/covid net-mini-and-gan-generated-chest-Xray</a>。这是一个三级分类任务，我们需要对患者的x光片进行分类，看它是属于这三级电晕、正常还是肺炎。</p><p id="18de" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">分类完成后，我将保存已训练模型的权重，并使用移除了顶层形状(1，2048)的模型作为我们模型的特征向量，并准备数据集。</p><p id="ce50" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">下面是这个分类任务的模型架构。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">迁移学习</figcaption></figure><p id="2f52" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">分类任务的准确度图。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es nm"><img src="../Images/c6a8ab264c814b4e932efcc06fe76d41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*A0b8gsvwUxD4Yh_r6AJ2pQ.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">准确度图</figcaption></figure><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="cf79" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">模型权重保存为hdf5文件，以备将来使用。</p><p id="8db5" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我已经使用图像净权重和无图像净权重训练了该模型，其中图像净权重在该分类中表现良好。</p><p id="cad1" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">有了训练好的权重，我将像下面这样为我们的图像数据提取特征。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">装载重量</figcaption></figure><p id="0dc2" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我将为所有可用的图像创建一个图像张量，如下图所示。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">创建图像张量</figcaption></figure><p id="324d" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">这些图像张量用于TensorFlow数据集准备，基本上我在这里做一个缓存机制，以备将来使用。</p><p id="eb38" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">使用tf.data创建张量流</strong></p><p id="fc4c" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">关于TF . data:<a class="ae jv" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/guide/data</a>的进一步阅读，请参考链接</p><p id="a4b5" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">需要阅读的教程:<a class="ae jv" href="https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/" rel="noopener ugc nofollow" target="_blank">https://adventuresinmachinehlearning . com/tensor flow-dataset-tutorial/</a></p><p id="bf2b" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">现在我们有了图像张量和文本向量，我们可以构建tf.data数据集</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">tf .数据创建</figcaption></figure><p id="c085" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">Multi_image()函数将形状的双输入张量(1，2048)和(1，2048)转换为(2，1，2048)。提到了Batch_size、嵌入维数和单元大小，这些是我们可以根据我们的模型进行调整的超参数。</p><p id="b794" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">因此，我们已经为我们的模型工作完成了特征提取和标记化，并且我们有了tf.data数据集，现在让我们构建所需的模型。</p><h1 id="2cd0" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">12.基本模型</h1><h2 id="6731" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">12.1.模型架构</h2><p id="f56b" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">正如我已经解释过的子类模型。我将直接跳到模型架构中。</p><p id="7696" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我已经构建了用于检查模型架构的功能Api模型</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nn"><img src="../Images/96d2ebcd8c97672254d450f6cbe07953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bZKrBmbxIh8fi_TYK5xT6w.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">基本模型架构</figcaption></figure><h2 id="8eb7" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">编码器架构:</h2><p id="2238" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">具有单个全连接层线性输出。在我们传递到FC层之前，我们将两个图像张量相加并传递到FC层。该层输出(批量大小，1，嵌入尺寸)的形状</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">编码器</figcaption></figure><h2 id="9b8c" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">解码器架构:</h2><p id="22b1" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">在这一部分中，我们有一个嵌入层LSTM层和输出形状的密集层</p><p id="aaf0" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">LSTM层是长短期记忆网络——通常简称为“lstm”——是一种特殊的RNN，能够学习长期依赖关系。</p><p id="2f30" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">要了解更多关于LSTM的信息，请点击此链接:<a class="ae jv" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解LSTM网络</a></p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">解码器</figcaption></figure><h2 id="bcc2" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">12.2模型度量和优化器初始化</h2><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div></figure><h2 id="8f86" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">12.3模型培训</h2><p id="57db" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">对于培训阶段，我们使用教师强制。教师强制是一种训练递归神经网络的策略，它使用前一时间步的模型输出作为输入。</p><p id="a0c3" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">在训练中，可以使用“序列开始”标记来开始该过程，并且在输出序列中生成的单词被用作后续时间步骤的输入，可能与图像或源文本等其他输入一起使用。</p><p id="c770" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">这个相同的递归输出-输入过程被使用，直到模型收敛到更好的结果。下面我已经提到了出处。</p><p id="cfdc" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">关于教师强迫的进一步阅读:链接到<a class="ae jv" href="https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/" rel="noopener ugc nofollow" target="_blank">教师强迫</a></p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">训练步骤</figcaption></figure><h2 id="b89b" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">12.4.在TensorBoard中可视化模型性能</h2><p id="53c6" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">我们已经使用<code class="du no np nq nr b"><a class="ae jv" href="https://www.tensorflow.org/api_docs/python/tf/summary" rel="noopener ugc nofollow" target="_blank">tf.summary</a></code>记录了损失和精确度</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ns"><img src="../Images/87de3472675102c622f8c779eb1a836b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cqiN1-4KixP0bCegLJRTEw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">准确(性)</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nt"><img src="../Images/f11ccbfcb77d085d9274fbc91e62ce98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZGR-aHjMmvA03oYn4hKXQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">失败</figcaption></figure><h2 id="4547" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">12.5.模型评估</h2><p id="33f5" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">在评估或测试阶段，我使用了基于argmax搜索的教师强制查找输出句子。在时间步骤t中，我们使用<start>令牌生成一个字，预测的字再次反馈到下一个步骤，并在时间t+1成为解码器的输入。下面提到了argmax搜索的代码。</start></p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">评估阶段</figcaption></figure><p id="215d" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">样本输出如下图所示</strong></p><p id="47f9" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">让我们先试一个长一点的句子</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nu"><img src="../Images/5249a4527fbd32f53e963028ec0174af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7bi_U_Moi1f8iIGHS5F-yg.jpeg"/></div></div></figure><p id="a0ad" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">在较长的句子中预测是不完美的，让我们看看较短的句子。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nv"><img src="../Images/62178da97734ac410c7393eb67ef82f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ql7_IQlpb7XSlAjlApxQPg.png"/></div></div></figure><p id="64ec" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">即使在短句模式中表现不佳。</p><h2 id="6000" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">12.6.基本模型结论</h2><ul class=""><li id="73ec" class="ko kp hi kq b kr ks kt ku kv kw kx ky kz la lb na ld le lf bi translated">这个模型是建立在一个简单的LSTM编码器和解码器上的。</li><li id="2324" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">得到不完美或不最坏的预测</li><li id="407e" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">验证的准确性并没有提高很多，但损失是趋同的</li><li id="8575" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">我们甚至可以对这个模型进行微调，使其表现良好。</li></ul><p id="14c9" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我们将看到一个更好的性能和修改的架构，具有双向LSTM层和附加注意机制。</p><h1 id="2788" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">13.主模型</h1><h2 id="b20b" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">13.1模型架构</h2><p id="68d6" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">这个模型架构是使用我看到的一篇研究论文<a class="ae jv" href="https://www.aclweb.org/anthology/P16-2034.pdf" rel="noopener ugc nofollow" target="_blank">重新实现的，基于注意力的双向长短期记忆网络用于关系分类</a></p><p id="2fab" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">在我们理解这个模型的模型架构之前，请参考这篇论文。</p><p id="a234" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">在这篇文章中，他们详细地提出了注意力-BLSTM模型。</p><p id="bcb5" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">如下图所示，本文提出的模型包含五个组件:</p><ul class=""><li id="25c9" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">输入层:将句子输入到这个模型中，并与图像特征向量相加</li><li id="10f6" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">嵌入层:将每个单词映射到一个低维向量中</li><li id="61d4" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">LSTM层:利用BLSTM从步骤(2)获得高级特征。为了更深入地理解特征，BLSTM层被重复两次</li><li id="ae55" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">注意层:产生一个权重向量，通过乘以权重向量，将每个时间步的单词级特征合并为句子级特征向量</li><li id="efab" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">输出层:句子级特征向量最终用于关系分类。这些组件将在接下来的章节中以详细的功能视图呈现。</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nw"><img src="../Images/198e8309c4f1a261796e3c15da2a4b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sGqZ-Zt0LEU6aCzXRVDgoQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated"><a class="ae jv" href="https://www.aclweb.org/anthology/P16-2034.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="83fd" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">让我们看看使用功能Api的模型功能层。在该模型中，超参数与基本模型相同，唯一的变化是最大句子长度取为80。</p><p id="7106" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">下面相同的功能模型将使用模型子类实现，正如我已经提到的，子类模型在调试你的架构时更容易。我们可以控制每一层。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nx"><img src="../Images/af70c399456113c43d5f3357153646f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mX-aXUAQBp_NFtM_XNcbIw.png"/></div></div></figure><p id="8eb2" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">现在我们可以看到上面的架构是如何使用模型子类实现的。具有独立的编码器和解码器部分。</p><h2 id="9638" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">编码器架构:</h2><p id="0724" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">在编码器部分，它与具有单个全连接层的图像向量相加的基本模型架构相同。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">编码器最终</figcaption></figure><h2 id="4515" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">解码器架构:</h2><p id="1527" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">与上述论文类似的架构，我修改了一个额外的BiLSTM层，以获得更好的特性表示。使用注意机制。看看这个<a class="ae jv" rel="noopener" href="/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129">环节</a>中注意力机制的快速概览。参考资料部分提到了更多关于注意力机制的阅读材料(注意力是你所需要的)</p><p id="6033" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">在我们的模型中，我使用了张量流附加注意，它只不过是Bahdanau式的注意。请参考参考部分中的实现细节。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">解码器最终模型</figcaption></figure><p id="8174" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">模型度量和优化初始化器的简要说明和实现，提到了模型训练基本模型部分，同样在主模型中使用。</p><h2 id="c78f" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">13.2在TensorBoard中可视化的模型性能</h2><p id="5e9c" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">我们已经使用<code class="du no np nq nr b"><a class="ae jv" href="https://www.tensorflow.org/api_docs/python/tf/summary" rel="noopener ugc nofollow" target="_blank">tf.summary</a></code>记录了损失和精确度</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ny"><img src="../Images/603399e010e796ae914fb7c81b6e18fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cV8XWK9zK75bY74gV_2UCQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">准确(性)</figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nj"><img src="../Images/5ed369b70c55c59b9c8589490c4f5722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0CKGccTKszEwfe003BhYOA.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">失败</figcaption></figure><h1 id="a47b" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">13.3模型评估</h1><p id="90e2" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">在评估或测试阶段，我使用了基于Beam搜索的教师强制查找输出句子。正如我们已经看到的教师强迫简单来说，让我们移动到实施部分。</p><p id="6dc7" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj"> Bleu评分标准:</strong></p><p id="0f91" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我使用Bleu(双语评估替角)分数作为衡量机器翻译单词到实际单词的质量的标准。快速浏览一下维基蓝<a class="ae jv" href="https://en.wikipedia.org/wiki/BLEU" rel="noopener ugc nofollow" target="_blank">这里</a></p><p id="73b8" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">光束搜索:</strong></p><p id="e991" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">不是贪婪地(通常选择单个最高概率字)在构建序列时选择最可能的下一步，波束搜索扩展所有可能的下一步，并保持<em class="ne"> k </em>最可能的k是波束索引。换句话说，其中<em class="ne"> k </em>是用户指定的参数，并通过概率序列控制波束或并行搜索的数量。快速浏览一下这个<a class="ae jv" href="https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/" rel="noopener ugc nofollow" target="_blank">源链接</a>。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">波束搜索</figcaption></figure><p id="eddd" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">样本输出如下所示，其中bleu得分为累积得分和N gram得分。</strong></p><p id="06cd" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">短句</p><p id="f8b0" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">短句</p><p id="365b" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">正如我们所看到的，预测的输出是好的。从bleu评分上也能看出来。</p><p id="6da5" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">较长的句子</p><p id="bd5d" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">使用Bleu评分不是一个好的预测，但我们可以看到两个句子的意思有一些相似之处，这两个句子都解释了这个记录没有疾病。这些论文的主要问题在于Bleu评分没有考虑单词的含义。</p><p id="6109" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">让我们试试另一个长句</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nh"><img src="../Images/f7ba56985f6892506d95ceb6ff7b74bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CD6a32XsTJT1UL897KthJQ.png"/></div></div></figure><p id="035d" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">这一次也和先前的预测一样。前4个单词与预测的单词完全匹配，但我们仍然没有得到好的Bleu分数，因为单词数很高。</p><p id="69a3" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">以上是几个随机预测。</p><h2 id="665e" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">13.4模型结论</h2><ul class=""><li id="7b48" class="ko kp hi kq b kr ks kt ku kv kw kx ky kz la lb na ld le lf bi translated">建立在双向LSTM上的模型似乎比基本模型表现得更好。</li><li id="b9e6" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">根据在用于关系分类的基于源注意力的双向长短期记忆网络中提到的模型架构，它在分类任务中工作良好。</li><li id="6238" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">Loss收敛到0.3，训练准确率为89%，验证准确率为92%。从结果中可以看出，每个预测输出和实际输出之间存在相似性。</li></ul><h1 id="fb72" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">14.结论</h1><ul class=""><li id="5e8c" class="ko kp hi kq b kr ks kt ku kv kw kx ky kz la lb na ld le lf bi translated">与普通ImageNet训练的InceptionV3模型相比，在X射线分类中训练相同的InceptionV3，并使用该权重提取图像特征，可以提高模型的性能。</li><li id="c029" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">BiLSTM体系结构给出了很好的结果，即使在非常少的Bleu分值中，我们也能够在预测的句子中看到真实句子的含义。</li><li id="606d" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">我们可以对预测进行误差分析，以确定是模型问题还是数据点问题。</li></ul><h1 id="f377" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">15.误差分析</h1><p id="2563" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">在本节中，我们将看到错误分析，它是对导致此错误的原因的分析，并使用该发现来改进模式。我们将研究低Bleu分数数据点，这是这种情况下的错误，以及如何理解它。在错误识别之后，我们将看到如何使用它来改进模型。</p><p id="3de8" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">模型中的误差可以是可约的或不可约的，我们将研究可约的误差。在训练模型之后，我们检查验证集以找到错误并进行分析。一旦我们发现错误，如果它是可减少的错误，那么我们在我们未来的模型训练中修正这些错误，这样模型将比你以前的模型有所改进。</p><p id="c7eb" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">让我们看看我是如何进行这个错误分析的。</p><p id="8599" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">具有Bleu分数的验证集，</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es nz"><img src="../Images/bf7ad98337028051e47a3e354a0159b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o0TeSH0iurJAHQEX2PCrCQ.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">开发集上带有Bleu分数的数据框</figcaption></figure><p id="33e7" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">在初始步骤中，我们将取小于0.08 Bleu的分数，并检查数据点。</p><p id="7f2c" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">我们也可以忽略重复的数据点。我们在模型中使用了重复的数据点。在两个输入中具有相同的图像，我们认为这是噪声点检查9。数据点构造一节查找数据构造的细节。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es oa"><img src="../Images/2b3ae63a35bd8c08f42d134ec71907cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yaeqnFfvAGp2Mp6CA2YQ4A.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">带有重复图像</figcaption></figure><p id="3b50" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">从总共139个差Bleu分数数据集，我们有22个重复数据集。</p><p id="bd14" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">最终数据点，</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ob"><img src="../Images/818f0ecfc677c5e111d63bd2e83f83a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8cgNX09QWVrOM827ND5AYw.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">带字数统计</figcaption></figure><p id="7e51" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">让我们来分析每个数据点，</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es oc"><img src="../Images/ee1ac0064b970f8453c50dfb2ba6ea1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u5ZM1L1plIcqmFR9PQpe5Q.png"/></div></div></figure><ul class=""><li id="3f82" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">单词长度为26，实际值中有一个单词重叠“activeacute ”,找不到任何图像问题</li><li id="6dd9" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">预测词给出了来自实际的部分含义，这是一个不错的预测。</li><li id="886c" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">我们得到的值很低，因为Bleu分数没有考虑到意义</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es oc"><img src="../Images/8c46d23397571071f3d5bc8f3562191c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qDSLz6b4lofd5u4oPFnMJg.png"/></div></div></figure><ul class=""><li id="db89" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">字数为24，实际字数没有错误。仍然无法找到任何图像模式问题</li><li id="2440" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">预测词差没有给出任何类似的意思</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es od"><img src="../Images/9fe91bc629d4c10d11eee94ddc1c9e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S9D7Uj3jO1t68CO0bdWwPw.png"/></div></div></figure><ul class=""><li id="3c4d" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">字数是11，没有实际的话错误，图像是不完美的捕捉，当我们与他人比较。</li><li id="7692" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">预测给出了相同的含义，问题与Bleu评分和图像。</li></ul><p id="4419" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">正如我们可以看到的，值大于0的bleu分数给出了被认为是良好预测的实际值的部分含义，让我们取值为0的bleu分数。</strong></p><p id="773d" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">另一个发现是，当我们拥有的单词超过20个时，给出0值。这表明我们的模型对于较长的句子表现不好。让我们考虑小于20的单词。</strong></p><p id="b461" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">最终数据，有62个数据点。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es oe"><img src="../Images/cfc40190f70ea91ae0910403701f11fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jr4PkhKE2X5irgFF4QgZxA.png"/></div></div></figure><p id="1602" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">因为我们已经区分了最佳和最差情况的数据点，所以让我们直观地观察并寻找模式</p><p id="a33f" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">下面是最好的结果数据点。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es of"><img src="../Images/98dcb47196a03b6a96aa58c57d729012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GVnsnPPZHKLEKYqJxM7sSw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">好的Bleu分数数据随机25</figcaption></figure><p id="06de" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">拍摄最佳效果图像的点数</strong></p><ul class=""><li id="677c" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">图像的正确对齐</li><li id="673d" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">更明亮的胸骨视图</li><li id="d298" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">没有任何额外的暗线</li><li id="0111" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">即使在暗淡的图像中，我们也能清楚地看到胸骨</li></ul><p id="44c5" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">下面是糟糕的结果数据点</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es of"><img src="../Images/2624ad23a7417f8dbd318f59ca495ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JLSjwEGXcRM4CciwMewsKw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">0 Bleu得分数据点</figcaption></figure><p id="e5a5" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><strong class="kq hj">成绩差的分数</strong></p><ul class=""><li id="de94" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">图像在某些情况下有阴影(行，列)(3，2)，(3，4)，(4，2)，(4，3)，(4，4)</li><li id="2c87" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">在某些情况下(1，2)，(3，4)，(5，3)，图像太亮</li><li id="f8a7" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">让我们在一个数据点中查看两个图像，以检查是否至少有一个图像存在上述问题。</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es og"><img src="../Images/aad8ecba94a2c434759ac436b6f0077c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrDsvIueFLcH1Cp2BgVgWg.png"/></div></div></figure><ul class=""><li id="9bbc" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">在该数据点中，我们看到第二幅图像拍摄不当。图片底部有明显的指纹，重大错误。</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es oh"><img src="../Images/139489c295274eb7b5b782ecade44514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DCf2cq39Ao4nZfo_x99Egg.png"/></div></div></figure><ul class=""><li id="c7a8" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">不良x光捕捉的清晰视野也覆盖了双手</li><li id="c583" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">右侧图像在左下边缘有额外的暗条纹</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es oi"><img src="../Images/6f9a44678768eb1f973d45974088ea20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Co2trdtXxyZa9y2HKRtGWw.png"/></div></div></figure><ul class=""><li id="ba0d" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">清楚地看到两幅图像中较差的图像质量。</li></ul><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es oj"><img src="../Images/dc0c8ef1fe11df1c2da7e90f102fbdea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m6qTtDVLeQuV_36zkxuPww.png"/></div></div></figure><ul class=""><li id="97ed" class="ko kp hi kq b kr ly kt lz kv mx kx my kz mz lb na ld le lf bi translated">图像质量差的清晰视图。两幅图像中都有珠宝的x射线这在任何x射线中都找不到，甚至在x射线分类任务数据集中也找不到。</li></ul><h2 id="5ca6" class="mf jx hi bd jy mg mh mi kc mj mk ml kg kv mm mn ki kx mo mp kk kz mq mr km ms bi translated">15.1结论</h2><ul class=""><li id="9caa" class="ko kp hi kq b kr ks kt ku kv kw kx ky kz la lb na ld le lf bi translated">从上面的分析中我们发现图像的质量起着主要的作用。大多数错误的数据点都是图像质量差，胸片质量差，这是主要的问题。</li><li id="bb26" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">我们还看到了一些指纹，病人的首饰在图像中清晰可见。</li><li id="c438" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">该模型在清晰可见的胸部骨骼上运行良好。我们已经看到了这一点，并比较了最佳和最差情况下的图像。</li><li id="ccf0" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">在模型失败的情况下，有些图像会更亮。我们也看到了没有更亮图像的最佳结果图像。更亮意味着更高的白色像素。</li><li id="a133" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">另一个发现是，我们的模型在超过20个单词的情况下表现不佳。我们可以通过改变架构来改善这一点。比这更好，但我们的模型并不表明它的模型不好。399个错误中有62个，几乎是数据的15%。并不表明这是一个糟糕的预测。</li><li id="3132" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">有些情况下，我们在正确的句子中有不正确的单词。</li><li id="55bb" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">我们可以在今后的工作中忽略这些误差，以获得更好的性能。这些都是误差分析中可减少的误差。</li></ul><p id="f9b3" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">本博客源代码<a class="ae jv" href="https://github.com/Anand2805/Automatic-Impression-Generation-From-Medical-Imaging-Report/tree/master" rel="noopener ugc nofollow" target="_blank"> <strong class="kq hj"> <em class="ne"> GitHub </em> </strong> </a></p><h1 id="3158" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">16未来的工作</h1><ul class=""><li id="05ce" class="ko kp hi kq b kr ks kt ku kv kw kx ky kz la lb na ld le lf bi translated">我们还可以用最先进的BERT变压器代替att-BiLSTM来修改整个架构。这可以通过在时间步长中将图像特征和文本输入作为单个向量发送来预测下一句话来实现。这是一种使用变压器的方法。以下是一些其他参考资料</li></ul><blockquote class="ok"><p id="9a09" class="ol om hi bd on oo op oq or os ot lb dx translated">1.<a class="ae jv" href="https://arxiv.org/pdf/1909.11059v3.pdf" rel="noopener ugc nofollow" target="_blank">图像字幕和VQA的统一视觉语言预培训</a></p><p id="716a" class="ol om hi bd on oo ou ov ow ox oy lb dx translated">2.<a class="ae jv" href="https://papers.nips.cc/paper/9293-image-captioning-transforming-objects-into-words.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/9293-image-captioning-transforming-objects-into-words . pdf</a></p><p id="912b" class="ol om hi bd on oo ou ov ow ox oy lb dx translated">3.<a class="ae jv" href="https://arxiv.org/pdf/2004.08070v2.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2004.08070v2.pdf</a>—使用Transformer的实体感知新闻图像字幕</p><p id="c8b2" class="ol om hi bd on oo ou ov ow ox oy lb dx translated">4.<a class="ae jv" href="http://papers.nips.cc/paper/8297-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks.pdf" rel="noopener ugc nofollow" target="_blank">http://papers . nips . cc/paper/8297-vil Bert-pre training-Task-Agnostic-Vision Language-Representations-for-Vision-and-Language-Tasks . pdf</a>-pre training-Task-Agnostic Vision Language Representations for-Vision-and-Language-Tasks</p></blockquote><ul class=""><li id="1834" class="ko kp hi kq b kr oz kt pa kv pb kx pc kz pd lb na ld le lf bi translated">我们可以进一步增加编码器CNN层到深层进行改进。</li><li id="4dbe" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">图像可以在不同的基于ImageNet的keras模型上进行训练，就像我已经实现了训练阶段来对X射线进行分类，并使用该权重来提取特征。</li><li id="1fff" class="ko kp hi kq b kr lg kt lh kv li kx lj kz lk lb na ld le lf bi translated">从误差分析中，我们发现有一些数据点质量差，图像捕捉不好导致我们性能差。我们可以在今后的工作中消除这些问题。</li></ul><h1 id="2540" class="jw jx hi bd jy jz ka kb kc kd ke kf kg io kh ip ki ir kj is kk iu kl iv km kn bi translated">17篇参考文献</h1><p id="12c6" class="pw-post-body-paragraph ll lm hi kq b kr ks ij ln kt ku im lo kv lp lq lr kx ls lt lu kz lv lw lx lb hb bi translated">1.<a class="ae jv" href="https://machinelearningmastery.com/prepare-text-data-deep-learning-keras/" rel="noopener ugc nofollow" target="_blank">https://machine Learning mastery . com/Prepare-Text-Data-Deep-Learning-keras/</a>—深度学习如何准备文本数据</p><p id="35fc" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">2.<a class="ae jv" href="https://towardsdatascience.com/what-is-teacher-forcing-3da6217fed1c" rel="noopener" target="_blank">https://towards data science . com/what-is-Teacher-forcing-3da 6217 fed1c</a>—老师强制进一步阅读</p><p id="19b0" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">3.<a class="ae jv" href="https://www.aclweb.org/anthology/P16-2034.pdf" rel="noopener ugc nofollow" target="_blank">用于关系分类的基于注意力的双向长短期记忆网络</a> — BiLSTM架构</p><p id="5b7b" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">4.<a class="ae jv" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的一切</a></p><p id="7779" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">5.<a class="ae jv" href="https://arxiv.org/pdf/1805.09019.pdf" rel="noopener ugc nofollow" target="_blank"> CNN+CNN:用于图像字幕的卷积解码器</a></p><p id="ffa4" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">6.<a class="ae jv" href="https://arxiv.org/pdf/1605.07912.pdf" rel="noopener ugc nofollow" target="_blank">查看字幕生成网络</a></p><p id="c095" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">7.<a class="ae jv" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">附加注意</a> —巴赫达瑙式的注意</p><p id="33cf" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">8.<a class="ae jv" href="https://yashk2810.github.io/Image-Captioning-using-InceptionV3-and-Beam-Search/" rel="noopener ugc nofollow" target="_blank">https://yashk 2810 . github . io/Image-Captioning-using-InceptionV3-and-Beam-Search/</a>—波束搜索教程</p><p id="1b5e" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">9.<a class="ae jv" href="https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213" rel="noopener" target="_blank">使用Bleu评估自然语言处理中的文本输出</a> — Bleu教程</p><p id="fd5d" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">10.<a class="ae jv" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">应用人工智能课程</a></p></div><div class="ab cl pe pf gp pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="hb hc hd he hf"><p id="ec09" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">感谢您的阅读！</p><p id="6e14" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated">如果您有任何意见，请告诉我！！！</p><p id="8fb2" class="pw-post-body-paragraph ll lm hi kq b kr ly ij ln kt lz im lo kv ma lq lr kx mb lt lu kz mc lw lx lb hb bi translated"><em class="ne">你可以在</em><strong class="kq hj"><em class="ne"/></strong><a class="ae jv" href="https://www.linkedin.com/in/anand-pandiyan-409818180/" rel="noopener ugc nofollow" target="_blank"><strong class="kq hj"><em class="ne">LinkedIn</em></strong></a><strong class="kq hj"><em class="ne"/></strong><em class="ne">和</em><a class="ae jv" href="https://github.com/Anand2805" rel="noopener ugc nofollow" target="_blank"><strong class="kq hj"><em class="ne">GitHub</em></strong></a><em class="ne">上找到我。</em></p></div></div>    
</body>
</html>