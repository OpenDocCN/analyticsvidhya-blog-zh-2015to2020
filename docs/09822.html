<html>
<head>
<title>Schema Definition and Ranking using DataFrame and SQL in PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark中使用DataFrame和SQL的模式定义和排序</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/schema-definition-and-ranking-using-dataframe-and-sql-in-pyspark-6492c6c06668?source=collection_archive---------20-----------------------#2020-09-21">https://medium.com/analytics-vidhya/schema-definition-and-ranking-using-dataframe-and-sql-in-pyspark-6492c6c06668?source=collection_archive---------20-----------------------#2020-09-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b1a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就分析用例而言，排名是Spark中最重要的概念之一。这篇博客将会讨论在Spark中实现等级、密集等级和行数。除此之外，还将介绍如何在数据之上创建一个模式。</p><blockquote class="jd je jf"><p id="5fc9" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">rank和denseRank的区别在于，当出现平局时，denseRank不会在排名序列中留下空白。也就是说，如果你用denseRank对一场比赛进行排名，有三个人并列第二，你会说这三个人都是第二名，下一个人是第三名。</p></blockquote><p id="7ddd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">问题陈述:</strong>根据‘TX’状态客户总数排名前25个城市。</p><p id="139e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">先决条件:</strong> Hortonworks或Cloudera VM</p><p id="0b65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">执行步骤:</strong></p><ol class=""><li id="6a08" class="jk jl hi ih b ii ij im in iq jm iu jn iy jo jc jp jq jr js bi translated">将数据从MySQL导入HDFS</li><li id="827d" class="jk jl hi ih b ii jt im ju iq jv iu jw iy jx jc jp jq jr js bi translated">火花执行</li></ol><p id="38d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">从MySQL导入数据到HDFS </strong></p><p id="8ca9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该解决方案要求从MySQL导入CSV格式的“客户”表。</p><blockquote class="jd je jf"><p id="c37d" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><em class="hi">查看我的帖子“</em>关于如何将数据从MySQL导入HDFS的Sqoop、HDFS、Hive和Spark <em class="hi">的用例，以及如何将数据加载到MySQL的更多信息。</em></p></blockquote><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="84bb" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#Create folder in HDFS</strong><br/>hdfs dfs -mkdir /user/hdfs/spark_usecase</span><span id="2d94" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Customers Table: CSV Format <br/></strong>sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username retail_dba --password hadoop --table customers --target-dir /user/cloudera/spark_usecase/customers;</span></pre><p id="6958" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">使用数据帧的火花执行</strong></p><p id="2c6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所需数据现已导入HDFS。使用Spark，现在通过在读取时定义模式来加载数据。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="45c5" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#Spark shell<br/></strong>pyspark</span><span id="e1fb" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Reading CSV data and defining schema<br/></strong>customers = spark.read.csv('/user/cloudera/spark_usecase/customers', schema='id int, fname string, lname string, email string, password string, street string, city string, state string, zipcode string')</span><span id="c645" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Importing required function(s)<br/></strong>from pyspark.sql.functions import count, col</span><span id="0dae" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Filtering TX customers, finding # of customers per city<br/></strong>cus_count = customers.where(customers.state == 'TX').groupBy('state', 'city').agg(count('id').alias('cus_count')).orderBy('cus_count', ascending=False).limit(25)</span><span id="9025" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Importing required function(s)<br/></strong>from pyspark.sql import Window</span><span id="0e8e" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Creating window based on state and order by # of customers</strong><br/>window = <br/>Window.partitionBy('state').orderBy(col('cus_count').desc())</span><span id="7fb2" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Importing required function(s)<br/></strong>from pyspark.sql.functions import rank, dense_rank, row_number</span><span id="3f6a" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Finding rank, dense rank and row number on data<br/></strong>result = cus_count.withColumn('rank', rank().over(window)).withColumn('dense_rank', dense_rank().over(window)).withColumn('row_num', row_number().over(window))</span><span id="1364" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Printing result<br/></strong>result.show(25)</span><span id="dc5d" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Writing result in JSON into HDFS<br/></strong>result.write.json('/user/cloudera/spark_usecase/output')</span></pre><p id="6cfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">使用SQL执行Spark】</strong></p><p id="0ec9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过以下方法使用Spark SQL可以实现相同的用例。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="90a5" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#Spark shell<br/></strong>pyspark</span><span id="67e6" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Reading CSV data and defining schema<br/></strong>customers = spark.read.csv('/user/cloudera/spark_usecase/customers', schema='id int, fname string, lname string, email string, password string, street string, city string, state string, zipcode string')</span><span id="05d4" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Create a global temporary view with this DataFrame<br/></strong>customers.createTempView('customers')</span><span id="0e4f" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Create a global temporary view with this DataFrame</strong><br/>result = spark.sql('select state, city, cus_count, rank() over (partition by state order by cus_count desc) as rank, dense_rank() over (partition by state order by cus_count desc) as dense_rank, row_number() over (partition by state order by cus_count desc) as row_num from (select state, city, count(*) as cus_count from customers where state = "TX" group by state, city order by cus_count desc limit 25)')</span><span id="8dba" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Printing result<br/></strong>result.show(25)</span><span id="aab7" class="kh ki hi kd b fi kn kk l kl km"><strong class="kd hj">#Writing result in JSON into HDFS<br/></strong>result.write.json('/user/cloudera/spark_usecase/output')</span></pre><p id="dc3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结果</strong></p><p id="b54e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“休斯顿”排名第一，拥有91名客户，“布朗斯维尔”和“普莱诺”排名第十，拥有相同数量的客户。对于城市“圣贝尼托”,可以观察到等级和密集等级之间的差异，在该城市中，dense rank在进行等级划分时没有留下间隙。</p><p id="6d09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:下面的结果是基于上面提到的MySQL中加载的数据。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="2217" class="kh ki hi kd b fi kj kk l kl km">+-----+--------------------+---------+----+----------+-------+<br/>|state|                city|cus_count|rank|dense_rank|row_num|<br/>+-----+--------------------+---------+----+----------+-------+<br/>|   TX|             Houston|       91|   1|         1|      1|<br/>|   TX|              Dallas|       75|   2|         2|      2|<br/>|   TX|         San Antonio|       53|   3|         3|      3|<br/>|   TX|             El Paso|       42|   4|         4|      4|<br/>|   TX|          Fort Worth|       27|   5|         5|      5|<br/>|   TX|              Austin|       25|   6|         6|      6|<br/>|   TX|              Laredo|       23|   7|         7|      7|<br/>|   TX|            Amarillo|       19|   8|         8|      8|<br/>|   TX|          Sugar Land|       17|   9|         9|      9|<br/>|   TX|         Brownsville|       16|  10|        10|     10|<br/>|   TX|               Plano|       16|  10|        10|     11|<br/>|   TX|              Irving|       15|  12|        11|     12|<br/>|   TX|          San Benito|       13|  13|        12|     13|<br/>|   TX|     College Station|       13|  13|        12|     14|<br/>|   TX|          Carrollton|       12|  15|        13|     15|<br/>|   TX|             Weslaco|       12|  15|        13|     16|<br/>|   TX|North Richland Hills|       12|  15|        13|     17|<br/>|   TX|             Mission|       11|  18|        14|     18|<br/>|   TX|             Del Rio|       11|  18|        14|     19|<br/>|   TX|            Richmond|       10|  20|        15|     20|<br/>|   TX|            Mesquite|       10|  20|        15|     21|<br/>|   TX|           Harlingen|       10|  20|        15|     22|<br/>|   TX|                Katy|       10|  20|        15|     23|<br/>|   TX|               Pharr|        9|  24|        16|     24|<br/>|   TX|          San Marcos|        9|  24|        16|     25|<br/>+-----+--------------------+---------+----+----------+-------+</span></pre><p id="46f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考</strong></p><p id="8b0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看我的帖子“我是如何完成CCA Spark和Hadoop Developer (CCA175)认证的”来了解我的学习细节。此外，Spark关于方法定义的文档。</p></div></div>    
</body>
</html>