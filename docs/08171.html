<html>
<head>
<title>German Traffic Sign Recognition Benchmark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">德国交通标志识别基准</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/german-traffic-sign-recognition-benchmark-5477ca13daa0?source=collection_archive---------2-----------------------#2020-07-20">https://medium.com/analytics-vidhya/german-traffic-sign-recognition-benchmark-5477ca13daa0?source=collection_archive---------2-----------------------#2020-07-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9769" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用深度学习和计算机视觉准确检测和分类交通标志图像。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/510d52a43d9fd78d19db4069666757e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kOFlI363NfKpbv_nF4532A.jpeg"/></div></div></figure></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="d285" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">目录:</h1><ol class=""><li id="8d99" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc lb lc ld le bi translated">业务/现实世界的限制。</li><li id="40bc" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">探索性数据分析。</li><li id="466c" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">设置数据输入管道。</li><li id="c8da" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">模型定义和训练。</li><li id="0d99" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">预测测试数据的输出。</li><li id="c524" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">训练更快-RCNN。</li><li id="3e11" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">将fast-RCNN与我们的定制模型相结合。</li><li id="93d2" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">结论和未来工作。</li><li id="9dd8" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">有用的参考。</li></ol></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="6dd0" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">1-业务/现实世界的限制:</h1><h2 id="2427" class="lk jx hi bd jy ll lm ln kc lo lp lq kg iq lr ls kk iu lt lu ko iy lv lw ks lx bi translated">1.1-问题陈述:</h2><ul class=""><li id="0bd4" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated">交通标志是道路基础设施不可或缺的一部分。如果没有这些有用的标志，我们很可能会面临更多的事故，因为司机不会得到关于他们可以安全行驶多快的重要反馈，或者关于道路工程、急转弯或前方学校路口的信息。</li><li id="45a3" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">自然，自动驾驶汽车也必须遵守道路法规，因此能够识别和理解交通标志。</li><li id="489c" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">交通标志检测的目标是在大规模搜索图像中的候选标志后，识别应该发现交通标志的感兴趣区域(ROI)并验证该标志。</li><li id="b66c" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">它在自动驾驶汽车中有非常重要的作用，这是汽车工业的未来。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lz ma l"/></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="ak">你的车所见……(特斯拉Autopilot全自动驾驶)</strong></figcaption></figure><ul class=""><li id="1623" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">德国交通标志基准是在2011年国际神经网络联合会议(IJCNN)上举行的多类别、单图像分类挑战赛。</li><li id="2c46" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">数据集可以从这个<a class="ae mi" href="https://www.kaggle.com/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign" rel="noopener ugc nofollow" target="_blank">链接</a>下载。</li><li id="a0f7" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">与此案例研究相关的所有文件都可以在此<a class="ae mi" href="https://github.com/HardikVagadia1997/German-Traffic-Sign-Recognition-Benchmark.git" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>找到。</li></ul><h2 id="521e" class="lk jx hi bd jy ll lm ln kc lo lp lq kg iq lr ls kk iu lt lu ko iy lv lw ks lx bi translated">1.2-目标和限制:</h2><ul class=""><li id="6c73" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated">每个交通标志都应该被正确识别。因此，<a class="ae mi" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy" rel="noopener ugc nofollow" target="_blank">多级对数损耗</a>需要改进。</li><li id="08b5" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">加权的<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html" rel="noopener ugc nofollow" target="_blank"> F1分数</a>将用于判断分类性能，而<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html" rel="noopener ugc nofollow" target="_blank">均方差(MSE) </a>用于判断包围盒检测性能。</li><li id="b6c9" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">此外，时间是一个主要的制约因素，因为即使是一秒钟的延迟也可能是生死攸关的问题。</li></ul></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="8c1c" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">2-探索性数据分析:</h1><p id="9b91" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq mj is it iu mk iw ix iy ml ja jb jc hb bi translated"><strong class="ih hj"> 2.1-文件夹结构:</strong></p><ul class=""><li id="cf15" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">数据集包含训练和测试文件夹。火车文件夹有43个不同的文件夹，从0到42命名。每个文件夹包含各自类别的图像。</li><li id="da50" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">有39，209个训练图像和12，630个测试图像。所有图像都是3通道RGB图像。</li><li id="4616" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">还有Train.csv和Test.csv文件，其中包含有关训练和测试图像的信息。</li><li id="e9d8" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">所有的图像分布在43个不同的类中。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/459bca13fa4e89fba03db99d6aec0f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*wIz1jnD-yQ0BaEoOLOVF-A.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy"> 43类交通标志</strong></figcaption></figure><ul class=""><li id="e068" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">Train.csv文件包含每个训练图像的以下信息:</li></ul><ol class=""><li id="89ba" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc lb lc ld le bi translated"><strong class="ih hj">宽度:</strong>图像的宽度，以像素为单位。</li><li id="bc46" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><strong class="ih hj">高度:</strong>图像的高度，以像素为单位。</li><li id="9d60" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><strong class="ih hj">投资回报率。X1 : </strong>边框左上角的X坐标。</li><li id="95bb" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><strong class="ih hj">投资回报率。Y1 : </strong>边界框的左上角Y坐标。</li><li id="16db" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><strong class="ih hj">投资回报率。X2 : </strong>包围盒右下角的X坐标。</li><li id="a430" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><strong class="ih hj">投资回报率。Y2 : </strong>边框的右下Y坐标。</li><li id="7ee8" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><strong class="ih hj"> ClassId </strong>:图像的类标签。它是一个介于0和43之间的整数。</li><li id="ad01" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated"><strong class="ih hj">路径:</strong>图像在Train文件夹中出现的路径。其格式为:“/Train/ClassId/image_name.png”。</li></ol><p id="afbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.2-检查训练集中的类不平衡:</strong></p><ul class=""><li id="4a03" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">下图显示了训练图像的类别分布。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mn"><img src="../Images/078ee607ab518ea4baa0bcb8cc09ca7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q1i7Q_TpRZFsbKPAORz9mQ.png"/></div></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">训练集中的类分布</strong></figcaption></figure><h1 id="68b3" class="jw jx hi bd jy jz mo kb kc kd mp kf kg kh mq kj kk kl mr kn ko kp ms kr ks kt bi translated">观察结果:</h1><ul class=""><li id="a9fa" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated">目标类显然不是均匀分布的。</li><li id="a12c" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">这是很符合逻辑的，因为像“保持速度低于30 K-mph”或“前方颠簸”这样的标志比“前方道路施工”这样的标志出现得更频繁。</li><li id="2b54" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">各个阶层的形象分布不均。一些班级有大约2500张图片，而其他班级只有250张图片。</li><li id="58ff" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">较少数量的图像会破坏训练过程。我们可以使用数据扩充来克服某些类别中图像数量不足的问题。要了解更多关于数据扩充的信息，请参考此<a class="ae mi" href="https://www.geeksforgeeks.org/python-data-augmentation/" rel="noopener ugc nofollow" target="_blank">链接</a>。</li></ul><p id="a936" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.3-检查测试集中的类别不平衡:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mt"><img src="../Images/9cf5b506a405b424e3208fe54431aed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a2EcyaDyPkGWJLl4vakQug.png"/></div></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">测试集中的类别分布</strong></figcaption></figure><h1 id="bff3" class="jw jx hi bd jy jz mo kb kc kd mp kf kg kh mq kj kk kl mr kn ko kp ms kr ks kt bi translated">观察结果:</h1><ul class=""><li id="55e0" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated">对测试数据的观察与对训练数据的观察非常相似。</li><li id="7c7d" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">这里也有一些课程比其他课程更频繁。</li></ul><p id="072f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.4-分析图像尺寸:</strong></p><ul class=""><li id="ee0d" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">下面的直方图显示了训练集和测试集图像的高度和宽度分布:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mu"><img src="../Images/2e77ad760e27a3bc14ebc02dbc4fb412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_nx-AvNOZzMaiiWyZO2-oA.png"/></div></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">训练图像的高度和宽度分布显示，大多数图像的尺寸在30到50像素之间。</strong></figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mv"><img src="../Images/7f2e39a97d6156e57fe5f506f232d7bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*0OepIhVJtDH47bgHacc5PA.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">饼图显示大多数图像的尺寸小于64。</strong></figcaption></figure><h1 id="fc43" class="jw jx hi bd jy jz mo kb kc kd mp kf kg kh mq kj kk kl mr kn ko kp ms kr ks kt bi translated">观察结果:</h1><ul class=""><li id="2d54" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated">从上面的图中可以明显看出，我们的训练集和测试集图像遵循相似的高度和宽度分布。</li><li id="e573" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">由于所有的图像都有不同的尺寸，我们需要固定每个图像的高度和宽度常数。</li><li id="8bd9" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">我们需要以数据丢失最少的方式执行此操作。</li><li id="35b5" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">对于较小的图像，我们需要做适当的填充。</li><li id="ccc5" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">对于这个案例研究，我们将调整我们的图像的高度和宽度为(32，32 ),因为大多数。我们将在这篇文章的后面讨论如何调整图片的大小。</li><li id="ab87" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">还要注意的是，截断图像比添加更多的噪声要好。因此，保持尽可能低的尺寸，使得大多数图像在其附近具有它们的原始尺寸，但是也不要太低。</li></ul><p id="e641" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.5-现在我们来看一些图片:</strong></p><ul class=""><li id="6dee" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">我们数据集中的图像有不同的大小，如下图所示:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mw"><img src="../Images/092953a6236ec8f2341678a5f0ea5d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F1y3PibfWXaih8vsnUAykA.png"/></div></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">图像有不同的宽度和高度。有些相当大，有些像邮票一样小。</strong></figcaption></figure><ul class=""><li id="000f" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">有些是亮的，有些是暗的。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mx"><img src="../Images/d8dd068574ee0f178b0d80975b386ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BZfi6s-hVLrMWFzf8sGlfw.png"/></div></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">有些图像是在合适的日光下拍摄的，而有些图像似乎是在黑暗中拍摄的，难以辨认。</strong></figcaption></figure><ul class=""><li id="3e81" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">有些图像很清晰，而有些图像看起来很模糊。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es my"><img src="../Images/abc9c0f079897360ba83f66b368ce34e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_is9UHRA9xSHlU_fI5cAcg.png"/></div></div></figure></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="e0fa" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">3-设置数据输入管道:</h1><p id="8e70" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq mj is it iu mk iw ix iy ml ja jb jc hb bi translated"><strong class="ih hj"> 3.1-导入库并设置随机种子:</strong></p><ul class=""><li id="3beb" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">让我们从导入必要的库并为python、numpy和tensorflow设置随机种子开始。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><p id="0aeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3.2-从kaggle下载数据集:</strong></p><ul class=""><li id="9e38" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">我们将数据集从kaggle下载到google-colab的本地内存中。你也可以把它下载到你的google drive上，但是在训练期间获取这些图片会很慢。因此，我将它们保存在colab的本地内存中。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="b217" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">在执行上述函数时，系统会提示您上传kaggle api令牌，之后会下载并解压缩数据集。</li></ul><p id="5f43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3.3-将训练数据分为训练集和验证集:</strong></p><ul class=""><li id="7604" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">在训练我们的模型时，我们将使用25%的图像进行交叉验证。下面是创建验证集的代码。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="7991" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在，我们需要设置一些常数，如每个图像的高度和宽度。我们需要有相同的高度和宽度的每幅图像，然后再将它们输入神经网络。</li><li id="03f3" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">正如我们在探索性数据分析部分所讨论的，我们将宽度和高度设置为(32，32)。</li><li id="5f3a" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">此外，我们在训练集和验证集中获取每个图像的路径和名称。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="726a" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在，我们从路径中提取每个图像的标签。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><p id="be5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3.4-更新图像边界框坐标:</strong></p><ul class=""><li id="2245" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">因为我们要将每张图片的形状调整为(32，32)，所以我们也需要更新坐标。</li><li id="585b" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">请注意，这些坐标是像素数，而不是图像比例。因此，更新它们非常容易。我们需要做简单的加法/减法。下面的代码将使它变得清晰:</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="8707" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在，我们将把数据帧分成训练和验证数据帧:</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><p id="b48a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3.5-创建数据生成器:</strong></p><ul class=""><li id="f1ca" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">下面是创建和返回数据生成器的函数。循序渐进地正确理解是非常重要的。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ol class=""><li id="7049" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc lb lc ld le bi translated">首先，我们读取图像，对其进行解码，并将数据类型转换为“float32”。</li><li id="69a9" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc lb lc ld le bi translated">然后我们调整亮度和对比度。我们将只增强大约5%最暗图像的亮度。以下是增强亮度前后的图像示例:</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es na"><img src="../Images/c8d6da6ebfaf2dd2578473dfd04b5238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*fLCWEFsIC3hPH16eYbxPjg.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">增强亮度前后的图像。</strong></figcaption></figure><p id="a3c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.在下一步中，我们将调整图像的大小为(32，32)并将其归一化。</p><p id="3010" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.注意，我们的数据集返回图像、它的标签和边界框坐标。</p><p id="a53b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.我们使用“<em class="nb">tensor flow . data”</em>API来生成数据生成器。它是一个Tensorflow api，用于生成复杂的输入管道。要了解更多，请参考这个<a class="ae mi" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank">链接</a>。</p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="90d8" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">4-模型定义和培训:</h1><p id="19e3" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq mj is it iu mk iw ix iy ml ja jb jc hb bi translated"><strong class="ih hj"> 4.1-定义模型:</strong></p><ul class=""><li id="931b" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">我们要定义的模型既能分类，又能包围盒回归。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="4d20" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">上面代码中的“<strong class="ih hj">shape n-layer”</strong>锐化了图像中对象的边缘。下图显示了图像锐化前后的一些示例。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nc"><img src="../Images/ebc517e1207da260fc0149af3bdbe2f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*_VnFj5_j1DLr4C8Du0D2kw.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">左侧图像为原始图像，右侧图像为锐化图像。我们可以清楚地看到，它加强了图像中物体的边缘。</strong></figcaption></figure><ul class=""><li id="0c24" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">模型的其余部分是卷积层、最大池层和全连接层的组合。</li><li id="bde1" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated"><strong class="ih hj">“分类”</strong>层返回适当的类标签，而<strong class="ih hj">“回归”</strong>层返回预测边界框的左上角和右下角的x和y坐标。</li><li id="4789" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">下图显示了模型图:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nd"><img src="../Images/ac9721f691dddc9ff881a4fd6fffd511.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*5iMjIXeWPne2Lotjwzo8zA.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">模型图</strong></figcaption></figure><p id="d9b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4.2-培训:</strong></p><ul class=""><li id="c935" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在我们已经有了模型，让我们定义损失函数。请注意，将有2个不同的损失函数。对于分类，我们将使用<a class="ae mi" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">【SparseCategoricalCrossentropy】</strong></a><strong class="ih hj"/>，对于包围盒回归，我们将使用<a class="ae mi" href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/34019" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">【R平方】</strong> </a>。</li><li id="33e1" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">如上所述，我们将根据加权F1分数来判断我们的模型的分类性能。我们将定义一个自定义回调，在每个时期结束时打印F1分数。下面是做同样事情的代码，也是用于训练模型的代码。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="b54f" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">下图显示了张量板图。要了解更多关于Tensorboard的信息，请参考此<a class="ae mi" href="https://www.tensorflow.org/tensorboard" rel="noopener ugc nofollow" target="_blank">链接</a>。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ne"><img src="../Images/4e55710ef999439b36e1dce9a5e0521c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*nReanYa1ZxAEqQtSwXc3vw.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">上述图表显示，由于训练和验证损失之间没有重大差异，因此不存在过度拟合。</strong></figcaption></figure></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="3ca4" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">5-预测测试数据的输出:</h1><ul class=""><li id="c115" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated">现在让我们在看不见的测试数据图像上评估我们的模型。我们的数据集中有一个包含12，630张测试图像的测试文件夹。</li><li id="64e9" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated"><strong class="ih hj"> "Test.csv" </strong>文件包含测试图像的路径以及它们的实际标签和边界框坐标。请注意，我们将不得不在这里更新坐标，就像我们对训练和验证图像所做的那样，因为我们要将每个图像的大小调整为(32，32)。我不打算重复代码，因为我已经附上了上述代码。</li><li id="74f2" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">下面是评估测试集图像的函数。它返回每个测试集图像的预测标签和边界框。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="cf1e" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在让我们看看我们模型的测试集性能:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nf"><img src="../Images/b17740e2619910a936d659a7b441c295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*71zGNFy5Su5HlgALKhwMnQ.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">测试我们模型的性能</strong></figcaption></figure><ul class=""><li id="e50b" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">下图描绘了预测的和实际的边界框:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ng"><img src="../Images/9e5eb8dcd4d1e4a1891dbd3f25914c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*l4EgawWVnEL-2obTBouVZQ.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">实际的包围盒是红色的，而绿色是预测的。我们可以清楚地看到，它们几乎相互重叠。</strong></figcaption></figure><ul class=""><li id="0ee8" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在，让我们看看一些标签被我们的模型错误分类的图像:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nh"><img src="../Images/dcb8048716672102de2c4c759de9fa7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TyFoIyRGhMNO8XWMHuLESg.png"/></div></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">误分类图像</strong></figcaption></figure><h1 id="6ff9" class="jw jx hi bd jy jz mo kb kc kd mp kf kg kh mq kj kk kl mr kn ko kp ms kr ks kt bi translated">观察:</h1><ul class=""><li id="0a0e" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated">少于2.5%的图像被错误分类。</li><li id="5294" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">大多数误分类的图像在尺寸上非常小，并且非常不清晰。</li><li id="c72f" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">有些甚至不在相框里。</li><li id="c5c4" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">其中许多甚至连人类都无法识别。</li></ul></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="be69" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">6-训练速度更快-RCNN:</h1><ul class=""><li id="984c" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated">让我们尝试一种用于对象检测目的的高级模型。对于我们之前的模型，我们得到了1.27 MSE，这是相当好的，预测的边界框也几乎是完美的。但是有一些先进的模型在目标检测问题上表现出色。让我们用一个这样的模型来解决包围盒问题。</li><li id="6cb1" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">fast-RCNN是RCNN家族中使用最广泛的最新版本。在这篇博客中，我不会详细介绍更快的RCNN架构。要了解更多信息，请点击此<a class="ae mi" href="https://towardsdatascience.com/faster-r-cnn-for-object-detection-a-technical-summary-474c5b857b46" rel="noopener" target="_blank">链接</a>。</li><li id="e6a9" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">现在让我们在数据集上训练更快的RCNN。首先下载数据集，创建验证目录，并像前面一样设置输入管道。在这种情况下，不需要调整图像的大小，因此不需要更新坐标。然后我们需要安装一些依赖项。</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="82e4" class="lk jx hi nj b fi nn no l np nq">!apt-get install protobuf-compiler python-pil python-lxml python-tk<br/>!pip install Cython</span></pre><ul class=""><li id="1b7b" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">首先创建一个名为“桌面”的文件夹。这将是我们的项目目录。将这个<a class="ae mi" href="https://github.com/tensorflow/models" rel="noopener ugc nofollow" target="_blank"> Git-hub存储库</a>克隆到其中。</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="c713" class="lk jx hi nj b fi nn no l np nq">!mkdir "/content/Desktop"<br/>%cd "/content/Desktop"<br/>!git clone <a class="ae mi" href="https://github.com/tensorflow/models.git" rel="noopener ugc nofollow" target="_blank">https://github.com/tensorflow/models.git</a></span></pre><ul class=""><li id="7532" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">然后在object_detection文件夹中创建两个文件夹，如下所示。“images”文件夹将保存所有数据,“training”文件夹将保存我们接下来将看到的培训所需的文件。</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="693e" class="lk jx hi nj b fi nn no l np nq">!mkdir "/content/Desktop/models/research/object_detection/images"<br/>!mkdir "/content/Desktop/models/research/object_detection/training"</span></pre><ul class=""><li id="ae5f" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">将所有数据文件夹移动到“object_detection/images”文件夹中。</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="a73c" class="lk jx hi nj b fi nn no l np nq">!mv "/content/Data/Train" "/content/Desktop/models/research/object_detection/images/"</span><span id="705d" class="lk jx hi nj b fi nr no l np nq">!mv "/content/Data/Validation" "/content/Desktop/models/research/object_detection/images/"</span><span id="66db" class="lk jx hi nj b fi nr no l np nq">!mv "/content/Data/Test" "/content/Desktop/models/research/object_detection/images/"</span></pre><ul class=""><li id="af4d" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在运行以下命令:</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="9f84" class="lk jx hi nj b fi nn no l np nq">!apt-get install protobuf-compiler python-pil python-lxml python-tk<br/>!pip install Cython</span></pre><ul class=""><li id="e262" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在，将工作目录更改为研究文件夹，并运行以下命令集:</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="b9c3" class="lk jx hi nj b fi nn no l np nq">%cd "/content/Desktop/models/research"<br/>!protoc object_detection/protos/*.proto --python_out=.</span><span id="d9eb" class="lk jx hi nj b fi nr no l np nq">#Setting enviornment variable<br/>os.environ['PYTHONPATH'] += ':/content/Desktop/models/research/:/content/Desktop/models/research/slim'</span></pre><ul class=""><li id="495f" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在使用以下参数运行“setup.py”文件:</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="82ad" class="lk jx hi nj b fi nn no l np nq">!python setup.py build<br/>!python setup.py install</span></pre><ul class=""><li id="b852" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在安装“tf_slim”，将目录更改为object_detection/builders并运行model_builder.py文件:</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="85cd" class="lk jx hi nj b fi nn no l np nq">!pip install tf_slim<br/>%cd /content/Desktop/models/research/object_detection/builders/<br/>!python model_builder_test.py</span></pre><ul class=""><li id="dc51" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在创建一个名为“generate_tfrecord.py”的文件，将以下代码复制到该文件中，并将其复制到object_detection文件夹中:</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="a3e5" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">创建一个名为“label_map.pbtxt”的标签映射文件，并将如下所示的标签映射复制到该文件中:</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="e7dd" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">将模型配置文件移动到object_detection/training文件夹中:</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="7ec8" class="lk jx hi nj b fi nn no l np nq">!cp -r /content/Desktop/models/research/object_detection/samples/ configs/faster_rcnn_resnet101.config.config /content/Desktop/models/research/object_detection/training</span></pre><ul class=""><li id="6c99" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">在配置文件中，我们需要编辑一些代码段。现在，简单的复制下面的代码到里面。config文件非常容易理解，如果需要，可以很容易地调整任何参数。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="b3ef" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在，我们必须创建培训、验证和测试记录。将目录更改为object_detection文件夹，并运行以下命令:</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="fa9f" class="lk jx hi nj b fi nn no l np nq">%cd "/content/Desktop/models/research/object_detection"</span><span id="231f" class="lk jx hi nj b fi nr no l np nq">!python generate_tfrecord.py --label='GTSRB' --csv_input=data/train_labels.csv --img_path=images/Train  --output_path=training/train.record</span><span id="b935" class="lk jx hi nj b fi nr no l np nq">!python generate_tfrecord.py --label='GTSRB' --csv_input=data/validation_labels.csv --img_path=images/Validation  --output_path=training/validation.record</span><span id="4114" class="lk jx hi nj b fi nr no l np nq">!python generate_tfrecord.py --label='GTSRB' --csv_input=data/test_labels.csv --img_path=images/Test  --output_path=training/test.record</span></pre><ul class=""><li id="2b33" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">从tensorflow.org下载Faster-RCNN模型并解压zip文件:</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="6d57" class="lk jx hi nj b fi nn no l np nq">!wget http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz</span><span id="52f4" class="lk jx hi nj b fi nr no l np nq">!tar -xvf faster_rcnn_resnet101_coco_2018_01_28.tar.gz</span></pre><ul class=""><li id="f9e9" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">将“train.py”和“eval.py”文件移到object_detection文件夹中:</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="98c1" class="lk jx hi nj b fi nn no l np nq">!mv "/content/Desktop/models/research/object_detection/legacy/train.py" "/content/Desktop/models/research/object_detection/"</span><span id="fe7a" class="lk jx hi nj b fi nr no l np nq">!mv "/content/Desktop/models/research/object_detection/legacy/eval.py" "/content/Desktop/models/research/object_detection/"</span></pre><ul class=""><li id="1171" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">现在，是时候训练我们的模型了。我要训练它走3000步。</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="40cc" class="lk jx hi nj b fi nn no l np nq">!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_resnet101.config</span></pre><blockquote class="ns nt nu"><p id="8ffb" class="if ig nb ih b ii ij ik il im in io ip nv ir is it nw iv iw ix nx iz ja jb jc hb bi translated">注意:如果关于Faster-RCNN的训练还有什么不清楚的，那么我已经在这个<a class="ae mi" href="https://github.com/HardikVagadia1997/German-Traffic-Sign-Recognition-Benchmark.git" rel="noopener ugc nofollow" target="_blank"> git-hub库</a>上传了IPython笔记本，上面有全功能的分步代码，你可以用来训练和推断。</p></blockquote></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="2a83" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">7-将fast-RCNN与我们的定制模型相结合:</h1><ul class=""><li id="b7cc" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated">现在，作为最终的解决方案，我们将结合我们的两个模型。我们将使用我们定制的模型进行分类，但丢弃其边界框输出。我们将使用fast-RCNN进行包围盒预测。下图描述了我们的最终解决方案架构:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ny"><img src="../Images/b2424fe87ce23e1e5d6bfb25c727824a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EhintfoBi_cGXBiLKZfiCg.png"/></div></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">最终解决方案图</strong></figcaption></figure><ul class=""><li id="e02c" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">首先运行下面的代码来导入推理图并设置一些路径和变量:</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="daad" class="lk jx hi nj b fi nn no l np nq"># This is needed since the notebook is stored in the #object_detection folder.<br/>sys.path.append("..")</span><span id="8e66" class="lk jx hi nj b fi nr no l np nq">#Importing essential libraries<br/>from object_detection.utils import ops as utils_ops<br/>from object_detection.utils import label_map_util<br/>from object_detection.utils import visualization_utils as vis_util</span><span id="cc45" class="lk jx hi nj b fi nr no l np nq">### Model preparation variable<br/>MODEL_NAME = 'trained_inference_graph'<br/>PATH_TO_FROZEN_GRAPH = '/content/drive/My Drive/CaseStudy2/frozen_inference_graph.pb'</span><span id="7816" class="lk jx hi nj b fi nr no l np nq">PATH_TO_LABELS = 'training/label_map.pbtxt'<br/>NUM_CLASSES = 1 #remember number of objects you are training? cool.</span><span id="4bf6" class="lk jx hi nj b fi nr no l np nq">### Load a (frozen) Tensorflow model into memory.<br/>detection_graph = tf.Graph()<br/>with detection_graph.as_default():<br/>   od_graph_def = tf.GraphDef()<br/>   with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:<br/>      serialized_graph = fid.read()<br/>      od_graph_def.ParseFromString(serialized_graph)<br/>      tf.import_graph_def(od_graph_def, name='')</span><span id="d0f4" class="lk jx hi nj b fi nr no l np nq">###Loading label map<br/>category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)</span><span id="c35d" class="lk jx hi nj b fi nr no l np nq">### Load image into numpy function<br/>def load_image_into_numpy_array(image):<br/>   (im_width, im_height) = image.size<br/>   return np.array(image.getdata()).reshape((im_height, im_width,   3)).astype(np.uint8)</span><span id="ee01" class="lk jx hi nj b fi nr no l np nq">###STATING THE PATH TO IMAGES TO BE TESTED<br/>PATH_TO_TEST_IMAGES_DIR = 'test_images/'</span><span id="6644" class="lk jx hi nj b fi nr no l np nq">#TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.png'.format(i)) for i in range(1, 10) ]</span><span id="2e90" class="lk jx hi nj b fi nr no l np nq">TEST_IMAGE_PATHS = test_df.iloc[:, 0]<br/>IMAGE_SIZE = (256, 256)</span></pre><ul class=""><li id="98ab" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">负载分类模型:</li></ul><pre class="je jf jg jh fd ni nj nk nl aw nm bi"><span id="30c0" class="lk jx hi nj b fi nn no l np nq">#Loading classification model</span><span id="310e" class="lk jx hi nj b fi nr no l np nq">IMG_WIDTH, IMG_HEIGHT = 32, 32<br/>N_CHANNELS = 3<br/>N_CLASSES = 43</span><span id="c4b8" class="lk jx hi nj b fi nr no l np nq">object_detection_model = u.get_model(IMG_WIDTH, IMG_HEIGHT, N_CHANNELS, N_CLASSES)</span><span id="b421" class="lk jx hi nj b fi nr no l np nq">object_detection_model_path = "/content/drive/My Drive/CaseStudy2/BestScoreTillNow.h5"</span><span id="90ba" class="lk jx hi nj b fi nr no l np nq">object_detection_model.load_weights(object_detection_model_path)<br/>object_detection_model.compile()</span></pre><ul class=""><li id="5f42" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">以下函数将图像路径作为输入，并使用自定义对象检测模型返回预测的类别标签，使用fast-RCNN模型返回边界框:</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mz ma l"/></div></figure><ul class=""><li id="8cb3" class="ku kv hi ih b ii ij im in iq mf iu mg iy mh jc ly lc ld le bi translated">与我们以前的模型相比，为包围盒检测获得的测试均方误差要好得多。以下是我们的最终得分:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nz"><img src="../Images/edab9e68478c95c96fd52d95c23db0cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*c6HgxUimFhYvBjWWGiokAw.png"/></div><figcaption class="mb mc et er es md me bd b be z dx translated"><strong class="bd jy">最终F1分数和MSE </strong></figcaption></figure></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="232c" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">8-结论和未来工作:</h1><ul class=""><li id="8855" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated">更快-RCNN在对象检测方面表现更好。然而，这对于分类来说并不是那么好。因此，我们将自定义模型与fast-RCNN相结合。</li><li id="a226" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">为了更好的概括而扩充数据。</li><li id="688c" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">作为未来工作的一部分，尝试不同的模型，如RetinaNet和yolov3。</li><li id="206d" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">对分类错误的图像尝试不同的图像预处理技术。</li><li id="b2d5" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">我们还可以使用像vgg-16这样的模型来获取特征地图，并将其传递给我们的模型。</li><li id="8c95" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">这样的例子不胜枚举…</li></ul></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="2d33" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">9-有用的参考资料:</h1><ul class=""><li id="64eb" class="ku kv hi ih b ii kw im kx iq ky iu kz iy la jc ly lc ld le bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">fast-RCNN研究论文</a>。</li><li id="c08f" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated">RetinaNet <a class="ae mi" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a>和<a class="ae mi" href="https://github.com/fizyr/keras-retinanet" rel="noopener ugc nofollow" target="_blank">实施</a>。</li><li id="df52" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated"><a class="ae mi" href="http://benchmark.ini.rub.de/?section=home&amp;subsection=news" rel="noopener ugc nofollow" target="_blank"> INI基准网站</a>。</li><li id="9afd" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated"><a class="ae mi" href="http://Building a very powerful image classification model with very little data" rel="noopener ugc nofollow" target="_blank">用很少的数据构建非常强大的图像分类模型</a>。</li><li id="2cfb" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated"><a class="ae mi" href="https://cg.cs.tsinghua.edu.cn/traffic-sign/" rel="noopener ugc nofollow" target="_blank">野外交通标志分类</a>。</li><li id="e221" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated"><a class="ae mi" href="https://udai.gitbook.io/practical-ml/nn/training-and-debugging-of-nn" rel="noopener ugc nofollow" target="_blank">神经网络的有效训练和调试</a>。</li><li id="875e" class="ku kv hi ih b ii lf im lg iq lh iu li iy lj jc ly lc ld le bi translated"><a class="ae mi" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank">用Tensorflow创建数据管道</a>。</li></ul></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><p id="39c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢我的作品，那么请在这个帖子上鼓掌。</p><p id="b1fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae mi" href="https://www.linkedin.com/in/hardik-vagadia/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p><p id="d7d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的<a class="ae mi" href="https://github.com/HardikVagadia1997" rel="noopener ugc nofollow" target="_blank"> Github简介</a>。</p></div></div>    
</body>
</html>