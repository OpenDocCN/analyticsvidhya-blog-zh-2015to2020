<html>
<head>
<title>Maximum Likelihood Estimation (MLE) for Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的最大似然估计</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/maximum-likelihood-estimation-mle-for-machine-learning-a633116b85fa?source=collection_archive---------5-----------------------#2020-06-18">https://medium.com/analytics-vidhya/maximum-likelihood-estimation-mle-for-machine-learning-a633116b85fa?source=collection_archive---------5-----------------------#2020-06-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/23736615165df22bf0e719bb0e14952f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a6OuuEEPLM9sk5Od.png"/></div></div></figure><p id="7a93" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在<a class="ae jo" href="https://www.nucleusbox.com/logistic-regression-for-machine-learning-using-python/" rel="noopener ugc nofollow" target="_blank">使用Python进行机器学习的逻辑回归</a>博客中，我已经介绍了逻辑函数的基本思想。可能性，找到最适合的s形曲线。</p><p id="c6f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们已经讨论了<a class="ae jo" href="https://www.nucleusbox.com/the-intuition-behind-cost-function/" rel="noopener ugc nofollow" target="_blank">成本函数</a>。我们也看到了优化成本函数的两种方法</p><ol class=""><li id="4106" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn ju jv jw jx bi translated">封闭解</li><li id="07c8" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">迭代形式解</li></ol><p id="17de" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在迭代法中，我们重点研究了梯度下降优化方法。(<a class="ae jo" href="https://www.nucleusbox.com/an-intuition-behind-gradient-descent-using-python/" rel="noopener ugc nofollow" target="_blank">使用Python的渐变下降背后的直觉</a>)。</p><p id="ab00" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这一节，我们将介绍最大似然成本函数。我们希望最大化这个成本函数。</p><h1 id="a806" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">最大似然成本函数</h1><p id="915c" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated"><a class="ae jo" href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-discrete/v/discrete-and-continuous-random-variables" rel="noopener ugc nofollow" target="_blank">随机变量</a>有两种。</p><ul class=""><li id="bf32" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn lg jv jw jx bi translated">分离的</li><li id="16d5" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn lg jv jw jx bi translated">连续的</li></ul><p id="c66a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">离散变量可以取一个有限的数。一个离散变量可以分离。例如，在抛硬币实验中，只会出现正面或反面。如果掷骰子，只有1到6的值会出现。<br/>一个连续变量的例子是男人或女人的身高。例如5英尺、5.5英尺、6英尺等。</p><p id="4a5a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其值由概率分布决定的随机变量。</p><p id="8528" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设你有<strong class="is hj"> N </strong>个观察值x1，x2，x3，…xN。</p><p id="08da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，每个数据点代表人的高度。对于这些数据点，我们将假设数据生成过程由一个<strong class="is hj">高斯(正态)分布</strong>描述。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d5611f6ae83183eb1e1f5a11e7cae76e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uQKBkQnJY0Y0zMIm.png"/></div></div></figure><p id="64b2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">众所周知，任何高斯(正态)分布都有两个参数。平均值μ和标准偏差σ。如果我们根据需要最小化或最大化成本函数。我们会得到优化后的<strong class="is hj"> μ </strong>和<strong class="is hj"> σ </strong>。</p><p id="6fcf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上例中，<strong class="is hj">红色</strong>曲线是成本函数最大化的最佳分布。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b95dda701707001cbf676a2c79ddd1aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Vv8Mh3aAcTjUSXHO.png"/></div></div></figure><p id="04b2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">既然我们选择<strong class="is hj">θ红</strong>，那么我们想要这个的概率应该很高。基于θ的较高概率，我们希望最大化观察x1、x2、x3、xN的概率。</p><p id="17c2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，一旦我们用<strong class="is hj"> θ </strong>定义了这个成本函数。为了简化，我们需要添加一些假设。</p><p id="4724" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">X1，X2，X3… XN是独立的。假设X1，X2，X3，…XN是一个联合分布，这意味着观察样本是随机选择的。通过这种随机抽样，我们可以选择它作为成本函数的乘积。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d4fdc4dda82f0269bdf9869fc0d971de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CeQjSr1XHCUJGmnT.png"/></div></div></figure><h2 id="b6ec" class="ll ke hi bd kf lm ln lo kj lp lq lr kn jb ls lt kr jf lu lv kv jj lw lx kz ly bi translated">一般步骤</h2><p id="d79d" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">我们选择对数将指数项简化为线性形式。所以一般来说这三个步骤。</p><ul class=""><li id="b3fd" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn lg jv jw jx bi translated">定义成本函数</li><li id="dafa" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn lg jv jw jx bi translated">做出独立的假设</li><li id="1a86" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn lg jv jw jx bi translated">将日志简化</li></ul><p id="6ee6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，让我们遵循高斯分布的所有三个步骤，其中<strong class="is hj"> θ </strong>只不过是<strong class="is hj"> μ </strong>和<strong class="is hj"> σ </strong>。</p><h1 id="f725" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">连续分布的极大似然估计</h1><p id="01f9" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">最大似然估计技术寻找最大化观察可能性的参数。例如，在正态(或高斯)分布中，参数是均值μ和标准差σ。</p><p id="1150" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，我们有1000个随机人的年龄数据，它们是正态分布的。自然界遵循高斯分布，这是一个普遍的经验法则。中心极限定理起着gin的作用，但只适用于大数据集。</p><h2 id="0800" class="ll ke hi bd kf lm ln lo kj lp lq lr kn jb ls lt kr jf lu lv kv jj lw lx kz ly bi translated"><a class="ae jo" href="https://www.nucleusbox.com/maximum-likelihood-estimation-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">阅读更多</a></h2></div></div>    
</body>
</html>