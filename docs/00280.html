<html>
<head>
<title>Semantic Segmentation: Introduction to the Deep Learning Technique Behind Google Pixel’s Camera!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语义分割:Google Pixel的摄像头背后的深度学习技术介绍！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/semantic-segmentation-introduction-to-the-deep-learning-technique-behind-google-pixels-camera-cae43f9dd844?source=collection_archive---------0-----------------------#2019-02-26">https://medium.com/analytics-vidhya/semantic-segmentation-introduction-to-the-deep-learning-technique-behind-google-pixels-camera-cae43f9dd844?source=collection_archive---------0-----------------------#2019-02-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="9c6d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="f6c2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们人类极其擅长扫视任何图像并理解其中的内容。事实上，这是我们几乎察觉不到的反应。我们只需要几分之一秒的时间来分析。</p><p id="02b9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对于机器来说，这是完全不同的游戏。在过去的几十年里，已经有无数次尝试让机器更智能地完成这项任务——多亏了深度学习(和<a class="ae kg" href="https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning?utm_source=blog&amp;utm_medium=semanticsegmentationarticle" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>)技术，我们可能最终解决了这个问题！</p><p id="9ca5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这些深度学习算法在我们的智能手机相机中尤其普遍。他们分析给定图像中的每个像素，以检测物体、模糊背景和一系列技巧。</p><p id="03c6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这些智能手机大多使用多个摄像头来营造这种氛围。不过，谷歌自成一派。我很高兴在这篇文章中分享一种使用他们的DeepLab V3+模型的方法，这种模型存在于Google Pixel手机中！</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/2ce1c473bd30377f81c09d512d1e7f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6wYQZdb0Ag8i7NhF.jpg"/></div></div></figure><p id="0efa" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们一起建立你的第一个图像分割模型！</p><p id="c0f3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="kt">本文要求对卷积神经网络(CNN)有很好的理解。阅读下面的文章来了解CNN(或者快速复习一下):</em></p><ul class=""><li id="25c8" class="ku kv hi jf b jg kb jk kc jo kw js kx jw ky ka kz la lb lc bi translated"><a class="ae kg" href="https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/" rel="noopener ugc nofollow" target="_blank"> <em class="kt">从头开始学习卷积神经网络的综合教程</em> </a></li></ul><h1 id="8d38" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">目录</h1><ul class=""><li id="1d1d" class="ku kv hi jf b jg jh jk jl jo ld js le jw lf ka kz la lb lc bi translated">图像分割简介<br/>语义分割<br/>实例分割</li><li id="3815" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated">开始使用Google的DeepLab</li><li id="3015" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated">阿特鲁卷积导论</li><li id="2463" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated">深度可分卷积是什么？</li><li id="deb3" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated">了解DeepLab模型架构</li><li id="7943" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated">训练我们的语义分割模型</li><li id="f20e" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated">自定义数据集上的DeepLabV3+</li></ul><h1 id="d714" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">图像分割导论</h1><p id="b630" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">图像分割是将图像分割成多个片段的任务。这使得分析给定的图像更加容易。从本质上说，这不正是我们在计算机视觉领域一直努力追求的吗？</p><p id="6b91" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这非常类似于基于特定特征将像素分组在一起。现在这些特征常常会导致不同类型的图像分割，我们可以将其分为以下几种:</p><ul class=""><li id="efd4" class="ku kv hi jf b jg kb jk kc jo kw js kx jw ky ka kz la lb lc bi translated">语义分割</li><li id="cd2d" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated">实例分割</li></ul><p id="2eca" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们花点时间来理解这些概念。</p><h1 id="1043" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">1.语义分割</h1><p id="35f5" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">看看下面的图片:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ll"><img src="../Images/fc97cddb13126c82031c915c88ce044b.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/0*UrPARuxsoURpkfdD.jpg"/></div></figure><p id="7698" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这是语义分段工作的一个经典例子。图像中的每个像素都属于一个特定的类别——汽车、建筑物、窗户等。并且属于特定类别的所有像素都被分配了单一颜色。太棒了，对吧？</p><p id="d751" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">为了给这个概念下一个正式的定义，</p><blockquote class="lm"><p id="9928" class="ln lo hi bd lp lq lr ls lt lu lv ka dx translated">语义分割是给定图像中的每个像素分配一个类别的任务。</p></blockquote><p id="b14e" class="pw-post-body-paragraph jd je hi jf b jg lx ji jj jk ly jm jn jo lz jq jr js ma ju jv jw mb jy jz ka hb bi translated">请注意，这与分类有显著的不同。<strong class="jf hj">分类将单个类别分配给整个图像，而语义分割将图像的每个像素分类到其中一个类别。</strong></p><p id="ff69" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">语义分段的两个流行应用包括:</p><ul class=""><li id="a786" class="ku kv hi jf b jg kb jk kc jo kw js kx jw ky ka kz la lb lc bi translated"><strong class="jf hj">无人驾驶车辆:</strong>这些车辆严重依赖这种分割图像来导航路线</li><li id="dbb1" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated"><strong class="jf hj">谷歌Pixel手机上的肖像模式:</strong>这里，我们需要将每个像素分类为属于前景或背景，然后模糊图像的背景部分，而不是多个类别</li></ul><h1 id="f0a1" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">2.实例分割</h1><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mc"><img src="../Images/edbdc8e1f4f2c452948856b3c033c678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*efBEDkq3BVxVtVEi.png"/></div></div></figure><p id="258e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我喜欢上面的图片！它巧妙地展示了实例分割与语义分割的不同之处。在进一步阅读之前，先花点时间分析一下。</p><p id="e979" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在实例分段中，同一类的不同实例被单独分段。换句话说，这些段是实例感知的。在上图中我们可以看到，同一个类(person)的不同实例被赋予了不同的标签。</p><h1 id="0319" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">图像分割算法</h1><p id="33ec" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">图像分割是一个长期存在的计算机视觉问题。已经设计了相当多的算法来解决这个任务，例如分水岭算法、图像阈值化、K-均值聚类、图划分方法等。</p><p id="aa6b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">也有人提出了许多深度学习架构(如用于图像分割的全连接网络)，但谷歌的DeepLab模型迄今为止给出了最好的结果。这就是为什么我们将在本文中重点关注DeepLab的使用。</p><h1 id="c8a4" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">开始使用Google的DeepLab</h1><p id="a75e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">DeepLab是谷歌在2016年设计并开源的最先进的语义分割模型。此后对该模型进行了多次改进，包括DeepLab V2、DeepLab V3和最新的DeepLab V3+。</p><p id="1c56" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在本节中，我们将了解DeepLab V3+背后的架构，并学习如何在我们的自定义数据集上使用它。</p><p id="f8f6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">DeepLab模型大致由两个步骤组成:</p><ul class=""><li id="6ae9" class="ku kv hi jf b jg kb jk kc jo kw js kx jw ky ka kz la lb lc bi translated"><strong class="jf hj">编码阶段:</strong>该阶段的目的是从图像中提取必要的信息。这是用一个预先训练好的卷积神经网络完成的，现在你可能想知道为什么CNN？<br/>如果您之前曾使用CNN进行图像分类，那么您可能知道卷积层会在图像中寻找不同的特征，并将这些信息传递给后续层，现在对于分割任务，基本信息是什么，图像中存在的对象及其位置，由于CNN在执行分类方面表现出色，因此它们可以轻松找出存在的对象。</li><li id="e409" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated"><strong class="jf hj">解码阶段:</strong>此处使用编码阶段提取的信息来重建适当维度的输出</li></ul><p id="9552" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这两个阶段都使用了什么样的技术？让我们来了解一下！</p><h1 id="5f2b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">理解编码和解码阶段使用的技术</h1><p id="03c9" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae kg" href="https://github.com/tensorflow/models/tree/master/research/deeplab" rel="noopener ugc nofollow" target="_blank"> DeepLab </a>架构基于结合两种流行的神经网络架构:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es md"><img src="../Images/47a3fbd7187ebc2eb9cf51c3af6a9baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rAdb7YPmzVBFMape.png"/></div></div></figure><p id="19ca" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在使用CNN时，我们需要确保我们的模型对对象大小的变化具有鲁棒性。这是因为如果我们的模型只使用小物体的图像来训练，那么它可能不会很好地执行输入图像的缩放版本。</p><p id="66d6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这个问题可以通过使用空间金字塔池网络来解决。这些使用输入的多尺度版本进行训练，因此捕获多尺度信息。</p><p id="8d31" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">空间金字塔池网络能够编码多尺度上下文信息。这是通过以多种速率和有效视野探测输入特征或汇集操作来实现的。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es me"><img src="../Images/3f25379bc7ef689b49424b21d0398ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XnJrMDtFmZV3KQPm.png"/></div></div></figure><p id="3e83" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">空间金字塔池网络通常使用同一基础网络的并行版本来训练不同比例的输入，并在后续步骤中组合这些要素。</p><p id="64c9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">并非输入中的所有内容都对我们的模型有用。我们希望只提取可以用来表示大部分信息的关键特征。一般来说，这只是一个很好的经验法则。</p><p id="4d7c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这是编码器-解码器网络表现良好的地方。他们学习将输入转换成可用于表示所有输入信息的密集形式(甚至重构输入)。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mf"><img src="../Images/7251473bd59bf5abc1c47cd27226b01b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QnHEHP3ZZ5eP5bcI.png"/></div></div></figure><h1 id="37bf" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">阿特鲁卷积导论</h1><p id="f3bc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">空间金字塔池使用同一架构的多个实例。这导致训练的计算复杂度和存储需求的增加。并非所有人都有自由运行的GPU，那么我们如何着手减轻这一点呢？</p><p id="5772" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">像往常一样，谷歌有答案。</p><p id="db20" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">DeepLab引入了atrous卷积的概念，这是卷积运算的一种推广形式。阿特鲁卷积需要一个称为速率的参数，用于明确控制卷积的有效视场。阿特鲁斯卷积的一般形式如下:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mg"><img src="../Images/e2f651141721c5d8a9255ca054985d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/0*pgo_Qdl-P7LqV8YT.png"/></div></figure><p id="3993" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">正常卷积是r = 1的萎缩卷积的特例。</strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es me"><img src="../Images/45e7c9440cbf95602a174b3f026c6eb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lrBrPnMqukVgaxMW.png"/></div></div></figure><p id="2858" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">因此，atrous卷积可以从更大的有效视场中捕获信息，同时使用相同数量的参数和计算复杂度。</p><blockquote class="lm"><p id="47da" class="ln lo hi bd lp lq lr ls lt lu lv ka dx translated"><em class="lw"> DeepLab使用速率为6、12和18的atrous卷积</em></p></blockquote><p id="ee1b" class="pw-post-body-paragraph jd je hi jf b jg lx ji jj jk ly jm jn jo lz jq jr js ma ju jv jw mb jy jz ka hb bi translated">名字<strong class="jf hj">阿特鲁空间金字塔池(ASPP) </strong>的诞生得益于DeepLab使用了带有Atrous卷积的空间金字塔池。这里，ASPP使用4种并行运算，即1×1卷积和3×3 atrous卷积，速率为[6，12，18]。它还添加了具有全局平均池的映像级功能。双线性上采样用于将特征缩放到正确的尺寸。</p><h1 id="b0f2" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">深度可分卷积</h1><p id="721e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">深度方向卷积是一种用比标准卷积运算更少的计算次数来执行卷积的技术。这包括将卷积运算分解为两个步骤:</p><ul class=""><li id="b95d" class="ku kv hi jf b jg kb jk kc jo kw js kx jw ky ka kz la lb lc bi translated">深度方向卷积</li><li id="6adf" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated">逐点卷积</li></ul><p id="e78c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们用一个例子来理解这一点。</p><p id="7041" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">假设我们有一个由3个通道组成的大小为12 x 12的图像。因此，输入的形状将是12 x 12 x 3。我们希望对此输入应用5 x 5的卷积。</p><p id="d7af" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">由于每个输入通道有3个5×5的内核，因此对这些内核应用卷积得到8×8×1的输出形状。我们需要使用更多的内核，并将输出堆叠在一起，以增加输出通道的数量。</p><p id="f240" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我将使用图表来说明这两个概念，以便让您对我们正在讨论的内容有一个直观的理解。</p><h1 id="2bec" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">深度方向卷积</h1><p id="a74c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在第一步中，我们对形状为5 x 5 x 1的单个内核应用卷积，得到大小为8 x 8 x 3的输出:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es me"><img src="../Images/6f8a8bfa274fa55a33b9d49899732004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iMF9LmS5b8hIG1tG.png"/></div></div></figure><h1 id="f1a2" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">逐点卷积</h1><p id="3198" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，我们想增加频道的数量。我们将使用深度与输入图像的深度相匹配的1 x 1个内核(在我们的例子中是3个)。这个1 x 1 x 3卷积给出了形状为8 x 8 x 1的输出。我们可以根据需要使用尽可能多的1 x 1 x 3卷积来增加通道数量:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es me"><img src="../Images/d623e6b280c045b67b0c8e963e018d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6qAPtz50aAA4suHe.png"/></div></div></figure><p id="e049" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">假设我们希望将通道数量增加到256个。我们做什么呢我希望你在看到解决方案之前好好想想。</p><p id="1463" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们可以在8×8×3的输入上使用256个1×1×3，得到8×8×256的输出形状。</p><h1 id="0786" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">了解DeepLab模型架构</h1><p id="b267" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">DeepLab V3使用ImageNet的预训练<a class="ae kg" href="https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/" rel="noopener ugc nofollow" target="_blank"> Resnet-101 </a>和atrous convolutions作为其主要特征提取器。在改进的ResNet模型中，最后一个ResNet块使用具有不同膨胀率的各种卷积。它在修改的ResNet块的顶部为解码器模块使用阿特鲁空间金字塔池和双线性上采样。</p><p id="0b7e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">DeepLab V3+使用对齐的<a class="ae kg" href="https://www.analyticsvidhya.com/blog/2017/08/10-advanced-deep-learning-architectures-data-scientists/" rel="noopener ugc nofollow" target="_blank">异常</a>作为其主要特征提取器，并做了以下修改:</p><ol class=""><li id="fa2f" class="ku kv hi jf b jg kb jk kc jo kw js kx jw ky ka mh la lb lc bi translated">所有最大池操作都被具有步长的深度方向可分离卷积所取代</li><li id="abc9" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka mh la lb lc bi translated">额外的<a class="ae kg" href="https://www.analyticsvidhya.com/blog/2017/05/25-must-know-terms-concepts-for-beginners-in-deep-learning/" rel="noopener ugc nofollow" target="_blank">批量标准化</a>和<a class="ae kg" href="https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/" rel="noopener ugc nofollow" target="_blank"> ReLU </a>激活在每个3 x 3深度方向卷积后添加</li><li id="1fd9" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka mh la lb lc bi translated">在不改变入口流网络结构的情况下，增加了模型的深度</li></ol><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mi"><img src="../Images/2ddf6fb04b73c5d262876ae209b62562.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/0*O6R5uUm660VDE2Vr.png"/></div></figure><h1 id="5be3" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">DeepLab V3+解码器</h1><p id="fff5" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">编码器基于16的<strong class="jf hj">输出步幅</strong>(原始图像大小与最终编码特征大小的比率)。不是使用因子为16的双线性上采样，而是首先用因子4对编码特征进行上采样，并与来自具有相同空间维度的编码器模块的相应低级特征连接。</p><p id="7280" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在连接之前，在低级特征上应用1×1卷积，以减少通道的数量。在连接之后，应用一些3×3卷积，并且特征以因子4被上采样。这给出了与输入图像相同大小的输出。</p><h1 id="3d19" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">在自定义数据集上训练DeepLabV3+</h1><p id="182f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们用编码来弄脏我们的手吧！首先，克隆谷歌研究公司的<a class="ae kg" href="https://github.com/tensorflow/models/tree/master/research/deeplab" rel="noopener ugc nofollow" target="_blank"> Github repo </a>，将所有代码下载到你的本地机器上。</p><p id="d9aa" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">准备数据集</strong>:为了在我们的定制数据集上训练DeepLab模型，我们需要将数据转换为TFRecord格式。将数据集移动到model/research/deep lab/datasets。我们的数据集目录应该具有以下结构:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mj"><img src="../Images/1bbc5393f041044c757fdd8aafd55ecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/0*eaeCSxHIAkLYCAb6.png"/></div></figure><p id="2031" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> TFRecord是</strong><a class="ae kg" href="https://www.analyticsvidhya.com/blog/2017/03/tensorflow-understanding-tensors-and-graphs/" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj">tensor flow</strong></a><strong class="jf hj">的自定义二进制数据存储格式。</strong>由于二进制数据占用的空间少得多，并且可以非常高效地读取，因此处理大型数据集变得更加容易。</p><p id="bc2b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">当处理太大而无法存储在内存中的数据集时，TFRecords格式非常方便。现在，只从磁盘中读取当时需要的数据。听起来像是双赢！</p><ul class=""><li id="1c2b" class="ku kv hi jf b jg kb jk kc jo kw js kx jw ky ka kz la lb lc bi translated"><em class="kt"> JPEGImages </em>文件夹包含原始图像</li><li id="2720" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated"><em class="kt"> SegmentationClass </em>文件夹包含将类别标签作为像素值的图像(它需要一个单通道图像，其中每个像素的值都是<em class="kt"> classID </em>)</li><li id="b956" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated"><em class="kt"> train.txt </em>和<em class="kt"> val.txt </em>分别包含用于训练和验证的图像名称</li><li id="438b" class="ku kv hi jf b jg lg jk lh jo li js lj jw lk ka kz la lb lc bi translated"><em class="kt"> trainval.txt </em>包含所有图像的名称</li></ul><p id="59da" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在，运行<em class="kt"> build_voc2012_data.py </em>，根据我们的目录结构改变标志的值。这会将您的数据转换为TFRecord格式，并将其保存到“<em class="kt"> — output_dir </em>”所指的位置。</p><p id="8460" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">打开<em class="kt">segmentation _ dataset . py</em>，添加一个<em class="kt"> DatasetDescriptor </em>对应您的自定义数据集。例如，我们使用Pascal数据集，其中1464幅图像用于训练，1449幅图像用于验证。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es me"><img src="../Images/d162efa2ac47eef1b3a7cf004929db0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*57YCCZlBRYC_kmrk.png"/></div></div></figure><p id="dc5c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在是时候训练我们自己的图像分割模型了！</p><h1 id="11bd" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">训练我们的图像分割模型</h1><p id="c39c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们需要运行models/research/deeplab/文件夹中的<em class="kt"> train.py </em>文件。根据您的要求更改标志。</p><pre class="ki kj kk kl fd mk ml mm mn aw mo bi"><span id="5fa9" class="mp ig hi ml b fi mq mr l ms mt"># From tensorflow/models/research/ python deeplab/train.py \ --logtostderr \ --training_number_of_steps=90000 \ --train_split="train" \ --model_variant="xception_65" \ --atrous_rates=6 \ --atrous_rates=12 \ --atrous_rates=18 \ --output_stride=16 \ --decoder_output_stride=4 \ --train_crop_size=769 \ --train_crop_size=769 \ --train_batch_size=1 \ --dataset="cityscapes" \ --tf_initial_checkpoint=${PATH_TO_INITIAL_CHECKPOINT} \</span></pre><p id="a0f3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这将在您的数据集上训练模型，并将检查点文件保存到<em class="kt"> train_logdir </em>。</p><h1 id="7aa5" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">评估我们的图像分割模型</h1><p id="5ec8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在我们已经有了训练模型的检查点文件，我们可以使用它们来评估它的性能。使用更改后的标志运行<em class="kt"> eval.py </em>脚本。这将在<em class="kt"> val.txt </em>文件中提到的图像上评估模型。</p><pre class="ki kj kk kl fd mk ml mm mn aw mo bi"><span id="6799" class="mp ig hi ml b fi mq mr l ms mt">python "${WORK_DIR}"/eval.py \ --logtostderr \ --eval_split="val" \ --model_variant="xception_65" \ --atrous_rates=6 \ --atrous_rates=12 \ --atrous_rates=18 \ --output_stride=16 \ --decoder_output_stride=4 \ --eval_crop_size=513 \ --eval_crop_size=513 \ --checkpoint_dir="${TRAIN_LOGDIR}" \ --eval_logdir="${EVAL_LOGDIR}" \ --dataset_dir="${PASCAL_DATASET}" \ --max_number_of_evaluations=1</span></pre><blockquote class="lm"><p id="7aac" class="ln lo hi bd lp lq mu mv mw mx my ka dx translated"><em class="lw">我们运行了1000步的训练阶段，得到</em>mean intersection overunion<em class="lw">为0.834894478 </em></p></blockquote><figure class="na nb nc nd ne km er es paragraph-image"><div class="er es mz"><img src="../Images/0509d8012ecb2ba515155110524765be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/0*sXxChvvZbiJ__aIj.png"/></div></figure><p id="440f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">请记住，用于培训和评估的<em class="kt">型号_变量</em>必须相同。</p><p id="1d8a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">类似地，使用各自的标志运行<em class="kt"> vis.py </em>来可视化我们的结果:</p><pre class="ki kj kk kl fd mk ml mm mn aw mo bi"><span id="71ca" class="mp ig hi ml b fi mq mr l ms mt">python "${WORK_DIR}"/vis.py \ --logtostderr \ --vis_split="val" \ --model_variant="xception_65" \ --atrous_rates=6 \ --atrous_rates=12 \ --atrous_rates=18 \ --output_stride=16 \ --decoder_output_stride=4 \ --vis_crop_size=513 \ --vis_crop_size=513 \ --checkpoint_dir="${TRAIN_LOGDIR}" \ --vis_logdir="${VIS_LOGDIR}" \ --dataset_dir="${PASCAL_DATASET}" \ --max_number_of_iterations=1</span></pre><p id="0c64" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们看看我们的训练模型的一些结果。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es nf"><img src="../Images/f96368d3dd99b6c8dc99ce1dad1c8bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*ymu9MDUtICL8YKJq.png"/></div></figure><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es nf"><img src="../Images/34d2af74fb6bb510220646e16bb07681.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*mlvC1MHsqGZdCorX.png"/></div></figure><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ng"><img src="../Images/08f6a9b13ae2590eb5e55722a2bf91d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/0*IHvt_3IyiAKzPIuf.png"/></div></figure><p id="113c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">看起来不错！祝贺您训练并运行了您的第一个图像分割模型。</p><h1 id="effc" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结束注释</h1><p id="ac51" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">那是相当多的学习消化！一旦您熟悉了这些概念，就可以尝试将它用于您的自定义数据集(Kitti是一个很好的选择，因为它很小)，并找到更多很酷的实际用例。</p><p id="b4b6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我强烈建议你去看看关于这次发布的DeepLab论文和Google AI博客文章:</p><p id="8e6d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我期待着分享您的反馈、建议和使用DeepLab的经验。你可以在下面的评论区和我联系。</p><p id="f4ac" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">你也可以在分析Vidhya的Android应用上阅读这篇文章</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es nh"><img src="../Images/b1f3d9038c6c92b48e6232fe8d824fa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/0*8zEla0iq6vxfKSbH.png"/></div></div></figure></div><div class="ab cl ni nj gp nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="hb hc hd he hf"><p id="ceea" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="kt">原载于2019年2月26日</em><a class="ae kg" href="https://www.analyticsvidhya.com/blog/2019/02/tutorial-semantic-segmentation-google-deeplab/" rel="noopener ugc nofollow" target="_blank"><em class="kt">https://www.analyticsvidhya.com</em></a><em class="kt">。</em></p></div></div>    
</body>
</html>