<html>
<head>
<title>Theory and Implementation of Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的理论与实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/theory-and-implementation-of-linear-regression-2283be73337f?source=collection_archive---------20-----------------------#2020-07-26">https://medium.com/analytics-vidhya/theory-and-implementation-of-linear-regression-2283be73337f?source=collection_archive---------20-----------------------#2020-07-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/99f238fcddef037f80667fab7ee160ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JG1D2277-IiMXo2Q54PLWw.png"/></div></div></figure><p id="611b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本文中，我们将解码线性回归。本文将包含以下部分:-</p><p id="d62a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1)什么是线性回归？</p><p id="6eba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2)样本示例</p><p id="9782" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3)梯度下降</p><p id="3169" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4)波士顿房价示例</p><p id="50c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5)结论</p><p id="0172" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 1)什么是线性回归？</strong></p><p id="bffb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一种受监督的机器学习形式，其中特征之间的潜在关系是线性的。</p><p id="f0ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">试猜线性回归的例子:-</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jo"><img src="../Images/e6aa37f3e1cf8ec9feaafc07cf6fe87f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gPDwLMAjvWATUj8uI-0DfA.png"/></div></div></figure><p id="014b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正确答案是。</p><p id="c912" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们以一个包含两个特征x1和x2的数据集为例。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/a3637c0e95ec4f9d58c2d51274e377ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*IOSMiEqJqZ5yhbVh-Sk5ww.png"/></div></figure><p id="3482" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种情况下，特征x1和x2之间的基本关系是线性的。</p><p id="8d09" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们的任务是在这个数据集中拟合一条线，这条线可以很好地概括我们的数据集，并在用看不见的特征x1和x2处理时给我们准确的结果。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/896c6110cc05a724915a43605f3b2143.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*zjJGqX-gnNEPQ2ZlEUe-dA.png"/></div></figure><p id="a35f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这似乎很适合数据点。</p><p id="7dc4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在线性回归中，我们以一阶线性方程的形式表示我们的模型。</p><p id="52d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">h(x)=w0+w1*x1+w2*x2</p><p id="9bb8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">权重数(w)由特征数+1定义(对于w0或偏差)。</p><p id="c2dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">试着这样想象，假设有一些人(相当于特征的数量)，每个人都说了些什么，他们的合作说的是唯一出来的。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ju"><img src="../Images/8994efc918dcbf95b36975a016630f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*xjEGXciJZdG5X-dlXJD1Sw.png"/></div></figure><p id="04ac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过调整w1和w2的设置，可以产生完美的声音。</p><p id="2b1f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">尝试将相同的类比应用到我们的机器学习算法中，即特征x1的贡献很小，x2的贡献也很小。如果我们简单地将它们相加，即x1+x2=y，那么谁的值更大，谁对y的值的贡献就更大，但是这几乎没有错(即谁更重要，谁就应该得到相关的权重值)。考虑房子的价格取决于两个因素(特征)房间数量和土地面积，比如说1275+2 = 1277美元。现在考虑将1275+6=1281房间的数量增加两倍。我们已将房间数量增加了两倍，但价格上涨幅度可忽略不计。(回到人的例子)因此，为了处理这个问题，我们粘贴权重(w ),这将使每个特征的语音根据它们的相关性而不是它们的值来决定最终的答案。</p><p id="bfce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于w0，想象这是房价中土地的基础价格。</p><p id="5d0c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们定义线性回归的逐步过程(或伪代码)。</p><p id="4e23" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">鉴于，</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jv"><img src="../Images/4fdaeec11d96836fdd1e470049748bcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*zA8ROyx1WKvxk5km57lw7w.png"/></div></figure><p id="56e8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">加一列1调整w0。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="ab fe cl jw"><img src="../Images/192473a83c808f60fe15820e06a49d8f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*boeEQz8P7RCUQeameNxGKw.png"/></div></figure><p id="9cb0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">猜测重量w0、w1、w2</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jx"><img src="../Images/c289edebb9a788330dd188799c439956.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*OoMvB--3smDO4pgSMOk_zA.png"/></div></figure><p id="b5b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">取w和x的点积</p><p id="cda6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">h(x) = xͼw (n*3 ͼ 3*1=n*1)</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jy"><img src="../Images/b831af8e0d72328e10b29e13f69cfd81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*nifKNYkdLyjXWNT7X0TI6w.png"/></div></div></figure><h1 id="290a" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">步骤:——</strong></h1><ol class=""><li id="7f16" class="kx ky hi is b it kz ix la jb lb jf lc jj ld jn le lf lg lh bi translated">根据权重的猜测值和实际答案计算我们的答案之间的差异。</li></ol><p id="b3f8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.将这种差异传递给优化算法(梯度下降)并改进我们的猜测。</p><p id="0ae2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.更新权重值。</p><p id="ac71" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.重复，直到我们得到的差异接近零或最小(全局或局部)。</p><h1 id="30a0" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">梯度下降和成本函数:</strong> -</h1><p id="1ddf" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb li jd je jf lj jh ji jj lk jl jm jn hb bi translated"><strong class="is hj">成本函数或损失函数— </strong>在上面定义的算法中，我们不采用传递到梯度下降的直接差分。我们使用平均绝对误差、均方误差等损失/成本函数来计算这种差异。这些成本函数告诉我们，如果将特定的预测值考虑在内，需要支付的成本是多少。</p><p id="eefb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑一个简单的例子，假设我们开发了一个预测一个人是否患有癌症的ML模型。如果我们的模型预测一个人有90%的机会患癌症，而实际上这个人只有10%的机会患癌症。因此，在这种情况下，由于我们机器的错误预测，这个人由于错误预测而不得不支付的成本是如此之高(80%)。</p><p id="63b6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意，上面的解释只是一个理论背景，只是为了让你了解成本函数是什么，下面是数学上的成本函数。</p><p id="7166" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">设y_pred(或h(w))为预测值，y_actual为真实标签。</p><p id="e3eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，</p><p id="b4ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">均方差:-</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/62ad9b9658bead677ebb14f2a85ed524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*DX8jbTa0AUjaAsmFSodLXg.png"/></div></figure><p id="a914" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们知道了代价函数是什么，我们来定义梯度下降。</p><p id="261f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">梯度下降</strong> : —梯度下降是一种优化算法，可用于达到成本函数的最小值。</p><p id="82ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这意味着，当梯度下降与成本函数值一起传递时，它会相应地更新权重，以便在几次迭代后达到成本函数值的最小值。</p><p id="b003" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设你站在一座小山上，想尽快下山。你决定在你站的地方向最陡的方向迈出一小步(由alpha定义)。什么样的梯度下降能告诉我们最陡的方向。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/c2d5f1aeb9ddd75e3eefd6eac9ad7fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*mHpemrCTQ2kbUUUE3BU7WQ.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">轮廓</figcaption></figure><p id="ed40" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设我们在A点，我们必须到达O点，梯度下降有助于到达那里。</p><p id="bd02" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了更好地理解这个问题，我们举一个只有一个权重需要最小化的数据集为例。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/1fdeb0a39440254f535e6d49ebeac60e.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*ebZqN7qEmpXITaQYbSy0PA.png"/></div></figure><p id="f216" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">比方说，我们已经从点A开始计算梯度，更新权重让我们到达A1、A2、A3，最后是Ai。</p><p id="17f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，对于多个特征，将多个权重想象为多个维度，梯度下降让我们达到最小值。</p><p id="ddb8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">到目前为止，我们还没有看到梯度下降的数学面貌。</p><p id="9bff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，从数学上来说，</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/b210179ead0c4faa9411a9b7f4935808.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*qO6hGDbpvNUtncELEfASfg.png"/></div></figure><p id="4f88" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">α被称为学习率，它是我们给ML模型的超参数之一。</p><p id="0b6a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">更清晰的图片，对于n个特征，我们有我们的假设:—</p><p id="b80a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">h(w)=w0x0+w1x1+w2x2+………..+wnxn</p><p id="3f27" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，成本函数变成(MSE):-</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/62ad9b9658bead677ebb14f2a85ed524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*DX8jbTa0AUjaAsmFSodLXg.png"/></div></figure><p id="e4d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/5a0f500eaebf64d4dbb6310520e8bf38.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*xHOZ0IGBgXXqmoNE5nKEXQ.png"/></div></figure><p id="9cfe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，更新如下所示</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/5edc49cf480d61a0c5c04eb404318d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*gc2PlYTzb0hd1ld5ArDyiw.png"/></div></figure><h1 id="b4c3" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">波斯顿房价举例:——</strong></h1><p id="f26b" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb li jd je jf lj jh ji jj lk jl jm jn hb bi translated">现在，在文章的这一部分，我们将讨论线性回归的编程实现。有一点需要注意，我们将使用sckit-learn库来实现线性回归。我们采用的数据集是波士顿房价数据集，其中的目标变量是房价。所以让我们开始吧。</p><p id="3fb4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意:-我将讨论程序的片段。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/ffeb3a694732c27247e7d477f0572a36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dqSI27nwfTfx2DdIVtEt1w.png"/></div></div></figure><p id="605e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这段代码加载了我们在程序中需要的所有库。</p><p id="cb39" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.Numpy用于将我们的数据集组织成NumPy数组，我们使用这些数组将数据输入到线性回归中。</p><p id="51b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.Seaborn在我们的程序中用于绘制pairplot，它给出了每个特性相对于每个特性的图。</p><p id="cee4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.Pandas用于将我们的数据集转换为dataframe，以便更好地呈现数据集。</p><p id="2167" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.Sklearn的数据集导入是为了导入一个数据集。</p><p id="d831" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.Train_test_split用于将数据集拆分成测试和训练数据集。</p><p id="10c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">6.标准缩放器用于缩放数据集的每个要素。</p><p id="293d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">7.LinearRegression用于实现线性回归算法。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/ab19cda63a870f8370d87df3368e30aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFZdJh9waGUuLdJQWIA95Q.png"/></div></div></figure><p id="9f1e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.dataset.load_boston()将波士顿房价数据加载到名为data的变量中。</p><p id="e9bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.pd.DataFrame(data.data，columns=data.feature_names)创建数据框。</p><p id="76b6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.第三行在数据集中创建另一列，并在每个单元格中分配y值。</p><p id="c8fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.根据每个可能的特征绘制配对图</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/dc52494a74d079f6104ff1ebe34d3800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9xfQW_IOxITvOMdQ-XtQdg.png"/></div></div></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/760ef979ec19208bace2ed14afd3ea60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yGyBXWQHQLa7ViGpnYal6g.png"/></div></div></figure><p id="3709" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.第一行将数据集分为训练和测试数据集。测试规模的大小是数据集的20%。</p><p id="0f16" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.这一行启动一个StandardScaler类的对象，以便它可以用于在相同的比例上扩展数据集。</p><p id="746f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.这条线与X_train成比例。</p><p id="58ee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.这会缩放X_test。</p><p id="1499" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.第5行和第6行将Y_test和Y_train从数据帧转换为NumPy数组。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/6d1f11bcdd9b5fbaf05fc654ed421397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_kvhYC_Eoz94t9vFne7ymQ.png"/></div></div></figure><ol class=""><li id="1a9a" class="kx ky hi is b it iu ix iy jb lz jf ma jj mb jn le lf lg lh bi translated">此部分打印每组中的项目数(培训和测试)。</li></ol><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/a62d84b9d7b1c49422d31cb5efa48a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9VDZ_X2bmYiyfvBnr22QWg.png"/></div></div></figure><p id="edce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.这一行实例化了类线性回归的对象。我们对矩阵乘法等的所有解释。正在这里自我实现。</p><p id="cdcd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.Lr.fit(X_train，Y_train)这条线将训练数据拟合到线性回归模型中。这叫训练。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/8499765199fd36bcd2c3b050831887c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHValn-IhGmbGcQbWuckvA.png"/></div></div></figure><p id="ad6e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.这条线使用从训练数据本身学习到的参数来预测训练数据集的目标值。</p><p id="426a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.这条线使用相同的学习数据集预测测试数据集的目标值。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/0de009a0c34eaf16e3290f1f691cf4d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3zS9niLZlnQY7JLe_CrpZg.png"/></div></div></figure><p id="6042" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.该行加载均方误差成本函数。</p><p id="e5b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.这条线比较了我们在训练数据集中的预测有多好。</p><p id="51a9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.这一行测试了测试数据集上的预测，我们只是为了测试的目的而单独发布的。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/fc325c72a43390ecd4c08d95a4997a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3gXB6YI5dTxYXo6XY6Fe6A.png"/></div></div></figure><ol class=""><li id="1ae6" class="kx ky hi is b it iu ix iy jb lz jf ma jj mb jn le lf lg lh bi translated">该行给出了学习参数的值</li></ol><h1 id="c2ad" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">结论:- </strong></h1><p id="55a2" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb li jd je jf lj jh ji jj lk jl jm jn hb bi translated">在这篇文章中，我们讨论了可能是最基本的机器学习算法。在接下来的系列文章中，我们将处理一些复杂的机器学习算法。此外，请注意，我们没有使用纯数字实现线性回归，这是因为在实际应用中，我们通常不会使用纯数学实现算法。无论如何，我们都要使用为我们工作的库。</p><p id="6af3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">希望你喜欢这篇文章，非常感谢你的宝贵反馈。</p><p id="2835" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">谢谢你</p><p id="ba16" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">雅利安·施里瓦斯塔瓦</p></div></div>    
</body>
</html>