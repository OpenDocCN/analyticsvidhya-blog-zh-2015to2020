<html>
<head>
<title>Exploring Reinforcement Learning &amp; Neural Networks basics | Python implementation of the Cross-Entropy Method on CartPole OpenAI’s environment.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索强化学习和神经网络基础|交叉熵方法在CartPole OpenAI环境中的Python实现。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/exploring-reinforcement-learning-neural-networks-basics-python-implementation-of-the-aaec0beee4c4?source=collection_archive---------12-----------------------#2020-07-20">https://medium.com/analytics-vidhya/exploring-reinforcement-learning-neural-networks-basics-python-implementation-of-the-aaec0beee4c4?source=collection_archive---------12-----------------------#2020-07-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c8270b9f8d6d423c694f406270cebcff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XIUt3iH29rcNmP_YinwSpA.png"/></div></div></figure><h2 id="c7b0" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">和往常一样，这个标题听起来很复杂，但绝对不是，相信我。</h2><h2 id="c16a" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">首先，什么是强化学习？</h2><p id="fd46" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">强化学习是一种机器学习方法，通过这种方法，智能程序，也称为<strong class="jq hj">代理</strong>在已知或未知的<strong class="jq hj">环境</strong>中采取<strong class="jq hj">动作</strong>，通过<strong class="jq hj">反馈</strong>不断适应和学习它们之前的动作。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kj"><img src="../Images/1ab8d5985dfc3af19ad27a2ad48ea9c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QAeSPlH71sPXiSKofcZXVw.jpeg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">行动——奖励、观察</figcaption></figure><p id="426e" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">如上图所示，代理在环境中采取行动后收到的反馈是<strong class="jq hj">奖励</strong>和<strong class="jq hj">观察</strong>。</p><h2 id="cdb5" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">那么这些是什么，为什么我们需要它们？</h2><p id="79f0" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">强化学习的概念实际上是非常直观的，因为我们在日常生活中使用它，想想看，作为一个孩子，你会因为你的好行为/行为而受到奖励，因为你的坏行为而受到惩罚(或不受到奖励)，除非你喜欢被惩罚，否则你会自然而然地做更多你受到奖励的行为，而不是其他你受到惩罚的行为，这就是强化学习。</p><p id="28a7" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">代理人采取行动，基于反馈(奖励和观察)，我们根据我们想要完成的目标来评估这些行动有多好，然后我们奖励或惩罚代理人，然后他一次又一次地重复，直到他学会每种情况或状态下的最佳行动。</p><p id="0872" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">我们定义了什么是奖励，现在观察是环境提供给代理的信息片段，观察可以是代理的位置，速度..简单到一串数字，复杂到包含来自几台相机的彩色图像的几个多维张量👐。</p><p id="be7e" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">代理以迭代的方式作用于环境以进行改进，强化学习中的迭代被称为<strong class="jq hj">集</strong>，因此集简单地说就是模拟的长度，在该长度结束时，系统以终止状态结束。例如，如果我们愿意教一个代理人下棋，一集可能会在其中一个玩家输掉游戏时结束，或者可能会持续一段时间…无论何时一集结束，游戏都会回到初始状态。</p><p id="9562" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated"><strong class="jq hj">总结</strong> : <strong class="jq hj"> </strong>智能体在与环境互动的同时，以回报最大化为目标。</p><blockquote class="kx ky kz"><p id="e3f0" class="jo jp la jq b jr ks jt ju jv kt jx jy lb ku ka kb lc kv kd ke ld kw kg kh ki hb bi translated"><strong class="jq hj"> <em class="hi">思想</em> </strong> <em class="hi">:监督学习，特别是深度学习在过去十年里在结果和受欢迎程度方面取得了巨大的进步，这种进步是由ImageNet这样的大型标记数据集推动的，强化学习中的等价物将是一个大型和多样化的环境集合，这不是今天的情况，现有的开源环境没有足够的多样性，只是因为环境很难设置。</em></p></blockquote></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><h1 id="bf4a" class="ll ir hi bd is lm ln lo iw lp lq lr ja ls lt lu je lv lw lx ji ly lz ma jm mb bi translated">如果我们不了解马尔可夫过程或马尔可夫链的概念，我们将一事无成</h1><p id="e476" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">马尔可夫过程广泛应用于不同的工程领域，因此阅读它不仅在强化学习方面对你有益。</p><p id="807e" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">想象你有一个系统，你不能影响，你只能观察，你观察到的被称为状态，系统可以在状态之间切换。系统中所有可能的状态被称为状态空间，在马尔可夫过程中，我们要求这个空间是有限的，但它可能非常大，以补偿这种限制，你的观察形成一个状态序列或一个链(这就是为什么它也被称为马尔可夫链)，这个链序列被称为历史。</p><p id="7b69" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">要称一个系统为马尔可夫过程，它需要满足马尔可夫性质，一个非常简单的性质:状态(t+1)必须只依赖于状态(t)。这使得每个状态可以独立描述系统的未来。</p><h2 id="df20" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">转换矩阵</h2><p id="1d46" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">假设我们有一个有N个状态的系统，这个系统符合马尔可夫性质。然后我们可以创建一个如下所示的转换矩阵:</p><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/f3742d8f7ae4af64f31251866932c8b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*IpTPBxvU4CYo16An28tnvQ.jpeg"/></div></figure><p id="d310" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">行<strong class="jq hj"> i </strong>和列<strong class="jq hj"> j </strong>中的每个单元包含系统从状态<strong class="jq hj"> i </strong>到状态<strong class="jq hj"> j </strong>的概率。</p><p id="a82e" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">下面是马尔可夫过程的另一种表示法(也广泛用于有限状态机表示法) :</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/81c552f1c68aebb99ffd99974875e500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nyFvwph1A3HOTZBI_hKz-w.jpeg"/></div></div></figure><p id="050f" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">现在，让我们结合第一部分和刚刚看到的内容:</p><p id="2f9c" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">我们处于一种状态，我们观察(获得观察)，然后我们采取行动，而对于我们采取的行动，我们要么得到奖励，要么受到惩罚😐或者只是没有得到足够的回报，这意味着我们本可以做得更好。</p><h2 id="9145" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">马尔可夫奖励过程</h2><p id="f478" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">我们之前提到了奖励，以及它们在学习过程中的重要性，现在我们将深入探讨更多细节。</p><p id="3891" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">首先，我们需要为我们从一个状态到另一个状态的转换添加另一个值，这将是相关的奖励。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/c02686097ba838e15053277382ff5614.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xccc9r9rhi6ZAMQXKwmv3w.jpeg"/></div></div></figure><p id="68b9" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated"><strong class="jq hj">总结:</strong>我们观察到马尔可夫过程中的状态转移链，这仍然是马尔可夫奖励过程的情况，但是对于每个转移，我们都有这个额外的量“奖励”，所以现在，我们所有的观察都有一个奖励值附加到系统的每个转移上。</p><h2 id="658f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">预期收益</strong></h2><p id="d042" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">如前所述，代理人的目标是最大化其累积回报，为了用数学方法形式化这一点，我们引入了<em class="la">预期回报</em>，它基本上是未来回报的总和。</p><p id="f66b" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">返回时间<strong class="jq hj"> t </strong>:</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/ae7e3465480db5853c37df3e24fccb93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQWtrsiizczaEuHwBvr4yg.jpeg"/></div></div></figure><p id="0515" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">t是最后一个时间步长，</p><p id="8054" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">所以回到我们之前提到的“插曲”的概念，T将是插曲的持续时间。</p><p id="6ab2" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">有一些代理人-环境的相互作用不会自然地分裂成一段一段的，相反，它们会无限制地持续下去。</p><p id="b19a" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">所以我们有阶段性任务和持续性任务。持续的任务，因为它们是无限持续的，它们使我们对每个时间T的收益的定义变得有问题，仅仅因为我们最后的时间步长T现在等于无穷大。</p><p id="fc7b" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">为了解决这个问题，我们在预期回报等式<em class="la">、</em>中加入一个贴现因子ɣ<em class="la">、【gamma】、</em>，这是一个从[0到1]的数字。那么什么是ɣ，它意味着什么？</p><h2 id="0d2a" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">贴现预期回报:</h2><p id="48e6" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">回报等式变为:</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/64474788c4b310be090a2da6e20c923b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3qWbBqKGVHcU5QwozP-1iw.jpeg"/></div></div></figure><p id="b6e3" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">贴现回报的这一定义意味着我们的代理人将更关心眼前的回报而不是未来的回报，因为未来的回报将被更大程度地贴现。</p><p id="8161" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">有了贴现因子ɣ，即使t时刻的收益是无穷多项之和，收益本身实际上也是有限的。(0≤ɣ&lt;1)</p></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><h1 id="e87c" class="ll ir hi bd is lm ln lo iw lp lq lr ja ls lt lu je lv lw lx ji ly lz ma jm mb bi translated">Reinforcement learning methods</h1><p id="c1f7" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">There are multiple techniques to implement a reinforcement learning algorithm, the one I’m going to cover on this article is called the <strong class="jq hj">交叉熵方法。</strong></p><h2 id="acdf" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">为什么是交叉熵方法？</h2><p id="3333" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">嗯，因为它简单而且非常直观，而且它还允许我们再次使用进化算法(遗传算法在我的上一篇文章<a class="ae mh" rel="noopener" href="/@HamzaELHANBALI/genetic-algorithm-learning-from-nature-to-solve-complexe-optimization-problems-8744ae7bffed">链接</a>中有涉及)。</p><h2 id="1f2c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">它是如何工作的？</h2><p id="22b0" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">代理人在与环境互动时，要么得到奖励，要么受到惩罚，目标是通过采取正确的行动来最大化奖励。</p><p id="940d" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">我们要做的是将问题转化为监督学习问题，其中观察构成特征(输入数据),动作构成标签。</p><h2 id="c4f0" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">人工神经网络</h2><p id="0ce9" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">你可以把神经网络想象成一个黑匣子，它把特征作为输入，把预测作为输出。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/d9d2b1f90c35bb1a3a08d4cadc0d2bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hNOgnRABrLKMjhBDMfsPuw.jpeg"/></div></div></figure><p id="6b49" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">然后，我们将这些预测与实际<em class="la">(期望)</em><em class="la">(或所谓的标签)</em>输出进行比较，预测输出和实际输出之间的差异就是误差<em class="la">(目的是让预测输出=期望输出)。</em></p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/5d0f97063e5f3596ef8013552b5b14e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EARtPluOtQvH56f4qJ08Rw.jpeg"/></div></div></figure><p id="b9ac" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">基于该误差，我们通过“反向传播”来调整神经网络的权重。<a class="ae mh" href="https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;t" rel="noopener ugc nofollow" target="_blank">反向传播清楚地解释了</a></p><p id="9e1e" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">这就是神经网络的训练方式。训练过程是调整权重以获得更好的预测。权重是可学习的参数。</p><p id="b6c8" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated"><strong class="jq hj">总结:</strong>我们通过神经网络输入观察值，首先随机初始化权重，这也意味着神经网络在开始时会给我们不好的结果，当通过反向传播调整权重时，预测会得到改善。</p><h2 id="88a4" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">让我们看看黑盒里面</h2><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/03d6cb8bb781f188f0937d513638256c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NqXVhyPC0qLNUpNDip7WrQ.jpeg"/></div></div></figure><p id="9e26" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">神经元(红色)也称为节点或单元，实际上是称为激活函数的非线性函数，其背后的目的是将非线性引入神经元的输出，因为真实世界的数据或至少大部分数据是非线性的。</p><h2 id="2bee" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">在神经元内部</h2><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/cb85000f533779d1a18ccd3a47e1d609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a_nkA64cseDeG201o3Sm1g.jpeg"/></div></div></figure><p id="eb6f" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">一些最著名和最常用的激活函数是Sigmoid、ReLu、Tanh和Softmax。<a class="ae mh" href="https://www.youtube.com/watch?v=9vB5nzrL4hY" rel="noopener ugc nofollow" target="_blank">激活功能解释。</a></p><blockquote class="kx ky kz"><p id="4bd6" class="jo jp la jq b jr ks jt ju jv kt jx jy lb ku ka kb lc kv kd ke ld kw kg kh ki hb bi translated"><strong class="jq hj">偏置项:</strong><strong class="jq hj">偏置项</strong>用于像y截距一样调整最终输出矩阵。例如，在经典方程y = mx + c中，如果c = 0，那么直线将总是通过0。添加<strong class="jq hj">偏差项</strong>为我们的<strong class="jq hj">神经网络</strong>模型提供了更多的灵活性和更好的泛化能力。(定义取自Stackoverflow)。</p></blockquote></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><h1 id="244a" class="ll ir hi bd is lm ln lo iw lp lq lr ja ls lt lu je lv lw lx ji ly lz ma jm mb bi translated">用Python实现交叉熵方法</h1><p id="0d09" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">我们的环境:</p><blockquote class="kx ky kz"><p id="0054" class="jo jp la jq b jr ks jt ju jv kt jx jy lb ku ka kb lc kv kd ke ld kw kg kh ki hb bi translated">一根杆子通过一个非驱动关节连接到一辆小车上，小车沿着一条无摩擦的轨道移动。通过对推车施加+1或-1的力来控制该系统。钟摆开始直立，目标是防止它翻倒。杆保持直立的每个时间步长提供+1的奖励。当柱子偏离垂直方向超过15度，或者手推车偏离中心超过2.4个单位时，该集结束。</p><p id="ebb9" class="jo jp la jq b jr ks jt ju jv kt jx jy lb ku ka kb lc kv kd ke ld kw kg kh ki hb bi translated">链接:<a class="ae mh" href="https://gym.openai.com/envs/CartPole-v1/" rel="noopener ugc nofollow" target="_blank">https://gym.openai.com/envs/CartPole-v1/</a></p></blockquote><p id="c915" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated"><a class="ae mh" href="https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/CartPole-v1/original.mp4" rel="noopener ugc nofollow" target="_blank">https://gym.openai.com/videos/2019-10-21-MQT 8 QJ 1 mwo/cart pole-v1/original . MP4</a></p><h2 id="2970" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">我之前提到过我们将使用一种进化算法，让我们来讨论一下吧！</h2><p id="f128" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">我说过我们需要数据(特征、标签)来训练神经网络，我还说过强化学习并不直接与数据相关，而是与环境互动，并在反复做这件事的同时努力改进。</p><p id="f99f" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">我是不是很矛盾？不😄我们使用进化算法，在强化学习问题中的监督学习，不是很棒吗😁？</p><p id="035e" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">我们要做的实际上是通过运行许多集来收集数据(尽可能多，以从神经网络获得最佳结果)。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/458f6f0f53f3ec8d7492a58b89b98a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q_2o6E8lfKcBbI23isZ_Ng.jpeg"/></div></div></figure><p id="2670" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">对于每一集，我们可能有不同数量的步骤(只要没有达到该集的结束条件，我们就会采取步骤)，由于环境的随机性和代理选择采取行动的方式，一些集会比其他集更好(就奖励而言)。</p><p id="3de5" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">我们分批保存这些剧集。<em class="la">(我们更关心剧集的步骤和奖励，而不是剧集本身)。</em></p><p id="cda3" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">批次可以被想象成储存剧集的盒子。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/44339bbb79f65a13ec817d0663fe57f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dqmzLH030WMljsKfLb-hCg.jpeg"/></div></div></figure><h2 id="9acb" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">自然选择部分</h2><p id="ca00" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">交叉熵方法的核心是扔掉不好的剧集，对更好的进行训练，那么我们如何找到更好的呢？</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/34bd74c3d86680ac1281b4062712374a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGhrXp3f9Bgx5m4OhelglA.jpeg"/></div></div></figure><p id="8fbd" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">我们计算每集的总奖励，并决定奖励界限，哪些集我们保留，哪些集扔掉，我使用了所有奖励的第70个百分点，这意味着我只保留了比其他70%做得更好的30%。<a class="ae mh" href="https://www.youtube.com/watch?v=mDJvDRvvDXo&amp;t=41s" rel="noopener ugc nofollow" target="_blank">百分位数</a></p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/e5c67ecbce2cd25d97553b12cb8fc426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fb1KT-sS8TNvbNtCChwJkg.jpeg"/></div></div></figure><p id="8f42" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">我们在剩下的精英剧集上训练神经网络，这个过程一直重复，直到我们对结果满意为止。</p><p id="e98b" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">神经网络学习重复导致更大奖励的动作，并且由于神经网络变得更好，奖励边界也不断变得越来越高。</p><p id="3db1" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">这就是关于交叉熵方法的全部内容，如果你已经做到了，恭喜你💪。</p><h2 id="840e" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">我们的神经网络架构</h2><p id="bfe6" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">没什么特别的。它将单个观察向量作为输入，并输出要执行的动作。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/f9c112c5c9afe1e3cd1a70697d121e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQi4_rS8j0NcW3NJM4GZ3g.jpeg"/></div></div></figure><blockquote class="kx ky kz"><p id="d0c5" class="jo jp la jq b jr ks jt ju jv kt jx jy lb ku ka kb lc kv kd ke ld kw kg kh ki hb bi translated"><strong class="jq hj">一个常见问题:</strong>为什么有64个神经元，为什么只有一个隐藏层而不是更多？实际上，关于我们如何选择这两个参数(隐藏层的Nb，每层中神经元的Nb)有很多争论，没有最好的方法，但我喜欢的方法是尝试不同的架构，直到我得到满意的结果，记住这样一个事实，你添加的层和节点越多，需要的计算和内存就越多。</p></blockquote><h2 id="1c0f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">Softmax &amp;交叉熵损失函数</h2><p id="dba3" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">我提到过Softmax是最常用的激活功能之一，我将简要介绍它的使用时间和作用。</p><p id="e0bf" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated"><strong class="jq hj">何时使用:</strong></p><p id="ba9b" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">当我们旨在建立多类分类器时，在神经网络中使用Softmax。(我们有3个可能的动作要执行，对于有3个类的神经网络，这使得我们的问题成为多类分类问题)。</p><p id="00b8" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated"><strong class="jq hj">它的作用:</strong></p><p id="88b6" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">首先，这是Softmax激活函数的公式:</p><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/1c93a170e9854a61bd4c2e51ac903f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*A_V1hYZ5yBFiapzsYrZOLQ.jpeg"/></div></figure><p id="318b" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">它将一个实数向量作为输入，输出一个概率分布。</p><p id="3299" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">我们可以这样看</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/9bc6c92cb2a0a8133943d106f5b84ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DtSx-JpiGhA6mdpLoVODoQ.jpeg"/></div></div></figure><p id="4926" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">或者这边</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/1787f1ba1ae5a92f9c8e7bee628c2a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mno1Tk3NK5EeLe4uxEX89g.jpeg"/></div></div></figure><p id="9bd0" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">这就是Softmax的全部内容。</p><h1 id="7851" class="ll ir hi bd is lm mu lo iw lp mv lr ja ls mw lu je lv mx lx ji ly my ma jm mb bi translated">交叉熵损失函数</h1><p id="b6c9" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">通常，softmax与所谓的交叉熵损失相结合，所以让我们再次看看它做什么和如何做。</p><p id="b527" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated"><strong class="jq hj">它的作用和方式:</strong></p><p id="bcc8" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">它测量输出是概率的分类模型的性能。我们的预测越准确，损失就越低。</p><p id="3f5c" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">数学公式:</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/54b7378b2e1e13a6c068da0d736c4925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3rQGFq8_veyFDf5dqYgJgw.jpeg"/></div></div></figure><p id="b649" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated"><strong class="jq hj">例如:</strong></p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/e2e1e042a824db61207167f1325a296c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2quI9co7bCS0I5kAAgEjw.jpeg"/></div></div></figure><p id="eb4a" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">然后基于损失值，通过反向传播来调整权重。</p><p id="6eeb" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">这就是关于交叉熵损失函数的全部内容。</p></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><h1 id="e7c9" class="ll ir hi bd is lm ln lo iw lp lq lr ja ls lt lu je lv lw lx ji ly lz ma jm mb bi translated">代码和演示</h1><p id="2152" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated"><strong class="jq hj">使用两种不同神经网络架构的演示:</strong></p><figure class="kk kl km kn fd ij"><div class="bz dy l di"><div class="nb nc l"/></div></figure><p id="5394" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">【Github库】为Python代码:<a class="ae mh" href="https://github.com/HamzaELHANBALI/DeepReinforcemnetLearning" rel="noopener ugc nofollow" target="_blank">https://github.com/HamzaELHANBALI/DeepReinforcemnetLearning</a></p><p id="7190" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">一如既往，我希望这对你有用，并能启发你用它做其他事情。</p><p id="8c15" class="pw-post-body-paragraph jo jp hi jq b jr ks jt ju jv kt jx jy jb ku ka kb jf kv kd ke jj kw kg kh ki hb bi translated">下一期《✋.邮报》再见</p></div></div>    
</body>
</html>