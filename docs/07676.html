<html>
<head>
<title>A Deep Dive into Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入研究线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-deep-dive-into-linear-regression-5a37745ec5eb?source=collection_archive---------19-----------------------#2020-07-03">https://medium.com/analytics-vidhya/a-deep-dive-into-linear-regression-5a37745ec5eb?source=collection_archive---------19-----------------------#2020-07-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d7ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从真相开始我们的旅程——机器永远不会学习。典型的机器学习算法所做的是找到一个数学方程，当应用于一组给定的训练数据时，产生一个非常接近实际输出的预测。<br/>为什么这不是学习？因为如果你稍微改变训练数据或环境，算法就会失控！而不是人类学习的方式。如果你学会了直视屏幕玩游戏，那么当屏幕被某人轻微倾斜时，你仍然会是一个好玩家，而在ML算法中不会出现这种情况。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/dfccf672be4a2d5bd11f2561499721c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i76r80YHOp2Kx5NgoBE4sw.jpeg"/></div></div></figure><p id="a98b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，大多数算法是如此复杂和令人生畏，以至于它给我们纯粹的人类智能实际学习的感觉，有效地隐藏了潜在的数学。有一句格言说，如果你能实现算法，你就知道算法。这种说法在编程语言提供的库和内置模块的密集丛林中消失了，将我们简化为调用API的普通程序员，并进一步强化了黑盒的概念。我们的追求将是揭开这个所谓的“黑匣子”的神秘面纱，它神奇地产生准确的预测，检测物体，诊断疾病，并声称有一天超过人类的智力。</p><p id="d7c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将从最大似然范式中一种不太复杂且易于可视化的算法开始——线性回归。本文分为以下几个部分:<br/> 1)线性回归的需求<br/> 2)可视化线性回归<br/> 3)推导权重矩阵W <br/>的公式4)使用该公式并对真实世界数据集执行线性回归</p><blockquote class="jp jq jr"><p id="2980" class="if ig js ih b ii ij ik il im in io ip jt ir is it ju iv iw ix jv iz ja jb jc hb bi translated">注意:了解线性代数、一点微积分和矩阵是理解这篇文章的先决条件<br/>，同时，对python、NumPy和Matplotlib的基本理解也是必须的。</p></blockquote></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h2 id="dc0e" class="kd ke hi bd kf kg kh ki kj kk kl km kn iq ko kp kq iu kr ks kt iy ku kv kw kx bi translated">1)线性回归的需要</h2><p id="07ae" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">回归意味着从一组给定的输入变量中预测一个实数值。根据一年中的月份、湿度、海拔高度等来预测温度。因此，线性回归意味着预测一个遵循线性趋势的实数值。线性回归是发现数据相关性的第一道防线。现在，当我们听到“线性”这个词时，首先想到的是一条<strong class="ih hj">线。<br/> </strong>没错！在线性回归中，我们试图拟合一条能够最好地概括数据集中所有数据点的直线。概括是指我们试图拟合一条非常接近所有数据点的线。但是我们如何确保这种情况发生？为了理解这一点，让我们设想一个一维线性回归。这也称为简单线性回归</p><h2 id="db45" class="kd ke hi bd kf kg kh ki kj kk kl km kn iq ko kp kq iu kr ks kt iy ku kv kw kx bi translated">2)可视化线性回归</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ld"><img src="../Images/72cba1cb72adcef1996b0464b3876f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDmVwGalM2AKrhtMUkeP3w.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">典型的线性回归模型</figcaption></figure><p id="f2ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们可视化了mtcars汽车数据集的一个属性，其中x轴上绘制了以吨为单位的汽车重量，y轴上绘制了要预测的<strong class="ih hj">目标变量</strong>，每加仑英里数。目标变量也叫<strong class="ih hj">因变量。预测变量</strong>——在这种情况下，汽车的重量——也被称为<strong class="ih hj">独立变量。<br/> </strong>我们将最佳拟合线定义为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/fb51f784209290e14d69e32210492e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*Sz4hoG2b1jKaAZapz7vcYw.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">y是我们的实际值，y-hat是我们的预测值，a和b是我们的模型将输出的系数，x是我们的输入数据</figcaption></figure><p id="ea3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们希望我们所有的yhats(蓝点)尽可能靠近我们的ys(黑点)。也就是说，我们必须尽量缩小它们之间的距离。为了用数学方法表示这一点，我们将对每个数据点的误差求平方，这样我们可以忽略误差的符号，并对更大的误差进行更多的惩罚。这也被称为<strong class="ih hj">误差平方和。</strong>我们的错误表达式将是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lj"><img src="../Images/2b792bb8523c31c1149add7bf4c8b2f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*LOq-jtQWDIsmGdx8feBW9Q.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">这也被称为误差函数</figcaption></figure><p id="8a63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们用等式中的yhat值替换上述等式中的最佳直线拟合:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lk"><img src="../Images/ad599ce4b4e065c655b5ca98b9658599.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*E67N0k3gGd_kgjK0L5Vt0w.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated"><strong class="bd kf">注意，这里，易和是已知的，我们求解a和b，我们的线性回归模型的输出</strong></figcaption></figure><p id="bf2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步将是最小化我们的误差函数，为此我们使用微积分。但这是一维模型的情况。在现实世界中，我们使用多个预测变量来解释目标变量的方差。我们如何用数学术语来表示它？如果你已经猜到矩阵，靶心！</p><h2 id="2b8b" class="kd ke hi bd kf kg kh ki kj kk kl km kn iq ko kp kq iu kr ks kt iy ku kv kw kx bi translated">3)推导权重矩阵W的公式</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ll"><img src="../Images/5e0f5aa647584b2fb47a44206c61fc6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1KkF69kUsHDl06KOl-BoRg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">二维线性回归图(来源:<a class="ae lm" href="http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/R5_Correlation-Regression_print.html" rel="noopener ugc nofollow" target="_blank">http://SPH web . bumc . bu . edu/otlt/MPH-Modules/BS/R/R5 _ Correlation-Regression/R5 _ Correlation-Regression _ print . html</a>)</figcaption></figure><p id="feec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们转向一个以上的预测变量时，我们就进入了多元线性回归的领域。在MLR中，我们拟合一个超平面(参考图来可视化一个平面)而不是一条线。仍然通过最小化误差平方和来获得系数。</p><p id="0ff3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于MLR，我们的最佳拟合方程变成</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/10f54904c716ad3ed1b114099a003a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*VXF-P-eT8SyzX6E54_rPTg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated"><strong class="bd kf"> w1 … wD是相应预测变量x1 … xD的系数，b是偏差</strong></figcaption></figure><p id="7d66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，D是我们将用于线性回归的预测变量的数量，也称为模型的“维度”。<br/>我们也可以把<strong class="ih hj"> b </strong>写成<strong class="ih hj"> w0，</strong>再乘以<strong class="ih hj"> x0 </strong>其中x0永远是1。因此，我们现在可以将上面的等式改写为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/e6af63cd0a3aa45fdf5ad09408ff9b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*duRC78ORYe0a57h0vjYwrQ.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">请注意，这只是针对一个预测的y值。有N个这样的数据点</figcaption></figure><p id="a698" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于I的每个预测yhat，等式为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/71269675ea00bdc8b6c3cb92d73cd839.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*CEVqeg0XHR8ucgqgG5BvNA.png"/></div></figure><p id="b436" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里我们可以说对任何一个<em class="js"> i </em>都是这样概括的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/6e0e29b23d4997d54139d4f6d9d23efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*s_Wd4szk8jrkLUUwrbI2Mw.png"/></div></figure><p id="efd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在可以使用点积法则将其转换为矩阵形式:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/d0a11a7b4553260aef3d921a23d40dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*CB52OsnJwALHtAFe2wRIAw.png"/></div></figure><p id="6cd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> w </strong>以及<strong class="ih hj"> Xi </strong>是一个<strong class="ih hj"> 1*D矩阵。</strong>注意<strong class="ih hj"> Xi </strong>是一个一维矩阵。说服自己<strong class="ih hj"> X </strong>因此将是一个<strong class="ih hj"> n*D矩阵</strong>。现在的问题是，为什么我把w的<em class="js">转置为</em>？在向量代数中，当我们取两个向量的标量积时，它看起来像这样:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/60041cc156be30440e599010cee85db6.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*mkdxVS8Vm3oscONKXTOcsg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">这就是点积的定义</figcaption></figure><p id="1c54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的例子中，我们从右到左，也就是从向量符号转换到矩阵符号。这里B是w，A是x。这样，我们就有了最佳拟合方程。现在，我们必须用公式表示误差函数，它也是MLR情况下的误差平方和:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/df27f710e45ceed19349313177091387.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*-Pdld1hIWl35XDnrXwnTAQ.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">从最佳拟合方程中代入I的yhat值</figcaption></figure><p id="3525" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们终于来到微积分，我们将使用它来最小化我们的误差函数！</p><p id="e544" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，未知数是w矩阵中的值，已知的是y和x。w中有D个分量。因此，我们必须对w的每个分量求<em class="js">偏导数</em>。</p><p id="b928" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们对误差函数E w.r.t的任意一般分量求导，比如说<em class="js"> wj，</em>其中<em class="js"> j </em>的范围从<em class="js"> 1到d</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/32e2661cde64f9a28cf347dcf7735851.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7IVBMdTMEkuAo0GCI5rdeQ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">使用基本规则——( a+XB)的导数= 2(a+xb) * b*d/dx(x)</figcaption></figure><p id="0809" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二个括号的导数比看起来更容易计算。不是开玩笑！我们只对第<em class="js">第j</em>个分量求导，因此所有其他分量都是常数，它们的导数w.r.t <em class="js"> wj </em>将为0。同样，由于它是点积，除了Xi的第<em class="js">j</em>项，Xi的所有项都将乘以0。说服自己，下面的表达式将是第二个括号的导数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/a050801209a5fe7f8000a016a286dd34.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*Penple-uypw7uAk9mpAhpA.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">c w.r.t c的导数是1。在那里，上述表达式与1 * <em class="lw"> Xij = Xij </em>相同</figcaption></figure><p id="ab5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意<em class="js"> Xij </em>是一个<strong class="ih hj">标量</strong>而<em class="js"> Xi </em>是一个<strong class="ih hj">向量，</strong>其中<em class="js"> i </em>告诉你<strong class="ih hj">哪个样本，</strong>和<em class="js"> j </em>告诉你<strong class="ih hj">哪个特征。</strong></p><p id="220a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，整个等式变成了:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/4c3876d38700dd977e65813330abec0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*VhK9BbRwhD5t8sVE7NhklA.png"/></div></figure><p id="4858" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们必须将这个最小化，为此我们将微积分的最大值-最小值特性应用到上面的方程中。要找到一个函数的局部最大值或最小值，你需要对一个函数的独立变量求导，然后使它等于0。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/633e13e6166d2bad32629181a59129f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*HPAjIniA0OO4USe0LjWpiw.png"/></div></figure><p id="3184" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过将括号相乘，将负项放在等式的一边并清理混乱，我们得到:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/e654181652ea9841de46c22e1e8938e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*HEv3rmf8Rb8KroA4BPQm3A.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">因为wT独立于I，所以从求和中去除了wT</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lz"><img src="../Images/56f8a99072c5065368d2352e4bbb4fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ddvKYnsz-5EaFE0dLgdGQ.png"/></div></div></figure><p id="6984" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们正在求解w，所以我们在两边进行转置，我们得到:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ma"><img src="../Images/bba85a466d4c84d169ab2b44304ffdc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*B3Q0oAUs4qdehdopqLcScg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">这里w是1xD矩阵，X是NxD矩阵，Y是NX1矩阵</figcaption></figure><p id="d1cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">哇哦！！！！！这是一些激烈的数学！我们终于有了线性回归模型的解——w矩阵。既然我们已经亲眼看到了简单ML模型(如线性回归)的数学，我们可以更好地理解复杂神经网络中的基础数学是如何工作的。但这是以后的讨论。现在，让我们将本文中的知识应用到真实世界的数据集上！</p></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><h2 id="0574" class="kd ke hi bd kf kg kh ki kj kk kl km kn iq ko kp kq iu kr ks kt iy ku kv kw kx bi translated">4)使用该公式并对真实世界数据集执行线性回归</h2><p id="bf56" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">数据集是取自<a class="ae lm" href="http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/frame.html" rel="noopener ugc nofollow" target="_blank"><em class="js"/></a>的收缩压</p><p id="e81e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它由11名患者的数据组成，分为三列——收缩压、年龄(年)、体重(磅)。我们的目标是建立一个最适合这些数据的线性回归模型。我们将用于评估该模型的度量标准是<strong class="ih hj">的r平方。</strong>现在，r平方和评估指标本身是一篇独立的中型文章，但就目前而言，让我们先来看看这个指标的范围通常在0和1之间，1是最适合的模型，0是平均适合的模型<strong class="ih hj"/>。我使用了术语'<em class="js">,通常范围从'</em>,因为r平方也可以为负，这意味着您的模型比平均拟合模型表现更差，它有严重的问题。</p><p id="0eea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们跳到python代码</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mb"><img src="../Images/8add2421c625ea706d4457d4fa5a5cb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5re_UhB4Cn-88-2LSMVvQQ.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">我们在这里导入库</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mc"><img src="../Images/41717dc8176f96cc0cb654d07c22865d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3OdGGNTxKqnfCqRcrWHl1Q.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">我们将Excel文件作为dataframe读取，并将其作为NumPy数组存储在变量X中</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es md"><img src="../Images/f4de580ef6d60bac6c5f8ef0f4fe753c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TFjRpIfBvgfsjcqTgrsJpw.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">瞥一眼数组，我们发现这里有3列11行，按血压、年龄和体重的顺序排列</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es me"><img src="../Images/9dfdf4ac575100c1e17e0935383db7df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_xdMJNIagCZvqbN1hGBFg.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">我们绘制了年龄和血压之间的散点图，我们发现它们之间有一个相当好的线性趋势</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mf"><img src="../Images/d9c6c9162b87697673554a1aa6d2eeab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aedXE9_OPV7jqj_IFTgvJw.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">我们对体重和血压做了同样的研究，发现它们之间也有相似的线性趋势</figcaption></figure><p id="5a9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该图强烈表明，线性回归模型将很好地适用于这一问题陈述。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/1b59f8149dfee2abe23386d864ac3a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*KFW2BjqLte4GXTCh4rbXHg.png"/></div><figcaption class="le lf et er es lg lh bd b be z dx translated">给我们的数据框架添加偏差</figcaption></figure><p id="e3df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还记得在最佳拟合等式中，我们将b重命名为w0，并乘以x0，从而始终为1吗？在数据帧中添加1的意思是一样的——添加一个新的特性列x0！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mh"><img src="../Images/d6fe2a30503ad0f2d5404758d428e8f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fp1ZreTwyhvUlJPXsmvAbA.png"/></div></div><figcaption class="le lf et er es lg lh bd b be z dx translated">将数据集分成输入变量和实际输出变量。</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mi"><img src="../Images/0bec7e6da3c6472092cf107edc8947c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ybeb5D5f-HShynW3G1n2cg.png"/></div></div></figure><p id="6667" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">w = np.linalg.solve(np.dot(X.T，X)，np.dot(X.T，Y))</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mj"><img src="../Images/af2251f2712fa50dd4cb031eda66e738.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*r3r53RRI5Dfw_tURBjLYNw.png"/></div></figure><p id="a12f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">比较python代码和w的方程，说服自己它们是一样的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mk"><img src="../Images/ef00f5eadee29b03606e258b9841ebc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9a_Ud-rKJYaJiY_i3c4YUw.png"/></div></div></figure><p id="32cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的模型r2得分为0.97！！！这不是很好吗？！还要注意，我们没有使用任何API进行线性回归，而是使用纯数学来得到这个结果！</p><p id="273e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经到了这篇文章的结尾！在下一篇文章中，我计划以同样的方式实现逻辑回归。敬请关注，祝分析愉快！</p></div></div>    
</body>
</html>