<html>
<head>
<title>BERT: The theory you need to know!!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特:你需要知道的理论！！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bert-the-theory-you-need-to-know-ddd316794395?source=collection_archive---------11-----------------------#2020-04-08">https://medium.com/analytics-vidhya/bert-the-theory-you-need-to-know-ddd316794395?source=collection_archive---------11-----------------------#2020-04-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2cb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自从BERT登上舞台以来，自然语言处理一直是人工智能界的新话题。SOTA超越了当时大多数精英模型之前实现的许多里程碑，并提出了甚至彻底改变谷歌搜索的结果。我将尝试以最简洁的形式解释来自变压器(伯特) 的<strong class="ih hj"> <em class="jd">双向编码器表示。</em></strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/6183ec474c76b57a2016292a00f67612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dDkIb5nL-mkJx8jnnYM2zQ.jpeg"/></div></div></figure><p id="71aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT paper于2018年秋季提交，并通过超越当时成熟的模型，如<strong class="ih hj"> GPT、nlnet、QAnet </strong>和<strong class="ih hj">团队</strong>中的许多其他模型以及<strong class="ih hj"> GLUE </strong>基准，实现了最先进的性能。<em class="jd">(我附上了博客链接，以防你想看《胶水和小队》</em>。一个普遍的看法是，伯特也超过了人类的分数，这在我看来有些错误。创建基准分数时只有3个人参与，这显然是不合理的。想想吧！</p><h1 id="5c66" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">建筑理论</strong></h1><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ko"><img src="../Images/eb6f661a60e29379368d28f8a191d1f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*r8MW4OomALVN3ZJDYgMhPQ.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">BERT架构:<a class="ae kt" href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/bert-encoder" rel="noopener ugc nofollow" target="_blank">链接</a> (Peltarion Bert博客)</figcaption></figure><p id="08d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT基于由12个编码器层、12个注意头和110M参数<strong class="ih hj"> <em class="jd"> (BERT base) </em> </strong>组成的变压器，同时还有多达24个编码器层、16个注意头和340M参数<strong class="ih hj"> <em class="jd"> (BERT Large)。</em> </strong></p><p id="9116" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个输入令牌被表示为一个<strong class="ih hj"> 768长的向量</strong>，然后乘以<strong class="ih hj"> 12键、查询和值嵌入</strong>。普通变压器也有相同数量的解码器，但在BERT中，我们根本没有解码器。<strong class="ih hj">单个编码器层有12个自关注头和一个前馈神经网络</strong>。最后，可以根据问题添加分类器层或任何其他合适的层。关于键、查询和其他嵌入的进一步研究，可以在最后找到附加的博客链接。</p><h1 id="8171" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">输入/输出原理</strong></h1><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ku"><img src="../Images/403a17eee80d5628c409417ab7137d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*uztxBQZiSEftaWbwBYjKLg.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">伯特输入表示</figcaption></figure><p id="2eb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT的默认输入长度容量为512个标记，包括<strong class="ih hj">【CLS】</strong>和<strong class="ih hj">【SEP】</strong>标记。</p><p id="1696" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[CLS](分类记号)是输入序列中的第一个记号，它携带了序列的大部分信息，并且通常对于分类问题来说是足够的。提取该令牌，因为它包含大部分信息。</p><p id="c7c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[SEP](分隔符号)分隔输入序列的两个句子。使用BERT时，必须找到最大可能的输入长度(应小于512)，并在所有比找到的最大序列长度短的序列中执行<strong class="ih hj"> <em class="jd">填充和截断</em> </strong>。</p><p id="9979" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">填充</strong>是将填充标记放在所有较短句子末尾的过程，这样所有输入的大小必须相同。填充越多，模型的训练时间就越长，因此填充标记的数量应该最小。</p><p id="ceed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">截断</strong>是根据输出长度容量缩短输入长度的过程。这有时可能对模型无益，并导致性能下降，因此在大多数情况下应该避免。</p><p id="a659" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT使用不同种类的单词嵌入方法，这与<strong class="ih hj"> <em class="jd"> FastText使用的方法非常相似(不完全相同)。</em> </strong>该方法称为<strong class="ih hj"> <em class="jd">字块</em> </strong> <strong class="ih hj"> <em class="jd">嵌入</em> </strong>。它将词汇表中缺失的单词分解成一组已知单词，并在每个子单词前面加上“<strong class="ih hj"> ## </strong>”。它给出了包含这些散列的输出，因此我们需要显式地删除和连接它们。</p><p id="27be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了单词嵌入之外，段和位置嵌入也在输入中传递。<strong class="ih hj"> <em class="jd">片段嵌入</em> </strong>在标记级别分离两个句子，并赋予每个句子身份。此外，如果执行填充，则需要输入<strong class="ih hj"> <em class="jd">注意屏蔽</em> </strong>，其通过分别标记单词标记1和0来区分单词标记和填充标记。</p><h1 id="053f" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">预培训任务</strong></h1><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kv"><img src="../Images/9734cd46a8d62b7c09b5fc63b21ac7f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*yB_yDy8KTRcY2DCdnHjzxg.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">查看橙色(MLM)的掩码令牌输出:<a class="ae kt" href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270" rel="noopener" target="_blank">链接</a></figcaption></figure><p id="b019" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">伯特诱人表现的一半秘诀是前期训练任务。伯特接受了来自未标记文本语料库的3200万单词(维基百科+书籍)的预训练，这大概教会了伯特足够多的术语，以进一步理解任何其他问题。这个过程中涉及的两个无监督学习是-</p><p id="a912" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">屏蔽语言建模(MLM): </strong>总输入标记的15%被<strong class="ih hj">【屏蔽】</strong>标记屏蔽，并且只有这些屏蔽的标记被模型预测。在预测过程中，BERT使用其双向能力来填充序列中的屏蔽词。掩蔽过程进一步遵循一些规则和分布策略，这些规则和分布策略可以在原始论文中读到。</p><p id="a907" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">下一句预测(NSP): </strong>为了更好地理解句子之间的关系，原始的‘B’标记的句子有50%的时间与‘A’句子对齐，而对于剩余的50%，‘B’部分被标记为<strong class="ih hj">‘not Next’</strong>。这提高了在问答任务中有用的BERT的句子解释能力。</p><h1 id="7b6f" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak">应用领域</strong></h1><p id="8839" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">BERT通过在9个不同的任务中表现出前所未有的成绩证明了它的优势。通过研究，我发现在一些语言任务上如<strong class="ih hj"> <em class="jd">分类、命名实体识别(NER)、词性</em> </strong>和<strong class="ih hj"> <em class="jd">问答、</em> </strong> BERT表现出惊人的结果。然而，这并不符合我们谈到的<strong class="ih hj"> <em class="jd">文字生成</em> </strong>和<strong class="ih hj"> <em class="jd">语言翻译</em> </strong>。在我看来，这是因为它缺少解码层。</p><p id="8f38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">你可以在BERT应用上找到我的Kaggle笔记本进行问答</strong>。<a class="ae kt" href="https://www.kaggle.com/bunnyyy/bert-example-of-q-a-from-chris-mccormick-blog" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/bunnyyy/Bert-example-of-q-a-from-Chris-McCormick-blog</a></p><p id="4a9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考资料:</strong> <a class="ae kt" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">杰·阿拉玛</a>，<a class="ae kt" href="https://www.youtube.com/watch?v=yIdF-17HwSk" rel="noopener ugc nofollow" target="_blank"> CS224N视频</a>，<a class="ae kt" href="https://mccormickml.com/2019/11/05/GLUE/" rel="noopener ugc nofollow" target="_blank">胶水</a></p><p id="8bb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">壁纸:</strong> <a class="ae kt" href="https://wallpapercave.com/language-wallpapers" rel="noopener ugc nofollow" target="_blank">壁纸洞穴</a></p></div></div>    
</body>
</html>