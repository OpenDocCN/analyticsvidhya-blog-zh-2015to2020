<html>
<head>
<title>Linear Regression — Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归—第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-part-i-47eae9d45ee3?source=collection_archive---------18-----------------------#2020-06-09">https://medium.com/analytics-vidhya/linear-regression-part-i-47eae9d45ee3?source=collection_archive---------18-----------------------#2020-06-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="866f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">线性回归</strong>是一种线性方法，通过拟合直线(即线性)来模拟两个或多个变量之间的关系，从而预测给定输入数据的输出。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/9a8db8b66d40f5e8371c6ad52b1830a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fyR4WxFSAQgT9QQnZ-M2DQ.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:【https://unsplash.com/】的<a class="ae jt" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">作者:</a><a class="ae jt" href="https://unsplash.com/@pavelanoshin/portfolio" rel="noopener ugc nofollow" target="_blank">的</a></figcaption></figure><p id="da76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要执行线性回归，数据应满足以下约束条件:</p><ol class=""><li id="71b4" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">数据应该是连续的。</li><li id="cbb5" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">自变量和因变量之间应该存在相关性和因果关系。</li></ol><p id="e0c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要更深入地研究为什么对连续和相关数据进行回归，请查看<a class="ae jt" href="https://devskrol.com/2020/07/16/what-is-regression-in-terms-of-ml/" rel="noopener ugc nofollow" target="_blank">什么是ML意义上的回归？</a>。</p><p id="67bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果对一个独立变量进行线性回归，数据的绘制将类似于下面的散点图。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ki"><img src="../Images/b3b415322c4217122ee92be2051210c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*eRHWTUi-OLoFF6bZEfcAAw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">线性回归</figcaption></figure><p id="1914" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">X —独立变量，即输入训练数据</p><p id="821f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Y —因变量，即输出数据</p><p id="9eca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用数学的方法，每条线都有一个方程。Y = mX + c</p><p id="cd18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于直线，m &amp; c值将是常数，X是一组值。</p><p id="5930" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">斜率= m =上升/下降= dy/dx，即Y的变化/X的变化</p><p id="ffa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，如果(m，c)是(2，1 ),则等式变为Y = 2X + 1</p><p id="f8e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的线性回归模型示例中，由于数据具有良好的相关性，因此可以绘制一条线，使得图中所有点最有可能靠近该线，该线被称为最佳拟合的<strong class="ih hj">线。因此，我们可以确定，对于任何新的X值，Y值是从X点开始的垂线与直线的交点。</strong></p><p id="3957" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设ŷi是当x = xi时由回归线预测的y值。</p><p id="2203" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么对于每个点，在预测值和实际值之间存在误差/差异，其可以被定义为ei = yi-ŷi.</p><p id="d8a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为总体误差，我们需要找出所有现有输入数据的差异，并对其平方求和(SSE)，我们将在本文中看到(成本函数部分)。</p><p id="c808" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在的问题是，对于什么样的m &amp; c值，这个SSE值将是最可能的最小值？</p><p id="546b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要找到线性回归模型的m &amp; c值，有许多方法可用。</p><p id="a45c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是最广为人知的方法是，</p><ol class=""><li id="e01c" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">普通最小二乘法。—非迭代法。使用公式从给定的数据中找出m &amp; c。</li><li id="06f6" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">梯度下降。—随机取m $ c，并通过迭代减少误差。</li><li id="cace" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">最大似然估计。</li></ol><p id="d8fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我们将学习普通最小二乘法来寻找最佳拟合线。</p><p id="5be8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于梯度下降需要更清楚的理解，我将把它作为一个单独的文章。</p><p id="2977" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最小二乘法:</strong></p><p id="2a8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了找到我们的数据的最佳拟合线，最小二乘法是一种数学方法来寻找斜率和y截距。</p><p id="5c4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">也就是说，我们需要找到一个映射函数/模式，其中Y值根据X值而变化。对于线性回归，它是一条直线。</p><p id="e4e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">映射函数为Y = mX + c。</p><p id="c8f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了找到m-斜率和c-截距，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kj"><img src="../Images/26108e29e4a316fbb7e1644f4f4656c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*u7H6E5-hcsCYXKidqalG0A.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">斜率和截距</figcaption></figure><p id="1877" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们在MS. Excel中做，就很容易理解这个概念。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kk"><img src="../Images/dc63827483a7285469e28379dd61333d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E5yBPTF4NFIq3NFeSZx4Fg.png"/></div></div></figure><p id="4c95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上表中，我们已经找到了斜率公式的分子和分母。</p><p id="a4ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们找到斜率，</p><p id="2ee5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">斜率m = 615/4206 = 0.146</p><p id="ac28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们可以通过上面提到的公式找到C。</p><p id="ed5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">c = 10-(0.146 * 74)=-0.818</p><p id="a635" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，斜率为0.146，y轴截距为-0.818的直线是我们数据的最佳拟合直线。</p><p id="96af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">成本函数:</strong></p><p id="bd7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">即使找到的线是最佳拟合线，也不是所有的点都在最佳拟合线上。不是吗？</p><p id="527d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那我们怎么能称这种差异/错误呢？</p><p id="951d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如何知道最佳拟合线的误差度量？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kl"><img src="../Images/b8bc02bb2075d086fcf5c6dacd4d8f98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*7JmF5Bh3FHhXWWMwa9_9yw.png"/></div></figure><p id="b446" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，你可以看到y和y之间的距离——预测误差。</p><p id="e48c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们收集所有误差并对其平方求和时，该值就成为最佳拟合线的成本函数。</p><p id="733b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于这是斜率为0.146的直线，y截距为-0.818的直线是最佳拟合直线，因此计算出的成本在本例中是最小的。</p><p id="f593" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就数学而言，成本函数的公式是，</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es km"><img src="../Images/2baba18d86b3befc6d72bca8040e3ebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*-bgszo3bwFxrlIo4s53i3w.png"/></div></figure><p id="d0f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于上面给出的例子，上证综指是30.075。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kn"><img src="../Images/ec14f85001ffa195734239d0656a82b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*L7c4au-hXeOWRRNiDylF9Q.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">误差平方和</figcaption></figure><h1 id="fcca" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated"><strong class="ak">结论:</strong></h1><p id="3cee" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">我们已经学习了如何使用普通最小二乘法进行线性回归，以及什么是成本函数。</p><p id="b3ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">恭喜你迈出了机器学习的第一步。</p><p id="14e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请继续阅读<a class="ae jt" href="https://devskrol.com/2020/07/19/linear-regression-part-ii-gradient-descent/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">线性回归—第二部分—梯度下降</strong> </a> <strong class="ih hj"> </strong>了解最小化SSE的梯度下降法。</p><p id="f723" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到那时再见&amp;祝编程愉快。</p><h1 id="4e3d" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">您还希望:</h1><ol class=""><li id="4e44" class="ju jv hi ih b ii lm im ln iq lr iu ls iy lt jc jz ka kb kc bi translated"><a class="ae jt" href="https://devskrol.com/category/machine-learning/linear-regression/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">线性回归</strong> </a></li><li id="b9a3" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jt" href="https://devskrol.com/category/machine-learning/logistic-regression/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">逻辑回归</strong> </a></li><li id="3100" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jt" href="https://devskrol.com/2020/07/25/decision-tree/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">决策树</strong> </a></li><li id="0627" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jt" href="https://devskrol.com/2020/07/26/random-forest-how-random-forest-works/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">随机森林是如何运作的？—为什么我们需要随机森林？</strong>T25】</a></li><li id="c1bf" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jt" href="https://devskrol.com/2020/07/19/underfitted-generalized-overfitted/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">欠发—广义—过发</strong> </a></li><li id="2777" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated"><a class="ae jt" href="https://devskrol.com/2020/07/19/overfitting-bias-variance-regularization/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">过拟合—偏差—方差—正则化</strong> </a></li></ol></div></div>    
</body>
</html>