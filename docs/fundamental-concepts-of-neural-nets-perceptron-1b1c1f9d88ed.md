# 神经网络的基本概念:感知器

> 原文：<https://medium.com/analytics-vidhya/fundamental-concepts-of-neural-nets-perceptron-1b1c1f9d88ed?source=collection_archive---------32----------------------->

![](img/94f82021c28f46711e84664476613634.png)

神经网络是分析过去和现在数据的非常有效的工具。它们被制作成在非常基础的水平上反映神经元的生物网络。然而，在高处理能力和大容量存储系统等计算资源的帮助下，即使是我们大脑的基本图像也创造了奇迹；随着每一天的过去，我们越来越接近于建立更接近心灵的表象。人类思维和外部机器的融合不仅有能力改变我们生活的世界，还能创造新的世界。几十年甚至几个世纪的研究进入了这样的倡议，但这里也出现了在如此接近奇迹的成就之前大规模灭绝的问题。

回到基础，神经网络的研究被称为深度学习，它扩展到几个有趣的子主题。

神经网络的起源可以归功于 1957 年弗兰克 T2 罗森布拉特**对感知机的最初设计。理解感知机的功能是掌握深度学习的高级概念必须采取的最基本的步骤。它是最简单的神经网络模型，非常接近逻辑回归模型，只是在损失函数上略有不同。事实上，感知器可以被视为一个单层的神经网络。**

# 感知器的直觉

![](img/9303f6b20531cccfab8a2b812ad99669.png)

图片提供:维基百科

感知器是一个系统，它接受一组输入(x1 到 xn)，然后对每个输入(w1 到 wn)应用各自的权重。此后，它对加权输入使用简单的求和函数，并通过**激活函数**传递该值。简单来说，它就像一个只有三个功能的计算器:

*   -乘法(权重*输入)
*   -求和(将加权输入相加)
*   -激活功能(根据要求可以是任何功能)

激活函数可以是处理总和值并提供特定结果的任何函数。例如，感知器可能使用校正线性单元或 ReLU 函数。由于其简单、高效和易于计算，它是神经网络中最常用的函数之一。如果它是正的，它直接输出输入，否则它输出零。

感知器的整个模型是由一个单一的生物神经元松散地激发出来的，这个神经元构成了人类大脑复杂网络的基础。

为了简单起见，我们可以将感知器与逻辑回归模型进行比较。除了损失函数之外，它们基本上与前面提到的相同。

Y = sigmoid(wx + b)

这是表示逻辑回归模型的方程式，其中 x 是已知变量，w 是分配的权重，b 是常数。sigmoid 函数用于向 0 或 1 加权概率输出。这是一个复杂的函数，因为计算涉及 logit 函数的反函数。

另一方面，感知器遵循精确的方程，但最后，使用一个非常简单的激活函数。通常 ReLU 函数有助于加速任何神经网络流形。

![](img/3e00cdd2aac14cf2f3e4012f2788caa1.png)

图片来源:towardsdatascience

# 多层感知器

研究人员开始想知道是否有可能复制人脑的网络状结构。因此，他们提出了这样的想法:如果在感知器中插入多层神经元，它可能会开始给出改进的结果。多层感知器的概念由此产生。

![](img/e8b39981babd8f090b92a1a63db000b1.png)

图片提供:维基百科

大量的研究工作将这一基本想法转化为成功的故事，一位最著名的科学家和这一概念的主要信徒 Jeff Hinton 带头。

1986 年，Jeff Hinton 和他的团队提出了具有里程碑意义的想法**反向传播**，这后来成为我们今天使用的主要神经网络结构的主干。

反向传播是简单的微分链式法则，但它是强大的人工智能计划的主要基础。

二十多年来，在杰夫·辛顿和他的团队的领导下，这项研究继续创造出世界上大多数人几乎不抱希望的东西。认为机器可以复制人脑并完成奇迹般的任务的观点。即使在**人工智能寒冬**期间，当神经网络研究的资金因计算限制而枯竭，人们转向更可行的技术如支持向量机和梯度推进时，Hinton 仍在神经网络领域的大量研究工作中保持领先地位。

2006 年，他终于提出了一篇有影响力的论文，该论文谈到了具有许多隐藏层的深度神经网络。众所周知，多层显著改善了结果(2012 年的 ImageNet 竞赛也证明了这一点，在该竞赛中，深度神经网络以巨大优势击败了其他结果)。这鼓励了技术部门的强大公司大量投资于神经网络研究，因为他们拥有必要的数据量和足够的计算资源。

今天，神经网络已经将人工智能领域带到了难以想象的高度，仍然有很多事情要做！

[观看此空间，了解关于多层感知器的详细讨论。]