<html>
<head>
<title>Multi-collinearity Key Aspect of Regression Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多重共线性回归问题的关键方面</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-collinearity-key-aspect-of-regression-problem-9918a1a52559?source=collection_archive---------23-----------------------#2020-11-22">https://medium.com/analytics-vidhya/multi-collinearity-key-aspect-of-regression-problem-9918a1a52559?source=collection_archive---------23-----------------------#2020-11-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/e2b56162d21271292445b12a6a8d497a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/0*iwyrsK8dBFdGgfH9"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://www.google.com/search?q=people+in+deep+thinking&amp;tbm=isch&amp;ved=2ahUKEwj87Pr_p5btAhWI1XMBHYVrDegQ2-cCegQIABAA&amp;oq=people+in+deep+thinking&amp;gs_lcp=CgNpbWcQAzoECCMQJzoECAAQQzoCCAA6BwgjEOoCECc6BQgAELEDUI4bWOJQYIBSaAFwAHgEgAHOAYgB9R6SAQYwLjI3LjGYAQCgAQGqAQtnd3Mtd2l6LWltZ7ABCsABAQ&amp;sclient=img&amp;ei=_G66X_z-B4irz7sPhde1wA4&amp;bih=657&amp;biw=1366#imgrc=FiXsK4VVn_PhqM" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><blockquote class="ir is it"><p id="3e5b" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">什么是多重共线性？</strong></p></blockquote><p id="b6bf" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">多重共线性发生在两个或更多的独立变量(<strong class="ix hj">特征</strong>)高度相关的时候。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es jw"><img src="../Images/18dfa33a099e58661712155387984c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*VqpDhJo5zDwybyITbek_WQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图 1</figcaption></figure><p id="525e" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">假设您有一个数据集来预测一个人的工资，这个人有独立变量<strong class="ix hj">年龄、服务年限。</strong>这里，两个独立变量都是强相关的，并形成它们自己的关系<strong class="ix hj"> x1=m*x2+c. </strong>由于相互依赖，这些独立变量与 y 的相关性较小</p><p id="e069" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">多重共线性不会影响模型性能，但独立变量(多重共线性)对输出变量的影响会较小，因此会降低可解释性。</p><blockquote class="ir is it"><p id="d70b" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">多重共线性的原因有哪些？</strong></p></blockquote><p id="bd4f" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">一、假陷阱</strong></p><p id="569f" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">假设我们有一个由<strong class="ix hj">单身</strong>和<strong class="ix hj">已婚</strong>列组成的数据集，如果我们已经将<strong class="ix hj">已婚</strong>作为自变量，那么就没有必要将<strong class="ix hj">单身</strong>作为自变量，从而导致数据集中的冗余，导致多重共线性。</p><p id="8b6c" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">重要的是消除从<strong class="ix hj">单热编码</strong>获得的输出的第一列，否则将导致伪陷阱。</p><p id="9cf1" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">二。数据集中的相同变量</strong></p><p id="b8f1" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">假设我们在数据集中有两个不同的工资列，一个表示以卢比表示的工资，另一个表示以美元表示的工资。</p><p id="5f2e" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">三世。创建一个依赖于数据集中其他变量的变量</strong></p><p id="5a17" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">假设您已经有了<strong class="ix hj">出生年份</strong>列，并且您已经添加了另一个名为<strong class="ix hj">年龄</strong>的列，这两个列显示了彼此之间的高度相关性，并减少了它们对因变量的个体影响。</p><blockquote class="ir is it"><p id="9e3d" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">差异膨胀系数</strong></p></blockquote><p id="4b6c" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">方差膨胀因子检查一个独立变量如何与另一个独立变量相关。数学上它被定义为:</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/fc99d01f263a4d353ae8df764c1aeca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/0*gtVpDX0KgIx0VAyp.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><strong class="bd kc"> R </strong>是残差的平方。</figcaption></figure><p id="ca57" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">V . I . f 的代码</strong></p><p id="47eb" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">我已经使用了<strong class="ix hj">入院预测</strong>数据集来检查 V.I.F。</p><pre class="jx jy jz ka fd kd ke kf kg aw kh bi"><span id="aea2" class="ki kj hi ke b fi kk kl l km kn">from sklearn.preprocessing import StandardScaler<br/>scaler=StandardScaler()<br/>from statsmodels.stats.outliers_influence import<br/>   variance_inflation_factor<br/>scaler=StandardScaler()<br/>def cal_vif(x):<br/>    x_scaled=scaler.fit_transform(df)<br/>    vif_data=pd.DataFrame()<br/>    vif_data["Features"]=df.columns<br/>    vif_data["VIF"]=[variance_inflation_factor(x_scaled,i) for i in <br/>                      range(x_scaled.shape[1])]<br/>    return(vif_data)<br/>cal_vif(df)</span></pre><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/0a9cf5e2e45ab553e7f72fe7ec73bc70.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*AzqaWopWcvihN_-UvLEIMw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">输出</figcaption></figure><p id="8b9e" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">我们将设置一个阈值。如果任何要素的值大于阈值，则被视为多重共线。</p><p id="3bd2" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">我们可以通过两个过程来消除特征之间多重共线性；</p><p id="ca1b" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj"> i .删除具有高 V.I.F 的功能</strong></p><p id="37ec" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">在上面的输出中，我们有两个具有高 V.I.F 值的特征，因此我们可以通过移除这两个特征中的任何一个来减少它。</p><pre class="jx jy jz ka fd kd ke kf kg aw kh bi"><span id="3859" class="ki kj hi ke b fi kk kl l km kn">df.drop("Chance of Admit",axis=1,inplace=True)<br/>df.head()</span></pre><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/7a7e0d5757bcfd89f5659d15ec5bbd42.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*PxzGARxiOjz6ksQZblF_cQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">去除录取机会后获得的输出</figcaption></figure><p id="e27e" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">二世。将两列合并成一列，然后移除两列</strong></p><pre class="jx jy jz ka fd kd ke kf kg aw kh bi"><span id="da2b" class="ki kj hi ke b fi kk kl l km kn">df["CGPA &amp; Chance of Admit"]=df.apply(lambda df:df["CGPA"]- <br/>                                     df["Chance of Admit"] ,axis=1)<br/>df.drop(["CGPA","Chance of Admit"],axis=1)<br/>df.head()</span></pre><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/9f788f2cc9945d89fbc15e5acb33a9c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*aTPWm7RSeBEjqHkYlMyd_Q.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">输出</figcaption></figure></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><p id="35ad" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">结论:- </strong></p><p id="a3ad" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">希望你喜欢这个博客，如果你有任何疑问和建议，请在下面评论。继续探索，继续学习。</p></div></div>    
</body>
</html>