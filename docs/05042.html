<html>
<head>
<title>Introduction to Reinforcement Learning (Q-Learning) by Maze Solving Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过迷宫解决示例介绍强化学习(Q-Learning)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-q-learning-by-maze-solving-example-c34039019317?source=collection_archive---------1-----------------------#2020-04-09">https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-q-learning-by-maze-solving-example-c34039019317?source=collection_archive---------1-----------------------#2020-04-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class="if ig ez fb ih ii"><a href="https://github.com/stschoberg/mazeGame" rel="noopener  ugc nofollow" target="_blank"><div class="ij ab dw"><div class="ik ab il cl cj im"><h2 class="bd hj fi z dy in ea eb io ed ef hh bi translated">stschoberg/mazeGame</h2><div class="ip l"><h3 class="bd b fi z dy in ea eb io ed ef dx translated">用强化学习(QLearning)玩迷宫游戏。你可以体验不同的电路板布局，终端…</h3></div><div class="iq l"><p class="bd b fp z dy in ea eb io ed ef dx translated">github.com</p></div></div><div class="ir l"><div class="is l it iu iv ir iw ix ii"/></div></div></a></div><h1 id="72e7" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">介绍</h1><p id="11db" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">强化学习(RL)算法是ML算法的子集，希望在未知环境中最大化软件代理的累积回报。</p><p id="caa1" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">这个定义很拗口，如果没有对RL问题或算法的直觉，很难理解。然而，在看完下面的例子后，你会发现为了更好的理解，重温一下这个例子是很有用的。在此之前，我们将通过一个简单的迷宫解决RL示例来轻松了解该理论。我们将从10，000英尺的高空看到这些概念，将问题可视化，深入代码，用算法识别问题，然后更正式地重温RL/QLearning。</p><h1 id="7f4d" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">问题概述</h1><p id="b943" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">我认为在我们开始任何想法之前，先了解我们的问题是有用的。这样，您就能够将正式的定义和概念链接回一个文字示例。</p><p id="d9db" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">你会在下面看到我们的游戏。我们有一个10x10的格子，一个蓝点代表我们的玩家，绿色的格子代表不同的奖励值，红色的方块代表不同的惩罚值。蓝点可以向四个方向移动:北、南、东、西。当蓝点到达奖励或惩罚方块时，游戏结束，方块的值为最终得分。这是一个非常简单的游戏，但是我们需要形式化它以便用RL来解决它。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es kz"><img src="../Images/6b3b9976033e72f61ceaabd2c61aa797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABOvAjsEGnfVUP8FYcyp2g.png"/></div></div></figure><h1 id="5139" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">定义:</h1><p id="335f" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">回想一下RL的最初定义提到了软件代理和环境。这两件事是形式化RL问题的概念构建块。</p><p id="5d88" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><strong class="jy hj">环境</strong>:我们的环境就是整个游戏棋盘。游戏板是一组简单的单元。我们可以用一组或一列单元格来表示，这些单元格由它们在棋盘上的(x，y)位置来标识<em class="lk">。</em></p><pre class="la lb lc ld fd ll lm ln lo aw lp bi"><span id="d357" class="lq iz hi lm b fi lr ls l lt lu">ENV = { cell00, cell01, … , cell99 }</span></pre><p id="8b5b" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><strong class="jy hj">状态:</strong>状态是环境中的单一元素。在我们的例子中，一个国家就是一个细胞。形式上，我们可以说环境是一组状态。</p><p id="b987" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">代理:代理是一个可以通过动作与环境交互的实体。代理以一种状态存在。我们的代理是蓝点。值得注意的是，我们只需要一个当前位置和我们的累积分数来定义我们的代理。</p><pre class="la lb lc ld fd ll lm ln lo aw lp bi"><span id="6750" class="lq iz hi lm b fi lr ls l lt lu">AGENT = { CurrPos: Cell, Score: 0 }</span></pre><p id="3c25" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><strong class="jy hj">动作</strong>:动作是代理可以从一个给定状态调用到另一个状态的功能。我们的行动是北，南，东，西。</p><p id="8db6" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">我们几乎可以形式化我们的整个RL问题了！首先，我们需要几个函数。</p><p id="8f1b" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><strong class="jy hj"><em class="lk">Is terminal state(s):</em></strong>Is terminal state以状态<em class="lk"> s </em>为输入，返回一个布尔值，表示<em class="lk"> s </em>是否为结束状态。在我们的例子中，奖励和惩罚方块是最终状态。</p><p id="978b" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><strong class="jy hj"><em class="lk">【Actions】:</em></strong>返回一组代理可以从状态<em class="lk"/>采取的有效动作，例如<em class="lk"> Actions(cell00) </em>将返回<em class="lk"> { UP，RIGHT } </em>，而<em class="lk"> Actions(cell55) </em>将返回<em class="lk"> {UP，DOWN，LEFT，RIGHT} </em>。这是因为棋盘的边界。</p><p id="baf1" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><strong class="jy hj"><em class="lk"/></strong>:返回一个状态的值。例如，<em class="lk">值(单元格00) </em>为0，<em class="lk">值(单元格09) </em>为15。</p><p id="7bea" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">我们现在可以形式化整个RL问题了！</p><h2 id="7745" class="lq iz hi bd ja lv lw lx je ly lz ma ji kh mb mc jm kl md me jq kp mf mg ju mh bi translated">强化学习问题:</h2><ul class=""><li id="0648" class="mi mj hi jy b jz ka kd ke kh mk kl ml kp mm kt mn mo mp mq bi translated">一组状态(环境)</li><li id="aa28" class="mi mj hi jy b jz mr kd ms kh mt kl mu kp mv kt mn mo mp mq bi translated">一系列动作</li><li id="0de1" class="mi mj hi jy b jz mr kd ms kh mt kl mu kp mv kt mn mo mp mq bi translated">一个代理(具有开始状态<em class="lk"> S⁰ </em></li><li id="a997" class="mi mj hi jy b jz mr kd ms kh mt kl mu kp mv kt mn mo mp mq bi translated">动作:来自状态<em class="lk"> s </em>的可用动作</li><li id="796b" class="mi mj hi jy b jz mr kd ms kh mt kl mu kp mv kt mn mo mp mq bi translated">值:一个状态的值<em class="lk"> s </em></li><li id="94fb" class="mi mj hi jy b jz mr kd ms kh mt kl mu kp mv kt mn mo mp mq bi translated">IsTerminalState:是否<em class="lk"> s </em>是结束状态</li></ul><p id="4d81" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">就是这样。RL定义的一般公式。我们可以用这6样东西代表任何RL问题。我们的最终目标是在到达终点状态之前通过访问各州来最大化累积分数(即在游戏结束之前获得尽可能多的分数)。</p><p id="f0a1" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">在我们着眼于最大化报酬之前，重要的是要注意到代理人所拥有的唯一知识是他自己的当前状态和分数。它不知道有多少个状态，奖励是什么，或者结束状态在哪里。代理可以学习的唯一方法是通过动作移动到不同的状态，调用<em class="lk">值</em>和<em class="lk">IsTerminalState</em>并记录结果。</p><p id="3e96" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">凭直觉，你应该能够猜到寻找最优分数的算法必须探索状态空间，以找到产生最大回报的路径。</p><h1 id="9993" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">学习</h1><p id="4c96" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">QLearning (QL)是一种在给定RL问题的情况下评估最优路径的技术。它包括一个记录由代理学习的数据的q表和一个确定给定状态的最佳回报的q函数。QLearning是一个迭代的动态编程算法，只有几个参数，所以一开始看起来可能会令人困惑。我会尽力去划分，但是彻底的了解来自于曝光和时间。</p><h2 id="8a71" class="lq iz hi bd ja lv lw lx je ly lz ma ji kh mb mc jm kl md me jq kp mf mg ju mh bi translated">数量表</h2><p id="bd1b" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">QTable记录采取行动时状态的期望值(例如<em class="lk"> Q(Cell00，' UP') </em>)。它为所有状态和动作组合记录这一点。换句话说，一个量化表将一个奖励映射到每个状态、动作对。为了启动QLearning算法，我们将所有的q值都设置为0。当算法运行并且代理探索状态时，我们更新QTable。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es mw"><img src="../Images/0f9db798f2bb7c140913b98e96b7ab50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FMjhxEh0ePDBJ95Pw6kIPA.png"/></div></div></figure><p id="6a70" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">在迷宫游戏的代码中，我们使用嵌套字典作为我们的QTable。外部字典的关键字是一个状态名(例如Cell00 ),它映射到一个有效的、可能的动作的字典。该字典的关键字是映射到Q值的动作(例如，右)。</p><pre class="la lb lc ld fd ll lm ln lo aw lp bi"><span id="a60f" class="lq iz hi lm b fi lr ls l lt lu">qTable = {cell00 : {‘Right’: 0, ‘Up’: 0 }, cell01: {‘Right’: 0, ‘Left’: 0, ‘Up’: }, …}</span></pre><h2 id="32b5" class="lq iz hi bd ja lv lw lx je ly lz ma ji kh mb mc jm kl md me jq kp mf mg ju mh bi translated">q函数</h2><p id="1f5e" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">QFunction确定给定状态、动作对的q值。在看下面的等式之前，让我们考虑一下如何计算给定状态和动作的最佳值。给定状态<em class="lk"> s </em>和导致新状态<em class="lk">s’</em>的动作<em class="lk"> a </em>，最佳回报将是<em class="lk">值(s’)</em>加上从<em class="lk">s’</em>可用的所有动作中留下<em class="lk">s’</em>可获得的最大值(即<em class="lk">动作(s’)</em>)。最佳值取决于当前值、从<em class="lk">、</em>中获取<em class="lk"> a </em>的奖励以及所有有效动作中下一状态的最佳值<em class="lk">s’</em>。我们用同样的方法计算<em class="lk">s’</em>的最优值。正如你所想象的，这会导致一些非常讨厌的递归。在实践中，我们用更新迭代计算Q(s，a)。通过对上述直觉的一些代数操作，我们推导出以下等式:</p><p id="4a64" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><strong class="jy hj"> Q(s，a)= Q(s，a)+α⋅[value(s')+γ⋅maxq(s′)−q(s,a)】</strong></p><p id="8965" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">我不会去调整<strong class="jy hj"> α </strong>和<strong class="jy hj"> γ。这些被称为学习率和折现率。它们分别修改算法学习的速度和算法对未来行动的重视程度。</strong></p><h2 id="e3a1" class="lq iz hi bd ja lv lw lx je ly lz ma ji kh mb mc jm kl md me jq kp mf mg ju mh bi translated">该算法</h2><p id="46b8" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">理解QLearning算法的一个关键定义是插曲。一个情节就是一个代理从开始状态到结束状态的执行。我们想要运行大量的剧集来探索从开始状态到结束状态的大多数路径。我们的剧集越多，探索的路径越多，找到最佳路径的几率就越大。随着剧集数量的增加，QTable的值会收敛到它们的真实值。</p><pre class="la lb lc ld fd ll lm ln lo aw lp bi"><span id="2c29" class="lq iz hi lm b fi lr ls l lt lu">Init Q(s, a) to 0 for all (s, a) pairs<br/>Repeat for episode = 1 ... numEpisodes<br/>     Initialize s to startState<br/>     While (s is not TerminalState)<br/>           Choose an action a from Actions(s)<br/>           s' = new state after action a from s<br/>           Q(s,a)= Q(s, a) + α⋅[Value(s’)+γ⋅maxQ(s′)−Q(s,a)]<br/>           s = s'</span></pre><p id="60a8" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">重要的是要意识到算法只是填充了q表。它不会输出最佳路径。但是，从QTable和start状态来看，找到最优路径是很容易的。</p><h2 id="80b3" class="lq iz hi bd ja lv lw lx je ly lz ma ji kh mb mc jm kl md me jq kp mf mg ju mh bi translated">行动选择</h2><p id="68c9" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">RL中的一个基本问题和算法中的一个微妙选择，如何从<em class="lk">Actions</em>(即while循环中的第一行)<em class="lk">中选择一个action <em class="lk"> a </em>。我们如何选择行动会影响代理如何探索环境。一种解决方案是从一组有效动作中选择具有最大奖励值或Q(s，a)的动作。一开始，当所有的q值都是0时，这将是一个任意的选择。随着量化表的更新，代理将只探索它知道有回报的方向。似乎是个可行的选择，对吧？我们来模拟一下。考虑下面的环境。</em></p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es mx"><img src="../Images/b3c455db2488c5fea1c69101bf763084.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZcFfOCOP-Kw595pNmdRryA.png"/></div></div></figure><p id="d11d" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">我们假设我们总是从状态<em class="lk"> s </em>中选择奖励值最大的行动。一开始，所有的值都是0，所以代理会选择探索随机的方向。在许多随机决策之后，Q(第1列中的任何空格，右侧)将变为-10。由于我们总是选择最大动作进行探索，而0 &gt; -10，代理不会选择向右移动。因此，它最终会发现左下角的15。</p><p id="86f9" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">在发现15之后，选择产生最大价值的行动成为一个问题。代理人不会去探索棋盘右侧可以获得更高奖励的地方，而是总是会移动到15。这是因为棋盘其余部分的q值仍然初始化为0，而左下角的值现在为15。代理将总是选择导致15的动作。具体看QTable[(2，9)]看看为什么代理永远不会移动到棋盘的右边。</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es my"><img src="../Images/6a0bf3c3098b188ed08f7eae9449f81d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*473mrgZ0XYQjnJf1oCGc9g.png"/></div></div></figure><pre class="la lb lc ld fd ll lm ln lo aw lp bi"><span id="9b43" class="lq iz hi lm b fi lr ls l lt lu">k: (0, 0), v: {'right': 5.230176601500001, 'up': 6.457008150000001}</span><span id="f145" class="lq iz hi lm b fi mz ls l lt lu">k: (0, 1), v: {'right': 5.811307335000001, 'up': 7.174453500000001, 'down': 5.811307335000001}</span><span id="097d" class="lq iz hi lm b fi mz ls l lt lu">k: (0, 2), v: {'right': 6.457008150000001, 'up': 7.971615000000001, 'down': 6.457008150000001}</span><span id="8419" class="lq iz hi lm b fi mz ls l lt lu">k: (0, 3), v: {'right': 7.174453500000001, 'up': 8.85735, 'down': 7.174453500000001}</span><span id="b9e3" class="lq iz hi lm b fi mz ls l lt lu">k: (0, 4), v: {'right': 7.971615000000001, 'up': 9.8415, 'down': 7.971615000000001}</span><span id="52e7" class="lq iz hi lm b fi mz ls l lt lu">k: (0, 5), v: {'right': 8.85735, 'up': 10.935, 'down': 8.85735}</span><span id="7484" class="lq iz hi lm b fi mz ls l lt lu">k: (0, 6), v: {'right': 9.8415, 'up': 12.15, 'down': 9.8415}</span><span id="e274" class="lq iz hi lm b fi mz ls l lt lu">k: (0, 7), v: {'right': 10.935, 'up': 13.5, 'down': 10.935}</span><span id="be33" class="lq iz hi lm b fi mz ls l lt lu">k: (0, 8), v: {'right': 12.15, 'up': 15.0, 'down': 12.15}</span><span id="af67" class="lq iz hi lm b fi mz ls l lt lu">k: (0, 9), v: {'right': 0, 'down': 0}</span><span id="719e" class="lq iz hi lm b fi mz ls l lt lu">k: (1, 0), v: {'left': 5.811307335000001, 'right': -10.0, 'up': 5.811307335000001}</span><span id="e91c" class="lq iz hi lm b fi mz ls l lt lu">k: (1, 1), v: {'left': 6.457008150000001, 'right': -10.0, 'up': 6.457008149999355, 'down': 5.230176601499478}</span><span id="929d" class="lq iz hi lm b fi mz ls l lt lu">k: (1, 2), v: {'left': 7.174453500000001, 'right': -10.0, 'up': 7.174453499999994, 'down': 5.811307334999995}</span><span id="045f" class="lq iz hi lm b fi mz ls l lt lu">k: (1, 3), v: {'left': 7.971615000000001, 'right': -10.0, 'up': 7.971615, 'down': 6.457008137731096}</span><span id="2bab" class="lq iz hi lm b fi mz ls l lt lu">k: (1, 4), v: {'left': 8.85735, 'right': -10.0, 'up': 8.857349999114264, 'down': 7.174453500000001}</span><span id="6537" class="lq iz hi lm b fi mz ls l lt lu">k: (1, 5), v: {'left': 9.8415, 'right': -9.99999999999999, 'up': 9.841499999901576, 'down': 7.971614999999203}</span><span id="5877" class="lq iz hi lm b fi mz ls l lt lu">k: (1, 6), v: {'left': 10.935, 'right': -9.99999999999999, 'up': 10.93499999999989, 'down': 8.857349999999991}</span><span id="f5a5" class="lq iz hi lm b fi mz ls l lt lu">k: (1, 7), v: {'left': 12.15, 'right': -9.999999999999998, 'up': 12.149999878500001, 'down': 9.841499999998927}</span><span id="e626" class="lq iz hi lm b fi mz ls l lt lu">k: (1, 8), v: {'left': 13.5, 'right': -9.9999999999, 'up': 13.49999999999085, 'down': 10.934999988079628}</span><span id="b883" class="lq iz hi lm b fi mz ls l lt lu">k: (1, 9), v: {'left': 14.99999999999985, 'right': 10.628820000000001, 'down': 12.13784999999989}</span><span id="6a29" class="lq iz hi lm b fi mz ls l lt lu">k: (2, 0), v: {'left': 0, 'right': 0, 'up': 0}<br/> <br/><strong class="lm hj">... (a bunch of zeroes)</strong></span><span id="9742" class="lq iz hi lm b fi mz ls l lt lu">k: (2, 8), v: {'left': 0, 'right': 0, 'up': 0, 'down': 0}</span><span id="7f68" class="lq iz hi lm b fi mz ls l lt lu">k: (2, 9), v: {'left': 13.3528499998785, 'right': 0.0, 'down': 0}</span><span id="440d" class="lq iz hi lm b fi mz ls l lt lu"><strong class="lm hj"> ... (more zeroes)</strong></span><span id="8fb5" class="lq iz hi lm b fi mz ls l lt lu">k: (9, 8), v: {'left': 0, 'up': 0, 'down': 0}</span><span id="3413" class="lq iz hi lm b fi mz ls l lt lu">k: (9, 9), v: {'left': 0, 'down': 0}</span></pre><h2 id="2b9a" class="lq iz hi bd ja lv lw lx je ly lz ma ji kh mb mc jm kl md me jq kp mf mg ju mh bi translated">探索与开发</h2><p id="6a01" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">上述问题是勘探与开发问题的实质。代理可以利用已知的高回报状态，也可以探索更多的状态空间。重要的是在这两种策略之间取得平衡，以找到获得最佳价值的有效途径。</p><p id="c048" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">找到平衡的一种方法是使用<strong class="jy hj">ε贪婪</strong>函数。在一个ε贪婪函数中，你指定一个ε值，<strong class="jy hj"> ε </strong>。该值是一个百分比，表示您希望代理选择随机操作而不是具有最大结果的操作的频率。换句话说，这是你引入的混乱程度。代理人将在其决策的<strong class="jy hj"> ε </strong> % <strong class="jy hj"> </strong>中选择一个随机动作。ε值通常需要一些手动调整来找到最佳拟合。</p><pre class="la lb lc ld fd ll lm ln lo aw lp bi"><span id="dc41" class="lq iz hi lm b fi lr ls l lt lu">EpsilonGreedy(state, ε):<br/>       validActions = Actions(state)<br/>       ε% of the time:<br/>          choose from validActions randomly<br/>       (1-ε)% of the time:<br/>          chose action a from validActions with max(Q(state, a))</span></pre><p id="4aa5" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">让我们考虑一下之前epsilon为0.7的游戏环境(是的，可能有些夸张)。虽然代理可能会首先发现15，但它在70%的情况下会采取随机行动，所以它不会总是移动到那个方块。超过1000000集(是的，也可能是多余的)代理商找到了50集！</p><figure class="la lb lc ld fd le er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es na"><img src="../Images/a32134ae43587d7df1148d8804b0df4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uww-9_mVFA_WvrVqBHTXDA.png"/></div></div></figure><p id="2ffa" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">探索与利用是代理避免陷入发现局部极大值的关键概念。还有其他方法可以平衡这两者。Epsilon greedy只是其中之一。</p><h1 id="a189" class="iy iz hi bd ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv bi translated">结论</h1><p id="ce27" class="pw-post-body-paragraph jw jx hi jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt hb bi translated">在这个例子中，我们已经讨论了很多，但也触及了RL和QLearning中的很多主要主题。现在，您应该能够回到RL的原始定义，以便更完整地理解这个主题。</p><ul class=""><li id="fe76" class="mi mj hi jy b jz ku kd kv kh nb kl nc kp nd kt mn mo mp mq bi translated">形式化RL问题</li><li id="6d49" class="mi mj hi jy b jz mr kd ms kh mt kl mu kp mv kt mn mo mp mq bi translated">引入了探索环境寻找奖励的直觉</li><li id="2aa4" class="mi mj hi jy b jz mr kd ms kh mt kl mu kp mv kt mn mo mp mq bi translated">q learning:q表和q函数</li><li id="f072" class="mi mj hi jy b jz mr kd ms kh mt kl mu kp mv kt mn mo mp mq bi translated">探索与开发</li></ul><p id="4fce" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">一些结束语:</p><p id="fbcb" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><strong class="jy hj">为什么我们不能在这个例子中只使用搜索？</strong></p><p id="83f3" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">嗯……我们可以。这是一个相当小的例子，规则很简单。例如，我们可以添加更多带有奖励或惩罚的单元格。然后，我们可以说只有某些细胞是终态(这意味着在到达终态之前，你可以穿过多个奖励或惩罚细胞)。这将极大地增加可能的解集。随着事情变得越来越复杂，RL被证明比搜索更有效。</p><p id="59ff" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">我能得到密码吗？ <a class="ae ne" href="https://github.com/stschoberg/mazeGame" rel="noopener ugc nofollow" target="_blank">确定</a>。</p><p id="07b4" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><strong class="jy hj">你跳过的QFunction中的那些常数是什么？</strong></p><p id="1d93" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><a class="ae ne" href="https://en.wikipedia.org/wiki/Q-learning#Influence_of_variables" rel="noopener ugc nofollow" target="_blank">维基百科</a>解释的比我好。还有一个有趣的部分是关于更改QTable初始化值如何影响探索。</p><p id="15be" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated"><strong class="jy hj">您是否运行过更复杂、更有趣的例子？</strong>是的。这很难。</p><ol class=""><li id="705a" class="mi mj hi jy b jz ku kd kv kh nb kl nc kp nd kt nf mo mp mq bi translated">终端状态是绿色方块。ε= 0.2。NumEpisodes = 1M。最终得分= 40</li></ol><div class="la lb lc ld fd ab cb"><figure class="ng le nh ni nj nk nl paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><img src="../Images/74cafe00ed207777d456ff6a75545a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*bdL5H7JPIxi2vmgS3SrhDg.png"/></div></figure><figure class="ng le nm ni nj nk nl paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><img src="../Images/2a8348835f86e9cd8de21f5153d3c004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*baLwDDs2TrE311qrX3evYg.png"/></div></figure></div><p id="4e1c" class="pw-post-body-paragraph jw jx hi jy b jz ku kb kc kd kv kf kg kh kw kj kk kl kx kn ko kp ky kr ks kt hb bi translated">更多考验即将到来…</p></div></div>    
</body>
</html>