<html>
<head>
<title>Understanding XGBoost &amp; it’s growing popularity among the ML community</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解XGBoost &amp;它在ML社区中越来越受欢迎</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-xgboost-its-growing-popularity-among-the-ml-community-6f12dc25b44b?source=collection_archive---------16-----------------------#2019-12-09">https://medium.com/analytics-vidhya/understanding-xgboost-its-growing-popularity-among-the-ml-community-6f12dc25b44b?source=collection_archive---------16-----------------------#2019-12-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e35cad13249ed5188caefc0283c8209b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yhE3CBwTrlXcAIvNJNTQiA.png"/></div></div></figure><h1 id="41db" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">简介</strong></h1><p id="fcce" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">梯度推进是最流行的机器学习技术之一，并被广泛撰写。在这个算法的所有实现中，有一个非常突出，并成为了人们谈论的话题，那就是XGBoost，也就是极端的梯度增强。它是由华盛顿大学的陈天琦创建的，作为一个可扩展的树增强系统。</p><p id="6496" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在本文中，我打算讨论XGBoost如何在梯度增强技术的基础上增加特性和改进，使其更具可伸缩性、更高效和更快速。</p><p id="a3ed" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在我们试图理解XGBoost做什么之前，让我们先来理解它的组件，以及它们是如何组合在一起成为XGBoost的。</p><h1 id="128a" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">助推</strong></h1><p id="681c" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">助推遵循了华特·培顿的名言“我们团结起来比单独行动更强大”的原则。Boosting是一种集成学习技术，用于建立弱学习者的集合，这些弱学习者一起预测比单独使用时好得多。简单来说，这些弱学习者从彼此的错误中学习，并呈现一个组合的结果。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/afdac843d6f43da78740cb73b0a5e0de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*I_z_YtmOt6HhIxv5VdoZ2A.jpeg"/></div></figure><p id="6d5d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在图中，W1是弱学习器，它根据复杂的规则预测FW1(x)。下一个弱学习器试图将第一个模型的误差拟合到新模型W2中，新模型W2试图校正先前的预测。重复这个过程，直到达到最高精度。</p><p id="21c0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">弱学习者有各种方法可以纠正其前一个学习者的预测。当我们通过增加(提升)不正确预测的权重来构建新的学习器时，考虑为所有预测分配相等的权重并调整它们的权重。这种技术被称为AdaBoosting，它通过操纵权重来最小化误差。</p><p id="4d4c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">到目前为止，boosting算法的目标是:</p><p id="0994" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">1.建立一个弱学习者并预测</p><p id="7ff3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">2.最小化损失函数(误差)</p><p id="6110" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">3.安装新模型并预测。重复2</p><p id="6a89" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在让我们用梯度下降法尽量减小误差。这是最有效的方法之一，因此在社区中很受欢迎。通过向梯度移动使损失函数最小化来提升学习器使得梯度提升算法。</p><p id="5752" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">微积分警报！让我们看看梯度下降是如何工作的。</p><p id="94a7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对于给定的成本函数，我们需要找到它的最小点。</p><p id="36cb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi">Ө𝑖 = Ө𝑖 — ρ 𝜕𝐽 / 𝜕Ө𝑖</p><p id="056c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们在一个随机点初始化，并向梯度方向移动，这是增长最快的方向。它通过对迭代的参数取函数的一阶导数来计算。梯度的负号是向给定函数的最小值移动。这个过程逐步重复，直到达到最小值。</p><p id="6702" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">ρ称为学习率，也是GradientBoosting算法中的超参数。此参数表示步长的大小。如果学习率太小，要花很长时间才能找到最小值。如果它太大，它可能会一直绕着曲线移动，并且可能无法找到最佳最小值。</p><p id="5241" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这些是构成XGBoost的组件。但是XGBoost和GradientBoosting有什么不同呢？</p><h1 id="9a34" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">并行学习</strong></h1><p id="b170" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">正如我们看到的GradientBoosting，按顺序构建树。这需要很长时间来训练模型。XGBoost通过并行构建树来提高速度。它通过将排序后的数据存储在列已经排序的压缩列(CSC)块中来实现这一点。这允许模型在算法中重用这些块。这些块也可以存储在分布式机器上。这使得它更具可扩展性和时间效率。</p><p id="1d34" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">正规化术语</strong></p><p id="13a9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">XGBoost还在损失函数中增加了一个正则项。</p><p id="5d7b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">目标函数=损失函数+正则项</p><p id="58c7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">正则项控制模型的复杂性，以避免过度拟合。该算法旨在优化该目标函数以提高梯度增强的性能。</p><p id="e046" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">稀疏感知算法</strong></p><p id="bbad" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">XGBoost引入了稀疏感知的分裂查找，用于处理缺失数据或有大量零的数据。它为每个特征x添加一个默认方向。它使用非缺失数据对此方向进行分类。与基本算法相比，这种稀疏感知算法工作得更快。</p><p id="f0cc" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">XGBoost的构建具有可伸缩性和快速性，这也是它在数据科学行业如此受欢迎并被用于寻找商业解决方案的原因。</p><p id="6bee" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">高速缓存感知访问</strong></p><p id="f5ec" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">XGBoost在内存中分配一个内部缓冲区，以小批量方式提取用于计算的梯度统计数据。这优化了内存分配并加快了进程。</p><p id="56e0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">核外计算</strong></p><p id="c61e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">为了使XGBoost成为可伸缩的解决方案，它需要以高效的方式使用所有的硬件资源。为了实现这一点，XGBoost使用磁盘内存上的块结构，一个线程在这些块上执行计算。它使用两种方法来提高效率，称为块压缩和块分片。</p><p id="43cb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">块压缩会根据需要压缩和解压缩列，从而节省内存。块分片交替地将数据分片到磁盘中，由分配给该磁盘的线程读取。</p><h1 id="cb47" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">结论</strong></h1><p id="c171" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">XGBoost改进了GradientBoosting算法，使其在不牺牲性能的情况下更加灵活、高效和快速。这是业界认为它是最佳机器学习算法之一的有力竞争者的关键原因。</p></div></div>    
</body>
</html>