<html>
<head>
<title>Introduction to Text Classification in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的文本分类介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-text-classification-in-python-659eccf6b2e?source=collection_archive---------3-----------------------#2019-09-04">https://medium.com/analytics-vidhya/introduction-to-text-classification-in-python-659eccf6b2e?source=collection_archive---------3-----------------------#2019-09-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/94a2a13a35d8b323941da3a51f3f19c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4f1QDndGSZiGRFow"/></div></div></figure><p id="3d5c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">被困在付费墙后面？点击 <a class="ae jp" rel="noopener" href="/analytics-vidhya/introduction-to-text-classification-in-python-659eccf6b2e?source=friends_link&amp;sk=53c909794e63b2799ebef9c556509b04"> <em class="jo">这里</em> </a> <em class="jo">阅读全文与我的朋友链接。</em></p></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><p id="8d34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">自然语言处理(NLP) </strong>是一个巨大且不断发展的领域，具有无数的应用，如情感分析、命名实体识别(NER)、文本分类等。</p><p id="7220" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文旨在成为使用Python进行基本文本分类的初学者指南。作为先决条件，Python的基本机器学习经验更好，因为我们不会讨论常用的库、数据结构和其他Python功能。</p><p id="e5b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将使用来自Kaggle的<a class="ae jp" href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank">新闻类别数据集</a>。使用的内核在这里可以得到<a class="ae jp" href="https://www.kaggle.com/siddhantsadangi/classification-using-linearsvc-val-acc-64" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="6510" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们直接跳进来吧！</p><h2 id="1e16" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">首先，导入库…</h2><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="00c2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">自然语言工具包(NLTK) </strong>是Python中NLP的主干。它提供了各种文本处理功能和语料库，使任何数据科学家的工作变得容易得多！在这里找到官方文档<a class="ae jp" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="f896" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">计数矢量器</strong>将语料库转换成一种叫做<strong class="is hj">单词袋(BoW) </strong>的东西。对于机器学习算法来说，这是表示文本数据的最简单方法之一。它基本上将语料库中的所有单词放在一起，并创建一个矩阵，其中包含语料库的每个文档(或者在我们的例子中，每个新闻故事)中每个单词的计数。来自<a class="ae jp" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">官方文件</a>的一个例子:</p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="ce29" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，vectorizer.get_feature_names()为我们提供了单词包，即语料库中所有不同的单词。该矩阵可以被形象化为:</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ky"><img src="../Images/16ac2cb9fbf2acb5f8893d116f02b0eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SRLEQ-_MI4XhFhvyyugySA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">计数矢量器</figcaption></figure><p id="ed25" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它被称为“单词包”，因为它将所有的单词放在一起，而不考虑它们在文档中的位置。在这个方法中，“这是第一个文档”和“第一个是这个文档”将具有相同的表示。有一些方法会考虑单词的位置，但我们不会在本文中讨论这些方法。</p><p id="f314" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">CountVectorizer()函数中的一个重要参数是“ngram_range”。“n元语法”最简单的定义是“n”个单词的序列。例如，双字母组合意味着两个单词的序列。ngram_range指定了将从语料库中提取的这个范围的边界。例如，对于(1，2)的ngram_range，我们将提取所有的uni和bi-gram。</p><p id="362d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们使用(1，2): <br/> 'This '，' is '，' the '，' first '，' document '，' This is '，' is the '，' first '，' first document '，' This is '，' is the '，' first '，' first document '的ngram范围，这就是句子“这是第一个文档”的标记化方式。</p><p id="4ef9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用较大范围的优势在于，它们有助于模型从文本序列中学习，从而提高模型的准确性。如果只使用单字，这些信息就会丢失。代价是增加了特征空间，从而增加了所需的时间和计算能力。请注意，句子“这是第一个文档”将减少到只有5个单字元，但5+4=9个双字元，5+4+3=12个三字元。很少使用大于3的ngram_range。</p><h2 id="7875" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">然后，我们将数据集加载到熊猫数据框架中:</h2><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="810e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">一些探索性的数据分析(EDA)对数据:</strong></p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="ddee" class="jx jy hi le b fi li lj l lk ll">df.head()</span></pre><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/684087b5de2f96923ec53dfaa34726a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JMZ5MG-7fA4nHLq5VGLzwQ.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">新闻类别数据集</figcaption></figure><p id="945d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“类别”列将是我们的目标列，从现在起，我们将只使用“标题”和“简短描述”列作为我们的功能</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="9303" class="jx jy hi le b fi li lj l lk ll">df.info()</span><span id="df02" class="jx jy hi le b fi ln lj l lk ll">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 200853 entries, 0 to 200852<br/>Data columns (total 6 columns):<br/>category             200853 non-null object<br/>headline             200853 non-null object<br/>authors              200853 non-null object<br/>link                 200853 non-null object<br/>short_description    200853 non-null object<br/>date                 200853 non-null datetime64[ns]<br/>dtypes: datetime64[ns](1), object(5)<br/>memory usage: 9.2+ MB</span></pre><p id="f323" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个数据集中没有空值，这很好。然而，对于真实世界的数据集来说，这种情况并不常见，空值需要作为预处理的一部分进行处理，要么删除空行，要么用空行(“”)替换它们。</p><p id="3bbb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在让我们看看数据集中不同的类别…</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="053f" class="jx jy hi le b fi li lj l lk ll">labels = list(df.category.unique())<br/>labels.sort()<br/>print(labels)</span><span id="39cd" class="jx jy hi le b fi ln lj l lk ll">['ARTS', 'ARTS &amp; CULTURE', 'BLACK VOICES', 'BUSINESS', 'COLLEGE', 'COMEDY', 'CRIME', 'CULTURE &amp; ARTS', 'DIVORCE', 'EDUCATION', 'ENTERTAINMENT', 'ENVIRONMENT', 'FIFTY', 'FOOD &amp; DRINK', 'GOOD NEWS', 'GREEN', 'HEALTHY LIVING', 'HOME &amp; LIVING', 'IMPACT', 'LATINO VOICES', 'MEDIA', 'MONEY', 'PARENTING', 'PARENTS', 'POLITICS', 'QUEER VOICES', 'RELIGION', 'SCIENCE', 'SPORTS', 'STYLE', 'STYLE &amp; BEAUTY', 'TASTE', 'TECH', 'THE WORLDPOST', 'TRAVEL', 'WEDDINGS', 'WEIRD NEWS', 'WELLNESS', 'WOMEN', 'WORLD NEWS', 'WORLDPOST']</span></pre><p id="b610" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们看到有几个类别可以合并在一起，如“艺术”、“艺术与文化”和“文化与艺术”。让我们这样做:</p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/de692c5e5740f31d8a0158f4ed6d9051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3P8xslJvPX7wlvC7nsIiZA.jpeg"/></div></div></figure><p id="07c1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个看起来更好。我们已经将标签的数量从41个减少到36个。此外，我们看到数据集非常不平衡。我们有大约35000条政治新闻，但是不到1000条教育新闻(基本上也概括了当前的事态)。我们通常希望一个平衡的数据集来训练我们的模型，但是现实世界中的大多数数据集几乎永远不会平衡。有一些扩充和采样技术可以用来平衡数据集，但是这些不在本文的讨论范围之内。</p><p id="ef3c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">现在到了预处理，到目前为止最重要的一步！</strong></p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="78fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是我使用的标准文本预处理用户定义函数(UDF)。让我们详细讨论一下。</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="edb4" class="jx jy hi le b fi li lj l lk ll">lower = col.apply(str.lower)</span></pre><p id="de78" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这会将语料库转换为小写，因为否则CountVectorizer会将“hello”、“hElLo”和“HELLO”视为不同的单词，这不是一个好主意。</p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="2d55" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这将从语料库中删除HTML标签。如果语料库是从网页上搜集的，这是非常重要的。BeautifulSoup库提供了一种更精确的方法。你可以点击查看<a class="ae jp" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="1b7b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">词干化</strong>‘是产生词根/基本词的形态变体的过程’。词干算法将单词“chocolate”、“chocolatey”、“choco”简化为词根，“chocolate”和“retrieve”，“retrieved”，“retrieved”简化为词干“retrieve”。词干法的工作原理是去掉一个单词的尾随字符，以“尝试”到达词根。因此，词根可能不是字典中的单词。词干提取的主要优点是减少了特征空间，即减少了语料库中用于训练模型的不同单词的数量。到达词根的另一种方法是<strong class="is hj">词汇化</strong>。与词干化不同，词汇化遵循一种基于词典的方法，因此单词通常被简化为它们实际的词典词根。这样做的代价是处理速度。点击了解更多关于词干化和词汇化的信息<a class="ae jp" href="https://www.datacamp.com/community/tutorials/stemming-lemmatization-python" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="1308" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">停用词</strong>是常用词，通常不会给数据增加太多意义。从语料库中移除停用词，因为这将显著减小特征空间的大小。但是，停用词不能盲目使用。NLTK停用词语料库中的一些词可能在数据集中有意义。例如，您不希望从正在进行情感分析的语料库中删除单词“not”(这是一个NLTK停用词)。这样做将导致类似“这是一部好电影”和“这不是一部好电影”的句子意思相同。</p><p id="ad14" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们的例子中，删除停用词提高了模型性能，所以我们将继续这样做。</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="8b4d" class="jx jy hi le b fi li lj l lk ll">rem_lngth1 = rem_num.apply(lambda x: re.sub(r'[^\w\s]',' ',x))</span></pre><p id="e975" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里我们删除了所有长度为1的单词，因为它们通常不会给语料库增加意义。像“a”这样的词将被删除。你可能会问，还有哪些单词的长度是1？还记得之前我们用空格代替标点符号吗？这将像“约翰·多伊的内核”一样转化为“约翰·多伊的内核”。这里的s没有增加任何意义。现在，在我们删除长度为1的单词后，我们将剩下“John Doe kernel”。所以除非所有权在你的内核中很重要，否则这是一件好事。</p><p id="3fe6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们在python re模块中使用<strong class="is hj">正则表达式(regex) </strong>来实现这一点。正则表达式在NLP中广泛用于各种任务，如信息提取(电子邮件地址、电话号码、邮政编码等。)，数据清理等。关于正则表达式的官方python文档是让你熟悉正则表达式的好地方。</p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="9cc4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这可能有点难以理解。让我们更深入地了解一下这个问题。</p><p id="ae46" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">h_pct是我们想要移除的语料库中最频繁出现的单词的百分比。l_pct是我们要删除的最不常用单词的百分比。</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="e6f8" class="jx jy hi le b fi li lj l lk ll">counts = pd.Series(''.join(df.short_description).split()).value_counts()<br/>counts</span><span id="da43" class="jx jy hi le b fi ln lj l lk ll">the                166126<br/>to                 111620<br/>of                  95175<br/>a                   94604<br/>and                 89678<br/>                    ...  <br/>catch!"                 1<br/>big-day                 1<br/>incarcerates            1<br/>323-square-foot         1<br/>co-trustee,             1<br/>Length: 208227, dtype: int64</span></pre><p id="90f2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些是所有单词在我们的数据集中出现的次数。我们的数据集有208227个不同的单词，因此h_pct为1.0将删除语料库中最常用的前1%的单词。</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="4374" class="jx jy hi le b fi li lj l lk ll">high_freq = counts[:int(pd.Series(''.join(df.short_description).split()).count()*1/100)]<br/>high_freq</span><span id="c9d5" class="jx jy hi le b fi ln lj l lk ll">the            166126<br/>to             111620<br/>of              95175<br/>a               94604<br/>and             89678<br/>                ...  <br/>butternut           5<br/>NGO                 5<br/>Mary,               5<br/>songwriter,         5<br/>distracted,         5<br/>Length: 39624, dtype: int64</span></pre><p id="2bdb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些是将从数据集中删除的最常见的单词。</p><p id="dc9f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这样做背后的直觉是，由于这些单词如此常见，我们希望它们分布在多个不相关的文档中(或者在我们的例子中是新闻)，因此，它们在对文本进行分类时没有多大用处。</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="4c13" class="jx jy hi le b fi li lj l lk ll">low_freq = counts[:-int(pd.Series(''.join(df.short_description).split()).count()*1/100):-1]<br/>low_freq</span><span id="a7dd" class="jx jy hi le b fi ln lj l lk ll">co-trustee,        1<br/>323-square-foot    1<br/>incarcerates       1<br/>big-day            1<br/>catch!"            1<br/>                  ..<br/>Brie.              1<br/>non-plant          1<br/>fetus?             1<br/>Techtopus”         1<br/>Look).             1<br/>Length: 39623, dtype: int64</span></pre><p id="3a4a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些是最不常用的前1%的单词。所有这些单词在词汇表中只出现一次，因此没有多大意义，可以删除。</p><p id="d89d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">和停用词一样，没有固定数量的词可以被删除。这取决于你的语料库，你应该尝试最适合你的不同值。这正是我接下来要做的。</p><p id="91d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">检查最佳h_pct和l_pct组合</strong></p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="2f8e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这就找到了h_pct和l_pct的最佳值。我们从0到10的整数值开始，根据结果，可以将百分比进一步调整到0.5%的步长。请记住，这是非常耗时的。该模型将被训练i*j次，其中I是h_pct值的数量，j是l_pct值的数量。所以对于0到10(包括0和10)之间的h_pct和l_pct值，我的模型总共被训练了121次。</p><p id="d709" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对我来说，第一次迭代分别为h_pct和l_pct返回0.0和1.0的值，下面是运行h_pct的0.0到0.5%和l_pct的0.5到1.5%之间的值时的结果:</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="6287" class="jx jy hi le b fi li lj l lk ll">SVC max: 63.79560061555173%, pct:0.0|1.0</span></pre><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/d69abfbbb062002bb13e997a3c2e3d86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c4Tw98oHPs8xUz02HlYfPA.png"/></div></div></figure><p id="f92b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们看到h_pct和l_pct的最佳值仍然分别是0.0和1.0。我们将继续使用这些值。</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="2810" class="jx jy hi le b fi li lj l lk ll">df.loc[df.short_description.str.len()==df.short_description.str.len().max()]</span></pre><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/d4a3dbece94c5fbe7b777cda6c73ae21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*PsMERnrsfLfP8TIVg0RufA.png"/></div></figure><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="c175" class="jx jy hi le b fi li lj l lk ll">df.loc[58142]['short_description']</span><span id="7c20" class="jx jy hi le b fi ln lj l lk ll">'This week the nation watched as the #NeverTrump movement folded faster than one of the presumptive nominee\'s beachfront developments. As many tried to explain away Trump\'s reckless, racist extremism, a few put principle over party. The wife of former Republican Senator Bob Bennett, who died on May 4, revealed that her husband spent his dying hours reaching out to Muslims. "He would go to people with the hijab [on] and tell them he was glad they were in America," she told the Daily Beast. "He wanted to apologize on behalf of the Republican Party." In the U.K., Prime Minister David Cameron called Trump\'s proposal to ban Muslims from entering the U.S., "divisive, stupid and wrong." Trump\'s reply was that he didn\'t think he and Cameron would "have a very good relationship." The press is also doing its part to whitewash extremism. The New York Times called Trump\'s racism "a reductive approach to ethnicity," and said Trump\'s attitude toward women is "complex" and "defies simple categorization," as if sexism is suddenly as complicated as string theory. Not everybody\'s going along. Bob Garfield, co-host of "On the Media," warned the press of the danger of normalizing Trump. "Every interview with Donald Trump, every single one should hold him accountable for bigotry, incitement, juvenile conduct and blithe contempt for the Constitution," he said. "The voters will do what the voters will do, but it must not be, cannot be because the press did not do enough."'</span></pre><p id="644e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是我们数据集中最长的故事。我们将以此为参考，看看我们的预处理函数是如何工作的。</p><p id="94e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">模型构建</strong></p><figure class="ks kt ku kv fd ij"><div class="bz dy l di"><div class="kw kx l"/></div></figure><p id="2128" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个函数将数据帧、h_pct和l_pct的值、模型和详细度标志作为输入，并返回预测、模型精度和训练模型。</p><p id="c359" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们分析一下组件:</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="095e" class="jx jy hi le b fi li lj l lk ll">df['short_description_processed'] = preprocessing(df['short_description'],h_pct,l_pct)<br/>df['concatenated'] = df['headline'] + '\n' + df['short_description_processed']<br/>df['concat_processed'] = preprocessing(df['concatenated'],0,0)</span></pre><p id="a1cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们使用h_pct和l_pct的out值对“short_description”列运行预处理函数，并将结果存储在“short_description_processed”中。然后，我们将“标题”添加到该列，并将结果存储在“连接”中。<br/>最后，我们对“concatenated”再次运行预处理函数，但这次不删除任何频繁和不频繁的单词，并将结果存储在“concat_processed”中。不从这个集合中删除单词是一种增加出现在标题中的单词比出现在故事中的单词多的方法。</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="76f7" class="jx jy hi le b fi li lj l lk ll">X = df['concat_processed']<br/>y = df['category']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y) <br/>    <br/>bow_xtrain = bow.fit_transform(X_train)<br/>bow_xtest = bow.transform(X_test)</span></pre><p id="7f54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用' concat_processed '作为我们的变量列，使用' category '作为我们的目标。</p><p id="e07d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，我们使用CountVectorizer的bow对象分别为训练和测试语料生成词袋。根据经验，CountVectorizer适用于训练集并在其上进行转换，但仅在测试集上进行转换。这样模型就不会从测试集中学到任何东西。</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="a609" class="jx jy hi le b fi li lj l lk ll">model.fit(bow_xtrain,y_train)<br/>preds = model.predict(bow_xtest)</span></pre><p id="89ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在训练弓上训练模型，并为测试弓生成预测。</p><p id="cb6b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">拼凑</strong></p><p id="2c9b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在数据帧上运行prep_fit_pred函数，将0和1作为h_pct和l_pct(如前所述)，并使用<a class="ae jp" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html" rel="noopener ugc nofollow" target="_blank"> LinearSVC() </a>模块，将verbose设为True。</p><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="5d94" class="jx jy hi le b fi li lj l lk ll">preds_abc, acc_abc, abc = prep_fit_pred(df, 0, 1, LinearSVC(), verbose=True)</span><span id="00a8" class="jx jy hi le b fi ln lj l lk ll">Number of words in corpus before processing: 3985816<br/>Number of words in corpus after processing: 2192635 (55.0%)<br/>Number of words in final corpus: 3498319 (88.0%)</span><span id="108b" class="jx jy hi le b fi ln lj l lk ll">Raw story:<br/>This week the nation watched as the #NeverTrump movement folded faster than one of the presumptive nominee's beachfront developments. As many tried to explain away Trump's reckless, racist extremism, a few put principle over party. The wife of former Republican Senator Bob Bennett, who died on May 4, revealed that her husband spent his dying hours reaching out to Muslims. "He would go to people with the hijab [on] and tell them he was glad they were in America," she told the Daily Beast. "He wanted to apologize on behalf of the Republican Party." In the U.K., Prime Minister David Cameron called Trump's proposal to ban Muslims from entering the U.S., "divisive, stupid and wrong." Trump's reply was that he didn't think he and Cameron would "have a very good relationship." The press is also doing its part to whitewash extremism. The New York Times called Trump's racism "a reductive approach to ethnicity," and said Trump's attitude toward women is "complex" and "defies simple categorization," as if sexism is suddenly as complicated as string theory. Not everybody's going along. Bob Garfield, co-host of "On the Media," warned the press of the danger of normalizing Trump. "Every interview with Donald Trump, every single one should hold him accountable for bigotry, incitement, juvenile conduct and blithe contempt for the Constitution," he said. "The voters will do what the voters will do, but it must not be, cannot be because the press did not do enough."</span><span id="e65a" class="jx jy hi le b fi ln lj l lk ll">Processed story:<br/>week nation watch nevertrump movement fold faster one presumpt nomine beachfront developments mani tri explain away trump reckless racist extremism put principl party wife former republican senat bob bennett die may reveal husband spent die hour reach muslims would go peopl hijab tell glad america told daili beast want apolog behalf republican party u k prime minist david cameron call trump propos ban muslim enter u divisive stupid wrong trump repli think cameron would veri good relationship press also part whitewash extremism new york time call trump racism reduct approach ethnicity said trump attitud toward women complex defi simpl categorization sexism sudden complic string theory everybodi go along bob garfield co host media warn press danger normal trump everi interview donald trump everi singl one hold account bigotry incitement juvenil conduct blith contempt constitution said voter voter must cannot becaus press enough</span><span id="0768" class="jx jy hi le b fi ln lj l lk ll">Adding additional columns to story:<br/>Sunday Roundup<br/>week nation watch nevertrump movement fold faster one presumpt nomine beachfront developments mani tri explain away trump reckless racist extremism put principl party wife former republican senat bob bennett die may reveal husband spent die hour reach muslims would go peopl hijab tell glad america told daili beast want apolog behalf republican party u k prime minist david cameron call trump propos ban muslim enter u divisive stupid wrong trump repli think cameron would veri good relationship press also part whitewash extremism new york time call trump racism reduct approach ethnicity said trump attitud toward women complex defi simpl categorization sexism sudden complic string theory everybodi go along bob garfield co host media warn press danger normal trump everi interview donald trump everi singl one hold account bigotry incitement juvenil conduct blith contempt constitution said voter voter must cannot becaus press enough</span><span id="5083" class="jx jy hi le b fi ln lj l lk ll">Final story:<br/>sunday roundup week nation watch nevertrump movement fold faster one presumpt nomin beachfront develop mani tri explain away trump reckless racist extrem put principl parti wife former republican senat bob bennett die may reveal husband spent die hour reach muslim would go peopl hijab tell glad america told daili beast want apolog behalf republican parti u k prime minist david cameron call trump propo ban muslim enter u divis stupid wrong trump repli think cameron would veri good relationship press also part whitewash extrem new york time call trump racism reduct approach ethnic said trump attitud toward women complex defi simpl categor sexism sudden complic string theori everybodi go along bob garfield co host media warn press danger normal trump everi interview donald trump everi singl one hold account bigotri incit juvenil conduct blith contempt constitut said voter voter must cannot becaus press enough</span><span id="2fda" class="jx jy hi le b fi ln lj l lk ll">Predicted class: POLITICS<br/>Actual class: POLITICS</span></pre><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/570c010134884c3eb959c129ff7fe5c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z-x0LtJn2VeabiNPnfrKmA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">混淆矩阵</figcaption></figure><pre class="ks kt ku kv fd ld le lf lg aw lh bi"><span id="5ef3" class="jx jy hi le b fi li lj l lk ll">precision    recall  f1-score   support<br/><br/>ARTS &amp; CULTURE       0.56      0.47      0.51      1280<br/>  BLACK VOICES       0.59      0.40      0.48      1494<br/>      BUSINESS       0.51      0.48      0.49      1959<br/>       COLLEGE       0.48      0.42      0.45       377<br/>        COMEDY       0.48      0.43      0.45      1708<br/>         CRIME       0.57      0.59      0.58      1124<br/>       DIVORCE       0.85      0.72      0.78      1131<br/>     EDUCATION       0.43      0.31      0.36       331<br/> ENTERTAINMENT       0.64      0.75      0.69      5299<br/>   ENVIRONMENT       0.67      0.26      0.37       437<br/>         FIFTY       0.37      0.15      0.22       462<br/>  FOOD &amp; DRINK       0.64      0.73      0.68      2055<br/>     GOOD NEWS       0.40      0.20      0.27       461<br/>         GREEN       0.41      0.37      0.39       865<br/>HEALTHY LIVING       0.35      0.33      0.34      2209<br/> HOME &amp; LIVING       0.75      0.72      0.73      1384<br/>        IMPACT       0.44      0.26      0.33      1141<br/> LATINO VOICES       0.66      0.29      0.40       373<br/>         MEDIA       0.55      0.46      0.50       929<br/>         MONEY       0.56      0.32      0.41       563<br/>     PARENTING       0.66      0.76      0.71      4169<br/>      POLITICS       0.71      0.84      0.77     10804<br/>  QUEER VOICES       0.79      0.69      0.74      2084<br/>      RELIGION       0.55      0.50      0.53       843<br/>       SCIENCE       0.59      0.47      0.53       719<br/>        SPORTS       0.68      0.74      0.71      1612<br/>STYLE &amp; BEAUTY       0.78      0.81      0.80      3928<br/>         TASTE       0.37      0.16      0.22       692<br/>          TECH       0.58      0.41      0.48       687<br/>        TRAVEL       0.69      0.76      0.73      3263<br/>      WEDDINGS       0.80      0.78      0.79      1205<br/>    WEIRD NEWS       0.41      0.26      0.32       881<br/>      WELLNESS       0.63      0.74      0.68      5883<br/>         WOMEN       0.41      0.29      0.34      1152<br/>    WORLD NEWS       0.51      0.17      0.26       718<br/>     WORLDPOST       0.56      0.59      0.57      2060<br/><br/>      accuracy                           0.64     66282<br/>     macro avg       0.57      0.49      0.52     66282<br/>  weighted avg       0.63      0.64      0.62     66282<br/><br/>Accuracy: 63.83%</span></pre><p id="8077" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“short_description”列有3985816个单词。在应用预处理函数后，这个值减少到2192635(减少了45%)。添加标题并再次运行预处理后，最终的语料库有3498319个单词。</p><p id="e867" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到最长的故事的原始版本和最终版本。你可以看到最终的版本全部是小写的，没有标点符号，符号和数字，并且由于删除了单词而变得相当短。</p><p id="7731" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">分类报告显示，F1得分最低的是“五十”和“品味”,这两个品牌的新闻故事数量都很少，而最高的是“风格和美感”,这两个品牌的新闻故事数量很多。</p><p id="f546" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">平均准确率为63.83%。虽然这看起来并不吸引人，但对于36个标签来说，随机猜测的准确率只有2.78%。所以我们的模型比随机猜测要好23倍。这样听起来好多了！😄</p><p id="ecc0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这就到此为止(短？)引言。希望你现在能够开始你自己的文本分类项目。</p><p id="f168" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您的下一步应该是尝试不同的模型(只需将不同的模型传递给prep_fit_pred函数就可以轻松完成)，探索和试验预处理步骤，特征工程(故事的长度和它的标签之间有什么关系吗？)，并更详细地解释为什么大约40%的故事被错误分类(提示:20%的教育故事被归类为政治)。</p><p id="d8c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦你对基础知识有了信心，你可能会想要遵循一些顶级Kagglers在他们的NLP提交中使用的一些技术。Neptune . ai的好朋友们已经帮你搞定了。</p><div class="ls lt ez fb lu lv"><a href="https://neptune.ai/blog/text-classification-tips-and-tricks-kaggle-competitions" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab dw"><div class="lx ab ly cl cj lz"><h2 class="bd hj fi z dy ma ea eb mb ed ef hh bi translated">文本分类:来自5场Kaggle比赛的所有提示和技巧</h2><div class="mc l"><h3 class="bd b fi z dy ma ea eb mb ed ef dx translated">在这篇文章中，我将讨论一些伟大的提示和技巧，以提高您的文本分类的性能…</h3></div><div class="md l"><p class="bd b fp z dy ma ea eb mb ed ef dx translated">海王星. ai</p></div></div><div class="me l"><div class="mf l mg mh mi me mj io lv"/></div></div></a></div></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><p id="5bb1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">正如开头已经提到的，Kaggle上既有</em> <a class="ae jp" href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank"> <em class="jo">数据集</em> </a> <em class="jo">又有</em> <a class="ae jp" href="https://www.kaggle.com/siddhantsadangi/classification-using-linearsvc-val-acc-64" rel="noopener ugc nofollow" target="_blank"> <em class="jo">代码</em> </a> <em class="jo">。如果你想在本地或Google Colab上尝试一下，也可以在GitHub</em><a class="ae jp" href="https://github.com/SiddhantSadangi/TextClassification" rel="noopener ugc nofollow" target="_blank"><em class="jo">上找到它们。</em></a></p><p id="beb3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">谢谢你坚持到现在。任何反馈都将非常受欢迎！</p><p id="f246" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">你可以通过</em><a class="ae jp" href="mailto:siddhant.sadangi@gmail.com" rel="noopener ugc nofollow" target="_blank"><em class="jo">siddhant.sadangi@gmail.com、</em> </a> <em class="jo">联系我和/或通过</em><a class="ae jp" href="https://www.linkedin.com/in/siddhantsadangi" rel="noopener ugc nofollow" target="_blank"><em class="jo">LinkedIn</em></a><em class="jo">联系我。</em></p></div><div class="ab cl jq jr gp js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="hb hc hd he hf"><p id="ca82" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Medium仍然不支持向印度以外的作者支付费用。如果你喜欢我的内容，你可以给我买杯咖啡:)</p><div class="ls lt ez fb lu lv"><a href="https://www.buymeacoffee.com/siddhantsadangi" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab dw"><div class="lx ab ly cl cj lz"><h2 class="bd hj fi z dy ma ea eb mb ed ef hh bi translated">Siddhant Sadangi正在Streamlit上创建python网络应用程序</h2><div class="mc l"><h3 class="bd b fi z dy ma ea eb mb ed ef dx translated">嘿👋我刚刚在这里创建了一个页面。你现在可以给我买杯咖啡了！</h3></div><div class="md l"><p class="bd b fp z dy ma ea eb mb ed ef dx translated">www.buymeacoffee.com</p></div></div><div class="me l"><div class="mk l mg mh mi me mj io lv"/></div></div></a></div></div></div>    
</body>
</html>