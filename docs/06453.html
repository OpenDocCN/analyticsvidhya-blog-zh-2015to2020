<html>
<head>
<title>AI Writing Poems: Building LSTM model using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">è‰¾å†™è¯—:ç”¨ç«ç‚¬æ„ç­‘æ¨¡å¼</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/analytics-vidhya/ai-writing-poems-building-lstm-model-using-pytorch-d1c58a24bb64?source=collection_archive---------8-----------------------#2020-05-23">https://medium.com/analytics-vidhya/ai-writing-poems-building-lstm-model-using-pytorch-d1c58a24bb64?source=collection_archive---------8-----------------------#2020-05-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/948265820c2fc9979798748bebc50d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NTIyHs63RZrfflgElCVQLw.png"/></div></div></figure><div class=""/><p id="ff50" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">å¤§å®¶å¥½ï¼ï¼åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨PyTorchæ„å»ºä¸€ä¸ªæ¨¡å‹æ¥é¢„æµ‹æ®µè½ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†äº†è§£RNNå’ŒLSTMï¼Œä»¥åŠä»–ä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ç„¶åæˆ‘ä»¬å°†åˆ›å»ºæˆ‘ä»¬çš„æ¨¡å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åŠ è½½æ•°æ®å¹¶å¯¹å…¶è¿›è¡Œé¢„å¤„ç†ã€‚ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨PyTorchæ¥è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜å®ƒã€‚åœ¨é‚£ä¹‹åï¼Œæˆ‘ä»¬å°†é€šè¿‡ç»™å®ƒä¸€ä¸ªèµ·å§‹æ–‡æœ¬æ¥ä»è¯¥æ¨¡å‹ä¸­è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥ç”Ÿæˆå®Œæ•´çš„æ®µè½ã€‚</p><h1 id="a54e" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">ä»€ä¹ˆæ˜¯RNNï¼Ÿ</h1><p id="b99f" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œåƒå¯¹ç‹—æˆ–çŒ«çš„å›¾åƒè¿›è¡Œåˆ†ç±»è¿™æ ·çš„ç®€å•é—®é¢˜å¯ä»¥é€šè¿‡ç®€å•åœ°å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒå¹¶ä½¿ç”¨åˆ†ç±»å™¨æ¥è§£å†³ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬çš„é—®é¢˜æ›´å¤æ‚ï¼Œæ¯”å¦‚æˆ‘ä»¬å¿…é¡»é¢„æµ‹æ®µè½ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬ä»”ç»†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¼šå‘ç°ï¼Œæˆ‘ä»¬äººç±»ä¸èƒ½ä»…ä»…é€šè¿‡ä½¿ç”¨æˆ‘ä»¬ç°æœ‰çš„è¯­è¨€å’Œè¯­æ³•çŸ¥è¯†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨è¿™ç§ç±»å‹çš„é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ®µè½çš„å‰ä¸€ä¸ªå•è¯å’Œæ®µè½çš„ä¸Šä¸‹æ–‡æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚</p><p id="dc18" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œæ— æ³•åšåˆ°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åœ¨å›ºå®šçš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œç„¶åç”¨æ¥è¿›è¡Œé¢„æµ‹ã€‚RNNè¢«ç”¨æ¥è§£å†³è¿™ç±»é—®é¢˜ã€‚RNNä»£è¡¨<strong class="is hu">é€’å½’ç¥ç»ç½‘ç»œ</strong>ã€‚æˆ‘ä»¬å¯ä»¥æŠŠRNNæƒ³è±¡æˆä¸€ä¸ªæœ‰å›è·¯çš„ç¥ç»ç½‘ç»œã€‚å®ƒå°†ä¸€ä¸ªçŠ¶æ€çš„ä¿¡æ¯ä¼ é€’ç»™ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚å› æ­¤ï¼Œä¿¡æ¯åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­æŒç»­å­˜åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å®ƒæ¥ç†è§£ä¹‹å‰çš„èƒŒæ™¯ï¼Œå¹¶åšå‡ºå‡†ç¡®çš„é¢„æµ‹ã€‚</p><h1 id="2aeb" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">ä»€ä¹ˆæ˜¯LSTMï¼Ÿ</h1><p id="3978" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬é€šè¿‡ä½¿ç”¨RNNæ¥è§£å†³æ•°æ®åºåˆ—çš„é—®é¢˜ï¼Œè€Œä»¥å‰çš„ä¸Šä¸‹æ–‡æ˜¯è¢«ä½¿ç”¨çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸ºä»€ä¹ˆéœ€è¦LSTMå‘¢ï¼Ÿè¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»çœ‹çœ‹è¿™ä¸¤ä¸ªä¾‹å­ã€‚</p><p id="fc51" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">ä¾‹ä¸€</strong>:â€œé¸Ÿä½åœ¨<em class="kr">å·¢</em>ã€‚â€è¿™é‡Œå¾ˆå®¹æ˜“é¢„æµ‹å•è¯â€œnest â€,å› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº†birdçš„ä¸Šä¸‹æ–‡ï¼ŒRNNåœ¨è¿™ç§æƒ…å†µä¸‹ä¼šå·¥ä½œå¾—å¾ˆå¥½ã€‚</p><p id="2471" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">ä¾‹2: </strong></p><p id="1827" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">â€œæˆ‘åœ¨å°åº¦é•¿å¤§â€¦â€¦..æ‰€ä»¥æˆ‘ä¼šè¯´H <em class="kr"> indi </em>â€æ‰€ä»¥è¿™é‡Œé¢„æµ‹å•è¯â€œHindiâ€çš„ä»»åŠ¡å¯¹äºä¸€ä¸ªRNNäººæ¥è¯´æ˜¯å›°éš¾çš„ï¼Œå› ä¸ºè¿™é‡Œè¯­å¢ƒä¹‹é—´çš„å·®è·å¾ˆå¤§ã€‚é€šè¿‡çœ‹â€œæˆ‘èƒ½è¯´â€¦â€¦â€è¿™ä¸€è¡Œï¼Œæˆ‘ä»¬æ— æ³•é¢„æµ‹è¯­è¨€ï¼Œæˆ‘ä»¬å°†éœ€è¦é¢å¤–çš„å°åº¦èƒŒæ™¯ã€‚æ‰€ä»¥åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦å¯¹æˆ‘ä»¬çš„æ®µè½æœ‰ä¸€äº›é•¿æœŸçš„ä¾èµ–ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½ç†è§£ä¸Šä¸‹æ–‡ã€‚</p><p id="f754" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨LSTM( <strong class="is hu">é•¿çŸ­æœŸè®°å¿†</strong>)ã€‚é¡¾åæ€ä¹‰ï¼Œå®ƒä»¬æœ‰é•¿æ—¶è®°å¿†å’ŒçŸ­æ—¶è®°å¿†(gate ),ä¸¤è€…éƒ½ç”¨åœ¨è¿è¯ä¸­æ¥åšé¢„æµ‹ã€‚å¦‚æœæˆ‘ä»¬è°ˆè®ºLSTMçš„å»ºç­‘ï¼Œå®ƒä»¬åŒ…å«4ä¸ªé—¨ï¼Œå³å­¦ä¹ é—¨ã€å¿˜è®°é—¨ã€è®°å¿†é—¨ã€ä½¿ç”¨é—¨ã€‚ä¸ºäº†è®©è¿™ç¯‡æ–‡ç« ç®€å•æ˜“æ‡‚ï¼Œæˆ‘ä¸æ‰“ç®—æ·±å…¥LSTMçš„å»ºç­‘ç†è®ºã€‚ä½†æ˜¯ä¹Ÿè®¸æˆ‘ä»¬ä¼šåœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­è®¨è®ºå®ƒã€‚(ä¹Ÿè®¸åœ¨ä¸‹ä¸€éƒ¨ğŸ˜‰).</p><h1 id="ad69" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">è®©æˆ‘ä»¬å»ºç«‹æˆ‘ä»¬çš„æ¨¡å‹ã€‚</h1><p id="b6c1" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†ç†è®ºï¼Œè®©æˆ‘ä»¬å¼€å§‹æœ‰è¶£çš„éƒ¨åˆ†â€”â€”å»ºç«‹æˆ‘ä»¬çš„æ¨¡å‹ã€‚</p><h1 id="8466" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">åŠ è½½å’Œé¢„å¤„ç†æ•°æ®</h1><p id="8808" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">æˆ‘å°†ä½¿ç”¨æ¥è‡ªKaggleçš„è¯—æ­Œæ•°æ®é›†ã€‚å®ƒæ€»å…±æœ‰15ï¼Œ000é¦–è¯—ï¼Œå› æ­¤å¯¹äºæˆ‘ä»¬æ¨¡å‹å­¦ä¹ å’Œåˆ›å»ºæ¨¡å¼æ¥è¯´è¶³å¤Ÿäº†ã€‚ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹æŠŠå®ƒè½½å…¥æˆ‘ä»¬çš„ç¬”è®°æœ¬ã€‚</p><p id="4c85" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.é¦–å…ˆå¯¼å…¥åº“ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="a0e6" class="lc jp ht ky b fi ld le l lf lg">import numpy as np<br/>import torch<br/>from torch import nn<br/>import torch.nn.functional as F</span></pre><p id="ad9e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.ç°åœ¨ä»æ–‡æœ¬æ–‡ä»¶åŠ è½½æ•°æ®ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="c71e" class="lc jp ht ky b fi ld le l lf lg"># open text file and read in data as `text`<br/>with open('/data/poems_data.txt', 'r') as f:<br/>    text = f.read()</span></pre><p id="9a6a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.æˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰“å°å‰100ä¸ªå­—ç¬¦æ¥éªŒè¯æˆ‘ä»¬çš„æ•°æ®ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="214b" class="lc jp ht ky b fi ld le l lf lg">text[:100]</span></pre><p id="06ff" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.æ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œä¸ç†è§£æ–‡æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»å°†æˆ‘ä»¬çš„txtæ•°æ®è½¬æ¢ä¸ºæ•´æ•°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä»¤ç‰Œå­—å…¸ï¼Œå¹¶å°†å­—ç¬¦æ˜ å°„åˆ°æ•´æ•°ï¼Œåä¹‹äº¦ç„¶ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="9b55" class="lc jp ht ky b fi ld le l lf lg"># encode the text and map each character to an integer and vice versa</span><span id="b0b3" class="lc jp ht ky b fi lh le l lf lg"># we create two dictionaries:<br/># 1. int2char, which maps integers to characters<br/># 2. char2int, which maps characters to unique integers<br/>chars = tuple(set(text))<br/>int2char = dict(enumerate(chars))<br/>char2int = {ch: ii for ii, ch in int2char.items()}</span><span id="e101" class="lc jp ht ky b fi lh le l lf lg"># encode the text<br/>encoded = np.array([char2int[ch] for ch in text])</span></pre><p id="454f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.ä¸€ç§çƒ­ç¼–ç ç”¨äºè¡¨ç¤ºå­—ç¬¦ã€‚æ¯”å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸‰ä¸ªå­—ç¬¦aï¼Œbï¼Œcï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¿™æ ·æ¥è¡¨ç¤ºå®ƒä»¬[1ï¼Œ0ï¼Œ0]ï¼Œ[0ï¼Œ1ï¼Œ0]ï¼Œ[0ï¼Œ0ï¼Œ1]è¿™é‡Œæˆ‘ä»¬ç”¨1æ¥è¡¨ç¤ºè¿™ä¸ªå­—ç¬¦ï¼Œå…¶ä»–çš„éƒ½æ˜¯0ã€‚å¯¹äºæˆ‘ä»¬çš„ç”¨ä¾‹ï¼Œæˆ‘ä»¬æœ‰è®¸å¤šå­—ç¬¦å’Œç¬¦å·ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ä¸€ä¸ªçƒ­ç‚¹å‘é‡ä¼šå¾ˆé•¿ã€‚ä½†æ˜¯æ²¡å…³ç³»ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="507f" class="lc jp ht ky b fi ld le l lf lg">def one_hot_encode(arr, n_labels):<br/>    <br/>    # Initialize the the encoded array<br/>    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)<br/>    <br/>    # Fill the appropriate elements with ones<br/>    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.<br/>    <br/>    # Finally reshape it to get back to the original array<br/>    one_hot = one_hot.reshape((*arr.shape, n_labels))<br/>    <br/>    return one_hot</span></pre><p id="1077" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ç°åœ¨ç”¨è¿™ç§æ–¹æ³•æµ‹è¯•å®ƒã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="e743" class="lc jp ht ky b fi ld le l lf lg"># check that the function works as expected<br/>test_seq = np.array([[0, 5, 1]])<br/>one_hot = one_hot_encode(test_seq, 8)</span><span id="dfba" class="lc jp ht ky b fi lh le l lf lg">print(one_hot)</span></pre><p id="c737" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">6.ç°åœ¨æˆ‘ä»¬å¿…é¡»ä¸ºæˆ‘ä»¬çš„æ¨¡å‹åˆ›å»ºæ‰¹æ¬¡ï¼Œè¿™æ˜¯éå¸¸å…³é”®çš„éƒ¨åˆ†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†é€‰æ‹©ä¸€ä¸ªæ‰¹é‡å¤§å°ï¼Œå³è¡Œæ•°ï¼Œç„¶ååºåˆ—é•¿åº¦æ˜¯ä¸€ä¸ªæ‰¹é‡ä¸­è¦ä½¿ç”¨çš„åˆ—æ•°ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="cf43" class="lc jp ht ky b fi ld le l lf lg">def get_batches(arr, batch_size, seq_length):<br/>    '''Create a generator that returns batches of size<br/>       batch_size x seq_length from arr.<br/>       <br/>       Arguments<br/>       ---------<br/>       arr: Array you want to make batches from<br/>       batch_size: Batch size, the number of sequences per batch<br/>       seq_length: Number of encoded chars in a sequence<br/>    '''<br/>    <br/>    batch_size_total = batch_size * seq_length<br/>    # total number of batches we can make<br/>    n_batches = len(arr)//batch_size_total<br/>    <br/>    # Keep only enough characters to make full batches<br/>    arr = arr[:n_batches * batch_size_total]<br/>    # Reshape into batch_size rows<br/>    arr = arr.reshape((batch_size, -1))<br/>    <br/>    # iterate through the array, one sequence at a time<br/>    for n in range(0, arr.shape[1], seq_length):<br/>        # The features<br/>        x = arr[:, n:n+seq_length]<br/>        # The targets, shifted by one<br/>        y = np.zeros_like(x)<br/>        try:<br/>            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]<br/>        except IndexError:<br/>            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]<br/>        yield x, y</span></pre><p id="fd35" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">7 .ç°åœ¨æˆ‘ä»¬å¯ä»¥æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨ã€‚(å¦‚æœGPUä¸å¯ç”¨ï¼Œè¯·ä¿æŒè¾ƒä½çš„epochsæ•°)</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="2bc3" class="lc jp ht ky b fi ld le l lf lg"># check if GPU is available<br/>train_on_gpu = torch.cuda.is_available()<br/>if(train_on_gpu):<br/>    print('Training on GPU!')<br/>else: <br/>    print('No GPU available, training on CPU; consider making n_epochs very small.')</span></pre><p id="a652" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">8.è¿™é‡Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸ºCharRNNçš„ç±»ã€‚è¿™æ˜¯æˆ‘ä»¬çš„æ¨¡ç‰¹ç­ã€‚åœ¨initæ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»ä¸ºæˆ‘ä»¬çš„æ¨¡å‹å®šä¹‰å±‚ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªLSTMå±‚ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº†dropout(è¿™æœ‰åŠ©äºé¿å…è¿‡åº¦æ‹Ÿåˆ)ã€‚å¯¹äºè¾“å‡ºï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€å•çš„çº¿æ€§å±‚ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="a6be" class="lc jp ht ky b fi ld le l lf lg">class CharRNN(nn.Module):<br/>    <br/>    def __init__(self, tokens, n_hidden=256, n_layers=2,<br/>                               drop_prob=0.5, lr=0.001):<br/>        super().__init__()<br/>        self.drop_prob = drop_prob<br/>        self.n_layers = n_layers<br/>        self.n_hidden = n_hidden<br/>        self.lr = lr<br/>        <br/>        # creating character dictionaries<br/>        self.chars = tokens<br/>        self.int2char = dict(enumerate(self.chars))<br/>        self.char2int = {ch: ii for ii, ch in self.int2char.items()}<br/>        <br/>        #lstm layer<br/>        self.lstm=nn.LSTM(len(self.chars),n_hidden,n_layers,<br/>                          dropout=drop_prob,batch_first=True)<br/>        <br/>        #dropout layer<br/>        self.dropout=nn.Dropout(drop_prob)<br/>        <br/>        #output layer<br/>        self.fc=nn.Linear(n_hidden,len(self.chars))</span><span id="2112" class="lc jp ht ky b fi lh le l lf lg">    <br/>    def forward(self, x, hidden):<br/>        ''' Forward pass through the network. <br/>            These inputs are x, and the hidden/cell state `hidden`. '''<br/>        ## Get the outputs and the new hidden state from the lstm<br/>        r_output, hidden = self.lstm(x, hidden)<br/>        <br/>        ## pass through a dropout layer<br/>        out = self.dropout(r_output)<br/>        <br/>        # Stack up LSTM outputs using view<br/>        # you may need to use contiguous to reshape the output<br/>        out = out.contiguous().view(-1, self.n_hidden)<br/>        <br/>        ## put x through the fully-connected layer<br/>        out = self.fc(out)<br/>        return out, hidden<br/>    <br/>    <br/>    def init_hidden(self, batch_size):<br/>        ''' Initializes hidden state '''<br/>        # Create two new tensors with sizes n_layers x batch_size x n_hidden,<br/>        # initialized to zero, for hidden state and cell state of LSTM<br/>        weight = next(self.parameters()).data<br/>        <br/>        if (train_on_gpu):<br/>            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),<br/>                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())<br/>        else:<br/>            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),<br/>                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())<br/>        <br/>        return hidden</span></pre><p id="bcdc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">9.ç°åœ¨æˆ‘ä»¬æœ‰äº†æ¨¡å‹ï¼Œæ˜¯æ—¶å€™è®­ç»ƒæ¨¡å‹äº†ã€‚å¯¹äºè®­ç»ƒï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬ç®€å•åœ°è®¡ç®—æ¯ä¸€æ­¥åçš„æŸå¤±ï¼Œç„¶åä¼˜åŒ–å™¨é˜¶è·ƒå‡½æ•°å°†å…¶åå‘ä¼ æ’­ï¼Œå¹¶é€‚å½“åœ°æ”¹å˜æƒé‡ã€‚æŸå¤±ä¼šæ…¢æ…¢å‡å°‘ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å¼æ­£åœ¨å˜å¾—æ›´å¥½ã€‚</p><p id="eb1b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">æˆ‘ä»¬è¿˜ä½¿ç”¨ä¸¤ä¸ªæ—¶æœŸä¹‹é—´çš„éªŒè¯æ¥è·å¾—éªŒè¯æŸå¤±ï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å†³å®šæˆ‘ä»¬çš„æ¨¡å‹æ˜¯æ¬ æ‹Ÿåˆè¿˜æ˜¯è¿‡æ‹Ÿåˆã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="0dca" class="lc jp ht ky b fi ld le l lf lg">def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):<br/>    ''' Training a network <br/>    <br/>        Arguments<br/>        ---------<br/>        <br/>        net: CharRNN network<br/>        data: text data to train the network<br/>        epochs: Number of epochs to train<br/>        batch_size: Number of mini-sequences per mini-batch, aka batch size<br/>        seq_length: Number of character steps per mini-batch<br/>        lr: learning rate<br/>        clip: gradient clipping<br/>        val_frac: Fraction of data to hold out for validation<br/>        print_every: Number of steps for printing training and validation loss<br/>    <br/>    '''<br/>    net.train()<br/>    <br/>    opt = torch.optim.Adam(net.parameters(), lr=lr)<br/>    criterion = nn.CrossEntropyLoss()<br/>    <br/>    # create training and validation data<br/>    val_idx = int(len(data)*(1-val_frac))<br/>    data, val_data = data[:val_idx], data[val_idx:]<br/>    <br/>    if(train_on_gpu):<br/>        net.cuda()<br/>    <br/>    counter = 0<br/>    n_chars = len(net.chars)<br/>    for e in range(epochs):<br/>        # initialize hidden state<br/>        h = net.init_hidden(batch_size)<br/>        <br/>        for x, y in get_batches(data, batch_size, seq_length):<br/>            counter += 1<br/>            <br/>            # One-hot encode our data and make them Torch tensors<br/>            x = one_hot_encode(x, n_chars)<br/>            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)<br/>            <br/>            if(train_on_gpu):<br/>                inputs, targets = inputs.cuda(), targets.cuda()</span><span id="8c71" class="lc jp ht ky b fi lh le l lf lg">            # Creating new variables for the hidden state, otherwise<br/>            # we'd backprop through the entire training history<br/>            h = tuple([each.data for each in h])</span><span id="5163" class="lc jp ht ky b fi lh le l lf lg">            # zero accumulated gradients<br/>            net.zero_grad()<br/>            <br/>            # get the output from the model<br/>            output, h = net(inputs, h)<br/>            <br/>            # calculate the loss and perform backprop<br/>            loss = criterion(output, targets.view(batch_size*seq_length).long())<br/>            loss.backward()<br/>            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.<br/>            nn.utils.clip_grad_norm_(net.parameters(), clip)<br/>            opt.step()<br/>            <br/>            # loss stats<br/>            if counter % print_every == 0:<br/>                # Get validation loss<br/>                val_h = net.init_hidden(batch_size)<br/>                val_losses = []<br/>                net.eval()<br/>                for x, y in get_batches(val_data, batch_size, seq_length):<br/>                    # One-hot encode our data and make them Torch tensors<br/>                    x = one_hot_encode(x, n_chars)<br/>                    x, y = torch.from_numpy(x), torch.from_numpy(y)<br/>                    <br/>                    # Creating new variables for the hidden state, otherwise<br/>                    # we'd backprop through the entire training history<br/>                    val_h = tuple([each.data for each in val_h])<br/>                    <br/>                    inputs, targets = x, y<br/>                    if(train_on_gpu):<br/>                        inputs, targets = inputs.cuda(), targets.cuda()</span><span id="17b8" class="lc jp ht ky b fi lh le l lf lg">                    output, val_h = net(inputs, val_h)<br/>                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())<br/>                <br/>                    val_losses.append(val_loss.item())<br/>                <br/>                net.train() # reset to train mode after iterationg through validation data<br/>                <br/>                print("Epoch: {}/{}...".format(e+1, epochs),<br/>                      "Step: {}...".format(counter),<br/>                      "Loss: {:.4f}...".format(loss.item()),<br/>                      "Val Loss: {:.4f}".format(np.mean(val_losses)))</span></pre><p id="cdf7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ç°åœ¨ç”¨ä¸‹é¢çš„æ–¹æ³•è®­ç»ƒå®ƒã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="4e8e" class="lc jp ht ky b fi ld le l lf lg"># define and print the net<br/>n_hidden = 512<br/>n_layers = 2</span><span id="5525" class="lc jp ht ky b fi lh le l lf lg">net = CharRNN(chars, n_hidden, n_layers)<br/>print(net)</span><span id="b1aa" class="lc jp ht ky b fi lh le l lf lg">batch_size = 128<br/>seq_length = 100<br/>n_epochs =  10# start small if you are just testing initial behavior</span><span id="b85e" class="lc jp ht ky b fi lh le l lf lg"># train the model<br/>train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)</span></pre><p id="05dd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">10.æˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„æ–¹æ³•ä¿å­˜æ¨¡å‹ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="36e1" class="lc jp ht ky b fi ld le l lf lg"># change the name, for saving multiple files<br/>model_name = 'poem_4_epoch.net'</span><span id="ec94" class="lc jp ht ky b fi lh le l lf lg">checkpoint = {'n_hidden': net.n_hidden,<br/>              'n_layers': net.n_layers,<br/>              'state_dict': net.state_dict(),<br/>              'tokens': net.chars}</span><span id="9351" class="lc jp ht ky b fi lh le l lf lg">with open(model_name, 'wb') as f:<br/>    torch.save(checkpoint, f)</span></pre><p id="302e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">11.æ—¢ç„¶æ¨¡å‹å·²ç»è®­ç»ƒå¥½äº†ï¼Œæˆ‘ä»¬å°±è¦ä»ä¸­å–æ ·å¹¶é¢„æµ‹ä¸‹ä¸€ä¸ªè§’è‰²ï¼ä¸ºäº†é‡‡æ ·ï¼Œæˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªå­—ç¬¦ï¼Œè®©ç½‘ç»œé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚ç„¶åæˆ‘ä»¬æŠŠé‚£ä¸ªå­—ç¬¦ä¼ å›æ¥ï¼Œå¾—åˆ°å¦ä¸€ä¸ªé¢„æµ‹çš„å­—ç¬¦ã€‚åªè¦ç»§ç»­è¿™æ ·åšï¼Œä½ å°±ä¼šç”Ÿæˆä¸€å †æ–‡æœ¬ï¼è¿™é‡Œï¼Œå‰kä¸ªæ ·æœ¬åªæ˜¯æˆ‘ä»¬çš„æ¨¡å‹å°†é¢„æµ‹å¹¶ä»ä¸­ä½¿ç”¨æœ€ç›¸å…³çš„ä¸€ä¸ªå­—æ¯çš„æ•°é‡ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="859e" class="lc jp ht ky b fi ld le l lf lg">def predict(net, char, h=None, top_k=None):<br/>        ''' Given a character, predict the next character.<br/>            Returns the predicted character and the hidden state.<br/>        '''<br/>        <br/>        # tensor inputs<br/>        x = np.array([[net.char2int[char]]])<br/>        x = one_hot_encode(x, len(net.chars))<br/>        inputs = torch.from_numpy(x)<br/>        <br/>        if(train_on_gpu):<br/>            inputs = inputs.cuda()<br/>        <br/>        # detach hidden state from history<br/>        h = tuple([each.data for each in h])<br/>        # get the output of the model<br/>        out, h = net(inputs, h)</span><span id="4c74" class="lc jp ht ky b fi lh le l lf lg">        # get the character probabilities<br/>        p = F.softmax(out, dim=1).data<br/>        if(train_on_gpu):<br/>            p = p.cpu() # move to cpu<br/>        <br/>        # get top characters<br/>        if top_k is None:<br/>            top_ch = np.arange(len(net.chars))<br/>        else:<br/>            p, top_ch = p.topk(top_k)<br/>            top_ch = top_ch.numpy().squeeze()<br/>        <br/>        # select the likely next character with some element of randomness<br/>        p = p.numpy().squeeze()<br/>        char = np.random.choice(top_ch, p=p/p.sum())<br/>        <br/>        # return the encoded value of the predicted char and the hidden state<br/>        return net.int2char[char], h</span><span id="ad84" class="lc jp ht ky b fi lh le l lf lg">def sample(net, size, prime='The', top_k=None):<br/>        <br/>    if(train_on_gpu):<br/>        net.cuda()<br/>    else:<br/>        net.cpu()<br/>    <br/>    net.eval() # eval mode<br/>    <br/>    # First off, run through the prime characters<br/>    chars = [ch for ch in prime]<br/>    h = net.init_hidden(1)<br/>    for ch in prime:<br/>        char, h = predict(net, ch, h, top_k=top_k)</span><span id="4177" class="lc jp ht ky b fi lh le l lf lg">    chars.append(char)<br/>    <br/>    # Now pass in the previous character and get a new one<br/>    for ii in range(size):<br/>        char, h = predict(net, chars[-1], h, top_k=top_k)<br/>        chars.append(char)</span><span id="9bd8" class="lc jp ht ky b fi lh le l lf lg">    return ''.join(chars)</span></pre><p id="018d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">12.ç°åœ¨æˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªæ ·æœ¬æ–¹æ³•æ¥åšé¢„æµ‹ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="40b8" class="lc jp ht ky b fi ld le l lf lg">print(sample(net, 500, prime='christmas', top_k=2))</span></pre><p id="047c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">è¾“å‡ºçœ‹èµ·æ¥ä¼šåƒè¿™æ ·ã€‚</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="3480" class="lc jp ht ky b fi ld le l lf lg">christmas a son of this</span><span id="ef18" class="lc jp ht ky b fi lh le l lf lg">the sun wants the street of the stars, and the way the way<br/>they went and too man and the star of the words<br/>of a body of a street and the strange shoulder of the sky</span><span id="cd9f" class="lc jp ht ky b fi lh le l lf lg">and the sun, an end on the sun and the sun and so to the stars are stars<br/>and the words of the water and the streets of the world<br/>to see them to start a posture of the streets<br/>on the street of the streets, and the sun and soul of the station<br/>and so too too the world of a sound and stranger and to the world<br/>to the sun a</span></pre><p id="2bae" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸€äº›å¥½çš„çº¿æ¡ã€‚å†…å®¹æ²¡æœ‰å¤ªå¤šæ„ä¹‰ï¼Œä½†å®ƒèƒ½å¤Ÿç”Ÿæˆä¸€äº›è¯­æ³•æ­£ç¡®çš„è¡Œã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤šè®­ç»ƒå®ƒä¸€æ®µæ—¶é—´ï¼Œå®ƒä¼šè¡¨ç°å¾—æ›´å¥½ã€‚</p><h1 id="fffe" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">ç»“è®º</h1><p id="0aba" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬äº†è§£äº†RNNå’ŒLSTMã€‚æˆ‘ä»¬è¿˜ç”¨PyTorchå»ºç«‹äº†æˆ‘ä»¬çš„è¯—æ­Œæ¨¡å‹ã€‚æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« å¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•ç–‘é—®æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹é¢çš„è¯„è®ºåŒºå‘è¡¨ï¼Œæˆ–è€…é€šè¿‡<a class="ae ks" href="mailto:yash.yn59@gmail.com" rel="noopener ugc nofollow" target="_blank">yash.yn59@gmail.com</a>è”ç³»æˆ‘ï¼Œæˆ‘å°†éå¸¸ä¹æ„ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚</p></div></div>    
</body>
</html>