<html>
<head>
<title>ELMo Embedding — The Entire Intent of a Query</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ELMo嵌入——查询的全部意图</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/elmo-embedding-the-entire-intent-of-a-query-530b268c4cd?source=collection_archive---------1-----------------------#2019-08-17">https://medium.com/analytics-vidhya/elmo-embedding-the-entire-intent-of-a-query-530b268c4cd?source=collection_archive---------1-----------------------#2019-08-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0df24a664f35536d6de07bbe9d95e760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wp7Taq6Mnk5IH8zM.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">电子商务搜索</figcaption></figure><p id="ca46" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">作为搜索查询理解的继续，接下来要解决的问题是<a class="ae js" rel="noopener" href="/@dtunkelang/query-understanding-divided-into-three-parts-d9cbc81a5d09">整体查询理解</a>。我们将详细讨论这一点，因为我们已经在这里讨论了简化查询理解部分<a class="ae js" href="https://towardsdatascience.com/understanding-the-search-query-part-i-632d1b323b50" rel="noopener" target="_blank">。这里，我们以电子商务搜索为例。问题是在特定的分类中找到查询的意图，如L1/L2/L3类别，也称为类别分类。</a></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jt"><img src="../Images/064adae2423d282530eb1fae1486c9d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K51FyLR6TN5gSQayGdSFVQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">电子商务网站上的客户搜索</figcaption></figure><p id="235b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们来谈谈一个现实生活中的网上购物的例子，你想寻找类似“洋葱1公斤鲜”的东西。现在，为了使结果更加精确，搜索结果会与您的查询意图所在的可能类别名称一起显示，即“杂货和美食”。现在，如果你点击预测类别，你会看到属于该特定类别的产品，因此，这是一个增强体验的精确结果。</p><p id="be1c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这个问题的解决方案是我们将要讨论的基于机器学习的。</p><h1 id="030b" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">查询的目的</h1><p id="d44d" class="pw-post-body-paragraph iu iv hi iw b ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr hb bi translated">这是一个多类多标签分类问题，其中输入将是一组标有其类别的搜索查询，并且特定的搜索查询可能属于许多互斥类别中的一个以上的类别。更详细地说，假设我们有3个层次的类别L1，L2和L3。搜索查询可以属于每个层级的一个或多个类别，例如搜索查询——“苹果”在L1可以是“电子产品”和“杂货”，在L2可以是“笔记本电脑”和“水果”，等等。我们有一个基于深度学习的模型，使用Elmo作为嵌入层。</p><h2 id="60de" class="lb jz hi bd ka lc ld le ke lf lg lh ki jf li lj km jj lk ll kq jn lm ln ku lo bi translated">ELMo嵌入</h2><p id="68c9" class="pw-post-body-paragraph iu iv hi iw b ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr hb bi translated">ELMo是由<a class="ae js" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank"> AllenNLP </a>创造的，它不同于Glove、Fasttext、Word2Vec等。提供上下文化的单词嵌入，其单词的向量表示在句子与句子之间是不同的。</p><blockquote class="lp lq lr"><p id="4bcb" class="iu iv ls iw b ix iy iz ja jb jc jd je lt jg jh ji lu jk jl jm lv jo jp jq jr hb bi translated">ELMo是一种深度语境化的单词表示，它模拟(1)单词使用的复杂特征(例如，句法和语义)，以及(2)这些使用如何在语言语境中变化(即，模拟多义性)。这些单词向量是深度双向语言模型(biLM)的内部状态的学习函数，该模型是在大型文本语料库上预先训练的。</p></blockquote><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/05a001fceb31b06c86fbaa46796a7b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fhaQv6ogNE-fpros.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">取自<a class="ae js" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="0ab7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">ELMo在<a class="ae js" href="http://www.statmt.org/lm-benchmark/" rel="noopener ugc nofollow" target="_blank"><strong class="iw hj">10亿字基准</strong> </a>上接受训练，这是来自2011年WMT新闻抓取数据的大约8亿个令牌。</p><h1 id="ff31" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">为什么是埃尔莫？使用ELMo的动机</h1><p id="3f7f" class="pw-post-body-paragraph iu iv hi iw b ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr hb bi translated">其他单词表示模型如<a class="ae js" href="https://github.com/facebookresearch/fastText" rel="noopener ugc nofollow" target="_blank"> Fasttext </a>，Glove等。也生成单词的嵌入，但是我们选择Elmo有几个原因。让我们一个一个地参观:</p><ol class=""><li id="a61f" class="lx ly hi iw b ix iy jb jc jf lz jj ma jn mb jr mc md me mf bi translated">Elmo提供了一个单词在句子中的嵌入，即一个单词可能有不同的含义，这取决于它在上下文中的使用，类似于上面照片中的例子。“苹果”这个词可能是一个“品牌”名称，也可能是一种“水果”。因此，如果给出一个类似'<strong class="iw hj">苹果汁'</strong>的查询，这里为标记' Apple '生成的嵌入将不同于'<strong class="iw hj"> Apple Laptop </strong>'中的嵌入。而在一般的电子商务搜索查询中，很可能会发生这种情况。</li><li id="e3ea" class="lx ly hi iw b ix mg jb mh jf mi jj mj jn mk jr mc md me mf bi translated">另一个原因是，由于LSTM网络在ELMo模型中被内部使用，我们需要担心在训练数据集的字典中不存在的单词，因为它也生成字符嵌入。它允许网络使用形态学线索来为训练中看不见的词汇外标记形成健壮的表示。在品牌名称的情况下，我们通常会面临词汇外的标记问题。</li><li id="ce2f" class="lx ly hi iw b ix mg jb mh jf mi jj mj jn mk jr mc md me mf bi translated">此外，ELMo还在该领域的顶级会议之一NAACL-HLT 2018 上获得了最佳<a class="ae js" href="https://naacl2018.wordpress.com/2018/04/11/outstanding-papers/" rel="noopener ugc nofollow" target="_blank">论文奖。</a></li></ol><h1 id="8701" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">使用TF Hub实现</h1><p id="9135" class="pw-post-body-paragraph iu iv hi iw b ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr hb bi translated"><a class="ae js" href="http://tensorflow.org/hub" rel="noopener ugc nofollow" target="_blank"> TensorFlow Hub </a>是一个发布、发现和重用TensorFlow中部分机器学习模块的平台。对于模块，它意味着张量流图的一个独立部分及其权重，可以在其他类似的任务中重用。通过重用模块，开发者可以进行迁移学习。预训练的Elmo模型也出现在<a class="ae js" href="https://tfhub.dev/google/elmo/2" rel="noopener ugc nofollow" target="_blank"> Tensorflow Hub </a>上。</p><pre class="ju jv jw jx fd ml mm mn mo aw mp bi"><span id="727b" class="lb jz hi mm b fi mq mr l ms mt">#Sample Code to get instant embedding<br/>elmo = hub.Module("<a class="ae js" href="https://tfhub.dev/google/elmo/2" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/elmo/2</a>", trainable=True)<br/>embeddings = elmo(["apple juice", "apple tablet"],  signature="default", as_dict=True)["elmo"]</span></pre><p id="5f03" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于每个单词，嵌入的输出形状将是1024，因此，在上面的例子中，对于每个都有两个单词的两个句子，形状为[2，2，1024]。如果使用Keras建模，则需要在<code class="du mu mv mw mm b">Lambda layer</code>中使用该模型。Elmo模型权重已经下载到一个文件夹中，因此我们在训练时不需要对Tensorflow hub进行网络呼叫。</p><pre class="ju jv jw jx fd ml mm mn mo aw mp bi"><span id="a3e5" class="lb jz hi mm b fi mq mr l ms mt">curl -L "<a class="ae js" href="https://tfhub.dev/google/elmo/2?tf-hub-format=compressed" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/elmo/2?tf-hub-format=compressed</a>" | tar -zxvC /content/elmo_module</span></pre><p id="8902" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">记得在调用Elmo嵌入层之前初始化图形和表格。</p><pre class="ju jv jw jx fd ml mm mn mo aw mp bi"><span id="86e9" class="lb jz hi mm b fi mq mr l ms mt">with tf.Session() as sess:<br/>  sess.run(tf.global_variables_initializer())  <br/>  sess.run(tf.tables_initializer())<br/>  embed=sess.run(embeddings)</span></pre><h1 id="9988" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">集成后的模型</h1><figure class="ju jv jw jx fd ij"><div class="bz dy l di"><div class="mx my l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">查询分类模型</figcaption></figure><p id="5106" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里我们通过使用python的<a class="ae js" href="https://pypi.org/project/cachetools/" rel="noopener ugc nofollow" target="_blank"> cachetools </a>库来使用内存缓存。由于Elmo模型中的权重非常大，所以每次遍历图来寻找之前已经生成过一次的句子的嵌入并不是一个好主意，因为这将增加训练时间，并且我们还可能耗尽RAM存储器。一个时期后，所有搜索查询的嵌入将被缓存并轻松返回，而无需通过TensorFlow hub库。</p><p id="bde7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在获得句子的嵌入后，我们使用BiLSTM来提取更多的上下文表示，用于词汇表外的单词，然后是两个密集层，最后是sigmoid激活层。由于这是一个多标签分类，并且一个查询属于三个不同的层次<a class="ae js" href="https://stats.stackexchange.com/questions/395934/activation-function-at-output-layer-for-multi-label-classification" rel="noopener ugc nofollow" target="_blank">，我们使用Sigmoid层</a>，因为概率之和不需要为1。这里，在这个例子中，我们有1500个类别，比方说L1、L2和L3每个层级有500个。为了显示每个层次结构中的前2个类别，我们需要从最后的sigmoid层中选择概率最高的类别。</p><h2 id="48ea" class="lb jz hi bd ka lc ld le ke lf lg lh ki jf li lj km jj lk ll kq jn lm ln ku lo bi translated">TF-服务响应</h2><p id="bcfc" class="pw-post-body-paragraph iu iv hi iw b ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr hb bi translated">由于模型有点复杂，有大量的权重，这将影响TF服务的整体响应时间。它还依赖于最终被预测的类别的数量。模型大小也增长到大约850MB。</p><p id="90fa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Elmo也可以用<a class="ae js" href="https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1" rel="noopener ugc nofollow" target="_blank"> BERT </a>代替。BERT在TF Hub上有几个可用的变体。Elmo/BERT模型可用于无监督学习，也可通过在 <code class="du mu mv mw mm b"><a class="ae js" href="https://keras.io/layers/writing-your-own-keras-layers/" rel="noopener ugc nofollow" target="_blank">Keras</a></code>中<a class="ae js" href="https://keras.io/layers/writing-your-own-keras-layers/" rel="noopener ugc nofollow" target="_blank">创建一个自定义层在自定义数据集上进行训练。但是，请记住，在继续执行此任务之前，请确保您有足够的资源，如GPU、内存等。</a></p><h1 id="eb99" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论</h1><p id="e96d" class="pw-post-body-paragraph iu iv hi iw b ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn la jp jq jr hb bi translated">因此，现在我们可以预测特定搜索查询的分类层次结构，我们可以利用它向最终用户显示相同的内容，以便通过增强对引擎的搜索请求来选择和显示更精确的结果，这些结果包含用户想要购买的产品。由于这是NLP 的<a class="ae js" href="http://ruder.io/nlp-imagenet/" rel="noopener ugc nofollow" target="_blank"> ImageNet时刻，像BERT这样基于转换器的语言建模算法在这个时代占据统治地位。其他语言模型有</a><a class="ae js" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">通用语言模型微调(ULMFiT) </a>和<a class="ae js" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank"> OpenAI Transformer </a>，后者在自然语言处理的各种任务上取得了最先进的水平。</p><p id="07f7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">感谢您阅读这篇文章。请在未来关注更多的更新，也请阅读其他的故事。</p></div></div>    
</body>
</html>