<html>
<head>
<title>Regularization in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的正则化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/regularization-in-machine-learning-19640386d7a1?source=collection_archive---------20-----------------------#2020-03-02">https://medium.com/analytics-vidhya/regularization-in-machine-learning-19640386d7a1?source=collection_archive---------20-----------------------#2020-03-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4b8d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">解决过度拟合问题</h2></div><p id="de6d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">过拟合是人们在训练机器学习模型时面临的主要问题之一，它导致对其进行训练的数据具有极好的准确性，而对模型从未见过的<em class="jt">测试数据</em>具有最差的准确性。<strong class="iz hj">过拟合是因为模型从训练数据中学习细节和噪声，对模型产生负面影响，从而影响模型的准确性。</strong></p><p id="03e9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">过度拟合的模型具有低偏差和高方差，好的模型具有低偏差和低方差。</em></p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es ju"><img src="../Images/02640565a14fc83efea79aee966ca32e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*wqDhhG2BjkBCl5WuHojddw.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">图一。偏差与方差权衡</figcaption></figure><p id="002c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有许多处理过度拟合的方法，</p><ul class=""><li id="da5b" class="kg kh hi iz b ja jb jd je jg ki jk kj jo kk js kl km kn ko bi translated">交叉验证</li><li id="04bd" class="kg kh hi iz b ja kp jd kq jg kr jk ks jo kt js kl km kn ko bi translated">移除功能</li><li id="e948" class="kg kh hi iz b ja kp jd kq jg kr jk ks jo kt js kl km kn ko bi translated">使用更多数据进行培训</li><li id="66a9" class="kg kh hi iz b ja kp jd kq jg kr jk ks jo kt js kl km kn ko bi translated">组装</li><li id="54d3" class="kg kh hi iz b ja kp jd kq jg kr jk ks jo kt js kl km kn ko bi translated">提前停止</li><li id="ac71" class="kg kh hi iz b ja kp jd kq jg kr jk ks jo kt js kl km kn ko bi translated">正规化</li></ul><p id="e37b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本帖中，我们将重点关注<strong class="iz hj">正规化</strong>。</p><h2 id="39b4" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated"><strong class="ak">什么是正规化？</strong></h2><p id="e072" class="pw-post-body-paragraph ix iy hi iz b ja lp ij jc jd lq im jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">正则化是一种通过将特征的系数值向零最小化来拯救回归模型免于过度拟合的方法。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lu"><img src="../Images/e7ad635995ed03b9c76af580e99465ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*LbNfJzZ-Kt3JYzaeV-ZNHA.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">图二。红色曲线是正则化之前，蓝色曲线是正则化之后。</figcaption></figure><p id="7fc7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们考虑以下线性表达式:</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/f272f48275bbb8c82615ab9edc756efa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cNJ4WtaZ1J1nqUXWnrbzxg.png"/></div></div></figure><p id="7310" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，h(x)是目标值<em class="jt">，θ</em>是系数，x是特征，n是特征的数量。</p><p id="7ef7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">成本函数:</strong></p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es ma"><img src="../Images/69a5db4477d86c8e38fa372bb4c1eee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*WrWyhqE2eMqxKys3B3JNjA.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">多元线性回归的成本函数</figcaption></figure><p id="3d81" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的成本函数等式中,“m”是文件中存在的数据数量。</p><p id="551e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正则化将带有正则化参数(λ)的惩罚添加到成本函数中，以有效的方式最小化成本函数。因此，成本函数的等式变成如下:</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mb"><img src="../Images/166caf28f85b3db378e145c209864ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*U0FWe6ILUjXaJjlxyP1eOg.png"/></div><figcaption class="kc kd et er es ke kf bd b be z dx translated">岭回归的成本函数</figcaption></figure><blockquote class="mc md me"><p id="f515" class="ix iy jt iz b ja jb ij jc jd je im jf mf jh ji jj mg jl jm jn mh jp jq jr js hb bi translated"><strong class="iz hj">正则化通过缩小系数的值来工作。</strong></p></blockquote><p id="20aa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有三种不同类型的正则化技术，</p><ul class=""><li id="f709" class="kg kh hi iz b ja jb jd je jg ki jk kj jo kk js kl km kn ko bi translated">岭回归(L2正则化)</li><li id="b0fe" class="kg kh hi iz b ja kp jd kq jg kr jk ks jo kt js kl km kn ko bi translated">套索回归(L1正则化)</li><li id="669f" class="kg kh hi iz b ja kp jd kq jg kr jk ks jo kt js kl km kn ko bi translated">弹性网络回归(L1/L2正则化)</li></ul><h2 id="aee2" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated"><strong class="ak">岭回归(L2回归)</strong></h2><p id="e222" class="pw-post-body-paragraph ix iy hi iz b ja lp ij jc jd lq im jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">岭回归的成本函数是，</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mb"><img src="../Images/166caf28f85b3db378e145c209864ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*U0FWe6ILUjXaJjlxyP1eOg.png"/></div></figure><p id="8c53" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过增加等于系数大小的平方的惩罚来改变成本函数。正则化参数(λ)确定惩罚的严重性，即λ正则化系数，使得如果系数取大值，优化函数被惩罚。岭回归的工作原理是使系数变小，从而降低模型的灵活性和复杂性。</p><p id="773d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当λ= 0时，罚值没有影响，岭回归的成本函数将表现为简单线性回归模型的成本函数。随着λ值的增加，罚值增加，系数将接近零，但不会变为零。<strong class="iz hj">因此，岭回归把系数渐近地缩小到接近于零。</strong></p><blockquote class="mc md me"><p id="93fa" class="ix iy jt iz b ja jb ij jc jd je im jf mf jh ji jj mg jl jm jn mh jp jq jr js hb bi translated">当大多数特征都有用时，岭回归效果最好。</p><p id="9d92" class="ix iy jt iz b ja jb ij jc jd je im jf mf jh ji jj mg jl jm jn mh jp jq jr js hb bi translated">岭回归也可以应用于逻辑回归。</p></blockquote><h2 id="0d79" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated"><strong class="ak">拉索回归(L1正则化)</strong></h2><p id="7e19" class="pw-post-body-paragraph ix iy hi iz b ja lp ij jc jd lq im jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">Lasso回归的成本函数可以写成:</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es mi"><img src="../Images/1437e9fb5b802ceb147f0bb0f59fb996.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*GuboYeQtyJJFD2Yr2BtNNA.png"/></div></figure><p id="6e29" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过增加等于系数模的惩罚来改变成本函数。</p><p id="0a82" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">套索回归与岭回归有些相似，因为作为岭回归，当λ= 0时，罚函数没有影响，套索回归的成本函数将表现为简单线性模型的成本函数。岭回归和套索回归的主要区别在于，在岭回归中，我们取系数的平方，在套索回归中，我们取系数的大小，这是套索比岭回归的主要作用和优势，因为套索回归中的<strong class="iz hj">系数可能为零，因此，某些特征可以完全忽略。</strong></p><blockquote class="mc md me"><p id="6fae" class="ix iy jt iz b ja jb ij jc jd je im jf mf jh ji jj mg jl jm jn mh jp jq jr js hb bi translated">套索回归也有助于特征选择。</p></blockquote><h2 id="6c7b" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated"><strong class="ak">弹性网回归(L1/L2正则化)</strong></h2><p id="d885" class="pw-post-body-paragraph ix iy hi iz b ja lp ij jc jd lq im jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">弹性网回归是岭回归和套索回归的结合。它结合了岭回归罚函数和套索回归罚函数。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mj"><img src="../Images/32023ff51c8cc40cf23827de8620a9c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*GNKaOH1gigb7gY4-OrVlMg.png"/></div></div></figure><blockquote class="mc md me"><p id="7485" class="ix iy jt iz b ja jb ij jc jd je im jf mf jh ji jj mg jl jm jn mh jp jq jr js hb bi translated">当我们有大量的特征时，使用弹性网络回归。</p></blockquote><p id="a1f3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">弹性网回归有两个不同值的λ<strong class="iz hj"><em class="jt"/></strong><em class="jt"/>，可以通过使用<em class="jt">交叉验证</em>来获取λ的正确值。如果两个λ都等于零，那么我们将得到简单线性回归模型的成本函数。</p><blockquote class="mc md me"><p id="7998" class="ix iy jt iz b ja jb ij jc jd je im jf mf jh ji jj mg jl jm jn mh jp jq jr js hb bi translated"><strong class="iz hj">弹性网回归是岭回归和套索回归的混合。</strong></p></blockquote><h2 id="17a7" class="ku kv hi bd kw kx ky kz la lb lc ld le jg lf lg lh jk li lj lk jo ll lm ln lo bi translated"><strong class="ak">结论</strong></h2><p id="e91e" class="pw-post-body-paragraph ix iy hi iz b ja lp ij jc jd lq im jf jg lr ji jj jk ls jm jn jo lt jq jr js hb bi translated">在这篇文章中，我们学习了正则化，这是一种解决线性模型过度拟合的方法。我们还学习了三种不同类型的正则化，即岭回归、套索回归和弹性网回归，以及这些正则化技术之间的差异。</p></div></div>    
</body>
</html>