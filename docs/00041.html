<html>
<head>
<title>Learn How to Code and Deploy Machine Learning Models on Spark Structured Streaming</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解如何在Spark结构化流上编码和部署机器学习模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/learn-how-to-code-and-deploy-machine-learning-models-on-structured-streaming-868b4081d242?source=collection_archive---------0-----------------------#2018-07-07">https://medium.com/analytics-vidhya/learn-how-to-code-and-deploy-machine-learning-models-on-structured-streaming-868b4081d242?source=collection_archive---------0-----------------------#2018-07-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/27f18382048e56cae4074b29515cd98b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-K4bpH1IXEua6hcshfESw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片由unsplash上的Carlos Muza @kmuza提供</figcaption></figure><p id="a2bf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这篇文章是对令人惊叹的数据科学开源社区的感谢，我从这个社区中学到了很多东西。</p><p id="6ff5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在过去的几个月里，我一直在做我的一个副业项目，开发关于流数据的机器学习应用程序。这是一次很棒的学习经历，有很多挑战和学习，其中一些我已经在这里分享了。</p><p id="e271" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这篇文章关注如何在流数据上部署机器学习模型，并涵盖了一个成功的生产应用程序的所有3个必要领域:<strong class="iw hj">基础设施、技术和监控。</strong></p><p id="9682" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">用SPARK-ML和结构化流开发机器学习模型</strong></p><p id="0873" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">任何成功的应用程序的第一步都是根据业务需求，确定编写应用程序的技术堆栈。<strong class="iw hj">一般来说，当数据量巨大时，使用Spark。</strong></p><p id="5f24" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用Spark的主要好处是它在大数据处理方面的成熟能力，以及内置分布式机器学习库的可用性。使用spark的最初挑战是RDD的使用，它不直观，与数据科学家习惯使用的数据框架非常不同。然而，随着Spark2.0 中数据集API的<strong class="iw hj">引入，现在编写机器学习算法变得比以前更容易。</strong></p><blockquote class="js jt ju"><p id="11a2" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr hb bi translated"><strong class="iw hj">在我的经历中，我发现通过适当利用“管道”框架，使用机器学习模型变得极其容易。管道所做的是提供一个结构，包括处理&amp;清理数据、训练模型、然后将其作为对象写出所需的所有步骤。</strong></p><p id="6ea2" class="iu iv jv iw b ix iy iz ja jb jc jd je jw jg jh ji jx jk jl jm jy jo jp jq jr hb bi translated">然后，可以直接导入该对象来处理新数据并获得结果，从而使开发人员不必为新数据重新编写和维护处理步骤的精确副本，然后使用训练数据构建模型。</p></blockquote><p id="f571" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在下面的代码片段中，我试图介绍如何使用这个API来构建、保存和使用预测模型。为了构建和保存模型，可以遵循下面的代码结构。</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="d36a" class="ki kj hi ke b fi kk kl l km kn">// Create a sqlContext<br/> var sqlContext = new SQLContext(sparkContext)</span><span id="d697" class="ki kj hi ke b fi ko kl l km kn">// Read the training data from a source. In this case i am reading it from a s3 location<br/>var data = sqlContext.read.format(“csv”).option(“header”, “true”).option(“inferSchema”, “true”).load(“pathToFile”)</span><span id="5903" class="ki kj hi ke b fi ko kl l km kn">// Select the needed messages <br/> data = data.select(“field1”,”field2",”field3")</span><span id="da36" class="ki kj hi ke b fi ko kl l km kn">// Perform pre-processing on the data <br/> val process1 = … some process … <br/> val process2 = … some process …</span><span id="2c12" class="ki kj hi ke b fi ko kl l km kn">// Define an evaluator<br/>val evaluator = … evaluator of your choice …</span><span id="1b73" class="ki kj hi ke b fi ko kl l km kn">// split the data into training and test <br/> val Array(trainingData, testData) = data.randomSplit(Array(ratio1, ratio2))</span><span id="fcd6" class="ki kj hi ke b fi ko kl l km kn">// Define the algorithm to train. For example decision tree <br/> val dt = new DecisionTree()<br/> .setFeaturesCol(featureColumn).setLabelCol(labelColumn)</span><span id="c175" class="ki kj hi ke b fi ko kl l km kn">// Define the linear pipeline. Methods specified in the pipeline are executed in a linear order. Sequence of steps is binding<br/> val pipelineDT = new Pipeline().setStages(Array(process1, process2, dt))</span><span id="1ae6" class="ki kj hi ke b fi ko kl l km kn">// Define the cross validator for executing the pipeline and performing cross validation. <br/> val cvLR = new CrossValidator()<br/> .setEstimator(pipelineDT)<br/> .setEvaluator(evaluator)<br/> .setNumFolds(3) // Use 3+ in practice</span><span id="01cb" class="ki kj hi ke b fi ko kl l km kn">// Fit the model on training data <br/> val cvModelLR = cvLR.fit(trainingData)</span><span id="b6c7" class="ki kj hi ke b fi ko kl l km kn">// extract the best trained pipeline model from the cross validator. <br/> val bestPipelineModel = cvModelLR.bestModel.asInstanceOf[PipelineModel]</span><span id="1f32" class="ki kj hi ke b fi ko kl l km kn">// Save the model in a s3 bucket <br/> cvModelLR.write.overwrite().save(mlModelPath)</span></pre><p id="94cd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">模型保存后，可通过以下步骤轻松用于预测流数据。</p><p id="e453" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 1。从卡夫卡主题中读取数据</strong></p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="3c58" class="ki kj hi ke b fi kk kl l km kn">// Create a spark session object <br/> val ss = SparkSession.builder.getOrCreate()</span><span id="a7eb" class="ki kj hi ke b fi ko kl l km kn">// Define schema of the topic to be consumed <br/> val schema= StructType( Seq(<br/> StructField(“Field1”,StringType,true),<br/> StructField(“Field2”,IntType,true)<br/> )<br/> )</span><span id="50c9" class="ki kj hi ke b fi ko kl l km kn">// Start reading from a Kafka topic<br/> val records = ss.readStream<br/> .format(“kafka”)<br/> .option(“kafka.bootstrap.servers”, kafkaServer)<br/> .option(“subscribe”,kafkaTopic)<br/> .load()<br/> .selectExpr(“cast (value as string) as json”)<br/> .select(from_json($”json”,schema).as(“data”))<br/> .select(“data.*”)</span></pre><p id="bb49" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 2。加载保存的ML模型，并将其用于预测</strong></p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="107b" class="ki kj hi ke b fi kk kl l km kn">// Load the classification model from saved location<br/> val classificationModel = CrossValidatorModel.read.load(mlModelPath)</span><span id="4f50" class="ki kj hi ke b fi ko kl l km kn">// Use the model to perform predictions. By default best model is used<br/> val results = classificationModel.transform(records)</span></pre><p id="1dfe" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 3。将结果保存到s3或其他位置</strong></p><p id="ab77" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以<strong class="iw hj"> csv </strong>格式</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="cc4c" class="ki kj hi ke b fi kk kl l km kn">// Saving results to a location as csv <br/> results.writeStream.format(“csv”).outputMode(“append”)<br/> .option(“path”, destination_path) .option(“checkpointLocation”, checkpointPath)<br/> .start()</span></pre><p id="39e8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在<strong class="iw hj">拼花地板</strong>格式</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="64e2" class="ki kj hi ke b fi kk kl l km kn">// Saving results to a location as parquet <br/> results.writeStream.format(“parquet”).outputMode(“append”)<br/> .option(“path”, destination_path) .option(“checkpointLocation”, checkpointPath)<br/> .start()</span></pre><p id="52a6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">或者如果我们想将结果发送到某个数据库或任何其他扩展</p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="d534" class="ki kj hi ke b fi kk kl l km kn">val writer = new JDBCSink(url, user, password)<br/> results.writeStream<br/> .foreach(writer)<br/> .outputMode(“append”)<br/> .option(“checkpointLocation”, checkpointPath)<br/> .start()</span></pre><p id="b89f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jv">为此，需要通过扩展spark structured streamin </em> g提供的ForeachWriter接口来实现一个单独的编写器。下面显示了jdbc的示例代码，摘自<a class="ae kp" href="https://docs.databricks.com/_static/notebooks/structured-streaming-etl-kafka.html" rel="noopener ugc nofollow" target="_blank">https://docs . data bricks . com/_ static/notebooks/structured-streaming-ETL-Kafka . html</a></p><pre class="jz ka kb kc fd kd ke kf kg aw kh bi"><span id="f998" class="ki kj hi ke b fi kk kl l km kn"><strong class="ke hj">import</strong> java.sql.<strong class="ke hj">_</strong></span><span id="204b" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">class</strong> JDBCSink(url:String, user:String, pwd:String) <strong class="ke hj">extends</strong> ForeachWriter[(String, String)] {<br/> <strong class="ke hj">val</strong> driver = “com.mysql.jdbc.Driver”<br/> <strong class="ke hj">var</strong> connection:Connection = <strong class="ke hj">_</strong><br/> <strong class="ke hj">var</strong> statement:Statement = <strong class="ke hj">_</strong><br/> <br/> <strong class="ke hj">def</strong> open(partitionId: Long,version: Long): Boolean = {<br/> Class.forName(driver)<br/> connection = DriverManager.getConnection(url, user, pwd)<br/> statement = connection.createStatement<br/> true<br/> }</span><span id="a579" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">def</strong> process(value: (String, String)): Unit = {<br/> statement.executeUpdate(“INSERT INTO zip_test “ + <br/> “VALUES (“ + value._1 + “,” + value._2 + “)”)<br/> }</span><span id="660c" class="ki kj hi ke b fi ko kl l km kn"><strong class="ke hj">def</strong> close(errorOrNull: Throwable): Unit = {<br/> connection.close<br/> }<br/> }<br/> <br/> }<br/> }</span></pre><p id="eff2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">监控、记录和警报</strong></p><p id="e14b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下一步是在应用程序中集成监控、警报和日志记录服务，以便获得即时警报并记录应用程序的工作情况。AWS堆栈中有许多工具可以利用。其中经常使用的是用于监控的CloudWatch和用于日志记录的Elastic Search。</p><p id="1a15" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一个示例监控仪表板将类似于这样</p><figure class="jz ka kb kc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/e0f9b402dc055f053cb2f08c7de213a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UULwPvENtlRFA4xLK8BXPA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片提供:<a class="ae kp" href="https://github.com/amazon-archives/cloudwatch-logs-subscription-consumer" rel="noopener ugc nofollow" target="_blank">https://github . com/Amazon-archives/cloud watch-logs-subscription-consumer</a></figcaption></figure><p id="209a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">基础设施</strong></p><p id="8288" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一旦代码准备好进行部署，就该选择合适的基础设施来部署它了。我发现最好的基础设施是Kafka(主要是因为它的多发行商/消费者架构和为不同主题设置保留期的能力)，以及AWS EMR作为运行应用程序的核心基础设施</p><p id="13ec" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">由于预装spark和内部资源管理的集群的可用性，AWS EMR成为了显而易见的选择。在短时间内全面部署新集群的能力也是一大优势。</p><p id="bd08" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一个简化的架构图将会是这样的。</p><figure class="jz ka kb kc fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/9ff45c0e2125cab591487d94db368366.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*WgiKdzNgj-7Bjp0IRWzkkA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片提供—<a class="ae kp" href="https://dmhnzl5mp9mj6.cloudfront.net/bigdata_awsblog/images/Spark_SQL_Ben_Image_1.PNG" rel="noopener ugc nofollow" target="_blank">https://dmhnzl 5 MP 9 mj 6 . cloudfront . net/big data _ AWS blog/images/Spark _ SQL _ Ben _ Image _ 1。巴布亚新几内亚</a></figcaption></figure><p id="c894" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">调谐火花作业</strong></p><p id="49be" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最后，与任何其他spark作业一样，在流作业的情况下，为了获得最大效率，有必要对其进行调优。调优spark作业的第一步是为作业选择合适的实例。在对<strong class="iw hj"> M4(通用)与</strong>C4(计算量大)实例类型进行的几个实验中，我发现M4的性能更好，主要是因为它还能够提供虚拟内核。</p><p id="cd8b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">spark中的DynamicAllocation </strong>属性在以稳定的方式最大化利用率方面也非常有用。我发现还有许多其他参数对调整性能很有用:</p><p id="e310" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)<strong class="iw hj">—conf spark . yarn . executor . memory overhead</strong>= 1024:为作业定义的内存开销</p><p id="60d4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)<strong class="iw hj">—conf spark . yarn . maxappattempts</strong>= 4:该属性定义提交应用程序的最大尝试次数。这对于将多个spark作业提交给一个集群，有时由于缺少可用资源而导致提交作业失败的情况非常有用。</p><p id="09d6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">c)<strong class="iw hj">—conf spark . task . max failures = 8</strong>:该属性设置在spark作业自身失败之前任务可以失败的最大次数。默认值为2。保持这个数字较高总是一个好主意</p><p id="890a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">d)<strong class="iw hj">—conf spark . speculation = false</strong>:当该属性设置为true时，yarn会根据任务消耗的时间自动终止并重新分配任务(如果yarn认为任务被阻塞)。在我们的例子中，我们没有发现这在性能上有很大贡献，但是在处理倾斜的数据集时，这是一个很好的属性</p><p id="585b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">e)<strong class="iw hj">—conf spark . yarn . max . executor . failures</strong>= 15:应用程序失败前的最大执行器失败次数。请始终将其设置为较高的数值。</p><p id="222e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">f)<strong class="iw hj">—conf spark . yarn . executor . failures validity interval</strong>= 1h:定义执行器故障有效的时间间隔。结合以上性质基本上在一个小时内最多有15个执行者会在任务结束前失败。</p><p id="c97a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">g) <strong class="iw hj"> —驱动内存</strong> 10g:提供足够高的驱动内存，以便在要处理突发消息的情况下不会失败。</p><p id="90d7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我希望这份材料对那些刚开始学习结构化流的人有用。能够为开源社区做出贡献将是一件愉快的事情，通过这个社区我学到了很多东西。</p><p id="ab7c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有关更详细的技术概述，请访问<a class="ae kp" href="https://spark.apache.org/docs/2.0.0/structured-streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/2 . 0 . 0/structured-streaming-programming-guide . html</a></p></div></div>    
</body>
</html>