<html>
<head>
<title>Adversarial Audio Synthesis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对抗性音频合成</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/adversarial-audio-synthesis-6250e297d4b2?source=collection_archive---------13-----------------------#2020-04-18">https://medium.com/analytics-vidhya/adversarial-audio-synthesis-6250e297d4b2?source=collection_archive---------13-----------------------#2020-04-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="da2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">很久以来，GANs一直用于生成高质量的图像和视频。</p><p id="9211" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着一个又一个最新模型的出现，它们在听觉数据方面的适用性已经成为最近研究的焦点。</p><p id="60bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然这一过程需要大量的培训，但结果是值得注意的。</p><p id="b596" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有这些过程都是无人监管的，可以用来生成类似人类的语音或音乐。</p><p id="256d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将解释以下三篇论文。</p><ol class=""><li id="e7f3" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">(WaveGAN)对抗性音频合成-ICLR 2019</li><li id="9b66" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">甘辛斯-ICLR 2019</li><li id="e79e" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">MelGAN-neur IPS 2019</li></ol><h1 id="c201" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">韦弗根</h1><ol class=""><li id="13bc" class="jd je hi ih b ii kp im kq iq kr iu ks iy kt jc ji jj jk jl bi translated">该论文基于新颖的DCGAN基础设施(生成图像)。</li><li id="7ef6" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">它将迎合图像数据的模型的属性改变为音频波的属性。</li><li id="f082" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">因为图像可以被认为是2D矩阵，音频可以被认为是1D阵列，所有的操作都相应地改变了。</li><li id="badc" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">所有的5×5 2D卷积运算都变成了25个1D滤波器。</li><li id="cbe6" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated"><strong class="ih hj">相位混洗</strong>:避免了标准转置卷积，因为它导致发生器学习丢弃人工频率的琐碎策略，并使优化更加困难。在替换中引入了相位混洗。</li></ol><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/94ef5911676263eb8e27698f5e2e3292.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*f4YbtfWrCqJIM7CQ8amCZg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">在WaveGAN鉴频器的每一层，相位混洗操作通过均匀的{[ n，n]样本扰动每个特征图的相位，通过反射填充缺失的样本(虚线轮廓)。在这里，我们描绘了一个具有四个要素地图(n = 1)的图层的所有可能结果。来源:https://arxiv.org/pdf/1802.04208.pdf<a class="ae lk" href="https://arxiv.org/pdf/1802.04208.pdf" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><blockquote class="ll lm ln"><p id="bb55" class="if ig lo ih b ii ij ik il im in io ip lp ir is it lq iv iw ix lr iz ja jb jc hb bi translated">相位混洗在输入到下一层之前，通过n至n个样本随机扰动各层激活的相位。这使得鉴频器的工作更加困难，因为它要求与输入波形的相位不一致。</p></blockquote><p id="f3e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该论文使用最近邻方法和上采样层，在鉴别器和发生器中都应用了因子4x。</p><p id="b917" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我亲自在特斯拉K80上训练模型。该模型需要至少20，000个步骤来产生可识别的结果，但是，由于重音变化太高而无法收敛，所以仍然存在一些噪声。</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><h1 id="ea2c" class="jr js hi bd jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk md km kn ko bi translated">甘辛斯</h1><ol class=""><li id="69b6" class="jd je hi ih b ii kp im kq iq kr iu ks iy kt jc ji jj jk jl bi translated">这个建筑被用来制作音乐和类似的声音。</li><li id="f59f" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">GANsynth以并行方式生成整个音频序列，比标准WaveNet快大约50，000倍。</li><li id="4480" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">GANSynth从单个潜在向量生成整个音频片段，从而更容易理清音高和音色等全局特征。</li><li id="3650" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">GANSynth使用<a class="ae lk" href="https://arxiv.org/abs/1710.10196" rel="noopener ugc nofollow" target="_blank">渐进式GAN </a>架构，通过卷积从单个矢量到完整声音进行增量上采样。</li><li id="f409" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">该论文指出，卷积上采样的标准方法很困难，因为它没有很好地对准相位。</li><li id="f8c8" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">这篇论文使用的是瞬时频率。</li></ol><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es me"><img src="../Images/7b400b9ee2a4fdcf7279a310efbd4ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q6omGbJHvYbGjq8F8P-8rg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">来源:https://magenta.tensorflow.org/gansynth</figcaption></figure><blockquote class="ll lm ln"><p id="f578" class="if ig lo ih b ii ij ik il im in io ip lp ir is it lq iv iw ix lr iz ja jb jc hb bi translated">它是帧步距和信号周期之间的角度差的导数。</p></blockquote><p id="cbed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果产生了高保真音频。</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><h1 id="eada" class="jr js hi bd jt ju lz jw jx jy ma ka kb kc mb ke kf kg mc ki kj kk md km kn ko bi translated">梅尔根</h1><ol class=""><li id="dbbf" class="jd je hi ih b ii kp im kq iq kr iu ks iy kt jc ji jj jk jl bi translated">Lyrebird AI制作的非自回归模型。</li><li id="6aea" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">该模型非常轻，速度快得令人难以置信。</li><li id="6963" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">这可能是文本到语音转换系统中最接近的模型。</li><li id="4f61" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">这是<strong class="ih hj">第一篇</strong>在没有任何感知损失函数的情况下成功将频谱图转换为语音的论文。</li><li id="b164" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">他们把问题分成两步。</li></ol><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es mf"><img src="../Images/f05bdab1432925d7503305e0806b147a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*uy5UxF46JqaDGHPnJta1uw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">来源:<a class="ae lk" href="https://www.descript.com/post/ultra-fast-audio-synthesis-with-melgan" rel="noopener ugc nofollow" target="_blank">https://www . descript . com/post/ultra-fast-audio-synthesis-with-melgan</a></figcaption></figure><ol class=""><li id="8a36" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">以文本为条件对较低分辨率的表示建模，例如mel谱图序列</li><li id="8baa" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">根据mel频谱图序列(或另一种中间表示)对原始音频波形进行建模</li></ol><p id="1059" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lo">Mel-GAN是非自回归的意味着它不局限于一次产生一个音频，并且对先前的音频块没有因果依赖性。</em></p><ol class=""><li id="0362" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">他们使用<a class="ae lk" href="https://towardsdatascience.com/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5" rel="noopener" target="_blank">扩展卷积模块</a>，使过程完全并行。</li><li id="397a" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">他们使用基于窗口的三个鉴别器的方案。</li></ol><blockquote class="ll lm ln"><p id="a660" class="if ig lo ih b ii ij ik il im in io ip lp ir is it lq iv iw ix lr iz ja jb jc hb bi translated"><em class="hi">基于窗口的鉴别器学习在小音频块的分布之间进行分类。</em></p></blockquote><h1 id="9cb1" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">梅尔根显然重新定义了SOTA标准。</h1><p id="130c" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq mg is it iu mh iw ix iy mi ja jb jc hb bi translated">希望2020年能看到更多的论文！</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><p id="72c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考文献—文字、图片均已转载。：</p><p id="85b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lk" href="https://magenta.tensorflow.org/gansynth" rel="noopener ugc nofollow" target="_blank">https://magenta.tensorflow.org/gansynth</a></p><p id="4605" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lk" href="https://arxiv.org/pdf/1902.08710.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1902.08710.pdf</a></p><p id="7fb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lk" href="https://arxiv.org/pdf/1910.06711.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1910.06711.pdf</a></p><div class="mj mk ez fb ml mm"><a href="https://www.descript.com/post/ultra-fast-audio-synthesis-with-melgan" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab dw"><div class="mo ab mp cl cj mq"><h2 class="bd hj fi z dy mr ea eb ms ed ef hh bi translated">利用MelGAN实现超快速音频合成</h2><div class="mt l"><h3 class="bd b fi z dy mr ea eb ms ed ef dx translated">在这篇文章中，我们介绍MelGAN，一个由Lyrebird团队创建的新的原始音频波形生成模型，它是…</h3></div><div class="mu l"><p class="bd b fp z dy mr ea eb ms ed ef dx translated">www.descript.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na le mm"/></div></div></a></div><p id="5f11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lk" href="https://towardsdatascience.com/synthesizing-audio-with-generative-adversarial-networks-8e0308184edd" rel="noopener" target="_blank">https://towards data science . com/synthesizing-audio-with-generative-adversarial-networks-8e 0308184 edd</a></p></div></div>    
</body>
</html>