# ä½¿ç”¨ Tensorflow 2.0 æ„å»ºç”¨äºå›¾åƒåˆ†ç±»çš„ CNNğŸ˜

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/using-tensorflow-2-0-to-build-a-cnn-for-image-classification-4e50848ce1c0?source=collection_archive---------6----------------------->

**ç›®æ ‡å—ä¼—:ç†Ÿæ‚‰ Pythonï¼Œå¯¹ç¥ç»ç½‘ç»œæœ‰åŸºæœ¬äº†è§£çš„ ML çˆ±å¥½è€…ã€‚æˆ‘ç»ä¸æ˜¯ä¸“å®¶ï¼Œæƒ³åˆ†äº«æˆ‘å­¦åˆ°çš„ä¸œè¥¿ï¼**

TensorFlow ç‰ˆæœ¬äº 2019 å¹´ 9 æœˆ(å·®ä¸å¤š 1 å¹´å‰)å‘å¸ƒï¼Œä¿ƒè¿›äº†æœºå™¨å­¦ä¹ æ¨¡å‹çš„åˆ›å»ºå’Œä½¿ç”¨ã€‚å³ä½¿æœ‰ä¸€äº›ä½¿ç”¨ TensorFlow ç‰ˆçš„ç»éªŒï¼Œæˆ‘å‘ç°åœ¨ MNIST åŸºå‡†æ•°æ®é›†ä¸Šå®ç°ä¸€ä¸ªå‡†ç¡®ç‡çº¦ä¸º 98.5%çš„å·¥ä½œæ¨¡å‹æœ‰ä¸€ä¸ªé™¡å³­çš„å­¦ä¹ æ›²çº¿ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘æƒ³åˆ†äº«æˆ‘å¦‚ä½•åœ¨ Python 3 ä¸­ç”¨ TensorFlow 2.0 å®ç°äº†ä¸€ä¸ªå·ç§¯ç¥ç»ç½‘ç»œï¼Œä»¥ä¾¿å¸®åŠ©é‚£äº›å¯èƒ½æ­£åœ¨å­¦ä¹ æ›²çº¿ä¸­æŒ£æ‰çš„äººã€‚

# å®‰è£…æ–¹æ³•:

ä¸‹é¢æ˜¯æˆ‘ä½¿ç”¨çš„ç¡¬ä»¶å’Œè½¯ä»¶è§„æ ¼ï¼Œå®ƒä»¬å¯¼è‡´äº†ä¸€ä¸ªå·¥ä½œå®‰è£…å’Œä¸€ä¸ªå·¥ä½œæ¨¡å‹:

![](img/43c90b68370a1517e7df9227f93ed53f.png)

ç”±[éº¦æ–¯å¨å°”Â·å°¼å°”æ£®](https://unsplash.com/@maxcodes?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

*   æ“ä½œç³»ç»Ÿ:Windows 10
*   IDE: PyCharm 2019.3.5
*   Python 3.6.7
*   å¼ é‡æµ 2.3.0
*   NVIDIA GeForce GTX1050
*   NVIDIA é©±åŠ¨ç¨‹åºç‰ˆæœ¬ 451.67
*   CUDA 10.1(æ›´æ–° 2)
*   CUPTI 10.1(è‡ªå¸¦ CUDA)
*   cuDNN v7.6.5

ä¹‹å‰åœ¨ Tensorflow v1.0 (TF1)ä¸­ï¼Œæœ‰ä¸€ä¸ªå•ç‹¬çš„ GPU tensorflow åŒ…ï¼Œè€Œåœ¨ TF2ï¼Œæœ‰ä¸€ä¸ªé’ˆå¯¹ CPUã€GPU å’Œå¤š GPU tensorflow ç‰ˆæœ¬çš„å…¨å±€åŒ…ã€‚ç”¨ä½ çš„ Python åŒ…ç®¡ç†å™¨å®‰è£… TF2 (æˆ‘åœ¨ PyCharm ä¸­ä½¿ç”¨äº† pip)ã€‚ğŸ‘

TF2 åº”è¯¥é»˜è®¤ä½¿ç”¨ä½ çš„ CPUï¼Œä½†æ˜¯å¦‚æœä½ æœ‰ä¸€ä¸ª[æœ‰æ•ˆçš„ NVIDIA GPU](https://developer.nvidia.com/cuda-gpus) å¹¶ä¸”æƒ³åƒæˆ‘ä¸€æ ·ä½¿ç”¨å®ƒï¼Œé‚£ä¹ˆä½ å°±éœ€è¦å†éµå¾ªå‡ ä¸ª[å®‰è£…æ­¥éª¤](https://www.tensorflow.org/install/gpu)ã€‚ä½ å¿…é¡»å®‰è£…å„è‡ªçš„ [CUDA](https://developer.nvidia.com/cuda-toolkit-archive) ã€ [CUPTI](https://developer.nvidia.com/cuda-toolkit-archive) å’Œ [cuDNN](https://developer.nvidia.com/rdp/cudnn-archive) åº“ã€‚TF ç½‘ç«™ä¿å­˜äº†è¿™äº›åº“çš„å…¼å®¹ç‰ˆæœ¬åˆ—è¡¨[è¿™é‡Œ](https://www.tensorflow.org/install/source#linux)ã€‚

æ£€æŸ¥ TF2 æ˜¯å¦å·²æˆåŠŸå®‰è£…åœ¨æ‚¨çš„ç³»ç»Ÿä¸Š:

```
import tensorflow as tf
print(tf.constant('Hello from TensorFlow ' + tf.__version__))
```

å¯¹äº GPU æ„å»ºï¼Œè¯·æ£€æŸ¥æ‚¨æ˜¯å¦æœ‰ä¸€ä¸ª GPUï¼Œå¹¶ä¸ºå…¶å®‰è£…äº† TF2:

```
import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
print(tf.test.is_built_with_cuda())
```

å¦‚æœè¿™äº›è¡Œå¦‚é¢„æœŸè¿è¡Œï¼Œé‚£ä¹ˆæ­å–œä½ ğŸ‰ï¼Œæ‚¨å°±å¯ä»¥åœ¨ TensorFlow 2.0 ä¸­å»ºç«‹æ¨¡å‹äº†ã€‚å¦‚æœä½ åœ¨å®‰è£…è¿‡ç¨‹ä¸­æœ‰é—®é¢˜(æˆ‘è‚¯å®šæœ‰)ï¼Œè¯·åœ¨ä¸‹é¢è¯„è®ºï¼Œæˆ‘ä¼šå°½åŠ›æä¾›åé¦ˆã€‚

# æ„å»ºæ¨¡å‹æ¡†æ¶:

ç°åœ¨è®©æˆ‘ä»¬æ¥çœ‹çœ‹æˆ‘ä»¬å°†ç”¨æ¥åœ¨ TF2 æ„å»ºå®šåˆ¶æœºå™¨å­¦ä¹ (ML)æ¨¡å‹çš„ä¸»å¹²ã€‚æ³¨æ„ï¼Œè¿™ä¸ªæ¡†æ¶ä½¿ç”¨äº† Keras åŒ…ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜çº§ TensorFlow åŒ…è£…å™¨ï¼Œä½¿å¾—å®šåˆ¶ ML æ¨¡å‹(å°¤å…¶æ˜¯ CNN çš„)æ›´åŠ å®¹æ˜“ã€‚è¿™é‡Œæˆ‘ç”¨ 32c3p 2â€“32c3p 2â€“32c5s 2-D128-D10 æ¶æ„åšäº†ä¸€ä¸ª CNN å›¾åƒåˆ†ç±»å™¨:

```
import os
# Turn off TensorFlow warning messages in program output
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, BatchNormalization, Dropout

class Image_CNN(Model):
  def __init__(self):
    super().__init__()
    self.conv1 = Conv2D(32, 3, input_shape=(..., 3), strides=1, activation='relu')
    self.conv2 = Conv2D(32, 3, strides=1, activation='relu')
    self.conv3 = Conv2D(32, 5, strides=2, activation='relu')

    self.pool1 = MaxPool2D(pool_size=(2,2))
    self.batchnorm = BatchNormalization()
    self.dropout40 = Dropout(rate=0.4)

    self.flatten = Flatten()
    self.d128 = Dense(128, activation='relu')
    self.d10softmax = Dense(10, activation='softmax')

  def call(self, x, training=False):
    x = self.conv1(x)
    x = self.pool1(x)
    x = self.conv2(x)
    x = self.pool1(x)
    x = self.conv3(x)
    x = self.batchnorm(x)
    if training:
        x = self.dropout40(x, training=training)

    x = self.flatten(x)
    x = self.d128(x)
    x = self.d10softmax(x)

    return x
```

å¦‚ä½ æ‰€è§ï¼Œæˆ‘ä»¬å¯¼å…¥äº† CNN çš„æ„å»ºå—ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„â€œImage_CNNâ€ç±»ä¸­å®ç°äº†å®ƒä»¬ã€‚è‡ªå®šä¹‰æ¨¡å‹ç»§æ‰¿è‡ª Keras æ¨¡å‹ï¼Œå¿…é¡»åŒ…å« 2 ä¸ªæ–¹æ³•(å‡½æ•°):â€œ__init__â€å’Œâ€œcallâ€ã€‚â€œ__init__â€æ–¹æ³•å®šä¹‰äº†å±‚,â€œcallâ€æ–¹æ³•å®šä¹‰äº†åœ¨æ€¥åˆ‡æ‰§è¡Œä¸‹ä½¿ç”¨çš„å±‚çš„é¡ºåºå’Œæ•°é‡ï¼Œè¿™æ˜¯ TF2 æœ€å¤§çš„æ–°ç‰¹æ€§ä¹‹ä¸€ã€‚

**æ³¨æ„#1** :ä¸ºäº†ä½¿ä½ çš„ ML æ¨¡å‹å®Œå…¨å®šåˆ¶åŒ–ï¼Œä½ åº”è¯¥å®šä¹‰ä½ çš„æ¨¡å‹çš„æ¯ä¸€ä¸ªæ„å»ºå—(å±‚),è€Œä¸æ˜¯ä½¿ç”¨é¢„å…ˆæ‰“åŒ…çš„ Keras å±‚ã€‚

**æ³¨æ„#2** :åœ¨è¿™ä¸ª CNN ä¸­ï¼Œæˆ‘åœ¨å·ç§¯æ­¥éª¤åæ·»åŠ äº†ä¸€ä¸ªæ¼å±‚ã€‚è¿™ä¸ä»…æœ‰åŠ©äºè®­ç»ƒç¥ç»ç½‘ç»œï¼Œå®ƒåº”è¯¥åªç”¨äºè®­ç»ƒ(æˆ‘ä»¬ä¸æƒ³åˆ é™¤æˆ‘ä»¬çš„ä»»ä½•æµ‹è¯•ç»“æœ)ã€‚

# å®šä¹‰è®­ç»ƒå’Œæµ‹è¯•æ­¥éª¤:

æˆ‘ä»¬å®šä¹‰äº†å°†ä¸ºæ¯ä¸ªæ•°æ®ç‚¹å’Œæ¯ä¸ªæ—¶æœŸæ‰§è¡Œçš„è®­ç»ƒå‡½æ•°å’Œæµ‹è¯•å‡½æ•°ã€‚

```
@tf.function
def train_step(images, labels):
  with tf.GradientTape() as tape:
    # training=True is only needed if there are layers with different
    # behavior during training versus inference (e.g. Dropout).
    predictions = model(images, training=True)
    loss = loss_object(labels, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_loss(loss)
  train_accuracy(labels, predictions)

@tf.function
def test_step(images, labels):
  # training=False is only needed if there are layers with different
  # behavior during training versus inference (e.g. Dropout).
  predictions = model(images, training=False)
  t_loss = loss_object(labels, predictions)

  test_loss(t_loss)
  test_accuracy(labels, predictions)
```

æ¯ä¸ªå‡½æ•°éƒ½ç”±ä¸€ä¸ª tensorflow è£…é¥°å‡½æ•°åŒ…è£…ï¼Œè¯¥å‡½æ•°å°† Python å‡½æ•°è½¬æ¢ä¸ºé™æ€ tensorflow å›¾ã€‚å› ä¸º TF2 ä½¿ç”¨æ€¥åˆ‡æ‰§è¡Œï¼Œè¿™äº›å‡½æ•°çš„é€è¡Œæ±‚å€¼å¯èƒ½ä¼šå¾ˆæ…¢ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å‡½æ•°è½¬æ¢æˆä¸€ä¸ªé™æ€å›¾æ¥åŠ é€Ÿä»£ç æ‰§è¡Œã€‚ğŸ‘

æ‚¨è¿˜ä¼šæ³¨æ„åˆ°ï¼Œåœ¨â€œtrain_stepâ€æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† tfã€‚TF2 çš„å¦ä¸€ä¸ªæ–°ç‰¹è‰²ã€‚è¿™ä¹Ÿæ˜¯å‘ TensorFlow çš„æ€¥åˆ‡æ‰§è¡Œæ–¹æ³•è½¬å˜çš„ç»“æœã€‚å› ä¸ºæˆ‘ä»¬çš„æ¨¡å‹æ€¥åˆ‡åœ°æ‰§è¡Œ(è€Œä¸æ˜¯ä½œä¸ºé™æ€å›¾å½¢)ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å®ƒä»¬è¿è¡Œæ—¶è·Ÿè¸ªæ¯ä¸€å±‚çš„æ¢¯åº¦ï¼Œæˆ‘ä»¬ä½¿ç”¨ GradientTape æ¥å®Œæˆè¿™é¡¹å·¥ä½œã€‚è¿™äº›æ¢¯åº¦ç„¶åè¢«é¦ˆé€åˆ°æ‰€é€‰æ‹©çš„ä¼˜åŒ–å™¨ä¸­ï¼Œä»¥é€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•°æ¥ç»§ç»­å­¦ä¹ è¿‡ç¨‹ã€‚ğŸ§ 

# å‡†å¤‡æ•°æ®é›†å¹¶è¿è¡Œæ¨¡å‹:

![](img/a44dad8c82db9612dbefb062830eb91f.png)

ç”±[æ‹æ‘„çš„äºšå†å±±å¤§Â·è¾›æ©](https://unsplash.com/@swimstaralex?utm_source=medium&utm_medium=referral)åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

æˆ‘ä»¬ç°åœ¨å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æˆ‘ä»¬çš„ç¥ç»ç½‘ç»œï¼åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åŠ è½½äº†ç»å…¸çš„æ‰‹å†™æ•°å­— MNIST æ•°æ®é›†ï¼Œå®ƒå¯ä»¥ç›´æ¥ä» tf.keras.datasets å¯¼å…¥ã€‚æœ‰ 60ï¼Œ000 ä¸ªè®­ç»ƒå›¾åƒå’Œ 10ï¼Œ000 ä¸ªæµ‹è¯•å›¾åƒï¼Œæ¯ä¸ªå›¾åƒçš„ç»´æ•°ä¸º 28x28ã€‚ç„¶åä½¿ç”¨ tf.data.Dataset å°†è¿™äº›å›¾åƒä»¥ 32 ä¸ªä¸€æ‰¹çš„æ–¹å¼è¾“å…¥åˆ°æˆ‘ä»¬çš„æ¨¡å‹ä¸­ã€‚

```
if __name__ == '__main__':
    import time
    start_time = time.time()

    # Load MNIST images and normalize pixel range to 0-1.
    mnist = tf.keras.datasets.mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0

    # Add a channels dimension.
    x_train = x_train[..., tf.newaxis].astype("float32")
    x_test = x_test[..., tf.newaxis].astype("float32")

    # Set up a data pipeline for feeding the training and testing data into the model.
    shuff_size = int(0.25 * len(y_train))
    batch_size = 32
    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(shuff_size).batch(batch_size)
    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)

    # Instantiate our neural network model from the predefined class. Also define the loss function and optimizer.
    model = Image_CNN()
    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    optimizer = tf.keras.optimizers.Adam()

    # Define the metrics for loss and accuracy.
    train_loss = tf.keras.metrics.Mean(name='train_loss')
    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
    test_loss = tf.keras.metrics.Mean(name='test_loss')
    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

    # Run and iterate model over epochs
    EPOCHS = 5
    for epoch in range(EPOCHS):

      # Reset the metrics at the start of the next epoch
      train_loss.reset_states()
      train_accuracy.reset_states()
      test_loss.reset_states()
      test_accuracy.reset_states()

      # Train then test the model
      for images, labels in train_ds:
        train_step(images, labels)
      for test_images, test_labels in test_ds:
        test_step(test_images, test_labels)

      # Print results
      template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
      print(template.format(epoch + 1,
                            train_loss.result(),
                            train_accuracy.result() * 100,
                            test_loss.result(),
                            test_accuracy.result() * 100))
    print("time elapsed: {:.2f}s".format(time.time() - start_time))
```

å› ä¸ºæˆ‘ä»¬æ­£åœ¨æ‰§è¡Œå¤šæ ‡ç­¾å›¾åƒåˆ†ç±»ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ç²¾åº¦åº¦é‡å°†æ˜¯åˆ†ç±»äº¤å‰ç†µï¼Œå¹¶ä¸”æˆ‘ä»¬å°†åœ¨ 5 ä¸ªæ—¶æœŸå†…è®­ç»ƒæˆ‘ä»¬çš„ CNNã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åšå¾—æ€ä¹ˆæ ·:

```
Epoch 1, Loss: 1.5532550811767578, Accuracy: 92.288330078125, Test Loss: 1.4825093746185303, Test Accuracy: 98.0Epoch 2, Loss: 1.4951773881912231, Accuracy: 96.80332946777344, Test Loss: 1.4800351858139038, Test Accuracy: 98.12999725341797Epoch 3, Loss: 1.488990306854248, Accuracy: 97.30999755859375, Test Loss: 1.4788316488265991, Test Accuracy: 98.2699966430664Epoch 4, Loss: 1.4862409830093384, Accuracy: 97.55833435058594, Test Loss: 1.4759035110473633, Test Accuracy: 98.58000183105469Epoch 5, Loss: 1.484948992729187, Accuracy: 97.66667175292969, Test Loss: 1.475019931793213, Test Accuracy: 98.63999938964844time elapsed: 26.20s
```

æˆ‘ä»¬å·²ç»æˆåŠŸå®‰è£…äº† TF2ï¼Œåˆ›å»ºäº† CNN å›¾åƒåˆ†ç±»å™¨ï¼Œå¹¶ä½¿ç”¨ GPU åœ¨ 30 ç§’å†…å®ç°äº†è‰¯å¥½çš„æµ‹è¯•å‡†ç¡®æ€§ï¼

å¦‚æœä½ å¯¹æˆ‘ä»¬åœ¨âœŒçš„è¿‡ç¨‹æœ‰ä»»ä½•é—®é¢˜å’Œè¯„è®ºï¼Œæ¬¢è¿åœ¨ä¸‹é¢å‘å¸–

```
import os
# Turn off TensorFlow warning messages in program output
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, BatchNormalization, Dropout

class Image_CNN(Model):
  def __init__(self):
    super().__init__()
    self.conv1 = Conv2D(32, 3, input_shape=(..., 3), strides=1, activation='relu')
    self.conv2 = Conv2D(32, 3, strides=1, activation='relu')
    self.conv3 = Conv2D(32, 5, strides=2, activation='relu')

    self.pool1 = MaxPool2D(pool_size=(2,2))
    self.batchnorm = BatchNormalization()
    self.dropout40 = Dropout(rate=0.4)

    self.flatten = Flatten()
    self.d128 = Dense(128, activation='relu')
    self.d10softmax = Dense(10, activation='softmax')

  def call(self, x, training=False):
    x = self.conv1(x)
    x = self.pool1(x)
    x = self.conv2(x)
    x = self.pool1(x)
    x = self.conv3(x)
    x = self.batchnorm(x)
    if training:
        x = self.dropout40(x, training=training)

    x = self.flatten(x)
    x = self.d128(x)
    x = self.d10softmax(x)

    return x

@tf.function
def train_step(images, labels):
  with tf.GradientTape() as tape:
    # training=True is only needed if there are layers with different
    # behavior during training versus inference (e.g. Dropout).
    predictions = model(images, training=True)
    loss = loss_object(labels, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_loss(loss)
  train_accuracy(labels, predictions)

@tf.function
def test_step(images, labels):
  # training=False is only needed if there are layers with different
  # behavior during training versus inference (e.g. Dropout).
  predictions = model(images, training=False)
  t_loss = loss_object(labels, predictions)

  test_loss(t_loss)
  test_accuracy(labels, predictions)

if __name__ == '__main__':
    import time
    start_time = time.time()

    # Load MNIST images and normalize pixel range to 0-1.
    mnist = tf.keras.datasets.mnist
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0

    # Add a channels dimension.
    x_train = x_train[..., tf.newaxis].astype("float32")
    x_test = x_test[..., tf.newaxis].astype("float32")

    # Set up a data pipeline for feeding the training and testing data into the model.
    shuff_size = int(0.25 * len(y_train))
    batch_size = 32
    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(shuff_size).batch(batch_size)
    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)

    # Instantiate our neural network model from the predefined class. Also define the loss function and optimizer.
    model = Image_CNN()
    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    optimizer = tf.keras.optimizers.Adam()

    # Define the metrics for loss and accuracy.
    train_loss = tf.keras.metrics.Mean(name='train_loss')
    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')
    test_loss = tf.keras.metrics.Mean(name='test_loss')
    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')

    # Run and iterate model over epochs
    EPOCHS = 5
    for epoch in range(EPOCHS):

      # Reset the metrics at the start of the next epoch
      train_loss.reset_states()
      train_accuracy.reset_states()
      test_loss.reset_states()
      test_accuracy.reset_states()

      # Train then test the model
      for images, labels in train_ds:
        train_step(images, labels)
      for test_images, test_labels in test_ds:
        test_step(test_images, test_labels)

      # Print results
      template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
      print(template.format(epoch + 1,
                            train_loss.result(),
                            train_accuracy.result() * 100,
                            test_loss.result(),
                            test_accuracy.result() * 100))
    print("time elapsed: {:.2f}s".format(time.time() - start_time))
```