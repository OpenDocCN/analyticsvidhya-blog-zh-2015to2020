<html>
<head>
<title>Running a super-resolution neural network on Raspberry Pi GPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Raspberry Pi GPU上运行超分辨率神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/inferring-a-super-resolution-neural-network-on-raspberry-pi-gpu-89b5456d21ef?source=collection_archive---------2-----------------------#2020-05-17">https://medium.com/analytics-vidhya/inferring-a-super-resolution-neural-network-on-raspberry-pi-gpu-89b5456d21ef?source=collection_archive---------2-----------------------#2020-05-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/850fcff3973296fb2066330550000785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kP7NA4l-vepxgaf2pnywUw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">向Raspberry Pi Zero W问好，这是一台10美元的计算机，具有可编程的GPU。在接下来的大约3700字中，我们将抑制它推断神经网络的能力，以便以一种好的方式放大图片。</figcaption></figure><p id="06ee" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi js translated">假设你有一张图像，比如说4K (3840*2160像素)被重采样到一个更小的分辨率，比如全高清(1920*1080)。较小的图像占用较少的存储空间，处理和通过互联网发送速度更快。然而，当要在你的4K屏幕上显示这个图像时，你可能更喜欢原始的4K图像。</p><p id="3d7e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">你确实可以使用标准的插值方法(双线性、双三次等)将全高清图像插值到4K分辨率。)无论如何，当画面被拉伸到整个屏幕时都会发生这种情况。这肯定不会产生最好的结果，尽管在全高清/4K比例下，除非你的屏幕非常大或者你放大了，否则你可能感觉不到差别。尽管如此，如果你有一个4K/8K屏幕，你可能会关心视觉质量。因此，对您来说，放大图像的一个选择是一种更精细的方法，以增加处理时间为代价来呈现更好的图片。这篇文章就是关于这种方法的。</p><p id="f5c4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在没有其他输入的情况下从低分辨率图像(<em class="kb"> LR </em>)重建较高分辨率图像(<em class="kb"> HR </em>)的问题是不适定的，因为从HR图像产生LR图像的下采样过程通常会导致信息损失，因此对于给定的LR图像和插值方法，有许多HR图像可能导致相同的LR输出。标准插值方法重建一个可能的HR对应物，不一定是最自然出现的那个。为了重建视觉上更好的HR图像，可能需要引入先验知识，而这正是神经网络所擅长的。</p><p id="6ec0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这些不恰当的问题，加上容易获得的数据，以及为购买8K屏幕的人提供更清晰图片的某种情感背景，导致了数千篇论文，每篇都声称自己是最好的。</p><p id="c779" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">本文讨论</p><ul class=""><li id="3ae2" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr kh ki kj kk bi translated"><strong class="iw hj">一个小型的易于训练的全卷积架构</strong>渲染2倍高分辨率的图像受到Twitter工程师的启发<a class="ae kl" href="https://arxiv.org/pdf/1609.05158.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="kb"> ESPCN network </em> </a>，</li><li id="ddc8" class="kc kd hi iw b ix km jb kn jf ko jj kp jn kq jr kh ki kj kk bi translated">一种仅使用OpenGL ES 2.0兼容着色器实现推理的方法，完全不需要CPU计算。后者允许<strong class="iw hj">在笔记本电脑、Android智能手机和</strong> <strong class="iw hj"> Raspberry Pi上运行该模型，所有这些都在GPU上运行。</strong></li></ul><p id="2ea9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我没有8K屏幕，但我有一个树莓派。在接下来的内容中，我不会试图超越最先进的结果，所以不会有很高的PSNR数字。相反，这里的重点是让事情变得实用。在Raspberry Pi GPU上运行推理是这个项目的目标，因此主要成果是在大量设备上使用OpenGL进行神经网络的推理:如果Raspberry Pi GPU做到了这一点，那么几乎任何像样的GPU都可以做到。</p><p id="ca51" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面有图像、PSNR和时间测量以及一些代码。让我们开始吧。</p><h1 id="655d" class="kr ks hi bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">体系结构</h1><p id="98f3" class="pw-post-body-paragraph iu iv hi iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr hb bi translated">这里的模型是ESPCN的化身。使用OpenGL ES 2.0作为推理后端和它的Raspberry Pi实现对架构施加了一些限制，使得网络根据现代ML实践有些不常见，但是让我们暂时接受它，稍后讨论这个问题。</p><p id="25cb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">与ESPCN的主要区别在于:</p><ul class=""><li id="e850" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr kh ki kj kk bi translated">使用分组卷积，然后是逐点(1×1)卷积，</li><li id="01a9" class="kc kd hi iw b ix km jb kn jf ko jj kp jn kq jr kh ki kj kk bi translated">激活函数是在[0，1]范围内<em class="kb">有界的ReLU </em>。它应用于所有卷积的顶部。</li></ul><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/84da52d09d6f96ec5c408b0c3f758103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9B1SoFKN0-5AwmmKz2wnmQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">提议的架构。主要成分是5x5、分组3x3、1x1卷积和BReLU激活。</figcaption></figure><p id="0aa3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该模型对LR图像中给定像素的9x9像素邻域进行操作，并以如下所示的方式在输出中产生2x2像素。我们处理灰度图像:训练是在亮度(Y)通道上进行的，而推理可以分别在R、G、B通道上进行，或者只在Y通道上进行，并且色度分量通过标准内插进行放大。后一种技巧非常常见:基本原理是任何图像或视频编码器都会忽略色度分量，并对其进行额外的缩减，这一切都是因为我们的眼睛对色度分辨率不太敏感。</p><h1 id="7e8b" class="kr ks hi bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">培训和验证</h1><p id="86b5" class="pw-post-body-paragraph iu iv hi iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr hb bi translated">有完整TensorFlow 2 / Keras食谱的食谱可以在<a class="ae kl" href="https://gist.github.com/lnstadrum/0e2fec1e96bc6791f0ee3487969632ee" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="cae7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">该模型只有7340个可训练参数。</strong>不需要数十亿的图像来训练这样一个微小的东西，没有过拟合的风险，所以<a class="ae kl" href="https://data.vision.ee.ethz.ch/cvl/DIV2K/" rel="noopener ugc nofollow" target="_blank">一个非常经典的超分辨率数据集DIV2K </a>被用于训练。它包含800 HR图像和它们的LR对应物。由于图像大小不同，我将它们切割成固定大小的小块，例如LR域中的128x128像素。使用均方误差作为损失函数是一种简单的方法，并且只要PSNR被用作质量度量，它就非常合适。</p><p id="8a46" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然后以一种简单的方式进行训练:使用Adam optimizer，做大约200+200个周期，中间学习率从1e-3下降到1e-4。在我的GeForce RTX2070上花了大约12个小时。</p><p id="3221" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">训练好的模型在DIV2K验证集上达到33.26 dB。</strong>这不是一个非常好的分数，尽管对于一个7K可训练参数的模型来说，这已经很了不起了。下面是一个图表来定位这个结果在最先进的状态。另一个图表:<a class="ae kl" href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank"> EDSR+网络</a>在同一数据集上达到35.12 dB，这是当今最高的分数之一，有大约4300万个参数。我们正处于双三次和EDSR+的中间，模型容量如此之小！</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/c8b8d8e913ebf79edc7a0c3f516c6031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EPcFgiigKKv8oTvb7wKpxg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">相同验证集(DIV2K x2双三次)上的PSNR/推理时间记分板。我们的模型位于蓝线上的某处，这取决于哪个GPU用于推理。但是等等，这在下一章。原图来自<a class="ae kl" href="http://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Agustsson_NTIRE_2017_Challenge_CVPR_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank"> NITRE2017挑战论文</a>顺便说一下。</figcaption></figure><p id="1f0d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们来看看图片。</p><p id="83fd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">超分辨率很像锐化，所以我们可以认为我们的模型是向上扩展+锐化的(奇怪的是，它实际上是在最后向上扩展)。我们的网络渲染的图像看起来更清晰，即使你必须放大很多才能看到差异。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/2b9536671d5ca21e89d0816676a07f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQq_JUifDMazciboI394dA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">DIV2K的一些图片的结果。在列中，从上到下是:(1) LR输入，(2) OpenCV的双三次上界(基线)，(3)我们的结果和(4) HR地面实况。我们的模型在这两幅图像上分别达到了比基线高<strong class="bd kt"> 2.83 </strong>和<strong class="bd kt"> 2.92 </strong> dB的PSNR。不完美，但也不坏:在大多数情况下，我们的结果比基线更接近HR图像。</figcaption></figure><h2 id="d682" class="mb ks hi bd kt mc md me kx mf mg mh lb jf mi mj lf jj mk ml lj jn mm mn ln mo bi translated">微调</h2><p id="e55c" class="pw-post-body-paragraph iu iv hi iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr hb bi translated">现在让我们考虑一下<strong class="iw hj">在我们没有真实的HR图像时的真实应用。</strong>然而，为了执行一些客观的(基于PSNR的)比较，我们可以将任何图像视为HR，产生其LR代表，然后用不同的方法对其进行放大并进行比较。</p><p id="212d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这里，我们使用另一个非常经典的SR数据集，<a class="ae kl" href="https://www.kaggle.com/ll01dm/set-5-14-super-resolution-dataset" rel="noopener ugc nofollow" target="_blank"> Set5和Set14 </a>。例如，我们将蝴蝶图像作为HR，并用手头的双三次缩减器(OpenCV <code class="du mp mq mr ms b">resize</code>函数中有一个)将其缩小。这就变成了LR图像。然后我们用同样的双三次插值和训练好的模型把它放大。我们得到的结果令人惊讶地失望:图像是丑陋的夏普和PSNR比双三次结果更糟！</p><p id="912e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">结果是，我们的训练集中的LR和HR图像由另一个双三次插值器<a class="ae kl" href="https://www.mathworks.com/help/images/ref/imresize.html" rel="noopener ugc nofollow" target="_blank">相关，可能是来自Matlab的插值器，</a>其前面可以是抗混叠滤波器，并且网络在学习近似其反演时自然会对其产生影响。这个问题非常普遍:OpenCV的双三次<a class="ae kl" href="https://stackoverflow.com/questions/22092744/what-is-the-difference-between-opencvs-and-matlabs-bicubic-algorithm" rel="noopener ugc nofollow" target="_blank">确实不同于Matlab one的</a>(人们<a class="ae kl" href="https://github.com/fatheral/matlab_imresize" rel="noopener ugc nofollow" target="_blank">试图用Python重新实现它</a>)，我们的模型并不是唯一遭受这种差异的模型:例如，超分辨率场景中的明星<a class="ae kl" href="https://github.com/xinntao/ESRGAN" rel="noopener ugc nofollow" target="_blank"> ESRGAN </a>，<a class="ae kl" href="https://github.com/xinntao/ESRGAN/tree/master/models" rel="noopener ugc nofollow" target="_blank">可能会产生伪像</a>，如果你的双三次插值器不同于训练中使用的插值器。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/4a513f7be9225005350ce4313764c2fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BxyqgeUmGgbOuYJknE9wxQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">从左到右:原始图像(HR)，双三次插值结果，我们在DIV2K上训练的模型的输出，以及我们在用一堆退化内核扩展的相同数据上微调的模型的输出(细节如下)。双三次插值达到<strong class="bd kt"> 26.90 dB </strong>。如果不进行微调，我们的模型无法产生这种严重锐化的图像(<strong class="bd kt"> 25.05 dB </strong>)，但一旦进行微调(<strong class="bd kt"> 29.92 dB </strong>)，就会赶上并超过基线。</figcaption></figure><p id="62d9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从研究的角度来看，这一切都是有意义的……但却令人沮丧地不切实际。如果您没有参考HR图像，并且您没有掌握整个图像形成过程，您很可能没有任何种类的“退化内核”。在一个非常普通的环境中，你只有一个以清晰的视觉愉悦的方式放大的图像，你不知道它被重新采样了多少次，也不知道它在模糊的过去还经历了什么。</p><p id="cf89" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了以简单的方式解决这个问题，我用尽可能多的退化内核产生的LR图像扩展了训练集，简单地采用OpenCV的<code class="du mp mq mr ms b">resize</code>中提供的东西:双线性、双三次和Lanczos插值器，渲染新的LR图像并将它们添加到原始图像中。然后，预训练模型以同样的方式重新适应新的更大的数据。这使得在Set5和Set14上的性能优于双三次基线，但仍有轻微的增益，产生了明显的视觉差异(并且没有难看的锐化图像)。付出的代价是原始DIV2K验证集上的PSNR:微调后的模型只达到了<strong class="iw hj"> 32.57 dB </strong>而不是33.26 dB。</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用多个降级内核进行微调对Set5和Set14上的PSNR的影响。</figcaption></figure><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/8140a267fbfc3504ce9c8fced5f2e597.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*69ANNnqpQD2Ze_tN_512Xw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">Set14中图像的双三次基线(左，24.99 dB)与微调模型(右，26.11 dB)。当进行双三次缩减时，没有应用抗锯齿，因此我们的网络学习校正锯齿伪像并产生平滑的边缘。不用放大就能看出区别，对吧？</figcaption></figure><p id="2232" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">虽然模型及其训练可以在许多方面进一步探索和改进，但这将是我们最终的模型。所以让我在这里停下来，机器学习部分，并进行推理实现。</p><h1 id="bf6a" class="kr ks hi bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">使用OpenGL实现推理</h1><p id="0f4a" class="pw-post-body-paragraph iu iv hi iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr hb bi translated">在图像上推断卷积神经网络通常需要大量的计算。幸运的是，它很容易并行化，因此适合GPU。</p><p id="5960" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">GPU本来就是来渲染图片的。很久以前，图形流水线是固定的T4(不可编程的),能够执行一组预定义的标准计算机图形操作。但是用它们来做更多的通用计算让我们很多人感到困惑。曾经有一个专门的术语叫“GPGPU”，今天你几乎找不到了。</p><p id="ea86" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是因为事情发生了变化，现在我们通过CUDA、OpenGL计算着色器、OpenCL等特定接口使用GPU来计算几乎任何东西。比如TensorFlow用CUDA和GPU说话。CUDA是Nvidia的专有技术，所以如果你有另一家供应商的显卡，你可能不会很快通过TensorFlow充分利用你的硬件。但也许我太悲观了:前一段时间<a class="ae kl" href="https://blog.tensorflow.org/2019/01/tensorflow-lite-now-faster-with-mobile.html" rel="noopener ugc nofollow" target="_blank"> TensorFlow Lite为一些模型和应用程序引入了OpenGL计算着色器支持</a>。这使得在没有大量使用Nvidia GPU的Android设备上进行人脸检测时，可以帮助CPU。</p><p id="fcff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">OpenGL无处不在。来自任何厂商的任何像样的GPU都符合通常提供某种程度的可编程性的某种版本的OpenGL。</p><p id="f5ce" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">树莓派也是。我说的不是目前最新的Pi模型，<em class="kb"> 4 Model B </em>，它的<a class="ae kl" href="https://www.khronos.org/conformance/adopters/conformant-products/opengles#submission_882" rel="noopener ugc nofollow" target="_blank"> GPU符合OpenGL ES 3.1标准</a>，这使得它几乎能够完成一部像样的Android智能手机能够完成的任何事情。我在这里说的是所有其他型号的Pi只符合OpenGL ES 2.0标准。这意味着:没有计算着色器，只有顶点和片段着色器，没有输入/输出浮点，无法在单个着色器中产生多维输出(只有四个8位标量)…</p><p id="fa8c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">不管怎样，运行我们刚刚建立的模型的推论就足够了。</p><p id="07f6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">值得注意的是，如果你爱上了Raspberry Pi，有更高效的方法可以在没有OpenGL开销的情况下访问它的GPU计算能力:<a class="ae kl" href="https://petewarden.com/2014/08/07/how-to-optimize-raspberry-pi-code-using-its-gpu/" rel="noopener ugc nofollow" target="_blank">这里</a>、<a class="ae kl" href="https://rpiplayground.wordpress.com/2014/05/03/hacking-the-gpu-for-fun-and-profit-pt-1/" rel="noopener ugc nofollow" target="_blank">这里</a>甚至是一个在Pi上做GPU的Python库<a class="ae kl" href="https://github.com/nineties/py-videocore" rel="noopener ugc nofollow" target="_blank">这里</a>。这变得非常特定于Pi，但是您可能会运行得更快。在这里，我继续使用OpenGL，愿意在其他设备上运行推理。</p><h2 id="420e" class="mb ks hi bd kt mc md me kx mf mg mh lb jf mi mj lf jj mk ml lj jn mm mn ln mo bi translated">概观</h2><p id="3f40" class="pw-post-body-paragraph iu iv hi iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr hb bi translated">简单来说，<strong class="iw hj">我们用GLSL </strong> ( <em class="kb"> OpenGL着色语言</em>)编写的小程序(着色器)的形式实现推理过程中执行的操作。着色器还将包含硬编码的训练网络权重。所有的图像和特征图变成<em class="kb">纹理</em>，所有的输入LR图像分辨率。</p><p id="acf0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">GLSL很像C，只是有一些语法差异和限制。着色器在运行时由GPU驱动程序编译成GPU能够执行的硬件特定的二进制代码，就像CPU一样。但是还是有区别的，主要是由于GPU背后的硬件的SIMD性质。例如，GLSL不是一种图灵完全语言，所以你不能像在C++或Python中那样在GLSL代码中递归。幸运的是，对于前馈卷积神经网络的推断，我们不需要这一点。</p><p id="30e6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">由于在OpenGL ES 2.0标准中没有计算着色器，我们以传统的方式进行，我们需要一个顶点着色器和一个片段着色器来执行渲染过程。</p><ul class=""><li id="02ef" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr kh ki kj kk bi translated">我们的顶点着色器很简单:它们渲染一个四边形，将整个输入投影到整个视口。我不会在这里详述它们的代码。</li><li id="b0bc" class="kc kd hi iw b ix km jb kn jf ko jj kp jn kq jr kh ki kj kk bi translated"><strong class="iw hj">碎片着色器是神奇发生的地方。</strong>他们将对包含LR输入(用于输入层)或特征映射(用于隐藏和输出层)的输入纹理进行采样，并计算输出特征映射。片段着色器的GLSL代码由来自训练模型的Python脚本生成。</li></ul><p id="d520" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">要运行GLSL着色器，通常需要编写一些讨厌的平台相关代码来设置OpenGL上下文，并实现所有机器来执行渲染过程。我在这里跳过细节；反正整个代码是<a class="ae kl" href="https://github.com/lnstadrum/beatmup" rel="noopener ugc nofollow" target="_blank">可用</a>。</p><p id="0be8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们以上面解释的方式进行:输入的Y分量被神经网络放大，而Cb和Cr色度通道被放大为常规纹理。OpenGL天生支持双线性插值和最近邻插值(顺便说一下，最近邻插值用于采样所有的特征图)，因此色度可以双线性插值。这不是唯一的方法；可以在着色器中实现双三次色度插值，或者对R、G和B输入连续应用神经网络。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/f3250168da3d2d57dbf97505690bc2db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZG36S4L4g5-KSty9C4kDJQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">这是我们的模型，每个砖块都是一个着色器。他们总共有32人。每个着色器的输出是一个包含4个特征映射的纹理。它们都是输入LR分辨率，除了输出图像。</figcaption></figure><h2 id="e6a4" class="mb ks hi bd kt mc md me kx mf mg mh lb jf mi mj lf jj mk ml lj jn mm mn ln mo bi translated">限制</h2><p id="aafa" class="pw-post-body-paragraph iu iv hi iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr hb bi translated">如上所述，我们的模型是由来自Raspberry Pi OpenGL ES实现的约束形成的。让我最后解释一下。</p><ul class=""><li id="f697" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr kh ki kj kk bi translated">片段着色器是为每个像素执行的程序。它有一个单独的输出，四分量像素颜色写入<code class="du mp mq mr ms b">gl_FragColor </code>变量。因此<strong class="iw hj">我们只能在一个着色器中计算(最多)四个特征通道。它增加了我们需要的着色器的数量，但没有限制模型的大小，所以我们可以接受。这也极大地增加了内存带宽，因为特征贴图纹理被采样了很多次…但是据我所知，在Raspberry Pi上使用GL ES 2.0没有其他方法。</strong></li><li id="3535" class="kc kd hi iw b ix km jb kn jf ko jj kp jn kq jr kh ki kj kk bi translated">所有特征映射值都用[0，1]范围内的8位定点值采样。克服这一点的方法是使用一个激活函数，其输出范围适合[0，1]。这就是为什么<strong class="iw hj">我们处处用[0，1]-有界的ReLU作为激活函数</strong>。实际上，写入<code class="du mp mq mr ms b">gl_FragColor</code>的简单事实将值限制在[0，1]范围内，因此我们甚至不需要显式实现它:GLSL无论如何都会应用有界ReLU。酷！</li><li id="3736" class="kc kd hi iw b ix km jb kn jf ko jj kp jn kq jr kh ki kj kk bi translated">片段着色器的输入纹理数量有限，根据标准至少为8个。对于树莓派，它正好是8。由于纹理(最多)是包含RGBA颜色的4通道图像，我们最终在输入上最多有8*4=32个特征地图。这是一个实际的约束:为了计算2D卷积，我们需要在一个着色器中访问所有的特征映射。否则，我们必须在不同的着色器之间分割卷积，每个着色器最多采样32个通道，然后使用另一个着色器将部分结果放在一起…这很快就会变得一团糟，并且可能由于8位着色器输出采样限制而不可行。因此，<strong class="iw hj">所有的特征地图最多可以有32个通道</strong>。</li><li id="30f8" class="kc kd hi iw b ix km jb kn jf ko jj kp jn kq jr kh ki kj kk bi translated">有两个额外的条件限制了输入通道的数量。首先，每个着色器的纹理采样操作数量有限制(Pi为64)。为了计算3×3卷积，每个纹理被采样3×3次。在64个样本的限制下，我们最多可以有7个纹理，也就是28个特征图。但是对于1x1卷积，这不是问题。</li><li id="ee0e" class="kc kd hi iw b ix km jb kn jf ko jj kp jn kq jr kh ki kj kk bi translated">其次，每个着色器的指令总数是有限制的。在这个意义上，多个输入特征映射的3×3卷积是最贪婪的。一个具有12个输入和8个输出特征映射的实现通过了我手头的所有硬件(尽管它可能会进一步调整)。可能有一种方法可以像MobileNet一样通过深度方向的卷积获得更多的特征通道，但它会导致一个容量更小的模型，这在我所做的几次测试中似乎表现不佳。因此<strong class="iw hj">我们依靠12个特征映射的分组3×3卷积</strong>将逐点卷积放在分组块的顶部来混合它们的特征通道。这是塑造模型并在输出中给出48–32–24–16个特征图的关键设计决策。</li></ul><p id="bc3c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">就是这样，<strong class="iw hj">我们现在有了一个适合硬件的经过训练的模型。要让它运行起来，剩下的事情不多了！</strong></p><h2 id="c10e" class="mb ks hi bd kt mc md me kx mf mg mh lb jf mi mj lf jj mk ml lj jn mm mn ln mo bi translated">GLSL实施</h2><p id="faf4" class="pw-post-body-paragraph iu iv hi iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr hb bi translated">一旦考虑到硬件限制，将这样一个完全卷积的模型转换成一堆GLSL着色器就变得简单了:我们所需要的就是从经过训练的模型中获取权重和偏差，并在片段着色器中实现卷积。在您最喜欢的机器学习框架中访问层的训练参数通常不是问题(就像在TensorFlow 2 / Keras中使用<code class="du mp mq mr ms b">layer.kernel.numpy()</code>和<code class="du mp mq mr ms b">layer.bias.numpy()</code>获得Numpy数组一样简单)，因此Python脚本可以完成这项工作。</p><p id="ab4a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">至于在GLSL中存储网络参数，有不同的选择，例如将它们放入单独的纹理或统一的变量中。然而，这个模型很小，所以最简单的选择是将权重和偏差作为硬编码的常数在GLSL代码中公开。这可能也是最有效的方法。</p><p id="e1f0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是最后一个1x1卷积着色器(模型的第五层)的样子。就代码大小而言，它是32个着色器中最小的。它所做的只是采样16个输入特征图(每个4个通道的4个纹理)，将它们与学习的内核卷积，添加偏差并将结果写入片段颜色变量。</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="98a6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我使用<code class="du mp mq mr ms b">dot</code> GLSL函数来实现卷积。在我对Raspberry Pi进行的一些初步试验中，它看起来非常有效，但确实有其他方法来组织计算。</p><p id="bac7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">与卷积着色器一起，我们需要的最后一个片段着色器是一个合并了良好的上采样亮度和廉价的上采样色度的着色器(上面方案中的鲑鱼色着色器)。事实上，第五层输出是LR输入大小的4通道纹理；在每个像素位置，其4个通道包含输出HR亮度的4个像素值。因此，我们使用<code class="du mp mq mr ms b">gl_FragCoord</code> GLSL变量对这些值进行解复用，并在最后一个片段着色器中添加来自输入图像的色度:</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="mu mv l"/></div></figure><h1 id="4f2e" class="kr ks hi bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">有趣的部分:在所有硬件上进行测试</h1><h2 id="1daa" class="mb ks hi bd kt mc md me kx mf mg mh lb jf mi mj lf jj mk ml lj jn mm mn ln mo bi translated">树莓派</h2><p id="b020" class="pw-post-body-paragraph iu iv hi iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr hb bi translated">整个事情被设计成在Pi上运行，所以让测试在Pi上开始吧！</p><p id="1aa2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了将256*256的输入图像放大到512*512像素，<strong class="iw hj"> Raspberry Pi 3 Model B+平均在大约130 ms </strong>内运行推断(10次重复的标准偏差为9.5 ms)。Raspberry Pi Zero W <strong class="iw hj"> </strong>运行速度稍慢(171毫秒)，拥有相同的Broadcom video core IV GPU板载<a class="ae kl" href="https://en.wikipedia.org/wiki/VideoCore#Table_of_SoCs_adopting_VideoCore_SIP_blocks" rel="noopener ugc nofollow" target="_blank">以更低的时钟频率运行</a>。着色器在3.5到4秒内完成编译(提醒一下，这是一次性完成的，并不是针对我们可能需要处理的每张图像)。</p><p id="3c92" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这样慢吗？嗯，是的。但我们刚刚设法在Raspberry Pi GPU上运行了神经网络的推理，这仍然很棒。</p><figure class="lv lw lx ly fd ij er es paragraph-image"><div class="er es my"><img src="../Images/f3bd217f9162a6a7c5840680797092ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*eQ6n_QkshYITWe6N9Z0hGw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">为了便于查看，Set5(左上)的图像在这里缩小了四倍(右上)。然后，使用双三次插值(左下)或应用我们的模型两次(右下)，将它放大。我们得到了22.82分贝，双三次完成21.41分贝。是的，<strong class="bd kt">右下角的图片直接来自树莓派</strong>。</figcaption></figure><h2 id="5b9b" class="mb ks hi bd kt mc md me kx mf mg mh lb jf mi mj lf jj mk ml lj jn mm mn ln mo bi translated">安卓智能手机</h2><p id="14c0" class="pw-post-body-paragraph iu iv hi iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr hb bi translated">Android智能手机也有符合OpenGL ES的GPU，因此它们没有办法逃脱我们的测试。下面是在一些Android手机上执行同样的256*256到512*512上采样测试的图。</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="2701" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我没有把任何PSNR数字放在这里:来自不同GPU的图像之间没有明显的区别。在不同的硬件上，结果可能确实不同，但它们仍然非常相似。因此，在Pi(一台Android智能手机和一台高端桌面GPU)上从同一来源渲染的三幅图像中，两个最不同的图像彼此相差45.8 dB。</p><p id="727f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">另一个测试是通过网络实时传递小分辨率的相机预览。在没有任何额外像素传输的情况下，相机图像可以通过GLSL的<code class="du mp mq mr ms b">samplerExternalOES</code>采样器以OpenGL纹理的形式访问，所以我们可以直接将其插入我们的网络。</p><p id="a10e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="kb">华为P20 Lite </em>设法以6到7 FPS的速度将352*288的输入上采样到704*576的输出。我的老款<em class="kb">华硕K016 (Fonepad 8) </em>以~13.2 FPS做同样的工作。<strong class="iw hj"> <em class="kb">华为P10 </em>运行速度30 FPS！</strong></p><figure class="lv lw lx ly fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/7fa6be7a1f57fbdcb44b0753b98f3e2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-IizB0Xg8GFtctsPG2Tuw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在华为P20 Lite上运行352*288相机预览图像的推理。左:本机(双语)升级相机预览；右图:我们的网络结果。仔细看看中间的灯柱，右边的电缆，屋顶轮廓，纤细的树枝，路标。事实上，这个应用程序有点可笑，因为相机能够有更好的分辨率。尽管如此，将相机预览实时传递给Android GPU上的SR神经网络还是很花哨的，不是吗？</figcaption></figure><h2 id="d4cd" class="mb ks hi bd kt mc md me kx mf mg mh lb jf mi mj lf jj mk ml lj jn mm mn ln mo bi translated">桌面</h2><p id="60c9" class="pw-post-body-paragraph iu iv hi iw b ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn lt jp jq jr hb bi translated">桌面GPU通常更强大。因此，除了向他们提供来自Set5的256*256蝴蝶图像，我们还向他们提出了更大的挑战:<strong class="iw hj">全高清输入，以升级到4K </strong>。</p><p id="6c1f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以下是一些数字:</p><figure class="lv lw lx ly fd ij"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="58cf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在桌面GPU的数字中，也有Nvidia Jetson Nano的推理时间。<strong class="iw hj"> </strong>它更像是一个移动GPU，而不是桌面GPU，但在这次测试中，它很容易与集成桌面显卡竞争。</p><p id="f97d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">运行时间实际上是一个相当可变的量，不仅取决于GPU型号和硬件特性。例如，当系统使用相同的GPU来渲染GUI时，当笔记本电脑上的电源线没有插好时，速度会变慢。</p><h1 id="9b87" class="kr ks hi bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">包扎</h1><blockquote class="na nb nc"><p id="e0cc" class="iu iv kb iw b ix iy iz ja jb jc jd je nd jg jh ji ne jk jl jm nf jo jp jq jr hb bi translated">如果你的洗衣机有GPU，我们这里设计的神经网络很有可能可以在上面运行。</p></blockquote><p id="b7a0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">到目前为止，我所知道的就是一个运行在(几乎)你能找到的任何GPU上的小巧可爱的神经上采样器。为了到达那里，</p><ul class=""><li id="231a" class="kc kd hi iw b ix iy jb jc jf ke jj kf jn kg jr kh ki kj kk bi translated">我们首先训练了一个小型(7.3K参数)神经网络，用于受ESPCN论文启发的x2向上扩展。虽然PSNR没有岩石相比，奶油顶端的艺术状态，图像相当不错(比双三次基线好得多)。模型设计、训练、微调都有改进的空间。</li><li id="42c9" class="kc kd hi iw b ix km jb kn jf ko jj kp jn kq jr kh ki kj kk bi translated">然后我们把它转换成一堆OpenGL着色器。推论运行在Raspberry Pi、Android智能手机和台式机上，它只使用GPU进行计算。低端GPU并不太快，但一个好的桌面GPU可以将全高清实时提升到4K(约100 FPS) 。你可以想象现在在你的4K屏幕上看一部全高清编码的电影被你的GPU实时升级。</li></ul><p id="0a2a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Tensorflow/Keras实现在这里<a class="ae kl" href="https://gist.github.com/lnstadrum/0e2fec1e96bc6791f0ee3487969632ee" rel="noopener ugc nofollow" target="_blank">可用</a>，推理实现在这里<a class="ae kl" href="https://github.com/lnstadrum/beatmup" rel="noopener ugc nofollow" target="_blank">可用</a>。</p></div></div>    
</body>
</html>