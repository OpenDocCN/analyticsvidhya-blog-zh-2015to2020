<html>
<head>
<title>The Neural Network at its Simplest</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最简单的神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-networks-in-nutshell-7d1cc3ae6443?source=collection_archive---------15-----------------------#2020-02-02">https://medium.com/analytics-vidhya/neural-networks-in-nutshell-7d1cc3ae6443?source=collection_archive---------15-----------------------#2020-02-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/be6f56083992f394fbe9e4892840b834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vRGcCzHc3-ksQWtm"/></div></div></figure><p id="279d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">神经网络:</strong></p><p id="d810" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">神经网络是强化领域下机器学习分支的一个分支，用于通过类似网络的结构从给定的原始数据中预测任意变量。神经网络的结构最有可能是人类的神经系统。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jo"><img src="../Images/9c2d09a0cf6346773558fa0ea32be49d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/0*3gCqX48zachEsGV-"/></div></figure><p id="3d96" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，它的中心部分有核，树突用于接收电信号，轴突用于信号传递，轴突末端用于另一个神经元的树突的信号输出。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/7a3d4fbdee8131e618698d59741f9909.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/0*vofnRzMh3DkdmaxG"/></div></figure><p id="8bbf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">x =输入</p><p id="6ad2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">h =输出</p><p id="fc19" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">a =隐藏层</p><p id="7884" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">b =偏差系数</p><p id="3a45" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">w =重量</p><p id="6c3b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Perpectron: </strong></p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ju"><img src="../Images/68b66b2ee0db685318f85bb7d9b8da38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*o0bi6Rfqwzta3Ys4"/></div></figure><p id="e9d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里“x”是具有一些值的输入变量，并且每个输入节点都连接到隐藏层，在该隐藏层中，输入与其各自的权重相加，并且最后添加偏置因子。添加了进一步的“激活函数”来获取输出“y”hat。</p><p id="37df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据特定节点对输出的影响，权重的值从0到1变化，在初始阶段，权重被随机初始化，稍后它将被相应地调整。</p><p id="c06c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">激活函数根据输入变量与偏差的和来决定是否激活节点。激活函数应用于隐藏层和输出层。</p><p id="8129" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一些激活函数是阈值激活函数、Sigmoid激活函数、正切双曲线激活函数、整流器激活函数等等。</p><p id="e54c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里将有一些决定值被设置，上面的值将被设置为高或激活节点，下面的值将被设置为低或停用节点。</p><p id="c1c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">阈值激活功能:</strong></p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jv"><img src="../Images/dea3bbddf100d71b251fc7b6c0686a6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*sA2C-O58VL0ZNO_B"/></div></figure><p id="1fdf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它也被称为二元函数。</p><p id="45ce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">乙状结肠激活功能:</strong></p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jw"><img src="../Images/2eeb234463e1056ba8fbd819d4bf46e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/0*DsyHbLhYxmlTA7oB"/></div></figure><p id="a3f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它最常用于概率情况。</p><p id="1060" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">整流器功能(ReLU): </strong></p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jx"><img src="../Images/2efb832bc842437d1e2be0a6060ffc5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/0*wWq2mQraWvhWenh3"/></div></figure><p id="f3ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ReLU是大多数人工神经网络中最常用的。</p><p id="a385" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">正向传播:</strong></p><p id="4511" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">顾名思义，前向传播是前向数据流网络，因为在这里，数据在输入层被馈送，并且进一步被执行到隐藏层。隐藏层中的输入变量的处理是根据应用于该节点的激活函数来完成的，随后它被带到输出层，在输出层，将根据激活函数进行输出。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jy"><img src="../Images/b5b39c5f15b6a36e5d5896b1000c40ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/0*vnDmklKYUhwYumzD"/></div></figure><p id="5eab" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">注:</strong>数据仅正向流动的网络称为<strong class="is hj">前馈网络。</strong>在<strong class="is hj">前馈网络中不可能出现回流。</strong></p><p id="d5f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">损失函数:</strong></p><p id="9a55" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它定义了模型的性能。这是通过计算实际输出和预测输出之间的差值来实现的。模型的损失值越低，模型的精度越高。</p><p id="e156" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">损失值有三种取法，有的一次取全组值作为<strong class="is hj">梯度下降</strong>，或者在数据集的每个单点取一次作为<strong class="is hj">随机下降</strong>，或者将全组分成N份作为<strong class="is hj">小批量随机下降。</strong></p><p id="c656" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据对神经元的影响来设置权重将减少损失函数。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jz"><img src="../Images/dc782074fedfd1330b9a5bd7f70b176a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ug_fPTEKM_o8QW6y"/></div></div></figure><h2 id="33fc" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jb kl km kn jf ko kp kq jj kr ks kt ku bi translated"><strong class="ak">反向传播:</strong></h2><p id="68d6" class="pw-post-body-paragraph iq ir hi is b it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj kz jl jm jn hb bi translated">反向传播与正向传播相同，但也与网络的反向传播相同。反向传播简单地说就是神经网络中的反馈，因为反向传播只是一个误差计算输出值。</p><p id="92fc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">神经网络的误差值简单地通过网络输出和期望输出之间的差来计算。</p><p id="6000" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">误差=实际输出—期望输出</strong></p><p id="48d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计算的误差值用于反向传播，这意味着从输出层返回到隐藏层的所有神经元，以调整分配给它的权重，从而最小化输出中的误差，并获得期望的输出或更接近的值。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/0de90b898feeaeed69c989848519b676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XX04qbR9NGjcU40z"/></div></div></figure><p id="3f52" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">梯度下降:</strong></p><p id="9871" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">梯度下降是一种广泛使用的算法，用于优化神经网络模型的参数，如系数和权重。考虑到2D曲线图，</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/fdeb184665ded27298df9b5188f927f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/0*kW5yohAU9sOvf-nG"/></div></figure><p id="d629" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">梯度下降只是下降到最小成本值/损失值，这是通过计算点的导数的斜率来完成的，当斜率为零时(向下的斜率为负，向上的斜率为正)，点被称为处于最小值，并且使用该算法的学习速率完成了成本函数的降低，其中必须设置正确的学习速率以达到图形的最小值。</p><p id="422d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">随机梯度下降:</strong></p><p id="f5b8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然梯度下降是伟大的，但其缓慢和高计算算法，需要大量的时间来执行。为了提高算法的速度，它被演化为随机GD，这是一样的，但计算是在每个单点上进行的，这实际上更快，因为它同时使用多个线程来执行</p><p id="f511" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随机梯度的另一个用途是它能有效地从几个极小值中找到全局极小值。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/f005e1d49a214da760045d1f5879965f.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*aLwbWwZC3pKNsdl0.png"/></div></figure><p id="dd26" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有时在成本函数中有许多极小值(局部极小值),但是在整个图中会有一个全局极小值(成本函数的极小值)。而适当的学习率会将点射向全局极小值。还是不能完全确定只有全局最小值。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/06e987e66132e46b4e9657fe8e5443c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2K98a247xeGhmRiZ"/></div></div></figure><p id="9bb7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">三维成本函数图</strong></p><p id="6407" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">最小随机梯度下降:</strong></p><p id="317b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Mini SGD只是随机GD，但误差计算是成批进行的(一组数据点，不像每个点的计算一样)。这比普通的SGD要好，因为它比SGD使用更少的线程，所以计算因素更少。</p></div></div>    
</body>
</html>