<html>
<head>
<title>Apache Beam — From Zero to Hero Pt. 1: Batch Pipelines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阿帕奇光束-从零到英雄 Pt。1:批处理管道</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/apache-beam-from-zero-to-hero-pt-1-batch-pipelines-ee117c37abc8?source=collection_archive---------4-----------------------#2020-12-13">https://medium.com/analytics-vidhya/apache-beam-from-zero-to-hero-pt-1-batch-pipelines-ee117c37abc8?source=collection_archive---------4-----------------------#2020-12-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/d86719e1e094dd6946be740d2b7b3fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gLgdrKd-s0PkkACp"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">Emile Guillemot 在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><div class=""/><p id="9d2b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在<a class="ae hv" href="https://ilanuzan.medium.com/building-a-data-pipeline-with-python-generators-a80a4d19019e" rel="noopener">的上一篇文章</a>中，我谈到了我们如何使用 Python 生成器来创建简单的数据管道。在这篇文章中，我将介绍用于构建生产级数据管道的 Apache Beam 框架，并用它构建一个批处理管道，同时解释它的一些主要概念。</p><h1 id="a043" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">介绍</h1><blockquote class="kr ks kt"><p id="4bc0" class="iv iw ku ix b iy iz ja jb jc jd je jf kv jh ji jj kw jl jm jn kx jp jq jr js hb bi translated">Apache Beam 是一个开源的统一模型，用于定义批处理和流数据并行处理管道。</p></blockquote><p id="4093" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用 Apache Beam，我们可以以简洁、清晰的方式实现复杂且可伸缩的数据管道。Apache Beam 有 3 个 SDK——Python、Java &amp; Go(这里我们要用 Python)。使用该框架编写的管道可以通过不同的运行器在不同的平台上运行——一些可用的运行器是 Spark、Flink 和 Google Cloud Dataflow。默认情况下，正在使用的 runner 是 DirectRunner，它用在开发阶段。</p><p id="7cc0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">管道的典型结构如下所示:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="1d5b" class="lh ju hy ld b fi li lj l lk ll">import argparse<br/>import apache_beam as beam<br/>from apache_beam.options.pipeline_options import PipelineOptions</span><span id="58e7" class="lh ju hy ld b fi lm lj l lk ll">def run(argv=None, save_main_session=True):<br/>  <strong class="ld hz"># Defining the pipeline's arguments</strong><br/>  parser = argparse.ArgumentParser()<br/>  parser.add_argument(<br/>    --input',<br/>    default='#SOME GCS PATH#,<br/>    help='Input file pattern to process.')<br/>  ...</span><span id="fbdb" class="lh ju hy ld b fi lm lj l lk ll">  known_args, pipeline_args = parser.parse_known_args(argv)<br/>  pipeline_options = PipelineOptions(pipeline_args)</span><span id="8941" class="lh ju hy ld b fi lm lj l lk ll"><strong class="ld hz">  # Defining the pipeline's steps</strong><br/>  with beam.Pipeline(options=pipeline_options) as p:<br/>    (p<br/>      | 'ReadData' &gt;&gt; ReadFromText()<br/>      ....</span><span id="749a" class="lh ju hy ld b fi lm lj l lk ll">if __name__ == '__main__':<br/>  run()</span></pre><p id="4de3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们已经知道了什么是 Apache Beam 管道，并且或多或少知道了它的样子，让我们开始编写管道。</p><p id="f440" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将要实现的管道是一个基本的 wordcount 管道，但有所改变。我们将使用三位著名作家的九本教科书——儒勒·凡尔纳、简·奥斯汀和刘易斯·卡罗尔。管道需要执行以下操作:</p><ol class=""><li id="75de" class="ln lo hy ix b iy iz jc jd jg lp jk lq jo lr js ls lt lu lv bi translated">从谷歌云存储(GCS)中读取书籍的文本文件，同时保持文本与其作者之间的关联——这里我们将了解<strong class="ix hz">连接器。</strong></li><li id="8291" class="ln lo hy ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated">拆分每行中的单词并过滤掉连接词——这里我们将学习<strong class="ix hz">有界集合&amp;帕尔多变换。</strong></li><li id="4c67" class="ln lo hy ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated">统计每个作者和单词的出现次数——这里我们将了解一些内置的转换—<strong class="ix hz">Map&amp;Aggregation</strong><strong class="ix hz">转换。</strong></li><li id="4a4b" class="ln lo hy ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated">将结果写入 BigQuery —我们将了解<strong class="ix hz"> BigQuery 连接器。</strong></li></ol><p id="1a52" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的第一步是从 GCS 桶中读取文本文件。我们通过使用连接器来实现，所以让我们首先解释什么是连接器，以及它们在管道中扮演什么角色。</p><h1 id="1927" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">连接器</h1><p id="0fc1" class="pw-post-body-paragraph iv iw hy ix b iy mb ja jb jc mc je jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">连接器是 Apache Beam 的组件，用于从 GCS、BigQuery、Snowflake、Hadoop 等外部数据源读取和写入数据——关于所有可用连接器的信息可以在这里找到<a class="ae hv" href="https://beam.apache.org/documentation/io/built-in/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="0d42" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通常，管道中的第一个和最后一个步骤将使用连接器——我们需要从某个地方获得管道的初始数据，并且我们通常希望将管道的结果保存在某个地方。</p><p id="0d1c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用 TextIO 连接器，它允许我们从本地文本文件或 GCS 路径中读取数据。我们需要从 GCS 中读取带有我们正在读取的文件名的文本文件(以便我们知道将文本与作者联系起来)。每个作者的文本文件都位于一个单独的目录中，因此以后提取作者的名字相当容易:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="bed2" class="lh ju hy ld b fi li lj l lk ll">gs://blog-data-resources/books_txt_files/<strong class="ld hz">jane-austen</strong>/<br/>gs://blog-data-resources/books_txt_files/<strong class="ld hz">jules-verne</strong>/<br/>gs://blog-data-resources/books_txt_files/<strong class="ld hz">lewis-carroll</strong>/</span></pre><p id="e5e0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用<em class="ku"> ReadFromTextWithFilename </em>函数获取文本和文件名:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="4693" class="lh ju hy ld b fi li lj l lk ll">import argparse<br/>import apache_beam as beam<br/>from apache_beam.options.pipeline_options import PipelineOptions<br/>from apache_beam.io.textio import ReadFromTextWithFilename</span><span id="c0f2" class="lh ju hy ld b fi lm lj l lk ll">def run(argv=None, save_main_session=True):<br/>  parser = argparse.ArgumentParser()<br/>  parser.add_argument(<br/>    '--input',<br/>    default='gs://blog-data-resources/books_txt_files/**',<br/>    help='Input file pattern to process.')<br/>  known_args, pipeline_args = parser.parse_known_args(argv)</span><span id="4c94" class="lh ju hy ld b fi lm lj l lk ll">pipeline_options = PipelineOptions(pipeline_args)<br/>with beam.Pipeline(options=pipeline_options) as p:<br/>  (p <br/>    <strong class="ld hz">| 'Read files' &gt;&gt; ReadFromTextWithFilename(known_args.input))</strong></span><span id="7b44" class="lh ju hy ld b fi lm lj l lk ll">if __name__ == '__main__':<br/>    run()</span></pre><p id="c06b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该函数返回一个在整个框架中使用的名为<em class="ku">p 集合</em>的对象，在我们开始实现管道的下一步之前，我们需要知道什么是<em class="ku">p 集合</em>。</p><h1 id="c8e2" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">有界集合</h1><p id="36cd" class="pw-post-body-paragraph iv iw hy ix b iy mb ja jb jc mc je jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">PCollection 是作为 Apache 射束操作的管道数据的对象。这是连接器(以及框架的其余组件)接收、返回或接收和返回的对象。如果我们将它与我上一篇文章中的生成器进行比较，PCollection 对象的作用与数据管道中的 Generator 对象相似——数据流入和流出它。</p><p id="3223" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以使用一个可用的连接器从数据源创建一个初始 PCollection，或者手动创建它，如下所示:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="b7d0" class="lh ju hy ld b fi li lj l lk ll">beam<strong class="ld hz">.</strong>Create([<br/>          'To be, or not to be: that is the question: ',<br/>          "Whether 'tis nobler in the mind to suffer ",<br/>          'The slings and arrows of outrageous fortune, ',<br/>          'Or to take arms against a sea of troubles, ',<br/>      ])</span></pre><p id="2b2c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">PCollection 的大小可以是有界的，也可以是无界的。根据经验，有界 p 集合用于批处理数据流，无界 p 集合用于流式数据流。</p><p id="4ec3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们下一步要做的是将这一行拆分成单词。为了做到这一点，我们首先需要了解变换——特别是 ParDo 变换。</p><h1 id="42a1" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">帕尔多变换</h1><p id="dfe7" class="pw-post-body-paragraph iv iw hy ix b iy mb ja jb jc mc je jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">转换是管道中的不同操作。如果我们想以任何方式修改、聚合或扁平化数据，都可以通过转换来完成。</p><p id="efff" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">框架中内置了几个转换(我们将在下一节讨论其中的 3 个)，但是这里我们需要自己实现一个。为了构建一个自定义转换，您需要创建一个 DoFN 的子类，如下所示:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="ed91" class="lh ju hy ld b fi li lj l lk ll">class WordExtractingDoFn(beam.DoFn):<br/>  def process(self, element):<br/>    pass</span></pre><p id="dd7d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的 ParDo 变换需要做 3 件事。将行中的单词分开。<br/> 2。过滤掉连词。<br/> 3。从文件名中提取作者的名字。</p><p id="c313" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们这样拆分单词:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="9652" class="lh ju hy ld b fi li lj l lk ll">import re</span><span id="55d9" class="lh ju hy ld b fi lm lj l lk ll">class WordExtractingDoFn(beam.DoFn):<br/>  def process(self, element):<br/>    author, line = element<br/>    for word in re.findall(r"[\w']+", line, re.UNICODE):<br/>      <strong class="ld hz">yield</strong> word</span></pre><p id="8a04" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意这里——我们使用的是我们在上一篇文章中学过的关键字<em class="ku"> yield </em>,所以<em class="ku"> process </em>函数直到该行中的所有单词都被生成后才完成执行。</p><p id="1588" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们使用 NLTK 模块过滤掉合取词，如下所示:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="e18f" class="lh ju hy ld b fi li lj l lk ll">import re<br/>from nltk.corpus import stopwords</span><span id="cf40" class="lh ju hy ld b fi lm lj l lk ll">class WordExtractingDoFn(beam.DoFn):<br/>  def __init__(self):<br/>    <strong class="ld hz">self.stopwords = set(stopwords.words('english'))</strong></span><span id="31bd" class="lh ju hy ld b fi lm lj l lk ll">def process(self, element):<br/>    filename, line = element<br/>    for word in re.findall(r"[\w']+", line, re.UNICODE):<br/>      <strong class="ld hz">if word not in self.stopwords:</strong><br/>        yield word</span></pre><p id="298f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们要做的就是从文件名中提取作者的名字，并把它和每个单词一起作为一个元组:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="bd5e" class="lh ju hy ld b fi li lj l lk ll">class WordExtractingDoFn(beam.DoFn):<br/>  def __init__(self):<br/>    self.stopwords = set(stopwords.words('english'))</span><span id="0601" class="lh ju hy ld b fi lm lj l lk ll"><strong class="ld hz">def __extract_author_from_filename(self, filename):<br/>    pattern = re.compile(r'.*\/([\w\-]+)\/[\w\-]+\.txt')<br/>    match = pattern.match(filename)</strong></span><span id="8333" class="lh ju hy ld b fi lm lj l lk ll">if match is not None:<br/>      return match.group(1)</span><span id="fab0" class="lh ju hy ld b fi lm lj l lk ll">return None</span><span id="2962" class="lh ju hy ld b fi lm lj l lk ll">def process(self, element):<br/>    filename, line = element<br/>    for word in re.findall(r"[\w']+", line, re.UNICODE):<br/>      if word not in self.stopwords:<br/>        yield (<strong class="ld hz">self.__extract_author_from_filename(filename)</strong>, word)</span></pre><p id="1f95" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我们将转换添加到管道的方式:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="51d8" class="lh ju hy ld b fi li lj l lk ll">with beam.Pipeline(options=pipeline_options) as p:<br/>  (p<br/>    | 'Read files' &gt;&gt; ReadFromTextWithFilename(known_args.input)<br/>    <strong class="ld hz">| 'Split lines' &gt;&gt; beam.ParDo(WordExtractingDoFn()))</strong></span></pre><p id="bcc3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">管道中的下一步是统计每个单词和作者的出现次数——为此，我们将学习 3 个内置转换。</p><h1 id="5ec1" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">映射和聚合转换</h1><p id="9178" class="pw-post-body-paragraph iv iw hy ix b iy mb ja jb jc mc je jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">我们可以使用两种变换来计算单词出现的次数—<em class="ku">group by key</em>&amp;<em class="ku">combine perkey</em>。我们可以在这里互换使用两者(下一步将需要对每一个稍有不同)，但是作为一般规则，我们更喜欢使用<em class="ku"> CombinePerKey </em>，因为它的伸缩性更好——详细解释为什么可以在这里找到<a class="ae hv" href="https://blog.softwaremill.com/let-mortal-combat-begin-apache-beams-groupbykey-vs-combine-perkey-29e4feb7291" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="990f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这两个函数都接收一个由两个值组成的元组，并将它们视为键-值对，按键分组并对值执行聚合函数。在我们的例子中，我们的键由单词和作者组成，我们的聚合函数应该是<em class="ku"> sum。</em>我们可以通过将每个元组映射到 1，然后对每个元组的 1 值进行汇总来统计每个(作者，单词)元组出现的次数(这是实现 wordcount 的标准方式)。它要求我们添加一个初步的步骤，使用<em class="ku"> Map </em>转换，将每个元组映射为 1 值。</p><p id="0e2f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ku">映射</em>转换通过 lambda 函数将其每个输入元素映射到输出元素。我们包含两个新步骤的管道如下所示:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="81ec" class="lh ju hy ld b fi li lj l lk ll">(p<br/>  | 'Read files' &gt;&gt; ReadFromTextWithFilename(known_args.input)<br/>  | 'Split lines' &gt;&gt; beam.ParDo(WordExtractingDoFn())<br/><strong class="ld hz">  | 'Pair with 1' &gt;&gt; beam.Map(lambda x: ((x[0], x[1]), 1))<br/>  | 'Sum per author &amp; word' &gt;&gt; beam.CombinePerKey(sum))</strong></span></pre><p id="2a21" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们差不多完成了——现在我们要做的就是使用 BigQuery 连接器将结果写入 BigQuery。</p><h1 id="cb8c" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">BigQuery 连接器</h1><p id="85a2" class="pw-post-body-paragraph iv iw hy ix b iy mb ja jb jc mc je jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">首先，让我们在 BigQuery 中添加一个指定目标表名的参数:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="371f" class="lh ju hy ld b fi li lj l lk ll">parser = argparse.ArgumentParser()<br/>    parser.add_argument(<br/>        '--input',<br/>        dest='input',<br/>        default='gs://blog-data-resources/books_txt_files/**',<br/>        help='Input file pattern to process.')<br/><strong class="ld hz">    parser.add_argument(<br/>        '--table_spec ',<br/>        dest='table_spec',<br/>        default='ilan-uzan-297514:tests.author_wordcount',<br/>        help='Destination BigQuery table.')</strong><br/>    known_args, pipeline_args = parser.parse_known_args(argv)</span></pre><p id="3918" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们需要为目的表定义我们的模式(您可以在这里的<a class="ae hv" href="https://beam.apache.org/documentation/io/built-in/google-bigquery/#schemas" rel="noopener ugc nofollow" target="_blank">中了解更多关于如何在 BigQuery 和 Apache Beam 中定义模式的信息)。我们的模式非常简单:</a></p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="62e5" class="lh ju hy ld b fi li lj l lk ll">table_schema = {<br/>  'fields': [<br/>    {'name':'author','type': 'STRING', 'mode': 'NULLABLE'}, <br/>    {'name': 'word', 'type': 'STRING', 'mode': 'NULLABLE'},<br/>    {'name': 'cnt', 'type': 'INTEGER', 'mode': 'NULLABLE'}<br/>  ]<br/>}</span></pre><p id="3d04" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ku"> WriteToBigQuery </em>函数接受字典作为值，因此为了使用它，我们首先需要将元素从元组转换为字典——最简单的方法是再次使用<em class="ku"> Map </em>转换:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="d2a9" class="lh ju hy ld b fi li lj l lk ll"><strong class="ld hz">def to_json_row(element):<br/>  key, cnt = element<br/>  author, word = key</strong></span><span id="9abe" class="lh ju hy ld b fi lm lj l lk ll"><strong class="ld hz">return {"author": author, "word": word, "cnt": cnt}</strong></span><span id="9115" class="lh ju hy ld b fi lm lj l lk ll">(p<br/>  | 'Read files' &gt;&gt; ReadFromTextWithFilename(known_args.input)<br/>  | 'Split lines' &gt;&gt; beam.ParDo(WordExtractingDoFn())<br/>  | 'Pair with 1' &gt;&gt; beam.Map(lambda x: ((x[0], x[1]), 1))<br/>  | 'Sum per author &amp; word' &gt;&gt; beam.CombinePerKey(sum)<br/> <strong class="ld hz"> | 'Format records to JSON' &gt;&gt; beam.Map(to_json_row))</strong></span></pre><p id="6055" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">太好了，现在我们终于可以给 BigQuery 写信了！如果需要，我们选择创建表(根据我们定义的模式)***并在每次加载之前清空数据—这样我们可以多次运行管道，并且结果不会重复。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="bf6b" class="lh ju hy ld b fi li lj l lk ll">(p<br/>  | 'Read files' &gt;&gt; ReadFromTextWithFilename(known_args.input)<br/>  | 'Split lines' &gt;&gt; beam.ParDo(WordExtractingDoFn())<br/>  | 'Pair with 1' &gt;&gt; beam.Map(lambda x: ((x[0], x[1]), 1))<br/>  | 'Sum per author &amp; word' &gt;&gt; beam.CombinePerKey(sum)<br/>  | 'Format records to JSON' &gt;&gt; beam.Map(to_json_row)<br/><strong class="ld hz">  | 'Write to BigQuery' &gt;&gt; beam.io.WriteToBigQuery(<br/>                             known_args.table_spec,<br/>                             schema=table_schema,<br/>                             write_disposition=..WRITE_TRUNCATE,<br/>                             create_disposition=..CREATE_IF_NEEDED))</strong></span></pre><p id="8023" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">***如果我们想使用<em class="ku">CREATE _ IF _ NEEDED</em>CREATE disposition，我们只需定义一个模式——如果表已经存在，也可以不提供模式，如下所示:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="3f2f" class="lh ju hy ld b fi li lj l lk ll">| 'Write to BigQuery' &gt;&gt; beam.io.WriteToBigQuery(<br/>                           known_args.table_spec,<br/>                           write_disposition=..WRITE_TRUNCATE,<br/>                           create_disposition=..CREATE_NEVER))</span></pre><p id="0990" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">就这样——我们的管道完工了。我们像运行 python 中的任何模块一样运行管道:</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="8c0f" class="lh ju hy ld b fi li lj l lk ll">$ python -m main --temp_location gs://tests_tmp/tmp</span></pre><p id="ac4c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为我们正在将数据加载到 BigQuery 中，所以我们还需要指定<em class="ku"> temp_location </em>参数——这是一个 GCS 路径，Apache Beam Runner 会在将文件加载到 BigQuery 之前将它们存放在这个路径中。</p><p id="da55" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">运行管道后，数据将在 BigQuery 中可见:</p><figure class="ky kz la lb fd hk er es paragraph-image"><div class="ab fe cl mg"><img src="../Images/24e032471875e74c3924ee7e591bdb3c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*7B08Pp-SrOAz5nAYseuhEA.png"/></div></figure><h1 id="1c20" class="jt ju hy bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="d342" class="pw-post-body-paragraph iv iw hy ix b iy mb ja jb jc mc je jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">帖子中的所有代码都可以在<a class="ae hv" href="https://github.com/SockworkOrange/blog-posts/tree/main/apache-beam-batch-pipelines" rel="noopener ugc nofollow" target="_blank">这里</a>找到。我希望您喜欢阅读，熟悉 Apache Beam 框架，并了解如何用它编写批处理管道。如果有不清楚的地方，当然欢迎你在评论中提问。</p><p id="9840" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下一篇文章中，我们将实现一个流管道，我们将学习一些新概念，如无边界 p 集合、窗口和触发器。</p></div></div>    
</body>
</html>