<html>
<head>
<title>Generative modelling using Variational AutoEncoders(VAE) and Beta-VAE’s</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用变分自动编码器(VAE)和贝塔-VAE的生成模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/generative-modelling-using-variational-autoencoders-vae-and-beta-vaes-81a56ef0bc9f?source=collection_archive---------1-----------------------#2020-04-22">https://medium.com/analytics-vidhya/generative-modelling-using-variational-autoencoders-vae-and-beta-vaes-81a56ef0bc9f?source=collection_archive---------1-----------------------#2020-04-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/afc60704878b39fd0f9a44eb5e231c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/0*XEZTAN4dbZSEhY53.gif"/></div></figure><p id="dd7d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现代机器学习的一个主要部分是区分模型和生成模型。A <strong class="io hj">判别模型</strong>指的是基于概率估计学习分类的模型类别，即p(y/X ),其中y是类别标签，X是数据点。其中作为一个<strong class="io hj">生成模型</strong>通过学习输入和类别标签之间的联合概率p(X，y)显式地对每个类别的分布建模，然后使用贝叶斯规则对p(y/X)进行分类。有太多的理由可以解释为什么一个人会觉得一个生殖过程非常迷人。其中一个原因是，通过使用生成模型，我们可以理解数据变化和输出观察值之间的因果关系，并在此基础上形成一个可解释的假设。使用生成模型的另一个重要特征是能够在构成各种数据生成因素的数据中找到解开的因素。两个最常用的数据生成模型是(生成广告串行网络)甘模型和(变分自动编码器)VAE模型。这篇文章主要集中在使用VAE的生成模型。我们将学习什么是自动编码器，并展示自动编码器模型如何用于重建输入数据。然后，我们将介绍AE和VAE的相似之处和不同之处，以及VAE是如何用于生成模型的。然后我们讨论β-VAE——VAE的一种变体，它有助于学习解开的表象。</p><p id="8443" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你想了解更多关于生成性模型和区别性模型的知识，请参考安德鲁·吴教授在<a class="ae jk" href="http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf" rel="noopener ugc nofollow" target="_blank">发表的论文</a></p><p id="4df1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你不熟悉联合概率和条件概率，请参考这个<a class="ae jk" rel="noopener" href="/@mlengineer/joint-probability-vs-conditional-probability-fa2d47d95c4a">博客</a></p><p id="80d6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">声明:为了便于解释和理解，我们假设所有模型的输入数据都是图像数据。然而，同样的理论也适用于其他类型的数据，如音频、文本等。</p><h1 id="7479" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">自动编码器:了解VAE的自动编码器是什么和如何工作的；</h1><p id="0a67" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">在这一节，我们将简要介绍自动编码器及其架构，非常熟悉自动编码器概念的人可以跳过这一节。</p><p id="cef7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在我们开始理解VAE的之前，有必要理解什么是自动编码器。AutoEncoder是一个无监督的基于编码器-解码器的神经网络框架。典型的自动编码器将数据(图像/文本)作为输入，并使用一系列卷积和上卷积将其再现为输出。自动编码器的典型架构如下图所示。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ko"><img src="../Images/df79ed95f790dc0cf899a87f2867cacc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vzcMMuxlsGhtoySv"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">图一。传统的自动编码器由3层组成。编码器层、瓶颈层和解码器层。请注意，瓶颈层的大小小于原始输入的大小。</figcaption></figure><p id="4bc9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">经典的自动编码器是简单的前馈神经网络，由三部分组成:编码器、瓶颈和解码器层。编码器是标准的人工神经网络，其将图像作为输入，并输出特征向量，在自动编码器的情况下，该特征向量被称为瓶颈层或潜在表示。这里，使用术语潜在的，因为这个向量现在包含了重建原始图像(X)所需的所有有意义的信息。解码器的工作是接收这个潜在向量并重建原始图像(x `)。然后基于x′和x之间的重建损失训练网络</p><p id="35e9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数学上，设g(。)表示由ϕ和f(参数化的编码器函数。)表示由θ参数化的解码器函数。然后，潜在向量z被表示为g_ϕ(X).解码器获取输入z和输出的重构输入x`。x’可以表示为f_θ(g_ϕ(X)).然后给出网络损耗<em class="lb"> L </em> (θ，ϕ，x，x `)，在MSE(均方误差)的情况下如下。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/caed3698e055c5ff48db3920e369f2e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*MXar-ByTiFYvBokMQlmxgg.png"/></div></figure><blockquote class="ld le lf"><p id="15f8" class="im in lb io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated">注意:如果瓶颈层的大小与原始输入相同，则编码器简单地将所有原始输入复制到潜在向量中，瓶颈层大小的约束迫使编码器学习原始数据的有用低维表示。</p></blockquote><p id="6c51" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">瓶颈层中的向量现在充当x的压缩表示。自动编码器是降维的强大工具，它通过利用神经网络的非线性表示学习能力来实现这一点。虽然PCA试图找到捕捉数据中最高变化的低维超平面，但autoencoder通过学习数据中的非线性流形来实现它。下图显示了与自动编码器相比，PCA学习的低维流形的差异。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/4247e265a5bc51ae56795b5f3dd8b3a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/0*M5-8utqcyZI5_aOA"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">图二。PCA与Autoencoder实现的降维。当PCA找到最佳线性超平面来投影数据时，自动编码器学习数据的低维非线性流形表示。来源:https://www.jeremyjordan.me/autoencoders/<a class="ae jk" href="https://www.jeremyjordan.me/autoencoders/" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="87e6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">自动编码器的一些有趣变化</p><ol class=""><li id="2ab2" class="lk ll hi io b ip iq it iu ix lm jb ln jf lo jj lp lq lr ls bi translated">去噪自动编码器</li><li id="5f8b" class="lk ll hi io b ip lt it lu ix lv jb lw jf lx jj lp lq lr ls bi translated">稀疏自动编码器</li></ol><p id="5110" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">讨论这些网络超出了本文的范围。</p><p id="0cc4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">尽管有有趣的框架，自动编码器的应用有限，因为潜在表示的范围仅限于已经可用的数据。如果训练数据不能代表您的测试数据，那么我们最终会模糊信息而不是澄清信息。AutoEncoder只学习有效地表示训练数据所在的流形，而不能用于生成它没有见过的新数据。</p><h1 id="ac0d" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><strong class="ak">变型自动编码器</strong></h1><p id="7c4e" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">VAE的变分自动编码器的简写是一类深度生成网络，它具有类似于经典自动编码器的编码器(推理)和解码器(生成)部分。不同于旨在学习固定函数g(.)通过将输入数据X映射到潜在表示(z)，VAE学习输入数据的概率分布函数Q(z/X)。</p><p id="94bc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">变分自动编码器强烈假设原始输入X和潜在向量z都具有各向同性高斯分布。各向同性高斯分布是协方差矩阵σ=σ* I(其中I是单位矩阵)的分布，即每个维度都可以被视为独立的一维高斯分布(不存在协方差)。对于任何给定的输入x，当自动编码器的标准编码器给出维数为‘n’的z的潜在向量时，VAE输出维数为n的两个向量均值(𝝁)和方差(𝞂),它们形成维数为‘n’的𝝁和𝞂。新的潜在向量z是通过对来自分布N(𝝁ᵢ，𝞂ᵢ的每个zᵢ进行采样而形成的，其中i=1到n。下面的图3示出了VAE架构的概况。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ly"><img src="../Images/a7d47f66089375b216f6183d8c663226.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vcbSFQifW_krqBuX.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">图3 : VAE建筑。编码器网络近似于pdf q(z/X)。假设z遵循高斯分布，编码器的输出将是2个向量，均值向量由所有均值组成，方差向量由我们正在逼近的‘n’个高斯分布的所有标准偏差组成。每个z单位的取样在𝞂ᵢn(𝝁ᵢ进行)。解码器从z重构x，因为我们假设输入x遵循各向同性高斯分布。解码器试图通过找出x的每个维度的平均值和标准偏差并对Xᵢ进行采样来逼近概率分布p(X/z)</figcaption></figure><p id="0ec7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">重新参数化技巧:</strong>z的这种类型的采样是随机的，使得z成为神经网络中的随机节点，因此我们不能通过该节点反向传播。为了克服这一点，VAE使用了一个聪明的技巧，不是从𝞂ᵢn(𝝁ᵢ随机采样zᵢ，而是通过引入误差项ϵ，其中我们从z的每个维度的N(0，1)中随机采样，以将zᵢ构造为𝝁ᵢ + 𝜖*𝞂ᵢ (*这里是指元素方面的点积)。现在，误差项ϵ是随机的，z是确定的，因此我们可以通过z反向传播。采样过程和重新参数化技巧如图(3)所示。采样过程如图(3)所示。通过从q(z|X)采样z，意味着如果我们两次尝试相同的输入x，我们应该得到z的两个不同的值。这是通过引入随机层𝜖.来实现的</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lz"><img src="../Images/9d08c4a071739fdbe57a363220b438be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPMcZXV1BBPXeh57Su9jgQ.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">图4:来源:【https://arxiv.org/pdf/1906.02691.pdf】</figcaption></figure><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ma"><img src="../Images/3c6231bce40604c4e613ebfa72c36f85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4vaeupA18oeGUXrw"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">图5: <strong class="bd jn">没有/有重新参数化的VAE架构</strong>:左侧显示了从生成的平均值和sigma中采样数据点，这是一个随机过程。图像的右侧显示了重新参数化的采样，其中我们从标准法线采样e。</figcaption></figure><p id="5c1e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于生成的每个潜在向量z，我们自问这个潜在向量在重构原始数据X方面有多好，即使用解码器用p(X/z)表示X的可能性。先验p(z)被选择为标准的多元高斯N(0，1)。后验概率是编码器对给定输入的近似值，用q(z/X)表示。</p><ul class=""><li id="9f5b" class="lk ll hi io b ip iq it iu ix lm jb ln jf lo jj mb lq lr ls bi translated">条件概率p_θ(X|z)定义了一个生成模型，类似于上面在自动编码器部分介绍的解码器f_θ(X|z)。p_θ(X|z)也称为概率解码器。</li><li id="ce94" class="lk ll hi io b ip lt it lu ix lv jb lw jf lx jj mb lq lr ls bi translated">近似函数q_ϕ(z|X)是概率编码器，起着与g_ϕ(z|x).相似的作用</li></ul><blockquote class="ld le lf"><p id="1bfb" class="im in lb io b ip iq ir is it iu iv iw lg iy iz ja lh jc jd je li jg jh ji jj hb bi translated">为了便于表示，我们将省略像q_ϕ(.这样的参数化函数表示的使用)和p_θ(。)而只使用q(。)和p(。).</p></blockquote><p id="d2bb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">VAE的目的是在这样一个生成过程中学习数据的边际可能性:</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/07072afaefd0a63a6da0ec6962dcb9f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/0*rAAipdnbzu3_nU85"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">—等式(1)</figcaption></figure><h1 id="8583" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">损失函数</h1><p id="7d2b" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">我们希望后验概率q(z|X)尽可能接近我们的先验概率p(z ),以便解码图像虽然不同，但仍接近实际图像。我们将使用KL散度来度量先验分布p(z)和近似分布q(z/X)之间的信息损失。2分布p(原始)和q(近似)的KL-散度被写成加权函数p(z)上的“惩罚”函数log(p(z)/q(z))的期望。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es md"><img src="../Images/284370b303bdc500b5296ef497911feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*OUexN8VpVdbp87gEjkxyyA.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">从Q到P的kull back-lei bler散度，其中P和Q是定义在同一概率空间中的概率分布函数</figcaption></figure><p id="1150" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">KL散度不是对称距离函数，即KL(P | | Q)≠KL(Q | | P)(Q≡P时除外)前者称为“正向KL”，后者称为“反向KL”。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es me"><img src="../Images/f94f90a1240f8fdd42ee1178c8f8ebce.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*ikuhujVfZj-19hBB1e-Ykg.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">等式(2):2个分布P和Q之间的前向KL散度</figcaption></figure><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es me"><img src="../Images/ab5d259548e1d5217eee82d71f4b13f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*zzaYmGqcxHbMXoJwoNyo2A.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">等式(3)2个分布P和Q之间的反向KL散度</figcaption></figure><p id="ca57" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在前向k1的情况下，让我们考虑p(z) &gt; 0和q(z) -&gt; 0，那么lim(q(z)-&gt;0) log p(z)/q(z) =无穷大。每当估计的分布不覆盖原始分布时，前向KL就很大。图(3)示出了如何使用前向KL优化获得结果Q(z)。当Q(z)在P(z)上被“拉伸”时，KL很小，结果我们得到很多假阳性。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/1718abb7874fac1b0db16337712b3fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*ZXZzUbIjbt-YVA-Y"/></div></figure><p id="10c4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在逆向KL的情况下，如果p(z)=0，我们必须保证在分母p(z)=0的地方加权函数q(z)=0，否则KL吹爆。图(4)示出了如何使用前向KL优化获得结果Q(z)。当Q(z)在P(z)上被“挤压”时，KL很小。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/ac6f45c97670a0b7d25ce102ac265210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*aICL_Db0lN75QBWy"/></div></figure><p id="03c4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我强烈建议参考下面的博客(<a class="ae jk" href="https://blog.evjang.com/2016/08/variational-bayes.html" rel="noopener ugc nofollow" target="_blank">https://blog.evjang.com/2016/08/variational-bayes.html</a>)来更具体地了解正向和反向KL。</p><p id="c579" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">有了上面的知识，我们现在将进一步使用反向KL散度来推导要最小化的目标函数</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mg"><img src="../Images/20eee545355bac2920364ec0aa98711f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ah8NJm1X7KKAKl7O8byZ4w.png"/></div></div></figure><p id="ba2f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用贝叶斯规则p(z/X) = p(z，X)/p(X)其中p(z，X)是z和X之间的连接概率，给出为p(z，X) = p(X/z)*p(z)</p><p id="3ded" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">将p(z，X)代入我们得到的方程4，</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mh"><img src="../Images/71e41b898dbcb7b80c139e493a8ea787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*17yDAsKwMeSbEld-joBusw.png"/></div></div></figure><p id="7fe1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因为p(X)与z无关，并且可以导出</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mi"><img src="../Images/dc700d01b8c55bfc8c7df5d01d87c498.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NP0z_kWEML5lOdIUDm8MeQ.png"/></div></div></figure><p id="9087" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，让我们把log(p(X))带到LHS，对两边求反，我们现在有:</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mj"><img src="../Images/6f0877b049edfe0b52e91780d471273e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7-TkEGEmsXibVntM6w6Hhw.png"/></div></div></figure><p id="5ed9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上述方程是变分自动编码器的核心。等式的左手边正是我们想要最大化的项P(X)(参考等式1)，生成新数据点的概率应该最大化，同时我们想要最小化潜在向量的先验p(z)和后验q(z/X)概率之间的差异。DKL项作为正则项。</p><p id="0379" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于我们的目标是最大化LHS，我们将使用梯度下降来最小化LHS的负值，即:</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mk"><img src="../Images/a66a6b51792bfc124c6c029cfca4acd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*alLjcjrqR6WudvpUOmctOw.png"/></div></div></figure><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ml"><img src="../Images/c318102dd86144711c0c452d46665142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sW7g2BXZ6JtvW9Rr8EYbRw.png"/></div></div></figure><p id="7ee7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">任何两个分布之间的KL散度总是正的，因此</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/42dc83b026ae0170cdabbe3f172b1c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*wWtiJw9BrtOyrhnur6GyAg.png"/></div></figure><p id="f600" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，-loss或-ez∞q[log(pθ(x/z)]+dkl(qɸ(z/x)||p(z](来自等式7)是产生数据的下限。这在变分贝叶斯方法中被称为<strong class="io hj"> <em class="lb">变分下界或证据下界(ELBO)。</em> </strong>因此，通过最小化损失，我们最大化了生成新样本的概率的下限。</p><p id="12d9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">关于VAE的要点:</p><ol class=""><li id="9d28" class="lk ll hi io b ip iq it iu ix lm jb ln jf lo jj lp lq lr ls bi translated">VAE属于概率生成模型的范畴</li><li id="ea40" class="lk ll hi io b ip lt it lu ix lv jb lw jf lx jj lp lq lr ls bi translated">先验p(z)和后验qɸ(z/X)被假设为具有对角协方差矩阵的各向同性高斯分布。先验通常被设置为N(0，1)</li><li id="3844" class="lk ll hi io b ip lt it lu ix lv jb lw jf lx jj lp lq lr ls bi translated">类似于普通的自动编码器，VAE也有一个编码器，它产生2个潜在向量mean和sigma，每个dim n对应于z的n个各向同性高斯单元的平均值和方差</li><li id="3239" class="lk ll hi io b ip lt it lu ix lv jb lw jf lx jj lp lq lr ls bi translated">从每个独立的高斯n(𝝁(xσ(x))中进行采样。采样过程是随机的，因此反向传播是不可能的。重新参数化技巧用于促进反向传播。</li><li id="f5f2" class="lk ll hi io b ip lt it lu ix lv jb lw jf lx jj lp lq lr ls bi translated">KL-散度用于测量近似的qɸ(z/X和先前的p(z)之间的信息损失。</li><li id="da27" class="lk ll hi io b ip lt it lu ix lv jb lw jf lx jj lp lq lr ls bi translated">在等式5中，我们已经表明VAE的目标是生成尽可能接近真实的样本，即最大化<strong class="io hj"> log(p(X)) </strong></li><li id="adf1" class="lk ll hi io b ip lt it lu ix lv jb lw jf lx jj lp lq lr ls bi translated">在等式7中，网络的损耗被导出为<strong class="io hj">D _ KL(q(z/X)| | p(z/X))—log(p(X))</strong></li><li id="4893" class="lk ll hi io b ip lt it lu ix lv jb lw jf lx jj lp lq lr ls bi translated">在等式8中，我们已经建立了负损失如何作为生成样本的下限，也称为<strong class="io hj"> ELBO </strong>，因此通过最小化损失，我们最大化了生成新样本的概率的下限。</li></ol><h1 id="0aec" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">β-变分自动编码器；</h1><p id="7d75" class="pw-post-body-paragraph im in hi io b ip kj ir is it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj hb bi translated">𝛃-VAE是一种深度无监督的生成方法，是用于解开因子学习的变分自动编码器的变体，可以发现无监督数据中独立的潜在变异因子。一个不纠缠的表示可以定义为z的单个潜在单元对X的单个生成因子的变化敏感，而对其他因子的变化相对不变。一个不纠缠的模型学习对单个独立的数据生成因素敏感的独立的潜在单元。因此，一个不纠缠的表示是因式分解的，并且通常是可解释的，由此不同的独立潜在单元学习编码数据中变化的不同独立基本事实生成因素。</p><p id="23ff" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">β-VAE和VAE的区别是在原始VAE公式中KL散度项上使用了拉格朗日乘数β。β-VAE的目标函数是</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mn"><img src="../Images/a68ab7e9ae4dd034b4db1480c4c6cf27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nvoD_xhYnTCDQBIDB6ywzw.png"/></div></div></figure><p id="556a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">β-VAE试图通过使用超参数β &gt; 1来优化先验分布和近似分布之间的严重惩罚KL散度，来学习条件独立数据生成因子的解纠缠表示。这种约束限制了z的能力，结合最大化训练数据X的对数似然性的压力，促使模型学习数据的最有效表示。我们假设数据x具有一些条件独立的生成基础真值因子，并且β-VAE目标函数的KL散度项鼓励qφ(z|x)中的条件独立。因此，较高的β值应该鼓励学习一种不纠缠的表示。然而，来自高β值的额外压力可能会在重建保真度和学习的潜在表征内的解缠结质量之间产生折衷。当在信息保存之间找到适当的平衡时，就出现了清晰的表示</p><h1 id="e70d" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">参考资料:</h1><div class="mo mp ez fb mq mr"><a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hj fi z dy mw ea eb mx ed ef hh bi translated">从自动编码器到贝塔VAE</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">自动编码器是一组神经网络模型，旨在学习高维数据的压缩潜变量…</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">lilianweng.github.io</p></div></div></div></a></div><div class="mo mp ez fb mq mr"><a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" rel="noopener follow" target="_blank"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hj fi z dy mw ea eb mx ed ef hh bi translated">了解变分自动编码器(VAEs)</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">逐步建立导致VAEs的推理。</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">towardsdatascience.com</p></div></div><div class="na l"><div class="nb l nc nd ne na nf ik mr"/></div></div></a></div><p id="dcfd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" href="https://openreview.net/pdf?id=Sy2fzU9gl" rel="noopener ugc nofollow" target="_blank">https://openreview.net/pdf?id=Sy2fzU9gl</a></p><p id="666a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" href="https://arxiv.org/pdf/1804.03599.pdf" rel="noopener ugc nofollow" target="_blank">了解贝塔VAE中的纠缠</a></p><p id="7113" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" href="https://arxiv.org/pdf/1606.05908.pdf" rel="noopener ugc nofollow" target="_blank">变型自动编码器教程</a></p><p id="3da5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" href="https://arxiv.org/pdf/1906.02691.pdf" rel="noopener ugc nofollow" target="_blank">变型自动编码器简介</a></p></div></div>    
</body>
</html>