<html>
<head>
<title>Activation Functions- How the Neuron Triggers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">激活功能——神经元如何触发</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-how-should-the-neurons-trigger-1349a383ffeb?source=collection_archive---------16-----------------------#2020-08-26">https://medium.com/analytics-vidhya/activation-functions-how-should-the-neurons-trigger-1349a383ffeb?source=collection_archive---------16-----------------------#2020-08-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="efa7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们都知道，神经网络是一种信息处理范式，其灵感来自科学家能够观察到的大脑中的生物过程。因此，人工神经元通常包括输入、输出和决定神经元是否应该被激活的激活函数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/75f676e1f7f008c78bf5580588c72536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*mzWqY5snL5pYrKo7r5-HZg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">一个生物神经元和一个人工神经元</figcaption></figure><p id="b683" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本上，激活函数的重要性在于<strong class="ih hj">将非线性</strong>引入神经元的输出。</p><blockquote class="jp jq jr"><p id="4f52" class="if ig js ih b ii ij ik il im in io ip jt ir is it ju iv iw ix jv iz ja jb jc hb bi translated">当一个人不小心触摸到一个<strong class="ih hj">热物体</strong>时，他们会不假思索地自动把手抽走。当脊髓运动神经元被激活并向大脑发送信号时，就会发生这种情况。</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jw"><img src="../Images/884db8d2e6eb8a7557e4f0a0fce19ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*Uarb1l98122LLDSx8PDXUQ.jpeg"/></div></figure><h1 id="1251" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">为什么激活函数很重要？</h1><p id="9d58" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">让我们考虑一个神经元，其净输入为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es la"><img src="../Images/bdb35ca0757af1aad6b00af4c632801f.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*9waqAk6RdJih9omh_fYPtA.png"/></div></figure><p id="3a77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，这个净输入的值可以是从-inf到+inf的任何值。神经元并不真正知道如何绑定到值，因此无法决定触发模式。因此，激活函数是神经网络的重要组成部分。它们基本上决定了一个神经元是否应该被激活。因此，它限制了净投入的价值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lb"><img src="../Images/ca1080d938d905b1f63a1c4b022904c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*JHsyPwnSOi0etw8Px26l9Q.png"/></div></figure><h1 id="866d" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">非线性的重要性</h1><p id="13b1" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">我们知道，激活函数是一种非线性转换，在将输入发送到下一层神经元或最终作为输出之前，我们对输入进行转换。但是引入非线性的必要性是什么呢？</p><p id="d14a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你把几个线性变换链接起来，你得到的只是一个线性变换。例如，假设f(x) = 2 x + 3，g(x) = 5 x — 1，那么将这两个线性函数链接起来，就会得到另一个线性函数:f(g(x)) = 2(5 x — 1) + 3 = 10 x + 1。因此，如果层与层之间没有一些非线性，那么即使是很深的一堆层也相当于一个单层:你无法用它来解决非常复杂的问题。</p><p id="7dbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，如果我们看到一个简单线性函数p(x)=ax的表示</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lc"><img src="../Images/239e62ce77504de58dd53781723e71c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wIHmoCgMh1t0le1y2cLnVg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">线性函数(左)及其导数(右)</figcaption></figure><p id="b152" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们对函数关于x求导时，结果就是x的系数，它是一个常数(q(x)= <strong class="ih hj"> a </strong>)。</p><p id="6adb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的梯度(或导数)是常数，完全不依赖于输入值x。这意味着权重和偏差将在反向传播过程中被更新，但是更新因子将是相同的。在这种情况下，神经网络不会真正改善误差，因为梯度对于每次迭代都是相同的。网络将不能很好地训练并从数据中捕获复杂的模式。</p><h1 id="16b8" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">激活功能的类型<strong class="ak"/></h1><h2 id="ef27" class="lh jy hi bd jz li lj lk kd ll lm ln kh iq lo lp kl iu lq lr kp iy ls lt kt lu bi translated"><strong class="ak">乙状结肠功能</strong></h2><p id="7466" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">Sigmoid函数是一种广泛使用的激活函数，其数学定义为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/eca1d3d9a93fa385a9de590f36c209b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*l-0tcjC7uTRashzvEias_w.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/abb81befadf16dee71e5bfdd4239869c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*GiSUg69EJuDuo4A4YQWEzA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">SIGMOID函数和导数</figcaption></figure><p id="da75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个S形函数已经被证明非常适用于结果为<strong class="ih hj"> 0 </strong>或<strong class="ih hj"> 1 </strong>的<strong class="ih hj">二元分类</strong>问题。如果值大于<strong class="ih hj"> 0.5 </strong>，则结果很容易预测为<strong class="ih hj"> 1 </strong>，否则为<strong class="ih hj"> 0 </strong>。</p><p id="6920" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个光滑函数，并且是连续可微的。函数是非线性的，这种非线性使网络更加复杂，并允许我们用它来完成更多的任务。</p><h2 id="4aca" class="lh jy hi bd jz li lj lk kd ll lm ln kh iq lo lp kl iu lq lr kp iy ls lt kt lu bi translated">Tanh函数</h2><p id="6749" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">双曲正切函数只是放大的sigmoid函数。输出不是从0到1，而是从-1到1。因此，下一层的输入不会总是相同的符号。它在数学上可以定义为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/5d542d1e89f30fa7423ea3fb737e7b2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*uqX_6P8b8nSnRecPmBiCww.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/cc73f6b75c562cce292302ffddab96e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*8brynP2wkgKxjiF59PQb_A.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">双曲正切函数和导数</figcaption></figure><p id="7cf7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与sigmoid类似，双曲正切函数在所有点上都是连续且可微的。与sigmoid函数相比，tanh函数的梯度更陡。</p><p id="5c1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常双曲正切函数优于sigmoid函数，通过比较两个函数的导数图可以明显看出这一点。可以看出，tanh的导数图比sigmoid的导数图更陡，即我们具有更高的梯度(导数)强度(范围在0和1之间，而sigmoid的梯度强度范围在0和0.25之间)，因此有助于快速学习(或收敛)。</p><h2 id="2d94" class="lh jy hi bd jz li lj lk kd ll lm ln kh iq lo lp kl iu lq lr kp iy ls lt kt lu bi translated">消失梯度问题</h2><p id="3da3" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated"><strong class="ih hj"> Sigmoid </strong>和<strong class="ih hj"> Tanh </strong>函数共有的一个主要缺点是消失梯度问题。</p><p id="786e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道，为了减少神经网络的损失函数，通过反向传播来修改每一层的权重。我们知道Sigmoid的梯度范围是0到0.25，Tanh的梯度范围是0到1。然后，对于一个n层神经网络，如果我们对每一层使用sigmoid/tanh函数，那么随着信号的反向传播，梯度将变得越来越小。</p><p id="bb5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">反向传播使用链式法则，该法则具有将这些小分数的n倍相乘的效果，以计算梯度，该梯度将它们指数地减小，变得几乎等于0。这意味着第一层几乎没有会使网络学习瘫痪的梯度。</p><h2 id="3697" class="lh jy hi bd jz li lj lk kd ll lm ln kh iq lo lp kl iu lq lr kp iy ls lt kt lu bi translated">RELU</h2><p id="2502" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">校正线性单元或RELU是最受欢迎的激活函数之一，通常用于语音识别和计算机视觉的深度学习神经网络。它在数学上可以定义为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/3bd59d65dcfa06ab43244695e4df7995.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*khYWIuatPwnyD0LX4Dml5g.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ma"><img src="../Images/8daa30eae29c7892a04d257ebf6c3100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*ahXv0bxHfyiZMGR4ME3gMQ.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">ReLU函数和导数</figcaption></figure><p id="5f94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RELU函数图可能看起来与正轴上的线性函数具有相同的特征。但最重要的是，ReLU本质上不是线性的。</p><p id="b97b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RELU的主要优点是一次只有几个神经元被激活，使网络变得稀疏，使其高效且易于计算，即如果输入为<strong class="ih hj">负</strong>则输出为<strong class="ih hj"> 0 </strong>，如果输入为<strong class="ih hj">正则输出为<strong class="ih hj"> 1 </strong>。</strong></p><p id="7490" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RELU的另一个优势是它没有消失梯度的问题。由于导数只是一个常数值，所以成本函数曲线收敛得很好。</p><blockquote class="jp jq jr"><p id="a33e" class="if ig js ih b ii ij ik il im in io ip jt ir is it ju iv iw ix jv iz ja jb jc hb bi translated">哇，这是一个职业吨。肯定有附加条件。</p></blockquote><p id="39a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，对于负值，导数是<strong class="ih hj"> 0。</strong>由于这个原因，在反向传播过程中，一些神经元的权重和偏差没有更新。这会产生永远不会被激活的死亡神经元。这是由漏RELU处理的。</p><h2 id="386d" class="lh jy hi bd jz li lj lk kd ll lm ln kh iq lo lp kl iu lq lr kp iy ls lt kt lu bi translated">漏水的RELU</h2><p id="84d4" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">Leaky ReLU函数只不过是ReLU函数的改进版本。顾名思义，这个函数没有负值的0值，而是引入了一个称为泄漏的小值。</p><p id="049a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它在数学上可以定义为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/46919e7d0ce06b17356a6f8ce55423b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*s0bKQTkpbr15hqVwk2Zs9w.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/a5dc69f29e182fe34004f6c62fec69da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*33Xbv5EPIKEyfi2nGx4Dnw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">漏ReLU函数及其导数</figcaption></figure><p id="7dae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种泄漏或修改是试图修复垂死的ReLU问题，以便我们不再遇到该区域的死亡神经元。</p><h2 id="4502" class="lh jy hi bd jz li lj lk kd ll lm ln kh iq lo lp kl iu lq lr kp iy ls lt kt lu bi translated">Softmax函数</h2><p id="fa46" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">softmax函数是一种类似于sigmoid函数的激活函数。当我们试图处理分类问题时，这是很方便的。T <strong class="ih hj">最重要的一点是，它在深度学习模型的输出层是首选的，尤其是在需要分类两个以上的时候。</strong>它允许确定输入属于特定类别的概率，报告每个类别的“置信度分数”。因为我们在这里处理的是概率，所以softmax函数返回的分数总和将为1。</p><p id="0ef3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，预测类是列表中置信度得分最高的项目。</p><p id="99b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它在数学上可以定义为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/d269724cfcefaffd03ef2408a4b12407.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*mMslwFbRNrTt_ZS9cLhPgA.png"/></div></figure><p id="9dbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它指出，我们对输出层的每个元素应用一个标准指数函数，然后通过除以所有指数的总和来归一化这些值。这样做可以确保所有取幂值的总和等于1。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es me"><img src="../Images/9673d40d738e0ac01fefdfe0d9106fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i1DW4ZDTZr6KRfjrG6V-mQ.png"/></div></div></figure><h2 id="d8e2" class="lh jy hi bd jz li lj lk kd ll lm ln kh iq lo lp kl iu lq lr kp iy ls lt kt lu bi translated">要使用哪个激活功能</h2><p id="9545" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">现在，在所有这些讨论之后，唯一和最重要的问题仍然是使用哪个激活函数。这是不是意味着我们做什么都用ReLu？还是乙状结肠还是坦？嗯，是也不是。推荐一个适用于所有用例的激活功能是非常困难的。有许多考虑因素——计算导数有多困难(如果它是可微的话！)，你选择的激活函数的网络收敛速度有多快，有多光滑，是否满足泛逼近定理的条件，是否保持归一化等等。</p><p id="8612" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过这篇文章，我尽力描述了一些常用的重要激活函数。还有其他的激活功能，但总体思路是一样的。希望你了解激活函数背后的概念，为什么使用它们，以及我们如何决定使用哪一个。</p><p id="9b14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读。</p></div></div>    
</body>
</html>