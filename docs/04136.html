<html>
<head>
<title>Music Generation through LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM的音乐一代</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/music-generation-through-lstm-efd2c6ea9377?source=collection_archive---------8-----------------------#2020-03-06">https://medium.com/analytics-vidhya/music-generation-through-lstm-efd2c6ea9377?source=collection_archive---------8-----------------------#2020-03-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bd2685c7da07c37d47483d3da9da5424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jBxd_q-KHFZEOJB5DEN-bA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源<a class="ae iu" href="https://djmag.com/news/world%E2%80%99s-first-artificial-intelligence-music-producer-has-arrived" rel="noopener ugc nofollow" target="_blank">此处</a></figcaption></figure><p id="dc90" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇文章中，我们解释的程序和方法用于建立一个自动音乐生成系统使用长短期记忆(LSTM)。LSTM是递归神经网络的一种变体，广泛用于基于序列的模型。我们的项目旨在使用LSTM单元在现有音乐文件上训练模型后生成新的音乐作品。</p><h1 id="7d0a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">介绍</h1><p id="031e" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">音乐生成是一种依赖于时间的艺术形式，就像语音和视频一样。因此，为了生成音乐，我们不得不使用基于序列的模型。递归神经网络具有处理顺序数据的趋势，因此我们选择RNNs进行建模。传统的RNNs有两个主要问题，即消失梯度和爆炸梯度。为了解决这些主要问题，使用了长短期记忆(LSTM)和门控循环单位(GRU)。在这两项决议中，我们选择了LSTM。LSTMs需要学习更多的参数，但大多数情况下，与GRUs相比，LSTMs给出的结果更好。该项目主要分为两个阶段。</p><p id="892b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第一阶段主要涉及数据预处理，第二阶段围绕模型构建。这两个阶段使用的工具和技术将在后面详细讨论。在第一阶段的数据收集阶段，我们没有遇到太多障碍。音乐文件很容易获得。处理阶段花费了大量的时间，因为我们必须为数据预处理探索新的工具和库。第二阶段，模型构建，也花费了相当多的时间，因为我们必须试验不同的模型，最终得出一个更适合我们数据的模型。</p><h1 id="7d22" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">数据收集</h1><p id="9af7" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们收集的数据是midi格式的音频音乐文件。收集的数据类型是爵士乐。为了处理数据，我们使用了麻省理工学院的music21图书馆。该库使得从MIDI音乐文件中提取音符变得更加容易。根据要求对笔记进行了过滤。提取笔记名称后，笔记被保存在一个文件中供以后使用。音乐文件中的音符序列以一种可以输入模型进行训练的方式进行排列和分割。</p><h1 id="2aa3" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">模型架构</h1><p id="e165" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们尝试了具有不同数量学习参数的不同模型架构，并比较了它们的结果。我们最终选择了最适合我们数据的模型配置。该配置包含两个LSTM层，随后是具有Softmax激活的完全连接的致密层。模型架构的框图如下所示。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/0937a263739016f322afd31d74b22b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*IlduqGnVIgiWn01ZXfUIUg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">模型架构</figcaption></figure><h1 id="b0ad" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结果</h1><p id="a2ad" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们用不同的超参数尝试了我们的最终模型。超参数包括学习率和时期数。最初，我们用5个时期训练我们的模型。模型结果非常糟糕。获得的损失值为4.8656。该模型的性能如下图所示。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/2a5af0d67d8e7b07a16486610f961b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*BVAsAbA1SxVm-M1gCyC5Ug.jpeg"/></div></figure><p id="c5f8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">后来，我们在50个时期上尝试我们的模型，并且我们观察到损失减少到0.0695。模特开始创作一些非常好听的钢琴曲子。模型性能可以可视化如下。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/e7dfee014f675d12707c6d2ef2c3808d.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*T94oRuBdUjCijxfrxFUatA.jpeg"/></div></figure><h1 id="4bb2" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="2fa1" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们关于通过LSTMs生成音乐的项目接受MIDI文件作为输入，从音乐文件中提取音符，将它们转换成文本表示，然后经过一些训练过程，在给定一些初始种子音符的情况下，生成新的音乐曲调。该模型在运行了50个时代后，产生了一些非常有效和流畅的曲调。最后记录的损失值为0.0436。</p></div></div>    
</body>
</html>