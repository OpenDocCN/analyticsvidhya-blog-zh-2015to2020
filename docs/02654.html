<html>
<head>
<title>Numpy vs PyTorch: Linear Regression from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Numpy vs PyTorch:从零开始的线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/numpy-vs-pytorch-linear-regression-from-scratch-452a121fb0e8?source=collection_archive---------15-----------------------#2019-12-27">https://medium.com/analytics-vidhya/numpy-vs-pytorch-linear-regression-from-scratch-452a121fb0e8?source=collection_archive---------15-----------------------#2019-12-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a924918835204b7b9650d6b42136e9a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*al5uxthcQ4C37On6"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@jeroendenotter?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">耶鲁安穴獭</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="664e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上一篇<a class="ae iu" rel="noopener" href="/analytics-vidhya/introduction-to-pytorch-e5df512b1079">文章</a>中，我们比较了Numpy数组和PyTorch张量。现在让我们使用Numpy和PyTorch构建简单的线性回归模型。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="1f31" class="kc kd hi jy b fi ke kf l kg kh">#Create dummy dataset<br/>X = np.array([1,2,4,6,8,10,12,13,14,16,16,18,20,22,24])<br/>Y = np.array([39,42,43,46,47,56,60,59,64,66,68,72,71,75,80])</span></pre><p id="81a0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们所见，X和y之间存在线性关系(我们将在另一篇文章中讨论更多的相关性)。这里，我们将使用线性回归来建立预测模型。</p><p id="00cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">线性回归基础:</strong></p><p id="3cf4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Y = a*X+b是直线/线性回归模型的方程。</p><p id="3a87" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">目标是找到a和b的值。</p><p id="c76f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有多种技术可以实现这一点:</p><p id="4e36" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1.矩阵计算:将所有数据放入矩阵中进行优化。由于内存限制，用于小型数据集。</p><p id="64b3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.梯度下降:使用导数尽量减小实际值和预测值之间的误差/差异。</p><p id="b580" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.正则化:在最小化错误的同时，也要尽量减少不必要特征的影响。</p><p id="6df3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4.简单线性回归:如果有单个输入变量和单个输出变量，用协方差和方差求a和b。</p><p id="0273" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以上技术的更详细的解释不在这里的范围内。我们将实现方法2，即梯度下降，更具体地说，批量梯度下降。</p><p id="946f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">重量(a，b)在整个批次/所有行结束时更新，如下所示:</p><p id="69ec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">新a =旧a-(学习率*梯度a)</p><p id="e562" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">新b =旧b-(学习率*梯度b)</p><p id="39f6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">使用Numpy的线性回归:</strong></p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/e4cd4c964d4955d82f5f61192d1553c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TdWIA4nNVk5FooWHw8hwCQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">梯度计算</figcaption></figure><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="61eb" class="kc kd hi jy b fi ke kf l kg kh">np.random.seed(2)<br/>epochs=15<br/>learning_rate = 0.001<br/>w = np.random.randn()<br/>b = np.random.randn()<br/>y_pred = np.empty(len(Y))</span><span id="09bd" class="kc kd hi jy b fi kj kf l kg kh">for i in range(epochs):</span><span id="fcf7" class="kc kd hi jy b fi kj kf l kg kh">   print("-----------epoch:{}--------".format(i))<br/>   #prediction<br/>   y_pred = w*X +b<br/>   <br/>   #Error/loss calculation is Mean Squared Error<br/>   error = np.mean((Y - y_pred)**2)<br/>   print('Total Error:{}'.format(error))<br/>   <br/>   #Gradient calculation<br/>   gradient_a = np.mean(-2*X*(Y-y_pred))<br/>   gradient_b = np.mean(-2*(Y-y_pred))<br/>   <br/>   #Update weights<br/>   w -= learning_rate*gradient_a<br/>   b -= learning_rate*gradient_b</span></pre><p id="5985" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">误差随着历元的增加而减小。时期数和学习率是需要调整的超参数。</p><p id="92b0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们不要玩弄它，并跳转到PyTorch等效。</p><p id="a648" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">使用PyTorch的线性回归:</strong></p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="2c17" class="kc kd hi jy b fi ke kf l kg kh">#initialise data/features and target<br/>X_tensor = torch.from_numpy(X)<br/>Y_tensor = torch.from_numpy(Y)</span><span id="a285" class="kc kd hi jy b fi kj kf l kg kh">#Initialise weights<br/>'''Here unlike numpy we have to mention that these variables are trainable(need to calculate derivatives).This can be done using requires_grad'''<br/><br/>w_tensor = torch.randn(1,requires_grad=True,dtype=torch.float)<br/>b_tensor = torch.randn(1,requires_grad=True,dtype=torch.float)</span><span id="1ccb" class="kc kd hi jy b fi kj kf l kg kh">torch.random.seed = 2<br/>epochs=15<br/>learning_rate = 0.001</span></pre><p id="0243" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们将尝试在不借助内置PyTorch方法的情况下构建一个模型。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="6f83" class="kc kd hi jy b fi ke kf l kg kh">#Model without PyTorch in-built methods</span><span id="4935" class="kc kd hi jy b fi kj kf l kg kh">for i in range(epochs):</span><span id="3fb7" class="kc kd hi jy b fi kj kf l kg kh">  print("-----------epoch:{}--------".format(i))</span><span id="6737" class="kc kd hi jy b fi kj kf l kg kh">  #prediction<br/>  y_pred = w_tensor*X_tensor +b_tensor<br/>  <br/>  #Error/loss calculation is Mean Squared Error<br/>  error = ((Y_tensor - y_pred)**2).mean()<br/>  print('Total Error:{}'.format(error))</span><span id="04ea" class="kc kd hi jy b fi kj kf l kg kh">  '''Now no need to calculate gradients,PyTorch will do it if we     tell which function/variable needs gradient  calculation  using      backward()'''<br/>  error.backward()</span><span id="6f70" class="kc kd hi jy b fi kj kf l kg kh">  '''Actual values of gradients can be seen using grad attribute'''<br/>#print(w_tensor.grad,b_tensor.grad)</span><span id="7e8c" class="kc kd hi jy b fi kj kf l kg kh">  '''We can not directly use gradients in normal calculation,so use  no_grad() method to get variables out of scope of computation graph   '''<br/>  with torch.no_grad():<br/>      w_tensor-= learning_rate*w_tensor.grad <br/>      b_tensor-= learning_rate*b_tensor.grad</span><span id="66a7" class="kc kd hi jy b fi kj kf l kg kh">  #After each step,Reinitialize gradients because PyTorch holds on   to gradients and we need to ask it to release it.<br/>  w_tensor.grad.zero_()<br/>  b_tensor.grad.zero_()</span></pre><p id="e85e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们使用内置的PyTorch方法</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="bd85" class="kc kd hi jy b fi ke kf l kg kh">#Model with PyTorch in-built methods<br/>optimizer = torch.optim.SGD([w_tensor, b_tensor], lr=learning_rate)<br/>loss = torch.nn.MSELoss(reduction='mean')</span><span id="6a2d" class="kc kd hi jy b fi kj kf l kg kh">for i in range(epochs):<br/>   print("-----------epoch:{}--------".format(i))</span><span id="fa8e" class="kc kd hi jy b fi kj kf l kg kh">   #prediction<br/>   y_pred = w_tensor*X_tensor +b_tensor <br/>   <br/>   #Error/loss calculation is Mean Squared Error<br/>   error = loss(Y_tensor, y_pred)<br/>   print('Total Error:{}'.format(error))</span><span id="e45e" class="kc kd hi jy b fi kj kf l kg kh">   error.backward()</span><span id="7f68" class="kc kd hi jy b fi kj kf l kg kh"> #Update weights using Optimizer<br/>  optimizer.step()</span><span id="cc18" class="kc kd hi jy b fi kj kf l kg kh"> #After each step,Reinitialize gradients because PyTorch holds on to  gradients,reinitialize gradients using Optimizer</span><span id="3a6f" class="kc kd hi jy b fi kj kf l kg kh"> optimizer.zero_grad()</span></pre><p id="ace4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，我们已经探索了损失计算和优化。让我们也删除手动步骤。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="7e4f" class="kc kd hi jy b fi ke kf l kg kh">#Create Network by extending parent nn.Module.</span><span id="8f49" class="kc kd hi jy b fi kj kf l kg kh">'''We have to implement __init__ and forward methods '''</span><span id="dec0" class="kc kd hi jy b fi kj kf l kg kh">class Network(torch.nn.Module):<br/>    def __init__(self):<br/>       super().__init__()<br/>     #Intialise parameters whcih should be trained. Note that parameters need to be wrapped under nn.Parameter<br/>      self.w_tensor = torch.nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))<br/>      self.b_tensor = torch.nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))</span><span id="0829" class="kc kd hi jy b fi kj kf l kg kh">def forward(self,x):<br/>#Output prediction calculation<br/>return  w_tensor*x +b_tensor</span></pre><p id="8a7b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们使用这个网络进行培训:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="b142" class="kc kd hi jy b fi ke kf l kg kh">#Model with PyTorch in-built methods</span><span id="fb11" class="kc kd hi jy b fi kj kf l kg kh">model = Network()<br/>optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)<br/>loss = torch.nn.MSELoss(reduction='mean')</span><span id="7d6f" class="kc kd hi jy b fi kj kf l kg kh">for i in range(epochs):<br/>     print("-----------epoch:{}--------".format(i))</span><span id="2bfa" class="kc kd hi jy b fi kj kf l kg kh">    #This will not do actual training but will set model in training       mode.<br/>    model.train()</span><span id="66a1" class="kc kd hi jy b fi kj kf l kg kh">   #prediction<br/>    y_pred = model(X_tensor)</span><span id="47ad" class="kc kd hi jy b fi kj kf l kg kh">   #Error/loss calculation is Mean Squared Error<br/>    error = loss(Y_tensor, y_pred)<br/>    print('Total Error:{}'.format(error))</span><span id="2d18" class="kc kd hi jy b fi kj kf l kg kh">    '''Now no need to calculate gradients,PyTorch will do it if we  tell which function/variable needs gradient calculation using backward()'''<br/>    error.backward()<br/>   <br/>   #Update weights using Optimizer<br/>   optimizer.step()</span><span id="c2e7" class="kc kd hi jy b fi kj kf l kg kh">   #Reinitialize gradients using Optimizer<br/>   optimizer.zero_grad()</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><p id="17c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">总结:</strong></p><p id="aa66" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">总之，下面是PyTorch模型创建的步骤:</p><p id="296f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1.创建模型类，其中__init__()方法包含可训练参数，forward方法包含预测计算</p><p id="6fed" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.初始化优化器和损失函数</p><p id="d051" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.训练循环:</p><p id="3b5f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">model . train()-在训练模式下设置模型</p><p id="da44" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">pred =模型(X) —预测</p><p id="71a0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">损失=损失函数(预测，实际)-损失计算</p><p id="03fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">loss . backward()-梯度计算</p><p id="ce04" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">optimizer . step()-更新权重/参数</p><p id="e94b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">optimizer . zero _ grad()-重置渐变</p><p id="f318" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在<a class="ae iu" href="https://github.com/sarang0909/Explore-PyTorch/blob/master/PyTorch_Regression.ipynb" rel="noopener ugc nofollow" target="_blank"> git-repo </a>可获得完整的笔记本。</p><p id="79f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们知道如何在PyTorch中创建简单的模型。在下一篇文章中，我们将为PyTorch中的情感分析创建一个稍微复杂一点的神经网络。</p><p id="dbba" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你喜欢这篇文章或有任何建议/意见，请在下面分享！</p><p id="67f1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://www.linkedin.com/in/sarang-mete-6797065a/" rel="noopener ugc nofollow" target="_blank">领英</a></p></div></div>    
</body>
</html>