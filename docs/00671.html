<html>
<head>
<title>Expectation-Maximization Algorithm Step-by-Step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逐步期望最大化算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/expectation-maximization-algorithm-step-by-step-30157192de9f?source=collection_archive---------3-----------------------#2019-08-25">https://medium.com/analytics-vidhya/expectation-maximization-algorithm-step-by-step-30157192de9f?source=collection_archive---------3-----------------------#2019-08-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="d315" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">高斯混合模型、贝叶斯推理、硬聚类与软聚类</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/11b90ca8e4fba35983782bf9116ef224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*txFun6eAMI_WvGdeAeiJZQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="http://georgepavlides.info/expectation-maximization-gaussian-mixtures-vectorized-matlab-octave-approach/" rel="noopener ugc nofollow" target="_blank">赛德克</a></figcaption></figure><p id="5746" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">由于EM算法涉及到对贝叶斯推理框架(先验、似然和后验)的理解，我想在这篇文章中一步一步地介绍该算法，作为贝叶斯推理的回顾和应用示例。</p><h2 id="c155" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">问题设置</h2><p id="8172" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">假设我们在ℝᵈ有<strong class="jq hj"> n个训练数据点</strong>(即每个数据点x有d个属性)，我们知道它们是由一个<strong class="jq hj">高斯混合模型(GMM) </strong>生成的。现在我们的任务是估计最有可能产生这些数据点的GMM。</p><p id="a09c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这个问题的一个简单版本是假设数据点x1是由混合模型的一个高斯分量产生的。换句话说，它要么属于一个集群，要么不属于。这被认为是<strong class="jq hj">硬聚类</strong>。</p><p id="c88b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一个更柔和的版本，或者更精确的版本，是假设数据点x是由高斯分量以某种概率产生的。它被认为是<strong class="jq hj">软聚类</strong>，将是我演示的一个。</p><p id="67f9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因为我想练习用一般术语思考这些算法，所以我将抽象地设置这个问题。</p><p id="8e50" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">假设我们要估计的GMM有以下参数-</p><ol class=""><li id="89c8" class="lk ll hi jq b jr js ju jv jx lm kb ln kf lo kj lp lq lr ls bi translated"><strong class="jq hj"> K个分量</strong> — K个高斯分布在这个混合模型中，</li><li id="2f79" class="lk ll hi jq b jr lt ju lu jx lv kb lw kf lx kj lp lq lr ls bi translated">每个高斯j(并且j = 1，2，… K)由其自身的<strong class="jq hj"> μ </strong>和<strong class="jq hj"> 𝜎 </strong>定义，如下所示</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ly"><img src="../Images/0b628abcbb05563abc4370f8c8c3d2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bHQb0gseMsGivk698xvN0g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">注意:我没有放单位矩阵，但是这不应该影响我们如何理解它</figcaption></figure><p id="f410" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">请注意:</p><ul class=""><li id="c38a" class="lk ll hi jq b jr js ju jv jx lm kb ln kf lo kj lz lq lr ls bi translated">μ是ℝᵈ中的向量，就像随机变量x一样；</li><li id="dbc2" class="lk ll hi jq b jr lt ju lu jx lv kb lw kf lx kj lz lq lr ls bi translated">𝜎(以及𝜎)是一个定标员；</li><li id="eb56" class="lk ll hi jq b jr lt ju lu jx lv kb lw kf lx kj lz lq lr ls bi translated">⟦x-μ⟧表示向量范数，即这两个向量之间的距离，因此也是一个标量</li></ul><p id="c8a2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">3.每个高斯分量都有一个<strong class="jq hj">混合</strong> <strong class="jq hj">权重P </strong>，表示该分量的似然性。我们也可以将其视为生成数据点的第一步。我们首先掷出一个不公平的K边骰子，并确定我们将从哪个高斯分量中抽取数据点。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/720a984fdec789cf6a6c16f5c088b369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_-jpkcCo3_qHN2zcSwqJ4Q.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">组件j的“出现”遵循具有K个概率参数的多项式分布</figcaption></figure><p id="888f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">4.让我们把所有要估计的参数记为θ</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/6a3f07c8391886fdf1cbed714391eb82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M55kZw8u5YZ5Jxe56RNH0A.png"/></div></div></figure><h2 id="128b" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">最大似然估计</h2><p id="170b" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">本质上，最大似然估计寻找的是最大化观察我们所观察到的可能性的参数。在这种情况下，我们可以用公式表示如下所示的似然方程，最大化它将为我们提供每个高斯分量的最佳参数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mc"><img src="../Images/f3574e56df4a06c488d4be43bc643cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6KYcwm1yG5z1CMFsZTOXgg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">我们通常取这种可能性的对数，即对数可能性，因此乘积成为对数项的和。</figcaption></figure><p id="b11f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然而，考虑到要估计的参数数量，这可能非常复杂。因此，这种方法的替代方法是EM算法。</p><h2 id="a1e1" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">初始化</h2><p id="ad05" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">就像在k-means聚类中，我们为每个聚类初始化一个代表，我们需要初始化θ。</p><h2 id="2154" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">电子步骤(期望)</h2><p id="14df" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">给定所有定义(初始化)的参数，然后我们可以计算<strong class="jq hj">每个数据点I属于高斯分量j的后验概率</strong>，换句话说，给定所有观察到的数据点，每个分量的权重是多少？</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es md"><img src="../Images/284a4e288d02066d2eeb414294e0d39d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wsBogDYnElWCeeW9J_eG5Q.png"/></div></div></figure><p id="35ee" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">先验</strong>为P(j)，属于分量j的数据点的<strong class="jq hj">似然</strong>为P(Xn|j)。所有高斯分量的似然性之和，即观察到我们所观察到的事物的似然性，是我们之前定义的P(X |θ),并且是这个后验概率方程中的分母。</p><h2 id="81d7" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">m步(最大化)</h2><p id="37a1" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">在E-step中，我们估计了属于高斯分量j的每个数据点的后验概率。由于一个数据点可以属于多个聚类，因此它们也可以被视为软计数。</p><p id="96cc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这样，我们就可以重新估计所有的参数，使我们观察到的可能性最大化。</p><p id="b5db" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，我们想要在给定P(j|i)的情况下重新估计先验P(j)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/e81c8f04a7fd4b56e54ce4c28bd9589b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*svxZ_FAldnqP-CY7wA9F8w.png"/></div></div></figure><p id="3693" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">分子是我们的软计数；对于分量j，我们将所有数据点的“软计数”相加，即后验概率。</p><p id="960d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">接下来，我们将使用MLE来估计高斯分量参数μ <strong class="jq hj"> </strong>和𝜎。我们不是直接最大化下面的似然函数，而是最大化它的对数形式。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mc"><img src="../Images/f3574e56df4a06c488d4be43bc643cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6KYcwm1yG5z1CMFsZTOXgg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">这是我们之前指定的可能性函数</figcaption></figure><p id="d7b6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们将可能性的对数表示为l，它是所有观察到的数据点和参数的函数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/0866436f41d77fd1858446b5c909bce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Fok9cxG66-qJ9AHVt_yCA.png"/></div></div></figure><p id="1ca6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">注意，先验Pj被替换为我们刚刚基于后验概率P(j|i)估计的先验。</p><p id="c52b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果您分别根据μ <strong class="jq hj"> </strong>和𝜎对此方程求导，并求解两者，那么您将得出以下估计值，这些估计值很直观，因为它们可以被解读为某种形式的加权平均值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mg"><img src="../Images/7e0e481d35e5c84a8ae5cfbfeb46c859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ROA1NnrzQIlOysmkCnSXw.png"/></div></div></figure><h2 id="8aa4" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">循环</h2><p id="0f18" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">我们将用新估计的参数继续E步，然后M步，如此等等，直到结果收敛，即对数似然开始变平(不再显著增加)。</p><h2 id="fc5d" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">附加注释</h2><p id="20d0" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">应该注意，EM算法仅保证局部最优。换句话说，不同的初始化参数可能导致不同的最优值。因此，带有一些洞察力的初始化可以帮助避免不太喜欢的最佳结果。例如，我们可以使用k-means来决定代表μs，并使用全局变量作为开始𝜎。</p></div></div>    
</body>
</html>