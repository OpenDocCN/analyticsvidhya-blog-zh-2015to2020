<html>
<head>
<title>8 Excellent Pretrained Models to get you Started with Natural Language Processing (NLP)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">8个优秀的预训练模型，帮助您开始自然语言处理(NLP)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/8-pretrained-models-to-learn-natural-language-processing-nlp-5f14d82e9621?source=collection_archive---------2-----------------------#2019-03-18">https://medium.com/analytics-vidhya/8-pretrained-models-to-learn-natural-language-processing-nlp-5f14d82e9621?source=collection_archive---------2-----------------------#2019-03-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/139c9a0dd58dcdbe7043917404d65531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Cr3DyCxJxz7eCwYV.png"/></div></div></figure><p id="5116" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如今，自然语言处理(NLP)应用程序已经变得无处不在。我似乎经常偶然发现以这样或那样的形式利用NLP的网站和应用程序。简而言之，这是参与NLP领域的绝佳时机。</p><p id="ee27" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">NLP采用率的快速增长很大程度上要归功于通过预训练模型实现的<a class="ae jo" href="https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/?utm_source=blog&amp;utm_medium=top-pretrained-models-nlp-article" rel="noopener ugc nofollow" target="_blank">迁移学习概念。在NLP的环境中，迁移学习本质上是在一个数据集上训练模型，然后调整该模型以在不同的数据集上执行不同的NLP功能的能力。</a></p><p id="d0dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这一突破使事情变得非常容易和简单，尤其是对于那些没有时间和资源从头开始构建NLP模型的人。对于想要学习或过渡到NLP的初学者来说，它也是完美的。</p><h1 id="a9e6" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">为什么使用预训练模型？</h1><ul class=""><li id="6e65" class="kn ko hi is b it kp ix kq jb kr jf ks jj kt jn ku kv kw kx bi translated">作者已经努力为您设计了一个基准模型！我们可以在自己的NLP数据集上使用预先训练的模型，而不是从头开始构建模型来解决类似的NLP问题</li><li id="78fe" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">需要进行一些微调，但这可以节省我们大量的时间和计算资源</li></ul><p id="f749" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本文中，我展示了一些顶级的预训练模型，您可以使用它们开始您的NLP之旅，并复制该领域的最新研究成果。你可以点击这里查看我关于计算机视觉<a class="ae jo" href="https://www.analyticsvidhya.com/blog/2018/07/top-10-pretrained-models-get-started-deep-learning-part-1-computer-vision/?utm_source=blog&amp;utm_medium=top-pretrained-models-nlp-article" rel="noopener ugc nofollow" target="_blank">顶级预训练模型的文章。</a></p><p id="ac20" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ld">如果你是NLP初学者，我推荐参加我们的热门课程——‘使用Python的</em><em class="ld"/><a class="ae jo" href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&amp;utm_medium=top-pretrained-models-nlp-article" rel="noopener ugc nofollow" target="_blank"><em class="ld">NLP’。</em></a></p><h1 id="45c5" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">本文涉及的预训练NLP模型</h1><p id="6645" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">我根据应用将预训练模型分为三个不同的类别:</p><ul class=""><li id="06ad" class="kn ko hi is b it iu ix iy jb lh jf li jj lj jn ku kv kw kx bi translated"><strong class="is hj">多用途NLP模型</strong></li><li id="c91e" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">乌尔菲特</li><li id="bfa7" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">变压器</li><li id="069c" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">谷歌的伯特</li><li id="c00c" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">变压器-XL</li><li id="8315" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">OpenAI的GPT-2</li><li id="0c61" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><strong class="is hj">单词嵌入</strong></li><li id="120a" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">工程与后勤管理局</li><li id="3698" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">天资</li><li id="4615" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><strong class="is hj">其他预训练模型</strong></li><li id="e302" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">斯坦福大学</li></ul><h1 id="f82c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">多用途NLP模型</h1><p id="fc4c" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">多用途模型是NLP世界的话题。这些模型推动了我们兴奋不已的自然语言处理应用——机器翻译、问答系统、聊天机器人、情感分析等。这些多用途NLP模型的一个核心组成部分是语言建模的概念。</p><p id="dc82" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">简单地说，语言模型的目的是预测序列中的下一个单词或字符。当我们看到这里的每个模型时，我们就会明白这一点。</p><p id="eb97" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你是一个NLP爱好者，你会喜欢这个部分。现在，让我们深入研究5个最先进的多用途NLP模型框架。我提供了研究论文的链接和每个模型的预训练模型。去探索它们吧！</p><h1 id="3be7" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><a class="ae jo" href="https://github.com/fastai/fastai/tree/master/courses/dl2/imdb_scripts" rel="noopener ugc nofollow" target="_blank">乌尔姆菲特</a></h1><p id="ff80" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">ULMFiT是由fast.ai的<a class="lk ll ge" href="https://medium.com/u/34ab754f8c5e?source=post_page-----5f14d82e9621--------------------------------" rel="noopener" target="_blank">杰瑞米·霍华德</a> DeepMind的<a class="lk ll ge" href="https://medium.com/u/e3999e445181?source=post_page-----5f14d82e9621--------------------------------" rel="noopener" target="_blank">塞巴斯蒂安鲁德</a>提出并设计的。你可以说ULMFiT是去年启动迁移学习聚会的版本。</p><p id="9cb8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们在本文的<a class="ae jo" href="https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/" rel="noopener ugc nofollow" target="_blank">中所介绍的，ULMFiT使用新颖的NLP技术实现了最先进的结果。这种方法包括将在</a><a class="ae jo" href="https://einstein.ai/research/the-wikitext-long-term-dependency-language-modeling-dataset" rel="noopener ugc nofollow" target="_blank"> Wikitext 103数据集</a>上训练的预训练语言模型微调到新的数据集，以使其不会忘记之前学习的内容。</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/35943354b9a166c5415f83d5bc3daaf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*qS1xdXJeJj5EbLNl.png"/></div></figure><p id="9f9d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ULMFiT在文本分类任务上优于许多先进的技术。我喜欢ULMFiT的一点是，它只需要很少的例子就能产生这些令人印象深刻的结果。让像你我这样的人更容易理解并在我们的机器上实现它！</p><p id="dc52" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你想知道，ULMFiT代表通用语言模型微调。“通用”这个词用在这里很恰当——这个框架可以应用于几乎所有的NLP任务。</p><h1 id="4f39" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">了解和阅读有关ULMFiT更多信息的资源:</h1><ul class=""><li id="a773" class="kn ko hi is b it kp ix kq jb kr jf ks jj kt jn ku kv kw kx bi translated"><a class="ae jo" href="https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/?utm_source=blog&amp;utm_medium=top-pretrained-models-nlp-article" rel="noopener ugc nofollow" target="_blank">使用Python中的ULMFiT和fastai库进行文本分类(NLP)的教程</a></li><li id="edd8" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://www.paperswithcode.com/paper/universal-language-model-fine-tuning-for-text" rel="noopener ugc nofollow" target="_blank">ul mfit的预训练模型</a></li><li id="e93c" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">研究论文</a></li></ul><h1 id="832d" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><a class="ae jo" href="https://github.com/tensorflow/models/tree/master/official/transformer" rel="noopener ugc nofollow" target="_blank">变压器</a></h1><p id="3a04" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">变压器架构是NLP中几乎所有最新主要发展的核心。它是由谷歌在2017年推出的。当时，递归神经网络(RNN)被用于语言任务，如机器翻译和问答系统。</p><p id="7663" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种变压器架构优于RNNs和CNN(卷积神经网络)。训练模型所需的计算资源也减少了。对NLP的每个人都是双赢。看看下面的对比:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/5d4a54490177430bd811b28abae8dc05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*Mp8nZe0Gh1_636dO.png"/></div></figure><p id="8277" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据谷歌的说法，Transformer“应用了一种自我关注机制，直接模拟句子中所有单词之间的关系，而不管它们各自的位置”。它使用固定大小的上下文(也就是前面的单词)来实现。复杂到拿不到？我们举个例子来简化一下这个。</p><p id="1aa6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">"她在河岸上发现了贝壳。"模型需要明白，这里的“银行”指的是岸，而不是金融机构。Transformer一步就明白了这一点。我鼓励你阅读我下面链接的全文，以了解这是如何工作的。它会让你大吃一惊。</p><p id="909c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的动画精彩地展示了Transformer如何处理机器翻译任务:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/4426dfb0bd6fb7ce882ebf3419a35d61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*o-B1QliYdHU7KWHr.gif"/></div></figure><p id="4f5e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">谷歌去年发布了名为<a class="ae jo" href="https://arxiv.org/pdf/1807.03819v3.pdf" rel="noopener ugc nofollow" target="_blank">通用变形金刚</a>的变形金刚改良版。还有一个更新更直观的版本，叫做Transformer-XL，我们将在下面介绍。</p><h1 id="e5bd" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">学习和阅读更多关于Transformer的资源:</h1><ul class=""><li id="f1ff" class="kn ko hi is b it kp ix kq jb kr jf ks jj kt jn ku kv kw kx bi translated"><a class="ae jo" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank">谷歌官方博文</a></li><li id="d241" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://www.paperswithcode.com/paper/attention-is-all-you-need" rel="noopener ugc nofollow" target="_blank">变压器的预训练模型</a></li><li id="80d1" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">研究论文</a></li></ul><h1 id="310f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><a class="ae jo" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">谷歌的伯特</a></h1><p id="5b69" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">自从Google发布了他们的结果，然后开源了背后的代码，BERT框架就一直在掀起波澜。我们可以争论这是否标志着“NLP的新时代”，但是毫无疑问，BERT是一个非常有用的框架，可以很好地概括各种NLP任务。</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/e885bb7039dd89f96ae4846600bfdf99.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*bpwuCfllB26OPYxo.png"/></div></figure><p id="de13" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">伯特(BERT)是一个词的两个方面(左和右)的缩写，它考虑的是一个词的上下文。以前所有的努力都是一次考虑一个单词的一边——要么是左边，要么是右边。这种双向性有助于模型更好地理解使用单词的上下文。此外，BERT被设计为进行多任务学习，也就是说，它可以同时执行不同的NLP任务。</p><p id="30dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">BERT是第一个用于预训练NLP模型的无监督、深度双向系统。它仅使用纯文本语料库进行训练。</p><p id="02be" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在它发布的时候，BERT在11个自然语言处理(NLP)任务上产生了最先进的结果。相当大的壮举！只需几个小时(在单个GPU上)，就可以使用BERT训练出自己的NLP模型(比如问答系统)。</p><h1 id="9c61" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">学习和阅读更多关于BERT的资源:</h1><ul class=""><li id="c8f4" class="kn ko hi is b it kp ix kq jb kr jf ks jj kt jn ku kv kw kx bi translated"><a class="ae jo" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">谷歌官方博文</a></li><li id="69d7" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">【BERT的预训练模型</li><li id="391b" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a></li></ul><h1 id="a280" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><a class="ae jo" href="https://github.com/kimiyoung/transformer-xl" rel="noopener ugc nofollow" target="_blank">谷歌的Transformer-XL </a></h1><p id="772c" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">从长期来看，谷歌的这次发布可能是NLP非常重要的一次发布。如果你是初学者，这个概念可能会变得有点棘手，所以我鼓励你多读几遍来掌握它。我还在本节下面提供了多种资源来帮助您开始使用Transformer-XL。</p><p id="1dcd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">想象一下，你正在看一本书，突然一个单词或句子出现在书的开头。现在，你或我都可以回忆起那是什么。但可以理解的是，一台机器很难对长期依赖进行建模。</p><p id="dd59" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如上所述，一种方法是使用变压器。但是它们是用固定长度的上下文实现的。换句话说，如果使用这种方法，就没有太多的灵活性。</p><p id="556f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Transformer-XL很好地弥合了这一差距。由谷歌人工智能团队开发，它是一种新颖的<a class="ae jo" href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&amp;utm_medium=top-pretrained-models-nlp-article" rel="noopener ugc nofollow" target="_blank"> NLP </a>架构，帮助机器理解超出固定长度限制的上下文。<strong class="is hj"> Transformer-XL比典型变压器快1800倍。</strong></p><p id="ae1b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过谷歌发布的以下两张gif图，你会明白这种区别:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/858177c761befce2b30379d3e59c75b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*ZUk_USvqM1M7EZ7r.gif"/></div></figure><p id="2384" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ld">普通变压器</em></p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/550df0c653b6f9255411b1b8596da5cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*n9_fjS2UDBPYyH90.gif"/></div></figure><p id="4ca3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ld"> Transformer-XL </em></p><p id="1119" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Transformer-XL，正如您现在可能已经预测到的，在各种语言建模基准/数据集上实现了新的最先进的结果。这里有一个取自他们页面的小表格来说明这一点:</p><p id="5608" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">方法enwiki8 text8十亿字WT-103 PTB (w/o finetuning)此前最好1.06 1.13 23.7 20.5 55.5 Transformer-XL<strong class="is hj">0.99</strong><strong class="is hj">1.08</strong><strong class="is hj">21.8</strong><strong class="is hj">18.3</strong><strong class="is hj">54.5</strong></p><p id="4ab8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Transformer-XL GitHub存储库，上面有链接，下面会提到，包含PyTorch和TensorFlow中的代码。</p><h1 id="836b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">学习和阅读更多关于Transformer-XL的资源:</h1><ul class=""><li id="33b7" class="kn ko hi is b it kp ix kq jb kr jf ks jj kt jn ku kv kw kx bi translated"><a class="ae jo" href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html" rel="noopener ugc nofollow" target="_blank">谷歌官方博文</a></li><li id="4b71" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://www.paperswithcode.com/paper/transformer-xl-attentive-language-models" rel="noopener ugc nofollow" target="_blank">变压器-XL的预调模型</a></li><li id="a8a8" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://arxiv.org/abs/1901.02860" rel="noopener ugc nofollow" target="_blank">研究论文</a></li></ul><h1 id="12c4" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><a class="ae jo" href="https://github.com/openai/gpt-2" rel="noopener ugc nofollow" target="_blank"> OpenAI的GPT-2 </a></h1><p id="6d19" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">现在，这是一个非常有争议的条目。一些人可能会认为GPT-2的发布是OpenAI的一个营销噱头。我当然明白他们从哪里来。然而，我认为至少尝试一下OpenAI已经发布的代码是很重要的。</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/c612c7867bb8e32196365f6824849483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_6T2SJyxVPJG_AbQ.png"/></div></div></figure><p id="afbe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，给那些不知道我在说什么的人一些背景。OpenAI在2月份写了一篇博文(链接如下)，他们声称已经设计了一个名为GPT-2的NLP模型，这个模型非常好，以至于他们因为担心恶意使用而无法发布完整版本。这当然引起了社区的关注。</p><p id="5a5e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">GPT-2被训练来预测40GB互联网文本数据中的下一个出现的单词。这个框架也是一个基于transformer的模型，在800万个网页的数据集上进行训练。他们在网站上公布的结果令人震惊。该模型能够根据我们输入的几句话编织一个完全清晰的故事。看看这个例子:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/4a343b2cc33c6e699aca461e4db41e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Nl_cyLy35BN0CXeA.png"/></div></div></figure><p id="5e62" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">不可思议吧。</p><p id="0cf2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">开发人员发布了一个小得多的GPT-2版本，供研究人员和工程师测试。原始模型有15亿个参数——开源样本模型有1.17亿个。</p><h1 id="4c4e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">了解和阅读更多关于GPT-2的资源:</h1><ul class=""><li id="574e" class="kn ko hi is b it kp ix kq jb kr jf ks jj kt jn ku kv kw kx bi translated"><a class="ae jo" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> OpenAI官方博文</a></li><li id="735d" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://github.com/openai/gpt-2" rel="noopener ugc nofollow" target="_blank">GPT-2的预训练模型</a></li><li id="2b79" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a></li></ul><h1 id="f97c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">单词嵌入</h1><p id="4852" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">我们使用的大多数机器学习和深度学习算法都无法直接处理字符串和纯文本。这些技术要求我们将文本数据转换成数字，然后才能执行任何任务(例如回归或分类)。</p><p id="cf72" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，简单地说，单词嵌入是被转换成数字以执行NLP任务的文本块。单词基本格式通常尝试使用字典将单词映射到向量。</p><p id="fbdd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在下一篇文章中，你可以更深入地了解单词嵌入，它的不同类型，以及如何在数据集上使用它们。如果你不熟悉这个概念，我认为这个指南是必读的:</p><p id="5807" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这一节中，我们将看看NLP的两种最先进的单词嵌入。我还提供了教程链接，这样你可以对每个主题有一个实际的理解。</p><h1 id="600a" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">埃尔莫</h1><p id="a688" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">不，这个埃尔莫不是《芝麻街》中那个(公认的令人敬畏的)角色。但是这个ELMo，语言模型嵌入的缩写，在构建NLP模型的上下文中非常有用。</p><p id="e988" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ELMo是一种在向量和嵌入中表示单词的新方法。这些ELMo单词嵌入帮助我们在多个NLP任务上实现了最先进的结果，如下所示:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/885512b0e4a93c02557d0817589b488c.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/0*cZ_5a51FZ-ygDfCN.png"/></div></figure><p id="ebf3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们花一点时间来理解ELMo是如何工作的。回想一下我们之前讨论的双向语言模型。根据这篇文章的提示，“ELMo单词向量是在两层双向语言模型(biLM)之上计算的。这个biLM模型有两层堆叠在一起。每层有两个通道—向前通道和向后通道:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/8db6955131df6020e4c88abdc5933efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yNYEqPrGs41f8P8A.gif"/></div></div></figure><p id="2a07" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ELMo单词表示考虑完整的输入句子来计算单词嵌入。因此，术语“读”在不同的上下文中具有不同的ELMo向量。这与旧的单词嵌入大相径庭，在旧的单词嵌入中，无论单词“read”在什么上下文中使用，都会被赋予相同的向量。</p><h1 id="65bc" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">学习和阅读更多关于ELMo的资源:</h1><ul class=""><li id="ac35" class="kn ko hi is b it kp ix kq jb kr jf ks jj kt jn ku kv kw kx bi translated"><a class="ae jo" href="https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/?utm_source=blog&amp;utm_medium=top-pretrained-models-nlp-article" rel="noopener ugc nofollow" target="_blank">学习ELMo从文本中提取特征的分步NLP指南</a></li><li id="7e9a" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md" rel="noopener ugc nofollow" target="_blank">预训练模型的GitHub库</a></li><li id="cd74" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://arxiv.org/pdf/1802.05365.pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a></li></ul><h1 id="0a43" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><a class="ae jo" href="https://github.com/zalandoresearch/flair" rel="noopener ugc nofollow" target="_blank">天赋</a></h1><p id="812c" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">Flair不完全是单词嵌入，而是单词嵌入的组合。我们可以称Flair更像是一个结合了GloVe、BERT、ELMo等嵌入的<a class="ae jo" href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&amp;utm_medium=top-pretrained-models-nlp-article" rel="noopener ugc nofollow" target="_blank"> NLP </a>库。Zalando Research的优秀人员开发并开源了Flair。</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/e0ca99a24bef76c6fea0b8b407664e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4BcbX24fjK9Eikvm.png"/></div></div></figure><p id="689a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该团队已经为以下NLP任务发布了几个预训练模型:</p><ul class=""><li id="9d9b" class="kn ko hi is b it iu ix iy jb lh jf li jj lj jn ku kv kw kx bi translated"><strong class="is hj">名称-实体识别(NER) </strong></li><li id="8b22" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><strong class="is hj">词性标注</strong></li><li id="3f86" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><strong class="is hj">文本分类</strong></li><li id="0128" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><strong class="is hj">培训定制车型</strong></li></ul><p id="c250" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">还不信服？嗯，这张对照表可以帮你找到答案:</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/ece3bc5cec787fed9a30ee5c2b009239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WJsgo9ei2Oqb5Ej5.png"/></div></div></figure><p id="2b40" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“Flair嵌入”是Flair库中打包的签名嵌入。它由上下文字符串嵌入提供支持。你应该通读一下这篇文章,了解增强能力的核心要素。</p><p id="f2ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我特别喜欢Flair的一点是它支持多种语言。所以许多NLP版本都停留在英文任务上。如果NLP要在全球范围内获得关注，我们需要扩大范围！</p><h1 id="7ca3" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">了解和阅读更多关于Flair的资源:</h1><ul class=""><li id="d3bd" class="kn ko hi is b it kp ix kq jb kr jf ks jj kt jn ku kv kw kx bi translated"><a class="ae jo" href="https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/?utm_source=blog&amp;utm_medium=top-pretrained-models-nlp-article" rel="noopener ugc nofollow" target="_blank">NLP天赋介绍:一个简单而强大的最先进的NLP库</a></li><li id="db7a" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated"><a class="ae jo" href="https://github.com/zalandoresearch/flair" rel="noopener ugc nofollow" target="_blank">用于天赋的预训练模型</a></li></ul><h1 id="4b2b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">其他预训练模型</h1><h1 id="efe0" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><a class="ae jo" href="https://github.com/stanfordnlp/stanfordnlp" rel="noopener ugc nofollow" target="_blank">斯坦福大学</a></h1><p id="a635" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">说到将NLP扩展到英语之外，这里有一个已经在设定基准的库。作者声称StanfordNLP支持超过53种语言——这当然引起了我们的注意！</p><figure class="ln lo lp lq fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/3ba116b97ea3022c172f7f9327496890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*kvvFcc0vFUXI5c14.jpg"/></div></figure><p id="19f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的团队是首批与该库合作并在真实世界数据集上发布结果的团队之一。我们尝试了一下，发现StanfordNLP确实为在非英语语言上应用NLP技术开辟了许多可能性。比如印地语，汉语，日语。</p><p id="97c4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> StanfordNLP是一组经过预训练的最先进的NLP模型。</strong>这些模型不只是经过实验室测试——它们被作者用于2017年和2018年的<a class="ae jo" href="http://www.conll.org/2019" rel="noopener ugc nofollow" target="_blank"> CoNLL </a>比赛。StanfordNLP中打包的所有预训练的NLP模型都是在PyTorch上构建的，可以根据自己的带注释的数据进行训练和评估。</p><p id="adfa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们认为您应该考虑StanfordNLP的两个主要原因是:</p><ul class=""><li id="edef" class="kn ko hi is b it iu ix iy jb lh jf li jj lj jn ku kv kw kx bi translated">用于执行文本分析的全神经网络管道，包括:</li><li id="6f7f" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">标记化</li><li id="bbf0" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">多字令牌(MWT)扩展</li><li id="9f31" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">词汇化</li><li id="a9b8" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">词性和形态特征标注</li><li id="2745" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">依存句法分析</li><li id="b0cf" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">一个稳定的官方维护的Python接口到<a class="ae jo" href="https://www.analyticsvidhya.com/blog/2017/12/introduction-computational-linguistics-dependency-trees/?utm_source=blog&amp;utm_medium=stanfordnlp-nlp-library-python" rel="noopener ugc nofollow" target="_blank"> CoreNLP </a></li></ul><h1 id="bcbd" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">了解和阅读更多关于StanfordNLP的资源:</h1><ul class=""><li id="daed" class="kn ko hi is b it kp ix kq jb kr jf ks jj kt jn ku kv kw kx bi translated"><a class="ae jo" href="https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/?utm_source=blog&amp;utm_medium=top-pretrained-models-nlp-article" rel="noopener ugc nofollow" target="_blank">Stanford NLP简介:一个令人难以置信的53种语言的一流NLP库(带Python代码)</a></li><li id="3b48" class="kn ko hi is b it ky ix kz jb la jf lb jj lc jn ku kv kw kx bi translated">【StanfordNLP的预训练模型</li></ul><h1 id="3e93" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结束注释</h1><p id="e8b8" class="pw-post-body-paragraph iq ir hi is b it kp iv iw ix kq iz ja jb le jd je jf lf jh ji jj lg jl jm jn hb bi translated">这绝不是预训练NLP模型的详尽列表。还有很多可用的，你可以在<a class="ae jo" href="https://paperswithcode.com/" rel="noopener ugc nofollow" target="_blank">这个网站</a>上查看其中的一些。</p><p id="9a03" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里有一些学习NLP的有用资源:</p><p id="f3c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我很想听听你对这个列表的看法。你以前用过这些预训练模型吗？或者你可能已经探索了其他的选择？请在下面的评论区告诉我——我很乐意查看它们并将它们添加到这个列表中。</p></div><div class="ab cl lz ma gp mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hb hc hd he hf"><p id="fbbf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ld">原载于2019年3月18日</em><a class="ae jo" href="https://www.analyticsvidhya.com/blog/2019/03/pretrained-models-get-started-nlp/" rel="noopener ugc nofollow" target="_blank"><em class="ld">www.analyticsvidhya.com</em></a><em class="ld">。</em></p></div></div>    
</body>
</html>